I haven't considered Ocaml seriously, no. I'm not aware of any great disadvantages... but I just don't see a compelling case for that over Haskell, and I'm personally a lot less familiar with the community and libraries.
I haven't looked at Helium myself, but ya, you'd probably lose gloss, which is of course too much to lose.
Yes! Came here to just say that.
&gt; Arguably Haskell's type system gives too much rope. The compiler's error messages can be unfriendly because of it. But that's a prevalent problem for every language commonly used in first programming classes, if recent research is to be believed. Yes, but type errors usually become *much* more understandable and less obtuse the more type annotations you give to GHC. A simple type declaration for each top-level definition goes a long way.
Maybe you can make an alternative Prelude with specialized types? [Int] vs [Char] is much easier to decipher than typeclass errors. Maybe there's a way to turn off overloaded literal ints too?
We could/should have BeginnerPrelude with map specialized, and Prelude with map=fmap.
My problem with replacing bits of the Prelude, though, is that the programming language is one that people use to build software all the time, and you'd need only replace gloss with something more complex like SDL or GLW or Gtk2Hs. Even gloss, while it's designed for education, is a library that's generally available and installed from somewhere not related to this course. I feel like it's important, for motivation reasons, to be teaching something more than "how to use the library that I've carefully prepared for this course". Replacing fundamental bits like numeric literals and operators does rather tend to break that illusion a bit. It starts to feel like I don't trust them with a real language.
That's why I emphasized that you don't have to be teaching them *Haskell* proper, if you don't want to. The approach in question is really defining a smaller language within Haskell--an EDSL for teaching introductory programming, in essence. I don't like this approach as a "stepping stone" to the "real" language, which is why I'm unenthused about a "beginner prelude" in general, but as a general intro to programming I think it's fine; inventing languages for teaching purposes has a long and respectable history.
No - I don't think that.
Give you a break? What do you mean by that? F#'s popularity is currently rocketing, according to Tiobe and is now in the top 20 languages. I'm not sure how accurate that is, but here's the graph: http://www.tiobe.com/index.php/paperinfo/tpci/F_.html Also it's notable that, this month, "functional languages" (including LISP) account for 4.9% popularity (according to Tiobe). In 2008 it was 1.4%, and a similar ratio for Haskell over that period. http://web.archive.org/web/20080302025522/http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html 
Agreed. Comming from C/++ where some errors are like another language you need to learn to speak, GHC's errors are very clean and easy.
It's still [alive](http://www.haskell.org/pipermail/glasgow-haskell-users/2011-August/020658.html) and the project can be found [here](https://github.com/sviperll/ghcjs)
As someone who has been looking into this for some time, I know that many people want it, many people are trying it, and I don't think anyone has made a complete, stable implementation yet. * [ghcjs](https://github.com/sviperll/ghcjs) — Alpha. Works. Incomplete. Nicely designed. Will be maintained around September. Compiles most pure Haskell libraries no problem. FFI to JS works, and the author, sviperll is a helpful guy. * [lambdascript](https://github.com/valderman/lambdascript/) — Alpha. Not Haskell. No polymorphism. Bad syntax (Haskell-ish, but not quite, should use the haskell-src-exts parser—will do in the future). Maintained. Nice effort. * [hjscript](http://hackage.haskell.org/package/HJScript) — Beta. EDSL, not Haskell→JS. Works. Not very annoying to program in, but is JS semantics, not Haskell. * [UHC JS runtime](https://github.com/atzedijkstra/javascript-runtime-for-UHC), [blog post here](http://utrechthaskellcompiler.wordpress.com/2010/10/18/haskell-to-javascript-backend/) — Alpha, only works for UHC, but promising. UHC compiles enough of Hackage to be very useful. Doesn't produce an explosion of code, seemingly. * [Haskell interpreter in JS](https://github.com/johang88/haskellinjavascript) — An interpreter. Haven't tried but is apparently dead. * [YHC JS backend](http://www.haskell.org/haskellwiki/Yhc/Javascript) — Beta-ish. Apparently works, but I was unable to compile YHC, so haven't tried yet. I would be interested in anyone's experience using it. * [Emscripten](https://github.com/kripken/emscripten) — not Haskell→JS, but compiles LLVM/Clang output to JavaScript. Could possibly be used for GHC→LLVM→JS compiling, which I tried, and works, but would have to also compile the GHC runtime which is not straight-forward (to me) for it to actually run. * I've also tried writing a Haskell→JS compiler to make a more direct JS-aware translation of code (to not have huge code output a la GHCJS, YHC, Emscripten), but will prefer to avoid NIH and hack on GHCJS, and the UHC backend looks to be doing what I described, so I hold out hope for that, too. I will probably hack on GHCJs or UHC's backend at CamHac. I'd be interested in anyone else sharing their experience with compiling Haskell→JS, which implementations they tried, how far they got, drawbacks, future plans.
You took a joke post and provided a lot of useful information. Have all the upvotes I can give.
I think the greatest problem with GHCJS is built into (hardly splittable) GHC desugaring, I've been [discussing](http://www.haskell.org/pipermail/glasgow-haskell-users/2011-August/020658.html) it on GHC mailing list for some time.
Also the [AwesomePrelude](https://github.com/tomlokhorst/AwesomePrelude) has a JavaScript backend (and a C# backend): https://github.com/tomlokhorst/AwesomePrelude/tree/master/src/Lang
I don't really understand what "desugaring" means, which makes the email thread tough going. But it seems like you are getting a lot of responses from top GHC folks.
Desugaring in general means stripping out syntactic sugar (hence the term) and replacing it with primitive operations. For instance, something like `do { x &lt;- getLine; putStrLn x; return x; }` desugars in Haskell to roughly `getLine &gt;&gt;= (\x -&gt; putStrLn x &gt;&gt; return x)`. This is in the context of GHC's compilation process, though, so I suspect this means the much stronger idea GHC has of "desugaring", which considers most of Haskell to be syntactic sugar for an intermediate representation GHC uses internally. I *think* GHC Core is the output of the desugaring step, but I'm not certain.
Did you know about [RWST](http://hackage.haskell.org/packages/archive/transformers/0.2.2.0/doc/html/Control-Monad-Trans-RWS-Lazy.html)? I'm not saying you can definitely use it -- I see you have an ErrorT in there between the R and W, and I haven't done the mental gymnastics required to tell whether that can be pulled out -- just that it's not well known, so it's possible you missed this useful conglomeration monad transformer.
More information on using Haskell to create an "executable" specification can be found here: [Running the manual: an approach to high-assurance microkernel development](http://www.cse.unsw.edu.au/~chak/papers/sel4-model.pdf) (also a PDF link). EDIT: I just realized that [there is a self post related to this](http://www.reddit.com/r/compsci/comments/j9ez0/i_have_questions_about_automatic_theorem_proving/) on the compsci subreddit. Entirely coincidental! But there's more discussion there.
It's been a week so I doubt many will see this comment but I thought I'd post it anyway. Why is offering oneself as a mod so controversial? I see that my suggestion is sitting at zero and I was actually expecting worse. This isn't the first time that this has happened to me and so I'm just curious about why this is. Because I would make a great mod. I would do little more than fish stuff out of the spam filter and remove blatantly spammy stuff. But it doesn't matter. I thought I would offer, that offer was rejected and that's fine. I just don't see why it's such a sore point to make an offer.
Unfortunately, I believe YHC is dead, so that probably isn't a very productive avenue.
In 10 to 20 years, yes.
Very nice.
Just steer every beginner to: [Learn You a Haskell for Great Good](http://learnyouahaskell.com/) [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) [Real World Haskell](http://book.realworldhaskell.org/read/) Done. That's my meta-tutorial.
This is a cool project. Please don't let it die like so many other cool projects. &lt;3
I work on this project, although I only started semi-recently. If anyone has any questions, I'm happy to answer them to the best of my ability.
Waiting a month or so till the 7.2 release stabilises. Thanks for the reminder -- I'll update the schedule.
So is 2011.4.0 going to be GHC-7.2 based already?
Hmm.. compare x y = LT &lt;== x &lt; y ! GT &lt;== x &gt; y ! EQ 
I do have a question, albeit a practical one. Is the code for seL4 -- by which I mean both the final C code, the Haskell specification, the Isabelle code, and the process used to get from one step to the other -- available anywhere, or do you need to be a researcher? I see NICTA has ARM binaries available for download, but that doesn't seem terribly interesting.
No, our entire code base is not available generally, however if you have a research interest we may be able to arrange access for you. You should probably contact Gerwin Klein or other higher-ups if you are interested. 
I do have interest, but not enough to trouble you guys with. I'm mostly interested in formal verification of system-level code and how it's done, practically. Most of the other implementations of L4 are free software, which is great. I'm kind of bummed that as a hobbyist I don't really have the opportunity to analyze the code, but I suppose that's your prerogative. I really enjoyed digging through CompCert, which is freely available, for example. Too bad about seL4.
Indeed he hits the problem point here. In many cases you don't need the flexibility of C++ and it's performance and in fact you need more restrictiveness. In fact C# + F# tries to achieve this up to a certain point. And even C++ with Haskell is possible, but it still is not really mature.
If you have any specific questions as to how we approach it, I'm happy to answer them :) Sorry I can't arrange any code-access though, I'm pretty sure it's a natural consequence of NICTA's IP policy rather than anything the ERTOS group decided.
I'd quite like to see what practical applications Carmack would come up with.
Quoted for truth and deaf guys: “It's not because people are no good. The very best programmers always make mistakes. This is something that I've really internalized; that, no matter how good you think you are, you are making mistakes all the time, and you have to have structures around you to try and help you limit the damage that your mistakes will cause, find them as early as possible, so that you can correct them, and the earliest possible time is at compile time. So I'm all about trying to be much more restrictive on what we can do, here. And on the one hand, I would entertain programming in—I'm very tempted to want to move to—a functional programming language; start programming in Haskell or OCaml or something, but that's not a credible thing to do in the game industry.”
you left off the bit where he lists his reasons for why he thinks it's not credible, he cites educating people in FP and hiring people for FP jobs is an untenable position, loosely speaking.
Comment made around 1:10:00 in the context of his "conversion" to rigorous static analysis and wanting to get away from scripting elements via a highly restricted subset of C++ or a functional language.
docs coming later today!
Cool stuff, but I have to ask: Why the excessive use of `unsafePerformIO`?
it's just a test driver. the translation parts are all pure
Of course, this raises the question of how he's going to educate people in "restricted C++-ish" that he has in mind... My recommendation would be: just take the hit and sit down for year to learn Haskell to increase your productivity. It's not any harder than the comp-sci courses back at university.
Fair enough, but still... it's even more complicated than doing it the right way y &lt;- return $ fromParseResult $ unsafePerformIO $ parseFile x This could be written y &lt;- fromParseResult `fmap` parseFile x 
I find it kinda amusing that he calls Java a restricted subset of C++ ... :-)))
Subsetting languages has been done effectively. It's not nearly as difficult in practice as switching to a whole new type of programming paradigm, and it can be done right away without losing too much productivity. That said, it's going to get some benefits but not nearly all that would be gained from switching to FP.
Games?
Hah. Obviously this guy just isn't enough of a Real Programmer to be able to handle C, and needs a crutch, unlike myself.
Rest of the quote: "...but that's not a credible thing to do in the game industry". In most industries, I would add. 
Maybe he should write a monad tutorial to help get them started. I kid... but it is definitely not an easy switch from an imperative language to a functional one. Wrapping one's heads around higher order constructs is difficult, although definitely worth the time investment in my opinion.
It kind of is. It's C++ with all of the useful features and syntax removed, and checked exceptions added in their place.
also these, from the [Yesod wiki](http://www.yesodweb.com/wiki/Javascript) * [jmacro](http://hackage.haskell.org/package/jmacro-0.5.1) a quasi-quoter that allows insertion of haskell values into raw javascript like Julius. Also supports a haskell-like version of javascript. * [Panther-Ajax](http://osdir.com/ml/general/2011-06/msg41431.html) Experimental library to write server side code that accesses the dom and uses continuations.
I like haskell, but it's very hard to switch from imperative to Full-Functional after years of Imperative Gamedev I think D would be a great match for what he wants! (A language with good guarantees, good performance and based on C + +) And i think D have all the points he mentioned: "No arrays unchecked, no uninitialized pointers, No used after free hazzards" Switching to D language would be a start point.
This is similar to Ruby's rake. There is already a project called [hake](http://hackage.haskell.org/package/hake) but I am unsure if it contains the task facilities of rake like zoom does.
BTW, are we going to do a 2011.2.0.2 release with GHC 7.0.4? Users have been asking about it on the mailing list.
btw, how is C++'s "exception specification" capability different from Java's "checked exceptions"?
Honestly I find my way much clearer, but I'll try to be more monadic when i get the rest of this together 
He is multi-talented and has his hands in other things besides games too. http://slashdot.org/comments.pl?sid=152533&amp;cid=12808374 
Even if it's clearer (which I wouldn't agree with, but let's go with it for argument's sake), it's not worth the `unsafePerformIO`. You shouldn't ever use it. By the time you know exactly when and why that's not true, you'll know enough to ignore me. If you aren't up for fmap (in which case, hie thee hence to [LYAH](http://learnyouahaskell.com/) and start reading, but I understand that's not necessarily trivial), then at least do this more newbie-friendly construct: parsedFile &lt;- parseFile x y &lt;- fromParseResult parsedFile Don't use `unsafePerformIO`. I guarantee that it will bite you, and _hard_, before the end of this project, if you don't stop. Also, when you have `x &lt;- return y`, you should write `let x = y`; the do syntax sugar can handle it. (By the way, the reason why I don't agree that your way is clearer is that use of `unsafePerformIO` is the code-clarity equivalent of launching a nuclear first strike. It says you've get big problems and only the absolutely biggest hammer can fix it. But that's not true here, you're just doing totally normal IO. You're sending all sorts of wrong signals there to people who know Haskell, and that's definitely a form of unclear code.) Much later edit: Whoops, fromParseResult isn't in IO.
I understand what you're saying. I've done haskell for about 5-6 years now but I've never found myself arguing *for* monadic IO. I tend to do whatever seems easiest at the time. I would be a good idea for me to actually use the type system in this project though
The author should check out Vagrant ( http://vagrantup.com ) and Chef ( http://www.opscode.com/chef/ ). This is what I use for automating my haskell projects ( actually any language ).
What's been happening with D in regards to the compiler? Has that whole mess been taken care off?
If you're going to antagonise the readers of the subreddit you're in, the least you could do would be make some attempt to justify your claims.
You accidentally checked in all your emacs temp/backup files. O_o
I don't know what particular mess are you talking about, but reference compiler is getting better really quick. Every release fixes tons of bugs, but there is still a lot of work.
There was a big hubbub about Mars's compiler not being free for certain types of projects. Can't remember much more than that. Plus, I believe it also didn't work for certain platforms, like OSX.
I enjoy Haskell for pet projects but you're going to have a hard time finding a day job writing Haskell. And before you say "I write Haskell at my work", I'm sure a lot of people on this reddit do exactly that, but this doesn't invalidate my point. 
they should be removed now
Perhaps we can get Steve Yegge's opinion.
C++'s exception specification is useless. It doesn't syntactically prevent throwing exceptions you've specified are impossible. It just converts any such exceptions into "unexpected exception" which is worse than getting the original unexpected exception in the first place! I think Java's checked exceptions are a step in the right direction. The problem is that checked exception specifications are not first-class and you cannot abstract over them. So you cannot make your function parameteric (has the same exceptions as the function in the interface it takes in), etc. Basically, checked exceptions are a step towards more static type safety -- but static type safety is only a good thing if your type system is sufficiently flexible, and checked exceptions are just too restrictive and rigid.
Well, we still have about 35 or 36 unaccounted people!
Checked exceptions are an effect system. Problem is, they *are* too restrictive and rigid, because there will always be unexpected exceptions our function doesn't know how to handle without just propagating them up the call stack. What I've gone for in my own language is checking exceptions as *negative* effects and throwing them as a *positive* effect. The compiler checks that the negative effects of an expression and the positive effects of an expression don't intersect. The upshot? We can deliberately restrict our functions from throwing certain exceptions and let the compiler infer which ones it can throw, or we can deliberately declare which exceptions our function can throw and let the compiler check that restriction for us. Flexibility *with and through* static checking. Unfortunately, this language is still in R&amp;D.
&gt; Problem is, they are too restrictive and rigid, because there will always be unexpected exceptions our function doesn't know how to handle without just propagating them up the call stack And why can't you specify these, even symbolically? &gt; What I've gone for in my own language is checking exceptions as negative effects and throwing them as a positive effect. The compiler checks that the negative effects of an expression and the positive effects of an expression don't intersect. Do your types specify whether or not you throw/catch things? If so, you still have a form of checked exceptions (And that's good!). The problem with your "negative" effects, is that they subtract exceptions from some exception set of some called code/expressions. The possible set of things you could subtract from is open, so the negative effects may not be useful on their own. &gt; Unfortunately, this language is still in R&amp;D. How does it relate to Haskell, Agda, and other state-of-the-art research languages?
&gt;The problem with your "negative" effects, is that they subtract exceptions from some exception set of some called code/expressions. The possible set of things you could subtract from is open, so the negative effects may not be useful on their own. The paper on negative effects is entitled "Typing Safe Deallocation". Go and study! The point is, if you definitely want to restrict your possibly-thrown exceptions to a closed set, you declare that set as the *positive* effect of the function (or the compiler can infer it). If you want to declare that certain exceptions in an open set can't occur, you use negative effects. &gt;Do your types specify whether or not you throw/catch things? If so, you still have a form of checked exceptions (And that's good!). Types don't specify it. Effect signatures, which are attached to function types, specify whether or not our function throws things without catching them. The effect signature of a function is just the set union of the individual effects that can occur within the function, possibly including effect variables (from function parameters) and other effect sets (from calls to named, module-level functions). It's a Good Thing to have static checking for this. The Bad Thing is Java-style checked exceptions, wherein you have to exhaustively and manually document the whole universe of things you want to *allow* while deliberately failing to mention the things you want to *disallow*. Having compiler-inferred positive-effect signatures makes them like types: let the inference work when you don't care much, and annotate them for documentation's sake when you want to be sure, but either way the compiler will verify the code's soundness. &gt;How does it relate to Haskell, Agda, and other state-of-the-art research languages? Some inspiration, but not much, has been drawn from Haskell, and the inspiration for putting in an effect system was Disciple. This being a systems language, much more inspiration has been drawn from BitC, Cyclone, Scala and various research papers whose languages/extensions never went mainstream. As noted above, the originator of negative effects was looking to create a static-checking discipline for manual region deallocation. It just so happens that expanding his effect system up to a fuller set of "effect constructors" (such as read, write, ffi call, throw) works very, very nicely and elegantly, and pretty much without other alteration.
https://github.com/github/gitignore
&gt; Effect signatures, which are attached to function types, specify whether or not our function throws things without catching them I don't really think there's a difference between effect signatures and type signatures. They're a part of the type signatures in Haskell, Agda, Epigram, Coq, Disciple, Habit, ... If you separate them, it's a superficial syntactic distinction. &gt; It's a Good Thing to have static checking for this. The Bad Thing is Java-style checked exceptions, wherein you have to exhaustively and manually document the whole universe of things you want to allow while deliberately failing to mention the things you want to disallow. I think the bad thing in Java is the lack of flexibility. It's like the people who positively decide static typing is not worth it, based on Java and C++. These are really straw men of static typing, similarly to how Java is a straw-man for checked exceptions. &gt; Some inspiration, but not much, has been drawn from Haskell, and the inspiration for putting in an effect system was Disciple. This being a systems language, much more inspiration has been drawn from BitC, Cyclone, Scala and various research papers whose languages/extensions never went mainstream. Habit seems to be a Haskell variant specifically tailored for systems programming. &gt; As noted above, the originator of negative effects was looking to create a static-checking discipline for manual region deallocation. It just so happens that expanding his effect system up to a fuller set of "effect constructors" (such as read, write, ffi call, throw) works very, very nicely and elegantly, and pretty much without other alteration. I wonder if Oleg's regions-in-Haskell is an application of that paper, or of something else.
Any time your code uses `do`, `&lt;-`, or `return`, you're using "monadic IO". So look at Main.hs gethaskellast :: String -&gt; Whatever gethaskellast x = fromParseResult $ unsafePerformIO $ parseFile x ... translateFile x = .... haskellAST &lt;- return $ gethaskellast x It would be a lot clearer and safer to simply do this: gethaskellast :: String -&gt; IO Whatever gethaskellast = fromparseresult `fmap` parsefile ... translatefile x = ... haskellAST &lt;- gethaskellast x But doing `x &lt;- return $ f y` is just silly, and usually should be replaced with `let x = f y`.
&gt;If you separate them, it's a superficial syntactic distinction. Yeah, the formal proof of this was recently featured on LtU. *HOWEVER*, there is one real difference: if you build-your-own effect system through monads, you have to prove the monad laws over your own monads and then build the various monad transformers and combiners yourself. If you make the "superficial syntactic distinction" of leaving certain monads/effects implicit, it becomes a fairly simple compiler feature whose underlying algorithm you can easily explain to anyone. &gt;Habit seems to be a Haskell variant specifically tailored for systems programming. I've heard of it. I've read the preliminary Habit report. I'm heading in a different direction from Haskell entirely, so I wish them well with it. &gt;I wonder if Oleg's regions-in-Haskell is an application of that paper, or of something else. Go check for a citation!
&gt; If you make the "superficial syntactic distinction" of leaving certain monads/effects implicit, But then you lose referential-transparency of expressions, and you gain it back only for specifically-typed expressions. For example, in Haskell, it is *always* safe to commute the order of a commutative operator. In Disciple, it is only safe in the Identity ambient Monad. Everything is also auto-lifted, so I'm not sure how you treat the monadic part of the values as first-class, the way you do with Functor/Applicative/Monad combinators. 
You might want to add `dist` to that to avoid checking in the binaries.
Did he just ramble for 1:20 without any prompting or apparent structure? Holy crap.
"clearer"? No way! Using unsafePerformIO is about as obscure as you can get. The consequences of using it is very hard to fathom.
&gt;But then you lose referential-transparency of expressions, and you gain it back only for specifically-typed expressions. For example, in Haskell, it is always safe to commute the order of a commutative operator. As the Disciple guy put it: *It is more important for a compiler to be able to reason about the behaviour of a program, than for the language to be purely functional in a formal sense.*
What about the importance of the programmer's reasoning about the program?
I would think that the programmer will have an easier time reasoning about their program when they can reason about it directly without having to embed their reasoning (effects) in another logic (monads).
The `&lt;-` suggests that you are already using `do` notation. Why not s &lt;- parseFile x let y = fromParseResult s ... ?
When in the video does he say this?
I am not getting your point here. Values that are wrapped in a monadic wrapper type can be reasoned about just as any other type. Referential transparency of values in Haskell means that when I *don't* have a monadic composition but a pure composition, I can perform mechanical refactorings and reason about equivalence. In Haskell, if I see: x `commutative` y it is always safe to change it to y `commutative` x. This has little to do with monads. In Disciple, that kind of transformation, due to implicit monadic compositions being auto-inserted automatically, no longer holds.
&gt;I am not getting your point here. Well yes, you're a Haskellite. Most non-Haskell programmers don't actually want to learn about monadic composition, they want to get on with reasoning about *what their program does* (its effects) and *what their program returns* (its type). They do not want to learn how to wrap the former into a logical system designed to reason about the latter in a way that will require them to do extra proof work. The more work the compiler can take off the programmer's plate, the better.
I disagree with the categorization of "what the program does" and "what it returns". There's "what the program means" (It's denotation, what it returns, its type), and how it achieves it (it's operational behavior, the algorithm expressed therein). You keep returning to a separation of return-type and effect-type, which I think is simply wrong: The "effect" (wrapper type) and return are tightly-coupled. Not all monads are about effects. Most monads have a denotational behavior you can reason about.
That looks cool - thanks!
&gt;Not all monads are about effects. Most monads have a denotational behavior you can reason about. But once again: that assumes you're capable of creating and proving-correct your own monads, and also of composing the monads the standard library gives you. Frankly, I'm creating a language for people not quite masochistic enough to want Haskell's ultimate freedom-of-type-encoding, people who want the compiler to do work for them. &gt;The "effect" (wrapper type) and return are tightly-coupled. Only in Haskell and its family of languages. In all the rest of programming and all outside programming languages, the two are loosely-coupled at best, but mostly just not coupled. In that land, far away from your home, impurity reigns and mutation is common. That land will not submit to your religion ;-) and wrap its immodest mutations in ~~burkas~~ monads.
Try getting a D2 compiler on OS X. It's not in macports, and they don't distribute a binary. I still have yet to successfully build one.
Was anyone else kind of scratching their heads in disbelief that in all these years Carmack hasn't put together some useful tests or proofs-of-concepts to at least talk about the viability of FP for this application? At this point I'm beginning to question his commitment to Sparkle Motion^h^h^h^h^h^hfunctional programming.
Was anyone else kind of scratching their heads in disbelief that in all these years Carmack hasn't put together some useful tests or proofs-of-concepts to at least talk about the viability of FP for this application? At this point I'm beginning to question his commitment to Sparkle Motion.
But if you haven't got a crutch, how do you stand up after you blow your whole leg off?
ahh thanks
It's natural deduction. Google it. 
So, if you want to read papers like that, you should probably get yourself a CS degree from a university worth its salt. Virtually any CS major would've been exposed to rules of logical inference and natural deduction at some point. I've seen it in 6 courses where I teach.
&gt; I've done haskell for about 5-6 years now but I've never found myself arguing for monadic IO. I tend to do whatever seems easiest at the time. Terrifying. That eliminates you from a whole bunch of haskell job openings. 
I want to believe that he is the only reason that people go to Quake-con.
Probably true. I did not finish my CS degree but from what I've seen from other graduates this was not where the degree focussed. I do not have the resources to study institutionally at the current time, so online learning it is :)
Brilliant. Thank you.
Today I learned that there are a bunch of haskell job openings.
We are all very serious programmers here. :-)
If you find `unsafePerfomIO` much clearer then maybe "you're doing it wrong" :-)
Details here: http://en.wikipedia.org/wiki/Natural_deduction#Proofs_and_type-theory To know *all* about it, you can look up e.g. the [Types &amp; Programming languages book](http://www.cis.upenn.edu/~bcpierce/tapl/) ([find it](http://en.wikipedia.org/w/index.php?title=Special%3ABookSources&amp;isbn=0262162091+))
These are reasons I'm excited about Rust. It has a lot of the nice stuff from FP but seems a lot more accessible to curly-brace language programmers.
The premises ---------- conclusion notation is called a [rule of inference](http://en.wikipedia.org/wiki/Rule_of_inference) - as augustss said, they're used in natural deduction, but they come up in other places as well.
For those who haven't heard about "Pastis": http://en.wikipedia.org/wiki/Pastis
Most US CS (even good ones) programs don't teach that stuff, as far as I know. And anyway, kind of inflammatory, don't you think? Some of the most knowledgeable people in the IRC channel don't even have university degrees.
I'm also a postdoc on a follow up project to CompCert (i.e. another verified C compiler) and can answer any questions. All our code will be open sourced in contrast to CompCert's which has a more restrictive licence.
Hence this particular case could be read something like: "If e1 has type (x:s) -&gt; t in context G, and e2 has type s in context G, then the application e1 e2 has type t (with e2 substituted for free occurrences of x) in context G". This is just the normal rule for giving a type to function applications, except that this is a *dependent* function application, so the argument can show up in the type.
For this kind of subject, you may run into limits learning it online. If so, the book that huitseeker recommended, "Types and Programming Languages" by Benjamin Pierce, is your best bet.
Hmm, any guide of where to start with Chef, the non-server version? The documentation seems sort of disorganized and aimed at installations much larger than I'm dealing with - multiple servers, a single server just for Chef, etc. I think it will scale down to what I want (ie, setting up ONE server), but I haven't found clear documentation on how to get that going. Vagrant, on the other hand, seems simple and well documented :)
Here's a secret: They'll sell the same books to people who aren't getting a CS degree.
&gt; Frankly, I'm creating a language for people not quite masochistic enough to want Haskell's ultimate freedom-of-type-encoding, people who want the compiler to do work for them. Your calling it "masochistic" sounds like you've not actually used it yourself. There are some painful things in Haskell. This isn't one of them. &gt; Only in Haskell and its family of languages It is the basis of denotational programming (based on denotational semantics). Some say denotational programming is the "true form" of functional programming (And I agree).
Anise flavored. Ugh, never again.
&gt; I think GHC Core is the output of the desugaring step, but I'm not certain. That's right. See [the GHC wiki section on the compiler pipeline](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Compiler/HscMain) for more details and terminology of the compilation process.
Girard's _Proofs and Types_ covers natural deduction and sequent calculus in a type theoretical setting. You can [get a copy online](http://www.paultaylor.eu/stable/Proofs+Types.html) from Paul Taylor's website.
Don't most schools offer some sort of programming language theory or compiler course(s)? I would think it would be covered there (likely as an elective, it was for me).
Many compilers courses don't teach this, and besides, they are usually graduate level courses and optional for undergrads.
Your bicycle is upside down
Let me get that. （╯°□°）╯︵ ⦿‾⦿ 
That could be a valid Haskell program :-)
I think the way to bring FP to industry is this: * Wait till the need is there (frustration with complexity of a project getting out of hand). * Bring functional techniques into the existing code base and language. This is where learning a hybrid language like Scala is useful, because it teaches you the techniques to make OO and FP code fit together. * Once the paradigm shift is (mostly) made, shift to an FP language to get better syntax. Haskell is a good choice for industry because it's pure, which is great for addressing the original problem (complexity out of control). For this, you just need one FP mentor with support from others. While it would be more efficient overall, moving directly to FP would cause a period of low productivity (and hence morale). The method above would give a gradual improvement in productivity with no downtime. I know one workplace currently in phase 2 of this process and another just about to enter phase 3 (adopting Haskell).
Dehydra does this for C++. https://developer.mozilla.org/en/Dehydra
I have been evaluating Haskell for use in signal processing for time to time. I think switch to Haskell would be possible if it would have uniqueness types and possibility to declare uniqueness when it's needed. Compiler is usually fast enough, but to write fast straight forward numeric code you actually need to destructively modify vectors and matrices. 
Okay, not to harp... but keep in mind that the semantics here are very different. y &lt;- fromParseResult `fmap` parseFile x That says that first you parse the file, and then you define `y` to be the result of calling fromParseResult. On the other hand, y &lt;- return $ fromParseResult $ unsafePerformIO $ parseFile x That says define a name `y`, which *if* it's ever forced, will be expanded to the definition of fromParseResult, which in turn *if* the parameter to fromParseResult is ever forced, will have the side effect of parsing the file. You're doing I/O at unspecified times, or perhaps not at all, and if so in parts of the program that are *not* necessarily expecting the possibility of exceptions from file I/O. So if you're not convinced by the "good practice" stuff, perhaps you should be convinced by the fact that your program just plain means something different, something far more complicated, far less predictable, and far more likely to be seriously broken. Incidentally, I think you probably mean "applicative I/O" rather than "monadic I/O". Monadic I/O means using the normal features of a do block, binding statements, `do` to group statements, etc... or the lower level `return` and `&gt;&gt;=` operators. Using `fmap`, `&lt;$&gt;` (a synonym for `fmap`), and `&lt;*&gt;` to compose I/O operations as expressions is applicative style, a la the `Control.Applicative` module. So you *are* using monadic I/O, in awkward ways, when adopting a more applicative style would help.
The converse is that it is easy to hire extremely talented Haskell programmers (if you get in early), which is a selling point to industry. I don't really agree with John Carmack about "hiring problems". I am not prepared to relocate but there are plenty who will.
I think Perl would run that!
You can take those points literally or take them as examples. Has there ever been a good language designed around a shopping list of features found in other languages? If you are not familiar with what exactly you get from Haskell, it is difficult to see past those simple examples. But there is much more to that and it is not easy to explain. I question usefulness of switching to D, I have yet to see a compelling argument to using D for anything over continuing to use C++, or better yet using C along with Haskell, Caml, SML or Scheme.
That's actually a fully compliant Haskell-98 compilter in Perl.
Really model theory.
*MATHS*
&gt; Try getting a D2 compiler on OS X. With Homebrew it's quite easy: "brew install dmd"
Mac downloads: i recommend using "brew install dmd". I don't see how using D2 should restrict you in how you license your project or what you could do with it: * The standard library Phobos is licensed under the Boost license. https://github.com/D-Programming-Language/phobos/blob/master/LICENSE_1_0.txt * The runtime "druntime" is licensed under the Boost license. https://github.com/D-Programming-Language/druntime/blob/master/LICENSE_1_0.txt
I think the author might have had a little too much of that stuff. `Left undefined` indeed! How about: pastisURL url = fmap (either (const url) rspBody) (simpleHTTP request) `catch` (return . const url) where ... 
&gt; That says define a name y, which if it's ever forced, will be expanded to the definition of fromParseResult, which in turn if the parameter to fromParseResult is ever forced, will have the side effect of parsing the file. You're doing I/O at unspecified times, or perhaps not at all, and if so in parts of the program that are not necessarily expecting the possibility of exceptions from file I/O. Also known as "Lazy IO". Actually, `return . unsafePerformIO :: IO a -&gt; IO a` is pretty close to the actual implementation of `unsafeInterleaveIO`, which is used in the implementation of lazy IO functions like `getContents`.
And with some minor touch-ups the code even type checks and works. :)
I like bash. I just wish it had closures and maybe some custom datastructures. I think a lack of type-ery fits the interactive shell model pretty well. That said, I'd like to have some sort of persistence in a form other than files-on-disk or environment variables. I have been considering ways to layer this kind of functionality via redis and some clever use of aliases/dynamically defined functions, but such things are less than ideal. I suppose the things I want out of a shell language are thus: 1) anonymous functions Lambdas would allow for some more object-oriented code, without dropping the "everything is an object" dogma on Bash. That's not to say that dogma is without merit, but I shy from dogma in all forms 2) datastructures w/ pattern matching It would be _fantastic_ to define arbitrary structures similar to how haskell does structures, and then get some sort of "free" (eg, no-work, environment-variable free way to persist those structures across entries in the ~~Bash REPL~~ command-line. Ideally I'd have a command `persist` which I can call on a datastructure, and have it be persisted somewhere with minimal issue. 3) easy namespaces All of the above could be simulated (perhaps not well) if it weren't for the fact that namespace pollution is annoyingly trivial in bash. 4) I'm not sure what you mean by "filesystem abstraction" -- there's a pretty uniform interface there to begin with, but I'd like to here more about that. I guess, in a nutshell, I want more ability to have semi-persistent structures for less. ATM, I end up writing to files in /tmp, ([see here, apparently some filesystems already do this](http://www.reddit.com/r/haskell/comments/jce8q/in_your_haskelldreams_how_does_a_shell_look/c2b76up)) I'd really like to store those things in memory, or -- at least -- have some sort of clever cache that recognizes when I read the same file often, and holds onto it in memory, so I don't have to pay the `cat` cost so much. 
Cross post to /r/linux maybe for more feedback?
[This shell](http://acko.net/) looks nice.
How easy would it be to give a browser the ability to interpret Haskell instead of a compiler for Javascript?
i already have a shell that is moderately useful (zsh). a shell that is incredibly useful is probably something like perl's zoidberg taken to its insane extreme. why? - its duck-typed. pulling random data from random sources is more useful in a duck-typed language. - it has an incredible number of modules i can pull in that are strongly focused on the types of problems we experience from the command line - it has primitives that are close to older shell standbys like sed and awk haskell for a shell is an amusing idea but i don't think it is a good fit. shell interactions are going to be strongly correlated to i/o, so it seems you are relegating people to the 'M' functions (seqM,foldM,etc), which is generally an unhappy place. yanking in unknown data from files is doable with read, but you're then asking users to jump through even more annoying hoops. in a program i deploy, these hoops are often desirable. on the command line, less so and how do things like typed fifos work in a practical sense? the whole point of a fifo is to send data from the unknown to the unknown. adding in strong typing makes fifos worthless to me furthermore, hackage still doesn't feature that many modules which are interesting for sysadmin/shell style work 
There was a project named [es](http://en.wikipedia.org/wiki/Es_shell) with exactly same goal. I believe that it is not being developed nowadays.
with all due respect to carmack, he is probably only vaguely aware of the black art of haskell performance tuning. his background is from a strict tool that offers fine-grained control of time and space tradeoffs. in particular, haskell's space tradeoffs probably means it is off bounds for most serious game development. he's right that something strong and binding like haskell's type system is amenable to reducing errors, but there's no way that the games now written in c++ are practically possible in haskell. and i'm talking about crysis, not nikki and the robots. what carmack is probably looking for is something like go: concurrency, garbage collection and strong support for types...but types that are more strongly correlated with the underlying machine architecture. 
updated
Fixed.
but most "unix" programs don't have even vague formal requirements, other than data is something that was read from a file descriptor
Have you taken a look at [Microsoft's PowerShell](http://en.wikipedia.org/wiki/Windows_PowerShell)? 
I'll bite. I've been contemplating this. I'd make a few changes, if I ran the zoo. 1. I'd replace the model of everything is a file/stream of bytes, with everything is DOM, specifically XHTML. So a program like ls, when listing things within things, would emit an XHTML document with a table in it. The table rows would replace the lines I'm accustomed to. Space around the table would be used for metadata ls likes to output. The idea here is to take the idea of completely unstructured data a la raw pipes and files, and add just a little bit of structure. And make sure to pick a structure that already is easy to parse and easy to display. XHTML is clearly a good idea there. 2. I'd split the responsibility of the shell into a couple of processes. A work process would be responsible for doing actual work, eval/fork/exec-king when necessary etc. A second process would be responsible for translating data between the work process and the UI. That second process might not actually be colocated with the first. The idea here is that there should be no use for a shell action named less. This UI should take responsibility for that. 3. I'd use a browser for the UI. And I'd use proportional fonts although others wouldn't have to. On the wimpy systems I like to build for myself, monitor space is my most expensive asset and I like how proportional fonts make better use of at least horizontal space. 4. I'd experiment with allowing more than one user to actively see and use the shell at the same time. Eclipse is neat but when I'm stuck with a windows workstation and a Linux dev machine at work, I'd like that experience to be more seamless. 5. I'd experiment with an editor being part this UI, allowing two people to work together without being in the same room. So I could go on #haskell and instead of pasting to hpaste, I could provide an open link to my shell/editor so people could help me. The security implications of this are totally awesome. 6. Actions available from this shell would provide more information about their inputs than current shell programs, although I don't know if I'd shoot for complete type safety. I think hints and conventions would scale better. 7. If actions provide enough metadata, I should be able to "explore" in a more graphical way, the capabilities of any action. In bash if I type ls /bin that's all well and good. If I want to sort the results by last modified date, I have to break out the manual. So I'd like to be able to see a form in my browser ui interactively presenting all the possibilities. Really, that's overly ambitious. I'd pick my battles, probably starting with the in browser UI and appearance and work my way down from there. Fonts and sharing between users are the things that would interest me the most. 
I believe it was the first version of D that had these problems. Like I said, I was just listening to this stuff on the outside, so I don't remember too much about everything, which is why I asked originally.
Regarding that one would have to use IO monad all the time; what I would imagine would be to have some sort of type safe abstraction monad over the file system and allow one to write pure functions dealing with files. This would probably require one to redesign everything from the file system and up though.
I love the thought of data types. Even better, if they where persistent, one could maybe envision a second file system, maybe more like a database, where one would store structured data data from haskell data types. If one then could make a way of easily reading from files and making it into structured data, one is on good way to doing something productive. That's where this very vague "file system abstraction" would come into play.
... a few of whom might actually have you. But who would want to work in a place like that?
Have you looked at the existing shells? Clean has Esther, and Oleg wrote an interesting thing called [ZFS](http://hackage.haskell.org/package/ZFS).
don't forget scsh: http://www.scsh.net/ edit: oh, and if you haven't read the acknowledgements, do so, now: http://www.scsh.net/docu/html/man.html
This is two questions from my view point. The first is about shell scripting and the second is about the usability of the shell as an interface to the system. There are plenty and obvious updates to Bourne style programming so I will focus on the second one. My serious issue with shells is the world divide between the GUI and the shell. For many things a browser of some sort whether it be web, file system, etc. is valuable in either domain. For me the annoying thing is once you step into GUI land you can no longer easily connect to the shell. Wanna pipe data from something into Firefox? document editor? It can be done, but it is not easy. But how about the reverse where you want to run a web page through a few filters? Or slice and dice an active document in an editor? I have some ability in emacs to make buffers from shell indirections but most apps treat every UI as its own walled garden. Someone else mentioned Powershell and this is a start. But it is not easy nor quick to use as a day to day shell. At least not for me with my Unix trained fingers. This is the balance that needs to be drawn -- the shell needs to be more efficient than point and click as well as easily driven by scripting. I am not certain all of Haskell's formalism is a boon here. That said, I could see this working as a sub/super set of Haskell where the developer was always in the IO Monad so they never need to explicitly reference it. 
&gt; x∗ = 1 + xx∗, which by simple algebra must be x∗ = (1 - x)^-1. We define 1∗ := ∞, and ∞∗ := ∞ to satisfy the recursion equation. Huh. Infinity is always kind of a "generic solution" for all x but 0. 1 makes sense as the limit x -&gt; 0. But the limit as x -&gt; ∞ is zero...
There's also [Haskal](https://github.com/thoughtpolice/haskal). It wasn't originally written by me but a few years ago I took the time to update it to modern GHC at the time (6.10 I think) and clean it up. Haven't touched it a whole lot since then, though. Patches welcome. :P
&gt; I'd replace the model of everything is a file/stream of bytes, with everything is DOM, specifically XHTML. I could get behind structured data output but please not DOM. The DOM api is really really annoying. I don't know anyone who actually likes using it. I'd rather it were json or even yaml over dom.
0 seems like the natural candidate for ∞∗, however 0 ≠ 1 + ∞·0 because we've demanded that 0 absorb all elements, including ∞. I will update my post to note that by simple algebra the solution **can be** x∗ := (1 - x)^-1.
It might be interesting to bring types to piping. For example, if I try to pipe a PDF document into a bitmap graphics editor, the system should just find the right transforming function for me. It might prompt me with the various choices I could make. As another commenter mentioned, being able to do xpath on documents right in the shell would be pretty cool. A standard text type that wasn't just lines-o-text, maybe XHTML, would be nifty. This isn't the shell per se, but a versioned filesystem would be great. Maybe even something like a DVCS. A filesystem that was sort of like a DOM would be interesting, where I could easily attach arbitrary attributes and events. Like "if file modified in this folder, execute such-and-such".
&gt; My serious issue with shells is the world divide between the GUI and the shell. Alas, this is more than just a superficial schism. Shell applications often discard useful information to render data in an appropriate form for I/O (string serialization, typically), while GUI applications often assume user interaction as the only way to do many tasks. So it's not just a matter of connecting things together somehow; most existing applications are actively hostile to what you want to do. &gt; That said, I could see this working as a sub/super set of Haskell where the developer was always in the IO Monad so they never need to explicitly reference it. So, like GHCi? It mostly behaves like a `do` block in `IO` that gets evaluated as you write it. That's why you have to define things with `let`, for instance.
I agree with you, it's disappointing. I have come to wish that having open-source code was required to get computer science work published. If one claims a scientific result, it should be reproducible by anyone else.
I think Carmack can speak for himself...
This would be interesting. Suppose you make the running of a program an atomic action on a directory tree: you could then make operations on any directories and regular files within it effectively pure. I can't really remember how journaling filesystems and ZFS do things, so this may already exist, but if you treat directory trees like immutable objects, you get some ability to roll back changes, as the old data structures are still on the disk. I think it would be interesting to see an OS where files are typed, and programs act as functions. A program with a config file may be declared as being within a reader monad environment. `xargs` is an example of a higher order function, and `|` is already `flip ($)`.
* Standardized argument syntax. I would prefer -verbose -list and --vl or -VL rather than the reverse. * Typed records as elements of each stream. And a terminal that can take advantage of the type to select suitable formatter/pretty printer * Better and unlimited streams (not just stderr and stdlog), and a meta language to switch them if need be (Hartmann pipelines are interesting in this regard for naming streams. Or even a stack based language for specifying which stream is primary) * Non linear pipes (use process networks to predict if it will deadlock), and an optional graphical tool like Yahoo pipes for manipulating them (In addition to textual syntax) * Better exception handling. (If a record is faulty, don't kill the entire process, isolate faulty record and any others dependent on it.) * An event queue that we can hook into for file system events * Plan9 like user and session specific file system * Lexically scoped temporary files and named pipes so that they can be used without having to bother about cleanup and races. * commands that operates on pipes with one of three distinct types : maps, filters and folds and unification based pattern matching rather than awk. 
There is a fork called "xs" which is being actively developed, but I haven't been able to get it to compile yet. (Some issue with autoconf)
As far as I can tell from the documentation I can make a Haskal config file and I know how to set my prompt from reading ConfigAPI.hs. Now ... why would I want to use it? What does it do? What features does it have? What commands do I have available to me.
&gt; ...but there's no way that the games now written in c++ are practically possible in haskell. and i'm talking about crysis, not nikki and the robots. Obviously at iPwn Studios we disagree with this.
I like the idea of well-typed pipes. I don't think that format conversion should necessarily be fully automatic -- that would be rather error prone -- but I could imagine a generic "coerce" program which could be inserted into a pipeline if you want it to try and guess an acceptable conversion. This has the added benefit that there can be multiple competing type coercion programs -- one particular solution isn't hard-coded into the shell.
Multi-line GHCi input? Where'd I put that me gusta face gif... Prelude&gt; :set +m Prelude&gt; let x = 3 Prelude| y = 4 Prelude| in x + y 7 Prelude&gt;
One way to think about the problem is identifying existing problems with current shells. I'm familiar with Bash, so this may not apply to all shells, but I think most of these are typical of Unix shells in general. Some of this spills a little into how programs in unix work more than how the shell works, but I think that's inevitable. * Pipes aren't type-safe. * Handling failure in a shell script is awkward (no transactions or undo). * The shell cannot know if program arguments are syntactically correct without first running the program. * The shell cannot tab-complete program arguments (save for filenames). * There is no distinction between programs that cause side-effects and those which only read from stdin and write to stdout (which are essentially pure functions). * Programs which do not cause side-effects could be subject to lazy evaluation, but aren't. * Program output is not automatically cached in a useful way. ("Oh, wait, I forgot to pipe 'svn blame' to less, I guess I'll have to run it again...") * A program may (crudely) infer the type of its input by examining the data, but it has no way to automatically (i.e. without command line options or the like) infer what it's output type should be. * There isn't a standard way to insert a placeholder into a pipeline, and let the shell automatically insert the right program that happens to convert from the proper input type to the proper output type. * Saving stdout and stderr to separate files does not preserve ordering, whereas redirecting both to the same file fails to preserve which came from stdout and which came from stderr. * There's no user-mode equivalent to /proc. I would like for programs to be able to define in-memory files which don't have any real in-memory representation unless you read or write them. * There is no standard un-intrusive way for a command-line program to indicate progress. (This could be done perhaps with a proc-like interface, or as a companion to stderr and stdout.) * File globs seem to support union but not intersection or difference of sets. *.jpeg is straightforward, but "show me everything that isn't a jpeg" isn't.
&gt; have some sort of clever cache that recognizes when I read the same file often, and holds onto it in memory, so I don't have to pay the cat cost so much. Not sure I'm catching your meaning here; operating systems (linux at least) typically cache files in this way for you already. 
what do command line options have to do with trying to apply strong typing to unix commands?
A lot of articles about D are old and refer to D1. I myself have learned only by incident some weeks ago that D2 is quite open now and has even moved to GitHub. Walter Bright also has asked to merge DMD into gcc!
That's Eddie, bitches!
Really? Hmm, I'll have to do some benchmarking... .'d appreciate any info you have in how that feature works. I've been on a devops kick for the last few weeks, learning lots about bash and *nix... 
Intriguingly before its release code-named Monad Shell, though after Leibnitz' metaphysical theories rather than the category theory concept.
DOM documents are "naturally visible" in a browser, a property they share with raw text in a terminal. Neither JSON or YAML share that, although I like both better when I'm coding. I agree with you about the DOM api being a pain. Having programs such as ls use conventions and declare (or at least hint at) their output "type" helps. If we know that ls is one of the programs that outputs a single table and that a lot of programs do that, then we can build higher abstractions over finding and processing the table rows sequentially as maps or maybe even records. Meanwhile, we retain the ability to "just" dump the document and view it's raw output if things aren't behaving as we expect.
You typically encounter the ext2/3 filesystem cache when looking at memory usage in linux with tools such as "free" or "vmstat". Basically when your running programs aren't using all your available memory, the kernel will quietly use unused memory to cache files you open so it doesn't have to retrieve them from disk if you ask for them again. But if your programs need the memory, it'll free up some from cache. So the cache never interferes with the running of programs but ensures the free memory assists in disk lookups. I had a quick hunt for a site which mentions this and found this: &gt; http://www.linuxhowtos.org/System/Linux%20Memory%20Management.htm I only skimmed it. But it should hopefully give you some idea. If it turns out to not be a good page, I suggest just googling around for tools like free and vmstat and explanations of the cache reading. xfs works similarly, but as far as I can tell seems to use its own kernel slab allocation for caching so it doesn't show up in tools such as free. Instead you see it with tools such as slabtop.
no, i'm sorry, he's john carmack not god. if he hasn't spent a great deal of time programming in haskell he would not be aware of these limitations. how would he? or are you presuming that by virtue of being a great c coder, he invariably has a full understanding of coding in haskell having never done it?
you can already lock files (and directories) rolling back every change to a filesystem isn't useful. we already have raid and/or zfs snapshots and version control systems to apply rollbacks where needed 
I didn't say he had a full understanding of it. But saying something like: "if he only knew of this aspect of Haskell as well, he would be speaking differently" is a bit much. Just as he may not know of some disadvantage in Haskell, he may also not know of plenty of advantages. So that's basically putting words in his mouth. Also, to claim that he'd go for a language like Go, after he explicitly said *type safety* interests him, sounds quite silly.
jasper consistently has some of the best-explained blog posts in the haskellverse
Sadly, the link you provided redirected me to an annoying "don't use adblock" link, however, I think this provides a good starting point for me to go digging. That's a very clever optimization in any case, I was thinking I would have to do something manually in order to achieve that effect, but it appears that common filesystems do it for me, which is awesome. I'm going to edit my OC to link down to here for posterity, thanks for the info! 
No, I haven't, but i will now =)
your notion of abstraction is broken in my opinion. any XHTML representation must ultimately be backed onto something that is like a file (even if you are reading it over a network port), but you have eliminated this abstraction. if you like the concept of browser-as-ui, trying chromeos, this is basically what they have implemented
Clean and Esther looks like what I've been thinking about. I wonder what the usability of such a system would be. If one begins with a Typed file system, that support atomic actions on it, a bit like STM for files. Then you has a quite interesting place to write programs that take advantage of all these features. One could imagine "pure" IO, or am I totally tripping? This of course requires that everything that runs on this system has to obey the rules. Would it be possible to create such a system without making it all in one programming language?
I don't know. I understand that mainframes had/have rather complex filesystems where files were heavily typed, but Unix's 'array of bytes' model seems to have won out pretty decisively - things like WinFS get unceremoniously canceled.
This would be wonderful if it works. To my mind, difficulty debugging is haskell's second largest problem (after diagnosing space leaks).
&gt; It should also be possible to have a compile-time option to enable stack tracing, but information is lost when interacting with code compiled without stack tracing. It remains to be seen how well this works in practice - it's not just that there will be gaps in the stack, thunks need to save and restore the stack otherwise we are back to the evaluation stack rather than the call stack. Sounds like a good excuse to bump GHC's major version number. ;)
...dId all Haskell reddits join google+ at once? ;-)
One of my favorite new features: &gt; GHCi now has a multiline-input mode, enabled with `:set +m.`
Yes. 
On the lazy evaluation, I think it is already available in the form of pipes. On last point, I think it would be better if they did not support more features than they do now. This makes them much easiser to pick up than regex. Instead it might be better to utilize piping for that. &gt; ls * | grep -v \.jpg
Please have a look at Jacob Stanley's recently posted answer suggesting `castSTUArray` as an alternative to FFI marshaling. He points out that this is what they use in GHC itself. I hope John Milliken sees it; I'd like to hear his comments. This is one of the reasons I dislike SO. It encourages selecting a quick response as "accepted", thus shutting down a useful conversation too early.
I'd do my phd over for this :)
Diagnosing space leaks and debugging are not harder in Haskell than in any other language. It takes a lot of experience to do them well, as in any language. And in Haskell they are significantly different in concept and practice than in traditional languages, so many intuitions and techniques transferred from other languages are not very helpful. That said, I certainly am just as excited about the possibilities of Simon's new tool. It would be extremely useful; I hope it pans out. But even if it does, people who think that this will transform Haskell debugging into anything remotely resembling C or Java debugging are going to be acutely disappointed.
 *** Exception: Prelude.head: empty list Moar info plz!
Some of us less cool people didn't get the invite.
wasn't this posted earlier today? anyway, why don't you link to release notes ( http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/release-7-2-1.html ) which contain useful info instead of that web2.0-social mumbo-jumbo?
I'm fairly sure it was posted a day or so ago, with a link to the mailing list announcement (which in turn linked to release notes), and then disappeared.
I think what you've seen was about the Release Condidate. Also a [list of tickets closed in 7.2.1](http://hackage.haskell.org/trac/ghc/query?status=closed&amp;order=priority&amp;col=id&amp;col=summary&amp;col=milestone&amp;col=status&amp;col=type&amp;col=priority&amp;col=component&amp;milestone=7.2.1&amp;resolution=fixed)
it was... see here: http://www.reddit.com/r/haskell/comments/jdw5y/ghc_version_721_released/ I have no idea why it disappeared though...
look for some reddit posts, a few folks have web pages that you just need to visit for an invite.
Something to do with reddit's spam filter, I think. I didn't notice it had vanished and apparently there's no indication that new stuff is in the spam filter for a given time period. Sorry about that. In the future, if something mysteriously disappears, message the moderators about it. If I had to guess I'd say that linking to a mailing list archive is what seemed "suspicious". But I really have no idea.
Oh, that's easy to debug, it means you used `head` somewhere, which is usually a bug. Just grep your source to find the location and rewrite that code to not call partial functions. ;] But yes, better diagnostic information would be nice.
Released this from the spam filter, even though now we have duplicate posts. Maybe think of the duplication as indicating that the new GHC version is *just that awesome*.
Noob question: if you wanted backtraces, you'd have to turn tail recursion optimization off, correct?
Right, just get in the habit of never using partial functions. Better diagnostic information is always nice. But in real-life programming, diagnostic information from the compiler and runtime is just a small start. As in any language, the key points are: * designing the system from top to bottom safely so the hard-to-find problems are unlikely to arise * thinking ahead to pre-wire the system for the kinds of investigative tools you'll need in case a problem does arise, tailor-made for the specific system design * having the intuition to know where to look for the likely culprit when there is a problem * being a good sleuth - knowing how to investigate It's hard to compare the overall quality of diagnostic information available in Haskell to most other languages, because it's very different in nature. But having architected and developed large systems in a number of traditional languages as well as in Haskell, my feeling is that the diagnostic support I get from the Haskell toolchain is as good or better than for any other language.
Send me a message with your email, I'll send you an invite.
This is a very nice post. I'd like to see some of this translated into some practical types and algorithms. The [dtd-types](http://hackage.haskell.org/package/dtd-types) package already contains a copy of the `RE` type. Seems like that really ought to be imported from a more general library, and there should be some useful algorithms we could apply. For example, how about a function that decides whether a RE is deterministic? That is hard in general I think - there is a well-known exponential algorithm - but that would have been very useful to me recently.
I've also got plenty of invites if you run out.
GHC doesn't (I believe) have anything you'd properly call tail recursion optimization, because the evaluation model typical for Haskell simply doesn't need a stack for function calls. So the closest you could come to a 'yes' answer would be that a function call stack of some sort is artificially added, rather than some optimization being turned off. However, I don't actually have any idea what his thoughts are on the matter (he didn't really elaborate), so the phrase 'call stack' might have just been a slip of the tongue. Maybe the tracing he's talking about isn't really about conventional call stacks at all.
Monad comprehensions! :)
The advice to search reddit for posts was good. I managed to get in.
Where did System go? What's the fix? /usr/local/src/ghc-7.2.1/bin/ghc --make -O2 -XBangPatterns -threaded -rtsopts -fasm fannkuchredux.ghc-3.hs -o fannkuchredux.ghc-3.ghc_run fannkuchredux.ghc-3.hs:13:8: Could not find module `System' It is a member of the hidden package `haskell98-2.0.0.0'. Use -v to see a list of the files searched for. [fannkuchredux.ghc-3.hs source code](http://shootout.alioth.debian.org/u32q/program.php?test=fannkuchredux&amp;lang=ghc&amp;id=3)
The Haskell98 modules have been deprecated for some time (I get warnings about them in 6.12) -- looks like they finally got removed. You have two options: 1. Update the code to use the new import locations. That module is using `getArgs`, so you want to `import System.Environment`. Use [Hoogle](http://www.haskell.org/hoogle/) to find where functions were moved to. 2. Continue using the old names via the `haskell98` package. Add the `-package haskell98` flag to the compilation command.
Thanks. Is one of these options officially blessed?
#1 is preferred, since it uses the modern module names. Both are "official".
This is just a quick package I whipped up out of frustration with test-framework scrolling an error message out of sight, for the millionth time. Chell has the same general purpose (aggregate your assertions + properties + whatever into a single executable), but only prints when a test fails or aborts. It also has a small built-in test library, similar to HUnit, so you don't need to depend on 2-3 separate libraries if you're just doing simple tests. Cool features thereof: * it reports the line number of failed assertions * you can use `$expect` instead of `$assert`, so even if it fails, the test keeps going (all the failures are printed at the end) * you can add "notes" to a test, which are saved in logs and reports. you can put in any sort of metadata you want (nice for figuring out why a test is failing) * assertions for text diffs, so if you're testing two big chunks of text for equality you don't have to copy+paste to see what's different.
Ah, amusing name.
I hear there's work on a similarly quiet debugger called Freeman.
While I get the references, my initial reaction to a "Freeman Debugger" would be Morgan Freeman doing a narrative of my code. It would make debugging far more pleasant.
Thanks for the tip; I switched the accepted answer (not to take away from Brian's, which is still good!). I like to think SO is good about notifying askers of further answers, but I agree that seeing the checkmark could discourage later answerers. I'll keep that in mind when asking there in the future. Also, thanks to everyone here for the thoughts and answers.
Yes, Brian's answer was, as usual, excellent and enlightening. Sorry if I implied otherwise.
&gt; If I had to guess I'd say that linking to a mailing list archive is what seemed "suspicious". I sure hope not. A large percentage of the posts here point at mailing list posts.
Thanks for your patient answer to my silly question - I just went away and pasted `System.Environment` as appropriate.
Typo in [release notes](http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/release-7-2-1.html) s/usde/used
While it is possible that ∞·0 = ∞, it is also possible that it equals a finite number. After all, if you define ∞ = 1 / dx and 0 = dx, then ∞·0 = 1. The exact answer requires a more rigorous definition of what limit gives rise to the infinity (and zero).
One of the semiring laws requires that x·0 = 0 for all x, so there is no choice in this particular case as to what ∞·0 is.
If you are interested in chell you may want to checkout hspec, which is very similar. See discussion: http://article.gmane.org/gmane.comp.lang.haskell.cafe/91631
Someone in /r/programming mentioned something, I think it was [haskell-src-exts](http://hackage.haskell.org/package/haskell-src-exts), which creates easier-to-read error messages. Haven't used it myself but sounds quite nice.
How about breakages on http://shootout.alioth.debian.org/u64q/haskell.php ? It's pointless to talk about implementation and ignore shootout.
If they don't have a test framework that found those problems then they have bigger problems than worrying about the shootout.
To be fair, those are probably just incompatibilities between 6.x and 7.x version, so I doesn't have to be that those are bugs with 7.2. But still, speaking about GHC performance and not even looking and fixing obvious compile errors on shootout is ... funny, at least.
Yeah, I missed that. My apologies.
I think some financial companies have been pretty damned successful hiring FP programmers. The even use smalltalk at JP Morgan Chase (not functional, but interesting offbeat language choice) People are coming around to realizing that there are potential gains to be made over C/C++ or even Java and I think that's the point.
Er, they do have a test framework (with plenty of tests,) but GHC HQ does *not* really care about or update the shootout. Other people do that, because other people are the ones who wrote and contributed programs according to the shootout guidelines. It's not GHC HQ's responsibility - go bug the people who contributed the programs. FWIW, none of the shootout programs have failed because of bugs in 7.2.1. It should be just fine on that note. The reason a few have failed to compile is due to some shuffling around of the libraries (`haskell98` in particular) and parsing among other things. Also, because `-fvia-C` is now gone and people thought it would be a good idea to still use that for whatever reason. None of these are hard in any way to fix, and if the shootout is so critically important, it should be well worth your time to either fix them yourself or contact the people who wrote them. EDIT: I emailed Isaac and gave him fixes for the problems he had. There. It wasn't so hard, was it?
Contact the people who wrote the benchmarks. It's not Simon Marlow's responsibility to make sure the shootout programs are always working. EDIT: Because people like to complain about the shootout for whatever reason, I took the liberty of emailing Isaac Gouy on -cafe and giving him fixes for the build failures he was experiencing under 7.2.1. That was not hard. So expect updated benchmarks soon (where 'updated' means 'most all of them are old, ugly and crufty in tons of ways and probably not entirely representative of GHC now anyway.')
I don't think toy programs on the shootout have much to do with real-world performance on anything people actually care about. Shootout results are interesting, but not really all that important.
btw... the `gtk2hs-buildtools-0.12.0` package doesn't install under GHC 7.2.1
You could ask the mailing list: gtk2hs-devel@lists.sourceforge.net edit: reddit markdown doesn't like mailto :o
On OS X, I just spent days of yak-shaving getting chart up, which depends on gtk2hs-buildtools and gtk. A day ago, I would have been thrilled to downdate to gtk(-n) for any n. gtk3? Any particular reason? Installing by hand, MacPorts, fink, jhbuild all failed badly. I had so messed up my system that I couldn't build any cabal packages on new GHC builds from source. Trying to repair my system, I discovered HomeBrew, first for the simple task "brew install pcre". I was astonished that "brew install gtk" just worked, as did "brew install --64bit ghc". In no time I'm compiling 64 bit GHC from source, with the chart package. HomeBrew JUST WORKS. It's getting pretty good mindshare, once one notices.
http://www.gtk.org/language-bindings.php
That page claims the Haskell binding works with 3.0. Is that true?
I tried looking around for it, couldn't find anything. This page should be accurate though...
Feel free to email me at paul.hudak@yale.edu with any comments, suggestions, or questions. 
It's being developed. The reason they are taking so long for a release is because they want the next one to be 1.0 and they are trying to iron out as many bugs as possible before then.
This release, among some major code and documentation cleanups, adds the ability to derive partial labels for multi-constructor datatypes.
Documentation note: in the package description, did you mean to direct people to [Data.Record.Label](http://hackage.haskell.org/packages/archive/fclabels/latest/doc/html/Data-Record-Label.html) and [Data.Record.Label.Monadic](http://hackage.haskell.org/packages/archive/fclabels/latest/doc/html/Data-Record-Label-Monadic.html) instead of Data.Label and Data.Label.Maybe? Because I can't find the latter two in your docs. Though that might be b/c [the docs for 1.0 don't seem to be on hackage](http://hackage.haskell.org/packages/archive/fclabels/1.0/).
Building documentation might take while.
Dear Op: I was playing with this today and noticed the performance was about 20 times slower than the built-in method of changing fields. moveToAmsterdam :: Person -&gt; Person moveToAmsterdam = set (city . place) "Amsterdam" moveToAmsterdam' :: Person -&gt; Person moveToAmsterdam' person = person{_place = (_place person){_city = "Amsterdam"}} ageByOneYear :: Person -&gt; Person ageByOneYear = modify age (+1) ageByOneYear' :: Person -&gt; Person ageByOneYear' person = person{_age = (+1) $ _age person} moveAndAge :: Person -&gt; Person moveAndAge = ageByOneYear . moveToAmsterdam . ageByOneYear . ageByOneYear . ageByOneYear moveAndAge' :: Person -&gt; Person moveAndAge' = ageByOneYear' . moveToAmsterdam' . ageByOneYear' . ageByOneYear' . ageByOneYear' On Windows with -fllvm -O, the Data.Labels method took 100s of nanoseconds, and the record syntax method took ~14ns. Compositions of changes to fields were even worse. I modified `Data\Label\Abstract.hs` and added `{-# INLINE #-}` pragmas to everything, but this had a smaller impact than I desired. So I modified `Data\Label\Derive.hs` in this manner: 77 then [sign, inline, body] 78 else [inline, body] And inserted this at line `114`: -- Generate an inline declaration for the label. inline = PragmaD (InlineP labelName (InlineSpec True True (Just (True, 0)))) I must admit to being somewhat hamfisted with my Haskell. Nevertheless, these changes when rebuilt resulted in these performance stats: benchmarking ageByOneYear mean: 14.43046 ns, lb 14.35430 ns, ub 14.48422 ns, ci 0.950 std dev: 400.4984 ps, lb 357.7391 ps, ub 451.4654 ps, ci 0.950 benchmarking ageByOneYear' mean: 14.22087 ns, lb 14.11162 ns, ub 14.27551 ns, ci 0.950 std dev: 582.7410 ps, lb 509.0278 ps, ub 629.3880 ps, ci 0.950 benchmarking moveToAmsterdam mean: 14.32915 ns, lb 14.23871 ns, ub NaN s, ci 0.950 std dev: 641.2060 ps, lb 593.3684 ps, ub 701.4530 ps, ci 0.950 benchmarking moveToAmsterdam' mean: 14.35755 ns, lb 14.29381 ns, ub 14.42129 ns, ci 0.950 std dev: 572.9854 ps, lb 635.5495 ps, ub 700.4271 ps, ci 0.950 benchmarking moveAndAge mean: 15.61487 ns, lb NaN s, ub 15.69007 ns, ci 0.950 std dev: 596.8406 ps, lb 521.8538 ps, ub 674.0852 ps, ci 0.950 benchmarking moveAndAge' mean: 15.41593 ns, lb 15.34427 ns, ub 15.49207 ns, ci 0.950 std dev: 594.4019 ps, lb 526.5795 ps, ub 696.4952 ps, ci 0.950 Compared to some prior iterations before inlining the labels: benchmarking ageByOneYear mean: 190.0841 ns, lb 188.4183 ns, ub 191.9168 ns, ci 0.950 std dev: 9.097680 ns, lb 5.728966 ns, ub 13.50795 ns, ci 0.950 benchmarking ageByOneYear' mean: 14.23443 ns, lb 14.14696 ns, ub 14.31314 ns, ci 0.950 std dev: 641.0785 ps, lb 536.4181 ps, ub 787.4053 ps, ci 0.950 benchmarking moveToAmsterdam mean: 228.4463 ns, lb 226.9558 ns, ub 229.6657 ns, ci 0.950 std dev: 7.211201 ns, lb 6.952037 ns, ub 7.969696 ns, ci 0.950 benchmarking moveToAmsterdam' mean: 14.28651 ns, lb 14.18488 ns, ub 14.38389 ns, ci 0.950 std dev: 652.7106 ps, lb 538.9212 ps, ub NaN s, ci 0.950 benchmarking moveAndAge mean: 455.8472 ns, lb 453.0925 ns, ub 459.1524 ns, ci 0.950 std dev: 19.91427 ns, lb 18.81354 ns, ub 21.83716 ns, ci 0.950 benchmarking moveAndAge' mean: 15.72424 ns, lb 15.62192 ns, ub 15.78471 ns, ci 0.950 std dev: 679.6129 ps, lb 629.5289 ps, ub 750.7595 ps, ci 0.950 **Edited**: I added additional tests and corrected the last one, "moveAndAge", which shows that when the labels are fully inlined, composing operations on the labels together has minimal performance impact.
Can you compare these results to the previous release of fclabels, and perhaps the data-lens package too?
In my experience writing shell code some things that should be improved are (in no particular order): * needs some kind of record type system and/or tuples since you often end up adding "just one more" piece of information and this is awkward if you have to use something like multiple arrays with the same index * Safer file/directory path operations (so you don't end up accidently deleting e.g. a parent directory because your path component string was empty) * better indication which parts of the language used are POSIX-safe or conform to some other more portable subset of the shell language (probably not a problem if you implement something so different it will use the same implementation on all platforms anyway) For interactive use it would also be nice if there was a standard way for programs to communicate to the shell the types of arguments (and not just "file" either, as specific as possible) and to have the shell derive auto-complete and such things automatically from that. Other than that it needs a syntax that is significantly more terse than most programming languages to be any good for interactive use, favoring often used constructs by making them as short as possible.
Which GHC version did you use? Would you mind posting (e.g. on hpaste or gist.github) the complete code you used for benchmarking?
GHC 7.0.3 on Windows, compiled with -O -fllvm. Source is here: http://hpaste.org/50280
Gentoo packages are not always the most reliable source.
Thanks! I've reproduced it on Linux/x86-64 w/ GHC 7.0.4 and GHC 7.2.1 w/ various `-Ox`/`-fllvm` combinations... curiously, GHC 7.2.1 is slightly faster than GHC 7.0.4 for the lens versions, but significantly slower (takes ~70% more time according to Criterion) for the hand-coded record updates... PS: on 64bit the discrepancy seems even worse for the `moveAndAge` case (GHC 7.0.4): benchmarking moveAndAge mean: 874.3251 ns, lb 874.1108 ns, ub 874.5836 ns, ci 0.950 std dev: 1.196483 ns, lb 986.0476 ps, ub 1.515450 ns, ci 0.950 benchmarking moveAndAge' mean: 11.32464 ns, lb 11.31225 ns, ub 11.34604 ns, ci 0.950 std dev: 81.10763 ps, lb 55.12448 ps, ub 147.0986 ps, ci 0.950 
This is absolutely incredible, thanks a lot for testing this! I'll definitely going to integrate this into the package. Never even knew it was possible to generate PRAGMAs with Template Haskell. 
I've released fclabels-1.0.1 that fixes the performance problems when setting and modifying. Thanks Anpheus!
I'm a bit disappointed to see fclabels move towards arrows, which I had been happily sensing a trend away from in the Haskell community.
So far, this series of posts is reinforcing my original gut feeling that there is more to Arrow than Applicative. It is saying that if an Applicative has additional structure that makes it look like an Arrow, then it is Arrow. Not that I am a big fan of Arrows in practice (other than the trivial instance for functions). I still don't see any sign that Arrows will ever become a subject for which you can assume every Haskell programmer understands fully. So it should only be used in a situation where there is a big payoff to offset the presumed difficulty of maintaining the code base. In any case, this is a fantastic series of posts. This topic was crying out to be carefully studied, and then explained in a way than can be understood by the average Haskeller. cdsmith is doing a wonderful job of it. Thanks!
Why would you want to move away from arrows? Arrows seemed to be exactly the right abstraction for I wanted to do with this package. The fact that the applicative+category are equal in power to arrows doesn't mean they are equally convenient.
Ha! I spoke too soon - [first class record labels](http://www.reddit.com/r/haskell/comments/jhoay/live_from_camhac_ann_fclabels10_first_class/), currently being pushed forward at CamHac, use Arrows! 
I'm sporadically [working on one](http://www.haskell.org/haskellwiki/GObjectIntrospection) with Dafydd Harries.
I dislike arrows themselves for reasons other than that; it is a large typeclass with no obvious explanation as to its structure (why does it have the methods it has? why isn't it broken up into more modular classes?), and I, personally, have a very hard time developing an intuition of what an arrow is; a sentiment I've seen echoed throughout the community. Additionally, the `proc` notation is really ugly and very heavy-weight.
&gt; I still don't see any sign that Arrows will ever become a subject for which you can assume every Haskell programmer understands fully. So it should only be used in a situation where there is a big payoff to offset the presumed difficulty of maintaining the code base. This is probably the main reason I don't care for `Arrow`. It's a useful but somewhat ad-hoc combination of concepts, much like `Num`. If `Arrow` is apparently already too arcane for many people, then unlike `Num` I don't see any reason whatsoever to burden myself with it. I'd rather just hand-roll the stuff I need, or use some [pre-made general abstract nonsense](http://hackage.haskell.org/packages/archive/categories/0.58.0.5/doc/html/Control-Category-Cartesian.html).
But arrows are http://www-mmm.is.s.u-tokyo.ac.jp/~ichiro/papers/fromComptoComp.pdf
Lenses still aren't `Arrow`s, are they? I only glanced, but it seemed like the main difference was abstracting the getter/setter stuff to use arbitrary `Arrow`s rather than just `(-&gt;)`. That's not really a move towards `Arrow`, just being more flexible about how a lens is implemented. An obvious motivating example here, and by that I mean one that I will be personally motivated to use, is lenses built with Kleisli arrows.
Ok. That should certainly clear things up nicely and make `Arrow` easier to understand. I'm still annoyed that there aren't separate type classes based on `Category` for "has a functor from Hask" and "lifts tuples preserving monoidal structure", though.
&gt; So far, this series of posts is reinforcing my original gut feeling that there is more to Arrow than Applicative. It's hinting that way, but don't give up yet. My plan for the future of the series is: 1. Finish up the correspondence... I'm still missing proofs for one Arrow law and a few other details like showing that the translations are inverse to each other and the new laws are all implied by Arrow... which I think ought to be doable, but I've not quite got it yet. That'll be "part 2b". 2. Consider the question of whether there actually exist (at all? useful?) Applicative+Category instances that do *not* satisfy those extra laws. I need to go back and review free theorems, as I hope that they'll shed some light on this question. 3. Look at some non-trivial uses of Arrow, translated into applicative terms, and see how they look. So let's see where this goes.
For what it's worth, the `Arrow` methods *do* all derive easily from `Category` and `Applicative`. The only difference is the laws they're expected to satisfy. In other words, you can write an instance of `Arrow` given those, it just isn't guaranteed to be well-behaved in the general case.
Thanks for the reference. I look forward to reading it, since I've been spending a lot of time lately thinking about questions of how to compose systems with state, and from the abstract, it looks like this might be relevant!
I noticed that, too. That said, fclabels isn't so much *using* arrows as it is providing a more general kind of lens that *you* can use with arrows. The lenses themselves are still not arrows, and can't be since you can't promote an arbitrary function to a lens.
&gt; "lifts tuples preserving monoidal structure" My thoughts are going a different direction. To me, the plumbing necessary with Arrow and tuples is a sign that the abstraction is too low-level. Of course, GHC offers special syntax to work with it... which is a sign that I'm not the only one that finds it too low-level. (Note that unlike do-notation for monads, arrow's special notation is *not* merely solving a syntactic problem; see, for example, section 7.10.1 of the GHC user manual, where the compiler to build this example has to work out where to write and insert new auxiliary `arr` functions to fiddle with tuple structure, and insert uses of `first` to apply intermediate functions inside the tuple.) Applicative, on the other hand, is nicely usable and quite readable without any syntactic sugar at all. *If* (many/most/all) uses of Arrow could be reduced to Applicative+Category, and *if* that turns out to have the same sort of nice readability properties as Applicative by itself, then that would have some benefits. So one of my goals is to explicitly remove the tuple-fiddling, hence why I worked hard to reformulate all of the new laws in terms of functions, not tuples.
I, too, have been following cdsmith's interesting series of posts. :P
btw, this blog posting dates back to *March 16, 2010*
&gt; Applicative, on the other hand, is nicely usable and quite readable without any syntactic sugar at all. Is it really? Remember, we're not really dealing with `Applicative` as a whole here, but rather a generalization of the instance for `((-&gt;) e)`. This instance--defined by function composition and the K and S combinators--is probably most often used to write extreme point-free code, as in e.g. the extremely obfuscated output of lambdabot's @pl. A while back on Stack Overflow [someone asked for an explanation](http://stackoverflow.com/q/6287785/157360) of how these expressions work: filter ((&amp;&amp;) &lt;$&gt; (&gt;2) &lt;*&gt; (&lt;7)) [1..10] filter ((&gt;2) &amp;&amp;&amp; (&lt;7) &gt;&gt;&gt; uncurry (&amp;&amp;)) [1..10] Is the `Applicative` version really any more readable here? In short, there *is* a special syntax to make `Applicative` more readable here, namely *lambdas*.
 filter ((&amp;&amp;) &lt;$&gt; (&gt;2) &lt;*&gt; (&lt;7)) [1..10] filter ((&gt;2) &amp;&amp;&amp; (&lt;7) &gt;&gt;&gt; uncurry (&amp;&amp;)) [1..10] I'd say yes, the Applicative version is definitely the more readable of the two here. Sure, it takes some mind-wrangling to get used to the idea of manipulating functions instead of points, and you have to lose the infix notation for the operator, but those are common to both expressions. In the arrow version, you also have to pack the results into a tuple, and then uncurry the operator, which is exactly the tuple-fiddling I was referring to.
Ok. That surprises me a bit, I would really expect the `Arrow` version to be clearer to most people. But I can't argue the point very well because I actually prefer the `Applicative` style myself, ha. Out of curiosity, what would you think about a combinator that combines the two, e.g.: infixr 2 ~$&gt; x ~$&gt; f = x &gt;&gt;&gt; arr (uncurry f) filter ((&gt;2) &amp;&amp;&amp; (&lt;7) ~$&gt; (&amp;&amp;)) [1..10] 
I guess I'm surprised that you're surprised! :) The arrow version isn't just aesthetically lacking; it involves strictly more different concepts that you have to worry about to write or understand that expression. I don't think well of `(~$&gt;)`, actually. Now you've got a new operator that addresses the special-case of functions of exactly two parameters, and the `(&amp;&amp;&amp;)` operator itself becomes quite difficult to use when you've got arity other than 2, as well -- you have to start worrying about which of the many isomorphic-but-unequal tuple shapes you've got. Meanwhile, on the applicative side, things just work, because functions can be (and *are*, practically everywhere in Haskell) curried, and once you know what function you want to call and what you want to pass to it, there's only one obvious structure for them to have. A better question, I think, is whether things will remain natural like that when you start combining Applicative with Category... there are certainly many ways to combine the applicative terms with composition and identities. One thing I hope to do is find a good non-trivial example of an arrow other than the trivial function one or kliesli arrows, and see if I can work with it and get a feel for how often the arcane bits of these proofs of mine come up in practice.
I do know that, yes. But it's less *necessary* for Applicative. Plenty of people make heavy use of Applicative and don't use SHE (me included). I get the feeling that Arrow, like Monad, would be far less popular without `proc`. By Arrow, there, I mean as a type class. Certainly I heavily use many of the arrow constants, like `(***)` and `(&amp;&amp;&amp;)`, but mainly specialized to their function types when I'm playing around with stuff in GHCi, where I just want to see some information quickly and don't care about meaningful types or such things.
&gt; One thing I hope to do is find a good non-trivial example of an arrow other than the trivial function one or kliesli arrows How's [this for an example](https://gist.github.com/1145278)? It's certainly related to both `(-&gt;)` and Kleisli arrows, but distinct from both. Make sure to look at the example functions at the bottom used to construct arrows, and observe what they do. Note that this is a simplified toy implementation. There are more complicated variations I've seen in actual use.
Sounds exciting, looking forward to it. Thanks for all this!
"proc" notation makes the Arrow version more readable. import Control.Arrow constraints = proc x -&gt; do gr2 &lt;-(&gt; 2)-&lt; x lt7 &lt;-(&lt; 7)-&lt; x returnA -&lt; (gr2 &amp;&amp; lt7) main = print $ filter constraints [1 .. 10]
Btw, if you want some largeish examples of using the Category+Applicative style, have a look at my "FRP" library peakachu and game DefendTheKing.
Now I want to call it Waskell
No, it really doesn't in this case. When you're familiar with the combinators used above, figuring out what the version using proc does doesn't take any less time and introduces more visual noise.
Yeah, I don't like the `proc` version, in roughly the same way that I prefer the `Applicative` example to an equivalent version using `do` notation.
Thanks to the gir, after some initial groundwork, it should be possible to implement a version that is much closer to the original API and stay current without much work.
The blog post is indeed more than a year old. [HTF](http://hackage.haskell.org/package/HTF) on hackage. Most recent update was Jul 10 2011. It appears to be in somewhat active development.
I don't know. But what about a browser, that could interpret something like LLVM code?
Out of curiosity (as its not that obvious to me), what would be a motivating practical example for wanting to use Kleisli arrows based lenses?
Does anyone have any more info on Context synonym families? 
Oh, all the usual reasons of being able to interleave stuff with some other computational context. The included `Data.Label.Maybe` module is an obvious example, or with something like `ST` you could have lenses that do actual in-place updates. Kleisli arrows sort of combine the compositional properties of both regular functions and monadic binding. Makes it easy to build monadic computations in a functional and pointfree style, rather than the imperative and pointful style of a `do` block.
The idea is to allow stuff described in my blog post: http://blog.omega-prime.co.uk/?p=61 More information on the kind of facts here: http://hackage.haskell.org/trac/ghc/wiki/KindFact The partial implementation is pushed to the no-pred-ty branch of http://darcs.haskell.org/ghc.git but is utterly broken at the moment. I'm working on getting it to compile right now.
Congrats, Sebastiaan! looking forward to integrating partial lenses into [pez](http://hackage.haskell.org/package/pez)
66 Haskell hackers in one room, that's impressive.
incredible progress for one event! congrats really interested in checking out CmdArgs, i've always felt arg handling in haskell was too hard a little confused about the notes on Data.Text....i thought it was utf8 from the get-go?
Nope. Opaque API, but utf-16 under the hood.
This is awsome! I'd wondered if that was ever going to make it into GHC. I think it'll help a LOT with presentation of some APIs.
I'm unable to see much on this odd video.
This is a neat idea, but the video is hard to see and while the animation ends after about 30 seconds the music keeps going.
Have another fun example, just to do something more like real programming than just declaring symbols. import Graphics.Gloss picture = kochSnowflake 3 kochSnowflake n = pictures [ rotate 0 (translate 0 (-sqrt 3 * 100 / 6) (kochLine 100 n)), rotate 120 (translate 0 (-sqrt 3 * 100 / 6) (kochLine 100 n)), rotate 240 (translate 0 (-sqrt 3 * 100 / 6) (kochLine 100 n)) ] kochLine k 0 = line [(-k/2, 0), (k/2, 0) ] kochLine k n = pictures [ translate ( k/3) 0 (kochLine (k/3) (n-1)), translate (-k/3) 0 (kochLine (k/3) (n-1)), translate (-k/12) (-sqrt 3 * k/12) (rotate 300 (kochLine (k/3) (n-1))), translate ( k/12) (-sqrt 3 * k/12) (rotate 60 (kochLine (k/3) (n-1))) ] 
 import Graphics.Gloss picture = Scale 0.8 0.8 $ tree' 4 10 tree' degree time = Translate 0 (-300) $ tree degree time (dim $ dim brown) stump color = Color color $ Polygon [(30,0), (15,300), (-15,300), (-30,0)] tree 0 time color = stump color tree n time color = let smallTree = Rotate (sin time) $ Scale 0.5 0.5 $ tree (n-1) (- time) (greener color) in Pictures [ stump color , Translate 0 300 $ smallTree , Translate 0 240 $ Rotate 20 smallTree , Translate 0 180 $ Rotate (-20) smallTree , Translate 0 120 $ Rotate 40 smallTree , Translate 0 60 $ Rotate (-40) smallTree ] brown = makeColor8 139 100 35 255 greener c = mixColors 1 10 green c
I'm thinking about how to do animations, so that `time` variable will be meaningful. It'll be tricky, though, as a server round trip per frame is out of the question...
Poking through the tmp directory, I noticed someone (maybe you?) has been trying more stuff from gloss-examples, and that they often don't work particularly well because scaling was changing the line widths. I've re-implemented the JavaScript drawing engine to avoid that problem, if that was you and you want to try again. The trick was to realize that with the JavaScript canvas API, you can apply a transformation like scaling, set a path, but then clear the scaling before actually drawing to keep the original pen width. JavaScript is weird...
Well, if you can get the necessary types symbolic enough (i.e. no traditional conditionals or pattern matching on them), you can just operations on them to JavaScript with little fuss.
Oh, well, the architecture of this thing is based on evaluation to gloss's Picture type on the server... so once that's done, it's easy to convert that to JSON, and I've already got the rendering code in JavaScript to draw the result. So the only remaining challenge is to build a system where the server sends sequences of such images without waiting for the JavaScript client to request each one, and the client reads them incrementally and draws them as they are received. All in all, shouldn't be too difficult. I'm fairly sure I don't want to try to build a client-side Haskell runtime for arbitrary code, if that's what you meant. Nor do I want to limit to a restricted subset of Haskell, since the whole goal here is to have students be able to run the same code in the web-based setting that they wrote with gloss on a local system.
&gt;I'm fairly sure I don't want to try to build a client-side Haskell runtime for arbitrary code, if that's what you meant. No, no. If you have an abstract type `Time` defined by a few combinators, and a function `Time -&gt; Picture`, then you can symbolically compile this function to JavaScript code easily. For instance, if it needs to be a `Num` instance, then your `Eq` and `Ord` instances have to be errors, but you can implement everything in `Num` symbolically, in the standard method. Then you can just pass `TimeVariable "t"` or whatever to the function, and get back a symbolic tree denoting the transformation to `Picture` (which also becomes a simple symbolic type). It... sounds more complicated than it is. GPipe, for instance, uses this to compile Haskell code to shaders. AwesomePrelude is basically the systematic application of this to every basic Haskell type. But it would be a lot more efficient than the server sending down every frame, and I think not much more effort too. But if things like `time == 9` are OK then that's out, unless you hide Prelude functions...
Does this technique have a name? AwesomePrelude's foldr implementation is pretty scary looking I might add.
"Symbolic programming"? It's also used by the [sbv](http://hackage.haskell.org/package/sbv) package. Yeah, I don't think AwesomePrelude is The Way, since it's basically constructing its own language with a more flexible type system on top of Haskell. But for certain use-cases, this method can make Haskell do the seemingly impossible.
This means that at top-level in a module (g, _) = (id, ()) will infer the polymorphic `g :: a -&gt; a` instead of the monomorphic `g :: GHC.Prim.Any -&gt; GHC.Prim.Any`, as it does now in GHC 7.0 and 7.2. 
&gt; it's ok to say (f,g) = (\x -&gt; x, \y -&gt; True) &gt; and f and g will get propertly inferred types f :: a -&gt; a g :: a -&gt; Int whoops :-)
I'm partisan, of course, but can I just give APPLAUSE for progress on KindFact and PatternSynonyms? Great to see!
Ah, okay I see the idea now. In this case, it won't work, because times in gloss are just floating point numbers. So not only can they be compared, but they can also be fixed by type signatures. If I were inventing the library from scratch, that would be one thing... but I'm trying to implement the existing gloss API as much as possible.
I love the Homebrew + GitHub combination. I've create a formula in a few hours, which got accepted. I don't think i would have the time/nerves to successfully contribute a package to any other distribution.
Oh well; that's a shame. (Especially the choice of floating-point to represent time.)
Clearly, in an effort to improve compatibility with C, data types consisting entirely of nullary constructors are now implicitly interpreted as type synonyms for `Int`. An upcoming version will similarly improve compatibility with Java by wrapping all values of type `Maybe a` in an implicit `fromJust`. Eventually compatibility with C++ will be improved by adding an arcane Turing-complete compile-time template system that produces baffling error messages. Er, wait.
From the "Thoughts on SafeHaskell" section: &gt; There is one thing that confuses me, though… in order to get this working, I had to patch the gloss library to add “Trustworthy” declarations. &gt; There are plenty of Haskell modules out there that GHC could easily prove are safe: if nothing else, just try to build them with the Safe extension, and if it fails, try again without it. Perhaps I'm missing something, but I though that this was exactly what the `Safe` language extension does. That is, if you add `{-# LANGUAGE Safe #-}` then GHC will indeed check for you that you are sticking to the safe subset. &gt; Even worse, in order to make this feasible at all, I had to declare gloss not just safe, but trustworthy. You only need to use `{-# LANGUAGE Trustworthy #-}` when you are using *unsafe* features, and you want to make a claim that you are using them in such as way that the module presents an interface that can still only be used safely. Summary: * Safe means "please GHC check that I'm sticking to the safe subset of the language and libraries" * Trustworthy means "really GHC, I know I'm using unsafePerformIO but the external interface is pure, honest!" Note that while you would usually believe your own `Trustworthy` annotation, you might not believe such annotations in other people's packages. That's where the package trust system comes in, you get to decide if you're prepared to believe that other authors have audited things correctly.
Yes, HTF is under active development and we are also using it in our company for our tests. (I'm the main author of HTF.)
Just idle curiosity, but why do you think this is the case?
`insertWith f` will insert `f new_value old_value`, so using `(++)` it _prepends_ the new item, rather than appending as you might expect. Since the run time of `(++)` depends on the length of its first argument, this does not cause quadratic appending as one might think. 
A real use case for SafeHaskell is going to give great info for how to improve it.
The problem with `Safe` is that it has to be specified, so in particular, libraries whose authors didn't think about them being used with SafeHaskell will likely have to be patched. If you patch them to use `Safe`, then their *dependencies* also have to be patched, and *their* dependencies... so in practice, when you patch the library, you'll add a `Trustworthy` annotation instead. That's bad, but maybe less bad than spending a few days making rote changes to other people's code and installing dozens of packages that you've locally modified just to add Safe annotations to the tops of modules. This is all unnecessary, too. GHC is perfectly capable of deciding for itself when a module is safe or not... the only reason you'd need the Safe annotation should be when you want compilation to actually *fail* when the module is unsafe. That should really only be necessary for compiling the untrusted code itself; GHC is quite capable of inferring safety for the modules that one imports.
Ah ok, I see what you mean. Yes, it's a problem until more core packages have safety annotations. Yes, I think that'd make a lot of sense. There are many packages of ordinary code that consist only of safe code but where the author isn't aware or doesn't care. As an interim workaround, you could try compiling those packages with `--ghc-options=-XSafe`. That will tell you if your assumption is right. Perhaps it will not be so bad and we don't really need safety inference. The feature is still very new so of course packages don't have the annotations yet, but it's easy enough for people to add `extensions: Safe` in the `.cabal` file for the simple case where no `Trustworthy` annotations are required, so perhaps within a release cycle or so we'll be in a better situation.
I see cdsmith's point. GHC could check safety on every compile and mark the module safe or not (in the .hi file I presume). ghc-pkg could read and report this to the user. When compiling a top-level module you can specify if you want to reject unsafe code.
It's a little more complicated than "safe or not", but yes, that's the idea. The only change is that the determination wouldn't just be yes-or-no, but would really look something like a set of trusted packages needed, as a boolean expression. For example, suppose this module `Main`, in a package `main`, directly imports modules `Foo` and `Bar` from packages `foo` and `bar` respectively, both marked as trustworthy... and `Foo` and `Bar` both import a module `Baz` from package `baz`, which is also marked trustworthy *and* uses unsafePerformIO. Then you want to consider the module `Main` to be safe precisely when *either* `baz` is trusted, or *both* of `foo` and `bar` are trusted. But note that package trust can change due to `ghc-pkg trust` commands without recompiling, so those conditions need to be stored in the .hi file and checked when `Main` is compiled. My understanding is that this is already done, though.
&gt; Perhaps it will not be so bad and we don't really need safety inference. The feature is still very new so of course packages don't have the annotations yet, but it's easy enough for people to add extensions: Safe in the .cabal file for the simple case where no Trustworthy annotations are required This is only easy if all of the packages you depend on have already fixed their safety-correctness. If they haven't, then you're breaking your build, since adding Safe has *both* the effect you want (storing safety information) *and* the effect you don't want (breaking the build if safety can't be proven). So basically, the whole Haskell community has to coordinate doing this in a sort of topological order. And inevitably there will be package authors that don't care much about safety and decline to worry too much about it.
"Strings will no longer be represented as a list of characters, but sequences of bytes that must be explicitly null terminatedW??R ???61F`u?7?7"
Would be interested to see it updated to ghc7. I think the plugins library doesn't work with ghc-6.12 and later?
Thanks to Oscar Picasso for [pointing this out](http://www.haskell.org/pipermail/haskell-cafe/2011-August/094524.html).
Looks very interesting, and I can hardly wait to try it out, but alas I can't install it w/ GHC 7.2.1: $ cabal-dev install tkyprof Resolving dependencies... cabal: cannot configure tkyprof-0.0.6.2. It requires yesod-form ==0.1.* For the dependency on yesod-form ==0.1.* there are these packages: yesod-form-0.1.0 and yesod-form-0.1.0.1. However none of them are available. yesod-form-0.1.0 was excluded because it could not be configured. It requires email-validate &gt;=0.2.6 &amp;&amp; &lt;0.3 yesod-form-0.1.0.1 was excluded because it could not be configured. It requires email-validate &gt;=0.2.6 &amp;&amp; &lt;0.3 
this is not a new thing, posterous accepts github gists, and therefore haskell syntax highlighting, for over 2 years, it's even in your link.
But why can't we have a "best-effort safe"-mode (as opossed to the current `-XSafe` which requires success to validate, and thus isn't suitable for setting as global option in `~/.cabal/config`) by default? I.e. why not automate one which would probably handle most packages on hackage w/o any user intervention?
I'd still expect `[v] ++ old_value` to be as fast as `((:) . head) [v] old_value` at most.
I suppose you're right about pipes being fairly lazy by default. I could imagine a more aggressively lazy variation (though I'm not sure if this is really a good idea). Let's say we have a "pure" program called primes that generates primes on stdout: $ primes &gt; primes1.txt $ cp 1primes.txt primes2.txt $ cp 1primes.txt primes3.txt Now, let's say that the thee text files have an infinite size, but they are stored as a thunk that is only evaluated when any one of the three files is read, and then it's only evaluated as much as it needs to be, like an infinite list. I suppose (thinking out loud) there needn't be much of a distinction between files and variables in the shell programming language -- it would be clever to contrive a way to use them relatively interchangeably.
Part of what you say (storing as a thunk) is possible today by using named pipes. However it is evaluated fully on cp. But I can imagine a different semantics for cp that does it correctly. However I get your idea. Variables shouldn't be required in a shell language, rather we should be able to use temporary files that are session specific (and perhaps scoped) as variables instead.
You might also want to benchmark `insertWith'`, the value-strict version of `insertWith`. Right now your `Map` might have unevaluated thunks in it (which won't be forced by Criterion's whnf).
It looks beautiful. Besides "look for the largest outer arc" i find the charts hard to grasp, though.
Yes, I think it would be clearer if the arcs were labeled with the corresponding cost center.
I managed to install it under GHC 7.2.1 by manually relaxing some upper bounds in cabal files. See the [Haskell Cafe thread](http://www.haskell.org/pipermail/haskell-cafe/2011-August/094693.html) for more details.
Thanks for trying. I'm an author of TKYProf. Currently, the radial scale is a power scale (d3.scale.sqrt), accodingly the outer arc is thinner than the inner arc. I'll change the scale to a linear scale, and try to label arcs too.
This is very nice, I tried it and like it a lot, but I agree that it is a bit confusing at the moment. With labels I think it will be very useful.
Because you can implement process calculus in the lambda calculus.
You might be interested in Pict, which is a language with that particular goal: http://www.cis.upenn.edu/~bcpierce/papers/pict-design.ps (PS)
Maybe you could also consider non-arc visualization. Radial visualizations like pie-charts are not so suitable for comparing quantities: http://blog.revolutionanalytics.com/2009/08/how-pie-charts-fail.html
I don't recall concurrency being one of the goals of the original Haskell committee.
When I did a quick google search before posting the link, I found a statement that the λ-calculus can be implemented using the π-calculus, but nothing about the inverse. Does that mean the two are equivalent, and is there a proof of this statement? EDIT: phrasing/spelling
Interesting. The compiler I found hasn't seen any changes since 1998, so I guess the project is dead. I wonder if that's because the developers lost interest or if there were issues with the language itself.
And you can implement the lambda calculus on a Turing machine. (And I suspect that many process calculi can also implement the lambda calculus.) What gives? Choosing a semantics for a programming language involves more considerations than simply expressivity. One semantics for a language may be preferable to another because it makes proofs easier, or simplifies complexity analysis, or makes it clearer how one might _implement_ the language.
To show that you can implement lambda in pi is sufficient to show equivalence. Lambda was proven to be equivalent to Turing machines, and the two formalisms are the bases for defining computability in the first place. Basically, that pi calculus is implementable at all is proof of the other way around.
I think the advantage of using a sqrt scale is that all rings have the same area. This is a good property because area corresponds to cost and the cost of each ring are the same. If outer rings are too thin to write a label into them, maybe the labels could be written outside like in [this chart](http://www.databasejournal.com/img/2009/07/DJ_RS067-034.jpg)? Probably, labels for the outermost ring would suffice.
That makes sense. I'm still a little unsure about what that really means, though. I understand that Turing equivalence has implications on what can be computed. Does this also mean the *ways* in which something can be computed are essentially equivalent? Given my limited understanding of the matter, I see process calculi as a way to implicitly include time in the model of computation, something which seems to be missing from the lambda calculus.
LISP was based on the lambda calculus. Haskell is based on typed category theory. (The lambda calculus is still in there. Look how our vision works; every system back to prehistoric fish is still in the messy code base.) Future functional languages may be based on something like univalent type theory; see http://www.math.ias.edu/sp/univalent
There's a CSP implementation by the name of [CHP](http://hackage.haskell.org/package/chp) if you want a process calculus. `par`, though, just uses a completely different model. You can't [dead|life]lock with it, so any process calculus loses its appeal. Then, the ghc guys are just too busy to implement process calculi, but it's not necessary, either, because all the primitives and language flexibility are there. Implementing any process calculus without having anything like forkIO, is, TBH, an exercise of ivory-tower proportions.
Fair point. I guess the current haskell community and the original committee are quite distinct in multiple ways.
How much of the current community is focused on concurrency? More interest is probably shown in parallelism than concurrency, and process calculi aren't necessary for that.
In *practical* terms, Turing-equivalence is roughly a generalization of [Greenspun's Tenth Rule](http://en.wikipedia.org/wiki/Greenspun's_Tenth_Rule)--you can write any possible program in a Turing-complete language, but nothing guarantees that there's an easier way to do so than first writing an interpreter for a better language. For instance, while Java is Turing-complete, there is a non-empty class of programs for which the easiest way to write them in Java involves first inventing Clojure.
And let me add for completeness: In a process calculi like the pi-calculus, you can similarly embed the lambda calculus. In my point of view, it doesn't really matter if you are *based on* as much as you *support* concurrency. Also note that there is a striking similarity between some process calculi and the lambda calculus. So in the spirit of Haskell being LC+Stuff, you could regard it as LC+Concurrency+Stuff.
Personally, I use Haskell and don't care one whit about concurrency or parallelism. Haskell is first and foremost a nice language, with properties that make it easy to understand, analyze, and work with. I'm thrilled that some people find those properties useful for solving problems in concurrent and parallel programming. I find them useful for solving problems in organization of code, and plenty of other problems, too. So that's why Haskell isn't based on a process calculus: because Haskell is *not* about concurrency; it's a general purpose language that can be used for everything from high-performance scientific computation down to a nicer way to approach scripting tasks. Haskell doesn't need to become a single-purpose language to be useful for solving any specific kind of problem.
As someone thinking of switching his crappy blog to posterous, this was relevant to my interests. Thanks.
Or `(const (v :)) [v] old_value`.
&gt; Haskell is based on typed category theory Er, no it isn't. It's based on the typed lambda calculus (specifically system F omega). No category theory need be applied (Although a correspondence exists, just as there is a correspondence between category theory and virtually every other math discipline).
Shh, don't tell Caterpillar. They still use it for their financial systems.
Won't this be accounted for by forcing the last element of the value list into whnf (as `one` does)?
Is this available to non-UK people?
I also migrated my blog there, the only thing that sucks is that posterous does not allow MathJax, because you can't enable custom javascript.
I agree - it's pretty, but the circular layout is utterly confounding. What do the concentric rings mean? What about the colours? The chunks that stick out? The chunks that are missing? I generally dislike whizzy infographics that hide their meaning behind slick but confusing visuals, and this is unfortunately a perfect example of the genre right now.
Pure functional languages enable you (potentially) to use concurrency without explicitly reasoning about processes and communication - when you don't have side effects, you don't need to worry about serializations. (The language implementation needs to take care of everything, but it doesn't need to expose all the nasty bits that you need in C++ or Java). IMO, process calculus is a mechanism for describing concurrency problems and looking for solutions, but not a good way to avoid them.
Are you sure `whnf one foo` does what you think it does? It looks very odd to me. In particular, the weak-head normal form for a _function_ is anything that starts with a lambda... which `one` already does. What happens if you try `whnf (one foo)` instead?
It probably won't help you learn Haskell, if that's what you mean.
My understanding is that the goal of the Haskell committee was to make a free version of Miranda. :)
Thankyou for making the distinction between the two. It is an important one. 
Haskell is more about parallelism than concurrency. Concurrency was never one of the initial goals.
Take a closer look at the [Criterion docs](http://hackage.haskell.org/packages/archive/criterion/0.5.0.0/doc/html/Criterion-Main.html). `whnf one pushX` tests the time required to compel the result of `one pushX` into weak head normal form.
That's the point of category theory, right? To sort of abstract over various math disciplines.
Never heard of it. LYAH is in the "frequently bought together" section of the Amazon page for it, though.
A student here, What I just learned seems to imply that Turing equivalence is applicable only to functions. (some thing that calculates an output based on an input.) It does not apply to processes that are not functions. A typical concurrent program (or even ordinary programs that do IO) does not seem to fit that model. 
Awesome. I assumed `whnf` came from the Strategies family of functions.
Doesn't Turing completeness only specify functions? A program that includes the concept of time (i.e IO) does not seem to fit in the same category. 
It's been a while since I took theory, but I believe this can be resolved by considering a program and its IO as the inputs/outputs of a universal Turing machine. It's been shown that one infinite tape is equivalent to arbitrarily many tapes, and you may intuitively consider every input or output action as reading from or writing to another tape. As to concurrent processing, I would argue that I could build a Turing machine that takes any two operating on the same tape, and interleave their execution such that concurrency is simulated. And a UTM, of course, could take your two programs and their inputs, along with my program, and simulate their behavior. If my logic is serving me correctly, from the premises that UTMs are a subset of TMs, and all TMs are, as you say, functions, it follows that IO performing and concurrent programs qualify as functions. Besides, if the pi calculus were strictly more powerful than Turing machines, you could bet a hoopla would have been made and the Church-Turing thesis would have been disproven.
Thanks. I suppose, bar charts are suitable for flat data. On the other hand, profiling results are hierarchical. To visualize quantities for hierarchical data, multi-level pie charts or treemaps are used commonly. That's why I used a pie chart. But I agree with your opinion that it is hard to compare the quantities for now. I want to fix it.
I agree with most of your logic except for a minor part. Your definition assumes infinite input. However TM does not include infinite input (only infinite input tape - not the amount of input it starts with). To see why it is a problem, imagine how you would convert such a TM with infinite input to lambda calculus. Pi calculus is not strictly powerful than T.M as long as finite input is considered. However T.M says nothing about infinite input and hence Pi Calculus does not violate Church Turing thesis when it deals with infinite input. Also see limitations: concurrency. [TM](http://en.wikipedia.org/wiki/Turing_machine)
Hmm. I see. &gt; a sqrt scale I'm going to label rings. Thanks.
Thanks for your comment. Ideally, the chart is identical to the original profiling report. But current charts lose some infomation, such as "hot" cost centers in the middle of the original report, and put extra information, such as misleading colors that you mentioned above. I'll try to improve the chart so that the chart and the original report contain the same infomation.
Yup. It's very interesting, but not at all foundational to Haskell. 
Most directly it relates to decision problems, but you can also discuss "Turing computable functions." You can model time with lambda calculus--it may not be the most convenient formalism and it depends on what you want to prove
And it wasn't about parallelism to begin with. It's a happy accident that Haskell's pure, single-assignment code is perfect for parallelization.
You have to be suspicious of claims like "strictly more powerful." Powerful according to what? If you pose a question to a problem formulated in terms of the Pi calculus, and you have an effective procedure to get a yes or no answer to that question, then you are guaranteed to be able to program a Turing machine to answer that exact same question. The Turing machine--itself--may not be a very good picture of an actual concurrent machine (in terms of deadlock analysis etc) but that doesn't mean you can't embed concurrency-related logic into a Turing machine program.
Looks awesome!
Thanks for posting this! I was planning to spare Reddit having to read *every* week's report, but I also got a few emails asking me to submit it to Reddit. I guess I'll keep holding off myself, and let other people submit these if they turn out to be sufficiently interesting.
I believe that "strictly more powerful" is that it can accept/decide languages that a TM cannot.
Yeah, the purpose of category theory is basically to abstract over abstractions. Also, "points" are more a set theory thing, whereas category theory prefers working with the behaviors of arrow composition while eliding elements "inside" the objects. So category theory tends to pretty consistently be what Haskell programmers would call "pointless".
Nice. Have you thought about putting "Graphics.Gloss" into some kind of Prelude?
Hey Chris, great to see you are off to a good start! If you are not familiar with it already the paper [Children's Mental Models of Recursive Logo Programs](http://www.stanford.edu/~roypea/RoyPDF%20folder/A27_Kurland_Pea_85.pdf) is very relevant for where you are going. I wish you the best of luck in sculpting accurate mental models in the kids' minds, and please let us know what works and what doesn't!
Something like this has been suggested a few times. But I've reached the conclusion that it's not a good idea. I really want to present things as they are: Haskell as a general purpose programming language, and Gloss as a library for graphics. I'm making a deliberate effort to avoid sending the message that we need to use a toy language because normal programming is too hard. Needing that import has not confused anyone, and it actually provides a place to explain that we *happen* to be writing Haskell programs using Gloss, but that there are other libraries for doing other things in Haskell, too.
Just submit I would say. Some people are obviously interested, when not you'll get downvoted soon enough. Nice job!
I see. So you are not aiming for a Scratch-like system which hides the underlying language/system.
I agree, just submit. For one the Haskell Reddit is not high-volume and i believe, stories about non-academic use of Haskell are not over-represented yet.
I cannot believe that you went through that much in an hour either. Very encouraging and interesting read. On thing though - I know it is tempting to get these kids to introduce themselves and have pictures of them etc. I wouldn't do that without parental consent, and even with parental consent, I don't think it is a good idea. Their parents should be the only ones that leave any sort of digital footprint of their children.
Some ideas for visualizing hierarchical data: * http://www.timshowers.com/2008/12/visualization-strategies-hierarchical-data/ * http://flowingdata.com/2009/11/25/9-ways-to-visualize-proportions-a-guide/
Do you know Google Performance Tools? Its output is not pretty but incredibly useful, for example: http://www-flc.desy.de/flc/flcwiki/J%C3%B6rgenSamson/ProfilingWithGooglePerformanceTools?action=AttachFile&amp;do=get&amp;target=perfout.png * The bigger the box, the more often the function is called * The numbers along the arrows are the gross time You immediately see the call-hierarchy and where the time is spent.
Also /r/haskell is sparse enough that I think most people are happy for any new content.
Title: An Introduction to Functional Programming Through Lambda Calculus
"The last good thing written in C was Schubert's Ninth Symphony." LOL!
The webpage says "The position is a fully-funded post for a UK or EU student, and includes both coverage of fees and an EPSRC-level stipend for each of the three years." so I guess so
It also seems to me that enabling Safe in the package's .cabal or with LANGUAGE pragmas breaks compatibility with any GHC &lt; 7.2.1. I don't see this fact helping the near-term proliferation of SafeHaskell. What is the workaround? IFDEFs?
I assume the documentation like for instance http://gloss.ouroborus.net/ isn't enough to get you going?
Hotwire-shell is a libre python-based loose clone with some limited "typing" of its builtins' IO. Conceptually not a bad start with hideable output buffers, though I found it somewhat frustrating to use in practice. It seems no longer actively maintained.
Especially when you look at how benchmark codes include manually crafted calls to Hardware-APIs like MMX, e.g. in http://shootout.alioth.debian.org/u64q/program.php?test=spectralnorm&amp;lang=gpp&amp;id=6 What they're benchmarking is libraries not language implementations. Haskell entries could just FFI those too. What would be the point?
It was nice seeing girls enjoying programming.
 one push0 ==&gt; last $ (foldl' (flip $ (\k v -&gt; insertWith (++) k [v]) 'a') empty [1..10000]) ! 'a' ==&gt; last $ (foldl' ((\v k -&gt; insertWith (++) k [v]) 'a') empty [1..10000]) ! 'a' ==&gt; last $ (foldl' (\k -&gt; insertWith (++) k ['a']) empty [1..10000]) ! 'a' So there's no forcing of the elements happening here. `foldl'` only forces the map to WHNF.
Right, but the element forcing is being done by `last . (!'a')`. update: I benchedmarked with `insertWith'`. It shaved off a couple tenths of a millisecond, but didn't change the ordering.
It is not an accident. We were very well aware of the potential for parallelism in pure functional languages. Already in the 80s. And we had some good implementations of parallelism that predate Haskell. 
I'm certainly no expert, but the pi calculus doesn't appear to say anything about "infinite" input, either.
No worries, it's been discussed, and parents are on board, and are reading the blog and helping out. Kids are being referred to by first name. If a kid posts with too much information, I'll edit the comment.
there is an amusing precis of the vicissitudes of parallelism + functional programming in the 80's in this lecture by Peyton Jones starting about 5 minutes in. augustss can fill in the details for sure. http://www.youtube.com/watch?v=NWSZ4c9yqW8
I guess you'll need to provide more information. If you just want to graph some functions and see what they look like, I'd probably suggest using an application like gnuplot instead of writing your own code to do it. On the other hand, if you need to graph functions inside of some code, then there are plenty of questions you'd need to answer: 1. What's your environment? Is this a GUI application? Web application? Command line app that writes images? 2. If you're already doing graphical stuff elsewhere (for example, if it's a GUI app), then what other libraries are you using? Note that while it's really easy to draw decent graphs for most functions, it can actually be somewhat difficult to get graphs right around discontinuous points, so writing your own code for graphing functions can be not exactly trivial, depending on how picky you are about the result.
Wouldn't it be easier to create FFI bindings to existing library (pdflib) ? Reimplementing the entire pdf standard is a huge project. And we have examples (gtk2hs, qtHaskell, wxHaskell) where it is more feasible just to use existing libraries. 
well, I assumed reactive systems could be modeled using pi calculus. But at this point my understanding is limited. (Though I have trouble seeing how continuous input can be made to work with lambda calculus.) However I plan to study further on this topic. 
If you have gtk bindings working, the chart library is pretty powerful: http://hackage.haskell.org/package/Chart
Gloss is neat but not really a graphing utility. Perhaps [Chart](http://dockerz.net/twd/HaskellCharts) would be more appropriate?
I don't get it. Why not just use the unicode symbols?
I think in Fortress, their main motivation was that ASCII is easier to write (and more portable) and Unicode is easier to read. For me, I find that for writing Unicode without an OCR input system, you either need keyboard shortcuts (that have to be remembered) or you can memorize the hexadecimal ids of symbols, or you can engineer some sort of pop-up frequently used character map. None of these solutions is particularly appealing.
Exactly. In this case, as you've phrased it (language acceptance), it is now impossible to built a computer that faithfully approximates the pi calculus. I.e. if the pi-calculus models computations that read an infinite amount of input and produce a "yes/no" answer in finite time, then these computations would not be implementable in any machine we currently know about. If you're modelling machines that pass messages based on an infinite stream of input, process calculi are certainly more amenable to analysis than thinking of them as Turing machines (Turing machines are a clunky way to represent most systems, actually, and it's hardly ever done). But we don't make claims about those machines we are modelling (the objects described by our calculus) as being able to give answers to a wider set of language acceptance problems than Turing machines. We model infinite processes in a convenient way--we don't offer a {machine/rewrite-system/effective procedure} that can supply answers that a Turing machine cannot.
I'm normally sympathetic to that position, and dread seeing unicode math symbols in source code. But when you are already going to these extremes to memorize large numbers of ASCII "shortcuts", you might as well memorize the unicode code points, or shortcuts for inputting them, instead.
I'd love to be wrong on that. Who does not want native haskell library? The reality though is i need advanced features, not just simple pdf file with some text in it. I need to integrate tiff files (and multipage tiff files), i need annotations, bookmarks, table of contents, encryption, embedded fonts. I need to manipulate existing files, extract pages, add to them. Unfortunately hpdf is just a toy. And until someone spends money and time and implements all that, there's no way me or anyone involved professionally with pdf files will touch hpdf. 
Actually, I was thinking that the regularity and visual similarity of these ASCII translations would make them a good worst-case fallback shortcut for entering the unicode--just type the ASCII-art version and use some sort of auto-replace or whatnot to get the unicode. Which doesn't answer the question of when you'd need *that many* different kinds of arrows, but still...
TeX input mode gets you reasonably far without requiring OCR, chorded shortcuts, or memorization of hexadecimal ids. I seldom forget what to type to get an α or a β.
We use commercial pdflib and open source iText. We do use encryption. btw iText is a failry complete library and is open source. 
The url crashes Chrome 14 for me. I'm downloading at 10KB/sec right now via firefox. Would be nice to have a Youtube or torrent link. 
Just historical organic growth. Over the years we used and switched between several libraries. Pdflib, preptool, iText. None of them is complete. We were often in situation where feature A was in one library and feature B was in other. So we bought both (preptool could do byte serving, but pdflib could do encryption). Over the years though the libraries evolved too. Btw i [looked once](http://stackoverflow.com/questions/5570460/haskell-ffi-for-pdflib) into writing FFI for pdflib, but quickly hit the wall. pdflib requires G++ and haskell FFI works only with gcc. So you would have to write a wrapper in C first. Or you can download one of their packages for python, perl, php etc, and see what dll they use there. It could be that one of them already converted to gcc (i did not look that far).
Do you have a link for background on this? The report didn't link to anything either. edit: Nevermind! I just scrolled down a bit :-/
My interest here is purely practical. I'd love ANY haskell pdf library at this time as long as i do not have to wait 5 years for it. And let's face it, ffi to existing library can be made much sooner than creating a complex and feature rich library from scratch. So i am not at all against native haskell library. I just do not believe we will see the useful one in near future. 
&gt; As far as I can tell, the NY Functional Programming Meetup is pretty dark Don't you guys have the hilariously named [Functional Alcoholics](http://www.meetup.com/Functional-Alcoholics/)
I did not know of this. Much thanks! *Update:* So I went to Socially Functional and it was just the organizers, Chris and Nathan. They're pretty cool actually. I enjoyed talking to them a bit about what they're doing with unfiltered (https://github.com/n8han/Unfiltered), and one of them asked me about referentially transparent IO. Am I gravely mistaken? Because I told them IO is not referentially transparent in the sense that they were thinking, but the values in the IO monad can be treated as such. You can write pure functions and lift them into the IO monad, and write most of your program in that referentially-transparent manner. I didn't get into what you can do with iteratees/enumeratees. Anyway, they are both Scala guys, and I don't think they would mind if any of us dropped in on their thing if we were bored one Saturday noontime.
My experience is that when people want to talk about something being "more powerful" than a TM, lambda calculus, etc., they're generally either introducing nonsense that can't be physically realized (such as *actual* infinities, rather than the merely *unbounded* resources a TM demands), talking about underspecified open systems (which thus can't be proven to not be chatting with a Halting oracle on the side, or some such rot) or modeling concepts that are impossible to work with directly using standard formalisms (which is, y'know, the whole reason we have *compilers* in real life). 
I see it as more of a reference than a memorization table. For example, a Chemistry DSL might use these for equilibria or reactions. It's for those rare cases when you use an arrow symbol for something in your industry and want to carry that directly into your Haskell code.
iText is Open Source in the sense that you have to Open Source your code as well, even when your code runs on a server. This might bring you legal trouble: ["Why AGPL (and to a lesser degree GPL) is a business problem"](http://whoeversaysit.blogspot.com/2010/07/why-agpl-and-to-lesser-degree-gpl-is.html) You might consider Apache PDFBox also.
Chris, do you have a need for it or would it be just nice to have it? I see a lot of "it would be nice to have"-projects fail, because the cost of the huge effort is not compensated by solving any huge problem.
I feel for you. I'd prefer to write everything in Haskell but found it is not realistic. For example writing iOS apps or Mac OS GUIs in Haskell is not realistic for me. I'd spend a lot of time and energy in making the infrastructure work, which i'd rather spend in the product. The less sparetime i have, the more carefully i think "will this investment ever pay off?". I fear even with Haskell there is no silver bullet. Maybe we should rather concentrate on making the integration of a those tools and languages more pleasant.
I am in the middle of a project that requires parsing PDFs. For this project, my needs were adequately served by using Xpdf tools to unpack the PDFs as PostScript and extract the graphics, then parse the PostScript. (Good thing it's easy to write parsers in Haskell.) But I would have loved to have either a native Haskell PDF library or a binding. The features I would have needed are: text, fonts, graphics, page layout, and line art. I.e., all of the original features of PDF as a layout format that allow you to render or print PDF pages. Without all the fancy interactive feature-creep that was added later. That would still be significant work to implement, but not unreasonable.
These are good ideas for how to think about selecting operator names in Haskell. Using Unicode directly as names of operators in Haskell is controversial. Even if you are not opposed to that in principle, I would recommend waiting until support is more solid across the entire tool chain: compilers, editors, IDEs, blogs, syntax highlighters, refactoring tools, etc.
In the Synopsis Hamlet example, the `link` element has no closing pointy bracket.
That's intentional. Closing brackets, while encouraged for legibility, are in fact optional.
lol at the *'if I can't use it from ruby, it has no reason to exist'* guy
I'm in.
Cosign on this as a good idea.
The group is sadly named "socially functional" now, and meets (outside of what appear to be very unattended weeking hacking things) infrequently. Also, last I checked, very much Scala-oriented.
My preference is to define operations first with sensible names, then provide ASCII and/or Unicode operators as optional aliases. This not only avoids the potential awkwardness of Unicode, but also makes it easier to deal with name clashes by being able to fall back to the longer, more explicit names. For example, leaving aside the problems with `Num`, there's more than one type signature an overloaded "multiplication" operator could sensibly have. Rather than trying to reconcile things like type-Nat-indexed matrix multiplication vs. scalar product in a vector space vs. regular multiplication in a ring, I'd define each separately as needed and alias `(*)` to whatever I'm using the most in a given module.
Do you plan to keep code like the following? stateState == 3 I think HPDF's monadic approach is more Haskellish than FPDF's state manipulation code like beginPage() { state= 2; }
I use the Haskell gnuplot package to graph from code. It has worked well for my fairly simple needs thus far.
I thought GHC's code generator creates C-- code.
Don't think so.
GHC uses a variant of C-- for its internal representation. It can also read this variant, and parts of the runtime are written in it. (look for *.cmm files)
had trouble commenting on the blog, so: If I'm in my cabal project Foo directory and I have a tests.hs file that imports Foo, and I do a &gt; cabal repl tests.hs would that have tests.hs import Foo from the working Foo code tree (or maybe the most recent 'cabal build')? If so that would be killer.
&gt; Also, last I checked, very much Scala-oriented ...and who wants to hang with *those* guys.
Today I learned. Thanks for the info.
It is rather different from what is specified by the C-- project. So for the purposes of that being some kind of cross-compiler high-level assembly, one still shouldn't really count GHC.
http://hackage.haskell.org/trac/ghc/wiki/Commentary/Compiler/CmmType 
Yes, it is pretty much dead. But some of the C-- people are doing other awesome things. Like [Hoopl](http://hackage.haskell.org/package/hoopl) which is a generic framework for optimizations of low level code using data flow analysis. Hoopl doesn't provide any of the syntax that C-- provided, but if you want to write a backend then Hoopl can do some pretty amazing things for you.
Variations of the name C-- are not uncommon among compiler writers. The ocaml compiler has had an internal representation called Cmm long before the C-- project started.
all the yesod stuff is coming along nicely. good work michael.
I can't take the credit this time. Greg's really been pushing the bulk of the stuff on this release, along with some of our newer contributors like Luite and Blake. (Sorry for anyone I'm leaving out, Greg's done more merging than I have.)
Well, Simon Peyton-Jones is one of the main names listed on the C-- project page. So I'd be surprised if he didn't, at one point, intend actual C-- (as the project defines) to be part of the GHC back end. However, what is actually implemented in GHC feels significantly closer to assembly, I think, while C-- tries to allow you to write in a slimmed down C for the most part.
Nice SSL certificate.
Sounds like a great job. Nothing like some cold hard benchmarks to demystify the internecine tradeoffs involved in unicode encodings.
Well they do try a little too hard to be successful. Don't get me wrong, I respect them. I just think that we have somewhat different goals and it doesn't hurt for us to have our own space.
I just did not go to the site last time, but now that you point it out the certificate is rather hilarious. SomeState, StomeCity, SomeOrganization, Inspires confidence all around.
It's more like once you start talking about your respective ecosystems rather than big plt-type things, you tend not to have much in common. And even on the plt level, the concerns are pretty different.
Yes, but it largely died because of Norman Ramsey not receiving tenure at Harvard and having to scramble around to find a place to land academically before restarting over at Tufts, not because of its technical merits. A variant is still used heavily in GHC; I'm actually handwriting a lot of c-- at the moment myself.
Yes! The fatal flaw of the Turing machine is that the average person can actually picture it as a whirling gizmo that operates on a real tape; one feels confident, then, invoking other whirling gizmos with crazier gadgets under the impression that one is breaking important theoretical ground.
I suspect this will be too easy for many /r/haskell readers but I think a lot of us are beginner-ish enough that this might be a fun experience!
It seems that if you already have UTF8 you will want a UTF8 library, and if you already have UTF16 you will want UTF16. Rather than having a goal of one true Text package, why not have a goal of releasing the UTF8 fork as a separate package?
You forgot to mention: keyboards. Do I really want to use cut and paste or memorize unicode sequences to program?
This is a fun idea! I strongly urge you to post! (Someday I'll try and do the same for dependent types...)
Because then they would be different types, which is not a good thing since strings/text are supposed to be used in module interfaces, so a proliferation of types there is not good. In my opinion, in an ideal world, we would have only two such types: * `Bytes` (what is currently called `ByteString`) * and `String` (what is currently called `Text`) Then there'd still be `[Char]` for when it's needed, but without any special alias.
Perhaps we should also be looking for an alternative to the ICU lib to provide us with the higher level unicode text handling functions, but using utf8 encoding. Keeping the fork alive is also a good idea. I think there's something in Johan's assertion that UTF8 should be the same speed to decode in the usual case (ie all ASCII) because it's one comparison in each case. But I can attest to the fact that getting GHC to give us the low level code we want there is pretty tricky.
`* [Strict, Lazy]`
They're defaults in OpenSSL when generating and considering it's self-signed, that does inspire confidence.
In practice there is still a fair amount of converting to UTF16&amp;mdash; if the goal is to reduce types, I think the switch to UTF8 is necessary. I still think it would still be good to have a UTF16 package, if for no other reason than to make the transition easier. We could rely on social pressure to convert most modules to use the UTF8 type.
Two interfaces down, two to go. The gloss "simulate" interface type should be easy. The "game" interface type might be awfully tricky, but I figure I have a few months before my programming class gets that far.
So I was talking to @ezyang, and he said he was still in the city till Thursday. I proposed meeting up with him on Tuesday, 7pm. How does that sound to you? If you want to hit me directly at any time, my email is lambdasquirrel (google mail).
Sounds good! Where should we meet? And do we want to say it publicly here or take it to a private conversation?
We could just make it public so people can drop in. Edward and I didn't set a place actually. Do you have any suggestions? I was going to suggest Rockefeller Park, Kaffe 1668, or the Whole Foods Tribeca's Cafeteria, reasons being that these are nice, relatively quiet places that we could still talk, and are located downtown near a subway. I just came back to NY not too long ago though so I'm hardly in tune with what's good, and I'm always open to suggestions.
Coincidentally, today was my first time to visit Kaffe 1668. Really, I'm open to almost anywhere, but based on today's experience I certainly wouldn't mind going back to Kaffe 1668.
Why is Ashley Yakely talking about Monoids in comments meant for first-hour Haskellers?
I got it running on Debian and Ubuntu reasonably easily. EclipseFP has also enjoyed some significant improvements (especially wrt ease of installation) over the last year. They're both pretty good, but will need some love before they are ready for day-to-day use on a large project.
emacs, brah
Ok, I think we're set on Kaffe 1668 then. :)
which packages did you need to instal for leksah besides ghc and the leksah package? EDIT: nevermind I've gotten it working. its actually surprisingly good. 
Yep, emacs + ghc-mod
Textmate on OS X.
Obligatory response: vim
See also: http://stackoverflow.com/questions/68504/what-are-my-ide-editor-choices-for-haskell
Rather than submit another link, I'll just put this here... http://dac4.designacourse.com:8000/sim.html
I've been using gedit. I feel like I ought to be embarrassed, as if it were nostalgic for writing C++ in notepad in the mid 90's, but it actually works surprisingly well and has Haskell syntax highlighting.
How is this hardware feature a Haskell topic?
It could be used to accelerate software transactional memory, though only for transactions with fairly small numbers of store instructions. That's the main relevance I can see. Oh, and it can also be used to make nice lock-free data structures (hash tables, etc.) and can sometimes be used by a clever compiler to automatically convert lock-based code into lock-free code, which is sometimes useful. Anyway, I'm happy to see hardware transactional memory showing up, even in this rudimentary form. I hope that HTM becomes mainstream, because it is really a nice thing to have.
Pretty much vim in one terminal, and ghci in another. Lately I've also been using gobby a lot because it's convenient to have my co-workers be able to join my session at any point.
I also find Textmate great for Haskell.
Transactional memory has been a fairly prominent topic in the Haskell context; is it really that hard to make the connection? It's interesting to see it starting to be implemented in hardware.
I'm using emacs + scion 
IIRC it just works if you have haskell-platform and then just use cabal install.
There's also haskell-mode for vim. I had ignored it for some time because it doesn't help with indenting the code like Emacs but other than that it's quite good.
I also have this in my .vimrc: autocmd ColorScheme * highlight TrailingWhitespace ctermbg=darkgreen guibg=darkgreen autocmd ColorScheme * highlight UnwantedWhitespace ctermbg=red guibg=red autocmd Syntax * syntax match TrailingWhitespace /\s\+$/ containedin=ALL autocmd Syntax * syntax match UnwantedWhitespace /\t/ containedin=ALL It highlights trailing whitespace in green, and tab characters in red. Of course, I always have expandtab on, so it's mostly to warn me if some file I've opened contains tabs.
I got Leksah working on Windows over the weekend - was a bit of a mission but I got there in the end. I had to modify several cabal scripts and mess about with pkg-config - yay for Windows ;) If anyone is interested in some instructions or a script or something, reply here or PM me and I'll see what I can do.
Also the new GHCi debugging integration in Leksah is awesome :)
Pff, vim? Seriously? People really need to remember [that `ed` is the standard editor](http://www.gnu.org/fun/jokes/ed.msg.html).
For a slightly more configurable low-tech approach, there's also SciTE, which is what I use.
Obligatory response: http://xkcd.com/378/
That looks nice!
I couldn't get scion work with emacs. I just use haskell-mode.
There's a lot of work going on with EclipseFP, the Haskell plugin for Eclipse.
Haskell has one of the few viable implementations of STM so far, and hardware support would allow for even faster and safer TM. It's like SSE intrinsics for vector ops, it does not add any capability, it makes existing features much, much more efficient.
nice! please post updates in the haskell reddit
I do most of my coding in Sublime Text 2, but I've been meaning to give Leksah a proper try one of these days.
Yes, the cases I’ve seen of code trying to keep up with all the changing LANGUAGE pragmas supported by GHC uses CPP and the \__GLASGOW_HASKELL__ variable.
Why, in an ideal world, would you like to use the word `String`? I always felt the name of the `text` package (and type) was very well chosen. I like it better that the arbitrary "string".
What about the Yi editor, written in Haskell? I see their last blog post was in 2009, is the project still alive? 
Yep! :) * https://github.com/yi-editor/yi * http://permalink.gmane.org/gmane.editors.yi.devel/5550
Thanks, looks very interesting. Any insight as to why no one in this thread is using it?
How come there's no ML option in the replacement language question? Seems like a significant replacement, since it would be a haskell replacement. 
Thanks a lot. Some visualization feedback: * Please use barcharts for the first graphic and sort them by ascending experience for the first graphic. This would make it easier to compare experience. * Please sort the bar charts descending by % in the 4th - 7th graphics. This would make it easier to see what are the most and lesser favorite options. * Would it be possible to use "poor" .. "excellent" on the following graphics, instead of or in additon to 1..5?
Good question. I probably didn't think of it. People were able to enter any language under "Other" though.
Feedback taken. It's been quite a PITA generating these graphs using Google Docs. Next year I'll look into some other solution.
I see. Your hard work is very much appreciated. Do you have any expectations on how this report will be perceived / used? For example, i often read a lot of good suggestions on how HackageDB could be improved. What i miss a bit is a list of * who organizes those improvements * what is currently going on * what is missing * where might people lend a hand
The results of last year's report was typically used informally in discussions between me and other people working on Haskell infrastructure. My hope is that the results will convince people to e.g. improve the documentation of their libraries, etc. As for Hackage, Duncan Coutts is the main contact person. He should be able to give a status update and point you to other people who are hacking on the project. If you have some spare cycles I'm sure he has plenty of things that you could spend them on.
&gt; We can get better speedup by ditching `ghc --make` and instead compiling each module separately Why not also just parallelise `ghc --make`? In fact, seeing as most packages are composed of multiple modules, this would seem to me like a more obvious place to start than cabal.
It's much harder. Thomas Schilling tried that and here's what he has to say on the issue: &gt; I have my doubts about the feasibility of parallel "ghc &gt; --make". My attempts at working towards it have partly been &gt; reverted because they caused performance issues. There are &gt; some bigger refactorings of GHC necessary before it'll be &gt; straightforward. The "ghc -M" solution is more realistic (even more now since the parallel infrastructure in cabal-install is implemented).
Hm, okay thanks.
As a side note, I'm reading through the raw HTML table of results now, specifically the "What do you think is Haskell's most glaring weakness / blind spot / problem?" question There seems to be quite a lot of decent replies and ideas there, it would be a shame to not bring some of them forward for review by the community!
Yeah, that column makes for an interesting read indeed... btw, I had the impression, that Haskell's record-syntax issue was the most mentioned one in the language-deficiency category
Last time I tried, it was too slow. I also use a pretty large number of vim's features, so it's likely that I'll sacrifice some productivity if I switch to yi. I keep telling myself that I should switch and start contributing to yi though.
The end of this post reflects a persistent, though understandable, confusion about type families. Being unable to write instances for type families is not a spurious limitation; it's fundamentally impossible in the general case. As I said in a comment there: &gt; Type families are open functions on types, type classes alone are open functions from types to terms. Both are distinguished from their parametric equivalents (type constructors and polymorphic types, roughly) by inspecting their arguments in a manner similar to pattern matching. The type variables bound by the "patterns" can then be used to construct the result. &gt; &gt; So, to define an instance for a type family as you'd like to do is essentially using function application in a pattern match; an instance for some type "Foo a" where Foo is a type family means taking a type "t" and reconstructing a type "a" such that "Foo a ~ t". Given that type families, like term-level functions, are not required to be injective, this is clearly impossible in the general case. So, while the parametric nature of type synonyms--due to not inspecting their arguments--makes them easily invertible, and data/newtype are trivially injective by introducing entirely new type-level values, this is not at all the case with type families in general, despite the similar syntax leading people to expect otherwise. The example here, of writing an instance on the result of an addition, is even trickier because it relies not even on simply inverting the function, but rather the knowledge that any choice of splitting X into N + M is equivalent for the author's purposes. It may also be worth noting that the one example where function application in a pattern match *would* be used for terms, namely n+k patterns, is both closely related to what he's trying to do at the type level, and also slated for removal from the language.
Synonyms aren't invertible in general either, as they contain things like: type Foo a = Integer There's no way to recover `a` from `Integer`. There may also be non-trivial examples, I'm not sure. You're of course correct, though. Type families and type classes are predicated on the idea of `*` being inductively generated by the type constructors (in a loose sense), analogous to data types being inductively generated by the constructors. `n+k` patterns are based on wanting to treat all `Integral` instances as natural numbers, where the `+k` signifies a series of successor constructors (although the implementation is much more evil). Data types are not generated inductively by arbitrary functions, though.
Oh, true. Yes, invertible isn't actually what I wanted to say there. Sorry, wasn't thinking clearly.
Oh, I know another problem with the desire; consider: type List a = [a] instance Eq a =&gt; Eq (List a) where [] == [] = True (x:_) == (y:_) = x == y Now we write `[1,2,3] == [1,2,4]`. Which instance is it supposed to pick? The regular `[Integer]` instance or the `List Integer` instance? Does the answer change if you annotate the values or something? What if they're each annotated differently (the application is still well-typed because `List a ~ [a]`). This is analogous to having cases with identical left-hand sides in a sequence of pattern matches. In a function definition we take the one that appears first syntactically, but that solution is a lot more dubious for type classes/families. Basing the case analysis on induction, this problem doesn't even come under consideration, because the sums are disjoint.
"The proportion that use Haskell for web development rose from 23% to 32%, most likely because we now have two quality web frameworks: Snap and Yesod." Two *additional* quality web frameworks -- the way it is phrased makes it seem like Happstack is not a quality web framework! It has its quirks and history, but is battle-tested, well supported, has been used in many places, and is under active development. I know the new kids on the block are shiny and also very good, but let's not forget some of the veterans :-)
Parallelizing `ghc --make` is quite difficult. Someone already tried once but didn't complete the project.
Sorry! I didn't want to imply that these were the only two, just that they have gotten lots of attention lately.
Very good talk. Thanks for posting.
For that matter, just define `type Id a = a`, then choose instances based on how many times the identity function has been applied. :] &gt; This is analogous to having cases with identical left-hand sides in a sequence of pattern matches. I'd perhaps say it's analogous to patterns that are identical except the names of the identifiers being bound, where choosing instances based on type annotations would thus be analogous to selecting a pattern by comparing the names used in the patterns to the name of the expression the function is being applied to. All of this sounds remarkably silly at the term level, of course. And with good reason, because it is. Type synonyms don't seem to trip people up too much, despite not being invertible (my earlier mistake aside), because any type parameter that's *actually used* can be recovered from the synonym definition and result type, and they're easy to think of as transparent aliases barely one step above preprocessor macros (though this breaks down to some extent in the presence of certain GHC extensions). I suspect the main reason this causes difficulty with type families is that they go against the intuitive assumption that anything that looks like type constructor application will allow recovering the type arguments you care about. &gt; In a function definition we take the one that appears first syntactically, but that solution is a lot more dubious for type classes/families. Well, it would be fine given closed type functions. Given the open-world assumption of type classes/families and no well-defined syntactic ordering between modules, that solution becomes utterly incoherent.
Yes, Alejandro Serrano has done a tremendous work over the summer on it: http://serras-haskell-gsoc.blogspot.com/2011/08/end-of-summer.html#comment-form
Interestingly as a haskeller i find clojure to be much closer to haskell than scala. This is despite clojure being dynamic language and scala being static. Clojure is my main (only?) language when i have to fall back to JVM. 
I wonder if [this](http://clojure.org/state) may have something to do with it? &gt; Thus an identity can be in different states at different times, but the state itself doesn't change. That is, an identity is not a state, an identity has a state. Exactly one state at any point in time. And that state is a true value, i.e. it never changes. If an identity appears to change, it is because it becomes associated with different state values over time. &gt; &gt; *(...)* &gt; &gt; In Clojure's model, value calculation is purely functional. Values never change. New values are functions of old, not mutations. *(...)* Changes to references are controlled/coordinated by the system - i.e. cooperation is not optional and not manual. The world moves forward due to the cooperative efforts of its participants and the programming language/system, Clojure, is in charge of world consistency management. The value of a reference (state of an identity) is always observable without coordination, and freely shareable between threads. &gt; &gt; *(...)* &gt; &gt; Programs are easier to understand/test when functional value calculation is independent of identity/value association. Honestly, this is a better elucidation of *Haskell's* philosophy toward state and mutability than some of the explanations I've seen that weren't actually talking about Clojure. It's also probably no coincidence that Clojure offers STM primitives that are just right there and working as part of the basic language. As much as we all love playing with the type system, the implicit assumption that functional programming involves calculating with immutable values and making mutable references and updates to them explicit is probably far more significant in day-to-day Haskell programming. From what I've seen, Scala doesn't really emphasize that aspect nearly as much, not to mention the strong overall OO bias compared to Clojure.
I was afraid this would another of those "You guys are wrong, the TRUE meaning of the word is this because that's what it originally meant" kinds of talk and completely miss the point, but there's some good discussion after that. I agree that much of the appeal of FP (in both Haskell and Clojure flavours) is that it isolates many capabalities (type declarations, data structures, behaviour-as-data, interfaces, naming values, mutable storage, etc.) which in traditional OOP are all conflated in the big mighty class. This is what allows us to express our ideas as what they are without a forest of extraneous attachments. On the other hand, as valuable as this notion of simplicity is I don't think it's usable as a fondation for a stable design. As Halloway briefly aknowledges when he talks about introspection, "Is this compound?" really means "Will anyone want to work with smaller pieces than this?", and the answer to that is likely to change with time. Sure if you're designing Clojure 1.3 you can just redo it all when new user stories emerge and your community will swallow it, but there's a limit to that. If you want to design a programming language (which is expected to be pretty long-lived API) around a single foundational principle, then it's probably better to choose one that's a little less fickle. This way 20 years down the road, when your creation has become part of the necessary evil of programming (which is what a practicality-oriented language such as Clojure seems to aim for), as outdated as it may be it'll still have conceptual integrity going for it. Examples of what in my mind constitutes a "less fickle principle" would be "everything is an A" where A ranges from "string" to "object" to "function" to "file", or adherence to a particular, well established model of program construction (type lambda calculi and process calculi, anyone?) or machine architecture (the original C, which is the highest-level language I know in which everything clearly maps to machine concepts, remains remarkably consistent today).
Gah, if this is tomorrow I can't make it :-(. Maybe another time.
TL;DR: "simple" means "not compound".
we'll figure something out. xD
It would be unspeakably awesome if you actually did write that in-depth guide to writing/profiling production Haskell code. I've had a few issues lately where a project relatively far along hit a leak of one kind or another, and the learning curve is steep!
Interesting, I'm really glad I read your comment. I always assumed Scala would be the "next best thing" in Java-land, and was thoroughly disappointed with it each time I tried learning it. I'll give Clojure a chance next time it's relevant.
I think the problem with Happstack is the documentation seems to jump up in complexity very quickly
Clojure has lazy sequence functions, so you can use infinite lists like (cycle ["odd" "even"]) has succinct, elegant syntax for the anonymous functions: (filter (&gt; 5 %) [1 2 3 4 5 7 8]) % is a placeholder for argument. Or %1 %2 %3 etc for more than one argument. Destructuring is awesome. Kinda like pattern matching for structures. (let [[x y &amp;rest-of-them] [1 2 3 4 5 6 7]]) x = 1 y = 2 rest-of-them = [3 4 5 6 7] It has builtin STM. And i can just go on and on and on. So many gems scattered around the language. It's a pleasure to work with. Especially for web development with its awesome combinatoric libraries (hiccup) and xml/html templates (enlive), killer emacs intergration (slime and swank) Clojure is what makes programming on JVM a joy. As for scala, i had [similar experience](http://www.reddit.com/r/programming/comments/gis0z/guardiancouk_switching_from_java_to_scala/c1nxx3k) to yours.
I read everything on that column. I regrouped all comment in four highly subjective points. 1st - Libraries Organization : - hard to choose (not find) a good one, - lack of documentation, - lack of coherence between themselves - hard to install (cabal dependencies problem for example) - backward compatibility issues Solution proposed : Add a rating system on Hackage 2nd - Difficult to understand the behavior of the program at runtime ; slow, memory leak, no debugger, etc... 3rd - Difficult to learn, too much math, lazyness by default, Monad are hard, etc... 4th - Lack of Commercial Use. I put with this: lack of IDE, GUI libraries (OS X, Qt), Databases libraries... But also LGPL My personal note: Many (most) messages are about a lack of "Authoritative decision" in the haskell community. Concerning both libraries and the language (as GHC is mainly equivalent to Haskell). Everybody like to have the choice, but for haskell people seems lost. Too much time is taken to _choose_ a library (or framework). The idea to put a rating system on hackage seem a very nice idea to limit this mess feeling. Without denying the ability to write different libraries, it will also help people chose one. Another subject was the fact that haskell is mainly ghc. And therefore the language seems not robust enough as any change in ghc is a change in haskell. One solution might be to have a "haskell language specification". It may be based on ghc but which will ensure backward compatibility. On the other hand, I don't know what to do for people who find haskell to be "too difficult". I recently read an article saying in substance, that "learning haskell is hard and this is why you should learn it". Haskell introduces new notions which are hard to learn but essentially good for your programming skills like "pointers" in C.
I think the main problem for me (and what a lot of people have highlighted) is the lack of 'use case' documentation for libraries. Someone put it succinctly when they said something along the lines of "Haskell libraries are just reference documents with type signatures, whereas Ruby libraries tend to document use-case examples but neglect the reference documentation" Haskell needs a bit of both to make it helpful for people to use libraries. When I try to evaluate a library I seem to spend a while searching for tutorials or trying to mess about in GHCI trying to get something working. It would be nice to have some examples like "this is how you would use this to do A/B/C" etc
I think I remember one of the ideas for the next Hackage was to have a wiki associated with each package? This would be a great way to allow people to contribute tutorials and examples for a package in a place where they can be found easily.
One point regarding that -- it would be a shame to get default functionality for extracting a dependency graph and building in parallel that *only* worked on cabal-packaged Haskell projects. There are repos of production code that for various reasons don't package up neatly in cabal but could benefit from parallel compilation. I don't mind the equiv of "ghc --parmake" being implemented at the cabal layer, but it would be unfortunate if it didn't work for anything that "ghc --make" could work on.
Is this meeting happening today?
Well, I'm still planning to go, but if enough people don't chime in I might reconsider...
I'll show up.
&gt; Destructuring is awesome. Kinda like pattern matching for structures. &gt; &gt; (let [[x y &amp;rest-of-them] [1 2 3 4 5 6 7]]) &gt; &gt; x = 1 &gt; &gt; y = 2 &gt; &gt; rest-of-them = [3 4 5 6 7] I don't understand how it differs from pattern matching. let (x : y : restOfThem) = [1 .. 7]
There is OCaml. Is it not a ML?
If I remember right, Clojure only has three types of "structured patterns" corresponding to the literal syntax for vectors, sets and maps. On the other hand, anything that implements the corresponding collection interface can be matched against these patterns, so you can, for example, match anything against a vector pattern as long as it implements the sequence API. It's similar in spirit to Scala's "unapply" methods, but more in line with the way programmers in dynamic languages often use primitive collection types instead of new application-specific types.
Forgot to mention: there are two chapters written for Yesod 0.9 that aren't in the book yet (I'll add them once we make the official release): * [Shakespearean Templates](http://www.yesodweb.com/show/map/118) * [Widgets](http://www.yesodweb.com/show/map/123) More to follow.
I think it's likely that we will have at least 4 or 5 people, so I'm still in for sure.
It's not really pattern matching. It cannot match types or values. It only takes apart sequence structures like lists, maps etc. But there are of course several implementations of a true pattern matching for clojure: https://github.com/swannodette/match http://spin.atomicobject.com/2010/04/25/matchure-serious-clojure-pattern-matching/ https://github.com/brool/clojure-misc/tree In the end, clojure is lisp with full power of lisp macros, so it can do anything.
Are there any improvements on yesod devel ? (It was broken for quite some time) Shall i try it and stop using wai-handler-devel ? 
404 :[
Of course it is. I still think SML is notable in this instance though.
Oh that's because I changed some of the URLs... this is still experimental! Try http://dac4.designacourse.com:8000/ and follow the Simulate link
It's been improved a bit. I don't think it's ever been fully broken (please tell me if that's not a true statement), but it's definitely not doing what we'd like it to do. Unfortunately, it seems that both routes (compiling and interpreting) are having difficulties working well, and there doesn't seem to be anything we can do about it.
Sure, the "ghc --parmake" wrapper for ghc can be released separately on Hackage.
Okay, I was convinced to keep submitting these. Enjoy!
It may not have been fully broken, but I do recall it being sufficiently unclear how to get it to work that I stopped trying and used `wai-handler-devel` instead. I recall hitting several points of confusion in getting things wired up and going in a new Yesod project (both from scaffolding and without), but this was a while ago now and I don't remember the details.
I will.
Very cool! That's an awesome collection of quotes at the bottom. I'm a little worried that it would be easy to fall into the Logo trap, which fairly widely taught in American elementary and middle schools in the late 80s and early 90s, but it seems most people didn't learn much beyond basic movement commands, pen up/pen down, and maybe repeat. Logo students who actually got to variables or recursion seem rare, and in the end the effort seemed to be effective at propagating misconceptions about programming. To be clear, I think this was in part due to teachers who didn't know enough about programming, and insufficient time/access on the part of the students. So if your environment takes off it's something to consider... but this doesn't describe you and I can't wait to see where this particular class ends up going! On the other hand, I think what you have is far better than Logo, as it's easier to develop more interesting pictures. I remember there were kids that developed pictures at this approximate level of visual complexity, but it seems that those were a minority and they had to work much harder at it. And it seems the move into functions and variables might be easier to motivate and understand in Haskell. It does bug me that this code represents a point as two parameters instead of one, which has been a pet peeve of mine from approximately the age of 12-14. I suppose it does simplify the syntax a bit, but wonder if a little extra syntax couldn't pay off in fairly short order. (On the other hand, there can definitely be value to first learning a bad way to do it, especially at this stage. It's then easier to appreciate the advantages of a better approach... Moving from BASIC to Pascal was not an easy transition for me, but once I did so I pretty quickly came to appreciate how much better Pascal was as a language, which was a good lesson to have early in life) 
After programming for so many years, I don't know if I could remember to enumerate each of those things that you remember to list. It seems so obvious that I'd forget the audience hasn't been programming for years too :-) Nice work!
I think that's a great aspect of cdsmith publicizing his class' progress. He's being explicit about things most experienced programmers take for granted, and I hope the public exposure will allow others to help keep him from the logo trap if necessary!
If Haskell disappeared, I think I would take a look at Agda.
This looks great! May get me to come back to the land of emacs :) Thanks Chris!
So how shall we recognize each other?
I'm there right now and I have a Haskell logo stickered to my laptop?
Win.
Related: [STM debunked (at least for imperative langs)](http://blogs.msdn.com/b/brandonwerner/archive/2009/01/14/software-transactional-memory-debunked.aspx) [The problem with STM is that your languages still suck](http://enfranchisedmind.com/blog/posts/the-problem-with-stm-your-languages-still-suck/) 
Your blog has some serious problems when viewed on an iPad. Some information on how to fix them (it is a simple setting change) is in the [comment](http://www.reddit.com/r/programming/comments/js96x/haskell_for_kids_week_2/c2eq3md) I posted on the cross post of your submission someone submitted to /r/programming.
The statement of Marcello is especially delightful: &gt; I have done sooooo much with our programming teacher Chris &gt; I have learned how to program Haskell in the smallest definition.
Man, I'm jealous. I was just thinking a couple weeks ago that I would enjoy the hell out of teaching (interested) kids programming, and I was wondering how I could do it. I thought it might work to offer 'computer lessons' using musical instrument lessons as a guide for how to handle things. I'm not sure if in my area (West Virginia) there would be much interest, though I bet if I put up fliers saying "Want to learn how to make videogames? Take computer lessons!" or something like that I could get some interest. If I ever end up unemployed, I'd definitely try such a thing. And thank you for helping these kids out, especially by helping them out without pandering to them. I learned to program starting at 9 years old just because back then I had never heard of the idea that it was supposed to be hard, or that some adults do it for a job, none of that, I just found these books with my parents Commodore Vic-20 and started taking them everywhere and finding out that I could make the computer do stuff. I'm always a bit disappointed when I see people wanting to teach kids a toy language. Unless you're dealing with seriously young kids (like earlier than 8 years old or so I imagine), I think that does them a disservice. They can learn a real language as well as, if not better than, an adult can. Good luck, and yes, please keep posting these, I will upvote every single one. edit: Oh, and an idea I had to get kids interested in more application-oriented programming: Find out something that they collect. Every kid collects SOMETHING, whether its stamps, coins, business cards, Pokemon cards, baseball cards, dolls, whatever. Find out what they collect and help them create a program they can use to keep track of their collection. This kind of thing, I think, could help hook kids into an aspect of programming that might seem boring at first. Once they realize all the things they can do with it, I'd expect they'd get excited and want to make programs for everything.
Yeah, the other post sounds a little too good to be written by a middle schooler without help. She's a fantastic writer.
Oh, she wrote it alright! Check out the first chapter of the novel she's writing, linked from an older entry in her blog.
One comment I'll make is that it's been an immeasurable blessing to work side-by-side with an experienced teacher whose understanding of children and education I respect and admire... even if she is more focused on English and history! :) So if you do decide to do this, the one suggestion I can offer from my experience so far is to *not* "put up flyers" and try to wing it, but instead talk to schools and educators and be willing to sit down and spend the time to make sure you're really convinced you're approaching things the right way. Chances are there are people out there who know a lot more than you about children and how they learn!
Hmm, I think the first logo program I was shown was how to draw a square recursively, and I remember changing the repeat and angle to draw other polygons. OTOH I rememeber me or a friend drawing a simple house and it took a week. With a line segment and some recursion and rotation, you can do neat spirograph things, maybe they'll get into that. (My pet peeve was calling the functions "variables", when they never vary in haskell. :)
I'm not jealous. I have some wonderful tutorials to use some time!
I'm seriously impressed at the sophisticated level of attention to detail and code readability so soon -- if you got them through that while keeping their interest, and the ideas stick, it seems a massive leg up. I say this thinking of my old BASIC spaghetti code.
I certainly intend to keep calling them variables, since that's what they are! Variables don't "vary" in mathematics, either, if by that you mean changing their value over time. It's imperative programming languages that have changed the definition of "variable" and imposed a new meaning on the word. One reason I chose a functional language, when I was debating between Haskell and Python, is that lessons learned in Haskell are far better preparation for elementary algebra and other high school mathematics. Functions have parameters, have `a -&gt; b` (for some `a` and `b`) as their type. The variables we're defining there don't have parameters, and usually have `Picture` as a type. They are not functions.
This series is inspiring. Please keep posting.
I believe Armin addresses the MS issues (at least in part) in [this comment on his previous post](http://morepypy.blogspot.com/2011/06/global-interpreter-lock-or-how-to-kill.html?showComment=1309429708435#c2696537202720074437).
So is this mode a fork of the normal haskell-mode or is it an entirely new mode? Great job by the way, I'm really excited to see it released.
Reading through the code examples: Why is Yesod a type-class rather than a record? It seems the instances are void of any data content anyway...
Which (major) GHC versions (7.2, 7.0, 6.12, ...) are (officially) supported by Yesod 0.9.0 so far?
We're officially supporting 7.0, though it has been tested with 7.2. I haven't tested 6.12 myself, but I believe it's still working there. The only change that might have broken it on 6.12 was the combination of GADTs and TypeFamilies in persistent, and that was tested.
&gt; It does bug me that this code represents a point as two parameters instead of one At least it represents the offsets for `translate` as two numbers... Polygons and lines *do* use 2-tuples to represent points. We haven't covered that in the class yet, though.
That question has come up before (see end of [this email](http://www.haskell.org/pipermail/web-devel/2011/000676.html)). The wiki page it references is down, but fortunately I found the email response I wrote in the first place. Here it is: The question: A few minutes ago, reading the Yesod book, I thought that Yesod should not be a type class but a record. For three reasons: 1. It requires a hack to work (approot needs a parameter), 2. records would suffice, 3. records can be generated at runtime. Answer approot isn't really a hack: there are legitimate times when approot *needs* that parameter. For example, people sometimes want their approot to be specified in a config file, in which case you would add a record to the Foundation datatype and pull it out with approot. Now, let's address why I designed Yesod with typeclasses and see if records would work here (I haven't fully thought this through yet): * Typeclasses make it easy to provide default functionality. That could be words around by having a defaultYesod value. * Typeclasses also let the compiler know when a user forgets to write a required method (ie, approot). Using defaultYesod would *not* work well for this use case. * By having a typeclass with an instance on the foundation datatype, we only need to pass around the foundation value and get the entire typeclass implicitly. With records, I suppose we could work this the other way (pass around a value that contains the foundation in it). * It is (subjectively) easier syntactically to write methods in a typeclass than provide lambdas to a record. For example, see the case of overriding errorHandler. As far as reasons to use records: * More "correct"; I consider this in my decisions, but especially for Yesod I put usability first to an extent. * You can have multiple record values for a single foundation datatype. I'm not sure how often that would really be useful, and could be hacked around with a newtype wrapper (though it would end up being an ugly hack). * For generating values at runtime: we can always add the values to the foundation datatype itself and have the typeclass overridden methods pull from there. That is less user-friendly than a record, but I think it's a less common case. This is definitely at least theoretically an appealing approach, and I do agree that often times using records is superior to using typeclasses. However, in this specific use case, I'm not sure we should: a lot of the normal benefits of records would not commonly be needed, and there are the downsides mentioned above. If there are points I'm missing, feel free to bring them up. After all, I *did* just wake up ;).
From the article: &gt; The reason for creating a new project and not patching the existing haskell-mode is that I find its codebase too disjointed and fairly messy to work with, and that it has different motivations and concerns than mine. I assume that means "new code", not "fork".
Upvote. should have read the intro more carefully.
I've just uploaded a new version that derives slightly prettier labels.
You might want to email maintainers of specific packages that don't work and let them know. Including a patch is even better. :)
My main issue is that packages like unix were upgraded, but not uploaded to hackage, so now many of my packages can't build their haddocks.
For web-based development, you might check [quid2.org](http://quid2.org). It is still very much in its infancy but improving fast.
Hi Chris, I like your idea of using Gloss to teach Haskell to kids. In fact, I liked so much that I decided to copy it and to port Gloss to [quid2.org](http://quid2.org). There is a little introduction [here](http://quid2.org/?ref=Link.Hash_1b6e513e2d7f444574229b81d1f1b9262d6963e8452f33b89ad10c4f9b7d6da9.mdlSubjectQuid2ForHaskellers). Best
I got a couple of emails already :) I wish there was a better way to deal with (e.g.) `base` versions.
Is there one of these for typed lambda calculus? I'm not to sure about anything in this area but I'm aware of things called System-F, Hindley–Milner type inference algorithm, but not where to look besides wikipedia to jump into them (or even if it'd provide any benefit knowing them).
I saw the mention of plain text template. But it is not described anywhere in the templates chapter. Could you please provide a simple usage example of it ? Right now i am using interpolatedstring-perl6 package for simple text templates (sqls). It would be great to drop yet another dependency.
Yes (it's probably time) and no (stupid print editions don't automatically update :p).
People here always recommend Pierce's [_Types and Programming Languages_](http://www.cis.upenn.edu/~bcpierce/tapl/) for this. There's also the presentation in Girard's [_Proofs and Types_](http://www.paultaylor.eu/stable/Proofs+Types.html), which is freely available for download. However, I agree that a single approachable tutorial paper would be great. I have yet to see one, but perhaps there is one out there which I am unaware of.
Just a note: There's a "bonus" field under the "would you buy a revised edition" question.
If your haddocks aren't building, there's a bigger problem. Mere missing packages just mean your docs won't have links to some types. For example, [dbus-core](http://hackage.haskell.org/package/dbus-core) had to have its unix version limit bumped to compile on 7.2. Once it *compiled*, the docs were built without any problems.
yes but the future should be focused on small incremental changes to the site, forget about a print version.
Thanks I'll look into those. Something I've found is that although the history isn't important to the actual knowledge, it's often quite interesting. That paper on the history of haskell for example, and knowing the time of untyped lambda calculus makes the emergence of lisp more clear, and the alternative model provided by fortran as well. Would you be aware of any good sources covering the timeline/chronological ordering of the discoveries/inventions of types? Although I ask this knowing that if a single approachable tutorial paper is not available then a single approachable historical summary is even less likely. E: link http://www.haskell.org/haskellwiki/History_of_Haskell
It also means that [packdeps](http://packdeps.haskellers.com/) can't help us.
You can see it in action now if you generate a new scaffolded site. There is "st" for strict text and "lt" for lazy text.
I noticed that O'Reilly's Safari is not on the list. This is a book that I've been meaning to read and, were there to be a new addition, I would do so through Safari since I'm already paying for that.
Why not a print version?
The eastern economy edition will take a while to come out :-( and here in the east the US edition's price is really steep. But hell yeah I would like a second edition.
Not many quotes this week. Also I think the Haskell Weekly News should be in video form with an attractive female presenter.
By now your copy of RWH should be so worn out that it needs to be replaced by a new copy anyway... at least mine is =)
Fsck. It's a hour-long video. PDF slides anyone?
Small typo in the survey: First question has three checkbox options but message underneath reads: &gt; If you haven't read any, just leave *both* boxes unchecked.
Or Clutter perhaps? Don't know if it's stable or complete enough though.
_Proofs and Types_ gives a little bit of a flavour of the history, because it covers Gödel's system T and Girard and Reynolds' System F. Robert Constable has a good recent survey paper, ['The Triumph of Types'](http://www.srcf.ucam.org/principia/files/rc.pdf), which lists a number of histories such as [_A Modern Perspective on Type Theory_](http://www.springer.com/mathematics/book/978-1-4020-2334-7). Hindley and Seldin's [_Lambda-Calculus and Combinators: an Introduction_](http://www.cambridge.org/gb/knowledge/isbn/item1175709/?site_locale=en_GB) also contains some history, as well as being an excellent textbook.
You may have heard the rumours, and I can confirm that it's true: HakkuTaikai is happening! After spending a long week at ICFP 2011 talking about Haskell, why not relax by actually hacking some Haskell? Full details on the wiki page: http://www.haskell.org/haskellwiki/HakkuTaikai * When: Sunday, 25th September 2011 * Where: Room 1208 of the NII building; the same location as ICFP. Registration is free! See the wiki page above for details. Feel free to email or IRC or twit me with any questions. Many thanks to http://www.nii.ac.jp/ for hosting the event!
&gt; Haskell Weekly News should be in video form with an attractive female presenter. ...that would just draw more attention to Haskell... which might lead to success, and which should be avoided at all costs!
&gt; cabal: cannot configure unix-2.4.2.0. It requires base &gt;=4.2 &amp;&amp; &lt;4.4 For the dependency on base &gt;=4.2 &amp;&amp; &lt;4.4 there are these packages: base-4.2.0.0, base-4.2.0.1, base-4.2.0.2, base-4.3.0.0 and base-4.3.1.0. Is what shows up in the cabal log. unix-2.5 is distributed with the GHC 7.2, but it is not on hackage. My package builds successfully for users with 7.2, which comes with unix-2.5. It builds successfully for users with 7.0, which has a version of base that is compatible with unix-2.4.2.0. It does not build on hackage. I have taken to building the docs locally and publishing them to github as an interim solution, but it is definitely suboptimal. I should have said it doesn't build at all on hackage.
Lectures on the Curry-Howard Isomorphism.
That doesn't sound like a good idea: Version 0.1 Upload date Sun Nov 29 04:12:01 UTC 2009 Build failure ghc-6.12 (log), ghc-7.0 (log) Last blog update from the author (2010.05.13): &gt; I've mostly spent the semester doing homework; I didn't make as much progress on Clutterhs as I had planned. I hope once that I'm done with finals, I'll have time to make the various changes I want and finish binding Clutter 1.2. The release of Clutter 0.2 is still waiting on those, as well as a release of gtk2hs. In general, I'd hope a book like Real World Haskell would cover a well-supported toolkit, not something experimental or maintained by a single guy. Having to change GUI toolkits can be a huge blow to a 'real world' project. Clutter does sound interesting as a library, but the Windows support seems lacking.
what's the problem with gtk?
Thanks. This is the kinda of advice google isn't capable of. Proofs and Types looks very good, so I'm going to get my hands on a copy of that, and I'll write down the rest for a time when I'll understand the basics.
Option c: I'm a poor/cheap bastard who's still relying on the online edition.
Interesting way how the first edition has gone out of date is that some of the example transcripts now show error messages in the web version: http://book.realworldhaskell.org/read/efficient-file-processing-regular-expressions-and-file-name-matching.html#id617251 . Apparently a library API got changed at some point, the website generator started using the new library, and this broke the examples which were generated with a real REPL.
The main thing that seems to be missing from the quotes are the usual 3-4 witticisms from kmc on #haskell. Maybe he was too busy that week to spend his typical 25+ hours/day on IRC? ;]
&gt; unix-2.5 is distributed with the GHC 7.2, but it is not on hackage. The *documentation* for unix-2.5 is not on Hackage. The package itself is. &gt; My package builds successfully for users with 7.2, which comes with unix-2.5. From a quick check of Hackage, I'm assuming you're talking about the "trifecta" package. This is the only one I could find that fails due to "unix"-related build errors. It also fails to install on my local system, with GHC 7.2.1: $ cabal-dev install trifecta Resolving dependencies... cabal: cannot configure unix-2.4.2.0. It requires base &gt;=4.2 &amp;&amp; &lt;4.4 For the dependency on base &gt;=4.2 &amp;&amp; &lt;4.4 there are these packages: base-4.2.0.0, base-4.2.0.1, base-4.2.0.2, base-4.3.0.0 and base-4.3.1.0. However none of them are available. base-4.2.0.0 was excluded because base-4.4.0.0 was selected instead base-4.2.0.0 was excluded because of the top level dependency base -any base-4.2.0.1 was excluded because base-4.4.0.0 was selected instead base-4.2.0.1 was excluded because of the top level dependency base -any base-4.2.0.2 was excluded because base-4.4.0.0 was selected instead base-4.2.0.2 was excluded because of the top level dependency base -any base-4.3.0.0 was excluded because base-4.4.0.0 was selected instead base-4.3.0.0 was excluded because of the top level dependency base -any base-4.3.1.0 was excluded because base-4.4.0.0 was selected instead base-4.3.1.0 was excluded because of the top level dependency base -any This error is occuring because `trifecta` depends on `wl-pprint-terminfo` which depends on [`hscurses`](http://hackage.haskell.org/package/hscurses) which depends on `unix` 2.4.*. If you want your package to build, contact the maintainer of `hscurses` and ask him to bump his dependency on `unix`. Alternatively, you could use my [`ncurses`](http://hackage.haskell.org/package/ncurses) package.
Interesting. I hadn't heard of quid2 before, and to be honest, I still don't have a good feel for what it is. But in the description there, you mentioned possibly wanting a JavaScript-based implementation of the gloss renderer: that's precisely what I've written, at https://github.com/cdsmith/gloss-web -- the core of it is the drawing function in web/draw.js
I have a related problem: I can't hold on to a book on something I love for very long, without giving it to someone else and making them promise to pass it on to someone new when they are done.
It's always worked fine for me! It does seem like lately, `wx` has been gaining momentum as the "cool kids" GUI toolkit. But that momentum has been blunted, for me, by not being able to build it on the five or six occasions I've tried. :)
One of the wxHaskell maintainers has written a fair comparison: http://stackoverflow.com/questions/553317/what-are-the-relative-merits-of-wxhaskell-and-gtk2hs
Print versions are nice for helping people find you in the coffee shop. Especially when they are titled in magenta.
Ouch. Yeah. No Clutter.
My experience is the same, except with reversed roles for wx and gtk. It all depends on your primary platform, so the community is split down the middle with each side trying to figure out how the other side can cope.
because it isn't 1998 anymore the print model abhors small updates - updates are batched for a half-decade or so and then deployed at once the web model is better: you can make changes as the software changes. no more warnings about out-of-date such and such 
Good catch. I only use curses for about 3 lines of calls. Long enough to get the screen width. I'll probably either switch to ncurses or just FFI to do the few bindings I need directly.
or Qt
I hate digital copies myself.
There seem to be several packages that try to solve the problem of catching IO exceptions in monads built on top of IO. There's MonadCatchIO-{mtl,transformers}, monad-control, monad-peel, exception-transformers and I think a few others too. Is there any consensus at the moment on which one should be used?
Damn, I am not in Japan at that time and a paper deadline beginning of October. =*( I hope 1208 has a nice view over the city, so that you will be hacking Haskell and enjoying the wonderful night view of Tokyo.
The web model is optimized for mutability. The print model is optimized for persistence, browsability, and reference.
I think a lot of people's definition of "reading the HWN" is "reading kmc's quotes" :)
There are a LOT of interesting things Emacs doesn't do. But managing your build and integrating with version control are some of the few IDE-ish things that emacs *does* do and even do well.
Well, here's my take on things: MonadCatchIO should __never__ be used, it's [inherently broken](http://www.haskell.org/pipermail/haskell-cafe/2010-October/084890.html). The aftermath of that email is that I wrote the MonadInvert typeclass into neither, which I then deprecated in favor of monad-peel. After that, monad-control was released. I can't really give a strong argument for monad-peel versus monad-control, except that the latter benchmarks as 2.5x faster. It's what we use throughout the Yesod stack. You can also look at the reverse dependencies: http://packdeps.haskellers.com/reverse/monad-peel http://packdeps.haskellers.com/reverse/monad-control exception-transformers is conceptually simpler, but less powerful. In the past, it was also broken like MonadCatchIO, but that appears to have been fixed.
Well, assuming you use composition of a new set of primitives which are themselves implemented as chunked I/O, then maybe you get the same compositional benefits?
Well that narrows it down to exception-transformers versus monad-control then at least (which is what I suspected). One place where the behaviour of exception-transformers differs from that of monad-control is that it doesn't lose state in the way that you describe in your blog post. I just looked at exception-transformers a bit more closely: the difference between it and MonadCatchIO* is its MonadException class has a finally method, the default the definition of which is overridden when a short-circuiting monad transformer is used. However, I just look at it a bit more closely: its implementation of bracket doesn't use finally anywhere, and therefore is no better than MonadCatchIO's. If we modify the example code in your email to use bracket instead of finally: {-# LANGUAGE PackageImports #-} import Control.Monad.Trans.Error --import "MonadCatchIO-transformers" Control.Monad.CatchIO (finally) import Control.Monad.Exception import Control.Monad.IO.Class main = runErrorT $ bracket_ (return ()) (liftIO $ putStrLn "sequel called") go go :: ErrorT String IO String --go = return "return" --go = error "error" go = throwError "throwError It will simply print: &gt; Left "throwError" Just like MonadCatchIO. I think this is probably unintentional though, and is therefore a bug, right?
Although referencing it online is considerably easier if you can link to it. Many people like linking to RWH chapters on Stack Overflow, for example.
Actually, now it is! There was a [recent post on reddit](http://www.reddit.com/tb/jigpj) that may be of benefit.
Sounds about right, good catch. You should probably report that to the maintainer.
Sorry, meant personal reference. The structured document model (chapter 5, section 2, paragraph 3...) is better for pointing someone else to. A good web model uses the structured document model.
&gt; Knowing that all the function “myFunc :: String → Int → Int” does is return an integer (and not write to disk, open windows, or steal your passwords and mail them to me) is pretty awesome. I hear this argument alot (and believed it myself for some time), but actually this isn't true as there's the `unsafePerformIO` "backdoor" (which is part of at least Haskell2010 iirc)... I guess You'd have to use the SafeHaskell extension to be able to trust the type-signature... or am I wrong?
Yeah, unsafePerformIO is widely used in hackage libraries to install viruses on your pc. Be careful, and don't use haskell ^^
This is a really great article - a useful resource for my Haskell evangelism - thank you for taking the time to write it.
&gt; Yeah, unsafePerformIO is widely used in hackage libraries to install viruses on your pc. Or worse. One time when I tried to use something from `mtl`, it took over my system and started downloading Category Theory textbooks.
unsafePerformIO is common in many implementations, but it isn't part of Haskell 2010.
Agreed! To the point, clear and concise. Add to the new haskeller's must-read list.
I'm fairly certain it's part of the FFI addendum?
I sent them an email. This is enough to convince me to use monad-control for the time being.
And theoretically, a Python library could be written in such a way that it's completely pure and has no unmanaged side effects. It's true, but it's not something you have to spend a lot of time worrying about.
Thanks. I love how the first thing it does is mention the prerequisites. That's a convention I'd like to see more of.
The Haskell Skynet is too busy pondering advanced abstract mathematics to worry about humans.
I cannot find it in the Haskell 2010 report.
Haskell 2010 includes most of the FFI that base has. * [Base Foreign](http://hackage.haskell.org/packages/archive/base/4.4.0.0/doc/html/Foreign.html) * [Haskell 2010 Foreign](http://hackage.haskell.org/packages/archive/haskell2010/1.1.0.0/doc/html/Foreign.html)
ah, hm, well it does have unsafeLocalState :: IO a -&gt; a totally counts :p
Out of curiosity, what's your use case? Are you just dealing with exception handling, or will you be dealing with other things (modifyMVar, alloca) as well? In my mind, the huge advantage of monad-control/monad-peel is that you automatically have support for the entire class of issues, not just the specific case of exceptions.
 -- a function with no arguments pi :: Double pi = 3.14 That's not true. Why do new Haskellers try and present constants this way? They're not functions in any way.
yes, to really trust the type signature you would need SafeHaskell. The problem with the argument is that it focuses on the wrong thing. Pure functions in Haskell were never intended to be a security feature. The purpose of unsafePerformIO is to allow you to create functions which behave like pure functions, but actually do use some IO for whatever reason. If a function has a pure type signature, but acts in an impure manner -- that would almost always be considered a bug, IMO. To do that intentionally would bring shame and dishonor on your family for a 1000 generations. In practice, unsafePeformIO is seldom used. Some people joke that you should only ever call it if you name is Simon. I have probably use it less than once per year, and I am fulltime Haskell developer. So, while people *could* make functions that look pure but act impure, in practice, they don't. From a security standpoint, that is not useful. But from a development standpoint -- it is exceedingly useful. 
then print the fricking website. this is a stupid debate
The right way of thinking is probably that square :: Double -&gt; Double square x = x * x is actually an ordinary value, which happens to be a function.
Btw, is there a trick to print webpages like those w/o the navigation bar on the left leaving an empty left border/margin on all subsequent pages? Some websites seem to provide a printing CSS which changes the layout to be more printing friendly...
Yes, it's not very complicated to do, we just need to add a print stylesheet. Consider it on my todo list, thanks for pointing it out.
I wonder why the pay is half of that of a developer (an a par with an intern).
I'm writing a high-level networking library for Haskell. It incorporates a lot of the idioms common in modern Haskell networking code (Iteratees, sendfile, concurrency and timeouts) but it also has don't-even-have-to-think-about-it IPv6 support and eventually, full TLS support out of the box. So yeah, I'm doing a lot of alloca stuff, and a bit of MVar stuff too, so monad-control probably suits me better. I had been using exception-transformers (and was doing bracket (liftIO malloc) (liftIO . free)) - mainly because when I looked at the documentation of monad-control, the funny-looking name and the Greek characters in the type signature of liftIOOp_ put me off (at the time I didn't even realise that it meant lift - IO - Op(eration), I thought that there was this new thing called an IOOp that I'd have to read some paper to understand in order to use the library). I also wasn't sure about the difference between MonadTransControl and MonadControlIO and if I should care about that. I get all of that now, mainly thanks to your blog post, and it's actually all fairly simple. It's just that with exception-transformers I didn't even have to think about it. Edit: The example at the bottom [here](http://hackage.haskell.org/packages/archive/monad-control/0.2.0.2/doc/html/Control-Monad-Trans-Control.html) is also a big part of what scared me away from monad-control!
Yeah, pretty weak pay for a DevOps guy considering all they'll have to know.
I'm not debating the amount per se. I'm curious why it's half of a developer's pay. It seems that there are more and more articles about DevOps (and their importance), but somehow they are considered "second class employees" (compared to developers) in many companies. This is not specific to Tsuru Capital, but I took the opportunity to ask the question and hope somebody will answer :)
And, heck, if enough people want it, maybe we can pool our resources and get it printed well, and cheaply. And, while we're at it, we can remove all of the user comments. Should be an easy scripting task. Add an index. Maybe do some systematic reformatting: move the footnotes to sidenotes, arrange the code snippets and info bubbles so they don't interrupt the flow of reading... Oh, wait, that's exactly what a print version is. Assuming there's enough interest, let's do that instead, so it's cheaper all around.
It should be better now, please give it a shot and let me know if you still have problems.
What is the typical pay of a developer?
It seems pretty unusual for them to be listing their pay on their website, so I assume it can be negotiated up.
It says so right above this posting :P
How about `forall a. Num a =&gt; a`. Is that a function?
Do software developers really get paid $144,000 a year? Is that only developers in finance? I get paid less than a third that as a post-doc researcher.
Depends on if you view typeclasses as implicit dictionary passing or merely as type constraints. Dictionary passing is a compiler implementation detail in my view. I view that merely as a constrained type. It's not of a function type, hence it's not a function.
Post-doc researchers are known to be paid substantially less.
For finance, that kind of pay is not out of line. Check this [link](http://www.glassdoor.com/Salary/Bloomberg-L-P-Financial-Software-Developer-New-York-City-Salaries-EJI_IE3096.0,13_KO14,42_IL.43,56_IM615.htm). Most academics are woefully underpaid. 
How about `forall a. [a]`, then? What if I write it like this, instead: foo :: ∀a. [a] foo = Λa -&gt; [] Now, is `foo` a function? :]
I actually think it's very good that they put the salaries in the ad. I wish more companies would do this. Sometimes the job sounds good, but in the end the compensation package might be lower than you'd accept and this leads to waste of time for both parties.
Research is very underpaid.
Your link suggest that $144,000 isn't out of line, but it is still very much above average.
True, but I'm willing to bet that the median-salaried programmer at Bloomberg is a Java/C# grunt. (In my day job, I am just such a grunt, so no disrespect intended.) Skilled Haskell developers are rare these days, and so where these skills are in demand, market reality dictates that they should command a premium. Also, my experience has been that average Haskell programmers possess skills very much above the average for all other programmers. 
Depends on where you are, and the sort of company. In the SF Bay Area, that figure wouldn't be at all out of line for a moderately experienced person at an established company.
&gt; Edit: The example at the bottom [here](http://hackage.haskell.org/packages/archive/monad-control/0.2.0.2/doc/html/Control-Monad-Trans-Control.html) is also a big part of what scared me away from monad-control! My god, it's full of types!
This is deviating even further away from Haskell semantics and towards System F - simply because a denotational mapping exists between Haskell and System F does not make System F constructs suddenly valid Haskell. In system F, sure, instantiation takes the form of a type lambda. That would make it a kind of function, I guess - but it's not valid Haskell.
You could also see it as a function from a type (that happens to be an instance of Num) to a value of that type
See my reply to camccann, but that's essentially a view that works for System F, but it's still not at all directly applicable to Haskell. Universal quantification need not be a type lambda. It's a notational idea that is perhaps convenient, but hardly necessary to write a Haskell compiler, nor is it written in the Haskell specification.
I think the correspondence is a bit closer than you suggest. Otherwise, yes, that's the correct answer--it may be conceptually a function in some sense, but in Haskell it isn't, by definition. It's less clear what the correct answer would be if, instead of Haskell, we were discussing the Haskell-based language defined as "the set of programs GHC can compile".
I wonder why do people care so much about relative status inside a company. For comparison: in investment banks and hedge funds, developers whine because they're considered "second-class employees" compared to traders and quants, even though they earn **way** more than developers in other fields (at least in Tokyo).
In constructive logic, which we like to use all over the place around here, forall is basically a function type.
I'm way richer than a monkey, but do I care about that? Hell no, I want a lamborghini or aston martin or something. That fucker across the street has one and if I can't get one better I'm gonna key his.
Well, Tokyo also has the highest cost of living indexes in the world.
Standardized living cost doesn't mean anything. Typical budgets for a new yorker, a californian and a japanese person are very different. Besides, U$144k is waaay beyond what most Japanese *senior* developers earn. 
In Israel, I think an average developer makes 66K$/yr. I think the top developers (that don't own their own business) get as much as 200K$/yr.
Sure, but GHC Haskell is almost dependently typed :P
&gt; I wonder why do people care so much about relative status inside a company. This is a general fact of life - people care about *relative* status, absolute status matters little. There are plenty of studies on this - the hedonic treadmill, the Easterlin paradox, happiness correlates with relative status, not absolute status, low relative status correlates with increased death rates, receiving a major award like the Nobel is correlated with additional years of life compared to nominees who didn't win, when asked about the future people would prefer to be relatively wealthier than absolutely wealthier, people are happier when living in a rich neighborhood in poor counties than vice versa (saw that last one on [LessWrong](http://lesswrong.com/lw/7am/rational_home_buying/) today), etc etc. I've been seeing these results for years - there's a relatively obvious evolutionary explanation if you think about it. See books like _The Spirit Level_ if you're curious - relative vs absolute status has major implications for economic policy.
so lets get this straight to end this: you want to read a book about programming, but you apparently think that rearranging text in an html file is something a printer needs to do for you???? you could.....write a haskell program to do it!!!! or just write a greasemonkey script and do it on the fly. we have these things called computers
&gt; you want to read a book about programming, but you apparently think that rearranging text in an html file is something a printer needs to do for you???? Needs to? Of course not. I'd certainly hope, however, that a book publisher would be better at it than me, and capable of doing it faster and cheaper, too. Now, let me get this straight: you are of the opinion that, regardless of how many people actually want it, this book should never see a print run?
much better, thanks!
Is Haskell fast enough to be used in trading platforms? Maybe I don't understand what DevOps is.
There's a side-note in the text: &gt; [...]since the IO monad doesn't have the concept of returning a MyError when something goes wrong, it will always succeed in the lifting phase. (Note: This has nothing to do with runtime exceptions, don't even think about them.) ...why doesn't this have anything to do with runtime exceptions? does this mean we can't transform IO errors into ErrorT errors?
I'd like a companion piece for Haskell game programmers: Imaginary World Haskell
Thanks for the detailed reply. I guess it's hard to argue against the evidence, but maybe one would still be better off earning $50 in a company where most people earn $80, as long as $50 is more than the average income in one's area?
Haskell should be comparable to Java in terms of performance. A reasonably well written Haskell implementation should be usually within a factor of 2 or 3 near an efficient C/C++ implementation, and you can always call into optimized C libraries for computation intensive stuff like vector ops and linear algebra...
Hi Chris, I was aware of your code and I was already planning to steal it :-) so today I did (and not for the first time, if you look at the Language.BasicXML module, in the quid2 Library, you will find more code from you). Clicking on snowflake on [this page](http://quid2.org/?ref=Link.Hash_7cd9b60e16059a580f9eeddb14b85f136a487038502454d876bb478c66670c25.mdlGraphicsGlossExample) should now show the corresponding image (you might need to click your browser cache). Regarding [quid2](http://quid2.org), for the purpose of this thread you might just think of it as a web-based IDE to quickly develop applications such as yours. By just porting the Gloss data model into quid2 and writing a viewer for the Picture datatype, a few hours of work in total, you basically get your "haskell for kids" application with authentication, source code editing, storage of public and private files, etc. Now, obviously quid2 is still in its infancy and is still very limited and inefficient but these limitations will progressively disappear and quid2 will hopefully soon provide a good infrastructure to develop haskell based web apps of all kinds.
I don't know about that example specifically. One spends a *lot* of time at work, after all. And you've already mentioned examples - the financial industry programmers are making a lot more than the median in NYC or wherever, yet...
This is an awesome goal! I hope the result isn't too complex to be conveniently usable, which would be the real danger in this sort of undertaking.
If this succeeds it would be an momentous lift for Hackage, Cabal and the whole Haskell community. I wish you good luck!
I don't understand why it's not a decision each developer can make on his own, whether to accept a silent by-passing of IO using unsafePerformIO. Why does not there exist a compiler flag that lets people to say: I don't want any code that is not respecting the type signature.
Thumbs up!
I tried this one and didn't get too far. I needed more exercises to master the primitive parts they cover in the first few pages.
This is fantastic!! Definitely an upgrade to the overall idea of literate programming. I've been waiting to nab some of your code ever since you posted this http://pnyf.inf.elte.hu/fp/Economic_en.xml?yes :D 
Sure, you *could* do that, but we're not working on it here. I debated putting in the comment; I wasn't sure if it would clarify or confuse. You can safely ignore it.
I don't see what the objection is. Some people like to think of it as a function that takes no arguments. Whatever helps them understand it better.
Thanks! We need even more experience in this area. (I corrected the css in the link, reload may be needed.)
extractThree crashes if you have less than 1 or 2 elements in it, but works on 0 or above 3, which seems really weird. I'd have written it as: constructList text = [Triplet x y z | x:y:z:_ &lt;- tails $ words text] 
Yes, I see; mostly an omission from my part because I knew my text wouldn't start with less than 3 words. Edit: applied your suggestion to the code.
That would be pretty killer, I must say.
Fun. I'm hoping you were looking for style tips rather than algorithm ones, since I can't help you there. In that vein though the function **makeSenseOf** is probably all I'd change - just a bit of tidying and making some things point-free: makeSenseOf :: [Triplet] -&gt; String makeSenseOf (x:xs) = showAll x ++ findNextTriplet (thirdWord x) (reverse xs) where removeFirstItem [] = id removeFirstItem i = filter (/= head i) findNextTriplet n n' | null t = "" | otherwise = " " ++ showPartial (head t) ++ findNextTriplet (thirdWord (head t)) m where t = filter ((== n) . firstWord) n' m = removeFirstItem t n'
Thanks for the suggestion. Oh, I totally forgot about the identity function :)
I wonder how Haskell's minimal (some say impoverished) module system will help or hurt this effort.
Excellent news! So how does this relate to the runtime system? Do you need a full libc on the target system?
This is wonderful, putting the paper up on the arXiv. CS conference proceedings use two columns to save trees, but they're a century off in how one does this. Radically brilliant CS authors are typically earnestly conventional in playing Simon Says with these rules. This is "conservation of hipness", and the lack of use of ArXiv is a deadly combination. With ArXiv, one can edit the document class to include a 'onecolumn' option, reTeX, and be off to the races on an iPad. Kudos to these authors, for making their source available.
Like SafeHaskell ? http://hackage.haskell.org/trac/ghc/wiki/SafeHaskell The reason that this flag has only recently been added is because there isn't any widespread abuse of unsafePerformIO and friends leading to bugs. And, to write most any real program you are going to need to call some functions that rely on unsafePerformIO, etc. I think the primary motivation for adding SafeHaskell now is because people want to make applications like lambdabot and web apps which run untrusted code entered by anonymous users on the Internet. So, it is a security issue. Using SafeHaskell is not likely to reduce the number of bugs in code, IMO. 
You've got the right idea, working with the tails. One wants to randomly choose a continuation with a given prefix, if one exists. I don't see a random function in your code, and it would be faster to look up keys using a map.
uclibc/dietlibc should be more than enough. If you replace GMP with something else, the only thing the RTS really needs should be a way to malloc, and possibly spawn an OS thread... the RTS has been made to run on bare x86 metal, shouldn't be too hard to do the same for ARM.
Yeah about that, initially I was about to use random selections, or better said shuffle the list between pickups. But the problem for which I abandoned that and kept going on was because I couldn't figure how to "unwrap" a `IO [a] -&gt; [a]` **edit:** I'm not really getting it, how would you do lookups with map?
There are three kinds of mathematicians: Those who can count, and those who can't.
You can't unwrap IO. You could wrap whatever computations need to make arbitrary decisions in IO, though (make them return `IO String` instead of `String`), or just use the State monad to track a [`RandomGen`](http://hackage.haskell.org/packages/archive/random/latest/doc/html/System-Random.html#t:RandomGen). [Or, you could create all the possible expansions, and just choose between them later](https://gist.github.com/1176235). Note how I use `Map` to easily lookup follow on pairs for each word, and `Set` to avoid having multiple copies of each triple.
Anyone know any links to the source?
Does this in any way put us any closer to Haskell on Android, or is that utterly unrelated?
As far as I know there's native binary support for android, so this should bring us closer.
You are referring to the paragraph starting with &gt; The trouble with this formulation is in the notion of merge. It will not do to merge the two streams by simply taking alternately one request from Peter and one request from Paul. ? I don't see much of a problem here. Yes, you need to know about the order in which the requests occurred. In an imperative model, this order is implicit in the order of function calls. In a functional model, you have to introduce an explicit notion of time, for instance type RequestStream a = [(Time,a)] Writing a corresponding `merge` function is left as an exercise. PS: The functional model above does have a problem, namely that it cannot be used in real-time. This is a known issue in *functional reactive programming* (FRP) and every FRP library worth its salt should be able to handle this, like my own [reactive-banana](http://www.haskell.org/haskellwiki/Reactive-banana). 
Nope, thats not the problem. The problem is sort of similar to Halting Problem, where you don't know whether you are in an infnite loop or a ridiculously long computation. That's here, you don't know if you have to wait for a stream to produce its value or it simply won't produce it. You don't have a deterministic strategy without a "state" called time. From what I understand, this can't be achieved by such an augmented structure with "time" either. Imperative world does something like "time slicing", and choose to move to the next stream, if it has timed out to get the result on a stream.
It appears to me that you are describing precisely the real-time issue I mentioned. In particular, the problem is that the representation `[(Time,a)]` cannot distinguish between "`_|_`" and "`5 seconds have passed and then _|_`".
`standard` would be much more regular if coded as: standard :: Cube standard (x,y,z) | abs x &gt;= abs y &amp;&amp; abs x &gt;= abs z = if x &lt; 0 then green else blue | abs y &gt;= abs x &amp;&amp; abs y &gt;= abs z = if y &lt; 0 then yellow else white | abs z &gt;= abs x &amp;&amp; abs z &gt;= abs y = if z &lt; 0 then orange else red Proving exhaustiveness of the matches is a simple exercise in your CAS of choice.
Exciting!
Or, put another way, _now_ what is the state of Haskell on iPhone and Android?
There's a short video discussing the merge problem for anyone interested: [SICP / Whats wrong with Purely Function Paradigm / Part 2.4](http://www.youtube.com/watch?v=YyuBG-TDBcw) It's an extract from the [SICP Video Lectures (1986)](http://groups.csail.mit.edu/mac/classes/6.001/abelson-sussman-lectures/)
It's related. For android we will need ghc cross-compilation support which Stephen is working on. I'll work on rts/linker to smash some issues and also I'd like to have GHCi running on android tablet...
I found [this little gem on Hackage](http://hackage.haskell.org/package/markov-chain-0.0.3.1). It's a nice concise version of what you are trying to do with variable length contexts. A little tricky to grok, but once you finish with writing your attempt, you can see how someone else solved the problem.
The person asking the first question looks like Alan Key. :-)
Thanks! Looks like FRP is what I was looking for. Is there any resource like FRP 101, which gives the basic idea of how "time slicing" is implemented? I guess "time slicing" solves everything which would need an internal state of time and every other problem involving an internal state is solved by lazy lists/structures or what SICP calls "streams", right?
&gt; It seems that there are more and more articles about DevOps (and their importance), but somehow they are considered "second class employees" (compared to developers) in many companies. Well, devops in most companies is a role where you take responsibility for amateurish mistakes made by devs. Any "fall guy" is necessarily "second class". 
Sorry to put on my feminist hat where it might not be welcome, but comments like this contribute to an atmosphere which makes some women feel unwelcome in the field of computer science (and other technical fields). A relatively recent [blog post](http://blogs.scientificamerican.com/cocktail-party-physics/2011/07/20/is-it-cold-in-here/) for Scientific American does a better job explaining than I would, and it's a pretty good read and something worth thinking about.
A perhaps related problem: How do you split streams by a key efficiently, preserving laziness? splitStream::Ord key =&gt; [(key,value)] -&gt; [(key,[value])] Each (key,value) pair should be visited O(1) times for the solution to be considered efficient.
If you by "visited" mean the number of comparisons used on the key then this is impossible in any language. 
You are right. Let's say we have Enum key also.
[accumArray](http://hackage.haskell.org/packages/archive/array/latest/doc/html/Data-Array-IArray.html#v:accumArray) assocs . accumArray (flip (:)) [] (minBound, maxBound) :: (Bounded i, Ix i, IArray a [e']) =&gt; [(i, e)] -&gt; [(i, [e])]
&gt; I’d be curious to know what more experienced Haskell programmers think about Maybe Just use whatever type holds the information you need. If you need error messages, use Either; if you don't, use Maybe; if there's more than one failure mode, define a type with more alternatives.
I'm only a Haskell newbie, but I think this article sums up all the ways to report errors in Haskell very concisely: http://www.randomhacks.net/articles/2007/03/10/haskell-8-ways-to-report-errors 
&gt; Use Monad and fail [...] If you’re writing new Haskell libraries for public consumption, and all your errors are strings, please consider using this error-reporting method. Cue the intense disagreement... now!
Exactly! Trying to classify possible results as "error" and "not an error" is bound to be a fuzzy, inexact art. How do you know, when you're building an API, whether certain possibilities will be considered an "error" or not. That depends on the person using it. On the other hand, you *do* know how many different kinds of results something can have, and what information is associated with each.
&gt; Posted by Eric Kidd on Saturday, March 10, 2007 Perhaps a *bit* dated at this point? I'm pretty sure that sure that several of these can be summarily filed under "hell no", `Maybe` isn't really suitable for *errors* as opposed to non-results as an expected possibility, and it's reasonable to have more than one error-handling method in the end because one-size-sorta-fits-most solutions are unappealing. So we've got, what, a couple different flavors of `Either` and exceptions in `IO`. That's two options, or three if you pretend that `ErrorT` is completely different from `Either`.
&gt; Perhaps a bit dated at this point? Someone mentioned it in [another thread](http://www.reddit.com/r/haskell/comments/jxj8i/data_maybe_harmful/) and I wanted to ask a question about it. I figured it had probably been submitted before, so by "submitting" it I could see if someone had already answered my question. (I figured it out anyway, I was misreading something.)
Thanks for all the feedback, we've updated the job titles and descriptions to match reality a bit better. Requirements for our DevOps and Programmer positions are similar, but one has an emphasis on systems engineering and the other on writing applications.
&gt; How do you know, when you're building an API, whether certain possibilities will be considered an "error" or not. Generally, the difference between "no result" being an error or not comes down to whether it's expected to be a sensible outcome and, perhaps more importantly, whether a non-answer is sufficiently informative on its own. Getting `Nothing` from looking up a value in a map isn't an error; you got a non-answer because you asked for something that wasn't there, and that's all there is to say on the matter. An error typically implies at least some degree of explaining *why* something failed, generally because failures are edge cases (and you want to collect a bunch of possible failure modes and handle them as a group, but still need to know which one you have) or because they can't be enumerated easily (so you need whatever found the error to attach an explanation). It's a matter of degree more than a simple yes/no. If you think of something like a parser, failure is very much expected as a possibility, but you have to have general error handling because of how complicated the errors can be. On the other hand, a lot of I/O operations can be reasonably assumed to succeed most of the time, but can fail for all kinds of peculiar reasons, so you'd like to just handle them together in order to log the failure or report the error, possibly retry or just clean things up and exit, or whatever. Two cases that need "error handling" but nearly opposite scenarios in how they occur!
Yes, I realize that now. You may notice I just posted a longer-ish comment over there, since I made the mistake of looking at this one first. Welp, anyway... :T
Is *removeFirstItem* necessary? It doesn't make the code easier to read for me. What about simply this: m = filter (/= head t) n' *m* is only used within the *otherwise* clause, so *t* is guaranteed to be non-null. Another, minor nitpick: when I see *foo n n'* I expect *n* and *n'* to be of the same type. But here *n* is a String while *n'* is a [Triplet]. Instead, I would call the parameters something like *word* and *triplets* or *w* and *ws* if you want to be more concise.
This article makes a good case for distinguinshing between *errors* and *exceptions*: http://www.haskell.org/haskellwiki/Error_vs._Exception
&gt; ...because one-size-sorta-fits-most solutions are unappealing. ...alas it gets annoying, if every library uses a different error/exception handling, and you need to write impedance-match boilerplate code in your code to convert between those... :-/ 
There is an old [discussion][2] on the mailing list about this problem, with a very nice [solution][1] by Bertram Felgenhauer. [2]: http://haskell.org/pipermail/haskell-cafe/2006-September/018112.html [1]: http://haskell.org/pipermail/haskell-cafe/2006-September/018133.html
&gt; Now I definitely don’t think that Maybe is not useful sometimes
&gt; Is there any resource like FRP 101, which gives the basic idea of how "time slicing" is implemented? I'm not aware of any FRP resource that describes a working implementation for this particular problem. (Conal Elliott has [described a solution][1] that changes Haskell's semantics to include some form of concurrency, but it appears that this didn't work out so well.) You may want to have a look at the [model implementation][model] in reactive-banana. Basically, the idea is to represent event streams as type Event a = [(Time,[a])] and require that all input events are synchronized. For the bank account example, this would mean that the inputs look something like this: alice = [(t1, [put]), (t2,[] ), ... ] bob = [(t1, [] ), (t2,[get]), ... ] In a sense, I merely shift the problem to the driver code that has to hook up the pure FRP world to the real world. For a general introduction to FRP, have a look at the seminal paper [Functional Reactive Animation][2]. &gt; I guess "time slicing" solves everything which would need an internal state of time and The special problem here is that time is not just an additional datum, but related to the execution time of the program. That's why the simple `[(Time,a)]` model does not work, even thought it's semantically correct. &gt; every other problem involving an internal state is solved by lazy lists/structures or what SICP calls "streams", right? No. There are many forms of state and many different ways to represent it. Here a selection: * accumulator values -&gt; simply pass a parameter * mostly implicit state -&gt; state monad * automata -&gt; streams, continuations or existential quantification The basic idea for modeling state is simply to pass it around explicitely myfunction :: State -&gt; (Result, State) (result, newstate) = myfunction oldstate [1]: http://conal.net/papers/push-pull-frp/ [2]: http://conal.net/papers/icfp97/ [model]: http://hackage.haskell.org/packages/archive/reactive-banana/0.4.1.1/doc/html/src/Reactive-Banana-Model.html#Model
FRP resources are somewhat scattered around the place. I think FRP is a revolutionary idea, so it's a technology worth taking the time to understand. Here's at least an answer to your specific question: There are generally two approaches in common use. One is to model a stream of events using a lazy value of type [(Time, a)]. The other is to model it imperatively using either a stateful system (Yampa) or the observer pattern (Scala.React). The idea is that it doesn't matter that the implementation is horrible, because you only have to do it once, and it's hidden away behind a clean FRP abstraction.
One thing I don't like about Maybe is that it's still easy to sometimes create an abstraction that is just basically type checked nulls. It's good, because at least all the pieces fit together, assuming you're actually *handling* the Nothing case at some point. Often, if you spend time thinking about what the absence of a value would really mean, you end up with a much more workable abstraction, and can play to the richness of an API. As an example, I've recently been working on an application that interacts with a database. Lets say it has Book objects for now. Now a Book in the database has a unique ID (as much as I hate them, that's another story) - so we want: data Book = Book { bookId :: Int, bookName :: Text } Right? Well, this is great if we never need to create or update books. If we do though, we're stuck. We either "invent" an ID, which is dangerous as we're just masking the problem; we trickle "Maybe" up into the Book type, which makes client code more painful *or* we realise that there's more here than meets the eye: data InDatabase a = data { dbRowId :: Int, dbInfo :: a } instance Copointed InDatabase where copoint = dbInfo Aha! The row ID part actually means that our database object, whatever it is is actually in the database. In this sense, it represents the context of being within the database. And while we can't get into this context (at least, not as a -&gt; InDatabase a) it's not Pointed, we can go the other way (InDatabase a -&gt; a), so it's at least Copointed. I suppose my tl;dr is a strange little argument, which is one that feels very unique to Haskell. Once you start learning how all the pieces glue together, you can get caught up writing stuff so fast, because half of the time it really does just work. But with a little bit more thought, you can get a lot more mileage.
Yes, but the point is that accumArray cannot(?) be written in Haskell with the desired complexity.
I've always thought of them the other way around. Exceptions are unrecoverable situations (e.g. OOM), while errors can be fixed I.e. should be handled by the "user". I believe Go has them defined this way.
Good find.
Fantastic plan! As a distro maintainer of quite a lot of packages this could really simplify life too, assuming it could also replace ghc's current ABI hashes.
Ever heard of exception handling?
My main issue with that article is that using the Monad class's fail function is not recommended by almost anybody. The default implementation throws an error, but other implementations don't, so you can't really know what you're going to get if you use it in functions that are over all instances of Monad.
There's also the aspect that Maybe is used ubiquitously because it's in the Prelude. The truth is, lots of the Prelude loses its usefulness the longer that one uses Haskell. foldl? Use Data.List.foldl' instead. map? You might as well use fmap to keep things more general. That's not to say that there's not a place for the Prelude, because plenty of it is still useful, but using data types and functions that are more domain-oriented can certainly make code clearer.
Reworded: Maybe may be useful sometimes.
Does anyone have the answer to the exercises?
For the first exercise, bottom, I have the following answer. I only get a hungarian message though: bottom c f = c `cube` (down . down) `top` f `top` f `top` f `cube` (up . up)
He might be thinking of OCAML http://en.wikipedia.org/wiki/Categorical_abstract_machine
Unfortunately, most of the world uses these terms differently.
Yeah, this post needs to be updated. I’m working on an update to the article, should be out in the next half hour.
Yes, it's a matter of degree... but even more than that, it's a matter of perspective. I've used plenty of maps where looking up something that isn't there is an "error" in the strongest possible sense of that term: for example, when I'm communicating between instances of a distributed application using an internal network protocol. Then if the command name doesn't occur in the map, that's every bit as much an error as if I'd tried to refer to an undefined variable. Indeed, one wishes it could be caught at compile time. So you can't even say to what *extent* something is an error, without knowing about the person *using* your library. That's why, in the end, it's better to give up this artificial classification where you might say "use Maybe for normal returns, and not for errors", and instead just say what db4n did: use the type that conveys the information you have to convey.
http://blog.ezyang.com/2011/08/8-ways-to-report-errors-in-haskell-revisited/
&gt; That's why, in the end, it's better to give up this artificial classification where you might say "use Maybe for normal returns, and not for errors", and instead just say what db4n did: use the type that conveys the information you have to convey. That's certainly fair. The issue with `Maybe` is that it's something of a degenerate case, because it conveys no information other than "nope!". There are very few cases where that's acceptable for error conditions; for the example you give, though, where the error is of the "this should be impossible" variety anyway, I can agree that it makes sense.
`Maybe` is a a fundamentally important type, both in practice and in theory. It deserves to be used copiously, and it deserves to be in the Prelude. Of course, anything can be misused and abused. But I don't share much of the OP's concern. If anything, `Maybe` is used less than it should be. I find myself using it more and more over the years.
Oh noes! The misconception of using `fail` for failure is resurrected yet again. `fail` refers to **pattern match failure**, not general failure. It's really not supposed to be part of the `Monad` class; it was only added to support the special magic syntax of `do` notation. This has got to be one of the most unfortunate choices of method names ever.
nice! &gt; I’ll also note that canonicalizing errors that the libraries you are interoperating is a good thing what do you mean by "canonicalizing errors"? could you elaborate on that a bit?
There are lots of names that unfortunate for people coming from say, Java, to Haskell. The most obvious ones would be `instance`, `class`, and `return`. But I'm sure Haskell had those names first, right? So it's entirely Java's fault?
You're right, I was exaggerating. There are some *very* unfortunate names out there, a lot worse than this one. But this one has taken people down the wrong road a surprising number of times. `return` is pretty bad. But it's so wrong that it doesn't take people very long to realize that they should just ignore the usual meaning of the word. So it doesn't cause much trouble. I don't think `class` and `instance` are bad in either Haskell or OOP. But yeah, that historical coincidence often causes confusion.
Nice! Thanks for doing that. :]
I tend to find `Either String` fairly useful. Say you want to run a fancy computation on a bunch of inputs and report the results. There's a million ways for the computation to fail -- but ultimately, all you ever do is report why it failed -- i.e. there's no special logic for different recovery paths. `Either String` is perfect for this sort of thing. If you're producing an app where "bad" user input could result in lots of errors, but the app is interactive enough that you just want to tell the user what the errors were, `Either String` can take you pretty far...
&gt; One thing I don't like about Maybe is that it's still easy to sometimes create an abstraction that is just basically type checked nulls. Interesting that you say that, because my thought was the opposite: Maybe is a great starting point *because* it is basically type-checked nulls. Coming from mainstream languages where nullable types are ubiquitous and unchecked, having Maybe feels like a step in the right direction to me. I get what you and the article are saying, that you tend to use Maybe less as you gain experience / your API evolves, but as a baseline tool in the Prelude, I would hardly call it "harmful".
I don't disagree that Maybe is an important type, especially for theory. I probably should have indicated in the first place that this commentary was geared towards the concept of using Maybe as a way of dealing with errors or unexpected results. It is simple enough for beginners to understand, and it undoubtedly helps them reduce the chances of things going wrong in their code, but at the cost of semantic distinction. I ran into this problem a lot myself as a beginner. If I did something wrong somewhere, I had a heck of a time trying to figure out where things went wrong when I evaluated a function that operated on large sets of data and all that popped out was... Nothing. As I learned about extensible exceptions and other error handling methodologies, my code gained a lot of clarity.
"Yes but." I can do the same thing with imprecise exceptions (error), and without needing to bring out the monadic guns.
As in, converting them into your application wide error type. Maybe you don't actually want to report some low level library error, but some other error message. Thinking about that is a good thing.
I am reminded of Harper's post from a month or two ago about Boolean being a bad type, because True and False are very low on information, and according to him one should generally define a real type like `data Found = WasFound | WasNotFound` so that booleans of different types (in the type theoretic sense as well as the casual English sense) can't get crossed up. In theory, it's not a bad point. In practice, I find that I'm often combining Nothings from, say, both Map and Set operations, and if they both defined `data MapItem a = MapNotFound | MapItem a` and `data SetItem a = SetNotFound | SetItem a`, where I've gained in specificity and type safety I've just suffered a major setback in composability and code clarity, as my code boilerplate has grown a lot to deal with those possibilities since I can no longer just use Maybe monadically/applicatively/whatever, and even if one defines all the relevant instances on \*Item it's still not obvious to me how to use them together without doing something more exotic. Any "fix" of this issue would have to balance the utility of the ability to compose two things that use Nothing together easily, or perhaps somehow fix it so the things can be composed easily.
Except, imprecise exceptions are a royal freaking pain if you're not careful, since you have to be very thoughtful about what you evaluate when otherwise they slip through to weird places. Meanwhile, `Either String` is utterly straightforward. And who says you need to use a transformer for it at all? Sometimes its appropriate -- other times you can just operate on it directly with `either` or pattern matching. Basically, I tend to represent errors as explicitly as possible, most of the time. The cost of a bit of extra plumbing is more than repaid by the clarity of keeping everything completely above-the-board.
Really? Not even with Ptr and friends?
The 'error' function is out-of-band and invisible from the outside of the function implementation(!) I also think Either String or Maybe are nice monads for specific "pipelines" and simple streams of computations. I'm just a haskell learner but I really appreciate the part in haskell where the pieces of the program fit together snugly like the gears of a clockwork.
I'm kidding, but it would be cool if we had one of those fund raising like thermometers with a status of how much is done, or how many months to Android liftoff. :)
thanks. We will fix that. We are hoping for a small redesign of the home page soon.
...therefore, every library should invent its own method of handling errors, to ensure that callers are forced to convert them?
I just submitted my application to Tsuru, but what I ended up doing thanks to the fact that we can assume that whatever packets exist will show up at most 3 seconds later is keep a rolling buffer of packets that fit into that three second window. Whatever fell outside of the window on the low end was therefore able to be output in sorted order, which meant that as long as there was always approximately enough memory to hold the last 3 seconds worth of data, we wouldn't exceed memory constraints. In terms of an efficient data structure for this strategy, I used minheaps for the buffer and unioned new data into the old heap, which ended up being performant enough. On my machine without any parallelism or concurrency tuning, I could churn through 30 seconds of data and output (print to the terminal) a sufficiently sorted dataset in 3 seconds (5.9 MB of captured network traffic) or so with only about 5% of that time being GC. For reference, printing unsorted took about 1.3 seconds, and was essentially constant in memory usage at about 7000 bytes (according to heap profiling). The -r version ended up topping out at 120k max of heap space usage (which couldn't really be helped, since we have to hold on to 3 seconds worth of data no matter which way I look at it), but due to the rolling buffer strategy ended up looking more like a see-saw with periods of allocation and deallocation. Hopefully I'm not giving away too much secret sauce info here, but that was how my solution to this program ended up looking. 
&gt;Apache/Nginx are no longer required for static file serving. Does that mean that session will not be generated / looked up for static files ?
With static types and closed variants, if you want to match against it you’ll have to add an explicit case anyway.
Right right, so I'm envisioning two cases: you have some error case which is buried deep within some pure computation, and monad-ifying everything is extremely annoying and requires large transformations, or you have some error case which occurs in IO, in which case, why bother with either? Recall our constraints here are no recovery is necessary. If recovery is necessary (in which case you would want to use either or pattern match), there is a place, but I don’t see it here.
Note that laziness (and nontermination) is out-of-band and invisible from the type signature too! So you have to account for it anyway.
We're not talking "monadifying" a whole bunch of stuff. I have a computation that can fail a bunch of ways. If it hits a failure condition, it returns `Left "oh this thing happened that shouldn't"` with an appropriate string. Otherwise it returns `Right answer`. Then I collect up all the errors -- one reason Eithers are great is I can unzip/partition them--and return them in one place, and collect up all the successes and display them a different way. Simple! And *nowhere*, unlike with exceptions, do I ever have to worry about explicitly forcing a single thing. If I have an occasional error case buried in a pure computation, and I backed myself into a corner, then sure, using an `error` is the way to go. But if I have a computation where the *majority* of code is distinguishing error paths, then the picture looks quite different. Part of the issue is that either is the *desired* result -- so that I can partition them and separate failures and successes. If I'm just, conceptually, going to throw errors so that I can catch them and turn them into eithers, then why not just use eithers to begin with and save a bunch of confusion and hassle?
So don't write nonterminating computations then!
`error` is rather scary in what GHC's optimiser is allowed to do with it, though. Since it is (defined to be) equivalent to `_|_`, GHC may replace it with any other bottom-like value. The same is true for `throw`. That means that you may try to catch an asynchronous exception, but you won't be able to, because a different exception is being thrown. As scary as that may sound, I've never actually seen this happen in practise. Apparently, when you're in `IO` you should always use `throwIO` rather than `throw`. This is what the Simons told me (or at least how I understood what they told me). I'd like to see some examples of where that has actually bitten someone. I suppose I need to read the asynchronous exception paper...
Thanks. Seems I've asked this question before! I don't think that solution is very nice at all -- why should I have to reimplement some special-case version of Data.Map to implement this simple function? A real-world analogy would be sorting mail addressed to different people in your office, but I'm sure readers can come up with their other, better, analogies. Rewriting Data.Map is arguably barking up the wrong tree. In an imperative or a message-passing solution, Data.Map is just fine, so the problem must lie elsewhere.
Sessions are stored in a cookie, so the only way to avoid that overhead is to serve static files from a subdomain. You can do so with either nginx or warp.
In fact, why doesn't the compiler just check your functions for you and make sure that they always terminate? ;)
Thanks.
Haha, I didn't realize that it's you who asked this very question. :D Keep in mind that your function is, apparently, not simple at all when you want to preserve laziness. The point is that the consumer can access the different channels in any order he wishes and `splitStreams` has to advance the stream just enough to fulfill his demands. Also, the data structure is persistent, i.e. no mutation. Implementing all this is quite tricky in an imperative language, too. Bertram's solution is nice because his technique is novel and generalizes to many other situations. If you want something that is easier to implement, you can try my [infinite random access list](http://haskell.org/pipermail/haskell-cafe/2006-September/018152.html). 
Ever heard of error handling? The terms have different meanings indifferent languages. In C, etc, everything is an error, so I've always thought of functions returning errors, rather than exceptions. Exceptions are thrown (terminate a function abnormally), not errors.
I sometimes use `Either String` for development. I use `fail` for error cases, then when testing I use the type `Either String a`, and I can see the error message; while in other parts of the code I use `Maybe a`. I often write `fail "some reason"` instead of `mzero`, just for documentation purposes.
Generally, I avoid `fail`. If you're living in `Either String`, then `throwError` is what you should be using. `fail` is an abomination, frankly. It exists to handle pattern matching failures in a `do` block. In the old days we had `MonadZero`, a type class between `Monad` and `MonadPlus`, which provided `mzero` but not `mplus`. It was ideal for capturing failure monads. Stupidly it was removed, `fail` was added to `Monad` (which is idiotic, since most monads can't fail gracefully). I think this was all part of the Haskell 98 stupidity. Why even have `MonadPlus`, anyway? We already have a `Monoid` type class. This is right up there with all the list-specific functions in the Prelude. We have `Functor`, `Traversable`, `Foldable`, etc. So much feature duplication for no gain whatsoever that I can see.
snoyberg, can you tell us what the outcome of the fork of aeson-native from aeson will be? Are the changes being merged back to aeson? What is the long-/mid-term goal?
Many of the theorem provers using dependent types (Coq, Agda) do in fact do just that. Of course, because of the halting problem, this necessarily means that there are algorithms that will always halt that nonetheless cannot be expressed in those languages. Sometimes, explicit bounding is necessary to get around this. On the other hand, it feels good to live in a total world. No worrying about laziness or strictness, and everything is clean and clear.
This experience of "monadification" that so many people talk about is foreign to me. If I have something deep in a pure computation that I want to change to be in a functor, I extract that part of the computation, do what I need to it, then fmap the rest of the computation into that. In my opinion, a large number of binds where a few fmaps will do is a code smell. Even a sequence of fmaps can be a code smell, since you can often collapse them into one.
Yeah, the fact that you're unable to tell whether a function will throw an exception in Haskell just by looking at the type is really annoying.
I've asked Bryan to implement this change. All it boils down to is whether the native or C++ backend is the default. If Bryan decides to switch to native-by-default, the fork will disappear. But Yesod can't rely on a package that, out of the box, crashes Yesod. Assuming Bryan does not implement this change, then I guess the goals are: * Mid-term: keep aeson-native and blaze-textual-native synced with their parent packages. * Long-term: when the underlying GHC bugs that caused it are resolved, and Yesod stops officially supporting the affected versions, we'll drop the fork.
[Item 9 in the article](http://hackage.haskell.org/package/control-monad-exception) seems to implement something like it.
Return is rough on us n00bs. I don't think Haskell had that one first either. No cookie for whoever picked that.
&gt; If you need to distinguish errors in pure code, for the love of god, don’t use strings, make an enumerable type! I am sympathetic to this view. I'd even like to go further, following [the view from the left](http://strictlypositive.org/view.ps.gz), and using an isomorphism to transform the input `T` into `Either Bad Good` where the type Bad exactly captures the type of invalid data and return `Either Bad Result`. However in practice I fear it isn't worth the effort, even for enumerations. Every function that can fail in a complex way would need its own type. You would need more types sum the enumerations (or construct complex `Bad` types out of little `Bad` types). This could give great error messages, but this is a lot of work just to handle erroneous inputs. The reality appears to be that either the errors will be ignored by using some sort of default value (possibly logging an error message) or have an error message printed out to stdout/stderr and halting the program. Since in the end all that happens is either the error is printed or ignored, why not use `Either String`? `Either String` is easy to use, can provide a simple call trace that is often sufficient to pinpoint the error in the input, and can be easily ignored. 
Full set of documents (from [this course](http://www.cs.columbia.edu/~sedwards/classes/2007/w4115-fall/) website): * Proposal [[pdf](http://www.cs.columbia.edu/~sedwards/classes/2007/w4115-fall/proposals/HCAS.pdf)] * Language reference manual [[pdf](http://www.cs.columbia.edu/~sedwards/classes/2007/w4115-fall/lrms/HCAS.pdf)] * Final report [[pdf](http://www.cs.columbia.edu/~sedwards/classes/2007/w4115-fall/reports/HCAS.pdf)] * Slides [[pdf](http://www.cs.columbia.edu/~sedwards/classes/2007/w4115-fall/reports/HCAS-slides.pdf)]
Maybe I'm misremembering, but I don't really think he'd suggest that `Found | NotFound` is much better than `True | False`. It isn't just the name that is bad, or that you use the same type for several situations. The problem is that it's just one bit of information without any necessary connection to where it came from. For instance, in: case null l of True -&gt; ... False -&gt; ... head l ... The problem isn't that it should instead be: case null l of IsNull -&gt; ... NotNull -&gt; ... head l ... but that it should instead be: case l of [] -&gt; ... x:xs -&gt; ... x ... where the test somehow hands us refined versions of the input that can only be used properly. As opposed to the first two, which are just variations on, "I have these bits floating around indicating whether it's okay to use unsafe operations on these unrelated values." The map/set version of this is lookup returning a `Maybe` that you match on versus using a `contains` and a partial `get :: Map k v -&gt; k -&gt; v`.
And we (Well-Typed) are running the Haskell track.
Well, sure, except checked exceptions are really-really annoying. Plus there's asynchronious exceptions, which can be anywhere.
I agree there are a lot of implicit requirements that make the problem harder than it should be, and under those constraints, maybe the proposed solutions are the only possible. While the solutions are cool in a geeky sense, it's not the kind of code I'd be excited to see in a project I'd have to maintain. A possible solution in an imperative language/sublanguage would involve a bunch of FIFO queues and a demux and some output streams that tug on the FIFO queues and ask the demux for more data when empty. Using something like SML's functional IO layer that wraps the imperative IO layer, you could even make the data structure persistent and referentially transparent. Robert Harper blogged about "benign side effects" a while ago; I think this is a nice example of that. Found the article: http://existentialtype.wordpress.com/2011/05/01/of-course-ml-has-monads/ 
The #1 thing I do not like about exceptions is that it's still not clear to me how they work. Can anybody provide a good explanation of how throw and catch are implemented at the most basic level? Is it actually implemented purely in Haskell or does it use "out-of-band" features to work? The other thing I don't like is the idea of catching errors. An error is something that I would think would be uncatchable, like somebody unplugging the computer or an unterminating computation. It seems wrong to conflate error handling with exception handling.
&gt; FP Day is a new event for the Functional Programming community. Not entirely new. The Dutch have had annual Functioneel Programmeren Dagen for quite some time now.
I could be misremembering too, but I'm pretty sure that in some cases you do irreducibly have a boolean of some sort, like for a simple "contains" check, and he would prefer separate types for that. Extracting an element from a container is a special case, and he would have you use case statements for that, but that doesn't cover everything, I don't think. I suppose I should go back and re-read it, to be fair. Stupid fairness.
&gt; A possible solution in an imperative language/sublanguage would involve a bunch of FIFO queues and a demux and some output streams that tug on the FIFO queues and ask the demux for more data when empty. You'll have a hard time making the FIFO queues to be persistent. After all, tugging something at the end needs to be invisible to the consumer. &gt; While the solutions are cool in a geeky sense, it's not the kind of code I'd be excited to see in a project I'd have to maintain. I don't see a problem with maintaining a solid abstraction like "infinite random access list". Can't help it that they're not in the standard libraries yet. ;-) In other words: There are more data structures than just FIFO queues and finite maps. If you constrain yourself to the latter, then you're necessarily constrained and "benign" side effects may look like a familiar option. 
the second point is just a terminological issue. as to the former issue -- the actual implementation is deep in the runtime system. if we pretend that we're only using "throw" and "catch" (and this ignoring pure and asynchronous exceptions) then it works just as it would in the EitherT monad, semantics-wise, and you can ignore the implementation. The semantics of imprecise exceptions (and asynchronous exceptions) are a more tricky beast. See SPJ's "Tackling the Akward Squad": http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/ Asynchronous exceptions are both very useful and a royal pain. It would be interesting to see if there was a good set of library primitives/tools that would make it easy to program in the large without them, since my sense is that dealing with them has a large impact on the complexity of the runtime system.
You'd still have to worry about laziness/eagerness due to operational concerns (though you are freed from worrying about strictness/non-strictness).
&gt; You'll have a hard time making the FIFO queues to be persistent. After &gt; all, tugging something at the end needs to be invisible to the consumer. From the API point of view it's persistent enough -- the functional/stream layer takes care of that. &gt; I don't see a problem with maintaining a solid abstraction like "infinite random access list" Me neither. We can assume that building blocks already exist if they are reasonably general. The problem is that the splitStream' code itself is not very readable/understandable. I understand how it works, but it seems hard to describe in a few sentences. Responsibilities seem tangled up in that function.
Everybody says checked exceptions are really annoying -- but is it really true, or is it just Java's strawman implementation of them that everyone hates? `Either e` is really a form of checked exceptions, and it's great.
The article explains how `fail` is no longer `Left` in `Either`, but `mzero` is probably still `Left`.
I've seen `trace`s disappear in practice, though...
£132 - I'm guessing this isn't aimed at students then.
&gt; The problem is that the splitStream' code itself is not very readable/understandable It's probably not my finest work. Maybe the follow-up version is nicer.
That article was pretty confusing when I read it too, but someone explained it on reddit (basically what doliorules said) and it became all clear. IIRC his article was more than just using case statements - you want to make predicates to also give a "witness" of its answer. Another example is the "obj instanceof Foo" check found in languages like Java, which is not good because when returning true, it doesn't give you the information that you're really after: "(Foo)obj".
Yeah, it's nicer. Especially if identifiers were a little more revealing of intent. It's the best solution so far.
What is the difference between non-strict/strict and lazy/eager? I wasn't aware that there was one.
Strict/non-strict are properties of the *denotational* semantics of functions: If `f _|_ == _|_` then `f` is strict on its argument, regardless of whether it actually evaluates it in runtime or not. If `f _|_ /= _|_` then `f` is non-strict. Eager/Lazy are properties of the *operational* behavior of functions. If `f` is eager on its argument, it means that at runtime, its argument will be evaluated (to WHNF or some other form) before `f` is evaluated. If `f` is lazy on its argument, it means that it will only be evaluated once (memoized if result is re-used) and only if it is actually needed. Functions that are lazy on their argument are typically also non-strict. Functions that are eager on their argument are typically strict in that argument. A function can be eager and non-strict (e.g: If it takes as an argument a valid proof that its argument is never `_|_`). A function can be lazy and strict (e.g: if it can infer whether the argument is _|_ without evaluating it, and evaluate to `_|_`).
Awesome! Great to hear that this is happening, always thought something along this line would be cool (though I was usually thinking of it in terms of contracts for modules provided as parameters ala newspeak). Hopefully there will be tools to come up with a minimal dependency description? No point in constraining on unnecessary module contracts. Something I've always wanted to see is a system for expressing the implementation of deprecated / changed functions of prior versions in terms of the newer API. Combined with an inlining refactoring, ideally knowing about rewrite rules, Haskell identities, etc, this would allow for automatically (likely user-involved) re-targeting for updated API versions. This might sound overkill / unnecessary / in-feasible for the general case, but I think it would be very healthy for the library ecosystem. By removing most of the library-consumer pain of trivial API changes, library writers will feel free-er to fix little annoyances. Seems like this could actually be done now (of course the refactorer would need to be implemented), the only question is whether it ever makes sense to automatically use these version-compatibility interfaces. If so, then it would seem necessary to allow one cabal version to offer multiple API versions. This could potentially help with some of the dependency-hell issues, where there's no tenable set of packages, as you could conceivably use two package versions at once. An issue with this is that using multiple API versions could imply multiple ADT versions, which necessitate automatic conversions (breaking strong typing..). And that's even before considering typeclasses. It would be quite reasonable to not automatically use a compatibility layer, though, as this would be more likely to encourage the user to port (hopefully mostly automagically) their code to the new API. One question here, is what kind of notification (if any) the user will receive when an older version of a library is required. Hopefully some warning!
This event is aimed more at programmers from industry rather than academics and enthusiasts. It's more like the CUFP tutorial sessions. So not unique but a "Good Thing"tm.
I wonder what it goes towards.
Will the lectures be available in VOD form afterwards?
What about partial application?
&gt; fooBar def { fbAmount = 3, fbFruit = Pear } btw your record is made up of Maybes but you update the fields in your example as if those weren't 
I've done something similar myself in the past. I like it, at least for some use cases. In simpler cases you might as well just use positional parameters, and in more complex ones, I start craving something more extensible, but it'd definitely got its niche.
according to your definition, what if `f = const ⊥`? is `f` non-strict then? ;-)
This was indeed what I was after; nice answer! As you say, this is perfectly acceptable most of the time, and if it isn't you can always define a helper function somewhere. I wasn't aware of the Data.Default package, thanks for bringing it to my attention! 
f is strict.
This is not so different from how things work in my [Multiplate](http://haskell.org/haskellwiki/Multiplate).
If you want to be a little more sophisticated, in the optional argument case you can replace `Maybe` with `Last` (or `First`) and then make the record structure into a `Monoid`, which allows you to cascade a series of partial sets of arguments. `mempty` becomes your `def`. IIRC there is at least one Haskell project that does this sort of thing.
Great example of a strict and lazy function.
The Haskell code sum (replicate 3 (2 * 8) ++ replicate 3 8) * maybe 4 ( * 8) (just (x - 2))) is turned into the following JavaScript: var mul = function (v1) { return function (v2) { return v1 * v2; }; }; var fix = function (v1) { return fix = arguments.callee, v1(function (i) { return fix(v1)(i) }); }; var list = function (v1) { return function (v2) { return function (v3) { return v3.nil ? v1 : v2(v3.head)(v3.tail); }; }; }; var add = function (v1) { return function (v2) { return v1 + v2; }; }; var bool = function (v1) { return function (v2) { return function (v3) { return v3 ? v1(/*force*/) : v2(/*force*/); }; }; }; var cons = function (v1) { return function (v2) { return { head : v1, tail : v2 }; }; }; var sub = function (v1) { return function (v2) { return v1 - v2; }; }; var eq = function (v1) { return function (v2) { return v1 == v2; }; }; var maybe = function (v1) { return function (v2) { return function (v3) { return v3.nothing ? v1 : v2(v3.just); }; }; }; var just = function (v1) { return { just : v1 }; }; var c10_11 = list(0); var c10_12 = function (v1) { return function (v2) { return c10_11(function (v3) { return function (v4) { return add(v3)(v1(v4)); }; }) (v2); }; }; var c10_13 = fix(c10_12); var c10_14 = function (v1) { return function (v2) { return v1; }; }; var c10_15 = c10_14({ nil : 1 }); var c10_16 = function (v1) { return c10_15(v1); }; var c10_17 = bool( c10_16); var c10_19 = cons(8); var c10_20 = function (v1) { return function (v2) { return c10_17(function (v3) { return c10_14(c10_19(v1(sub(v2)(1))))(v3); })(eq(v2)(0)); }; }; var c10_21 = fix(c10_20); var c10_22 = c10_21(3); var c10_23 = list(c10_22); var c10_24 = function (v1) { return function (v2) { return c10_23( function (v3) { return function (v4) { return cons(v3)(v1(v4)); }; })(v2); }; }; var c10_25 = fix(c10_24); var c10_31 = mul(2); var c10_32 = c10_31(8); var c10_33 = cons(c10_32); var c10_34 = function (v1) { return function (v2) { return c10_17(function (v3) { return c10_14(c10_33(v1(sub(v2)(1))))(v3); })(eq(v2) (0)); }; }; var c10_35 = fix(c10_34); var c10_36 = c10_35(3); var c10_37 = c10_25(c10_36); var c10_38 = c10_13(c10_37); var c10_39 = mul(c10_38); var c10_40 = maybe(4); var c10_41 = function (v1) { return mul(v1)(8); }; var c10_42 = c10_40(c10_41); var __main = function (v1) { return c10_39(c10_42(just(sub(v1) (2)))); }; http://tom.lokhorst.eu/media/presentation-awesomeprelude-dhug-feb-2010.pdf
I've thought about abusing classes for variadic optional arguments (Text.Printf style). fooBar (FBAmount 3) (FBFruit Pear) I don't like it much (the result type must be specified or inferrable) though the syntax sure is pretty. Maybe someone will find it interesting... Equip your goggles. {-# Language FlexibleInstances, MultiParamTypeClasses, FunctionalDependencies, FlexibleContexts #-} class ResultType args result where apply :: args -&gt; result class ArgType arg args where setArg :: arg -&gt; args -&gt; args instance (ArgType arg args, ResultType args result) =&gt; ResultType args (arg -&gt; result) where apply args = \arg-&gt; apply (setArg arg args) ----------------------------------------------------- data MyArgs = MyArgs { myArgA :: Maybe Int, myArgB :: Maybe String } deriving Show myDefArgs = MyArgs Nothing Nothing myFun :: ResultType MyArgs r =&gt; r myFun = apply myDefArgs instance ResultType MyArgs [Char] where apply args = "The result is " ++ (show args) data MyArgA = MyArgA Int instance ArgType MyArgA MyArgs where setArg (MyArgA v) args = args { myArgA = Just v } data MyArgB = MyArgB String instance ArgType MyArgB MyArgs where setArg (MyArgB v) args = args { myArgB = Just v } example0 :: String example0 = myFun example1 :: String example1 = myFun (MyArgA 4) example2 :: String example2 = myFun (MyArgA 4) (MyArgB "string") -- edit: Supports currying too :) example3 :: ResultType MyArgs r =&gt; r example3 = myFun (MyArgA 4) (MyArgB "string") example4 :: String example4 = example3 (MyArgA 2) 
It doesn't look that inefficient to me, actually, if you consider that the definitions of all the necessary functions are given explicitly... I mean its clearly not transforming recursion to iteration on the other hand :-)
This is a commercial event, aimed at programmers from industry where it's assumed that the company pays. If you compare to other similar events or training days, then it's actually relatively cheap. You'll easily find 1-day introductory training events at around the $£€ 1k mark (erlang, scala, F# etc).
See for example the new [System.Process API](http://www.haskell.org/ghc/docs/latest/html/libraries/process-1.1.0.0/System-Process.html#v:createProcess).
This is actually a fairly well known idiom. http://hackage.haskell.org/packages/archive/data-default/0.2/doc/html/Data-Default.html has a lot of users, many of whom use it for just this http://81.26.216.99/~roel/cgi-bin/hackage-scripts/revdeps/data-default-0.3.0 going farther back there is of course http://hackage.haskell.org/packages/archive/parsec/3.1.1/doc/html/Text-Parsec-Language.html 
I never liked the `Monoid` instance for configurations, mainly because it forces everything to be optional. I think the right monoid is not the configuration itself, but the endomorphisms Configuration -&gt; Configuration This plays nicely with functional lenses. 
What does the profiler say? Especially on the more modern JS engines? And against a semantically-roughly-equivalent pure-JS frapment? Pasting a huge hunk of code isn't strong evidence for or against whether the result is "reasonably efficient". One of the oldest known things about coding is that you do not have a good profiler in your head. I don't expect miracles but it may not suck.
This reminds me of Yaron Minsky's Effective ML video. He talks about making illegal states unrepresentable (in ML). [Effective ML video](http://ocaml.janestreet.com/?q=node/85) 
I started reading this code. `mul` isn't that bad...a curried multiplication function, ok. Then comes fix (added whitespace to clear it up a bit): var fix = function(v1) { return fix = arguments.callee, v1(function (i) { return fix(v1)(i) }); }; This kinda scares me, specifically the `return fix =` part. Then there's this: var list = function(v1) { return function(v2) { return function(v3) { return v3.nil ? v1 : v2(v3.head)(v3.tail); }; }; }; This is not looking "efficient" so far. iirc, JS functions are costly to invoke, and to simulate Haskell's currying and other things, they are using a lot of functions.
From a quick glance, it seems to use basically the same untyped representation as all the usual systems. I would be more interested in a typed system (which is of course very nontrivial)
It's been documented before, too, e.g. [here](http://byorgey.wordpress.com/2010/04/03/haskell-anti-pattern-incremental-ad-hoc-parameter-abstraction/), which was even submitted to [/r/haskell](http://www.reddit.com/r/haskell/comments/bnknw/haskell_antipattern_incremental_adhoc_parameter/) before.
The argument for profiling is fair, but I agree that this doesn't look like efficient code. The thing is, even a super-modern JIT makes assumptions about how the language is used. If you use a language in an unanticipated manner, it will likely be slow. JS is very difficult to optimise and compiler writers make simplifying assumptions. In particular, they optimize for the common case possibly at the expense of the assumed uncommon cases. This code uses lots of curried functions and function applications. I'd be surprised if current JS engines are optimised for this kind of code.
This is really great. To be honest, even if we just had static library cross compilation support for Android I could write a C JNI wrapper around the library and this would be a huge win for my project (Do not need to call Android APIs from Haskell really, it is mostly data processing). I really wanted to contribute to this effort but am in the process of launching a business. The effort put into this is really appreciated!
First/Last give you a different Monoid instance. If both values are Just, `mappend` for First will use the first argument, for Last will use the second argument. I believe the Monoid instance for `Maybe a` requires/uses a Monoid instance for `a`. Using Last, you can keep 'writing' to your configuration model, and have it always use the latest value. eg. https://github.com/snoyberg/yesod-core/commit/70eba502de2ae0e36c66cad45e0cbbd5750c7774
That's actually been a subject of debate on the libraries mailing list recently. Since base follows the PVP, technically no it's not safe. However, a lot of folks have been using (&lt;5) since, in the past, major changes to base have always meant adjusting the first version number (instead of just one of the first two, as specified by the PVP). The debate is: should base be stricter than required by the PVP, in order to ensure that (&lt;5) is a good idea? I don't think any conclusion has been reached on the matter.
The other option is to just use a semigroup on the configuration itself. Then again, for most systems you'll want to perform a sanity check at the end anyways, in order to ensure that the configuration is coherent (because often configuring different options isn't entirely orthogonal). But if you're performing a sanity check at the end, that means non-sane configurations are possible in the interim. In which case you might as well just use the monoid and treat the `mempty` as non-sane for configuration options which are requisite.
What I mean is that you can't turn data Configuration = Conf { cSize :: Int, cCheck :: Bool } into a monoid. You have to change the types to something like data Configuration = Conf { cSize :: Last Int, cCheck :: Last Bool } I think this is ugly and unnecessary if you use the really nice monoid given by type ConfMonoid = Configuration -&gt; Configuration 
If doesn't seem like there has been any optimisation at all. One of the great things about a pure language is that if there are no variables in a block of code, it can be turned into a constant. The 'sum' part, for instance, should be a single number.
"This library provides native Haskell TLS and SSL protocol implementation for server and client." (For those who - like me - didn't know what this is about.)
The link on youtube says November 2007, has there been any further development in this area?
I guess you are referring to the thread associated with http://www.haskell.org/pipermail/libraries/2011-August/016576.html ?
Vincent has done absolutely amazing work with this package. It's been nothing but a pleasure using it in http-enumerator, and it has worked remarkably well with a huge number of different HTTPS server out there. Keep up the good work!
I've been looking for haskell code to experiment with Burgers' equation using fourier colocation. Henning Thieleman's Numeric.Prelude is a stable, well-maintained package. The repa library for high-perfomance, regular, polymorphic shaped arrays is the basis for paraiso - a potentially massively-parallel generator of pde solvers. These packages are on hackage.haskell.org. Jerzy Karczmarczuk has papers on Differential Algebra, Scientific Computation and Functional Programming (fun with hodge operators) and (in French) on numeric optimisation - all with interesting but incomplete code. Serge Mechveliani has "an old project" computer algebra system with excellent accompanying paper - docon http://bit.ly/q3hDVn. HTH 
Jerzy Karczmarczuk http://bit.ly/nOAsBM
Can anybody tell me what changed in base in GHC 7.2 so 4.3 had to become 4.4 and not 4.3.2?
Some people find the idea of a no-argument function confusing, so spreading that idea could make their learning more difficult. Especially since due to currying, people ALSO say "every function has exactly one argument", and those two ideas just don't work together at all. Functions are things that can be applied. If it can't be applied it isn't a function. See also: http://conal.net/blog/posts/everything-is-a-function-in-haskell
See [the release notes](http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/release-7-2-1.html#id567292). From those, we can see that it's mostly adding functions, but there are also some removed functions and added type class instances. So in general, it's not safe to depend on base &lt; 5.
As one of the authors of this (strictly experimental) package I can tell there are no real optimizations going on in the compiler. Although we do some preprocessing of the code to make sure we don't generate too much garbage. The generated JavaScript code can be seen as a strictly specializing machine which has some interesting properties. But lets start with stating that the functions we write in Haskell, like the sum (replicate 3 (2 * 8) ++ replicate 3 8) * maybe 4 ( * 8) (just (x - 2))) are not by definition lazy. It is more like a symbolic definition written in our DSL that gets compiled into strict JavaScript. We use Haskell as a meta-programming language, which is of course lazy. Now a quick list of steps the compiler performs: - The compiler[1] first instantiates all lambda variables with fresh identifiers to ensure no name clashes appear in the generated code. - Next we lift all named definitions (sum, replicate, etc) to the top level. Instead of a single expression we now have a list of definitions. - Now we annotate all lambdas in the syntax tree with a list of free variables and lift all closed terms to the top level. - We reindex all variables per definition which makes the generated code a bit more easy to read. - After this we do full common sub-expression elimination which is able to clean up a lot of redundant junk that might have been produced by using our EDSL. Now we end up with a list of definitions in the generated JavaScript code of which the last one is the 'main' functions we compiled. All the separate definitions in the JavaScript code are strictly evaluated before ever using the main function. This means when compiling a function with two arguments of which one is already given at compile time, that part of the computation is already computed before you use ever use the function. This can have benefits when when applying a function to an input more than once. For example, you want to compile something like: \input -&gt; parse myCombinator input your specific parser now gets inlined into the parse function before ever applying it to an input. That's about it. This is all proof of concept a not that much magic is going on. [1] https://github.com/tomlokhorst/AwesomePrelude/blob/master/src/Compiler/Pipeline.hs
This code blowup is of course because we also transitively compile all used functions like sum, replicate, maybe, just, fix, nil, cons etc.
Maybe is neither about errors or exceptions. Maybe is about non-existence. Non-existence in say a dictionary is neither an error or an exception.
This appears to be pure Haskell from top to bottom, as it depends on Crypto.Cipher.X, which is pure Haskell implementations of various encryption algorithms. I can't help but imagine that Haskell being what it is, this library is forever going to be extremely vulnerable to timing attacks. This stuff may be suitable for offline use or stuff that you don't _really_ care about, but I would never use this for anything truly serious.
Adding instances requires a major version bump per the PVP.
Why would haskell code be more susceptible to timing attacks? is the implementation slower and so easier to measure timing information?
Any laziness anywhere that has any condition on some aspect of the cryptotext or key could leak information. Any difference in strictness between two paths could leak information. And it has proved surprisingly practical to obtain lots of information from these attacks, it's not just theoretical. It has been demonstrated to be practical even when you're hundreds of milliseconds away from the attack target. Even getting it right in C is a challenge. There are times and places for deep, low-level control over the machine. There aren't many, but it's not zero. This is one of them.
Any slides available?
Yes, cryptography is not a simple thing. it's easy to get things wrong. that said it's true for every language, and blaming haskell laziness as a blanket for "pure haskell implementation = timing attack" sounds really wrong to me. It's possible to use a fairly imperative style and force strictness and thus get guarantee about evaluations. I strongly welcome auditing in any case :-)
Here you go: [paper](http://conal.net/papers/Eros/eros.pdf) and [slides](http://conal.net/misc/tangible-functional-programming-2007-11.pdf).
From the discussion there it seems like `base` just is too monolithic.
Blaming is the wrong word. It is simply a property of Haskell that it attempts to abstract you away from properties of the execution of the code that are highly desirable to be controlled when writing encryption code, and difficult to do correctly even when you _have_ that control. But even _that's_ the wrong question. The question is, am I wrong? (Quite possible, after all.) I mean, with all due respect, the responsibility is on the writer of the cryptographic code to establish its fundamental security, not mine to sit here and write the exploit (against a moving target). Can the relevant level of guarantee really be obtained? Has it been done?
interesting, thanks.
That makes sense. When you realize that all functions are unary functions then it is more "elegant" conceptually to treat values differently than it is to make an exception to the unary function rule.
Some good discussion on timing attacks and haskell here: http://www.haskell.org/pipermail/haskell-cafe/2010-September/083264.html
As does removing exports (e.g. String from Data.Char).
You had this project confused with a company or governement backed projects i think :-) In any case, I'm going to put your statement in perspective: "this library is forever going to be extremely vulnerable to timing attacks. This stuff may be suitable for offline use or stuff that you don't really care about, but I would never use this for anything truly serious." out of the 39 CVE for openssl (which is the only alternative stack available to haskell users) * 3 due to timing attacks (CVE-2003-0147 CVE-2003-0131 CVE-2003-0078) however * 9 due to incorrect memory handling (CVE-2010-1633 CVE-2010-0742 CVE-2010-3864 CVE-2006-3738 CVE-2003-0544 CVE-2003-0543 CVE-2002-0656 CVE-2002-0657 CVE-2002-0655) * 10 ASN1 mishandling (1 in common with memory handling) (CVE-2003-0544 CVE-2002-0659 CVE-2003-0851 CVE-2003-0545 CVE-2003-0544 CVE-2004-0975 CVE-2006-2937 CVE-2009-0789 CVE-2009-0590) * 4 NULL pointers problems (CVE-2010-0740 CVE-2010-0433 CVE-2009-1387 CVE-2004-0079) So while I'm still going to stress that i never received the same level of audit that openssl got, and the library may have security problems, maybe focusing only on crypto timing attacks for making such a strong statement seems to fail to see the big picture.
Conal stopped working on GUIs because of the "Status of Haskell + Mac + GUIs &amp; graphics": http://www.haskell.org/pipermail/haskell-cafe/2011-May/091991.html
&gt; What does the profiler say? I value implementation efficiency as high as runtime efficiency. The code should be readable and elegant, especially when it is written for an interpreted language.
Composable GUIs are a two-edged sword: they are extremely simple to compose, but they are also quite limited in what they can do. If you are interested in GUis that don't compose easily but are not limited to a fixed pattern, have a look at functional reactive programming (FRP). In particular, I'd like to advertise my [reactive-banana](http://www.haskell.org/haskellwiki/Reactive-banana/Examples) library.
Thanks hammar
jerf is incorrect to say that "this library is forever going to be extremely vulnerable to timing attacks". What you need is what I will call a Timing Black-Box to wrap around all of your primitives which might be susceptible to timing attacks. This must busy wait until a standard elapsed processing time for the primitive has been reached then exit. It needs to be a busy wait, preferably indistinguishable from the normal processing of the primitive, to prevent other processes on the machine receiving timing information. To know how long it should wait it would probably have to retain some timing statistics, or just a maximum processing time for the particular primitive. Quantising and adding jitter to the timing would help, but a constant time is the ideal. I do not see why this Timing Black-Box would be difficult to write, though it might be tricky to apply to all the right places in an efficient manner. (The discussion linked by sclv suggests the low-level primitives are the most important routines to tackle.)
I'd recommend using cabal to specify dependencies. Also, if you use WAI instead of FastCGI, then you could run this on multiple backends (FastCGI/SCGI via lighttpd, Warp, wai-handler-launch...).
I've heard and read about your library, and FRP in general, but I feel when I sit down and learn FRP it'll take some effort and will likely need a motivating application for me to work on. As it is when I'm free I'm currently spending my time looking into corecursion/self-referential data and experimenting with STM. I've been lucky enough to be able to use Haskell in two of my subjects this semester so I'm using the needs of those subjects as the direction behind my Haskell education. This google talk was just something interesting I happened across which presented a concept I'd never thought of.
I agree that it's possible to make a timing black box for just the crypto, but actually, most known timing attacks were handled at the protocol level (through the latest revision of the spec), and not at the crypto level: for example for the RSA key exchange, instead of responding that something is wrong, you just generate a random key and let the mismatch of master key fail the whole protocol later (after a network round trip, possibly multiple GCs,.. making timing moot).