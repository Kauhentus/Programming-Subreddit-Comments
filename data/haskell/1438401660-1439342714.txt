For the specific types mentioned, [roche](https://www.reddit.com/r/haskell/comments/3f8p5x/efficiency_tuples_vs_records/ctn3ti4) nails it. However, records do offer one difference, namely: you can give them strictness/unpacking annotations. Types themselves cannot be strict, and unpacked types aren't first-class, so because tuples are generic over types there's no way to specify tuples of types with these qualities/pragmata. But this is just to say that the collection of all record types is larger than the collection of all tuple types; where the two overlap, their runtime representations coincide.
You would find a wealth of comments in the [data dump](http://download.fpcomplete.com/haskell-survey-2015-05.zip) from the Haskell survey we ran.
They get erased when the polymorphic function is specialized. That's important for any tight loop. It would be a catastrophic to carry around a `Num` dictionary in that scenario.
Then we agree. It's only "safe" given a particular implementation of Haskell. 
Can you elaborate about the violation part?
yes, we do.
&gt; you don't want too many keywords, as it makes it harder to skim/read once you learn the syntax. Tell me, when you write monadic code, do you use `do` notation or write everything out with `&gt;&gt;=`? Do you use layout, or curly brackets and semicolons? Haskell already recognises the utility of multiple syntaxes for the same thing, and already recognises that there can be more and less helpful notations. Doesn't have to be one or the other. *edit:* it's ` not ‘
Indeed. &gt; It seems to me that 2 and 3 are actually the same issue NB I didn't write those "problems", Michael Snoyman did in the blog post that /u/edsko linked.
"rich hypertext documents" could be quite misleading here, since the world wide web was only invented in 1989, and interactive, distributed, multi-user, internet-connected computing was only in widespread usage in academia until the mid 90s. Online business took even longer to get going than home hobby usage, so the current market forces in software development just didn't exist last century. Your comment makes it sound like today's online environment has existed since the 60s, whereas the 1960s' online environment is completely and utterly different from today's. It's important to get the facts right, but it's also important to get the big picture right. /u/agocorona has the big picture pretty accurate. 
That actually looks really nice and explicit! Great job.
I agree, mostly. I'm not advocating _every_ exception becomes a checked exception. I'm merely proposing a way to make _some_ exceptions checked. 
Berners-Lee says himself that HTTP and HTML embody ideas which long predate them, so the late appearance of them tells us little. What the world-wide web mainly did was to release hypertext from the proprietary world. Meanwhile, this &gt; In the 50s and 60s, before the OS could interleave IO and processing in console applications, people solved the problems that they could solve with the tools they had, basically sequential processing of registers with COBOL/RPG and scientific calculations with FORTRAN. seems to suggest that before the 70s there was only sequential off-live batch processing, which is plainly untrue.
&gt;Many people claim that learning Haskell is like learning to program from scratch; although these people are wrong that's one hell of a bold and subjective statement you are making there. Also you are wrong. It is reminiscent of learning to program from scratch. I've come to Haskell from C++ and C#, both statically typed languages (if a lot worse than Haskell) and it took me a good year or such to become productive in Haskell, simply because while it was obvious how to make small functions, large scale design in Haskell just works completely different and takes a while and a project or two to adapt to.
Wow! That's indeed amazing! I shall keep this in mind
then you could always just have a HOF. where you got 90% of the same code and then pass in the appropirate small function for that exact instrument. computeValueWith :: (Instrument -&gt; Value) -&gt; Instrument -&gt; Value computeValue Bond = computeValueWith bondFunc computeValue Stock = computeValueWith stockFunc and so on and so forth. I got to agree with materialdesigner though - if you got partial functions (and your proposes computeBondValue and computeStockValue most definitly are partial functions, as would my stockFunc and bondFunc be here) you might want to look at your model, ideally there shouldn't be many if any partial functions in the code.
Indeed, I wrote up about using `Writer Endo` for little DSLs here: https://ocharles.org.uk/blog/posts/2013-02-12-quick-dsls-with-endo-writers.html /u/BRPOPLPUSH - this might help!
Yep, normally I would use Endo instead of [x], but the code in question was built to be a bit more accessible for Clojure devs who were new to Haskell.
I don’t plan on giving up, don’t worry. :) It’ll take the time it’ll take, but I’ll eventually release it at some day! :)
Thanks! This worked perfectly.
I like extensibility a lot (e.g. emacs). and sometimes you have to provide unsafe access to internals to allow the level of extensibility you want. instead of the author proving things manually, exposing internals means that when a library user (a) ignores the internals (most will), it's the same and (b) when they use the internals, they must manually prove something, so they have to do the work the library author does. that's all. as long as your API hides the internals, and most uses are uses of the API, it's about the same. I haven't looked at unsafeSqlFunction, but it might cause a runtime error or a segfault, or even worse, nothing immediately noticeable. or one could argue that it violates some properties that the module guarantees. but that's okay! that's part of the power. I was just in a mood, having read about encapsulation recently and being told to encapsulate more in a code review. you should encapsulate, but you should also provide the internals (somehow) too. a more constrictive comment on my part might have been (I've never used the library btw, I just liked the blog post): """esqueleto has a good API, and exposes internals when the API isn't good enough; it shows why people should expose internals in a ".Internal" module, or at least provide "unsafe" functions, while still working as hard as they can on the API itself""" or else, "Edward Kmett will fork your package"; they had a great talk about encapsulation.
how do you mean? Do-notation is still non-alphabetic, besides the "do", it's just "&lt;-". Now, if Haskell had extensible mixfix syntax, that would be cool. But in this case, I don't think I would use: do bind readFile to contents let reversed be reverse contents return reversed
well, if Haskell had true macros, that would be cool, as we could write these ourselves :-) I guess we'll have to agree to disagree on the keyword issue.
Thanks for the update!
Do you have any results to share about the built in random vs tf-random?
"do you have no taste" "pay me to open source"* "stop asking stupid questions" roflmfao *sure, open source devs should get paid, but you have to get funding and set it up 
Wouldn't it be much better to create your programs as an AST within a single Haskell program? Every individual has to be evaluated once, I don't think the compile times are worth it. Furthermore I think this will be painfully slow for big populations. But maybe I missed your idea.
You're correct, each individual has to be evaluated once for fitness plus once for every offspring they're asked to generate. That's indeed not quite worth the compile times, but then again, I can't quite figure out how to pull it off in an interpreting fashion. I have looked into Plugin without much success. It isn't super fast, but the speed, well you can currently work with it. I'd say 10k genomes in an hour or so on an i7-860, if I remember correctly? Give or take a factor of two. And regarding ASTs: That too is a method I had considered, but I haven't yet had invested enough research into that, as just spewing out random words was easier to code and sophisticated enough to test the framework. If you can point me to relevant libraries, I'd be glad to try and make it work that way.
Both the paper about SplitMix and the paper about TestU1 are hidden behind an expensive paywall. Is there any information about them that is publicly available in practice? In particular, what kinds of tests were done for independence of split generators? I would expect to see significant test results at least for a single split, many splits to the right, many splits to the left, and some kind of random sampling of splitting graph shapes. Also, what is the maximum size of the generated set while still preserving good pseudo-random properties, both for a single generator and in the presence of various splitting graphs? EDIT: To be clear - I am very excited about this work. It's about time! Thanks for doing it.
Exactly! Sorry.
Just for future reference, it's spelled yesod, with only one s.
well to be fair, with an appropriately generic approach to the data, there's no difference between "genetic algorithms" and "genetic programming", mainly because all code is data as well. I got a little toy example here https://github.com/Stratege/Coevolutionary-Neural-Network which doesn't care about the shape of the data it is working on or what the data itself is. The downside of that approach is that you have to implement generation, mutation, crossing and fitness on the datatype yourself.
Well, as stated above, a proof in concept would be to just achieve some self-optimization of the code generator. Use up less compilations in optimizing a problem with a population that has already been trained on an unrelated problem. Whether I can get there, I'm not sure, but I'll see it then. I'm hopeful, and I've got more resources at my disposal than that one old gaming rig. Regarding ASTs: Well, I kind of hope for someone to just present me with a library where a program can be put into a AST data structure iff it compiles. That would make writing the code generator much easier. The obvious complicating factor is that I'm having a code generator in here. Depending on the type of generator I use (I'm right now just spewing out words at random), I'll have a varying number of atoms in use. Which functions are good fits for atoms in my case, I don't know. I could probably go with that tree structure you proposed and expand it a lot. But then I have the problem that I (a) introduce another intermediate form of genome, the show () of that AST printed to a file, or (b) have to deal with parsing a .hs file into that syntax tree. ( (a) seems like the less unpleasant option. And about that evaluate function: I'm not sure that'll be all too trivial when working with haskell. Then again... Right now I'm limiting myself to a known predefined set of function anyways, might as well, right? Same pattern as your math function, just a bit more complex. And that would solve the above problem of having another intermediate form. And having to introduce plugin. Only remaining problem is, as you said, type safety. (Can you tell I only really understood your post when writing this? I should probably rewrite the whole thing, but I kinda like seeing my own thought process before me :D ) Ideally though, the genetic code gen will take care of type checking, at least heuristically. ~~The compiler~~ (oh wait, I won't have a compiler, I'll have to type check myself.) The interpreter will take care of the rest. Thanks a lot for your thoughts on this, they are very helpful.
I do :/ 
Well typed core is reassuring. But if this is turning into a regular trick I think some more general mechanism for dynamically created dictionaries should be considered. 
And the bug's title: "Haskell-mode was invaded by Eclipse users" (Well, at least the guy revealed his trolling from the start.)
Yeah, would really have appreciated it :p Any way we can flag it or let the author know? 
In my opinion, the most important result of work like this would be not the random generator itself but the test suite. If we had something like TestU01 but for splitting generators, we would be much wiser about tf-random, SplitMix, and whatever else we could then come up with.
haha I just love how that's the title. as software engineers, we must identify the root cause of problems. Obviously, we must delete Haskell mode from the file system if it detects an eclipse installation.
I think you can install it from [here](https://copr.fedoraproject.org/coprs/petersen/ghc-7.10.2/) like this: sudo dnf copr enable petersen/ghc-7.10.2 sudo dnf install ghc cabal-install 
It's really about correctness, here. From an implementors point of view, you don't even need GADTs for the first example, you can use phantom types - and even then, the real difference is in the type signature of `eval :: Expr a -&gt; a` vs `eval :: Expr -&gt; Evaled`. Remember, when you ask for something like `id :: a -&gt; a` there are really only so many actual definitions of this function that can exist. That is, parametric type variables are a form of abstraction/constraint, in the sense you must work with any `a` - so you can't touch it. What this basically means is that *it is much more likely your GADT version is correct*. That's the real benefit - because it has more constraints imposed on the implementation: for your second program, there are a lot more 'type checkable' definitions that would be an incorrect implementation, e.g. `eval _ = EvaledB $ B True` type checks but is totally wrong. You can't make such mistakes with a GADT or phantom-types approach. As a side note, a GADT like this: data Foo a where Frob :: Int -&gt; Foo Int Blob :: Bool -&gt; Foo Bool Translates roughly into a thing like this (you can actually write this too if you use `-XTypeFamilies`): data Foo a = (a ~ Int) =&gt; Frob a | (a ~ Bool) =&gt; Blob a So when you say something like `Frob 2` you're really carrying around a type equality constraint, saying that the type variable `a` is `Int`. So GADTs form a kind of witness for a type equality. But also GADTs are really about how you index/access the `a` too: pattern matching will automatically refine the types of some variables for those particular cases. For example, GHC refines the type variable in your first example to be `Int` or `Bool` depending on if you match the first two patterns.
Could you do something like this to get a tree with its shape in the type? data Tree s a where Leaf :: a -&gt; Tree () a Branch :: Tree l a -&gt; Tree r a -&gt; Tree (l,r) a zipTree :: Tree s a -&gt; Tree s b -&gt; Tree s (a,b) zipTree (Leaf a) (Leaf b) = Leaf (a,b) zipTree (Branch la ra) (Branch lb rb) = Branch (zipTree la lb) (zipTree ra rb)
Nitpick. 'Correctness' is an inappropriate term. I can write totally incorrect code with either. The term is best reserved for describing the language with respect to various formal properties. (For instance, Haskell's base typesystem guarantees you can't segfault). A better term, IMO, in this context is 'expressiveness', (although the term is harder to pin down). The distinction may seem unimportant, but [some language](http://reddit.com/r/rust) have made it a pasttime to cheer themselves on by using words like "safety" and "correctness" without much effort to be precise about what they mean. Haskellers know better though ;O
GADTs in general enforce compile time restraints that you would normally have to check at runtime. Think head :: [a] -&gt; a head [] = error rather than head :: List a nonEmpty -&gt; a -- guaranteed to have nonEmpty lists The another example is that GADTs give you static typing in Domain Specific Languages and ADTs only give you dynamic typing. What this means is that you can write DSLs with GADTs that can be verified at compile time not at run time. So your example doesn't really describe a very useful language in that equality only works on numbers but not bools and you don't have conditional jumps. data Expr = If Expr Expr Expr | Eq Expr Expr | I Int | B Bool If (I 3) (B True) (I 4) introduces a run time error as I 3 doesn't evaluate to a bool so your eval function needs to work like this data Value = I' Int | B' Bool eval :: Expr -&gt; Value eval If c t f = case (eval c) of (I' _) -&gt; error "bad type" (B' b) -&gt; if b then eval t else eval f with a GADT you can simply ignore that case data Expr a where If :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr a Eq :: Expr a -&gt; Expr a -&gt; Expr Bool B :: Bool -&gt; Expr Bool I :: Int -&gt; Expr Int eval (If c t f) = case (eval c) then eval t else eval f
Do you think there will be benefits to doing this in Haskell?
You mean [`NullaryTypeClasses`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/type-class-extensions.html#nullary-type-classes).
Great, thanks!
Very cool! Have you looked at [GPipe](https://wiki.haskell.org/GPipe)? I think especially the next version that is almost done on [github](https://github.com/tobbebex/GPipe-Core) share a lot of ideas with luminance (and there you'll find a type safe solution to the problem with vertex buffer formats ;) ). I love that there are more investigations being done in this domain, functional programming really *should* be the paradigm of choice for GPU programming!
As you are interested in learning about monad transformers in general, not just State: if I was going to use a monad transformer here, I would use MaybeT. playGame :: IO () playGame = maybeT (putStrLn "you win!") (const $ putStrLn "you're out of turns") $ replicateM_ 3 playGameTurn playGameTurn :: MaybeT IO () playGameTurn = do guess &lt;- liftIO $ do putStrLn "guess the number" read &lt;$&gt; getLine guard (guess /= 42) (Using `readMaybe` instead of `read` left as an exercise for the reader.)
There is a `Given`-like hazard associated with this. If you bring multiple handlers into scope for the same exception using this machinery, which one wins is rather poorly defined.
The SplitMix paper can be accessed here for free: http://dl.acm.org/authorize?N80773
It is not always possible to perform this decomposition into two independent types. Consider adding the following case to your GADT declaration: Pair : Expr a -&gt; Expr b -&gt; Expr (a, b) Fst : Expr (a, b) -&gt; Expr a Snd : Expr (a, b) -&gt; Expr b Now when you want to separate into two distinct datatypes, you have to either make pairs monomorphic (`NPair NExpr NExpr`) (strictly less expressive), have different structures for a "pair on which you use `Fst`" and a "pair on which you use `Snd`" (`NPairFst NExpr Expr`, `NPairSnd Expr NExpr`) (strictly less expressive, as you have to make a choice at value-construction choice about future usage), or insert a dynamic check (strictly less pure: dynamic failure is a side-effect). P.S.: the GADT declaration also lets you build expressions that return pairs (of pairs of pairs...), while to have this flexibility with the type-specialized versions you would have to define infinitely many datatype declarations.
I am using a [GADT](https://github.com/Peaker/AlgoWMutable/blob/a89e1564c350c282bd147e0b62b7e292a298b066/Type.hs#L137) in an implementation of type inference with row/column polymorphism (for record/variant types). It allows me to have a single AST type that is actually 3 different similar ASTs in a single type (One that builds types, one that builds records, one that builds variants). This is useful because it allows: * Constructor Sharing: Records/sums are actually the same AST but with a type-safe distinction. Type variables are the same GADT constructor in types, sums and variants. * A single fixpoint to parameterize. Use of mutually-recursive ASTs with open recursion is a pain if you have multiple fixpoints you need to parameterize. Using this GADT approach, I can change a single fixpoint. 
Much nicer. Thanks a lot.
The GADT is just more direct with less duplication and boilerplate (both of which are enemies of readable programs). Try adding an `if` expression to your types. In the GADT approach it's two lines, in the ADT decomposition it's four. 
I don't think so, the GADT approach will just be easier to write and read without all the duplication and boilerplate.
I can't test this at the moment, but it seems like guessing correctly will also result in a nothing. The only way to win is to fail to guess 3 times.
Oh right. You're using nothing as a win. Makes sense.
Could you give an example?
No, I´m serious. It is great since genetic programming is something far more interesting for me. I learned Haskell for doing some genetic programming. My idea was to assemble programs out of pieces like combinators in an EDSL or patterns in a OOP language. Haskell gives more guaranties with the strong type system and the composability. This narrow the space of valid programs and reduce the search.
What the transformers are commonly used for is unifying a lot of functions (both in libraries and applications) with a common 'interface' to make them compose better. For example, say a large number of functions in your application require access to some configuration options (`ReaderT YourConfigurationType`), a simple logging facility (`WriterT String`) and an error handling facility (`EitherT YourErrorType`).[1] Then it becomes much easier to define a custom transformer newtype YourAppT m a = YourAppT { fromYourAppT :: ReaderT YourConfigurationType (WriterT String (EitherT YourErrorType m)) a } [insert lots of instance declarations here] and have all functions in your application be of the form a -&gt; b -&gt; YourAppT m c which compose effortlessly using combinators from `Control.Monad`. Contrast this with a solution without transformers, which requires all your functions to be of the form YourConfigurationType -&gt; String -&gt; a -&gt; b -&gt; Either YourErrorType c where you have to manually pass the environment and log along while still using the monad combinators for the `Either` result. On the other hand, if you don't have a lot of functions with similar structure, the utility of transformers is very limited. You essentially end up writing the same amount of boilerplate or more because you constantly need to wrap and unwrap your functions using `StateT/runStateT` and friends. The only advantage of transformers in those situations is that they can make the code a bit more self-documenting. [1] Some discourage the use of `EitherT` in a transformer stack that includes `IO`; I'm undecided. So, take the example with a grain of salt.
Anthony Cowley's `Frames` is capable of taking data in separate column stores and twisting it around with vinyl to present an easier row-at-a-time view of the data. Those columns can be stored in nice compressed, flat manners. The thing the record is holding is 'how to get the real accessor to use repeatedly, rather than the data itself, and in a situation like that, when working concretely, discharging the constraint usually does the lookup of the particular retrieval method at compile time. Even when it doesn't it is typically moved out of the inner loop. It is really quite a clever technique.
A nice introduction, but it really doesn't cover anywhere near as much as a 300+ page book as claimed.
One minor comment: for phantom type parameters you could give them full names, such as: Format readOrWrite color depth Then you wouldn't need to explain what they are for
Same feeling here.
It's not ambiguous when specifically comparing to GADTs. GADT is not an ambiguous abbreviation, and the implied difference between the two is the G.
Nothing if not that! :)
The short answer is "Algebraic Data Types", but then I'm not sure I got the question. In my book, ADT in this context means "algebraic" (not "abstract" but I that is not pigworker's question), and it denotes a (1) inductive (recursive) datatype with (2) a sums-of-products structure and (3) type parameters. GADTs (I prefer "Guarded" to "General") add constructor-bound (4) existential quantification and (5) type equalities. If we had (4) but not (5) as in 1992 the Läufer-Odersky paper, we could call them ADTEs. Existentials being integral to many usages of GADTs, I'm not sure it would make sense to have (5) but not (4). There is a shady zone around the question of type parameters: is non-regular instantiation of recursive occurrences allowed for plain ADTs? OCaml says yes, I would guess SML says no, and I have no clue which options to enable to see Haskell's answer; and dependent languages say "then those are indices, not parameters".
In this approach there isn't really an *incoherence* issue, as such: the dictionary for a `Throws` constraint will always contain `throwIO` (assuming no manual `Throws` instances are given, and no other use of `unsafeCoerce`, of course). In principle, `Throws` could be an empty class, and `throwChecked` could be a top-level definition. Scoping of these exception handlers works exactly like `throwIO`/`catch` in `base`; the types just make it easier to spot if you've missed a `catch`. I think your point otherwise boils down to the caveat mentioned in the post: we don't get a cast-iron guarantee that `throwChecked` is used only in the scope of a `catchChecked`.
yeah! I've played around with d3 graphs, they're super easy and nice. as for terminal use, it's easy to end your script with "open file.html" (or the equivalent on Linux or windows), no?
Oh, I hadn't noticed that. I am sure you can edit the wiki to fix it.
No one has in the last 10 years: https://ghc.haskell.org/trac/haskell-prime/wiki/GADTs?action=history
Huh, isn't that just describing `GeneralizedAlgebraicDatatypes`?
I'm sorry if I'm reading something wrong, but that seems to just be a typo in the heading... right?
It's not a typo. "Abstract" is correctly spelt. It is a *thinko*. It is not, by a long chalk, the only instance of that thinko. Why d'you think it keeps happening?
Fair. I missed the notion that you are always producing the exact same instance. That would clear up any incoherence issues.
Have you checked out DigitalOcean?
I am looking at it now! I see "Full HTML5 enabled console access," but do they offer regular SSH...? And I see many distros, but nothing cutting-edge like Arch Linux. But they do look slick. If you've used them for Haskell backends before, I'd be interested...
Never used it sorry, I just came across it when I was looking for an alternative to AWS. I used an AWS EC2 server for data analysis and I was looking for an alternative to host a site that was cheaper than AWS. I'm still in the process or learning Haskell.
Explanation: http://english.stackexchange.com/questions/30939/is-used-in-anger-a-britishism-for-something 
You don't need a package manager for anything Haskell related on the server for the setup you described. Compile a static binary locally and scp it up to the server.
I'm very happy with Linode and their prices. The admin tools for the VPNs are really great too.
[Image](http://imgs.xkcd.com/comics/python.png) **Title:** Python **Title-text:** I wrote 20 short programs in Python yesterday. It was wonderful. Perl, I'm leaving you. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/353#Explanation) **Stats:** This comic has been referenced 157 times, representing 0.2097% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_ctpc686)
“Guys” is masculine. “People” or “folks” might be preferable in a technical article, because they’re gender-neutral. Note, however, that “you guys” (plural “you”) *is* neutral. 
I'm using http://crowncloud.net/ for VPS right now. No problems, no downtime so far (only 40 days, but still), and only $7/month, though they have options starting at $3.5/month. And you can pay with BTC. Box is debian 7 running a web server and mail server.
As others have said, as long as you can get a reasonable Linux distro (Debian or Ubuntu Server are safe bets), SSH access, and a decent price, you're good. All you need on the server is libc, libgmp, and C libs you depend on via FFI (which might not be any). Build on your dev machine, scp the binary over, add something watchdog-like to restart it when it crashes, done. Practically, this means you'll want to look into providers like DigitalOcean, Linode, AWS, etc.; also, lowendbox.com may have interesting options for you, but then, the entry-level tiers of the big VPS providers are hard to beat on bang-for-bucks, especially if your bandwidth requirements are low.
&gt; but do they offer regular SSH Yep. You essentially get a server to do with as you please. I haven't used them a lot, but when I have they've been very convenient - much easier to use than AWS. There's not really any Haskell-specific advantage or disadvantage.
My understanding of "used in anger" was that it means that somebody uses the tool despite what they perceive to be numerous flaws (i.e. they are angry at the tool, but they use it anyway). In other words, the tool must be solving some really compelling problem for them to still use it despite its flaws.
I really like the way that Haskell enables these kinds of things. When I made the realization that I could use IO values repeatedly to perform the same action, without having to call a function each time, it was pretty mind-blowing.
As an alternative to setting up a server, you could check out haskellonheroku.com. You can run a simple application on a free heroku account (I have a little tool running like that right now). That's probably the easiest thing to do.
I pay about $3 CDN a month for a VPS hosted in Toronto. If that's in your pricepoint, check out [Luna Node](https://www.lunanode.com).
Minor bug, in the last code chunk you changed the type of `main` from `IO Integer` to `IO String`, but you kept `return (2015 - read year)`, which should be returning an `Integer`.
You absolutely don't want to be building on the server. The reason is that GHC will require literally several gigs of memory to compile certain libraries, and renting that much server is going to be a big waste of money.
Try [vultr](http://www.vultr.com/?ref=6807703) instead (affiliate link). It is a proper VPS so you can install whatever OS you want, no weird forced kernel nonsense like DO. I run OpenBSD on mine and have a few haskell web apps. $5/month for the cheapest option is plenty enough to run a typical snap/happstack/yesod app.
Thanks! Fixed
So I actually only need to be able to build on my local machine!? This is fantastic news, and I feel silly for not thinking about it, haha.
Thank you; it never even occurred to me that a GHC-produced binary would just be portable to another machine like that. I'm glad to discover my worries were needless.
Thanks! I didn't even realize GHC could just produce a statically linked binary.
Yes!
Worth noting is that you can change your `sources.list` on Debian to testing or unstable and you'd have more recent packages.
I currently use budgetVM to host my blog.
Your point about the equalities in `Foo` is important. GADTs aren't, and can't be (?), injective; in other words, neither the programmer nor compiler may assume that a value of type `Foo Bool` must have the constructor `Blob`. It carries around the type equality, even at runtime, unless inlined away. To recover injectivity, you need to use type families directly, which is sometimes what you actually want and sometimes a huge PITA because you can't do value-level pattern matching to discover the types like you can with GADTs.
I'm not sure how familiar you are with binary compatibility issues, so I should give you a fair warning: you'll want to make sure you build it for the right operating system and architecture. i.e., your hosting service is probably going to provide a 64-bit linux VM, so you'll need to make sure to build a 64-bit Linux binary (you can probably build a 32-bit one too, but matching the architecture will make everything easier). There can also be compatibility issues between different versions of libc or distros, but you're less likely to run into those. If your local development machine is Mac or Windows, then you'll want to set up a Linux virtual machine on your development box to do the compilation. You can probably also use something like Travis-CI to make builds for you, but I'm not totally sure how that works (I use Travis for testing a lot, but I've never used it to produce build artifacts).
I see... that'll then be taken care of by the new nix-style package db feature...
It does so by default, even, at least on Linux.
I would thank you to refrain from attacking people for innocuous use of the English language simply because they don't conform to your politics. I see /r/haskell as a safe space for discussing programming and your derailment of the conversation makes me highly uncomfortable.
Just use CoreOS with an arch container.
&gt; women and other non-guy people
Um, if people are referring to you as guys, and everyone there is not your definition, that's kind of silly to think there's exclusion.
Could you give an example of this? Not sure I understand.
&gt;Some discourage the use of `EitherT` in a transformer stack that includes `IO`... Why is that?
Attacking? That's a stretch. The guy you're responding to is expressing concern over some questionable language. What is the harm in simply using alternate language?
I think a good way to make people feel welcome is to respond to concerns like this. You're assuming that this person does not personally feel excluded by this language, which I think is inappropriate. It's not very difficult to use gender neutral language, and while attacking people over not using the language is inappropriate, it does not seem like you're being attacked to me. Furthermore, a simple request like this is miles away from creating an environment where people are afraid to speak, in my opinion. 
Could we not just use gender neutral language? 
&gt; It's not very difficult to use gender neutral language It isn't, but sarcasm and whining are even easier.
I think he's referring to the realization that `IO a` is a value, and not something that performs side effects on its own. That means you can re-use an `IO a` value to perform the same effect multiple times, eg: myAction :: IO () myAction = putStrLn "hello" main = do myAction myAction 
Pretty sure static linking isn't default - GHC executables link to libc and possibly others.
I'll point out again that the stackage team first tried to work with the cabal team on cabal install and got rejected, and only then did the stack project get of the ground. Blaming the stack team for not working with the cabal team is unfair. 
Presumably the discussion has to happen on /r/Haskell somewhere, since it is a concern of /r/Haskell's. I see this as an appropriate place only because the issue was taken with this post in particular.
Interesting answer, but I'm still wondering if this kind of code is a good thing or not. This is highly compact, but you really need to be a trained haskell programer to write, understand and modify it.
Michael Snoyman is the lead developer on yesod and one of the developers and driving forces behind stack. Use stack for the least pain generally, but certainly with yesod, that's the most official advice for yesod. 
If you want something that just builds and deploys, use CircleCI. Free for open source projects.
Why should we?
Nobody says gals meaning everyone so people would be confused and think you meant literally just the women. Communication requires us to agree on the meaning of words. 
One relatively-well-known person from Haskell community is going to write a "haskell-friendly" DSL for shared-description code-generation, personally I have high hopes for it, so "bear with us" :)
&gt; They didn't have that functional stuff when I was programming in Java! Lambdas are a rather new feature, it got introduced with Java 8 if I remember correctly. I haven't really tried it yet, but from what I can tell from a distance it makes Java that much more bearable. Passing functions around without having to wrap everything into classes is something that I've been wishing for a lot when I was writing Java. That was even before I got into FP.
Also, there's /r/reflexfrp announcement done by its author https://www.reddit.com/r/reflexfrp/comments/3fd272/announce_reflex03/
No, you sound like you have an axe to grind because none of your posts under this question give any hint as to how the OP can solve their problem, but all 6 of them are fiercely critical of stack. Complaining about missing upper bounds can't help the OP, complaining about the newness of stack, the fracturing of build tools and the previous preeminence of cabal install doesn't help the OP. Advising them to use stack, the build tool that yesod's lead developer recommends and helped create, especially given that it just works, _is_ helpful to the OP. Cabal install will be a continuing source of pain for the OP if they don't use stack. New users have enough on their plate learning haskell. They shouldn't have to learn cabal at the same time; it should just work. Stack just works. Advising folks to use stack is helpful to them. The people who you need to convince that they should use cabal install are package maintainers, not new users. I personally would recommend that package maintainers test their packages under both, so that the experience of the maximum number of people is good. If cabal install becomes easiest, fastest, with the best choices for new users by default, and loses its ability to lead unsuspecting folk into cabal hell, it will have deserved its place as the primary build tool, but at the moment, the best choices for how to do things with cabal are not obvious, not simple and scattered across the Haskell community's web presence, and the tool makes no attempt to warn users they might be making a stupid mistake by just typing something as reckless as, you know, `cabal install something` every time they want to install something. There's some trivial changes to the defaults that the cabal install folks could do to ease some of this pain, but they've argued against that. Wouldn't it be great if cabal install's dependency solver could reject a build plan instead of preceding with it and then failing after 20 minutes of downloading and installing? I would honestly prefer it if it stopped at the start and said "not with this setup matey, you'd better nuke me and start again". It can't even undo its mistakes or suggest a workaround. Microsoft supporters would have been wrong to complain about the rise of OS X when the standard advice for troubleshooting windows was "reboot, and if that fails, reinstall windows". When other operating systems are preventing programs from crashing each other or crashing the entire system, and yours isn't, it's time to change yours. Microsoft was changed by the competition, and Windows is now much more pleasant to use. Cabal install is beginning to show signs of being changed by the competition (despite the cabal devs' objection to many of stack's choices), and hopefully we'll have a much improved cabal install in the future. That'll be a good thing. I'm no Thatcherite, but I can see that sometimes competition is a good thing, not a bad thing. 
I'm a big fan of the VPS services from [hetzner.de](http://hetzner.de). They are cheap and their power is certified CO2 neutral. :)
This is because the design of glibc makes it rather difficult to link it statically. GHC generally links statically against everything except libc*. (*) One notable exception that may cause you grief is `text-icu`. If it creeps into the dependencies, it will bring a particular version of a shared C++ lib, difficult to satisfy when you move a binary to a different distro.
Yesterday, this link redirected to different page, where the PDF was instantly downloadable by simply clicking "Get the PDF". I read it there myself, whithout paying or even registering.
Because in some cultures there is no alternative. If I said "folks" people would look at me weird. "guys" is gender neutral in my location, and I can't think of an alternative that wouldn't sound strange to say and hear. 
In addition to all the answers here, check out fpco's post about running [Haskell in minimal containers](https://www.fpcomplete.com/blog/2015/05/haskell-web-server-in-5mb). Minimal = 5Mb. There were also other posts on that and AFAIK some issues were already fixed for ghc7.10.2 (sorry, don't remember specifics).
Is the sample application (or any sample application, really) hosted anywhere, so we can play with it before installing the library? This looks cool though, happy to see FRP-UI becoming a thing over the past couple of years.
People, those, everyone, you, us, programmers, individuals would all work in the given context, and all are completely gender neutral in any culture. You don't have to look very far. :)
Hey, I'm sorry that my comment made you uncomfortable. It's definitely not my intent to police people's language and I'm concerned that my participation in the discussion caused you to feel that way. In fact, I just want for the space to be welcoming for all people regardless of gender. When I hear someone use the word "guys" in a generic way, my intent for correcting is not "You are a bad person and should be silenced!" but rather "Hey, this word you're using might have some negative side effects that you're unaware of for some people." I am making an assumption: that the speaker also shares my goal of "a welcoming space regardless of gender." It might be that my assumption is wrong, and the person doesn't care, in which case they can just get on with their life and we can agree to disagree on that front. I hope that my clarification helps you understand my intent more, and I additionally hope that it makes you feel less attacked. People feeling welcome in the community is important to me, and if there's a way I can improve my communication to serve that goal, I'm glad to hear suggestions.
OTOH, I've heard from a couple of female coders that increased amounts of walking on egg-shells now prevent them from "being one of the guys". That is, the notion that they could be offended by something and resulting over-caution leads to the exclusion of people who were perfectly integrated before, at least among people who knew them and thus knew them to be not girlfriends (such are the trappings of statistics, ask male midwives about it). My personal favourite, btw, would be "gals" and "lads" for gender-specific and "guys" for neutral.
Ryan's [slide deck](https://obsidian.systems/reflex-nyhug/) is itself a reflex app. Around slide 10 you can see some examples. I have a couple silly examples [here](http://web.mit.edu/greghale/Public/my-reflex-recipes/) - some of them depend on features not yet in master.
 exceptionElemAt :: [a] -&gt; Int -&gt; a exceptionElemAt _ n | n &lt; 0 = throw NegativeIndex exceptionElemAt [] _ = throw IndexTooLarge exceptionElemAt (x:_) 0 = x exceptionElemAt (_:xs) n = exceptionElemAt xs (n - 1) This is no more useful than using `error`. The error is scarcely more specific, it doesn't help you locate the problem (anyone who ever hunted a `fromJust: Nothing` can sympathize), and you can't catch exceptions in pure code. It's far better to make partial functions instances of `MonadThrow`, so that you can write something like this: safeHead :: MonadThrow m =&gt; [a] -&gt; m a safeHead [] = throwM EmptyListException safeHead (x:_) = return x myFunc xs = ... where headVal = fromMaybe (error "myFunc: headVal called with empty list") $ safeHead xs
Actually, I think this is an important and pertinent point. I am hoping that this work will become the new standard implementation of System.Random. Since this is an open library, I would - unfortunately - be forced to oppose that strenuously if a full description of the work is not freely and publicly available. EDIT: That said, I agree that this is not what your students should be spending their time on. I am hoping that the appropriate person will simply point out where that information is available for everyone. EDIT2: And of course it doesn't need to be the actual papers published by ACM. Often in these cases draft versions are made available. Or other material which fully describes the work in a way that satisfies the needs of library users is made available online in a different format.
To be considerate, welcoming, and friendly. We don't have to be divisive and stubborn.
I'm not sure; but I think `coerce` is generally speaking about newtypes, not phantom types.
If you think of it as people jumping into victim mode,f compared to people feeling genuinely left out, then it seems you are making that assumption I was talking about. 
A function like `() -&gt; a` isn't that nice but it's a first class value and isn't that much of a hack (assuming impurity of course).
It is definitely way out of date in many ways and should have been replaced long ago. But is it really so fundamentally broken that no one should use it, ever, for anything? The algorithm is early published work of L'Écuyer, whose work you are relying on for this project. Although no one has ever been able to say anything useful about splitting properties, everything else has been tested quite thoroughly using an earlier version of the very test suite you are using. For lightweight use at least, why not continue using it in the meantime?
Being a nazi over an imagined slight perceived by few and meant by none contained in common language the majority uses is itself decisive and unfriendly. 
Oh, sure. They link dynamically to libc and libgmp, as well as all C libraries the code depends on, but all the Haskell libraries are linked statically. You can link C libraries statically, even libc, but the latter is not usually a good idea due to how libc depends on OS internals. And AFAIK you can link Haskell libraries dynamically, but this isn't very useful usually, because you'll end up shipping purpose-built dynamic libs for your project anyway, might as well link them into the binary.
The current implementation allows you to include anything in the `commit` field that `git rev-parse` will accept, but I'd recommend sticking to a SHA1 hash. This functionality of stack is meant to work similarly to git submodules (precise tracking of the remote code to check out), but with less hassle (no need to check out a copy of the remote code in your working tree). Including anything other than the SHA1 hash hinders reproducibility, because then your `stack.yaml` isn't specifying precisely what version of upstream you want to be building against.
&gt; cutting-edge like Arch Linux. What is 'cutting-edge' about Arch Linux?
[This](https://ghc.haskell.org/trac/ghc/wiki/Roles#Phantomparameters) paragraph of the roles documentation seems to suggest that the machinery expands to phantom parameters.
Only one can say that their request is a *reductio* of the other's, though.
Yes, I think we're all agreed on the value of being considerate. The question before us is whether our consideration should lead to us endorsing PC language policing, or not endorsing it. Don't take that for granted: that's the whole question.
I applied! I am a new Haskeller but I am in Berlin and I did some work with 3D, http://www.orouiller.net/projects/ray-hs
Well, you can of course "presume" on your own behalf. But you're not offering any reason to anyone else to agree when you just say "presumably." I can give you a reason not to presume in this case: the social endorsement of the idea that this language is exclusive may lead to people to be hypersensitive to meaningless language choices, and feel *more* exclusion, as a result of other language or as a result of other people using the same language. That could also lead to people making more demands, creating a spiral of hostility and distrust (as I mentioned). What I said in the previous paragraph isn't a result of a priori theorizing. I didn't come up with it like that, and there's no way that I could have. Instead, it's based on observation of actual people, of actual social groups, of what has factually happened in the past when people with your ideas are listened to. But again, I don't claim that that's *proof*. Giving me actual evidence can change my mind. Telling me your presumptions won't.
what attack?
but how did you find so many words??? :)
&gt; What is the alternative to adjusting our language for the sake of being considerate? There are multiple possible concepts of what is "considerate," not all of which revolve around word choice. Actually, I think we should get away from the word "considerate," as really that's not what we're talking about. Right? Being inconsiderate means doing something unthinkingly, without taking others into account. So once we start talking about the question of what should be expected socially, what is a reasonable or unreasonable social demand, we're already beyond being inconsiderate (we've already entered the phase of "considering," to be concluded by a phase of "deciding.") The general question is, what is an acceptable social demand? What exactly is a person entitled to ask for, when it comes to being respected and included? In other words: assuming we're trying to be respectful and inclusive of someone, what requests of theirs should we grant, and what requests should we not? Or put another way: at what point has a person overstepped the boundaries of social welcome? At what point is a person asking for too much? This is the proper framing of the question, don't you agree? &gt; From what I've heard it sounds like "forcing the people who take offense to toughen up" which is quite clearly a hostile move. People who take offense will always be "forced to toughen up." That is inevitable. The only active question is which people, which offenses, under which circumstances, etc.. Undoubtedly some people in this thread will perceive your own comments as casting aspersions -- implying they are inconsiderate, saying that they are making "poorly veiled claims" that others' "opinions and emotions are worth less," implying that they are being "hostile." Those are things that you said, and they aren't things that you can realistically claim would not cause any offense. But you don't believe that you ought to be silenced in order to prevent "taking offense" by others, yes? That's how society has got to work, no matter which choices we make about what particular things are acceptable socially. Someone will always "take offense" -- if we forbid one means of giving offense, the very act of forbidding will itself give offense, though to different people.
I'm not offering an argument. I'm explaining why I personally don't presume what you say that you presume. And again I'm just asking you for evidence, for an actual reason to believe what you claim, namely this: &gt; The thing is that you haven't established that strict PC language policing does, in fact, "broaden the minority." Why should anyone believe that it does? NB. I don't mean, by repeating this, to *demand* that you answer the question. I just emphasize that that's what I was asking for. Not "hyper logical hyper scientific debate," but an answer to "why should it be believed?"
Well, because people claim that they feel excluded by this language. It seems absolutely true that reducing their feeling of exclusion will promote their desire to participate.
It seems that we are caught in a loop here. I don't like to repeat myself, but I guess I can be more clear. You're telling me what "seems absolutely true" to you, but I'm asking you for actual empirical evidence. (By the way, your a priori argument does not seem unreasonable to me, as an a priori argument. If I had no social experience, I'd probably be convinced, at least contingently. It's where theory meets social reality that I see it break down.)
Suppose someone has taken a course in discrete math and read "Cakes, Custards, and Category Theory". What are some next steps towards learning category theory with an aim for using it in programming? 
The error function would be more useful if it printed a line number and backtrace. AFAIK the technology exists in ghc 7.10, but I don't know how to actually make it happen.
1. Yes 2. Not sure, but I highly recommend that you treat it as required. This gives you reproducible builds that are not a function of time. 3. Not sure. I recommend filing a bug if it doesn't Just Work and documentation doesn't cover it.
How would you implement a pairwise force calculation in Haskell? I'm a physicist by trade (with a playful interest in FP), and conventionally an n-body force loop would look something like forces = 0 for i = 0, n - 2 for j = i + 1, n - 1 fij = force(body[i], body[j]) forces[i] += fij forces[j] -= fij but I have yet to figure out an elegant way that captures the very last line (Newton's third law) without in-place updates. Really important consideration if your force calculation is particularly expensive, though. Thoughts?
Nothing useful in Haskell. You flip the arrows (This isn't a technical term, you literally just turn `a -&gt; b` into `b -&gt; a`) in the signature for a monoid to figure out what it's operations are Monoid: unit : () -&gt; m -- Normally just written m since () -&gt; X and X are equivalent mult : m * m -&gt; m -- Normally written curried, but we want this Comonoid counit : m -&gt; () comult : m -&gt; m * m So `counit` is trivial since there's only one occupant of `()`, it's always equivalent to `const ()`. We also have some laws. Before with monoids we had - `mult` was associative - `unit ()` was a unit for `mult` on the left and right. What this means is that when we `comult` an `x` and apply `counit` to either component, what remains must be isomorphic to `x`, the thing we split. This ought only to be true if the non-`()` component must be the original argument. This makes everything all possible lawful instances comult x = (x, x) counit x = () I've heard whispers this is more interesting in a substructural type system, but this well beyond what's possible with Haskell :( PS The way I derived the laws of a comonoid was to draw the commutative diagrams we expect to hold with a monoid, flip all the arrows and read off the resulting equations. Not sure how best to show this in pure text though, sorry!
If I was writing library for an evolutionary neural network that has a user supplied function (in this case the function that provides feedback for the neural network) buried deep down inside the algorithm that may or may not be IO depending on the context, would an acceptable solution be to duplicate the whole algorithm for pure and IO or is there a better way to do it? Also is there a policy on cross-posting unanswered stack overflow questions here?
Can I call Haskell functions from OCaml? In a sane way (e.g. I don't have to redefine all my datatypes)? ~~Also, if I can ask *anything*, why does OCaml lack polymophic + (i.e. there this a +. :: float -&gt; float -&gt; float). Isn't ad hoc polymorphism **the** point of OOP?~~ [SO](https://stackoverflow.com/questions/8017172/why-is-ocamls-not-polymorphic)
I know how to profile Haskell code, thanks to the chapter in Real World Haskell and similar tutorials. But how do I learn to make the problem areas fast? Haskell optimization is a confusing black box to me.
&gt; $ stack init --resolver lts-2.19 &gt; cabal init creates a stack.yaml file based on an existing cabal file in the current directory. The --resolver option can be used to pick a specific snapshot. Do you mean "stack init" there?
Fun fact: *man* was explicitly gender-neutral in English all the way through the first millennium C.E.
Time varying values, such as force and velocity, are usually very well represented through FRP. Look into netwire or reactive-banana. 
no ones mentioned Vultr, which allows you to upload an iso if you need a OS that isn't officially supported and they have a cheap 756MB option.
I'm interested in using Haskell for security research and tools. Is there any prior art in this space?
This is a remarkably limited point of view. First, I'd like to remind you that just because people elsewhere in the world are struggling does not mean your struggling is invalid. Second, I'd like to point out that your only understanding of these people is as "a persnickity desire to control others choice of words," which is both unprovable and quite clearly a hostile attitude. I'd advise you that when faced with an overwhelming number of people claiming "this upsets me" not to immediately jump to the conclusion "theyre lying! theyre trying to control me!" In actuality, I would bet most of the people here who argue against me would not wish to associate with you at all, because youre creating an incredibly abusive image of yourself. I really cannot express cleanly how ridiculous and offensive your attitude is.
Fun fact: ask any linguist (or use your own reasoning), words are defined by how we used them, not how they were conceived.
I'm on a lookout for anything involving pure representations of "business logic," especially in any kind of real-world context. Feel free to interpret that liberally. SPJ's work on contracts is interesting, almost too interesting; it'd also be nice to see stuff that's more related to typical non-sexy business applications, perhaps involving users, permissions, databases, etc. Also, I'm always interested in links to examples of particularly understandable and clear Haskell code.
Then we agree: "guys" is gender neutral because I use it that way. :)
Can you elaborate on this please?
Sure, I understand that. The author of the post probably comes from one of those places, but once they had been enlightened that that is not the case everywhere they changed the article to one of the truly neutral alternatives and everyone is happy. :)
I may or may not be a technical writer. ;)
Is there a strong reason to use the sha1 over a tag, since both are immutable references?
On the other hand, there are women that *don't want* to be "one of the guys" – they want to be themselves and accepted for that. When I was regularly in a social situation where males were in a pretty clear minority, I didn't want to be "one of the gals". I understand if some people would have, but I was not one of them. I was highly appreciative of the people who didn't address the group with me in it as "ladies".
I've implemented parI/parE for iteratees about 3-4 years ago. http://hackage.haskell.org/package/iteratee-0.8.9.6/docs/Data-Iteratee-Parallel.html
I guess you're not talking about crypto, but in case you're interested, Cryptol is implemented in Haskell: http://www.cryptol.net/
If your two functions have types f :: a -&gt; Maybe b and g :: b -&gt; Maybe c, the fact that Maybe is a monad lets you "compose" them even though the sources and targets don't exactly agree. See https://www.youtube.com/watch?v=ZhuHCtR3xq8
Short answer: no. There is no general way to check if a given value is a valid parameter to a function, under the assumption that the types match. As a side note function `f` would return `infinity` iff `x = 0`. Hence you can use ordinary function composition `f . g`. To explain the `Maybe` monad, consider the following example. We have the functions `safeHead`, `safeTail` defined as safeHead :: [a] -&gt; Maybe a safeHead [] = Nothing -- undefined for empty lists safeHead x:_ = Just x safeTail :: [a] -&gt; Maybe [a] safeTail [] = Nothing -- undefined for empty lists safeTail _:xs = Just xs Both return `Nothing` for undefined inputs, i.e. the empty list. If we want to get the second value of any list using only `safeHead` and `safeTail` we could do so with: secondElem :: [a] -&gt; Maybe a secondElem xs = do t &lt;- safeTail xs -- cut off the first element safeHead t -- return the first element of tail So what the `Maybe` monad does is, it lets you compose functions that may not return values. If any of the function in the composition chain returns `Nothing` the composed function will also return `Nothing`. In other words the `Maybe` monad takes care of all the wrapping and unwrapping and checks if any function produces `Nothing` before handing it to the next function.
Is there such a thing as junior haskell developer or do I need to be extremely proficient in haskell? It seems like there are few jobs for haskell coders but I would also image that there is a fair amount of competition for these jobs.
Which data structures are cheap to modify while retaining referential transparency? I find myself reallocating a lot of things when modifying in the middle of a bytestring or updating values in a vector.
I have a Lazy Map that pulls from a list that is of the form: problemList = [ (1, Answer 1 $ show P001.main) , (2, Answer 2 $ show P002.main) , (3, Answer 3 $ show P003.main) , (4, Answer 4 $ show P004.main) , (5, Answer 5 $ show P005.main) , (6, Answer 6 $ show P006.main) ] Where the `main` functions comprise pure output.I have a few modules where I am doing file IO, which means I cannot keep all my Answers in this map. I also have a CLI component which will run one of the above mentioned problems and format its output: runProblems :: Instruction -&gt; IO () runProblems (Instruction ps a) | a == True = do showAnswers $ map lookupAnswer [1..21] | otherwise = do showAnswers $ map lookupAnswer ps Does anyone have an idea about how I can concisely isolate my solutions of type IO from my non IO solutions? I apologize for the vagueness here.
You replied to me in multiple places and I think [my other reply to you](https://www.reddit.com/r/haskell/comments/3fk2b3/io_is_your_command_pattern/ctqhzs9) covers this too. 
Thanks. I played with nix a while back. My goal is binaries that run on a variety of systems which may have old kernel, old glibc and old CPU. The problem I hit is nix helpfully builds a new glibc, so even if you can get nix up on an old system, the binaries it produces can be incompatible with that same system without the nix-built glibc and every library it also builds against that glibc. I discovered there's a native-libc mode nix uses on osx and freebsd. I was able to setup a native-libc linux stdenv and after a day, I was able to get some basic things building. However, there was too much breakage in the formulas that I never achieved ghc. I tried a different route using pkgsrc and this way I was able to get ghc going but only by first building gcc49. I don't think I've understood the full consequences of this, but I suspect it's behind some failures I've had when trying to send FFI-using shared objects to other machines. Or maybe not. There's a lot to learn about linux at this low level of linkers and loaders and toolchains.
To be fair it *is* factually incorrect to claim that "no one cares" because at least four people care. You might be saying that we haven't established that a statistically significant proportion of readers care, but that's kind of the thing with dealing with minority opinions... they're in the minority.
You mean you want to be able to use dlopen() too? That makes this much harder. Then your only real option is to build ghc on the oldest possible glibc+kernel combination you need to run on to work around versioning in glibc causing issues. You'll never get dlopen() to work across libc's and I wouldn't even try to link two libc's objects together.
its absolutely impossible for the author to manage the interpretations of all their readers. And yet, there are consequences to the misinterpretations. I havent got a good solution.
Yeah. That's kind of what I'm doing. I want to join the rest of you in the future, but these legacy machines are never going away.
+10 that. Bartosz's series is excellent and fun as well. 
I agree it's difficult. [But it's also really interesting from an academic perspective!](https://en.wikipedia.org/wiki/Authorial_intent)
But we're not even talking about whether it's true or not. We're talking about whether it's asking too much to ask someone not to say it at all.
Looking at it, I've certainly heard new criticism before, I meant more from the authors perspective, relevant to the discussion in this thread, if they should write with all possible interpretations in mind, and try to restrict them only to the ones intended, which appears to be an impossible task.
Here's the thing, of course I don't want that, and I bring it back to the bit about slippery slope. No, you mess up, you say the wrong thing, you're not a bad person. But if you refuse to change for the good of others, of course you are, you're being an intentional asshole. The item motivating resistance to these requests is fear of being forced to comply. That's not what I advocate, I advocate being considerate, and complying willingly. I am in opposition to those who understand they are hurting others, and continue to do so for the above reason. I have no qualms about fighting against people behaving as assholes. You're arguing against criticism of this persons kanguage because you think a world where everyone's liable to these criticisms is the same as a world where people who don't already know better are attacked. These are different worlds, and the first does not guarantee the second. 
&gt; Here's the thing, of course I don't want that, I don't mean to imply that you do want that. (Quite the opposite, I'm assuming you want to avoid it.) &gt; and I bring it back to the bit about slippery slope. Just saying "slippery slope" isn't showing that it doesn't happen that way. In fact it has happened that way. That doesn't mean it always has to, but at least it has; I know because I've seen it. &gt; No, you mess up, you say the wrong thing, you're not a bad person. But if you refuse to change for the good of others, of course you are, you're being an intentional asshole. &gt; The item motivating resistance to these requests is fear of being forced to comply. That's not what I advocate, I advocate being considerate, and complying willingly. I am in opposition to those who understand they are hurting others, and continue to do so for the above reason. I have no qualms about fighting against people behaving as assholes. I hope you can see that you're actually reproducing (in a mild form, to be sure) the mentality that I described in my previous post. You're doing it right here! You're providing confirmation of what I said, even as you argue against it. Let me explain. When you say this: &gt; No, you mess up, you say the wrong thing, you're not a bad person. But if you refuse to change for the good of others, of course you are, you're being an intentional asshole. ...you're implicitly relying on the assumption that "refusing to change" is "being an intentional asshole" because the asshole who refuses to change is refusing to do something "for the good of others." But this is not the only possibility! In fact, this is not even the most likely possibility. It is also possible (and likely) that the person who "refuses" to change how they speak *does not agree* that such a change is "for the good of others." That person has not been convinced that they are, in fact, acting against the good of others. They could be completely wrong, of course. But still, they don't, in their honest opinion, believe that that is correct. And yet that makes them, not just an asshole, but an "*intentional* asshole," and someone whom you "have no qualms about fighting against" because they "understand they are hurting others." All of this is revealed because of a word choice that is realistically much more likely to be a product of *not agreeing* that such a word choice is harmful. This kind of disagreement is not even admitted as a possibility -- no, either you agree, or else you're the intentional asshole who doesn't care about hurting others. (I should at this point acknowledge that in following that train of thought, I've definitely gone from the empirical description of what I've seen to the more theoretic attempt to explain what I've seen.) Can you see how that's the same mentality I was talking about? The same mentality that was described in that quote? Don't get me wrong, I don't mean to say that you're the worst example or that you're calling anybody or viewing anybody as a Nazi. You're not taking the explosion of implications anywhere near as far as the worst examples I could think of. But the structure is the same. You reject the possibility of any disagreement that is non-hostile. You reject the possibility of any disagreement from someone who is not intentionally and knowingly choosing to hurt others. You see what I mean?
SCC has contained similar functionality based on [pipeP](http://hackage.haskell.org/package/scc-0.8.2.2/docs/Control-Concurrent-SCC-Streams.html#v:pipeP) for about six years, but I never got it to perform faster than the sequential version. Congratulations! I'm not sure if it is possible, but consider generalizing the type to pipelineC :: MonadParallel m =&gt; Int -&gt; Consumer o m r -&gt; Consumer o m r 
I probably should've been more clear, but I'm absolutely aware that that is not the only reason to deny doing something like this. There is a problem with your position I believe but I'm having such a hard time articulating it that I've given up for now. There is something circular about refusing to change out of the fear of being treated poorly for refusing to change. But I haven't fully understood this yet, so unless you can figure out what I'm getting at, I guess we can just ignore this,
ghcjs has been under massive development actually! and i've heard of increasing amounts of people using it in production. Reflex, for example, which is "frp for the browser" (and of which there's an announcement on this reddit for a new release) is designed to be compiled via ghcjs. The new ghc release includes features which should make it more "out of the box" compatible with ghcjs to make the install process easier as well. At this point, I suspect that some of the effort on other fronts such as fay has petered out because the momentum is now really behind the full-featured ghcjs approach. If I was in a position where I really wanted a haskell-for-js solution, I would be ready to commit to ghcjs.
Im no expert at all, but I have been having some success while reading "Parallel and Concurrent Programming in Haskell" using ThreadScope. I also am using [Criterion](http://www.serpentine.com/blog/2009/09/29/criterion-a-new-benchmarking-library-for-haskell/) for general benchmarking.
The assumption you're making is that they have any choice on how to interpret it, and further, they're 'choosing' that interpretation for potentially bad reasons. I should add you're also assuming I'm lying to you right now, which is utter nonsense.
&gt;that is not assumption, that is reality I have given you many reasons why this is not the case, and y have totally failed to rebut them. Lying is an action of intentionally sharing a falsehood. Saying something that is false is not lying. I'm only calling you out for assumptions when you assume, which is clearly quite often. I'll give you the second one though.
[Frames](http://acowley.github.io/Frames/) comes to mind.
How can a person who only knows one way to interpret something make a choice about how to interpret it? I already made this point.
... You don't believe people exist who don't know the interpretation you're using here?
parI/psequence_ helped to reduce processing time in one program by factor of 4...
I'm probably not following, but how different is this problem from one typical for `pipes-concurrency`? import Control.Monad import Control.Concurrent.Async import Control.Concurrent hiding (yield) import Pipes.Concurrent import qualified Pipes.Prelude as P import Pipes main = do (output, input, seal) &lt;- spawn' $ bounded 5 b &lt;- async $ do runEffect $ pretend_parser &gt;-&gt; P.take 10 &gt;-&gt; toOutput output atomically seal a &lt;- async $ do runEffect $ fromInput input &gt;-&gt; slow_worker atomically seal waitBoth a b putStrLn "All done!" pretend_parser = for (each [1..]) $ \n -&gt; do lift $ threadDelay 20000 lift $ putStrLn $ "sending " ++ show n ++ " to queue" yield n slow_worker = forever $ do n &lt;- await lift $ do putStrLn $ show n ++ " removed from queue" threadDelay 50000 putStrLn $ "*** work done on " ++ show n ++ " ***" -- $ ./conc -- sending 1 to queue -- 1 removed from queue -- sending 2 to queue -- *** work done on 1 *** -- sending 3 to queue -- 2 removed from queue -- sending 4 to queue -- *** work done on 2 *** -- sending 5 to queue -- 3 removed from queue -- sending 6 to queue -- sending 7 to queue -- *** work done on 3 *** -- 4 removed from queue -- sending 8 to queue -- sending 9 to queue -- *** work done on 4 *** -- 5 removed from queue -- sending 10 to queue -- *** work done on 5 *** -- 6 removed from queue -- *** work done on 6 *** -- 7 removed from queue -- *** work done on 7 *** -- 8 removed from queue -- *** work done on 8 *** -- 9 removed from queue -- *** work done on 9 *** -- 10 removed from queue -- *** work done on 10 *** -- All done! 
And the presentation is [here](https://obsidian.systems/reflex-nyhug/#/step-1). It's interactive! (Chrome recommended for certain sections)
&gt; There is something circular about refusing to change out of the fear of being treated poorly for refusing to change. But that simply isn't the conclusion I'm defending. We -- the two of us -- haven't been talking here on the micro level of "should `guys` be used or not." We've been talking about a much more macro issue about whether, to quote you: &gt; "It'd be better to be a community that recognizes when we make mistakes, even small like this, and adapts, than one that only panders to the needs of the majority" ...or whether, instead (to quote not you or I, but the one to whom you responded with the above quote): &gt; "Being a nazi over an imagined slight perceived by few and meant by none contained in common language the majority uses is itself decisive and unfriendly." In other words, what's the better kind of community? One where people can be expected to be "called out" for using politically incorrect language, and where people can expect to issue such call outs with general social sanction (and to influence behavior this way)? Or one where the act of "calling out" and of accusing others is, itself, a violation of social norms and an act of hostility (unless justified by some very strong basis)? Or, what is the balance between them? For my part, in this particular subthread, I've been arguing primarily the narrow point that it's unwarranted to merely *assume* that the language policing *will* (in this case) "broaden the minority" (to use your phrase). And of course I have also addressed the broader question. ----- &gt; refusing to change To hammer that in a bit more, I should point out that *I'm* not refusing to change. Nobody has asked *me* to change anything. That can't be what we're talking about.
&gt; There's a couple libraries available for communicating with databases. What do people recommend? It really depends on what you're doing and with what database you're working with. Postgresql-simple and mysql-simple are both medium level libraries for their respective databases. I personally prefer using them over hdbc. If you're look for a high level library, there's Hasql, but it only has a backend for postgres so far. &gt;How exactly does stack 'solve' the problems with cabal dependency hell without utilizing sandboxes? Stack uses a database of packages that are all compatible with each other.
I've been trying to figure this out for a while. I have a USB MIDI controller. When I dump the ALSA sequencer I get control codes from it, so it works. How do I obtain and use these control codes in Haskell? 
Well I must admit I'm a bit list at this point. I leave the argument in your capable hands. 
I don't see why. That's exactly what FRP was made for. FRP may not be very good with interactive systems (and [Conal himself wrote about that](http://conal.net/blog/posts/why-classic-frp-does-not-fit-interactive-behavior)), but for non-interactive time-varying values, it just works.
I've had success with this library: https://hackage.haskell.org/package/alsa-seq The source comes with an example program called ``dump.hs`` which I believe is what I had to read to figure out how to use it. (That's not necessarily a reflection of the docs, as this was literally my first Haskell project ever and I probably couldn't have figured anything out from any docs.)
I don't know much about `ByteString`s, but at least for `Vector`s, `Seq` makes a good alternative when you want *both* immutability and efficiency of updates. (Of course, if you don't care about immutability, `MVector` is an even better choice.) I know *lazy* `ByteString`s are good for concatenation-like updates, but not sure about more complicated kinds of updates like changing a specific character.
We had someone come in on a Haskell-only project at work. He learned it on the job as an intern, and they really wanted to hire him for full-time Haskelling by the end of the summer. He was pretty good with SQL and Java before learning Haskell. Existence proof :)
I've spent a bit of time with that, and I think I've read the dump example. I'll give it another go though, thanks for sharing your success!
&gt; There's a couple libraries available for communicating with databases. What do people recommend? I've had success with Persistent + esqueleto, but (perhaps due to my own failures) persistent-mysql was slow. hasql claims and seems to have very good performance as well as be strongly typed.
[concurrent-machines](http://hackage.haskell.org/package/concurrent-machines) offers various pipeline topologies with concurrent evaluation.
What's the state of the art of numerical calculations in Haskell? Is there something comparable in terms of ease of use and stability to Python's numpy (that is, multidimensional arrays with some supporting functions such as linear algebra or RNGs)? I would love to use Haskell for my scientific work (and I did write some specialized symbolic algebra code), but I simply have not yet seen a good library for array numerics. I know of [Accelerate](https://hackage.haskell.org/package/accelerate), but it seems like it only has very basic functionality, and the code that uses it looks rather complicated (or am I just spoiled by Python?). There's also [Numerical](https://github.com/wellposed/numerical), but it's in alpha stage at the moment.
To answer the question about stack, basically it relies on curated sets of packages called "snapshots" which are defined by Stackage (stackage.org/snapshots). Each of your projects will specify which snapshot to use (such as an `lts-2.21` or `nightly-2015-08-01` or whatever) and stack will install dependencies from that snapshot into snapshot-specific package databases on your machine. Since snapshots are immutable, you don't have to worry about your package databases getting "messed up". And, all of your projects that use the same snapshot will share package databases, so you don't need to rebuild stuff constantly.
Do i personally feel excluded? No. Im arguing in defense of those who do. You don't appreciate how difficult it can be to feel routinely ignored.
I see people cross post both unanswered and interesting answered stack overflow questions. So go for it. Also, FYI /r/haskellquestions
We could give you more suggestions if you told us which database you are using.
And isn't it funny you say nobody means to make me feel excluded while actively trying to do that (albeit for a different reason)
Alright I've thought a little bit and come up with something worth discussing. Earlier you mentioned how being silenced draws offense. There are two situations where you are being silenced * being an intentional asshole * being anything else In the first case, the silence is clearly justified. In the second case the silence may or may not be justified. Again, we can break it down * you object to the persons feeling of offense * something else Virtually all people I've seen fall under the first case, however I'm sure there are people in the second case. I'll only argue about the first case. If you object to the feeling of offense, that person has every right to view you as an asshole. Even if you have reasons. That person is not wrong to silence you. There is no issue here, but then we scale up. A community, which promotes certain values, and wishes to create a safe space for people, also has every right to silence you. They also have every right to silence the person who spoke up. **they do not have to be consistent**. A nation is held to standards of consistency. They probably can't do anything, but I don't think it's worth thinking about anyways. Looking at /r/Haskell. Who should be silenced? In my opinion the non adoptive offenders. This is what we were arguing about. The answer comes down to what the members of the community prioritize. I don't believe an objective answer exists. Some people claim that since the rule cannot be reduced to a logical consistent outline, it should not exist. Ultimately, some rule should exist, because there are clear problems with allowing the majority to run free.
You might be interested by David Mazieres' Safe Haskell paper
I was writing a reply when I read your comment. It's spot on, so take my upvote and have a good day.
And I dont know if it was you who made this point, but if it comes to a situation where one person feels hurt by divisive language, and another feels hurt by being accused, surely they can resolve the obvious misunderstanding through communication? I might add i tend to lean on the side of the person misunderstanding phrasing, not the person whos been accused, although I understand why that can be hurtful.
&gt; Stack uses a database of packages that are all compatible with each other. ...so does `cabal-install`, which uses the Hackage database where each package describes which other package versions it's compatible with, and it's the task of the Cabal solver to compute a valid install plan based on this information. Stack "solves" the problem of (some) package authors not living up to quality standards (by not properly maintaining their `.cabal` files) by getting rid of the Cabal solver and instead using pre-computed global install-plans (called snapshots).
This all breaks down though if you happen to need a different version than the one frozen into the stackage set... same problem we had with the Haskell Platform which often lags behind one major version for some packages... :-/
So /u/kylio wasn't completely honest when he claimed that &gt; When I installed my yesod project with cabal sandbox it took about 2 hours to download and compile all dependencies and build the project; stack reduced this to like 10-15 minutes. as `stack` would have needed the very same time of 2 hours if he had just freshly installed GHC and Stack (and had no package built yet)?
Tags are not immutable.
I wouldn't go so far as to say it wasn't honest but yes it would have required the same amount of time with a fresh install. Still, the savings on install times with stack are quite real, because almost nobody has exactly one haskell project they need to compile.
Can't help but wonder if the reason stack exists is due to the yesod ecosystem having inaccurate `.cabal` meta-data. I use a lot of kmett's packages which form a dependency forest at least as complex as yesod's and for some reason have never encountered all these problems yesod users seem to suffer from. It's like Yesod created the demand for Stackage and then Stack in the first place by being a bit too carefree with the inter-package dependency specification... ;-)
So not letting people police your choice of words is hostility but running around nitpicking others words is what friendly advice? 
I wish, but 7.10.2 was too late for that too.
Here's my understanding of which sql libraries to pick in Haskell. If you're new to Haskell and want to do something simple, postgresql-simple or mysql-simple are good choices. They are simple to install, and simple to use. If you're more familiar with Haskell and starting a completely new project, Persistent is a good choice. There are lots of libraries that make using it with Yesod, Snap, Servant, etc very easy. You can also combine Persistent with esqueleto to do more complicated queries. If you're more familiar with Haskell and need to access, for example, a DB maintained by a different team, [Opaleye](https://hackage.haskell.org/package/opaleye) or [HRR](http://khibino.github.io/haskell-relational-record/) are nice. They are much more typesafe/composable than postgres-simple and mysql-simple. I don't personally know about Opaleye, but HRR is being used in production at a large ISP in Japan. It should be very solid.
Like I said in [another comment](https://www.reddit.com/r/haskell/comments/3f6lu9/trouble_with_yessod/ctqum3e) it seems Yesod *requires* Stack to cope with its inaccurate interpackage-dependency meta-data. So I guess I agree with you that the OP is best helped by using the special tooling needed to cope with Yesod (if he can't avoid using Yesod)... ;-) I see no reason for package authors to test their packages under Stack, as Stack is supposed to be compatible w/ `cabal-install`. So if anything builds with `cabal-install` it's guaranteed to build w/ `Stack` as well as `cabal-install` has stronger requirements (it needs accurate lower/upper bound specifications of its build-dependencies, otherwise the cabal solver is lacking essential information to come up with a sound install-plan). Conversely, if wanted to promote the use of Stack, I'd try to write packages that `cabal-install` can't cope with (that's quite easy actually, just leave off most version bounds then `cabal-install` either exhausts the backjump limit or picks up wrong install-plans) but Stack+Stackage can... That said, I can see how some of Stack features are convenient, but I'd rather have them available in `cabal-install`. Just like if I am a Windows user, I won't install OSX just to benefit from some neat application... I'd rather have it ported to Windows and get on with my work... PS: What are you referring to specifically when you say that `cabal install` should reject bad install-plan earlier? When does `cabal install` do that?
`[citation needed]`
IIRC stack has some docker-specific commands. Try stack docker
Btw, IIRC I saw some rather simple proof-of-concept patch to `cabal-install` that would enable `cabal-install` to cache/reuse already built packages. There were some issues that needed to be ironed out, but it didn't seem impossible. PS: https://github.com/haskell/cabal/issues/2365
So then you have no problem with me spouting off all this stuff in this subreddit, logically. You don't believe banning should be a thing? Good for you. I do. Most community leaders do. Youre trying to argue as though I ought to see it as ridiculous to want to silence someone for being a dick. Guess what. I dont.
&gt; I see this sort of thing quite a lot, i.e. friendly reminders in GitHub issue trackers like "please upload new version to hackage reflecting commit XYZ" when a library maintainer has been vacant from the web for some time. While this is definitely a convenient feature, it only provides a local fix/workaround (and it doesn't scale, as everyone affected by the same project has to replicate this local config, and moreover you run into a commit-sha Hell if two packages in the same install-plan would require differently pinned commit-shas). You can't upload such a package to Hackage (or at least you'd violate Hackage's terms if you uploaded a package that relied on Stack features, assuming Stack can handle .yaml files in source-tarballs...), you still have to wait for the vacant author to upload a fix before you can publish your own package :-/
I strongly disagree. There's definitely no 100% guarantee that the packages won't run into compile errors. The underlying issue is that 95% of compile errors are caused by package authors not deeming it necessary to maintain their `.cabal` meta-data and/or following the PVP. The other 5% are due to local environment issues mostly caused by FFI imports (e.g. missing C header, wrong FFI library version). Then sometimes a package only works with `gcc` but not for a `clang` environment. Or it's one of those packages which doesn't work on Windows (some packages have platform-conditional exported modules/constructors/etc.). Or maybe your Linux distribution is too old or too bleeding edge, or you're using the latest OSX version which broke stuff yet again. So unless Stackage has a huge build-farm test-compiling on at least a dozen of platform configurations, you won't have any guarantee whatsoever, except for their specific configuration they tested on...
How do you keep nice flow if the compilation errors leaves you in effectively unusable REPL? Often to fix those errors you need to just inspect type signature of some value you've used wrongly, thus `:t foo` would be more than enough, but the 'foo' is not there (if it's imported, fully qualified name will do, but even this is a bit cumbersome).
[Since June 18](https://github.com/commercialhaskell/stack/issues/116).
Thanks! http://hackage.haskell.org/package/concurrent-machines-0.1.0.0/docs/Data-Machine-Concurrent.html#v:buffer looks like my function.
I haven't used it much, but you can turn off static type checking using the flag `-fdefer-type-errors`. You can now load a file to the ghci using `ghci -fdefer-type-errors File.hs` and examine it. [relevant link](https://ghc.haskell.org/trac/ghc/wiki/DeferErrorsToRuntime)
*I'm* not being asked to agree to use some particular language though. If I'm being asked to do anything, I'm being asked to agree with a certain idea about what effect language has, and also what effect making demands about language has.
I support your right to say things I disagree with. I merely wish to disagree not silence you. 
I see... and they still relay on `cabal-install` for that ironically as they haven't yet reimplemented that part of `cabal-install` in Stack... &gt; I've just implemented a first version of this feature on top of `cabal-install`. 
Yes, Stack evades the underlying issue and it provides short-term convenience for users, but reduces the pressure to fix the underlying issue in Hackage... because the problem becomes less perceivable in the Stack bubble, and people wonder why crazy people like me get so upset about the wrong meta-data on Hackage in the first place... I don't think this is sustainable in the long-term... it's like ignoring the social downard spiral happening outside of gated communities, because inside the gated community all is fine and dandy... or other examples where you can shield yourself from the rest of the environment for a short period of time until it catches up to you :-( Btw, what is the "definition of cabal hell" anyway? People seem to use that term to name different effects most of which are used as reason to use Stack instead... Ironically, IMO, cabal hell only refers to the problem of not being able to find an install plan after an exhaustive search (i.e. with `--max-backjumps=-1`) due to conflicting dependencies. And this is the one thing Stack can't avoid either, unless it overrides the inter-package dependency specification...
So you're saying that asking someone to use more inclusive language is being a silly drama queen? 
### Why is there no operator overloading ala C++ for function resolution? For example, unwords exists in many flavors, ([String -&gt; String, [Text] -&gt; Text). Each time I'm using one of them, I need to fully qualify it, even if the variable used as first argument is a [String] or a [Text], making the definition not ambiguous. There is the same issue with record fields, when two records share the same fields, you have to fully qualified the field function, even if it appears unambiguous. For some operations, it exists typeclass which make them generic (such as Traversable for fmap), but some implementation does not use them (such as unboxed Vector which is, for reasons that I don't get, not Traversable neither Foldable). ### How do you handle complex applications? I'm writing a small TCP server. A server answers requests from clients, who read/write a shared database. - I need a State to store the leftover of my Socket read. (I'm reading Network.Socket.recv, and sometime I may receive more than what is expected, and the leftover char must be stored for the next command). - I needed to store my database, which can be modified by ALL clients, so I put it in an MVar, stored in a Reader. - Then a connected client is also associated with a status (for example, authenticated or not), so another State for the client. (Obviously I can share the Socket leftover State with the Client State in one State, but I want to avoid coupling as much as possible) Then, ALL my functions ended as a transformer stack looking like: ReaderT Database (StateT Client (StateT SocketLeftOver IO)) () And because I tried to separate a bit everything, some of my functions only works on a limited part of the monad stack, such as: readMsg :: Socket -&gt; State SocketLeftOver IO () connectClient :: State Client () addUserInDb :: Reader Database () So I need to hoist/lift in complex way in my top level functions, and this is really painful, unreadable and complex (And I'm talking about a 150 lines basic TCP server). Thank you.
They make some kinds of code much easier to read and write. Once you know what the symbols mean, you can quickly glance at code and separate the important things from the unimportant things when you are looking for something in particular. Imagine doing arithmetic without operators, so instead of saying 4 * 5 - 7 / 8 + 3 you'd have to say plus (minus (mult 4 5) (div 7 8)) 3 Some Lisp guys might like that, but I prefer the symbol heavy version.
one thing we clearly see from your example is that operator precedence matter, and if you do not know what the precedence is it becomes impossible to read (sometimes, even if you do). operators are very useful, but i'll definitely add some parenthesis to your example :)
Great to see more introductory material for sweet type-level programming stuff! I haven't used Servant yet but I plan to start. More compile time checks = more happy.
I was going to mention PCPH as well. It's a fantastic book in general. Might not necessarily help you much if you need to optimise a tight loop, but knowing about the different ways you can split your problem up and get easy wins from parallelism is great. The chapters on Accelerate and Repa are helpful, too - especially considering the recent paper about an LLVM/SIMD backend for Accelerate.
Say I have a background in Object Oriented programming. How best to 'translate' my thinking into functional?
Just the examples in the repo and some random tutorials. It seems that it implements a lot of nice ideas, but just needs that final API layer to make it actually convenient to use. I mean, take the example from the main page (a function that calculates the dot product of two vectors): dotp xs ys = fold (+) 0 (zipWith (*) xs ys) My knowledge of Haskell is not enough to understand why can't it be made possible to write dotp xs ys = sum (xs * ys) (in other words, why arrays are not Num instances). And yes, having some tutorials for people coming over from Python/Matlab would be nice --- explain how to create zero/ones/random arrays, how to take slices, broadcasting, transpositions, matrix operations, this kind of stuff. Numpy [does it very well](http://wiki.scipy.org/NumPy_for_Matlab_Users) for people coming from Matlab.
You don't feel that calling someone a Nazi for asking people to use more inclusive language might be a tiny bit of an overreaction? Perhaps that you might possibly come across as a little bit unfriendly yourself?
&gt; HRR is being used in production at a large ISP in Japan It would be awesome if you could share more about that!
Not at all. While it's possible to translate your thinking into functional thinking, it's probably easier to just learn functional thinking from scratch. The basic principles for good code are the same (implementation hiding, low coupling and so on) but you achieve them in different ways when programming functional code.
Ok. Next savior of Haskell.
Thanks for sharing this. It's always good to hear about different experiences people have with a programming language (both good and bad).
GHCJS is very alive. I have used it to build part of an in-house tool at work.
I've also faced problems with Haskell on Windows, although I've never tried Leksah. Installing haskell platform is easy, but when you want to install a package through cabal which requires e.g. mSys to compile, it takes hours to setup everything. I tried Scala and Clojure but finally I returned to Haskell, it's a much nicer language (imho). Recently I switched to Ubuntu and everything is much easier on linux. I use Sublime Text 3 as an IDE, it has a nice plugin which automatically runs hlint on save, and able to compile and run cabal based applications.
I think MonadParallel probably is the magic sauce I need to generalise the monad - thanks! I'll probably leave it at IO only for my library, but nice to know about MonadParallel.
Have some imaginary gold.
The program above uses the style of the ['work stealing' examples](https://hackage.haskell.org/package/pipes-concurrency-2.0.3/docs/Pipes-Concurrent-Tutorial.html#g:2) from the tutorial. We can abstract something like this buffer :: Int -&gt; Producer a IO () -&gt; Consumer a IO () -&gt; IO () buffer n producer consumer = do (output, input, seal) &lt;- spawn' $ bounded n b &lt;- async $ do runEffect $ producer &gt;-&gt; toOutput output atomically seal a &lt;- async $ do runEffect $ fromInput input &gt;-&gt; consumer atomically seal waitBoth a b &gt;&gt; return () -- a sort of buffered, concurrent version of `Conduit.$$` but the machinery articulated in the tutorial admits all sorts of other arrangements and 'topologies'. In particular, the restriction to one consumer or producer is unnecessary. For this use-case it would be clearer to use the names `toBuffer` and `fromBuffer` for `toOutput` and `fromInput`. 
...and despite *not* using `stack` "everything installed without any problems" ;-)
This is not (yet) ideal for IDEs, but it's true that the stack experience is amazing on Windows. Put `stack.exe` on your `PATH`. Done. MinGHC will soon (later this week) include `stack`.
I have long concluded that if only the `error` function was named `bug`, there would be no source of confusion.
Cabal hell is where the concern comes in, I think.
That's interesting. It might be a preference thing -- in Scala in cases where there are both English-named and symbol-named methods, I've tended to stick with the English names. Of course even atrocities like all the .equals()'ing in Java, or Java's bigdecimal don't bother me that much. (Re: .equals, I know that the main issue is confusing behavior of ==.)
 Maybe Gold
I do appreciate that -- I was not aware that there were folks out there that found this sort of thing counterproductive. It isn't surprising to me, though, that with the wide diversity in people and women in technology that there are multiple opinions and perspectives. I would be interested in seeing work that addressed the combination of these two issues and attempted to come to a good compromise. For me personally, I had a difficult time understanding your comments as a suggestion/correction due to language like "hypersensitive PC nonsense." The comment that I responded to claimed that no one cared about these things, and I said that many people do with a brief explanation. Your comment didn't read like "There are unintended side effects," but "Trying to make XYZ more comfortable makes ABC less comfortable and we should not care about XYZ."
Ok, so I've got a fairly doable problem that I don't understand because I haven't grokked monad transformers yet. I have an evaluator for an untyped (or rather, dynamically typed/unityped) lambda calculus. It uses an error/exception monad to "throw" in the case of an evaluation error, like an unbound variable or a dynamic type-tag error (ie: don't use a numerical constant where a lambda expression is expected). Since it's an untyped lambda calculus, it's not normalizing, and I want to handle the partiality explicitly via a partiality monad. Since I already have an error/exception monad, I think I need to compose a partiality monad transformer over my exception monad, right? How do I define/import stuff and code that up?
Cool, good to know
&gt; myFunction :: (MonadState Foo m, MonadState Bar m) =&gt; m r myFunction = do (foo :: Foo) &lt;- get (bar :: Bar) &lt;- get return $ myPureFn foo bar Why not myFunction :: MonadState (Foo, Bar) m =&gt; m r myFunction = do foo &lt;- gets fst bar &lt;- gets snd return $ myPureFn foo bar ?
 Proxy Gold
I like my metaphors :).
Where does the term "substructural" come from?
Yes, I am using lens, but you can only zoom in on one piece of state at a time - when I needed two out of three I couldn't see a way forward.
I didn't know of `stack`. I hope I won't need it, but now I know what to do if things go horribly wrong and I need to uninstall the `Haskell Plaftorm`.
 newtonsthird :: Num a =&gt; [[a]] -&gt; [[a]] newtonsthird = ap (zipWith . zipWith $ (-)) transpose
It generally does when a platform release is brand new. The trouble starts when the platform becomes outdated.
Couldn't Servant work the other way around by using the singleton library? That way users would define APIs as values, and all these bells and whistles of the type system syntax wouldn't be needed.
I've seen the SO post, but unfortunately every page on the partiality monad (and transformer) presents it in a slightly different formulation (sometimes it's `Free Identity`, other times `Free Maybe`, then `Free ((-&gt;) t)`, etc.). What I actually want is more the former type (`PartialT (Either MyException)`) than the latter: if my untyped lambda expression has not yet reached an exception *or* a normalized value, I want to treat that as a partially-evaluated thingy, whereas wrapping exception-handling around a partiality/maybe monad doesn't seem *quite* right. But on the other hand, the latter option does seem more "New Jersey", in terms of being easy to implement, since my `do`-notation will only have to take account of the exceptions and I can just destructure the partiality with `case`.
But if you already are already using `lens`, then you already have the classes : data S = S { _sFoo :: Foo, _sBar :: Bar } $(makeFields ''S) x :: (HasFoo s Foo, HasBar s Bar, MonadState s m) =&gt; m (Foo, Bar) x = (,) &lt;$&gt; use foo &lt;*&gt; use bar (Yes this is not as nice as your proposal)
Why does everyone call it built in? There's nothing built in about it these days, it's just an inferior package that people use or if habit or due to a convenient name.
Interesting, I will try this.
Why the constraints are applied that way? If an operation is defined without any constraint (here, a and b are not constrained in the definition of traverse), it should work on something which is more constrained. Where am I wrong? Said differently, When I read the definition of traverse, I read that f must be Applicative and that a, b and t can be anything, especially, a and b can be Unbox. The Unbox constrainte is such as data Vector t = Unbox t =&gt; Vector t, so the unification of traverse should fail if the resulting b is not Unbox, which depends on the (a -&gt; f b) argument of traverse, but not on the fact that the second argument of traverse is "Vector a".
How do you get this mechanism to work?
This looks like it is probably better than mtl-unleashed. (Edit: I'm no longer certain of this.)
wait... is it that hard? I write Java at work and have been learning Haskell at home. it seemed like there were enough Haskell jobs, given how few Haskell programmers there are. I kinda hoped that if I took my time researching/interviewing after my current job, I'd find a Haskell shop :(
This is where `Vinyl` really shines: sane subset operations. /u/dsfox, if you're interested I (or someone else) can show you how this works.
that would be cool. it needs Template Haskell though, right? until -XDependentTypes comes! maybe.
This'd is why I use Racket instead of Haskell.
In a logic, the structural rules are things like "If you can prove `A` using an two assumptions of `B` you can prove it using just one" and similar. They operate on the "structure" of the judgement rather than a particular proposition. In substructural logics you remove structural rules - Affine = no contraction (Can't duplicate things) - Linear = no weakening and no contraction (Can't duplicate or drop things) - Ordered = no weakening, contraction, or exchange (Must also use things in order) - Relevant = no weakening (Can't drop things)
actually, do you have any examples of what this might look like in a dependently typed language? e: found https://github.com/idris-hackers/IdrisWeb/blob/master/examples/MessageBoard/Main.idr but it doesn't seem to have any "type level API" stuff
The way I see it is as though someone asked you to stop doing something cause it was bugging them and you kept doing it. Sometimes, they might've asked you for bad reasons, but often try didn't. If they didn't and you keep doing it, it's clear you're not being very nice. Depending on what it is, it might not be enough to justify "banning", but I'd say depending on the particular subreddit, something like this easily could warrant banning. On r/Haskell, I don't think one slip up justifies a ban. But if it turned into harassment, I think it could.
I have almost given up a couple times when I was struggling to get my code to typecheck. Glad I haven't because each time I have managed to break through a sticking point, it's advanced my understanding of the language a bit.
I think you can switch them if you just provide a `MonadFree` instance for `ExceptT`
Actually it can be quite easily. It is never updated between releases so if e.g. an author of one of your dependencies releases a critical bug or security fix you are practically forced not to support the HP.
feel free to use non-emoticons like: Or = (:&lt;|&gt;) type API = Or ("x" :&gt; Get '[JSON] Int) (Or ... (Or ...)) which is less readable
Lack of IDE support doesn't bother me; I don't use IDEs. Lack of syntax highlighting and decent autoindent for Emacs, Vim, TextMate-likes, and GitHub would bother me.
where? Europe? US Midwest? in general?
Don't forget the infrastructure like sbt using macros for no reason whatsoever. When I develop in Scala I really don't understand what's the point? Pure code is freaking awkward, libraries use some arcane features, so why the hell am I not using Ruby or Java 8 or C#? Actually after reading this comment I realized I'm fed up with Scala and it's time to make the final push and start writing Haskell programs that do something useful.
&gt; I'm currently on the fence about FRP. I really like its ideas, though I'm put off as it can't be hot swapped easily. Have you seen hot-swapping in Elm? The [Mario example](http://elm-lang.org/examples/mario) is a good one to play with.
haha which 11!?
There are plenty more than that.
What's interesting is that Conal Elliot [seems to suggest](http://begriffs.com/posts/2015-07-22-essence-of-frp.html) that none of those is a true formulation of FRP.
Really depends on the context. If you follow the prior posters logic we would only be able to say things that offended nobody. It puts the responsibility of responding to anyone's hurt feelings on the speaker who must modify his behavior according to the wishes of the offended without criticism or analysis which is defined as hostility. This is especially silly in net discussions addressed as they are to a potential audience of thousands / millions. There is a difference between modifying how you address one person who you know feels a certain way and changing how you address the entire net because one person like that is waiting for the opportunity to make points off correcting you. There is also a huge difference between a request made casually and one made with an implied threat. Even if the poster can't now he very clearly wants to arrange it to be so. Real inclusion isn't about word choice it's about fighting our inherent bias towards those who are like self and treating everyone fairly. It's about fairness in social groups at work and within family.
My understanding is that Elm records and replays all the input events, which the GP specifically wants to avoid, presumably because he has a long-running application.
I think he is taking about IO-based reification. http://ku-fpg.github.io/papers/Gill-09-TypeSafeReification/
I find that using racket instead of haskell doesn't make haskell package management any easier?
FRP does not necessarily require remembering the entire history of your application, even if it has a monadic interface. Check out the [FRPNow library](https://hackage.haskell.org/package/frpnow) and the associated [paper](http://www.cse.chalmers.se/~atze/papers/prprfrp.pdf). It sounds like that might not be what you want anyway, but I just wanted to mention it in case people thought that FRP necessitated space/time leaks.
quoth Bradbury http://www.rjgeib.com/thoughts/451/451.html
Point-free is definitely more idiomatic, but a lot of people can take it too far. I don't really like point-free when `flip` is necessary, or `(fmap .) . fmap` composition sections come into play. It reminds me of code golfing in a way that I don't like. I'm also a total noob, so my opinion doesn't reflect Haskellers at large :)
I'm getting ready to leave a job where I spear-headed a Haskell project. Losing that project is the only thing I'm going to regret about leaving the job. I hope I can spread the Haskell again at my next gig!
My blurb above was very much a sketch. You'd first need to choose some framework for record types. There are many, and some include combinators for interacting with MonadState in this kind of way (eg lens). HTH. Good luck.
I'm very optimistic about FRP and really appreciative of the work people are doing to find the sweet spots. Thank you Conal, Evan, Heinrich, Ryan, Atze &amp; Koen, etc. (Those are just the ones whose work has affected my optimism and that I happen to remember right now.) My first dive into Elm and its magnificent ecosystem reminded me of my joy when first learning Haskell. But if prefer a Haskell library. I'm excited about Reflex and FRPNow (lowercase package names in Hackage) right now and look forward to finding time to do some experimentation again. And Ollie's comment above has me excited to take another look at reactive-banana. Edit: counterpunch auto correct; refer to younger comment.
I disagree. I would even go the other way and ban infix definitions so you couldn't write: xs ++ ys = ... But would have to write: (++) xs ys = ... That way things like bang patterns stop being ambiguous and lazy patterns don't have to steal an operator. 
Oh boy, FRP! &gt; What are the main differences between different FRP libraries and systems? The term FRP is thrown around fairly generally, but it does have quite a specific meaning. True to its owners intentions, FRP should be *continuous* and *denotational*. Apparently a few popular contenders that appear FRP-like fail this criteria, Elm probably being the most popular. Of course, this isn't necessarily a bad thing - Elm is clearly a very powerful tool. For those systems that are true to FRP, we have two styles in Haskell - "classic" FRP and arrowized FRP. In classic FRP we pass around behaviors and events as first class values, but early on this quickly showed to be unimplementable - it leads to the dreaded "space leak" where behaviors have to hold on to the past, sometimes indefinitely, in order to be sampled. We've found out this isn't the case, but you just have to be a little smarter. There are a few solutions, but most resolve around trying to introduce some notion of relative time into the equation. `reflex` does this by requiring a few operations to be performed inside a monadic context, `reactive-banana` uses parametricity to capture a notion of "now or later", and the recently released `frpnow` changes some primitive operations to never implicitly sample the past, hence the paper title "Forget the Past!" Before we got to that though, another option was explored - what if we made the *functions* operating on time be first class, rather than the values themselves? This leads to a style known as arrowized FRP, where you are now programming with the arrow combinators and`proc` notation. This is the approach taken by `yampa` and `netwire`. &gt; Do different systems fit different domains? `reflex` comes with `reflex-dom` which makes it a very strong contender for doing web based programming, though I'm hoping to shake that up in the future with my Francium library (which might use `reactive-banana` or might use `frpnow`, it's still in a design phase). Yampa has been used a lot in the game programming community, and arrowized FRP could be seen as a good fit for this type of domain. Yampa and netwire "frame" the simulation, which corresponds natural to video games and animations where you do have clear frames. That said, my preference has moved over to the `frpnow` and `reactive-banana` API. I found arrowized FRP lead to code that still had an important top-to-bottom ordering, but the aforementioned have liberated me from this concentrate. I also find it much more natural in Haskell to program with `Functor`, `Applicative` and `Monad` than using `proc` and arrow notation. &gt; Do they make trade-offs to deal with performance problems or something like that? I don't think any are trading performance, all want to be as fast as possible. I believe only recently `reactive-banana` started to bring performance to the front, and `frpnow` is so new it's questionable how performant it actually is. &gt; Which FRP library do you recommend? why? Probably `reactive-banana` right now, and I'd encourage a look at `frpnow`. These both use common Haskell type classes, so you'll hopefully eliminate a bit of the learning with that. I'd suggest `reactive-banana` as it's more mature, has a beautifully compact API, and I haven't once managed to break it. With `frpnow` I've found a few things that might be bugs or user error - mainly in the error of mutually recursive definitions. In `reactive-banana`, no matter how crazy I am, the whole system still seems to evaluate naturally. Nice! &gt; Do you think FRP is even interesting/important? Yes, but I'm not sure how to phrase it. One major aspect to me is it as a solution to callback hell. By passing around events, consumers can choose how to build on top of these events locally (by manipulating the event), rather than remotely (within a callback at the callsite) - and that's a huge win for me. I also really like the separation of concerns it promotes - behaviors denote time varying values, but you don't get to change them directly (that is, no direct mutation). Rather, you have to define events where they switch. This can lead to interesting ways of thinking about problems in terms of who's producing data and who's consuming data. I probably am such a fan of it because I've found it so enjoyable to work with.
It's been almost 6 months since we started using reflex for a large web GUI in one of our projects at Soostone. Reflex allows us to create very reusable GUI components and build up complex interactive interfaces from them. Here is a random collection of thoughts that come to mind at the moment. 1. Reflex's choice to use a mostly pure, occasionally monadic interface instead of an arrowized interface makes for a more approachable API due to the use of familiar type classes like Functor, Applicative, and Monad. My personal advice would be to stay away from arrowized FRP systems for this reason. 2. The ability of components to have their own local state that doesn't have to be included in a central data structure is a really nice property that helps keep code simpler. 3. Sooner or later you're going to *need* dynamic graph modification (at least if you're building UIs). I don't know that I can say categorically that the things I've done would be impossible without it, but I suspect that is the case. Dynamic graphs give you the ability to simplify your interface to pure (non-FRP) values as needed. This can be a useful tool for reducing complexity. If you have a widget that takes two dynamic values a and b as inputs, you have to consider all possible orderings of event firings between the two. If you have dynamic graph modification, you can instead make a and b pure inputs. Then your widget code doesn't have to worry about the relative ordering of a and b events at all. It sees them as static values and the whole widget gets redrawn if either of them changes. 4. A lot of attention has been paid to performance in reflex. So while the initial announcements have focused on web UIs, it should also be an attractive platform for any other application that you might use FRP for. We don't have concrete benchmarks yet, but I would expect reflex to do quite well in this area. As someone who has usually preferred backend to frontend work, reflex is the first thing I've used that makes building web UIs enjoyable for me, which is very exciting. I think FRP has the potential to allow Haskell developers to build much more robust GUIs at a faster rate than the competition. It will take a good bit of manpower to get there, but I think the payoff will be big.
I've read that using the Free Monad is slow, but they never explain why it is slow. The free package implements some sort of church modules that are supposed to be faster, but references posts (free monads for less) that I don't understand. It is supposedly slow because it is a linked list of monadic actions, and every new action is an append to that list, but does that lead to long compile times or run times? I love the idea of rewriting some applications but I would hate to run into performance problems. Is this even a consideration in a real application?
Another useful feature is "typed holes", which work quite well with `-fdefer-type-errors`: https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/typed-holes.html
The documentation says `Seq.adjust` is `O(log(min(i, n-i)))`, so no, it's not very expensive.
If you're walking that line, you might as well just adopt Agda's mixfix operators. 
The most serious issue I have with Haskell is that it has more or less ruined every other language for me. That would not be a problem if I didn't have to work only with those...
&gt; Now, HP includes MSys Oh I didn't know that. Good news.
Saying that "a, b and t can be anything" is a little ambiguous. If they can be anything, the crucial question is: who gets to choose? Thinking about who gets to choose (and conversely, who has to take what they're given) is often useful for illuminating tricky polymorphism problems. In this case, when I am writing an instance for class Traversable t where traverse :: Applicative f =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) I get to choose the `t` (in the instance declaration) but the caller gets to choose `f`, `a` and `b`. I know that `f` will have an `Applicative` instance, but I can't rely on the assumption that an `Unbox b` instance exists, because I'm not the one choosing `b`. On the other hand, if I am calling `traverse`, I can choose whatever types I like for `a`, `b`, `f` and `t` provided I satisfy the `Traversable t` and `Applicative f` constraints.
Just Gold
Of the TextMate-likes, I'm definitely enjoying Atom the most! Being FOSS and cross-platform is really important to me.
I use this in Haskell. I make 2 helpers: hole = undefined -- quiet hole data Hole = Hole -- noisy hole Now I can do things like: instance Monad (MaybeIO a) where return = hole (&gt;&gt;=) = hole ...and start replacing `hole`, which compiles fine, and silently, with `Hole`, which fails compilation, telling me what's missing. Then I just let the holes drive the design. For example, replacing the first `hole` with `Hole` errors that it's looking for type `a -&gt; MaybeIO a`, so I'd change the `return` line to: return x = hole That would compile. Changing that `hole` to `Hole`, I'd see that it now wants `MaybeIO a`, so I'd change it to `Just hole`, which would compile, etc. I walked my way to a working - and apparently somewhat idiomatic - MaybeIO monad instance this way (and did a live demo for a meetup with it).
Most certainly! [The Haskell Wiki has a large list of compilers for programming languages.](https://wiki.haskell.org/Applications_and_libraries/Compilers_and_interpreters)
The approach I use in git-annex and propellor is to buid a dynamic binary, ldd it, bundle copies of all the listed dynamic libs with it (along with some other glibc cruft) and a shell script that makes the linker use them rather than the system libraries. This is sufficiently portable that 32 bit binaries can be run on 64 bit systems, and that I've had arm builds run on a wide array of embedded arm devices. http://joeyh.name/blog/entry/completely_linux_distribution-independent_packaging/ This also works for those annoying dns resolver libs that are always dynamically loaded by glibc, even from otherwise fully statically linked C programs.
As /u/creichert mentioned, typed holes are very useful. To have the compiler help you out even more, you can use [partial type signatures](https://www.fpcomplete.com/user/thomasw/new-in-ghc-7-10-partial-type-signatures).
Consider a simple free monad, such as leaf-labelled binary trees: data Tree a = Leaf a | Node (Tree a) (Tree a) the (&gt;&gt;=) operator walks over the tree and substitutes subtrees at the leaves: Leaf x &gt;&gt;= f = f x Node s t &gt;&gt;= f = Node (s &gt;&gt;= f) (t &gt;&gt;= f) Now suppose we have a left-associated chain of binds, like this: ((t &gt;&gt;= f) &gt;&gt;= g) &gt;&gt;= h What happens when this is evaluated? First we have to walk over the tree `t` applying the substitution `f` at the leaves. This gives a bigger tree, and then we have to walk over the whole thing applying `g` at its leaves. And so on... This can potentially lead to unnecessarily long run times, as the same structure is repeatedly re-traversed. Is it a problem in practice? Not necessarily; as usual, I'd write the simplest program first, and optimize if necessary. In this case, the problem can be fixed by fairly mechanically applying the transformation you refer to, without digging into the details of how it works. Janis Voigtlander's paper "Asymptotic Improvement of Computations over Free Monads" describes the technique in general, but starts off by explaining the binary tree example in detail.
[Sean Leather](https://www.linkedin.com/in/seanleather) did his Masters at Texas University, Austin, and he seemed to know his stuff :) Utrecht University (Netherlands) also has a very strong FP (mainly Haskell) presence.
If you're open to outside the US, I hear they might do some Haskell at University of Glasgow. ;)
&gt; leaving you with a weird C#, with fold and map tacked on. That's a bit melodramatic. Scala is big and complex, but it's a bit more than C# with fold and map tacked on. C# doesn't allow you to properly work with typeclasses like monads and monoids. Scala does and it's pretty easy.
Does the Leksah for GHC 7.10.1 work ok with the 7.10.2-a haskell platform install?
Conal Elliott has a pretty nice description of it in [this talk](http://begriffs.com/posts/2015-07-22-essence-of-frp.html). /u/tel states it pretty concisely in his comment below as "sampling rate independence".
Thanks -- and good luck to you!
Nothing
Why is "sampling rate independence" beneficial vs just having discrete event streams? I mean beneficial as in it enables a solution to some kind of software engineering problem.
This is also my basic design principle. If rearranging tuples become unmanageable or code-smelly, you can use `vinyl` to plumb around the inputs and outputs automatically.
yay
There are no plans to add LLVM to the platform that I know of. However, starting in the 7.12 series, last I heard, there _is_ a plan to make available LLVM releases pegged specifically to GHC releases, rather than the current situation where ghc tries to work with "whatever llvm is there". Source: http://smart-cactus.org/~ben/posts/2014-11-28-state-of-llvm-backend.html
&gt; If an operation is defined without any constraint (here, a and b are not constrained in the definition of traverse), it should work on something which is more constrained. Where am I wrong? You're not really wrong; it's just the other way around. When you *consume* a function like `fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b`, you can use it in a more constrained setting, like `fmap :: (a -&gt; b) -&gt; [a] -&gt; [b]` or `fmap :: (a -&gt; a) -&gt; [a] -&gt; [a]`. But if you want to *implement* such a function, e.g. for an Unboxed Vector, you must be at least as generalized as the signature promises. And you can't do that with an Unboxed Vector, since you can't put most types into it; it needs to be more constrained. The `mono-traversable` package provides [a collection of monomorphic analogues for Functor etc.](http://hackage.haskell.org/package/mono-traversable-0.9.2.1/docs/Data-MonoTraversable.html) that include instances for Unboxed Vectors. 
While Conal did invent FRP and so it would make sense for him to have the authority to define what the word means, the word "FRP" has already been adopted by too many people to mean something else that I believe it is counter-productive to worry about what counts as "true" FRP. In the FRP Zoo, I use the word "continuous FRP" to refer to Conal-style FRP systems in which signals are functions from time to values. This is as opposed to discrete systems in which the value of a signal can only change at event occurrences, and otherwise remains constant in between occurrences. I would say that the term FRP has evolved to mean any interactive system which eschews the traditional approach of defining event handlers, and instead describes the behavior of the system as a whole, by describing smaller interactive systems (with fewer or simpler inputs and outputs) and combining them into larger interactive systems via combinators such as map and filter.
Your original question was an excellent beginner question! You got a few expert responses right away. Please feel free to ask more questions here. Point-free style *is* idiomatic, but there's a fine line between "elegant" and "obfuscated." Since you're new to the language, that difference is perhaps not obvious! Go ahead and try to write everything both ways, *and* spend the time to write pointfree code you come across in a normal ("pointed") style and vice-versa, and think about which is more appropriate for the circumstance. It's an excellent technical exercise in its own right, like playing scales in music.
I'm sure [Christian Skalka](http://www.cs.uvm.edu/~ceskalka/) could give you some information about his current research. I think he mainly works in OCaml and Scala, though.
&gt; postgres-simple OK I think others covered most of your options: postgres-simple, opaleye, persistent + esqueleto, hasql, etc.
Galois, Wagon, Facebook, Standards Chartered, Tsuru Capital, Silk. Note that the latter four are not currently hiring for Haskell positions, but they seem to continue using Haskell.
Have you tried using stack, or cabal sandboxes? Actually I find Haskell dependency management superior to other languages and their dependency systems. Haskell's tools will tell you up front if code will fail, based on version requirements. Other languages will just fail silently at runtime, or lack a dependency solver, etc.
Well, now I've got it all at least type-checking.
You may want to describe something which has a natural representation outside of any particular sampling rate—adding that discretization strictly weakens your model. You may want to run your system a various rates and expect it to perform comparably. You may have different sampling rates on different inputs and want as much of your computation to be free of that negotiation as possible. You may want to invoke some kind of domain transformation (a continuous one, of course) and would have to re-sample or subsample to properly handle that transformation.
Not sure what you mean by "open interface." I google the term and couldn't find a hard definition. But it sounds to me that typeclasses in Haskell qualify? In that case, Scala supports that with implicit parameters and generic traits. C# and Java do not. So once again, it's hilariously asinine of you to call Scala a weird C# with fold and map tacked on.
By "open interface", I think /u/ForTheFunctionGod means the ability to add methods/properties to any class-like-thing. Ruby is the only language I know of which actually has this feature. Scala lets you simulate it with implicit conversions, but one wonders why it couldn't just support them outright. Same with typeclasses.
Emacs indentation is decent, but it's not perfect.
Some great things to look into here! I definitely feel the lack of parties interested in this particular field in the Haskell ecosystem, but as I've gotten more responses I've become more confident that the building blocks are there, it's just a matter of putting them together. Thanks very much. :)
&gt; Pure code is freaking awkward Seriously. I recently tried to use `State` from scalaz, only to discover just how weak Scala's type inference is (and how important type inference is for monadic programming). And you can't even use `StateT` without a type-level lambda, since Scala apparently doesn't curry polymorphic types.
&gt; at Texas University, Austin You mean The University of Texas at Austin. But his LinkedIn also says his Haskell experience started during his PhD at Utrecht University. So I'm not sure UT is really relevant there.
I will there are a ton http://pkgs.racket-lang.org/ I didn't say it as a troll but as someone who was really interested in Functional Programming I found Haskell to be a mess for my own personal use and that Racket with its awesome JIT DrRacket was perfect for me. A big surprise.
anyone tried scalaz?
Hmm Scala has built-in syntax for this implicit class SOMENAME(val mything: ThingImAddingMethodsTo) { def NewMethod() = ??? } And unlike Ruby, this monkey-patching is importable and statically-checked
It doesn't seem to be working well for me, constantly using ~20% of my CPU and lagging pretty hard. Switched to Atom, but it looks like GHC-MOD in Hackage won't build in the latest version. Gotta compile it from the repo.
Trying to find a guest and schedule the next one.
sounds like an incredible job i will never have. 
&gt; That way if I google a package I should end up at the latest version. Yes I agree this is a major pain. I'm not sure how to fix this however as I know very little about SEO. &gt; Frankly I never keep track of which version of base is for which version of GHC. It's another annoyance. Someone needs to like make a table of base vs GHC version or something. Better yet, just add "packaged with GHC X.X" in the synopsis of `base`. &gt; This is a pretty minor issue, but sometimes I want to provide someone a link to a specific function in a package. That feature has been merged into Haddock just a few days ago :P But yeah, I think there's a lot that can be done to improve the UX of Haddock and Hackage. It would best if you submit some tickets outlining them so people can discuss and try to implement them! - https://github.com/haskell/haddock/issues - https://github.com/haskell/hackage-server/issues
While reading your post, I started thinking about things that would help. I've suggested before that contributions to the Haskell Wiki would be a good way to get involved. Another idea is for library developers to solicit volunteers to document their libraries, supply examples and maintain the documentation and examples as the library evolves. For anyone who has written a helpful blog entry about using a particular library, please consider contributing it to the library's maintainer to add to the distribution. While many computer science professors and their graduate students work on adding new features to GHC, the professors might consider asking their undergrads to write documentation and examples for class credit. I also hope the *Real World Haskell* authors and O'Reilly will consider an updated edition.
Try ocaml or F#, then move to haskell
http://m.youtube.com/watch?v=KxGRhd_iWuE
Could you please elaborate more on 3rd point? Preferably with some type signatures:)
Word is, /u/davidchristiansen made some example in Idris during one of Servant talks. Never seen it though.
I've been a one-man thing my whole career (tech art), so I made all the Python (and MEL (the horror!) before that), and everything was clean, and always getting cleaner. Then I found Lisp, and Haskell, and FP, and math, and Vim, and Linux, and Git, and all this amazing stuff, and started to not like Python anymore, as FP thoughts cleaned my efforts and reduced them even more, down to incredibly composable lambda one-liners for virtually everything, while the key-based language of Vim had me moving at speeds I'd never thought possible. Now I'm finally sliding into a dev position with a team, and omg my eyes. The MonoDevelop does nothing! C# in Unity is pure insanity. The team is great, so there's that. The one guy has been helping a lot, and sat with me for hours today, and actually working with people is as fun as I thought it would be, but holy hell, I don't know how anyone can work like this. He even kept saying "Yeah, that sucks," and "Welcome to Unity!" Everything is everywhere. Methods have no inputs - 13 methods in this class, and they all work not just on shared state, but on state from other modules that isn't even implicitly imported or referenced in any way. Everything from everywhere is just in the global space. I have to right-click on everything and ask "Go To Declaration" in MonoDevelop, or "Find All References," and 10 hops later, I don't even remember what I'm doing anymore. It's like Wikipedia, except I'm going to get fired if I don't get out of Philosophy, back through Quality (Philosophy), Consciousness, Awareness, Knowledge, Science, Natural science, Measurement, Radiometry, etc., and get back the sprite I was trying to figure out how to rotate 3 hours ago. There are opto-isolators everywhere - called prefabs - which have GUI-only entry fields you can drag objects and scripts onto, and any time these show up in the call chain, I can't find anything beyond that point. It's like the program waded through a small stream to throw my dogs off the scent. "Find References" - None. Then how is this working? I was going to bring the codebase over to Linux to put some real power into graphing out call chains and such, but these effectively kill that plan. And *just* for fun, an "OnEffectEnd" callback today seemed to be referenced from absolutely nowhere, but it turns out it was 2 screens to the right of my screen in a very long chain of arguments to a hashing function handed to a tweening library, which takes an endless number of arguments, and pairs them up in chunks of 2, making the first the key, and second the value. The key and value in this case were "onComplete" and "OnEffectEnd", both strings, and MonoDevelop couldn't find them. I found them by accident while wondering what all this stuff to this long call was, 2 days after wondering how "OnEffectEnd" worked when nothing seemed to reference it. plzcanihazhazkul
As that ticket says, the most active (these days) developer on ghc-mod had it working on 7.10 7 months ago. The relevant changes were merged into master some months ago. It wasn't on hackage, but it's worked for over half a year. It's fine if you want to reject the common tooling choice because it's unacceptable to you. I'm just asking that you give the folks who work on those things a little more credit. I don't know why you see it as okay to assume Windows and VS for .NET development, and Eclipse for Java dev, but are angry at people who use a different tooling setup for another technology. You absolutely can use Haskell without emacs or vim, but to do so is to deliberately take a less-traveled road, the same as .NET developers working on Linux or Java devs who've sworn off Eclipse. I guess you can be angry at the handful of people who work on less-used tooling options, but that really seems counter-productive.
Check out how Django solves this by displaying the version in the bottom right and allowing you to change to other versions. There is also a warning if you are on development versions.
F# is quite nice, if it's an option. 
ghc-mod is pretty inadequate compared to most other languages, even those without IDEs. Look at what Rust has in racer, for example, or Agda and Idris with their hole-driven development.
I followed the `ghc-mod` thing for a while. Daniel did fix it but it was tied up with a bunch of other changes that caused bugs. His response at one point was: &gt; The workaround is basically to downgrade ghc for now as work has me rather tied up at the moment so I don't have any time to fix this properly, sorry. The GHC 7.10 support is all mangled up with a whole lot of other far reaching changes in my fork so It's not really feasible to release anything without finishing all that up first. (See: master...DanielG:master) &gt; &gt; - DanielG commented on Mar 31 It was finally fixed around May but as you say, not on Hackage. &gt; It's fine if you want to reject the common tooling choice because it's unacceptable to you. I suppose my complaint could be rephrased as "I wish the Haskell tooling that we do have was easier to learn". But it doesn't look like Emacs/Vim will ever be easy to learn. I'll compare Haskell to Java - for Java there are 3 main IDEs all of which are incredibly easy to set up and user friendly. Haskell has Emacs/Vim, they are hard to set up and people generally want to avoid them. The attitude from the Haskell community then seems to be "so what?" / "if you don't like it, too bad". As I reflect in my blog post - this causes a poor experience for new Haskellers. We're frequently told to suck it up and learn Emacs/Vim as that's the dominant tooling. It would be easier to learn Haskell if the dominant tooling didn't require such a massive learning curve. I said it in my previous comment - if the truth of the matter is that Haskell is best developed in Emacs/Vim then perhaps we should (a) advertise that fact better, and (b) make learning those approachable for people who are also about to start learning Haskell.
I think the background build needs to be tweaked a bit for different OS and hardware configs. Also I think the way it polls open files for changes on disk may be causing problems. * What OS are you running? * Do you have an SSD drive? * Does the idle CPU usage go down if you close all open files `File -&gt; Close All`? * Does the idle CPU usage go down if you turn off background builds (it is the button to the left of the green tick on the toolbar) and use `Ctrl+B` to start builds (`Command+B` on OS X)?
So I've heard, but I don't think it's an option. I'm on a team now, and I think they're all quite happy with C#.
And this is why there is a lot of work invested in [ide-haskell](https://atom.io/packages/ide-haskell) for Atom.
OH YEAH! Thank you!
Please note that documentation is not generated by the default doc-builders that way yet. I've manually generated hyperlinked docs with a recent Haddock snapshot for a couple of GHC core libraries to expose this feature for testing by a wider audience.
Looks good. In fact, with that change I think I'd rather browse sources by default!
This is incredible, great work.
ghc-mod isn't really a hard requirement for editing Haskell, it's a nice-to-have, but plain Haskell-mode works just fine by itself. 
https://moz.com/blog/canonical-url-tag-the-most-important-advancement-in-seo-practices-since-sitemaps I think this can be used to have google pick up the latest version of a package.
Windows 8.1 update 1 all patched up to recent, it's a pretty newish Lenovo W540 laptop, fastest CPU they offer with 32GB of ram. I have an 1TB Samsung 950 SSD I'm running this on. I'll have to try 3 and 4 back when I'm in the office tomorrow 
Really nice talk though I think the original title that he proposed is a lot more 'exciting' and 'informative'. I also think that it's too limiting that this 'just' improves EDSL's. It's a way of improving haskell error messages in general. Of course the line between a library function and an EDSL in haskell is very small, but I think the benefits should still be generalized. But yeh his points are strong. EDSLs should focus on one domain and should be easy to understand in that domain so that even a novice could work on it. But once you get a type error, the abstraction is broken and people are faced with the dirty reality that their EDSL is just haskell with scary error messages. I love that people are working on a solution to that.
I'm enjoying learning from _Beginning Haskell, A Project Based Approach_. It starts out right away introducing the ghci app, and installing packages. It doesn't seem to suffer from the problems mentioned by the OP. I've wondered about [Helium](http://foswiki.cs.uu.nl/foswiki/Helium/WebHome) and whether it's UX features could be integrated into Haskell.
Actually Standard Chartered are hiring.
I've never installed yesod but have had plenty of cabal installation woes. I was one of the people who responded to the survey saying that cabal install was the worst thing about haskell (by far). Stack is much faster and works. I like it. 
Indeed, TH is not strictly needed for singletons but you end up writing a lot of boilerplate yourself if you don't use it.
Argghhh! Everything I know about this is written on /r/haskell or linked from it. I could look it all up for you again, but I already summarised it in some other stack-bashing thread here, and perhaps I need to start a blog just so I can send people to a set of links, but the depressing thing is that there are people who unlike me clearly haven't been avidly reading for news of improvements to package installation on reddit are pouring skepticism on stuff that's linked right in this subreddit. 
&gt; I'll compare Haskell to Java - for Java there are 3 main IDEs all of which are incredibly easy to set up and user friendly. Haskell has Emacs/Vim, they are hard to set up and people generally want to avoid them. Wait what ? I have briefly used eclipse, and have used intellij a lot more. With eclipse, there are tons of plugins you have to install to be productive with, and as a result there are many distributions. I stopped using it very quickly as it was so slow. And believe me, having to wait for characters you just typed to appear on screen *isn't user friendly* ! Idea intellij isn't as bad, even though it still feels slow coming from vi, and I ended up finding it quite usable for java/scala work once I had it properly configured. I had help from an experienced user though. This was definitely *not* easy to setup. And it is not user friendly either : tons of options are stored in non intuitive places, I still struggle to find the place where you choose the JDK you want to use, it is a huge and convoluted piece of software.
To expand on this : it is like people finding MacOS "more intuitive" than Windows. I am used to Windows and don't find MacOS intuitive at all. And I only find tiled window managers to be ergonomic at all. The most user friendly thing is often the thing you used first, or the thing you enjoy the most, often for cultural reasons.
You're objecting to considerate language choices. You're also doing the "You're probably wrong unless you can prove it with lots of science and data" form of argument. It positions you as aligned with rigorous fact checking, whereas on your side of the argument (as in most arguments on both sides of any debate), there are unevidenced assertions and anecdotal examples. (IIRC, those were from an Internet argument which had already descended to the irrational "that's like the Nazis" point. ) Pretty much uniformly, "show me the evidence or I can't believe you" is a hypocritical gambit. The non - hypocritical version is "look, you really need to read this paper from this respectable journal where they did this study and found that overwhelmingly,..." 
&gt; There is also a warning if you are on development versions. And on deprecated versions! Not sure what that'd mean in a Hackage context but still something to consider.
&gt;EclipseFP is dead This is a misleading statement. IMO, EclipseFP is still the best solution for Haskell development by far. It doesn't currently have a maintainer, but that doesn't mean it couldn't get one and that also doesn't mean EclipseFP suddenly stopped working.
Well, that's what intuitive means, which makes it a fairly useless description. Something being intuitive means that the person doing it already know how it works – most commonly from previous experiences with very similar things.
&gt;You're objecting to considerate language choices. I'm really not. I don't object to that. OK? Acknowledged? &gt; You're also doing the "You're probably wrong unless you can prove it with lots of science and data" form of argument. It positions you as aligned with rigorous fact checking whereas on your side of the argument (as in most arguments on both sides of any debate), there are unevidenced assertions and anecdotal examples. (IIRC, those were from an Internet argument which had already descended to the irrational "that's like the Nazis" point. ) This isn't fair to me. I *very explicitly* stated, three times in this thread, that I was not claiming that my experiences and intuitions constitute proof or that they ought to be convincing to other people. I don't know what could be further than that from "position[ing] [myself] as aligned with rigorous fact checking." &gt; Pretty much uniformly, "show me the evidence or I can't believe you" is a hypocritical gambit. The non - hypocritical version is "look, you really need to read this paper from this respectable journal where they did this study and found that overwhelmingly,..." I'm just being honest. I don't believe it and I said why. And given that I was very deliberate about saying that I didn't claim to have proved anything, I do think you're being completely unfair here. I'm the one being asked to believe something, and I am explaining why I remain unconvinced. By the way, this is a very good article: http://foucault.info/foucault/interview.html The conversation here reminded me of it.
Especially since `haskell-mode` has recently gained navigable error overlays.
I guess the redirect confuses the Google ratings for that URL. :(
I've tried that, in 5 minutes I've found at least 3-4 annoying bugs... 
I´m using the 0.13.4.2 with windows 10 just actualized and surprisingly leksah run a lot better than with Windows 8. It runs fast as never before. I switched to this relatively old version since from the 0.14 on has the CPU problem you mention while in this version the problems is less evident. I will soon try to run the latest version of Leksah in windows 10 to see if the speed improvement also happens. Maybe some bad thing in the GUI toolkit and the windows 8 caused the problem
Hamish: I´m using the 0.13.4.2 with windows 10 just actualized and surprisingly leksah run a lot better than with Windows 8. It runs fast as never before. I switched to this relatively old version since from the 0.14 on has the CPU problem you mention while in this version the problems are less evident. I will soon try to run the latest version of Leksah in windows 10 to see if the speed improvement also happens. Maybe some bad thing in the GUI toolkit and the windows 8 caused the problem 
I can recommend [hcoop](https://hcoop.net/), which is a democratically run hosting service.
Hamish: I´m using the 0.13.4.2 with windows 10 just actualized and surprisingly leksah run a lot better than with Windows 8. It runs fast as never before. I switched to this relatively old version since from the 0.14 on has the CPU problem while in this version the problems is less evident. I will soon try to run the latest version of Leksah in windows 10 to see if the speed improvement also happens. Maybe some bad interaction between the GUI toolkit and windows 8 caused the problem 
&gt; "Pure FRP" has no known (to me) efficient, leak-free implementations That is not the case anymore. Eleara was the first library I know that had no time leaks. Sodium and reactive-banana have been free of time leaks since then — they do not remember history.
By you wanting to use the newer version in your project, but a dependency makes an upper limit at a lower version.
&gt; The Developer is interested in examples, tutorials, use-cases, etc. Here’s an example of what the Developer might want – an tutorial on how to connect to an API and parse a deeply nested JSON response. Here’s an example of an excellent tutorial that answers just that – a wreq tutorial. Well, the trouble with Developer tutorials is that they are very transient. For instance, I had never heard about the `wreq` library until I clicked on your link, it seems to be new. Chances are that in 2 years, the current tutorial will be wholly outdated. (Of course, chances are also that Bryan will update it. :-)) It's not that Developer tutorials are not needed, but that they are short-lived. The Academic, however, learns for much longer time scales. Monad and Applicative are here to stay for the next decades. Understand them once, and you have learned the key aspects of many libraries. Parser combinators are probably a good example -- many libraries are variations on the same theme, but understanding the theme gives you quick access to all of them.
&gt;&gt; * "Pure FRP" is an interface (think ML) which has a denotation where `Behavior a ~ RealTime -&gt; a` are continuously varying values &gt;&gt; &gt;&gt; [...] &gt;&gt; &gt;&gt; * Sometimes people drop continuous time behaviors (essentially: they enable the act of sampling to participate in the FRP interface) as this leads to simpler implementations. It's probably wise to not call these "FRP" exactly, but they're clearly related. &gt;&gt; &gt;&gt; [...] &gt;&gt; &gt;&gt; * "Pure FRP" has no known (to me) efficient, leak-free implementations I think the above quotes make it clear that /u/tel uses "pure FRP" to refer to "continuous", or "sampling-rate independent" FRP. &gt; Eleara was the first library I know that had no time leaks. Sodium and reactive-banana have been free of time leaks since then I don't think Elerea, Sodium and reactive-banana are examples of "pure FRP" in the sense above. And that's why we should stop using confusing terms like "pure FRP" and "true FRP", and instead use "continuous FRP" or "sampling-rate independent FRP" :)
What I can tell you is that, in my case, the ~20% cpu problem was probably related to I/O. Unfortunately, I didn't use a file monitor to see what was going on and now the problem is gone.
We have been using opaleye in production at [Silk](http://www.silk.co) for about 6 months now and we are very happy with it (hence I haven't looked into relational-record). We haven't run across any important bugs in opaleye itself in this time. There are also other professional users using Opaleye and producing a steady stream of contributions. I'm confident that others (including us) would step up if Tom stopped maintaining it. Here is my review from the release, I think most of it still holds: https://www.reddit.com/r/haskell/comments/2nxx7n/announcing_opaleye_sqlgenerating_embedded_domain/cmi1uze I'm also spending extra time here and there trying to release the opaleye wrapper-library we use internally. It adds more type-fu and TH to make it easier and safer to write queries and handle transactions, I'll post it when it's ready. 
I agree, starting out was pretty daunting. Our library solves this problem using magic ;-)
Thanks
I'm using Opaleye in production and not as happy as everyone else with it. Left joins don't infer at all, which leads to huge type annotations - to the point where when I have to left join I just say "eh, screw it" and end up just writing out SQL by hand. I also find it requires quite a lot of boiler plate to actually get setup. As I don't want to use Opaleyes somewhat unique approach to records (where every field has a type parameter) this leads to another data type for every table, a bunch of type aliases, a coercion function and the table definition itself - something between 50-100 lines. I don't really want to bring arrow notation into my project, as I didn't have a great experience with it when writing FRP systems. This might be a bad choice, because working with just `fmap` and `Applicative` leads to a lot of tuple-packing and unpacking. For example getCreditPackOffersByCustomerProfileId cPId = fmap (fmap Db.interpretCreditPackOffer) (runOpaleye (Op.orderBy (Op.asc Db.creditPackOfferSalesStrategy &lt;&gt; Op.asc Db.creditPackOfferNCredits &lt;&gt; Op.asc Db.creditPackOfferPriceAmount) (Op.unionAll (Db.restrictJoin (\cpo pc -&gt; Db.profilesCustomerBusinessId pc Op..== Db.creditPackOfferBusinessId cpo) (Db.restrictWith ((Db.lit CreditPackOffer.Standard Op..==) . Db.creditPackOfferSalesStrategy) Db.creditPackOffer) (Db.restrictWith ((Db.lit cPId Op..==) . Db.profilesCustomerId) Db.profilesCustomer)) (Db.restrictJoin (\cpo pc -&gt; Db.profilesCustomerBusinessId pc Op..== Db.creditPackOfferBusinessId cpo) (Db.restrictWith ((Db.lit CreditPackOffer.InitialCreditPackOffer Op..==) . Db.creditPackOfferSalesStrategy) Db.creditPackOffer) (Db.restrictWith (\pc -&gt; (Db.lit cPId Op..== Db.profilesCustomerId pc) Op..&amp;&amp; (Db.profilesCustomerRedeemedWelcomeCredits pc Op..== Op.pgBool False)) Db.profilesCustomer))))) That said, these are the painful parts - and it's likely that the pain of writing queries like this is self-inflicted and would be avoidable if we just used arrow notation. Once the boilerplate is out of the way, Opaleye truly does work - it hasn't given me the wrong result once, which is great! The queries that have been generated have been performant, so I'm not bothered about the queries its generating - PostgreSQL works magic, as always. If I had to choose today, I'd probably give up a tiny bit of the safety that Opaleye guarantees and would choose `relational-record`, just because I can use ordinary `do` notation. Ultimately I'm not truly happy with any of the solutions, but I'm yet to present a viable alternative myself. I would certainly advise using anything instead of SQL (which I admit is contradictory, seeing as I said I write SQL by hand for `LEFT JOIN` queries!) Hope this doesn't come across too negative.
1) Conal FRP evolved in the context of real-time systems like circuit simulation or game simulation. 2) Most of the computing problems are not of this kind, but still need to respond to asyncronous things called events. 3) Talking about Behaviours in a context that is not time dependent is a mixtification that confuse rather than clarify things. It is better to talk in terms of plain events and then particularize, if necessary to timed events. Time or even real time -if necessary- can be introduced as a timestamp in the events and a state value in the computación can be used to do interpolations. 4) In this context simpler but wider, reactive is anything that respond to events. That includes things like event handling frameworks for GUIs or Web apps. But also asyncronous IO framewoks of any kind, either created with OOP or functional languages. Functional reactive would be something more strong: it is any algebra that deal with asynchronous things called events. Thus, functional reactive is made of composable elements, while plain reactive is not made of composable elements. 5) Most if not all IT problems are not related with a single reactive domain, but it is necessary to pipeline reactive domains, that I will define by examples: A game simulation may be a single reactive domain, but a distributed database is not, since a distributed database must manage asyncronous things that happens at different places, even in different machines. To pipeline reactive domains it is necessary a monad instance. A client side application with widgets that pipeline input results to other widgets is another example. 6) The association "functional -&gt; pure -&gt; no imperative look -&gt; non monadic" float around in the discussions of haskellers but it is never mentioned since -obviously- it is absurd. It is an aestetic rule rather than something rational. It is necessary to make this explicit since FRP suffers a lot from that myth. For me, functional means composability, gluing things together in precise ways with combinators with a mathematical foundation. whether have effects or not, whether look imperative or not. 7) If we want to unify so disparate things like distrubuted databases and a GUI applications, and solve them within a composable solution -and these things desperately need a common and composable solution- we need to see functional reactive as something more wide than the original formulation, and the cornerstone of an event algebra that would make these things composable is a monad instance. FRP seen globally is the functional (i.e. composable) response to the [reactive manifesto](http://www.reactivemanifesto.org/) and must be formulated to deal with all the scenarios of the manifesto. 
Thanks /u/hvr_, this looks amazing!
Discrete event streams might be a way to implement sampling rate independence, so it doesn't have to be at odds. An example where sampling rate independence would be helpful is a physics system. You might want to know the world state once a second, but the system has objects that move continuously. Some objects might need a higher sampling rate while others only need lower sampling rate. A system independent of sampling rate could have both kinds of objects and will return "correct" results at each sampling point of the whole network, regardless of how things are done internally.
I've been making is trying to ask more questions about Haskell on stack overflow, even if a I know I'll eventually figure the problem out. I think it's an easy way to contribute even if you're not an expert and it really makes a difference to beginners to have a large base of questions on stackoverflow.
If when someone points out that you use `Lazy.ByteString` you immediately rush to see why and then find that there is a better solution, is that an indication that `Lazy.ByteString` is actually "a trap"? +0.1? :)
I called a class "you guys" once, to the response "hey, we're not all guys!". I found it rather easy to get defensive when pulled up on a non-intended slight, so after apologising I explained that "guys" had lost its gender for me when a (female Australian) lecturer repeatedly used it to refer to mathematical objects like vectors, variables and injective homomorphisms. Nevertheless, keen to be welcoming to female students who are now increasingly well-represented in my traditionally male-dominated class, I simply avoid using "guys" to refer to groups that aren't exclusively male. I don't want _any_ students to feel they don't belong in my class, unless they're not prepared to work to learn. TL;DR I've come across people feeling excluded by "guys" before today. 
I feel the same about Clojure.
Thanks. Yes, Netflix has a closed private API that I use. It used to be open, but then Netflix limited it to a few select applications including instantwatcher.com. You can read the story here: http://www.engadget.com/2014/11/15/netflix-public-api-shutdown/
I guess instances would still have to leak into the outer scope.
The exercise is in [List.hs](https://github.com/NICTA/course/blob/master/src/Course/List.hs#L319), btw. So you're asking whether one can write a function `notReverse` that fulfills the following laws? * `notReverse [] = []` * `notReverse [x] = [x]` * `notReverse x ++ notReverse y = notReverse (y ++ x)` * `exists x. notReverse x != reverse x` No, because the third law is pretty much the specification of `reverse`. You gave the inductive proof yourself.
Don't confuse simple and easy. Haskell is a far simpler language than Ruby, Python, Java, C#, or especially C++. However, that doesn't mean it's easy to learn. It just means there are fewer concepts involved and their interactions are more predictable. For instance, monads are a *far* simpler concept than Rust's borrow checker. They have no corner cases. They have no escape hatches. Finally, as a working developer, I absolutely *hate* tutorials. They're nothing but lies. I *love* documentation like most of hackage. It describes what the pieces are. I'm a reasonably intelligent and experienced programmer. I can put the pieces together myself if you tell me what they do. I cannot magically derive the thousands of corner cases from a tutorial that doesn't tell me a thing about what the individual pieces do. (*cough*rails*cough*)
Package authors maintainig their `build-depends` honour the package versioning policy, and as such are trying to be good citizens by following the common agreed upon rules. The motivation of the PVP is to be able to "make up" such upper bounds in the first place. If we just relied on CI tests to infer upper bounds there would be no reason to go through the hassle to conform to the PVP altogether, as we'd just need a monotonically increasing versioning scheme (and fixup wrong upper bounds after the fact) rather than the elaborate PVP-scheme which requires ahead-of-time effort.
Looks good to me!
To give another reading suggestion (atop /u/adamgundry's), "Reflection without remorse: revealing a hidden sequence to speed up monadic reflection" discusses the downsides of both normal monadic sequencing and of the codensity transformation and provides an asymptotic (but not yet practical) improvement.
&gt; And the others mentioned don't offer full IDE functionality - mostly just syntax highlighting. If only they did at least the syntax highlighting well. I have found that Sublime Text 3 (without any Haskell syntax highlighting packages) and atom with ide-haskell have multiple problems with syntax highlighting my Haskell code. Don't remember if Leksah had any problems like that, but I uninstalled it for other reasons. The only IDE (or pseudo-IDE in case of Atom and Sublime Text) that I found had good Haskell syntax highlighting was IntelliJ with some Haskell package/addon.
When I last looked at that, admittedly some time ago, I recall it could not do type safe joins. Has this changed?
There's [regex-applicative](hackage.haskell.org/package/regex-applicative) and [polyparse](https://hackage.haskell.org/package/polyparse) (which IIRC does full backtracking). The reason I've tended to prefer happy is ambiguity detection.
We [solved](https://github.com/haskell/hackage-server/issues/284#issuecomment-111265364) half of the problem with a canonical link tag in the header - the other half would probably be making the `latest` page url directly display the latest content, instead of a just a redirect (the Node.js docs do this well, e.x.: https://nodejs.org/docs/latest/api/fs.html)
Because SQL is just a String to the Haskell compiler, which means you don't get any guarantees under refactorings, or even a guarantee that your query is well-formed at the point of compilation. The most frequent bug we used to have in production would be making a change to a database table, and forgetting to update a query somewhere - only to find out at runtime, or worse - a customer finding out at runtime. That's unacceptable, given what this language *could* offer in terms of guarantees. `relational-record` and `opaleye` do not replace a nice API, but they facilitate its creation. Notice in my paste above that big query is itself a function that operates in some monad with access to a database connection. The caller doesn't know anything else about what's happening.
Pedantic but also interesting note: it's Opaleye, which is the name of [a fish](http://www.pierfishing.com/fish_of_the_month/opaleye.html) :) 
Ghc mod has a limited form of hole driven development. The difference is more that those languages can infer more about the holes due to their type systems 
&gt; One question: using reflex, are you able to compile your code also to native applications? Yes. The reflex-dom library has made a strong commitment to always build both with GHCJS and with GHC to a native app.
`esqueleto` definitely can.
You can, however, create a simple GADT wrapper around `Vector` that can be an instance of `Foldable`: {-# LANGUAGE GADTs #-} import Data.Foldable as F import Data.Vector.Unboxed as U data WrappedVector a where WrapVector :: Unbox a =&gt; U.Vector a -&gt; WrappedVector a instance Foldable WrappedVector where foldr f z (WrapVector v) = U.foldr f z v example :: Vector Int -&gt; Int example = F.foldl' (+) 0 . WrapVector This trick works with `Foldable`, but not `Functor` or `Traversable` (for an explanation, see [here](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/DeriveFunctor#RelaxeduniversalitycheckforDeriveFoldable)).
I'm using Rx to do this sort of thing. To handle it much of the system is parametric in the "scheduler" being used, but this is pretty difficult to enforce, easy to screw up, and confusing. In practice what it means is that you try as hard as you can to think of most of your code as being continuous—but you call it "evented" and just ignore the existence of schedulers—to push scheduling complexity to the borders. You also sort of hope and pray a lot.
Yeah I was able to reproduce that to 1GB in less than 10 minutes: http://imgur.com/vMnAx83 
&gt; Continuous and "denotational". I'm essentially deferring to Conal here. I spoke with him a few weeks ago and he confirmed that in his opinion there weren't any implementations which are "just right" ane space leak free. So this means it is still an open research problem ?
Why are you avoiding arrows? I appreciate that you may have been bitten in the past, but that was quite a different domain, so I suspect that arrows here might work for you.
I think this is a very promising idea! Also, Yuras Shumovich's suggestion to limit it to the existing top level imports will be crucial. * It preserves the comprehensiveness of the existing import statement block. * It simplifies the instance story that the thread raises. (I highly doubt the other idea in the thread of importing only names but not instances is viable --- I find it far more confusing than Yuras's.) * Therefore, it would allow Template Haskell to generate this new syntax without much complexity. Because of that restriction, I think reusing "import" is a misnomer. "using" etc. would be better. If you have real hopes for this, the next two steps are very likely to: * start a GHC Dev Wiki page for the proposal and open a matching GHC Trac ticket, * and then start relating the semantics to Backpack, ideally showing orthogonality/synergy.
The CPU didn't spike til I did a build and then a run on the initial program that's there when you install it and run it for the first time.
How do you refine/case-split?
&gt; problems with syntax highlighting my Haskell code. Don't remember if Leksah had any problems like that If it does, then it might not be hard to fix. The GtkSourceView spec file is [here](https://github.com/leksah/leksah/blob/master/language-specs/haskell.lang). You can edit the copy that gets installed when you install leksah and restart leksah to check the changes. Alternatively submit an issue with some sample code. &gt; but I uninstalled it for other reasons. Do you recall what the other reasons were?
As someone else suggested in the thread, I would rather require all imports be listed at the top as usual, and then introduce new syntax for selectively "unqualifying" imports. import qualified X.Y.Z as HTML import qualified A.B.C as CSS page = html where html = ... where using HTML (div, span) css = ... where using CSS Warnings should be emitted if any such "using" statement causes name shadowing within its scope. [edit] yitz guessed right about what I meant.
I'm not at a computer, but check out "Case handling" in the [emacs instructions](http://www.mew.org/~kazu/proj/ghc-mod/en/emacs.html).
Changing that behavior basically destroys type classes as they exist in Haskell. Implicits (e.g. Scala's "type classes") are a different, but related, thing. See /u/edwardkmett's [Type Classes vs. the World](https://www.youtube.com/watch?v=hIZxTQP1ifo) talk.
I'd love to read more about your experiences reimplementing the STG machine. Hint. Hint.
Cool. * What OS are you running? * Do you have an SSD drive? * Does the idle CPU usage go down if you close all open files `File -&gt; Close All`? * Does the idle CPU usage go down if you turn off background builds (it is the button to the left of the green tick on the toolbar) and use `Ctrl+B` to start builds (`Command+B` on OS X)? * Did the metadata finish loading (does the Modules pane have stuff in it)?
This is going to be one super useful feature. Thanks!
It's definitely cool. I wish the .hs-identifier colour were simply black and some of the other colours (particularly .hs-identifier.hs-type and especially .hs-comment) were made a good bit darker for better contrast against the light background. Here's an [example](http://i.imgur.com/5LvB15L.png) using #000000 for the identifier colour, #333399 for types, and #336633 for comments. To be honest, I think the comments could maybe be even darker than this for better readability at the expense of perhaps making them look more similar to code.
You're entirely right, I was trying to suggest that my avoidance is irrational in my post :) I don't have the time to go back and rewrite all this stuff now.
The point would be that, for example, in import qualified X.Y.Z as HTML page = html where html = ... where using HTML (div, span) you could use `div` and `span` unqualified within `...` (and only within `...`), the idea being that this would reduce the clutter and obscurity of the code.
Eh? I have always intended reactive-banana to be "sampling-rating independent" and I believe it is. If not, which combinator breaks this? Because in this case, I would like to fix it. Also, sodium can be used in a sampling-rate independent way. 
I've encountered much the same issues as ItsNotMineISwear. That said, I use it every day and enjoy my time quite a bit. I like scala/scalaz warts and all. Of course, I was a C++ programmer for a long time. So the ugly bits look fine in comparison? ;-) My largest issue learning scalaz was navigating the library. There is little documentation on "why" a given module exists. After a while the naming convention becomes clear, but even a short blurb on each would go a long way. Scalaz also feels like early monad transformer libs in Haskell. Everything in one big pile. I suspect Scalaz will be replaced by a number of smaller libraries.
Instances must have unlimited scope to ensure coherence. There's an example of hiding instance exports going wrong [in this SO answer](http://stackoverflow.com/questions/8728596/explicitly-import-instances/8731340#8731340). There was also [a pretty extensive discussion here on reddit](https://www.reddit.com/r/haskell/comments/1id0p7/backpack_retrofitting_haskell_with_interfaces/cb3eb2n) some time ago; see in particular /u/edwardkmett's [memorable comment: "You basically throw out the correctness of almost all of the code I have written." (with more explanation)](https://www.reddit.com/r/haskell/comments/1id0p7/backpack_retrofitting_haskell_with_interfaces/cb3ptdr?context=10000)
Thanks!
We need to do a better job of advertising this fact. Nevertheless, I'm very happy that this paper cut is finally resolved. Thanks to Randy Polen for making this happen!
http://i.imgur.com/j7dPvjH.gif
But esqueleto doesn't have a policy of being type-safe above all so a number of other things yield invalid queries.
&gt; knowing Emacs or Vim shouldn't be a prerequisite for learning Haskell This is the key point that I think you should emphasize. Both vim and emacs have historically had *catastrophic* learning/setup costs. (In my experience, emacs or rather its packages are shockingly unreliable.) They're wonderful once you get going, and have the huge benefit of allowing you to use the same tooling for many purposes, but they absolutely must not be a *requirement* to have a good development experience.
I might be the only one but, personally, I find the Cabal guide to be pretty tough to read. Every time I'm googling an issue and the top link is the Cabal guide, I start reading it and then often give up and try to find some examples in actual open source projects. The other day, I was trying to figure out how to use extra data-files in a project, and the best way I could figure out how to do it was to look through Haskell projects on Github until I found one that used the feature. I tend to appreciate examples, perhaps, and the Cabal guide seems maybe light on those? I would definitely like to have a readable, approachable guide to Cabal with lots of examples.
Thanks for reminding me that reflex-dom builds natively--I had worked out how to build it reliably on Windows, and then I completely forgot about it. I'll try to get some instructions written up soon. I want to kick this "compiling UI code on Windows is impossible" idea ASAP.
What does "denotational" mean in this context, and how does something like `reactive-banana` fail that requirement? 
&gt; I don't mean to sound angry but knowing Emacs or Vim shouldn't be a prerequisite for learning Haskell. They are not. I do all my Haskell development in TextMate, which essentially means some basic syntax highlighting and a script for formatting import declarations that I wrote myself. I have no idea how vim or emacs work and I don't intend to find out.
It's the subtitle, not an abstract. This paper is a survey overview of Haskell, EDSLs, and generating Kansas Lava code using a Haskell EDSL.
This is pretty much why the choices here are just those two, which offer type safety of query composition/construction.
Denotation means that there's a mapping from values (or syntax) of your library to another, usually mathematical domain, which preserves and reflects the meaning of your code. So, for instance, 99% of the time we denote `Double`s as elements of the real numbers and run into trouble exactly where that denotation fails. Denotation helps define what a proper implementation means and helps document meaning. Denotations can be overly complex as well and obscure meaning, thus, what is being asked for is not merely to have *a* denotation but to have a "nice" one. To my understanding `reactive-banana` does have a denotation via its [`Reactive.Banana.Model`](https://hackage.haskell.org/package/reactive-banana-0.8.1.2/docs/Reactive-Banana-Model.html) module, but that `Model` is itself not easily inspected without starting to talk about event networks. "Pure FRP" in its original description has a denotation based on `[[Beh a]] = Real -&gt; a` which is fairly remarkably simple. If a performant library could be made off of this denotation then that would be a win. It's also (reasonably) fair to describe the *specification* of "Pure FRP" or even "FRP" itself, read strictly, by this denotation.
WOOOOOHHOOOO tis awesome
This is awesome! Might it be possible to have a discussion about the color scheme, though? The beige background isn't very... happy. I am a fan of github's styling for Haskell code.
Nice! I have friends who moved over there for work. They're not programmers, but it makes me feel closer to the opportunity.
Thanks for taking the time to organize these observations and thoughts. I too thought the Haskell toolchain was difficult to get started with but it was so many years ago for me now that I'm blind to it and what needs to be improved.
I made a program in haskell a little while ago and there was a space leak that I couldn't track down - most likely because I only have a very high level understanding of lazy evaluation. My attempt at a fix was to blindly throw (!) from the BangPatterns extension in all my functions, but it didn't work very well. I'm familiar with one example where lazy evaluation can lead to undesired behavior (calculating the sum of a list using foldr). Are there any other common situations where lazy evaluation is going build up lots of undesired thunks? If so, can I have some concrete examples of when this will happen and a few possible fixes?
The question is a little ill-formed I think, because even if you didn't have seq, provided you know the evaluation order at all (e.g. if it's fixed to be lazy evaluation specifically, so outermost-first), there are various things you can do to force the evaluation of the partial sums to occur in a sensible fashion just by pattern matching. An adversarial evaluator with non-strict semantics might always be able to choose redexes poorly though, I don't know.
Interesting! Can you give an example in Haskell that would do the trick without seq, just by pattern matching?
I'd also like to see this fixed. I typically have two half-screen windows open, and this has become a problem (otherwise, the new theme looks great!). Thanks for taking the time to report.
Sorry, yeah, that's right. And yes, an adjacency list approach seems like it will make everything easier.
Thanks a lot! I had no idea UC Sand Diego had so much FP research. I will definitely be looking into it. That Bonus video made my day!!! XD
Thanks, I have found it difficult to google FP research schools. Most results turn up courses in FP... I will look more into UPenn!
I will also be looking outside of US. A lot of awesome schools in England! I just wanted to see what was closer to home also.
Sure. It's a little bit awkward with Integer, because Integer is an abstract type for which we don't have access to the constructors, but we can still match on the result of an operation like x == 0 in order to force the evaluation of x. At one point in Haskell's history (Haskell 1.3), there was an Eval type class: class Eval a where strict :: (a -&gt; b) -&gt; a -&gt; b seq :: a -&gt; b -&gt; b strict f x = x `seq` f x and this was implemented for each type separately just by pattern matching on some constructor of the type involved. Eval class constraints polluted a fair number of type signatures, so later editions of Haskell removed the type class, instead opting to make seq primitive. This also meant that seq could operate on arbitrary function types, where formerly there was no possible working instance. (As an aside, taking this into account complicates the semantics of Haskell and rigorous equational reasoning a fair bit, because an undefined function is not the same as (\x -&gt; undefined) any longer, since seq can distinguish them. We often choose to ignore it, but it's something that compiler writers have to worry about.) So we can write something like: seqInteger x y | x == 0 = y | otherwise = y or a little more desugared: seqInteger x y = case x == 0 of False -&gt; y True -&gt; y and then use this to implement a more memory efficient sum: sum' n [] = n sum' n (x:xs) = let m = n + x in seqInteger m (sum' m xs) sum = sum' 0 Under proper lazy evaluation (outermost first reduction with sharing), this will avoid building up large summation terms, because testing whether an Integer is 0 is enough to ensure that it must be in normal form. If we had access to the constructors of Integer, we could do better by just pattern matching: seqInteger x y = case x of S# n -&gt; y _ -&gt; y This would avoid the spurious equality tests.
Great I didn't know you can apply a function and get as return a polymorphic thing (it was bugging me since I thought the return value must be something well defined like an Int or a Int-&gt;Int)
Reflex has something of a denotation in its [Pure.hs](https://github.com/ryantrinkle/reflex/blob/master/test/Reflex/Pure.hs). It might not be perfect yet, but it is really simple. The reflex test suite verifies that the actual performant reflex implementation matches the behavior of Pure. I haven't talked to Conal about it, but I wouldn't be surprised if reflex is one of the closest to meeting his criteria of continuous and denotational while also being performant and leak-free.
Yes, you can still generate bad queries with esqueleto. You can also generate bad queries with the other options as well, but in esqueleto it's more likely to happen. What I like about `persistent` is that it will build your migrations for you (usually) and that you can easily treat your database as a key-value store. I would not be surprised if the alternatives eventually beat the `persistent` + `esqueleto` feature set, but at the moment I still find it to be the easiest to use.
Do you have a sense of whether using arrow notation would alleviate the pain you mentioned above? Effectively your comment says "I used Opaleye against it's intended design and it didn't work well". That's good to know but I'd like to know what you think if you'd used it as intended.
Wow, fish of the month!
Yes, but /u/drb226 accidentally forgot "qualified" so the unqualified names were already in scope without having to use "using".
How can the be visible outside their scope? They close over variables that are only dynamically known.
Could be of interest to you: http://apfelmus.nfshost.com/blog/2013/08/21-space-invariants.html
I think Ryan was working with Conal on this actually.
Thank you! That answers my original question pretty thoroughly.
Wesleyan University. 4/6 of the [CS faculty](http://www.wesleyan.edu/mathcs/cs/faculty.html) do PL research and it sometimes smells like they're doing homotopy type theory up there if your into that sort of thing.
Dang. I hadn't checked since 0.4.0.0 came out, but it looks like you've fixed the issues I was thinking of. Good work! I look forward to seeing this get better and better, and I've got my fingers crossed for [#37](https://github.com/tomjaguarpaw/haskell-opaleye/issues/37).
I just spent an hour building Leksah on NixOS (one of the packages it depends on does not cabal build so a patch was needed: https://github.com/obadz/nixpkgs/commit/d654eace270cde24c4d5bdc847198399b8d58c61) Was very disappointed when Leksah crashed 3 times before I could open a file and then it crashed again when I got to File-&gt;Open. I gave up :( 
This is the case with so many subreddit themes that I had started thinking it was a problem inherent to reddit.
Yes, this happens on a very large number of other subreddits.
&gt; No, because the third law is pretty much the specification of reverse. Well not quite, if it wasn't for the second law you could just write `notReverse _ = []` and be done with it :)
&gt; working with just fmap and Applicative leads to a lot of tuple-packing and unpacking Could you say more about the tuple-packing and unpacking? I couldn't actually see any in the code you posted.
Hence my sly caveat: "pretty much". :)
Yup, for now we have settled that reverting to previous theme is a good idea. If you have more specific suggestions, you can join discussion [here](https://github.com/haskell/haddock/issues/424).
Ah, yes, I see what you mean. Thanks for clarifying.
Thanks; that's very clear. The `Reactive.Banana.Model` module preserves the meaning of an event network, but doesn't really enable an interpretation in terms of `Time -&gt; a`.
Thanks. It is interesting to me that the responses in this thread are about Opaleye, not relational-record. To be fair, the Japanese time zone has not been active in the life of this thread yet, and I am not sure how many users of relational-record from that region hang out here. I have the impression that both are actively used in production, and both have the type-safe property that is so important to me, having been bitten by enough string sql bugs in my time. And right now Opaleye wins it on community. But I will continue my experiments, thanks for taking the time to respond.
Maybe relational-record users don't hang out on English speaking sites. I don't know.
If you need it outside the `where` clause and it doesn't reference local bindings, why not just move it to top level outright?
`using` is currently used in Control.Parallel.Strategies for what it's worth, though I do agree it's very mnemonic. Granted, I'm not sure it's ambiguous, if the `using` clause can't be introduced in an expression position.
This is something we're going to handle in GHC itself, because the LLVM backend is a component of the compiler and needs quality assurance controls on *our* end (not just for your benefit, but for ours too!) See the plan I wrote up a few months ago: https://ghc.haskell.org/trac/ghc/wiki/ImprovedLLVMBackend I need to actually execute on this, but it's a bit of boring work in some places to get it all started.
&gt; Asimov's "Profession" Heh... so, close to 20 years ago (I was 20 or 21) a manager had a chat with me. My performance was best described as "sporadic" and I was under no illusions to the contrary. But this wasn't a review, it was actually a pep talk. Among other things, he described an Asimov story he'd once read, and told me how I reminded him of it. I wasn't at that job for terribly long, but the conversation really stuck with me, to the point that his doctorate in philosophy spurred me to pursue higher education on that same major. I never did look up that story he'd mentioned, though. Thanks. :)
I first read the story as a boy around 1960. It stuck with me since then as well.
I suggest Chapter 7 of the book "Thinking Functionally with Haskell". It's rigorous due to its mathematical reasoning and proves. And you'll see why Kamatsu's answer works. The book is a very good read except for some minor formatting problems in some the equations of the kindle version. * 7 Efficiency 145 * 7.1 Lazy evaluation 145 * 7.2 Controlling space 149 * 7.3 Controlling time 154 * 7.4 Analysing time 156 * 7.5 Accumulating parameters 159 * 7.6 Tupling 164 * 7.7 Sorting 167 * 7.8 Exercises 172 * 7.9 Answers 175 * 7.10 Chapter notes 180 
When you lift them to top-level, stick all arguments of the surrounding function(s) at the front, perhaps. (Since Agda was brought up, as well as instantiating parametrised modules there, you can also use functions directly by passing the module args before the function's own. So, that, I guess.)
Yeah, that's what I meant. Haskell just handles "multiple" arguments as tuples, which makes sense given function composition and all that. Although it's hard to not say this function takes multiple arguments: divide x y = x/y 
When you write something like: f x y = e The compiler translates that to: f = \x -&gt; \y -&gt; e In other words, it's really a function of one argument (x) that returns a function of one argument (y) that returns an expression (e). That's why you can apply the above function to one argument at a time.
Very interesting. Is `seq` still useful for function types (like if you were doing a left-fold where your accumulator was a function)? I'm curious if there is any fundamental reason to allow it for functions, or if it's simply too ad-hoc to type it in a way that works for only sums/products/primitives but not functions.
But your original question was whether such summing can be done /without strictness/ within O(1) additional space (unless I misunderstood you). This answer doesn't refute the assumption you're questioning, just demonstrates how to get strictness in Haskell. What am I missing?
I'm not discounting that possibility, but I've found cabal/stack flawless for all of my projects too. Many of these systems have the same theoretical flaws but it doesn't crop up as often. I also think that Haskell's level of abstraction aggravates the problem since so much code can be abstracted into libraries. I think the common dependency graph in Haskell is much larger than in other languages. Most languages don't let you import new types of "for" loops...
Where does the `HasX` classes go when you do this? I'm assuming you would want multiple types to implement the class, so does `makeFields` check if the `HasX` class has already been generated in some other module that has similarly named fields?
Hey, is it possible to get a taste of what type of contracts come in?
When working on an amortized persistent data structure, I wanted laziness. I used the evaluation strategy of the STG machine for laziness. In C. It was pretty painless. Since then I've used the same trampolining and evaluation strategies as a general pattern. The thunking layout can come in handy in some concurrent problems. It also makes complex state machines easier to describe.
Ghc-mod building on this one??
you should definitly try that. Or atleast put the "length lng" and "length png" functions into a where clause to guarantee some sharing. But vectors should basically annihilate any performance problems from that function. In a less than related note: You could consider reducing code duplication by combining new_watch1i and new_watch2i. Not entirely sure if that would be more idiomatic though - I'm still relatively new to Haskell.
for the second question, it depends. an object (not just lists) is shared in memory if it's given a name. like a function argument or a let binding. e.g. sum (map (^2) [1..1000] ++ map (^2) [1..1000]) -- (1) let xs = map (^2) [1..1000] in sum (xs ++ xs) -- (2) in (1), the expensive list-building and mapping are both duplicated. in (2), the list is evaluated to [1,4,9,etc,1000000] only once. and in between: let ys = [1..1000] in sum (map (^2) ys ++ map (^2) ys) -- (3) since ys are just [1..1000], the building-up is cached but the mapping is rerun. Google "Haskell sharing" for more info :)
http://acowley.github.io/Frames/ is a tutorial (very readable) that compares the frames library with R and Python.
Some sort of field punning thing for module scope does sound like a good idea.
Out of curiosity, what makes you prefer reactive-banana/frpnow over reflex?
This is something that relational-record does. It connects to the database and pulls the types out from it for each table defined in the haskell code. See defineTableFromDB used in https://github.com/khibino/haskell-relational-record/wiki/Tutorial
LTS 3 using GHC-7.10.2 should also come fairly soon from what I heard.
Yes, this helps. I think maybe it gets messy faster than the instances I now write for mtl-unleashed
I don't really know. But it wouldn't work anyway if you have `HasX` defined in two distinct modules that don't know anything about each other and then import them both at the same time ...
No, unfortunately the ghc-mod on Hackage still does not compile with GHC 7.10.
...when you upload a package for the first time, please make use of the package candidate upload feature at https://hackage.haskell.org/packages/candidates/upload &gt; *Package candidates are a way to preview the package page, view any warnings or possible errors you might encounter, and let others install it before publishing it to the main index.*
I don't believe ghc has constant time pattern matching. The time will depend on the depth of the pattern. Also, to avoid exponential code size it uses an algorithm that will perform redundant tests for certain patterns. 
No, `sumAcc 0 (x:xs)` matches the last case, `sumAcc a (x:xs)`.
It's just, if the claim was made (I haven't read, but you say it was) that *provably* it was impossible to do in a lazy environment, and it turns out so incredibly easy on Haskell (where Laziness, the only kind I understand, has to break when some result is needed) looks fishy... Certainly the mentioned 'proofs' meant something else O.o
Will it become stack-aware? I'm on the phone so cannot find the references to the github issue for this, but there was quite some discussion and people working on various bits.
I can't speak for the ghc-mod devs, but I understand the process is to first stabilise what is there to work with GHC 7.10.2 (and all the way back to 7.6, maybe even 7.4), and then look at new developments such as stack. I understand there will be full time dev work on it for Aug/Sep
Could you post a [Github gist](https://gist.github.com/) of the resulting `Pandoc` data structure? I think you should be able to just filter out the information, see [Pandoc scripting](http://pandoc.org/scripting.html).
It compiled for me out-of-the-box.
I have not really tried `reflex`, mostly because I'm happy with `reactive-banana` and `frpnow` is not a huge step away from that in terms of its API. You should probably stop reading there, becaue that's all I'm qualified to say, but... for `reflex` but I was a little put off by it's large API, partly inflated by the need for `fmap`-like functions on things that aren't Functors (or at least, weren't when I looked, I understand Ryan has improved stuff since then). I'm also put off by the fact that almost everything is built around type classes, which makes it hard for me to jump in and get going with it. These are just barriers to entry though, it looks like a powerful FRP framework that many are having success with, that's building a good community, and is continuing to grow. I'd say if you are entirely new to FRP, `reflex` is well worth your time to look at.
Have you looked at https://halcyon.sh/? Halcyon builds on top of and integrates well with `cabal-install` (unlike Stack) while extending it with the ability to setup the needed environment including GHC.
Such threads/comments will inevitably come up as long as we have developers publishing to the same Hackage package pool with a disagreement over the PVP. The disagreement is actually a good thing, as it keeps us aware of the issues. That said, if you publish packages to Hackage, do follow the frickin PVP **or** get the PVP changed. Hackage is grown to a point where we need rules that are followed, or it will degrade to a point where the Cabal solver will be completely incapable of finding any valid install plan without the help of Stackage and the likes.
Yes, hopefully by next week if all goes smoothly.
On that note, how would one start to debug a LLVM related bug?, my OpenGL app works fine without LLVM, but just locks up using LLVM backend.
I think it helps, because it is basically packing and unpacking tuples for you - behind the syntax.
Oh right yes, all the `liftA2 (,,) ...` and then `fmap \(x,y,z) -&gt; ...` is brushed behind the scenes in arrow notation.
If laziness is so great, why are over 50% of "Real World Haskell" and of "Learn You a Haskell for a Great Good!" about programming in the IO monad?
Ah well. That was my mistake. Somewhere inside of huge composition there was a mistake, so by mistake it was read by wrong function. After I've used `readOrg` only - everything worked and then I found a mistake in my code. Sorry for false alarm! 
&gt; put the "length lng" and "length png" functions into a where clause to guarantee some sharing. I don't understand. What exactly would be shared?
Did you use TDD or any other way of stating what you expected to happen (and *not* happen) as a result of running your code before you wrote it?
Nice write-up! I liked it a lot. The last point about re-ordering method calls is exactly the idea behind [Pureli](http://soupi.github.io/pureli/) (in my case, running them it parallel).
Are there any plans already on how long LTS 2 and 3 will be supported or is LTS Haskell still in the experimental phase where no commitments of that kind will be made?
Yes, but exhaustively testing that no side effects occur is nigh on impossible and I find types to be much more convenient.
Currently I don't think there is a schedule, just as ghc releases are not time-based. GHC 7.10.2 was released last week and Stackage Nightly has been on ghc-7.10.1 for while now: the general plan was to move to LTS 3 once 7.10.2 was out. The answer may depend on your definition of support. :) We plan to stop updating lts-2 and I imagine lts-3 will be updated weekly until lts-4. But as Stackage matures and gains broader adoption I would not be surprised if this evolves down the road. At the same time many upstream package maintainers are less likely to target updates at older LTS versions anyway. If there is strong demand maybe monthly updates to the previous lts version could be considered. This is my take anyway.
Not sure what you are asking. A decent URL library would be able to parse most URLs, which means being permissive (like a web browser), unlike say what the standard Java library does. Then there are IRIs which are mostly not considered by anyone and various other incarnations related to character encoding.
I'd have trouble coming up with a really good example for that off the top of my head, but certainly programs exist which would be made more efficient by using seq on functions. Evaluating something of function type *can* do real work, and affect the space behaviour of the program.
What do you want to do? You can easily construct URLs with the `network-uri` library. You can always use a smart constructor to get rid of the boilerplatey stuff like "http" and the website's name.
Well, you are right in that it's not necessarily obvious that the implementation in `Model` describes continuous-time semantics — it does look discrete, after all. :-) I think I should write down an argument at some point that these can be equivalent.
Ooh, I like this. Hover over a URL, and it does an AJAX request to the URL, but suffixed with ?hover=true or something, which just fetches the relevant information and displays it in a tooltip.
Thanks! Unfortunately it's not a -real- language, just a toy :)
The servant peoples and I have been considering moving to https://github.com/Soostone/uri-bytestring We need it to do a few more things, but it seems -- prima facie -- nicer, and less burdened by history. 
I think it might be interesting to read Edgar W. Dijkstra's 2001 [letter to the members of UT Austin budget council](http://www.cs.utexas.edu/users/EWD/transcriptions/OtherDocs/Haskell.html) here: &gt; I write to you because of a rumour of efforts to replace in the introductory programming course of our undergraduate curriculum the functional language Haskell by the imperative language Java, and because I think that in this case the Budget Council has to take responsibility lest the decision be taken at the wrong level. &gt; &gt; You see, it is no minor matter. Colleagues from outside the state (still!) often wonder how I can survive in a place like Austin, Texas, automatically assuming that Texas’s solid conservatism guarantees equally solid mediocrity. My usual answer is something like “Don’t worry. The CS Department is quite an enlightened place, for instance for introductory programming we introduce our freshmen to Haskell”; they react first almost with disbelief, and then with envy — usually it turns out that their undergraduate curriculum has not recovered from the transition from Pascal to something like C++ or Java. I don't know about the outcome of this letter.
Yes, but going forward, do you expect more than one lts for 7.10.2?
Exhaustively testing that no side effects occur *is* nigh on impossible. Luckily, we very rarely need to do that. Were you writing life-critical systems in python? Then you have bigger worries. Anyway, what we do often need to do is be confident enough that we have few enough unexpected^1 side effects with low enough impact when they do occur. And that's not so hard. There's a database connection? Test against a mock which expects no updates. There's a network connection? Test agains a mock that expects no traffic. There's a hardware driver? …you get the idea. That's not for free, of course. But then neither is devising a solution to a problem solely in terms of pure functions when an imperative solution might be easier, simpler, or a better fit to the domain. ---- ^1 People around here keep saying “no side effects”, but who's going to pay for code that really has *no* side effects? *edit:* markdown is weird
The author of that post seems to like making life difficult. If I were faced with that strange repeated test arrangement I would use a couple of `print` statement to see if `profile.verified_email` does change or not. If matters were still unclear, and since this is python we're talking about, and since we have the source, I'd might try pushing in my own implementation and have it tell me when (whatever is behind) `profile.verified_email` does get changed, maybe even spit out a stack trace. We have more ways to understand code than just reading it. *edit:* typo
Isn't this just a pretty guarded case statement?
and that person has written a lot of correct code!
I've been using Opaleye in production for almost two years now (I started with an early, pre-release version of Opaleye). As [@ndmitchell](https://www.reddit.com/r/haskell/comments/3fuq4s/opaleye_or_relationalrecord/cts4eft) and [@ocharles](https://www.reddit.com/r/haskell/comments/3fuq4s/opaleye_or_relationalrecord/cts6ioi) pointed out, initial setup is time-consuming and complex. It looks like the work done by Silk that [@bergmark mentions](https://www.reddit.com/r/haskell/comments/3fuq4s/opaleye_or_relationalrecord/ctspeoq), [https://github.com/silkapp/silk-opaleye](https://github.com/silkapp/silk-opaleye), will do a lot to ease some of that pain, but I haven't had time to experiment with the library, so I can't comment further. I would encourage anyone starting out with or using Opaleye to give it a look. Ultimately, I'm okay with the complicated setup, because I know there are things that could be done to alleviate that pain point, they're just not there yet, and unlike what seems to be the current trend, I'm not interested in how fast I can slap together some functionality, but how maintainable over the long-term what I build is. Towards that end, as many have pointed out, the type-safety and composability of Opaleye queries are big wins. As [@ocharles](https://www.reddit.com/r/haskell/comments/3fuq4s/opaleye_or_relationalrecord/cts6ioi) pointed out, left joins are a pain. Additionally, I've occasionally needed to use analytical queries (e.g. `avg(x) OVER (PARTITION BY Y`) for performance which Opaleye doesn't natively support. Much like [@dbpatterson](https://www.reddit.com/r/haskell/comments/3fuq4s/opaleye_or_relationalrecord/ctst8fw), I can't comment on `relational-record`, because I've been happy with Opaleye and haven't needed to look for anything else. Ultimately, I think the decision of what relational data access library to use probably comes down to the needs of the application: if you're going to do a lot of simple, single-table queries and data manipulation, i.e. a simple CRUD application, Opaleye might be overkill; however, if you're doing more complicated queries, approaching the OLAP space, then you're going to be better off with something like Opaleye. 
To be fair I'd say this is an argument in the other direction. They said there was a problem based on their own research elsewhere, then serendipitously another example appeared independently and totally unplanned. To me that's evidence of the problem, not of "making things up as you go along ." That said, I don't write database code and have no shoe in the pudding.
(Unrelated) I still cannot figure out what you guys do or who is using it. Can you explain?
&gt; The Academic is interested in other things – what problem does the library solve? Is it a problem worth solving? If there are type classes do they have laws? This type of Haskeller will go straight for type signatures and function definitions to understand how it works. Maybe "Developers" should also go straight for type signatures class laws and definitions? Just a thought... 
Alternatively it could adopt a YYYY-MM suffix, eg: LTS-2015-08
My approach is that lists are basically control structures and vectors are basically data structures. Coming from an imperative environment, lists are loops and vectors are arrays. In performance contexts, the main question is: will the list operations be subject to fusion, e.g. a bunch of maps followed by a fold? If so, there's often little reason not to use an ordinary list. In lazy or pseudo-streaming contexts, converting to `Vector` may be impossible or catastrophic for performance. Some good indications that you're looking for a data rather than control structure are: repeated or long-term use of the same value; sharing (sometimes); indexing or other lookups; mutation or, in the same vein, `fmap`ping with a function of type `a -&gt; a`. The `Vector`s have their uses, but there are other structures that may be more appropriate depending on usage, including the old favorites from `containers` and `unordered-containers`. (By the way, there's not a sharp or principled distinction between "data" and "control" structures. I'm using those terms as convenient broad generalizations.)
bpython is pretty amazing in this regard, but it should be possible to at least make a ghci command to call a script to look something up, assuming you had such a script.
There's some overlap with [formatting](http://hackage.haskell.org/package/formatting-6.2.2/docs/Formatting-Formatters.html#g:2), but I'm not sure to what extent. I was just now considering using it in formatting because the `text-format` package has [problems building for Windows users](https://github.com/bos/double-conversion/issues/7) to the extent I'm pretty sure nobody on Windows is using formatting, but I see this package is using `double-conversion` too. Woe.
&gt; You can easily construct URLs with the network-uri library. That is the one that uses strings.
This looks like the best answer. One thing that bugs me, and that I should probably more explicitly request this of authors, is that if your library/module is providing a parser (and uri-bytestring is), then please actually expose the parser (in addition to the `ByteString -&gt; Either …` function), i.e. the attoparsec parser. E.g. [like this](http://hackage.haskell.org/package/stack-0.1.0.0/docs/Stack-Types-PackageName.html#v:packageNameParser), this means that if I'm writing an attoparsec parser for something and there happens to be a URI inside the thing I'm consuming, I can just go ahead and use the attoparsec parser directly, instead of doing something ad-hoc and probably-inefficient. `Parser` monads are composable after all. If you were thinking it would be inappropriate to expose this, I'm telling you it would be very welcome.
Not the complete haddock, but we can already get some help: Prelude&gt; :i Maybe data Maybe a = Nothing | Just a -- Defined in ‘Data.Maybe’ instance Eq a =&gt; Eq (Maybe a) -- Defined in ‘Data.Maybe’ instance Monad Maybe -- Defined in ‘Data.Maybe’ instance Functor Maybe -- Defined in ‘Data.Maybe’ instance Ord a =&gt; Ord (Maybe a) -- Defined in ‘Data.Maybe’ instance Read a =&gt; Read (Maybe a) -- Defined in ‘GHC.Read’ instance Show a =&gt; Show (Maybe a) -- Defined in ‘GHC.Show’ Prelude&gt; :i maybe maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b -- Defined in ‘Data.Maybe’ Prelude&gt; :browse Data.Maybe data Maybe a = Nothing | Just a Data.Maybe.catMaybes :: [Maybe a] -&gt; [a] Data.Maybe.fromJust :: Maybe a -&gt; a Data.Maybe.fromMaybe :: a -&gt; Maybe a -&gt; a Data.Maybe.isJust :: Maybe a -&gt; Bool Data.Maybe.isNothing :: Maybe a -&gt; Bool Data.Maybe.listToMaybe :: [a] -&gt; Maybe a Data.Maybe.mapMaybe :: (a -&gt; Maybe b) -&gt; [a] -&gt; [b] maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b Data.Maybe.maybeToList :: Maybe a -&gt; [a] Just types tell a lot!
Yeah, there's definitely some overlap. The big goal for formattable was to make the complete format specification (including things like prefixes, suffixes, etc) serializable and provide a uniform interface for formatting things given that serialized representation. One [todo item](https://github.com/Soostone/formattable#todo) is to get formattable working with GHCJS which will require somehow addressing the double-conversion problem. I'm not sure what the easiest approach is there. One approach could be to get double-conversion working on GHCJS the same way other packages like text work. But maybe the best long-term solution is to bite the bullet and do pure Haskell floating point rendering. :/
Library co-author here. Thanks for the comment! Makes complete sense. I've added an issue for myself to implement that [here](https://github.com/Soostone/uri-bytestring/issues/16).
I'm aware of this feature, but this is only type information. My question regards description of a function or any other object in a loaded library. Especially when type information is not sufficient, e.g.: filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a] dropWhile :: (a -&gt; Bool) -&gt; [a] -&gt; [a] I know that, in this example, we can guess the functionality from the name, but in general case, this is not always that easy.
&gt; But maybe the best long-term solution is to bite the bullet and do pure Haskell floating point rendering. How much work is this? Anyone tried? Porting the C++ to Haskell and re-using the test suite is an option…
LOL, I just knew the moment I wrote "comprehensive" that someone would point out an obscure case that I missed. That being said, the intent is to capture as many of the real-world use cases as possible, so I'm open to pull requests that generalize to handle stuff like this.
Here's an old related GHC ticket: https://ghc.haskell.org/trac/ghc/ticket/2168 /u/fuuzetsu may know what the problems are (as I have been nagging him some while ago about getting the haddock strings into the `.hi` files...)
Also, very good sidestepping of liturgical vocabulary in favor direct wording that more people will understand: * Instead of "monads" or "IO monad", the author says "controlled side effects". * Instead of "you can reason about your code", the author says "you can use regular proof techniques when trying to figure out stuff about your program".
Personally, I generally thought of decoupling and separation of concerns as a way to improve organization and reduce the cognitive load of understanding how things work/what pieces are responsible for what
I believe this is part of the problem with them though. Unlike typeclasses, you don't know if the method that's currently in scope is the same one that you used elsewhere.
Nice! Do you have a blog post or some examples for common use cases? Or is this a "build upon" library?
I come from an Objective-C background which influences me on this, but I'd definitely prefer complete words for some of the names instead of `_nfThouStep`, `_nfPrec`, `formatPct`, etc.
Right, in general, this is not applicable (however, GHC does a binary tree search in that case which is O(log n) instead of your O(n)). I was only responding to your claim that there is no substantial difference between the linear search apporach (checking each with a single if) and the jump table: adding an extra constructor to the T constructor will cause the linear search to take longer, while the jump-table will in theory still take the same time. 
&gt;&gt; You're objecting to considerate language choices. &gt; &gt; I'm really not. I don't object to that. OK? Acknowledged? My first reaction to this was "Eh? So why are you arguing the position you are then?", but your other post explains which bit you were disagreeing with more clearly, so I think I've got a better handle on that now. &gt;&gt; You're also doing the "You're probably wrong unless you can prove it with lots of science and data" form of argument. It positions you as aligned with rigorous fact checking whereas on your side of the argument (as in most arguments on both sides of any debate), there are unevidenced assertions and anecdotal examples. (IIRC, those were from an Internet argument which had already descended to the irrational "that's like the Nazis" point. ) &gt; &gt; This isn't fair to me. I *very explicitly* stated, three times in this thread, that I was not claiming that my experiences and intuitions constitute proof or that they ought to be convincing to other people. &gt; &gt; I don't know what could be further than that from "position[ing] [myself] as aligned with rigorous fact checking." It was the asymmetry in the requirement for evidence I was picking up on. OK, I've unfairly lumped you with a whole bunch of other folk who like to sound rational about disbelieving other people whilst applying no rigour whatsoever in their own arguments. It's just that their favourite is "If you can't provide scientific evidence for that, you can't claim it's true.", and what you said reminded me of that, but that isn't what you said. &gt;&gt; Pretty much uniformly, "show me the evidence or I can't believe you" is a hypocritical gambit. The non - hypocritical version is "look, you really need to read this paper from this respectable journal where they did this study and found that overwhelmingly,..." &gt; &gt; I'm just being honest. I don't believe it and I said why. And given that I was very deliberate about saying that I didn't claim to have proved anything, I do think you're being completely unfair here. I'm the one being asked to believe something, and I am explaining why I remain unconvinced. That's a valid point. Nevertheless, perhaps "I'm skeptical about that" is a fairer way of explaining than "Why should anyone believe that it does? ... You could show that to be non-representative of the general trend, if you have evidence to back up what you're claiming. Do you have that evidence?" Can you see the asymmetry in the requirement for proof here? Both parties have anecdotal evidence, neither has some scientific paper backing them up, because I expect very few participants in /r/haskell read a lot of social science research papers or even textbooks. &gt; By the way, this is a very good article: http://foucault.info/foucault/interview.html &gt; &gt; The conversation here reminded me of it. That was a fascinating read, thanks. I read most of it yesterday while travelling and the rest this morning. The initial bit about polemics was particularly relevant. I do feel that the polemics are worsening on both sides of this argument, although I'm feeling increasingly hopeful that you and I are working towards more productive discourse between us in particular. The thing is that the Haskell community is normally a great, positive and welcoming place to be. I think we should perhaps agree to disagree about how it came to be not so lovely in this thread! 
The two most popular forms of concurrency in Haskell (MVars and STM) both have ways to prevent this, and use the type system to guide the programmer to write thread-safe code.
&gt;&gt; creating a hostile environment for most people is good because a tiny minority like it &gt; &gt; That's a shitty argument. Good job I didn't use it then. It's probably a good job I stuck to polite adjectives too. Did you want to address anything I _did_ say?
If we could get ahold of the fully qualified name, doing a lookup in the haddock documentation shouldn't be too hard?
I agree with your point here. However: &gt; Instead of "monads" or "IO monad", the author says "controlled side effects". In context, "monads" would not only be too technical, it would be slightly incorrect. What the writer is talking about actually has more the flavor of effect types than monads. Of course, Haskell doesn't have these; the usual substitute is a monad. But one can also encapsulate side effects using a fold.
Whether code is monadic or not doesn't really change the debugging approach IMO. The thing with debugging Haskell is that lazy evaluation and purity make step debugging a less useful debugging technique - we don't really care about evaluation order, we don't even care whether something gets evaluated at all most of the time; code evaluation in Haskell isn't necessarily linear, but follows a dependency graph. So it makes sense to debug following that dependency graph, and in my experience, this is best done *outside* the live application - instead of hooking a debugger like gdb into a running process, you take the code and run it in a different context - a unit test framework, or a REPL. Since Haskell code is pure, this is very reliable and representative: unlike an impure language, where the execution context can drastically alter the behavior of a piece of code, a pure function reliably evaluates to the same value regardless of the situation you call it in. Exceptions kind of bypass the whole purity thing, so I'd say if anything, they make debugging much harder; monads, by contrast, aren't magic, or special, in any way, they're just one more way of structuring your dependency graph, and that particular structure happens to have some convenient syntax sugar built into the language to make it look a bit more imperative, such that out linear dependency chains look a bit like imperative statement sequences (which they sometimes, but not necessarily, represent in some way). If, however, by "monadic code" you mean "IO code", then the imperative approach is sometimes valid, and techniques like trace logging ("printf debugging") tend to prove rather useful occasionally. Another common approach, e.g. in a monadic parser, is to add meta-information to your parser monad, so that you can tag parser errors with line numbers - but those are line numbers in the source you're parsing, not the parser itself. In the "chain of Maybes" example, the answer is that if you really need to know, you may want to run the functions separately in a REPL - that's how I'd go about it anyway. But chances are that if it is important which one of them failed, that Maybe is not the appropriate type to return your results: you're probably better off using Either, or ErrorT/ExceptT, or a custom failure monad instead, where you can tag the failure values with extra information that tells you what exactly went wrong. There are even convenience functions to convert between Maybes and Eithers etc. to make the transition even easier.
&gt;But one can also encapsulate side effects using a fold. How so? 
&gt; There is also a huge difference between a request made casually and one made with an implied threat. That's news to me. What's the implied threat?
That would be good.
An imperative programmer would think of a fold as a loop in which a value is (1) initialized, (2) repeatedly operated on (side effect!) based on the items in a list, one by one, and then (3) returned. For example, consider the following Python code. def process_list(the_list): r = initial_value for x in the_list: r = process(r, x) return r The above is mostly a loop that does side effects: r is modified repeatedly, based on each x in the_list. The equivalent Haskell is a foldl: process_list the_list = foldl process initial_value the_list (Yes, I know the two occurrences of `the_list` can be removed.) Generally, when translating imperative code to functional code, loops that repeatedly do side effects become folds, while other loops become maps, filters, etc. 
this was horrifying and hilarious! and true... (Java for me).
I too prefer my build errors today as runtime errors tomorrow... at least, that's how it is with Elisp and Python. sarcasm aside, how does Racket (language and community) manage dependencies? don't you ever find dependency mismatches that come up as name errors or type errors?
Hmm, `Pattern a` is already defined as a Monoid, where mappend means overlay the two patterns. Overriding this behaviour for OscPattern seems the wrong way to go, even if Haskell let me do it (I'm not sure if it does, maybe with more ghc extensions..). 
In fact stack is built using the Cabal library. Where's the irony in using cabal install too?
This is true. The sample in the article however looks like it is written in Python, where it may well be an issue - I don't know the specifics of Python.
A good resource is the ["Spineless, Tagless, G-Machines"](http://research.microsoft.com/apps/pubs/default.aspx?id=67083) Paper by Simon Peyton Jones, which describes in some detail how haskell solves this exact problem efficiently. A crude summary of this is that haskell values are actually a structure containing a function pointer to the 'thunk' that computes the value. Initially that will be the haskell code that computes the desired value, but once the value is in WHNF, the function pointer is a 'constructor', which does nothing besides pick out the correct case-body. The other half of that is that a case expression gets compiled into a table of continuations, one per constructor of the type being destructured (nested patterns are desugared into separate case statements, one per level). When evaluating a case expression, a continuation per constructor in the type (or an error continuation for un-matched cases) is pushed into a continuation stack, and the value's thunk is invoked. Some computation may happen, and in any case, once the thunk knows which case to use (which is immediate for already-normalized values), the cases' continuations are removed from the stack, and the corresponding continuation for the constructor (the first continuation for the first constructor, the second for the second, and so-on) is called with the arguments for that constructor.
exactly what I was looking for, thanks /u/badzergling
I am *extremely* surprised to read this: &gt; lens-family is a nice package which is mostly compatible with lens, which has few dependencies, and which provides Template Haskell in a separate package as well. &gt;I'm afraid to depend on it, because it isn't guaranteed to be compatible and doesn't even try to be compatible. I thought compatibility with `lens` was the whole point of `lens-family`.
That's what I use (containers) :)
I can't agree more. Most functions and types have really brief documentation, and with docs in ghci we'll never need to fall back to the browser again!
&gt; Yes, Stack evades the underlying issue and it provides short-term convenience for users You rarely miss an opportunity to have a go at stack. More fairly, stack by default uses package sets that are known to be mutually compilable, and unlike cabal install applies by default community-known best practices for sandboxing, thus repeatedly introducing convenience for users by failing to reproduce cabal-install's (default) error-laden behaviour. &gt; but reduces the pressure to fix the underlying issue in Hackage... because the problem becomes less perceivable in the Stack bubble, I don't agree with you that all users should share the pain that only the individual package authors need to feel. &gt; and people wonder why crazy people like me get so upset about the wrong meta-data on Hackage in the first place I think you have **unfairly** and repeatedly implied that becoming a stack user causes you to post inaccurate meta data on Hackage, and that becoming a stack user is tantamount to hiding in a basement while the city burns or worse. Go moan at the specific package authors whose meta data you feel is incorrect, instead of blaming everyone who uses stack for the future end of the Haskell ecosystem. You could be a little less condemnatory by saying _"If you use stack and you're uploading packages to hackage, please make sure you specify accurate upper bounds for your dependencies, because it's easy to forget this is helpful for cabal-install users when stack makes it easy for you to work around the inaccuracy by freezing versions."_ (While I'm on the topic, upper bounds can be incorrect by being too tight as well as by being too slack/absent.) &gt; I don't think this is sustainable in the long-term If you solve the mutual dependency problem too often manually, the computer-generated solution will stop working? Sorry, I know what you really mean here, and I've taken that point more seriously elsewhere in this post. &gt; Btw, what is the "definition of cabal hell" anyway? People seem to use that term to name different effects most of which are used as reason to use Stack instead... Hehe. People are confused about the confusing ways cabal fails for them, and fail to use terminology consistently with each other? This is perhaps because the phrase "cabal hell" describes how they feel about their problems, and they don't care at the time whether it's technically correct! (I tend to use the phrase "cabal hell" to mean a situation in which I need to delete all my cabal data to proceed. But quibbling over what the term means won't magically make cabal pleasant to use.) &gt; Ironically, IMO, cabal hell .... is the one thing Stack can't avoid either Funny thing is, it doesn't seem to _ever_ happen with stack. 
There are 1.2 billion people in India. They have computers, and their world is as "real" as yours. You don't have to support them, but...
&gt;&gt; Stack uses a database of packages that are all compatible with each other. &gt; &gt; ...so does cabal-install, which uses the Hackage database No, that's a different thing. Many packages on Hackage are _not_ compatible with each other. If they were, there would be no need for dependency version bounds; you could just delete all the old versions. &gt; where each package describes which other package versions it's compatible with, ... so not "all compatible with each other" then. &gt; Stack "solves" the problem of (some) package authors not living up to quality standards (by not properly maintaining their .cabal files) by getting rid of the Cabal solver and instead using pre-computed global install-plans (called snapshots). Apart from the inverted commas, this is perhaps the least contentious thing I can remember you saying about stack, and you've instead aimed the criticism at _some_ package authors. I know you have things to say about which ones, but on the face of it that was fairly neutral towards stack. It cheered me up.
yeah, I don't know what GHC does. whatever GHC is doing is constant time on the input tho, which was, afaik, OPs question
Quite a few I imagine. I want to cover everything you would need to know to feel comfortable using Nix for Haskell development, testing, and deployment to the cloud. So, that includes really understanding the Nix language, the Nix package manager, NixOS, Hydra, and NixOps. I also need to cover the haskell specific aspects of the nixpkgs collection and also dealing with ghc vs ghcjs.
I usually dislike the breezy "I'm being cool and informal" style that this README is written in, but it's beautifully executed here--mainly because the text is also extremely clear and straightforward, and the little side-comments are completely on-topic. &gt; It definitely doesn't make much sense to create lens and leave TH bits out, just as it doesn't make sense to produce a 2000$ tv set where you can't switch between sound tracks when watching a file from a usb stick. Fuck you, Philips, so much.
I never meant to minimize Indian people. The scheme currently used in formattable works for the vast majority countries' digit separation schemes, so by that metric the Indian one certainly qualifies as obscure. Also, I specifically said that I was willing to support them.
&gt; You don't, and you shouldn't have to care. If you write that long `Maybe` computation, it's because you _want_ the result to be `Nothing` if any step returns `Nothing`, so there is nothing to debug. Shouldn't, but maybe you do. I've actually been running into this in code I'm writing: I have a series of map lookups, and if any of them fails, the result should be `Nothing`. I have a bug in the code that generates the maps. It's _extremely_ helpful to know which one of them failed in debugging this. To be fair, this is exactly what (the relatively new) [`traceM`](https://hackage.haskell.org/package/base-4.8.1.0/docs/Debug-Trace.html#v:traceM) is for.
I can't speak for AI or machine learning, but, to the best of my knowledge, typed languages are not that commonly used in those areas. For general robotics, there's not a lot, but I do try. I talk about my work a lot, so I won't get too detailed here. I've done work where the types are used to constrain the programs you can write based on models of robot capabilities, and, in an entirely different direction, I try to demonstrate that typed languages are an under-utilized weapon to help engineers write code. There's a bit of a stigma of academic-ese around FP, so I try to do state-of-the-art robotics using these tools because they let me work better. The thing about experimental robotics is that, by and large, nothing works. So there's a real benefit to be had in getting help from something as easy to pick up as a compiler.
I don’t have concrete examples on hand at the moment, but generally speaking, you should make a data structure strict if you expect to compute the whole thing (so you can avoid laziness overhead), and lazy if you expect to traverse only part of it (so you can avoid computation overhead). I tend to use strict data structures, and make all the fields of my data structures strict unless they need to be lazy. Then I make all my functions lazy unless [profiling](http://book.realworldhaskell.org/read/profiling-and-optimization.html) reveals that they should be strict. (The profiler tools will also tell you which functions are allocating lots of thunks.) 
&gt; The situation you are dealing with is Haskell for Windows developers, which is absolutely not a good story. I'm not sure what your point is here. There are working ports of emacs and vim on windows. Or is it that windows users expect a more graphical interface and shortcuts like Ctrl-S rather than C-x C-s and Alt, F, A instead of M-x? In short, windows users like IDE type tooling while linux users like keyboard-controlled tools? &gt; Is learning emacs or vim such a barrier for the Linux developer who already uses one or the other? Trivially, no. I think the OP thinks emacs and vim users are in general a minority, and would like a good default IDE, feeling that would suit the majority of new users. Windows is definitely a numerically popular OS. &gt; The learning Haskell part is another red herring introduced by an assumption that preparing something like an IDE should be step one. Chris Allen (/u/bitemyapp) does a good job at reiterating that learning the language can, and arguably should, come before figuring out what development tool setup is best. This is advice IDE-craving new users don't hear and don't want to hear. 
Try errorWithStackTrace https://hackage.haskell.org/package/base-4.8.1.0/docs/GHC-Stack.html
Ah, I just had a better look at the code and I didn't realise the relationships you have between Pattern and OscPattern. So from what I understand you have: data Pattern a = Pattern {arc :: Arc -&gt; [Event a]} data Pattern (Map.Map Param (Maybe Datum)) = Pattern {arc :: Arc -&gt; [Event (Map.Map Param (Maybe Datum))]} The `(&gt;+&lt;)` function is essentially the Free Monoid - which basically just makes one pattern happen after the next pattern. Meanwhile `(|+|)` is also a Monoid but does something more complicated with `Map` unions and its `(&lt;*&gt;)` instance. I'm trying to understand the Applicative implementation but it's reasonably complex - though I think I get the concept of it creating a bunch of new loops from old loops. So you're right, my suggestion to give OscPattern a Monoid instance probably wouldn't work for you. Sorry I sent you down the garden path. As a side note - what's really interesting is that the two operators `(&gt;+&lt;)` and `(|+|)` could *almost* form a semiring. They don't because (amongst other things) the types don't match, but they come close. I don't know how this would work out for you... but if they did form a semiring you could give them a `Num` instance. Then you could do something like: sound [something] * (sound "[whatever]" + sound "[whatever]") == (sound [something] * sound "[whatever]") + (sound [something] * sound "[whatever]")) or sound [something] (|+|) (sound "[whatever]" (&gt;+&lt;) sound "[whatever]") == (sound [something] (|+|) sound "[whatever]") (&gt;+&lt;) (sound [something] (|+|) sound "[whatever]")) I.e. your `(|+|)` operator could distribute over your `(&gt;+&lt;)` operation. I have no idea if that's possible, reasonable or practical. I just though it was kind of neat.
As far as arrowized FRP is concerned, instead of proc notation, can't you always write an equivalent signal in terms of Functor, Applicative and Monad? I don't think I've run into situations where I couldn't and having the option of proc notation does make certain problems much easier.
Awesome intro. I've been curious about nix for a while so I'll keep an eye on your channel for more videos.
Very nice package. I'm actually mining this for examples and documentation to supplement the mainline ekmett package at https://github.com/ekmett/lens/wiki/Operators page.
Thanks, I'll have a look at that. Although, at the first glance it seems that it's mostly targeted on Pandas users, while I'm personally interested in numerical simulations, so I'd rather read about doing FFTs and fancy indexing on multidimensional arrays than queries and column names. 
I mean that I'm not going to casually suggest that a Windows developer used to VisualStudio just deal with the pain of getting Emacs and ghc-mod working and comfortable for them. If it's a Linux developer, there are wider benefits to learning Emacs and Vim, so I'd more strongly suggest it. As for numeric popularity, [here's what little data](https://www.reddit.com/r/haskell/comments/3c5383/development_tools_survey_results/?) we have for /r/haskell . The point is that rejecting Emacs and Vim is a significant step, akin to rejecting Windows and VS for .NET development and having to contend with MonoDevelop, Eclipse, or a text editor. It absolutely *can* be done, but it's not something you do and then complain that the .NET ecosystem is dysfunctional. You've selected a tiny niche of the ecosystem in which to survive; life will be harder. It doesn't mean you're wrong for wanting to live in that niche (Haskell programmers are used to the niche life), but complaining that there aren't enough people to help you isn't going to get you very far. By all means, thrive in your niche and make it flourish, but you have to be aware of the battles you're choosing.
If finished I could see this being a really great resource for getting into Nix. After watching the videos it certainly seems a lot less intimidating to me.
Awesome! Thank you!
What I'm looking for is a type-level distinction between the name and the locator. I want to be able to parse a text as a URL only (and fail if it's a URN). I want to be able to say that a function, which works on a location, only accepts URLs. There's a whole list of other reasons why I consider that distinction essential.
If you add a monad to that type you can accomplish getting the internal state out (serialization): newtype Auto m b c = Auto { runAuto :: b -&gt; m (c, Auto m b c) } Which is how we do it in [varying](http://hackage.haskell.org/package/varying). Which is not to say that signals are serializable out of the box, but that the monadic effect very well could be serialization.
I suppose I'd have argued for not including `(&amp;)`. It isn't really needed nor all that convienent for working with lenses. It's presence seems mostly to emulate common imperative language ordering... which I think is a poor thing to do in Haskell.
It's emulating record update syntax. Also, it's got the same fixity as &lt;&amp;&gt; and &gt;&gt;= which is extremely nice.
I very much appreciate the README.MD. It's essentially a blog post to introduce the library, *but it's packaged with the repository*. A lot of people seem to provide sparse documentation on hackage or github and then make a detailed post on their blog or Reddit, which gets lost and forgotten after a month or so.
Thing is, it's in the Prelude as of 7.10. In fact, from the `microlens` source: #if __GLASGOW_HASKELL__ &gt;= 710 import Data.Function ((&amp;)) #endif
Look at the type of Lens s t a b = (a -&gt; f b) -&gt; s -&gt; f t pick `f = (,) a` to get `(a -&gt; (a, b)) -&gt; s -&gt; (a, t)` and this lets you give back the 'old' value _2 (\a -&gt; (a, "new value")) (1,2) If we instead pick `f = (,) b` (a -&gt; (b, b)) -&gt; s -&gt; (b, t) this lets you give back the 'new' value with something like _2 (\a -&gt; join (,) (a + 1)) 
Might I suggest putting that example on the Github README.MD file?
There's a [url](https://hackage.haskell.org/package/url) package, but I wouldn't call it decent (it's quite awkward to use, to be honest).
No need for MultiWay If: new_watch1i :: Clause -&gt; Assignment -&gt; Int -&gt; Maybe Int new_watch1i (c,w,v) a i | i &gt;= Vector.length c = Nothing | c!i == 0 || i == v || (a!i &gt; 0 &amp;&amp; c!i &gt; 0) || (a!i &lt; 0 &amp;&amp; c!i &lt; 0) -&gt; new_watch1i (c,w,v) a (i+1) | otherwise -&gt; Just i 
Vectors can also be run in the ST monad. Unlike arrays, Vector uses a fusion framework, so it can optimize combinations of highlevel functions. arrays may be faster when hand optimized into a tight loop, but Vector is more convenient.
You need to install Hoogle and then you can add :def doc \x -&gt; return $ ":!hoogle --info \"" ++ x ++ "\"" to your ~/.ghci file and use it like this ~ % ghci GHCi, version 7.8.4: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ghc-paths-0.1.0.9 ... linking ... done. Prelude &gt; :doc Maybe Prelude data Maybe a The Maybe type encapsulates an optional value. A value of type Maybe a either contains a value of type a (represented as Just a), or it is empty (represented as Nothing). Using Maybe is a good way to deal with errors or exceptional cases without resorting to drastic measures such as error. The Maybe type is also a monad. It is a simple kind of error monad, error monad can be built using the Data.Either.Either type. From package base data Maybe a 
&gt;&gt; Did you want to address anything I _did_ say? &gt; &gt; I did. All I saw was you summarising your own argument, but swapping out "bad" for "good" and putting a `&gt;` in front of it as if I'd said it. &gt; Go ahead and keep chasing women away from tech in the name of inclusiveness though. I think this is where we disagree about the effects of what we say. You seem to think that arguing angrily with the one woman who expressed her opinion and anyone who has a similar viewpoint is the way to make a harmonious community where women want to participate, whereas I think that's the source of the very hostility you claim to want to avoid. I think that using words like "people" instead of "guys" is more welcoming to women, and you think it'll drive them away. I think your viewpoint lacks logic and introspection. You tell us how your wife didn't feel part of the team until they relaxed around her, and I tell how some female students said "hey, we're not all guys!" when I'd referred to the class as "you guys". I apologised and explained that I'd heard a few times in a gender-free context at university. The thing is, that I then went on to not call them guys instead of having a big argument with them about whether they should have objected. _That_, in my view, was the conflict-free, welcoming approach that doesn't drive them away. Future students will have no idea that the conversation happened, I'll just be using words like "people" and "students", "folk" and "you lot", and I **cannot** believe that the absence of the word "guys" in reference to the whole group will drive anyone away. How can it? No-one will notice. 
That’s a very good idea. I like `lens`, but it’s way *too much* to me. Thanks for that, I’ll definitely give it a try!
More concise answer: It's the closest thing there is to SystemF with a mature compiler. SystemF has most of the advantages listed here, and the ones it's missing are the ones I could typically find a good argument against. ("Typeclass prolog" is a double-edged sword, and type promotions/type families are a leading source of astronaut architecture). So it really just boils down to Haskell is the only language which is both pure and (relatively) popular.
They also have an implementation [of the checker](http://www.cs.rpi.edu/~huangw5/cf-inference/). It would be nice to have this as a sonar plugin. Then you have an objective measure on purity and could convince your colleagues that "purer code is better". 
Ooh, that's really neat!
We can't stop users from doing that anyway; there's no way that we can stop users from defining their own `Throws` instances. If they really want to circumvent the annotation, sure, they can. But it wouldn't happen accidentally, and if they're going to try so hard to circumvent it, why would they use checked exceptions in the first place? :) In fact, you could remove an annotation even in terms of "safe" primitives, simply by using `handleChecked throwIO`. In a way this also justifies the role annotation: `handleChecked (throwChecked . coerce)` gives you that representational role even without the annotation. 
Oh! Nice trick :) Must remember that. That would be a good addition alright, mostly to stop accidental definitions of the `Throws` class by people who are confused about it's purpose. After all, if you forget a type annotation on a function that throws a checked exception, and you use a custom type `T`, `ghc` will complain "no instance for Throws T", and the instinctive "solution" might be to try and define that instance. 
http://icfpcontest.org/spec.html So it's a lot like Tetris, except you know what pieces are coming. There's also a twist. You get bonus points for issuing certain movement commands.
Understood. I'm talking about two things here that I didn't separate well. Provisioning and config management (Ansible and NixOps do both). Ansible supports provisioning netboot/PXEboot, Arista routers, all the way up to EC2 and Droplets. This I feel is the tradeoff between NixOps and Ansible provisioning: atomic &amp; isolated vs. mass provider support. Is NixOps atomic &amp; isolated even for AWS provisioning (I'm assuming it is which is impressive)?
Because it very much depends on what you're saying and how you're saying it. It was an example of how you _can_ take a request for change on the chin without kicking up a big fuss about it. 
Well, this is why it's important to at least agree on common language (c.f. "cabal hell"), otherwise we talk past each other... If you define two packages being compatible because *each* pair of version of both packages can be part of the same install-plan, then Stackage by definition can't ever have incompatible packages at the cost of flattening down Hackage to a rigid low-dimensional configuration-space. It's a bit like creating world-peace by killing every human on the planet... Also, by that definition, On Hackage you probably will find hardly any packages that are compatible with each other, as as soon there's a need for upper/lower bounds, you already become incompatible. I rather thought of package compatibility in terms of existential quantification over the package versions (IOW, satisfiability under the constraints given by `build-depends`), rather than the "universal" compatibility definition you seem to demand. 
I think you'll have to make that distinction based on the semantics of the larger document, rather than the syntax of the URI. For instance, an XML namespace declaration consists of a URI naming the namespace and an optional URL that gives the location of the schema document. However, it's quite common to use an "http:" scheme URI for the name portion, even if it doesn't actually point to anything. I'm wondering how you expect a parser to draw this distinction without outside information? I think it could be productive to expose the scheme of a URI in its type. That way, you could write functions that accept "http:" or "https:" scheme URLs but reject "ftp:" or "file:" URLs, for example. I don't think it's reasonable for a function to claim that it can resolve a URL with any scheme, including those that have not been invented yet.
What does it mean to evaluate something of function type? There doesn't seem to be anything analogous to "evaluate up to the outermost constructor".
Do you write libraries or applications? I suspect that library authors appreciate the ability to generalize more than application authors.
I want a "mesolens" package with a profunctors dependency and proper prisms and isos.
Right now I'm working with JavaScript stuff where the type systems are ad hoc custom preprocessors that half work, with unclear semantics, dependence on strange stuff like the compiler pattern matching against certain symbols (`React.createClass` is entirely different from `(R = React, R.createClass)`, the availability of type signatures is up to the whims and preferences of library maintainers, etc etc. So one the one hand it's very nice to approach some modicum of type safety in JavaScript... On the other hand, it makes me revere Haskell's type system deep in my heart. Thank you, you're a true friend.
I wouldn't doubt that. But part of the matter is that generic libraries are usually harder to learn. Especially in the absence of good documentation and example code, it can be very frustrating to work out what instantiations were intended by the authors for the basic usecases. There's a balance point, and where it is is debatable. I feel like needless genericism should be avoided in cases where the abstraction is not well-known (whatever "well-known" might mean). Preferring `fmap` to `map` seems acceptable, since `Functor` is perhaps the first typeclass you learn about, and something you become very familiar with in Haskell. But library-specific or lesser-used typeclasses become a burden on the application programmer. For example, how many non-`Monoid` types instantiate `Semigroup`? I'm sure a clever person could come up with one or two. But of the two, `Monoid`s are much more universal. (And unless I'm mistaken, given any semigroup, you can generate a monoid by appointing an identity element if one is not already present). Another example might be `Foldable`. (And correct me if I'm wrong with this one. I have never been able to get anyone to give me a reasonable explanation for this typeclass). As far as I can tell, this class just allows a user to specify a canonical `toList :: t a -&gt; [a]` function. But then you could (at least, in principle), factor all such functions which specify the `Foldable` constraint to operate directly on lists instead. The principle of You Ain't Gonna Need It applies to Haskell as much as any other language. A textbook need not develop a general theory of left R-modules when all you need is a vectorspace. A library need not parametrize arrays with `Ix` since programmers are most familiar with and interface most frequently using 0-based indexing. A library should strike a balance between generality and ease-of-use. And I guess my point is that Haskell culture tends to hyperfocus on the former at the expense of the latter. The thought is always, "What would an expert user of my library *think* to do?" rather than, "What would a typical user of my library *want* to do?" One idea that I've seen a few times in Haskell (that I would love to see more of) is multiple libraries which cater to programmers of different experience levels. For sure, there is nothing wrong with the theory of left R-modules. But everyone would progress much faster to that point in their comfort level if they could play with vectorspaces first.
Perhaps this is why I am so dismissive of type wizardry in Haskell while the majority of the community doesn't seem to care. My internal model for Haskell only 'clicked' once I understood dependent types. And when you see how elegant generic programming can be (at least in theory), what Haskellers do looks absolutely horrifying. &gt; Although there probably won't ever be things like Lens in F# due to it's lack of typeclasses as well. It wouldn't be a trade-off if you didn't have to give some things up. Lenses are neat, but they are the posterchild example of a tool I have never felt a need for. My pains in Haskell have not been from the lack of first-class records, but from the lack of second-class records. Perhaps I just have yet to realize their full potential, but if it means I don't have to see GHC expand out a partially-applied coyoneda encoding of a getter-setter pair in my error messages, I can redirect my spare frustration to writing boilerplate JSON munging code.
There's a reasonable number of Haskell jobs, although you might have to either live in a tech hub (Bay Area, NYC, Boston, London... etc), work remotely or get lucky. (Of the three, "remotely" is probably the best option :).) It gets harder if you want to combine Haskell with other criteria though. (Finding a Haskell game development job is hard, as somebody down thread mentioned.) But if you're happy with some sort of generic Haskell work, you should be fine.
It's not as easy as it sounds (if you want it to play nice with microlens – and I do want it to play nice with microlens). If you provide proper `Prism` and `Iso`, you also want to provide actual prisms, and now microlens and mesolens both have `_Just`, etc. with different types. (And it gets worse if you want to do something like lens-aeson – should it be microlens-aeson? or mesolens-aeson? or both? on one hand, things in lens-aeson are prisms, but on another hand, I've only ever seen them used as traversals, so should I make one package for the common use-case and another – for correctness? and then people would say “too many packages” and they would be right.)
Russell O'Connor will surely be happy to right any genuine incompatibility between lens-family (and thus lens-simple) and `Control.Lens`; the original library has already been brought in line with the later developments in `Control.Lens` once before https://mail.haskell.org/pipermail/haskell-cafe/2013-October/111154.html and other emendations have been made since. This also holds of `lens-family-th`. The statement that the library has no intention to keep 'compatible' with `Control.Lens` is just not true. The part of `Control.Lens` that it overlaps with is completely stable anyway - and was within a few months, a few years ago now - and any change in `Control.Lens` in that domain would break the 'compatibility' with `lens-family` would be a crisis for `lens` users too. So the alarmist element in the microlens readme here seems a little silly. The meaning of 'compatible' and 'incompatible' is in any case totally unclarified in the documentation above, and seems to hide considerable confusion. For example, it is a confusion to think that the increased generality of `_1` and `_2` in `Control.Lens` makes for an "incompatibility." If I use `_1` and `_2` from `lens-family` / `lens-simple` and then need to switch to `import Control.Lens`, there will be no problem. The author is clearly aware of this, as he knows that only a `profunctor` dependency could give some of the important combinators the same generality they have in `Control.Lens`. This holds pretty generally: occasionally there will be a subtlety about type inference; a few other problems arise sometimes. This is the only 'compatibility' `lens-family` or `micro-lens` can intelligibly strive for, and `lens-family` / `lens-simple` already have it. Every actual complaint seems to be about the lack of generality of the `lens-family` combinators, which can be viewed as a plus, for the most part. Of course it doesn't matter. I have definitions of `view` `set` `over` in a tab trigger in my editor and they are frequently all I need to employ lenses exported by a library I am using; they should just be in Prelude anyway. A tide of libraries that do this for me won't hurt anything. The crucial thing is to condemn library writers who incur a `lens` dependency just to avoid writing a few ordinary lenses by hand; this is not just irritating, but basically deeply incompetent. The library would be more interesting, I think, if it took the step of dispensing completely with type synonyms. These are basically a distraction and an increase in cognitive load once one gets the idea. 
Makes me curious about whether it would be possible to extend support for providing more concrete interfaces to an abstract library. That could possibly be viewed as a question of documentation. For example, I can imagine a Haddock page with the option of toggling between different instantiations... Eliminating type parameter constraints by assuming some environment saying e.g. `MonadIO m =&gt; m ~ IO`. Or some other interactive way of exploring such substitutions.
&gt; A library should strike a balance between generality and ease-of-use. And I guess my point is that Haskell culture tends to hyperfocus on the former at the expense of the latter. The thought is always, "What would an expert user of my library think to do?" rather than, "What would a typical user of my library want to do?" A thousand times, this. 
It means to evaluate until the function is a lambda (and not e.g. an application).
&gt; It's a bit like creating world-peace by killing every human on the planet... Hehe. You don't feel you might have overstated that a little? &gt; I rather thought of package compatibility in terms of existential quantification over the package versions (IOW, satisfiability under the constraints given by `build-depends`), rather than the "universal" compatibility definition you seem to demand. Well in the statement &gt;&gt; Stack uses a database of packages that are all compatible with each other. &gt; &gt; ... so does cabal-install "All compatible with each other" is asserting that stackage's package sets have each package pegged to a specific version known to be mutually compilable with any and all of the other packages in the set. This "universal compatibility" as you called it is indeed very convenient for users, and is currently not a feature of cabal install. (But I don't think we should call this universal compatibility; that would be misleading because stackage currently doesn't even cover all of hackage.)
&gt;&gt; It was an example of how you _can_ take a request for change on the chin without kicking up a big fuss about it. &gt; &gt; And doing so drives away more women than it attracts, like I said to begin with. You honestly think that because I don't use the word "guys" to refer to groups including women, women will be driven away? Or is it that you think that my failure to get cross with them, failure to argue with them, failure to call them a hypersensitive minority and failure to tell them no-one cares is what's going to drive them away? 
That kind of tooling would be useful, but it doesn't alleviate the need for the library authors to express their *intent* behind how a library should be used. For instance, I believe I saw something like `MonadBaseControl` in conduit, which had instances for `IO`, `STM`, `Identity`, and `ST s`. Automated tooling might decide to pick out `Identity`, either because it is alphabetically first, or because it is the "simplest" `Monad` by any measure. But the most natural place for anything conduit-related to take place is `IO`. But it's not always `IO`. For `fmap`, the most basic example should probably be `[]`. In lens, I believe it's `Identity` and `Cont` that are your go-to `Applicative` instances. You can't really guess at these things. And getting *a* well-typing of your program isn't solving the problem of which instance makes the most sense pedagogically.
The distinction is considered to be obsolete by most standards today, and there is a host of other differences such as the fact that you need to convert between URI, URL, and IRI representations which makes it essential to know the context.
Indeed, library authors should be encouraged to indicate relevant and interesting instances. I feel like I've seen submodules on Hackage that export specialized interfaces... That's similar to what I'm imagining, but I imagine doing such a thing is pretty tedious for larger APIs. With ML module systems, I think you can easily reexport a parameterized module with its parameters specified.
I don't understand how that helps. Consider the following definition (I'm not familiar with auto so please forgive me if this is not the canonical way to implement it): -- given input [x1, x2, x3, ...], -- output [0, x1, x1+x2, x1+x2+x3, ...] sumSoFar :: (Num a, Monad m) =&gt; Auto m a a sumSoFar = go 0 where go acc = Auto (\n -&gt; return (acc, go (acc+n))) You can certainly compose this with an `Auto IO Int Int` which serializes and saves the input or output somewhere, but that won't allow you to serialize the *internal* state, in this case `acc`.
Great way to avoid those questions I asked: You honestly think that because I don't use the word "guys" to refer to groups including women, women will be driven away? Or is it that you think that my failure to get cross with them, failure to argue with them, failure to call them a hypersensitive minority and failure to tell them no-one cares is what's going to drive them away? 
I mean Haskell has abysmal record syntax. And there is no good justification for why that is. As far as I understand, it's a weird mixture of feelings towards type annotations, misplaced idealism, and nostalgia for the (.) operator.
There used to be a tutorial with this library, can I ask why was it removed? What is the recommended way to learn how to use this library without prior lens knowledge?
[Sum types. ](https://s-media-cache-ak0.pinimg.com/736x/66/79/8a/66798a0940a01c4ccc5c2be810d8a678.jpg)
Very good marketing, and popular misunderstanding of theory
How is `undefined` different to `null`?
I don't know. I've written a lot of OCaml, and always having to choose a specific `Monad` module was a real pain. I might not *write* much code that's parametrized over different monads, but I'd like to *use* lots of code like that! Doing that with more than one monad at a time in OCaml was a pain. This made my code worse. Full stop. Worse yet, it perversely made the OCaml *code style* worse: people are pushed to less expressive code patterns to avoid the awkwardness of using different monads at the same time. There's a lot of code where I want to use both `Maybe` and `IO` (or `Option` and `Async` in OCaml) in different parts of the same function or module; doing this without typeclasses (or implicits of some sort, I suppose) was a real pain. It's even worse for things like `Num`, `Read`, `Show`, `Monoid`… I'm increasingly convinced that, for all their faults and awkwardness, typeclasses are one of the most important , most expressive features in Haskell. It's probably the single thing I miss the most in other languages in day-to-day programming.
&gt; You are maintaining this hallucination by suggesting that if a lens-exporting library doesn't depend on lens, then of course it will depend on another, competing lens library. Nope, you misunderstood. I wasn't talking about lens-exporting libraries, I was talking about libraries that were using lenses internally (possibly without exporting any lenses at all!). Now, if you want to use lenses in your code (even if you just want to replace `second` with `over _2`), yes, I think you should depend on any of competing lens libraries instead of copying definitions of `over` and `_2` or writing them by hand or whatever. (I guess we disagree about this – to me it's *of course* that you would use a lens library if you need to *use* `view` or `_2`, to you copying definitions is preferable.) &gt; Why are you telling people this? basic-lens works just fine with e.g. the lenses exported by the pipes libraries. Because I have once again made myself insufficiently clear, sorry. By “any lenses” I meant “any stock lenses”. (Perhaps I'm underestimating the number of people who use lenses from other libraries but don't write any “lensy” code of their own.) I have reworded the paragraph as follows (and also got rid of “dead sure”): &gt; Otherwise, you probably should still use a bigger lens library instead unless you're sure you won't actually need any standard lenses (e.g. `_1`) in your code, or if you're fine with copying definitions of those lenses by hand when you need them (i.e. replicating parts of some other lens library in your project). &gt; even the remark about `view _1 . set _1 True` seems to contain the characteristic confusion I wasn't talking about compatibility anymore, I was talking about differences in implementation that affect observable behavior (as opposed to harmless type-level differences, which we have agreed to ignore). I can't say it's a *particularly* nasty surprise, but still: I don't like it that a switch to `Control.Lens` can make a failing program succeed, even if I can't currently imagine a non-contrived scenario when I would actually want the `set _1 True undefined === undefined` behavior.
[Reddit Enhancement Suite](http://redditenhancementsuite.com/) lets you disable subreddit style. I haven't seen custom CSS for years.
This is the only response, here or on stackexchange which comes close to answering the question. All the other comments are explaining why the haskell type system is a powerful one (although we know that there are more powerful ones). But the question was: &gt;What exactly makes the Haskell type system so revered (vs say, Java)? which is a question about people and culture, not about technical capability. Interesting that so few commentators can see the difference.
That depends on what you think is being marketed, and to whom. Does haskell have a lot of marketing effective at encouraging mainstream adoption? No. Does it have a lot of marketing effective at making it seem like learning haskell is the rite of passage to some semi-secret society of enlightened über-programmers? Yes. The question asked was not “what makes the haskell type system quite powerful?” but “what exactly makes [it] revered?” which is not primarily a technical question.
&gt; Asking for a "proper" or "decent" alternative to a library implies that the authors of that library did a bad job I only implied what I said: the libraries I referred to did implement the standard improperly (and I explained why) and I was looking for a decent library. The rest is your interpretation. I had no intention to offend anybody and, if I did, I apologise. "Why didn't you post it on Hackage?" - that was a clearly neutral and succinct question, which expressed honest interest. What in the world could be impolite about that? Concerning the sugar-coating of implications, I tend to disagree. My belief is that expressing thoughts directly and succinctly is the most respectful way. In case of professional communities it's also all the more valuable, since it reduces the signal to noise ratio and saves time to many people in the end. OTOH preaching a stranger on how to talk does look quite arrogantly. I take no offence however since I suspect you had good intentions there.
Would it surprise you if I said I don't believe monads are the answer to IO?
You expect value null on your code You expect value undefined **not** be on your code
I think the source of that phrase is from Joel Spolsky's (on software) blog: http://www.joelonsoftware.com/articles/fog0000000018.html. Soundbyte: "When you go too far up, abstraction-wise, you run out of oxygen. Sometimes smart thinkers just don't know when to stop, and they create these absurd, all-encompassing, high-level pictures of the universe that are all good and fine, but don't actually mean anything at all."
&gt; Higher-kinded types show up all the ty**p**e Got a bit carried away there
What do you get for `:info (+)`? Does `let doubleMe x = x + x` work?
That doesn't have anything to do with the question I asked?
have you copy pasted or did you enter that by hand?
You can't practically make such a change to Haskell, but I don't see any fundamental obstacle that would not allow it to work in a new language, do you? That's not very convincing at all, but given the potential upside it's worth investigating. Or look at it from a negative perspective: the monad-applicative changes are an indication that *something* is not right. Somebody will always come along and want to insert their own type class somewhere in the hierarchy. Yesterday it was monad-applicative, today it's monoid-semigroup, and tomorrow it's magma, field, ring, group, etc. This should not be a breaking language or ecosystem change, and the more complex hierarchy should not be forced on people who don't care about it. These problems mirror the motivation for structural record types.
no, not strange, just natural for a single line file.
I copied it and handwrote it :/ I rewrote everything by hand and it still didn't work
Well... That's one reading of the question's title. In the body text, the person asks: &gt; One of the 'memes' that people familiar with it have often talked about is the whole if it compiles, it will work thing - which I think is related to the strength of the type system. &gt; I'm trying to understand why exactly Haskell is better than other statically typed languages [e.g., Java] in this regard. It seems to me like the thrust of the question is indeed technical, and not that much about marketing and cultural notions and so on.
The example he copied from is only one line, and it has + at character 16, so that part at least does make sense. :)
Yeah, ghci :l baby.hs And boom nothing works 
Ahh I would but I'm out now :/ I'll do it in maybe an hour or two when I have access to my laptop 
As a follow-up, what text editor are you using? 
You can't compare a value in Haskell with `undefined`, that is you can't do x == undefined to see if `x` is undefined because the result would itself be undefined. But `null` can be compared to any object so it is frequently used to guide the control flow.
How did you install GHC, and on what OS?
I installed it via pacman on arch. I've updated my system and package manager just in case 
Oh okay, I misunderstood, it seems like you were saying that the . was used because of nostalgia. 
&gt; OTOH preaching a stranger on how to talk does look quite arrogantly. Then I apologize as well.
I'm glad that it is not at the forefront of research. If it was, I wouldn't be comfortable using it pervasively in production! That said, I think your other claims only hold if you have a narrow definition of "Haskell research" as "research about the Haskell language itself" and this is only true because the Haskell language is not undergoing as rapid evolution as in the past. It is no longer a research language, but it is nonetheless a language that much research is done _in_. If you look at the papers accepted to the latest symposium, for example (https://www.haskell.org/haskell-symposium/2015/), you will find papers on code generation libraries, probabilistic programming, concurrency libraries, testing libraries, distributed programming libraries, etc. So yes, Haskell isn't the language of twenty years from now. It is the language _of now_. :-P
Haskell's record syntax has always been just fine. It just hasn't been very powerful. Every proposal to "change syntax" has actually amounted to introducing new type system features without admitting it.
Putting an `undefined` in your code is the moral equivalent of writing `while (true) {}`. You shouldn't do that in production code, even though the compiler can't really stop you from doing it.
&gt;Comult is like doing a "deep copy" of an object to obtain an independent version of it. This is interesting. So are 'co' things really about the types only? I would presume that flipping the arrows on `mappend` would require an implementation that can pull apart, or otherwise split the input into something that could be merged again to the original value with `mappend`. A deep copy would satisfy the type, but not the inverse of the implementation. Is this how I should think of duals?
I don't think there is any "misunderstanding of theory" there as much as you may not like all of the choices Haskell makes. I think it is an excellent rundown of _many_ features of the type system, and the possibilities they enable, which seems to answer well what was asked.
what does it say in front of your prompt when you open it up?? does it say `Prelude&gt;` ?
what happens if you simply input the function in ghci? `let doubleMe x = x + x` and then use it `doubleMe 1` 
While that is unfortunate, I can empathise with Edward's explanation. This is not a garden-variety documentation PR, but a full-blown, mildly opinionated tutorial meant as an entry point to the package. Pedagogy matters, and there are non-obvious choices to be made in that domain. (I don't mean *at all* to say that the choices in the tutorial were wrong, but merely that there was scope for disagreement.)
Definitely have a look at [this chapter](http://book.realworldhaskell.org/read/profiling-and-optimization.html) of RWH and try to do some profiling. Be warned, though, getting the profiling versions of GHC and libraries installed can be a pain. If you're using Docker, have a look at [this PR](https://github.com/freebroccolo/docker-haskell/pull/33) for how to build your own container with profiling installed. Are any of these projects open-source?
Interesting question - and I don't think I have the expertise or insight to offer you an answer. But you certainly _can_ just think in terms of the types, which gives rise to the resource-management interpretation I outlined - which does seem useful, so I think there's something there. I think it's also probably reasonable to use "comult" to split things, and find an operation that is more obviously dual to "mult"... but it's not obvious to me how you would do that in general, whereas the "copy/drop" ideas seem pretty generally applicable - even showing up to some extent outside of type theory, such as in the "resource acquisition is initialization" (RAII) idea in C++, with copy constructors and destructors acting like the comonoid functions. Anyway, I guess it seems to me like the "co" things are just about the types... and it would be _cool_ to find a particular monoid instance and a particular comonoid instance that "inverts" the monoid, but we can also in many cases more easily find some monoid instance, and then find some other (not-necessarily-related) comonoid instance - it's a comonoid because it has the right structure at the type level, but not necessarily the particular comonoid we would expect given the monoid. I also don't know of a "less general" interpretation of comonoids that corresponds more closely to a particular datatype - this one is pretty general and I could envision it working for many kinds of data - but I suspect that such things must exist and probably some of them are more like what you presume. 
how about posting an example of when things like this happen :) maybe we can look through and tell you what's going on? 
Also a nice suggestion! You'd limit yourself to ghc &gt;= 7.4 though, which is probably okay for most applications, but perhaps not for all (cabal currently officially supports from 7.2 and up).
XPost Subreddit Link: /r/compsci Original post: https://www.reddit.com/r/compsci/comments/3g86pl/type_comprehension_an_idea_i_had_for_more/
Admittedly, I hadn't heard of that. But this is a bit more complex as far as I can tell. Refinement types seem to be centered on predicates. While what I'm writing about focuses on treating types as representation of computation. As in, given an input set, the type system knows the output set exactly. This can be used for type safety as well as letting compilers perform greater optimizations. Additionally, having the return type represent the computation makes for an interesting concept where the entirety of a programming language lies on a foundation of set theory, as there are no function bodies, only return sets.
As I said in another comment, refinement types restrict on predicates, not on comprehending the return type. With a type comprehension, you can define the entire computation in the type, which lets callers of the function determine exactly what set of values they can get in return. Because it represents a computation, it can call other functions and return a type that depends on the contextual return of that function.
&gt; You say you're doing lots of maps, zips, etc. You should definitely look into stream fusion in that case. map f . map g will usually mean you're making two different vectors for each map (unless I'm absolutely wrong and there's a rewrite rule in-place; someone smarter please correct me). This is already done by the library itself. For example, `map` in [Data.Vector.Generic](http://hackage.haskell.org/package/vector-0.11.0.0/docs/src/Data-Vector-Generic.html) is implemented as: map :: (Vector v a, Vector v b) =&gt; (a -&gt; b) -&gt; v a -&gt; v b {-# INLINE map #-} map f = unstream . inplace (S.map f) id . stream
&gt; we need a proper treatment of benign effects It's always been very unclear to me what a benign effect actually is. Can you explain?
&gt; which is a question about people and culture, not about technical capability. Interesting that so few commentators can see the difference. Ha ha, it's a question about the intersection of the two of course! And obviously programmer geek types are going to want to explain the latter rather than the former. If you're so interested in explaining the people and culture side go and write you own answer. If not, please stop harassing us with your snide comments about mainstream adoption of Haskell.
&gt; Does it have a lot of marketing effective at making it seem like learning haskell is the rite of passage to some semi-secret society of enlightened über-programmers? Yes. Wow! Who's doing this marketing?! Are they doing it by accident!?! Sheesh, it's hard enough to get a code sample on the front page of haskell.org changed, let alone market Haskell so it seems like we're "some semi-secret society of enlightened über-programmers". Either brighten up your tone of get out of this Reddit. Your snide remarks aren't welcome here. 
&gt; Be warned, though, getting the profiling versions of GHC and libraries installed can be a pain. An understatement
[Coq has exactly this, even with the set comprehension notation](https://coq.inria.fr/distrib/current/stdlib/Coq.Init.Specif.html). They're called sigma types and implementable in many type systems.
Idea: **LTS 3 (GHC 7.10.2)**, [LTS &lt;num&gt; (&lt;most distinctive difference&gt;)] Or something similar
I haven't been using either of those things.
I was thinking about that but the code isn't the greatest and I didn't want to put you guys through that ;) I'll do it though since you asked.
&gt; But to be quite blunt, Haskell hasn't been at the forefront of anything for many years. This seriously overstates the case. While the core type theory of GHC has a low power-to-weight ratio (it's both more complex and less flexible than a full dependent type theory) the surface language of GHC Haskell definitely marks out where the forefront of research on type inference is. The other important thing is that research is **re**-search -- we often have to revisit the same ideas numerous times before the common structure becomes visible. For example, Haskell's most distinctive type system features -- higher-kinded polymorphism, type families and type synonyms, typeclasses, newtypes, and GADTs -- can all be understood as arising from the same fundamental design principle, which is the importance of maintaining the admissibility of injectivity for all type operators. (I'll blog about this sometime -- I tried to explain it briefly but it's either too long or an incomprehensible mess.) 
Then you seem to be getting strangely worked up about my approach to doing something that you aren't actually interested in doing. 
See my blog post, "Functionality, Mutability and Non-Determinism in Type Theory" for a detailed explanation. In short, a benign effect is an effect used in the service of implementing an extensional function.
I'm not worked up about your approach. I'm worked up about your rudeness. I don't want to see it here.
I don't know if you have tried, but programming in Coq is actually rather natural. It feels like a pure functional language (unlike Haskell, which has exceptions and non-termination). It lacks some useful syntactic sugar (Idris feels more user-friendly in this regard), and expressing general recursion is difficult (that arguably is a problem shared with Idris or Agda, unless you disable termination checking, but Coq termination checker is arguably more naive than those), but from a ML or Haskell background you really feel at home.
It's well defined given the direction of the fold. Left folds associate one direction, right folds associate the other. 
This is awesome. Please make more
OK, that gives me a bit of a handle on what's going on, thanks.
No problem! I apologize if anything I said was cryptic, and I'm always available to help.
This blew me away, good work!
Yes, particularly for `vector`-based code
If an operation is non-associative, then there are a, b, and c such that (a ⊕ b) ⊕ c ≠ a ⊕ (b ⊕ c), but then the ⊕/ function would obviously not be well-defined for all lists of 3 elements.
&gt; universes Slightly off-topic, but could you give/link to an explanation of what "universe" means in this context? I've seen you use the term before, notably in `vinyl`, but I don't think I really understand.
&gt; the category is of constraints and instances is thin I think there's a typo in here, and I can't figure it out. Could you briefly explain what you mean here?
&gt; Your snide remarks aren't welcome here. FWIW, I hadn't really taken this particular comment from /u/keithb's to be snide. As clarified in follow-up posts, the point isn't that there's *actually* a secret society, but rather that non-Haskellers get that impression for whatever reason: Haskell still has a bit of an image as cryptic, elite, difficult, etc. It's a serious topic for conversation, and not an issue with a clear resolution: we're still a strongly academic and research-focused community, but there's a lot of pushing towards accessibility. I don't think the two things are mutually incompatible, especially because a lot of the academic tendency is to make things simple and unified, with the result that in my experience Haskell is a *simpler* language in many regards than other languages. But that's not (yet!) the popular perception of the language.
I did use the term in vinyl~~, and it was more evocative than accurate~~ (that is, there are technical differences that make * in Haskell not really a universe at all, even if it feels like one). (EDIT: Now, I remember what I had said; the thing I described in Vinyl is a perfectly fine universe à la Tarski; I was just now thinking of something different. In my above post, I was talking about universes à la Russell) Universes in type theory arose from the earliest efforts to enforce predicativity; check out the various Martin-Löf tech reports and introductions to type theory to learn about universes. Polymorphism in System F is impredicative, and this leads to a really nasty complication of its semantics; the logical relations are just gross. Another good source on universes and polymorphism in a predicative setting is the Nuprl literature. When i get to my computer, I can try and pull together some links. EDIT: - [Constructive Mathematics and Computer Programming](http://cs.ioc.ee/~james/ITT9200/martin-lof-computer.pdf) - [Intuitionistic Type Theory](http://www.cip.ifi.lmu.de/~langeh/test/1984%20-%20Loef%20-%20Intuitionistic%20Type%20Theory.pdf) - [Programming in Martin-Löf's Type Theory](http://www.cse.chalmers.se/research/group/logic/book/book.pdf) Also, Peter Dybjer's lectures at OPLSS may be of help: https://www.cs.uoregon.edu/research/summerschool/summer15/curriculum.html
Maybe, but that seem to address the same problem.
When I was asked earlier what I thought a smaller, cleaner language would look like I mistakenly tried to aswer that question, but it was the wrong question, because I had expressed the wrong desire before hand. The Haskell language is very small and clean (I stand by my observations about wonky syntax). What I really want, and what I think the mainstream would prefer, is a language with a set of stable libraries where it's pretty obvious which one is the right choice for a given task. And the use of those should be illustrated primarily by examples of use, and not, as now, by an exegesis of the implementation. Right now there are too many options and choosing between them often requires an appreciation of some very subtle theoretical issues. It's easy to understand how Haskell's libraries got this way, but for mainstream success the target has to be not plaudits on the academic conference circuit for most elegant application of the most subtle abstractions but rather lots of people getting stuff done. I suggest that too many haskell advocates are too tied up in the theoretical reasons why writing functions in Haskell-the-langauge should be quicker and safer and don't pay enough attention to the whole journey of a working programmer in industry from a user's idea of what they want to a working feature. 
mimeRender and mineUnreder are one liners too, thanks to JuicyPixels Content-Type Bliss https://hackage.haskell.org/package/servant-JuicyPixels-0.1.0.0/docs/src/Servant-JuicyPixels.html wow
"James Earl Douglas, Technical Lead at the Wikimedia Foundation, shares his recent experience using Nix to manage system state. He discusses installing it in Ubuntu, using Nix to install other packages, and finally how it interacts with GHC and Cabal" Wikipedia doesn't run any Haskell does it?
I have a long history of being feeling that the way /u/keithb expresses himself here is inappropriate, so my comment was in the context of that history.
~~There is *no* code that can be parameterized over `Functor` other than `fmap` itself! There's vanishingly little that you can write parameterized over `Monad` either, unless you combine it with other typeclasses.~~ Hmm, this isn't really true. See /u/mjmrotek below.
&gt; this complaint never seems to come from practitioners using the language Of course not. If they disiked it they'd use Haskell instead! NB Those that use F# for work are probably so grateful they don't have to use C# that they don't complain about it.
&gt; And when you see how elegant generic programming can be (at least in theory), what Haskellers do looks absolutely horrifying. Any examples of this elegant generic programming you can point us to?
&gt; somebody suggested that error should have been named bug I love this idea!
You talk a lot of Nuprl. Is it the best proof-assistant-thingy in your opinion?
I've ran into this very same exercise/problem. I don't think there's any solution. I sent Fokkinga an email at the time, but I never heard from him. :(
Java coders never whine about the lack of ~~ADTs~~ algebraic datatypes either... but they should! do those F# programmers know Haskell? in fact, when I'm "in the moment" writing idiomatic Java, sometimes it's like I've forgotten other languages. which is good, as other times, i remember and get mad at Java. 
I think it has the best type theory; I don't know if it is the best proof assistant... (It has a very special place in my heart, but I think that maybe today we can do a bit better.)
I might have said that, but I think I more likely would have said that I don't know of any.
I'm totally going to use this to render all huge libraries I can think of and post pictures. Hang on. Edit: it would be *so* awesome to see this being rendered in real time as `cabal install` is working. I'd even stop complaining about libraries with huge dependencies, I guess. ^(Actually no.) Edit #2: [here you go](http://imgur.com/a/EPmSl).
I also wanted to know which package depends on which, and that's not seen in the `cabal install` output. Luckily it's easy to dump it from the package database with `ghc-pkg dot`.
Partial applications of fmap. :-p
As for the package-graph, there's also ongoing work in https://github.com/haskell/cabal/issues/2728 which would enable you to get the relevant sub-graph as a side-effect/byproduct of `cabal install`
&gt; The only feature I might miss would be typeclasses. But I wonder if I will miss it all that much at all. ... &gt; I can't remember any time I felt compelled to write code parametrized over Monad or Functor. For most of my day-to-day programming, concrete implementations would typically suffice, and would relieve me of the complexities of typeclass resolution. Typeclasses offer a number of benefits that are not limited to having Functor and Monad typeclasses. Typeclasses actually had to be extended to represent those things. One thing that I definitely miss in other languages is the ability to have 'return-type polymorphism', that is to say, to write functions like fromJSON :: FromJSON a =&gt; Value -&gt; Result a For another thing, in ML how do you do something like mconcat :: Monoid a =&gt; [a] -&gt; a sort :: Ord a =&gt; [a] -&gt; [a] where lists magically have extra functionality if the type in the list is monoidal, orderable, has equality, a Show instance, etc. etc.?
&gt; I know that, in this example, we can guess the functionality from the name, but in general case, this is not always that easy. In the general case, if you can't guess the functionality from the name plus the type information, it's a bad name and/or type. That said, of course we still need to address that case.
Heh, a great point :)
Java has ADTs, for at least one meaning of ADT.
Lens?
Thinking about it. I guess the main problem is every single functions you are using needs to be annoted. Am I wrong ?
yes that. typo.
It's not so much the way Haskell does it as Haskell itself isn't able to do them as cleanly as a language with full dependent types. I don't have any nice examples off-hand to point to, but there are lots of examples in Idris and Agda.
I think they use PHP and that James has been learning these things for personal use.
To be real, I'd probably be pretty happy with a much more basic type system as long as it had sum types. Like Golang with sum types. edit: actually rust or maybe ocaml are probably good examples
Sounds like a case where you just have two instances of the same class. I don't think the data structure has much preference if we call it "left" or "right". [Or from the "past"](http://package.elm-lang.org/packages/elm-lang/core/1.1.0/Signal#foldp) :)
&gt; I would really like type classes to be a bit more structural rather than nominal. I feel exactly this way. Classes should represent a "structure" on a type. Something like `Num` tells me nothing about the type. I'd prefer to introduce semirings into the general programmer's vernacular. &gt; This would work particularly well in dependently typed languages One idea I've had kicking around is that properties could be encoded, even if the language didn't have full dependent types. Then, these properties could be fed into a quickcheck-like framework for easy testing. It would also discourage people from breaking the rules.
LiquidHaskell can infer properties of functions based on how they're used, so no, you do not need to annotate every function you use. There are, however, two limitations to our inference capabilities: 1. The properties we infer come from predicate fragments that we call qualifiers (or sometimes templates). A qualifier is a logical expression like `x &gt; 0` where `x` can be replaced with any program variable. There are two sources of qualifiers. First, the programmer can explicitly supply them as special comments, just like our specifications. More importantly, we mine qualifiers from all of the provided specifications. For example, if I annotate a function with the type `x:Int -&gt; {v:Int | v = x + 1}`, we'll extract the `v = x+1` as a qualifier, and then try to prove it for each pair of integer variables in your program. The way this often works in practice is that you write out the top-level specifications for your API functions, and LH fills in the blanks for internal functions based on the templates it extracted from your API. 2. We cannot soundly infer properties of exported functions, because we only see a subset of the concrete inputs that may be supplied (precisely the call-sites in the defining module). If we tried to perform inference based on only the visible call-sites, we could easily infer overly restrictive preconditions, which LH would not know to check in client modules. So we just require that you supply a specification for exported functions, or accept an unrefined type. In practice, we haven't found this to be a particularly harsh restriction, as the exported functions tend to be part of the API that you would want to document anyway. In that case, the liquid types become a form of machine-checked documentation! Hope that clears things up a bit!
This is almost like writing a type and having the compiler fill in the only sensible code that fits the type. This is awesome!
You can promote any semigroup to a monoid *by introducing an element that might not actually make sense in your domain*. Don't do that.
No. You can be simultaneously infinite in both directions and still usefully foldable. With trees you can even be infinite in more interesting ways, just along specified paths...
It still sounds like this would just be a matter of having multiple instances of the same class. Perhaps you can give a concrete example if I'm misunderstanding.
&gt; Then, Nuprl's type theory is really a system of type refinements overlayed atop this syntactic type system This is how I want to think about it also. Unfortuently, it is inconsistent with the view of type refinements as functors that Zeilberg came up with (since types are interpreted as PERs not subsets the map from typed terms to their underlying term is not functional in the sense that it does not respect equality) and I find that really annoying. 
&gt; I'm honestly not certain how a non-directional fold would work Keep in mind that this is a mathematical definition, not a Haskell program. Mathematical definitions don't need to specify how a value is computed, they only need to define a value sufficiently precisely that only one value fits the bill. In this case, the expression `a_0 ⊕ ... ⊕ a_{n−1}` appears ambiguous, because the text did not specify whether `⊕` was left or right associative. Normally this would mean that `⊕/` is ill-defined. However, the text did specify that the operation is associative, so it doesn't matter where we put the parentheses, the result will always be the same value, and that value is the value which this mathematical definition is giving to `⊕/as`.
I can promote a functor to a monad by Free. Are functors useless?
I'm not sure what the proposition you just proved was. But it doesn't alleviate the pain this causes. All I want is to define a datatype with named slots of a concrete type so I can read in records from a database. But currently in Haskell, I have to do my own name-mangling, and I have to use function calls, which is slightly unnatural.
I don't particularly disagree with you, although I'm not sure that "tone" is precisely the issue. I try to keep conversations with authors like him "shallow," in the sense of making only one or two responses in any given conversation, in an attempt to extract what constructive discussion there is to be had without getting caught up in a personal argument.
I think you don't know what you're saying. Why should I ask for something with an extra empty element if I know I'm working over a structure that cannot possibly be empty. That seems like a waste to me.
http://comonad.com/reader/2015/free-monoids-in-haskell/ &gt; And thus, the most fundamental operation of `Foldable` is not `toList`, but `toFreeMonoid`, and lists are not free monoids in Haskell. 
That's a really interesting point. Do you have any ideas on how to get around it? EDIT: I would just mention that for my purposes, as Philip mentions, refinements *HAVE* to be based on PERs, my comments basically rule out any formulation based on subsets. Luckily, there are plenty of ways to look at type refinements that are based on PERs.
I'll repeat that I'm open to any convincing examples of semigroups that come up often enough to warrant the semigroup class.
Point taken that lists are not free monoids. Although it seems to be an issue due to the non-distinction between finite and potentially infinite lists in Haskell. EDIT: This is actually an interesting article. Thanks for the link. 
Morally I don't have a problem with `error`, but I consider it generally to be a mistake in user-facing application code - because if it is recording an impossibility, I want both to inform the user that something went wrong (usually with minimal detail), and somehow send back to me in great detail what happened. If it's some little tool that I'm using myself, then `error` works fine. And if it were in library code... I would consider that a pretty serious mistake. Library code, when it can't eliminate error conditions by design, should hand back the decision of what to do in error conditions to the user, not explode in your face.
I've used it to prototype web apps before, it's astoundingly powerful and concise. Unfortunately for most of the ones I've done I didn't have the freedom to implement the application in Haskell, but it definitely makes for fast prototyping and design. The URL types read like a design specification that gets automatically turned into code.
Ah... I thought "non-directional" was an *assertion* of some sort. You're saying that with the provided constraints, it can be *derived* that this fold's direction is irrelevant. That makes more sense. :) P.S. `associative and has a neutral element` means `⊕` forms a monoid, right? Did I has a learning?
Please provide a .cabal file - it makes building your code a lot easier. FYI - it seems to require `MissingH`, `mtl` and `monad-loops`.
Isn't it straightforward to disprove this? Any well-defined fold ⊕/ will be well-defined on `a, b, c` which entails that `(a ⊕ b) ⊕ c` and `a ⊕ (b ⊕ c)` are the same. Therefore well-definedness implies associativity and it is necessary.
cool! Have you worked remotely? I personally prefer coming to the office even on days when I can stay at home, as it's nice to talk to people. But being able to work remotely all the time gives you a lot of flexibility for traveling and rent and stuff. Which doesn't sound too bad :-)
My experience is that for any function that is annotated, that function is checked anywhere it is used. For instance, if some function `f` has an annotation, and you call it within `g`, if `g` breaks the invariant of `f`, liquidhaskell will yell at you. What this means for you, is not *every* function requires an annotation. You can annotate certain important functions, and any time you use this function, even in an un-annotated function, the system will check that call.
One thing that jumps at me is your use of inefficient list operations, including `last` and expressions like `xs ++ [x]`. Since `positions` is essentially a stack, one trick is store the list in reverse so that you push a pop from the front of the list, e.g.: popPosCollection = do (RegexS g t ps) &lt;- get put $ RegexS g t (tail ps) return $ head ps Alternatively use a better data structure like a [Seq](https://hackage.haskell.org/package/containers/docs/Data-Sequence.html) Another place where you are using an inefficient list operation is in `currentChar` - which I'm sure gets called often. How about defining the `text` field of a RegexS as a `Text` value and use `Text.index` instead of `!!`? Just change line 238 to: let (matched, st) = runState re (RegexS [] (Text.pack str) [[i]]) to create the the RegexS state with a Text value. 
[already posted before](https://www.reddit.com/r/haskell/comments/3evhy8/optimising_garbage_collection_overhead_in_sigma/?ref=search_posts)
Great! Would be nice to see an announcement, when you do.
I agree pedagogy maters. Excuse me if I get up on my soap box, but I think this is a very important issue. What do you think one of the major complaints about Haskell is? It's the ivory tower attitude. Now lens is a very important library, not only in Haskell, but in the entire field of computer science. But it's not a mathematical textbook, it's an open source library. I'm not saying anyone should be have to accept a pull request that is not up to the quality of the library, but Gabriel is known to be one of the best tutorial writers in the Haskell community. I read through it, and the tutorial struck me as a entirely pragmatic introduction to the concept of lenses, and a step above the average introduction to lense blog post. Yes, it's a totally different tone than /u/edwardkmett materials (where's `f s t a b`?) I've studied mathematics, so I know how you're not really getting the point if you ignore all the details. At the same time, I've been a new Haskell user spending countless hours deciphering the lens package. And will most people ever care about the about lenses and prisms being categorical duals of each other? Probably not, so why are simple well written guides being rejected?! Downvote away, but I think there are many questions around documentation that really need a debate. Are some guides just "too simple" to be in some of the more advanced libraries? Should we even ask that library maintainers provide accessible documentation? In any case, I look forward to seeing a better tutorial in this ones place, I am very glad to see that it seems to being prioritized highly.
Thanks. (It wasn't me who was insisting, but I was curious too and I'm glad to read an explanation.)
I haven't, but it's something I'd definitely like to try at some point.
Without an actual example of what you're doing, we're just guessing. Why would you rather put us through that? ;)
&gt; using more memory unexpectedly also resulted in longer pause times. Why is that unexpected? The GC has to traverse more memory which takes more time. Sounds straightforward to me.
Ah, the /u/Tekmo gambit!
Executor is "Codensity" http://hackage.haskell.org/package/kan-extensions-4.2.2/docs/Control-Monad-Codensity.html I wrote an article series several years back introducing folks to the concept: http://comonad.com/reader/2008/kan-extensions/
The code for the instances is all the same. The quantifier is enough to rule out `callCC` though, which can be a good thing or not depending on your usecases.
Err, well I thought like /u/sclv's answer the only objections would be trivial ones, but actually it seems that you are right!
Thank you! The post is very insightful. It's so cool to see the things I come up with be already implemented and well thought thru!
Does the Codensity you refer to have anything to do in theory with [the one of "profunctors"](http://hackage.haskell.org/package/profunctors-5.1.1/docs/Data-Profunctor-Codensity.html)?
Another construction that is useful for working with applicative functors in particular is the "right kan lift". http://hackage.haskell.org/package/kan-extensions-4.2.2/docs/Data-Functor-Kan-Rift.html We use this in lens for one of the most confusing combinators we have: http://hackage.haskell.org/package/lens-4.12.3/docs/Control-Lens-Traversal.html#v:confusing `confusing` lets you fuse together multiple traversals more efficiently, forcing the associativity of (&lt;*&gt;)'s to the left with Rift, and fusing (&lt;$&gt;)'s with Yoneda to make it so deeply nested (or uninlined) traversals perform well. It associates in the other direction compared to `Codensity`, though.
It does, but you need a fair bit of theory to see how they relate. They are two instances of the same concept, just on different categories.
With `forall z` this is very similar to `Codensity` which is less powerful than `ContT` (no way to express `callCC` for example). There is a very nice Edward Kmett's article about this: http://comonad.com/reader/2011/free-monads-for-less/
Eyup. And that's also the basis of Haskell's non-directional fold: fold :: Monoid m, Foldable t =&gt; t m -&gt; m
Have you looked at how algebraic structures are modeled in the Agda standard library? For example, [something `IsMonoid` if it `IsSemigroup` and has an `Identity`](https://agda.github.io/agda-stdlib/Algebra.Structures.html#935), and it re-exports most of the `IsSemigroup` fields so that when you open an `IsMonoid`, you also get the semigroup stuff in scope. I think it is very close to what you envision (and /u/runT1ME, it's not science fiction:) )
That was a good read, especially the part about choices. Currently, my engine doesn't make many choices and applies the patterns naively. I was thinking of evaluating all possible outcomes by branching every time a pattern has more than one match.
Abandon is an overly strong way of putting it. `relational-record` is wrong only under a few specific circumstances. Rather than orphaning that instance though, I think a `newtype` would go down well :)
Tracking here https://github.com/tomjaguarpaw/haskell-opaleye/issues/95
First, last, min, max...
Is there any other output? I'm not familiar with OpenShift but similar issues have come up on the Yesod mailing lists before: https://groups.google.com/forum/#!searchin/yesodweb/openshift
I've managed to run a Yesod app on Openshift as a proof of concept. The effort is squirreled away on a virtual machine somewhere, so I can't provide any details (yet) -- apologies in advance. My recollection is that I did not use the Haskell cartridge(s). I used a "DIY" cartridge that would allow me to upload a static binary (created locally) and run it.
I wrote up a post that explains the intuition behind the `Functor`/`Applicative`/`Monad` instances for the continuation monad (which is basically the same as `Executor`/`Codensity` in terms of how the instances are implemented) and provides simpler versions: http://www.haskellforall.com/2014/04/how-continuation-monad-works.html
You replied in the wrong place
It is because GHC is very memory hungry, and the 3 gears (1,5GB) isn't enough, so it fails.
You can get this with univalence, I think. Though that may seem a rather large hammer for such a specific problem. And you still have to write the proof yourself.
List+
Max and Min fail to be an example, as it usually makes sense to extend your ordering with a maximal or minimal element.
It's interesting that the actual time in the famously complained-about `lens` doesn't appear to be `lens` *per se*, but the fact that a lot of it depends on first compiling `text` and `vector`. In fact, those two show up as long poles everywhere they're included. The price of heavy optimization?
I'm thinking that there has to be a way to replicate the GC advantages Erlang has, in the GHC RTC. In Erlang this is done by partitioning the heap by actors. In Haskell, it seems like there could be a way to partition the heap both statically by type and dynamically by thread.
Thanks a lot for the feedback! Seems like I'll have to upgrade my gears, or try to upload the application as a binary blob and run it that way.
Thanks for this. It's excellent.
In your example, the divide function only takes one argument, x, and returns a function. (divide _doesn't_ take the y argument.) The function it returns takes the y argument and returns x/y. 
&gt; although we know that there are more powerful ones In a mature language with industry-strength libraries, an optimizing compiler, and a sizable userbase? Not that I know of. I mean, I write in Agda, and while its type system is certainly powerful, I wouldn't even think about doing any "real programming" in it yet.
(This is totally tangential, but) one thing that kinda bothers me about GADTs is how the type argument in cases like these no longer needs to have any clear and direct relationship to the contained data. It's now just a mnemonic. You could as well have done: Pair : Expr a -&gt; Expr b -&gt; Expr (a -&gt; b) Fst : Expr (a -&gt; b) -&gt; Expr a Snd : Expr (a -&gt; b) -&gt; Expr b or any other binary type constructor. (No real conclusion, just... felt like writing that down.)
I don't know of a language with those characteristics and a stronger typesystem that Haskell's, either. What interests me is how rarely that set of tradeoffs is mentioned while people are propagandising in favour of Haskell over languages also with excellent library coverage, very good compilers and VMs, *very* large user bases and *much* better tooling, but weaker type systems. 
I think anyone *promoting* something is going to downplay its downsides. That's kind of what it means to *promote*. Besides, I doubt anyone needs to be told that Haskell has its downsides. There's also a kind of chicken-and-egg problem: the only way for Haskell to get broader library support is if it gains a larger userbase.
Certainly. I'd just prefer it if people who are spreading biased, favourable propaganda, which isn't reprehensible thing in general, admitted to it rather than giving the impression that, for example it is the [manifest destiny](https://www.reddit.com/r/haskell/comments/3eschl/examples_of_functional_thinking_successes_in/ctiu1aj) of (the kind of programming that) Haskell (affords) to solve all software construction issues definitively as if that's simply a fact. As I've said elsewhere, in too many areas where Haskell does have library cover, it has too many libraries—partly as consequence of researchers' turf wars. Also not reprehensible in general, but it does make life needlessly complicated for people who just want to get stuff done. *Edit:* wrong link, fixed.
[**@brian\_bilston**](https://twitter.com/brian_bilston/) &gt; [2015-08-08 20:37 UTC](https://twitter.com/brian_bilston/status/630115561599561729) &gt; Here is a poem about selfies. I have called this "Selfie Stick". &gt;[[Attached pic]](http://pbs.twimg.com/media/CL6ewJSWgAAbQvs.jpg) [[Imgur rehost]](http://i.imgur.com/bhJuAuv.jpg) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
Sometimes, things are a bit more complicated. Did you try to view some of the functions types and names in the lens or diagrams library?
It was just a quick hack, to see how it would work to do a universe of HTTP APIs. I haven't really touched it since, and it's not really something that can stand on its own. If someone wants to run with that as a way to learn Idris, though, I'm happy to help out.
Thanks, Gabriel!
Thanks!
You're welcome!
Well, not really, because in choosing the type here you are encoding a function from the GADTs values to a type (the constrained parameter instantiation). The type-level function I want is "the one I'll need in `eval`", not the one you suggest. This aspect also highlights the limitations of GADTs: encoding functions when defining the datatype is not modular.
You should try to implement Thompson's DFA construction.
If you're prepared to discard the ability to enumerate the elements of a set (which doesn't make sense for 'complemented' sets) there's also the "set" type `a -&gt; Bool`, for which it's trivial to implement a ring :)
Hmm, good point.
Yep, the ghc version is already clearly mentioned like that on the lts webpage: eg http://www.stackage.org/lts-2
lts is updated weekly
What inference algorithm do you use? [My Master Thesis explored the use of a modified Algorithm W + HM to do so, with encouraging results](http://dspace.library.uu.nl/bitstream/handle/1874/294073/thesis.pdf?sequence=2).
Sorry for that, I just fixed it. The repos is here: https://github.com/agrafix/elm-bridge This library implements exactly what you are asking for: You can use types you defined in Haskell in Elm and get JSON (de)serialisation for free.
&gt;I like to write and use parametric code because to me the typeclasses are much less "magical" than concrete types and concrete operations on them. I'd like to expand on this: with a type class, the set of possible operations is closed and easy to find in the documentation. So it's easier to reason about the implementation of a parametric function just by looking at the type. With concrete types, anything goes.
That's another way. OTOH many have .travis.yml and the transition to apt-plugin based infrastructure is (IMHO) easier in this way (very little editing of existing configuration file) than via conversion to Haskell eDSL. I also hope that this feature will be in travis itself at some point. (Or they drop features from .travis.yml "language", so it'll become better "compilation target").
Currently I have only implemented what I need for my projects, but of corse this can and will change so I'll update the library accordingly. Pull requests are welcome, too! ;-) Two things to keep in mind though: Some Haskell types don't make any sense in Elm, or you can not send them over the wire (e.g. functions). For sum types with named fields: I personally dislike (and not use) them because they result in partial functions...
&gt; For sum types with named fields: I personally dislike (and not use) them because they result in partial functions... That makes sense. I think I will adopt this practice.
For access in a shader, yeah. For vertex buffers, I’m not that sure. We use the `stride` parameter to tell OpenGL how to build vertices back.
&gt; somebody suggested that error should have been named bug My usage of the function is limited to `error "foo: Should never happen"`. `bug` would be perfect. 
I do rather like sum types with named fields in Elm. E.g., type Shape = Circle { radius : Float, center : (Float, Float) } | Rectangle { width : Float, height : Float, center : (Float, Float } rather than type Shape = Circle Float (Float, Float) | Rectangle Float Float (Float, Float) since I would forget things like if the first argument should be width or height.
Based on the same idea, but here is a script specific to a large multi-project/version build plan in a purely-functional-comes-with-many-dependents language: * [Script](https://github.com/brendanhay/amazonka/blob/7fec5dff0fe12c078af49b5ea6c5970c82766e40/script/travis-matrix) to generate the matrix. * [Example output](https://github.com/brendanhay/amazonka/blob/7fec5dff0fe12c078af49b5ea6c5970c82766e40/.travis.yml#L36).
Unfortunately it doesn't have a huge impact - It's just grabbing the apt packages from a S3 bucket.
According to [documentation](http://docs.travis-ci.com/user/caching/#Caching-Ubuntu-packages) it's only available to private repos :-(
Author here. I wrote this up as a summary of some recent work which I thought was interesting, in PureScript's core libraries. It's not really meant to be rigorous in any sense. That said, any feedback is greatly appreciated. _Edit_: Also, here is the code from the paper: [FreeT](https://github.com/paf31/purescript-freet), [Coroutines](https://github.com/paf31/purescript-coroutines), [Operators](https://github.com/paf31/purescript-safely), [Misc.](https://github.com/functorial/stack-safety-for-free)
Yes, I know. I was saying I like it elm since this problem doesn't exist there. 
Yeah, the point is that in Elm records are first class. So, e.g., { width : Float, height : Float, center : (Float, Float } is just a standalone type, like a tuple could be in Haskell.
I've created Inkpot inspired color scheme https://github.com/niteria/haddock/commit/3b6645b57a6dcc7693074af269685238f5f6e707 for my own use. You can see it in action here http://covers.nmstudio.pl/ghc/src/PrelNames.html
I'll try and graph them out. Thank you!
I will even send you a message :-)
`Floating` is a type class in Haskell. You need it because `/` is a function that only the `Fractional` types know about. `Fractional` is a 'superclass' of `Floating`. To see this, open `ghci` and type in :i Floating You will see all functions that are described in the `Floating` type class, as well as `class Fractional a =&gt; Floating a where` which describes the relation. The `=&gt;` just means that is the end of the type class information. We don't just use `Int` or `Float` because we don't want to restrict this function from only working on a single type. Haskell has (parametric) polymorphism which allows us to write a single function that can work on multiple types. ---------------- As for your function, I think you want something like this. isEven :: Integral a =&gt; a -&gt; Bool isEven = (== 0) . (`mod` 2) This may look confusing because of the `.` function. It has the following type signature. (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c This is an example of how it can be defined. (f . g) x = f (g x) Essentially, it takes two functions that both take a single argument. The argument of the second function is given, it evaluates `g x`, which becomes the input to the `f` function.
While I appreciate the goal of this post, it has some real practical problems. &gt; For example, the character in your game can be alive with some HP, unconscious, or dead with a high score. The type system can't prevent me from creating a `CharacterStatus` of `Alive (HP (-10))`. Unless this is a legal HP value in your system, you'd have to introduce non-zero nats in order for this to be compelling. This *could* be done and polymorphic `Num` literals will even make it not completely awful to use. &gt; [Monads are] just a few functions that deal with putting things in containers and taking them out. &gt; That’s it. &gt; IO is a container, so it would make sense be able to get the value out. No. I don't want to be overly negative, but literally everything about this is wrong. Monads are not containers. They have nothing to do with "putting things in containers and taking them out". The Monad interface doesn't provide any way of "taking things out". IO is actually a great example of a monad that is *not* a container. IO actions don't hold anything so there is nothing to "take out" of the "IO container". This is an almost perfectly wrong explanation of monads. &gt; It knows by the type signature which says ‘this function will give a string that is in an IO container’. Again, a value of type `IO String` is *not* "an IO container" of a String in the same way that `ls` is not a container of file names. An `IO String` is a *recipe* for performing IO that can be executed by the runtime to provide a String. Lest you think I am being overly pedantic, this is something that many newcomers are *frequently* confused about and which shows up *repeatedly* on IRC and mailing lists, and I would really not like to see this misunderstanding perpetuated by obviously well-meaning authors of monad tutorials. Frankly, it's irresponsible to commit yet another [Monad tutorial fallacy](https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/) after we've already had [so many of them](https://wiki.haskell.org/Monad_tutorials_timeline). 
I'd consider just using the [retry](http://hackage.haskell.org/package/retry) package to implement a handler for that specific status code using `recovering`. This way you can let the `HttpException` be thrown and then catch it in a `Handler` which returns `True | False` if the action should be retried (ie. the status code was `503`). You can then specify the delay, number of attempts and use the offered exponential backoff strategy etc as part of the `RetryPolicy`.
I hear you. The CharacterStatus example was just meant to show the existence of sum types to someone who is completely new to Haskell, or even completely new to type systems. It's not comprehensive because it is just a basic introduction. I would argue that the basic form it takes is a vast improvement over similar code in Python, and so still has quite a bit of value. Viewed from a Haskeller, it's too basic. From a Python programmer, it's potentially revelatory. You're absolutely right, IO is not a container. Except that shortcut-idea was how I got my foot in the door to understanding them. Speaking of which, and I'm asking honestly, can you point me to the best Monad description you know of that's meant for coders from Dynamic languages? One of the challenges I see is that every Monad explanation that is comprehensively true....is kinda terrifying and offputting to coders who are new to this stuff. So, I feel there are two options. Give people an incomplete idea of a Monad, but something that will allow them to start writing code. Or give them an overwhelming explanation and hope they don't think Haskell is too obtuse to use. However, I'm already thinking about what I can do to amend the article, or make a note. I'd hate to irresponsibly commit a fallacy. Any suggestions are very welcome. Anyway, thanks for reading my article and I really appreciate the feedback. **Edit:** So, after some thinking, I've removed the part about Monads, and made a note about the negative HP thing in CharacterStatus. Really it came down to a few reasons for removing the Monad section. One, describing monads needs some extra care and thought:) I think a purely practical article showing the same patterns across different Monads may be effective, and I like the build up of using then, but both are outside the scope of this article. Two, the monad section wasn't really adding anything to the article and sort of went sideways from the theme of 'the benefits of the haskell type system'. It was trying to be instructive while the article is really about getting more awareness of why the Haskell Type system is interesting. So, Thanks for your feedback! 
&gt; I would argue that the basic form it takes is a vast improvement over similar code in Python, and so still has quite a bit of value. Sure. I think mentioning as a caveat that HP values are not fully constrained by the type system would help there. &gt; Speaking of which, and I'm asking honestly, can you point me to the best Monad description you know of that's meant for coders from Dynamic languages? It isn't meant for that audience specifically but Wadler's original paper on [monads for functional programming](http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf) is a very approachable introduction with few prereqs that doesn't resort to making bad analogies. The thing that most people seem to get wrong about Monad tutorials, in my experience, is that it's very easy to explain *what* a monad is: It's a typeclass with two essential operations and three laws. The challenge is to motivate *why* one might want to use that typeclass without making sweeping but wrong generalizations like "monads are containers" or "monads are for side effects". The best solution seems to be the one that Wadler chose: motivate the Monad typeclass by giving specific examples and then showing how they have a common structure. This method of moving from concretion to abstraction can be very successful. I have seen less success with the dual approach. &gt; However, I'm already thinking about what I can do to amend the article, or make a note. I'd hate to irresponsibly commit a fallacy. Any suggestions are very welcome. I appreciate your receptiveness to feedback and I realize that it was worded rather strongly so I especially appreciate your willingness to engage with it despite its abrasiveness. I would suggest taking the wording and concepts from Wadler's tutorial, as they (imo) explain the most while misleading the least. Edit: I would add that giving people an incomplete idea of what a Monad is can be a very successful strategy. The behavior of do notation in a `main` function can be entirely explained without mentioning monads at all. First, introduce a function `then :: IO a -&gt; (a -&gt; IO b) -&gt; IO b` (it's `(&gt;&gt;=)`). Then, write main using this form: main = readFile "somefile.txt" `then` \contents -&gt; putStr contents Next, show a mechanical translation from `then` notation to do notation. Finally, introduce `return` as a simple constructor of `IO` values. Now you have shown how to use do notation in main successfully without even mentioning monads. For newcomers who just want to write a `main` that compiles and runs, this can be much more effective than going down the "first, you must learn monads" rabbit hole. If you do the same for State, you might have `thenIO` and `thenState` methods, `returnIO` and `returnState` methods, and you start to see why a typeclass is beneficial. Explaining monads first and then showing examples has it precisely backwards, both historically and pedagogically. *First* we figured out how to do IO and State, *then* we figured out that they shared a structure that can be encapsulated by "monads". Trying to explain monads without concrete examples to provide motivation similarly puts the cart before the horse and is understandably frustrating to newcomers who just want to get something working without all this abstract nonsense getting in the way.
&gt;&gt; Non-authenticated API calls are rate limited to 10 per minute. Authenticated API calls are rate limited to 60 per minute. API call rate limits allow bursts up to 5 consequitive calls. Exceeding the limit causes error 503 to be returned. While I don't know the `http-conduit` API well enough to know the idiomatic way, I've been facing the same problem with other web-apis using [`http-streams`](http://hackage.haskell.org/package/http-streams). To that end, I implemented [`token-bucket`](https://hackage.haskell.org/package/token-bucket) which operates in the `IO`-monad for simplicity, and can be used like this: callService :: TokenBucket -&gt; SvcRequest -&gt; IO SvcResponse callService tb req = do tokenBucketWait tb 5 (1000*1000) -- i.e. 1 token per sec, 5-token burst-size {- ...perform the HTTP transaction... -} I'd usually set the rate and burst-size slightly below the official API specification.
Monads are definitely not containers, but that doesn't mean containers is a bad way of describing them. Sure, it's a lie, but you often can explain things by giving an incomplete explanation (which I think containers is). 
I have seen a lot of damage caused by these lies told to children, so I am very wary of them. I think that explaining monads by faulty generalizations and analogies rather than by motivating with concrete examples is a poor strategy and I see evidence for this almost every day. I also think it's a much more successful strategy when you have more feedback, like 1-on-1 or small group mentoring, and can respond to confusions and provide clarifications inline. As a strategy for blog posts and similar, where you don't have the chance to do this, I don't think it's as effective.
Don't talk about monads. Talk about the IO type, the Maybe type, the List type. Then, pull back the curtains and show that these are all implemented *in a single way*.
I think it depends on your application. If you're using a lot of techniques like this, where you end up using hacks at a low level to make it work, then you probably should just be using GHCJS or something similar. My use case was to build a coroutine library, where the higher level API is much simpler, and you don't need to see the details. I could have specialized types to safe monads, but to deal with things as polymorphically as I wanted to, this seemed like the better approach.
They all share a common interface, I assume you mean.
&gt; I'm not encouraging lying. A "[lie-to-children](https://en.wikipedia.org/wiki/Lie-to-children)" is a term of art that means exactly this. &gt; a monad only "contains" a value in a very generous interpretation of the word If you essentially are saying "A monad is like a container except that it isn't like a container", what's the point of introducing the analogy? What value is it providing? Maybe that's an unfair interpretation, or maybe there's value there that I'm missing.
 Path=C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Users\Harold\AppData\Local\Programs\minghc-7.10.2-x86_64\switch;C:\Users\Harold\AppData\Local\Programs\minghc-7.10.2-x86_64\git-2.4.0.1\cmd;C:\Users\Harold\AppData\Local\Programs\minghc-7.10.2-x86_64\git-2.4.0.1\usr\bin;C:\Users\Harold\AppData\Local\Programs\minghc-7.10.2-x86_64\ghc-7.10.2\mingw\bin;C:\Users\Harold\AppData\Local\Programs\minghc-7.10.2-x86_64\ghc-7.10.2\bin;C:\Users\Harold\AppData\Local\Programs\minghc-7.10.2-x86_64\bin;C:\Users\Harold\AppData\Roaming\cabal\bin; I've looked at the contents of those folders and I noticed that the `git-2.4.0.1` folder doesn't even exist, nor is there any `sh.exe` in any of those folders. It really looks like MSYS is missing.
&gt; So do you think Newtonian mechanics should not be taught? This is like saying "the earth is flat" and "the earth is a sphere" are both equally wrong. No, one is more wrong than the other. "Physics is newtonian" holds up very well in a wide variety cases while "monads are containers" breaks down almost immediately. In order to see why the newtonian model breaks down, you have to look at cases involving exactly the high energies or high speeds that other models are meant to address. The weaknesses of the Newtonian model actually motivate explanations of relativity or quantum mechanics. If you want to see the "monads are containers" model break down, you can look at almost any instance. `[]`, `IO`, `Reader`, `Cont`, `Parser`, etc. To make it hold water, you need to define "container" so vaguely and in such an ad hoc fashion as to lose any connection with our intuition for "containers" (I see your "IO is a container" explanation as a case in point), at which point the analogy obfuscates rather than clarifies. I think the lie-to-children approach can be successful, but some of these "lies" are more successful than others. Newtonian physics is very successful. "Monads are containers" is, ime, not very successful at all. As mentioned elsewhere in this thread, I also think we have better ways of teaching this subject, so I don't see why we should prefer this one.
That's pretty impressive. Unfortunately nobody answer on this thread that they were using it :-(
So, after some thinking, I've removed the part about Monads, and made a note about the negative HP thing in CharacterStatus. Really it came down to a few reasons for the Monad thing. One, describing monads needs some extra care and thought:) I think a purely practical article showing the same patterns across different Monads may be effective, and I like the build up of using `then`, but both are outside the scope of this article. Two, the monad section wasn't really adding anything to the article and sort of went sideways from the theme of 'the benefits of the haskell type system'. It was trying to be instructive while the article is really about getting more awareness of why the Haskell Type system is interesting. So, Thanks for your feedback! 
If you haven't read Isaac Asimov's [essay](http://hermiene.net/essays-trans/relativity_of_wrong.html), you might like to; it uses the same example and a similar argument.
As it says, try running `stack setup`. And stack doesn't need sandboxes for the most part.
Glad it was useful. More on topic for your article, phantom types would be a great example to include. An alternative to your units example could be given using phantom types: newtype Unit t a = Unit a data Meter data Foot Now the type system prevents you from, e.g., (Unit 1 :: Unit Meter Int) + (Unit 1 :: Unit Foot Int) with the advantage that your units are now extensible.
Being uninformed about everything gl related, is this the only way to have vertex arrays?
Then we can end by agreeing. :)
I forget if the main code is in [`reddit`](https://github.com/intolerable/reddit) itself or its [`api-builder`](https://github.com/intolerable/api-builder) dependency, but there's a very straightforward rate limiting implementation in one of them. It uses a `TVar` to track the timeout so you're not just polling with every pending thread.
It's just the wrong c word for the approximation. I think it's a lot easier and better to introduce them as "monads as contexts". The list context is "there are many of these", the Maybe context is "this may or may not be there", the IO context is "this is an imperative action producing a:..." Then when moving on to comonads, we refine this to "monads are values with contexts that are easily produced" and "comonads are values with contexts that are easily consumed". Though personally I am of the opinion that comonads should be introduced before monads as a concept when learning haskell. They're simpler, and the rules governing them are easier to explain (though many of their uses are not, the opposite of monads). Once someone is used to the core rules defining what a comonad is, a monad is just a matter of rearranging that understanding. But learning Haskell generally isn't a structured activity.
Mod is a prefix function. The only infix functions you'll run across are operators. `1 + 2` is the fully applied `(+)` function.`mod`is a prefix function however, and you have to surround it by backticks as Ankhers did to make it an infix function before you can section it.
Which error does `stack solver` give? When I tried stack on Windows I noticed that `stack solver` wouldn't work out of the box, as it depends on cabal-install, but I didn't investigate it further as I didn't actually need the solver. If that's the only problem, perhaps `stack install cabal-install` will be enough to make `stack solver` work.
Cabal-install is already installed, or are you saying stack needs its own copy? Also this is on another computer in another city, so I can't give you any details for about 2 days. I can only try it on this laptop, but it has a newer version of Windows.
&gt; Cabal-install is already installed, or are you saying stack needs its own copy? It shouldn't need its own copy (in Linux it works fine with the system-wide cabal-install). It doesn't look like the same situation then, as I noticed the issue only after uninstalling the Platform.
Actually it looks like a ticket has been opened for the problem since I last checked, and is listed as potentially solved. I'll have to try it when I get home.
I think QuickCheck and SmallCheck are also great demonstrations of the power and practical use of Haskell's type system. Just yesterday I had to write JSON instances for some records, involving a slight bit of complexity that I of course managed to screw up... Even so, I didn't even have to try the program, or write a bunch of tedious unit tests... I just wrote: testProperty "JSON instances work" $ \(x :: MyRecord) -&gt; fromJSON (toJSON x) == Just x And right away I found two mistakes. (I should be more careful!) To balance out all this cheerful positivity, I'll just mention that every time GHC whines at me that my `Data.Text.Text` absolutely cannot be used as a `Data.Text.Lazy.Text`, I feel a quiet rage smouldering...
Nice! I wrote a really weird rate limiter based on an exponential moving average formulation that apparently exim, the mail server, uses for limiting. Tomorrow I can throw it out and put in `token-bucket` instead!
I'm sure a units system should prohibit adding a number representing meters to a number representing miles to result in something nonsensical (at least, without very carefully demanding it do so). Obviously, it must be possible to add meters to miles in a correct way. Whether that happens automagically with an overloaded + sign or requires a little more verbosity is a somewhat separate question, and there are some reasonable arguments for both depending on your circumstances.
To me the best explanation is no explanation. Just the definition of return and &gt;&gt;= and some examples. That's not to sound arrogant - I needed a lot of examples to get it. List, IO, Reader, State, Transformers and slowly it clicked. Context is probably right, but it is hard to get what that means as a newbie. It is quite vague. What makes it hard is that design patterns in OO, although abstract sounding are very specific, and the monad pattern is so so flexible and allows all sorts of wonderful bindable actions.
It seems to me that adding meters to miles shouldn't require any coercion, neither explicit nor implicit. The signature should be Distance -&gt; Distance -&gt; Distance. Similarly, time intervals shouldn't have separate types for minutes and hours, but instead use a single type (with underlying millisecond precision or something). Otherwise you go crazy when you try using combined units, like meters per second... feet per minute... feet per second... area in meters multiplied by feet... ugh.
Also, thanks for the link -- very interesting work! -- is there a repo somewhere with the benchmarks and the contracts that were inferred?
I'm not sure, but I think the aforementioned `uom-plugin` does just that, using 'base' units and conversion ratios.
I am not usually a fan of implicit coercion, but implicit coercion from regular Text to lazy Text would be great. Doesn't make as much sense to go the other way implicitly, but automatic `Text -&gt; LText` is fairly benign.
 instance Num a =&gt; Num (b -&gt; a) where (+) = liftA2 (+) (*) = liftA2 (*) . . 
Thanks for explaining how to use `stack` but I think I'll stick with the standard Haskell package tool `cabal` for the time being. Contrary to the claim that `stack` comes with less pain it seems to me that `stack` is more confusing than `cabal`. With `cabal` I actually understand what's going on, but with `stack` I seem to require a totally different mental model. Maybe I just need to invest more time learning how to use `stack` as right now I don't see the benefit for myself as `cabal` just works^*) while `stack` seems to only add accidental complexity to my task. Also, I need to use Hackage rather than Stackage. ^*) the problem I'm having had nothing to do with `cabal` but rather with a broken installation lacking MSYS
I followed a hunch and installed msysgit and let it add itself to the `PATH` variable during installation, and now `cabal install network` works! I'm still not sure whether something went wrong during minghc installation or whether the minghc release I used was broken. But by providing a 3rd party MSYS all seems to work now.
Yeah, it seems many people love to say monads are so overrated in difficulty, but it really is a very abstract concept. I, for one, had to dive into category theory, and once I understood categories, morphisms and the basic building blocks of functions, monads made much more sense. The basic idea of functors could be explained with the container analogy but monads not as much. Monads are not just containers, they're patterns of processing containers, which is more abstract and not as easy to convey.
just out of curiosity, how did you end up getting `NoImplicitPrelude` accidentally into your ghci dot files?
I can imagine the whole thing being *very* confusing for newbies who are for example trying to combine two libraries that use different `Text`s. (Especially since Haddock doesn't differentiate between them at first glance.) Maybe GHC should print a message like "don't worry, click here to learn about lazy and strict text and how to avoid these errors: ...". Of course `text` is an external library. Maybe a pragma like this: {-# ERRORHINT Data.Text.Lazy.Text "It looks like you're trying to..." #-} data Text = ...
Beautiful! I figured I had something wrong. Looks like the field `n` only needs to be named in the liquid annotation (which is nice so not to clutter up the namespace with named fields), so {-@ data Nat = Nat {n :: {v:Int | 0 &lt;= v}} @-} data Nat = Nat Int bad :: Nat bad = Nat (-3) -- this is rejected also does what it should :)
&gt; It is Linux-only. While Docker does have some support for other host operating systems using boot2docker this has not been reliable enough in practice. In particular, since it uses VirtualBox under the surface, it relies on VirtualBox's extremely slow "shared folders" for bind-mounting directories from the host into the container, which makes it nearly unusable for Haskell builds. That is a real shame. How slow are the shared folders? Fast enough for GHCi?
My favourite bit: &gt; It is the mark of the marvelous toleration of the Athenians that they let this continue for decades and that it wasn't till Socrates turned seventy that they broke down and forced him to drink poison.
The docker run image (phusion/baseimage) is actually quite big. It would be awesome to start with the 5M image from https://www.fpcomplete.com/blog/2015/05/haskell-web-server-in-5mb. Is it something that is still investigated ?
A problem with these mini-images is the missing system infrastructure. A web-server can be run, but if you need DNS, ca certificates, a package manager etc., the image is too extremely trimmed down.
It't not a big deal. But I was very happy to see it. 
Thats right, I just put the `n` in the Haskell definition to help explain the syntax of the refined definition!
The Mars Climate Orbiter crash is is more subtle that you might think. I've not succeeded in finding out the language actually used, since any language you like could have a library for dimensioned quantities (although few would be able to enforce use of it in the way that Haskell could), but it might not matter because I did find [this report of a personal communication from Peter Norvig](http://skeptics.stackexchange.com/a/7284), who was on the investigation board. He's reported to have said: &gt;The problem involved reading data from a file and a miscommunication about what the numbers in the file were. I don't know of any language, no matter how type-strict, that forces you to tag the string "123.45" in a file with the units of force (newtons vs foot-pounds), nor do I know of any language, no matter how type-loose, in which you could not impose such a convention if you wanted to. So this wasn't a case of one function returning a raw numeric type to another across some API boundary. And, he goes on: &gt;On the previous mission, this file was just a log file, not used during flight operations, and so was not subject to careful scrutiny. In MCO, the file and surrounding code was […] at some point […] promoted […] to be a part of actual navigation, and unfortunately nobody went back and subjected the relevant code to careful review. [it] was written by a new engineer -- first week (or maybe first month or so -- on the job. This was deemed ok because originally it was "just a log file", not mission-critical. So, you know, this problem has got bad management and poor practices running all though it. In that context, clever point solutions to technical issues would probably not have saved this mission. If they had used Haskell and if they had used `Numeric.Units.Dimensional` to generate that “just a log file” then it would have contained unit suffixes on values, and maybe that would have prevented this particular issue. But, we don't know what other colossal blunders this badly managed, untrained team of neophytes have made.
I honestly doubt that `stack` is an universal panacea for any `cabal` problems somebody may come up with. I haven't experienced all this "pain" you imply when doing Haskell development with plain `cabal` (I'm not saying `cabal` doesn't need improvement, but it's definitely *not* this pain-inducing torture tool you make it sound like)
Is it possible to define properties like this? (all Doubles that are multiples of 3.0): http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1439294454_1547.hs The error is something to do with a "Sort Error" and "Bool" being a "non-propositional type"
Unfortunately, not. At the moment LH has very limited support for reasoning about floating point and real values -- and it doesn't support these operators (floor, division) on floats. You can do it for `Int` though -- did you have a use-case in mind for floats? That said, the error displayed here is unhelpful, thanks for pointing it out, we'll fix it!
It's kind of like if the Haskell numerical types were grounded in algebraic concepts, so that the `(+)` operator would be a member of the `Ring` class. Then anyone looking at that type would be inclined to ask "What is a ring?" And that's a deep question that indeed is difficult for even mathematicians to explain beyond a literal definition and a handful of examples. One could attempt some "intuitive" explanation of rings in general—but that's a topic for a math class, not for a newbie trying to do basic integer arithmetic! Likewise, binding `IO` actions together is a practical concern from day 1 of Haskell coding; understanding the general theory behind monads definitely isn't. Then, after learning how to bind, you start to wonder how to do more interesting things with values in `IO`, loop-like constructs for example, and then you can learn about `mapM`, still without bothering so much about the abstract meaning of the `Monad` class. When you learn that these things can be reused with types other than `IO`, you might get curious about the theory behind monads and functors... But it's important to note that these concepts were created precisely in order to be very abstract, they come out of abstract algebra after all. And it makes sense that a language grounded in mathematical constructions should also make use of such concepts to allow polymorphic code. But it's not a topic for beginners, unless the beginners happen to be a courseful of math students with some understanding of abstract algebra! Of course, this is a question of teaching... And currently the tools are not really designed as teaching tools, so they won't hide these things from beginners. I've been thinking that it would be neat if we had documentation that allowed you to choose which type you're working with. So that a beginner could look at an API reference without being overwhelmed by abstract constraints everywhere...
(poker face) But almost every week someone assures me that learning Haskell is only hard for experienced programmers and that for people who haven't had their minds polluted by learning some other language first Haskell is as easy, if not easier, to learn than anything else. You can't have it both ways.
Learning Haskell and learning to be productive in Haskell are not the same thing, it turns out, and are farther apart than in many other languages.
The `run` image exists as a convenience -- a smaller-than-`build` image that's pretty-much guaranteed to run any executable you can build (since it contains every shared library that could be needed by any package in Stackage). If you want a more pared down image, best to craft that yourself with your specific needs in mind.
I've experimented with using NFS with Vagrant to use stack in a manner similar to boot2docker, and it works decently well (vastly better than boot2docker). I do occasionally see stale file handle errors. Care also has to be taken to match the user IDs in the VM with those on the host filesystem for stack to work properly. One of my co-workers tried it and still found it too slow compared to the local filesystem for large builds. NFS also isn't going to work on Windows, and I haven't had a chance to see how well Vagrant's SMB "synced folders" work. 
&gt; I guess you could use it for external dependencies and services. Yes, that's the main purpose. This way every dev has the same system packages and libraries for a build. For a large project with many external dependencies, this is a huge benefit! If you don't care about that, stack's normal sandboxing is all you need. &gt; init processes play an important role in linux; as garbage collectors for their children I didn't mention it in the post, but stack's images do run a minimal init daemon.
That's certainly been my observation. It's something that Haskell advocates should be more ~~honest~~explicit about.
&gt; "Physics is newtonian" holds up very well in a wide variety cases while "monads are containers" breaks down almost immediately. While I was reading through this thread, I stopped at the parent of this post and wondered where the asymmetry in the argument was. I came to the exact same conclusion: the containers analogy breaks down too quickly.
I don't understand the purpose of the following data structure: data WidgetSum (path :: WidgetPath) a where WidgetSumStop :: a -&gt; WidgetSum path a WidgetSumLeft :: WidgetSum (WidgetLeft path) a -&gt; WidgetSum path a WidgetSumRight :: WidgetSum (WidgetRight path) a -&gt; WidgetSum path a The index of a GADT usually restricts the values of that type which can be constructed, but here it's used as a phantom type. That is, it's trivial to change the path to anything else: changeType :: WidgetSum p1 a -&gt; WidgetSum p2 a changeType (WidgetSumStop x) = WidgetSumStop x changeType (WidgetSumLeft w) = WidgetSumLeft (changeType w) changeType (WidgetSumRight w) = WidgetSumRight (changeType w) Because such a `changeType` function makes type-checking useless, the constructors for such a data type are usually kept private, and more restricted ways of obtaining and consuming values are usually provided instead. What are the intended ways of obtaining and consuming `WidgetSum` values?
For your project, you can just do `stack haddock`, and you'll have offline version of all transitive dependencies' documentation too.
I have a high-resolution picture of his [spiritual son](https://c2.staticflickr.com/4/3781/13581964385_d1b77f39f1_h.jpg). Some even say Haskell Curry and Simon Peyton Jones, are isomorphic. All hail our celtic overlord!
Is this snippet assuming that the claims made by the API authors -- w.r.t. rate -- are accurate? I don't see how you are handling the situation where, after waiting the recommended time, the call still returns a ``503``. That is, it looks awfully optimistic.
It doesn't *seem* like you're doing anything wrong. It's quite possible this is a bug in MinGHC. However, I have not been able to reproduce it. Can you report it?
Since Curry is waifu material [here](http://imgur.com/hO8Wj1v) is his image upscaled using waifu2x.
My god that sweater.
Barendregt's book on the lambda calculus has a photo of curry with his daughter on the page before the first chapter. The quality isn't phenomenal, but it's different than the one you have.
Well, my thinking was like this: Nothing &lt;*&gt; _ = Nothing _ &lt;*&gt; Nothing = Nothing (Just f) &lt;*&gt; (Just a) = Just $ f a Simplified: Nothing &lt;*&gt; _ = Nothing (Just f) &lt;*&gt; x = fmap f x Now to get rid of the pattern matching on the right we should be able to fmap it I would think. EDIT: I see it now, we have to unpack the function and apply it, if we fmap it, we keep it inside it's context (nesting them).
beautiful, thank you
Your code seems to have been mangled by reddit's reformatting. To make code look properly in markdown, surround it by blank lines and indent each line by four extra spaces.
That's fair. Perhaps what I mean to say is that as you diverge from published stackage snapshots, you are approaching more advanced usage. When you're using `stack solver` you are very likely building relatively complex packages, or ones that simply don't nicely fit into a snapshot yet. I highly doubt the OP is in that realm yet.
Any chance of something similarly official for the iOS target? (:
Concerning schema validation, is there anything in https://hackage.haskell.org/package/hxt ? EDIT: Also, I would love to see other examples of enterprise functionality that is missing. Let's get some projects going!
I think the article should be named "Why I am not ready to use haskell for the Enterprise". Because a lot of us do use it in Enterprise :) 
The mispelling is intentional.
OK. Then I guess I must have missed your intention. Please explain for (at least) my edification. EDIT: Was the misspelling of "misspelling" also intentional?
Did you consider writing an XSD validator?
You want an entry level killer app? Something that makes you say "There's no way that little code implements a whole HTTP request/response"? What about this: http://haskell-servant.github.io/posts/2015-08-05-content-types.html
Thanks, this is exactly what I want to see more of!
You say that the learning materials suck. I'd like to know if you've looked at any of: https://github.com/bitemyapp/learnhaskell http://book.realworldhaskell.org/ [Beginning Haskell A Project-Based Approach](http://www.apress.com/9781430262503) &gt; the tutorials do a really crappy job of exposing these concepts, and are loaded down with enough technical mumbo-jumbo that detracts from the learning experience rather than explaining what is to be gained As a counter example, the introduction of the State type/monad in Beginning Haskell is well done. First an algorithm is developed without using State, then it's transformed to State. The motivation and the 'how it works' are both covered.
Sure thing. P.S. [reddit discussion](https://www.reddit.com/r/haskell/comments/3g8jb0/contenttype_bliss/)
You have time to spend on maintenance and refactoring? You don't spend all your time implementing new features that are constantly being asked for? Where do you work? Are they hiring?
This blog post is kind of grating in tone, but it's an absolute gold mine of insights on how some non-Haskellers approach Haskell. I think every point listed is worth digging into. Please don't shoot the messenger, even when the messenger is less than tactful. 
And if this isn't high enough I wonder if an artist can fill in the gaps? Or just someone who is nifty with photoshop.
My tone was meant to be self deprecating. I don't like to claim mastery of anything when I know there's always more to learn.
Personally, I find that if there's a particular tool like "validate against an xsd" that's missing I'll look to see A) if there's a command line tool available (there is) or B) if there's a C) library available (there is). In which case, wrapping either is typically about a day of work tops, and lets me rely on a hopefully well-maintained standard tool. Sure I'd like to be pure-haskell everything, but I know that there are always going to be things where there hasn't been enough attention focused, and I'm going to be building a tool that lives in a big ecosystem anyway, so I don't mind integrating a bit more with the non-haskell ecosystem in return for getting to write lots of my core stuff that matters in a language I find that lets me get things done more effectively.