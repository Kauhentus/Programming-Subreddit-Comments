It is not about writing functions without IO, it is about writing functions that work in monad stacks with the base monad being IO.
The former is more efficient, as it only has to round-trip through the transformer stack once. On the other hand, putting in an automatic lifting into `IO` is effectively free in most cases. If the author wants the former fused representation then they can just use your `MonadIO`-empowered combinators inside of a `liftIO` themselves. In that case working under the concrete choice of instance for `MonadIO IO` you get `liftIO = id` sprinkled a few places through the code and it inlines away. It isn't perfect. If you want to deal with bracketing, etc. you need something more advanced.
`putStrLn` was just the first `IO` function that popped into my head as an example.
`foldMap` isn't a separate function - it's a method in the `Foldable` instance. Therefore when instancing `Foldable`, it's the implementers responsibility to ensure reasonable performance properties. But of course there are constraints on that. I think you're right that the laws aren't sufficient to ensure good performance - the rule is that typeclass laws specify what's correct. The foldable may fold in an inconvenient order for any particular monoid. Of course a lot of monoids (`Sum`, `Product`...) don't have cost penalties for changing bracketing order, but some (like list) do. If this is a problem, one workaround is probably to switch to a difference-list monoid - difference lists in Haskell (IIRC - I haven't looked at them for a while) are functions of the form `\t -&gt; ... :t` - lists with a "hole" where the tail should be. They support efficient concatenation by function composition, and to get the final list at the end you just provide the empty list to fill the hole. More generally, if the folding order is wrong for your monoid, you can use another monoid that implements the same concept with different cost trade-offs. 
&gt; The former is more efficient, as it only has to round-trip through the transformer stack once. Indeed, and my concern is that because it's now syntactically so "easy" to use `MonadIO`, you risk shooting yourself - maybe in the toe - and losing a bit of efficiency. &gt; f the author wants the former fused representation then they can just use your MonadIO-empowered combinators inside of a liftIO themselves. Relying on the assumption that liftIO $ do liftIO foo liftIO bar Will compile down to liftIO $ do foo bar Seeing as `liftIO = id` for `IO`, I'd hope this does happen. Thanks for reminding me that you can even do that though.
Sounds very cool. Did you manage to figure out how to translate from morte-encodings of datatypes _back_ into the Haskell style datatypes? That seems nontrivial to me.
Use any GHC version you want, easily.
I tend to simply boot up the stock Haskell container and run bash in it, treating it as a regular VM with my source dir mounted.
I hadn't thought of having a Haskell image with stackage installed. I shall have to give this a try!
&gt; I'll give a couple of specific examples of problems that I was interested in applying this to: As I've told in my story above/below this comment, for the cases like your Examples 1 and 2 (and also for my case of responding with a function), I set out to implement this in my program by means of `static_ptr`s and HdpH and its closures. That's a rigid and clean approach, but I also had a "prankish" thought suitable for Examples 1 and 2, at least; something that shouldn't be used in production, probably -- look below &gt; Example 1: You're building a web API to filter and retrieve some data, so you end up defining your own bespoke query language using URL/form parameters or JSON. Wouldn't be nice if you could just directly transmit the function (in a standard format) that you want to use to filter the data? &gt; &gt; Example 2: You are writing a domain-specific tool that some sort of configuration file. Under the hood you have some logging logic that you wish to let users customize, so every time somebody requests a new way to log things you add a new field to your configuration file that only one person/team cares about, which leads to configuration file bloat. You could consolidate all these disposable fields into a single field where the user supplies their own logging logic. what if the program is run in GHCi? Then we can write the filtering expressions and the configuration stuff in GHCi while the program is already running, can't we? Then we skip over the use/need of `static_ptr`s from GHC. I haven't thought this over well. It seems that GHCi (at least, for testing) gives the flexibility `static_ptr`s would give for production work. Perhaps someone has some comments. Not very suitable though for one of my other goals: responding with functions. Unless there is a way to make sense of the output from `:sprint` which prints the thunks.
I develop on Linux (on a Mac in an Ubuntu VM) and it simplifies my life. I always assumed it adds some complications for those that primarily develop on a Mac. The benefits are isolation and environmental parity with production. Before docker I always saw issues that did not show up on developer's machines that showed up during a production build or while running in production. These issues are a lot fewer now and easier to figure out. I use some of the same install scripts that are used for building or running in production. These installs are isolated from the rest of my system. That way I can work on 2 projects at once without any worries about system conflicts. Additionally, if my computer turns into a brick, or if I switch to a new computer I can quickly bring up a project environment again because docker requires you to describe your environment. 
The O'Neil sieve seems from the `NumberSieves` package seems to edge out my sieve. That is some impressive optimisation. I tried to put together a benchmark using Criterion but was having some trouble. See [this comment](http://www.reddit.com/r/haskell/comments/36ez9t/i_came_up_with_a_different_kind_of_prime_sieve/crdz9s4). 
Random guess, but it usually means the alternative code is getting to weak normal form and stopping. What is g doing? "g" should specifically return the last encountered prime. This way I think it should work. If that's what it is doing, I don't know...
This is one of the best habits I've picked up from working on GHC, and we try to be very judicious about it. Of course, it helps more with Haskell (and especially a compiler) in my experience where the information density per LOC is fairly high. :) There's lots of great stuff to learn from them though, if you look. One I pointed out on Twitter a while ago - [Why does GHC eta-expand the RHS of an INLINE thing?](http://git.haskell.org/ghc.git/blob/ffc21506894c7887d3620423aaf86bc6113a1071:/compiler/deSugar/DsBinds.hs#l363)
How do you deal with this eta-reduction problem?
The current suggestions are: * display it at the end, like github does * display the first N lines, with a link or expander (like the stackage website does) and either * display the readme in place of the description, if there is no description * always display the reamde and description, with description (usually short) at the top and reamde (usually long) at the bottom Indeed this last option has some merit, we could encourage people to keep a description but really limit it to a paragraph or two, just for people to quickly scan and get an idea of what a package is for (useful for the search), while moving the longer stuff and examples / mini-tutorial into the readme. Currently we have some rather long and perhaps over-deailed descriptions which could be moved into the readme if we display the readme properly on the hackage page.
With hvr's Ubuntu PPA, I have that already at least in Linux. I just flip my PATH between /opt/ghc/7.10.1, 7.8.4, etc.
Cool. After pressing `&gt;&gt;&gt;`: http://haskell-exists.com/cc27.png
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Timing attack**](https://en.wikipedia.org/wiki/Timing%20attack): [](#sfw) --- &gt; &gt;In [cryptography](https://en.wikipedia.org/wiki/Cryptography), a __timing attack__ is a [side channel attack](https://en.wikipedia.org/wiki/Side_channel_attack) in which the attacker attempts to compromise a [cryptosystem](https://en.wikipedia.org/wiki/Cryptosystem) by analyzing the time taken to execute cryptographic algorithms. Every logical operation in a computer takes time to execute, and the time can differ based on the input; with precise measurements of the time for each operation, an attacker can work backwards to the input. &gt;Information can leak from a system through measurement of the time it takes to respond to certain queries. How much such information can help an attacker depends on many variables: crypto system design, the CPU running the system, the algorithms used, assorted implementation details, timing attack countermeasures, the accuracy of the timing measurements, etc. &gt;Timing attacks are often overlooked in the design phase because they are so dependent on the implementation. [*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*] &gt; --- ^Interesting: [^Lucky ^Thirteen ^attack](https://en.wikipedia.org/wiki/Lucky_Thirteen_attack) ^| [^Time ^attack](https://en.wikipedia.org/wiki/Time_attack) ^| [^Clock ^drift](https://en.wikipedia.org/wiki/Clock_drift) ^| [^Rotational ^cryptanalysis](https://en.wikipedia.org/wiki/Rotational_cryptanalysis) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cre6vra) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cre6vra)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
"GHC Haskell compiler" Glasgow Haskell compiler Haskell compiler
The literate haskell file that generates this pdf can be [found here](http://hackage.haskell.org/package/AlgorithmW-0.1.0.1/src/AlgorithmW.lhs) (in the [AlgorithmW package](http://hackage.haskell.org/package/AlgorithmW)).
&gt; oh, can we do the incantation with the google sauce where we make Google search results go to the newest version of the package, plz? If you know what the incantation is, that'd be great. The most recent thing we've tried is putting in &lt;link rel="canonical" href=...&gt; into each page to link to the unversioned page, but it's not clear if this will help. The issue is that we want to make sure that links to documentation doesn't go stale, so our unversioned links to docs do a temp redirect to the specific latest version. The problem then is that the search engines point to the specific latest version (which gets out of date) rather than to the unversioned url that redirects. Clearly we could have a concrete resource for the unversioned version but then we'd get the oppostie problem of stale links as docs change. Solutions welcome.
Thanks! I just fixed that typo.
Cool, it would be much better to have some support for hyperlinked notes even though it works just fine with plain text. And I guess it does have some similarities with literate programming, the notes are kind of similar to what they'd look with LaTeX.
The comment in http://www.reddit.com/r/haskell/comments/368i11/experimental_god_mode_sandbox/ says &gt; What was the final size of the installed libraries? &gt; After removing binaries it is 9.3G, compressed is 800M. And you won't deploy build image based images...
Authored by the denizens of /r/programming. 
I find rather reassuring that there are valid/good applicatives that should not be "extended" to monad. This might be because I have spent too much time using languages with subtypes ?
Not bad, although hasn't GHC itself moved on to using OutsideIn instead of Algorithm W?
Thanks. The linked issue describes all the problems pretty extensively. *haskell-src-exts* is the biggest source of output blowing. The main purpose of this app was to see myself and demonstrate the state of GHCJS with a simple example. I am glad that it's possible to just add *pointfree* and *pointful* in *build-depends* and use it right-away. It's also possible to have a normal haskell tooling and workflow. Compiling GHCJS project is no different from compiling a normal haskell project. All you need to do is to put *--ghcjs* parameter and that's it. Of course it's far from being perfect and there are problems with performance and output size, but they seem to arise only when you include some huge library from hackage which was never intended to be compiled into javascript.
And that stays in the monad of course!
Example is section 2.3 would be clearer if they were in Haskell syntax instead of AST literals. Let's presume a `parse` function that converts a limited subset of Haskell (as a `String`) to the AST and the examples become: e0 = parse "let id = \x -&gt; x in id" e1 = parse "let id = \x -&gt; x in id id" e2 = parse "let id = \x -&gt; let y = x in y in id id" e3 = parse "let id = \x -&gt; let y = x in y in id id 2" e4 = parse "let id = \x -&gt; x x in id" e5 = parse "\m -&gt; let y = m in let x = y True in x" GHCi gives the following types to them (if you want to check your own Algorithm W implementation against it): 1. `t -&gt; t` 2. `t -&gt; t` 3. `t -&gt; t` 4. `Num t =&gt; t` (`Int` would also be acceptable, I believe.) 5. Occurs check: cannot construct the infinite type: `t3 ~ t3 -&gt; t2` 6. `(Bool -&gt; t) -&gt; t`
I use GHC For Mac OS X on my Macbooks. I've only had bad experiences with Docker tho.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Betteridge's law of headlines**](https://en.wikipedia.org/wiki/Betteridge%27s%20law%20of%20headlines): [](#sfw) --- &gt;__Betteridge's law of headlines__ is an [adage](https://en.wikipedia.org/wiki/Adage) that states: "Any [headline](https://en.wikipedia.org/wiki/Headline) that ends in a [question mark](https://en.wikipedia.org/wiki/Question_mark) can be answered by the word *no*." It is named after Ian Betteridge, a British technology journalist, although the general concept is much older. The observation has also been called "__Davis' law__" or just the "__journalistic principle__". In the field of [particle physics](https://en.wikipedia.org/wiki/Particle_physics), the concept, referring to the titles of research papers, has been referred to as __Hinchliffe's Rule__ since before 1988. &gt; --- ^Interesting: [^Sensationalism](https://en.wikipedia.org/wiki/Sensationalism) ^| [^Rhetorical ^question](https://en.wikipedia.org/wiki/Rhetorical_question) ^| [^List ^of ^eponymous ^laws](https://en.wikipedia.org/wiki/List_of_eponymous_laws) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cree94q) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cree94q)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
True. It would be great to see improvements in this area.
Some things are just better written pointful. Try: \w x y z -&gt; ((x `y` x) `z` (x `y` x)) w w and watch v8 beg for mercy.
&gt; After researching this for a while, it seems that &lt;*&gt; = ap is not a verbatim rule [The docs of `Monad` literally say that "`(&lt;*&gt;) = ap`" should hold](http://hackage.haskell.org/package/base-4.8.0.0/docs/Prelude.html#t:Monad), which is as strong as laws in Haskell get. The Report itself mentions all laws in that "should" style, and I expect that the next Report iteration will mention the Applicative/Monad relation in the same spirit. There are good reasons for the "should", of course. The programmer is free to choose instances that only satisfy them observationally, and then there's our good friend ⊥ who breaks e.g. Reader: ⊥ &gt;&gt;= return = \r -&gt; return (⊥ r) r = \r -&gt; const (⊥ r) r = const ⊥ ≠ ⊥ 
For some reason I managed to overlook this – I will update my blogpost. It is worth noting though that this was added together with AMP, so reasonably recent.
It should wait indefinitely to lazily load a next strip.
You have to add a license to this, 'cause I want to use it in every presentation from now on! 
the Glasgow GHC Haskell Compiler ;)
Before the AMP (GHC 7.10/Base &lt; 4.8) the note was in `Applicative`'s docs, [at least since Base 4.0.0.0](http://hackage.haskell.org/package/base-4.0.0.0/docs/Control-Applicative.html). (And sorry to be that guy) ;-)
Oh god
It should force Roman to upload a new one :P
Ok, but that's great, it's good to know that there can be no doubt about this.
[Wikipedia's image](http://upload.wikimedia.org/math/0/c/e/0ce048ed5543b18f57b6bf03c78737a1.png) of the algorithm's rules is actually helpful (which differs from the pure version in Damas, Milner (1982), "Principal type-schemes for functional programs"). 
I agree with /u/SrPeixinho. The normal form that you're resolving to in primeBench is completing way faster than whatever happens in `time main oneill 10000000` which almost surely means that the benchmarking code is doing much less work. Without a gist I'm afraid I can't say much more.
So true ahah :)
https://github.com/lukexi/halive/blob/master/exec/Halive.hs indeed, short and well-documented. 10/10 would read again ;-)
Do you also poop rainbows? If so then you may actually *be* a unicorn! Something to consider...
[Extreme ironing](http://xtremeironing.com/) has a new contender.
Just as a point of interest, IIRC (from a presentation by Sébastien Doeraene), scala.js achieved *huge* reductions in size (roughly 10x or so) when they were able to use the Closure Compiler's ADVANCED mode (or whatever it's called). One can only hope that ghcjs will be able to fix the remaining problems preventing the use of ADVANCED mode.
Literate programming seems to focus on the wrong thing (in practice, in theory I'm all for it!). I'm sure we'd all like all our programs to read like prose, but writing comprehensible prose is a *lot* more difficult for us programmers than writing comprehensible programs. I think the "Notes" idea is exactly right[1]. Write prose about the things that are *actually* interesting and/or difficult to understand... but don't ask the programmer to also describe the mundane parts in prose. [1] Can't speak to how well it works in practice, I haven't had the pleasure of being asked to write one. Regardless, it seems to work for the GHC team and I'm inclined to believe them.
I'm definitely missing something, but... I don't get it. Halp?
Exactly! This is why org-mode links work so nicely: they don’t break up the code. Org also buys you some nice formatting, navigation, and organization.
OP recreated a bunch of the prelude (head, tail, Maybe) to write their nodejs app. 
I haven't tried IHaskell. The idea seems interesting and similar to literate programming at least based on what you say. I like it in some cases, but it may be overkill in other cases in which a simple plain note would do. There's no special syntax for these notes, which could be better at least in some cases. But I'm not sure if I'm getting the idea of IHaskell wrong because of not trying it yet...
But it makes a lot of sense (in both practice and theory) for articles and tutorials and things like that in which the prose is very important, as well as keeping the code with the text. However, perhaps writing that prose (even if it's harder for programmers) can help in writing better programs, which is also an idea from literate programming.
Well algebraic effects are a hot area of research.
Certainly, for tutorials and articles &amp;c! However, I don't think this is the typical situation. I'm just kind of questioning the assumption that "writing that prose" results in "writing better programs". (And I *do* think it is a completely unproven assumption(*).) Anecdotally, I personally think much better symbolically/algebraically than in terms of prose. (*) It may have worked for e.g. Knuth, but extrapolating from Knuth to all programmers is unbelievably naïve.
Don't forget graduate student or other academic slant.
so how do the "notes" work? they don't look like named comments in haddock.
I'm not sure what your point is... because I usually do. (As far as is practical in a non-dependently-typed language.)
Don't be sorry. Obviously others got the point without the explicit linkage, so it's probably just me that missed it.
/r/ProgrammerHumor 
True. Everyone knows that the awesome Iron Man suit is *actually* dependent types.
Did you mean to put "library-profiling:" twice? I think it still works if you only list it once, and I think you *might* want "executable-profiling". EDIT: Though, normally executable profiling would only be if you want to profile executables produced by a cabal package, and the Haskell source already contains all the cost-center annotations that you want. If you need to specify `-auto-all` or `-caf-all` or both, you'll probably need to rebuild the executable anyway, so `executable-profiling` isn't nearly as useful as `library-profiling`.
(Serious necromancy here...) ... what? I know Python quite well. I still don't like that fact that effects are not encoded in its type system (runtime-checked as it is).
I think if you are going to rebuild with all those GHC options, you probably don't need to `cabal configure --enable-library-profiling --enable-executable-profiling` or `cabal install -p`.
Yes, it's not the typical situation. Some of these (notes and comments, literate programming) are ways to help you think about your program, which matters more than how you get to think about it. Of course, Haskell is great for reasoning about programs with or without things like literate programming.
There's nothing special about the notes, they're just plain text, simply add them to your code. Since it's about implementation comments, Haddock wouldn't display them, but you could look at them in the source via Haddock.
Check out http://ramdajs.com/0.14/index.html Sadly, functions like ramda.head are not total, but at least they got currying right.
Yah...I dunno. I've been feeling more like http://imgur.com/nLIMpsf lately. Very safe, a little cumbersome and unwieldy. Been trying to pick up Prolog lately, looking for something expressive and intellectual without all the cruft. Haskell has been my hobby language...use it for programming puzzles, toy things, so my needs are not everyone's for sure. Talk me back from the ledge, bros!
`haskell-src-exts` is a pathological example, it's notorious for long compile times and large output, which makes it an interesting starting point for exploring optimization (it uncovered some memory leaks in earlier GHCJS). The haskell-src-exts dylib for OS X 64 bit is almost 22MB. The JavaScript code size will improve once the Tyr code generator lands: http://www.reddit.com/r/haskell/comments/31ui2h/whats_the_current_status_of_ghcjs_vs_haste/cq6070o but for the haskell-src-exts package in particular, I think more needs to be done for actually good results. Perhaps it's just a matter of tuning the GHC options. Now that hackage has been updated to Cabal 1.22, it should accept the `ghcjs-options` field for libraries and executables (the options get added to the `ghc-options` on the GHCJS command line). So if anyone wants to try and find a better optimum when compiling to JS, maybe it can be merged upstream.
The pointfree computation is best done in an asynchronous background thread, so it can be interleaved with the browser doing other things. When the user types something, existing computations should be aborted. See the `try-purescript` example for how to do these things asynchronously: https://github.com/ghcjs/ghcjs-examples/blob/master/try-purescript/Main.hs When the computation has been moved to a background thread, browser responsiveness can probably be improved by tuning the RTS options by passing `-D` preprocessor flags to GHCJS when linking: See: https://github.com/ghcjs/shims/blob/master/src/thread.js#L8-L21 In particular the `GHCJS_BUSY_YIELD` field is probably too high as a default: the GHCJS RTS keeps computing for 500ms if it has work to do, during this time the browser has no chance to process events.
Is wtfpl not legally rigorous, somehow?
I can have LaTeX typeset equations in my notes that are rendered right in emacs. I can put notes about alternative approaches that didn't work out (haven't you ever encountered some of your own code and thought, "Oh, I have an idea to make this better!" only to realize that you'd already tried that and it didn't work?). I can link to benchmark histories, sample data sets, annotated references. These are all useful notes for *you*, not just tutorials for others.
They are [using debian:jessie](https://github.com/freebroccolo/docker-haskell/blob/6ea0bc8500bddf2aa373d7c48e4ad9590d32cefa/7.10/Dockerfile#L2) as a base, which is 125.2mb
Notebook interfaces like IHaskell are **fantastic** for things like plots. If you treat it more like reading text, then it has the huge benefit that you can see exactly how the plot was generated. If you are just reading, it's no big whoop, but if you want to play with parameters like axis scaling, or you're suspicious of a quantitative result, you don't need to recapitulate all of the author's technical work to check it out.
Wow this looks excellent. You wouldn't happen to know a place to get a good used copy that is less than on Amazon, would you? It's a bit much for a paperback at the moment, but I'd love to get started on it in my spare time.
Found this reply by googling "MonoComonad". Any plans on adding this to mono-traversable? I have a library that implements MonoComonad myself but I'd much rather use one from a library
Given that Chris doesn't own any of the pictures, I doubt that he can actually put that post under any open license.
What were your bad experiences with Docker?
I like how `head`, `tail`, and friends are safer than their Prelude counterparts.
Mostly centered around production use. Not very stable, hard to do anything non-trivial with their provisioning stuff. This is partly why people end up provisioning Docker instances from Puppet. Intra-docker-instance networking (like...talking to PostgreSQL from a web app) made us pull our hair out. Problem is, if I've gone to the trouble of making a reliable Puppet config for my stuff, I don't need Docker anymore. Most AWS machines are so sad and overloaded that I have zero desire to slice and dice that EC2 instance any further.
How do linear types give rise to flexibility?
I know ramda. For the purpose of that tiny little thing, using a library seemed overkill (also it would take all the joy out of writing it!), but I very much like it and am looking forward to using it for real for the first time.
Ah yes the good old DRD dept
Of course. Safety was not a primary concern here, as I would rather consider it as an exploration in style (that sounds like more than it actually is, though). It is not idiomatic JS, especially when it comes to Maybe, that is true. The problem that I have with such "magic" values as `undefined` is a more general one (meaning that it's the semantic that bothers me), but one we cannot resolve anyway. It is there and it will stay, so I guess we just have to live with it. In a perfect world (for me anyway), there would not be an `undefined` or `null`, though. If you already know why such values are problematic, you can skip this part. If you like to find out why I consider them dangerous though, is that it is terrifying for me that every variable could potentially be an error value (say, `null` or `undefined`). Instead, I want to be explicitly told when something can fail and for that there is a data type: Maybe (or Either). That is clean, because looking at a function I know now I might have to deal with errors. That being said, my program does not do any error handling whatsoever, so I could have just done it the Prelude way: let it fall over and die if something is wrong.
&gt;My impression is that haskellers tend to be seen as head-in-the-clouds-impractical, purists to the point of fanaticism, and annoyingly prone to proselytizing. Here's my take on this - the software technology landscape consists of a bunch of disparate technologies (Javascript, C#, Ruby...) which are more or less equivalent in terms of productivity. Still, their users evangelize them regardless. Why? Because if you become proficient at using a certain technology, it's advantageous for you if that technology gets used by your peers. Haskell adopters recognize that, and they make very sure to say "but we're not like that! Our technology is *actually* better!" Outsiders hear that and get curious. They stop by and check out Haskell, and what they see is a language with little tooling, unusual syntax, few jobs, and a bunch of weirdo mathematicians speaking a language foreign to mere mortals. Above all, they see the **batshit crazy learning curve**. To them, this doesn't look like a superior language, it looks like a bunch of people who have invested themselves way too much in a technology and now really really want people to use it so they can benefit. I can't think of a quick and easy solution. Haskell is a nice language, but it sure did manage to "avoid success at all cost". Technical managers don't see Haskell as a potential tool, they see it as a source of competent (if academically oriented) developers for projects in other languages. For those who see software development primarily as a job, that's a show-stopper right there, Haskell is not something they'll try to learn. Side-note: on the orders of my manager, I am currently ramping up my team on the use of F#. Initial reactions are wary at best, people don't necessarily want to learn a new way to do things when their current way has been enough until now. And that's for a language with massive corporate support, tooling, imperative programming, object orientation, the works. I can't imagine bringing up Haskell.
You can implement a logic programming monad in Haskell :o) I definitely won't stop you from getting into LP, for some problem spaces it's an excellent paradigm. See also: [Propositions as Filenames, Builds as Proofs: The Essence of Make](http://bentnib.org/posts/2015-04-17-propositions-as-filenames-essence-of-make.html)
Yeah, that works well for this simple situation. But it still falls apart where you have nested bangs, and I think that the ! notation is easier to read for the cases that it works well for, which is when you want to evaluate effects from left to right.
meh
Implement a small Prolog interpreter in Haskell. This language really shines at this sort of task and it's a fun little project too!
Data.Foldable.foldMap is defined in terms of foldr. That is the default implementation, granted there is no guarantee the method is not overriden in the instance. foldr is really the default way of folding in lazy langauges, lazy foldl is suboptimal, so I would bet my money that whatever the instance implementation is it is still going to be accumulating on the right.
I enjoy haskell, but don't write anything professionally. My one wish is for haskellers to use fewer single letter type variables. If allowed, afurther wish would be more practical context to abstract generalizations which really exhibit why they are useful rather than just powerful.
moo
That does sound fun, actually :) I'll put in the queue.
&gt; C# and F# cannot coexist in the same project Eh? Really? Wow, that's pretty terrible. Is there any chance of that being fixed some day?
Making a &lt;- foo b &lt;- bar return $ baz foo bar have a different result than baz &lt;$&gt; foo &lt;*&gt; bar is just plain evil anyway. 
Implementing `tail` by splicing an array is dangerous, because unlike the haskell `tail`, it is O(n), instead of O(1). That can turn O(n) algorithms into O(n^(2)), leading to great loss in performance.
[I'll just leave this here.](https://github.com/tel/typescript-has/blob/master/lib/maybe.ts)
Does that mean ludwig compiler is written in Haskell?
Well, I guess it's not arbitrary linear types that give this flexibility. With linear types, you can know something only happens once, which means you can write code that cannot be repeated (stuff like closing a stream, etc.). Of course, we usually just describe those things in the documentation instead of the type system, so in practice, we still have that flexibility.
I doubt it. It's two different toolchains to compile them, and would require significant changes to the .Net and F# compilers and intellisense as well. But it would be awesome.
Sweet, thanks.
Could you not simply encode the regular tree types directly, with constructors and eliminators and reduction rules, rather than encode them using Böhm-Berarducci? This would at least give you an accounting of what things are types and what things are values.
My interpretation is that it's a light jab at Haskeller's self-perception. The implication is that this scenario is unrealistic for many reasons, of which two possible ones are that Haskell code still has security flaws and that no auditor would actually recommend Haskell.
;) I use Angular + RouterUI + Angular-Materials, js is managed via bower, I closure compile all static libraries and serve them via CDN, and generate all JS on the fly (possibly different js for every user), demo is still a work in progress. https://video.kio.sx/demos/about
Something that would be fun to figure out would be to have a look at servant's jQuery generation code and how you could add it to a HTML templating library of your choice, the goal being to be able to say "I want this data here, represented in this manner". Perhaps querying a servant API from React is also a good place to start.
Jasper, you're too modest! Your [profiteur](http://hackage.haskell.org/package/profiteur) visualisation tool is easily my favourite. I was visiting a client recently and showed them profiling with profiteur which easily let us drill down to identify the culprit in their code. They were very impressed. I'm not sure everyone knows about it, you should publicise it more. It's a great tool. For those of you here who've not seen it before, see Jasper's original [blog post](http://jaspervdj.be/posts/2014-02-25-profiteur-ghc-prof-visualiser.html).
Wow, that's incredibly disappointing, and I'm not even a .Net guy. Thanks for the info though!
How does this compare to [language-c-inline](https://hackage.haskell.org/package/language-c-inline)?
I want to keep the core language as small as possible so that if any target language lacks any higher-level feature (such as pattern matching), it could always fall back to a literal translation of the calculus of constructions (using lambdas) as a last resort, even if it generates non-idiomatic code. I want this to be as portable as possible.
Though, note that you can mix different project types in the same solution. So its not entirely impossible.
I think you can often get quite a long way without any of these frameworks - my usual approach is to start with just vanilla JS, and to get a good feel for what is needed in the UI, and then start looking at libraries/frameworks later only if it's clear that there's a problem with the initial approach, and if those libraries look like they could solve that problem. Also, purescript. :)
The recently released stackage-view tool by FPComplete (employer of the Yesod founder Michael), uses GHCJS and React for its UI. It is open source so the code can be studied. I think client/server-isomorphism and VirtualDOM are the way to go for browser UIs. Both are facilitated well by React, so it seems a natural choice. For my experiences with React it is quite small and understandable. But combined with GHCJS I see some overlapping functionality, and thus I believe a better solution is possible. I think an Haskell based FRP lib and an HTML templating lib combined with a VirtualDOM lib in JS should give most of what is React in a more Haskellish way. Anyway. My take on it: not much consensus on a browser UI strategy for Yesod at the moment. Mainstream availability of GHCJS will certainly change that when it comes true.
This looks great! It’s smart to avoid parsing the embedded code as that is a fatal maintenance burden for other approaches. I don’t think the intro is really on point, though. Traditional FFI approaches are often used in a “pay as you go” manner, too. You’re right that binding packages tend to rot, but the point, I think, is just to make it easier to write FFI bindings. If an extensive set of bindings is written with your approach, it will rot, too. A minor issue with the article: the Haskell type of `funIO` doesn’t look right; and grep for “translated translated”.
Check out [Elm](http://elm-lang.org/).
This looks like a wonderful library to me. For new low-level code I would almost certainty reach for Galois' [Ivory](https://github.com/galoisinc/ivory) yet, as the article describes, I really like this for quick and dirty integration with C. Nice! 
Unless you're going ghcjs/PureScript or similar, then I can heartily recommend React.
It needs --ghc-options=-opta-march=armv7a as an option every time you want to build something with GHC or cabal install something. With this, it works! (I just found this on StackOverflow this morning after doing the package install of 7.8.4 - including ghci - which just became available.)
I think the reality section should be a little different and there should be one more section after it. **Reality** [Haskeller building software](http://animated-gifs.org/wp-content/uploads/2012/01/construction-worker-010.gif) [Non-haskeller building software](http://animated-gifs.org/wp-content/uploads/2012/01/construction-worker-007.gif) **Five years later** [Haskell project](http://i.kinja-img.com/gawker-media/image/upload/s--Io5IeYpA--/c_fit,fl_progressive,q_80,w_636/i2ksv3uypjx2gto7rtwu.gif) [Non-haskell project](http://i.giphy.com/kMXpeWCOmBQE8.gif) ...or something along those lines
&gt; people don't necessarily want to learn a new way to do things That's the real probem
Functional is already pitted against OO. I think adversarial marketing does Haskell more harm than good. It only works to convince people who are already convinced, and it pushes away those who are invested in OO.
I figured that out and wrote it there. But there's still a problem, I can't get floating point stuff to work without runtime exceptions for segmentation faults and bad instructions. There are a bunch of similar flags for floating point things, but I can't find a combo that works. Could you share the sources.list file where you got your copy of 7.8.4 from?
Even the most passionate developers I know suffer from learning fatigue when it comes to technologies. The only reason I'm sticking it out with Haskell is that it tends to subsume other technologies, so whatever I learn here I can also apply elsewhere. Since I've started programming, I've picked up about one language every six months. At this point that shit all looks the same. We're good at coming up with new tech, but we suck at evolving the old tech into something relevant, hence fragmentation.
Uh I just lean back. Individual developers find their way to here. Everyone else will follow in a decade or two. No matter what old industry tells. 
Ctrl-Z should work for Windows.
This is amazing. Could one now write inline assembly using inline-c? That would blow my mind...
&gt; Even the most passionate developers I know suffer from learning fatigue when it comes to technologies. I had a little of this for a while just before learning about Haskell, because too much of what I was expected to "learn" was yet another way to do what I was already doing. Oh, occasionally there was an actual gem, but mostly it was mostly just trade out the syntax for the same mass of problems. Haskell was actually novel, and reinvigorated my passion for learning. But, then again, the "weirdo math" was always something I liked, and Haskell / HoTT / Idris / Agda just keep pressing that particular joy button of mine.
&gt; watch v8 beg for mercy. :)
I proselytize less these days, but purity is why I learned and use Haskell. The fact that it's practical at all is just glaze on the doughnut.
Actually they are a defense mechanism against AIs which don't have a termination checker included in their language subsystem. Gets them every time. ;)
deb http://http.debian.net/debian sid main contrib non-free Since that is the only thing in /etc/apt/sources.list and then you do this: sudo apt-get update &amp;&amp; sudo apt-get upgrade sudo apt-get install ghc sudo apt-get install cabal-install I'm not sure if there are other implications for the rpi2 since it looks like everything is upgraded to the newer Debian w/o any rpi2 source. (I just got my rpi2 a couple days ago, but have been using Ubuntu for a long time.) I have rebooted and everything seems OK. A LOT of stuff got updated - which feels good - I'm just wondering about compatability issues. Anyway, I think this is the way to go: forward - and you don't have to build from source on top of old components, plus it took about an hour, not a week. Let me know how things work out... original post: http://stackoverflow.com/questions/30201508/why-cant-i-install-any-packages-with-ghc-7-8-4-on-raspberry-pi
The `hmatrix` library is Haskell's wrapper around BLAS and LAPACK. Right now the documentation [does not seem to be building correctly](https://hackage.haskell.org/package/hmatrix-0.16.1.5) so you could try fixing that.
Given that this is emacs, you can do basically anything you want. I'll try to put together a short presentation/demo/article on using it for notes. I did record a complicated and boring [screencast](https://youtu.be/kxFvpJPetaw) showing off `outorg` a while ago. That's one way to use org facilities from within Haskell code.
I think that's actually a common problem in open source and one that I often have faced in the past. I really wanted to do something, but just couldn't find the right thing, or couldn't get in touch with it.
yeah I keep some history. in the comments, not Haddock's. but it helps remind me why this is a newtype not an alias (avoid impredicative types) or a data over a newtype (existential quantification) etc. one line is all it takes to trigger all those memories.
the quality of libraries on hackage is high enough that scrolling through them until one catches your eye would not be a waste of time. https://hackage.haskell.org/packages/#cat:Statistics
A sage math like library but use quasi quotes for equations?
Thirty seconds seems much too slow for prime sieving. A completely basic, [naive program](https://gist.github.com/anonymous/79c662e3fd34919f799e) in C++ finds all primes &lt;10^8 in 2.5 seconds (10^7 takes 90ms) on my laptop, and it won't be even remotely competitive with an optimized cache-friendly implementation that uses wheel factorization as well (like primesieve). Incidentally, taking 80ms to find the 10^7-th prime seems totally reasonable to me. 10^7 is just not a very large number, and the work being done with a sieve of erathosthenes is very simple and quick. While Haskell can be expected to be slower than C++, being too slow by a factor of 10-100 is much too slow: something is wrong with the approach.
The [diagrams](http://projects.haskell.org/diagrams) project has lots of potential places to contribute and is a lot of fun to work on. We're happy to help newcomers get up to speed and find ways to contribute. 
what's "results of abstract interpretation"?
does "elaborated" mean something different from "(type) inferred"? not too familiar with the jargon.
Yeah, it works great now on a fresh image. I didn't even do the sudo apt-get upgrade step, but I did have to futz around with sudo apt-get install debian-archive-keyring &amp;&amp; sudo apt-key update. With the updated GHC flags that I added to the stackoverflow post, everything works: GHCi, TH, GHC, and floating point. If you put those flags in the right place in ~/.cabal/config you can do everything just like you would on any other setup. It's awesome. Now time to build some robots. :)
Ok thanks. I will continue the tutorial now with my belief suspended. edit - I think maybe someone misinterpreted my meaning here, my fault. I was sincerely thanking tactics for the information provided. I understand it, and will continue with that understanding, but not all that confident about the consequences of it and how it applies to programming in general. Sheesh.
That's great to hear - especially about the floating point stuff. Could you tell me what you did with the keyring and sudo apt-key update and ~/.cabal/config stuff since I didn't do any of that but got it working with the upgrade. You may have a more elegant solution, however I was suprised at how much of the system got major upgrades which we will all obviously want at some point. It will also help others who read this in the future...
Don't feel bad about having difficulty understanding this -- you're not the first to stumble over it. The Grandfather of modern type theory, Per Martin-Löf, even messed this up in his original theory. He originally had Type be a type of Type (i.e., both a type and an object of that type). It was soon discovered that, as tactics mentions, this has a Russell's Paradox problem (in this case, called Girard's Paradox). Martin-Löf subsequently revised his original theory by introducing a cumulative hierarchy of type universes, and it's on this revised type theory that languages like Idris are based. Anyway, if you get stumped by this issue, don't feel bad, because even the world's leading type theorists were stumped by it for a while. If you actually want to understand exactly what's going on, read "An Intuitionistic Theory of Types: Predicative Part", but be warned it is fairly esoteric.
Perhaps have a look at [Curry](http://www-ps.informatik.uni-kiel.de/currywiki)?
Also see the answers from 2 months ago, when I asked the same question: http://www.reddit.com/r/haskell/comments/2y1i4x/haskell_projects_looking_for_contributors/
A reflection of creativity, very interesting! You obviously put some time and work into this, thanks, I needed a smile! 
I wrote a little personal wishlist for improvements to criterion, which could be some combination of Haskell, and JS/HTML/CSS http://www.reddit.com/r/haskell/comments/2wii6n/haskell_google_summer_of_code_proposal/cov11xo
&gt; what they see is a language with little tooling While I agree with the general sentiment of your post I have to disagree on that. Haskell is so far ahead of everything else out there in terms of available tools it surpasses many mainstream languages. And most of those tools are not exactly hidden either (e.g. profiling, editor support, hlint, hoogle, hayoo,...).
&gt; No. Defining Vect : Nat -&gt; String -&gt; Type is extremely misleading because the English label "Vect" is now completely wrong. This is not a Vect at all. I don't follow this reasoning. Why isn't it a vector? For example, let's say you want to be able to define vector of a specific length containing only a specific number (for whatever reason), wouldn't it be defined as: Vect : Nat -&gt; Nat -&gt; Type So, my question is how do you combine this with: Vect : Nat -&gt; Type -&gt; Type I.e. how do values (numbers etc.) relate to (singleton) types?
SDL was used over SDL2 as I was already familiar with it (I was already in new territory with Haskell and Yampa, so some amount of familiarity was nice!). Yampa was chosen because it was easier to use for a beginner like me, however something like netwire would have probably served just as well. Any comments or problems with the code that you see/are worth point out? 
Just from a quick glance your Haskell-foo already seems much better than mine so I don't think I have much to contribute.
Indeed it is not. The *only* things T which may stand to the right of : and classify other things are those things which are classified T : `Type`. Values in types other than `Type` are not "singleton types". It is not the case that 3 : 3. Correspondingly, if the second argument to `Vect` is to classify the vector's elements, it must be a type and thus have type `Type`. One might well write data Foo : Nat -&gt; String -&gt; Type where and it is perfectly reasonable to define a type constructor with arguments of type `Nat` and of type `String`. This would be ok... Nil : Foo Z a but the following would be in error (::) : a{-XXX-} -&gt; Foo k a -&gt; Foo (S k) a{-YYY-} because the use of `a` at `{-XXX-}` requires `a` to be in `Type`, while the use of `a` at `{-YYY-}` requires `a` to be in `String`. It is possible to define singleton types which wrap up copies of a given value. data Single : a -&gt; Type where -- implicitly, a : Type Only : (x : a) -&gt; Single x You could then construct the type `Vect 2 (Single "foo") : Type` which would be inhabited by and only by Only "foo" :: Only "foo" :: Nil All types are values. Not all values are types. The values which are types are exactly the values of type `Type`.
Ok, you're right, you could create a vector with only a single repeated value like that, but that's just... super weird and doesn't really make sense from a practical/usefulness standpoint, as without any dependency between the two types it's just isomorphic to (Nat,String) except much less efficient. While technically yes, I suppose you could say (Nat,String) is a vector of the same string over and over, that's a very unorthodox interpretation of a pair of Nat and String. &gt; I.e. how do values (numbers etc.) relate to (singleton) types? By singleton you mean a type like T(a) where T : (n:Nat) -&gt; Type? In type theory this is the big pi type and corresponds to the logical "for all" statement. "For any natural number n, I can construct a type T(n)." If that T(n) is the same for all n, then this corresponds to logical implication -- "For any natural number n, I can construct (prove) the type (statement) T; therefore, n implies T."
Thanks, that clarifies things. I'm interested to know if there is a type system that combines subtyping, singleton types, union types and dependent types, i.e. where `1 : 1`, `1 : Nat`, `1 : (1 | 2)`, `2 : (1 + 1)` etc. all are true? What would be the complications of such a type system (except type inference)?
Great! - Calling c-functions as easily as in julia-lang or www.torch.ch.
&gt; Thanks for the very relevant questions. Thanks for the answers. They are very much appreciated. I think I'm coming around to this approach; I am going to have to put some time in and experiment with it. I find the mostly positive answers very encouraging.
Yeah, but it would be better if it had type classes. It's hard for me to part with those.
**Update:** I now removed these calls and settled for working with catamorphisms instead. It's much cleaner now.
&gt; If everything is its own singleton type, then everything which is a type already is also its own singleton type. E.g., Nat : Nat, which means there are things of type Nat which are neither zero nor a successor, thus invalidating induction. There should still be a hierarchy of types (or sets), like in Idris, so `Nat : Nat` would not be true (but for example `Nat : (Nat | String)` would be true). If `&lt;:` is the subtype (or subset) operator, `(1 | 2) &lt;: Nat` and `Nat &lt;: Nat` would be true. I think I see your point though, `1 : 1` should probably be written `1 : {1}` or similar.
&gt;When you start using Haskell you will find that you do not need an IDE as a crutch in it (or in 50% of the other modern languages for that matter) nearly as much as you do in C++, Java or similar boilerplate-heavy languages. I'm not sure I agree with that line of thinking. Just because you can make do without an IDE doesn't mean you should. Either way, this is a moot point when discussing the sales pitch; you can't just hand-wave away people asking for first class $FAVORITE_IDE support. Even trying makes you look religious. If the people are asking for $FAVORITE_IDE, your job is to give it to them, or at least an equivalent alternative. Your job is not to convince them that they don't need it. Programming languages are a buyer's market, people can just go elsewhere. &gt;That said there is IDE integration, editor integration and even a full Haskell IDE (Leksah) which is much more than most languages offer, even languages with communities much larger than Haskell's. Think back to when Mono only had MonoDevelop and a few editor plugins. That wasn't a compelling tooling story for tooling back then. We should apply the same standard to Haskell; having a "full Haskell IDE" is not anywhere near as good as having Haskell integration in mainstream IDEs.
&gt; The basic tenet of programming with algebraic effects is that performing an effectful computation is separate from its interpretation. I am having some difficulty understanding this statement. In what respect is performing the effectful computation separate than the interpreting of the computation? What is meant by "context" in the lines following the above quoted text?
I think "performing" is perhaps a clumsy wording. Programs are written to an interface of the abilities they expect to be offered by their environment. Those programs can be run in any environment which implements the interface. The mere ability to issue a command chosen from the interface is indeed separate from the business of handling that command.
IMHO the obvious solution is to teach that stuff in undergrad, but we're not quite at that level of acceptance yet. I could definitely see a Monads 101 course in mid-to-late CS undergrad. They're useful as hell for decoupling processes/aspects, so even if you don't use them in your day job they can help you think about code better.
Understood.
Okay, the guy is perhaps coming off a bit strong, but it certainly got me thinking about this topic again. *I'm* asking the Haskellers who've invested a couple years into Haskell and are already sold. I've never been completely sold on laziness. I've explained its virtues to other people; certainly laziness beats macros. But sitting back to think and look through my own code, I find it difficult to immediately notice areas where I'm truly taking advantage of laziness. Some points that occur to me: * Lazy data structures other than [a] (which *is* nice for looping/as a control structure) appear to me mostly to be a novelty. * Self-recursive or otherwise infinite data structures are also a novelty to me. * I don't really take advantage of lazy bindings as much as I think I did. * It's not clear to me that fusion applies to much of my code and how much of that could be inlined nicely because the code is known to be pure. * I don't like that `a` can contain `_|_`. It really bugs me that exceptions are sitting in any value like implicit `null`, and the interaction between exceptions and laziness is scary. At least in ML only "a -&gt; a" can contain `_|_`. When you have an Int, it's really an Int. * Monads don't need laziness in their construction. * The way newbies have always been and continue to be confused as hell about foldl and foldl' when writing trying to code still irks me. * Lazy IO is a novelty that I never take advantage of. Conduits/pipes seem a much better model with reasonable behaviour and performance. * When I start to care about performance I start inserting bangs to make sure I know when something is evaluated at the right time. * It bothers me that record types and data structures in general are lazy by default, when almost 90% of the time I don't want any lazy slots. * Lazy and strict versions of Text/ByteString and converting between them ad nauseum bugs me. I prefer conduit/pipe streams. * Having implemented a couple Haskell runtimes and dealing with GHC and reading through Hugs's, there's definitely a cost paid to implementing it efficiently and a disconnect for the programmer when reasoning about it. * Despite what others say—I acknowledge their view—I *would* like a debugger and a reasonable stack trace. I feel like I can have a go at this part of Haskell because I love it so much, and these feelings are just a trade-off that I accepted years ago and still do. I wonder, do we all feel like this, or is pretty much everyone fully satisfied, etc.? Does Idris or strict-Haskell look more attractive? Today I don't worry about laziness because I've been trained on it for years, but what cost do newbies pay? Also I would, and judging from discussions I've seen in the past, love to see a nice collection of "this is regular everyday code I couldn't have written without laziness *by default*." 
I absolutely love laziness. It allows me to do away with the unecessary distinction between functions that produce values on demand and functions that produce collections of values. I can't imagine maintaining a compositional style nearly as clean as in Haskell in a strict language.
It was a fast attempt at writing a mapping from Type_0 to *, Type_1 to * -&gt; *, etc. 
Safer! I would have expected `data.maybe`to come with something like `defaultMaybe`. In Scala it's called `aMaybe.getOrElse(default)`, and in Haskell `fromMaybe default aMaybe`.
Holy hell is this a big ask. There's a second language standard now... Haskell2010. It supports a crazy number of language suggestions, and you'll probably get a lot by just learning about those. Also, were Monad Transformers a thing back then? I'd look into those as they can unify pure and impure functions 
It got an ecosystem with Hackage and Cabal and lots of packages. 
I guess one should not complain about too much abstraction. It just makes it harder to conceptualize for beginners or those of an intermediate skill level. I don't know if I'll ever count myself in the second of the two camps when I read or work with some of the haskell articles I read. Examples: [Send even more money](https://www.fpcomplete.com/user/chowells79/even-more-money) I haven't finished reading this article. I read some of the others in the series, which were interesting. An excerpt: -- CS = Counting Selections type CS a b = StateT [a] (ListT (Writer (Sum Int))) b This is my thoughts when I read this line: So it takes a list of type ``a`` in the list monad transformer context which writes the length of the remaining number of selections, and the ``b`` is... ah... well, if it's a ``StateT`` it should be the passed along state. Which is... ah... hmm. [Lenses, Stores, and Yoneda](https://www.fpcomplete.com/user/bartosz/lenses-stores-and-yoneda) Where almost every type level variable single letter. [Ollie Charle's asteroids](https://github.com/ocharles/netwire-classics/blob/master/asteroids/Asteroids.hs) asteroidsRound nAsteroids c d e f initialScore = proc keysDown -&gt; do [...] I look around to find out what the ``c d e f`` are from context. Lenses: type Lens a b c d = forall f. Functor f =&gt; (c -&gt; f d) -&gt; a -&gt; f b I use them, but I don't ever know I'll be proficient in their black magic. Don't get me wrong, I enjoy reading / extending / learning, and this is exemplary work by people much smarter than I am. I really should sit down and put more time into it. But I have more patience than most of the people I know who scoff at this writing style. Maybe I don't know the right people. :) When you have a list of arguments that goes on for a bit, or if you assume that the information is self-explanatory then the consequence is that you will lose people more quickly or at the very least the content becomes slightly less accessible.
If you take away my laziness, your language better bloody well be total and have a good accounting of codata.
Or footnotes.
My favorite aspect of laziness is the fact that it makes our language more combinator-centric. I.e. using foldr, foldl' instead of explicit recursion. I'm sure there are better examples too. :) I think most of us Haskellers agree that laziness is no silver bullet, but in my opinion it fits like a glove on Haskell. However, give me totality, a separation of data/codata and then strictness starts to look very appealing
&gt; Bottom is lurking, unseen and immortal. How is that different with strict languages? They have non-termination to deal with as well, only there you encounter it a bit more often if you write the same code with the same parameters.
Pulling bindings into `where` clauses is a great feature. It keeps the branching structure concise and readable, while also not confusing the binding definitions with the branching context.
Was Applicative around back then?
Are you also willing to sacrifice tail call optimizations for it? I honestly can't say that I ever needed to know the exact stack trace while writing Haskell, at most I would have liked the location of the error/undefined I encountered before I stopped using partial functions.
I feel it is essential. Though, I'd probably still use Haskell if it weren't lazy. I came for the purity. Lazy-by-default helps keep the purity honest.
In the future, when crosslinking, please use the `np` subdomain. I think the /r/haskell community knows better than to vote brigade, but it's generally considered good practice to `np` anyway.
I think calling them lazy and strict `Text`/`ByteString` may have been a mistake. Having rope-like structures where chunks of data can be worked with as though the chunk divisions weren't there is a distinctly different data structure design than a dense array. There is some niftyness in being able to switch between the variants just by changing the import, but the clumsiness of working with both looms far too large over too much code.
Some strict languages have lazy list, the result is often a best-of-both-worlds thing. See F#'s `'a seq` type.
&gt;a la combinatorial species I have that paper lying around, but I've never gotten around to reading it. How useful is it?
So what makes a language total? I'm new to functional programming.
TCO exists in strict languages too.
&gt;My favorite aspect of laziness is the fact that it makes our language more combinator-centric. I.e. using foldr, foldl' instead of explicit recursion. I'm sure there are better examples too. :) I guess you're saying that lazy combinators stop when they need to, while strict combinators always process the entirety of the output. I think that's a solid point.
zip [1..] is nasty. At work we have zipFrom 1, which is the strict equivalent of zip [1..]. I find it much more pleasant even in a lazy language. The fact zip can zip lists of different lists is somewhat ugly. I'll also add one to your list: * Do you ever write an exception of type IO () and not worry about whether you are defining bottom, or an IO action that when evaluated raises bottom. In a lazy language they are basically the same. In a strict language, they are totally different.
From my reading of this thread, 90% of the advantages of laziness have to do with lazy lists, and those also exist in strict languages.
You can have that in a strict language if you change the semantics of desugaring. Mu does that.
I think I've been a Haskeller for less than 15 years. I don't know that much has changed as far as initial learning, although you might spend more time learning about Applicative, Traversable, and Foldable. You know, since Applicative is now a superclass of Monad, Applicative and Traversable have a strong relationship, and Foldable is a superclass of Traversable. GADTs, Associated type aliases / data types, Type / data families, Constraint kinds, Data kinds, etc. are "advanced topics" that I think either got an overhaul or have been added to GHC in the last 15 years. Oh, most of us switched from Hugs to GHC, BTW. Platform came and went. I still like it, but most of the people that get to use more Haskell than I seem to tend toward just installing GHC and Cabal, even on MS Windows. Stackage (STable Hackage) is a good alternative to hackage, and it also inspired LTS Haskell and a few neat CLI tools. You should learn lens. Most people encourage an avoidance of Lazy IO, but the original streaming library as fallen by the wayside. Both pipes and conduit provide good streaming interfaces and both have a descent number of convenience libraries built around them. There's more, too, but I'm out of time.
&gt; Also, were Monad Transformers a thing back then? ...were Applicatives even a thing back then? 
&gt; second language standard Second? What was the Haskell 1.4 report?
It is another way to produce confusing stack traces though, quite independently of the strictness/laziness issue.
Haskell has stack traces: [Instructions here](https://wiki.haskell.org/Debugging#Stack_trace)
I don't understand your IO () point. Are you talking about error vs. throwIO? Because those are very different in Haskell. Can you given an example?
Oh, wow, that's neat! So you float bindings in through the conditional guarding their use?
I think it's misleading to say you can get the benefits of laziness by desugaring. You can push bindings closer to their uses, but that's not the same, and it's easy to construct examples where it doesn't get you enough laziness. 
Yes. 
&gt; Monads don't need laziness in their construction. I don't quite follow. Would something like main = loop where loop = do foo &lt;- doSomeIO if predicate foo then return () else loop still work without laziness? Wouldn't it run into an infinite loop trying to construct the IO () value?
All programs terminate. So no Turing completeness but you recover the useful case of nontermination with conduction/codata.
Explicit laziness gets messy. I've not seen a good proposal for that yet. 
Lack of general recursion. Limited to primitive recursion.
But that was a mistake. :)
Turing completeness is entirely compatible with totality. It is only bullshit completeness that totality excludes.
Yes, I can see how that would be surprising.
Something like this? [https://ghc.haskell.org/trac/ghc/wiki/StrictPragma]
Just that, in a total language, you can't write a program that doesn't terminate. The standard example would be [simply-typed lambda calculus](https://en.wikipedia.org/wiki/Simply_typed_lambda_calculus) - every well-typed expression in STLC can be reduced to normal form in a finite number of evaluation steps. Of course, a language where your programs can't run forever isn't very useful in the real world, and that's where the idea of codata that /u/kamatsu mentioned comes in. Informally, the idea is that you have two flavours of type, "data" and "codata": the "must terminate" rule only applies to "data" expressions; for "codata" expressions, the rule is instead that they must be "productive" - that is, always produce at least a partial result in a finite amount of time. For example, the Fibonacci sequence as an infinite list couldn't be "data", because it would take an infinite amount of time to evaluate it in full - but it can be "codata", because you can write a function that computes the next value every time it recurses, ie always in a finite number of steps (`f x y = x : f y (x+y)`). The type systems of total languages with codata are structured in such a way that you can never mix "data" with "codata" unsafely - your code will either always terminate or always be productive.
Something like that. My flavor would be a little more explicit about marking lazy values throughout code, would leverage the type system to track laziness, and would work for TLDs.
&gt;I have never heard the word "tradeoff" once in my life
But then you can blame the problem on the partial spin function. But it's true that every subexpression of type Int does not necessarily have a value of type Int. 
There is nothing new under the sun, the arguments around laziness have been going on for ever. Here are two blog posts worth reading including the long comments threads they generated: - [The Point of Laziness](https://existentialtype.wordpress.com/2011/04/) - [More Points for Lazy Evaluation](http://augustss.blogspot.hu/2011/05/more-points-for-lazy-evaluation-in.html) Kind of point and counterpoint. 
I think I'm missing the point. You're saying that you can have a total language that allows general recursion?
Interesting. It seems clear to me that laziness and strictness are both very important, and languages should try to support both as well as possible. How fleshed out are your ideas about encoding laziness in the type system? Are there any good papers on this topic? How well do language backends support mixed lazy/strict programming? I think I remember reading a paper that showed this to be difficult. Maybe it was *Bridging the gulf: a common intermediate language for ML and Haskell*? Would this lead to an explosion of different lazy/strict data structures? data List a = Nil | Cons a ~(List a) data List a = Nil | Cons ~a ~(List a) data List a = Nil | Cons a (List a) data List a = Nil | Cons ~a (List a)
Not everyone here is active enough in the broader reddit "community" to be aware of its peculiar mores regarding "brigading." `np` links are a good idea; maybe even a good rule? I'm always more afraid that I'll be mechanically shadowbanned if *comment* after following a cross-subreddit link like this. I've never seen any clear disclosure of when that's prohibited. In particular, I don't trust that the system recognizes when you're actually a subscriber and bona fide participant in both subs. As a result, my personal policy is never to comment or vote on a thread I didn't reach directly from the reddit front page.
I've programmed in strict imperative languages plenty, but I'm not sure I can think of an example of a space leak in such a language. Would you please give me an example of a space leak in a strict language?
Yes, * ~= Type_0. But no, Type_1 ~/= * -&gt; *. Instead, Type_0 -&gt; Type_0 ~= * -&gt; * and ☐ ~= Type_1. Even in GHC there's no symbols for the "type of" *. But, in general the "type of" kinds are called "sorts" and the one "sort" that Haskell has is sometime written as ☐ and sometimes as BOX.
Actual problem: I'm thinking of decorating each node of a tree with a pointer to its parent (the "decorated" parent, I mean). I think laziness would help here. How would I do this in a pure, strict language?
Ah, thanks for the clarification :)
I always felt the use of lazy iterators in strict languages was really cumbersome. Refactoring code to be lazy is quite annoying, too.
Generators/Streams are painful to combine with the rest of (usually nearly completely) strict ecosystem. I haven't encountered a single language that makes it easy to use or read.
Yes, but you have to formulate it a bit differently. You can't have have, say, an interpreter that loops forever, but you can have an interpreter that every 1,000,000 iterations asks permission to continue (or outputs a marker saying it's still working). 
Doesn't total mean guaranteed to terminate with the result?
That is correct. I think what people like /u/dogodel are talking about is that errors should be flagged for as close to the source as possible.
&gt; aziness. It allows me to do away with the unecessary distinction between functions that produce values on demand a and lazy if statements. 
Do you mean https://www.cis.upenn.edu/~byorgey/papers/species-pearl.pdf ? There are many papers, but that one's probably the best intro for haskellers. The most useful application for the working programmer is giving an economical language for equational reasoning about the shape of datatypes (think generic haskell). Beyond that, Species are deeply fundamental to programming, but in the same way cartesian closed categories are - you probably won't run into them directly. There's exciting research going on based in species, and I'd argue the next big programming revolutions (practical linear types, ornaments, etc) will come from them. If you like that paper, I also highly recommend Brent Yorgey's recent phd thesis, it's surprisingly readable. The citations in his thesis will keep you busy after that ;) 
Not necessarily. You can express that guarantee. You can also express whatever sort of recursion you like on a caveat emptor basis.
You could go with a variation on bang syntax. ifThenElse condition ~one ~two = if condition then !one else !two There is an interesting question of how you distinguish between forcing `one` and `two` and passing along `one` and `two`, I used `!` above but there may be other ways.
I'd love an `XCodata` language extension that made all data strict and provided a `codata` keyword. Backwards compatibility would be easy because you could find and replace all `data` to `codata` then incrementally add `data` in where you wanted it. Prelude would need to be rewritten to take advantage of it but given that we'd be able to prevent `length` from being called on infinite lists I think the added type safety might be worth it. Would you be able to replace `XBangPatterns` and `seq` with `data`/`codata`?
&gt;Species are deeply fundamental to programming, but in the same way cartesian closed categories are To be honest, I still don't get what's the big deal with cartesian closed categories. We have yet another construct that's equivalent to intuitionistic logic, what now? &gt;ornaments That one is also pretty elusive, though that's 100% on me for not doing the legwork and reading papers.
Would not having a default be possible? I.e. you need `!` to be strict and you need `?` to be lazy and function arguments are syntactically invalid without them?
I think it's also important to remember that the developers matter more than the language. If Haskell is the only language which is lazy be default, and new knowledge or experience can be gained for the developer by operating in a lazy language, then it would be a loss for developers if the only lazy language we have went strict. I'd rather Haskell never went mainstream but was still viable for education which I could apply to other languages, than lose an easy test bed for laziness.
Really? Pervasive laziness is only acceptable (from my point of view) in a total language; type theory has a lazy operational semantics by default, and this makes the meaning explanations more elegant. But laziness combined with partiality is pretty much a recipe for trouble. Strictness, on the other hand, makes dealing with partiality (and effects!) totally tractable. Of course, my preference is to mix laziness and strictness in the style of polarized type theory, or Levy's CBPV. But if I have to choose discretely, then to me there are only two choices: lazy + total, or strict + partial. EDIT: I want to stress one point about laziness + totality. I learnt from Conor McBride the following point, which is that if you have totality, then there a tons of things (including proofs) that you never need to actually normalize, so long as you make sure that other results do not depend on their particular value.
How is that in any meaningful way different from the lazy case where, effectively "spin 3 = \_|\_". I think analysis for calling a partial function is isomorphic to analysing for a potentially \_|\_ "value".
In Idris, part of the defense of the choice of struct-by-default is that the language is total-by-default so the strictness is just an optimization. But, IIRC /u/edwinb has stated the real reason that it is strict is that it was just the more practical choice. The `Lazy` type constructor and good decoration of the standard library with it might be fine, but I think that's yet to be seen.
I thought there was already a proposal out there for a `Strict` language pragma. Which basically thows `!` decorators everywhere and makes you write `~` explicitly on (e.g.) record fields that you want to be lazy.
&gt; Does Idris or strict-Haskell look more attractive? Idris is looking "right" to me. Explicit laziness, exposed at the type level, with implicit conversions back to the strict world, a totality checker, a more expressive type system without the need to write the code in the type system separately from the rest of the code. Looks like the perfect Haskell... Haven't written anything in it, though, so not sure how much of an extra burden dependent types are, in practice.
Does it still mean you have to compile executable for profiling? Because when I tried to run such program on production it was too slow to be usable, unfortunately.
If laziness makes optimisation easier, why do simple like `myArr.map(a =&gt; a + 5).filter(a =&gt; a&lt;10).fold(sum)` run faster in Rust, C++ or D than in Haskell? Serious question. Haskell can be pretty fast, competitive with Java, but I don't see why it's not as fast as C, so laziness seems like the most likely explanation, as those other languages aren't lazy.
A total program offers the observations appropriate to its type. For codata, that means you can always do a top-level case analysis. So, yes, that means at least emitting an apology for the lack of an answer. There are, however, perfectly sensible ways to express what it is to *be* a general recursive program, without fixing in advance one's (hopefully defensive) strategy for running such a thing. Just making your general recursive function the operation you can invoke in a free monad is a good initial (har har) step.
If you don't use the additional features of dependent types, you generally won't have to deal with their additional complexity. Now, you might have a library author that saddles their users with complexity, but I think that can *generally* be avoided when writing a dependently typed library and *should be* when possible.
It's not a typo. If you're still wondering in what way Haskell is pure, I'll tell you tomorrow.
That's some interesting reasoning. For years, strict languages like ML and Lisp benchmarked worse than imperative languages, and people blamed this on the fact that they were functional(-ish). Today, Haskell benchmarks faster than dozens of other languages. As those languages aren't lazy, is their strictness to blame? If you come across a benchmark where Haskell comes out faster than Rust, will you decry Rust's choice to not be lazy as a fatal mistake? EDIT: I gave you the benefit of the doubt that you'd tried this before making any claims, so was curious myself. Here's how your [suggested benchmark](https://gist.github.com/acowley/9472cc5eacdc9848f1c3) went for me where I've manually fused the C++ version. TLDR: The longer, manually fused C++ version takes 98% of the time of the high-level Haskell Vector version.
Yes.
Well, if a totality checker actually checked whether this program terminated, you would prove the conjecture.. But there's always *some* conjecture that your totality checker can't prove whether it's true or false.
I'm sure nobody would mind if you wrote something better.
I'm about to give a talk (in an hour or two) on using cofree comonads to do this kind of interpretation. The nice thing is that you can use the product of the functors on the cofree side to factor out the different bits of the interpreter. I'll be writing about this after I give my talk (and catch up on sleep). 
Nix as in the purely functional package manager? That's awesome news.
If that's how you feel, you might want to try stackage.
Of course (as twitter observes) it's quite possible that when checking out a particular candidate, we diverge rather than looping. In that situation, what's the deal? A total language can still express the infinitary process of trying to see if the candidate sequence loops.
&gt; run faster in Rust, C++ or D than in Haskell Does it? I compiled this Haskell program with the LLVM backend: import Criterion import Data.Vector.Unboxed as V test :: Int -&gt; Int test n = V.sum (V.filter even (V.map (+ 1) (V.enumFromN (1 :: Int) n ))) main = benchmark (whnf test 1000000000) ... and it runs in 600 ms (and I get the same results using `time`): $ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.6.3 $ ghc -O2 -fllvm test.hs time 597.5 ms (591.5 ms .. 605.0 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 596.3 ms (594.9 ms .. 597.1 ms) std dev 1.289 ms (0.0 s .. 1.462 ms) variance introduced by outliers: 19% (moderately inflated) When I hand-roll the same loop in C: #include &lt;stdio.h&gt; #include &lt;stdint.h&gt; int main() { uint64_t i; uint64_t total = 0; for (i = 0; i &lt; 1000000000; i++) { if (i % 2 == 0) { total += i + 1; } } printf("%llu\n", total); return 0; } ... and compile with `-O3`, the C code ~~runs almost 3x as slow as the Haskell code~~ (Edit: Runs about the same speed with `-O2`, see below): $ gcc --version gcc (Debian 4.9.2-10) 4.9.2 Copyright (C) 2014 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. $ gcc -O3 test.c $ time ./a.out 250000000000000000 real 0m1.591s user 0m1.592s sys 0m0.000s 
If you want to create any kind of EDSL, dependent types are your friends. Here is a cool example from the main docs of Idris: http://docs.idris-lang.org/en/latest/effects/hangman.html I think the main advantage of dependent types for general purpose programming hasn't been found yet. For instance, imagine if we could have web libraries written with a similar language like this one: https://github.com/edwinb/Protocols We are already seeing some "dependently typed web libraries" in Haskell, with servant for instance (http://haskell-servant.github.io/tutorial/api-type.html). Using dependent types for this kind of EDSL can become pretty popular. And that is just with web libraries, you could apply the same reasoning for any other domain. There may be a "perfect dependently typed library" for any domain you choose 
... or even just contribute an uninstall command
Like many benchmarks, I suspect it's a matter of how well you know the language :) In about five minutes, I got this result with Rust using this code: println!("{}", Range::new(0, 1000000000).filter( |i| i % 2 == 0).map( |i| i + 1 ).sum::&lt;u64&gt;()); (The `Range::new` should properly be `0..1000000000`. The iterator I am using is tweaked to include [a fix to make them do better codegen, which is already in the pipeline but hasn't made it onto master yet](https://github.com/rust-lang/rust/pull/24705/files); otherwise, this is completely idiomatic Rust). $ rustc foo.rs -C opt-level=2 -C prefer-dynamic -C target-cpu=native $ time ./foo 250000000000000000 real 0m0.524s user 0m0.513s sys 0m0.006s BTW, I think a benchmark like this is not very indicative of much of anything... the differences here mostly suggest that LLVM is really good at optimizing loop microbenchmarks, likely better than gcc is (and perhaps also better at doing codegen for 64-bit integers). The problems that real programs face (both performance-related and otherwise) are completely divorced from that. **Edit** Actually, I think that misoptimization only applies to 32 bit integers; when I just used the normal iterator I got essentially identical results. **Edit 2** Also, given the fact that Rust's iterators are lazy and Rust's few custom compiler passes are not related to anything like this, it would appear that optimizing lazy lists isn't so very different after all :P
the result of running `cabal uninstall` on HEAD $ dist/build/cabal/cabal uninstall cabal: This version of 'cabal-install' does not support the 'uninstall' operation. It will likely be implemented at some point in the future; in the meantime you're advised to use either 'ghc-pkg unregister PACKAGE_NAME' or 'cabal sandbox hc-pkg -- unregister PACKAGE_NAME'. Nowhere does it suggest that you nuke your installation. I don't know where you got such an idea. Whatever issues cabal has, it is the result of many years of effort by open-source contributors who worked on the system out of their love for the Haskell language and their desire to improve the lives of others. Language such as yours is provocative, dim, and useless in the context of a genuine technical conversation. If this is your preferred method of expressing your frustrations, then if you decide to spend the rest of your life coding in Java, I tend to doubt that you would be much missed. Warm Regards.
I agree; I've never actually felt the benefit of being able to switch interfaces with an import, because so often the interface I am forced to use is dictated by the libraries I am working with, which don't usually export an equivalent interface for using strict/lazy types underneath.
In my Masters projects, laziness makes debugging with print statements hell. I'd be all for it if we had a proper, eclipse or gdb (or better!) level debugger. In the meantime it's making my life quite difficult. 
Note that the `map` goes before the `filter`; your version is not doing as much work. You should also compare the Haskell version on your machine to make the timings comparable. However, my point is that laziness doesn't make everything go slower as the original commenter suggested. Everybody is quick to blame laziness when other issues like codegen are often more important.
My guess is that most of the hairy parts of laziness could be eliminated and the benefits retained if there were easy to use abstractions and libraries that would explicitly ensure that space invariants were held by construction http://apfelmus.nfshost.com/blog/2013/08/21-space-invariants.html I'm not sure how this would look or if it is possible but it feels like some patterns already informally exist in how seq and ! are used that could be enforced by a library. So a library where "spine-strict" and "value-strict" were native concepts. Seq and ! are like laziness assembly language. Another example is that with a plain list there is at least laziness on the whole list value, laziness on the list elements and laziness on the "rest of the list". It would be great if those could be controlled at a higher level, for example by restricting access to only functions from a certain package. The containers package already does this internally but it doesn't seem to be easy to spread this across an application. For records it seems like a useful balance would be if record accessors and constructors had strict and non-strict variants, maybe this exists already?
&gt; Note that the map goes before the filter; your version is not doing as much work. I translated from your C version, which also does this. I very much doubt that this matters to LLVM (it's "pure enough"); with the change, I get essentially the same time (within the margin of error): $ time ./foo 250000000500000000 real 0m0.554s user 0m0.541s sys 0m0.006s &gt; You should also compare the Haskell version on your machine to make the timings comparable. True, but I don't have the LLVM backend installed locally so it might take a while. I suspect the performance will be nearly identical. &gt; However, my point is that laziness doesn't make everything go slower as the original commenter suggested. Everybody is quick to blame laziness when other issues like codegen are often more important. Yep... and even in places where codegen isn't an issue (e.g., relational databases), whether your query is fast or slow comes down to heuristics / statistical information just as often as it does the existence of some specific class of optimizations. Another important factor in program performance is (obviously) whether you are using efficient libraries; people tend to treat this as a language-level concern, but I don't know if this is really fair.
Probably by using some sort of opt-in mutability like `ST`?
ghci has a simple debugger -- if you are an expert in gdb, it's not as good, but for relatively simple tracing and printing, it works.
Yeah, I noticed you used O3 as your optimization level. O3 is meant for experimental and unstable optimizations, you should use O2 instead.
Here's [the ticket](https://ghc.haskell.org/trac/ghc/ticket/3693). I'd say things are close, but they could still slip.
Really? That's pretty incredible, I'd love to see the assembly the generate. Could the result be affected by the fact that the input vals are hardcoded rather than read from stdin?
If the language is total, it can just as easily be lazy or strict.
I'm not sure what your mind is being blown by here. LLVM is an industrial strength optimizer and tearing through loop abstractions in code small enough to be fully inlined is its bread and butter. Rust does basically the most banal codegen imaginable (every move is sent to it as, literally, a memcpy) and it still ends up being fairly well optimized.
Spoilers for lazy language: data Tree a = Tree a [Tree a] deriving Show newtype DecoratedTree a = Decorated { runDecorated :: Tree (Decoration a) } data Decoration a = Decoration { parent :: DecoratedTree a , value :: a } decorate :: Tree a -&gt; DecoratedTree a decorate fa = root where root = decorateWithParent root fa decorateWithParent :: DecoratedTree a -&gt; Tree a -&gt; DecoratedTree a decorateWithParent parent (Tree a children) = t where t = Decorated $ Tree (Decoration parent a) children' children' = map (runDecorated . decorateWithParent t) children For a pure/strict language, you still need some way to tie the knot.
please do ;) I'd been thinking about this recently.
Okay so faster but not quite as fast as Haskell. Honestly I still find it rather unusual, maybe LLVM is better in this regard than GCC, at any rate it at least demonstrates the point that laziness in Haskell isn't a performance bottleneck as far as this case is concerned.
I was amazed by the initial result of Haskell being three times faster than C, which would have been pretty mind blowing if correct.
Eh... in a microbenchmark like this, small differences in instruction selection get greatly amplified. I wouldn't read too much into it unless the performance boost / hindrance comes from the existence of a language feature; it just so happens that that's rarely the case with C since it systematically strips out most guarantees that could conceivably have a performance impact on even mythical hardware (seriously, the C11 thread specification doesn't even require shared memory to be in the same address space, IIRC).
Repa and DPH and Accelerate are all made by the same research group.
I share most of your feelings. One thing I didn't saw in this thread. lazyness by default can discard (or make hard) some framework. In particular FRP. For example you could take a look at the thesis in the origin of elm. Also using clojure everyday, choosing strict by default except for sequences which are lazy by default feels a good compromise. 
Idris has exactly that, `Lazy : Type -&gt; Type`, with the elaborator inserting `Delay` and `Force` in the appropriate places automagically so there's even less noise than in your snippet above: http://docs.idris-lang.org/en/latest/faq/faq.html#how-can-i-make-lazy-control-structures
That's pretty impressive, I didn't realise the LLVM'd be able to vectorise Haskell like that. Seems I was wrong about laziness necessarily making code hard to optimise, but I stand by the assertion that it makes the runtime more complicated and makes local reasoning about time/space usage more difficult.
Excellent stuff!
Idris is looking nicer and nicer. I'm pretty sure it will win me over one of these days.
You can do `State (p,q) a`. You could also use a `StateT` transformer but if you just need two of the same thing that seems unnecessary.
I can't really see how to improve `zip` on `[]` without breaking something. If it tested lengths, it would no longer by total. If it were strict, you lose laziness which you might well want (particularly if the list is using itself zipped to something else in its own definition). This more seems to be about the fact that in situations where `zip` is used, Haskell lists may not be appropriate and we should work with infinite streams or length-typed vectors instead.
Equals for equals is so much better than what I used (easier to think about because you only have to do one step at a time).
How would one kill a thread, then? This is critical for e.g. async's "race" function. (I actually would like to get rid of async exceptions too *for normal code*, but if we're going to, then we need an answer for *all* the use cases.)
Nice. What you could probably do, then, would be to use "reacquire" to hold on to a single open websocket connection, and just send a refresh into that at the beginning of your main function. That way Halive wouldn't need to know anything about it. (if you want to try it out and need help, just drop me a line via @lukexi on Twitter or messages here!)
Sounds like a good use case for the enclosed-exceptions package.
Have a look at [Joachim Breitner's mail](https://mail.haskell.org/pipermail/haskell-cafe/2015-May/119770.html) from today.
I feel that laziness is a better default than strict. Personally I disagree with your points 1-3, as I use lazy data structures, self-recursion and lazy binding to great effect, but that might be personal style. The reason I believe that lazy is the better default is that it's easier to make lazy code selectively strict than it is to make strict code selectively lazy. The only reason we EVER want strictness is to avoid thunk buildup, because besides that laziness will never end up doing more work than strict. To avoid building up thunks it's sufficient to enforce strictness in exactly the one point where thunks would be build up and it will "Just Work",for example, forcing the thunk inside foldl' (note: I believe foldl has no reason for existing and would support renaming foldl' to foldl). The reverse does not hold for strictness, when we want to make something lazy we want to avoid unnecessary intermediate work, but this cannot be done by selectively marking an application as lazy. Why not? Suppose we have some strict function "foo" which, internally, maps a function on a list and we only care about, say, the first few elements of the list. Making the application of "foo" lazy will merely change WHEN foo is applied, it will NOT affect whether the entire list is mapped or not. To achieve this we need to put laziness annotations all throughout the internals of foo. This is what has eventually convinced me that laziness is the right default, because making my lazy code strict can be done as a single local change, whereas making strict code lazy would require me to spread laziness all throughout the entire call graph I'm invoking.
"all computable functions on the natural number can be written with the type `Nat -&gt; Nat`" doesn't seem like bullshit to me. An excessively strong condition perhaps, but not bullshit. 
&gt; Everybody is quick to blame laziness when other issues like codegen are often more important. Heap layout is another place where you pay a little extra for using Haskell: you lose control over unpacking polymorphic values (in C++ for example, compile-time template expansion will give you structure layouts appropriate for your types) and you can't represent an array with a header like you can in C. You pay for this both in terms of cache locality and also in terms of extra indirections. Another issue is that Haskell pins more registers than C which increases register selection pressure and results in more stack spills. Those things said, while it usually can't match C for performance, Haskell can actually be comparably zippy and more memory-efficient compared to e.g. Java.
I am a big fan of the slave-thread stuff - it seems a much better default than forkIO.
I would love to solve that in Haskell one day, but the mere suggestion that you can drop syntax and still have applicative effects seems to terrify some people.
But how? I mean, in a strict and pure language too? I can imagine something like this working in a strict language with mutable state: IO* loop; loop = new IO (bind (doSomeIO, \foo -&gt; if p foo then return () else *loop ())); *loop ();
Thanks for this. I've been thinking about how to make injection easy like that via the data types ala carte method for a while but wasn't sure it would work. This is looking like a pretty refined way forward. 
&gt; any expression that allocates memory live in the a Mem monad This change would break pretty much every piece of Haskell code ever written to date. I'm not saying it's a bad choice for some arbitrary new language, it's just hugely unlikely to land in Haskell due to backwards compatibility concerns.
&gt; doesn't document its exceptions This is horrible. &gt; Finally, we can catch only the relevant exceptions I think this is the "correct solution." In an ideal world, everything is documented with the sorts of exceptions it might throw. If this could be tracked by the type system, even better.
This question doesn't make much sense. What does "popular" mean ? Reddit's Haskell sub has roughly half as many subscribers as the Java sub, and about 70% of that of C++. Does that mean that it is hugely popular, or that Haskell "won" ? Which metric would ? Why ? Would it even matter ? 
I don't have much of a bias. If my only concern was semantics (i.e., input-output behavior) then I'd prefer call-by-name semantics, because I think it behaves more compositionally. But for practical concerns like resource consumption, debugging, stack trace, etc then strict evaluation is better. So there are good and bad things on both sides. (And, btw, I've written about 4 Haskell compilers.)
I'm always depressed when this particular line of unreason comes up. "That's only half a loaf so let's nobody have any bread." Remember that you are arguing to impose expressive weakness. Totality is just the beginning of the journey. Of course, those of us who currently want to make promises such as "it terminates eventually" have the ambition to make even stronger promises such as "it terminates in a nanosecond". At least we're working towards increasing the kinds of trust which are possible. Meanwhile, in a Curry-Howard setting, the mere knowledge that a proof object would normalize if you wanted it to means you can trust it without computing it at all. That is, the value of the strong promise is that it eliminates work. Contrast that with the situation in Haskell today, where type safety for GADT programs depends on evaluating terms not for their value as data but just to check they're not bottom.
But let us be clear who is giving up what. I am not giving up the liberty to write or run any programs at all. I am giving up only the liberty to lie about them. I am acquiring the liberty to tell some useful truths about some programs. It is surely the case that there are truths I have not the ability to tell, but that certainly doesn't convince me to give up what liberty I can have in this respect.
And the `#haskell` IRC channel is one of the bigger programming language IRC channels there is!
The biggest change for me (though this could just be recency effects) is discovering ghcid, which will automatically reload your project into a ghci-like (I believe people have already covered how 'things GHC can compile' is basically the working definition of Haskell now) interpreter as soon as any file in it changes, giving you virtually instant type error feedback no matter what editor you use. It's also trivial to set up. This isn't a change to the language, but I think it is the biggest change to the experience of using Haskell that I'm aware of. Right behind ghcid is cabal sandbox, though I suppose both of these are dwarfed by cabal-install itself.
I have never used it in production or in fact anything beyond a quick test. I am a fan of the design, but have no opinion on the implementation. 
Dunno why you're getting downvoted, the biggest non-technical thing to have changed in 15 years is the public profile and the nature of the community. These might be secondary issues but I don't think they are irrelevant to someone who wants to know what changed. A typical reaction from my colleagues when I mentioned Haskell back then was "Pascal? Whew, old school..." And the culture of programmers in general has changed quite a lot as I think most would agree. (I came across Haskell round about 15 years ago, haskell-cafe hadn't been going long and discussion there had an academic focus and even seemed a bit too polite compared to the all-flaming lisp community I was more used to.)
&gt; Remember that you are arguing to impose expressive weakness. Isn't it you who are arguing for a less expressive language. As far as I understand it a total language is strictly less expressive than one which might not terminate. I completely agree that we should have as many static guarantees as possible, I just don't see the benefit we gain for giving up intentional non-termination (e.g. in a main loop of a daemon program) considering we still have all those cases left that do not terminate on a practical level but would on a theoretical one. I also do not see a large number of bugs caused by non-termination. Your last paragraph sounds like you are talking about the type checker, perhaps it would be possible to have strict evaluation on the type level while keeping laziness on the value level? 
I like its semantics too. Also there is other interesting approach: https://github.com/adinapoli/threads-supervisor Unfortunately correct implementation is hard, so I'm still looking for better alternatives. I tried `async`, but is has drawbacks too. Right now I have my own set of primitives (based on `io-region` package), but I'd like to replace them in production with something widely used.
Let me be the kool-aid drinker here and say that OOP could easily be subsumed entirely by FP. Think about it for a moment... what, exactly, would an immutable class declaration look like? The answer is a data declaration. Consider: module Yarr where data SomeIntObj = IntObj { op :: SomeIntObj -&gt; SomeIntObj , otherOp :: SomeIntObj -&gt; SomeIntObj -&gt; SomeIntObj , self :: Int } instance Show SomeIntObj where show = show . self (.:) o m = m o o increment c i = i {self = self i + c} multOther i1 i2 = i1 {self = self i1 * self i2} newCounter c = IntObj (increment c) multOther counter1 = newCounter 1 0 counter2 = newCounter 3 0 *Yarr&gt; counter1 .: op 1 *Yarr&gt; counter1 .: op .: op 2 *Yarr&gt; counter1 .: op .: op .: otherOp $ counter2 .: op 6 This is some syntactic sugar and mutability away from actual OO code. And well, you know, the elephant in the room of record namespacing. If you think about it, a data declaration actually describes a *category* of OOP classes that can be specialized via method declaration and used in place of each other it some form of crazy *mutual subtype polymorphism.* &lt;/kool aid&gt; 
ParsecT as implemented by the parsec package is a continuation-based monad, so you are unlikely to come up with a MonadFix instance for it. However, there is an equivalent non-CPSed monad (IIRC, parsec v2 was based on it), and that one should admit a MonadFix instance.
Have you heard of [Kernel](http://klisp.org)? It took this about as far as it can be taken, all evaluation is explicit. I found actually programming in it frustrating unfortunately.
&gt; That's why you can't make a GADT pattern match irrefutable. Thanks for clearing that up. I always wondered about GHC's cryptic message when you try to do an irrefutable let binding of a GADT.
Some language changes are impossible to apply to existing languages (for less than the same effort required as doing so in an entirely new language and getting the exact same libraries written in the new language I mean). Take for example the Haskell separation of effects into the IO monad. Or the efforts to add static type checking to dynamically or even weakly typed languages.
I'm not really excited about the laziness as I never really got used to thinking that way. I even had an application I'd deployed in Haskell spiral out of control due to a non-strict data structure that was basically doing an "upsert" and should have run in flat memory space after a while. Due to another thread's lagging the thunks would pile up and kill my application. I felt I was "done" with the code after testing it, but I wasn't. Lazy (non-strict) by default feels like a mistake as a result. Yes, sometimes you really do want lazy sequences and they make thinking about problems very nice, but why on earth would I want to upsert into Data.Map or the like to be lazy?
You don’t take in account the fact that there’re a lot of services companies that don’t actually give a brezel about whether we could replace languages with better alternative. They have a lot of people who happily write shit, why would they want to change that, as long as they do profits?
Well, if I was a believer in market forces I would say that they will be out competed by those who do change and adopt better languages but since I have long ago lost faith in that particular mechanism I wrote that last paragraph in my previous post.
This is also acceptable, to me.
https://ghc.haskell.org/trac/ghc/ticket/10370#comment:13 is amusingly relevant, as a case where strictness caused a large space leak in ghc!
To start with. If you would like to hide memory management you should be using ForeignPtrs instead of raw pointers. When it comes to making sure that the library is initialised before using it you could have the initialisation function return a value of a token datatype that needs to be passed to the other functions. E.g init :: IO MyToken doStuff :: MyToken -&gt; IO () that way you can make it impossible to call those functions without initialising the library
So I looked and it is lazy, however it does process the entire sequence at once. I figure it would be easy to implement `Seq.lazierSort` or something though.
The latter `StateT p (StateT q Identity) a` also prevents you from using the *MonadState* type class (i.e. *get* / *set* / *put*) for the inner state. The best way is to use tuples or records, on more complex state.
The big deal of Cartesian closed categories is that you could overload lambas and function application to work for anything that implements a CCC type class. In essence, it would let you overload functional programming to work in a much wider variety of contexts. Imagine if you could print your program as a diagram or generate a circuit diagram from it without having to extend the compiler.
What I love about lazzines, is it unifies unary functions and variables (ome people argue that unary functions doesn't exist in haskell but i'm speakinp of things of type `a`). I can write things like maybe default id x or r = MyRecord {a=default} ... r { a = 1 } Without having to worry about the fact that default is a value or a function to call. In imperative, to not have `default` called unnecessarily, I would have to pass it as a function, and call it when needed. This is usuall fine, but for each function I write, I have to decide if it should take an `a` or an `() -&gt; a`. This problem is similar to accessors which needs to be evaluated, you have to do `r.getX()`. Ruby tries to solve this problem by desugarising `r.getX` or (`f`) to `r.getX()` or `f` but the (huge) drawback is you then have no way to talk about `f` (You need to use either `Proc.new { f }` which is heavy or `"f"` as string but then you have no garanty when you are evaluating it that you call function you intended too. Lazzy evaluation solves this problem in a really neat way. In fact, I don't really see the benefit of lazzyness until I need a sort of pointer to function until I realize that I don't need it. For example, I had this caching problem when I needed something a bit similar to a make file where I need to compute a value `IO a` depending on a file, and cache the result. I start creating a `cache` datatype until I realized I just needed `IO a`. All the dependency tracking could be done in the value itself. cache :: name -&gt; IO a -&gt; IO Bool -&gt; IO a cache name build needRebuild = do rebuild &lt;- needRebuild if rebuild then do value &lt;- build save name value return value else value &lt;- load name return value I can then created a cachable value x = cache "x" my_function (check_stale "x") and use it with any function expecting an `IO a` value main = print =&lt;&lt; x Ok, some people will say it's a bad example because I'm not using `a` but `IO a` therefore it's not really a unary function, but it is, in a strict language I would have to use `() -&gt; IO a` or something to make the distinction between something evaluated and something to evaluate. Then, indeed lazzynes as a huge cost, writting a program which works in test, but doesn't work in production because inputs are too big and not being able to do anything about it is a big problem. But that's what Haskell is about, it's pure and lazzy. If you don't want pure and lazzy there are plenty of alternative out there (OCaml F# etc ...) 
Reliable, no. Idiomatic and maintainable, definitely. I still haven't learned how to use iterated IO (`pipes`, `conduit`) or lenses yet.
 All you're doing by refusing to let the optimizer do its job in microbenchmarks is incentivizing compiler manufacturers not to bother making the optimizations too good. Deoptimized microbenchmarks aren't even that useful from the perspective of someone trying to actually optimize a routine, at least not by themselves, because you may be making codegen worse in cases where the information *is* known. Plus, if the compiler *was* just printing "theAnswer", but had generated enough instructions to make it *look* like it wasn't just printing the answer, you wouldn't have even noticed :) I'd be happy if every single benchmark on alioth just optimized to write(stdout, "theAnswer").
Does anyone have a link to the slides? 
&gt; I figure it would be easy to implement Seq.lazierSort or something though Or maybe not ;-)
http://cryp.to/slides.pdf
I'm sure I could teach most people to replace some part of their system with Haskell code in an afternoon if they already know a few languages. The problem is almost always that everyone seems to think they must know everything about Haskell to get started.
async exceptions are crucial for things like timeouts and (in the system I work on) resource limits. There's just nothing else that works. Dealing with async exceptions can be a pain, but the alternative - no async exceptions - is much worse.
&gt; Do you think Haskell can be a big deal in the future or not? Haskell has not reached it's peak; it will be even bigger in the future. But, on a longer timescale it will likely die out. I don't think it has crossed the threshold of immortality and I tend to doubt it will. (Despite my love for coherent type classes, it's a stumbling block for modularity and I think that may end up being what prevents Haskell from passing the threshold.) &gt; Does OOP offer more possibilities? OOP does not offer more possibilies. But, objects and messages are not an abstraction that will go away. Luckily, you can do OOP in Haskell (and most other lanaugages). Scala blends the two styles fairly well, though I do miss purity and parametricity guarantees. &gt; Or will they both coexist just as they do now? Yes, and things like Scala, F#, and OCaml illustrate how well that can work. Pure functional objects are a pattern that can be leveraged in Adga, Idris, Haskell, etc. today. Monadic / stateful objects may also arise in the future in languages that have "better" core calculi than Scala / F# / OCaml, though that is yet to be seen. (Dealing with some of the rough edges around monadic lenses may help here.) &gt; I just wonder if there clearly is a "winner" between OOP and FP. "Winner" is really quite vague. First-class function-like objects and a compact syntax for them will be a factor in most future languages. (Oddly enough, I think you can thank Javascript for that more than you can thank Haskell or Lisp.) But, I also don't think the core abstraction of OOP -- objects and messages -- will go away either. Some of the work in Idris around verifiable communication protocols might feed back into OOP; it's design-by-contract on a level Eiffel wishes it had. In effect, we'll all be winners if we can use any or all of the abstraction and techniques to more quickly produce better code that really improves quality of life.
Yes, I should really fix that.
&gt; Or the efforts to add static type checking to dynamically or even weakly typed languages. Gradual / incremental typing is a real thing. Parts of the program that can not be statically type-checked continue to carry around type tags and be type-checked at run time. Parts that can be statically type-checked can elide run-time checks and (if they are large enough) drop the type tags from values produced and consumed inside them. The boundary expressions are where compiler warnings or other diagnostics / reports are generated. There's still plenty of work to be done to make it all seemless without sacrificing any type-system power. (Many unityped or type-tagged languages existing internal calculi don't really deal well with higher kinds, dependent types, existentials, etc.) Arguably though, it's easier to start from something with a really nice static type system and then add a "dynamic dialect", but if done well it will still allow you do quickly whip up a program that works on your sample data without ever seeing a type error, and then gradually type parts of it rather than re-write in something other than `bash`. ;)
http://cryp.to/slides.pdf
(I read Brent Yorgey's blog posts about them many years ago, but haven't kept up much since.) What's the connection between species and linear types?
&gt; How would do you this such that e.g. "race" can be implemented in a completely generic way? I don't believe that can be made safe, nor do I think it is a good goal. With little loss of generality, you can use a `Step`/`Done` monad for your process and check for an early termination signal between `Step`s.
&gt; the Stackage process obligates the package authors to fix build errors as quickly as possible. Do you really mean build errors or rather cabal failures? &gt; if you pin your dependencies to a Stackage LTS, for example, it mostly does solve the Cabal Hell problem Why does it only "mostly" solve the problem, what's missing? Moreover (unless you develop a package not intended for public release) you'd still need to maintain proper version bounds on your build-deps despite locally freezing to stackage versions, as otherwise your package would only be usable for stackage users and effectively sidestep the PVP, wouldn't it? Otoh, if you have proper upper bounds, you don't really need Stackage anymore (except if you want the additional testing that stackage-approved versions come with -- but that's besides addressing the cabal-hell problem), as Hackage is being curated as well by trustees. But I'm still wondering about the relative merits of stackage curation vs hackage curation... &gt; Nobody claims it's the best solution ever and nothing better is possible in the future. I beg to differ, as the current wording on beta.stackage.org does indeed make that very claim...
On top of a pile of commonly known practical issues of laziness, I can't help but find that it could be presented as an abstraction, which itself forms a perfect monad. E.g., newtype Lazy a = Lazy (() -&gt; a) With evident instances for the following: instance Functor Lazy instance Applicative Lazy instance Monad Lazy Just imagine how cool it would be to have yet another essential functionality be implemented in a library with those perfect universal abstractions! As a teaser, here's how the lazy Stream, which we call List, could be implemented: data Stream a = Cons a (Lazy (Stream a)) | Nil Having some auto-memoization optimization in the compiler would result in `() -&gt; a` being computed just once, thus making this abstraction essentially completely reproduce the behaviour of thunks. 
*almost impossible doStuff undefined
The right wording might have been: "the best solution so far"
Probably a good idea to force evaluation of the `MyToken` argument so that the worst that happens is you get an exception (instead of segfaulting or whatever).
An even cooler observation is that this pure stream actually makes just a special case of a strict list monad transformer in that case: newtype ListT m a = ListT (Maybe (a, m (ListT m a))) type Stream a = ListT Lazy a while a strict pure list is over `Identity`: type List a = ListT Identity a 
Market forces work just fine! It's just that the market exerting said forces might not be the one you're looking at.
&gt; separate read-only functions from the potentially overwriting ones You'll notice that `MVar`, `TVar`, `STRef`, etc. all have their read function and their write functions operate in the same context. You can't **really** separate them. The read functions only exists to observe the changes made my the write functions and the reverse. &gt; guarantee precedence relations (e.g. computation can only occur after initialization) Well, you could use an indexed monad or somesuch. But, probably /u/dnaq's technique of just having the previous step generate a token that is required by next step should be fine for most cases. &gt; collect or even compose error codes, letting computation go through only if Err==0 (e.g. a n-ary Maybe ?) Sequencing in the IO monad causes all effectful functions to run and return their result and/or error code. Two approaches: 1. If you are already in IO, check Err after/before each call into the library and throw an exception when Err is not null. You can include the value of Err in the exception value that your throw. 1. Operate in the Either CInt monad, or transform your existing monad with EitherT CInt. It will short-circuit when any result is a `Left`. You'll still check Err often, but seeing it as a non-zero value will have you use `Left` to "throw" the value. &gt; reflect the deallocation of a C-land object on the Hs side. Haskell is garbage collected. You shouldn't expose deallocation directly to the user. Instead use a finalizer to deallocate the object once the GC is done with it. If you do want to provide your users with explicit deallocation functions you'll need to do something special to prevent both use-after-free and double-free. To get static guarantees these don't happen, you'll probably use regions and indexed monads for liveness tracking. You probably won't go that far, and instead detect it an throw and exception / return a default value / terminate the process / initiate user-specific recovery / magically allocate a new object / whatever. In this case, ensuring each allocated object correspond to exactly one reference (STRef, IORef, etc.) and then clearing / emptying / setting to a sentinel value that reference on deallocation should be sufficient. Good Luck. Have you considered just using `inline-c`? ;)
I started using the sandbox recently and it is much better than running without, although it adds some clumsiness. But does it make it possible to uninstall packages? As I understood it I still need to nuke everything and reinstall the sandbox if I want to remove some package. And installing can take quite a long time.
&gt; he didn't mention haskellngPackages because it has been already merged to the master branch as We don't need to use `haskellngPackages` anymore ? `haskellPackages` and `haskellngPackages` are now synonyms ?
Yeah, I think the `IO` argument is a bit weak. In my Python Effect library, I can (re-)use Effect objects in exactly the same way, even though Python is strict. Values of type IO (or Effect) are just data *describing* actions; that's a different issue entirely from laziness.
I meant that you don't need that much stuff for reliable code, just to make it idiomatic and maintainable and to be able to work on other people's code :)
Something I still don't understand about nix and haskell: how do we deal (as a user) with the fact that many packages keep breaking if you follow the master branch of https://github.com/NixOS/nixpkgs. Because Nix always tries to use the latest from hackage, it is sometimes perfectly legitimate for a package to break on hydra. For instance, [servant-client has been failing for days](http://hydra.cryp.to/build/875906/nixlog/1/). With the upperbound defined in its cabal file, the package does compile just fine. You cannot reasonably expect maintainers to make their lib working with all the latest versions from hackage, can you ? What kind of strategy works at that point ? 
&gt; I'd like to see it done with the input read in from stdin, to make sure neither compiler is optimising parts of the code out at compile-time. I believe he was asking someone to write a better benchmark. **Edit:** which /u/Tekmo did below, and received an excited positive response.
now if correctly understood peter, you could create separate set of packages that are working with servant project. ghcXXX sets are there just to give a best shot at getting more stable hackage. you could ofcourse also build nix expressions for servant yourself (using cabal2nix) and then have them built with your own hydra.
That would be really good. Note that there is an issue already about that on github. But what do you think about the "double throw" issue discussed in other thread? ( http://www.reddit.com/r/haskell/comments/36tjca/neil_mitchells_haskell_blog_handling_controlc_in/crhc9pu ) Do you consider it as a bug in asyn exceptions semantics? Can we fix it?
I'd say make the state a record containing the two maps, with descriptive names. data Foo = Foo { bar :: Map a b, baz :: Map c d } type MyState a = State Foo a 
Isn't this the price you pay for following master?
*Disclaimer: the following pedantic mini-rant should be understood as aimed toward the wider cosmos; not at the individual to whom I am replying, who has done nothing to deserve a bunch of hot air blown in his face.* You're using the politician's definition, which I agree is based on fantasy and wishful thinking. But scrape off a few layers of misdirection and willful ignorance, and the concept of market forces is identical to that of natural selection, which boils down to tautology: that which is not eliminated, continues. The easy and common fallacy when looking at any selective system is to classify things as either "agent" or "environment," when in the total analysis nearly everything acts as both in countless systems at once. Supposed agents can apply pressure to an environment which acts upon them, creating a completely different dynamic than naive prediction would suggest. Mankind pumps carbon into the atmosphere; corporations buy the loyalty of congressmen; the environment changes, invalidating assumptions based on the previous environment, but the underlying mechanism still applies. That's what I was getting at above. Market forces are as absolute as 1 + 1 = 2, and writing them off is simply wrong. If they're not working the way you expect, it's because you're looking at the wrong market.
http://cryp.to/nixos-meetup-3-slides.pdf
(Posting a new reply thread because it might be a tangent...) Java has an interesting answer to the "raising-an-exception-during-exception-handling" issue: Every exception (which must be derived from java.lang.Exception) has a list of *suppressed exceptions* which is initially empty. If an exception is thrown in a "finally" clause[1], that exception is added to the "suppressed exceptions" list and the original exception is re-thrown. In practice this seems sufficient and relatively sane given the non-composability of exceptions+cleanup in general. [1] EDIT: Actually, that's not true. It's "in the cleanup phase of a try-with-resources", but that's getting a bit too Java-centric. Still, I hope the point gets across.
No, AFAIK it was not implemented. (I was against it btw.)
Shame. (But this is not "the time to bicker and argue about who killed who".)
&gt; Haskell is not very popular yet As others pointed out: never it has been more popular. And in language geek communities it is even quite a well established name nowadays. &gt; Do you think Haskell can be a big deal in the future or not? I think it is already. See for instance [this graph](http://brendangriffen.com/blog/gow-programming-languages), it's a very influential language. It might that not Haskell, but one of the languages influenced by Haskell becomes the "big deal" (or at least a bigger deal then Haskell ever gets to be). In the cast a post-Haskell language becomes the must-bigger-then-Haskell-deal (which is quite likely), that community will most likely use Haskell as an important starting point for looking at reasonably mature libraries in a similar paradigm. This all stems from the fact that not many languages are in the same prog. lang. paradigm intersection as Haskell. In contrast look at OOP/imperative -- a very well explored design space with very little recent innovations. &gt; Does OOP offer more possibilities? Possibilities for what? &gt; Or will they both coexist just as they do now? Sure... Look at COBOL and C; still happily coexisting along with the others. &gt; I just wonder if there clearly is a "winner" between OOP and FP. For a winner you need to define a game. When you get to choose a technology for a project you work on, then after some time you can get the impression that X-paradigm or Y-language was a winning choice. By what criteria you want to define the winner? You only mention "popularity".... 
Why not add some OO features to haskell? Purescript has row types and that's enough apparently. What would it cost to add row types to haskell? I don't know. Ghc is a scary place. Would probably need a gsoc to get it done.
It might. On the other hand, if you don't, you can't have `haskellngPackages` and I believe you can't have ghc-7.10 either. 
You can by following unstable. IIRC, most haskell packages on there at the very least compile or are marked as broken
It might be interesting to step back one more step and consider what people actually use stack traces for and how one might be able to fulfil their needs with something else. 
Yum!
You can also use the `at` lens: ins = do sMapX . at 5 .= Just "five" sMapY . at 7 .= Just "seven" sMapY . at 5 .= Nothing -- Delete a key
Well, how it would work should follow in many ways from thinking about a "list of successes parser" as newtype Parser a = Parser { runParser :: String -&gt; [(a, String)] } being StateT String [] and the way that instance (MonadFix m) =&gt; MonadFix (StateT s m) where mfix f = StateT $ \ s -&gt; mfix $ \ ~(a, _) -&gt; runStateT (f a) s indicates your fixed point of a parser wouldn't consume input and instance MonadFix [] where mfix f = case fix (f . head) of [] -&gt; [] (x:_) -&gt; x : mfix (tail . f) indicates how we take the fixed point of a list. You can modify the code for `lookAhead` to make it work. The trick is it gets a bit more complicated in the presence of monadic streams and I'm not sure how you're going to get that.
IIRC, there's a way to do this using lens, but I don't know what it is (something about `zoom`, maybe): Suppose I have a computation that uses `State Int` and another that uses `State Bool` (or also, a different `State Int`). I would like to run them both in an overarching computation with with `State (Int,Bool)`. How do I do this conveniently?
Sorry I have a habit of not checking replies to my comments. Anyway the [nix pills series](http://lethalman.blogspot.com/2014/07/nix-pill-1-why-you-should-give-it-try.html) is the most in-depth tutorial I've seen. It might actually be too in-depth, as I haven't quite made it through yet. 
Chucklefish is a fairly well-established studio, though, and they've got other projects showing activity. And from what I've been told (by his boss iirc) palf was never a redditor to begin with; he just got assigned some thread-watching duty while they were expecting active questions. No news isn't great, but in this case I don't think it's a sign of abandonment.
I remember reading about a module which contains lifted versions of all the functions already ... does anyone remember the name off-hand?
yes. In `all-packages.nix` at the very end it now has the lines: haskell-ng = haskell; # 2015-04-19 haskellngPackages = haskellPackages; # 2015-04-19 
&gt; Do you ever write a where clause with a set of bindings without worrying about which ones are actually going to be needed? Can't it be easily derived by the compiler about which clauses are never used and them get thrown away? &gt; Ever use a list where you would use an iterator in a strict language? I would much rather go for an iterator, since that would specify my intent more clearly. &gt; Do you ever use a top-level definition of type Map k v, or something else that needs to be computed when it is first used? Nothing's stopping us from having an explicit "lazy" construct in a strict language, even [as a library](http://www.reddit.com/r/haskell/comments/36s0ii/how_do_we_all_feel_about_laziness/crhr2w4).
&gt; Having some auto-memoization optimization in the compiler would result in () -&gt; a being computed just once, thus making this abstraction essentially completely reproduce the behaviour of thunks. And if that's not an option, we can have an explicit implementation of a thunk: newtype Thunk a = Thunk (IORef (Either (() -&gt; a) a)) thunk :: (() -&gt; a) -&gt; Thunk a thunk f = Thunk $ unsafePerformIO $ newIORef (Left f) unthunk :: Thunk a -&gt; a unthunk (Thunk ref) = -- Synchronisation is not an issue here, -- since there's nothing dangerous in two threads occasionally computing the same result. -- That would be the price of not having to pay for locks-keeping overhead. unsafePerformIO $ readIORef ref &gt;&gt;= \case Left f -&gt; do let a = f () writeIORef ref (Right a) return a Right evaluated -&gt; return evaluated 
The main benefits that I get from an IDE are (roughly in order of importance to me): - Removal of the edit-save-compile cycle: errors are highlighted along the way - Instant API discoverability: listing all functions that operate on a given value, with documentation - Instant navigation: jump to implementation of function, declaration of variable, etc. - Refactoring tools: moving and renaming functional units, extracting interfaces, etc. - Finding all usages of a function or type - Running/debugging/profiling code without changing context As far as boilerplate, I occasionally use my IDE's project templates, such as standard folder layouts for web applications, but that's about it. I used to think a web browser on one screen, vim in another, and a couple of terminals was a good enough solution. You can do lots of work that way. But you can understand and manipulate a project much faster with an IDE than without, and now that I've used one for 40+ hours a week over 4+ years, I'd much rather have one.
&gt; Can't it be easily derived by the compiler about which clauses are never used and them get thrown away? Sometimes, but not in general. Whether a binding is used is a runtime property. &gt; Nothing's stopping us from having an explicit "lazy" construct in a strict language, even as a library. Sure, but (a) laziness becomes more cumbersome, and (b) you can't exploit the properties of non-strict semantics in the compiler or when reasoning about your code. That's a tradeoff you might want to make, but you do lose something.
Yes. I really just like that first part. &gt; I also consider OO to be a clear loser even without any competition Amen.
Ok, but you started this thread with an assertion that something I've spent a lot of time thinking about is wrong and should be removed from GHC. I was also mentally spitting expletives :) The alternative to async exceptions is some kind of polling. That requires your code to be in the IO monad often enough, which is incomplete (we need to interrupt pure code), fragile (we might forget to add polling somewhere), and inconvenient (I want async exceptions to be thrown in third-party library code without having to go into the library and add polling checks). I have a large system where it would be very inconvenient, error-prone, and probably slow, to add polling, and async exceptions just work.
&gt; bool' a b c = bool a b c () I can just imagine the layers of `Deferred` for when you don't want your result to be forced by `bool`...
That's however somehow what has to happen. Fortunately, the Stackage and Nix Haskell folks are very useful since they report any failure before it's too big a problem. In the case of *servant* we've had a very busy week, I'm hoping to fix that this weekend.
Well, in that particular case you'd simply use the standard `bool`, which you can consider as having the following signature, when you use it on `Deferred a`: bool :: Deferred a -&gt; Deferred a -&gt; Bool -&gt; Deferred a Generally though, yes, there'll be cases where you'll have to wrap things in deferred manually. But it's not boilerplate, but an explicit control over evaluation. Also don't forget about the power of abstraction! Both `Thunk` and `Deferred` are monads, which means that you can compose, while keeping abstracted away from the fuss of wrapping/unwrapping.
Lazy evaluation is one of the big drivers behind keeping Haskell pure. Side effects _have_ to be restricted to the magic `main =`. 
I hadn't seen your take on this b. Very nice! I'm glad I didn't cause I wouldn't have gone through the exercise. Have you proceeded with trying to get it into the bigger todoMVC repo? 
Isn't that very similar to the strict and imperative argument against keeping effects separate with the `IO`-like monads? I keep running into cases just recently where it would be convenient if those read-only accesses (via the FFI) weren't in `IO`, but they have to be because (1) they're sensitive to effects so ordering relative to effects is important, and (2) they're based on FFI actions which are in `IO` anyway. Forgive the IO-Monad 101, but in an imperative/strict language I wouldn't care because the effect ordering is the ~~execution~~ what-you-see ordering anyway, which is e.g. how an imperative `while` loop can have a condition that reacts to mutations of variables in the body of the loop. The `while` condition *is* an action, but in an imperative language doesn't need to say so. For example, the [Haskell while loop here](http://www.xoltar.org/old_site/2003//sep/09/haskellLoops.html)... while test action = do val &lt;- test if val then do {action;while test action} else return () The `test` is an "action", not a simple boolean value. That isn't as convenient as in imperative languages. Small price to pay etc, but the point is that it's not as if choosing lazy-and-pure over strict-and-impure has no downside. Then again, as SPJ often says, lazy evaluation kept Haskell honest. If it didn't have non-strict evaluation, it probably wouldn't have stuck to its pure functional principles through the tough times anyway. So if it wasn't for non-strict evaluation, we wouldn't have the rest of Haskell. **EDIT** Of course efforts to be non-strict are also why we have the impure evil of lazy IO. 
I use it in production a lot. Works great. But didn't thought on the case you're describing here.
The distribution part of this reminds of Java's now out of fashion Network Classloaders. You would out all your .class files on some web server and the network classloader world load each one as it reached the appropriate import statement. This is why import statement look like urls.
I think twistier may have meant that Applicative would be super annoying without laziness by default.
Only a single value needs to be read in from stdin, and with a large problem (that takes &gt; 1 second), the time spend reading from stdin would be insignificant.
But you need both, and can have both in a strict language, see for example C++ range-v3. It has composable views (lazy) and actions (eager). Each action forces a "data traversal". 
&gt; I would love to see that in an actual language http://www.cs.cornell.edu/courses/cs3110/2011sp/lectures/lec24-streams/streams.htm
&gt; You would have to turn almost everything into these dummy functions for those things to keep working. Not everything, but, yes, where required, I'd have to. However for that price I'll get an explicit reflection of the evaluation order in types, thus knowing for certain (or declaring) where and when things get evaluated, instead of deducing (often incorrectly) the way I have to do now. From what I've seen, the absolute majority of leaks, stem from incorrect assumptions about evaluation order. Just take a look at any post concerning leaks in Haskell. And leaks are a major pain, often taking days to resolve. We, Haskellers, of all people should know what a difference a type-checker makes in terms of safety. That's why I'd gladly pay the price for reflecting the evaluation order in type-system, because it would make the mentioned category of bugs perish.
yeah for that you need mutable state. here's an example in ocaml - https://realworldocaml.org/v1/en/html/imperative-programming-1.html#idm181616515952
Who is still suffering cabal hell? Now that we have sandboxes and --allow-newer, I haven't really ever struggled. What problems are people having?
This is great! I've read about Elm and watched a few presentations. I find a lot of concepts in the language very well thought thru. It's all the more satisfying to find out that I'm sharing the views on the laziness problem with the authors! If you're Evan, the author, then a high-five to you, and thanks for the excellent research you're doing there!
Amusing read! Thank you. The past week has shown that the camp of people who are frustrated with the laziness is actually huge in numbers. We might want to do a poll.
Yep, I'm the author, and thank you! :D Though Elm has academic roots and we do stuff like [this](http://elm-lang.org/blog/Introducing-Elm-Reactor.elm), Elm is not a research project: the goal is to make something so useful and accessible that typed FP can finally go mainstream. I'm hoping projects like [elm-html](http://elm-lang.org/blog/Blazing-Fast-Html.elm) will help make it clear that we can do real stuff and do it real fast!
It still takes &gt; 10 minutes to install scotty in a sandbox, and a lot more for a complex stack like yesod. Sandboxes are a band aid on more fundamental problems that needs to be fixed - binary downloads, and multiple concurrently installed package versions. I also strongly believe that package management and build tools should be separate concerns, not a single monolithic app. 
[Yep](https://github.com/chpatrick/codec)! I'll put it on Hackage soon.
I hope the results do not shed new light for anyone here. We know that package management is abysmal. We know that the documentation is abysmal. This is all very well documented. It does't help that, often, reactions to calls for help in these areas are along the lines of "well then why don't you fix it for us". Many people use the tools but don't know how to modify them (like me). I wonder what the goals of the Haskell community are with respective to the development effort spent. I saw a lot of work in abstracting type signatures recently. The polarization of this work aside, I'm sure it served as an improvement to the language for a number of Haskell users. A lot of work of great complexity went (and goes) into improving the language for those users who already know a lot about it. And yet, I see relatively little work go into the tooling that empowers new entrants to the language. For example, Haskell's "package manager" probably takes last place compared to those for other languages. Cabal Hell is just a sad state of affairs. Sandboxing is not so much a solution as a sidestepping of the issue. Why this difference in attention? Could it be because playing with the language is so much more interesting than trying to fix the plumbing? Possibly. Who knows. This all shows as (dare I say it) lack of robustness, maturity and user friendliness of the language. Which is especially sad because Haskell and GHC is such an incredible piece of technology. 
While the idea of strictness-polymorphic datatypes is [not new](http://h2.jaguarpaw.co.uk/posts/strictness-in-types/), it's the first time I see it expressed with such a nice API, as a monad transformer. I believe it should be possible to obtain the same nice API in ordinary Haskell: -- Note that this cannot be a newtype or the !(m a) below will collapse to !a -- and we will be strict instead of lazy. data Lazy a = Lazy { runLazy :: a } data Strict a = Strict { runStrict :: !a } data ListT m a = Cons !(m a) !(m (ListT m a)) | Nil type Stream a = ListT Lazy a type List a = ListT Strict a Full implementation [here](https://gist.github.com/gelisam/afb66de5d61d1c4815d7). It's much easier to reason about implementing laziness in a strict language than the other way around, so it's quite possible that I made a fatal mistake somewhere in there. At the very least, for my test computation (the monadic equivalent of `fmap (+1) [1..4]`), traces show that the strict version evaluates the entire list before mapping any of its elements, while the lazy version alternates between creating the next element of the input list and mapping one of its elements.
Maybe the results are not surprising but with data showing where problems are it could be easier to allocate resources.
The benefit I (not an expert) see from row types is straightforward FFI with OO libraries in a language like JavaScript or C++. It is practical to enable interoperability with classes, methods and inheritance for the purpose of FFI while in the same breath discouraging the practice in FP code. The reason I still use Python, C and C++ a lot is plenty of awesome libraries that I can't avoid. Something to learn from Microsoft is embrace, extend, extinguish.
Tom has actually [already referred here](http://www.reddit.com/r/haskell/comments/36ysso/if_haskell_were_strict_what_would_the_laziness_be/cricrna) to the post you mention. And I thought about the same thing as you, but then I read the "Wadler’s “strictness monad”" section of his post.
I have particular reasoning for answering that way. My company mostly deals in scientific software, and haskell just can't compete with python on that front. There are more flexible and mature libraries like numpy, scipy, pandas, matplotlib, and a host of others. Python also has a superior ability to interop with other libraries, not just C DLLs but COM, CLI, .NET, and even Matlab. In particular I looked into writing haskell bindings for a library we use and got stuck when I needed a variable number of arguments, which currently isn't possible with the FFI (although the inline-c library could help with this). In order to switch from python to haskell we would have to rewrite thousands of lines of our code, and thousands of lines of libraries and bindings that already exist for python. GUI development is very important for us too, and I can't say I've had much success getting that to work well on Windows with haskell. My coworkers also aren't programmers by trade, they're electrical engineers. Coding come second to them, and it was hard enough selling them on python over c and TCL. Maybe in a few years when my team morphs into more of a software team than an engineering one (we're headed that direction), but for now the combination of the community support, legacy code, technical requirements, and barrier of entry prevents me from even trying to convince my coworkers to learn haskell. 
Also, if you need to work with an older version of a package it is quite a lot of work to import all the older versions that it needs via cabal2nix, and then the older versions that those need and so forth. It is a lot of manual labour. Then if you put that package A (which depends on C) together with a package B that uses the newer version of C=C' from master, they are in conflict so you have to go and and change B to use C instead... Basically it would be nice if you could generate a complete set of package descriptions referring to cabal packages with specific versions calculated such that they all get along, based on version constraints from cabal files, like cabal configure does, but in nix. Then you could install that as a Haskell development environment :-)
&gt; Elm is not a research project: the goal is to make something so useful and accessible that typed FP can finally go mainstream I mean this constructively: but I think the focus needs to be more on docs and community then. I gave up on my attempts at playing with elm because I couldn't figure out how to do basic stuff I do in Haskell just fine, and I idled in IRC for a week without seeing a single question answered.
This is part 1 of two, covering the basics: constant types, functions, and polymorphism (over types of kind *). In part 2, to be published soon, we will deal type constructors, type classes, polymorphism over type constructors and type constructor classes. 
GUI development on windows has been horrible. hsqml and fltkhs has actually made it somewhat workable now. I have spent many hours trying to install gtk and wx before giving up. I even tried the win32 lib, unfortunately some of the controls I wanted were not implemented for some reason.
Perhaps you simply don't have `&gt;&gt;` and just use `do`, where: `do x; y` desugars to `x &gt;&gt;= \_ -&gt; y` That avoids situations like `do x; error ":("` throwing an exception before `x` or `&gt;&gt;=` has been evaluated. 
Re-reading Tom's article after reading yours, that section caught my attention as well. I don't know which combinations Tom tried at the time (my contribution to his article was limited to the datatypes part), but here's one combination which seems to work just fine: foldlM :: Monad m =&gt; (a -&gt; b -&gt; m a) -&gt; a -&gt; [b] -&gt; m a foldlM _ x [] = return x foldlM f x (y:ys) = do x' &lt;- f x y foldlM f x' ys f x y = return (x + y) Tracing `runLazy (foldlM f 0 [1..4])` and `runStrict (foldlM f 0 [1..4])` in ghci, I can see that `x'` is still a thunk during the recursive call in the lazy case, but is already evaluated to 1 in the strict case.
Interesting, good to know.
I'm in almost the exact same boat. It's pretty frustrating. We could get pretty far if there was cheap and smooth Python interop then I could at least try using Haskell for some backend processing. Forget the GUI though. Haskell is miles and miles behind on that front.
Actually, my biggest pain point is long compile times. Cabal issues I can usually work past - it burns a few minutes every so often, and a few hours much more rarely. Compile times burn minutes constantly. I still find it very much worthwhile to work in the language, but if I was going to change one thing...
&gt; It really does seem like that is best left to the OS package manager. Sadly, if we’re talking about real world applications in commercial environments, that policy alone is probably enough to lose much of the Windows developer community. The standard for developers working on Windows is set by Microsoft’s development tools. These are well-designed, well-documented, comprehensive, reliable, fast, and ubiquitous. If the equivalent for writing a new project in Haskell starts with figuring out how to install a year-old version of a compiler so an unfamiliar set of build tools can compile a simple binding to a not-very-good GUI library from source in less than an hour on a quad-Xeon workstation, then of course most development teams working in commercial environments aren’t going to consider that a serious proposition. And this *is* all too often the kind of first impression that developers used to working on Windows will form of the Haskell ecosystem today. (Edit: Haskell Platform does help with this to an extent, of course, but the issues with things like tools and documentation widely discussed elsewhere are still very relevant here.) None of those issues really have much to do with Haskell-the-language, but the gap between academic/personal use and widespread industrial use is much more about tools, libraries, and non-technical issues like training materials and ready availability of skilled developers than it is about the specific language chosen.
Yeah, we use wxpython mostly, but we're starting to hit its limitations and might be considering other frameworks soon. 
Allow me to peddle my wares: What do you think of [Threepenny-GUI][1]? It's as cheap as the name says, but at least it works on every machine out of the box. [1]: https://wiki.haskell.org/Threepenny-gui
The survey did not ask specifically about laziness, which would be interesting, but on first inspection of the open comments I didn't see much mention of it in the data. On what are you basing that hypothesis, please?
What a great post. Thanks for concretizing these terms for those of us who are not studying type theory day to day
&gt; you can not abstract away the effect state has on the behaviour of a program very well as long as you expose any hint of the fact that the state changed. You can hide the details of how you maintain that state, but yes hiding the fact that the state exists (and what operations cause or are sensitive to chages in it) does seem to have become a rather rotten failure.
another option for variadic c functions is libffi (see for example http://stackoverflow.com/a/5993358/1940209)
I was using `Control.Concurrent.Chan` to implement section 4.4 of [my thesis](http://elm-lang.org/papers/concurrent-frp.pdf) in Haskell. With the basic library, everything goes through, no computations occur, everything synchronizes at the end. This means putting computations on different threads did not actually say anything about when those computations happen, which was the whole point. This was not a viable path for me. I then looked into `Control.Concurrent.Chan.Strict` but that relies on on `deepseq` which does a full crawl over a data structure. Say you are passing a large record around and it goes through a couple channels. It has a bunch of fields, holding long lists or large dictionaries. With deepseq you are crawling the entire data structure on every channel for no reason. This turns `send` from O(1) to O(N) in the size of the value being sent across. Neither option here was viable as far as I could tell. Maybe I could have tried harder and figured out some other tricks, but I could also use a strict language and not have these problems.
Between HaPy and MissingPy it doesn't seem like there should be any problems. Could you expand on the difficulties you've faced interoperating between Haskell and Python?
My impression is that a lot of inexperienced programmers don't actually read error messages, so that might be less of an issue than it would've been :P (But having better error messages would be really neat IMO)
i'm experimenting with yesod+javascript web front end. should be able to take advantage of d3 and other js stuff that way. Web seems to be the way things are going anyway (I say this as someone who's done desktop work with Qt/C++). 
Stuff like Backpack/proper API compatibility checking sounds exciting, but wouldn't that make Stackage superfluous? What would Stackage be needed for if we had the tooling to find compatible packages based on their APIs rather than error-prone manually written version bounds? IMO, Stackage is the poor man's substitute for something like Backpack...
Anyone brand new to the language. I struggled mightily with cabal hell trying to install yesod until I realized I had two different GHCs on my Mac and the older version's global packages (alex, happy) were symlinked into my PATH. Once I fixed that issue, then of course it took two hours to compile yesod. I got through it and now I'm happily building a web app with yesod (which is a freaking amazingly well thought-out framework) but man, those first couple of days were rough. I almost gave up and went crawling back to ruby on rails... Glad I didn't though.
Slowness is annoying, but I think calling it"hell" is a bit much. I hope people know about cabal install -j. Even with that I agree that a Yesod application can take an extraordinary amount of time. There are some solutions to this. I've used Nix which will provide binary installs. Halcyon is supposed to re-use and share sandbox but I haven't tried it. I think another alternative may be system-wide installs using a stackage config. 
I would certainly use Haskell in my workplace. I certainly would not want to work with my current coworkers on a Haskell project. The issue is they are at the start of the curve of actually understanding FP, and we have real deadlines to meet in a short amount of time. On another note, I would love to move to a group where my coworkers actually understand and want to use FP.
I think it's high, personally. There's a lot of practical concerns that work against Haskell in the workplace that go beyond technical issues. Take me, for example. I love Haskell, but I wouldn't recommend it for a project at my job. Why? Because I'm the only person at the office who actually uses it. Unless I'm the only person who will ever work the project there will have to be training to get others up to speed, and nobody is going to want to foot the bill for that. Haskell is also not a language where we can take a dev with no experience in it and go "It's like Java, you'll figure it out" like we can with every mainstream language out there. I have enough trouble getting my friends with a passing interest in Haskell to actually sit down with it long enough to get past the initial hurdle of "This isn't programming the way I've been programming for the last X years". I've had to answer "What's a monad?" so many times it's infuriating. And then there's the nature of our work. We very often do work for clients with the understanding that after the contract ends they'll be taking the code and doing continuing development with an internal team. It's a hard sell to say "I know you're mostly a Java shop, but what if we wrote this in a language that likely no one in your company knows?" None of this even touches on the qualities of Haskell the language or the state of the libraries and tools, but it's still important to consider as a business. There's zero technical reasons why my current project couldn't be written in Haskell. I still would have been crazy to suggest it.
I only have a small amount of experience with d3, could it handle quickly graphing about 16000 points spread across 8 or so graphs at once? And then repeat that 8-16 times? I'd say my upper bound is a total of 320,000 points graphed at once. I'm not sure SVG was really meant for that sort of thing.
This would be mine, too. The talk of cabal hell and sandboxes being a hack sounds more like outsider commentary based on what people have heard over the years. If cabal just used sandboxes by default, the complaint would simply be that builds are slow. We know how to avoid rebuilds, but can't agree on how to fix it, and GHC itself is slow, but efforts at improving it are underway. While I am no fan of cabal, it is soaking up a long history of issues here caused by, e.g., the fine-grained package division the community tends towards, and a dominating desire for runtime performance. The survey just reminds us that there is pain.
I'm working on WinForms bindings at the moment that should work with Mono everywhere. Hopefully this makes the situation better (if I do end up finishing them).
I have been interested in learning your banana library some day. The thing with web based desktop gui is that it feels like a cop out. Both Brackets and Atom is somewhat sluggish compared to other editors. During work we use wpf and .NET and would probably be able to whip a working prototype of most of my project in a day. When using haskell I would be happy if I had a gui library installed. I am thinking about building a simple file manager that supports tagging of files, lots of them. So the gui need to have a table capable of handling thousands of them. On most cases this means some kind of virtual table.
Do you find the small "make a change, check if compiles" iterations the most painful, or the bigger ones that require a full "cabal install"?
Your comments seem very representative of the textual comments received from large numbers of survey respondents. If we want Haskell to move forward, we need to address this team learning issue and this future staffing issue, both of which appeared as prominent concerns in the survey results.
Here's how laziness works in Idris: http://docs.idris-lang.org/en/latest/tutorial/typesfuns.html?highlight=laziness#laziness
awesome :)
Based on the new survey of user needs, it appears that Stackage and sandboxes are two promising solutions that are already helping a lot.
For me, the latter. I've gotten iteration down to a matter of a second or two even on 20k projects, via GHCi. It's building new projects in a sandbox that kills me. Although with Stackage more recently I have one package database on an LTS snapshot that I re-use among 20 projects and then upgrade, so that pain point is mitigated.
Just for your info, I too do commercial scientific GUI development, and I absolutely love PyQt. I also use vispy and pyqtgraph for OpenGL-based plotting (2D and 3D). I would love to use Haskell for my work, but it would be very hard to make a compelling argument as to why I should drop my current Python GUI stack and the general Python scientific stack.
If there's any issues with [fltkhs](http://github.com/deech/fltkhs) on Windows please let me know here or open an issue. Having a GUI toolkit that works well on Windows was one of the reasons I wrote the bindings.
Will do :) Thank you for the hard work!
It's hard to recommend Haskell at the workplace when you know that it will take your colleagues unfamiliar with it a lot of time and effort to start being productive with it. It is super hard to learn, unfortunately. Compare it to Python or Go where everyone can become productive in a very short time.
Tangent: (A :+: B) &lt;=&gt; (B :+: A) They may not be *equal* but they are *equivalent*; it's trivial to write the isomorphism: swap :: Either l r -&gt; Either r l
Same here on switching, it just doesn't make good business sense. As for Qt, we've looked at it before but the licensing is a bit restrictive for us. We may take a look at it again and see what we can work out with legal. Wx is perfectly fine for simple interfaces, even with some graphs thrown up there, but we've found that it has sub-par performance and the more complex the gui the harder it is to do things that seem like they should be easy. Out of curiosity, what industry are you in? It's always nice to come across others doing similar work, everyone seems to be in web or enterprise these days. 
Using dependent types, all these cases can be understood as the expression of a single phenomenon (which can be summarised in just five equations). See: http://www.cse.chalmers.se/~bernardy/ParDep/pardep.pdf (top right of page 3). 
I would assume some % of those who would not use it are tied to the JVM and beyond reach. Out of thin air I'm going with 10%. If we don't take them into account the % of those who *can* switch and would want to goes up to ~ 70%.
Exactly...this is a particular problem when testing and you can't tell which test cases behave in identical ways anyway and which might differ. This means you can not easily get full branch coverage. The same problem also occurs when debugging of course, when you have to determine which exact state matters to reproducing the problem. And of course in a similar way in reverse, when writing code, you can't easily see which branch you didn't write at all because that particular combination of state didn't occur to you and it was not obvious that it could even occur.
I feel the same way about seeing non-web programmers haha. I'm in the oil and gas industry. 
&gt; Removal of the edit-save-compile cycle: errors are highlighted along the way This is available today in Haskell with e.g. tools like ghc-mod and their vim and emacs integration. &gt; Instant API discoverability: listing all functions that operate on a given value, with documentation Only really applicable in the way you are probably thinking about to single dispatch languages but tools like Hoogle which allow you to search for functions of a given type fulfill a similar purpose. &gt; Instant navigation: jump to implementation of function, declaration of variable, etc. Available in Emacs/vim/... as well for many more languages than your typical IDE supports. &gt; Refactoring tools: moving and renaming functional units, extracting interfaces, etc. Besides [HaRe](https://wiki.haskell.org/HaRe) which isn't currently usable I am not aware of any projects offering this kind of automated refactoring. On the other hand equational reasoning makes this kind of refactoring a lot simpler in Haskell (you can always factor out any duplicate code in Haskell, there is no worry about now executing a side-effect only once instead of twice). HLint includes code (and editor support with syntastic in vim and similar tools elsewhere) for duplicate code detection beyond a certain length. &gt; Finding all usages of a function or type Not really necessary in my experience other than when you change it...in this case you can rely on the compiler to tell you where you need to make changes. &gt; Running/debugging/profiling code without changing context Many editors like vim and emacs do support running REPLs or even full compile cycles from inside the editor now. I used to think an IDE was good enough but quite frankly none of them even include a decent text editor component which is why I used emacs and in recent years vim.
I dearly love focusing, but it doesn't scale up to the arguments that parametricity can handle. Basically, the kinds of arguments you sketch above rely upon the subformula property, which doesn't hold once you have induction (or impredicative f-style quantification). A nice example is to prove that every type X is isomorphic to forall a. (X -&gt; a) -&gt; a. This isn't even grammatical for focusing, since if X is positive then it asks you to equate a positive and negative type. In general my heuristic is to try focusing first, and if that doesn't work try parametricity. (I've never needed a step three.)
... with the comfort of automatic insertion of forcing and deferring instructions by the compiler. Pretty neat.
We could have `~` to mark things as lazy, just like `!` works today.
 type Either a b = forall r. (a -&gt; r) -&gt; (b -&gt; r) -&gt; r either :: (a -&gt; r) -&gt; (b -&gt; r) -&gt; Either a b -&gt; r either f g e = e f g left :: a -&gt; Either a b left a f _ = f a right :: b -&gt; Either a b right b _ g = g b Okay, I can see how this isn't necessarily a "tagged union". I could also try to argue that it *is* a tagged union where the "tag" can be calculated via: eitherTag :: Either a b -&gt; String eitherTag = either (const "left") (const "right") I have to adopt some rather abstract definitions for "tag" and "union" to have that view make sense, though. Note that `eitherTag` works regardless of implementation, as long as the `either` combinator does the right thing. -- imaginary Nullable syntax data Either a b = Either (Nullable a) (Nullable b) either f g (Either a b) = if isNull b then f a else g b 
Folks will also be interested in this awesome resource on HM by Ian Grant. http://steshaw.org/hm/
Could similar technique be used to for boxed/unboxed? Parametricity?
I feel as though cabal (actually, many command line tools) could take some cues from darcs on being interactive. Being prompted with a potential fix could really assist with discoverability. Instead of: Dependency tree exhaustively searched. Something like: Couldn't resolve dependencies. You might want to try one of these: --allow-newer=parsec,yesod --allow-newer=attoparsec I don't mean to make light of the amount of work required to make all of these commands friendly in this way though. Also, there needs to be a way to roll back (perhaps automatically) if/when playing with these flags breaks everything.
The phrasing of the post is indeed lacking. FWIW, the standard presentation of parametricity goes more like: First, consider: what does it *mean* for a term to have a particular type? We'll answer this by defining a mapping, `ℛ`, from (the syntactic expressions naming) types to sets of terms with that type. Then we will prove various properties about that mapping; e.g., if we can prove `∅ ⊢ e : t`, then if follows that `e ∈ ℛ(t)`. That is, if we can syntactically prove that a closed term `e` has type `t`, then we can prove that `e` behaves like something of type `t` should. The above approach is called "logical predicates" and gives a way of relating the syntax (`Γ ⊢ e : t`) to the semantics (`e ∈ ℛ(t)`). For parametricity we will generalize this approach to "logical relations". That is, rather than having `ℛ` map types to sets of terms, we will instead have `ℛ` map types to relations on terms (viz., sets of pairs of terms). Thus, the abstraction theorem (aka parametricity theorem) would be more standardly stated as "if `∅ ⊢ e : t`, then `(e,e) ∈ ℛ(t)`". Note that this is just the 'obvious' generalization of the usual theorem for logical predicates. Does that help?
[here](https://www.fpcomplete.com/user/bartosz/understanding-algebras) is a good one imo
&gt;Most packages can't be reused, if they could we wouldn't need sandboxes Huh? If I install the same version of the same package in two sandboxes, my computer is doing something twice that only needs to be done once. Right? That's what I was talking about. The compilation would be done only once if it were in the global package database (or if the same sandbox were used for two codebases), but the only way to enjoy that efficiency is to tolerate cabal hell proper. I literally don't understand what you're saying here. Please do explain. &gt; And that still isn't anything remotely like cabal hell No, it's a workaround that eliminates "cabal hell" but at the expense of redundant recompilation.
&gt; But if that were the case, you wouldn't need the sandboxes. But you always need the sandboxes because you never know if you'll want to depend on something *else* in one of the projects that will introduce the need for sandboxes. If you choose to say, "OK, they use the same dependencies, I don't need a sandbox!" you are taking a *risk*, namely that you will end up in cabal hell some time in the future when you add more dependencies to your projects.
Possibly, but it still ends up being that everyone on my team needs to be familiar with Haskell, HTML5/JS/CSS, and that we deploy a lot of our software (not all, but the most important ones) on machines that will ever only have IE on it, and usually it's an older version of IE. It wasn't too long ago (within the last year) that our IT upgraded from IE 8 to IE 10.
&gt; And everyone explicitly says "don't use platform". Because it's very out of date. A binary distribution based on Stackage would be fine. &gt; Because they don't have the same dependencies. Wouldn't Stackage resolve this? &gt; If they had the same dependencies, then you wouldn't need a sandbox. There is still no reason I shouldn't be able to compile multiple versions of the same package into a global cache. Sandboxes shouldn't be needed for this. &gt; I did not suggest anything of the sort. I said it is a big job, and then responded to the misconception that two platforms account for 99% of Haskell users. I'd be extremely surprised if more than 1% of developers are using non x64 workstations. I'm not even sure you can buy a non x64 workstation at this point. 
I don't think so, because there are relatively few fluent haskellers. Part of recommending a language is being able to maintain it, and for many projects/companies whatever perceived superiority of the language itself can be eclipsed by the learning curve/lack of devs. Fwiw, I answered the survey and would (and have) recommended. But I'm a decision maker directly, if I were in a shop as the only haskeller I would be far less likely to "recommend" "we" use haskell.
I repeatedly made comments of this nature in my survey response.
Can confirm. I usually just look for the first line number I can find and sniff around there. If nothing is obviously wrong, it's back for a detailed read of the error.
&gt; Just because it is there, doesn't mean you can use it. That's what I am saying. You can't just assume that because you have a Foo-3.0 that anything else needing a Foo-3.0 can use it. Absolutely correct. The way GHC compiles things, libA 1.0 compiled against libB 1.0 cannot be used on a project requiring libA 1.0 compiled against libB 1.1. But let's say you and I are both using the same version of Stackage. We both create a fresh sandbox, and we both type 'cabal install yesod' We are both very likely going to get the same version of Yesod, with all dependencies, all the way down the entire tree. I could compile this on my machine, tar it up, and send it to you, and it would work correctly for your project. The point is not that binary dependencies will alway work for everything in all cases. The point is that in practice we'd likely end up with the same set of dependencies, and could therefore both use the same cached version. Certainly if I started a project and pinned one of my dependencies to something that didn't line up then I'd need to recompile. But I could cache the results, and my colleague compiling the same project could use my cached binaries. And in fact this workflow is something we do on a regular basis with a bunch of custom scripts. This also means I don't need a sandbox. LibA can be compiled against both versions of LibB and stored with a separate hash. There are an effectively infinite number of possible package version combinations and we can't cache them all. But in practice, the same set of package dependencies tend to resolve, and we and simply skip recompilation in those scenarios. Just because something doesn't solve 100% of cases doesn't mean that we should throw the baby out with the bathwater and just never do it. There are improvements we can make to the ecosystem, so we should stop making excuses and start fixing things.
You can make a data type like data Once a = Once a () once :: NFData a =&gt; a -&gt; Once a once a = Once a (rnf a) instance NFData (Once a) where rnf (Once _ r) = r `seq` () and use it judiciously to avoid multiply-deepseq'ing the same value from causing an asymptotic hit.
Note that this comes at a large asymptotic cost for some operations, ensuring the asymptotics of an action written with (&gt;&gt;) can never benefit from sharing -- sharing the functions is useless -- taking a number of operations from O(log n) to O(n).
I am fairly new to Haskell and I don't think the documentation is that bad. I haven't had a hard time understanding what a package or function does after looking at the haddocks on Hackage. I have heard that there are a large number of low quality libraries people threw up on hackage, but plenty of other language ecosystems suffer this too, it's not unique to Haskell. Cabal hell is annoying if you want to have any globally installed packages, but at least it fails at compile time instead of runtime. I think that is the tradeoff with Haskell, there is more pain upfront to satisfy the compiler, but at runtime you can be pretty damn sure your shit will work.
I don't have any experience with it, but maybe you should check out [servant](http://haskell-servant.github.io/).
You don't have to use emacs there is a [Plugin](https://atom.io/packages/ide-haskell) for Atom that tries to provide an IDE like experience. I've used the vim and emacs equivalent with some success. You will find ghc-mod (which gives you at lot of the IDE like experience) is not compatible with newer versions of GHC and Cabal. There is an alternative called hdevtools, but it is not integrated in the Atom IDE. There are some amazing things Haskell can do, but I am still trying to get a decent environment to write Haskell in myself. If you just want to get your feet wet before spending hours setting up an actual development environment I recommend using the [FPcomplete](https://www.fpcomplete.com/page/project-build) site and their built in editor. If you do want to develop on your own machine I recommend installing NixOS on virtual box and running everything from there. It is the only sure way to deal with cabal hell. Cabal sandboxes didn't solve it, Halcyon is not there yet and [Stackage](http://www.stackage.org/) won't solve problems sandboxes don't fix. Despite all the hate, it really isn't cabal's fault since it isn't a package manager and the only way it deals with needing two versions of the same library is to sandbox the whole project, which doesn't always work :( The real underlying issue is Haskell builds are not repeatable unless you have the exact same GHC version, cabal version and a prayer. NixOS is the only fix I have found. The result is that you have to know Haskell inside and out before you can really use it and that is a major flaw at the moment.
This is basically right. Repa and Accelerate share a friendly rivalry, and there are advantages and disadvantages to each. Saying that, one of the goals was to beat Repa (;
I don't find any need for the lazy evaluation of `do x; error ":("`, because it seems like a senseless use case above all. In all other cases (which don't involve the bottom value) the execution order of the arguments of `(&gt;&gt;)` preserves, since it operates on monads, i.e. the wrapped values. As for `(&gt;&gt;)`, I've seen people using it as a counterargument before, but I believe it's contrived and stems from a misunderstanding. The misunderstanding is: strictly evaluating a value of the type `IO Int` is not the same thing as execution of that action. It merely provides you an "unthunked" action, but it's still not executed. So I don't see any problem with strict `(&gt;&gt;)`. (Pinging /u/Taladar)
This! Start with a project of your own, take your time. The question would disappear after you get experienced enough to answer it yourself.
&gt; it'd be trivial to solve, if Haskell were strict: do x; return (error ":(") That doesn't solve the problem!
Thanks! My mistake. I've removed that so that it won't spoil my general argument.
never is a long time. 3 years ago "no-one" in the corporate world had heard of node.js
For simple RESTful APIs, I think Haskell is a perfectly viable choice. I would use scotty or spock. aeson for the JSON (remember to use Generic to derive ToJSON/FromJSON instances automatically for your records). lucid if you want to generate HTML and don't want to deal with separate template files. optparse-applicative for the command-line. Put the shared state of the server (if any) into an MVar or a TVar. I wrote something of the sort [here](https://github.com/danidiaz/vdpt) (the code is not the most elegant ever, I fear). Use cabal sandboxes, and whatever editor you use, try to check small changes by reloading in "cabal repl": it is faster than "cabal build". Use [typed holes](https://downloads.haskell.org/~ghc/7.8.2/docs/html/users_guide/typed-holes.html) when developing, sometimes they help a lot. If the project allows it, try to generate a single self-contained executable. Sometimes Java services are a bit of a chore to start up and deploy, this could be a selling point for Haskell.
I use Sublime Text for my Haskell hacking. I don't use any Haskell plugins for it, just the built in syntax highlighting and simple things like "(un)indent selection" and "(un)comment selection." It also has very simple auto completion that simply helps you to type out a word that is already present in the file. It's no intellisense but it works great for me and makes it really easy to prevent spelling errors in medium to long identifier names. My main wishes for improving my personal workflow are: * Automatic import management and modifying the cabal file with dependencies. * Automatically adding language pragmas when suggested by the compiler (derive data typeable, lambda case, overloaded strings). * Jump to definition I just don't wish for these things hard enough to actually switch editors, because I'm very comfortable with what I've got already.
Reflecting laziness in the types is a bad idea, IMO. It leads to a combinatorial explosion of the amount of code you have two write and the number of APIs. (Similarly to "const" in C/C++, but at least in that case you can just decide that "everything is immutable" and use const everywhere.) Source: I've programmed extensively in OCaml a few years back. Doing anything with laziness (which is explicit) was so annoying that it wasn't practiced very much.
Maybe, bit if you've never programmed before, it will take you even longer to get productive. Probably never, even. It takes years to become a decent software engineer. 
OCaml suffers because it doesn't have higher kinded types and so you can't program polymorphically over strictness. In a hypothetical strict Haskell, on the other hand, you can abstract over the `Thunk` and `Identity` monads and comonads. This would be slightly syntactically awkward if we maintained all the other Haskell syntax precisely, but some sugar would surely make it pretty straightforward.
I mean, it might be easy to do, you won't know until you try it. The fact that the current of `Seq.sort` is not lazy enough might be a hint that it's not that easy.
Oh, I wasn't aware of this paper! Nice work, must study it in more detail. I'll include a reference in the blog post. 
You can be the more experienced engineer. It doesn't have to be someone else. But right now, you don't sound like you're more experienced.
I have been coding in haskell for about 6 years now. When I accidentally install something not in a sandbox which breaks the global package db then I have to wipe it too. Most stuff is fine when you use sandbox (except recompiling everything) but it's so easy to forget. 
Cabal, with all its flaws, is better than all of the current alternatives. That leaves a number of options: * Live with cabal as it is. * Help improve Cabal. * Develop an alternative. 
I'm not sure what any fixes are but if Cabal sees 2 dependencies of the same library, but different versions, it doesn't handle it very well. This would probably be the biggest problem IMO. I've had problems caused by this and it took a while to figure out.
Matter of preferences and mindset. My Development Environment is not one program, but the entire OS, and the "Integrated" part lies in how Unix allows me to glue all sorts of tiny programs together. Vim is not an IDE, and I don't use it as one; in my workflow, vim is the text editor component of the IDE, and it integrates nicely with all the other components - shell, coreutils, ack, git, rsync, ssh, you name it. For code navigation, a few shell tricks and, more importantly, tag files, go a long way - it's not 100% what an opinionated IDE can give me, but because it's unopinionated, it works exactly the same for all programming languages, which is a killer feature for a multi-language guy like me. The same goes for autocomplete; vim's Ctrl-n autocompletion works exactly the same for all languages, it is reliable, predictable, and crazy fast - it doesn't know a thing about the language's semantics, which means autocomplete-driven development is not an option, so it's a trade-off, but my choice is pretty clear on the matter. Autocorrection is a misfeature I absolutely do not want, ever. I write code; I need my tooling to assume that what I type is what I meant, and a tools that second-guess me and change my input without me explicitly asking for it make me angry. More so if they spend precious hardware resources to do something I didn't ask for in the first place. But; whatever work for you is fine. Just trying to give some background on where this "a good IDE is not a priority" attitude is coming from. People write Haskell just fine without an IDE, they don't feel the need for one, and they prefer working on actual language features and libraries instead. Attracting a huge user base isn't a priority either, at least not one that ranks higher than keeping the language pure, powerful, and honest. It's a pity that this means Haskell appears hostile to people who are used to an IDE workflow (and don't feel a need to abandon that), but as with all things open-source, Haskell development is still a very selfish endeavor for most contributors.
The usual way to implement a lazy sort is through something like selection sort. It's asymptotically slow. I'm a bit tired so I could be wrong, but I can't think of an O(n log n) sort that could be implemented lazily.
My problem with repeatability is that I can have something build in my environment, but I cannot just tell someone to download Haskell, git clone source and `cabal install`. I need to know which cabal version they are using, did they install Haskell from brew, apt-get, Haskell platform, [Haskell for Mac OS X](https://ghcformacosx.github.io/), or some other source. They are not the same and to someone New to Haskell they all look the same! Unfortunately, the changes in cabal make it so my source doesn't work smoothly (or at all) on all versions. I don't make any use of cabal-install library nor does my project have anything to do with cabal. The simple fact that I cannot easily tell someone to clone and use my code entirely due to fragmentation of places to install Haskell and build system is the biggest shortcoming of Haskell. Java script may be ugly, but at least it will work on other computers without a headache.
Because cabal is the best we have. And frankly, the problem isn't even cabal itself, but that it tries to do things with the assumption of "perfect" input data while in reality we have imperfect input. Particularly version bounds - I believe the single most notorious problem with cabal is "cabal hell", where resolving dependency issues is a matter of semi-randomly kicking cabal in various ways until it finds a way to meet your perfectly reasonable dependency requirements. The cause is usually that a package somewhere specifies version bounds that are either too strict or too loose. Providing perfect version bounds, however, is a problem that requires superhuman code auditing skills and a bit of clairvoyance, or at least unreasonable amounts of testing. In practice, it does sometimes mean that I won't use a certain package because it would take too much time convincing cabal that it can resolve the resulting project's dependencies, and that is a shame. Developing yet another tool however wouldn't solve anything here, because the choice would be between "do exactly what cabal does" and "give up". If anything, we would need better tooling to validate version numbers and code against the PVP (that is, a tool that we could run against two given versions of a package, and have it verify that the version number change is correct as per PVP). Recent additions to the Haskell ecosystem, such as cabal sandboxes, the ability to fix versions, and stackage, already do a lot to ease the pain, and improving those would IMO be a better use of precious developer time than rewriting cabal from scratch. That said, I hear people have been using nix as a proper package manager for haskell development instead of relying on cabal to install dependencies from source, so maybe that would be a viable route, but I don't know enough about this stuff to have a noteworthy opinion.
Two things that should be easy and would make cabal significantly more ergonomic than it currently is: first, sandbox by default; second, build in sandbox, install binaries to normal directory. Make those two changes (and make them *default*) and I bet 90% of cabal complaints would vanish.
Sorry, auto correct does give you options; it isn't automatic (probably a misnomer by me, I couldn't think what to call it). An example is using a symbol that isn't in scope: it would ask if you want to import the relevant module into scope, then do that for you without you losing context by scrolling to the top of the page and back. And it will give you a list of alternatives if there's more than one option. I tried tag files for python, but could never get it working beyond simple symbol based auto complete. No sorting by relevant type or other heuristics like variable names. I see where you are coming from with respect to the open source community, but commercial backing can only help. My hope is that facebook start using it more and provide a solution (or set of plugins) similar to what they are doing with Atom and react.js. Ultimately I think a workflow like you have is too much to expect of typical programmer at some company, thus my answer to the OPs question.
I use Haskell at work. Here is how I did it: 1. I made a POC during experimentation time (and a bit in my spare time too). 2. It proved to be very promising so it started to become a real project. Now it is essential and extremely robust part of our architecture. So, I had very few risk to introduce Haskell. For you it depends on your goals: 1. Prove Haskell is great to the team. For this you should already know how to use Haskell as you'll have already encountered most basic Haskell problems. For example cabal hell and stack overflow due to lazyness are common beginner problems. When you pass from beginner to intermediate these problem disappear completely. But in the mean time your return of experience will be that "Haskell is Hard(tm)" and people in your team certainly won't be convinced by Haskell. The real Haskell power take time to be appreciated. It is not like node nor clojure where benefits are immediate compared to java. 2. You wan to learn Haskell to experiment it. If your project is just a proof of concept. Then I would say go for it. But don't despair at the firsts problems. You'll hit many common beginner problems. If you have some weeks (months maybe), you'll soon see why people praise Haskell. So if you have the latitude to fail and take time to understand why and fix the issues, hopefully you'll love Haskell and be the first experienced Haskell developper in your team and you'll be here to show what not to do. If you want to try here are some things that will make your life easier: 1. To install Haskell use Haskell LTS or at least stackage (I made a script for this[^1]). **Never** do a cabal update (**never I mean it**). 2. As second advice, if like me you prefer to start the hard way. You might prefer to use an alternative prelude. I personally use classy-prelude. 3. Also a simple advice try not to use "foldl" (not even any fold* function). You generally only needs such function because you didn't search or represented your data correctly. It is of course not always the case. But mostly, it has been a long time I didn't used any fold* function in my code. The fold* functions are in fact quite low level functions. There are certainly a lot of other beginner advice to provide and to get on the many Haskell community channels. Best of luck! [^1]: http://yannesposito.com/Scratch/en/blog/Safer-Haskell-Install/ 
Teach yourself and get the experience you need. Why rely on someone else?
Yeah, he emailed the list about this [here](https://groups.google.com/forum/#!topic/elm-discuss/EZ17hBTXr2I/discussion), and that spawned [this thread](https://groups.google.com/forum/#!searchin/elm-discuss/design$20by$20example/elm-discuss/llXtie-2GzY/9wYaZXkXZq4J). If you look through at the specific thing he is trying to do, it is quite a specialist case for web programming. For 99% of websites or other online things, I don't think you will be in that position. That's not to say it's not important, but I have to make prioritization decisions to make any real progress and part of that is deciding how broad the impact of certain changes will be when budgeting time.
I think that's right on point. Summarizing this thread: - compiling is too slow (perhaps both a ghc and cabal issue) - people still having problems with cabal because they're not using it in the nice new way (sandboxes, `--allow-newer` when necessary, maybe dependency freezing). This is partly a documentation issue, and partly that opinion is divided on the "best" workflow and cabal in 2015 permits several (and it's UI has gotten even stranger and less intuitive) The latter constellation of issues is not difficult to solve; even a haskell newbie could make a great contribution by taking an hour to understand the sandbox workflw and try re-imagining a better UI. In any case it doesn't help to continue crying "cabal hell", and about how "we " must do something (certainly if I had helped implement sandboxes, or version freezing, or `-j` I'd be upset to see the goal posts moved and my hard work go unacknowledged). Clarity please.
Except for Cabal Hell, Cabal is great. 
But that is a bit like saying, "I love being a software engineer, except for the part about working with computers."
I have found the best way to use cabal is to keep a list of packages I need in a file called `user-goals-list.txt`, and every time I want to add a new package, I add it to my list, then do: cabal install $(cat "user-goals-list.txt") If I want to update my GHC installation, I just run the above command without modifying the user goals list. This is the best way I know to prevent Cabal Hell, and I have always wondered why I have to use this hack. Why doesn't cabal just keep it's own list of user goals? Also, I don't know if cabal has a feature to automatically clean-up older versions of packages after updating if it can prove that nothing else old depends on it. If it cannot do that, it would be a feature that I think Cabal should have. But so far I haven't been able to figure out how to make Cabal do "auto cleanup," I don't think it has this feature in any way. If it could do those two things, I think Cabal would be very pleasant to use. But as it is now, Cabal is very painful to use. Unfortunately, I do not know any better alternatives, so I am stuck with it.
fwiw, cabal maintains a "world" file of all manually installed goals
This work is so incredibly important. I was about to make a post asking what's happening around it. 
Frankly, I have found Cabal to be far better than most other language-specific build/package management systems. Yes, tracking dependencies is a hard problem and that is reflected in what people often call "Cabal-hell". Unfortunately there is no easy answer here. Either... 1. we freeze our entire ecosystem, ensuring library compatibility by definition, 2. we expend effort maintaining proper versioning and dependency bounds and ensure compatibility through shear force, or 3. we give up on versioning and accept that a given build may fail at compile-, or worse, run-time. Most users would be quite displeased with #1 and, as a community that values correctness, #3 isn't (in my opinion) a good fit for Haskell. The [PVP](https://wiki.haskell.org/Package_versioning_policy) tries to push us towards #2 although it does come at a cost. However, restrictive bounds often mean that users will occasionally experience problems with package breakage and impossible build plans. This is exacerbated by Cabal's inability to manage more than one installed instance of a given package, a limitation which, with luck, will soon be [lifted](https://mail.haskell.org/pipermail/cabal-devel/2015-May/010163.html). In the meantime, the problem is substantially eased if package dependency bounds are kept compatible with the current state of Hackage. In my experience, this requires relatively little effort but offers substantial rewards. Towards this end, I make an effort to review Cabal's installation plan before committing to running `cabal install`. If I see that a plan builds an out-of-date dependency then I take a minute to track down the culpable package, bump the bound, and try the build again. I have a `cabal-try` [script](https://gist.github.com/bgamari/d4911ccbe0f71ee5970f) which makes this check pain-free. If the build succeeds I open a pull request upstream where possible. This whole process is surprisingly quick and by fixing the bounds I have ensured that I (and other users) will never see the issue again and avoid painful rebuilds later on. This exercise is admittedly a bit mechanical and could benefit from automation. There have been a few attempts to contribute to this space (e.g. [Scoutess](http://projectscoutess.blogspot.co.uk/2012/08/mission-report.html)) although I've yet to see a service which offers low enough friction to enter wide-spread use. [Stackage](http://www.stackage.org/) is another approach which has gained quite some traction and for good reason.
It has pretty advanced types. I would recommend starting from something simpler like spock or yesod, and migrating to servant later.
&gt; Java script may be ugly, but at least it will work on other computers without a headache. Yesterday I wanted to play with ghcjs. It required me to install nodejs more recent than the one in Ubuntu 14.04 repo. It's just as a pain as getting GHC of specific version, so I disagree that JavaScript "just work" and doesn't have a headache. Also npm, gulp and bower related problems aren't much easier to deal with at all, I had constant problems working with them. p.s.: it's JavaScript, has nothing to do with Java, so please never write it separately
The problem of Cabal Hell is not the cause of it, i.e. dependency diamons or whatever. The Hell comes from the fact, that when it happen, it f*cks everything up, and you need a week to rollback and be in a state where you can try to maybe not use (or update) the library causing the problem.
In addition to /u/edwardkmett's suggestion of the `Once` data type, there's the recent [`nf`](https://hackage.haskell.org/package/nf) package.
&gt; I can't think of an O(n log n) sort that could be implemented lazily The quick sort from the haskell prelude is and this without having to think. This is one of the advantage of lazy by default.
lots of good comments here. i'd just add that you are probably over-thinking this. you're unlikely to change company culture. it's incredibly hard to do so, even with way more knowledge and experience. if they said you can use any language, just take the chance to enjoy yourself and learn. if it's awesome and converts everyone, cool. if it's a disaster, well, at least you learnt something. but most likely it will mean a lot more to you as a learning experience than it will to anyone else.
I'm not having a specific problem, I'm just describing what Cabal Hell is. There is indeed not real Cabal Hell using sandboxes, there are just diamond dependency problems. However, sandboxes only prevent to go to Hell but they don't help in escaping from it. I mean everythign is fine if you create your sandboxes before corrupting your entire system.
Docker? For deployment ideally you'd be shipping binaries.
Sorry for being completely offtopic. What exactly is the problem with fold*? As a newbie, every single tutorial that I read/watch teaches you that you should master and use the trio map/filter/fold.
Okay, I see. One repeat suggestion is to make sandboxing the default in Cabal. I wonder how many complaints about Cabal Hell come from people not using sandboxes (for whatever reason, including discoverability of the feature).
&gt; What we're after is a way of indicating, in the type of a function like `a -&gt; b -&gt; c -&gt; r`, which of `a`, `b` and `c` are evaluated strictly and which lazily. I see! I thought the goal was to have a single implementation whose behaviour could be switched from that of `foldl` to that of `foldl'` only by changing the type signature. So, just to make sure I really understand what you want this time, let's me rephrase it in my own words: 1. You already know which argument you want to `seq`or add a bang pattern on, you just want to advertise which ones at the type level. 1. And it would be even better if the types made sure you did not forget to add the `seq`. 1. And it would be even better if changing the type signature decided whether the `seq` would happen or not. The solution you gave in your article already solves all three: (1) the arguments which will be forced are the ones immediately before a `:-&gt;`, (2) it's not possible to construct a value of type `a :-&gt; b` which doesn't force its argument, and (3) using the `FunctionLike` typeclass, it's possible to write functions which can be instantiated to either `-&gt;` or `:-&gt;`. Since the goals of a strictness monad are very different (`mx &gt;&gt;= f` will force `mx` before evaluating the rest of the computation even if `f` ignores its argument), no wonder you found it inadequate. If you want the arguments to be forced before the recursive call, then there should be a `&gt;&gt;=` between those two: -- behaves like foldl' when m ~ Strict and like foldl when m ~ Lazy foldlM :: Monad m =&gt; (a -&gt; b -&gt; a) -&gt; m a -&gt; [b] -&gt; m a foldlM f mx [] = mx foldlM f mx (y:ys) = do x &lt;- mx foldlM f (return (f x y)) ys But nothing in the type system is forcing you to do so, which is why you were able to construct your counter-example in which you lazily build a bigger `mx` instead of evaluating the one you received as an argument first. Come to think of it, even `:-&gt;` relies on such a convention: `strictly (\x -&gt; ...)` will force `x` before evaluating the body, so if we want the arguments to be forced before the recursive call, that recursive call better be inside this body. Let's see if I can construct a counter-example: instance Functor ((:-&gt;) a) where fmap f g = strictly (\x -&gt; f (g ! x)) foldl' :: (a -&gt; b -&gt; a) -&gt; [b] -&gt; a :-&gt; a foldl' f [] = strictly id foldl' f (y:ys) = fmap (\z -&gt; f z y) (foldl' f ys) I had to swap the order of the `a` and `[b]` arguments, but I did manage to break the contract which `:-&gt;` was supposed to offer: even though the `a` is to the left of a `:-&gt;`, stepping into ghci indicates that `z` is still a thunk. In conclusion, it seems less and less likely that any technique would be able to enforce (2), so we'll have to ask our users to follow some coding convention such as "always put the recursive call inside `strictly`" or "always execute the monadic computations of the arguments before executing the recursive call". And if we're going to allow such coding conventions, then we might as well say "always use a bang pattern on arguments of type `Strict a`": foldl' :: (a -&gt; b -&gt; a) -&gt; Strict a -&gt; [b] -&gt; a foldl' f !z [] = runStrict z foldl' f !z (y:ys) = foldl' f (fmap f' z) ys where f' z' = f z' y Or, since pattern-matching on `Strict z` will have the same effect, we might say "always pattern-match on the Strict constructor, no pointfree style allowed": foldl' :: (a -&gt; b -&gt; a) -&gt; Strict a -&gt; [b] -&gt; a foldl' f (Strict z) [] = z foldl' f (Strict z) (y:ys) = foldl' f (Strict (f z y)) ys I like how readable this solution is.
I'm looking for FPComplete's docker image with all of stackage pre-built, but it doesn't seem to be published yet.
&gt; Sorry, auto correct does give you options; it isn't automatic (probably a misnomer by me, I couldn't think what to call it). An example is using a symbol that isn't in scope: it would ask if you want to import the relevant module into scope, then do that for you without you losing context by scrolling to the top of the page and back. And it will give you a list of alternatives if there's more than one option. Oh, that. Yeah, that's a useful feature, and I'd love to have it, but the problem with it is that the editor needs some in-depth semantic knowledge of the code I'm editing, and it would have to tie in with the build system and package repository, and there are a bunch of problems with that, most notably that the "works for all programming languages including ones yet to be invented" feature is no longer possible. It also tends to be quite slow and resource intensive, because basically your IDE is constantly compiling your code. I can totally understand why people want that and use it, but my priorities are different, and jumping to the top of the file (`gg`), adding the import (typically something like `wcaWthe.right.module (theFunctionINeed)&lt;Esc&gt;`), and then jumping back (`''`) is trivial enough in vim (and probably faster than your IDE can find and insert the import for you anyway). &gt; I tried tag files for python, but could never get it working beyond simple symbol based auto complete. No sorting by relevant type or other heuristics like variable names. Python is positively terrible in that regard, plain and simple. But then, I believe building a reliable fully-featured and completely correct autocompletion engine for Python would be an equally ghastly task, what with all the monkey patching and magic properties/methods - bottom line is if you want the full monty, you'll basically have to more or less run the code. &gt; I see where you are coming from with respect to the open source community, but commercial backing can only help. My hope is that facebook start using it more and provide a solution (or set of plugins) similar to what they are doing with Atom and react.js. FWIW, Haskell does have quite some commercial backing already, but the main stakeholder (Microsoft Research) has other goals than developing Haskell itself into a mainstream friendly programming language. Their agenda with supporting Haskell development seems to be "let the brainy guys play with their toy, then grab the good ideas and bang them into a shape palatable for the average C# programmer". A lot of ideas that originate in Haskell have made it into .NET that way. There's also FPComplete; I don't know exactly what their final goal is, but they sure are working hard on many things that are beneficial to Haskell mainstream adoption - libraries for common modern programming tasks, improving Cabal, a web-based IDE, documentation, mirroring Hackage, maintaining and curating Stackage, etc. Facebook might also play a bigger role in the future, in any case they seem to be willing to invest at least a bit in Haskell. &gt; Ultimately I think a workflow like you have is too much to expect of typical programmer at some company, thus my answer to the OPs question. I am very aware of this. I am very lucky to be in a position where my fellow programmers more or less share my views (although we do have fierce emacs vs. vim vs. Sublime battles though, and the Ubuntu vs. Debian fight isn't over yet); I understand very well that most programmers do not enjoy working that way, and frankly, for your typical assembly line programming job, it's not even a very good workflow - it is great when you need full control over the entire stack and ultimate flexibility, but if 99% of your job is applying standard approaches to standard problems, then a toolchain that allows you to generate 90% of your code with a few clicks is obviously superior.
I did something very similar where I work. I built up a Haskell reimplementation of something that was very high-traffic, but was noncritical. It was a perfect replica, so I had an easy time getting buy-in from everyone to try it out in production. (we were literally a single nginx reconfiguration away from reverting) Once I had that going, I had a lot of empirical figures that I could use to talk about Haskell concretely. It turns out that this is much more convincing to skeptics than handwaving and anecdotes. This approach also meant that I wasn't completely clueless when we came across an opportunity to use Haskell for something mission critical. :)
My strategy has been to do "cabal install foo --dry-run" to see which versions of libs will be installed. If I see something that could be "problematic", I try lower version numbers (cabal install foo-0.x.x --dry-run) until the situation looks "cleaner". Now I've just started trying to use Stackage LTS. 
You don't need to assume -- you can actually see something similar to this in the data! I haven't measured the number yet (Tristan probably will soon) but it's definitely an issue.
Does Scotty allow for headers and payloads? I built some REST endpoints but haven't seen how to do add that functionality. 
That is definitely helpful. So ℛ is used to denote type-indexed relations between values, and ⇔ is used to denote relations between types?
&gt;**without having to think** That's a bold assertion considering the cognitive overhead that comes with trying to assess memory usage in Haskell.
I may write a wrapper script around cabal that errors out unless a `cabal.sandbox.config` exists in the current working directory, thinking about this.
Also when you use `foldl`, you almost always really need `foldl'`, the strict version.
Javascript does "just work". GHCJS doesn't. With the industry standard tools like nodejs, npm, bower, and Chrome or Firefox you will be able to create solutions in a couple hours that would take days using Java or .NET.
Posts like this give the impression, outside the Haskell community, that everyone has issues with cabal. Which as replied by others is not true. @Mods please ban all future users that make sweeping generalizations in their post title, and update the titles to remove any such generalizations. I guess this should be a global reddiquette.
I sardonically mentioned Haskell as an option and my coworkers jumped at it. I was very surprised and I am now very happy.
Do you really think one should try to learn all possible web-frameworks out there?
If it wasn't mentioned in the error message from cabal, that's an obvious bug.
That link doesn't lead to any page. 
is highlight-versions, some script?
I just want to build everything from scratch (and then maybe get some work done in a hundred or so years from now :P)
It didn't work. But I went to the page manually, and it was very helpful and interesting. 
I think you are looking for Generics, which cassava has support for. Here is an example: https://github.com/tibbe/cassava/blob/master/examples/NamedBasedGeneric.hs Specifically, this is using GHC.Generics module and the DeriveGeneric extension.
The mantra I've heard is that "cabal is not a package manager." So that's what we need. A package manager. Build it on top of or alongside or in place of cabal, whatever works. But as long as we keep relying on cabal to do most of the job of a package manager, without it actually being a full blown package manager... we will keep running into the same pain points.
Cabbage is the one. It still has the idiosyncrasies of a single-developer program, but it does what it says on the tin. The big missing piece right now is that while it can work with Stackage LTS and Nightly, it doesn't do anything smart to reuse build plans computed by the Stackage server, so per-package build plans are still computed locally. It's not 100% trivial to reuse build plans, as one has to take care when non-Stackage packages are involved.
Thank you, that looks very useful! I'm not familiar with generics, so that example must have escaped me earlier. 
The title might be slightly inflammatory but the OP is asking an honest, constructive question. No need to censor. PS If you believe the post is not relevant then that's what the voting mechanism is for.
There are a million different ways to state the question without starting with an assertion. That's why I mentioned that mods should ban such users, remove the assertion and let the discussion take place. I hate nothing more than people making broad statements; guess it's the same reason I hate politicians as much as cancer.
&gt; It is super hard to learn, unfortunately I would replace "hard to learn" with "a lot to learn". As with design patterns in OO languages, it's one thing to read on the topic; and another to use them in practice.
Have you tried Stackage shared sandboxes? Absolutely great. All your projects can just use a single sandbox, so you don't have to rebuild all deps for each project separately. Haskell LTS is great!
If you use a big single sandbox for all your projects... isn't that like.... you know... using no sandbox at all? =)
Any particular reason we need to reimplement what nixpkgs already provides? From the gist, this should work in a similar way. 
&gt; Why can't I just click install on a package on hackage or indeed a version on stackage? If you mean something like you can have for `.deb`s and `.rpm`s, then we'd just need some custom established MIME-type set as `Content-Type` on the src-tarballs URLs to bind URL download handlers to. However, since we don't use a custom file extension for the src-tarballs there's an overlap with the generic `.tar.gz` extension and it can only work for `http(s)://` storage where one can override the default MIME-type inferred...
I made it until &gt;So to really understand return, we have to understand its dual: bind.
Well, the store takes some place and it'll get the full toolchain and its dependencies, but depending on your system, you'll need those anyway. On OSX, you'll have two compiler toolchains, the system one and the nix one, but on Linux, I think you could do with only the nix one. OTOH, if you reimplement it, it is another piece of software that needs to be maintained, whereas if it stays in nixpkgs, it will be maintained by the nixpkgs maintainers.
Thanks, corrected/updated
Thanks. The comments are helpful as I don't wish to mislead or misrepresent, so I'm updating as we speak
Maybe there is one, but I don't see it. I would think it would be ironic if I'd said something like "all people making broad statements are X"
You might want to look into this: https://www.youtube.com/watch?v=BsBhi_r-OeE
I keep getting.... "Pascal? Why would you want to program that in Pascal?"
&gt; And yet, I see relatively little work go into the tooling that empowers new entrants to the language. For example, Haskell's "package manager" probably takes last place compared to those for other languages. If you're saying (if I misunderstood, apologies) that Cabal is not getting the attention it deserves, just take a look at how much is going on over at https://github.com/haskell/cabal/pulse and https://github.com/haskell/cabal/graphs/contributors I'd rather say that Cabal is trying (i.e. it's not there yet) to solve a much harder problem than other languages are trying to address (and Stackage to some degree represents the traditional pragmatic solution to this kind of problem)
How is it the IHG's fault?
You missed a spot under "It all starts with IO" &gt; () is the empty type, meaning the function returns nothing: side-effects only.
No worries, no offence taken.
`highlight-versions` is a useful tool for making Cabal's output more readable and can be found on [Hackage](http://hackage.haskell.org/package/highlight-versions).
I'm not sure but I suspect they meant that executables produced by sandbox installations end up in a shared directory. I can certainly see how this would use useful; I generally end up creating symlinks to my sandboxed executables to achieve this same end.
This. I'm a complete Haskell fan and VP of software dev at a tech firm, so I live both sides. I get paid to manage risk, specifically to avoid oh s*** situations. To manage risk, we choose technology platforms very carefully. This includes hardware, OS, network infrastructure, server platforms, and programming languages. Haskell and other non-mainstream technologies create distinct risks. Whenever the net benefit outweighs the net risk, we take that risk with our eyes wide open, and we do everything practical to mitigate the risks. If I had to develop incredibly reliable software, like a medical device or a fly-by-wire system, Haskell might outweigh its risks. Clearly Haskell brings terrific benefits but it also brings substantial risks: - Haskell APIs evolve in incompatible ways. I wrote perfectly clean Haskell apps a few years ago using then-current APIs; these don't compile today. That's fine for a research platform but not OK for a production system. - Compared to mainstream languages like Python or c#, it's essentially impossible to quickly hire highly experienced Haskell developers. I need to staff projects very quickly, often with contractors, and a lengthy recruiting cycle is a complete showstopper. (Literally, the project stops.) - Laziness and leakage bugs creep up without notice, and these often have long tails in their distributions of debugging time. - Because the libraries aren't as rich or mature as other languages, we'd spend opportunity cost to write lots of low-level tools that come out-of-the-box in Python, c#, R, etc. - Much of the library documentation is weak. The practical consequence is that developers spend lots of time and budget experimenting with plumbing, when they could be developing awesome products for our customers. I'm totally not trying to be negative here since I *very* enthusiastically use Haskell for all my toy/throwaway/research projects. My concerns are all about probabilities, risks, costs, schedules, and rational decision making.
love it :) great job. small technical issues here and there as has been mentioned, but I like your explanative style and approach :) 
&gt; If you get stuck, the #haskell chatroom on irc.freenode.net is one of the best resources out there. Seconding this. I've gotten great help there (and recently given a bit, too).
You can still have as many sandboxes as you like; however, if you have a group of related projects and you pin them to the same version of Stackage (e.g., Haskell LTS), then these projects can share the sandbox. Since Stackage fixes the version numbers, all projects will use exactly the same versions of dependencies and therefore you only need to install them once. Note that it's significantly different than the standard global and user package DBs. @RedLambda, I encourage you to read up a bit more about how Stackage works and give the stackage cli tools a try. You can do like this cabal install stackage This will give you the cmd line tools. Then in your project: stk sandbox init nightly This will create the shared sandbox with the current stackage nightly. For me it created ~/.stackage/sandboxes/ghc-7.10.1/nightly-2015-05-22 That's it! Then just work as with a normal sandbox. All deps you need will be installed there once for projects pinned to stackage nightly and if the deps are part of stackage, they will build :-) Note that you can still use deps outside of stackage but then it's back to usual as they might be outdated. The next step would be to publicly provide binaries of Haskell LTS for different platforms - it would be like a shared sandbox for all projects pinned to Haskell LTS, and you wouldn't have to build anything - just download. Since FP Complete builds whole of Stackage anyway, they could also host the binaries for everybody to download. 
Also: Unity (game engine, Ubuntu Compiz screwer-upper, etc). There's something going on in the zeitgeist. I think we all feel encumbered by all of the choices we have today. We live an embarrassment of riches.
nix is even harder to use than cabal (it's UI is pretty non-begginner-friendly). You have to have an extra dependency on nix, which is also unsupported on Windows. Nixpkgs only provides the most recent versions of packages. Implementing nix's ideas in cabal makes the most sense for haskell. 
I actually find cabal sandboxes + stackage LTS + cabal's add-source to work pretty well, minus the part where it takes an hour or two to build all of our repos. Almost all of them rely on either persistent or yesod. If that could be sped up sanely through caching of already built object code, that'd be pretty great.
That even sound like Haskell characteristics. 
That, and there's a soft paradox that type systems give you: More static guarantees means it's easier to reason about code. This also makes it easier to change, which means people are more able (and so, sometimes more willing) to implement breaking changes. Despite the apparent rigidity of types, it ends up affording you a lot of flexibility with your design.
I haven't been on here in a while, so you're not a broken record for me! XD Thanks! I'll check it out, and sure, I would love to hear feedback as I continue learning. 
Thanks. Wow, I already have like 10 tabs of resources, lol. Good thing I have a lot of time.
I really don't think you get much from fiddling with a library for 3 hours. There is much you cannot predict, it might be buggy, it might not scale, it might not be flexible enough. Wasting hundreds of hours just to go through every web framework just doesn't pay off imo. Choose 3 frameworks and compare, choose the one that fits your needs most, is the most mature and actively developed, and the one you feel is flexible enough that you can fix or change when you need. Those are my inexperienced 2 cents anyway.
I read some of [the paper](http://www.cs.indiana.edu/~sabry/papers/exteff.pdf) and it looks like this demonstration uses the same ideas. The r in Eff r t is a list of functors, values of which may appear in the Eff r t term itself. These are suggestively-named functors like data Reader r t = Read (r -&gt; t) data Trace t = Trace String (() -&gt; t) or even the familiar data PlusF t = Plus Int Int (Int -&gt; t) The functors are summed into the type Union r, and the free monad over this sum is used (by another name) in the definition of Eff: -- This is another way to say Free (Union r) -- Free (Union r) t = Pure t | Join (Union r (Free (Union r) t)) data VE r t = V t | E (Union r (VE r t)) -- This is Codensity (Free (Union r)) t -- I only recently heard of codensity, so I don't know whether that's remarkable :) data Eff r t = Eff { runEff :: forall w . (t -&gt; VE r w) -&gt; VE r w } Running an Eff r t amounts to interpreting every functor which appears in that list of types r, but instead of producing a term of some monad transformer stack like we do here, the handlers of extensible-effects stay in Eff, reducing the Eff r t to an Eff Void t or Eff (Lift m) t, which then yield a value and in the latter case an effect in m. I can't say whether this direct free monad approach can do anything that extensible-effects cannot, or vice versa. It's cool to find the same ideas under the hood.
I guess it's because you can use the same handler for multiple events. It's how it is in .NET, not my invention.
Um, No. Present history has proven that they can't even operate what should be a fairly trivial package server correctly, both in uptime, and in functionality- let alone a non-profit. Not another penny.
It would be super nice if https://github.com/jwiegley/hnix we're resurrected and Cabal could use it—giving Cabal users the benefits of Nix without a bunch of non-Haskell deps.
&gt;Can you tell me if what I originally inteded to do with https://bpaste.net/show/ea9c9b525b1d is possible with actual Monads? No, you can't do it. You're version of `return` requires a `Show` constraint. `return` from the `Monad` typeclass does not have such a constraint. The two functions have different types, so it's not possible. &gt; The guy from irc also told me that one can make the second version, https://bpaste.net/show/717d1572d053 , to work with do notation using overloading 'do'. How is this done? https://ocharles.org.uk/blog/guest-posts/2014-12-06-rebindable-syntax.html &gt; I asked about these again in the IRC when someone else told me that it does not obey the Monad laws. so cannot work with composition. He also suggested changing the type signature and add a new function. showReturn :: (Show a) =&gt; a -&gt; Writer a showReturn x = Writer ("I got: " ++ show x) x So I tried doing this http://lpaste.net/133284 , but didn't work. He wanted you to write `showReturn` as you have, but also define `return` as return x = Writer "" x This version obeys monad laws. `showReturn` does not obey the Monad laws, and also (unrelatedly doesn't have the same type as `return`). These are both good reasons for why `return` should not be the same as `showReturn`.
But that's exactly my point. Suppose you have (I'm making up the types here but should be obvious enough what I mean) myHandler :: Button -&gt; ClickDetails -&gt; UI () myHandler b cd = case someControlSpecificID b of "foo" -&gt; ... "bar" -&gt; ... then instead of using it as b1 &lt;- button click b1 &gt;&gt;= handle myHandler b2 &lt;- button click b2 &gt;&gt;= handle myHandler you can do the, IMO, much cleaner b1 &lt;- button click b1 &gt;&gt;= handle (myHandler b1) b2 &lt;- button click b2 &gt;&gt;= handle (myHandler b2) since connecting to events should be completely orthogonal to the fact that your particular handler happens to depend on the particular control emitting the event. I know the WinForms API is like this, but it was originally geared towards an ecosystem where creating throwaway closures to record extra data like that was not really easy.
Thanks. got it working http://lpaste.net/133302
Also as a noob, I'm curious what the easiest of the easy is on there, just to look at for perspective
I learned Haskell in my free time, only doing production work after I was confident in my ability to use *and* teach Haskell. It wouldn't have mattered that I was comfortable if my coworkers were stranded. Not sure where that leaves most people. Is there a meetup group local to you where some people could learn w/ you?
The thing is that Scotty and Spock are a very lightweight microframeworks on top of WAI (which is the base for Yesod also), so they are really small to learn and really shouldn't take more than an hour to observe. Sure, everyone's speed is different, so it might be 2 or 3 hours for another person, still not that much for a "framework knowledge" IMO.
[NBViewer](http://nbviewer.ipython.org/) doesn't highlight the Haskell properly for now, but it still great if I want to share [some of my experiments](http://nbviewer.ipython.org/gist/astynax/c9b88c0a27eb7177b415) :)
I agree, which is why I put `unless it leads to more money` in my initial comment. However, it is a business decision to decide which level of happyness they needs to provides to their staff. Moreover, there is probably less good developers that developers thinking they are good, and even less people really indispensable (look at Apple).
&gt; I'll never use Haskell in a commercial environment? Right, I'm sure pretty much everybody on this channel, dreams of using Haskell in a commercial environment. So you won't be the only one. However, you should still try. 
Very true.
Thank you for the reply /u/dnaq/ ; this sounds quite interesting, any chance (part of) this project is available somewhere?
Also, I think the fact that so many of those dependencies are so fundamental means we get a lot more diamonds in our dependency graph. Things in text, bytestring, and containers are provided by the language or standard library in most other languages.
Wow, Idris never ceases to amaze me 
In indeed does allow headers. Main type of an action is called `ActionT`, which is defined in `Web.Scotty.Internal.Types` as: newtype ActionT e m a = ActionT { runAM :: ExceptT (ActionError e) (ReaderT ActionEnv (StateT ScottyResponse m)) a } deriving ( Functor, Applicative ) You can see `ScottyResponse` here, it describes a response as: data ScottyResponse = SR { srStatus :: Status , srHeaders :: ResponseHeaders , srContent :: Content } So there should be a helper for setting headers. Helloworld application shows how to send some html, so we can search somewhere near, in `Web/Scotty.hs`: -- | Set the HTTP response status. Default is 200. status :: Status -&gt; ActionM () status = Trans.status -- | Add to the response headers. Header names are case-insensitive. addHeader :: Text -&gt; Text -&gt; ActionM () addHeader = Trans.addHeader -- | Set one of the response headers. Will override any previously set value for that header. -- Header names are case-insensitive. setHeader :: Text -&gt; Text -&gt; ActionM () setHeader = Trans.setHeader -- | Set the body of the response to the given 'Text' value. Also sets \"Content-Type\" -- header to \"text/plain; charset=utf-8\" if it has not already been set. text :: Text -&gt; ActionM () text = Trans.text -- | Set the body of the response to the given 'Text' value. Also sets \"Content-Type\" -- header to \"text/html; charset=utf-8\" if it has not already been set. html :: Text -&gt; ActionM () html = Trans.html -- | Send a file as the response. Doesn't set the \"Content-Type\" header, so you probably -- want to do that on your own with 'setHeader'. file :: FilePath -&gt; ActionM () file = Trans.file -- | Set the body of the response to the JSON encoding of the given value. Also sets \"Content-Type\" -- header to \"application/json; charset=utf-8\" if it has not already been set. json :: ToJSON a =&gt; a -&gt; ActionM () json = Trans.json You can see helpers for many sort of things you want (many more than these that I've shown). So, setting header is as trivial as: {-# LANGUAGE OverloadedStrings #-} import Web.Scotty import Data.Monoid (mconcat) main :: IO () main = scotty 3000 $ do get "/:word" $ do beam &lt;- param "word" setHeader "X-SomeHeader" "SomeValue" html $ mconcat ["&lt;h1&gt;Scotty, ", beam, " me up!&lt;/h1&gt;"] 
If not, then by clicking install on the cabal / stackage gui. We all download and install software anyway, so I feel it's a little patronising to suggest that haskell users will go straight from hackage to dodgysoftware.com and click to their heart's content with their antivirus off. Setup.hs can already include arbitrary code, and I don't see anything in the current text interface protecting me more than a web frontend would. If you feel it's safe because it's text based that's because you're not old enough to remember DOS-based viruses. 
I'll have to try this out when I get home. Not a windows user but happy to support haskell/windows development.
Good luck! haha
Yeah, I like it. Also you can joke 'join Haskell Church'
Do you mean if you have a long chain of x &gt;&gt; (\_ -&gt; x) &gt;&gt; (\_ -&gt; x) &gt;&gt; (\_ -&gt; x) &gt;&gt; (\_ -&gt; x) &gt;&gt; ... then you have to apply the dummy argument each time?
Not that I suppose it is everything, but: https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/release-7-10-1.html Ctrl-f DWARF More in 7.12 I am told.
I have always thought the "cabal is not a package manager." argument is weak. Cabal does not only build, it also download and install packages in a global repo. If a program have a install command it really should have a uninstall command as well.
&gt; It seems to me that in your counter-example, `z` remains a thunk until all of the `[b]` has been consumed and a large function of type `a :-&gt; b` has been constructed in its place. At that point the `z` is forced. Do you agree with the reasoning? Yes, that's what I was going for. Although now that I'm taking a closer look, I can see that it's not what actually happens! I made several mistakes. First, there is a bug in my implementation: I'm applying the function to the elements of the list in reverse order. For this reason, when I reach `f z y` in ghci, it's normal that `z` is a thunk because it represents the result for the part of the list which hasn't been processed yet, not the partial result we have accumulated so far. Here is a fixed implementation. wrap :: ((a -&gt; b) -&gt; (a' -&gt; b')) -&gt; (a :-&gt; b) -&gt; (a' :-&gt; b') wrap f g = strictly (\x -&gt; f (g !) x) foldl' :: (a -&gt; b -&gt; a) -&gt; [b] -&gt; a :-&gt; a foldl' f [] = strictly id foldl' f (y:ys) = wrap (\cc z -&gt; cc (f z y)) (foldl' f ys) Second, using ghci to step until `f z y` is reached and checking whether `z` is a thunk or not is not a good way to verify whether the implementation is accumulating thunks or not. It is not in `z` which the thunks will be accumulating, but in the `a :-&gt; a` result! A proper counter-example would first accumulate a bunch of thunks describing an `a :-&gt; a` value, then the thunks would be forced in order to yield a large function of type `a :-&gt; a`, and finally this function would be applied to the original `z`. This would of course force `z`, and then proceed with the large body of the function, in which `f z y` would eventually be reached. At that point, `z` is clearly not a thunk anymore, but we still had a space leak. Third, neither my buggy nor my fixed implementation even succeeds at being a proper counter-example, because in both cases, the original `z` argument gets forced at the very beginning, before the tail of the list gets examined. Here's what happens with the buggy implementation for example: foldl' (+) [1..4] !(trace "z is evaluated" 0) ==&gt; fmap (\z -&gt; f z y) (foldl' f ys) !(trace "z is evaluated" 0) where f = (+) y = 1 ys = [2..4] ==&gt; strictly (\x -&gt; f (g ! x)) !(trace "z is evaluated" 0) where f z = f1 z y1 g = foldl' f1 ys1 f1 = (+) y1 = 1 ys1 = [2..4] ==&gt; Strict (\a -&gt; a `seq` f a) !(trace "z is evaluated" 0) where f x = f1 (g1 ! x) f1 z = f2 z y2 g1 = foldl' f2 ys2 f2 = (+) y2 = 1 ys2 = [2..4] ==&gt; f (trace "z is evaluated" 0) where f a = a `seq` f1 a f1 x = f2 (g2 ! x) f2 z = f3 z y3 g2 = foldl' f3 ys3 f3 = (+) y3 = 1 ys3 = [2..4] ==&gt; z is evaluated ==&gt; f 0 where ... In order to produce a proper counter-example, I must explicitly request the recursive call to be forced before `fmap` or `wrap` can return a computation of a form which `(!)` can understand. foldl' :: (a -&gt; b -&gt; a) -&gt; [b] -&gt; a :-&gt; a foldl' f [] = strictly id foldl' f (y:ys) = r `seq` wrap (\cc z -&gt; cc (f z y)) r where r = foldl' f ys All right, now that we have a proper counter-example, we can talk about it :) Let's try again: &gt; It seems to me that in your counter-example, `z` remains a thunk until all of the `[b]` has been consumed and a large function of type `a :-&gt; b` has been constructed in its place. At that point the `z` is forced. Do you agree with the reasoning? Yes, that's what my new counter-example is achieving. It's similar to your own counter-example about Wadler’s strictness monad, in which `z` remains a thunk of type `Str a` until all of the `[b]` has been consumed and a large computation of type `Str a` has been constructed in its place. At that point the computation is executed, forcing `z`. Of course, your counter-example is expressed in a natural style, while mine is hopelessly contrived, so `:-&gt;` seems much better than the strictness monad at enforcing correct implementations.
Fwiw, there's also the concept of a nested/lattice sandbox cache. I've got working `shake`-based code for that to *massively* reduce time and space needed for materialising the dependency sandboxes in a massive Hackage buildbot (which has been running for weeks already). The algorithm maximally reuses already compiled packages (if their configuration is compatible) required by an install-plan, and the nested sandbox cache is a persistent data-structure. The code (or the idea -- it's quite simple actually, it's just tricky to make sure all degree of freedoms are considered) could easily be extracted and either put into a cabal companion tool or a variant of it even being integrated into `cabal-install` (however, I expect the new multi-instance work being done for Cabal to achieve effectively the same, so I don't think it's worth the effort to merge it into `cabal-install` proper)
OP has edited this to &gt; to really understand return, we have to understand its partner in crime, bind. It is possible to suggest something needs improvement constructively, for example "I don't think bind and return are actually dual in the mathematical sense, so perhaps rephrase that." 
Unless `nix` has replicated cabal's install-plan solver I don't think `nix` can replace `cabal install`... and btw, if you're using `cabal` merely as build-tool, you can stop using it altogether, and just call `Setup.hs` directly, which does the actual work of building your code, as `cabal` (to some degree) is mostly a frontend around `Setup.hs`
IIRC, Haskell uses a merge sort, you can see the source code here: https://hackage.haskell.org/package/base-4.8.0.0/docs/src/Data-OldList.html#sortBy Following the links in the documentation, they mention a [heap sort](https://en.wikipedia.org/wiki/Heapsort), the comments specifically mentioning GHC's merge sort was replaced by a better implementation. Heap sort is also O(n log n)
So, you've made a great example "npm install --save". First time I saw this command I thought "what does that save flag do?" So I went to npm website, and saw this: https://web.archive.org/web/20140831064155/https://www.npmjs.com/ That website is very non-intuitive, you expect it to have a "documentation" button at least, don't you? Then you figure out "npm" is a link (not even underlined) in the middle of a description, it gets you to list of packages. It looks completely different from previous page. Still no documentation. Let's try "API" button. Aha! One more differently-looking page with something I'd call far-from-intuitive design. I'm really glad it has much nicer design now, it's just surprising that it was in previous state for so long (as for something being "intuitive" or "easy to use"). I also find bower confusing. Why do you need a package manager if npm is already a package manager? Bower website says: Web sites are made of lots of things — frameworks, libraries, assets, utilities, and rainbows. Bower manages all these things for you. So, why can't npm install frameworks, libraries and other things? Why do you need a package manager inside a package manager? And in real-world you need to "combine" all that with gulp, browserify and other stuff that complicates simple "hello world" app development. I always had an impression of node-world being very popular, but for some reason every time I developed something there I constantly asked myself "how's that possible that node world is such non-modern place to be at?" Of course, this all is very anecdotal, but yeah, I did have a node experience quite frustrating in general and have no idea how most people use it without rage.
really nice! Where are the instructions to get this up and running locally? I see a bunch of links to repositories but some step-by-step instructions would be useful.
You could certainly do the reverse, too, using things like `StrictIdentity`. The question becomes which construction you want without the overhead, or with it. So it doesn't really answer the question of which is better. Of course, you could opt to be polymorphic in it entirely and let the caller choose lazy or strict, too. Better support for something like `deepSeq` might be nice, which injects lazy values into some magical primitive monad along the lines of `IO`. Evaluation is just another side effect, after all, it makes your processor hotter.
I could have explained it better and in a less confrontational manner as well. Sorry about that.
The nice thing about parametricity is that it in some sense works "observationally". You could imagine a language where `f : forall a. a -&gt; a -&gt; a` was literally a function that takes a type as its first argument, and it is allowed to analyze the structure of that type, just any function is allowed to analyze the structure of a cons list. You could say that those functions are not parametric because parametric functions can't look at their type arguments, but I don't think that is the best perspective on it. Whether a function is parametric only depends on the observational behaviour of the function, not on what it's doing internally. It can analyze the type structure all it wants, and it may even compute the result differently based on that analysis, as long as in the end the result doesn't vary based on the type argument. The same goes for univalence.
I am unconvinced. If the function inspects its type but behaves uniformly over it, then it is equivalent (observationally) to a function that doesn't inspect its type. The proof search approach would then still work: you could have a meta-theoretical result saying that any uniform/parametric term is equivalent to a function of a smaller syntactic class of correct-by-construction terms, and do the proof search in that restricted space. (In fact proving that your edgy function is equivalent to a function in a smaller parametric-by-construction subset would probably be a good way to prove, in the first place, that it is parametric.)
For simple examples, yes. But if you have something like ``` type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t ``` for some constant types S, T, A and B, but polymorphic in f, then I'm not sure how your "search" approach would work? (Admittedly this isn't yet covered in part 1; part 2 will.)
I don't think there's anything ominous going on here, it's just a sign of increasing popularity. Haskell has been losing its niche status for better or worse. I like being in an inclusive community, even if it means dealing with more noise. We should take the context of the thread into account (the recent threads on cabal), the objective of the post (constructive questions about what to do to improve the state of affairs) and the history of the user. An overly aggressive response only invites more antagonistic behavior from everyone. We could instead simply suggest renaming the title. Btw, /u/Alyte, that might be a good idea if it's possible to do after you posted (I'm not sure). Lambda-the-ultimate is reasonably successful at keeping things civil and objective by politely correcting behavior. We would do well to emulate them.
&gt; that might be a good idea if it's possible to do after you posted (I'm not sure). Not possible as far as I know. You can change the body but not the title of an existing post, at least not as a regular user (admins are probably able to do so).
&gt; Proof search is essentially enumerating all programs of a given type in a more intelligent way, but this doesn't work for many type systems since it's even undecidable whether a type has an inhabitant at all, and precisely characterizing the entire set of values in a given type is even more impossible. That is correct, but often we don't study the parametricity theorem for *arbitrary* types of our system, but for specific types -- the fact that inhabitation is undecidable in the general case then does not matter. For the example of `forall a . a -&gt; a -&gt; a` above, I was able to easily derive an interesting result, despite the fact that inhabitation is undecidable for System F. (For example, in [Canonicity of groupoid laws using parametricity theory](https://who.rocq.inria.fr/Marc.Lasson/mfps.pdf), 2014, Marc Lasson profitably studies the parametricity theorem of a very constrained family of types that, in particular, are known to be inhabited. Note that I don't know (yet?) whether a similar result could have been obtained by proof search.) &gt; For types for which you don't know if there is an inhabitant you can still get a parametricity theorem. That is also correct, but you may not be able to tell much interesting things from this theorem. In fact, one could argue that the situation is not well-posed. We are able to say that inhabitation is undecidable because we formally consider algorithms that given any type will mechanically answer a given question (is this type inhabited?). Are there mechanical algorithms that derive a property of interest from any type's parametricity theorem? It could be the case that, for your property of interest, the question of "does this parametricity theorem imply the property" is undecidable as well.
cis194 is pretty good, the exercises are quite good and remind me of my CS roots going through the structure and interpretation of computer programs (SICP) blue book. LYAH is superficially user-friendly, but I think the monkey-see-monkey-do teaching style ends up giving the illusion of learning without real learning
The parent post starts out well, but it veers off course. Yes - learn Haskell first, including how to use the toolset in practice. But start out with *mainstream* Haskell. Classy Prelude is interesting, but it is a very different dialect of Haskell than the usual. Before you go off in that direction, become very experienced and confident with regular Haskell. Don't start out trying to show your co-workers Haskell using a non-standard dialect, even if you yourself like it. The same is true for the toolset. Stackage looks very promising, but first learn how to use the standard, which is cabal. Cabal works great if you follow best practices - find out from experienced Haskellers how to use it. (Hint: If someone tries to tell you that cabal is terrible and you should never even touch it, that means it's probably someone who doesn't know how to use modern cabal properly.)
I only skimmed the paper, but is there a function for getting all possible rewrites rather than just one? E.g., a function matchAll :: (Functor f, Foldable f, EqF f) ⇒ LHS f a → Term f → [Subst f]
[right here](https://github.com/gibiansky/IHaskell#run-ihaskell)
&gt; Are there mechanical algorithms that derive a property of interest from any type's parametricity theorem? It could be the case that, for your property of interest, the question of "does this parametricity theorem imply the property" is undecidable as well. Well, as a trivial example, if the property of interest is the type's parametricity theorem itself, then there exists a mechanical algorithm to derive that from the parametricity theorem (the identity function). Yet one would not be able to derive it using proof search unless it can fully characterize the type. Deriving parametricity results from a full characterization of a type is a bit like proving 1^3 + 1^3 != 1^3 in the following way: first, prove Fermat's last theorem, then, get the result using a=b=c=1, n=3.
 :browse Some.Module It's certainly got a LOT of flaws and it would be very nice if I could access the Haddocks from the command line, but I honestly spend more time here. Flaws include: * As noted, seeing documentation from the command line * Being able to browse all modules in a package or "below" some module * Search * GHCi scrolling
Kind of: In the video there is a section about using cabal2nix. It is really easy write a fully automatic script that fetches a specific version of a hackage-package and make it a nix-package via `cabal get`. That way don't event have to recompile the package for every sandbox you have. I'd say that if you get used to that workflow it is as easy as typing cabal sandbox init cabal install ${PACKAGE} You could do something like cabal get ${PACKAGE} --dest-dir ${MYSOURCETREE}/${PACKAGE} cabal2nix ${MYSOURCETREE}/${PACKAGE}/. &gt; ${MYSOURCETREE}/${PACKAGE}/default.nix # some awk or sed script that adds the appropriate entry to the override part of your ~/.nixpkgs/config.nix Should not be to hard for a programmer. This approach only makes sense if you WANT to use nix and work with a reasonably large amount of different compilers versions and package dependencies I guess. Otherwise just don't use a package manager and just stick to cabal sandboxes, should be totally fine for small/tiny projects.
Just for reference in this discussion, here's where I've found Haskell success stories / case studies with some quick searching: - https://www.fpcomplete.com/business/resources/case-studies/ -- generally pretty "polished" case studies, but it varies - https://wiki.haskell.org/Haskell_in_industry -- very big list of companies using Haskell, low on details in general Actually those are the only two significant lists I could find -- though the wiki page is quite long. Any others I missed?
&gt; Unless nix has replicated cabal's install-plan solver I don't think nix can replace cabal install... I've just asked in the #nixos irc channel and it appears that the following process is involved in generating nix-expressions from hackage: 1. Take the latest version of any package from hackage. 2. For each package, use cabal to check whether the version constraints are met. 3. If they are not met: remove upper bounds and try again. Maybe the developer forgot bumping the upper bounds. 4. If it still does not work: manually intervene to figure out which dependency is causing the problem and adding this dependency additionally with a different version (nix supports multiple versions of the same package, they just can't be active at the same time). 5. If after these steps there is not much breakage: make changes available to the public by pushing the new expressions to the nixpkgs git repository. Steps 1-3 are done completely automatically, and especially step 3 seems to make a lot of otherwise broken packages work. While being best effort and involving human interaction, this seems to work reasonably well in practice. I'm not sure how those issues are addressed with cabal, but I guess the choice of versions of dependencies is more sophisticated and based on solving the actual version bound constraints. So the good experiences I have with nix regarding package breakage might be heavily influenced by point 3 and 4. But I agree: it is not an exclusive replacement. Aiming for an automatically and guaranteed dependency resolution seems indeed like a good thing. :)
OK I misspoke, I meant *boring* java devs. Guava is fun and all but you'll still fall into a tarpit of code. Generics and even closures don't cut it. I've gone as far to write a little FP library in java (requiring overloads for every supported arity, wincing/ignoring useless generics errors etc). But where it really gets bad is trying to convince said ennui-embodying javaheads that it's actually *better* to code in this style. Forget selling Haskell to them, they're hostile to different paradigms within Java. I think the best thing to do is find a way to use Haskell and just leave them in the dust.
I think I can help with a few of these. I've been trying to find the time to do a write up but here are a few details. We are by no means at the scale of Facebook or some others, but we are growing and our current problem set matches a few of the bullets outlined. I co-founded a company which builds a web service and API to aggregate real estate data sources (https://simplyrets.com). These data sources are similar to local governments and are notoriously fragmented and hindered by a fairly complex data protocol and specification (RETS). Real Estate data sources are locale unique and normalizing them is difficult due to regulations and minor differences in location specific details (e.g. Miami real estate is often described and sold differently than real estate in New York City, in very pedantic ways) Developers interested in working with real-estate data have a hard time dealing with these sources as-is due to several complications (time-consuming, expensive, too many gotcha's). On top of that, the tools available to real estate brokers and agents is very poor for for the same set of reasons. Each unique data source's models are large and flat, 250-500 fields in a single record. There are over 900 sources in the U.S. and several in Canada. We haven't aggregated them all yet but are continually adding new sources to our backend. Each source potentiall has ~50,000-100,000 active listings at any given time with a few million in historical data. The listings receive hourly updates which means we pretty much have sources aggregating our data at all times. Each listing has 10-50 associate images (which we distribute as well). With the exception of each having their own set of quirks and gotcha's, they look something like this: https://github.com/NationalAssociationOfRealtors/reso-data-dictionary/blob/master/index.js#L2 Our code base is about 1 year old. Needless to say, the abstraction power of Haskell has helped us deal with the fragmented data sources seamlessly. I am constantly /removing/ source lines from first iterations in favor of higher level abstraction and it has been **extremely** robust. We deal with very few bugs on a regular basis, which is awesome. We have found the Haskell tooling quite good for our small team. The only problem we face (which we have found techniques to mitigate) are long compile times. # Benefits of using Haskell ## Haskell is an extremely nice option for a few reasons: * Express invariants across our distributed system using types. This is a _major_ advantage. We can move on to new exciting features without as much baggage and technical debt. * Reduced complexity in our code. Like I mentioned before, we are removing LOC in spite of changing requirements and unexpectedly introduced complexity. * Rapid deployment. We push to production several times a day. If a bug is introduced, we describe the problem in our test, try to introduce the invariant at the type-level, and almost never see a bug twice. * Extremely high code reusability. It's insane, actually. Functor, Applicative, Alternative, Monad are the tip of the iceberg and it becomes second nature to start developing .Extra module for high level combinators specific to our problem. For example, we reuse many of the same functions for XML and CSV parsing for describing possible flaws in the input data. They are very terse and easy to understand. * Rapid Prototyping : One of the best ways I have seen Haskell introduced in a corporate environment is simply for developers to hack on something small like a web service or command-line app in their flex time (or free time if you have it). This is low-pressure and often speaks for itself to management who will then jump at it because it's already built (prototypes become products). Added points if it saves time or money. ## Some Haskell tools we use: * Yesod/Keter * Scotty * WAI * Persistent * postgresql-simple * csv-conduit,xml-conduit * Shake * attoparsec * Clay / Blaze * HSpec (HUnit and QuickCheck, of course) ## Some Non-Haskell Tools: * Postgresql * PostGIS * AWS * Stripe * React * PHP (some wordpress) * We also used Swagger (and the Haskell DSL) to define our API specification. We were able to generate API clients for Python and Ruby (with many more possibilities). * Stackage LTS We honestly don't have a ton of issues with Cabal and Hackage proper but when we do we use Stackage cabal.config's. We do use Stackage LTS, though, as a target for tested/stable production builds. ## Responses &gt; Python and Haskell side-by-side - Transitions from Python to Haskell - GeoSpatial examples One neat service we wrote was our own Geocoder using PostGIS and TIGER data which falls back to Nominatim when the TIGER data is insufficient. We used Docker to encapsulate the entire database build process and wrapped it in a WAI/postgresql-simple web API. I talked about it a bit at the Houston Haskell Users Group and plan to open source it very soon. &gt; Specially stories reflecting how easy is to adapt an existing system to new requirements and changes in agile development and TDD. Each new data source we integrate introduces something unexpected. The systems we connect to have been maintained long-term and are slow to adopt new changes in technology. Again, our technique is to write a test, attempt to introduce the invariant at the type level, and move on from it. It's not that fixing the bugs is not possible in another language, it's that we don't see it escape compile-time again if we have described it properly. &gt; Haskell touts performance/concurrency, so any success stories regarding Haskell project working at very large scales would be valuable. I can't speak on concurrency at an extremely large scale. What I can say is that multi-threading and concurrency is very cheap to reason about, compose, and build upon in Haskell. We rarely have to consider minor technical details regarding our use of concurrency. I come from a C++ and Python background where concurrency was somewhat expensive time-wise and I almost always experienced issues in team settings. &gt; I'd like to see a success story with the tooling, in particular with IDEs :) It is just impossible to sell a "serious" language without it. The tooling is actually quite good. We have no dominant IDE yet, I suppose. But here are some solid options we have used: * EMacs (haskell-mode) * Haskforce IDE plugin : https://github.com/carymrobbins/intellij-haskforce * Codex (using Hasktags) : Generate CTAGS/ETAGS for an entire cabal project and it's dependencies, and your sandbox add-source dependencies. I use this exensively * GHC-Mod * Stylish-Haskell * Of course HLint, Hoogle, Haddock, DocTest, and HPC to some extent. See Haskeleton for introducing all of these tools into your testing suite. &gt; Fast adoption by company personnel without background using haskell that become productive in a short time This has become the toughest part about introducing Haskell to new teams which were only moderately interested, in my experience. More importantly, the prerequisite's seem to be that, developers are highly interested in improving their craft, have some functional programming experience, and are convinced that static/strong typing will help them reduce bugs in their code, saving time and effort. Otherwise, the learning curve is currently steep for many developers with no FP experience to come up to speed quickly. This might become a bit easier as FP and Haskell-like features are continually added to familiar languages. I'm hoping to find the time to due a more detailed writeup soon. I will be happy to provide more details to FPComplete if this would help. 
I've used `:browse` and `:hoogle` before, but I always run into issues and go back to using Hackage. It generally takes me too long to find things in the output. Also, as you said, not being able to browse to the other modules in that package in a problem. Any tricks to make this more effective? What makes you choose this approach over Hackage?
I think what negatively affects its popularity is the lack of a compelling, widely acclaimed useful project written in it. You know, like `xmonad` but bigger and more compelling. Instead, people are using Haskell to write web apps, which sets the bar at the level of "like Ruby on Rails or Node.js but you have to learn all those preprozygohistomorphisms to use it".
I'm a total beginner and I would day this is probably the most helpful explanation I've read on monads. I like that you don't go very deep into theory. Would I be somewhat correct to say that monads are basically wrappers that contain "impure" data and actions with some common functions?
I wish Happstack used something like this. As it is, it seems like the whole server persists as a ghost thread in GHCi and you have to :quit and relaunch instead of just :reload to try a new version. It's mostly just a minor annoyance for me at this point, but I could see it becoming quite a hindrance in the future.
I assume you mean in situations where there is no most general solution (see section 3.3)? There is no such function right now, but it wouldn't be too hard to add if there's a need.
Well, Makefiles, basically. But there was a time before `cabal-install` was ever around, many many moons ago, when developers had to manually install dependency chains for their Cabal packages by hand after `wget`'ing the packages from Hackage. And we lived with this! I remember doing this very often back then - it's been quite a while since those times, though. :)
It's interesting to see you describe Applicative and Monad as "low level concepts" that should be abstracted away.
Hmm, interesting. Even if I try and mess around with everything he gives me? Its been helpful so far, but maybe going through a course might expedite the process a bit.
I'm not sure if this covers the case I have in mind. I'm imagining you have a non-confluent system (so that the order in which you apply rewrites matters and one rule may match several subexpressions). Another example I guess would be playing with different evaluation orders.
I compile everything with `haddock` and `--hyperlink-source`, and I have a Safari bookmark to the local doc index in `~/.cabal/share/doc/`. I haven't been using GHC 7.10 because last time I looked, in 7.10.1 the Haddocks are broken.
To be honest, yes. It is not very responsible to use Haskell in some setups right now. But if a possible failure won't hurt anyone, go for it.
The intended purpose of type classes like `Applicative` and `Monad` is to provide a uniform interface to a wide variety of disparate libraries and application domains. If you abstract them away it defeats their purpose. Also, even if you were to abstract them away, it's not clear what interface you would want to replace them with. Maybe if you suggested a specific interface or syntax that you would prefer when writing a web app it would help clear things up.
Yes! I would never have learned to do useful work in haskell without Dash. It's a miracle.
I've tried and failed. Let me know if you can do it.
/u/ndmitchell, how did you manage to track down the source of the problem? I'm afraid more tracking down might be necessary: I have the github version of ghcid, which contains your Ctrl+C fix, but when I let ghcid run for too long, it still ignores Ctrl+C and I have to send it a SIGTERM to kill it.
No offense, but have you read the original post? Aaron Contorer, the CEO of FPComplete explicitly said that he had replies from 168 potential commercial users for Haskell who stated that sucess stories do mater.
In case you didn't catch it, my guide (strongly) recommends doing NICTA Course but people find it more tractable if they complete cis194 before NICTA course.
Oh, great, someone who knows everything about what makes a language popular. 
Thanks, glad to hear it! There's nothing impure about a Monad (or Applicative or Functor). The list monad example at the end of the article is there basically to unseat the notion that Monad has anything to do with IO or impurity. A Monad isn't necessarily a "container" either. IO certainly isn't. What Monad and Applicative share is their ability to "encode" functions having these types into a larger context; their two-kinded--ness is often there so that we can do *other* things in these functions while still *maintaining* the monadic context which might have nothing to do with the second type. This is certainly true of IO. The monadic context allows GHC to maintain a stable construct of the "world of IO" from one call to the next. (Getting IO to work in a lazy language is no joke. Check out http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/mark.pdf for a surprisingly approachable treatment on what's going on with IO.) Don't worry too much about what a monad is or isn't, worry about how to code with them, which is hard enough :)
While these are interesting and understandable desires, this study is based on the results of a survey of over 1240 people expressing their desires in response to a structured questionnaire. 
Yes, I just wanted an opportunity to say that and I'm not addressing the OP (I'm sorry). I don't know everything about anything, but I do believe that a language can't be popular if the average programmer can't use it. And I know the average programmer can't use it because I see they failing at it every single day. Also, no need to state that since the Haskell community is one of the most mature and I know you can take a negative feedback, but: maybe I'm just wrong. That is my opinion and nothing else. Edit: also I'm deleting the post because I noticed this thread is the worst place for that kind of thing (I'm really sorry!) but if you want to post it elsewhere let me know and I will (and take the downvotes again)!
&gt;Ideally though, it would be nice to bake this into GHC directly, but that's a long term project :) Indeed, that would be really nice. Are you thinking you might look to translate the Python program to Haskell, then figure out GHC integration after that?
There isn't much point in translating this script into a Haskell one. If anything, it would probably end up being more complicated. Ultimately, it would be better to add it directly to GHC as a compiler feature, similar to how GCC/Clang have it nowadays. In fact, there's even [a ticket on Trac](https://ghc.haskell.org/trac/ghc/ticket/8809) from over a year ago.
Yeah, I did. Sounds like I should put it close to the top of my stack of Haskell resources. *Ba-doom chick*
I just looked at the credit card and towers of hanoi exercises. Those look fun, haha. 
Ohh. I've only noticed this message today. Thanks, that's helpful I'll try to add this soon. 
Oh... :-) I think the main difference is that binary only takes lazy `ByteStrings` as input. AFAIK this makes `binary` slightly slower than `cereal`. But don't take my word on that. 
Ah, no there's nothing like that. The only available strategy is: traverse bottom-up and apply the first matching rule to each node. This covers my use case, and it has the advantage of being as efficient as first-order rewriting, because the set of free variables can be cached on the way up. I don't have much experience with term rewriting in general. If you have ideas for improvements, I'd be happy to incorporate them.
The associated code is here: &lt;https://github.com/emilaxelsson/ho-rewriting&gt; I plan to put it on Hackage at some point.
&gt; does not require your code to always typecheck or compile That's the main issue I have with the current tooling. When I need to ask my tools for something, chances are that my code is currently not in a working state. For instance I can't infer types with hdevtools while I'm actually writing the code, but that's when I need the feature the most. I've resorted to abusing typed holes and ghci for this but it's nowhere near optimal as a workflow.
Minor correction: use "bottom" instead of "undefined", because undefined is not a set of values, but an "alias" for error "Prelude.undefined". "Bottom", however, is what pattern matching is defined with (IIRC). (I know "set" sucks, but type, kind, sort, class are all taken)
Thanks to [this pull request](https://github.com/bgamari/hoogle-index/pull/7) `hoogle-index` now supports sandboxes. Usage is described in the [readme](https://github.com/bgamari/hoogle-index#sandbox-support).
Thanks so much !!! Im really a Haskell beginner :-)
thanks !
It's in Python. :(
Added a link to this on the [Haskell wiki](https://wiki.haskell.org/Safe_Haskell). Also cleaned up that page a little, 'cause *jesus* it was badly formatted.
Some other things you might do (that I would personally like) are - bold the line number (usually I don't even read type checker errors and just look for the line) - add whitespace around the filename so I can double click to select the text for middle-click pasting (maybe I'm the only person who does this a lot)
In most of my benchmarks **binary** was much faster than **cereal**, despite the lazy bytestring. edit: some of those benchmarks can be found in [packer's benchs directory](https://github.com/vincenthz/hs-packer/tree/master/Benchs); I've got some other benchs which are not part of this, but they all show roughly the same thing anyway.
Thrift is easy to work with once you see the implementation generated from the interface (easy to understand code, even if autogenerated). However I've posted this before on this subreddit the [Haskell implementation is slow](https://issues.apache.org/jira/browse/THRIFT-2236). Someone from the Haskell community with necessary knowledge could be nicely asked to optimize the implementation.
I've got a friend of mine which started to learn Haskell on is own last month, without asking me any advice. He tried to use twitter-conduit. And guess what? He fall in cabal hell. He call for my help. In the end, he uses cabal sandbox now and I believe he is happy like that. I always said cabal is great. The problem is neither cabal nor hackage. It is the fact that (until stackage) there were no "stable" version to make all package compile together. And stackage resolve this problem. If you want the bleeding edge version of some lib you could always try it with stackage by modifying the version manually in your cabal.config. For your other arguments: - fold is wrong : fold 0 + [1..1000000] =&gt; stack overflow Beginner should be told to use foldl' except on infinite list where they should consider foldr. I good move would be to replace fold and fold' in Prelude IMHO. - String are wrong: you should always use Text and Bytestring if you want to do some serious things. So while you are right to call `foldl`, `String` and `cabal` standard. They confused me. And this is why I prefer show the problem to beginner before they hit them. In fact I was really upset when I discovered that some people had chosen these standard instead of more secure one "to help beginner". While in my experience it was the complete opposite. There is no "best" way to learn Haskell. And I hope not to confuse beginner with my advices. But it would have been best to have chosen better standards at first. Seriously cabal hell almost made me quit Haskell. I couldn't easily convince people around me to use it. Just because making a dev environment was hardly reproductible. In my opinion all beginner should use stackage and should be warned by the problem they could encounter with Prelude.
Interesting stuff! Have you looked at using a [free applicative](https://hackage.haskell.org/package/free-4.12.1/docs/Control-Applicative-Free.html)? I ask as I'm curious if it does help save you writing code, or if it's fairly inconsequential. One area it might be nice is the ability to change the functor that you're working under with `hoistAp` - which means `unsafeEvaluate` could go away, if you were able to "evaluate down" to `Identity`. At that point, your `unsafeEvaluate` would just be `runIdentity . retractAp`
&gt; ...a friend of mine which started to learn Haskell... He fall in cabal hell... In the end, he uses cabal sandbox... That is not cabal hell - that is using cabal wrong. Any software does bad things if you use it the wrong way. We need to improve our documentation - on haskell.org and wherever else beginners will see it - to give correct information about how to use cabal. And we need to change the UI of cabal to do the right thing by default, or at least give useful messages about what you should do. (e.g., `--require-sandbox` should be the default). &gt; Beginner should be told to use `foldl'` Ah, that's what you mean by "wrong". Yes, I agree, it is a bad default. (but most of the time `foldl` does work in practice) The problem is that `foldl'` only solves the problem in simple cases. Usually you need to apply strictness deeper than that - but not *too* deep. It's a subtle problem with no simple solution.
Correct me if I'm wrong, but isn't the runtime really bad for many distinct priorities? Because it has to traverse the structure once for the minimum and once for the monadic sequencing, for each distinct priority value.
Yes, it's awful! There are ways to speed it up, as I mentioned in the footnotes, but I wanted to keep the blogpost as clean and simple to understand as possible.
Since `_|_` is an inhabitant of every type, is it proper to say that `data Void` has one inhabitant? Or does `_|_` not count as a "value"?
Your problem was too amusing to let go with just type signatures, so I gave in and wrote some self-contained code. Note how only `schedule` needs `Ord p` and only `resolve` needs `Monad m`, separation of concerns. {-# LANGUAGE GADTs #-} data Prio p m a where Pure :: a -&gt; Prio p m a App :: Prio p m (a -&gt; b) -&gt; Prio p m a -&gt; Prio p m b Prio :: p -&gt; m a -&gt; Prio p m a data PureAp a where Pu :: a -&gt; PureAp a Ap :: PureAp (a -&gt; b) -&gt; PureAp a -&gt; PureAp b data Request p m a where Done :: PureAp a -&gt; Request p m a Request :: p -&gt; m b -&gt; (b -&gt; Request p m a) -&gt; Request p m a schedule :: Ord p =&gt; Prio p m a -&gt; Request p m a schedule (Pure a) = Done $ Pu a schedule (Prio p m) = Request p m (Done . Pu) schedule (App a b) = app (schedule a) (schedule b) where app r1@(Request p1 m1 k1) r2@(Request p2 m2 k2) | p1 &gt;= p2 = Request p1 m1 (\v1 -&gt; app (k1 v1) r2) | p1 &lt; p2 = Request p2 m2 (\v2 -&gt; app r1 (k2 v2)) app (Request p m k) r2@(Done _) = Request p m (\v -&gt; app (k v) r2) app r1@(Done _) (Request p m k) = Request p m (\v -&gt; app r1 (k v)) app (Done a) (Done b) = Done (Ap a b) resolve :: Monad m =&gt; Request p m a -&gt; m (PureAp a) resolve (Done a) = return a resolve (Request _ m k) = do v &lt;- m resolve (k v) The efficiency of `schedule` is rather debatable: it does not sort fairly but according to the static applicative structure (so in particular a highly-skewed tree will behave badly, perhaps as bad as yours in the worst-case), and I expect the constant-factor bookeeping to be rather high.
Just for information of those who doesn't know, if you compile your project inside emacs via `M-x compile`, it will also colorize output, and will also let you navigate up/down by `M-p/n` and go to error-file by just hitting `Enter` when on it.
It's perfectly valid to say that but people often like to ignore partiality.
Ah, I commented below your blogpost before I saw the discussion here. I can understand your decision, but I think you should at least mention the free applicative, for people who want to dive a bit deeper.
It's not! I just think we should be a little cautious about bragging that Haskell doesn't share this problem because it's inherently immune. The truth is, Haskell has postponed this fate because people have worked very hard at it, but it remains a distinct threat.
&gt; I'm not referring to just things of type IO () but also String -&gt; IO () etc. From context, it is clear that you when you say "function", are talking about things of type `IO ()`. The only thing the term "IO function" does in this context is mislead. &gt; I'm impressed that you want to quibble with calling IO actions "impure". That was not the quibble I was trying to quabble, but sure: IO actions are not impure. &gt; I am using a decidedly stylized and informal notion of "effects" but I think it's warranted for the context. Well, as far as I can tell, you are using a notion of "effect" that is indistinguishable from "is a monad", which rather robs it of its expressive power. Instead, shoehorning every monad into this "effect" model requires gymnastics like "Non-local control flow (Maybe, Either)." This sort of thinking is characteristic of having succumbed to the monad tutorial fallacy.
Thanks!
IMHO the success of Ruby and Scala is due to some products and services: rails and twitter. rails for perceived ease of use, twitter for confidence. In the case of Haskell, the key for success could be some outstanding EDSL for easing some general IT problem and a well known site that may use it. The need of IDEs, toolkits, advertising how fine the language is etc sounds to me like following the marketing of all these products with visual aesthetics and dozens of screenshots and features on paper that we see in advertising and nobody uses. Nobody switch to another language for having a nice IDE or for abstract computer science. Ruby had no IDE and no theory. The kind of people that are prescriptors in the companies do not consider these details. They are seduced by a single _practical_ feature that no other language has. Something that let them say at first glance: "Wow this changes everithing. Makes my work easier!!" 
One great thing to add to protocol-buffers would be some way to generate lenses for fields, which greatly simplifies manipulating heavily-nested protobufs (which are fairly common in practice). I hacked up a script to add lenses after the fact (example usage at [1]), but it would be great to have something like that generated automatically. [1]: https://github.com/ajtulloch/dnngraph/blob/master/NN/Examples/AlexNet.hs
I'm not sure what to make of the following factorization. data Request p m a where Done :: a -&gt; Request p m a Request :: p -&gt; m b -&gt; Request p m (b -&gt; a) -&gt; Request p m a type ReqCont p m b1 b2 c = (b1 -&gt; b2 -&gt; c) -&gt; Request p m c schedule :: Ord p =&gt; Prio p m a -&gt; Request p m (PureAp a) schedule (Pure a) = Done $ Pu a schedule (Prio p m) = Request p m (Done Pu) schedule (App a b) = app (schedule a) (schedule b) Ap where app :: Ord p =&gt; Request p m b1 -&gt; Request p m b2 -&gt; ReqCont p m b1 b2 c app r1@(Request p1 m1 s1) r2@(Request p2 m2 s2) k | p1 &gt;= p2 = cons p1 m1 s1 r2 k | p1 &lt; p2 = cons p2 m2 s2 r1 (flip k) app (Request p m s) r@(Done _) k = cons p m s r k app r@(Done _) (Request p m s) k = cons p m s r (flip k) app (Done a) (Done b) k = Done (k a b) cons :: Ord p =&gt; p -&gt; m a1 -&gt; Request p m (a1 -&gt; b1) -&gt; Request p m b2 -&gt; ReqCont p m b1 b2 c cons p m s r k = Request p m (app s r (\f1 b2 a1 -&gt; k (f1 a1) b2)) resolve :: Applicative m =&gt; Request p m a -&gt; m a resolve (Done a) = pure a resolve (Request _ m s) = resolve s &lt;*&gt; m PS: I think it reorders the effect in such a way that, if you convert a `Monad` into an `Applicative` by forcing the argument first and the function second, you get the effect performed from highest to lowest priority, and if you force the function first and argument second you perform them from lowest to highest priority.
Ooh, nice.
The cereal vs attoparsec results are interesting and worth bearing in mind. The code looks so similar, but the performance difference is dramatic.
Does the inside have to be a Monad? If the inside is a MonadPlus, can Prio be an Alternative? 
It seems daunting at first, but I actually find it makes things easier - at least that has been my experience in the long run! Ever since I read the Essence of the Iterator pattern, my appreciation for building functionality incrementally has only grown. I understand it requires a little bit more of a base knowledge, but the agility you get from working with free structures, or building things generically (e.g., out of polynomial/strictly positive functors) is rather remarkable. The real benefit in this case is the ability to strip out all the non-`Prio` noise - so you can focus on just the interesting stuff. If `Prio` is code you're using at work, I'd encourage you to help train the rest of your team to be able to work with free structures - in this case the free applicative - you'll be amazed at the knowledge and flexibility that comes with it :)
This reminds me of a slightly different construction which we were discussing a little at LambdaConf this weekend. You can generalize the probability monad `[(a, p)]` to an arbitrary Semiring `p`. Choosing `p` to be the tropical (`min`, `+`) semiring gives an interesting "priority queue monad" in which tasks and their priorities can depend on parent tasks. I wrote it up [here](https://gist.github.com/paf31/8e9177b20ee920480fbc#day-20---purescript-algebra).
&gt; hackage is hard to explore, not very pretty, has lots of missing documentation due to a flaky builder, and has downtime issues Honestly, now that we've moved to rackspace, we've had some scheduled updates requiring reboots that could be better communicated, but unanticipated downtime/crashes haven't been an issue. There has also been tremendous work done on the builder, though there are still issues. The distributed build reports really need a "last mile" of work from anyone willing to devote a chunk of time and brainpower, and they'll then do a world of good. Other than that, though, I think your remarks are on point :-)
So you mean to say that this is a happy accident in GHC’s parser?
&gt; so I can double click to select the text for middle-click pasting (maybe I'm the only person who does this a lot) You're definitely not.
You need `RankNTypes` to write `String -&gt; Read a =&gt; a`, if that's what you mean (and I don't really know why anyone would do it, but whatever). It's technically rank-2 as written, even though it's equivalent to the rank-1 type of `read`—and normalised to it by GHC afterward: λ» :set -XRankNTypes λ» let f :: String -&gt; Read a =&gt; a; f = read f :: String -&gt; Read a =&gt; a λ» :t f f :: Read a =&gt; String -&gt; a btw, I'm still on GHC 7.8 so I can't check, but the changes to 7.10's `forall` rules probably mean that's deprecated or disallowed now anyway: it probably only worked for the same reason that `b -&gt; (Ord a =&gt; a) -&gt; c` was equivalent to `b -&gt; (forall a. Ord a =&gt; a) -&gt; c`.
The lack of this is what annoys me the most in my day-to-day type level haskell. Edit: Autocomplete
I currently use `M-x haskell-compile` which supports more of GHC's message syntax including `-ferror-spans`
Which editor btw?
/u/hrothen said: I'd like to see more material on properly using tricky libraries like `vector`. Whenever I'm writing anything nontrivial (and even trivial code sometimes) I always end up worrying about whether it's going to properly fuse or blow up the stack, and about what does and doesn't affect performance. For instance, say I'm using `JuicyPixels` and I want to extract the underlying vector of `PixelRGBA8` from an `Image`. The pixels are actually stored as a `Vector Word8`, so the two simplest ways to get them would be `map (\i-&amp;gt;PixelRGBA8 (v! i) (v! i+1) (v! i+2) (v! i+3)) (enumFromStepN 0 4 ((length v) / 4))` or the much shorter `pixelFold (\a _ _ p-&amp;gt; snoc a p) mempty img`. Do these two functions have the same performance? I suspect so, but without manually inspecting the generated Core it's hard to tell. Is there a better third way? I don't know, I could cast the underlying `ForeignPtr` to `Word32` and map `unpackPixel` across that, but is it faster? And this is for a fairly trivial case.
/r/haskelltil is another good place to post stuff like this to
We use both protocol buffers and thrift. Thrift is used for its RPC capabilities. We store a large amount of data in protocol buffers. We also built the protobuf library (https://hackage.haskell.org/package/protobuf). Some samples are included to get you going. I have not found thrift to be slow enough to warrant fixing it. Our bottlenecks are around database calls not serialization. Do you have more specific questions?
Emacs. When switching between files that I already have open, the autocompletion allows me to type only a few characters.
Yes, the advice I got here is really helpful and exactly what I was looking for! Thanks /r/haskell :) I'm working on [#155, Counting Capacitor Circuits](https://projecteuler.net/problem=155). For some reason this one's really stopped me in my tracks. I've since moved on from this approach, but my attempt was to store in a stateful map the number of circuits that could be formed using a certain number of capacitors in each of *two essentially different ways*: those that are fundamentally parallel and those that are fundamentally in series. So I needed a stateful Map Int [Values] for series and and a stateful Map Int [Values] for parallel. The computation of one depends on the other, so they needed to be in state at the same time. I've actually used State in a pretty good portion of the problems since around #100. I might not be being creative enough but I intuitively see maps as the way to reduce duplicate computation, maybe you have advice on asking Haskell to remember certain values?
Idle thought: would a surjective type family make sense / be useful? One that targeted each type of a kind created with DataKinds, for example.
If you are solving the later problems then that is completely understandable. I guess I just misread your post. I usually see people solving the earlier problems and just assumed you were as well.
Side question: Are you doing your analysis with haskell? Sadly there doesn't seem to be a lot of material out there on it and a quick look at your survey shows that there is interest for such things. So if you do use haskell, it would be awesome if you could make a blogpost or just publish the code you are using.
http://michaelxavier.net/posts/2014-04-27-Cool-Idea-Free-Monads-for-Testing-Redis-Calls.html
&gt; Monadic IO is used to sequence operations Not really. `(&gt;&gt;=)` / `(&gt;=&gt;)` are ordered in the same way that `flip ($)` / `flip (.)` are -- via data dependency. `join` and `(&lt;*&gt;)` are not really ordered, at least not always ordered left-to-right and top-to-bottom. For example, the reverse state monad is "backwards" and the tardis monad goes both directions.
Ah, I understand now. For some reason I kept thinking that proof search had to be done automatically.
Hi, I'm doing the data analysis in Haskell as part of my job. I am currently experimenting using the [Frames](https://github.com/acowley/Frames) library to do the data munging, and also the R interop [H](https://ifl2014.github.io/submissions/ifl2014_submission_16.pdf) to do more fancy analysis and plotting. H is currently scheduled for release in the near future, along with an Jupiter notebook walking through the survey analysis. It would be nice if the Haskell ecosystem had more support for data analysis, but I believe H can really start to provide the best of both worlds with a huge ecosystem already there.
For my specific use case, I intend the data store to be immutable, so order wouldn't matter.
What are some of the examples which motivated injective type families? 
I realize that I should've put this in my original question, but I intend the key-value store to be immutable, so results can never be stale.
Even if you intend the data to be immutable, you'd also have to ensure that your program can never observe values being populated. And even if you somehow manage to satisfy that constraint, most people would argue it is still worth tracking the effect in the types, since accessing a remote (even immutable) data store is the sort of thing one usually wants reflected in the types. As others have mentioned, using IO doesn't force you to completely sequentialize your computations, you're free to do things with as much concurrency as you think makes sense for what you're doing. That said, you may find this article interesting: https://pchiusano.github.io/2014-05-21/what-effects-are-worth-tracking.html That article should not be taken as an endorsement of using `unsafePerformIO` for your use case. :)
My recent [stable-heap](http://hackage.haskell.org/package/stable-heap-0.1.0.0/docs/Data-Heap-Stable.html) library has this instance. I put it up long before it was in a state resembling completion. I intend to flesh it out later.
Neat!
Segmented memory FTW. :-) Alas, while Windows has moved forward, it seems we have some catching up to do. I hope people find these high quality data points interesting and motivating.
I've done a bunch of things, but the one I get the most use out of by far is a little database to keep track of my books ([read-only demo](http://www.barrucadu.co.uk/bookdb/), [source](https://github.com/barrucadu/bookdb)). It's actually gone through a few iterations. It was the first non-trivial thing I wrote in Haskell when I was first starting out, although it used Yesod back then. After a little while I wanted to extend it, but had no idea how to change things, so I gave up and rewrote it in Python. After becoming significantly better at Haskell over a few years, I rewrote it using a little web framework I'd written to produce something else. Most recently, I have dropped that framework as an explicit dependency, and now it's just running on top of wai and web-routes.
&gt; `happy` accident I bet someone would've already filed a bug report then :)
title sounds really weird out of context :)
That's an excellent point. Not having seen any such system in practice, I reserve judgment :)
&gt; This could come in handy as a nice way to separate out class constraints vs equality constraints, or to generally group constraints together in big type signatures. I personally would just use `ConstraintKinds` since I'm a ~~GHC extension whore~~: type ReadShow a = (Read a, Show a) f :: ReadShow a =&gt; a -&gt; a f = read . show
Another extension I didn't know about! :)
I wrote a browser based solitaire game ( compiled using haste ). Follow this link to play it and for more information: http://dc25.github.io/myBlog/haskell/haste/2015/05/08/a-solitaire-game-written-in-haskell.html Here's a link to the github repository: https://github.com/dc25/solitaire 
It's pretty ugly, but I wrote a little utility for dwarf fortress players a while ago. [It turns images into quickfort blueprints.](https://github.com/Hrothen/dorfCAD/tree/v1.3)
Interesting. Haskell gets a little corporate sponsorship here and there. Microsoft has off-and-on, mostly through Microsoft research, supported some Haskell projects. Still, I'm not surprised that Windows is the least supported. Windows is a very strange platform and therefore hard to support.
All of the `_Foo` combinators follow the logic that they act like 'constructors'. `foo^._Left` -- extracts what is in the Left constructor, and works as a legal traversal of just that one constructor. `_Left # 3` constructs a `Left 3` `_Wrapped` is an `Iso` and so follows this constructor-like scheme. `_Wrapping Sum = _Wrapped` is a replacement for what would be `_Sum` which would be a constructor and hence match on the `Sum` constructor and extract the value. All isomorphisms and prisms are done this way because "prisms" _have_ to be done this way and it is the most consistent style we can offer. Whenever we really want to follow the other convention where its acting like a `Lens` that happens to be an isomorphism (and wire things up to follow your intuition here) we use a lowercased name. That said, I'm happy to take documentation patches that improve the state of documentation in `lens` or any of my other libraries.
If the values in the key value store really are read-only and persistent, then using `unsafePerformIO` shouldn't change the computation. It should always produce the same result. But... Something that may still be observable are the errors and exceptions. The middle ground here is `unsafeInterleaveIO`. You'd still be saddled with an `IO` type but you'd remove the sequencing of IO.
In what ways do their responses differ from not-Windows users?
A very common theme in these responses could be summed up by saying "We want assurances about risk." Specifically, the risk of adopting a technology that they don't currently use, Haskell.
Very exciting, especially GHCJS support :) But for me on MacOS, the main window loads (after the setup dialog) and then immediately closes without any error message.
I'm really glad about the improved build instructions for OSX. Anytime I managed to make `$ cabal install leksah` succeed before left me in a mixed state emotionally. On one hand, I'm glad it finally worked. On the other, I should have spent my luck at vegas instead. However, it's a pity the instructions are macports based rather than homebrew based. Also, how feasible would it be to have leksah on stackage?
Sadly Homebrew does not have a recent WebKitGTK+ yet (correct me if I am wrong). I would be happy to have leksah in stackage. I imagine the first step is to get the other Gtk2Hs packages added (I think cario is already in). I don't use stackage myself, but only because it might hide breaking changes in hackage that I should know about and fix in Leksah. We should really be relying more on travis-ci for that.
Thanks, I already looked a bit at Frames. Imho it's very promising.
Not very relevant to the discussion, but writing a breadth-first traversal of a binary tree is a fun exercise. data Tree a = Leaf a | Node (Leaf a) a (Leaf a) val :: Tree a -&gt; a val (Leaf x) = x val (Node _ x _) = x toListBF :: Tree a -&gt; [a] toListBF tree = val &lt;$&gt; fix (\trees -&gt; tree : gen trees) where gen :: [Tree a] -&gt; [Tree a] gen (Leaf _ : _) = [] gen (Node x _ y : zs) = x : y : gen zs &gt; toListBF (Node (Leaf 1) 2 (Leaf 3)) [2,1,3]
Haha, the thought of some dorky Haskeller at an A$AP Rocky concert... do you guys also throw sweet private Edward Kmett seminars?
Even if your store is immutable within a run, I guess it won't be between to runs, or to between my machine and your machine. So you can't write tests checking that `get "hello"` returns let's say `5` all the time. That's breaking referential transparency. That might no be a problem for function obviously calling your store, but what for function like f :: Int -&gt; Int f x = (get "hello") + x How do you know that this function return a different results depending on the environment and it's not testable ? You need to mark this function as *not pure*. IO is a way of doing it. If what's bothering you with IO is the fact that any function accessing the store can do anything (like launching nuclear missiles) you can build your own monad (using MonadIO) which only allows to access the store. This way all functions accessing the store directly AND indirectly will be tagged as "accessing the store" and won't be seen as full IO. Alternatively, if the number of keys is relatively small, you can read all of them upfront, store them in a map, and just pass the map around.
I wonder if those who request improvements in the package management and cabal could be helped a bit, if there was more documentation aimed to describe common workflows. I.e. whether those ppl aren't to some degree complaining about lack of good documentation which would bring more awareness of all the facilities that Cabal/cabal-install already provides (even if its UI has quite a few rough edges if you start naively or with wrong assumptions using it w/o proper understanding -- reminds me of Git where it's easy drive yourself into a "Git Hell" corner if you don't take the time to acquire a mental model of how Git works). I consider Cabal as revolutionary as Git was for SCMs in describing package dependencies (and Backpack goes even further), and we're still trying to figure out how to make it work w/o dumbing it down to what everyone else does.
The Haskell community is so clever with puns. I’m unreasonably glad that someone got it. :) Hints: Hackers know that `happy` is a Haskell parser generator—used in a certain compiler. Painters know that “There are no mistakes, only happy accidents”. 
One example that can come up is e.g. in DB-Libraries a type family mapping an entity type to its key type or to the sum type of unique constraints. Given a key it would be useful if the compiler could in turn infer the matching entity type but currently that is not possible since it can not assume the Key type family is injective.
(Animated) screenshots or some screencast would be nice to have (unless I missed them)!
Not only is it very strange in many ways (sometimes to the point where it seems like they are going out of their way just to be incompatible with everyone else), it also seems to have a culture that often leads to many people demanding improvements with few people contributing them compared to other platforms. I have heard of differences in user/contributor factors as high as 1 to 10 compared to the same factor on other platforms.
I wrote a calculator, which also has a plotting component. Take a look here: https://github.com/sumitsahrawat/calculator
And then there was that day wasted on using regexps for something that I did in Haskell with parsec the next day in an hour...
How can you use parsec for doing e.g. regexp-based string replace?
My primary issue with Haskell on Windows, by far, is that packages using any third party C/C++/etc libraries are often extremely hard to build, if not impossible (btw if you make a binding to a moderately sized C library, consider including the C sources in the package; then a simple cabal install works on Windows too) Also generated haddocks on Windows are most of time broken (individual packages are ok, but the links to them are broken).
There is also a significant cultural difference between OS X and Linux. It does not seem to have the same problem with contributors though.
The IO type and the IO monad are two different things, just as the Maybe type and the Maybe monad are two different things. One does not imply the other. As others have pointed out, tag functions doing IO with the IO type, and concurrently run these functions using a concurrency library.
1. Yes, I think so. I can't seem to write `runPrio` when `m` is only `Applicative`, but I didn't delve in it to deep, so there might be a smart trick which I'm not seeing. 2. Yes. In that case you could use another `Alt` constructor. The question is though, how do you interpret it in `runPrio`. I guess if you have `x &lt;|&gt; y`, it only executes either `x` or `y`, depending which one has the lowest priority. This makes the interpreter quite complicated though.
If you are using the 64-bit GHC, FFI is pretty broken right now when used with random vanilla DLLs floating around out there. But it works fine with DLLs produced by mingw-w64 builds. So the solution is: 1. Install MSYS2. 2. Use pacman to install the third party library you need (use the ming64 flavor of the package). Now your cabal install works just like on any other platform. If you want to limit yourself to 32-bits, it could be MinGHC would help. I never got it to work, but I never tried too hard, and some people have reported being successful.
Rather than downvoting - I don't think /u/dagit was being "not constructive" - let's discuss whether the post is self-promotion. While this is in fact a link to /u/FPGuy's own site, I do think the post meets the requirements in the FAQ for not being self-promotion. It achieved its purpose of sparking a useful community discussion about an important topic.
Why only educational? If it solves the problem you describe in your blog post, it can be reused.
&gt; My primary issue with Haskell on Windows, by far, is that packages using any third party C/C++/etc libraries are often extremely hard to build, if not impossible This is pretty much why I stopped using Windows a decade ago. It's hostile to developers.
Some strict languages (OCaml, Scheme) allow you to do recursive bindings of non-functions. Under the hood it will basically do what you suggest with a mutable cell. Of course, this only works if the recursive use is suitably guarded (like under a lambda). If your language does not support this, then you have to turn `loop` into a function, `loop () = ... loop () ...`.
For most simple stuff I don't see why lazy reading is a problem. I use it all the time.
/r/haskell has a bit of a different culture. People frequently post their own blogs and we're fine with that. Even corporate-sponsored blogs like FP Complete are warmly welcomed.
Unless I'm missing something or misunderstood cabal2nix, when you run cabal2nix on the package from cabal get it will just set the dependencies by name, it will ignore the constraints on the package. The issue arises when the package only builds with versions of packages different then the ones in nix, in that case you can't just use cabal2nix directly and ignore the constraints, you need to first figure out a set of versions of all dependencies that works, then get one by one with cabal2nix, import them all to the ~/.nixpkgs/config.nix and finally make the default.nix file of the initial package depend on the manually created versions of the packages. This is a long and boring process. Perhaps this process could be automated by creating a cabal sandbox, running cabal configure and then use the versions determined by cabal to have cabal2nix automatically create default.nix files for all those versions and then finally create a default.nix for the initial package depending on those specific versions. I know I can just use sandboxes, but sandboxes use a lot of space as they don't share their results (by default), and once you get used to the nix workflow it's very hard to go back... it's that good.
Using a given operating system is not always a choice, so please (in general, not necessarily just you) stop advising that everybody should use Linux for Haskell development. Windows is a very important target. I use all 3 major operating systems, and **even more importantly, people I write software for also use all kind of different OS-es**, and I want to be able to continue to do so. Furthermore, exactly the same way Windows is hostile to developers, Linux is very hostile to users (yes, you hear me, it's not that much better today in 2015 than it was 10 or even 15 years ago). Now, I'm both a user and a developer, as I guess most of us are (and thus personally I prefer OSX, at the moment at least; but again, it's not always a free choice).
As far as I'm aware, nix does not support/work well on Windows. And recently, as I tried to use it on OS X, I was met with some issues. As long as nix is not portable, nix will not be a solution for the company I work for. The most promising alternative is halcyon by /u/myetech, will probably save my/our lives here, though after a quick dive yesterday I was met with… some issues (will dive deeper and explore it very soon).
I tried to play this but I wasn't able to; I think it's failing to load on my machine because github appears to be forcing https now, and Chrome is therefore refusing to load d3 over plain http.
I'm currently working on [Tila](https://tila.io) which is a SaaS project management tool. The back-end is Haskell (scotty + postgresql-simple) and the front-end is written in CoffeeScript (but I'm thinking about re-writing it in PureScript at some point).
&gt; Using a given operating system is not always a choice, so please (in general, not necessarily just you) stop advising that everybody should use Linux for Haskell development. Windows is a very important target. I'm not advising anyone. Windows speaks for itself. It's awful and as a result nobody wants to touch it. I stopped using Windows a couple years before I started with Haskell.
I am working on a [library] (https://github.com/boundedvariation/quantfin) for pricing derivatives in a Monte Carlo framework. It's been fun.
Both OS X and Linux are in different ways based on Unix. Windows isn't. Criticising it for being different is like criticising your cat for being so unlike the other pet dogs you have. If your computer comes with standard screws in the casing, you see things differently to when it comes with warranty-void-if-removed stickers. You'll see large cultural differences between Windows users who stick with Internet Explorer and those who use Firefox or Chrome. Programmers who use Linux or OS X are probably more independently minded than Windows programmers, and more eager to edit something they feel needs change. My point is that Windows users are more likely to see tools as things someone else provides. 
I am currently computing the probability (and optimal solution) of finishing 2nd tome of the Lone Wolf gamebooks (can be accessed for free at http://www.projectaon.org/en/Main/Books), given a range of starting rolls, equipement and discipline choices. This is a lot of fun.
Maybe I did something wrong, but I wrote a tool that traversed a directory operating on each file and got burned as it ran out of filedescriptors... :(
Implementation of SENNA - a unified architecture for natural language processing. It's 3000 lines of C (original implementation), a lot less in Haskell, I'm improving the performance currently, I'll publish the repository (with whole history) when I'm done.
The way that Linux is bad for users is when device drivers don't work properly. Sound issues, Flash issues, wifi, things like that. Bugs aren't part of Linux's design philosophy. Nor can Windows's design philosophy be blamed for its virus-ridden, slows-down-over-time, comes-with-bloatware, BSOD issues. Those aren't part of the Windows philosophy. Otherwise my non-technical dad uses Ubuntu happily and all I did was install it as dual boot, didn't explain anything about it.
very impressive
Just a digression: wouldn't a language that supports both data and codata (http://blog.sigfpe.com/2007/07/data-and-codata.html) get the best of both worlds? Is there some reason (other that "no one got around to implement it", I mean some deeper difficulty) why all languages seem to provide, at best (for example Idris or Oz, or Haskell with strictness annotations), distinction between lazy and strict values, but not types? I mean, why there isn't a language where something like this: data List a = Cons a (List a) | Nil codata Stream a = Cons a (Stream a) | Nil is legal, and any List is sure to be finite, while any Stream can be infinite?
Typically the purpose of running code with type errors deferred is to test the refactoring you just did on a small component of the system before refactoring the rest of the system.
Absolutely. What I am asking about is like @c_wraith's comment below - the typical usages of deferred type errors. In dynamically-typed languages, running the program is the main means of debugging - you inspect the state of the system, step through, etc. In Haskell, the need for doing this is greatly diminished thanks to the types. But still there are cases when you might quickly check something, run a test case, etc. before you make the rest of the program consistent with the breaking change.
The majority of one niche market (desktop PCs) perhaps. Certainly nowhere close to the majority in the vast majority of the markets for computing devices with operating systems. 
## Summary Regarding users not into Windows and not into .Net: - Only 52% are worried about debugging &amp; profiling, vs 71% of Windows users - Only 42% are worried about IDE, vs 65% of Windows users - Only 39% are worried about deployment, vs 54% of Windows users - Only 33% are worried about installation, vs 46% of Windows users - 34% of nonstudents are at a workplace that uses Haskell, vs 17% of Windows users ## Details To get a clear answer to your question I'm omitting the moderate users -- those who rated Windows or .Net improvements as only _helpful_ (the middle of the rating scale) or _don't know._ Without them, leaving 379 v1.0 respondents who said *both* Windows and .Net improvements are _no impact_ or _sight help,_ here are their answers to the same questions (biggest changes boldfaced): - Package management &amp;amp; cabal. 34% _crucial_ + 27% _important_ - Debugging &amp;amp; profiling. __23%__ _crucial_ + 29% _important_ - Documentation &amp;amp; learning resources. __27%__ _crucial_ + 32% _important_ - Colleagues' perception of Haskell. __30%__ _crucial_ + 26% _important_ - IDE improvement. __19%__ _crucial_ + __23%__ _important_ - Ease of writing high-performance programs. 21% _crucial_ + __28%__ _important_ - Availability of skilled personnel. 22% _crucial_ + 27% _important_ - Deployment features. __12%__ _crucial_ + 27% _important_ - Installation. __13%__ _crucial_ + 22% _important_ - Application templates, frameworks, design patterns, reusable samples, etc. 16% _crucial_ + __25%__ _important_ - Scalability to large datasets. 19% _crucial_ + 24% _important_ - Server-side web application support. 19% _crucial_ + 23% _important_ - Thorough support for math or data analysis (as you might see in Matlab, R, or NumPy). 16% _crucial_ + __17%__ _important_ Especially notable are the reduced importance of improvements in __debugging &amp; profiling, IDE, deployment,__ and __installation__ and the much higher percentage __currently using Haskell at work:__ of the 300 non-students, __34%__ of them said their current organization uses Haskell, and only 24% said _agree_ or _strongly agree_ to "My organization would soon use a lot more Haskell in our work, if realistic improvements are made to it." Even so, __64%__ would recommend Haskell for a project at their workplace.
Yes, this is the usage that I was thinking about in the post. Based on tweets and recent articles I've read, it seems like many people think this is a benefit of dynamically-typed languages, when it's something that can be done just as easily in a statically-typed language like Haskell.
I would argue servers (not just but often webservers and related backend services) are indeed the main target platform not just for Haskell but for about 90% of all programming languages. Desktops aren't just hard to target on Windows but due to their heterogeneous nature in general, particularly if you want to support GUIs and support them with native look&amp;feel on all major platforms.
I recently read through the MIO paper as well! It's really well done and impressive work.
Good point. FPC does post a ton of links: http://www.reddit.com/domain/fpcomplete.com Most are highly upvoted and I can see many that are upvoted by me. Still, things like this may be over the line especially if the poster happens to work there (but to say that would be doxing): https://www.fpcomplete.com/blog/2015/01/hiring-software-engineer Speaking of doxing, there's loads casual, friendly doxing on /r/haskell. More than anywhere else I've been. But I think it's a social norm and that mostly those people are public figures within the haskell community and don't mind -- I mean if someone uses their full name, initials or github handle to logon to reddit while posting links to their own blogs, papers, repos and SO threads, they're probably not too concerned about privacy.
You probably didn't do anything wrong. If you're opening a lot of files you have to be careful. 
You can indeed have `Applicative m` without much difference. /u/gasche seems to have another implementation below. fastRunPrio :: forall p m a. (Applicative m, Ord p) =&gt; Prio p m a -&gt; m a fastRunPrio prio0 = let (queue, _) = buildQueue 0 prio0 PSQ.empty in fmap (\m -&gt; (fst (evalPrio m 0 prio0))) (evaluateQueue queue (pure M.empty)) where .... evaluateQueue :: PSQ.OrdPSQ Int p (m Any) -&gt; m (M.Map Int Any) -&gt; m (M.Map Int Any) evaluateQueue q acc = case PSQ.minView q of Nothing -&gt; acc Just (k, _, mx, q') -&gt; evaluateQueue q' (flip (M.insert k) &lt;$&gt; acc &lt;*&gt; mx) 
&gt;I'll publish the repository (with whole history) when I'm done. Why do people do this? Embarrassment?
That doesn't sound like such a high cost, since you'll have to recompile all your packages with your new version of ghc anyway. That being said, I frequently use `brew switch` to switch between ghc 7.8 and 7.10, and I can't say I have had much trouble with ghc. Cabal is a bit confused when it finds a sandbox from a future version though.
Did you encounter a problem where ghc piles all of your packages in the same directory and then gets confused when linking?
GHC is the reason I switched, as a user, from Windows to Linux. I ended up buying a second desktop to do it.
I did a clean install with brew a few weeks ago and it's great with a few extra workflow steps. Commit to always using sandboxes. `cabal sandbox init` is your friend. To speed up adhoc builds, start a common sandbox and use this together with the stackage nightly constraints. eg cd junk-box cabal sandbox init cd ../new-project cabal sandbox init --sandbox=../junk-box/.cabal-sandbox/ wget https://www.stackage.org/nightly/cabal.config Cabal hell is a fading memory. 
I'm afraid that (exactly because visiting those pages can induce significant computation) my material is restricted to users from my own department. I'd love to figure out how to make it available just as cleanly to single-session or cookie-accepting visitors.
I can hardly write anything in Haskell unless I know how to write it. This is very different from how I do C++ which is search the API docs, copy some example, change it until it does what I want. Often I can finish the job with no idea how it could possibly work. With Haskell, I know exactly how it works. Learning is fun but I doubt if it's practical or scalable for industry.
No, doesn't ring a bell.
(pedantic) it's "goddamn-motherfucking-type-level haskell"
Love that John Carmack is on there.
Don't get your hopes up. I'm teaching hardware in a degree programme which won't do functional programming seriously until third year. It's very basic stuff. Having said that, I'm not sure what is the easiest way to facilitate your wishes. The url where the material lives will demand departmental authentication. However, this [repo](https://github.com/pigworker/CVI) gives a flavour of what I get up to.
Well, if you count the number of `=`'s to be the depth at which `mx :: m a`'s `a` is applied, you can see that `&gt;&gt;====` applies `a` to the _fourth_ argument of the RHS. You could also do: mx &gt;&gt;= \d -&gt; return $ \a b c -&gt; func4 a b c d For the same purpose.
For the last five or six years, almost all of them... including * Hardware [that runs Brainfuck natively](http://unsafePerform.IO/blog/2013-01-19-a_brainfuck_cpu_in_fpga), [implements a virtual machine straight from the mid-'70's](http://unsafePerform.IO/blog/2014-03-29-my_first_computer) and [one of the first 8-bit home computers](http://unsafePerform.IO/blog/2015-03-02-initial_version_of_my_commodore_pet/) * Stupid toy transpilers [from Haskell to C++](http://unsafePerform.IO/blog/2010-05-16-compiling_haskell_into_c++_template_metaprograms/), [from Brainfuck to a one-instruction machine](http://unsafePerform.IO/blog/2013-10-05-yo_dawg_i_heard_you_like_esoteric_programming_languages.../), [from register machines to Brianfuck](http://unsafePerform.IO/blog/2010-09-07-from_register_machines_to_brainfuck,_part_2/), ...
I would recommend looking at nix if you have to switch versions a lot. There some [up to date documentation for nix Haskell stuff](https://nixos.org/wiki/Haskell) with [a tutorial](http://wiki.ocharles.org.uk/Nix) by /u/ocharles.
Not necessarily because people usually fix errors first and then warnings. With this option, it's harder to prioritize.
It's interesting there's some cult of personality developing on /r/haskell. It's fascinating to think: what are the stakes? Remember Noam Chomsky? He was a computer scientist but through ruthless self-propmotion, he's managed to become some kind of public intellectual. I'm not sure if that's the goal, but no question, there's something going on.
Wow. I've never had this problem. If my program ever segfaults I'll be done for the day, but it hasn't happened. I use trace a lot and -- maybe it's a failure of imagination -- I can't imagine what would be more helpful.
This made me think. I guess your solution is good but avoiding functions with four parameters would be better.
I *do* remember seeing and playing with a tool that represented code at runtime as a graph of values and thunks that could be forced by clicking on the appropriate node. It was killer and it sounds like what a lot if people are looking for. What happened to it?
Porting scikit-learn is no small feat!
I thing [subhask](https://github.com/mikeizbicki/subhask) is the start of a foundation for that sort of effort, and now would be the time to do it. It could lead to stuff that was way out to the left of what is in the current toolkit.
I'm the author of HLearn, and I would not recommend haskell for kaggle use. There's basically 0 chance of a Haskell team being successful at these sorts of competitions. The existing libraries simply do not support the kinds of models these competitions require.
Sounds like a challenge! I'm less concerned about actually winning, then learning about haskell and it's limitations. Exactly how, why and where haskell isn't the right tool for the job is an interesting enough question. At the very least, haskeller know how to fail gracefully ;-P 
There seems to be a weird pattern of rightrelevance trying to inject their names into things on here: This post was submitted by a brand new account whose only activity is posting this link (with a seemingly random mention of @rightrelevance) and commenting on another posting from another random-looking account (which also mentions @rightrelevance) in another subreddit. Combine that with http://www.reddit.com/r/haskell/comments/37iyd5/500_haskell_developers_experts_mined_from_200m/ and it feels like they're just spamming their name into FP subreddits. I'm not opposed to companies that use FP promoting themselves, but I want them to be upfront about it. I'm not sure what to do about it. I'm a mod here, but should this stuff be against the rules? If the links are genuinely useful, I don't really oppose people just using the voting mechanism, but it makes it feel "dirtier". Opinions?
Looking around some more, I'm inclined to agree. /u/rredit911, feel free to comment here if you want to justify what's going on.
Yes, at least for me :/
Thanks! I'll be sure to give it a go sometime.
i scraped every rick ross lyric off rap genius and combined it with the stanford wordnet library on hackage to make a website called wwrrd (as in, what would rick ross do) that took a string in, words'd it, looked at all related phrases (as given by wordnet), then compared it with pre-generated phrase sets for rick ross lyrics stored in a redis database and returned a line from a rick ross track weighted by similarity (and some inverse document frequency etc.) in an attempt to rephrase every comment i make in a code review as though rick ross was saying it turns out semantic similarity is actually a really hard problem, but was a fun intro to a website with snap, and the wordnet and redis modules. 
Usually, you can just remove `~/.ghc`, but not `~/.cabal`.
I'll be at BayHac as well. See you and David there, again!
How can I make sure when using lazy I/O that a filedescriptor gets closed reliably at a certain point?
Are you thinking of [ghc-vis](http://felsin9.de/nnis/ghc-vis/), by any chance?
Are you not using sandboxes?
One could imagine a Bayesian model doing alright. The main challenge would really be coding up the inference algorithm, which for a one-off thing isn't necessarily too demanding. Iain Murray [did quite well](http://blog.kaggle.com/2012/12/19/a-bayesian-approach-to-observing-dark-worlds/) in 2012 using a Bayesian model and slice sampling, though I'm not sure how intense the competition has since become.
I dislike the repositories where the first commit looks like magic.
Actually maybe it was reading FP Complete's reports of people's difficulties using Haskell that made me think of it.
 if remaining &lt;= 0 then ks (B.concat $ reverse bss) () I'm a bit surprised to see the trusty old "accumulate list backwards, then reverse at the end" technique used for a performance improvement. Wouldn't a difference list be faster?
I've made a very simple console file reader like less, a simple searchable movie catalog with tags, my blog software, a parallel evaluated lisp interpreter and repl (almost done) and a few other small things :)
Consume the entire list, ie, evaluate the final nil. 
I don't see why not. It has to have a web UI to work on the data center though.
I just find it a very unhealthy to decide that one makes software for other people voluntarily, and then simply ignores a very significant portion (possibly the majority) of people just because it is more convenient. Yes, maybe developing for Windows painful, still I believe we should everything in our power to support software on Windows (and other OS-es) too. I myself use Windows on my company desk, and feel the pains of this type of thinking on my skin first hand rather often, and it feels really bad. 
Now that GHC's stack is unbounded, just killing the tail recursion might be fastest. I haven't tried changing a bunch of tail recursive code this way, but I'm curious to.
This is absolutely hilarious (by that I mean the fact that you choose Rick Ross for the corpus). Thank you for sharing because reading it made my day.
Awesome. My one quibble is that binary caching doesn't seem to be enabled on OS X for the new haskell-ng framework, so installing cabal2nix/anything takes forever to install. Do you know if this is a known problem? 
This is fixed now. As hdgarrood correctly surmised, the problem was due to https. More specifically, with the "HTTPS Everywhere" extension installed, d3 would not load over plain http. Fixed by using a local copy of d3 and using https to embed the game as an iframe.
This looks super confusing.
I'm fearful for a /r/lolhaskell now.
No, I don't dare. I've done it once as well and it was a disaster. To upgrade I'll probably try docker or Nix (except I can't manage to install Nix on my mac).
No one is going to judge you. Most people don't have anything at all. If you share early, people can help you with design, finding bugs, etc. It's worth it!
I am working on a tutorial for deploying Happstack servers via nixops that you should find interesting. It will be posted here.
How do you repeat a single step debugging run? It is just a giant waste of time to tell the debugger over and over again which bits are interesting to you IMO as you change parts of the code to see which differences they make.
well, at least it's not Lisp! :v /me dodges the tomatoes
Snap's web server is very robust and has excellent test coverage (here are reports for the [upcoming](http://buildbot.snapframework.io/job/snap-core/ghc_ver=7.8.3/Test_coverage/) [release](http://buildbot.snapframework.io/job/snap-server/ghc_ver=7.8.3,variant=openssl/Test_coverage/)). At my work we've been using it for more than two years to serve mission critical applications that handle more than two million hits per day. We use two servers for redundancy, but a single server can handle that load without breaking a sweat. There is a "cabal freeze" feature that creates a file that locks your project down to a specific version of every package dependency. This is a great way to get build reproducibility. Nix is also a great option in that department if you're willing to spend some time getting up to speed with it. We are gradually making progress towards the goal of making Cabal more nix-like, but even though we're not quite to that point yet I very rarely have problems with the existing tools in my day-to-day work.
Lots of machine learning researchers think probabalistic programming languages are the future, and I think Haskell has a strong chance to shine in this area. That's the main direction that I'm hoping to take HLearn in the future.
&gt; sometimes to the point where it seems like they are going out of their way just to be incompatible with everyone else It's well documented that 90's Microsoft had this goal (see for eg. the Halloween documents), but I think they changed enough in recent times.
It looks really useful, but I find the format of your solutions kind of confusing :/ 
&gt; I've made a very simple console file reader like less Would you mind sharing that code? I'm not advanced enough to understand some of these complicated projects but this looks like it could be understandable.
When I read for the first time I tought nixops was a typo, but I decided to search, and it seems really close to what I was looking for. For anybody reading: https://nixos.org/nixops/
It seems that the Haskell community is very close to the Nix one. Do you think there is any technical limitation besides getting used to it? Is there something that makes Snap more attractive than Happstack or Yesod?
Well, I tend to prefer difference lists can be better also because they compose more, and they're less error prone. But you're right, in this self-contained case there should not be a difference.
Looks great! How's performance? Any perf.-critical bug fixes?
There is a formatting issue in the "Less Like Moore" section.
Usually apt-get (or the GUI equivalent) of some package `foo` adds it to my menu and it just works. What doesn't work for you?
I tend to think of it as "almost inverse". For instance, consider function types `(-&gt;) a`. Apply it to some type `b` and you get `a -&gt; b`. Using that type, how can you get something as close to `b` as possible? Well, you can attach an `a` so people can just apply the function to the argument and get a `b`. In that way, `(,) a` is almost the inverse of `(-&gt;) a`. This is formalized in adjunctions, which require these two functions: return :: (Adjoint l r) =&gt; a -&gt; r (l a) extract :: (Adjoint l r) =&gt; l (r a) -&gt; a (substitute the correct categories for adjunctions in non-Hask categories) As you can see, these functions witness that `l` and `r` are "almost inverse". This is, essentially, what adjunctions are about. More examples: * Free functors are adjoint to forgetful functors. My intuition: this means that generating structure freely is almost the inverse of forgetting structure. * Colimits and limits are the two different adjoints of the diagonal functor which sends an object `A` of some category `C` to the constant functor always returning `A` in some functor category `D -&gt; C`. This means that the almost-opposite of making a bunch of connected copies is taking their colimit/limit. * Since colimits and limits can be a bit confusing, it might help to fix `D = 2`, the discrete category with two objects and only identity arrows. In this case, `D -&gt; C` is the category of pairs of objects of `C`, whose morphisms are pairs of morphisms. The diagonal functor sends an object `A` to the pair `(A, A)` (note: this is not a product type! it is a pair of two types, not a type of pairs). The two adjoints are sums and products. This means that the "almost inverse" of sending an object to a pair containing that object is to take either the product or the sum of two elements in a pair. * Note that whenever a functor has an inverse, its inverse is in fact its adjoint. Edit: * `(-&gt; r)` is self-adjoint. Why? Because if you have the type `(a -&gt; r)`, the closest thing you can get to `a` is the type where the user gets to observe an `a`, and you give them the result of their observation: `(a -&gt; r) -&gt; r`.
Stay on the road. Keep clear of the Moores.
In the lambda calculus there aren't data types, just functions. So `Just` has to be represented as a function. In the case of `Maybe` we just define the following thing Nothing = λn. λj. n Just = λx. λn. λj. j x maybe = λx. λdefault. λuseVal. x default useVal It turns out that you can represent all sorts of things (including algebraic types like maybe) quite slickly in the lambda calculus with [church encodings](http://jozefg.bitbucket.org/posts/2014-07-19-church-tutorial.html). So in fact `return = Just` is still the case, but since `Just` is itself a lambda term we "unfold" the definition so we can actually manipulate it. This is a little strange to read but that's how a lot of these proofs will work, unfold all the definitions and then repeatedly beta and eta reduce.
I thought he invented DFAs or something but I guess not.
The biggest travesty about the issues with reasoning about laziness is that it could be fairly easily addressed. GHC knows how lazy each piece of code is. All we would have to do is export that and import it into something like ghc-mod in vim. We could then visualize how strict any piece of code is. With background compilation this could be updated as we type.
University of West Of England United Kingdom
&gt; Moore :: (f -| g) =&gt; f (g b) -&gt; (a -&gt; g (f ())) -&gt; Moore a b Actually, that makes me wonder, suppose we have an adjunction `f -| g` with `f : Hask -&gt; C`. This generates a monad `m = g . f` and a comonad `w = f . g`. Is there some natural structure that relates `m` and `w`, if "you don't know" `f` and `g`?
I think there is minor confusion here: on one hand, third party libraries are painful on *Windows*; on the other hand, I have all kind of issues with Linux, but installing (precompiled) third-party libraries using the distro package manager is not one of them.
/u/PokerPirate , don't be so negative! We should try to break down the requirements instead; many continuous-valued inference problems rely on either sampling or variational approximations (I mean, the Bayesian marginalization integrals can be solved in either way). I've seen a few interesting Hs examples so far, e.g. /u/jaredtobin ' s Hasty Hamiltonian sampler and idontgetoutmuch 's work. The variational approximations instead require large-scale optimization codes, and could perhaps benefit from binding legacy codes (I'm thinking e.g. about something from the Coin-OR or NLopt suites). The Hs layer could definitely shine in coordinating control and isolating effects. Let's get this conversation going!
TBH I don't really get all the fuss about whether things compose or not. Sure, if it's a function abstraction it should compose. If not, there are other operators, binary and otherwise, and other functions. Why should something that doesn't represent a function support function composition? Sure difference lists compose - they're functions. But that's an implementation detail. When you compose difference lists the intent is to concatentate lists. Fortunately, `Data.DList` doesn't overload the dot operator - the *standard* difference list knows that it's a specialised form of list, not a function. Though `apply` is a bit of an abstraction-leaky name IMO. **EDIT** Because I'm long-winded, bad at setting the right tone and generally a bit of an idiot, I've turned this into a big deal. I know it's not really. Sorry everyone. 
People don't mean just function composition when they talk about how abstractions compose.
I used to believe that, but found it rather odd the way people talk about monads not composing even though there certainly are ways to combine operations from different monads (and it's a good job since `main` is in the IO monad right from the start - *every* use of any other monad combines multiple monads in the same program). The only way I could make sense of that was that actually they really did mean composing like a function. In any case, what other sense is there where difference lists "compose more" than ordinary lists? They're a specialized representation which supports fewer operations overall. 
I've intentionally stopped producing documention over the past year or so to discourage people from using the library :) At this point, it's just meant to be a playground for me to discover what the "right" interface is for learning algorithms. That means breaking changes everytime I touch the code. Which means lots of frustration for anyone who tries to use it (and lots of frustration for me trying to help them). That said, I have a [paper](http://izbicki.me/public/papers/icml2015-faster-cover-trees.pdf) at this year's ICML that uses some of the library code. Basically, the paper shows a faster way to do nearest neighbor queries, and my proof of concept implementation is in haskell. So sometime over the next month or two you can expect better documentation on at least this aspect of the library.
I'm really surprised nobody has just wrapped `vector` in a matrix API
Your never say no to an extra tool for analysis, and that's what it is. The category theory is already there, whether you use it to reason about your programs or not is up to you. Turns out, being able to reason about programs is pretty nice. TL;DR: Premise is wrong, the theory is free, not expensive.
Kind of. It's not really a solvable problem because a REPL kind of needs to be ghci is. Really Haskell is not super well suited to a REPL. It is useful, but not for the same things python uses it for. 
All you need to know is that you use "let" to define a value or function, as in: let f x = x + x The whole IO monad thing is totally unnecessary (and a little wrong)
Laziness usually isn't a problem in 90% of the code though, it only becomes a problem in those few long-lived spots of our programs. How would you visualize something of low importance that needs annotations on almost every bit of code (since the compiler does not know which parts are long-lived in general).
I'm confused by the example program. It seems unrelated. Does that mean you should never use `Data.ByteString.Lazy.readFile`?
You can give IHaskell a try. It's a haskell backend for IPython, and uses the GHC API to provide a similar experience. This means that you can write free-flowing code as you'd write in any other repl. There's a online demo available too, take a look: https://try.jupyter.org (Choose: New -&gt; Haskell) To use it in console, install it following instructions here: https://github.com/gibiansky/IHaskell/
That's wrong: `let { foo (Just x) = 1 ; foo Nothing = 0 }`. You can use the same syntax to force other things onto a single line (e.g. case statements, monad sugar). Additionally, you can also create definitions over multiple lines, by using `:{` and `:}`. To be honest, the last time I came back from `ghci` to `python` trying to do simple things, like loading a file, I was almost beyond the point of exiting it after 2 minutes.
Yeah but I mean we're not talking about a general sampling backend for a PP language here. Just code up a working implementation of MH/slice sampling/HMC/whatever and run it on a posterior for awhile. Same thing statistics students have been doing for decades. :)
I'd also recommend using the REPL in combination with a `.hs` file. In this case the ghci command `:r` (short for `:reload`) might also be helpful, which reloads the currently `:load`ed module. This way you can change your definitions in your favorite text editor, then quickly `:reload`, and evaluate some expressions using those definitions.
We typically use C libraries to build the actual models, but our ML pipeline is mostly haskell otherwise. For large datasets we use spark to preprocess the data into something manageable. I see no reason for a haskell team would have to use nothing but pure haskell. FFI to the rescue.
What's a good background for learning HoTT and Directed TT? I already have MLTT locked down, I know a bit of set theory, model theory and category theory, but my math-fu is otherwise non-existent. I tried reading the HoTT book, but not only do I not get it, I don't get what it's for.
 zipWith' :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c] zipWith' _ [] _ = [] zipWith' _ _ [] = [] zipWith' f (x:xs) (y:ys) = f x y : zipWith' f xs ys How would the above code (from LYAH) be entered into the REPL?
I install Linux Mint, and I get a working web browser, with all my laptop devices, internet, etc, Just Working. Do you have anything specific that doesn't work?
You can also use :e to open an editor for the most recently loaded file.
My understanding is that HLearn's philosophy is such that it will only ever target a small subset of the kinds algorithms used in machine learning "This interpretation of machine learning is somewhat limiting in that not all models have obvious algebraic structure" [https://github.com/mikeizbicki/HLearn]
What am I doing wrong here? Throws an error and doesn't let me finish entering the code. λ :set +m λ let zipWith' :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c] *Main| zipWith' _ [] _ = [] &lt;interactive&gt;:16:1: parse error in let binding: missing required 'in' EDIT: Figured out the error. I actually need to indent each line, because of course, the REPL doesn't auto-indent. Well, at least, it's something. Hope this helps someone else.
FWIW, the first experience I had with Haskell was downloading ghc, opening up a tutorial, firing up ghci, typing in ghci&gt; square x = x * x and getting really confused why the tutorial didn't work.
I knew I wasn't alone. :)
I'm not big on the attitude of almost every reply to this submission so far, which are typical replies: "all you need to remember is this pointless difference to regular code, this is just the way things are". The real answer is that, yes, this is annoying and not necessary. Currently GHCi only tries to parse do-statements (expressions (`print 1`), generators (`x &lt;- getLine`) and let binders (`let x = 1`)), and a few declarations (import declarations and data declarations and instances etc). It could easily be updated to support function or pattern bindings, and avoid this source of confusion for newbies. There's no good reason for doing so other than "it's not there yet". I can add this to [ghci-ng](https://github.com/chrisdone/ghci-ng) to demonstrate.
Of course (good) software comes from an itch to scratch. But as I said, I am (as I guess most people here are) both a user and a programmer. With my user hat on, I really really feel the pain, so on the programmer side I try to aim for the best possible compatibility reasonably (and this is the key word) achievable. And I don't think "use linux, because windows sucks" is the best compromise here. Especially as probably the majority of computer users today use Windows. 
Last time I tried to upgrade GHC on a Linux (virtual-)box at work, I ended up to simply install a fresh Linux instead because it was simpler (that is, at least it not impossible). Then I ended up installing and immediately discarding 3 different distros because they were that painful (big name standards like Debian, Ubuntu, Fedora, whatever, I cannot recall and frankly I don't care). I think the one which remained at last was actually Mint, but only because I had to choose between the somewhat less pain of Mint than the other three, and the pain of searching for something a tiny bit better. I don't have too much concrete examples, because I don't feel I have to count those uncountable number of issues, but to have a very concrete example, maybe ~2 years ago I tried to install a VPN client (that's client, not server!) on my boss' computer, which had the then latest Ubuntu. Let's make it short: It. does. not. work. at. all. And that summarizes my general experience with Linux. Maybe other people are more tolerant, or maybe I'm too old(\*) to be patient enough to deal with this stuff, but for god's sake, it's fucking year 2015 here, and the fucking commodore 64 was, 32+ years ago, *way* more user friendly than Linux is in 2015 (and that's with 20 years of on/off, though mostly off, Linux experience, so it's not just that I cannot type apt-get into a terminal) (\* actually I'm not that old, which makes it even more serious)
Actually, it's GNU/Linux, not Linux!
It's a little more complicated than that. `glibc` has certain function calls which cause dynamic libraries to be loaded, for things like iconv or NSS - glibc NSS includes `getpwuid` for example, which needs to load `pam_unix.so` at runtime for account information. Your application is built with glibc X.Y, statically. You give it to a customer. It calls `getpwuid`, which causes your (statically linked) copy of glibc to issue a `dlopen` of `pam_unix.so`, bringing it into the executable address space. But their distro is different, and the `pam_unix.so` that is found by `dlopen()` on their system was built dynamically against glibc X.Y-1. And your application goes boom.
Worse yet is when the same single letter is used to refer to different items within different namespaces but within the same expression. I'm not joking. Some people seem deliberately obscure.
My usual method is to force myself through material on the subject until it starts to make sense. Last I checked, though, DTT was still in the beginning stages. I think I was told that they don't even have the syntax pinned down.
If you want me to develop for MS Windows, best get your wallet out. That's my compromise. I'm not the only one that feels this way.
Weird, I feel that way about using Windows. Stuff either doesn't work for me, or I get tons of nag-screens asking me to pay them. I think Ubuntu's Unity interface is total crap, so perhaps that is part of the problem. With Mint, VPN has a built-in GUI which works for me (with OpenVPN). Weird!
Fixed.
Yet one more thing that makes the REPL difficult for n00bs.
Adjunctions give you things like currying (, e) -| (-&gt;) e says that `(a, e) -&gt; b` is isomorphic to `a -&gt; (e -&gt; b)` It lets you "pull" f forward across the arrow to get `g` or pull `g` backwards across the arrow to get `f`. In this case you can curry and uncurry. Other examples. newtype Const a b = Const a newtype Lim f = Lim (forall a. f a) data Colim f where Colim :: f a -&gt; Colim f There we get the chain Colim -| Const -| Lim (it is weaker than the real category theory notion of a limit/colimit due to parametricity being stronger than naturality, but the adjunction still holds.) Adjunctions give you a pair of functors that are "almost inverses". In fact if your functor does have an inverse and an adjoint then the adjoint is the inverse. There are other intuitions for adjunctions. In a rather deep way all right adjoints forget something, and all left adjoints provide a way to fill in that forgotten structure.
If it bothers you so much, then contribute a patch to `ghci` to automatically indent
I find that I get a lot of mileage out of trying to broaden the applicability of category theory to Haskell. The moment I find any new category I can express in Haskell all of a sudden my bag of tricks comes out and I simultaneously find a way to make code work better and learn something.
I like that. A gentle reminder should be easy to add, unobtrusive and directly addressing a common misunderstanding. 
I took the definition of a machine that provides one output for every input, then I applied a bit of theory to allow it to memoize intermediate results, showed that this lets it exploit smarter folds, and that if we tweak the machinery used it inspires a way to find machines that return multiple outputs given an input, but which get crippled by their lack of state, then show that these are just a poor man's approximation of a `Fold`. In practical terms such machines arise a lot in code and this lets me make them faster.
I'm getting into Haskell *because* of the abstraction. That's what I intend to get out of it. All the "x", "y", "xs", "ys", etc are kind of the point, right? Abstract as much as you can about problems until they are so general, that "x" is as meaningful as anything else you could use as a name. But much shorter. :)
I concentrate on the type signatures. They are much more meaningful than any variable name.
I'm not talking about shorthanding "expression" into "e". That's perfectly fine. I'm talking about stuff more like `applyLog :: (a,[c]) -&gt; (a -&gt; (b,[c])) -&gt; (b,[c]) ` which I see often. I get that the types and order of the params are supposed to speak for themselves, but they don't. Not to me anyway.
I guess that makes sense.
You expect a "beginner" to patch ghci? I can't imagine what you would ask an expert.
Consider it top of my backlog then! :)
Actually, I rather like your idea: &gt; class Functor carton where &gt; fmap :: (apples -&gt; juiceboxes) -&gt; carton apples -&gt; carton juiceboxes Now Functor makes a heck of a lot more sense to me!
Yep. newtype Co w a = Co { runCo :: forall r. w (a -&gt; r) -&gt; r } lets you take such a comonad and twist it into a monad. Co (Store s) ~ State s Co (Coreader e) ~ Reader e Co (Cowriter e) ~ Writer e etc. This is the (right) Kan lift of Identity along `w`. You can also build a monad transformer from any comonad. newtype CoT w m a = Co { runCo :: forall r. w (a -&gt; m r) -&gt; m r } I use this tool to try to quickly figure out what monad is related to a comonad quite a bit. =) I wrote up a couple of posts on this topic a few years back.
I think OP is talking about when there is an 'a' type variable and an 'a' data variable in the same function. I agree that's a pain. Imagine map :: (a -&gt; b) -&gt; [a] -&gt; [b] map _ [] = [] map b (a:as) = b a : map b as
Or how about this: &gt; :: (apple -&gt; applejuice) -&gt; [apple] -&gt; [applejuice]
Yes, I know, but is there one that works when C != Hask?
Unlike most GHC projects, this one would be easy and could be done incrementally and without too much debate. All the printing in GHC is done using pretty-printing combinators. [There's already a library for ANSI combinators which is more-or-less backwards compatible](https://hackage.haskell.org/package/ansi-wl-pprint-0.5.0/docs/Text-PrettyPrint-ANSI-Leijen.html). This could be a fun weekend project attaching colors to various outputs. The only drawback is it adds a dependency and GHC already has plenty.
Man we vimers suffer for our preference.
The nature of the `stab` abbreviation was discussed in the [Haskell Cast episode 1](http://www.haskellcast.com/episode/001-edward-kmett-on-lenses/) with Edward Kmett. If I remember correctly, there was no special meaning to it, and it is just two pairs of types, which is signalized by consequent letters. Personally, I read `s` and `t` as *source* and *target*. Also I often see functions of type `a -&gt; b`, so I think of `a` and `b` also as some sort of *inner source* and *inner target*.
Suppose we take the diagonal functor and the product functor. You need to do some rejiggering to make it work in Haskell, but let's pretend Haskell had product kinds. Can a structure similar to Co still help?
You give good advice. But what about attaining intermediate knowledge? I've done all of those courses. I'm still baffled by half the discussions around here with the type families and such. GHC documentation (chapter 7) is the best resource I've found but it's not tutorial at all.
Ah I see. Yea that has caught me as well. Personally prefer (x :: a), (y :: b) to (a :: a) where possible. Dovetails nicely back into the general problem where a lot of time learning Haskell for those of us without a mentor can be spent rediscovering and jettisoning bad assumptions in a very roundabout way. 
Cabal sucks but it's not Cabal's fault. It's GHC's fault. The way linking works is inferior to the already sad way linking works in C. The upshot is that you can't upgrade a library with a backwards-compatible newer version. And if two versions of the same library meet in the same dependency tree they fight. Linking also sucks in other ways. What we need is to do is rethink and liberalize the linker. I've imagined making a system-wide database that indexes a hash of unique-up-to-renaming (core) functions to their bytecode. This way, two equivalent functions are recorded only once and you can dispense with modules referencing each other after compiling. They just keep track of the hashes they need from any outside module. I'm sure there's some problems with that idea, but let me dream!
You're not the only person to feel this way. The question comes up fairly regularly around here. I wrote a [blog post](http://softwaresimply.blogspot.com/2011/12/whats-in-name-lot.html) about it awhile back. Are there plenty of places where Haskell code uses one-letter names inappropriately? Absolutely. But I think there also plenty of places where a one-letter name is superior some longer name that a number of newcomers seem to want.
There's another language that I'm pretty sure isn't functional, that supports ADTs: [Haxe](http://haxe.org/manual/types-enum-instance.html). I don't know if it's "popular" but then again I don't really know what Julia is (though I've heard the name).
The beauty of open source :) I do wish I had more time to document and polish things though...
It's the first set I'd like to see changed. The second set is fine.
It compiles with a signature let (foo,bar) = case eitherDecode sampleJson of Right fb -&gt; fb :: (Foo,Bar) Left e -&gt; error e I take it you noticed that instance [incoherent] (FromJSON a, FromJSON b) =&gt; FromJSON (a, b) is already present. This is the instance it is thinking of in the error message.
Thanks! Wagon currently connects to Postgres, Redshift, and some other similarly flavored PG databases. We're likely adding MySQL next then possibly Hive (so many fun ones to choose from!). Today, the data is local between the cluster and the desktop tool for analysis-- many of our users asked have the tool work without a 3rd party hosting the data or to host it internally. For sharing, we use our backend web services (or can later deploy our service behind firewall). In the hosted case, we could have the data never touch the user's local machine. For us, that's a bit down the road. Good idea though. Would love for you to try out the tool and get your feedback. We have a hosted test database for you to play with.
I think it means they are *fixing* readFile...
first, you need zeroMQ from your package manager (libzmq, libzmq-dev etc) then do this in bash mkdir sandbox cd sandbox cabal sandbox init cabal install alex happy cabal install cpphs cabal install gtk2hs-buildtools cabal install ihaskell --reorder-goals ihaskell install # Adds ihaskell as a kernel for ipython 
Just putting this here too: http://chrisdone.com/fmap/
Sure. The problem is distinguishing the two. The difference is probably subjective. And if that is true, then "I'd like to see that changed" starts to sound a lot more like "You should use my preferred coding style".
`s` can also be read as "structure", which is what the lens documentation uses ([copied here](http://hackage.haskell.org/package/basic-lens-0.0.0/docs/Control-Lens-Basic.html), I can't find where in [lens](http://hackage.haskell.org/package/lens) the `Lens` type is defined anymore); and then `t` is just the next alphabetical letter indicating "s is to t as a to b".
Ah I'm totally guilty of this. In my defense, I think the example you gave is a pain too, because the "b" has type "a -&gt; b". But I would write map :: (a -&gt; b) -&gt; [a] -&gt; [b] map _ [] = [] map f (a:as) = f a : map f as and be happy with myself.
&gt; first, you need zeroMQ from your package manager (libzmq, libzmq-dev etc) What package manager? PIP?
&gt; If we swap the let binding for a monadic binding Usually that difference comes from `let x :: a` being generalized (`forall a. a`) whereas for the `a` in `x &lt;- … :: m a` to be polymorphic requires impredicative types (`m (forall a. a)`), which must be explicitly specified to activate; here the `a` must be monomorphic. I can see why that would affect instance resolution. But I haven't the foggiest regarding the general problem and why the GHCs differ.
Kind of depends. That's why I left them in quotes. It always felt like transducers were some kind of special blend of stream transformers, a weird core of mutability inherited from Clojure, and state machines. Replicating them exactly might be hard merely in finding the exactly blend that is entailed. Especially given the implied typing design which is supposedly essential to include and the desire to have early termination "without either", both of which feel a little less than completely solid.
Did it. Removed all the things. Reinstalled all the things. Was worth it.
Ok, I used Brew to install zeromq. But where is ihaskell located? I get "command not found" when I run that last line.
I'm not sure if having longer names is going to help. To try: does this make more sense? applyLog :: (source, [log]) -&gt; (source -&gt; (target, [log])) -&gt; (target, [log]) Essentially, this is function application (takes `source`, `source -&gt; target`, and returns `target`) that carries a 'log' along with it (in particular, the `source` has a log, and the function can produce more log), where a log is some list of values. But I don't think the _names_ are how you figure that out, but rather the _structure_. When used well, terse variable or type names are hints that it is the structure that matters, as they will lead you to an understanding of all the ways this could be used. Though often the short names are chosed to give some hint as to a common use (in your example, `a` and `b` feel idiomatic - `c` does not - I'd prefer `l` or something else, as it hints that it is a different class of things).
Yes, I did notice that. I don't know why it selected that instance, though, nor did I realize that instance was [incoherent]. I still don't understand why that instance is [incoherent]. I see that the source file is marked with the `IncoherentInstances` pragma, among others, but... why?
From what I saw in Clang's source code, one simply needs to call like 2-3 functions in terminfo to determine whether color is supported. After that, the ANSI color codes can be just done manually using escape codes if one doesn't want to incur a dependency on that lirbary. Personally, I don't really see a reason to bring in a full-fledged library for this.
Indeed, defining my own wrapper type is exactly what I did to fix the issue.
In Haskell we'd basically build different types for the different "untypeable" cases they describe as one thing. The common case is the one I described there in the post.
What worries me here is that you don't seem to be choosing the abstraction appropriate to the problem - more like choosing the abstraction first and forcing the problem to fit. One of the few things I know about category theory (I keep dipping my toe in, but then forgetting most of what I learned) is that there's an obvious reason why functions compose like arrows in a category - the original intent of categories was basically to abstract the functions out of function composition. Once you abstract away from functions and composition your objects can be some not-obviously-functionish things and your arrows can be some not-obviously-function-composition things, but the abstraction still reflects that original intent. So where it's appropriate that's really just my earlier "Sure, if it's a function abstraction it should compose.". And of course the point of abstracting away from functions is exactly that, but still, to me that doesn't mean I'd start out looking for ways to treat websites as categories. I've never been a web developer but, if I were looking for generalizations and ways to abstract away from that, I'd probably start with intent-related concepts like "document" - depending, of course, on why I want to be more abstract. Abstraction for its own sake just makes code less readable and less maintainable. &gt; But if it's a true composition, where the end Webiset is indistinguishable from the composition of two (or more) Websites, suddenly it's a piece of cake to add as many Websites as you want. Yes, but that's just closure - the idea of an operation being closed for a set isn't unique to function composition. Addition and multiplication are closed for integers so it's trivial to add and multiply as many integers as you want - you don't need to model addition as function composition. An example I'm thinking a lot about at the moment - Bezier curves are closed for translation, rotation, scaling but for perspective projections may yield rational Bezier curves. Rational Bezier curves are closed for all of those operations, but not for some interesting CAD operations such as offsetting. I didn't even know what implicit curves and surfaces are last week, but apparently there's some multivariate-rational-polynomial-implicit concept that's closed over just about everything interesting. The point being that there's often many different ways to combine values - not just one - depending on the intent and the abstraction. Also, in practice it's often necessary to force closure, e.g. by using approximations in the simple model rather than allowing generalization to a model that's more difficult/less efficient/impossible to work with. TL;DR - I know the benefits of closed sets without needing to think about categories and arrows. 
Hopefully I can try it and respond to my comment here about whether it works. Hopefully...
Can functors only deal with cartons of apples? What if I Have a list of strings?
It makes it sound like it can only deal with apples, which is untrue. You could have something like :: (oneType -&gt; anotherType) -&gt; [oneType] -&gt; [anotherType] which is just as general as the `a -&gt; b` stuff, but also much more tedious to read once you get used to the `a -&gt; b` convention.
&gt; I can add this to ghci-ng[1] to demonstrate. Please do!
When I hear "order wouldn't matter" I think of applicative and the [Haxl paper](http://community.haskell.org/~simonmar/papers/haxl-icfp14.pdf).
It's always easier to discuss with concrete examples! Can you link to some source on Hackage, for example?
By the way, I never really understood why the let keyword was even necessary in a do block. If really needed maybe something like := would have been better. 
Short scope - yes. It's no different than `i` and `j` as common loop induction variables in other languages. Longer, more explicit names help readability - but not when the code is perfectly readable without them. Of course any style damages readability when you're still getting used to it. 
Well, if you place your list of strings in a carton of apples...
One problem is that both styles have benefits, but we have to choose between the two. I would be perfectly happy for Haskell to allow the specification of multiple type signatures. The compiler would have to check that they agree (in general, probably by checking that they are both valid individually and by giving the expression the unified type). applyLog :: (a,[c]) -&gt; (a -&gt; (b,[c])) -&gt; (b,[c]) applyLog :: (source, [log]) -&gt; (source -&gt; (target, [log])) -&gt; (target, [log]) applyLog = ...
Love the idea of functional hazing rituals - purify the OO out of people!
And what are the advantages of the Machines over `Fold`s? Any reason to prefer not to program `Fold`s, but such machines?
Today, as long as you `patchelf` the resulting binary `rpath` to find the dynamic shared libraries properly (which you really, really have to do with _any_ binary on Nix anyway, so that the shared objects don't get garbage collected under your feet), any binary built on Ubuntu should work on Nix or NixOS just fine. The reverse is not necessarily true, but this is in fact mostly because NixOS ships with a more up to date glibc (probably, I haven't checked the actual versions). And while dynamically linking to `glibc` means your app will work OK with newer versions, it will not work OK with older version. In other words: if you build your application on a platform with an older glibc (like Ubuntu, or really Debian), it should work fine on any platform with that same version of glibc, or newer (like NixOS). The reverse is not true, which is something `musl` could actually fix.
&gt;how the heck do I instantiate a date type? import System.Time main = do t &lt;- getClockTime print t -- print nicely-formatted time let u = toUTCTime t print u -- print UTC time record print $ ctMonth u -- print just the month from the record Print month as a one-liner: fmap (ctMonth . toUTCTime) getClockTime
&gt;I'm talking about stuff more like applyLog :: (a,[c]) -&gt; (a -&gt; (b,[c])) -&gt; (b,[c]) I often work backwards, then use the name to clarify. Here I'd say "Okay, this gives me back a pair with a `b` and a list of `c`s. Where can I get those from? It looks like the function right before it gives me that. Where does *it* get that pair? It looks like it uses the `a` to create it. Okay, so where does *that* function get its input `a` from? Oh, the `a` is provided in the first argument. Hmm, there's also a list of `c`s there. I wonder if the list of `c`s is the same in the two inputs and the output... If I forget the lists of `c`s for a moment, then this just looks like `a -&gt; (a -&gt; b) -&gt; b`, which just looks like some form of `apply` - i.e. 'give me an `a`, and a function from `a` to `b`, and I can give you a `b`.' Alright, what's the name of this function? Hmm, `applyLog`... Okay, so maybe the incoming list of `c`s is the log so far, and in the act of generating the `b` from the `a`, we add more to the log, and then we get back the `b` and more log." I don't even know if that's right, but it's just to give you a sense of how I use the types - and the name, ultimately - to get a first impression of a function. I've been wrong (and may be here), but I've been right a lot more often. If the name isn't bad, then it and the type together usually tell me the general idea of what the function does.
This isn't quite what I wanted. See, the adjunction generates a monad Pair :: Hask -&gt; Hask, but it also generates a comonad PairW :: (Hask, Hask) -&gt; (Hask, Hask), and what I'm asking is whether there is a natural structure that relates Pair to PairW.
Ew, there's a string in my apple juice.
Strictly speaking, for most uses, it's not. You can translate almost all let-bindings x &lt;- return pure_expression assuming `return` has been defined to be proper and law abiding. But you couldn't translate this into an operator as follows: x := pure_expression Because user defined operators cannot new names. You need the compiler's help for that. Another thing is that you cannot translate this: let x = y y = x As y &lt;- return x x &lt;- return y Won't compile unless you're in a mdo block and working with a `monadFix`. So `let` bindings in a do block are very much primitives rather that syntactic sugar over other do-notation constructs.
This is in fact how ghci is designed to be used: define functions in a .hs file, load them in ghci and mess with them. 
I noticed that [diagrams](http://projects.haskell.org/diagrams/) isn't supported yet. Do you have plans to add it?
If you don't mind installing nix, it's relatively easy to install with the nix (I only managed to get version 0.5 working so far).
So subhask tries to enforce algebraic relations through typeclasses, as far as I can tell. What are the foreseeable limitations of this approach? I.e. can we expect that one day hlint will warn us, in constant time, that e.g. a singular matrix cannot be inverted without significant loss of precision?
You could just as easily do the let style translation on a different syntax as long as it used a unique and reserved operator that is built-in as well.
http://i.imgur.com/4mTG3Jv.png
The heap takes up memory too, but has more bookkeeping overhead associated with it.
Imagine the `negate` definition is a part of the `Num` instance for `CReal` sequences, then it is `Prelude.negate`.
No, you’re not the only person aggravated by this. I have occasionally considered writing a new repl that understood the same real Haskell code that you would have written for ghc to compile. I’m of the opinion that it’s a rough spot where Haskell is wilfully obtuse for no real good reason apart from a misguided appeal to some abstract idea of mathematical unity: I’d personally rather have a repl that understood the same code that the compiler / interpreters do.
Hehe. I've heard spacemacs is very-very nice (it default to `evil-mode`, which has vim keybindings).
I think the following variant is best: applyLog :: (a,[log]) -&gt; (a -&gt; (b,[log])) -&gt; (b,[log]) This indicates that the types `a` and `b` are completely generic, while the function does something "more specialized" with the `log` type.
Having read the attached post's suggestions with varying degrees of astonishment and disagreement until my point of understanding, I returned to reddit to find I should have read your comment with a little more care and thoughtfulness! :)
There's a little bit of structure to `s t a b`, though. In particular `s` and `t` and related to one another and `a` and `b` are related to one another—thus why they're next to one another in the alphabet. Further, the relationship between `s` and `t` is similar to that of the relationship between `a` and `b` s : t :: a : b But this is only one axis as `t` and `a` are similar to one another in the same way that `s` and `b` are, so maybe we should draw a box s === t | | | | a === b This was discussed a bit in ["Mirrored Lenses"](http://comonad.com/reader/2012/mirrored-lenses/) and also is highlighted a bit by thinking about which parameters are positive and negative in `Lens` type Lens s t a b = forall f . Functor f =&gt; (a -&gt; f b) -&gt; (s -&gt; f t) Lens - + + -
Or a stream of bits? Or an optional value (like Maybe)? Or an undo-able field? The moment you start naming the variables in fmap is the point when you miss all of the other applications of Functor.
Python's repl doesn't autoindent either. While the error message is probably less cryptic, I don't think a lack of auto indentation is a fair argument in the scope of this conversation. Indeed, my earlier posts weren't meant to argue that the repl situation was ideal, but rather to show how to have multi line equations in ghci. 
Haskell is tough to learn for most imperative programmers. I was always told to just write everything out to understand a function, or to understand type unification. Writing out a type unification on paper would become very tiresome indeed if the types were named "oldtype", "newtype", "oldtypetonewtype", etc. Unrolling a recursive function o' paper would also be very tiresome for the same reason. Proofs too.
Liquid Haskell? (though I've never tried it… yet)
Thanks!
You should say who will be supervising it too and name the research group, link to their work and so on...
Thanks for engaging my comment! I like your point about having different closed operations. I've only done thought experiments with composing websites (well, UIs in general) but it would indeed be wise to fit the solution to the problem, rather than the other way around.
MrCatholic has a point, though — `apple -&gt; applejuice` is a built-in example that would help (some? most?) people grasp the concept quicker. But to address that point, I think examples-for-learning are best left in media designed for instruction, and left out of standard libraries. (fmap vs map comes to mind...)
My short "oh so that's what this is" definitions. * Functor = Container. You can put stuff in it. (Among other things). I also like Mappable. * return = inject. We can inject a value into a monadic context. * monad = computational context. We take a type, and we construct a completely arbitrary computational context for it (well, they have to obey the monad laws). 
Is there any information what this site provides? The second page says that it is about functional programming, but it doesn't get more specific. *edit:* If anyone else wasn't familiar with [Slack](https://slack.com/) before: here's the feature list from the corresponding [wikipedia article](https://en.wikipedia.org/wiki/Slack_\(software\)): &gt; Slack offers persistent chat rooms organized by topic, as well as private groups and direct messaging.[13] All content inside Slack is searchable from one search box. Slack integrates with a large number of third-party services and supports community-built integrations. [20] Major integrations include services such as Google Drive, Dropbox, Heroku, Crashlytics, GitHub and Zendesk.[21][22]
I'm personally a big of Yesod and it's type-safety. In the end though, I think it boils down to personal preferences of choosing that over something like Snap. Both are very robust, and I can say there goes a lot of thinking into the choices made at Yesod. You can check some of the blog posts they have http://www.yesodweb.com/blog/. Michael Snoyman that is behind Yesod is also behind stackage and FPComplete. As for building, it usually takes a lot of memory. What I do is build it in a container/VM locally and then push the binary to the server. Yesod can be used with something like keter[0], which makes it very easy to deploy and update the site without loosing any connections. [0] https://github.com/snoyberg/keter
Thanks, super helpful!
Ah, thanks for the information. I haven't associated *Slack* with a product name and took it too literally.
I think it's working... can you try a different browser? In which one did the problem happen?
I just added a link to this: https://medium.com/backchannel/shut-down-your-office-you-now-work-in-slack-fa83cb7cce6c to the http://functionalslack.com page... So thanx to m0rphism for feedback regarding this.
Firefox with Ghostery and uBlock. I can try again in another later today.