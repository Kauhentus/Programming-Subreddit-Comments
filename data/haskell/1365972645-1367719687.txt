No, it's not optional in Haskell. Everybody uses the type system.
This is why I said in the post "pipes-concurrency provides the basic building blocks necessary to build functional reactive programming abstractions", followed by several examples of actually building such abstractions. This is why I didn't name the library `pipes-frp`, a name I'm reserving for the future library that actually implements these abstractions. Also, who decides what constitutes reactive programming? I've never actually seen anybody come out and give a clear definition of what functional reactive programming actually is, other than some vague notion that it is supposed describes "time-varying" behaviors, something which `pipes` already does very well. I mean, really, the whole `pipes` ecosystem a this point is only walking distance from libraries like Elm and reactive-banana. Do you really not see the parallels between the `Cell` example I gave and Elm's signals? It reads almost exactly like Elm code. Edit: To answer your second point, I would say that coroutines + channels = reactive programming. The coroutines (i.e. `pipes`) handle the deterministic concurrency and the channels handle the non-deterministic concurrency. The deadlock protection is just a necessary ingredient, but not what makes it reactive.
Yeah, I could. For now, I can briefly write it up here: I actually don't keep track of references to the `Input` and `Output` proper. Technically, I only keep track of whether a mailbox's `recv` and `send` commands are still floating around. For any given mailbox, both `send` and `recv` each have their own `IORef` which they touch each time they are run, and I use `mkWeakIORef` to attach a finalizer to each `IORef` which triggers when the last reference to that `IORef` disappears. This is the proof that a certain `send` or `recv` command is no longer available. When a mailbox's `send` command gets garbage collected, that adds a final `Nothing` element to the channel, which notifies all consumers that upstream cannot supply any new data. Any elements that were in the FIFO channel previously still get consumed correctly since they were deposited before the `Nothing`. The `recv` transaction is designed to not consume the `Nothing` so that you don't need to predict in advance how many readers have to be notified. Any repeated attempts to keep reading from the channel will just keep returning the same `Nothing`. This prevents the consumers from deadlocking when all the producers disappear. When a mailbox's `recv` command gets garbage collected, that sets an STM `TVar` to notify all `send` commands that nobody can read from the mailbox any longer, so there's no point depositing data into it any longer. This prevents `send` commands from deadlocking when all the consumers disappear. The [source code](http://hackage.haskell.org/packages/archive/pipes-concurrency/1.0.0/doc/html/src/Control-Proxy-Concurrent.html) is really well commented, too, so you can check that out to learn all the really fine details. Not that there tons of fine details; the core logic is about 20 lines of code.
I spent a lot of time in Erlang (similar enough semantics to Clojure) before learning Haskell and I found it very enlightening. Haskell's type system and by-default laziness make it a different paradigm altogether. I've found that when I learn (really learn, not just read about) different programming paradigms it makes me better at programming in every language that I use. It's hard to do that without trying out new languages. Haskell is a great choice because it has a lot of very interesting features and libraries that are very practical such as STM, cheap concurrency, data parallelism, Parsec, laziness, HM-typesystem, stream fusion, functors/applicative functors/monoids/monads/arrows, etc.
Apologies and thank you. 
&gt; which pretty closely matches how you would describe the problem in plain English if you were trying to tell me how to respond to you You think beautiful, and weird.
It looks like there's some indentation problems in addPerson as pasted, as well as some misplaced parentheses. For example, putStrLn is a function that takes one argument... but "putStrLn sortBy (comparing snd (personList))" tries to use it as a function that takes "sortBy" and "(comparing snd (personList))" as separate arguments. Function calls have a very high precedence so you need to use parentheses to make it do what you mean, which would be "putStrLn (sortBy (comparing snd) personList)". You should learn how to use GHCi, it can help you sort this out. $ ghci GHCi, version 7.4.2: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. h&gt; import Data.Ord h&gt; import Data.List h&gt; :info comparing comparing :: Ord a =&gt; (b -&gt; a) -&gt; b -&gt; b -&gt; Ordering -- Defined in `Data.Ord' h&gt; :info sortBy sortBy :: (a -&gt; a -&gt; Ordering) -&gt; [a] -&gt; [a] -- Defined in `Data.List' h&gt; :type sortBy (comparing snd) sortBy (comparing snd) :: Ord a =&gt; [(a1, a)] -&gt; [(a1, a)] It will also be helpful if you learn what a "do" block is. They don't serve any purpose for single expressions such as "do exitSuccess". http://en.wikibooks.org/wiki/Haskell/do_Notation As others have suggested, go read Learn You A Haskell for Great Good. It's free, and a great starting point. http://learnyouahaskell.com/ I wrote a little bit about what I found useful when I learned Haskell here: http://bob.ippoli.to/archives/2013/01/11/getting-started-with-haskell/
It looks like [FPH](http://research.microsoft.com/en-us/um/people/simonpj/papers/boxy/) was meant to support this. In fact, it's the motivating example; &gt; Consider this program fragment: &gt; &gt; ($) :: forall a b. (a-&gt;b) -&gt; a -&gt; b &gt; runST :: forall r. (forall s. ST s r) -&gt; r &gt; foo :: forall s. Int -&gt; ST s Int &gt; ...(runST $ foo 4)... &gt; &gt; Here ($), whose type is given, is the apply combinator, often used by Haskell programmers to avoid writing parentheses. &gt; &gt; From a programmer’s point of view there is nothing very complicated about this program, yet it goes well beyond the traditional Damas-Milner type system, by using two distinct forms of ﬁrst-class polymorphism: &gt; &gt; - `runST` takes an argument of polymorphic type—`runST` has a higher-rank type. &gt; - The quantiﬁed type variable `a` in the type of `($)` is instantiated to the polymorphic type `∀ s. Int → ST s Int`. Allowing the instantiation of quantiﬁed type variables with polytypes is called impredicative polymorphism. &gt; &gt; Our goal, which we share with other authors (Le Botlan and Remy 2003; Leijen 2007a), is to make such programs “just work” by lifting the restrictions imposed by the Damas-Milner type system.
There are many ways to understand monads. For me it was Dan Piponi's [talk at ICFP 2009](http://vimeo.com/6590617). I think Mr Piponi is cornering the market.
Thank you, this is the kind of answer I was looking for.
Why does GHC compile to native instead of VM bytecode? That makes little sense to me.
I'm looking to expand my mind. I know F#, Python, some perl, and javascript.
It actually compiles to C, which compiles to native. (That has the side effect of providing it with a C FFI, instead the Java one that Clojure likely provides).
If you don't mind me asking, what do you have against the JVM?
Well, nothing in particular really: I am just answering your question. I didn't mean anything against it with my second line. I do think that a C FFI can be more interesting in some cases because code programmed in it will be faster than Haskell (or JVM code), so it allows you to use that speed in a more high-level language like Haskell. (I have absolutely no idea how interfacing with native code works in the JVM, however, so, as I said, I am not saying anything against the JVM, because I really can't)
The biggest problem with the way our community talks about monads, is that people who learn it are never sure they actually got them, even though they did, because there’s always this aura that there must be “more”, since everybody parrots about them being to “hard”. And so people start thinking themselves that they hit a brick wall or something, and that it‘s so “hard” to learn them. Of course the bad tutorials are part of that problem. **To all the people learning monads:** Don’t let anyone tell you they’re hard. They’re not. * If you think of a constant as level 0, a variable as level 1, a function as level 2, a higher order function as level 3, then monads could be seen as level 4. Something you can apply to a higher order function to get another higher order function. (Chaining naturally follows from that.) * You can also think of them as “the semicolon as an operator, plus another variant of that semicolon, plus some helpers”. With said “semicolon” being programmable, to implement whatever magic you want. Error handling, logging, aspect-oriented programming, state, I/O… (To make that work in Haskell, the stuff that is separated by the “semicolons” must simply be higher-order functions.) * Yes, you can see the stuff that is passed through those normal functions, that are passed through the higher-order functions as something that is “wrapped”. But it’s only one way of seeing it. * And finally: I/O is just the concept of “wrapping” our whole universe with *everything* outside of the deterministic Haskell code, as if it were a state. The compiler does some “magic” to make that work. **That’s** ***it*****.** Now go and design your own monad. Easy as pie. and avoid do notation a bit. It obfuscates the feel monads should give one.
Congrats, you wrote yet another monad explanation.
You *could* have mentioned that “P” stands for “parametric”…
There are advantages and disadvantages. The JVM has its points. Compared to haskell, clojure on the jvm has a slower startup time, is slower in execution, and uses more memory. Also, haskell programs are native and don't require any other installations (ie the jvm) in order to run. That can make for a very minimal install with few dependencies. Haskell might be more appropriate in cpu or memory limited applications, ie embedded. Also for things like command line programs that have to start and stop quickly (rewriting 'grep' for instance). On the other hand, JVM bytecode is very portable. Compile once, run anywhere. As a compiled language, haskell can't do that, it has to have a different executable for each environment. If the application is not cpu limited or memory limited, then to me its great to be able to run it whereever without rebuilding. Plus, clojure compiles into javascript too which also runs many places. 
Equational/algebraic reasoning, plus often simplification via algebraic identities I "prove" using quickCheck.
It doesn't compile to C.
I usually copy and paste smaller and smaller parts of the code into GHCi to see which function goes wrong. I really should be writing tests instead.
I use both clojure and haskell at work. I love both of them But i would use clojure only if there are no libraries i need in haskell. After you get enough experience in both languages there's simply no contest. This is especially true for refactoring. Every big refactoring i have done in clojure broke my code and there was no help from compiler or any tool for me. Haskell on the other hand makes it quite safe to rewrite quite big chunks of code without introducing bugs. 
Lack of proper tail calls, for one?
For those who are less familiar with pipes here is the cyclic example that has the loop `a1 -&gt; a2` and `a2 -&gt; a1` initialized with the 1 and 2 in the loop. `PrintD` is used to print out the the numbers flowing through a1. import Control.Concurrent.Async import Control.Proxy import Control.Proxy.Concurrent main = do (in1, out1) &lt;- spawn Unbounded (in2, out2) &lt;- spawn Unbounded a1 &lt;- async $ do runProxy $ (fromListS [1,2] &gt;=&gt; recvS out1) &gt;-&gt; printD &gt;-&gt; sendD in2 performGC a2 &lt;- async $ do runProxy $ recvS out2 &gt;-&gt; sendD in1 performGC mapM_ wait [a1, a2]
Thanks for pointing that out. Also, here's a slight variation which also limits the loop to 10 elements flowing through to verify that the loop still cleanly shuts down afterwards despite being cyclic and being jump-started with a few values: import Control.Concurrent.Async import Control.Proxy import Control.Proxy.Concurrent main = do (in1, out1) &lt;- spawn Unbounded (in2, out2) &lt;- spawn Unbounded a1 &lt;- async $ do runProxy $ (fromListS [1,2] &gt;=&gt; recvS out1) &gt;-&gt; printD &gt;-&gt; sendD in2 performGC a2 &lt;- async $ do runProxy $ recvS out2 &gt;-&gt; takeB_ 10 &gt;-&gt; sendD in1 performGC mapM_ wait [a1, a2] The output becomes: $ ./cycle 1 2 1 2 1 2 1 2 1 2 1 2 $
But how much state do you really need to carry around in your head in order to think about how to solve most problems in a modern, high level imperative language compared to something like Haskell?
If there's anything the community in general parrots, it's "monads aren't as hard as everyone says they are". Which is exactly what you're parroting here, of course.
But I "get" it now as a result of his explanation.
Conal Elliot gives a number of fairly clear definitions of one brand of FRP. Push-pull semantics is a good paper about it.
The sentence I objected to was "streaming libraries overlap substantially with functional-reactive programming frameworks" and also the title of the blogpost itself that implies that `pipes-concurrency` _is_ reactive programming. Reactive programming is a pretty broad term, that in my mind constitutes any form of dependency management and change propagation. _functional reactive programming_ on the other hand I think should be reserved for purely denotational systems in the mold of what conal describes here: http://stackoverflow.com/a/1030631/371753 The sort of model you describe building seems like it might be related to the netwire style signals-and-wires model (http://www.haskell.org/haskellwiki/Netwire) of arrowised reactive programming. However, that's actually a world away from the semantics of e.g. reactive-banana or reactive, or even the discrete-time semantics of the stuff neelk has been doing with metric spaces: http://research.microsoft.com/apps/pubs/default.aspx?id=158828
http://www.haskell.org/haskellwiki/GHC:FAQ#Why_isn.27t_GHC_available_for_.NET_or_on_the_JVM.3F
I would only caution you to avoid thinking about any specific implementation of `IO`. The really important lesson to learn is that `IO` actions are just ordinary values and they are not given special treatment. `do` notation just combines `IO` actions into larger `IO` actions, and a Haskell program is nothing more than a pure way to assemble an impure program.
I still don't see the basis of your claim. The people who typically give up on haskell are the ones who never intended to actually learn it in the first place (in my opinion based on my experiences with many people learning the language). 
That Dan Piponi, talk led me to this [talk.](https://www.youtube.com/watch?v=Sr_tEm3Kxc0) Paul Andre Mellies. Uses string diagrams on game semantics, linear logic. edit and that talk has led me to http://www.pps.univ-paris-diderot.fr/~mellies/tensorial-logic.html
&gt; I use both clojure and haskell at work. Where does he come from, who are his people? These are the things I must know!
Studying haskell at the moment for my mid term. This came at exactly the right time! Thanks Internet! 
Disclaimer: I haven't followed the pipes, conduits, free monads et al. story since the initial rush of interest lead to a hydra of different implementations, blog posts, reddit threads... Isn't the monadic structure of pipes essentially the resumption monad? If so William Harrison has been working in this area with Resumption and the extension Reaction monads for quite a number of years. Particularly modelling OS schedulers and threads.
`pipes` are overkill for a resumption monad. A simpler way to implement the resumption monad is `FreeT Identity` (using `FreeT` from `Control.Monad.Trans.Free`). For me, the thing that most clearly organized my thoughts was reading Mario Blazevic's concurrency article from [issue 19 of The Monad Reader](http://themonadreader.files.wordpress.com/2011/10/issue19.pdf). The `Coroutine` monad transformer he introduces is identical to `FreeT`, and he explains many common concurrency abstractions by implementing all of them in terms of `Coroutine`/`FreeT`. The one you would be most interested is the very first abstraction, which he calls the "trampoline" monad, but it is equivalent to the resumption monad. The `Pipe` monad was directly stolen from that article and is equivalent to his `Coroutine InOrOut` monad, although I later generalized it to the bidirectional version that you see today. However, if you were to implement the resumption monad in terms of `pipes`, the closest equivalent would be `Producer p () m r`, i.e. a `Producer` that suspends by `respond`ing with empty values.
This is fun! I played with it a little and it seems like there's a lot of potential here. I'm hoping it'll improve, but so far I've noticed the following things. * It can feel a bit tedious to use and tends to wind up wordier than the equivalent javascript code * I can't really drop into purely functional code for anything more complex than math (but that's expected with kind of edsl) * The jQuery interface leaves a lot to be desired * The semantic differences (names vs values) also tends to break my brain. Names vs values example: a &lt;- empty :: JS t (JSArray JSNumber) a # insert' 1 1 let x = lookup' 1 a console # log x a # insert' 1 2 console # log x -- Compiles to: -- -- var main = (function() { -- var v0 = []; -- v0[1] = 1; -- console.log(v0[1]); -- v0[1] = 2; -- console.log(v0[1]); -- })(); 
The problem is that monads can be very different to each other think Maybe vs List vs State. Even though one may "get" Maybe, it doesn't apply to any of the other monads.
I've cleaned up the syntax, and the program. Compile it, test it, and compare it to your version http://hpaste.org/85825
One big thing that is definitely worth learning from Haskell that none of the languages you mentioned have, is type classes. If you're looking to expand your mind and already know F#, then except for type classes you might learn more from a completely different language, like an assembly language, a dependently typed language, or a logic programming language.
The JVM has GC built-in, but not a very good one for Haskell purposes. The runtime really needs to be built with a lazy language in mind for it to even be competitive. In addition, a whole bunch of work done on GHC with regards to multi-threading (STM, CloudHaskell, IO manager) would have been impossible.
In the page you linked to ifThenElse is not mentioned as rebindable. In the latest (stable) ghc however, it is: [http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#rebindable-syntax](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#rebindable-syntax). Which is pretty cool!
For those wondering: this is a combination of HIP and quickspec.
&gt; Which is exactly what you're parroting here, of course. Far be it from me to defend Evi1M4chine, but at least with respect to that much he’s right; monads don’t have to be as hard as newcomers expect. (I don’t endorse his own explanation, however.) To my mind there are three genuine problems: 1) Monads tend to represent a confluence of new ideas. Higher-order parametricity; imperative programming in purely functional code (IO, State); Haskell-specific idiosyncrasies (do-notation, the workings of the typeclass mechanism) … 2) Monads are too generic to have a single nice, simple, unifying example. Thus explanations of monads mix together with explanations of the motivating examples, which may be difficult to understand in their own right. Further, concentrating on one or two examples can give the wrong impression about what monads are “for”. 3) People want it *now*. Monads have traditionally gotten press, so we have people who’ve done almost no functional programming, let alone in Haskell, trying to understand monads on day one. Don’t introduce monads until the Haskell newcomer understands typeclasses and has written a nontrivial amount of non-monadic Haskell code, and the process will be much smoother.
&gt; To all the people learning monads: Don’t let anyone tell you they’re hard. They’re not. To some real extent I agree, although I’m not sure that your explanation does the trick. Newcomers need to understand some Haskell and come to terms with a few new ideas (e.g. typeclasses); the resulting process will be much smoother. The programmable semicolon and function-wrapping stuff may give the wrong impression about what monads are about, and will confuse newcomers besides. However, there’s definite merit to this: &gt; Something you can apply to a higher order function to get another higher order function. Using Kleisli composition instead of bind would make the idea of function-chaining clearer, I think. At the very least it’d make monad laws easier to understand: that return and bind are *kind of* like inverses, and bind is *kind of* associative, can seem clear as mud.
As far as I know there's insufficient material available to teach you how to program using Haskell as the first language. Sooner or later someone will provide that material, but until then... Bad Idea. Sorry. So, Python? I don't think so. Not Ruby either. Too many funny shaped corners and curved edges for a learner. Also, I can't quite think of any decent learn-to-program material for either. You can do better. I guess it should be clear that I think "learn to program" and "learn a programming language for the first time" are different. Scheme/Racket seems to be a good choice, and certainly a proven choice. You'll get excellent coverage of the fundamentals in a powerful and expressive programming language. And the knowledge will be transferrable. As for 'first working language'... Haskell might be a very good choice. It's a choice I've made, at the moment my current 'preferred set' of programming languages is Haskell, Ruby, JavaScript, and Clojure (for the JVM (Java Virtual Machine)).
&gt; No, it's not optional in Haskell. Everybody uses the type system. That doesn’t answer his question, although I’m not sure what the right answer is. I think the design of Typed Clojure was inspired by that of Typed Racket, if it helps, although the former seems to allow a greater degree of ad hoc polymorphism.
Oops! Thanks for catching that. I'll updated it in the next patch.
The concurrency stuff would be a bit difficult to integrate because it relies on the garbage collector, which is local to the program, but `pipes` in general could hook into cloud haskell. I do know how to suspend pipelines in the middle of computation, but the hard part is figuring out how to serialize and transfer them over the network, mainly because I haven't studied how cloud Haskell does it for processes.
The approved interface for suspending pipes is going to go in the pipes-free library (mainly because it uses a `free` dependency). It will greatly resemble io-streams, but also provide a pure interface, too.
Is `arr (uncurry mappend)` what you are asking for?
CPP already interferes with what should be legal literate Haskell even if CPP is not being used, so the first project could fix a small GHC noncompliance bug on the way.
I think Windows users should contribute project ideas, too, since they know that the Haskell experience is not as great on Windows. I remember there was an idea floating around for distributing precompiled external libraries for Haskell platform packages so that people wouldn't need to immediately install MingW.
&gt; so that people wouldn't need to immediately install MingW Can't they just use the gcc that comes with GHC? I usually only install MSYS and add `%PLATFORMDIR%/mingw/bin` to `%PATH%`.
While a worthy project on its own, I'm not sure adding Markdown to Haddock would be a summer-worthy project on its own, because Pandoc. GHCi, version 7.4.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; :m + Text.Pandoc Text.Blaze.Renderer.Utf8 Prelude ...&gt; renderHtml $ writeHtml def $ readMarkdown def "Hello world\n\n* I like Haskell\n\nYou should too." Chunk "&lt;p&gt;Hello world&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;I like Haskell&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;You should too.&lt;/p&gt;" Empty It might need to be wrapped into a more general project to update infrastructure. I mean, I know that's not the whole project, but it's not like it involves writing any sort of parser.
Integrating anything into Haddock is more effort than you might at first assume!
If anyone does decided to do individual modules in parallel I would suggest building on top of Shake, which already compiles individual modules in parallel. However, I would warn you that I've seen &gt; 2x slower. If you are building one project you need about 4 CPU to outperform (since you don't generally get a consistent parallelism level of 4), or if you are building multiple projects (where you can get better parallelism) then 3 CPU is enough.
This looks pretty nice. How does it compare with [jmacro](http://hackage.haskell.org/package/jmacro)?
Memory usage. It needs to load soooo much into memory that it is not even feasible to use it for writing small cmd line tools with. http://benchmarksgame.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=java&amp;lang2=ghc
This looks like a good idea, Peter :)
Good discussion on HN earlier today: https://news.ycombinator.com/item?id=5550930
Are there laws or properties you can state/prove about the behaviour of `pipes-concurrency` as used for reactive programming (i.e. the Event type you synthesized)? 
Oops, I probably meant `MSYS`. You can tell that I've never installed it on Windows. :)
I don't think it would be appropriate to use pandoc for this, for a number of reasons. Pandoc is a big project with lots of dependencies, but haddock should have minimal dependencies, as it is needed for everything. And haddock-markdown would need to extend standard (and pandoc) markdown with some haddock-specific things like single-quoted identifiers. Something like [this](https://github.com/jgm/Markdown) might be a better starting point. 
I don't know the author's motivation, but for me lens is just too big. The number of type synonyms, functions and operators is depressing. I don't see any benefit worth to learn all of those. And data-lens has been enough for anything I do with lenses so far.
Long live fclabels!
&gt; Note that LGtk use lots of impure lenses (lenses which do not fulfil the lens laws). &gt;Using lenses which do not fulfil the lens laws are safe, but one should take extra care when doing program transformations or reasoning about code with impure lenses I am not terribly concerned, but is there a good way to reason about these (not)lenses? I.e are there any laws that accompany them? 
You could try scala. They have type classes, higher kinds etc. like Haskell, but you could conceivably interop with your Clojure code and make use of the jvm libs you already know about. 
&gt; Far be it from me to defend Evi1M4chine, but at least with respect to that much he’s right; monads don’t have to be as hard as newcomers expect. Well, yes. That's why the sentiment is so often expressed. I was merely objecting to presenting a very common sentiment as if it were a radical and contrarian position.
[Section of Learn You a Haskell on Randomness](http://learnyouahaskell.com/input-and-output#randomness)
=(
I can't reason about "monadic lenses". I have no idea what the laws mean, and nobody has been able to tell me for anything more complicated than ones involving `Maybe`. There are no useful laws you can apply to the lenses used by this package. You can't use any of the laws to reason about fusing reads or writes. `lens` is admittedly rather poorly named. Most of its focus is on traversals and prisms and things that can't be expressed in these other frameworks. Unlike "monadic lenses" the laws _can_ be consistently applied across all of these variants. Given the large number of minimalist lens libraries, I feel few compunctions about deliberately choosing to write one that takes the opposite approach. Even if it does occasionally get mocked for one of the sets of combinators it offers. =P All of that said, `lens` doesn't provide a flavor of lens that even could be used to write this kind of code, so take all of that with a grain of salt. I don't give you a kind of lens like this because I can't reason about it. If you can accept living in the wild west, feel free to use them. I can't say too much bad about them, I mean, I wrote `data-lens`, too. =)
That is pretty much why `lens` still doesn't offer monadic lenses. There is no coherent set of laws for them. We offer 'actions' which are equivalent to Kleisli arrows, which can be composed with getters and folds, but nothing further.
That's true. Although literal Haskell sees very little use in practice so we care less about supporting it well.
Well that turned out to be wishful thinking...
It seems like there's _something_ interesting going on here, but it looks like it has very little to do with lenses in my mind. The key thing for declarative layouts is rather this `I` type and the interpreter over it. I'm quite confused as to why it is parametric over an arbitrary monad when the only interpreter given is for IO? I'd like to see a walkthrough of how `I` is structured, and perhaps some examples with more interaction between various bits of the structure -- i.e. what mechanisms are there for some updates triggering other things in a very non-local way (with regards to structure in an interface) and for some updates to be "bunched up" and only triggered later -- i.e. a form is filled out but another part is rendered only on click of a button? A calculator app is a good test-case for these sorts of things, I've found. Edit: ok, looking at this more I see that `Ref` and `ExtRef` are where all the action are. Basically this is a way to extend reads and writes to perform arbitrary actions along the way. So it provides, in essence an effectful map/comap. That's how we attach listeners to values, etc. So in the end we really have an "Observable" model along the way of some sort. It would be useful to see this spelled out/explored a bit more in how its used.
I find it interesting that the author chooses to implement his monadic lenses fused together. data MLens m a b = MLens (a -&gt; m (b, b -&gt; m a)) Once you put an effect on the read, there is an actual detriment to fusing together the read and the write, that is that you have to pay to read to write. Also the `data` there rather than the `newtype` strikes me as an odd decision. I would suspect that for this style of lens: data MLens m a b = MLens (a -&gt; m b) (a -&gt; b -&gt; m a) would be more efficient for many use cases and would avoid spurious read conflicts when you just want to write blindly, as for this kind of lens you are probaby more concerned with false serialization for things like `MLens STM` actions than you are with sharing the traversal down to a given point between the getter and the setter, and you can't do all the `lens` tricks to fuse multiple passes together for traversals anyways. If offers less efficient composition though, so perhaps the horrible compromise data MLens m a b = MLens (a -&gt; m (b, b -&gt; m a)) (a -&gt; b -&gt; m a) which provides the 'fused' read and a separate direct write might be appropriate, despite making me want to throw up.
why not newtype MLens m a b = MLens (a -&gt; (m b, b -&gt; m a)) 
Consider something that accesses an IORef, and then walks a tree inside and then walks through another IORef obtained by composing three "monadic lenses" together. In that scheme you have to rewalk the tree rather than share the original element traversal, so the author's concerns about needing to pay more than you need to go down and get the element get realized with that form of fusion.
So it is difficult to formally reason about concurrent IO, but if you take a common sense approach then I believe you could define an Alternative instance for Event, where union is `(&lt;|&gt;)` and the empty stream is `empty`.
Things I LOVE about `lens`: * Theoretically principled * Traversals and setters * Polymorphic update * No buy-in Things that I do not like as much: * Huge dependency graph * Beginner unfriendly * Lots of small inelegant type classes The problem is that lens is all about continuation passing style, which is a very unrestricted form of programming, and what made the original conception of lens so elegant was that you tamed the CPS wilderness by filtering all CPS computations through theoretically principled type classes like Functor and Applicative. However, once you depart from those standard type classes and just start coming up with these small arbitrary type class you lose the original rigor the library once had and descend back into the CPS wild west.
We've been steadily trying to bring the wild west under control, but to do that first you need to know what is out there. That is why with lens 3.9, you can finally define a `Getter` and `Review` using just `Contravariant` and `Profunctor`, which are available in Haskell 98 packages. Previously they had relied on small ad hoc classes defined in the package itself. The indexed machinery is indeed less friendly to newcomers, and in many ways the current state is a consequence of trying to compromise between the most general signatures possible and still having the 'no buy in' model for easy user lenses. We could achieve a higher level of overall elegance if we were willing to sacrifice the 'no lens dependency' principle and give up on `traverse` being a `Traversal` by going to "pure profunctor lenses", but this would take us to a place where we need more of those arbitrary-seeming classes you dislike so much. The ad hoc type classes for things like `At` and `Contains` are a somewhat artless compromise as well. My best defense for them is that "they are useful." We tried living without them and just having separate modules for `Data.IntMap.Lens`, `Data.Map.Lens`, that provided monomorphic versions of everything, and the result was pretty terrible to use. So at that point the compromise either becomes not providing them, which runs afoul of the "Batteries Included" goal, or providing them via small inelegant type classes, which is the path we ultimately selected.
Don't worry. I still love your library even if I keep nagging you about it. I use it all the time in my projects. Also, I am really glad that you stick to the no buy-in principle, which makes your library an excellent contender for fixing Haskell records.
Where is the list of GSoC project ideas? The [Ideas page](http://hackage.haskell.org/trac/summer-of-code/report/1) link has no 2013 timestamps at the moment.
I get it: you can't build the tuple without "escaping" the monad which forces you to use the slow composition operator 
We used Shake to make a custom build system [at work][spaceport] and it has worked remarkably well in fairly large scale. One pain point for me is the lack of type-safe file paths and dependency information—very little can be statically checked, so debugging is still not ideal. (I may have been spoiled by static typing.) Regardless, it’s very nice to have Haskell as the language for specifying build rules. [spaceport]: http://spaceport.io/ 
I'm a bit confused how it even works with the current setup. If I have two different lenses that 'traverse' the same source lens, I read from both of them, then apply a setter to one, it seems quite likely that the second setter is now completely stale and likely to trash my state. I can't come up with an example off the top of my head, but something with IORefs forming a tree or priority queue seems like a likely candidate for breakage.
The obvious laws for a monadic lens getM :: MLens a b -&gt; a -&gt; m b setM :: MLens a b -&gt; b -&gt; a -&gt; m a are setM l b a &gt;&gt;= getM l = setM l b a &gt;&gt; return b getM l a &gt;&gt;= \b -&gt; setM l b a = return a setM l b a &gt;&gt;= setM l b' = setM l b' a That doesn't look too bad for reasoning.
This is often only true these days if you have *very* compute-intensive code *and* you you know what you're doing when hand tuning. The old call of JVM/garbage collection being too slow is much less true today than in the past. Are you prepared to write your own high performance alloc, etc? 
I don't think that gtk getter/setters would satisfy any of those laws.
Really? IORefs would satisfy the laws, as would a normal lens composed with a monadic lens. Something like the label of a button or the contents of a text entry field should also satisfy the above laws, maybe except for redrawing of the control. Confusingly, I don't actually see any gtk getters/setters/lenses in the lgtk package.
For the lazy, I think this is the github repo: https://github.com/danr/hipspec This looks really interesting, I'll have to get Z3 running under wine again :) 
&gt; Also the data there rather than the newtype strikes me as an odd decision. Fixed, thanks!
Correct me if I'm wrong, but it seems like the `Ext` implementation has a memory leak and possibly a time leak. First, in terms of the memory leak, whenever `extRef` (or even `newRef`) is called, a record describing this new reference is added to a `Seq` embedded in the state and will only be deallocated when `runExt` or `runExt_` is called. The only way I see of getting around this is some trickery with weak references. The time leak I'm not so sure about, because I don't completely understand the implementation, but it seems that every time you write to a reference, it updates all lenses that are dependent on that reference. Unfortunately, given that, as described above, lenses build up over time, this could take arbitrarily long. Again, weak references are the only solution I can come up with. On the other hand, in terms of usability and ease of making a GUI, this library looks great. I'm looking forward to trying a big project with this library - it looks to me that from a project size standpoint, this library has no scalability issues. I can also thing of various other projects that could benefit from the ability to make ExtRefs.
On the other hand, this seems rather self fulfilling. 
You were digging into the source, great! &gt; I'm quite confused as to why it is parametric over an arbitrary monad when the only interpreter given is for IO? If you look at the examples, they have type like (forall m . ExtRef m =&gt; I m), so theoretially it is possible to test them purely without running them in the IO monad. &gt; It would be useful to see this spelled out/explored a bit more in how its used. You have seen only a rough overview so far; I'm going to document the details! &gt; A calculator app is a good test-case for these sorts of things, I've found. Thanks, I consider to present a calculator app.
At least we share some problems :)
&gt; I find it interesting that the author chooses to implement his monadic lenses fused together. We could also use newtype MLens m a b = MLens (a -&gt; m (m b, b -&gt; m a)) which may have both benefits (efficient composition and setting without getting).
If anybody is interested in proposing or working on anything games related, drop into #haskell-game and say something! The channel is overall not very active, but if you say something and then lurk for a while somebody will come along. A few of us there are beginning an initiative to improve the experience of making games in Haskell and are considering proposing and mentoring a GSoC project. We don't have anything concrete quite yet though. 
&gt; I can't reason about "monadic lenses". As twanvl said, there are nice laws for monadic lenses too. Consider that (forall m . Monad m =&gt; MLens m a b) is isomorphic to (Lens a b) and replacing Monad with some richer structure you may allow as many effects as you wish. You can also think of monadic lenses as structuring monadic code, which is of course better than unstructured monadic code.
This is so needed. The only testing tool I'm still longing for is the difference of two arbitrary data structures. `gdiff` might be enough to implement this, I'm just out of time to work on that at the moment sadly.
Remarks about impure lenses (maybe not for you). If you say like "non-commutative multiplication is not allowed" you lose quaternions. Or "multiplication should be at least associative" you lose octonions. The stricter you are, the more you lose. Intstead, we should allow everything which helps to write nice code and we should study which laws hold for the constructs we use. If we allow a nonassociative (%) operator for example, we may even code more information into the source: ((a % b) % c) will denote a different intention by the programmer than (a % (b % c)). 
Those two would still have the same type, right? It wouldn't be very much work to port the rematch matchers with diff in the error message instead of just both versions printed out. I wouldn't want to do that in core, but I might take a look at doing another library with the same matchers, just diff'd error messages. It might be a while though.
Don't get me wrong, the basic idea is beautiful. But any package with 90+ operators is scary.
&gt; the Ext implementation has a memory leak and possibly a time leak You are right, it has both! It is a reference implementation so I don't regret. An IORef based implementation should have much better performance. &gt; On the other hand, in terms of usability and ease of making a GUI, this library looks great. Thanks! &gt; I can also thing of various other projects that could benefit from the ability to make ExtRefs. Do you have some ideas already? Maybe the library should be split into two?
STLC = simply typed lambda calculus
Very nice!
Yes, I got similar results with my experimental "parallel ghc" tool. The way forward, IMO, is to add a "build server" mode to GHC that would allow to compile multiple modules using a single GHC process, thus sharing the .hi cache. Current thinking on this topic is summarised here: https://github.com/haskell/cabal/issues/976
Thank you for this comment, I apologise for my tardy response. I tried using a similar approach as you mention in your paper for Arrows but it became rather inelegant. I will put it online as it is now part of my private repository. I now build the R datatype directly without using the Category and Arrow instances. When RebindableSyntax in combination with the Arrow Syntax [is fixed](http://hackage.haskell.org/trac/ghc/ticket/7828). I can use the Arrow syntax.
Sure. I definitely can't say that the lens API isn't scary. =) I did try to mitigate the damage by providing them with a consistent morphology, though. They mostly follow the pattern of `operator=` or `operator~` depending on if they apply the operator to their target in state or "functionally". From there most of the variants are forced by the principle of least surprise, and minor morphological variations. `&lt;operator=` and `&lt;operator~` return the edited target, rather than round trip through the stack a second time. Half of these would have been completely unnecessary had the safety police not won and made it a crime (or at least a warning) to ignore a non `()` return type in a monad. These covers something like 80 of those operators that I get so much crap about. =/ I also accept that there are some people I'll never win over with that argument. To that end we've split it so you can import `Lens` qualified and import parts of `Control.Lens.Operators` to get whichever operators you want to bring in scope, and in the next major release we're looking to export a module that just provides the combinators and no operators so that you don't need to use the base library qualified just to avoid the operators. Everything you can write with an operator can be written without, or with a much more restricted subset of them, its just a heck of a lot more verbose.
IORefs only satisfy these laws in the absence of other mutations to the same IORef. If I mutate the same IORef later in the chain, by composing through the same one twice in the presence of the cycle I break a law. If someone else mutates my IORef behind my back in the middle of me doing a read and write, I break a law.
That is probably the least bad option.
Now that I thought about it, I think we should just ship MSYS with the Windows HP installer. 
I'm close, but I'm still missing something. The goal is to return two arbitrary elements from an array of (in this case, seven) Characters. I'm trying to be as functional as I can. choose :: [Char] -&gt; [Char] choose s = do gen &lt;- getStdGen let (randIndex1, gen2) = randomR (0, 6) gen :: (Int, StdGen) let (randIndex2, _) = randomR (0, 6) gen2 :: (Int, StdGen) (s !! randIndex1) : (s !! randIndex2) : []
"monadic lenses have no law" is a strong statement. Most lenses fulfil at least (getL k a &gt;&gt; return () === return ()). &gt; And as I replied to him, nothing here even satisfies those. What about (l = fstLens)? Edit: deleted a sentence, added another. 
`fstLens` is a trivial normal lens that happens to be hopped up into the monadic lens framework. ;) It doesn't take advantage of the new functionality so of course it satisfies the old laws.
 type Ref m a = MLens m () a Reference laws for pure references: * (readRef r &gt;&gt; return ()) === (return ()) * (readRef r &gt;&gt;= writeRef r) === (return ()) * (writeRef r a &gt;&gt; readRef r) === (writeRef r a &gt;&gt; return a) * (writeRef r a &gt;&gt; writeRef r a) === (writeRef r a) 
| I use (forall m . ExtRef m =&gt; Lens m a b) and I would like to reason about them. If I understand my program, there is a possibility for reasoning I guess. The problem is it requires global reasoning about the program. You need to know that you don't have a ref chain that passes through the same Ref twice, that nobody is mutating your values behind your back, etc. in order to pretend the laws hold. I'm not above writing such lenses, in fact I have several such not-quite-legal lenses floating around in the `lens` library. I just feel the need to document the bejeezus out of the fact that they aren't safe in general to try to put the onus on the user to check these properties.
So nobody is mutating the contents of your reference but you? e.g. you aren't handing it off to something in the gtk system and having it mutate in a user interface thread? Otherwise the * `readRef r &gt;&gt;= writeRef r` and * `writeRef r a &gt;&gt; readRef r` laws are dangerous. Also the last one can result in alternate interleavings in its observable state to that other thread.
Every time you use bind in the JS-monad a new name is generated for what you are computing. Everytime you use let or a top level declaration you are using it as a macro that just copies its contents.
&gt; So nobody is mutating the contents of your reference but you? Nobody, I work in a single-threaded environment currently. (Should be documented.) &gt; e.g. you aren't handing it off to something in the gtk system and having it mutate in a user interface thread? I handle references stored in (I m) interface descriptions to Gtk. Gtk mutates them as it wishes, but in a single-threaded way. There are no interleavings.
So then the only case you have to worry about are walking through the same ref multiple times. Assuming you take responsibility to ensure that that never happens, I am willing to accept that you can reason about it. ;) It is the need for all those caveats and global constraints that motivates why I shy away from monadic lenses to begin with. I rarely work in a single threaded environment, and rarely do I get to control the entire program as a library author.
&gt; You need to know that you don't have a ref chain that passes through the same Ref twice Could you elaborate this? &gt; that nobody is mutating your values behind your back This holds in my single-threaded environment. 
Consider something like: data Cell a = Nil | Bin a (Tree a) (Tree a) data Tree a = Tree (IORef (Cell a)) Now, if you create a cyclic tree, and you go down and mutate things on your way out, you actually wind up changing the cycled ref twice or more, depending on how deep you go through the cycle. Once for each deeply nested write to that cycle once for the final write. The nested writes are overwritten as you come out. This leaves the cyclic structure in a very hard to reason about state and the lens laws are no help. You can get away with this for limited acyclic uses of references such as unification with an occurs check, but not with arbitrary user references.
in an arbitrary environment (see the other discussion) 
Even `getL k a &gt;&gt; return ()` is somewhat problematic if you start using it in `STM`, because it can introduce a false dependency on a `TVar`.
Sorry if I came across strong. I don't mean to sound like I don't think this library should exist! It is a very useful point in the design space. ;) I just wanted to make sure it that the limitations of the approach you are using are clearly spelled out lest others unwittingly dive into the same approach without being aware of the caveats. =)
lens is not really about "taming" continuation passing style or anything like that. lens is about a bunch of interrelated abstractions that would be interrelated anyway, but the cps-like definitions put them in a common format so that more general concepts automatically specialize to less general ones, which vastly shrinks the number of duplicate things you have to write. Regardless of how you write anything, equal types are isomorphic, and isomorphic types induce lenses to one another. Lenses induce traversals (as they are like linear traversals), and traversals induce folds. And likewise there is a dual to lenses, prisms, which also induce traversals (in fact, people thought of prisms as duals to the `s -&gt; (a, a -&gt; s)` form of lenses, and then figured out how to write them in a way that'd fit into lens nicely). All of the above is true anyway. But there are multiple ways to write down any one of those things above. What people have been doing in lens is finding a sufficiently similar format for all of them so that "induces" becomes "is automatically usable as". That the common format looks like cps is, mostly, incidental.
Thank you, your comments really help to improve the documentation!
I'll try write code with the current interface to present this problem. I am afraid I have to ban cyclic user interfaces too to avoid this :) I don't know how serious this limitation will be though. 
Thanks for mentioning this, I'll try to remember this when I'm going to define TVar monadic lenses (not planned). Edit: It seems that monadic lenses have Iaws relative to the underlying monads. 
Yes, if I have a `Family` with a list of `Children`, where each `Children` has a `childrenName` - if I expect 5 `Children` with given `childrenName`s I want to have that pointed out to me exactly which don't match, without me having to do this work myself.
Thanks for this, from a quick glimpse of the API it looks nice :)
Could you please elaborate? `arr (uncurry mappend)` has the type `Monoid m =&gt; a (m,m) m` if I am correct. It is fine if I have just two `x`'s , but I don't know priorly how many of them do I have.
`arr mconcat`?
&gt; If I mutate the same IORef later in the chain, by composing through the same one twice in the presence of the cycle I break a law. That assumes composition. I think it is safe to only compose with monadic lenses on the right. I.e. to apply a pure lens to the result of a monadic one to get a new monadic lens. (And of course the IORefs should be used in a single thread, but that is true anyway when using them. It would be best to leave thread safety out of this discussion.)
Nice. Quick (very minor) comment: Add "(a synonym of @is@)" to the documentation of `equalTo`, so the reader is not left wondering what the difference between the two functions is. I am also missing a function for building a matcher from a predicate. I.e. `predicate :: (a -&gt; Bool) -&gt; Matcher a`. IMO this function should be called `is`, so for instance `isEmpty` becomes `is null`.
What is the point of lenses without composition? ..but point taken. ;) Thread safety is something we'll just have to agree to disagree on, because I pretty much always live in a multi-threaded world.
Looks great! Do you think it might be a good idea to generalise the list may hers to Traversable?
Firstly, I completely agree that the library should be split into two parts. The `ExtRef`s have nothing to do with GTK specifically, and, as mentioned before, have other uses. As to what those uses are, the main use case I am envisioning is transparently converting between two different data structure representations. For example, look at an imperative linked list. It can be either represented as `Ref m [a]`, or as follows: newtype List m a = List (Ref m (Node m a)) data Node m a = Nil | Cons a (List m a) With `ExtRef`s, you can write functions that convert between the two representations without copying the data. Also, with some ugly `unsafeInterleaveIO` hacks, the transformation can probably be made fully lazy. Without `ExtRef`s, it is hard to transparently change imperative data structure back ends. On the other hand, this use case requires (I think) a much less sophisticated implementation of such references, because then representations will be equivalent to one another and there is no loss of data due to slow synchronization: instance (NewRef m) =&gt; ExtRef m where extRef r k a0 = do store &lt;- newRef a0 return $ MLens $ \() -&gt; do val &lt;- readRef store inner &lt;- readRef r val' &lt;- setL k inner val writeRef store val' return (val', \outer -&gt; do inner' &lt;- getL k outer writeRef r inner' writeRef store outer)
Um, this is critically missing context, which makes it nearly impossible to follow. And I actually would like to follow it... When you say that you "a simple DSL for composing calculations and lifting Haskell functions into the language called R," do you mean the statistics software package?
Am I the only one who thought this was about a full remake of Git when reading the title?
Yep. However, it's a little misleading. Dependent types show up in a limited way in any language that has parametric polymorphism. The type: [a] -&gt; a In Haskell, is short for forall a. [a] -&gt; a Which, in Agda, we would write as: {a : Set} -&gt; List a -&gt; a The curly braces mean a parameter that can be inferred. But type variables are actually just (implicit) parameters whose type is Set (or Type in Idris) -- the type of types. Similarly, typeclasses in Haskell can be thought of as (implicit) parameters in a similar manner. EDIT - I am neglecting the impredicativity of System F above. 
It's named that because the theory allows for dependent sums and dependent products. These are useful because they correspond to existential and universal quantification in logic. It's also true that these type systems allow you to use types and values together whereas Haskell would create a split between values and types. Here are some links that may help you understand dependent types. The first one tells you how to fake dependent types in Haskell. The second one tells you how to implement the core of a dependently typed language (it's surprisingly easy!). The third explains the historical context and difference between classical and constructive logic (hopefully that's some good background reading). The fourth explains how type theory can be seen as an (important) refinement to set theory. And finally, the last one goes full type theory to explain what is special about dependent sums/products. * http://typesandkinds.wordpress.com/2012/12/01/decidable-propositional-equality-in-haskell/ * http://www.andres-loeh.de/LambdaPi/ * http://www.andrew.cmu.edu/user/avigad/Teaching/classical.pdf * http://golem.ph.utexas.edu/category/2013/01/from_set_theory_to_type_theory.html * http://ncatlab.org/nlab/show/existential+quantifier
Don't forget http://strictlypositive.org/Easy.pdf 
Sure, dependent types are strictly more powerful than System F, but the types that System F has are _not_ dependent. The separation of universes is exactly what makes it _not_ dependent!
You can formulate System F as a dependently typed language where Pi binders only bring a name into a type's scope when it's type is * (or Set, or Type, or however you want to spell it).
Yes. The typical example is vectors of a given length. You can write a "take" function which guarantees that the resultant vector will have the same length as its integer input. 
That's an old version of what you can find at the second link :)
The simplest way to do this is to fork two threads that communicate through an STM TVar of type: TVar Bool ... where the `Bool` indicates whether or not to "go". The first thread listens to user input (whatever you decide it to be) and sets the STM TVar in response to user input. The second thread then would then loop, where at each iteration of the loop it will block while the `TVar` is `False` until it switches back to `True`. The final result looks like this: import Control.Concurrent import Control.Concurrent.STM import Control.Monad main = do tvar &lt;- newTVarIO False forkIO (go tvar) forever $ do str &lt;- getLine atomically $ writeTVar tvar $ case str of "go" -&gt; True _ -&gt; False go :: TVar Bool -&gt; IO () go tvar = forever $ do -- Block while 'TVar' is false atomically $ do continue &lt;- readTVar tvar check continue putStrLn "GO" It will go when you enter `go` and then stop if you enter anything (including just hitting `Enter`).
Well, the title does say "git clone", so it's a reasonable assumption. "'git clone' clone" would be a better title.
d'oops!
As philipf [in his message](http://www.reddit.com/r/haskell/comments/1chxew/why_is_it_called_dependent_types/c9grocn), I disagree with that explanation. - We usually reserve the name "dependent types" for the dimension of the lambda-cube where types depends on values, not the other two (values depending on types, or values depending on values; is lambda dependent?). Sure, you can embed System F in pure type system, but that is indicative neither of the research history that came to System F, nor of the way people think about this system. - System F is impredicative and the dependent type systems you quote are not. Comparing it to the calculus of constructions would have been more appropriate. This is a fundamental difference. - (Subpoint of above) the fact that there are no "type of types" in Agda (without the switch making it inconsistent, of course) should have suggested that the analogy is flaky. Forcing yourself to think of System F quantification in term of dependencies encourage you to do in this way where madness or frustration lie.
Fantastic explanation.
Right, but how would I get a list of all `x`s then, without running an arrow using runX?
Wow, what an awesome answer! Am I right in saying that the way Either is used early on to collapse tuples of operations is similar to Free | Pure used with constructors of a data type (an abstract syntax tree for example?) **edit** I also don't understand this statement: &gt; The type ((Int, Int), String, (Int, Int) → c)—for any c—is a functor I don't understand how this is a functor, what is it's fmap? Is it something to do with MonoidArgument?
That was an incredibly good explanation that even I could understand. 
I usually read "dependent types" as short for "value-dependent types". To me, "value types" seems more appealing as it helps me remember *on what* dependent types depend. 
I just found this: http://hackage.haskell.org/package/data-treify
I am impressed by how thorough and clean this is. Great post.
But the (Int, Int) -&gt; c in inside the tuple?
Thanks for the doc tip. Re the function for building a matcher from a predicate, you actually need to pass an extra string in that case (to get good output from description ) At that point I barely consider it worth it to have that function. I guess you could do some TH magic to make it get the function name and the function value, but I want to keep this library pretty simple.
That's not a bad idea. I haven't really trawled the depths of haskell's typeclasses looking for things where having matchers are useful yet. I'll certainly take a look.
I think that's a really important point: forall and Pi are different and each has its place. I think of forall as "for a generic", an intersection describing one thing in common to the whole domain, and Pi as "for each", a product collecting things for the individuals in the domain. I'm very pleased to see Haskell acquiring something a bit Pi-like, albeit currently with the clunky singleton encoding. I'm also pleased to see dependent type theories acquiring "forall" (Agda's "irrelevant" quantification, and various experiments with internalized parametricity).
Yeah, it is, sorry. (My excuse is I'm ill and not thinking properly.) newtype F c = F ((Int, Int), String, (Int, Int) → c) instance Functor F where fmap f (F (x, y, z)) = F (x, y, fmap f z)
I can nearly understand it, and I'm ill! I'll have to read it again properly when I'm better.
Consider the type synonym `type F c = ((Int,Int),String,(Int,Int) -&gt; c)`. For technical reasons, this can't be made an instance of `Functor` (though you can newtype-wrap it), but if you could, `fmap` would have the type `(c -&gt; d) -&gt; F c -&gt; F d`. When expanding the type synonym `F` again, we get: fmap_F :: (c -&gt; d) -&gt; ((Int,Int),String,(Int,Int) -&gt; c) -&gt; ((Int,Int),String,(Int,Int) -&gt; d) One of the functor laws is that `fmap id === id`, so the function needs to leave the first two elements of the tuple alone. Function composition is used for the third element, as dave4420 said: fmap_F f (pos, name, setPos) = (pos, name, f . setPos) 
I agree, this is at least ambiguous - I've changed it to &gt; Reimplementing “git clone” in Haskell from the bottom up Thanks!
Thanks, glad that you like it!
Basically it's an embedding of Category plus the possibility to lift a Haskell function. The actual definition is more involved but this already shows the problem. `R` stands for reactive and not for the statistics software package.
I think virthualenv has been deprecated for hsenv now?
For what it's worth, I think that `(TId &gt;&gt;&gt; TId) &gt;&gt;&gt; f &gt;&gt;&gt; f &gt;&gt;&gt; TId` not sharing `TId` is the expected behavior. This is doubly true for tid :: forall x. Typeable x =&gt; RT x x because, I think it's rather unreasonable to expect sharing of values that have a different type. If you want to force sharing for all identical sub-expressions (instead of just detecting internal, compiler dependent sharing, which is what `data-reify` does), you might want to look at http://hackage.haskell.org/package/data-reify-cse
I found that one before but I believe it is geared towards GADTs with just one type paramater. I tried to add a class `MuRef2` to compensate this but got stuck in defining the instance `MuRef2 ty h =&gt; MuRef (ty ()) (h ())`. I believe I have to name the type parameters to `DeRef` explicitly which leads to errors further down the line that I don't know how to solve. I'm also not sure whether this is the correct way to go. class MuRef ty h where type DeRef h v a -- :: (* -&gt; *) -&gt; * -&gt; * -- DeRef h v a mapDeRef :: forall m v. (Applicative m) =&gt; (forall a. ty a -&gt; h a -&gt; m ( v a)) -&gt; (forall a. ty a -&gt; h a -&gt; m (DeRef h v a)) class MuRef2 ty h where type DeRef2 h v a -- :: (* -&gt; * -&gt; *) -&gt; * -&gt; * -&gt; * mapDeRef2 :: forall m v. (Applicative m) =&gt; (forall a b. ty a b -&gt; h a b -&gt; m ( v a b)) -&gt; (forall a b. ty a b -&gt; h a b -&gt; m (DeRef2 h v a b)) instance MuRef2 ty h =&gt; MuRef (ty ()) (h ()) where type DeRef (h ()) (v () a) a = DeRef2 (h) (v a) a () mapDeRef = mapDeRef2
&gt; Firstly, I completely agree that the library should be split into two parts. The ExtRefs have nothing to do with GTK specifically, and, as mentioned before, have other uses. OK, I put the split on the TODO list. Thanks for the usage example! Looks very interesting. &gt; On the other hand, this use case requires (I think) a much less sophisticated implementation of such references, because then representations will be equivalent to one another and there is no loss of data due to slow synchronization: Maybe this is good for your use case. I have found the following testcase which doesn't work with the given implementation: r &lt;- newRef 3 q &lt;- extRef r (lens head (:)) [] writeRef r 4 q ==&gt; [4, 3] -- readRef q should give [4,3] 
Could someone do the same thing for "codata"?
That someone is sigfpe: http://blog.sigfpe.com/2007/07/data-and-codata.html *edited to add*: I noticed a typo in sigfpe's explanation. His duality explanation should read like this: &gt; Note the duality: in structural recursion we 'deconstruct' the argument and then we're allowed to recurse. In guarded recursion we recurse first, and then we **must** use a constructor.
I feel like most of this thread degenerated into bickering over details before appropriately answering your question. The reason it's called dependent types is indeed that types can depend on terms. The are two ways this typically happens (a dependently typed language typically supports both). A function's type may use the value of its argument in the type of its result, and a tuple's type can use the value of its first component in the type of its second component. 
http://hackage.haskell.org/trac/ghc/ticket/5572 "Note that (n+k) patterns are no longer in Haskell. Use -XNPlusKPatterns to get them back"
Probably stating the obvious here, but just in case, the `(n+1)` pattern can be taken out in this case by changing to: fac 0 = 1 fac n = n * fac (n-1)
This feature, called `NPlusKPatterns`, was removed from Haskell with `Haskell2010`, because it's generally considered a Bad Idea. To solve your problem, you can try a few things: * Re-enable the language extension. This is not a very good idea, but your code will work again. {-# LANGUAGE NPlusKPatterns #-} fac 0 = 1 fac (n+1) = (n+1) * fac n The above code should work when loaded from a file, but only when the `{-# LANGUAGE #-}` pragmas appear before any other code inside that file. You could also say `:set -XNPlusKPatterns` while running GHCi. * Desugar the `NPlusKPatterns` yourself. This produces ugly code, but at least you won't need to enable the extension. fac 0 = 1 fac m | n &lt;- m - 1, n &gt;= 0 = (n+1) * fac n * Just rewrite your code. Note that this code has a slightly different type, because it doesn't compare `n` to `0`. fac 0 = 1 fac n = n * fac (n - 1) The difference becomes clear when you feed it a negative number: the above two versions will crash ("Non-exhaustive patterns in function `fac`"), the version below will either produce the wrong answer or get stuck in an infinite loop. To get around this problem, you could introduce the comparison guard again: fac 0 = 1 fac n | n &gt; 0 = n * fac (n - 1)
That version will have different behaviour when given a negative argument.
That was awesome. How have I never seen this blog before?
Сertainly, i see that version with n+1 accept well negative numbers =)) but version with n don't work with negative numbers. Thanks for all answers. =) 
I updated my post; the fourth version should be the most suitable.
(*Edit:* This assumes straight-forward Haskell98 types with exported constructors -- see counterexamples below using GADTs, or `IORef` or `DList`, for how this fails when you get too far outside of that.) There's a very simple test to determine if a data type is a functor or not. Just look at where the type variable occurs. If it only occurs in covariant positions, then it's a covariant (i.e., normal) functor. If it only occurs in contravariant positions, it's a contravariant functor. If it occurs in both, then it's no kind of functor at all. (Note that it's BOTH a covariant and a contravariant functor if and only if the type variable doesn't occur at all.) So what do I mean by covariant or contravariant position? Well, ordinary constructor parameters are covariant. The elements of products (that is, tuples) or sums (like Either) have the same variance as the whole product or sum. So if the tuple is contravariant, then its components are contravariant, and vice versa. Functions are the tricky parts: if a function is in a covariant position, then its parameters are in contravariant positions. And if a function is in a contravariant position, then its parameters are in covariant positions again. It might help with intuition to comment that covariant positions are sometimes called "positive" and contravariant positions are sometimes called "negative". So two negatives make a positive. A few examples might help: data F t = F1 t | F2 Int t `F` is a covariant (normal) functor. This is because `t` appears two places, but they are both immediate parameters to constructors, which are covariant positions. Indeed, you can write: instance Functor f where fmap f (F1 x) = F1 (f x) fmap f (F2 i x) = F2 i (f x) But then consider: data G t = G1 t | G2 (t -&gt; Int) `G` is not a functor at all, of either kind. The first occurrence of `t` is covariant, but in the second constructor, `(t -&gt; Int)` is covariant, so its parameter, `t`, is contravariant. One final example: data H t = H1 t | H2 ((t -&gt; Int) -&gt; Int) This is a covariant functor again. The first `t` is obviously covariant. The second constructor is more complicated. The `(t -&gt; Int) -&gt; Int` is in a covariant position. It's parameter, `t -&gt; Int` is therefore in a contravariant position. So *its* parameter, `t` is once again in a covariant position. Indeed, we can write: instance Functor H where fmap f (H1 x) = H1 (f x) fmap f (H2 g) = H2 (\h -&gt; g (h . f)) Hope that helps more than it confuses!
Value types are something different. Dependent types are types that depend on values, for example vectors with length in their type `Vec n` where `n` is a run-time integer. Value types says something about the run-time representation. It's the opposite of reference types or pointer types or boxed types. When passing a value type around, it is copied, rather than passed around as a pointer. This means that value types can often be stored on the stack or even in registers, rather than on the heap. For example in Haskell, a pair of integers is stored as a pointer to a memory location that has the pair of integers. In C with structs, a pair of integers is represented directly as a pair of integers on the stack, or in registers (depending on what the compiler decides to do).
Ok so you can take every hackage function ... I will give you a small/big hint: use hoogle (http://www.haskell.org/hoogle/) and search for "[a]-&gt;[[a]]" now just look at the results and try to find the right one/combination :D 
&gt;but it’s much easier to use This isn't the first time I've seen that stated. But I don't get that impression from your example. Your code doesn't appear to be simpler or easier or smaller than doing the same thing in snap or happstack. In what way is scotty easier to use than the more well known haskell frameworks?
I mainly use `Debug.Trace`, altho sometimes ghci debugger might come in handy: http://www.haskell.org/ghc/docs/7.6.2/html/users_guide/ghci-debugger.html
A Neighborhood of Infinity is my absolute favorite blog. There are tons and tons of awesome posts on there.
By "without recursion" I presume you mean using standard recursive functions like `map`, `foldr`, `unfoldr` instead of rolling your own?
Sounds like a trick question, lists are recursive data structures and you have to use recursion to traverse a list. Here's an (obfuscated) example that doesn't explicitly recurse: skips = \a -&gt; foldr ((:).(foldr ((:).fst) [] . filter ((==0).snd) . zip a)) [] . foldr ((:).(foldr (++) [] . repeat . foldl (flip (:)) [] . enumFromTo 0)) [] $ enumFromTo 0 (pred . foldr (+) 0 . foldr ((:).const 1) [] $ a) but of course foldr, foldl, zip and filter are defined recursively.
Break the problem into smaller pieces. I recommend first writing a function that you will apply to each element of the list. Because that function will not have access to the list structure, think carefully about what information you will need to pass to it. Once you've written that you can use a higher-order function to map the first function over the list.
Hm, why people are complaining about the presentation on their phones, I thought this was a "standard" pandoc presentation format?
Thanks! Added.
True, though I did mention that it was obfuscated. To OP: The example was complicated on purpose to discourage you from using it verbatim. It's not necessarily that complicated at all.
You probably can. It depends on the details though, polyhedral optimization can only represent certain very regular loops. Note that I was talking about the DPH flattening transformation, not stream fusion.
Please do your part to keep the Haskell community helpful and friendly. If you feel like someone is asking for verbatim homework answers, you could just not answer, or give answers with helpful hints on how to start.
Yeah, [we've been over this](http://www.reddit.com/r/haskell/comments/1acfbx/what_is_the_best_way_to_make_a_simple_web_service/c8w577j?context=3).
Yeah in general I agree (although personally I'll pass on the octonionian inspired operator). There is something very interesting going with these bidirectional programming primitives. I meant my comment more as a way to tease out a conversation. I do like the idea of keeping the definition of lenses to only those things that preserve the lens laws, but I still don't feel that lenses are enough for quasi reversible programming I am gravitating towards. 
Fair enough. Edited.
&gt; If it occurs in both, then it's no kind of functor at all. invmap :: (a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b ?
Personally, I think it's cool that someone actually put the time to document his/her use of these haskell packages. That said; you're being anal over a trivial matter. And since you're taking this tack, "simple' is relative to the speaker or writer and it may just be a personal preference for a slew of reasons unless you've defined a quantitative metric for "simple," e.g.: someone with a PhD in categorical logic will find category theory taught in 1st year under grad. simple. Obviously, this isn't generally so for most 1st year students.
Update: The wrong reference composition problem is solved with an abstract unit type. Now the type system checks the composition law for references.
&gt;Personally, I think it's cool that someone actually put the time to document his/her use of these haskell packages. So do I. &gt;you're being anal over a trivial matter Actually, I just asked a simple question. Like I said, I've seen people say this before, it appears to be a common theme, but I've never seen anyone say why or how it is simpler or easier. Asking for people to explain things is not a crime.
it's too hard for me to understand how this version works =) i am lost in this foldr's.
hxt (a great lib btw) provides listA within a proc call (arrow notation) you can call getChildren to navigate the tree, then from "ref" level call something like listA (getChildren&gt;&gt;&gt; getChildren &gt;&gt;&gt; isElem &gt;&gt;&gt; hasName "x") to get all your "x"s in a list (or &gt;&gt;&gt; getText to get just their text content) --- you can use runLA or runX depending on the context. if you dont want to use them you make your life more difficult than it should be imo - but ymmv.. --- then once you have them in a list you can get them in a monoid.. but wait.. a list is a monoid :)
well done! and so quick :) reddit-driven-development, anyone?
This responses page is great style!
Don't worry about it :) It does the same thing your list comprehension version does in its own way: zips the strings into (letter,number)-pairs, filters the pairs on the numbers with filter ((==0).snd) and then discards the number parts with map fst.
Cute, but I don't see anyone talking about invariant functors or applications of them outside of that package. Is there significant value in the abstraction? Do the stated laws characterize the definition completely, as they do for covariant and contravariant functors? If so, is "invariant functor" just another word for a type of kind `* -&gt; *`?
In some cases, a type of kind `* -&gt; *` is not an invariant functor. For instance, if you have GADTs, data Eq a b where Refl :: Eq a a `Eq a` does not have to be an invariant functor.
Another example would be IORef. 
Where should I submit bug reports?
As for the Monte-Carlo step in itself, you may want to use [Control.Monad.Random](http://hackage.haskell.org/packages/archive/MonadRandom/0.1.3/doc/html/Control-Monad-Random.html). Something (kinda ugly) that might work with your code like what you described in step *1 would look like: monteCarloStep :: (RandomGen g) =&gt; [Particle] -&gt; Rand g [Particle] monteCarloStep a = do let nParts = length a n &lt;- getRandomR (0, nParts - 1) dx1&lt;- getRandomR (0.0,1.0) dx2&lt;- getRandomR (0.0,1.0) dx3&lt;- getRandomR (0.0,1.0) let dx = vec3fromList [dx1,dx2,dx3] let b = moveParticle dx n a p'&lt;- getRandomR (0.0,1.0) let p = nParts / (nParts + 1) return $if (p' &lt; p) then a else b
http://www.reddit.com/r/haskell/comments/1bapa5/optimising_purely_functional_gpu_programs/
Oops, totally missed that.
Indeed, thanks for the counterexample. This is true only of a subset of types, including those representable in System F, for example.
Things are much simpler when you only consider the case of plain algebraic data types with all constructors exported. :]
F_ω (which you need to talk about functors within the language/have kinds like `* -&gt; *`) can represent (to certain degrees) GADTs. For instance: forall p. p a -&gt; p b Is an encoding of equality between `a` and `b`, and is not an invariant functor in `b`. And forall p. p Int -&gt; p String -&gt; p b is an encoding of something like: data Rep b where I :: Rep Int S :: Rep String which is also not an invariant functor.
Well, there's [accelerate-opencl](https://github.com/HIPERFIT/accelerate-opencl), but maybe you knew about it already and were commenting on how it doesn't seem to be in active development?
The "efficient" qualifier calls for some benchmarks against the state of the art.
You should check the new fork, from The current unofficial maintainer of hsenv: work awesomely on my Mac OS 10.8 and GHC 7.6.2; https://github.com/tmhedberg/hsenv?source=cr
There's a similar approach that avoids the need for `mod`, at the cost of using `zip` repeatedly. The idea is to use infinite cyclic lists to decide which items to keep - one cyclic list for each include-every-n "stride". The [`cycle`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Prelude.html#v:cycle) and [`replicate`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Prelude.html#v:replicate) functions can be used to build these. 
I believe [hub.darcs.net](http://hub.darcs.net) provides a pretty nice issue tracker but you might need to enable it manually.
Ah, ok. I enabled it.
A quick look at the code looks like this is absolutely not efficient. Sparse vectors are represented as (lazy) IntMaps...I wouldn't be surprised if this is 10x perhaps even 100x slower than the state of the art. However, the good news is that you can use only the basic linear algebra routines from an existing library written in Fortan or C. Since most time in the conjugate gradient method is spent in sparse matrix vector multiply, the bulk of the algorithm can still be written in Haskell, but because of most of the run-time would be spent in low level code it would still be pretty efficient. Perhaps with vectorized stream fusion in the recent paper it would be possible to write a reasonably efficient sparse matrix vector multiply in Haskell? However making matrix multiply fast is not easy.
I asked them about this. Apparently it was mostly a self-contained project being done by a very small team that has now completely moved on to something else. Unless someone adopts it, it's going to rot away until it's useless.
Isn't it just all about tails? &gt;&gt;&gt; :m Data.List &gt;&gt;&gt; tails "abcd" ["abcd","bcd","cd","d",""] 
What about accelerate-backend-kit? https://github.com/AccelerateHS/accelerate-backend-kit/tree/master/icc-opencl/Data/Array/Accelerate It looks like something is happening there.
why haskell? because of GasDetectionMonad ?
Awesome! Great to see that you guys are expanding =)
Thank you very much, this seems like a clean solution, although I should read a bit about monad transformers. To also incorporate the random generator state, do you think I should better include it in the Configuration or use the Random monad as notthemessiah describes (i.e. loop :: StateT (Rand g Configuration) IO () )?
Thanks for the feedback. The "efficient" was meant as a qualifier for the algorithm itself (i.e., the conjugate-gradient); not to the implementation in particular. So, it's "efficient" in comparison with direct methods such as LU decomposition. However, I do agree that the title is misleading in that sense. Unfortunately, reddit doesn't allow editing titles. My apologies for the confusion. Having said that, this module came out of a bigger project dealing with ASIC placement, where such matrices arise naturally. (This was the only part we were allowed to make public). In our experiments, the code dealt with sparse matrices of size 12000x12000 comfortably; using double-precision numbers. While real industrial applications would have to deal with even larger matrices, I think the module is suitable enough for moderate sized projects where one can rely purely on Haskell without spawning the job out to Fortran or C. If you do try it out and see opportunities for improving it (while staying purely in Haskell), I'd be extremely interested in hearing about it. Patches are always welcome! 
Cool, the fork worked!
In my humble opinion, the rvar package's RVar monad is pretty much the ideal abstraction for random number generation. The distributions provided by random-fu are also lovely (although the Lift class is a bit annoying).
Well, that certainly cleared things up for me :) 
I bet on purity. It's a detector, it must measure state, not alter it :D
Is it a requirement to live in Netherlands?
No.
Okay, I'm too lazy to read this right now... can someone just tell me if the title is a pun on (.)?
Hi Chad, it's awesome to see my series hitting the "pick of the week" on SOH. That boosts me even more in releasing new episodes :)
i understand very little of this post. anyone willing to explain what this is?
Option -ddump-splices was not designed to produce compilable code (see http://hackage.haskell.org/trac/ghc/ticket/5016). I think that more practical solution is to hack Yesod and insert pretty-print statements.
I'm a fan of macros in other languages but I haven't used Template Haskell. I suppose I presumed unrolling the special syntax extensions would happen once at the beginning of compilation. What about its functionality requires TH to have knowledge of the underlying machine?
It is not a requirement to live in the Netherlands. But you would have to be able to commute to Nijmegen daily.
Spoiler alert! I was just about to attempt this.
"unrolling the special syntax extensions" is actually running the code of other Haskell functions (which of course can run any other functions in any package they depend on). So how it works is that at compile time, ghc loads the object code for all the packages that might be needed, and the .o files of other modules in the current package (any bytecode for things in the current module) and the TH syntax directs ghc to run various functions at compile time. So because it's loading object code then that object code has to exist, and it has to be ABI compatible with ghc itself, because it is ghc that is calling it (in-process). So it must be compiled for the current architecture and without profiling (because ghc the application is compiled normally not for profiling, and profiling code has a different ABI). So it's not that ghc cares about the underlying machine, but because it has to be able to load and run code while it's compiling, then that code has to match the ABI that ghc is using. And this also suggests a solution for the cross-compiling case: compile first for the build machine so it can be used for TH, and then cross-compile for the host machine. This is very much like what Cabal does already for profiling: if your package uses TH and you want to build profiling libs then Cabal first compiles non-profiling so that it has .o files that are abi compatible for ghc for use by TH, and then it compiles again in profiling mode. We could do the same for cross-compiling. The restriction at the moment is we have no way to manage a package datbase with the same package compiled for two different platforms (like x86 + arm).
Do you mean thtat the Template Haskell code would be pretty-printing the Haskell code it generates? That does sound promising..
I have no Idea how familiar you are with haskell, so I'll tell you what I got from my basic understanding. Template Haskell is Haskell macro system (code that generate code, which is then compiled to be a program). For some reason, TH does not work during android compilation. In order to get it working, OP makes a hack: use -dump-splices to output the code generated from TH (/u/lykahb/ [states that it wasn't intended)](http://www.reddit.com/r/haskell/comments/1cm4n1/template_haskell_on_impossible_architectures/c9hxfnp) since for some reason (see lykahb post), the code outputted from -dump-splices needs to be fixed. OP follows on which cases the output is bad. TL;DR: TH doesn't work on android, but you can manage to compile by using -dump-slices and fixing the outputted code.
persistent has a non-TH interface that was specifically created for the mobile use case. yesod-pure is also available if you are willing to give up type-safe routes. But this code generator might also work: https://github.com/singpolyma/route-generator
I pointed that out on the [proggit side](http://www.reddit.com/r/programming/comments/1clb6v/template_haskell_on_impossible_architectures/c9hpb5h); joeyh repiled that some parts of yesod require TH to build. (I have not verified this myself, just relaying.)
&gt;As far as I know there is no tool to find functions with the same definition.. determining function equality is hard! I know of one such tool... an irc client. :P
OpenCL is a less mature target and most interesting hardware is supported by CUDA (in particular, Tesla compute GPUs). So, yes, we'd like Accelerate to support OpenCL, but there are other areas (e.g., nested data parallelism) that make much more sense for the small dev team to tackle. We'd very much welcome contributions, though, and would be happy to help with advise if anybody wants to contribute. At the moment, the best option would be to contribute to https://github.com/iu-parfunc/accelerate-backend-kit/tree/master/icc-opencl
There *is* a preprocessor that strips TH out of .hs files. It's been around since before native GHC support was around. Can't remember the name, though.
What about this memory leak issue: https://github.com/xich/scotty/issues/9 Sounds scary. 
Then I'm glad to have chosen it :) BTW, we're excited to be hosting your Snap tutorial - drop me a line if you're hitting any issues in getting active code going.
Related: [proving the solution to the MU puzzle in Agda](http://gergo.erdi.hu/projects/agda/sg-meetup/SGMeetup.pdf) (from slide 46 onwards). Literate Agda file [here](http://gergo.erdi.hu/projects/agda/sg-meetup/SGMeetup.lagda).
How would generics help?
Here's another visualization: http://blog.sigfpe.com/2006/10/monads-field-guide.html
Hmmm... comes close to committing the same metaphorical error that a lot of tutorials give where they use space suits, burritos, &amp;c. as metaphors for monads. Monads are as much about describing a computational structure as anything else. With that in mind, it might it might've been a good idea to discuss monoids (given they're relatively straightforward and commonplace) and use that as a jumping off point for explaining monads. It also might have been an idea to relate monads to things in imperative languages that people would be familiar with. After all, '&gt;&gt;' is essentially ';' in C-like languages, and 'x &gt;&gt;= λ y → ...' is essentially 'y = x; ...' in such languages too.
Seconding the question about how those illustrations are made. I have quite a big Haskell teaching project lying dormant, and one of the things I really lack is illustrations. I no doubt can make them, but I would like nice, LYAH-esque illustrations and I have no idea where to start to make them look that nice, really.
Well done!
Agreed, these are burrito analogies in disguise. And IMO trying to give an intuition for `&gt;&gt;=` in terms of boxes or burritos is a bad idea. I think `join` makes more sense for this kind of analogy. (You can only unwrap a box if it's inside another box. QED.)
Heh, I also thought "This should use *join* instead" while reading the article. Although more consistent, it might confuse readers with some Haskell knowledge, who do know *&gt;&gt;=* but never heard of *join*.
Exactly! While the linked article is nice, I would love an article that describes the concrete and technical advantages of category theory, with examples of where and how it lead to advances in programming language semantics or type theory or another field.
Funny you bring that up, I did something like this myself a couple of months ago. The result was a GraphViz .dot file showing the relationship between the theorems. You can find the results here: (Numbers are maximum intermediate theorem lengths) - 8: http://stupidname.org/files/temp/temp_2013_02/miu8.png - 9: http://stupidname.org/files/temp/temp_2013_02/miu9.png - 10: http://stupidname.org/files/temp/temp_2013_02/miu10.png - 11: http://stupidname.org/files/temp/temp_2013_02/miu11.png Basically, I misread the exercise and thought it's hard but solvable; in order to avoid spoilers, I stopped reading right after rule 4 was given. Turns out I should've kept going ;-)
I find this "context" explanation completely unhelpful, it generally seems to confuse people more than help.
This was the point of the exercise, to understand that the puzzle is not solvable and to "unquestion the question", right? But is there a proof that MU can not be produced?
Aah, burritos was exactly what I was trying to avoid! Edit: I added another functor example using functions that avoids a box analogy.
Hey, thanks! They were done with a fountain pen and then colored in Pixelmator (Gimp would work just fine too).
There is; see my Agda link from my other comment.
Kudos on the hand drawn illustrations. Might be good to mention the analogies and pictures are somewhat incomplete. Boxes are dangerous to the concept as pointed out here: http://learnyouahaskell.com/functors-applicative-functors-and-monoids Still this is nicely done
There is a proof, it's very simple (no need for Agda), and given later in the book. It's also on the Wikipedia page on the puzzle.
How would you go from monoids to monads in a newbie-friendly way? The usual monads-are-monoids saying is actually pretty mathematically involved, although it is often misunderstood.
Agreed. I added a new example for functors and added a note.
I guess it's not quite the same but ... hist2 xs = mapM_ putStrLn (map gr [0..9]) where gr a = show a ++ ":" ++ (maybe "" (flip replicate '*') . lookup a $ db) db = map (\x -&gt; (head x, length x)) . group . sort $ xs 
 line :: [Int] -&gt; Int -&gt; String line l n = show n ++ ":" ++ (map (const '*') $ filter (== n) l) hist :: [Int] -&gt; IO () hist l = mapM_ (putStrLn . line l) [0..9]
this is cool! but its more easy to do horizontal lines in histogram ;)
There is a 'field' lens in there already... the Riemann bindings are using it in [Types.hs](https://github.com/tel/riemann-hs/blob/master/src/Network/Monitoring/Riemann/Types.hs#L353). It does help when defining lenses but it makes for more boilerplate than I'd like. If $(makeLensesWith) had an option for specifying a wrapping lens... that'd be awesome, something like this: data Foo = Foo{_field1 :: Required 1 Int64} $(makeLensesWith (lensRules &amp; wrappedField .~ 'field) ''Foo Could generate this: field1 :: Iso' Foo Int64 instead of: field1 :: Iso' Foo (Required 1 Int64)
Use [`transpose`](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-List.html#v:transpose) (and perhaps `reverse`) to generate the vertical version from the horizontal version. (Though you do then need to ensure your horizontal version contains enough trailing spaces to make all the lines the same length.)
You can't put '----' or '***' at the beginning of a line without invoking some weird reddit markup features. starize :: (a -&gt; Bool) -&gt; a -&gt; Char starize f x = if f x then '*' else ' ' counts :: [Int] -&gt; [Int] counts = flip map [0..9] . (. (==)) . (length .) . flip filter line2 :: [Int] -&gt; Int -&gt; String line2 cs n = map (starize (&gt;= n)) cs hist2 :: [Int] -&gt; IO () hist2 l = do let cs = counts l mapM_ (putStrLn . line2 cs) (reverse [1..maximum cs]) putStr "----------\n0123456789\n"; * *** * *** ****** *** ---------- 0123456789 
julesjacobs: There's now a v2.1 release on hackage that uses strict int-maps for the underlying container. (This was an oversight in the previous release.) This simple change made my test cases go about 12% faster. Of course, benchmarking against libraries in other languages remains a TODO.
Sure, I will, and this weekend I should release also the third episode, so I'll keep you posted as well :)
&gt; Monads are as much about describing a computational structure as anything else. I think the idea is that computational structures are less accessible than certain other concepts, among them burritos and boxes. 
By the way, I still need to write a blog post announcing postgresql-simple 0.3, but it's out now, and it has good support for array types. I don't know if that will improve and/or simplify anything in your code, but it is available.
It's good to get many different perspectives on the same thing. One perspective is often not enough to really understand it. Viewing monads as a way to structure computation, and viewing monadic values as containers are both valuable.
Think compositionally! You are trying to jump straight from your original data to the specific histogram format you have in mind. This causes your code to be highly dependent on both your original data format and your histogram format. A better way is to proceed step by step, refining your representation of the data until it has the shape you need. The most obvious place in which to divide your task into substeps is that you first want to *extract* information from your data, and then you want to *display* it. For a histogram, the information you need to extract is the number of times each value appears in the list. This subproblem is much simpler! count :: Eq a =&gt; a -&gt; [a] -&gt; Int count x = length . filter (==x) -- &gt; counts "ab" "abcaaa" -- [4, 1] counts :: Eq a =&gt; [a] -&gt; [a] -&gt; [Int] counts vs xs = map (flip count xs) vs For displaying the histogram, we first need to realize that a horizontal histogram is much easier to draw. horizontal_histogram :: [Int] -&gt; [String] horizontal_histogram = map (flip replicate '*') -- &gt; print_horizontal_histogram "ab" [4, 1] -- a| **** -- b| * print_horizontal_histogram :: Show a =&gt; [a] -&gt; [String] -&gt; IO () print_horizontal_histogram vs rows = forM_ (zip vs rows) $ \(v, row) -&gt; putStrLn $ show v ++ "| " ++ row Since I have separated the part which computes the [String] representation of the histogram and the part which prints this representation, creating a vertical histogram is now much easier, as I can reuse this [String] representation. Specifically, I want to rotate those rows into columns. Haskell's transpose operation is almost what we need, except all the rows need to have the same length. The hard way to do this would be to compute the number of characters which are missing, and to append just the right number of spaces. If we proceed compositionally, there is a much easier way: first, convert the rows into infinitely-long rows. From there, trimming those rows so that they all have the same length is quite straightforward. rotate :: [[a]] -&gt; [[a]] rotate = reverse . transpose rectangular :: a -&gt; [[a]] -&gt; [[a]] rectangular padding rows = rectangle where width = maximum $ map length rows long_rows = map (++ repeat padding) rows rectangle = map (take width) long_rows Now that we have separated those helper functions, rotating our horizontal_histogram is a simple composition. Note how our helper functions are generic and reusable, because they know nothing about the histogram we are trying to build! vertical_histogram :: [Int] -&gt; [String] vertical_histogram = rotate . rectangular ' ' . horizontal_histogram -- &gt; print_vertical_histogram "ab" [1, 4] -- * -- * -- * -- ** -- == -- ab print_vertical_histogram :: Show a =&gt; [a] -&gt; [String] -&gt; IO () print_vertical_histogram vs columns = do mapM_ putStrLn columns putStrLn $ map (const '=') vs putStrLn $ concat $ map show vs hist :: [Int] -&gt; IO () hist = print_vertical_histogram values . vertical_histogram . counts values where values = [0..9]
Is this homework? These exercises look [very familiar](http://www.cis.upenn.edu/~cis194/hw/03-rec-poly.pdf).
You should mention that Monoids are associative by definition, an operation having a neutral element isn't enough. 
Nice illustrations, good work! In the code examples, though, # is used for comments. E.g.: fmap (+3) (Just 2) # Just 5 This can also be confusing because (#) is a valid operator name. If you copy pasted that code it would parse.
Does Groundhog have any documentation? I see example code, but it's not at all clear to me what the semantics are.
Good point, I'll change it to --. I used # because that's what my syntax highlighter understands.
The first few are easy enough to see, but I had trouble following the illustrations involving [] and the transformers wrapping []. I *do* have solid intuition about the list monad in practice but, for example, `ListT (State b)` just wasn't clear at all to me.
Thanks for the info! I understand the problems you have with bandwidth. I think it is amazing what you are already doing. For me it is quite interesting with OpenCL for GPUs on mobile phone platforms. Unfortunately I'm a newbie when it comes to OpenCL. But perhaps I can get some help from my colleagues.
brilliant description =) i look on transpose yesterday, but don't "see" that it can "rotate" my list. this is interesting: long_rows = map (++ repeat padding) rows i forgot about laziness. forM_ mapM_ is new functions for me (iam now learn IO,writer,state,list monads), but i understand now how they works.
I had a beginner Haskeller take a look. Bam. Right off the bat - "what in the world is a 'context'"? He had no idea what you were trying to get across as "context" without struggling through the whole article. Why did't you spend a sentence or two giving some idea what you mean by "context"?
They can give you a generic map over arbitrarily nested functor stacks.
http://www.haskell.org/haskellwiki/Scrap_your_boilerplate
How does that help compose functions of many arguments?
&gt; Nice, tutorials are always a good idea. And even better if the tutorial is itself in the Haddocks, so that it can be kept up to date along with the rest of the documentation easily.
Even better write it in prompt-response form, e.g. &gt; fmap (+3) Just 2 Just 5
If you know an imperative language, I don't think it really is. The problem with most explanations of monads is that they try and take some metaphor and try to explain monads with that rather than using something concrete that the person understands and work backwards, avoiding metaphor, to reveal that that thing is monadic. It'd go something like this: &gt; You know ';' in C? Imagine for a moment that ';' wasn't just a statement terminator and was actually an operator that caused each statement to be ran in sequence rather than whatever order the computer found convenient. Now imagine that the exact behaviour of ';' could vary in interesting and useful ways depending on the kind of values the statements it joined in sequence were operating on. It's that context represented by the kinds of values being acted upon that monads are about. In that one paragraph, I've related something somebody familiar with an imperative language would understand directly to monads by equating ';' with '&gt;&gt;'. Once you do that, you've got over the essence of monads: sequencing and thus computational structure.
Even better! Done.
There's no need to be tetchy. I wrote that because I wanted to state how I'd speak to the person I was explaining the concept to, step by step, keeping thing related to some concept that's concrete in the student's mind. And what I wrote would be the starting point, not an explanation by itself. I've taught college students before, and it helps to gradually bring people on like that rather than blurting out something like 'bind is just the semicolon in C except somehow magically overloaded'. People don't learn from explanation; it's only a little different than the 'monads are just monoids in the category of endofunctors' joke. Writing up an actual explanation of monads is something on my list of things never to do, but I have stepped through explaining things to people like that, and it work. If you *really* wanted to pick on my comment, you should have pointed out that I implied that there were no commutative monads, which would, of course, be incorrect.
I wrote that it comes close, not that it made the same mistake. Where it comes close is in the use of the box metaphor which *is* equivalent to the burrito metaphor. Where his explanation is better (and why I said it came close, not that it was the same) is that he related them to functors and gave a solid example of a functor. That *is* good. As far as monad tutorials go, it's one of the better ones. My comment wasn't meant as an attack, but as constructive criticism, and that's how the OP graciously took it.
Thanks!
I came up with a way of explaining it in a Twitter conversation I had an age ago. I can't recall exactly how I related the two, but I didn't have to use much in the way of maths to do it. I think it might've helped that, while the person I was explaining it to didn't have a grasp of category theory, he did have a solid grasp of group theory and was a programmer. Still, even then, I don't recall using anything that required more than an understanding of basic algebra and what a higher-order function is. If I remember in the morning, I'll see if I can dig it up and distill what I wrote to something less disjointed than a Twitter conversation would be.
Ah, that's a good idea, but would take a total re-work of these illustrations :/
Something that strongly resembles monoids and is what people often confuse the monoids/monads argument for is the statement "the kleisli category construction is indeed a category": `&gt;=&gt;` and `return` satisfy the following laws: * `f &gt;=&gt; return = f` * `return &gt;=&gt; f = f` * `f &gt;=&gt; (g =&gt; h) = (f &gt;=&gt; g) &gt;=&gt; h` While these look very similar to the monoid laws, they're actually just the category laws, since categories can be thought of as "typed monoids". That explains the laws and makes them look somewhat familiar, but I don't think it really gives any "intuition" for why they're interesting to programmers.
&gt; Once you do that, you've got over the essence of monads: sequencing and thus computational structure. [Monads are not about sequencing](http://www.haskell.org/haskellwiki/What_a_Monad_is_not#Monads_are_not_about_ordering.2Fsequencing), there's commutative ones, and all the sequencing you can do with monads [you can already do with applicative functors](http://www.haskell.org/haskellwiki/What_a_Monad_is_not#Monads_are_not_a_replacement_for_applicative_functors). The essence special to monads is influencing the further computation by analysing a pure value. Bind is actually the best example, there: (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b All the magic is in the `a -&gt; m b`: Given a pure value, one can construct a monadic action. `(&gt;&gt;)` doesn't provide that.
Yep, and I invented my own before discovering the one in Data.Functor.Syntax, which is what prompted the question I suppose. 
The trick to find functions of a given type is certainly useful! But finding particular definitions is indeed what I was after. It sounds like there is no such thing. (EDIT: Except IRC, of course)
All monad tutorials should be legally required to link to [What a Monad is not](http://www.haskell.org/haskellwiki/What_a_Monad_is_not).
I've a vague memory of something like that being brought up on [Good Math, Bad Math](http://scientopia.org/blogs/goodmath/) some time ago. Edit: nope, wasn't there. Must've misremembered. I think I must need some sleep.
What did you make the graphics with?
Thanks!
Forgive a non-Haskell programmer for not understanding why I should get excited about this. To my programmer's brain, it just looks like polymorphic methods with boxing and unboxing added. What's so exciting or novel about that?
[See my answer here](http://www.reddit.com/r/haskell/comments/1co0s5/functors_applicatives_and_monads_in_pictures/c9ifiu7).
The range notation uses the difference between the first two numbers to determine the step size. If the first two numbers are the same then your step size is 0, which is why it gets stuck at 3. You probably meant `[3,6..33]`
As a heavy active user of the LLVM bindings, I've encountered the same problems and have recently acquired co-maintainer status to the official repository to correct them. I've been caught up in the tail end of a semester until just a few days ago, but I expect to be able to within a week turn my attention to merging in the changes I and others have made since bos, the original maintainer, ran out of time to merge patches. Once that's complete and I've done enough testing to be sure I haven't broken anything gratuitously, I'll push out a new release on Hackage. If you're in a hurry, you can check out [my fork](https://github.com/Ralith/llvm) and use it immediately. You may also be interested in the [pure wrapper](https://github.com/Ralith/llvm-st) I've put together that relies on my fork. Regardless, you can expect all this to be on Hackage soon.
1. Maybe you could fix this: The monad plunger thing is not an actual explanation. Correctly, just like the functor, the &gt;&gt;= unwraps the value too, but then feeds it to a (self-)*wrapping* function, and hence also doesn’t need to re-wrap it itself, like with the functor. 3. [This belongs in your tutorial.](http://cdn.memegenerator.net/instances/400x/37143434.jpg) :)
&gt; When teaching you should show rather than tell. While this is a good rule of thumb, I enjoyed reading this precisely for the generalizations, which took a few points of view that were different from how I would have thought about it. Had it instead spent that time on examples showing that `Maybe` is a functor, I wouldn't have gotten anything from it.
&gt; You can only unwrap a box if it's inside another box I'd caution against the use of the word 'only', since many monads are completely unwrappable, just not with a standard interface. It's more appropriate to say that you can definitely unwrap a box if it's inside another box. Monads are defined by what they guarantee, not by what they forbid.
&gt; And this also suggests a solution for the cross-compiling case: compile first for the build machine so it can be used for TH, and then cross-compile for the host machine. I guess there can still be some fun problems like [sizeOf](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Foreign-Storable.html#v:sizeOf).
Who said anything about it being exciting or novel? Monads are known in mathematics since 1958 after all… The cool thing about all this wrapping is, how you can change what the function does to the value, by changing the box/context! So the box/context becomes a hidden state (like (Just x) and Nothing is a state). You can change the box, and every function after it will see the changed state. Since that box change is not a change of that contained value itself, it appears to be a hidden side-effect. Both states and side-effects would otherwise be impossible in a pure language. The IO monad goes so far as to add a double bottom to the box, and hide the whole outside real world inside it. :) (And since that’s not actually possible, the compiler does some hidden magic to make it look like that anyway to the Haskell code.) Finally, functors and applicatives are merely map functions generalized to all wrapping types in only the second and in both parameters, respectively. And the monad class is a set of functions that make dealing with that box in a more comfortable (and invisible) way.
 let takeEvery n x = (map snd) . (filter ((n==) . fst)) . zip (concat . repeat $ [1..n]) $ x
Great news. Thanks for spending time &amp; effort on this!
Yeah... again, it just makes me think of base-class method calls on polymorphic methods of derived classes. Am I missing the point?
I was expecting it to be strictness, but no, the core looks nicely unboxed.
I've included in the bug report some basic profiling. There is a huge allocation of Pinned memory: http://heap.ezyang.com/view/6fb7441b501c00f428126f6b6e767d2412f43c2c#form
You're welcome!
Fantastic! That's what I was hoping to hear. 
It's also incomplete, because monads also overload binding. Applicatives are enough to overload semicolons.
Hi, I plan to add dynamic color support for buttons and other widgets! For now I would like to exactly follow the Gtk2Hs binding -- if there is a general mechanism to alter the background of a widget I support that, otherwise I would support changing background for individual widget elements. By the way, if it is public, what kind of application would you like to implement?
I stopped working on it because I wasn't using llvm any more, and it's a very large amount of thankless make-work to keep up with upstream releases that break a substantial fraction of the API twice a year. I now think it's a bad idea to program to the llvm APIs directly for several reasons, unless you're stuck doing so for some reason. First is that llvm's FFI bindings are always very incomplete and underpowered compared to going through C++. Second is the aforementioned churn, which is a big pain in the ass. For those who enjoy needless suffering, I've opened up the github repo to some more committers, who will hopefully be able to collaborate to bring things up to date. I'm open to more people pitching in, just get in touch via the usual means.
Thanks for your hard work. I hope I didn't come across as accusing you of anything. I was simply trying to communicate my perception of the state of the project. 
Would you suggest instead of binding the LLVM APIs, some DSL which emits IR would be a better approach?
When reading about issues like these, where things seem to be OK in some GHC 7.0.x release but not in 7.6.y, I wish there was some CI setup which can run automated 'git bisect' runs to pinpoint the commit by which the issue was introduced. I got such setup running for some project on our internal CI systems at work, it has proven to be pretty useful once in a while: you start a run somewhere during daytime (builds &amp; tests take a *long* time), and by the next morning the issue has been pinpointed.
Yes, there are several packages that do exactly this. Obviously that doesn't suit all needs, but if it fits your constraints, it should work well.
&gt; My longest GHC error message seen on this odyessy was actually a full 500+ kilobytes in size. It included the complete text of Jquery and Bootstrap. Wow. Just... wow. How do you get jQuery in an error message?
 After more thought, I *think* I know where you're going with this, and I'm reconsidering my position that it's a bad analogy. Are you asserting that imperative statements using the semicolon is analogous to monad comprehension with the Identity monad? I like to think of monads as a type safe aspect oriented programming, where by changing the monad you can point-cut new behavior into an existing flow. So while the ; is useful for *sequencing* operations, monads do much more than that, unless you're talking about Id. If you mean that it's a ; that you can program, (by using transformers or changing the wrapping monad) then I would agree, similar to AOP. 
Great! I'm working on a simple simulator for a redundant file system as a school project. I thought it'd be nice to be able to visualize what happen when blocks are erased from the system with a matrix of colored buttons
Yup, that's my thinking exactly.
35 KiB is not huge.
Well, try doing that in a pure functional language… then you see its point. :) (Because it’s what you will end up with, and if not, you have a paper to publish. :)
For the record, here are some reasons why one might still want to use the C API: * It's more stable than the C++ API; in fact, its breakage corresponds to major changes that tend to affect the text IR format as well * Major LLVM users including Apple rely on the C API, so it is in no danger of becoming significantly less powerful for common purposes, and patches for additional functionality are simple and welcomed by the maintainers * It manipulates (and can write to disk) an efficient binary format which will perform better for very large modules * It has integrated x86 JIT compilation * It supports performing optimization and verification passes in-place
Can you elaborate?
Yeah, I'd just emphasize the fact that it's programmable, and by default ; is similar to the Id monad. Cause when I first had a monad 'a-hah moment' it was recognizing that Option and Future both follow similar patterns. I had no idea what Id was or the theory behind them. So when I heard "monads are like ;" I thought to myself "how the heck is a future or an option like a ;???"
It was half a second of execution. It takes GiGs very quickly.
No, it isn't.
Can we see your attempts? What are your specific problems? This is not your homework answer sheet. 
Use `unfoldr` with `last`. 
Do you need to click on the buttons? I guess so. Otherwise I could add a new widget which allows to put dynamic cairo graphics in it and you could use that too. 
The trick is to identify simple patterns and to use [hoogle](http://www.haskell.org/hoogle/) to find their names. For example, multiplying many elements is probably a common operation. In its simplest form, it requires a sequence of numbers and should give back a number ([`Num a =&gt; [a] -&gt; a`](http://www.haskell.org/hoogle/?hoogle=Num+a+%3D%3E+%5Ba%5D+-%3E+a)). Your `fun1'` should be more readable once the different operations are factored. To find recursions patterns, we need to generalize as much as possible. If we only care about the recursion, then your second homework looks like : fun2 arg = if shouldStop arg then result else fun2 (doSomething arg) What types does fun2 requires? * It doesn't makes sense to specialize `arg` to numbers, so `(arg :: a)` is probably a good choice. * `shouldStop` is a predicate over `arg`, hence `(a -&gt; Bool)`. * `doSomething` transforms the argument to a new one, so `(a -&gt; a)` seems appropriate. * We don't know much about `result`, so let's use `b`. We put it all together and ask hoogle : [`a -&gt; (a -&gt; Bool) -&gt; (a -&gt; a) -&gt; b`](http://www.haskell.org/hoogle/?hoogle=a+-%3E+%28a+-%3E+Bool%29+-%3E+%28a+-%3E+a%29+-%3E+b). It's not exactly what you want, but it captures your recursion. You still need to work out the details. Depending on your background, you might prefer other patterns (which you will learn to recognize -- I recommend that you implement them to understand their shapes). As a starting point, the `unfold` pattern is really nice : compare it to `until`, understand its relation with `fold`, etc. Others might prefer `iterate`. Try them all on your `fun2`, you are just lacking the vocabulary :-) .
Yeah. I guess I'm just jaded by my own experience when first encountering category theory. Everything I read about category theory when first learning Haskell had a really obtuse presentation that either: a) didn't explain the motivation or purpose behind anything, or b) only explained the motivation for mathematicians, not programmers
i write this: fun2''' = last . unfoldr (\x -&gt; if (even x) then Just (x, x + x `div` 2) else if x==1 then Just (x, 0) else Just (x, x * 3 + 1)) but it's working forever :D
i look at the until and iterate functions and write two helper functions: f1 = takeWhile (even) . ( iterate (\x -&gt; x + x `div` 2) ) f2 = takeWhile (odd) . ( iterate (\x -&gt; x * 3 + 1) ) or f1' = until (even) (\x -&gt; x + x `div` 2) f2' = until (odd) (\x -&gt; x * 3 + 1) where to move now from here? =) 
You finish the `unfoldr` by returning `Nothing` at some point.
You are a saint. I've been trying to learn LLVM myself, and anything that makes for easier integration with Haskell is awesome.
I think there is a confusion on `shouldStop` : the `even`/`odd` condition is a part of the `doSomething` (because it recurses in both cases). When does your function stop to recurse? Anyway, you should ask your questions on [stackoverflow](http://stackoverflow.com/questions/tagged/haskell), not here :-) .
That either means it's getting so good that it's hard to find bugs or it was one doosy of a bug.
 concat . repeat -&gt; cycle and removing unnecessary stuff: takeEvery n = map snd . filter ((==n) . fst) . zip (cycle [1..n])
Thank you, i am switch to stackoverflow - it's more suitable for questions as i understood. Reddit for the news mostly? 
Accepting an incorrect type and misbehaving at run-time? I vote for "one doosy of a bug".
It was a bug in v7.6.1 also (to a lesser extent) and the Haskell Platform stakeholders were uncomfortable with releasing in such a state.
I'm jealous
thanks :D
Tekmo essentially found a very natural implementation of much of io-streams using pipes. It's pretty cool.
&gt; It takes GiGs very quickly. Running with heap profiling enabled makes this bug disappear.
No if statements and the top level function is point-free. fun3 :: Integer -&gt; Integer fun3 = snd . head . filter ((== 1) . fst) . iterate f . (\n -&gt; (n,0)) where f = (\(n,c) -&gt; ((n `div` 2) + (n `mod` 2)*(3*n + 1 - (n `div` 2)), c + ((n+1)`mod`2)*n))
it certainly is, but wasn't the whole point about io-streams that they are conceptional simpler and faster than pipes? (afair tekmo himself benchmarked them in that spirit.)
Yep, it's something I've experienced myself and, equally puzzled, asked explanations on the ticket. We'll see.
Actually, `pipes` is NOT slower than `io-streams`. If you use `pipes` to implement `io-streams`, then `pipes` becomes neck and neck with `io-streams` and `bytestring`. Here's the benchmark code I used: **EDIT:** Like the post, this requires `pipes` HEAD to compile, because of the generalized type signature for `runProxyK`. import Control.Monad import Control.Proxy import Criterion.Main import qualified Data.ByteString as B import qualified Data.ByteString.Lazy as BL import Data.Conduit hiding (Producer) import Data.Conduit.Binary import Data.Maybe import Foreign.Storable (sizeOf) import System.IO import System.IO.Streams make :: (Monad m, Proxy p) =&gt; (q -&gt; m r) -&gt; (q -&gt; p a' a b' b m r) make f a = runIdentityP (lift (f a)) once :: (Monad m) =&gt; (q -&gt; ProxyFast a' () () b m r) -&gt; q -&gt; m r once = runProxyK fromHandle :: (Proxy p) =&gt; Handle -&gt; () -&gt; Session p IO (Maybe B.ByteString) fromHandle h () = runIdentityP $ lift $ do eof &lt;- hIsEOF h if eof then return Nothing else do bs &lt;- B.hGet h 32752 return (Just bs) toHandle :: (Proxy p) =&gt; Handle -&gt; Maybe B.ByteString -&gt; Session p IO () toHandle h mbs = runIdentityP $ lift $ maybe (return ()) (B.hPut h) mbs connect' input output = go where go = do ma &lt;- once input () once output ma when (isJust ma) go inFile = "/tmp/2GBP.pdb" -- 252 KB file outFile = "/tmp/out.txt" main = defaultMain [ bench "conduit" $ withFile inFile ReadMode $ \hIn -&gt; withFile outFile WriteMode $ \hOut -&gt; sourceHandle hIn $$ sinkHandle hOut , bench "pipes" $ withFile inFile ReadMode $ \hIn -&gt; withFile outFile WriteMode $ \hOut -&gt; connect' (fromHandle hIn) (toHandle hOut) , bench "bytestring" $ do bs &lt;- BL.readFile inFile BL.writeFile outFile bs , bench "iostreams" $ withFileAsInput inFile $ \hIn -&gt; withFileAsOutput outFile $ \hOut -&gt; connect hIn hOut ] ... and here are the results: warming up estimating clock resolution... mean is 1.788871 us (320001 iterations) found 79064 outliers among 319999 samples (24.7%) 76431 (23.9%) low severe 2633 (0.8%) high severe estimating cost of a clock call... mean is 98.75571 ns (12 iterations) benchmarking conduit mean: 877.1400 us, lb 863.4429 us, ub 909.5332 us, ci 0.950 std dev: 101.2028 us, lb 52.12181 us, ub 190.2264 us, ci 0.950 found 11 outliers among 100 samples (11.0%) 3 (3.0%) high mild 8 (8.0%) high severe variance introduced by outliers: 84.152% variance is severely inflated by outliers benchmarking pipes mean: 529.4666 us, lb 525.7950 us, ub 534.5843 us, ci 0.950 std dev: 21.94231 us, lb 16.80804 us, ub 27.51061 us, ci 0.950 found 11 outliers among 100 samples (11.0%) 10 (10.0%) high severe variance introduced by outliers: 38.544% variance is moderately inflated by outliers benchmarking bytestring mean: 526.3141 us, lb 521.7860 us, ub 532.1077 us, ci 0.950 std dev: 26.13953 us, lb 21.84213 us, ub 33.00674 us, ci 0.950 found 20 outliers among 100 samples (20.0%) 3 (3.0%) high mild 17 (17.0%) high severe variance introduced by outliers: 47.479% variance is moderately inflated by outliers benchmarking iostreams mean: 525.2812 us, lb 521.2918 us, ub 530.7624 us, ci 0.950 std dev: 23.84322 us, lb 18.86387 us, ub 30.60870 us, ci 0.950 found 14 outliers among 100 samples (14.0%) 3 (3.0%) high mild 11 (11.0%) high severe variance introduced by outliers: 43.464% variance is moderately inflated by outliers So actually, the reason `pipes` was slower had nothing to do with the implementation itself. Also, as far as simplicity goes, I implemented all the necessary `io-streams` machinery in 24 lines of `pipes` code and just tied `io-streams` in performance. And as far as API simplicity goes, I would both agree and disagree: I agree that the types are more complex and not as easy to reason about as the `io-streams` types. However, I consider the larger types justified both theoretically and practically. Theoretically, the reason for the larger types is that pipes sit at the intersection of 5 different categories. This is also the practical justification, too: you can easily switch between these 5 categories to do advanced things that `io-streams` cannot (like bidirectional communication, composition, etc.) However, API-wise, I think the `pipes` version of the `io-streams` API is hands-down much simpler: * You use the same operator for applying transformations and composing transformations * You use the same function to build input and output streams * You use the same function to read and write output streams * You don't have to awkwardly bind every single transformation in the `IO` monad just to apply it.
&gt; it's getting so good that it's hard to find bugs Said no one, ever! Except Donald Knuth, but he was being sarcastic.
I also found the types confusing at the beginning, but after actually using `pipes` I ended up appreciating how self-descriptive and precise these types are, considering the broad scope of features they enable.
SPJ was either observing the Haskell platform mailing list or made aware of the issue and very promptly found the bug fix in the source tree and merged it for a release. A beautiful thing... 
I had not heard of Configurator before and I'm wondering: Why yet another configuration file format? It gets frustrating very easily if you're sysadmining and you configure tons of different applications, each with an *ever so slightly* different configuration format. Do I use a comma or a semicolon as a separator here? Do I have to quote the variable name? Colon or equals? Not to say that the library itself isn't cool and easy to use, but I still find it very frustrating. Why not just use JSON or YAML? (There's the `yaml` package on Hackage which is based on `aeson` and also really nice!)
The bug: http://hackage.haskell.org/trac/ghc/ticket/7748
&gt; I'm actually not sure why Greg doesn't do it this way. His internal implementation is basically isomorphic to the above types, except in a very complicated way that involves a roundtrip through iteratees/generators and back for no good reason. Did you bring that to his attention?
Not yet. I still need to organize my thoughts on this more to make sure I can explain this more clearly. He's the kind of guy that's difficult to convince to change his ways.
Are you the author? Figure 1 has a typo: "abstrast syntax".
&gt; I expect that pipes-parse will require either IO or an ST base monad (your choice) to properly manage pushback like io-streams does. Can't pushback be handled by a proxy that keeps around pushed-back things and gives them again on the next request?
The thing that I found immensely disappointing about configurator was that there is no built-in way to *write* out a configuration file in the appropriate format. I had to roll my own.
Yes, the spell checker does not go through the graph.. thanks
At the end of the article, there's a note: &gt; Of course, this isomorphism is not canonical, the specifics depend on exactly how you wire lemma4, but it is an isomorphism nevertheless! The isomorphism in the original "7 trees in one" paper isn't canonical either. I wonder what such an isomorphism (if it exists) could mean? 
I teach a first year university course on FP. Wrote this short tutorial on QuickCheck testing based on a mid-term test exercise. This is mostly aimed at beginners but maybe experienced programmers may find some value in it too.
Yep, you'd interact with the simulation by clicking on the buttons, that's why I wanted to use [widgetModifyBg](http://hackage.haskell.org/packages/archive/gtk/latest/doc/html/Graphics-UI-Gtk-Abstract-Widget.html#v:widgetModifyBg).
On the contrary, the new expression mechanism makes creation of database-specific functionality much easier. For example, this function from Database.Groundhog.Postgresql.Array can be called only when using PostgreSQL: arrayNDims :: ExpressionOf Postgresql r a (Array elem) =&gt; a -&gt; Expr Postgresql r Int Now the expressions can fully use the power of the database system. It should be possible to implement spatial extensions for MySQL and PostgerSQL without any changes to the core. Doing joins and other multitable queries is still an open problem. Maybe merging esqueleto-like query DSL + groundhog expression DSL is a good idea.
Depends on the context. How much time do you have? What is the purpose? The practical merits of FP or a general mind-expander? In the former case I would focus on the kinds of errors that can be avoided with strong typing, the modularity provided by higher order functions. If the audience is fresh from the same java course this could be a good opportunity to make comparisons between past assignments. "Look how simple it is in haskell!" In the latter case I would focus on examples of FP's increased expressiveness. Wow people with elegant one-liners. Explain enough of how they work to give an appreciation of the tools, but don't get bogged down in the details. Funnily enough I originally picked up haskell after an off-handed comment during a java lecture on lazy structures "in haskell, 'ones = 1:ones' just works!" You can mix the two approaches as appropriate, but pick a main theme. The pragmatists will be scared off by the cute one-liners and the rest will be bored by the pragmatic focus. Throw in enough of each to keep both parties happy, but know your audience. In either case I would avoid technical blockades such as monads. While they're generally represented as much scarier than they really are, understanding constructs like monads, lenses etc. requires a bit of "absorbing" time that you can't afford during the talk. Your goal here should be to get them excited enough to pursue it on their own, because really understanding programming (especially something as different as Haskell) requires practice; there's no way around it.
Right. That is why I qualified that statement with my last sentence. In that particular sense of the word stream then getLine is also a stream, just one without push back. Maybe another way to think about this is that pipes shows that a single step like getLine is actually still a stream when viewed through the lens of request or respond composition.
Your join is actually intercalate I think. [Hoogle finds it](http://www.haskell.org/hoogle/?hoogle=Char%20-%3E%20%5BString%5D%20-%3E%20String). &gt; quickCheck $ \l -&gt; forAll (elements l) $ \c -&gt; intercalate c l == concat (intersperse c l) +++ OK, passed 100 tests. 
Thanks, I hadn't realised it! I actually try to avoid relying too much on libraries when doing tutorials for students because otherwise they tend to believe that for every programming problem there must be a pre-defined solution in some library...
That's actually not a bad assumption in general. :) At least it makes you look at existing libraries first before rolling your own solution.
Immensely disappointing? Does github not have "fork" or "pull request" buttons in your country?
It needs to be about 10-30 minutes. The thing is that I haven't really decided on the purpose yet. Your idea about comparing snippets is great. I think concepts like monads are probably too complicated for this situation. So, I think the main things I need to tell people about is: - First and foremost, the meaning of functional - Pure functionality - Immutability - Type safety - Laziness - Conciseness of functional solutions in general Anything you think I should add?
I was pretty new to Haskell when I started experimenting with configurator (indeed, I still consider myself pretty new). I looked into configurator because it seemed to be the most endorsed, including ocharles's 24 days of hackage post on it. When you're a newbie exploring the merits of a language and its development ecosystem, and you run across what seems like a glaring deficiency in a widely used package, you start to wonder what's up. Yes, the experience was immensely disappointing. My ability to fork doesn't change that.
I'm also trying to write a similar presentation at the moment, although not about Haskell (it's more about using functional techniques in C#). I decided to go "back in time" and think about what coding was like before OO techniques (since I figured that most professional coders nowadays have only ever been taught OO, and aren't really aware there are alternatives). So I focused on two main thoughts: 1. Before OO, managing mutable global state was a major problem. OO attempts to address this problem by restricting who can alter what state. FP attempts to address this problem by eliminating mutable state. I showed some techniques and benefits using immutable objects. 2. With current OO, we have good techniques for composing data (composition, inheritance). But we have/use virtually no techniques for composing functions. So I tried to find some patterns of code that were repeated. I used the example of finding a UI TreeNode based on some predicate - the recursive code to do this is reimplemented many times, as is code to traverse other kinds of trees. I showed how to separate the "how" (traversing a generic tree) from the "why" (the specific code itself). I did this in a LINQy sort of way for added familiarity. Since I wasn't covering Haskell I didn't go into strong type safety, laziness, etc, but they would be good topics to cover too.
&gt; Yes, the experience was immensely disappointing. Well! I hope you never get stood up on a date, or get a pair of socks instead of Lego for your birthday. Meanwhile, submit a patch if you'd like to see the code do what you want.
There is clearly a defect somewhere, but this program simply *does not* eat up all memory and cause the universe to collapse; I am writing while it runs; nothing alarming is happening! The program takes what it can get, apparently 2.4 gb on the macbook "air" I am using, and returns with the correct answer; the SO guy killed the process after 22s, for what seems to be no good reason. ghc-head is much more sensible with memory, but still takes 20 some seconds to answer, while e.g. ocamlopt takes 5 . I have not been able to replicate the op's difference between gcc and e.g. ocamlopt (I don't have the other compilers) -- I cannot get gcc to be faster than ocamlopt, just similar -- but he is a troll so it isn't worth investigating. $ ghc -O2 ackermanbug.hs -fforce-recomp # ghc-7.6 $ time ./ackermanbug 65533 real 1m47.372s; user 0m26.577s; sys 0m17.641s $ khc -O2 ackermanbug.hs -fforce-recomp # ghc HEAD $ time ./ackermanbug 65533 real 0m24.081; user 0m23.920s; sys 0m0.133s Note the difference between 'real' and 'user' in the first case, which does seem to show something is wrong... But I can cause the ghc in any shape or form to use all available resources and freeze everything with a few lines of Haskell in countably many ways, as can any other competent or semi-competent user -- to say nothing of incompetent users! To belabor it that ghc responds poorly to *what was known to be the Worlds Most Pathological Program* decades before there were programming languages is simple troll-feeding. For example, notice that if we begin to 'optimize' the program, e.g. by intimating to the compiler that, actually, `ack 2 n = n + 2`, ghc responds earnestly even with hopeless arguments, but `gcc` segfaults instantly on ack 4 2. Try http://hpaste.org/86212 at home. Should I file a 'bug report'? If the gcc devs took the pathology of Ackermann's function seriously they wouldn't have a compiler. ...Maybe it segfaults because they can tell what I'm up to and refuse to have anything to do with it ;)
I don't mean to denigrate configurator. Complaining is much easier than contributing, and few are as prolific as yourself. However, making the Haskell ecosystem friendlier to newbs (who are, rightfully or not, accustomed to having certain functionality in easy reach) is essential to fostering greater adoption. As for a configurator patch, I'll put a pull request in later this week, take it or leave it. Going a little off the topic of configurator now: another, more illustrative example was when I went to the haskell IRC channel asking for the name of the function that recursively copies the contents of a directory to another location. The response? "Believe it or not, there isn't one, but it's easy to roll your own." Is it hard? No, but why should I need to spend any time away from my project addressing something so incredibly basic?
&gt; Is it hard? No, but why should I need to spend any time away from my project addressing something so incredibly basic? Because these are open source projects, which means that people do the best they can within the framework of many constraints on their time and attention. You're certainly within your rights to feel however you like about a piece of software and its shortcomings, and while you can state your feelings in any number of ways, the way you choose has consequences. As a pragmatic matter of psychology, stating that you are "immensely disappointed" in what is essentially a gift given to you by a volunteer ensures that they will at least not want to help you, if not outright flame you for so clumsily expressing your sense of entitlement. Here's a possible alternative way to get what you want without pissing me off, and I assure you this technique will work with other humans too. * "I noticed that there is no built-in way to write out a configuration file in the appropriate format. I'd find this really useful, but I am a newcomer to Haskell, and don't feel like I can add this myself. I wonder if someone could help, please?" See? No emotional negativity; positive framing; asking for assistance.
Does this seem like a famously pathological function that should use gigabytes of memory and grind my system to a halt in a matter of seconds? main = print $ stk 65535 65535 stk :: Int -&gt; Int -&gt; Int stk _ 0 = 0 stk m n = case loop m of 0 -&gt; stk (m+1) (n-1) _ -&gt; stk m (n-1) loop :: Int -&gt; Int loop 0 = 1 loop m = case loop (m-1) of 0 -&gt; 1 ; _ -&gt; 2 Only with optimizations _on_, though, not off.
It looks like a number of famously pathological functions; I would advise rethinking loop, which could be much simpler. Help can be found on #haskell. But once again, the 'grinding to a halt' is evidently a bug in your OS. `time ./ackerdolio` is indeed not getting clos --- wait, it just returned! time ./ackerdolio 0 real 2m42.565s user 1m0.550s sys 0m26.973s 
`ghc-HEAD` is much more rational, as we already knew: the relationships are the same as with ack, in both cases, ghc-7.6 ends up taking over 2gb of memory, ghc-head never more than 5 MB: $ time ./ackerdolio 0 real 0m53.995s user 0m53.739s sys 0m0.230s
I meant to complete the unfinished sentence "./ackerdolio is indeed not getting clos ---" by affirming that a couple minutes in, I am not feeling that my open files are threatened, nor am I prevented from typing in this text-box. And so it was.
[The Algebra of Algebraic Data Types](https://www.youtube.com/watch?v=YScIPA8RbVE) @ London Haskell user group (28-Nov-12) ([ANNOUNCE](http://www.haskell.org/pipermail/haskell/2012-November/023569.html)) 
Heh. I made the page crash by clicking on a graph while it's already animating. Cool to see functional web languages gain on libraries, though!
To expand on what vincentrevelations described, chrome at the very lest, shuts down the whole tab when the clicking one of the morph buttons when it is already morphing. @OP Looks nice, I will have to give it a try.
Sounds like an SVG bug in Chrome. Works fine in Firefox.
&gt; Works fine in Firefox.
So. Cool. Really impressed by the simplicity and quality library/presentation. Great job! Added it to the Fay wiki.
It's already enough, if not too much.
It was my version of 'second!' 
Dear codensity, I did not kill the process. I let everything happen on its own and it was killed by the system after eating up my memory and filling the swap space. Thanks.
shd keep you busy for, oh, 4-6 months http://www.haskell.org/haskellwiki/Research_papers/Functional_pearls -------- Bird's book is great too http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383/
is there a word for a monad that can be unwrapped? I thought I had heard it before... don't remember. Like it doesn't work for Future, but that's kinda the point of Writer (I think...).
Thank you so much!
Slides for the talk here: https://github.com/texodus/ohml And this Wednesday we have two awesome talks coming up. First on the features of cabal and second on an introduction to denotation and domain theory: http://www.meetup.com/NY-Haskell/events/112692222/
All research papers are, by definition, non-introductory in the sense that (1) it's always part of a conversation stretching way, way back; and to make it as a research paper, it's got to (2) contribute something **new.** To a beginner, everything's new, so it ends up a confusing read. Success here requires persistence following backlinks from the biblio, repeated as required. What you could do is look for expository papers. It's a fact of academic existence today that writing expository papers is severely under-appreciated. They are few and rare. Generally, I'd look out for festschrifts. [The Fun of Programming](http://www.cs.ox.ac.uk/publications/books/fop/) is one I know, and some of the chapters are online.
Read [why functional programming matters](http://www.cse.chalmers.se/~rjmh/Papers/whyfp.html) before any others!
It depends on what they already know. How long was their Java course, and to what extent can they code? Do they understand recursion? Linked lists? In any case it looks like you're trying to talk about too much stuff. Personally, I'd keep it simple and concentrate on functional programming as in "programming with functions": recursion, specific functions like sort, map, fold, and higher-order functions in general. I would mention purity, type safety and laziness only as far as they're necessary to make sense of the examples above. In other words, instead of trying to sell Haskell, I'd try to sell FP first.
That isn't what you said, which in any case seems to have changed every few minutes, but it goes to prove my point, which is that we have to do, on the one hand, with an unpleasant greediness in ghc so esoteric that went untriggered *for years*, but which was finally noticed and repaired in the last few months, and, on the other hand, *an ongoing and dangerous linux bug* which simply cannot be reproduced in other operating systems. Your conviction that 'the ghc is eating my computer' is simply a demonstrable falsehood.
I've heard some people say that [Making Software: What Really Works, and Why We Believe It](http://www.amazon.com/Making-Software-Really-Works-Believe/dp/0596808321) is a little like reading research papers light stacked on end. It's basically a book about how we need to read and make more research papers in software engineering. Royalties from the book go to Amnesty International. One of the authors of the book have given [a talk on the sad standard of evidence in software engineering](http://vimeo.com/9270320), and I really recommend anyone to watch it. It changed the way I view arguments and discussions. Previously, I would have accepted "two glasses of beer and an anecdote about a startup in Warsaw" as "proof." If you like the talk, buy the book!
1. You're right! I updated the section on Monads. 2. Added :D
To read a lot of advanced Haskell research, it's worth working through Pierce's Types and Programming Languages book first, it will give you an understanding of the mathematics and the formalisms used.
Here's a fun one that doesn't seem to difficult at first, but trying to apply it can quickly become brain-bending: though with awesome results. Written in part by one of the Haskell designers. http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf
Some other functions that fit into this class are `partition` and `splitEithers`. Secondly, `fmapMaybe` could be factored into `fmap` and `catMaybes` partition :: (a -&gt; Bool) -&gt; f a -&gt; (f a, f a) splitEithers :: f (Either a b) -&gt; (f a, f b) mapEither :: (a -&gt; Either b c) -&gt; f a -&gt; (f b, f c) catMaybes :: f (Maybe a) -&gt; f a You don't need the law fmap id . afilter p == afilter p . fmap id == afilter p since `fmap id = id` by the functor laws.
Interesting class! Are there examples of types that are instances of `Alternative` but not of `Select`, or of `Select` but not of `Monad`? This law doesn't hold for `[]` afilter p r &lt;|&gt; afilter q r == afilter (\x -&gt; p x || q x ) r f.e. when `p` and `q` are the same.
I suspect this has no non trivial instances since afilter p (r &lt;|&gt; s) == afilter p r &lt;|&gt; afilter p s requires you to be working with a set (or something like it) and not a multiset or sequence. While afilter p r &lt;|&gt; afilter q r == afilter (\x -&gt; p x || q x ) r excludes things like `Maybe`. On the other hand, if you were working with constrained functors than Data.Set would provide an instance that should satisfy(ish) these laws. 
an alternative law afilter p r &lt;|&gt; afilter q s = afilter (\x -&gt; p x || q x ) (r &lt;|&gt; s) which should also replace afilter p (r &lt;|&gt; s) == afilter p r &lt;|&gt; afilter p s is satisfied by list. EDIT: Also `Maybe`! 
They don't know what linked lists are. I don't think they learned about recursion explicitely, but perhaps someone clever realised that you can use it anyway.
That is not correct unfortunately. Creating objects in Java (for example) is very expensive. Also, malloc() takes 10x as long when called from Java via the FFI. I didn't believe it myself until actually measuring it. We ended up having *faster* code talking to the C++ code via a pipe instead of via the FFI. Go figure :-)
The Haskell type system is leading edge. Only dedicated proof systems like Coq or dependently typed languages (Agda) are stronger. However Haskell's advantage is that it has industrial strength compilers available (GHC for example) and a large active community with a huge selection of ready to use libraries (http://hackage.haskell.org/packages/hackage.html)
I think a more minimal set of laws would suffice, perhaps fmapMaybe (Just . f) = fmap f fmapMaybe (const Nothing) = const empty fmapMaybe f (x &lt;|&gt; y) = fmapMaybe f x &lt;|&gt; fmapMaybe f y fmapMaybe f empty = empty fmap f . fmapMaybe g = fmapMaybe (Maybe.fmap f . g) fmapMaybe id . fmapMaybe id = fmapMaybe Maybe.join or equivalently (and IMO more cleanly): catMaybes . fmap Just = id catMaybes . fmap (const Nothing) = const empty catMaybes (x &lt;|&gt; y) = catMaybes x &lt;|&gt; catMaybes y catMaybes empty = empty fmap f . catMaybes = catMaybes . fmap (fmap f) catMaybes . catMaybes = catMaybes . fmap join From this (together with functor and alternative laws) you get for instance afilter p . afilter q = fmapMaybe (guarded p) . fmapMaybe (guarded q) = catMaybes . fmap (guarded p) . catMaybes . fmap (guarded q) = catMaybes . catMaybes . fmap (fmap (guarded p) . guarded q) = catMaybes . fmap (join . fmap (guarded p) . guarded q) = catMaybes . fmap (guarded (\x -&gt; p x &amp;&amp; q x)) = afilter (\x -&gt; p x &amp;&amp; q x) where guarded p x = guard (p x) &gt;&gt; return x The relation with fmap and join for the maybe monad also suggests that it might be possible to generalize this to other monads. EDIT: added law for `catMaybes empty`.
Nice, "Cinder" could be confused with http://libcinder.org/ though
I'm actually not sure if that is the right approach for this talk as top-level bullets - it's too focused on how Haskell *achieves* the things that @dogirando spoke about, without motivating why. I find that when you start out with these topics for someone who knows nothing about Haskell, often enough their first reaction is "Wow, I'm sure glad I don't use Haskell," and that ends their interest. I like @dogirando's suggestions much better. Use one of those approaches to get your top-level bullets. You can briefly mention some of your lower-level bullet topics once you have motivated them and piqued their curiosity about how @dogirando's higher-level bullet topics could possibly be achieved.
That's pretty interesting, although I wasn't trying to imply that the FFI is superior somehow, just that doing something in normal Java is no longer *definitely way slower* than doing the same in C++. As an aside is creating objects really the fault of the GC? I don't know what kind of scaffolding Java puts up around objects. In Python it's just an INCREF macro and little else. 
"Theorems for free! " From this list: http://www.haskell.org/haskellwiki/Research_papers/Type_systems
This may be a stupid question (a thing which exists, I should know), but how does that second law fail for `Maybe`? Assuming that `afilter` is defined in the obvious way — `afilter = mfilter`.
Here is another view. Take the function plus : F a * F b → F (a + b) Where `*` denotes tuples and `+` denotes Either. This is equivalent to `(&lt;|&gt;)`: plus (x,y) = fmap Left x &lt;|&gt; fmap Right y x &lt;|&gt; y = fmap (either id id) ∘ plus (x,y) The proposed class corresponds to a left inverse of plus, unplus : F (a + b) → F a * F b unplus-is-left-inverse : unplus ∘ plus = id Which in the Haskell library is called `partitionEithers`. Other laws for `unplus` would correspond to laws for `plus`, for example plus-fmap : fmap (f +++ g) ∘ plus = plus ∘ (fmap f *** fmap g) plus-left : plus ∘ (f &amp;&amp;&amp; const empty) = fmap Left ∘ f plus-right : plus ∘ (const empty &amp;&amp;&amp; g) = fmap Right ∘ g unplus-fmap : unplus ∘ fmap (f +++ g) = (fmap f *** fmap g) ∘ unplus unplus-left : (f &amp;&amp;&amp; const empty) = unplus ∘ fmap Left ∘ f unplus-right : (const empty &amp;&amp;&amp; g) = unplus ∘ fmap Right ∘ g For some functors `unplus` will also be a right inverse, i.e. `plus ∘ unplus = id`. This is not the case for lists or `Maybe`. But it will be true for multisets.
oh, you are right. 
woops, I switched the laws in my post mfilter even (Just 1 &lt;|&gt; Just 0) = mfilter even (Just 1) = Nothing mfilter even (Just 1) &lt;|&gt; mfilter even (Just 0) = Nothing &lt;|&gt; Just 0 = Just 0
Yeah, the intended interpretation did turn out to be very set-like, huh; I did adapt them from laws for the relational algebra, which works with sets. So I'll have to think about how to weaken the laws to admit at least of multisets, if not lists.
What would you say are the prereqs for understanding that book? Does it rely heavily on knowledge of discrete maths or can any programmer jump into it?
Didn't you use to have a nice website with examples, sample output to JS, etc? All I see now is the ugly GitHub wiki :-)
It does assume a mild mathematical maturity, but I would hope that "any programmer" would have that (although time and again my experience shows otherwise). If you're not familiar at all with basic proof-writing skills, logic, inference etc. then the very brief description at the beginning of the book may not be sufficient to be able to write and do proofs yourself, but learning the concepts in the book is still valuable just to read and understand the terminology used in the papers.
Conduit seems much slower, is this expected? If so, is Snoyman aware of this?
I'm glad you posted my new code. I spent the time writing the new benchmark entry, and then personal life got in the way and I dropped the ball. Thank you :) I'm very interested to see how Warp compares here, as there's definitely some level of overhead involved in using Yesod. (I could have bypassed some of that by user lower-level techniques, but that didn't seem like a fair comparison.) I think converting the Yesod entry to use Warp should be fairly straight-forward, if anyone is interested in that please follow up with me, as I don't think I'll get around to doing it myself any time soon.
I don't know if he's aware, but I'm still digging to the bottom of why this happens because it still matters for classic pipes usage. These kinds of differences are WAY above intrinsic implementation time scales. To put this in perspective, each implementation is transferring about 32 KB at a time, and the file in question is about 220 KB, so we are talking about 8 transfers tops and I highly doubt the overhead of each transfer is tens of microseconds. The only thing I know of that could possibly take that long is a full garbage collection on every single transfer, but I need to investigate further because I don't see equivalent overhead when benchmarking pure code. Something very specific to doing IO triggers this pathological behavior.
I'd like to see a benchmark using [scotty](https://github.com/xich/scotty) ("A Haskell web framework inspired by Ruby's Sinatra, using WAI and Warp").
You haven't posted the link.
https://www.fpcomplete.com/school/pick-of-the-week/the-mother-of-all-monads
No problem; I'm glad it got in, too. It's a huge boost compared to the original entry, and gives us some things to look into in improving the Snap entry.
This led me to hdevtools and its vim integration. Really nice!
There's also Harper "Practical Foundations", content of which is freely available http://blog.ezyang.com/2012/08/practical-foundations-for-programming-languages/ http://www.cs.cmu.edu/~rwh/plbook/book.pdf
*[Calculating Functional Programs](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/acmmpc-calcfp.pdf)* by Jeremy Gibbons
Welcome back :-)
&gt; One of the authors of the book have given a talk on the sad standard of evidence in software engineering, and I really recommend anyone to watch it. ...btw, he mentions Haskell at around 00:47 ;-)
I believe `Cont` is actually overkill for this, you can get polymorphic `do` notation with [just the reader monad](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html?showComment=1338305803531#c5117471807774445845). Basically, you replace `f a` with `Monad f -&gt; f a`, where `Monad f` is the dictionary. 
Sweet! Will have a go at it asap :)
I cannot recommend this enough. When just starting out, the only way to do things is to follow the references. The way I started was to: (1) find papers that look interesting, (2) try to read them, (3a) if they're totally illegible then find something else, (3b) if they're only vaguely illegible then finish reading it and keep track of the main references and prior work, (4) go read those references, (5) come back and re-read the initial paper if I think I'd get more out of it now. Don't worry about reading things chronologically. Nor in any particular order (depth-first, breadth-first,...). Nor worry about exhaustively exploring the space. The goal when starting out is just to get the lay of the land; both for the topic/content of the research, but also for the style of academic papers and common research practices in the field. The one helpful limitation on doing this survey is to try to stay "on topic" and to organize any off-topic papers for reading later. One important thing, and this relates to [ky3](http://www.reddit.com/r/haskell/comments/1cwfs3/introductory_research_papers/c9kulkv)'s point, is: be sure to take notes *while* you read. More particularly, make sure your notes indicate *what* you were reading at the time. It's all too easy to get to this point where you have some knowledge and perhaps some vague recollection of where you got it from, but you don't remember enough to be able to do the fifth step above, or to recommend the paper to others, or to recognize when some idea you have is actually some novel synthesis of everything you've read. If you're an academic, then this art is of dire importance for your career; but even if you're not an academic, keeping an annotated bibliography is crucial for when you need to touch up your skills or you want to explain something to your coworkers/boss/etc.
Thanks
I'm confused about your penultimate step (and the requested law from the OP). Should not we have instead that: afilter p . afilter q == afilter (\x -&gt; q x &amp;&amp; p x) This is "logically" equivalent to the requested law, but of course it is not equivalent once we allow bottoms (e.g., `p x` is undefined in certain cases where `q x` is false) nor if we care about runtime complexity.
I read the list of required maths skills in the front of TAPL and went out and got Vellemans's How to Prove It and then worked through it. Highly recommended in general, possibly better for me on the TAPL front because I feel the need to do all of the exercises when I work through texts.
Yes it's based on a 2008 article. It's great to be able to play with the live examples.
In general I am enjoying using it. Great job! A comments. First the groundhog-mysql package doesn't build. I had to add an import statement to Database.Groundhog.Expression to get it to work. Also why yaml? Why not Haskell? I might not be the typical user, but I would prefer if Maybe was not handled in a special way. I would prefer generic sum type flattening by default. As it stands right now I can't easily have a Maybe of record type. Personally I would just prefer to have a naming convention on the fields used for serialization and just be able to write makePersist ''SomeComplexType 
I don't like this whole discussion. `Cont` isn't the "mother of all monads" in any interesting sense. At best it's the mother of all do-notations, but that's about it. To do this at all, you still need monad instances for everything involved, you just don't need support for do notation for all monads. But you still have to define return and bind and that takes all the oomph out of this "mother of all monads" thing.
yeah :o so much less effort to just use the wiki though. 
please write one!
Cont uses the *existing* monad definition and do notation. What you propose is newtype AMonad m a = AMonad {runAMonad :: Monad m =&gt; m a} instance Monad (AMonad m) where return x = AMonad (return x) m &gt;&gt;= f = AMonad (m &gt;&gt;= (runAMonad . f)) which although it does not require `m` to be a monad, does require that to run it. On the other hand, `ContT` only requires `m` be pointed (have a `return` function) and only uses bind for lifting. EDIT: perhaps another way to say this is "what do you mean by just the reader monad". Fully expand this type and you get: newtype AMonad m a = AMonad ((forall b. b -&gt; m b) -&gt; (forall b c. m b -&gt; (b -&gt; m c) -&gt; m c) -&gt; m a) which is a bunch uglier than newtype Codensity m a = Codensity (forall r. (a -&gt; m r) -&gt; m r) but, more than that, you might wonder which is bigger. Well, one can go from `Codensity m a` to `AMonad m a` codensityToAMonad (Codensity f) = AMonad $ \r b -&gt; (f r) but, **you can't go the other way without a monad constraint.** If `m` is a monad, one can define aMonadToCodensity (AMonad f) = Codensity $ (&gt;&gt;=) (f return (&gt;&gt;=)) note that aMonadToCodensity . codensityToAMonad $ (Codensity f) = aMonadToCodensity $ AMonad $ \r b -&gt; (f r) = Codensity $ (&gt;&gt;=) (f return) which is not necessarily the same thing as `Codensity f` codensityToAMonad . aMonadToCodensity $ AMonad f = codensityToAMonad $ Codensity $ (&gt;&gt;=) (f return (&gt;&gt;=)) = AMonad $ \r b -&gt; (f return (&gt;&gt;=)) &gt;&gt;= r which if `r` is the same thing iff `AMonad` can only be called with the actual monad instance. Going from `AMonad` to codensity and back gets you back to something that when giving (the one singled out unique) monad instances is what you started with. Take from that what you will. There is a ton of confusion over the nature of the "mother of all monad" argument. I strongly sugest people read the original work by Filinski. However, the intuition that x &lt;=&gt; (x &gt;&gt;=) is I think really easy to get a grasp on. A monad is something that can be bound. `Codensity` is the type of everything after the first argument in bind. Binding with return is identity (monad law). Bind associativity does not matter (monad law).
Cont is the mother of all monad, in that any monad that supports delimited continuations allows you to use any monad in direct style (hence adding continuations to a CBV language lets you use monads). In Haskell this is do notation, but in other languages it is the built in semi-colon.
start with the first minute or two of this 6minute video, by Simon Peyton Jones, [Haskell is useless] (http://www.youtube.com/watch?v=iSmkqocn0oQ) Then maybe pilfer a few slides from Tim Sweeney's talk: [The Next Mainstream Programming Language](http://www.st.cs.uni-saarland.de/edu/seminare/2005/advanced-fp/docs/sweeny.pdf) And maybe pilfer a few from Simon's talk [Caging the effects monster: the next big challenge.](http://research.microsoft.com/en-us/people/simonpj/) Be sure to include this quote from this talk of Simon's: &gt;The trouble is that weak type systems give types a bad name; they get in your way and stop you writing the programs you want to write. And that leads to the familiar but fruitless static-vs-dynamic debate that I hope to avoid entirely. The trick instead is to develop type systems that have the good properties you love (compact, informative, detect errors early etc), without the ones you hate (get in your way). &gt;Haskell is the world’s most crazy adventure playground for innovation in type systems. In this talk, Simon will sketch some of Haskell’s recent developments, to give you a taste of the kind of things that might come over your horizon sometime soon. [Adventures with Types] (http://skillsmatter.com/podcast/java-jee/keynote-3842) If you want to code, maybe run thru a line-breaking algorithm, the first 8 pages of [Bridging the Algorithm Gap: A Linear-time Functional Program for Paragraph Formatting] (http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.3229) 
I don't think that's suitable as an introductory text. 
Codensity is just `Cont` where the return type is universal. Condensity m a = forall r. ContT r m a = forall r. Cont (m r) a Is `AMonad` really just syntax? What happens if you pass it something else? Like, what happens if you pass it in `const Nothing` as the first argument? isJustWeird = AMonad (\r b -&gt; case r undefined of Nothing -&gt; Just True Just _ -&gt; Just False) setBit boll (AMonad m) = AMonad (\r b -&gt; if bool then (m r b) else (m (const Nothing) b)) I assert that these two functions let you use `AMonad` as if it were `ReaderT Bool` so long as the underlying monad is the maybe monad (you can generalize to `MaybeT`). Now, doesn't that make you suspicious that just maybe (pun not intended) `AMonad` gives you "extra stuff". (Clearly this is avoided if we define `AMonad` with typeclasses, but that defeats the point). Now, you might be right that `AMonad` is still smaller. Maybe my observations about converting between them support that view. But I don't think it is obvious by any means. And, here is the crucial thing--they have different requirements * To get "into" Codensity/Cont requires `&gt;&gt;=` (and nothing else) * To get "out of" Codensity/Cont requires `return` (and nothing else) * To get into AMonad requires nothing * To get out of AMonad requires *both* `return` and `&gt;&gt;=`. To me, that `lift :: m a -&gt; ContT r m a` is just `&gt;&gt;=` (wrapped in newtypes) and that `runCont :: ContT r m r -&gt; m r` is just `return` seems signifigant. With `Cont` these are the *only* times you use bind and return. `AMonad` might be a reader monad, but it is a very special reader monad (one with a very different shaped monad instance). It doesn't get you out of using the monad you started with. `Cont` does. That is why languages with continuations let you "represent" all the monads in direct style, but languages with just configuration parameters don't.
The EC2 instance used in the test costs $5.76/day, you need two of those so that's about 12$/day if you want to run the tests on equal hardware. The readme has instructions how to set up the servers, it doesn't appear to be too complicated.
Scotty is wonderful, but it looks like pull requests aren't being merged lately. Really hoping that work on Scotty continues as it it my favorite lightweight Haskell web framework.
I intend to cover more stuff in future tutorials! The thing is that left inverse properties are very common in simple problems (data transforms) whereas more complex properties are more domain specific. For doing a QuickCheck tutorial this means you have to explain two things: the domain *and* the testing.
Not necessary. Just make things easier. [See this](http://www.yesodweb.com/blog/2012/10/yesod-pure) if you want to use yesod without template haskell but you'll end up being overly verbose and repetitive about declaring routes, persistent types and a may be awkward mixing html with haskell. Haskell is a great language but things can be more efficiently expressed (and maybe a little safer) if using domain specific languages.
Thank you. I recently updated Groundhog.hs so that it generates haddock docs for its reexports and removed export of some internal names of Expression which affected groundhog-mysql. I will fix it later today, when I have access to Linux. I chose YAML because of poor state of records in Haskell. For example, instead of a single field called dbName, you'd have to use Haskell functions like setFieldDbName, setConstructorDbName, setDbEmbeddedName, etc. I had similar approach in the first version of Groundhog and it did not look nice. QQ is not mandatory (examples/withoutQuasiQuotes.hs). If we could express PersistDefinitions creation (and adjusting after initialization by introspection) using combinators as concise as YAML, that would be great. Maybe is handled in a special way to refuse types like Maybe (Maybe Int). For such types you can insert Just Nothing and get Nothing back which does not match. The helper class NeverNull is just a precaution orthogonal to other design. Sum type flattening will prevent NOT NULL constraints so I chose multiple tables. However, you can define your own instances of PersistEntity and PersistField which do flattening. There is naming convention which allows to write something like mkPersist defaultCodegenConfig [groundhog| entity: SomeComplexType |]. If it is important for you, I can add another function which works with TH Name like in your example. What do you mean by having a Maybe of record type? The following example works. data LinkedList a = LinkedList {value :: a, next :: Maybe (LinkedList a) } deriving (Eq, Show) mkPersist defaultCodegenConfig [groundhog| entity: LinkedList |]
I looked at it, it is straightforward to add dynamic background for buttons. I'll do it after reshaping of the lgtk interface and implementation. This reshaping will make the lgtk interface stricter i.e. lots of laws will be checked by the type system. I encourage you to experiment with the current lgtk interface too, sane programs should be able to be transported to the next release of the lgtk. Until colored buttons, please show color names as buttons labels which is possible right now!
(For the uninitiated: routing is the process of mapping URLs like /users/1 into actions like showUser 1, and views are the ways of writing down the HTML views with embedded data) As I understand it, there's two main reasons for Template Haskell (TH): 1. For generating type-safe and compile-time checked boilerplate code. Things like the routing and so on involve a lot of boilerplate. The best way to implement the boilerplate in a *safe* manner is to use a code-generating system like TH that can then compile the code through Haskell's normal type checking. 2. For efficiency. Things like the views in Yesod are converted into Haskell code so that the strings can be built in an efficient manner (effectively burning the views into code, rather than reading them as data from the file at run-time), and can potentially gain the benefits of going through the compiler. By contrast, Rails uses dynamic code injection to do the routing, which means that any bad route destinations are not necessarily picked up until you try to access the bad route (i.e. the errors come as late as they could, not as early as they could, like in Yesod). In development mode, Rails reads and interpret the views at run-time, which is obviously slower than compiling them upfront like TH allows. I think in production Rails does cache the views, which is a bit like dynamic compilation -- Haskell doesn't have this, so it must do up-front compilation, and TH allows the conversion from the view DSL into Haskell for this compilation to occur. 
I think for `Maybe` `unplus` is only a right inverse of `plus`?
I think you are correct. But for lists it is the other way around. Interesting. Essentially `Maybe (Either a b)` is smaller than `(Maybe a, Maybe b)`, while `[Either a b]` is *larger* than `([a],[b])`.
No, it's not necessary at all! It just happens to be that the framework authors seem to like this approach. There are plenty of ways to write powerful, boilerplate-less web application without resorting Template Haskell. Plain old Haskell combinator languages come a very long way, but it takes a bit more creativity to model them properly. I think [Zwaluw](http://hackage.haskell.org/package/Zwaluw) is a neat proof of concept.
Another use is quasiquoters. While external templates are great, sometimes you want to embed snippets of HTML, CSS, or JS inside your Haskell code. TH quasiquoters allow you to do that easily, while still retaining the compile-time type safety of the templating language.
Ignoring hard limits on heap and stack space and eating gigabytes of memory is a serious bug. Eating gigabytes of memory where it shouldn't in arcane and unusual programs is also a bug, but a less important one. Even on an OS that grinds the machine to a halt in that case.
Ping.
I've used both Happstack and Snap without template haskell.
no... I expect to dispatch http request handlers based on method, path, content-type, accept at minimum. better yet, dispatch based on request line and headers.
Happstack does this, without Template Haskell.
Can someone explain why ex2 = do a &lt;- return 1 b &lt;- cont (\fred -&gt; 10) return $ a+b test2 = runCont ex2 show main = print test2 Doesn't compile? I don't think I'm really getting how these continuations work. I realize I don't understand why the "escape" text doesn't include anything about the "1" from `a &lt;- return 1`. I guess I thought continuations were basically lambdas, but this clearly isn't the case.
A good templating system doesn’t require to ever do add any of those inside your programming language. Then again, templating systems are by definition a case of the inner-platform anti-pattern. I rather use Haskell directly than some shitty template “language” that either is too limiting or just a bad remake of Haskell itself. But there you have very specific snippets of Haskell inside e.g. HTML. Usually only variable printing and flow control. No logic. That way you still retain the separation of logic and structure. The other way around (HTML in Haskell) this separation cannot be done, AFAIK.
Haskell was the first language I seriously looked at (I had a class on Java, but we didn't get passed printing to the screen and writing simple classes), but it's now been several years later that I've come back to the language and have been able to understand much more of the language, like Monads, Arrows, and more. In my 2-3 year break from Haskell I learned Python, .NET, Java for real, Tcl, C++, and a handful of other languages, which certainly helped, but now I find myself applying Haskell patterns in other languages. I'd say it's good to know about, and know how to do the basics, but learning the advanced topics that are truly useful can take some time.
Zwaluw uses Template Haskell though
for some apps, possibly.. but request body can be really large.. for most cases request identification can be defined in terms of request line and (few) headers. 
Looking at the source code of the `Web.Zwaluw` module on hackage, there's no TH in there. I'm not sure what you mean.
sigfpe (the person who originally commented on `Cont` being the mother of all monads) is well aware of what a monad is. I just think he was being sensationalist.
Maybe because `cont` parameter must have type (Int -&gt; String) -&gt; String ? `fred` is a continuation with composed `show`, that is whole monad experession is equivalent to continuation = \b -&gt; 1 + b test2 = (\fred -&gt; fred 10) $ (show . continuation)
Any install guide available?
Thanks, I didn't know of the existence of code generators like this one. Maybe it's just my background in other languages, but sometimes I think Haskell libraries rely too much on the compiler, resulting in overly complex code where a simpler solution might be appropriate. That said, as I learn more Haskell I guess I'll form a real opinion on quasi quoting and template Haskell vs code generators. They do sort of the same thing right? 
I don't know how good the examples in the article are. The types are left off everything, but when we look at the type of `i`: i :: m a -&gt; Cont (m r) a So when I go to embed multiple different monads at the same time in this 'mother of all monads', it isn't going to work out. Something like: do x &lt;- i [1, 2] ; y &lt;- i Nothing ; z &lt;- i ask ; return (x + y + z) is a type error. So this isn't making use of a single `Cont R` that many monads can be embedded into, it's picking a different `R` for each monad. It's kind of like saying that `Codensity` is the mother of all monads, because you can embed `m` into `Codensity m`. But `Codensity` isn't a monad; `Codensity m` is a monad for each choice of `m`, and it's potentially a different monad for each choice of `m`. I don't recall the Filinski paper having this flaw, but it's been a long time since I read it.
I can comment on why I added the first template haskell code to the snap starter project. Running in development mode requires having a set of compiler options at run time that is consistent with (though not identical to) the set used when compiling the application. It turns out that all the compile-time options are easily observable at compile-time. So the best way to figure out exactly what's going on is to look at exactly what's going on, at compile-time. And that means template haskell.
https://github.com/MedeaMelana/Zwaluw/blob/master/Web/Zwaluw/TH.hs
It's a bit of a stretch to say that Snap *relies* on Template Haskell. Snaplets use lenses and the lens library provides Template Haskell functions to automate the creation of lenses. You could easily create your own lenses in just a few lines of code without using any Template Haskell. In fact, you can create lenses compatible with the lens library without actually importing lens! Here's an example of how to define your own lens. data Point = Point { _x :: Double, _y :: Double } deriving (Show) xLens f (Point x1 y1) = (\x2 -&gt; Point x2 y1) &lt;$&gt; (f x1) And you can verify that xLens is indeed a lens... ghci&gt; let p = Point 2 3 ghci&gt; import Control.Lens ghci&gt; p ^. xLens 2.0 The same is not true for Yesod, because it has a much more substantial use of Template Haskell.
What it comes down to is I would like to be able to embed my sum types. When I run data Point = Point { x :: Int, y :: Int } instance NeverNull Point where data Foo = Foo { location :: Maybe Point } mkPersist (defaultCodegenConfig {migrationFunction = Just "migrateAll"}) [groundhog| definitions: - entity: Foo - embedded: Point |] testFoo = Foo . Just $ Point 1 1 test = withMySQLConn connectInfo $ runDbConn $ do runMigration defaultMigrationLogger migrateAll insert testFoo I get: *** Exception: mkColumns: datatype inside DbMaybe must be one column DbEmbedded (EmbeddedDef False [("x",DbInt64),("y",DbInt64)]) I think having both options would be nice. I can see reasons to have multiple tables and to embed sum types.
Taking my own advice, I'm strategizing on how to retain category-theoretic concepts when the vernacular seems so arbitrary. Someone once confessed that he could never tell meets from joins. In addition, I don't understand the 'over' and 'under' distinction of comma and slice categories. Strangely, I don't feel that way about '-' and 'co-', possibly because the non-prefixed half-concept is fully wrongly given priority? So far a search has turned up Marcel Jousse [1]. Not so much a sign language for CT. I need to be more organized studying hip-hop. [1] www.youtube.com/watch?v=ajFq-3ziBaE 
Thanks for that. I still don't have a real grasp on lenses. I'm a real beginner here, and I've just really started to understand how things fall into place. I didn't realize that lenses were something with a structure as simple as what you described above.
&gt;(and maybe a little safer) if using domain specific languages. How would they be safer? Quasi-quotation is generating haskell code. That haskell code is precisely as safe as it would be had you written it directly, is it not? The difference is just syntax.
Later this week. For now, you can check out the repo, install ipython, then run ipython notebook in the repo root. In the notebook start a cell with %load_ext ghcimagic and run the cell. That will turn all the cells into Haskell cells. Be careful about starting infinite computations, it doesn't recover well.
More precisely, it's GHCi embedded in the IPython Notebook. We're working on cutting the IPython kernel out of the picture and interfacing directly with the GHC API. More here: http://bfj7.com/posts/2013-04-22-innovation-week-days-1-2.html
How do you get set, your own van laarhoven lenses, without the lens library? I think they are awesome, but like vamega, I'm interested in non TH, web apps, for trying an arm server, or writing mobile clients. Is the TH in Lens, necessary for polymorphic update? 
&gt; That said, as I learn more Haskell I guess I'll form a real opinion on quasi quoting and template Haskell vs code generators. They do sort of the same thing right? Sort of. They both take some input and transform it into compiler input. TH does it inline at compile time, code generators make actual files full of code that you can read, etc. You can also do things like routing without code generation (Scotty does this) or templates without code generation (hastache does this), but you lose some performance and a lot of safety. Haskellers tend to like safety at compile time, so they give as much information to the compiler as they can.
&gt; Maybe it's just my background in other languages, but sometimes I think Haskell libraries rely too much on the compiler, resulting in overly complex code where a simpler solution might be appropriate. It's interesting, because I'm the other way around. I think most other languages rely *too little* on the compiler, and the programmers are spending hours upon hours doing the compiler's job, manually expanding `for` loops and writing out type declarations.
The TH in lens generates the code I gave above and does not rely on the lens library. That is useful because it allows you to write a library and export lenses for that library without needing to depend on lens. Now, if you want to *use* those lenses you'll either need to import lens or re-implement a subset of the functionality that you need.
Ahh, hadn't checked if there was a newer version on github. Cheers!
Ok, thanks.
A lens in Haskell is kind of like a pointer or reference in imperative languages. If you think back to C, what does a pointer allow you to do? It allows you to both get and set the value. Similarly, a lens is essentially a getter and setter.
Hi! Coauthor here. Yes, Zwaluw uses TH. There's a difference with Yesod's TH routing though: the functions Zwaluw derives from your data types are route-agnostic. The generated functions are just building blocks you can use later when you actually define the routes. With Yesod you write the entire routing table in TH quasiquotes. This makes writing routes in Yesod more succinct but also less modular: it's harder to capture common routing patterns in Haskell functions, for example. In Zwaluw, using TH is optional; you can write the generated functions by hand; they're not very big or complicated. The `constrX` functions in [Web.Zwaluw](http://hackage.haskell.org/packages/archive/Zwaluw/0.1/doc/html/Web-Zwaluw.html#3) show examples for some well-known constructors. You can also use generic programming to derive them; [this example](https://github.com/MedeaMelana/Zwaluw/blob/master/ExampleRegular.hs) uses the [regular generic programming library](http://hackage.haskell.org/package/regular) instead of TH. In fact I think it should be possible to derive them using the [GHC.Generics](http://www.haskell.org/haskellwiki/GHC.Generics) which is available in recent GHC versions. The route tables you write in Zwaluw carry around type information about the types of values they parse and produce and are therefore composable in a type-safe way. For example, you could take two route tables `r1` and `r2`, say for websites `example1.com` and `example2.com`, and compose them together to use in a single website where you use `/site1` as URL prefix for all routes in `r1` and `/site2` as prefix for all routes in `r2`: "site1" / r1 &lt;&gt; "site2" / r2 That said, I wouldn't recommend Zwaluw for serious use in large websites: there's no error handling and there's been no performance tuning. If I were to continue working on Zwaluw, I'd reuse [Data.Iso.Core](http://hackage.haskell.org/packages/archive/JsonGrammar/0.3.4/doc/html/Data-Iso-Core.html) from JsonGrammar, which uses the same trick, and I'd drop TH support in favor of `deriving Generic`.
I would avoid recursion as well. From the description of their audience, it sounds a little too technical in the same sense that monads or lenses are. I would focus on using the standard library functions (map, filter, forM_) over expressing anything with explicit recursion.
If `ST` works, then so will `State`; see [pure-st](https://github.com/pthariensflame/pure-st).
what is this i dont even
What exactly is your motivation to replace TH with code generators? 
Don't worry, I found a much better and elegant solution.
I use Warp (the webserver that underlies Yesod) without TH also.
You might want to have a look at the [ruby](https://github.com/minrk/iruby) and [Julia](https://github.com/JuliaLang/julia-ipython) kernels. Both are still rough-cut prototypes, but they point towards how to implement a 'native' ipython kernel in another language (though to be fair, with Julia I cheated quite a bit, by re-embedding Python via PyCall.jl so I could reuse the IPython communications code). The basic idea is that you 'just' have to implement the [IPython messaging protocol](http://ipython.org/ipython-doc/rel-0.13.1/development/messaging.html), and then all clients should work (with bugs we find along the way needing to be fixed, of course). Feel free to hop on the [mailing list](http://mail.scipy.org/mailman/listinfo/ipython-dev) with any other questions, we don't exactly hang out in the Haskell forums :)
The code generator itself can enforce additional invariants. As an example, `deriving Show` guarantees that the resulting instance follows a certain form. Sure, you could have written the generated code yourself, but you could also have written it incorrectly by accident.
Thanks. &gt; (1) avoiding language extensions in production code Why? Is it because they are unstable? Too bad that it is hard to live without some of the extensions.
To me TH is just cool meta programming... GHC widely supports it, and it is stable. I think `acidstate` is also pretty cool, and also commonly used with TH. Just trying to answer: &gt; Why do all the Haskell Web Frameworks rely on Template Haskell?
What's the right way to think about smaller/larger when both are infinite?
Language extensions are less portable. This matters more as people explore Haskell to Javascript compilers other than `ghc`.
The TH has nothing to do with that.. The lens video explains how to use that structure for a getter and setter. If you instantiate the functor type to be Const a, you get a getter: data Const a b = Const a lens :: (a -&gt; Const a b) -&gt; (s -&gt; Const a t) The b/t are not in the game. Similarly, if you instantiate with Identity, you get a modifier function: lens :: (a -&gt; Identity b) -&gt; (s -&gt; Identity t) Basically can read that ignoring the Identity parts. Ignore the a and give a new `b` and you have yourself a setter. 
Thank you. I didn't get through the video, have been doing the background reading before I take another stab at it.
Thanks Fernando, that's a great pointer.
No, but that sounds interesting. I thought I heard some of the ScalaZ guys talking about a name for monads that had an unwrap function. I may be thinking of a comonad, but that's more like an un-bind than an unwrap, eh? 
I have no trouble without extensions. Haskell98 is many times more powerful than most popular programming languages.
All comonads have `extract`: extract :: (Comonad w) =&gt; w a -&gt; a
This is because `Cont` is newtype Cont r a = Cont ((a -&gt; r) -&gt; r) so the return type of the continuation must match the type returned But, `Cont` could be defined newtype ContI r s a = ContI ((a -&gt; s) -&gt; r) and it becomes not a monad, but an indexed monad class IMonad m where return :: a -&gt; m r r a (&gt;&gt;=) :: m r s a -&gt; (a -&gt; m s t b) -&gt; m r t b and then your code would typecheck just fine. Actually, almost all monads have more general indexed versions. For example, state should have the form newtype StateI r s a = StateI (r -&gt; (a,s)) which allows you to vary the type of the state (just as `ContI` allows for varying the type of the continuation).
Wow, with the help of the helpful link, I actually understand nearly all of those words. I actually recall seeing this link before and being interested on looking at two recommended books (Lawvere and Rosebrugh, *Sets for Mathematics*, and Goldblatt, *Topoi, the Categorial Analysis of Logic*). But somehow I didn't notice the first time that [Goldblatt's book is available online](http://historical.library.cornell.edu/cgi-bin/cul.math/docviewer?did=Gold010&amp;seq=&amp;view=50&amp;frames=0&amp;pagenum=1). I'm not sure what you mean by "a filtration," though—is it some topos-related concept the link is not defining, or is it just a word you're using informally to refer to my `afilter` and similar operations?
I think there are two different ways to deal with Maybe and I think they are both valid. One is to interpret it as columns with NULL, the other is to just think of it as another sum type.
The core of happstack, `happstack-server`, does not rely on template haskell. [1] Some of the optional libraries use some template haskell. First we should backup and ask, "Why do all the Haskell Web Frameworks rely on the deriving mechanism?" aka, `deriving Read, Show, Eq, Ord`. And the answer is that the deriving mechanism reduces errors, reduces boilerplate, and reduces development time. The reason why almost nobody ever complaints about the use of the `deriving` mechanism is that it is predictable. In the Happstack family of libraries, most of the instances of TH are to implement things like `deriveSafeCopy`, `derivePathInfo`, etc. Really, what we want to do most of the time is just extend the `deriving` mechanism. But, since there is no official way to do that, we use TH. And, in the same way that you do sometimes implement instances like `Eq`, `Ord`, `Show`, `Read`, etc by hand instead of using `deriving`, you might also implement a `SafeCopy` instance by hand. For `PathInfo` we actually offer a very rich combinator language that you can use instead of letting TH do the dirty work. Though, I find that in practice `derivPathInfo` does exactly what I want, and I don't need anything more sophisticated. Another places that Template Haskell shows up in Happstack is when we us the newer `QuasiQuotation` feature. In Yesod, they use this mechanism to create all sorts of new syntax with its own special rules. For example, their routing stuff. In Happstack, we use QuasiQuotation to embed *existing* syntaxes into Haskell. For example, we use the `jmacro` library so that we can embed `Javascript` into our code. Because `jmacro` actually parses the javascript, we actually get compile time errors if the javascript contains syntax errors instead of mysterious runtime errors. So, here we are exploiting a syntax that people already know *and* making it easier to develop code using it by generating better error messages. Trying to embed Javascript as a Haskell DSL is actually far worse, IMO. You end up having to think in Javascript and then add an extra mental layer of translating that Javascript to some DSL. It's much easier to just stick with the Javascript syntax for the most part. Another place we used Template Haskell is in the `web-routes` library, which provides type-safe URLs. `web-routes` itself has no required way of converting a URL type to a url string and back. The core library does not care.. you just provide a set of functions like, `url -&gt; Text` and `Text -&gt; Either ErrorString url`. If you want to write those functions by hand, you certainly can. Every time I have ever done that, I have screwed up and made mistakes. Another options is to use the `derivePathInfo` TH function, which automatically derives those functions using a simple algorithm to convert the constructor names into path segments. However, you can also use [`web-routes-boomerang`](http://happstack.com/docs/crashcourse/WebRoutes.html#web-routes-boomerang), which provides normal, composable Haskell combinators which allow to specify how you want to map the types to urls and back. The `boomerang` combinators simultaneously describe how to parse *and* pretty print the urls, ensuring that the functions you provide to the core `web-routes` code are actually inverses of each other. You can even have a route where some parts are created using `derivePathInfo` and some parts use `boomerang`. So, to answer your question, Template Haskell is not required. Just like the `deriving` clause is not required. But it can be advantageous and not scary to use Template Haskell. The big issue is that Template Haskell and especially QuasiQuotation can be easily abused. So, while the three big frameworks have bits of Template Haskell and QQ in them.. they do not use those extensions the same way. And that matters. Another thing that is important to consider is user facing TH/QQ and internal TH/QQ. In hyperdrive (the upcoming HTTP(S) backend for Happstack), we use Template Haskell as part of a system that proves our parser is following aspects of the HTTP specifications. The end user never sees or directly interacts with this TH code -- they merely benefit from it indirectly. That would be different from TH code that the user might actually want to invoke like `deriveSafeCopy`, `derivePathInfo`, etc. [1] In fact, there is a tiny bit of Template Haskell used internally in `happstack-server` due to a long standing *really bad idea* in the `network` library. Specifically, its support for IPV6 is based on a compile-time 'flag' that changes the API of the `network` library. This is explicitly against cabal policy for good reason! But `network` predates cabal and this realization, so we have to hack around it.
&gt; 1. For generating type-safe and compile-time checked boilerplate code. Things like the routing and so on involve a lot of boilerplate. The best way to implement the boilerplate in a safe manner is to use a code-generating system like TH that can then compile the code through Haskell's normal type checking. Except, Template Haskell is not required for that. `web-routes` provides a similar type-safe URL library (which actually predates Yesod). While you can use the `derivePathInfo` function to automatically derive the route mapping from type, you also have the option to use the [`boomerang`](http://happstack.com/docs/crashcourse/WebRoutes.html#web-routes-boomerang) library to provide those bidirectional route mappings. Because it is based around normal Haskell combinators, you can easily combine those combinators to create more powerful, and reusable routing combinators. With `Yesod`, if you want to do something that is not supported by their non-extensible TH/QQ stuff, you either (1) can't or (2) have to write some class instances by hand with no real support for safety. 
Sure. That is pretty much how they all work. But, *how* do you dispatch on the `path`, for example? In the core of `happstack-server` there are some simple combinators like `dir` and `path` which allow you to dissect the path and match/capture values. But, where did those URLs come from originally? Someone had to generate them.. what happens if you change the route.. how do you realize that you have code that needs to be updated to use the new updated URLs. That is the problem that type-safe routes tries to solve. Instead of hard coding strings like "/foo/1" in your code and hoping that the route that matches on "/foo/$n" never changes.. you add an intermediate layer that adds some-types. You have a type like `data URL = Foo Int`. To create a route you do something like: &lt;a href=(Foo 1)&gt;A link to Foo 1&lt;/a&gt; And in your router you have something like: case url of (Foo n) -&gt; fooPage n Now, if you try do something like `&lt;a href=(Foo "bar")&gt;oops&lt;/a&gt;`. You get a compile time error. Or, if you Foo is changed to required two `Ints` instead of one you get an error. Of course, not everybody sees the benefit in that, which is why we offer both systems in Happstack. In fact, we have actually improved the non-type-safe path routing code for Happstack 8.0. 
One complaint, though, is that Yesod relies on QQ to implement DSLs when an normal, composable Haskell eDSL would be more flexible and just as safe.
filtrations: http://en.wikipedia.org/wiki/Filtration_(mathematics) After my post i realized by the way that spivak uses toposes as key tools in his stuff on databases as categories! So my intuition here corresponds to a precedent with some actual work put into it: http://math.mit.edu/~dspivak/informatics/FunctorialDataMigration.pdf 
Ah, yes, that Spivak PDF looks like a great lead, but I'm afraid that PDF is garbled or just radically unfinished. No worries, there's quite a bit more [in the directory that came from](http://math.mit.edu/~dspivak/informatics/notes/unorganized/), and now I'm going to have to look through all of it... Thanks!
I just wanted to follow up with a slightly more concrete example from boomerang (which is based entirely on Zwaluw). For a route we might create type like this: data Foo = Bar | Baz Int Char deriving (Eq, Show) If we could just do `deriving (Eq, Show, PrinterParsers)`, nobody would think twice, but we can't so we do: $(derivePrinterParsers ''Foo) Now, we have to actually define the route mapping. We can do what ever we want, but this is a pretty natural choice: foo :: StringPrinterParser () (Foo :- ()) foo = ( rBar &lt;&gt; rBaz . "baz-" . int . "-" . alpha ) We could instead go with the less obvious: foo2 :: StringPrinterParser () (Foo :- ()) foo2 = ( rBar &lt;&gt; rBaz . "bananas-" . int . "-pointless-" . alpha ) We can define whatever mapping we want. The `rBar` and `rBaz` functions were generated by the `derivePrinterParsers` call. All those functions do is apply or remove the `Bar` and `Baz` constructors during the Parsing/Printing process. Here is the code that is generated: rBar :: forall tok e r. PrinterParser e tok r (Foo :- r) rBar = xpure (\a -&gt; Bar :- a) (\b -&gt; case b of (Bar :- a) -&gt; Just a _ -&gt; Nothing ) rBaz :: forall tok e r. PrinterParser e tok (Int :- Char :- r) (Foo :- r) rBaz = xpure (arg (arg (:-)) Baz) (\ b -&gt; case b of (Baz x y :- r) -&gt; Just (x :- y :- r) _ -&gt; Nothing) There is a lot of boomerang specific noise in there, but looking past it, we can see that these functions are entirely boring. For example, in `rBaz` we have the expression: ((arg (arg (:-)) Baz) which just says that the `Baz` constructor needs to be applied to its two arguments when parsing. And, (Baz x y :- r) -&gt; Just (x :- y :- r) extracts the two arguments out of the `Baz` constructor when printing. Nobody in their right mind would ever want to write these functions by hand. There is really only one sensible implementation for them and it is entirely formulaic. The exact sort of thing that a machine would be good a doing. Using those generated functions to define a route mapping -- that is where the interesting stuff happens -- stuff that robots might not be able to do. As the parent says, in Zwaluw/Boomerang, all the interesting parts of the route mapping are done by defining the routes using normal Haskell combinators. That differs from Yesod where all the interesting parts are doing using some special little language they made up. 
epic!
whoops i linked to the wrong thing. i replaced it with something a bit more fleshed out...
I was thinking of 'x is isomorphic to a subset of y'. You are right that if both types are countably infinite then this is always the case. But, I think, not always continuously for all type variables that are quantified over. For `Maybe (Either a b)` vs. `(Maybe a, Maybe b)` you can talk about large and small when `a` and `b` are finite. For lists you need look at a certain fixed number of `a`s and `b`s, since you want to be able to universally quantify over those types. If there are 3 as and 5 bs, then `[Either a b] → ([a],[b]) ≈ 3!*5!` and `([a],[b]) → [Either a b] → ≈ 8!`.
Yeah, if we treat Maybe as a sum type, its mapping to columns is easier. It is not clear how to make Maybe fields switch between NULL and sum type approaches, but that's a solvable technical question.
I can't help but think this might just be beyond the scope of /r/Haskell.
You should try to learn some Haskell to cheer you up. A good starting point is [Learn You a Haskell](http://learnyouahaskell.com/), if you have any questions you can ask to the folks at the #haskell IRC channel on freenode. Hope that helps!
Well, as somebody who only engages in open relationships I would say it's fine. But that depends a lot on how you personally see this issue and what you have agreed on with your partner (if no agreements have been made I'd say it's natural to assume that he/she can do whatever they want, though many people would disagree on that). I can't really understand those who get all upset about their partner being interested in / doing things with other people as well. You're betraying yourself if you claim to never look at other people while in a relationship. I don't understand the big deal with cheating / focus on fidelity blabla, people should do what makes them happy. My 2 Öre ... PS: Can't believe I'm talking about open relationships on /r/haskell. I couldn't even think of any type-related puns to put in this reply.
Right, no problem with Maybe/Either (I didn't even consider that `a` and `b` might also be potentially infinite). And I was *going* to say that I can also see it for finite lists, but now I've made myself uncertain. Why is a fixed number of `a`s and `b`s the right approach? That's not how we look at the Maybe/Either example. It *is* basically the same way I was thinking about it earlier -- that the length of the `[Either a b]` would be the sum of the lengths of the `[a]` and the `[b]` -- but why is that justified? (And of course for infinite lists, I don't know what you can say at all.) I'm genuinely curious about how this kind of thing works in the general case for potentially (or always) infinite structures. With `Maybe (Either a b)` versus `(Maybe a, Maybe b)`, it's clear that `1 + (a + b) &lt;= (1 + a) * (1 + b)`, given `a &gt;= 0`, `b &gt;= 0`. But that breaks down if you have infinite sums on both sides. For finite structures, if you get the same expression (or just number, in the absence of type variables) on both sides, it means they're isomorphic. For infinite ones, says lists versus trees, it feels "obvious" that they're different, and that trees are bigger, but it's not clear how it falls out from the math (if it does -- like so many intuitions about infinity, it could simply be wrong). &gt; For lists you need look at a certain fixed number of as and bs, since you want to be able to universally quantify over those types Not sure I follow - why does the latter imply the former?
Like GP, I detect a whiff of selling Cont as a universal monad the way a limit is an ultracone. That Cont is just ultrado leaves me overwhelmingly underwhelmed.
&gt; Embedding multiple monads does not work out for the same reason you can't (easily) use multiple monads at the same time in Haskell. This isn't a very satisfying answer. I skimmed the Filinski paper, and it also looks like he kind of papers over the problem by using some `Any` type that you can manipulate in not-very-principled ways, like coercing an `m a` into, and then getting it back out to do useful work on the values you've hidden in it. And I would assume that any native continuations are expected to behave the same way. But this is not what I'd expect out of a principled, 'this one monad subsumes all other monads.' &gt; Filinski's later paper on representing layered monad showed how to do just that. And that uses a language with an effect system, with an effect per embedded monad. That's cool, but it also doesn't sound like one single monad subsuming all others, which is how this is usually sold.
Currently I use symbols in buttons to represent state. By the way, would you mind explaining how Free monads are used in the library?
there are several different formulations of lenses that all support the same basic API. The version from 'lens' is a bit more abstract, and you might find the flavor of lens in 'data-lens' to be more intuitive for learning purposes.
`Codensity` is just a more principled alternative to `ContT` in this setting. With `ContT` you can ignore the current continuation. With `Codensity` it is the only way to get the final `r` as you know nothing else about it.
I don't think it is just about `do`--the statement is a more something like "Proposition: for any monad `m` and any value `x` of type `m a` is uniquely charachterized by what happens when you bind a continuation to it. `forall r. Cont (m r) a` is the type of `(x &gt;&gt;=)`. Further, `Cont j` is a monad for all `j`. Corollary 1: Any monadic language providing operations equivalent in power to those of the `Cont` monad is able to embed any monad (encodable in that language) in direct style. Corallary 2: Do notation over the `Cont` monad is able to embed any monad (encodable in Haskell) in direct style. " Does Cayley's theorem leaved you underwhelmed also?
If you haven't seen it already, we're doing something similar with Scala at https://github.com/Bridgewater/scala-notebook. If you check the issues list on there, you'll see some of the IPython people talking about generalizing the interface so that it becomes easier to replace the back ends. You might want to try to coordinate that with them (and scala-notebook maybe) so we can all share as much of the front-end code as possible.
I'm also doing something similar, unreleased, but some old screenshots here: * http://hdiff.luite.com/wolfgang/ This is supposed to be a public hosted wiki with editable interactive Haskell notebooks (lhs files with markdown/mathjax, every page is a module, code can be shared between pages) similar to SoH (I started working on this well before fpcomplete existed though), and also locally installable. Problem was that I was never really satisfied with the interactivity allowed by such a public setup: All Haskell code is run by some interpreter in a secured (SELinux) environment on the servers. Long running incremental calculations aren't allowed, since interpreters get killed after a short timeout to preserve server memory. The current idea is to not run all Haskell on the server, but instead let the server compile the code to JavaScript with GHCJS. This way we can have responsive interactive user interfaces and interactive graphics, fully generated by Haskell. The public wiki will allow simple games, scrollable plots, interactive animations etc. very early examples, using the Gloss library with a custom html5 canvas backend instead of opengl: * http://hdiff.luite.com/gloss/pong/ * http://hdiff.luite.com/gloss/styrene/ Even though some more performance improvements for GHCJS are on the way, the code will always be a lot slower than Haskell running natively, so the local version will still be able to call native Haskell for heavier computations, with some restrictions in the results that can be produced. * https://github.com/ghcjs
&gt; Most of them got pretty bored. Presentations where most people don't get bored are pretty rare AFAICT, and no I'm not just talking about mine. Different people will be differently able to follow what you're saying. If no-one finds your presentation too slow and boring, lot's will find it too fast - and once they give up trying to understand it they'll get bored. 
&gt; Filinski/Piponi: Every Monad is "a submonad" (what ever that means) of a canonical continuation monad. I dare you to make this precise category-theoretically. No hand-waving, squinting, or same-diff "what ever that means". It's easy to invoke Cayley and Yoneda, much harder to get the analogies right. See for instance [1]: "Indeed, the obvious generalisation of Cayley’s theorem to monoids ... is false." Because of this and other reasons I give below, I don't think you can do it. [1] http://www.martinorr.name/blog/2009/05/10/cayley_and_yoneda 
How does quasi-quotation make that possible though? You can do that just fine with plain template haskell (or even generics now?). Quasi-quotation is literally just allowing you to use a different syntax, it isn't creating any safety.
&gt; "Proposition: for any monad m and any value x of type m a is uniquely charachterized by what happens when you bind a continuation to it. A value of *any type* is "uniquely characterized" by what happens when you bind a continuation to it, if you really want to push this CT-ish idea. &gt; forall r. Cont (m r) a is the type of (x &gt;&gt;=). Overlooking the fact that you've switched the type of `x` from `m a` to `a`, I observe, again, that there's nothing inherently monadic about this typing judgment. It's an undistinguished instance of the type swizzling embodied in ``let f = flip id in (x `f`) :: forall r. Cont r a``. &gt; Further, Cont j is a monad for all j. So is `State s` for all types `s`. Your point being? 
By the way, if it isn't clear, the talk isn't about learning ML, it's about writing a subset of ML, compiling to JavaScript, in _Haskell_, covering parsing, type inference, and generation all in the span of about an hour.
As stepcut has said on here before, a lot of the TH stuff in libraries like TH could be replaced with GHC's deriving function (the TH in AS mostly generates some class instances, I used it in a project recently where I could not make use of TH so I know what it looks like when you write it manually - check [here for example](https://github.com/tazjin/democrify/blob/master/Democrify/Acid.hs) and scroll down to the stuff at the end). If I got things right from what he said adding deriving support is currently impossible / very hard which stops them from doing exactly this.
Bad practice to insert new records into the database with GET requests. createNewProject should be a POST.
&gt; Yes, I know that using the GET method to create a new project isn’t exactly considered best practices. But dealing with POST requests in Snap presents a set of challenges that we can safely avoid for now. This aspect of Snap, however, is very much worth learning." It even says so in the article. He was specifically avoiding it for the purposes of the tutorial. EDIT: I was not attempting to justify the decision, just noting that the author specifically stated this in the tutorial...
A tutorial shouldn't specifically omit a very basic thing that everyone will need for even the simplest app. And it isn't just a case of "best practices", the HTTP spec explicitly says you should not do that. I'd really like to know what challenges he thinks handling POST requests present. I'm pretty sure his example code would work as-is if he changed the form to use POST.
I believe you are mistaken when you say that web-routes without TH predates Yesod. If you check the Hackage upload dates Yesod predates the first boomerang upload, even if you take the 0.7 version which is the earliest uploaded to Hackage but Yesod had been in development for quite some time then.
Tbh my biggest problem was I wrote the talk figuring most of the students that would attend would have taken the Haskell class, but only two had taken it. I ended up skipping through a lot of my slides and going over basic syntax and list processing stuff. My slides were mainly about monads, lifting, and minimizing writing code that unboxes data and reboxes using higher order functions.
Indeed. Saying that POST requests present "challenges" seems like a disservice to Snap.
Nope. If you look at the web-devel mailinglist archives you'll see how this stuff developed. And if you look at the darcs history for web-routes you'll see the first commit somewhere in 2008: http://src.seereason.com/web-routes/_darcs/inventory It's worth checking before you attempt to 'correct' people!
web-routes used to be named URLT, http://hackage.haskell.org/package/URLT-0.10 And it was around for a while before it was uploaded to hackage. I have darcs changelogs going back to 2008, and it was around as some blog posts before that. In fact, the URLT stuff is older than Happstack. The original versions were written for HAppS.
&gt; A value of any type is "uniquely characterized" by what happens when you bind a continuation to it, if you really want to push this CT-ish idea. Yes. Except that we are talking about a different bind. You have just repeated my claim but in the specific case of the identity monad. Perhaps if I had written "is uniquely charachterized by what happens when you `&gt;&gt;=` a continuation to it" this would have been clearer? &gt;Overlooking the fact that you've switched the type of x from m a to a, I observe, again, that there's nothing inherently monadic about this typing judgment. It's an undistinguished instance of the type swizzling embodied in let f = flip id in (x `f`) :: forall r. Cont r a. I don't think I switched the types. What are you talking about? &gt; So is State s for all types s. Your point being? In general I know of no monad morphism `m -&gt; State (m r)`. 
I don't want to discourage you from writing tutorials, as the act of writing is a great way of learning. However I do wish the people here on reddit would pause to ask themselves(is this tutorial of sufficient error free quality that it is worth up-voting?) before hitting the little grey arrow. There are multiple errors in this tutorial. But I think that the most egregious is your statement that "et’s unpack this a bit. First, notice that this is embedded in a do block, which always means one thing: it’s time to get our hands dirty with some I/O." This is patently false and highly misleading. do does not mean IO. Do blocks are lists of expressions that are strung together by the compiler with a combination of (&gt;&gt;)'s (&gt;&gt;=)'s and lambdas. do blocks can operate on any monad and not just IO. Indeed, often times do blocks appear deep in so called pure code and some of the most justified uses of do notation do so(for example using do notation with the Parsec monad). What you really want to do, is not look at the sugar(syntax) of a function but it's type.
That is a really minor point. s/always/usually/
It's still a bad way of figuring out if you're doing IO.
The point was not that web-routes did not exist, the point was that the whole idea of doing type-safe routing without TH was a pretty new development at some point when Yesod was already well in development and had committed to TH. If you look at the very repository link you posted the boomerang and zwaluw branches start in mid 2011 only.
&gt; Currently I use symbols in buttons to represent state. Great! Would you make a picture of your running app? &gt; By the way, would you mind explaining how Free monads are used in the library? Free monads are used only to reveal that the monadic value was created by a return or it is more complex. If it was created by a return, the implementation can be more efficient. I could use (Maybe (m a)) instead of (Free m a), but I find it more elegant with Free monads. 
That was obviously not your point. Frankly, I don't know what your _actual_ point is, since I have no idea why the relative timing of these things matters to you. I just remember the original discussions, and remember the evolution of these things, and which ideas were proposed roughly when. Type-safe routing without TH preceded type-safe routing with TH. However, applying bidirectional parsing to the former (to get correct-by-construction parsers/printers without repetition) came somewhat later. But not that much later! zwaluw itself (which isn't just a branch in that repo, but was a different project by different people) was uploaded in 2010. And regardless, the fact that Yesod has made some choices has never stopped it from ripping up some segment of those choices when it thinks there is a better approach. As a framework it iterates notoriously fast! Besides which, stepcut never claimed what you're disputing! Just that A) web-routes predates yesod's type-safe routing (it does). and B) web-routes gives a TH free approach that is still correct by construction using bidirectional parsing (it does). The claim was _not_ made that the TH free approach that is still correct by construction predates yesod. So, again, I have no idea what you're raising a fuss about?
Hmmm... `fac n = product [1..n]` is a hylomorphism because `[1..n]` is equivalent to (or as the computation itself is relevant, no worse than) `unfoldr (\i -&gt; if i &lt;= 10 then Just (i, i+1) else Nothing) 1`, and `product` is obviously a fold. I've seen the term before but didn't remember it - it seems more meaningful now. 
Exactly. You build a structure (the list) based on a "single" value, and then collapse it again to a similarly "single" value.
You're coming down kind of hard on the dude when, at least IMHO, the tutorial is pretty good. 
Does anyone else also find it jarring that GP refers to the casual redditor ("you") as author of said tutorial?
I didn't know you were not the author.
8:50 Noooooooo.
well, you need the bpm to be a certain number or at least pretty darn close. for dnb its about 172 bpm, thats pretty fast. then you have the music theory about keys that need to match up or at least be somewhat similar. its kind of abstract and hard to document but you can hear it when you listen. I just thought it was a decent analogy. not trying to write any books on it ya know
I kind of wish the author touched on ghcjs and fay a little bit in comparison to forml. OT: Does anyone what vim color scheme is that? 
&gt; (builds &amp; tests take a long time) To work around this, Libreoffice does bisect on repositories containing binaries: http://sweetshark.livejournal.com/7683.html https://wiki.documentfoundation.org/QA/HowToBibisect
Thanks for this - very nice! Looking forward to examples of logistic regression and multi-layer perceptron, as mentioned in comments :-)
The color scheme is [Solarized](http://ethanschoonover.com/solarized). The primary difference between Forml and Fay or GHCjs, is that Fay and GHCjs are compilers for Haskell (or a subset thereof), whereby Forml is a new language that shares many features with Haskell, but differs on many others. For example, compared to Haskell, Forml: * Is strict, except where explicitly otherwise. * Has a looser type inference system. * Uses Javascript's underlying datatypes pretty much directly. * Differs syntactically from Haskell, allowing much Ruby-inspired sugar. * Treats testing as a first class language feature. * Has [Tetris](http://texodus.github.io/forml/tetris.html). * yadda yadda many more features to be found [here](http://texodus.github.io/forml/). I'd love to put together a proper comparison (with also Roy and Elm and all comers), as they're all incredible projects. Perhaps nearer to 1.0 
I look forward to trying this out. I am starting to learn programming and hope this will be useful. 
At least the Binary instance for Integer [uses such a function](http://www.haskell.org/ghc/docs/latest/html/libraries/binary-0.5.1.1/src/Data-Binary.html#unroll), but alas it's hidden. The encode function is almost that, but not for small Integers.
Big-endian is a terrible mistake. 
Why would that be?
Royal edict in Lilliput requires cracking open one's soft-boiled egg at the small end. More seriously, the order of bits in bytes/words being irrelevant, this is about the order of long^n words in some data structure. Implementing a big integer library in C, you'd tend to put low-order words at low subscripts of an array. That way, when the integer grows, you don't (usually) have to copy/shift all the words - you just add an extra high-order word at the new highest position after a resize. This logic doesn't seem to translate well to a Haskell list, but I'd be surprised if the implementation for Integer is based on a list anyway. 
Very nice! Minor comment: Conduit example doesn't need `OverloadedStrings`, as it's only using `String`. Is the last example intended to be `active haskell`? Is currently can't be run..
&gt; Is the last example intended to be active haskell? Is currently can't be run.. Oh I see. Stackage doesn't currently include Snap?
Picture: http://i.imgur.com/ApfWA6C.png Sim.hs: http://sprunge.us/YMhF vis.hs: http://sprunge.us/SZDM
The closest thing I found would be using *fromInteger* and making *[Word8]* an instance of *Num*…
So, how is your second example better than the first? It looks like you're writing more code, and again you have to declare every command-line option. I think your idea is intriguing, but it need a little more motivation.
As far as I can tell, the second one allows the order of command-line arguments to differ from the order of fields in the record, since Vinyl record types are considered "the same" (or at least mutually convertible) when they share the same fields, regardless of their order. If you wish to have the order of arguments differ from order of fields in the first solution, you must do one of the following: 1. Use do-notation + return. This is the least fragile solution, but it leaves a bit to be desired. 2. Reorder the arguments of the constructor with an ad-hoc combinator (some kind of flippy-dippy thing). This is extremely fragile and obnoxious.
Cabal could be better but I'm glad it exists. Given the size of the Haskell community I am surprised we have any of these wonderful toys. Thanks a bunch for the script jp -- I am eager to try it and contribute back. Looks fun.
If you're willing to run cabal from head, cabal sandbox-install, cabal sandbox-* is all working. Near as I can tell, the solution exists and you could have it today. That is, if you knew about it (it's hard to find out about) and figured out how to use it (it's hard to get). The problem is, and I've vented this frustration before, no one seems to actually care about usability in the Haskell ecosystem. Experts have no problem blowing away their environment and starting over with their software development, or have already established workflows that accommodate the errors us novices see. The problem is, everyone else is stuck with the utter failure that is Cabal's stock dependency management. I don't know what else to say. The Haskell community is failing its users. If you're reading this and you have commit access to the Cabal repository, turn on the sandbox features, push out a release and write a doc on them. Do it.
Hmm, it seems so, yes. Of course, reverse fixes that.
Probably I will say something unpopular (don't get me wrong, I would love to see cabal's hell fixed): I quite like the sandboxing tool we have and the peace of mind they give me. Especially thanks to hsenv I am free to play with my toy projects installing any package I can without thinking "let's hope this last cabal install won't break the world".
Arrays and dicts in rows? YesSQL!
&gt; So, how is your second example better than the first? Besides the reordering ability which jonsterling mentioned, you also don't need to modify 'program' when *adding* command line options (besides actually using them of course). It's also easier to integrate command line options from other libraries (think test-framework or criterion) when these libraries export their options as a 'Rec rs Term': import Some.Library ( someLibArgs ) import Some.Other.Library ( someOtherLibArgs ) term = program &lt;$&gt; Vinyl.run args where args = myArgs &lt;+&gt; someLibArgs &lt;+&gt; someOtherLibArgs Besides accessing its own options, 'program' can now access the options from both libraries without doing anything special.
Yeah, I'm hoping cabal's new sandbox will help. Figured I'd try to plug up the hole in the meantime. I wrote this while waiting for yesod to compile&amp;mdash; for the 3rd time today.
Just updated the script. It now allows you to play around with which files should be blacklisted. Currently, I'm blacklisting 2 files. Do I need more?
I've been reading complaints about cabal over and over for last few months. What is actually bothering me is that even though comments like these led me expect devils and pitchforks instead of installed packages, *I haven't had any problems with cabal install*. I haven't had to delete ~/.cabal in in ages, I don't use cabal-dev, and I haven't had to reinstall haskell platform. I'm also on os x, which is reputedly worse supported than windows. Then I thought that hey, perhaps this is because I'm not using yesod. So I installed yesod and built and ran a default app which you get with `yesod init`. Still, no broken packages and all my other stuff still seems to build ok. All I'm doing to safeguard me in any way is that I'm using a relatively recent cabal and asking cabal to install all the packages I need with one install command. Also, not installing stuff when cabal says it will break other stuff goes really a long way. Do I need to install windows to get this cabal hell?
even if big endianness might be troublesome to use (which i don't agree with) you will have to live with it, because of network order.
In my experience, it seems like package maintainers are doing an increasingly better job maintaining correct and up-to-date dependency version bounds. The situation seems to be much better than a few years ago, it's been ages since I had to manually tweak a cabal file.
yeSQL?
&gt; I'm also on os x, which is reputedly worse supported than windows. I don't know where you got that idea, but I'm pretty sure that the reputation is that osx is way better supported for haskell programming than windows. For being unix-based and that.
&gt; not installing stuff when cabal says it will break other stuff goes really a long way. See, that's the part that bites. When the stuff I need/want will break other stuff. Still, upgrading does often fix that, too.
upper bound dependencies allow packages to make breaking changes.
&gt; When you are first installing things with a new ghc, install all the big things that you'll be needing right away preferably in one command -- at the moment when things are in 'scratch' condition. Somehow I can't find it in me to be 'appalled' by this reformulated but in fact identical advice. It could be appalling to someone who has no idea what constitutes a "big thing" and knows beforehand what "big things" they will need.
It depedens on your version of OS X. I was on 10.5 and it was terrible. It only had support for outdated Platform versions and Cabal for some reason crashed (I don't even remember the problem now, but it was bad) when I tried to install things.
exactly.
Besides the fact that big endian is easier to read and work with, it's also the standard for network protocols.
My partner had a female friend he used to chat to online, on few occassions they also cybersexed, masturbated on cam but they never met &amp; as time went on we became a couple,heard less of her, nearly 2.5years later I found out that he got back in touch with her last year, they reminised for 4 hrs including inappropriate chats &amp; how he was unhappy being with me, I was deeply shocked when i saw what he had wrote about me aswell as their sexual flirtings for each other. The chats continued, not only flirting but everyday chats, she even gave him contact number &amp; her address .....I had used his phone last month and activated facebook unknown to me it opened his facebook instead, so....I checked his messages, (yes I know I shouldnt have snooped) &amp; there it was then I was deeply shocked to see he had been in touch with her over a year....each time he kept telling her he was unhappy with me.......but at home he is adorable, loveable &amp; we are a family &amp; share a home....I was devastated what i had seen, so I confronted him, I asked him if he had ever flirted with anyone in our relationship &amp; he said no, in the end I told him i had seen messages on his phone, he assured me "that nothing was going on" &amp; that "they were just friends" I feel he had betrayed me, crossed the boundaries in our relationship, chatted to this woman behind my back, he made no attempt to make us as friends nor tell her i am not "just his girlfriend" but his partner &amp; all along I thought he was happy with me, not according to what he wrote to her, but at home he is happy,.....he even told this woman that i have accused him of emotional cheating and she laughed,.....eventually i told him to send me a copy of the chat log if nothing was going on.....yes he sent me but later on I found out he deleted some of the messages....it all seems so surreal, but it hurts.... 
[Woohoo!](https://github.com/metabrainz/musicbrainz-data/commit/f5fb0f4166bbf6aa9ca4fe9a3c52f69e1fe1bd5d) - thanks Leon, I've been waiting for this day :)
Why would I use anything but my operating system’s package manager? Cabal just creates a big mess outside of its control and then can't even do uninstalls. Unless of course you are using Windows… in which case I have to say: Why are you using a consumer OS for professional work?
[Portage](http://en.gentoo-wiki.com/wiki/Portage) has no trouble solving the dependency hell. Even that of Haskell packages. I use it every Friday. (And in newer versions it made haskell-updater unnecessary too.)
The positional number system (Hindu-Arabic) that we use come from languages that read and write right-to-left. They used little-endian notation, but when "we" imported the notation we didn't reverse the order of the digits. So big-endian is a mistake; to original was little endian. I just added the word "terrible" to make people more upset.
Have you actually looked at the activity on the Cabal repo? It *is* being worked on, at an astounding rate. Have patience.
I've been doing that repeatedly for a year and a half, with several projects. Not using any sandboxing. I've still had zero problems. For all the "cabal is broken" cries I see, I've had far more problems with packages in perl, python and ruby.
Are you suggesting that packages should never be allowed to make breaking changes?
Appalling, I know. But this is what (all?) the other-language-ecosystems have settled for.
:) If you'd like to submit a pull request that has the auxiliary functions you found useful, your wish could come true... http://github.com/jonsterling/vinyl
Want to profile? Reinstall packages with profiling and pray they don't break.
Cool; though the UUID type has a static oid, so you don't have to look at the typename. =)
Try installing Yesod and Snap at the same time. I had to do the delete ~/.cabal dance about 3 times on my mac before I could get them to play nice together
Does AD carry with it some kind of performance hit? Are there some common functions it can't handle? I looked into the AD package casually quite a while ago. It seems so powerful that the fact that is isn't used more often makes me kind of wonder what's wrong with it, but maybe it's nothing. I'd love to try it out with various estimation filters.
I may be missing the point somewhat but can't you just construct a single structure which contains all the options you want, rather than passing them bit by bit? data Options = Opt Bool Int String options = Opt &lt;$&gt; value (flag $ optInfo ["verbose"]) &lt;*&gt; value (opt 0 $ optInfo ["amount"]) &lt;*&gt; required (opt Nothing $ optInfo ["message"]) program :: Options -&gt; IO () program o = ...
I use the [Arrow support](http://hackage.haskell.org/packages/archive/optparse-applicative/0.5.2.1/doc/html/Options-Applicative-Arrows.html) in optparse-applicative to deal with reordering... since it should work for any Applicative it is another option for cmdtheline. I really just want Applicative do-notation though.
Nowadays those of us using stackage get notifications if the latest versions of our packages don't work with the latest versions of everyone else's. So as long as you stay current there generally isn't a problem.
Hey cool, didn't know there was a Zurich Haskeller's meetup group. Just signed up for the next meetup about `Lens`.
Thanks a lot!
There was a GSoC project about this last year, but it wasn't successful in the sense that the patches still haven't made it to the Cabal repo. Cabal sandboxes are close to release, on the other hand.
One thing that i would like to be fixed is that cabal-dev and ghc packages do not use absolute paths. This way i could simply copy entire project folder to another computer and continue developing. Right now it is not possible because paths will point to the wrong place. So if you have several developers in the team each one has to install all packages for himself. Waste of time. 
I'd rather not have my packages break spontaneously when libraries I depend on change their API.
No idea. We may be able to help you if you tell us the full error message. What is NSIS anyway? Are you getting this error from the download website, or from your web browser?
That is why the libraries should not be allowed to change their API in a backwards-incompatible way.
What did the Haskeller eat for breakfast? Cocomonads.
What would the downside be of making this progress publicly visible and usable *now*?
Haven't used it - cabal sandbox-* has worked flawlessly since I discovered it.
I prefer [cereal(s)](http://hackage.haskell.org/package/cereal-0.3.5.2), thanks.
That's a ridiculous restriction. It sounds nice in theory, but completely ignores the realities of evolving software projects.
You could but now you need to edit two places to add a new option.
Indeed. It turned out that the problem was on my side, namely an improper use of cURL. This has been fixed in the tutorial.
It's not just ridiculous, it's preposterous. And sounds extremely harsh, not nice at all. But it's the only solution that people have come up with by now, and had to instil in the rest of the industry.
I'm hoping to publish the logistic regression example this week.
Huh? Most languages settled for "make breaking changes and everything will just explode" rather than "make breaking changes and the package installer will tell you it won't work".
&gt; if you’re removing the very point you use it. Not at all, `cabal` solves the most stupendous dependency problems. It would be impossible to install something like yesod without it. But why should it be able to solve everything for yesod -- and then six months later, in a new universe, solve everything for snap in a way consistent with that? Is *that* the point of `cabal install` ?
This level of ignorance is astounding. You have no idea how portage works, and pretend it solves a problem that it simply doesn't even attempt to address. All it is doing is limiting you to a hand picked selection of packages that are known to work together.
I don't get it. Where is the paradox?
It's not "paradoxical" per se. I just find funny how real life colloquialism fits in the scene. And obviously a list is a monad, so the transformer had all the world's reason to make such an invitation :P
&gt; That is why the libraries should not be allowed to change their API in a backwards-incompatible way. Adding: Or when they do they must be upping the major version number (the x in x.y.z). http://semver.org
Yeeesh. You must be a riot at parties.
In most languages you do not make breaking changes because you know that everything will just explode. In Haskell you do, because you are told that the package manager will prevent the disaster. It does prevent the disaster, but also prevents things from working.
What do you mean? The progress is publicly [visible](https://github.com/haskell/cabal). Look [here](https://github.com/haskell/cabal/blob/master/cabal-install/Distribution/Client/Sandbox.hs) for the sandboxing code. If you are asking why the code isn't in the current release of Cabal, it's because it is not read yet. Pushing it out now might cause more problems than it would solve. If you want to be an early adopter and don't mind running into bugs and worts, nothing is stopping you from cloning the github repo and building it yourself.
`my other car is a \(_:xs) -&gt; xs`
Really? I got yesod, snap, and happstack all playing nice on the first try. Maybe the stars were right for me.
Why not just turn profiling on by default?
Yes, that's what I mean, publicly visible. If you think commits only visible to people running cabal out of the HEAD of the repository are "publicly visible" you're woefully unaware of what public means. Your concern over whether it is "ready" or not is misguided. Cabal isn't ready for end users either, what's the harm in pushing out a feature that might actually make it ready? The worst case scenario with the cabal sandbox is that they have to delete a dot-folder in their tree. The worst case scenario for users using Cabal for development as is is that they have to manually remove files in several locations, none of which are widely documented, and redownload all of the packages that were working up to the point of failure, and navigate the minefield that is broken dependencies over and over again until they have something that works. Alternatively, if cabal sandbox-configure/build/install works for them, you'd have solved the problem for the vast majority of developers.
Certain projects with widely-spread dependencies are going to hit the issue more often than others. I used to hit cabal hell on a weekly basis while developing. Your mileage may vary.
I don't get it :-( 
What bugs me the most about cabal is that often the versions of ghc libraries in Fedora's repos aren't fresh enough for some packages to build. And that's a problem with Fedora's package maintainers, not cabal. Although, yeah dependencies management would be.
His/her other car is a cdr. IIRC, for wacky old assembly language reasons, the first slot of a cons cell was called the car and the second the cdr.
car = current address register. cdr = current data register. edit: actually, a further google reveals a slightly more twisted story. &gt; Because of an unfortunate temporary lapse of inspiration, we couldn't think of any other names for the 2 pointers in a list node than "address" and "decrement", so we called the functions CAR for "Contents of Address of Register" and CDR for "Contents of Decrement of Register". (src: http://www.iwriteiam.nl/HaCAR_CDR.html)
I'm getting the error after running the installer. The error message reads: Installer integrity check as failed. Common causes include incomplete download and damaged media. Contact the installer's author to obtain a new copy.
I tried that, with no visible results.
On a very old IBM 704...
&gt; why should it be able to solve everything [...]? Because apt-get can do it. Because rpm can do it. Because emerge can do it. And because none of those can download straight from hackage.
This same problem exists for just about every C binary out there. And since Haskell currently relies on the same ecosystem (exe format, linker, etc) that'd be... hard.
Why reject the lift? Does the list have something against being lazy? It's being way too strict on itself. 
How about? import Data.Word8 import Data.Digits unroll :: Integral i =&gt; i -&gt; [Word8] unroll = map fromIntegral . digits 16
will it be included in next haskell platform?
Noob here. So what if its sorted?
In Haskell, the PVP states that the y should be increased for breaking changes.
they may be bad but they work...
Upper bound is not the problem in itself, like cabal is not the problem in itself. The problem is the culture of easily changing the API and hoping that some cabal magic (are "cabal" and "magic" basically the same thing? I guess they are.), will solve any incompatibilities. If you use packages A and B, both of them using package X, but A was last updated before X made a backward-incompatible change in its API, and B was updated after that, no amount of clever automation may save you. If the package X changed its name to X2, then there would be no problem: B can use X2 if it needs the new features, and not prevent A from using the "old" X.
A dependently-typed monad walks into a proof. QED.
Sure thing! Now I've got both `yesod init` webapp and `snap init` toy webapps running. Did you use old cabal-install? That would have certainly broken things by now. 
So what packages did you need that caused that? 
http://www.reddit.com/r/haskell/comments/137156/this_is_unacceptable_cabal/ http://www.reddit.com/r/haskell/comments/lg2jv/status_of_haskell_platform_cabalhackage/ Basically, this is a continual problem. It's a coin flip (or a dice roll, whatever) every time I install a new package in order to do something new. When I want to upgrade a fast-moving target package like, say, Conduit, it's almost certain to blow up. The ways it blows up are not impossible to resolve - there is usually some way to fix it, sometimes by just blowing away all of my Cabal data in its entirety - but they impede development and make the process far more frustrating than necessary. They also rely on implicit knowledge that is not documented well, anywhere, and only someone who has frequently had the problem will be able to resolve it without consulting source material, forums, IRC. Frankly, the state of development for newbies in Haskell is atrocious, and people involved in maintaining this state should know this and feel bad. The experiences those people have with Haskell is worlds apart from that of a newbie, and their rose-tinted glasses prevent them from seeing the tremendous difficulty that a dependency problem in Cabal presents to new users. I would not be surprised if half or more of all projects started by new users using Haskell are aborted due to dependency problems.
Does the sha1 checksum of the installer match the checksum posted on the website? If not, try redownloading the installer. Otherwise, repost your question (as lack of information led to it being downvoted into oblivion first time around). Include: * all information you've posted here * name and version of your OS * version of the Haskell Platform you're trying to install * link to the NSIS help page you used, so we know what you've tried from there * any other information you feel may be relevant; the point is to make it easy for people to help you Use a more informative title (e.g. *“Installer integrity check” error when trying to install the Haskell Platform on Windows Vista*, if you're using that version of Windows).
The lifting is cooked a little differently but [this](http://www.reddit.com/r/dependent_types/comments/18yb3k/deriving_induction_from_iteration_in_agda/) is not entirely unrelated. Another Strathclydey way of putting it is that the lifted functor is the algebraic ornament of the original by its initial algebra.
I'm not saying that you didn't have that problem, but I've now installed cabal-dev-0.9.2 on my machine without anything breaking or needing any vodou. Also, where did you get this "half new projects die" thing? Measuring by two Haskell courses I've taught during last year, I've seen perhaps two or three cabal related problems. With 40-50 students in total I'd expect that there should also be some dozen or so who'd try to do *some* hobby projects on the side and yell at me if cabal was actually that bad. Also, couldn't you make things a little bit better by yourself and document some of that implicit knowledge somewhere?
A spacesuit, a burrito, and a nuclear waste container walk into a bar, and then realise they're trapped.
The fact is, I've never been able to solve any of these problems I've encountered deterministically - only by following the advice of IRC communications or searching the Haskell wiki and trying things until it works. I honestly don't bother anymore, I don't know how ghc-pkg works, the way it is documented is **fucking awful**, I don't know how the cabal repository works, and it shouldn't be the end-user's job to conjure up this documentation. My solution for Cabal problems has been to wander aimlessly until they go away. Maybe the reason your students don't do hobby projects is because it's a pain in the arse, or maybe their projects are simple enough or over a short enough time span that they didn't run into Cabal dependency hell. I don't know - I ran into it regularly. I still do, developing against Yesod, but I use cabal sandbox-* commands and every time it breaks I just delete the sandbox directory and start over - which fixes it. This is the top hit on Google for ghc-pkg. Read it, spend some time re-reading it, and try to grok the frustration end-users might have: http://www.haskell.org/haskellwiki/Ghc-pkg Here's the "official documentation", aka, a list of command line options: http://www.haskell.org/ghc/docs/6.12.2/html/users_guide/packages.html#package-management It's an old link to the docs, which is nifty, but even the latest link to the package management docs is woeful: http://www.haskell.org/ghc/docs/7.6.3/html/users_guide/packages.html#package-management There's precisely no information on what to do if something goes wrong. The official Cabal FAQ has this suggestion for dependency conflicts: &gt; To avoid this problem in the future, avoid upgrading core packages. The latest version of cabal-install has disabled the upgrade command to make it a bit harder for people to break their systems in this way. Oh, just don't upgrade things. Got it. Don't bother upgrading Yesod to 1.1. Don't upgrade Conduit to 1.0. It's a problem, so just don't try.
So, the documentation is 'fucking awful'. Which part of it? How is it awful? What should be written instead? Also, people know about the difficulties with cabal. They are also doing lot towards fixing things, and I've seen huge improvements over last year or so. From the perspective of us end users, these people are doing it free of charge. I don't thing this entitles us swear at them for not providing better things. Did you file an issue when you encountered bad documentation or when something broke? How was it handled?
Why can't IO run? His mother told him it's unsafe.
Replying to your edit: So the wiki page just gives the basic usage and points to the old documentation. I can fix that when I get an account. So could you. That is the whole point about wikis. I myself found the ghc-pkg documentation to be quite alright, even if it is a bit dense. I've only used ghc-pkg for listing packages and unregistering some, which is easy to find in the documentation. Can you offer a more concrete complaint? Your complaint "It has no information on what to do when something goes wrong." is correct, but to write that, the writer should know what that "something" is. Again can you give a concrete case? If you can, and I happen to know how to fix that, I'll see if I can get it pushed to official documentation. Also, avoid upgrading core packages does not mean to avoid upgrading any packages. It only means to avoid upgrading/downgrading packages shipping as part of ghc, which was a source of huge problems in the past. I'm kinda just saying that 'frothing at the mouth' is not helping. If you want something to be better, you need to get involved in it. EDIT: I made the wiki page point to the latest documentation (should this be the latest ghc, or the latest shipped with HP?) and added some more information there. 
The fact we are commenting, replying and smirking reading this thread makes me realise how geeky I am :P I won't say "geeky you are" because I don't want to existential qualify you as well, unless we want to use a pragma, of course.
What. If never being able to fix terrible decisions and existing mistakes after something has been released is your definition of "works" I hope you stay out of the software development industry. Preserving backcompat rather than fixing fundamental mistakes is up there with stagnant adoption of unacceptably bad tools in terms of awful ideas that drag the entire industry down. Getting away from that insanity is a large part of why I'd rather do practical development in Haskell, all else equal.
&gt; Using a portion of the package version number as the indicator of the API version is possible, but it would require a change in the language. This is exactly what the standard [package versioning policy](http://www.haskell.org/haskellwiki/Package_versioning_policy) already requires, and most major packages follow those guidelines. &gt; The "import" directive must specify the package*API combination that will not change in backwards-incompatible manner. [GHC package-qualified imports](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#package-imports): &gt; With the `-XPackageImports` flag, GHC allows import declarations to be qualified by the package name that the module is intended to be imported from. For example: &gt; `import "network" Network.Socket` &gt; &gt; would import the module `Network.Socket` from the package `network` (any version). This may be used to disambiguate an import when the same module is available from multiple packages, or is present in both the current package being built and an external package. &gt; The special package name `this` can be used to refer to the current package being built. &gt; Note: you probably don't need to use this feature, it was added mainly so that we can build backwards-compatible versions of packages when APIs change. It can lead to fragile dependencies in the common case: modules occasionally move from one package to another, rendering any package-qualified imports broken. Note that you can also specify a package version there, i.e. `import "foo-1.2.3" Data.Foo.Bar` In practice this is rarely necessary because the `cabal` file specifies the exact dependencies and the cabal build process hides other packages. &gt; It must be impossible for the caller to accidentally use the package with changed API. And impossible for the caller to accidentally use the package with a trivially-changed API that would actually work just fine if the version bounds weren't unnecessarily narrow, thus contributing to the exact dependency management problem that people complain about so much. In practice, at least half of the dependency issues I've encountered have been caused by unnecessary version restrictions, rather than conflicts between truly necessary restrictions. &gt; Just changing the name of the package from "SuperApplicativeLenses" to "SuperApplicativeLenses2" at the same moment as you change the API has the same result, and does not require any changes in the language or infrastructure. It looks ugly, but maybe it's a good thing, an additional hint to the maintainer that changing the API is something that should not be taken lightly. No, that's a stupid idea.
Here you go: http://idontgetoutmuch.wordpress.com/2013/04/30/logistic-regression-and-automated-differentiation-3
Besides this implementation using manual recursion, and using reverse for the normal case, this library looks promising.
Why did Leibniz run his computations in the List monad? He wanted to see the rest of all possible worlds. (Sorry, the best I could do …)
You aren't hitting cabal hell, you are hitting a simple dependency mismatch. This is exactly what I mean. There's a small group of ignorant and loud people who insist cabal is broken because they have occasionally hit trivial issues that are present in every language's package management system.
&gt;In most languages you do not make breaking changes because you know that everything will just explode. Name one. I have never seen such a language. How on earth do you think libraries in other languages evolve if they can't make any changes?
None of those do it. Go spend a year working on packaging using one of those systems so you actually understand what they do and how they work. Then come back and talk about how they solved all these problems that they actually don't even attempt to address at all.
For some reason other language's package management systems seem to support upgrading packages and switching dependencies' versions to support it.
&gt; you can also specify a package version there, i.e. import "foo-1.2.3" Data.Foo.Bar but this is *not* what is needed. The version and even the package name may change, while the API may stay the same. &gt; And impossible for the caller to accidentally use the package with a trivially-changed API that would actually work just fine In my book, "trivially changed API that would actually work just fine" is an API that has changed in a backwards-compatible manner. No renaming needed, everybody carry on. &gt; the version bounds [being] unnecessarily narrow is what I am talking about. The notion of the upper version boundary does not correspond well enough with the notion of the backward-incompatible API changes. &gt;&gt; Just changing the name of the package &gt; No, that's a stupid idea. * It guarantees that the API version is (in a way) specified inside the code of the caller. * It guarantees that if your first-level dependencies are in turn dependent on a common package, but on different API versions of it, these different versions can coexist without causing any confusion. There are the two key requirements. If caller is unable to *not* specify the API version, and packages with different API versions can be installed in parallel, the dependency hell will not ensue. Encoding the API version in the name of the package is just the easiest way to enforce these rules. Other ways exist, they are just more complex, and require cooperation from the language. I am happy to agree that the idea is stupid. But I know that it works, while the clever interval dependency does not.
Well, for one thing, read another poster's thread here where he found the best way to maintain a sane workflow involves a great deal of ghc-pkg voodoo and knowing the right order to unregister things before upgrading a package. That is literally the first time I've seen anyone say they can successfully do that without blowing away their entire repository, and it's only documented here on Reddit, and I'm not confident enough to say I could tell someone else how to do it. So why, again, should I be editing the Wiki with such advice?
Who do I file the issue with?
Thanks for pointing that out.. So PVP is Haskell's semver.
The main thing I'd like to see is for `ghc-pkg unregister` to delete the files for the unregistered library. I set up stable package sets and use cabal-dev now, but in the past when I'd fix `ghc-pkg check` issues manually I'd end up with a bunch of old library code just hanging around.
&gt; In practice, at least half of the dependency issues I've encountered have been caused by unnecessary version restrictions, rather than conflicts between truly necessary restrictions. ...which is perfectly understandable since the percentage of API symbols that change in the average major version bump is probably correspondingly low. I think our best shot at resolving this issue is to track the intersection of API surface area that is changed and what is actually being used.
Why you? Because you care about it and it ain't going to get fixed by offending people. It is much more easy to get things fixed by getting involved in the process of fixing them. Now, in converse, who do you think should do it? Regardless, I think I see your problem better now. Since `ghc-pkg unregister &lt;thing-that-cabal-complained-about&gt;` is pretty much self documenting, (though often arduous) process, I think what is lacking is a concise documentation on what is going on in the general level and what tools are involved. What do you think this documentation should contain, and where should it be placed so that it is most useful?
NuGet with .NET languages, CPAN I don't think has had the same issues, Node.js' NPM seems to automagically resolve dependency conflicts. The fact that human beings who have a reliable workflow for working with Cabal have some process for resolving these conflicts suggests there might be an *algorithm* that could be implemented to solve their problem. I am not aware of what this algorithm is, I fumble around and follow instructions I find or I just delete things and reinstall Cabal to fix it. But other people, including someone in this thread, seems to have come across a solution. In fact, this post on Reddit brings light to said issue and suggests a programmatic solution to it. The thing is, this is *utter insanity*, the problem is staring the people who *know and understand Cabal* in the face, and they don't fix it because it isn't a problem for them. The people who complain don't know how to fix it, and are lambasted for complaining without contributing. You know what the node.js npm philosophy is? It should just work. Edit: I would be remiss not to mention **the single most successful and widely used dependency management system in the world**: apt. When was the last time apt blew up on you? Shit, I can do "apt-get dist-upgrade" to completely change my operating system and on a variety of systems It Just Works. It does not blow up, it doesn't admonish me for believing I have deigned myself good enough to upgrade my system, and it very, very, very rarely requires end-users to manually manipulate files. The last time I had a problem with Ubuntu was in the previous decade, and it was a dist-upgrade on a machine that had proprietary ATI drivers installed as blobs. It wasn't actually a problem with APT, though, it was a binary blob that I wedged into the system and naturally it couldn't upgrade it.
&gt;CPAN I don't think has had the same issues, Node.js' NPM seems to automagically resolve dependency conflicts. Both CPAN and NPM absolutely have the exact same issues. You are pretending cabal issues are unique simply because cabal shows you the error rather than having your code explode after its already too late. &gt;Edit: I would be remiss not to mention the single most successful and widely used dependency management system in the world: apt. No, you wouldn't. You would be ignorant if you did mention it. As has been repeatedly stated elsewhere in this thread, apt and friends do not even make an attempt at solving this problem, at all. They simply restrict you to a hand picked set of packages, versions and dependencies that are known to work together. You are being incredibly insulting to the people who spend thousands of hours doing that work for you when you pretend apt magically does it automatically. &gt;When was the last time apt blew up on you? 12 minutes and 36 seconds ago according to my shell history.
I've been working on designing a packaging system for another language, and after surveying all the different kind of design choices (and problems that people have with the various systems), I've come to the conclusion that there's one super massive feature that is required of a package manager, that Cabal lacks: Package managers **must** tolerate multiple installed versions of the same library. Most of the problems people have are totally incidental: they're not trying to use two different versions of a library in a single program (down that rabbit hole lies osgi), they just want to work with two separate programs that are written against different versions. RubyGems are a damn fine model for a packaging system. I've seen ruby devs sit down to a project that hadn't been touched in years, and boom, all dependencies installed and working, no problem. Right next to any number of other projects. It's slick. Anyway, I thought I'd offer my 2 cents here, since I see a lot of people blaming version constraints, and that's rarely the true issue, as far as I can tell. It's just the symptom.
&gt;At the very least, the solution has never been "Delete things until NPM is dead, reinstall NPM, and reinstall packages". Ditto with CPAN That is not the advice with cabal either. All 3 run into the same problems, with the same advice offered as solutions. Blowing away your installed packages does not require blowing away the package manager as well. Again, apt has absolutely nothing to do with the topic. &gt;As far as APT working I already told you why it works, and did so as one of the people you insult by pretending it does it automagically. We spend hours making sure each and every package has precise dependencies defined so that installing anything will work. We are not infallible, and we frequently deal with reports of packages that do not install correctly in various combinations. &gt;The current state of Cabal is obscenely broken and people complain about it frequently and you're insisting that it's not the case. *You* complain about it frequently. And you demonstrate a profound ignorance every time you do so. I do not believe this is a co-incidence.
Yes, I found that funny the first time too. Especially when it tells you it's going to break packages you installed five minutes ago.
That's what maven does, and whatever its faults, it works pretty well in this regard.
Yes please! I love darcs.
`callMeMaybe`?
Hoogle says [`catMaybes`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Maybe.html#v:catMaybes) is what you're looking for, in `Data.Maybe`.
I prefer f (Just x) xs = x:xs f Nothing = [] takeWhileJust = foldr f [] but then I like recursion patterns.
Unlikely. You can write it like this too: \list -&gt; [ x | (Just x) &lt;- takeWhile isJust list] Or less obviously: unfoldr (\l -&gt; case l of (Just x):xs -&gt; Just (x, xs) ; _ -&gt; Nothing) Or perhaps even: \list -&gt; [ x | (Just x) &lt;- concat $ take 1 $ groupBy (curry ((isJust *** isJust) &gt;&gt;&gt; (arr $ uncurry (&amp;&amp;)))) list] You could also implement: takeWhileJust' :: (a -&gt; Maybe b) -&gt; [a] -&gt; [b] And then just do, takeWhileJust = takeWhileJust' id
`last . concatMap sequence . inits . map maybeToList` (obviously :-P)
libc? *any* other? Not "any changes", only "backward incompatible changes". Surprise, surprise, in a few cases when backward incompatible changes happened the library was renamed.
I don't get it.
catMaybes returns all the Just values in the list. He wants to stop when the first Nothing is reached.
Missing an _ ?
Darcs is a really cool and unique experience compared to git, but I fear that the tipping point has past, with a large number of Haskell projects having gone to git through GitHub.
But ghc/cabal does let you have multiple installed versions of the same library! It just doesn't let you have multiple installations of the _same_ version of the _same_ library whose _only_ difference is that they're linked against _different_ versions of _other_ libraries. But yeah it should let you do that too!
&gt; Someone who understands ghc-pkg and Cabal. I am not that person, I have said it over and over yep, you sure have.
Thank you for being Candide with us.
 f ((Just x):xs) = x:xs that should do the trick
Anpheus, I'm saying this out loud now: it is your attitude that is not helping here. People **are** working on cabal. During this discussion there has been some 20 or so commits to it. So stop saying that no one is doing anything. Cabal is **not** horridly broken. Yes, you have lot of difficulties with it, but many people don't have that many. Are there problems that need to be fixed? Yes, but in the mean time, perhaps you could learn from people that don't have problems. I've started [a small and transient survival guide](http://www.haskell.org/haskellwiki/Cabal/Survival) telling what I do with the the current state of cabal and how I have solved problems in the past. Hopefully other people join in and actually make this a proper resource for beginners. At this point, I think you should either shut up, or start contributing. If you can't solve technical problems, atleast point out which parts need work.
The Department of Computer Science and Engineering (CSE) at the University of Gothenburg and Chalmers University of Technology announces two PhD positions within the research project Reliable Multilingual Digital Communication: Methods and Applications (REMU) funded by the Swedish Research Council (Vetenskapsrådet). The CSE Department provides a strong, international, and dynamic research environment with about 70 faculty and 70 PhD students from about 30 countries. The PhD positions are located jointly in two research groups: Language Technology and Formal Methods. The PhD supervisors will be professors from these groups: Aarne Ranta (REMU principal scientist), Koen Claessen, and Gerardo Schneider. **We welcome applicants with strong functional programming skills in particular!**
I use catMaybes all the time, but it doesn't do this :)
I use git because of github. If darcs explained how to host a darcs project on git(hub) through a compatibility layer much like I started using git with git-svn then it could still increase its usage.
Yes, thanks! It should have been `f Nothing _ = []`.
If desired this can be golfed down to takeWhileJust = foldr (maybe (const []) (:)) []
On the other hand, a lack of backwards compatibility in the Haskell infrastructure/ecosystem is the main reason that I still don't deploy Haskell code at work. I do prefer to code in Haskell, and for projects where I can be pretty sure the software will be needed for less than 6 months I use it where I can. But for anything long lived, I want to leave instructions on how to build and install which can be expected to work when followed by a typical Sys Admin on a typical linux box of a few years time. In theory Haskell does this well with a proper language spec, multiple conforming implementations and all that, but, in practice, real world libraries use many GHC extensions and can't be relied upon to maintain compatibility over time. That's not to say many infrastructure things haven't improved greatly over the years I've been using Haskell: a few years ago it would have been lack of available libraries and the fear of being unable to find other programmers to continue development which would have been the limiting factor; that's no longer the case. But when I come back to code I wrote 3 or 4 years ago and try to compile on the latest Haskell platform, there are almost always things I need to update to get it to work. 
You are trolling right? There's dozens of libraries called "libc", and every single one has had many breaking changes. The library is not renamed, the version number is bumped. Just like haskell libraries.
&gt;I suspect you're merely ignorant of other people complaining of the same issue, No, you are ignorant of how many people are complaining. It is a very small number, and they all share something important in common with you: ignorance. They heard "cabal is broken" from someone clueless, and then blame anything and everything on "cabal is broken" from then on. Just like you. &gt;Despite all of this, I'm not surprised you are ignorant about the problem of Cabal. I am not ignorant of the *problems* with cabal. I am also not ignorant of the exact same problems being present in every other package management tool. The fact that the people working on cabal are trying to solve very difficult problems does not mean "cabal is broken". The fact that you do not realize the same problems affect cpan, npm, etc does not mean that "cabal is broken".
Thanks for your thoughts on this! Projects should definitely use whatever DVCS ecosystem helps them advance. I also use Git/GitHub quite a lot (especially these days). But Darcs still has lots of interesting problems to work on, for example the git-svn style thing that eegreg mentioned (I do some of my GitHub stuff via Darcs, but this is finger-slicing stuff which we don't really want to inflict on people yet). I think however much marketshare Darcs may have now or in the future, our work is worth doing. Even if it results in research that Git people can look back on in the future and say “oh, that's how you should/should not do it”. Even if it's just that. The work is there to make Darcs better now, but in making Darcs better, hopefully expand our boundaries on how version control might work in the future. Oh I should also add that the reserved GSoC slots were specifically added on as *extra* for Darcs. That is, Google had a set number of Haskell slots and then graciously factored in additional one for Darcs. We're very grateful for this opportunity.
Yup. It's like Apple in the late 90s. Their already tiny market share dropped from 10% down to 3% and they were never able to recover. The rest is history!
1. You used putStrLn $ removeComments css ==&gt; removeNewLines ==&gt; condenseWhitespace ==&gt; removeUnnecessarySemicolons ==&gt; condenseSemicolons Why not `(.)` (from the `Prelude` or `Data.Function`; note that you'll have to reverse the order of the functions) or `(&gt;&gt;&gt;)` (from `Control.Category`)? putStrLn $ removeComments &gt;&gt;&gt; removeNewLines &gt;&gt;&gt; condenseWhitespace &gt;&gt;&gt; removeUnnecessarySemicolons &gt;&gt;&gt; condenseSemicolons $ css 2. `removeComments` doesn't deal with strings in CSS. Oops. 3. `removeComments` should use an ad-hoc data type instead of marker strings (`"*/"`, `"\n"`). data WhereAmI = Code | InLineComment | InBlockComment 4. I personally don't like the explicit recursion in `removeComments`, but that's perhaps a little harder to deal with.
So one thing you can do to easily get the first command line argument is: file:_ &lt;- getArgs Of course, that will throw an exception if the command line is empty. If you want to be safe, you can do: args &lt;- getArgs case args of [] -&gt; ... -- spit out usage info file:_ -&gt; ... -- process file Or even better: use something like `cmdTheLine` or `optparse-applicative` to get command line arguments "professionally". Instead of `==&gt;`, you can use `&amp;`, which is the more common name for this function. Alternatively, you can use flipped function composition using `&gt;&gt;&gt;` from Control.Category: putStrLn $ removeComments &gt;&gt;&gt; removeNewLines &gt;&gt;&gt; ... &gt;&gt;&gt; condenseSemicolons $ css You can speed up `removeComments` a lot by using `attoparsec`, and then you can also write higher-level code like: removeLineComment = do string "//" skipWhile (not . isEndOfLine) endOfLine ... condense = removeLineComment &lt;|&gt; removeBlockComment &lt;|&gt; skipCode &lt;|&gt; endOfInput You could perhaps even combine all the passes into a single pass if you did it that way although I haven't completely thought that out.
I kind of like your `(==&gt;)` operator (did you find it somewhere or just make it up?), but I'm curious what the everyday/professional Haskellers think about it. That is, I wonder if it would be more usual/normal/readable to simply use the familiar function application and composition operators. For example, instead of putStrLn $ removeComments css ==&gt; removeNewLines ==&gt; condenseWhitespace ==&gt; removeUnnecessarySemicolons ==&gt; condenseSemicolons do putStrLn $ minCss css where minCss = condenseSemicolons . removeUnnecessarySemicolons . condenseWhitespace . removeNewLines . removeComments I guess my reservation about `(==&gt;)` is that "feeding" operators like `(&gt;&gt;=)` usually operate on monadic values, whereas your `(==&gt;)` doesn't.
My "==&gt;" was an attempt to use something like &gt;&gt;= without monadic values, that's why I created it.
Your concern about open imports is the same I have in Python. I'll fix it. What is, exacly, "shadowing of tail"?
If you hadn't already, note my edit above. You named your pattern-matched variable "tail", which shadows the function "tail" from the Prelude. -Wall won't like it, and it can be a little confusing, especially if the scope was larger.
Take the function: f (h:tail) xs = tail xs -- doesn't work! The function `tail` in the Prelude is no longer accessible (without using `Prelude.tail`) because the pattern `tail` is hiding it. The lesson being, don't use standard names for nonstandard things.
http://ircbrowse.net/browse/haskell?events_page=11&amp;q=cabal Please. Just read the thousands (!) of pages of references to cabal and note that on nearly every page there is at least one complaint about Cabal, a question about it doing something unexpected, or a concern that something went wrong and they don't know what to do. If you think this is not an issue, you have issues with what you consider issues. This person never came back: http://ircbrowse.net/browse/haskell?q=ipetrovilya Read that. Read it. Read it again. If it doesn't make you angry that no one could answer their question except to say "installing into your home directory is safer" and "perhaps you should try cabal-dev", something is wrong with you. If it doesn't *bother you* that this happens on a regular basis, that people go to the IRC channel, are confounded by Cabal, and leave without an answer that actually solves their immediate problem, something is wrong with you.
If you take the `&gt;&gt;=` type: Prelude&gt; :t (&gt;&gt;=) (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b And strip out the Monad bits, you get: ? :: a -&gt; (a -&gt; b) -&gt; b Which turns out to just be function application, albeit flipped from its usual order. Prelude&gt; :t flip ($) flip ($) :: b -&gt; (b -&gt; c) -&gt; c At this point you might as well just use normal function composition, as seepeeyou suggests. It's idiomatic to understand it as a sort of pipeline. Incidentally, since we're in the area, the monadic equivalent of `.` is in Control.Monad: Prelude&gt; :t (.) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c Prelude&gt; :m + Control.Monad Prelude Control.Monad&gt; :t (&lt;=&lt;) (&lt;=&lt;) :: Monad m =&gt; (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; a -&gt; m c Although I personally find myself using `&gt;=&gt;` more often. `&gt;=&gt;` is in my personal set of "functions I reimplemented before [Hoogling](http://www.haskell.org/hoogle/) them and discovering they already existed in a standard module". I think everybody ends up with such a set, so don't worry about it.
http://ircbrowse.net/browse/haskell?q=ipetrovilya Read this sad, sad tale of woe that was easily found on the IRC channel (there was a complaint about cabal on every day of the IRC logs for the past month as near as I could tell, albeit this is just sample size = 1). Read it, it's sad. This person ran into an issue, no one told them how to fix it except to do it right next time and perhaps try a wrapper around cabal that fixes how fucked up it is. The fact that **cabal-dev** even exists should appall the maintainers of cabal. If the software is so broken for end users that one of the first recommendations is to install a wrapper that tries to make it behave sanely, there is a problem, and it doesn't lie in my complaints.
Yes, something like that. It's a common problem when parsing or tokenizing code. I think it's pretty rare in CSS (perhaps not so much with `content` attributes or `url()`s), but it's something you want to keep in mind. I removed the reference to `scanr`, because I couldn't really figure out how to use it elegantly in this case, but it's the most similar type of recursion.
And since arrows form a category (with `(&gt;&gt;&gt;)` and `(&lt;&lt;&lt;)` as composition operators: `(&gt;&gt;&gt;) = flip (.)`, `(&lt;&lt;&lt;) = (.)`), it's also from basic category theory ;)
Oh hell, thanks for correcting me. Apparently I wasn't paying close enough attention to the details. That's a more subtle problem.
My understanding is that the official Haskell language specification was not revised at all from 1998 until 2010, although work was going on that entire time. However, GHC and other interpreters and compilers were implementing new features (including some under discussion by the language committee) as language pragmas throughout the years, in advance of the revision's release. So this seems to me like a major change in the workings of Haskell the language, even though it might not necessarily much affect programming practice. I'm a bit under the weather so that may not have made sense, and I am not very knowledgeable about Haskell in any case, so my facts and/or interpretation may be wildly off-base. Can someone with more knowledge speak on the issue?
Haskell was originally designed -- by committee -- to be a language that people could use as a baseline for studying language extensions. The original goal of the language was to have a common lingua franca you could talk about your novel language extension against without having to start all over with the basic syntax/semantics. Having various implementations going nuts with custom pragmas, etc. is very much in the spirit of the original language goals. There was a committee formed to build Haskell 2010 nee Haskell' to start moving the language standard forward. Haskell' was already going on back clear in 2006 when I joined the community, but it originally had a much broader scope with tons of moving parts. That scope turned out to be so daunting that they eventually scaled it down to just cherry-picking the non-controversial extensions that everyone could agree on, and which wouldn't drown the committee in ink to document. The Haskell' process has since pretty much followed this model. A minor tweak here, a refinement there. That isn't to say that eventually it won't try to standardize something bigger, but just describe how it has worked to date. IIRC, the committee disbanded last year because there wasn't really anything proposed to go into it that was worth adding at the time, so this is more of a return to the status quo we've had for the last several years.
Just a suggestion about packages (this shouldn't require you to change any code): I would use [`regex-compat-tdfa`](http://hackage.haskell.org/package/regex-compat-tdfa) instead of `regex-compat`; see the linked page for why.
*Family of five died in tragic fire* is a sad tale of woe. That someone didn't manage to install Hakyll is, at most, a bit annoying anecdote. At the log you point me to, I see two unrelated people trying to help a third. Who is failing to install stuff because the _linker_ runs out of memory on his tiny netbook. Not something I'd use for calling cabal fucked up. (Also, see the above guide. I think that could have actually fixed his other problem.) But enough of this. I don't know who you are trying to convince that cabal is bad, or why. Personally, I think you are just trolling.
My other take 1 is a drop 1.
All of the other commenters have been spot-on; my only contribution to add would be to use Attoparsec - a little daunting at first but worth the learning curve and *very cool*. You might also be interested in looking at the source for the "Clay" project (a CSS pre-processor in Haskell, I think it has a function for outputting minified CSS).
I think that was `Tekmo` ... he, anyway, was describing what I think of as standard practice. `ghc-pkg` doesn't involve voodoo; it is much more important to understand it than to understand `cabal install`. It isn't an external tool but an integrated part of the compiler system. As aleator says, it is pretty self-explanatory even if you just look at the material under `--help`. It has a lot of cool features like `find-module` you will use all the time. I think once you master `ghc-pkg` (not that I know all that much!) your comprehension of whats up with `cabal install` disasters will massively improve. One thing this discussion suggests is that maybe there should be more explicit discussion of `ghc-pkg` in tutorial literature. That you think of it as voodoo does suggest that there really is a massive communication failure of the kind you are bemoaning. Of course it could do with more features too -- e.g. to remove the compiled libraries -- and maybe `cabal` could do with some feature to regress to an offending library and reinstall everything and so forth so one doesn't have to reason things out for yourself. I have seen bash scripts around that do things like that for `ghc-pkg` and `cabal`.
Thanks for the history! I found it very interesting and helpful, especially coming from such a prominent member of the community.
As I recall, there have been a few "patches" that don't change semantics, but clarify where something is imprecise, or incoherent.
I am not using cabal-dev. Haven't been for atleast a year. I don't think it is needed.
You may not need it, but it is surprisingly frequently recommended: http://ircbrowse.net/browse/haskell?q=cabal-dev And so is hsenv: http://ircbrowse.net/browse/haskell?q=hsenv http://ircbrowse.net/browse/haskell?q=virthualenv And other authors have tried to fix Cabal in other ways: * https://github.com/snoyberg/cabal-nirvana * https://github.com/yesodweb/cabal-meta * https://github.com/iquiw/cabal-delete * https://github.com/kazu-yamamoto/cab * https://github.com/plancalculus/cabal-uninstall * https://github.com/nushio3/cabal-reset * https://github.com/yesodweb/cabal-src * https://github.com/peti/jailbreak-cabal * https://github.com/akaspin/cabal-consistent * https://github.com/db81/hellno * https://github.com/philopon/hbrew * https://github.com/jpaugh/cabaret It's not that Cabal is broken, it's just not broken for your use case. Plenty of other people have, quite obviously, decided fixing Cabal is a problem that they had to undertake themselves.
Being used to functional programming, reading from right to left actually makes more sense. Just replacing it with putStrLn . condenseSemicolons . removeUnnecessarySemicolons . condenseWhitespace . removeNewLines $ removeComments css makes sense. This is simple, idiomatic Haskell and makes it more readable, in my opinion. His version makes sense, but it reads more imperatively to me. When I used to program a lot in Groovy, I'd chain functions in this manner all the time. It's not wrong, but I would definitely suggest that the OP try using `(.)` more often to get used to applying from the right to the left, which is more functional style. Great first script, though. Congrats on getting something done and working. :)
Awesome
Can you clarify? I don't understand your comment.
`NPlusKPatterns` have a special place in my heart. I first tried Haskell a long time ago, but I dismissed it based on basically what every one else does. Years later, I came back out of curiosity and now I'm completely in love. However, one of the things I learned the first time around was `NPlusKPatterns`. I promptly forgot about them, and when I relearned the second time I never encountered them. One night while reading about Haskell, I saw `NPlusKPatterns` flash by my vision and disappear quickly. After some time, I figured out what I had seen a vision of, and I did some research about it. But I still remember it as that mystical thing I once learned and then forgot everything about until much, much later.
`==&gt;` is basically a less general version of `&gt;&gt;&gt;` from `Control.Category`. (As a side note: F# does function application this way by default via the `|&gt;` ("pipe forwards") operator.)
As bad ideas go, n+k patterns are the germ of a really amazing idea (generalized unification) that we can't quite do in real programming languages yet.
There are theorem-proving languages that use dependent types to verify some property where logic is based on values inhabiting given type. And monad is just monoid in a category.. well, you know the drill... It assumes that a proposition consisting of step-by-step observations and/or modificatons is correct by construction ("walk into" part). The punchline is that the expression itself is valid and not a joke by itself, but the form, however, is a classical "X walks into a bar" piece.
Yeah, I figured there was a good reason. (And the patterns looked a little bit too much of a special case to be worth having in the standard.)
It defines the :^: as left-associative, which seems odd. Then on this line: &gt;`simplify (c :^: Const b :^: Const a) = c :^: (Const (a+b))` This isn't true. It wouldn't matter whether we interpret it at `a^(b^c)` or `(a^b)^c)`, either way it's not true [in general]. For instance, `(x^2)^6` is `x^12` and not `x^8`. `simplify ((c :^: Const b) :^: Const a) = simplify c :^: (Const (a * b))` 
&gt;&gt;At the very least, the solution has never been "Delete things until NPM is dead, reinstall NPM, and reinstall packages". Ditto with CPAN &gt;That is not the advice with cabal either. All 3 run into the same problems, with the same advice offered as solutions. Blowing away your installed packages does not require blowing away the package manager as well. Again, apt has absolutely nothing to do with the topic. &gt;&gt;As far as APT working &gt;I already told you why it works, and did so as one of the people you insult by pretending it does it automagically. We spend hours making sure each and every package has precise dependencies defined so that installing anything will work. We are not infallible, and we frequently deal with reports of packages that do not install correctly in various combinations. Spending that time is what's wrong with cabal. Upper bounds never ever represent the truth because the world changes constantly. Stackage is really a superior *theory* because it is a system that with some extensions actually solves the problem. To clarify, cabal solves the wrong problem. It solves the make-sure-it-builds problem. Stackage solves the make-sure-they-build problem. The latter is what ghc package management expects to be solved and that's giving the tension. &gt;&gt;The current state of Cabal is obscenely broken and people complain about it frequently and you're insisting that it's not the case. &gt;*You* complain about it frequently. And you demonstrate a profound ignorance every time you do so. I do not believe this is a co-incidence. 
By convention, I've found people prefer it to be right associative. And the reasoning for it is that if we say: e^(x^2) [no parentheses, so `e^x^2`] , it ought to refer to the exponent of x-squared. And the reason for that is that we can (and do) write e^(x^2 + 1) [`e^(x^2+1)`] which we could not if we were supposed to interpret it the other way [because we would need to use parentheses]. EDIT: [Wikipedia article here](http://en.wikipedia.org/wiki/Order_of_operations#Special_cases) mentions briefly how it's usually interpreted. It's mostly just whichever the designer of the language picked, but I think this form makes more sense. EDIT2: Also, the standard `^` operator defined in the prelude is right associative.
You mentioned using `TypeOperators`, but the code you posted doesn't actually need that extension. The `(:+:)` and `(:*:)` and friends you defined are data constructors that live on the value level, not type constructors that live on the type level. `TypeOperators` are needed if you wanted to do something like this (usually in the context of generic programming): data a :+: b = Inl a | Inr b data a :*: b = a :*: b Here, `(:+:)` and `(:*:)` are type constructors and you need the extension (though `(:*:)` is also a data constructor).
Awesome, I'll go ahead and make `:^:` right-associative as well. Thank you for your input!
Oh, interesting... I assumed I needed TypeOperators for this kind of thing but I guess not. I'll remove that bit. Thanks!
Best of luck! :)
Interesting article! This is the type of program ideally suited to quickcheck :) Test whether evalExpr (simplify e) x == evalExpr e x for all e and all x. You will find at least 1 other problem (though you probably already know about it). And another thing: you can define evalExpr in terms of simplification, since evaluation is a special case of simplifying an expression without variables (although then the above test becomes a little weaker). If you want to define the derivative on all expressions, you have to add a Log constructor and use this rule: derivative (a :^: b) = a :^: (b :+: (Const -1)) :*: (derivative a :*: b + a :*: Log b :*: derivative b) You can also test derivatives with quickcheck by testing against a numerical approximation.
Awesome, I'll look into both of these things! Great suggestions. Thank you!
Your power rule only supports constant exponents. However, you could also use a power rule which works for all cases: http://en.wikipedia.org/wiki/Differentiation_rules#Generalized_power_rule You will have to add logarithms, but these are also easy to differentiate.
&gt; something as simple as a capitalization typo could cause unacceptably sneaky bugs A project's .cabal file defines which packages are being used. Any typos in import statements will result in a compiler error. &gt; they should all be named Database.Sqlite, but the naming scheme has successfully defeated itself. Multiple packages are able to use the same path. &gt; typing Graphics.UI.Gtk over and over again Qualified import statements may help you here.
Thank you for your explanation. I figured it was about curry-howard, but I did not get the "walks into"-part. I'm not sure if it's a language barrier or what, but I'm not sure what "walk into" means (besides the "walks into a bar" sense). Can you perhaps give another sentence using that "construct"? Again, thank you.
I did some of this stuff on my own before posting the blog post (I added an E and Ln constructor and stuff) but it kind of took away from what I was trying to express. It's definitely possible to build on this system and I'm interested in doing so, but I wanted to keep it super simple for the purposes of the blog post because I just wanted to kind get the idea across -- not necessarily implement everything. However, if I continue to expand upon this, I'll definitely add in the functionality. Thanks for your input!
You're simplifying a fixed number of times - why not repeat it until the input matches the output? Then it would be as simplified as possible. 
So... where is this 'semantics of Haskell 98' you mention available?
You may also want to look into [automatic differentiation](http://www.haskell.org/haskellwiki/Automatic_Differentiation).
Historical note. This [functional program](http://strictlypositive.org/calculus/dad-d.jpg) was written and executed in 1970.
Not a bad idea. I didn't consider that -- thanks for the suggestion!
That's crazy awesome! Thanks for sharing!
&gt; Qualified import statements may help you here. Actually, `import ... as` may help there. Surprisingly, that feature is actually orthogonal to qualified imports (which just prevent the unqualified names from being added to the scope).
And it will only evaluate a list as required, as well. `take 1 (repeat '!')` works just fine, for instance. There are really only a few things that force evaluation: A function is forced when the result of applying it is forced, constructors are forced when an expression that pattern matches on them is forced, and `seq x y` forces `x` whenever `y` (the value of the expression) is forced. That's pretty much it, I think.
I dream of a better, brighter future where darcs and Minix rein supreme! I started playing with Darcs 1, and switched to it full time with Darcs 2. I have dozens of repos in darcs and use it daily and have never had an issue with it. Alas, darcs vs git was never about technological superiority but mindshare. And, its hard to compete with mindshare when one of the choices is created and promoted by one of the largest open source projects around. It's ironic that the best thing about a 'distributed revision control system' is the popular centralized website that uses it... I, for one, am still excited about the future of Darcs. Are we really going to be using git/github as we know it today in 20 years? Or, will we be using something better? And, if so, how do we get there? git is now constrained by its own popularity. With a huge user base, they can't really change too much. Darcs, on the other hand, still has a lot more flexibility to study the truth of revision control.
There is a rationale behind the current design that one should separate provenance from purpose. That way if you go and switch between packages you aren't stuck editing literally every single source file due to a change in provenance. http://hackage.haskell.org/trac/ghc/wiki/Commentary/Packages/PackageImportsProposal?redirectedfrom=PackageImports Has it worked out very well? I'm not sure. But I for one don't foresee it changing in Haskell.
Yes. Here is a concise example: &gt;&gt;&gt; let (x, y) = ("hello", y) &gt;&gt;&gt; x "hello"
I'd honestly be happy with a local only version of that sort of notebook. Though I agree that the full programs everyone can run direction is awesome too
The [PackageImports](http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#package-imports) GHC extension might help, it allows you to specify the package exporting the module, such as in: import "network" Network.Socket -- import the Network.Socket module from the network package. import qualified "network" Network.Socket as NS -- this also seems to work.
It "walks in" in a sense of that constructed monad "fits" the logic structure of a proof induced by those dep-type system. (disclaimer: i've only thought of that as a silly wordplay initially and only now found that it could possilbly contain some sound principles.)
Good point, still getting my head around Monads at this stage. My original implementation had almost every function ... -&gt; IO Result and was printing the debug as I went but I got told that you generally want to keep the pure and impure parts of your program as separate as possible. 
How the committee was selected? Is it normal that I don't see people like Simon Peyton Jones?
There where practically no backward-incompatible changes in libc since its inception. You can take code written in the 70's that uses functions of then-current libc, compile and link it against a modern libc (any one of the dozens), and it will just work.
Hah, you can't just copy paste the signature... You need to read and use your brain, too.
BTW on a quick note: you can learn a bit on this yourself if you use the GHCI debugger (http://www.haskell.org/ghc/docs/latest/html/users_guide/ghci-debugger.html) - it will show you unevaluated chunks
For those belittling `catMaybes` in this thread, that function is indeed useful here: takeWhileJust = catMaybes . takeWhile isJust Although `map fromJust` might be a small optimization (that would need testing in any given application), it is better to be in the habit of using safer functions like `catMaybes` and `mapMaybe`.
Sandboxing in Cabal is fantastic, it will always be useful. But the reason sandboxing is so overused right now is only to work around the two real problems: * cabal/ghc getting confused when more than one version of the same package is installed * the need for a better SAT solver for dependencies Is there any progress on these fronts?
&gt; as long as you stay current That's the problem. I am currently trying to support a product with about 100 dependencies that was last compiled about half a year ago. It's a nightmare. Stackage is a great tool, but a lot more needs to be done.
[Stackage](http://www.haskell.org/haskellwiki/Stackage) is a step in that direction.
Your example doesn't actually confirm what camccann said, because `"world" ++ a` is already in WHNF.
Any time you have an impure part that you interleave throughout your program, you can factor it out using the `pipes` library (or any other streaming library). All you have to do is something like: import Control.Monad import Control.Monad.Trans.State import Control.Proxy debug :: (Monad m, Proxy p) =&gt; str -&gt; p x' x r str m r debug = respond myProgram :: (Monad m, Proxy p) =&gt; () -&gt; Producer p String (StateT Int m) () myProgram () = runIdentityP $ forM_ [1..10] $ \i -&gt; do n &lt;- lift get debug $ "The value is now: " ++ show n lift $ put $! n + 1 The `debug` sends the value forward downstream for processing. You can either have the next stage downstream print out the value: main = flip runStateT 0 $ runProxy $ myProgram &gt;-&gt; hoist lift . stdoutD &gt;&gt;&gt; main The value is now: 0 The value is now: 1 The value is now: 2 The value is now: 3 The value is now: 4 The value is now: 5 The value is now: 6 The value is now: 7 The value is now: 8 The value is now: 9 ((),10) ... or you can ignore the value entirely and convert to a pure computation: main = print $ flip runState 0 $ runProxy myProgram &gt;&gt;&gt; main ((),10) The reason that works is because `myProgram` has a polymorphic base monad, `m`. Until we decide what the downstream stage is that `m` could be anything, like `IO` or `Identity`. When we attach the `stdoutD` stage downstream, then the compiler infers that `m` must be `IO`, but when we remove the downstream stage and switch to `runState` then the compiler infers that we meant `Identity`. `pipes` lets you factor out the impure parts of your program into these self-contained stages (like `stdoutD`), so you can write pure components like `myProgram` that aren't contaminated by `IO` and decide at the last minute whether or not you want to interleave `IO` or not. 
Yeah it is, I made a couple of mistakes in the simplification code. I'm fixing them now, thanks for pointing this out for me.
Another cool thing about representable functors in Haskell is that they always have a left adjoint. This is "obvious" because the left adjoint of `(r-&gt;)` is `(r,)`. Why is this cool? Because left adjoints choose indices inside their right adjoint. A value `r` is a particular index into `(r-&gt;)`. Edwardk defines in an early comonad reader post the idea of zapping two functor values together, and this relationship between left and right adjoints gets you two zap functions. Furthermore an adjunction pair defines both a monad transformer and a comonad transformer. There's all sorts of cool stuff you get with adjunction pairs. Representable functors give you a ton of structure to work with.
I've actually changed the post now, that's much better than what I was doing!
I was wondering whether the good people who work on Haskell Platform could rename the version to the actual version of ghc underlying said platform.
The debugger illustrates this sort of thing without the need for pathological terms. (I put responses on the right after `===&gt;`) &gt;&gt;&gt; let x = [1..10] &gt;&gt;&gt; let y = 2 + 3 &gt;&gt;&gt; let a = (x,y) &gt;&gt;&gt; :sprint x ===&gt; x = _ &gt;&gt;&gt; :sprint y ===&gt; y = _ &gt;&gt;&gt; :sprint a ===&gt; a = (_,_) &gt;&gt;&gt; take 1 x ===&gt; [1] &gt;&gt;&gt; :sprint x ===&gt; x = 1 : _ &gt;&gt;&gt; :sprint a ===&gt; a = (1 : _,_) &gt;&gt;&gt; y ===&gt; 5 &gt;&gt;&gt; :sprint y ===&gt; y = 5 &gt;&gt;&gt; :sprint a ===&gt; a = (1 : _,5)
you mean `'w':("orld" ++ a)` is in WHNF.
You can certainly do that, if you are lucky and the code in question doesn't use anything that changed. Same holds true for any haskell lib. Plenty of breaking changes have been made to libcs over the years.
&gt;If you think this is not an issue Why reply if you aren't going to read what you are replying to.
&gt;Spending that time is what's wrong with cabal Spending that time is with apt, it has nothing to do with cabal.
Thanks for another installment in this excellent series. I'm still trying to fully understand the previous two, but it's quite a bit of food for thought...
In contrast to the above behavior, observe what happens in this session: &gt;&gt;&gt; :set -XNoMonomorphismRestriction &gt;&gt;&gt; let x = [1..10] &gt;&gt;&gt; :type x ===&gt; x :: (Enum t, Num t) =&gt; [t] &gt;&gt;&gt; :sprint x ===&gt; x = _ &gt;&gt;&gt; take 1 x ===&gt; [1] &gt;&gt;&gt; :sprint x ===&gt; x = _ Because x is polymorphic, its value will be recomputed whenever it is evaluated.
What's wrong with modifying the data type to add an option? So what if there are 100 options? What problem are you trying to solve?
What are the significant differences between cmdtheline and optparse-applicative? I have been using the latter and have been very happy with it.
The [changelog](https://github.com/haskell/haskell-platform/blob/885272486261057d81f0095973a1256be0475f75/download-website/changelog.markdown).
Ok, got it. But even that case can be handled by Haskell Platform 7.6.3-N where N=0,1,2,3,... for updates to the platform. 
Glad to hear it helped! I actually don't know much Haskell (still haven't got Monads down) but I'm working on it!
Ah you're right! A small computation is indeed required.
It saddens me that Haskell is not competitive, not sure what anyone can do about it though.
It *is* competitive, just not in exact areas measured by these tests. And its raw speed is pretty competitive IMO, compared to other mainstream languages.
&gt; [...] In principle you have something like... &gt; x = (,) a b &gt;IIRC, this is even a legal extension syntax. [...] Actually, it is perfectly valid. `(,)` is a perfectly normal `a -&gt; b -&gt; (a,b)`. `(,,)` , `(,,,)` and so on are valid too until a certain point. What isn't valid and should be are things like `(,1)`: for that you will have to do the whole `\a -&gt; (a,1)`, or use the `{-# LANGUAGE TupleSections #-}` pragma. 
If what you want is debugging, you can just use use the `Debug.Trace` library: it provides a function `trace :: String -&gt; a -&gt; a` that uses black magic(actually `unsafePerformIO` and inlining) to output a debugging string whenever `a` is evaluated, without having to use IO.
Thanks - I think my confusion was from mis-remembering that tuple sections extension. 
Tell me about it. I work on web stuff with Ruby at work. Had to scroll to the very bottom of almost every test to see Ruby's "lightweight" web framework, sinatra.
Actually, for the JSON test there's both a Yesod entry and a raw WAI entry.
At work we just recently switched from REE MRI 1.8.7 to 1.9.3 w/ railsexpress patches. Our infrastructure currently has too many C dependencies and does too little multithreading (read: none) to justify the development effort of switching to JRuby.
Well, yes and no. Seeing Yesod and Snap get beaten by full stack Java frameworks (even sometimes beaten by frameworks in dynamic languages like Flask) is a bit dispiriting. On the other hand, Go and JVM languages seem to be doing very well.
There are multiple entries for Yesod now. The raw mongoDB entry is at half the top entry, similar to JSON. Some of the benchmarks will be improved on the next run, but I am surprised that everyone wants Haskell to match C &amp; Java. I thought being half as fast was a great achievement.
I'm fairly positive all the cost is in the db bindings, but that's just a super strong hunch.
I'm wondering if anybody can explain what are the major reasons why Haskell Web frameworks are twice slower than Java for instance. Is this because of purity/immutability/garbage collection? Or maybe JIT compilers are more mature, i.e. have better runtime optimizations? Maybe Yesod is at higher level of abstraction / overcomplicated?
It makes me a little uncomfortable that the assumption that "Nothing signals the end of processing" is pushed onto the consumer of the list of maybes. Who is giving you a list of maybes, and why that representation? Perhaps the producer should be using `unfoldr`?
Currently, I use takeWhile directly, however I want to move to using an `(a -&gt; Maybe b)` instead of an `(a -&gt; Bool)` so that I can extract data at the same time, basically.
Here's how it works and who the committees have been: http://hackage.haskell.org/trac/haskell-prime/wiki/Committee
Watch out for Topaz in the future.
There's also a pure JSON serialization test that doesn't touch any DB, and Snap's rank is roughly the same.
its been switched to use aeson but the code is crummy. for one it uses strings instead of bytestrings in the ToJSON instance. For another in the pure json test its using strings on one side and text on the other. If you want efficient encoding, don't force it to run around between different formats for no reason.
Its also still using inefficient system.random it seems. not much work at all has gone into tuning apparently.
Sounds to me like you should ditch that "orm" and use a more featureful db interface -- plenty exist in haskell!
Well, that is purely semantic, but my opinion would be that you don't think it is an issue according to a definition of issue that I would use, either by way of extent or emphasis. I don't think, reading your posts, you think it is an issue for people. You seem to suggest that only a tiny fraction of people run into these problems (citation needed?) and you essentially lay the blame on them. They're ignorant, and they only think it's broken because they heard it from someone else. Well, that is not what I believe.
That's the Yesod build, the Snap build still uses `System.Random`.
Thank you
They are measuring stuff that Java is good at. The makers of the benchmark also wrote the gemini implementation. Measure something that is memory bound (which a lot of things are nowadays), and you'll see JVM stuff will likely perform worse then compiled stuff.
Yeah it's still nowhere near complete. But so far it looks pretty promising. I know one of the developers and according to what he tells me they do some clever things to get really good performance (besides using pypy that is).
Wow! Haskell is scoring amazing! :) And that with only so little years of Haskell web frameworks being around, that excites me for a very bright future for Haskell in web land. The benchmarks also favour the JVM a little by testing stuff that it is know to be really good at. Memory-bound tasks are not in at the moment and I expect JVM to be less triumphant in that area. Also know that both Yesod, Snap, Wai and the db libs used are quite young, and improving fast. It may be so that they grow very competitive with the winners (like gemini and wicket) in the next years. But most importantly is the code -- WOW! To me the Haskell submission blow the "benchmark winners" out of the water in the beauty (read: would like to work with) contest! Congratulations Haskell community... Looking at what the dynamic langs (PHP, Python, Ruby, Perl, etc.) are doing --while being extensively used for web stuff over the last 10+ years-- I think the Haskell community has hereby proven be small yet in the league with the big boys. 
I was just dabbling with Go (analyzing for work). It's not entirely unfair to call it a web framework with a goal of being a systems engineering language in the future. As Haskell has had a lot of time poured in to various type system research and fusion etc etc, a large amount of Go's optimization work has been around almost the exact same test cases that this benchmark is testing. I don't mean that as a criticism. I did end up deciding I don't really like Go, but at this stage in its life focus is good for it.
Odd that they would list C# under languages when there are no C#/.NET frameworks represented.
So here's another half-baked theory. the json handler calls setContentType which both plumbs into a monad and ultimately modifies a hashmap of headers behind the scenes. So we're paying the cost of a hashmap modify on each request. On something this basic, that cost may dominate. It would be worth investigating (although pretty hard to do anything about in snap proper without some significant rearchitecture). If it is the problem, it really doesn't matter, honestly, because that sort of constant overhead cost shouldn't really matter in anything serious, i suspect?
We aren't debugging our Haskell program, we're debugging the input program (written in a very small, albeit turing complete, language). We've been specifically told we can't use unsafePerformIO although I have used it in the writing of my interpreter.
With a difference like this it is cheaper to buy a beefier server than spend extra on a maintenance of spaghetti classes and XML build systems.
I find it curious that Snap is getting respectable performance on AWS, but taking a drubbing when run on dedicated machines. I sure would like to know why. Of course, for these types of benchmarks, the benchmark code is often just as, if not more important than the surrounding libraries. But at the same time, I don't really care at the moment; this is all very irrelevant to my current web projects, and the ones I have planned in the near future.
Well that was Persistent which is thought to be somewhat modern. What I really want is templatePG to return extensible records like Data.Vinyl. I don't see why people can't just admit that SQL is awesomely expressive. The chances that there's some guy out there who understands Haskell and the insane type error messages that result from making everything "type-safe", and wants to use a relational database but somehow can't cope with learning SQL are nil. 
First of all, the latency numbers are really interesting. It is clear that EC2 is not production-ready cloud infrastructure if the latency numbers are correct. For example, go has a stddev of 100ms on requests on EC2 and 3.4ms on dedicated hardware! I think it would be interesting to understand the what and the why of this. I'm really curious of how Erlang both using the 'cowboy' and the 'elli' frameworks manage a response stddev of 3.2ms on EC2! No other framework, including the C-based 'onion' manage this. There are clearly some frameworks that play well with the EC2 scheduler and some that do not. My guess is that this must have something to do with the erlang process model. If we look at dedicated hardware, the max latency is also interesting, and by looking at that one we can see how the C-based 'onion' framework just crushes all competition. Its max latency is *an order of magnitude* lower than the second best. That is really incredible. Yesod/wai does really well wrt max latency. Much better than I would have expected giving the stop-the-world GC in GHC. It seems clear that the GHC garbage collector is competitive with Java. It might be that GHC produces a lot more garbage, and thus spends more time doing GC which gives lower performance. But when Java does GC, it seems to take slightly longer and thus the max latency hurts. Many of the top performers are worrysome wrt worst-case latency. Gemini (1st place) has 75ms, openresty(2nd) has 160ms, servlet (3rd) has 98ms, go (4th) 56ms, http-kit (5th) 52ms, scalatra (6th) 800ms. Ouch! You can sort of see that the highest performance numbers give rise to issues in max latency and std.dev, but then there are obviously broken frameworks like 'scalatra' which give excellent throughput but catastrophic worst case latency. If we take the best haskell result, wai with worst case latency of 60.2ms the following have lower max latency: onion, go, http-kit, elli, unfiltered, cowboy, nodejs, play-scala, In the multiple-database test there is some sort of occasional breakdown which makes the max latency pretty bad at 1200ms for yesod. However, notice how Erlang with cowboy and elli manage to keep the max latency at 230ms. This should be possible for Haskell as well. In the JSON-serialization test, wai and snap manage max latencies of around 20ms. Pretty respectable but I see a lot of Java frameworks being slightly lower. Again I'd like to point out elli at max latency 6.3ms. cowboy for some reason is at 27ms max latency. All in all I don't think the performance of Haskell frameworks is an issue. The only issue the Haskell frameworks have is worst-case latency in some cases. I think it would be wrong to do any optimization that could hurt latency, stddev latency or max latency. I think there is an important lesson from Erlang in all of this. Erlang does per-process GC. This can be competitive with even C in worst-case latency! The thing is, GHC also does per-process minor GC. If there was a way to further isolate heaps like Erlang does, Haskell could get excellent worst-case latencies. Better than Erlang because of the compiler, and possibly better than C! There might be something to learn from process scheduling as well given the extremely low stddev on EC2 that Erlang achieves. 
I think the whole analysis is wrong. If you look at the latency numbers, you will see that Haskell is competitive. In a production setting, the most precious resource you have is your request latency budget. You will have to buy more servers to keep the average latency down as well as the std.dev of those responses down. If worst-case latency is bad, you have to rewrite your app in another framework no matter what the throughput of your server is, because that 1% of users that get crappy service will complain and there is just nothing you can do to make them happy except ditching your system. So my advice is to ignore the throughput and focus on the latency numbers. However, it isn't clear to me what methodology is used for the latency numbers. Throughput is interesting, but it is a secondary goal to latency. First focus on latency, then throughput. But back to your question, I think garbage collection is really important. The worst-case GC is very important because it affects that 0.1% of users. Even if GHC does more GC than other frameworks, and even if that might affect the throughput, what is important is what happens when a global GC happens. How does this affect the latency of the request. Thus how fast can we do a global GC? (In a way, I think Haskell's biggest threat might be the use of a real-time GC for the java frameworks, but that hasn't been benchmarked in this test) 
No the upper bounds _is_ a problem. The assumption is that any version bump that includes an API change must involve the developer in verifying that the new upper bound is safe. This is not a good assumption. It is safe in that it solves the make-sure-it-builds problem, but it is detrimental in solving the make-sure-they-build problem. To solve the latter problem, a probabilistic system where we say "if it compiles and the tests run, it is probably fine" is a much preferred strategy. This latter strategy is what stackage implements. I think the upper bounds _are_ the problem. The upper bounds should be set by a machine, not humans. This leads me to disagree with your assumption that easily changing the API is a problem. It isn't a problem when it does not break a package. Changing the API in a _possibly incompatible_ manner requires changing the major version number. This _ensures_ breakage of the make-sure-they-build problem. This is highly problematic. When there is a _possible breakage_ a probabilistic approach is the best one. Do a build, see if it breaks. Only _when it breaks_, assume that a developer must be involved in fixing it. Thus upper bounds should not be the realm of developers, but machines.
Aw crap! Just when I'm planning on Odessa being my next destination, I learn that I will be a week or two late. 
Sounds like a dream job. Too bad I'm still working on my MSc :/
Even if the tuple were strict, it would only need to partially evaluate the right side (to "WHNF" - "weak head normal form"); since cons (:) is lazy, the form 'w':("orld" ++ a) is enough, the right argument to cons being a thunk. (WHNF means, roughly, that the "topmost" structure is an actual data structure and not a thunk.)
That said, it's also developed by one of the biggest web companies with the idea using it for their systems in mind. Haskell has never been a *language* developed for the web, so the results are still quite impressive.
No, you are assuming that because I know that you personally as an individual are an ignorant troll, that it applies to everyone else too.
No, all the Haskell results are dismal. I haven't liked at any of the benchmark code, but I'm surprised the numbers are so bad.
That's true. Also, buying a beefier server is never going to help those frameworks that have huge variations in latency. Those are the real losers in the benchmark IMO. go, erlang, and C seem to perform with the most consistent latency. Java is sort of like Haskell, not incredibly consistent latency, but not terribly bad either. I'd sort the results on something like avg latency + std.dev of latency, not throughput. That's what a beefier server doesn't help with much. CPU usage thus throughput is usually insignificant for 99.999% of requests. Latency isn't.
What benchmark code in Haskell would you like to see?
Cool! I just want to say thank you for going at it for more than 3 years! I am not using it currently, but the whole eco-system around Yesod has done so much for Haskell. Thanks! 
Erlang BEAM runtime sacrifices throughput for latency, so it's expected that it will have low latency. As per per-green-thread GC, it won't be possible in Haskell without big sacrifices, per-green-thread heaps eliminate sharing between green threads. For Haskell that would be a disaster. (For Erlang it is too, but only rarely).
This is a great read, with lots of historical context for y'all: http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/
You *really* need to work on your reading comprehension.
Here is the complete code for anyone interested. The class is "Survey of Programming Languages." Different people chose different languages and were all told to code the same program(s). I chose Haskell, because, hey, Haskell is cool, right?!? It turns out the example program was not a good fit for Haskell. So, here is my square plug for a round hole: --- -------------------------------- -- [Wiremaster] -- ITCS 4102 -- Haskell Three-Of-A-Crime -------------------------------- -- Imports import System.Environment import System.IO import Data.List import System.Random -------------------------------- -- Main function: "do" begins a list of tasks to be done in -- the order they are written, rather than as the "lazy" -- language sees fit. main = do -- THE SETUP -- Create list of suspects -- Haskell allows us to describe a pattern, and it will fill it when it's needed. let suspects = ['A'..'G'] -- Randomly choose zero-to-two entries from list and prepend to guilty list -- send the suspects and generator to a function. guilty &lt;- (choose suspects) -- Comment out these lines to hide the answer. putStrLn "Answer:" putStrLn guilty playGame 1 suspects guilty putStrLn "Thanks for playing!" -- This function receives a list of characters, and depending on the "IO" of the Pseudo-Random -- Number Generator (PRNG), returns a set of one or two characters. It unfortunately does not do -- zero criminals or have a very good probability distribution. choose :: [Char] -&gt; IO [Char] choose s = do -- Get the global PRNG gen &lt;- newStdGen -- Get two Random numbers, passing the resulting RNG to the next call of randomR. This is -- because Haskell requires each function to have reproducable input, so all randomness must -- come from a different RNG, or the results would be the same every time. let (randIndex1, gen2) = randomR (0, 6) gen :: (Int, StdGen) let (randIndex2, finalgen) = randomR (0, 6) gen2 :: (Int, StdGen) -- Before we leave, let's make sure the global PRNG is different! setStdGen finalgen -- This is my hackish way of letting single criminals through on ocassion. It's not well- -- distributed, but it works okay. School assignments do have deadlines! if randIndex1 == randIndex2 then return $ (s !! randIndex1) : [] else return $ (s !! randIndex1) : (s !! randIndex2) : [] -- Here's where the actual gameplay occurs. The function takes in a player number (for later -- expansion to multiple-player action), and the two lists of suspects and guilty. The result -- is simply IO. Haskell professionals would tear their hair out seeing this, but it can't be -- helped. We are indeed trying to code an imperative program in a functional language. playGame :: Int -&gt; [Char] -&gt; [Char] -&gt; IO () playGame player suspects guilty = do lineUp &lt;-(choose suspects) -- Basic Haskell IO! putStrLn $ "Of the following suspects, " ++ (show $ match lineUp guilty) ++ " are guilty." putStrLn lineUp putStrLn "Please enter a guess, or nothing to pass." guessString &lt;- getLine -- Again, the functional programmers of the world would have my head if they saw this. if (not $ null guessString) then do if (match guessString guilty) == (length guilty) then putStrLn "You Win!" else putStrLn $ "You lose. The criminals were: " ++ guilty else playGame player suspects guilty -- http://stackoverflow.com/questions/8518940/haskell-number-of-matches-between-two-lists-of-ints match :: Eq a =&gt; [a] -&gt; [a] -&gt; Int match xs ys = length (intersect xs ys)
FWIW, I know that myself and several other framework developers are not putting effort into this benchmark. And by the way, Haskell does have database-specific libraries like mysql-simple and postgresql-simple that don't support the least common denominator.
Haskell is supposed to be good at it too. Haskell is not intended to be "slow but uses little memory". General benchmarks like the shootout show haskell is competitive with java, so there is no fundamental reason haskell can not be made competitive with java at web serving, given some effort.
&gt; frankly I don't care that I am developing in an framework that is 3x slower than hand rolled GPU assembly Unfortunately for you, the benchmarks aren't dealing with GPU assembly. A correction to your statement is that Yesod is 3 to 4 times slower than the top 3 high-level language platforms. They're benchmarks, take them for what they are. Performance is not the only metric to choose a development or production platform. But who wouldn't want to see a haskell platform in the top 3 of these benchmarks. However, if you want a good mix of performance and ease of programming, scala is probably a good choice too.
I agree that latency is at least as important as throughput, but I disagree that worst-case GC is a serious problem here. Most users will find an occasional higher-latency request perfectly acceptable; GC pauses are rather unlikely to affect the same users over and over again. And even if you have consistently low latency, you are going to have to buy more servers if your throughput sucks. A larger website really needs both; although you are correct in that it tends to be easier to fix throughput problems than it is to fix latency problems, and adding more servers to fix latency may or may not work well, and is unlikely to be a real fix.
But on the same hardware, java and scala-based frameworks are faster than any haskell frameworks. As for your other comment on "spaghettic classes," it has nothing to do with the language. One can write crappy code in any language. Also, XML build is not a prerequisite for any java/scala project. It is disheartening, given that haskell compiles to machine code and the JVM is byte-code. Having said that, the JVM is very mature and the folks formerly at Sun managed to optimize it's performance very well. Average latency aside, since GC has a big impact on the average latency. 
wow! wow!! wow!!!! I must say, such a clear explanation for such a confusing issue!! kudos to you, and more kudos to you Sir / Madam! ("forcing depends on the type of the result", "`seq` is itself evaluated lazily" .... you've made my day today!). :) :) :) 
http://www.reddit.com/r/haskell/comments/1929xn/are_lispstyle_macros_a_code_smell/c9rtzg4
Yes, that definitely looks like a huge step in the right direction.
Do you really think this is a useful answer? There're tons of ways of separating pure and impure code in Haskell. Pipes are probably one of them, but your example really seems overly complicated to propose to someone who is just learning to use monads.
&gt; First of all, the latency numbers are really interesting. It is clear that EC2 is not production-ready cloud infrastructure if the latency numbers are correct. What? Half of the world is using EC2 nowadays to run there software. There must be more to production ready cloud infrastructure than just latency numbers.
Any advice on what the best way to upgrade is? The, alibet limited, set of cabal incantations I know all give scary warnings about breaking various things when I try to upgrade. 
Yes, I very strongly believe it is a useful answer. Not only that, I strongly believe that this is the simplest solution that actually works. All those other solutions you are referencing do not actually work for long-running or streaming programs and all of them hand-wave away the real challenge of how to repeatedly interleave effects without contaminating the entire program with `IO`. I hang around on /r/programming all the time, and you would not believe how many people say something to the effect of "Haskell is useless because everything ends up being contaminated by `IO` anyway, thus defeating the purpose". I spend a lot of time discussing this with them and they usually describe something like this process: * They hear about how Haskell is cool because it lets you separate pure from impure code * They try to write a long-running program (i.e. a game or interpreter) * Oops! Their inner loop has one `IO` monad * Oops! Now their entire loop is stuck in the `IO` monad * Oops! Now their entire program is stuck in the `IO` monad * They go on IRC or Stack Overflow asking how to separate out pure and impure parts * People tell them "It's so easy! All you have to do is separate the pure and impure parts" and then they proceed to "prove" this by demonstrating it on a toy example that does not loop or stream and is not at all related to the problem the beginner was to solve. * Everybody they ask keeps punting and not actually solving their problem by always providing falsely equivalent examples. * The beginner gets frustrated and quits and conclude that Haskell fails to live up to the ideal of separating purity from impurity for "real" programs * They then tell their friends this, spreading FUD about Haskell So, while I don't believe that he will understand how to use `pipes` yet, but I am planting that seed of knowledge so that he knows that there is a solution down the road if he is persistent enough to keep going. Moreover, this isn't some hypothetical straw-man I've built. He is very specifically developing a program (an interpreter) that requires streaming and interleaved effects, and by his own admission he has already encountered this exact problem of isolating effects, and nobody else other than me was able to provide a suitable solution to his concern. We really need to more strongly emphasize this design pattern of using streaming libraries to isolate effects. Otherwise, Haskell will continue to be marginalized as a language only suitable for short-lived scripts.
That also seems like something staged programming à la Megacz/Generalized Arrows could help with: http://www.cs.berkeley.edu/~megacz/garrows/
I wrote a summary of what I thought about the upgrade [on a Google+ post](https://plus.google.com/118323070335349339293/posts/JxZ2jpW8CfX). However, that probably won't help you with any specifics. Regarding cabal warnings, I've wiped my ~/.ghc and ~/.cabal directories before trying the upgrade. Then I've used [packdeps](http://hackage.haskell.org/package/packdeps) to find out which dependencies I had to upgrade. I've bumped them all and braced for the type errors =). Besides the two problems I had to fix on yesod-core, that was pretty much all of the process.
I'm sorry if I didn't read that correctly. I think I understand now what you intended. Two things, one, it seems to be a contradiction to suggest I'm a troll and I'm ignorant - trolling as it's colloquially used usually implies awareness of one's trolling. If I'm trolling, I'm being insincere in my ignorance, and if I'm ignorant, I'm being sincere in my ignorance. I hope you believe the latter. Two, what of all the other people who have complaints on #haskell, on /r/haskell, and so on? Are their complaints not valid?
per-process GC was implemented by Simon Marlow but found to be slightly slower than the existing GC
It's a design tradeoff--the alternative would be worse, so the current behavior is unlikely to change. It's called the "open world assumption"; there is a good explanation about the reasoning behind it on [StackOverflow](http://stackoverflow.com/questions/8728596/explicitly-import-instances).
It's also probably the case that instance overriding would be a clever way to break module boundaries. This I believe based on the kerfluffle around GenearlizedNewtypeDeriving.
Yeah, I was thinking about this exact application of generalized arrows earlier today and mourning that they are still not in GHC.
That is an excellent answer. Thankyou for pointing me that way and for explaining why it exists. I am going to look into the "open world assumption" it seems to be a critical part of the way that Haskell was designed that I missed.
Just need to find someone willing to maintain the code once it makes its way into GHC, and someone (else?) to push for putting it in in the first place :)
Using `newtype` is one alternative. Another option is to define your own class, corresponding to the binary encoding that you are working with. This is nice because you can customize the instances as much as you want, while still reusing the code from the `binary` package for instances that work the same way. 
Still, it can be a good trade-off to reduce throughput to improve max latency.
I know that statement is a bit combative. Here's some more: 100ms of latency is 30.000/2 km of server displacement. When you use EC2 with go, you don't need to care about the continent of your server, the EC2 scheduler simulates it for you :-). Seriously, I think there might be an issue on EC2 where the scheduler is tuned for throughput instead of latency.
GC pauses are actually *guaranteed* to affect users all the time when the system grows. When you have fan-out in a service (various backend services) the probabilities add up. So in your basic setup you have 1% of requests with bad latency. Later when you add 10 backend RPCs, 10% of your requests have bad latency. See the research by Jeff Dean for how this is dealt with on extreme scale at Google. http://research.google.com/people/jeff/latency.html The system Jeff Dean proposes works for a system optimized for throughput, but only when you have redundant state in a large system. A much simpler technique to reduce overall latency is to have a way to trade worst-case GC for less throughput. Edit: But fast handover of load during GC is probably the solution with the most bang for the buck, since that gives redundancy for free. 
I think there might be other paths to take as well. For example requests could be redirected to another instance while doing major GC. This could be done by integrating GC with nginx. Alternatively, some parts of the IO-manager could be running during GC so that the RTS more or less had nginx built-in (built-in support for Cloud Haskell). Both of those are incompatible with this test-setup though. Another option is to do incremental global GC. I guess that radically changing the GC in GHC makes less sense than engineering fast handover during GC.
I think this is non-issue. Data types like `Set` could be parametrized by the *instance* which would resolve this completly. Essentially you allow make each instance a new type. Scoped instances then become possible just like any other scoped construct (using genericity or an applicative/path dependent design). `mapMonotonic` and its relatives already break the invariant of `Set`. The advice with those is: just don't do those things. Orphan instances allready enable multiple instances (although the compiler warns you about that one). At least one compiler bug that has existed in GHC for years also allows multiple instances (and you can construct them at runtime!). Obvioussly, you can always get around the type system and construct new instances at runtime using `unsafeCoerce` but that has "unsafe" in its name. Several other haskell extensions produce many of the same "problems" (`ImplicitParams`, `GADTs`). I love Haskell, but this is a place where we went with the wrong approach (well not really, because "how to do typeclasses right" would only be thinkable after a language had typeclasses slightly wrong and that language was Haskell).
&gt; As for your other comment on "spaghettic classes," it has nothing to do with the language. One can write crappy code in any language. True, but even if you write "good" Java (for some measure of good), you are going to end with significantly more code (and worse, significantly more boilerplate) than with an equivalent Haskell project. &gt; Also, XML build is not a prerequisite for any java/scala project. Scala has its own tools, but 99% of Java projects use maven or ant.
`newtype` and `GeneralizedNewtypeDeriving` are you friends here.
There have been numerous named instance proposols that are 99% compatible with current haskell. The argument against them is that a few types (Set, Map) need *both* module like operations (taking the union of two `Set a` for some orderable `a`) and mapping. The first of these is well addressed by modules, the second isn't. I don't think this is that big a deal, but other people do. Named/scoped instances solve both of them if those few types like `Set` are parameterized by the instance used. You only need to parameterize on instances for types like `Set` and `Map`--all truely parametric containers continue to work how is. Now, we should have proper modules. But we don't need them to solve the multiple instance problem. My point is that I don't think doing it right is nearly as hard as the Haskell community seems to think it is. 
`GeneralizedNewtypeDeriving` is unsound (in GHC) and not only causes every possible harm that breaking the open world assumption for type class instances would cause, but also makes it possible to completely break the type system using common extensions that are widely believed to be safe on there own. It is very usefull, just please be carefull.
Please don't use gratuitous URL shorteners. It upsets reddit's deranged spam filter and results in your comment being automatically removed until next time I check the moderation queue.
Also, every Haskell functor that has a left adjoint is representable. So adjunctions are not very interesting as a concept in Haskell, as they are equivalent to representable functors, and representable functors are easier to work with.
Thanks. I am not far enough into things to have any significant codebases to upgrade, I was just asking about the wiping ~/.ghc and ~/.cabal step. I don't mind doing so, I was just curious about whether there was a better option. 
Using - - jobs=8 reduces the time spent recompiling a lot, so it's perhaps the easiest solution right now. Also, it guarantees that you'll get the latest versions of all packages. 
Probably some of both.
How do you go directly from representable functor to (co)monad transformer? With the adjunction point of view it is just composition of adjoints. Also, adjunctions are a bit interesting in Haskell when you take adjunctions between categories that are not both Hask. You can get some interesting things from looking at adjunctions between Hask and Hask^Op for instance.
This keynote contains a lot of retrospectives on the development of programming languages, programming methodology and abstract data types starting from the late 60s/early 70s. Papers she talks about include: Dijkstra's Go To Statement Considered Harmful, Wirth's Program Developement by Stepwise Refinement, Parnas' Information Design Aspects of Design Methodology. She also talks the development of CLU (around 26:10). Haskeller's will immediately notice the similarity between CLU's cluster polymorphism and Haskell's type classes including type class constraints. CLU considered operations as belonging to types not the OO view that operations belong to the object. It is interesting that she now expresses misgivings about that decision (around 28:00 and onwards). And finally at the end (41:00) she bemoans the fact that the popular programming languages of today that have abstract data types (i.e. Java/C#) don't make very good teaching languages, but that a language like Python which is good for beginners doesn't have data abstraction. She also aims a critique at Haskell for, I guess, for making managing state overly complex. 
Just compose `f` with `(Key f,)`. The representable-functors package also has contravariant representable functors. I think that every functor into Hask with a left adjoint is representable, but I don't know if it is always possible to find a functor like `(Key f,)` for a representable functor with a domain different from Hask. Maybe the domain category needs to be cartesian closed?
How is it overly complex?
Using Brook's terminology from The Mythical Man Month, Haskell reveals the essential underlying complexity of managing state. Most other languages oversimplify it, but then it bites you much harder in the end. But since everyone only uses these languages, they keep up this hope that it really is as simple as the oversimplified model, and often never even learn how to see the problems coming out of the oversimplified model since they never have anything to compare it to, and blame Haskell for making managing state complicated. I don't. I blame state management for being a legitimately hard problem, and think Haskell, by acknowledging this, gives you superior tools for managing it. But it's really, really cognitively easy to just blame Haskell for having hard state management.
mn-haskell-guy did a great job of summarizing the talk, giving fair a thorough coverage of it and insisting on some of its insights. Despite the manifest disagreement with the critique made, the summary was full of respect for Barbara Liskov. While I would generally agree with what you say about state, I think this is not the right place to discuss this -- again. I'd rather see an interesting discussion about the *other* parts of the talk that may turn out to be much more interesting.
The main problems I see with monadic interfaces is that the syntax is not the same one that is used for pure code (meaning its annoying when you want to add some effects to some previously-pure code) and that its tricky to use multiple effects together (I don't consider monad transformers to be a fully satisfactory solution). I don't know if she said something else on the keynote, I haven't gotten to that part yet. TBH, I think monads are more of a relic of Haskell being a lazily-evaluated language by default, meaning you need to encode the order of operations yourself, using a monadic interface. If your language has strict evaluation everything is already ordered for you by default so you don't need to worry about adding an extra layer on top to plumb your side effects. And while it is true that the natural evolution for a strict language is to start with impure, unverified effects and then move to add some sort of const checking and things like that, I don't see why you necessarily couldn't have it be the other way around, with things being pure by default and having the compiler do a good job of verifying your effects for you.
&gt; TBH, I think monads are more of a relic of Haskell being a lazily-evaluated language by default, meaning you need to encode the order of operations yourself, using a monadic interface. It's more correct to say that monadic `IO` is necessary in a pure language. You can think of Haskell as a "two-phase" execution environment. The first phase is an entirely pure computation that builds up an executable program and assigns it to `main`. The second phase actually executes whatever is stored in `main`. I consider this a feature because this separation of phases means that you still can equationally reason about code in the pure phase, whereas if you mix these two phases like imperative languages do, then you lose that valuable pure window. Separating side effects from the evaluation model is a really powerful tool. This makes the side effects inert so that you can pass them around as if they were ordinary values. This is why, for example, I can stick a bunch of side effects in a list without any wrappers or protection and not worry that I'm going to accidentally trigger their effects. For example, the following code just prints `2` and does not trigger any of the actions stored in the list: main = print (length [void getLine, print 1]) This enables a lot of powerful libraries like Haskell's streaming libraries, which would not be possible if you intertwined side effects with the evaluation model like you suggest. Most languages that mix side effects with evaluations rely on hacks to enforce the separation that Haskell provides by default, such as guarding functions with empty arguments or using macros to delay evaluation and even then they don't really work and don't provide the same benefits as Haskell's pervasive purity.
&gt; It's more correct to say that monadic IO is necessary in a pure language. Some not very popular pure languages do it using things like linear types (I'm sure you know about those). And your other example is more about lazyness than it is about purity. Don't get me wrong, Haskell is still my favourite language by far and I love using the typechecker to assert things at compile time, but I do think that using monads for IO is a bit of a historical accident since Haskell is a lazy language first and a pure language second (you kind of get forced into purity with monads once you are lazy by default). All in all, what I am trying to say is that while monadic code is sufficient for for compile-time verified effects (and its the most popular way to do it today), its not completely necessary that it should be that way :) For an example of the sort of fuzzy thing I am thinking about, the other day I saw a talk from the Idris guy where he made a DSL for handling effects. You could get the benefits of verifying effects at compile time without needing to use monad transformers to bundle them together.
It would be good to see `pipes` be part of it.
I think we sort of agree. I believe that monadic `IO` is still valuable even in a strict environment because it separates the evaluation model from the side effect order, which is very important for streaming libraries like `pipes`. It's just that this separation is even more valuable in a lazy language because it's very difficult to reason about the order of evaluation in a lazy language. I still think my favorite model for `IO` would still be monadic, but would use a free monad under the hood (with a corresponding FFI that added terms to the base functor) rather than a fake state monad. The free monad model makes it crystal clear that laziness vs. strictness has no impact because it reifies the effects as an actual data type, making it clear that the interpretation of side-effects is being done completely outside the evaluation model. I also agree that monad transformers are not the best way to combine effects, but I still think monadic `IO` is a big step in the right direction.
"every functor into Hask with a left adjoint is representable" Yep. Don't follow the second half of what you're saying though. As in "like (Key f,)" in what fashion.
I was trying to give the left adjoint of a representable functor, which for haskell functors is `(Key f,)` or more correctly `(,) (Key f)`. But since this functor can have a codomain different from Hask, products in that category could be different.
Thanks! I fixed it.
The method that the [wiki advises](http://www.haskell.org/haskellwiki/Pronunciation) might be even more long winded. "funfunc has type Int to Int to Int" Saying it is a function would be considered redundant since you know that from the type. However, I have spent little to no time talking to people who also use Haskell, unfortunately, so I do not know passes in general conversation.
&gt; funfunc is a function that takes two int parameters and returns an int If you say it like that it kind of drops the idea of currying from the type signature. I'd say "funfunc is of type Int to Int to Int"
Your gloss sounds fine to me, i.e. not long winded. My only qualm might be that your gloss actually refers to the uncurried version of `funfunc`. So a better gloss might be: &gt; funfunc is a function that takes an expression of type `Int` a returns a &gt; funtion that takes an expression of type `Int` and returns and expression of &gt; type `Int`. Obviously, this is even more long-winded than your version, but it's also more correct, strictly speaking. So long-windedness is a not a sign of newbishness -- quite the opposite, it can be a sign of really understanding what something is. And that's important. A less long-winded, but also more informal gloss would be, for example: &gt; funfunc is a function from a pair of `Ints` to an `Int`. Strictly speaking, though, `funfunc` does *not* take a "pair" of `Ints`, otherwise its type signature would be: (Int, Int) -&gt; Int which is the type signature of the uncurried version of `funfunc`.
Goodness me, you are correct. I was a bit perplexed when I replied.
I tend to read iterated function types to myself as e.g. “`funfunc` takes an `Int`, then another `Int`, and returns an `Int`.” This takes the arguments one at a time, and makes it clear it’s curried, but emphasises thinking of it as a multiple-argument function. There are some types that can be profitably read more than one way. For instance: map :: (a -&gt; b) -&gt; [a] -&gt; [b] could be read as either - `map` takes a function from `a`’s to `b`’s, and returns a function from lists of `a`’s to lists of `b`’s; or - `map` takes a function from `a`’s to `b`’s, then a list of `a`’s, and returns a list of `b`’s, depending on whether you want to emphasise the fact that it’s a functor, or think of it as fully applied.
The two phase notion makes sense for an Applicative. A Monad removes the phase distinction and allows interleaving of phases.
How would you manage effects? What would "verifying your effects for you" mean without an IO type?
In related news, I have often thought it would be interesting to use haskell-src-exts to parse haskell code and translate it into prose. ie, do exactly what the OP is asking, but automatically for the whole program. Alas, not interesting enough for me to actually implement it. :)
funfunc has type Int arrow Int arrow Int.
I mean two phase in the sense of the division between a free monad and its interpreter. The first phase is the pure assembly of the free monad and the second phase passes it to the interpreter, which produces the side effects.
One of the best explanation of lenses I have seen so far.
Thanks! :)
As others have said: "Int to Int to Int"
If we're citing Scala's implicits as an alternative, then there is no doubt in my mind that type classes make the right trade off. Scala's implementation is terrible by comparison. I can't speak to Idris or Coq. Maybe they actually implement something sane. Scala is an argument against your point of view.