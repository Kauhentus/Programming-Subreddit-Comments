I meant the docs-on-the-fly feature, which only works for happstack for the time being from what I understood from the post.
I assume he's referring to this one: http://www.seas.upenn.edu/%7Ecis194/hw/01-intro.pdf 
Did this vanish from "hot" for anyone else? It seems like an interesting paper—I hope it wasn't caught in the spam filter.
I still see it. 
no worries. :) I'm not totally surprised - I've seen some really weird behaviour from GHC-compiled code on this machine. even just when compiling, I get _markedly_ better performance from letting cabal run many single-threaded GHCs at once.
I build all my backend stuff as a rest services on yesod. Never felt the need for any special rest libs or tools.
You'll only have to do it two times instead of three 
This paper is amazing. I've been working it up in a branch of `free`: https://github.com/ekmett/free/tree/categories and am going to rewrite all `machines` using it -- and probably `trifecta`.
Getting client generation for free is the biggest win in my book. Especially for the haskell client, because types! I think the rest-example is pretty minimal, and I'm not sure how to shorten it without losing functionality we want to show off. An overview of the module hierarchy is in order though, I'll try to write something up.
This is very useful feedback, thank you. 
If you want some related work, google "Mikael Rittri query". It's 25 years old, but uses queries modulo type isomorphisms. 
What type-aligned efficient structures exist today? TList just defers all of the problems of left-associated bind into left-associated append. There needs to be a Data.TSequence. (Edit: just saw Control.Category.Free.Cat. Is that extracted anywhere else?)
See https://github.com/atzeus/reflectionwithoutremorse
The notion of "compile time" is just an artifact of the technology used to run the program. Programs are run, if this is in one, two, or 58 stages is irrelevant to the question of purity. I do understand what you are trying to say, though. But mentioning compile time just muddies the water. 
Terrible community? Whenever I need help someone seems to help me in #nixos. Also packaging with nixos is much simpler than packaging for debian.
I'm really sorry but I can't listen to this for more than about 5 minutes without zoning out as you didn't push the gain on your microphone up enough. You should experiment and try to find a comfortable level where it doesn't clip, but still beats out the background noise. Right now this screencast is too hard to hear to really be useful. Youtube does allow you to edit in new audio, I believe.
&gt; Also packaging with nixos is much simpler than packaging for debian. LOL. I don't think you understand how NixOS works yet... good luck with that.
&gt; Also bear in mind that regardless of its insufficiency now, Nix does offer a solution to dependency hell other than each application having its own shared libraries No, Nix depdendency management is not better than most other distros in any way whatsoever, in fact it is worse because there is a lot more overhead to partially accomplish what many other package managers already do quite well (Funtoo emerge). &gt; where you know very exactly what versions of packages you are testing against That sounds great but in practice is not very useful since you have a very limited selection of package versions/flavours. Its completelly inferior to using a language-specific package manager or maintaining your dev dependencies yourself where you actually have full control of what packages with what flags you are using. &gt; Also bear in mind that regardless of its insufficiency now It's insufficiency will only grow, more packages/better package expressions -&gt; more overhead in nixpkgs -&gt; worse nixpkgs. The pull request queue is already deadlocked, people are simply tired of working on nixpkgs, and that's a good thing because it is a complete waste of time. The main problem is the lack of organization in nixpkgs, its a comlete mess - mupltiple definitions of the same expression all over the place, half of which are broken and 99.99% of which are undocumented, a bunch of half working packages, or completelly broken untested packages, or packages that require packages that are not dependencies to be installed (see ocaml graphics, haddock, clang + c++11, scons).
I was planning to write a JSON-only REST-style api in Yesod, so it's interesting to see some good competition in this space, as far as frameworks go. Would you be so kind as to list what you think are the advantages/disadvantages of the two frameworks here?
You can start.
If that really is the case it is only so because you are using a trivial set of popular packages. There's no reason for me to be talking out of my ass, I've been a NixOS contributor for over two years, and I've had to deal with a lot of garbage that is caused by the nature of nix and nixpkgs. I also started as a wide-eyed believer that had no clue what he was talking about and wanted to use it cause it was 'the cool thing'. There is **absolutelly zero** benefit to using NixOS over a distro like Funtoo and many many downsides. It is a fact that the project is grinding to a halt and will be dead in a few months, and having spent so much time on it I feel quite sad and stupid at the same time.
Is there a way to specify to the compiler that an operation is associative? Likewise, it would be nice if there was a way to specify commutativity.
[Here are fresher docs!](http://silkapp.github.io/rest/docs/) Now to see if i can figure out how to upload these to hackage... 
The big advantage here, as /u/bergmark mentioned, is that you get client libraries for those backend services. So instead of doing a raw HTTP call, then doing deserialization, you just call Haskell functions with descriptive names resulting in your actual domain types.
I'm sorry to hear that. I struggled a bit with this when writing the tutorial. I tried to quickly get to a small working example, and explain the high level concepts without getting bogged down in the details. Clearly I failed, at least for you. Would you mind explaining in more detail where you would like to hear more details early on?
Of what two frameworks?
Thanks, I'll have a go at twice. I note that your example seems to be mostly IO, which makes sense and I imagine that most of the code is run pure. I can see that certain network based operations can create uncertainty about successful completion, but it seems that the main reason for a Monad in a simple algorithm is for state, so perhaps that is the best thing to look for examples of? I suppose what I am saying is how often do you find yourself writing functions with `do` statements relative to pure ones and, in the case of state, how often do you use it relative to simply passing state variables around explicitly?
Okay. Thank you.
Yesod and rest.
I'd like to know what the end goal is for one, including things like the interface we expect to be able to use to interract with it. The first thing you do after saying we'll be writing a blog is "defining a resource". What is that? Why do I want to do that? Then you jump into showing the type of Resource, explaining that the first parameter is something that comes from the resource parent... what on earth is that? does that mean they can be nested? Drived from somehow? There's also very little showing what the result of writing some piece of code is initially; we've now made a resource... but why? Des it do anything? When I do actually use a Resource, what will the example one produce? Though it might sound like it, I'm trying not to be critical, just letting you know that for someone who isn't super familliar with RESTful interfaces but has a decent idea of what they are it was quite confusing. I felt very lost very quickly because nothing was explained, just presented, meaning I was left with lots of questions had having no idea what I needed to keep in my head for future reference. I think a lot of this coule be rectified by first starting out by explaining exactly what is supposed to be achieved by the end of the tutorial. Also, perhaps stating the problem each example is supposed to solve and then showing how to solve it would be good. Insteading of saying "Here's a lump of code that ..... It will be used for the path /post/title/&lt;title&gt;", perhaps say "We'd like to be able to access posts at the path /post/title/&lt;title&gt; wherer &lt;title&gt; is ... Here is how we make a resource to do that:"
Hi! Thanks for the info, but I when I try from my computer with chrome I don't get it. Are you uisng it from your phone? Which version are you using? Do you always get that problem or just some times?
Well, I want it to be eventually actually used, that's why I need to know what people do need to feel comfortable using it, unfotunately I have no that much time, so development is going really slow....but is going at least XD.
Notebook, Chrome 34.0.1847.131. I have screen resolution 1280x800. Yes, always
Thanks, this is very useful! I see what you mean now, and you make good points. When I have a bit more time I'll try to rewrite the tutorial to take this into account. Don't worry about being critical. I'd like to improve the tutorial and the packages in general, and really welcome any feedback that helps with that.
Thanks for the clarification!
That's actually not a bad idea. Write a program that is a Basic interpreter to get ideas for learning about Monads. :)
I'd rather link the first message of the thread and name it "What's wrong with Haskell" :)
It would be nice if there was a major overhaul of the crufty bits of Haskell like String and so on :(
`String` in its current form keeps getting Haskell bad press. Can't we just make `ByteString` and `Text` part of the standard language (it's not something uncommon to have in a practical language...), and get rid of `type String = [Char]`? Otherwise we'll have to continue to put up with a couple of "Why is Haskell code so much slower than `${INTERPRETED_LANGUAGE}`?" questions and blogposts...
I think an efficient type-aligned data structure deserves its own package.
What do you expect the compiler to do with this information?
Yes I think this would be a great idea. It's high time to make this breaking change.
The `String` overhaul has already been done - it's called [Text](http://hackage.haskell.org/package/text). The truth is, `String` isn't crufty; it does what is it is supposed to do well, namely, to be a representation of text that is as semantically simple as possible. It's even fairly efficient sometimes. But for serious applications you probably want to use `Text`, even though it's a little more similar to the cruftier string representations in languages like C and Perl.
I should note that the author meant to say "ByteString *or Text*" (depending on what you want to do). I posted this because I thought it a neat reference for using String for file I/O is a bad idea, not to talk about String in general.
Using the String API for I/O with files is not great. But for other things it's good, because of reasons mentioned by others in this thread.
W.r.t. `ByteString` – the emphasis should be on `Byte`, not `String`.
As a practical matter, if we want to use text and bytestring in our base libraries (in particular in base itself), we need to split up base into at least two parts, one with the I/O layer and one with rest. That's needed so we can then slide bytestring and text in underneath the I/O layer, which would allow us to have `Handle` operations on these types defined in e.g. `System.IO`, instead of the odd places they're defined now (i.e. in the bytestring and text packages).
What is the correct term?
&gt; `String` is still great [...] for teaching purposes. 'Teaching purposes' also got us `foldl`, `head`, `fail` and `fmap`-rather-than-`map`. It's important to make Haskell accessible, but I'd prefer telling beginners "look, just use the magical phrase `import SimplePrelude`" (or similar) to burdening professional users with having to work around these warts.
I'm not strongly against that; I wouldn't pull out any hair if it's done. But I'm not in favor either. I don't mind occasionally answering the question "why is Haskell slower" when a newcomer compares a semantically more elegant but unoptimized Haskell program to an uglier but more optimized program in some other language. It should always be possible, and as easy as possible, to optimize Haskell programs and make them extremely fast. But I like the default to be semantic elegance - which is Haskell's real strength - rather than optimization.
Interestingly, that's what OCaml is going to call its corresponding type in the upcoming version 4.02, where they're splitting between immutable strings and mutable byte vectors.
Thanks for posting it; you get an upvote from me. I'm sorry I missed it last week when it went by on the café. But I disagree that using `String` is always a bad idea, even for file IO. I use it all the time for file IO; just not usually for serious applications. Frankly, the cute list representation of text is much more fun than the more C-like representation in `Text`.
Btw, the term `Bytes` is also used in Pythonland: https://docs.python.org/3/library/stdtypes.html#bytes
You know, it seems like we've got all the tools we need to fix this. We can override Prelude. We've got the libraries we need, and they've been in use for years and are stable. We've got a standardization committee. We've got people interested in fixing this. We've even got multiple stabs taken at the problem of creating a newer, better Prelude for multiple use cases, which are themselves in active use by some chunk of the community. What's stopping the Haskell committee from indicating that it is the sense of the committee that in a year or two, it would like to adopt a brand-new Prelude for the purpose of making it easier to learn Haskell and [whatever other purposes they'd like to focus on] and then start working towards a world where it truly becomes the standard Prelude? Why are we so stuck that the old Prelude is untouchable, despite the fact the community as a whole can darned near recite the Why Prelude Sucks (Especially For New Users) catechism in unison now? Is Haskell really so hidebound and sclerotic that this is just totally impossible? Or worse... _unthinkable_?
I'm completely unconvinced this even qualifies as semantic elegance. I do not see how using what is all-but-objectively the _wrong data type_, with wrong performance characteristics, wrong memory layout, wrong access pattern prices, and just generally the wrong thing is semantically elegant. Given that the semantics of `[Char]` are basically wrong for a text string, I do not see how they can be elegant. To be concrete, the mismatch is that when you have a "list", you generally have a list of independent elements of some sort. All of our list handling code optimizes for that case, operating on these elements one at a time with no memory (map), folding on them, extracting one element, etc. A text string, however, is something that you almost always want to operate on ranges, and functionalesque list-inspired APIs that allow you to do a true "map" operation that take one Char and return one Char for the new string verge on useless (especially in a Unicode world, where this API can't even do `toUpper`, and string manipulations don't get much simpler than that), you're always operating on ranges that are also themselves strings, and almost every operation will also grow or shrink the string itself as well, at least in potentia. Lists do not work that way. Lists of chars are not strings. They _do not_ have the same semantics, which is why `Text` is necessary; it is not merely that it can have a better representation under the hood, but that the _semantics_ of `Text` are not the semantics of a list. It would not be semantically "elegant" to remove the string datatype from Perl and instead give people hashes keyed by position in the string to "reduce the complexity in the language", no matter how cleverly the API was modified to cover over this change. And while `[Char]` isn't that bad... it's in that direction of badness.
If you can't upload those, you can find out why your build failed using these links. http://hackage.haskell.org/package/rest-core-0.31.1/reports http://hackage.haskell.org/package/rest-core-0.31.1/reports/1/log In this case the build is failing because aeson doesn't specify upper version bounds and cabal is choosing an older version. I solved it in snap with this constraint. aeson (&gt;= 0.6 &amp;&amp; &lt; 0.7) || (&gt;= 0.7.0.4 &amp;&amp; &lt; 0.8),
Eh. Pros can include `extensions: NoImplicitPrelude` and `import ProPrelude` in their templates for initializing their projects; it costs them nothing. Your tools should be pre-configured to flag uses of `foldl`, `head` and `fail`. Typing the extra `'f'` in `fmap` and the prime in `foldl'` just doesn't bother me. And even as a pro, I often do want to taste those candies of easy Haskell; just not in production code.
Currently I'm developing them in free because I need them to speed up free monads. I'm willing to discuss splitting it out into a separate free-categories package, but I want to get to where it works first. ;)
The only problem with that is that library writers are always going to want to use the standard prelude for compatibility reasons. Application writers are always free to use whatever prelude they like, therefore custom preludes should be specialised and the standard one should reflect the body of best practice that we all build on. So special prelude for beginners, with happy strings, and text for everyone else.
I tried the list with efficient (++) before, but the problem is that you still wind up paying to to reassociate as you walk down. The benefit of the catenable deque model is that the necessary reassociation happens en passant as you put the items into the deque to retain O(1) access to either end. When you just put in (++) you get the cheaper binds but at the cost that your access time to the outer constructor gets muddled.
I don't see how compatibility is an issue. The items mentioned in this thread don't require burdening your library with any dependency other than base, and possibly your pro-prelude package if you separate it out. We already have one or two "pro prelude" packages on hackage, and I don't see adding either as a huge burden. I already `import Prelude ()` in several of my codebases and `NoImplicitPrelude` is really no harder if you just *have* to avoid the typeclasses and instances defined there.
I'm living in fear for my productivity after of Hac Boston. I promised to allow John Wiegley to reconfigure my machine to use Nix then. ;)
I, too, would like to see this outlined explicitly, but the author elsewhere in the comments touts the automatic client generation as the biggest advantage. Automatic doc generation is the thing that interested me.
Beginners don't forget -- beginners don't know. When pros forget, they take professional responsibility seriously and fix the problem. The default should be good for instruction. 2-5 lines that are included in non-instructional code isn't that big a cost. That said, I'm not entirely convinced that we should be teaching things that must be unlearned later. It seems to me that there's a consensus that partial functions (like `head`) are bad and having calls to error in library code (like `fail`) is bad. There also a significant contingent that thinks that `foldl` (on cons lists) makes much more sense as accumulator-strict (like `foldl'`). Of course, I'd like input from people that do much more Haskell instruction than I, but it seems like the current Prelude isn't the best prelude for instruction either.
I'd probably make my own monad and evaluator, the evaluator can supply randomness and logging (or other things too). The logic written in your PokeMonad (oh, the puns) is still pure, and the evaluator can be ran in an IO-y world. You don't have to use transformers or layers on top of IO to do this. EDIT: A free monad is what I was thinking of.
Using some free monoid encoding is what make it semantically elegant in my mind.
My question is - why are we waiting for the committee? Committee processes are always slow. They are even slower when the committee members are volunteers holding down separate day jobs. You are right that we already have some alternate Preludes, more are easily possible, and there's no real barrier to using them - but people just aren't using them. So if no one really wants a changed Prelude - why go through the very painful process of changing it? Perhaps the main barrier to using an alternate Prelude is that you want community participation in your project, and you are afraid that some people won't feel comfortable with your code base. So the answer is: start with incremental changes to the Prelude. Start by using a Prelude with well-known uncontroversial improvements that everyone already uses and are familiar with. Then let's see how things evolve over time. If the community comes to a broad consensus about what the Prelude should look like by actually using that Prelude, the committees won't have any trouble making it canonical.
 type ShowS = String -&gt; String String gets a bad rep because * We all actually think like C programmers, making Text a better fit for how we want to push around characters. * We all focus on the conceptual "public face" of Haskell, rather than the crucial but hidden implementation details that affect performance. I rely on a ShowS-based parser. I've been meaning to recode it in Text as an experiment, but the old C programmer in me grits my teeth at the very thought, because of all the needless copying I'd be doing. Can anyone with extensive experience in both Haskell and C honestly say that they don't see a C library when they look at Text? It's as if we've given up on efficient string handling using a functional paradigm, so we've exposed a beautifully efficient implementation of "business as usual" for Haskell. For fine-grained slicing and dicing of strings, String is ideal as long as one systematically composes ShowS data. The problem is that all beginner sources run out of steam before properly teaching this, and the rest of us are too aware of C paradigms, so we embrace Text and ridicule String.
A: Defaults matter. It matters what Prelude is the default. When newbies encounter problems with the default Prelude, "use this other one" is not a bad thing to direct them to, but the newbies still have a fully legitimate point of their own. B: Without a sense that the committee will accept a new Prelude, and without a statement of goals they will be judging it by, I doubt anyone's going to spend effort building a Prelude that might actually be included. One that might be used by certain advanced users, sure (evidence: they already exist), but not a new one for the default. And without signoff by the committee, GHC and other Haskell compilers are not going to present it as the default. (And see A. Defaults matter.) So to get the defaults changed I imagine we need the committee to officially sign off, so, virtually by definition a new default Prelude requires committee support. &gt; So if no one really wants a changed Prelude - why go through the very painful process of changing it? I'd submit "Because it's holding back Haskell". For every newcomer who tries the Prelude and posts issues with it, you can bet dozens more simply leave quietly. And while that's probably the worst way it's holding back Haskell, it holds it back in other ways too... just recite the catechism again if you need the list of reasons ("Oh blessed Haskell Curry forgive us our sins, which are as follows: Partial functions, [Char] as strings, broken Num hierarchy, missing typeclasses on common functions, giving unto newbs bad paths to learn the truth, dangerous IO mechanisms...")
I don't believe so. That behavior doesn't happen on most other articles and links.
I disagree that `String` is wrong - in my opinion, it's right. Certainly much better than C strings. Better than Haskell's text library too. Performance characteristics? Memory layout? What is this, assembly language? Let the compiler worry about that. Text is semantically - and I mean denotational semantics, *not* operational semantics - an ordered sequence of characters, as mandated by the Unicode standard (see "Text Elements" in [Section 1.3 of the Core Specification](http://www.unicode.org/versions/Unicode6.2.0/ch01.pdf), and "Text Elements, Characters, and Text Processes" in [Section 2.1](http://www.unicode.org/versions/Unicode6.2.0/ch02.pdf)), and `[Char]` is absolutely the simplest and most elegant way of expressing those semantics. So that's what I want. Now in practice, in the compiled object code, operational semantics matter, too. And in some cases we don't have techniques yet to generate the operational semantics we want automatically from the best possible denotation, or it may even be provably impossible to do so in general. In that case, we have a technique: it's called "optimization". Haskell happens to be quite good at that. But for the default Prelude, I want absolutely the simplest and most elegant denotational semantics. Period. That is Haskell's strength and value over other languages. If it's anything else, I have a name for that, too: "premature optimization". Don't do that. Well. After that passionate plea, it may happen that some very commonly used optimization, like `Text`, might actually make it into our next Prelude. If so, I'll be forced to admit blushingly that I, too, use it all the time anyway, and you won't hear me complain too loudly. But it's not what I'm voting for.
`foldl'` is basically always better than `foldl` (iirc) `head` isnt total `fail` is a mess that shouldn't be in `Monad` (rather some `MonadFailure` typeclass somewhere) because it just clutters the otherwise-clean `Monad` class `map` used to work on `Functor`s but was changed because the typeclass errors were a pain for people who were new and just trying to `map` a `[]`, but the errors have since got less unwieldy so it's less useful now
Monad transformers are one solution to this problem. I believe the type you are looking for is `battle :: RandT g (Writer [String]) a`. You then have to change the type of `getRandomMove :: (Monad m) =&gt; RandT g m Move` or even just `getRandomMove :: (MonadRandom m) =&gt; m Move`. Because `tell ["blah"]` has the type of `Writer [String] ()` and not `RandT g (Writer [String]) ()`, you must use the function `lift` from Control.Monad.Trans.Class to embed it into your new transformed monad. The above should work but for this problem I would create a new type that combines the Writer and RandT functionality. In the long run, it could make the code easier to maintain. One way to do this is by defining a newtype around the above-described transformer stack, and using GeneralizedNewtypeDeriving to automatically derive Monad and other instances that you need. I think this is also a good candidate for a free monad solution but you specifically asked about transformers, and this is how you do it with transformers. **EDIT:** I am referring to the `transformers` package. `mtl` defines functions with the same names, but different types. Many people find `mtl` convenient because it uses a typeclass approach so that functions are polymorphic over different monad transformer stacks. I described `transformers` above because I don't think it is a good idea to try the typeclass approach until you are comfortable with the concrete types. Sorry for the confusion.
&gt; `fail` is a mess that shouldn't be in `Monad` (rather some `MonadFailure` typeclass somewhere) because it just clutters the otherwise-clean `Monad` class The problem it solves (pattern match failures in do-notation) [used to be solved using `MonadZero`](https://stackoverflow.com/questions/18761302/why-were-haskell-98s-standard-classes-made-inferior-to-haskell-1-3s) instead.
Now I remember hearing about aeson in particular here. Thanks for the links, i was trying to get to those urls but failed somehow. This can actually be fixed properly now by publishing a new revision of the cabal file, I created an issue for it https://github.com/bos/aeson/issues/212 
To clarify, it's only `foldl` that's bad, right? What does "total" mean with respect to `head`?
 head (x:_) = x `head` is not defined for some inputs something like `maybeHead` would be though: maybeHead (x:_) = Just x maybeHead _ = Nothing
&gt; Performance characteristics? Memory layout? What is this, assembly language? It's a language the community would like to propose to people investigating Haskell is a "fast language". Maybe not at every point C-speed, but a language that can be made fast. Evidence: Many of the people encountering Haskell and who encounter these problems with [Char] _directly complain_ in one form or another that the community claims Haskell is fast but the first thing they did runs at sub-Ruby speeds (and sometimes as a bonus fails due to lazy IO). You don't get to claim Haskell is fast to people and then turn around and pay _no_ attention to these matters. I'm not suggesting that they are the dominant problem, so one must pay attention to the balance. Unfortunately, in this case, the cost/benefits for `[Char]` plop an _enormous_ chunk of cost on the cost side, so even though I'm considering this in a balanced fashion, the balance is so lopsided that it may _appear_ I'm being dogmatic about "performance uber alles". I'm not. It's just the costs are so huge, human and computer, that it dominates everything else. &gt; and I mean denotational semantics, not operational semantics When theory and reality collide, I unapologetically side on reality's side. We do not use strings like we use lists. We can not easily perform trivially desired string manipulations with map, filter, and fold. We can perform them in theory, particularly with fold, but it is grossly less elegant than an actual string manipulation library. Write a fold on [Char] for the equivalent of `=~ /http(s)?:\/\/[a-z]+(\.[a-z])+/`. And that's merely "medium" complexity for strings, really. Were it closer, I might still side on your side, but in this case, it isn't close. The desired operations in the real world are _grossly_ different for Lists and Strings. They barely even overlap. Also, I'm going to stop here. I'm not trying to get into a flamewar and I believe this concludes my points. I say this without rancor in the spirit of politeness to the community.
I wouldn't mind knowing the free monad solution. Transformers were just my first instinct since I know that they are what you want when dealing with combined monads. I have read in those Transformers tutorials about building higher monads for the Transformer itself, but I didn't understand what they meant. Now I think I do. Thanks!
&gt;if you have any sort of interleaving between multiple IO resources, the initial simplicity of Lazy IO can vanish in the blink of an eye. That is because lazy IO was never simple in the first place. Strict IO is much simpler, and works for most of the same "teaching purposes" use cases. Lazy IO strikes me as gimmicky, it is only useful in certain, very simple scenarios.
Hadn't thought about building my own around the Trans. Thanks!
Of all the things that are actually wrong with Haskell, I really don't think that a newbie getting bad performance is all that worrisome.
You're welcome. If you want to learn more about transformers I recommend the paper [Monad Transformers Step-by-Step](http://www.grabmueller.de/martin/www/pub/Transformers.pdf). Note that many users seem to think that monad transformers are not a perfect model for combining effects, however they do have their conveniences (especially with GeneralizedNewtypeDeriving). If you want to learn about free monads I recommend Tekmo's [Why free monads matter](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html). The basic idea is that your monad is a pure type representing commands such as "get a random value" and "write a string", and at another stage that data is consumed by an interpreter that actually carries out those instructions in IO. I believe it is a superior solution for many problems but it took a little longer for me to learn than the monad transformer approach.
`main :: IO (IO ())` is not what you want. Think about what this means. Try main :: IO () main = do [f, g] &lt;- getArgs input &lt;- readFile f let output = unlines . map flatten . countUnique . ipOnly . spammers $ input writeFile g output (Untested.)
Maybe attach a sample django traceback output contents so that people can write alternatives easily; and provide suggestions.
It might be a bit early for Control.Applicative for you --- focus on getting it working with plain old do-syntax, first. Like so: main :: IO () main = do [f, g] &lt;- getArgs contents &lt;- readFile f writeFile g (( unlines . map flatten . countUnique . ipOnly . spammers) contents) As for what's going wrong: you're using infix fmap --- i.e., &lt;$&gt; --- where you ought to be using bind (that is, &gt;&gt;=). If you change your first `&lt;$&gt;` to a `=&lt;&lt;` (bind with the arguments flipped), it will work fine.
This is pretty cool! TL;DR: From the well-known result: `[a]` has a fast view that gives you the head and tail of the list, but slow `++` with the wrong association. `DList a` has fast `++` but slow `view`. `Data.Sequence` gives the best of both worlds at the cost of implementational complexity. Analogously, in monadic applications, `Free` has fast `view`, but slow `&gt;&gt;=` with the wrong association. `Prompt` uses the codensity transformation to fix that, but loses the ability to quickly inspect the first instruction in the sequence without observing the whole sequence. (See also ["Free monads for less"](http://comonad.com/reader/2011/free-monads-for-less/)). This paper fills in the equivalent of `Sequence` for monadic types, and points to other applications of type-directed sequences.
I should amend my statement to mean "file I/O in resource demanding situations", or something to that effect. I love String too!
The problem is that your `main` is of type `IO (IO ())` which is an `IO` action resulting yet another `IO` action. This is analogous to a side-effectful function returning a function that describes what your program is supposed to do in your typical `main` from other languages: it won't do anything the returned function does. There are two ways to solve this: the most obvious way is to use `=&lt;&lt; :: (a -&gt; IO b) -&gt; IO a -&gt; IO b`^[1], which runs the right-hand side and feeds the result to the left-hand side as an argument, i.e.: `writeFile g =&lt;&lt; fn &lt;$&gt; readFile f`. Note that this is exactly the same thing that the following `do` notation desugars into: do x &lt;- fn &lt;$&gt; readFile f writeFIle g x The other one is `join :: IO (IO a) -&gt; IO a`, which collapses nested `IO` computation together: i.e. `join $ writeFile g &lt;$&gt; fn &lt;$&gt; readFile`. In fact, these two must equivalent for any instances of `Monad`s given by: join m = m &gt;&gt;= id f =&lt;&lt; m = join $ fmap f m ^[1] `Monad` constraint specialized with `IO` for clarity. EDITED: Some suggestions for the code as I skim through: * `IpAddress` should be a `newtype`, and you can use record syntax to get the `getAddress` selector for free: `newtype IpAddress = IpAddress { getAddress :: String } deriving (Eq, Ord, Show)` * ``cleanIP = not . (`elem` "\",'")`` * `deLeadingZero = dropWhile (== '0')` * In general, people like to convert `String`s into better data types (e.g. `data IP4 = IP4 Word8 Word8 Word8 Word8`) rather than operating on `String`s themselves directly which is more error-phone.
Others have pointed out that your `main` is of the wrong type, so I'll leave that to them, but here's some other thoughts: One thing that's true about your program - and almost all command-line utilities - is that it reads the contents of a file, does something pure with the contents, and then writes out the result. This is a great use case for [`interact`](http://www.haskell.org/hoogle/?hoogle=interact). It takes a function `:: String -&gt; String` and returns an `IO ()`, so you can easily specify `main = interact doCoolStuff`. One difference between this and your program is that it uses stdin and stdout, rather than filenames specified as arguments. This is more UNIXy, of course, allowing everything to be piped around is nice. But if you did want to write `./Spammer a b` rather than `./Spammer &lt;a &gt;b`, then you could write your own equivalent of `interact` which does so. One of the benefits of doing this is that it's a lot easier to unit test - your program would essentially consist of a small pure core, wrapped in some file logic, and you could programmatically test just the pure bit fairly easily. 
I never understood why some people are so picky about `head` and `tail`. Is, it is a partial function on finite lists but so is `fromJust` on Maybe types; should the later be deprecated too? And what about general recursion, should we restrict to primitive recursion or evolve Haskell imto to a fully depent type language like Agda or Coq so that we can express precisely the the function domain in types?
Yes. Beginners are a temporary thing, experienced Haskellers remain experienced forever. Catering to the former at the expense of the latter leads to the problems Rhymoid mentioned (experts have to work around the bad defaults, and newbies experience the bad defaults and go with a bad taste in their mouth of Haskell) which yitz disregarded as non-issues.
This description is helpful. I think notation such as `=&lt;&lt;` can be a bit off-putting at first. That, in addition, to failing to understand `return` in this context can also be tough for a beginner. Thanks for the explanation: I think I can see how I can use this. 
Yes, ByteVector would've been better.
The type `[Char]` isn't crufty, but the pervasive use of it in a modern language's core facilities ranging from environment variables to piping processes to filenames to IO (and thus also to thousands of libraries written in terms of them), is what is crufty. Were the situation reversed to another language that I was unfamiliar with, I'd almost certainly laugh at that toy language. As it happens, I like Haskell a lot so I put up with its warts, but they _are_ warts.
Text is pure and functional and fuses. I have no idea where this comparison to “C” comes from. It's nothing like string handling in C.
I actually did see `interact` and initially had a version of it working that way. It seemed somehow more "complete" to make it accept command-line arguments, but maybe that was a poor assumption on my part.
edwardkmett doesn't have an answer.
Here's a sample Traceback (truncated request.META and other variables, but I think you get the idea): Traceback (most recent call last): File "/opt/python-env/django/core/handlers/base.py", line 111, in get_response response = callback(request, *callback_args, **callback_kwargs) File "/opt/python-env/django/utils/decorators.py", line 93, in _wrapped_view response = view_func(request, *args, **kwargs) File "/opt/python-env/django/views/decorators/cache.py", line 79, in _wrapped_view_func response = view_func(request, *args, **kwargs) File "/opt/python-env/django/contrib/auth/views.py", line 31, in login redirect_to = request.REQUEST.get(redirect_field_name, '') File "/opt/python-env/django/core/handlers/wsgi.py", line 196, in _get_request self._request = datastructures.MergeDict(self.POST, self.GET) File "/opt/python-env/django/core/handlers/wsgi.py", line 210, in _get_post self._load_post_and_files() File "/opt/python-env/django/http/__init__.py", line 295, in _load_post_and_files self._post, self._files = QueryDict(self.raw_post_data, encoding=self._encoding), MultiValueDict() File "/opt/python-env/django/http/__init__.py", line 255, in _get_raw_post_data self._raw_post_data = self.read(content_length) File "/opt/python-env/django/http/__init__.py", line 307, in read return self._stream.read(*args, **kwargs) IOError: request data read error &lt;WSGIRequest GET:&lt;QueryDict: {}&gt;, POST:&lt;QueryDict: {}&gt;, COOKIES:{}, META:{'CONTENT_LENGTH': '743', 'CONTENT_TYPE': 'application/x-www-form-urlencoded', 'DOCUMENT_ROOT': '/var/www/static/media/', 'GATEWAY_INTERFACE': 'CGI/1.1', 'HTTP_ACCEPT': 'image/gif, image/x-xbitmap, image/jpeg, image/pjpeg, application/x-shockwave-flash, */*', 'HTTP_ACCEPT_ENCODING': 'gzip', 'HTTP_COOKIE': '', 'HTTP_HOST': 'www.website.com.com', 'HTTP_REFERER': 'http://www.website.com/resource/path', 'HTTP_USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; Mozilla/4.0 (compatible; MSIE 6.0; Windows)' 'HTTP_X_FORWARDED_FOR': '27.159.219.159', 'PATH': '/usr/local/bin:/bin:/usr/bin', 'QUERY_STRING': '', 'REMOTE_ADDR': '27.159.219.159', 'REQUEST_METHOD': 'POST', 'SCRIPT_NAME': u'', 'SERVER_PORT': '80', 'SERVER_PROTOCOL': 'HTTP/1.1', 'SERVER_SIGNATURE': '', 'SERVER_SOFTWARE': 'Apache',&gt; 
Unfortunately I really can't recommend the purchase of this book to anyone. Neither beginners nor advanced users. In fact I don't understand what the intended audience of the book is supposed to be. There isn't much to be found for advanced users. Beginners on the other end will be lost with just the short code snippets given in each of the recipes. While some of the recipes might at least have some use, the worst are nothing more but: skein512_256 :: ByteString -&gt; Digest Skein512_256 skein512_256 bs = hash bs Beginners won't actually /learn/ anything, intermediate to advanced haskellers will be bored.
The very [same link](http://www.reddit.com/r/haskell/comments/296l80/today_i_published_an_introductory_book_on_haskell) has been posted recently in this channel.
Another way to make head total is to restrict it to NonEmpty.
I agree. Why should natural language text be a linked-list of characters rather than any other datatype containing a sequence of characters? 
That's a good point, actually. 
While I think I agree with both, it's a much stronger yes to the first than the second.
&gt; yet again, perpetuating the serious mistake of using ByteString instead of Text for strings. This is not clear. Look at the Perl reference implementation. It's counting bytes, not unicode characters. Arguably the Perl is wrong but that's a different issue. If the improved Haskell version had used Text it would have been slower because of the UTF decoding.
These suggestions are helpful. Is `Word8` part of Prelude or does it need to be imported through Data.Text or some other library?
The biggest problem with `head` and all other partial functions, is that Haskell has no stack traces. So you might know that a list will 'never' be empty, but it will be at some point in production, and you'll just get "head: empty list" and nothing else. At that point you will eradicate all uses of `head` from your program and vow to never use a partial function again.
I have been rather disappointed so far for the same obvious reason exposed by SirRockALot1. I also don't see what audience this book is for. It is not for advanced Haskeller, will probably disappointed many intermediate and I doubt it will please many newcomers. Much libraries are covered lightly with a contrived example such a `diagrams` or not at all such a `pipes`. I have the feeling that the materials does not cover much of the current haskell libraries cosmos. It does not seem to succeed giving a snapshot picture of the current situation. Then I have been surprised by the use of `foldl` everywhere. Shouldn't it be avoided in all cases but demos. There are some annoying spelling mistakes that makes you wonder how the book has been reviewed. Anyhow, I was expecting something a bit more inspiring. To be fair, I have only skimmed the book at this stage so I might be completely off.
&gt; Lazy IO strikes me as gimmicky But such a *cool* gimmick
Indeed, and some implementations have stack traces.
Apologies, it's a way of performing substitutions in Perl http://www.comp.leeds.ac.uk/Perl/sandtr.html that some use in 'normal conversation' as a way to indicate corrections.
I don't think anyone has ever accused Haskell of catering to beginners.
Just FYI, this function exists and is called `headMay`. It's exported from the `Safe` module, and is also exported from `classy-prelude` (that's how I got used to it). I use it all the time, and use `fromJust` when I purposefully want a partial function; `fromJust` is clearly partial, though, and I mentally categorize it as "dangerous", whereas with "head" I didn't. http://hackage.haskell.org/package/Safe-0.1/docs/Safe.html#v:headMay
See for instance the [safe](https://hackage.haskell.org/package/safe) package, which conveniently packages many of them up for you.
So String is part of the default Prelude, together with a bunch of other things that experienced haskellers don't like. Is there some more reasonable Prelude out there, and some way of defaulting to that instead? 
It's also in `base` as `listToMaybe`: http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Maybe.html#v:listToMaybe.
&gt; proof that your program will terminate or that a coprogram will be productive (services/servers/streaming fall under this) Most programs these days operate on codata, so termination on a per-destructed-codata-component basis is productivity if I understand correctly. (ejenk touches on this in the comments as well, but I wanted to add this)
I think most people who struggle with monad transformers have issues because they try to learn how to use `mtl` before first understanding how the underlying `transformers` library works. I would write out a concrete monad transformer stack using nothing but abstractions from the `transformers` library (i.e. no type classes except the `MonadTrans` type class) and use explicit `lift`s everywhere until you feel comfortable with monad transformers.
&gt; Text is semantically - and I mean denotational semantics, not operational semantics - an ordered sequence of characters, as mandated by the Unicode standard It's a bit more complicated than that, what with glyphs and all. The standard example is `reverse "lëon"` yielding `"nöel"` due to `¨` as a combining char. (Though don't copy and paste my example, I've used something like "e with diaeresis" rather than the combining char.) I think I heard something about `Text` being better at glyphs, though I don't have any source on hand and am too lazy to search for one. :)
I'm not as convinced that it's really holding back Haskell so much, but I'm in favor of moving forward the Prelude. But I really think you've got it backwards. You can beat the drum for committees to approve a new Prelude, but it's going to take a very long time for that to work, if ever. The Prelude is a very sensitive and opinionated topic. There will be endless bikeshedding and arguments. People with authority will lose patience, run out of time, and disappear from the scene, and then there will be a whole process to replace them. Etc. Whereas if you vote with your feet, it will happen. Just go ahead and use an alternative Prelude for your next library Yes, right now! Pick one - or write one - that you think is a reasonable step forward, right now. Lightweight, not a heavy extra dependency. Design choices that are not too controversial. Ask around, find out what other people think. And then: beat your drum about your choice of Prelude; get others to join you, create a *de facto* consensus.
Indeed. `getContents` and `interact` are very cool gimmicks that can often be used to great effect for toy programs or some coding contests.
The community doesn't come to a broad consensus on its own. You get shitty fragmentation and everything is a cabal hell dependency mess. that's what happened with transformers/mtl. Much better to shepherd the changes through in a controlled way. We have a good libraries committee now, who have been chugging away, and have in fact already given us the AMP proposal in a future release. If I release a new library, I'd much rather rely on the existing prelude than give my users some other ridiculous dependency to rely on which, given the history of all alternative preludes to date will either wither away and leave them in the lurch, or evolve semi-randomly and create further fragmentation.
You're totally right. I'll have to change that. It'll be good exercise. 
Good point. And there are some even in recent GHCs, with `+RTS -xc`, but that needs the program to be compiled with profiling.
I don't follow. Let's consider an expression of the form let e = ((…((m1 &gt;&gt;= k1) &gt;&gt;= k2) &gt;&gt;= …) &gt;&gt;= kn) My thinking is that accessing the first the first instruction, `m1`, will take at least `n` steps, regardless of how the monad is implemented. After all, the first instruction is not discernible unless you have "touched" all the binds. What does the catenable dequeue implementation add here? 
As /u/bergmark mentioned, we don't have anything currently. But I'd love to include something (for authorization in particular, not authentication), since that would mean we could automatically document the authorization required for different operations on different resources.
I can't think of a distribution where it is easier to package new software.
So if I divide by zero in a dependently typed language, what happens? ~~Won't there still be a run-time error somewhere?~~ Won't there still need to be a check at run-time to account for data that might not be known at compile-time? I understand that this idea of restricting/defining types based on others allows for more generality in situations like this, but I don't see how it really solves the problem of dividing by zero. Or am I missing something? EDITED
Yes, automatic generation of docs and clients is a big win, as mentioned elsewhere. The structure also makes sure you're defining rest-like APIs with a consistent interface and gives you some correctness guarantees regarding routing and such, but those are secondary benefits. 
I believe that can be included in the type information for a compile time error. Any number you are going to be dividing by can be specified as not equal to zero in the argument type.
It's impossible for you to divide by zero in a (well-built) dependently typed language; in order to divide two numbers, you first have to provide a proof that the denominator is non-zero.
Alright, I am always curious about these posts. I'm an American with a bachelor's degree, I majored in math(s) and Computer Science. I did some minor undergraduate research related to complex analysis. I'm two years out of school. Do I have a chance of being qualified for these positions (especially as an American without a Master's degree)? 
You should expand on this in a blog post. I don't understand what you mean. What is deadlocked? Nixpkgs could be refactored, but compare to the incredible legacy cruft in say Debian, it's heaven. NixOS HAS a beautiful testing story even if there are very few tests currently. What other distribution has tests for packages by declaratively spinning up virtual machines?
A program can't divide by zero, assuming the division operator prevents 0 as a value for its second operation. So then the question becomes "Why is this true?" In order to even use the division operation, you need to prove the second argument is non-zero at compile time. For constants, the proof is trivial. For numbers that come from other places, this is a slightly trickier process. You can think of it as the division operation having a hidden third argument, which is a proof object that proves the second argument isn't zero. The compiler will enforce that any place that calls the division operation must provide a proof. If you need to handle something like user input, you can write a function that examine the input and certify that it's not zero. (That function will have to have some way to handle the situation if it *is* zero.) This leads to a lot of potential complexity in code. I'm not convinced dependent types are really the way of the future yet - it's possible that they end up forcing you to write code that's too coupled to be easily maintainable. But the idea is worth exploring further, and lots of efforts are being made to do so.
Can that always realistically be caught at compile time though? What if outside data is being read in for some calculations? A check can still be performed that would prevent a program-crashing error to occur but there still has to be some kind of context like a Maybe to account for that, which the author didn't seem too happy with.
`Word8` is defined in `Data.Word`.
What happens if the proof fails? Would it return some sort of Maybe? If user input is involved then the failure can't be caught at compile-time. I'm just trying to understand why this is better than using a guard and returning a Maybe; the author was against that but for the dependent typing.
This reminds me of something I've been struggling with lately, and it's the fact that things like Haskell and Idris scare most programmers when they should really have the opposite reaction. The conclusion that I've come to is that most programmers are not engineers. Engineers (by my definition) are people that apply scientific principles to practical problems. Most programmers don't seem to want to think like engineers. They don't want to "deal with all that math stuff". Where as an engineer's reaction should be one of excitement for (or at least interest in) a new tool to better solve their problems. When I ask someone why they like Ruby, I usually get an answer like, "Because I was able to build a website without knowing anything about programming." This equates to someone building a shack out of twigs and hay and then expecting to be able to build a skyscrapers using the same materials and techniques. I think this shows how young computer science is as a science and how it could possibly take at least another 100 years before it reaches the state of other sciences.
I believe the program fails to compile if the proof fails. It is a compile time error, not a run time error.
If the data being divided is from an external source, then it can't be known at compile-time. A simplified example would be b &lt;- liftM read $ getLine print $ a / b Ignoring any parse errors for now, b can't be verified to be nonzero until run-time
&gt; What if outside data is being read in for some calculations? Well, it's not because you have dependent types in your language that all your user inputs are magically sanitized: you do need to write code checking that everything you load is ok. The plus side of dependent types is that the type system can track down whether these values have been sanitized or not and complain whenever you have forgotten to do it; or you now have a mismatch between the checks you implemented a long time ago and the assumption your recently refactored code relies on, etc. For instance: * When you work with files, you can make sure that the handle you have a hold of is compatible with reading / writing, is closed at the end of the manipulation, etc. [Cf. Idris' Effects library](http://www.idris-lang.org/documentation/effects/) * Prove that your compiler is semantics preserving. [Cf. the Compcert project](http://compcert.inria.fr/) * Reason about cryptographic protocols and make sure that implementations match various security criterion. [Cf. Pouillards' crypto-agda](https://events.ccc.de/congress/2013/Fahrplan/events/5394.html) Etc., etc. There are tons of potential applications.
I can't speak specifically about you without seeing your CV, but in general, being an American applying to a UK university can make things a little trickier (due to various restrictions on funding), but having some development experience definitely doesn't hurt. 
Awesome. Thanks for replying. I've always thought I shot myself in the foot by not being more proactive as an undergrad. I'll try to get a CV updated and sent over if you have time to take a look. 
I would guess that the answer is that you simply can't prove b to be nonzero in this case, thus you would not be able to compile your program.
 foldr :: (a' -&gt; b' -&gt; b') -&gt; b' -&gt; [a'] -&gt; b' In the use of `foldr` within `foo`, what are `a'` and `b'`? Prove it by induction?
You'd have to have the check that b is nonzero, and provide some other code path for the zero case. Maybe is fine, or whatever else you want. The point is, you can't pass the compiler without thinking about the problem. You're right, you can't know the value of b until runtime in that case, but / requires b be nonzero as a precondition for that operator. There is no "slipping a divide by zero past" something has to show that b is indeed nonzero to allow / to be called. Put it another way, stuff that you have to remember all the time can be stuffed into the type system, so the compiler can have your back. 
I see, so the author is looking forward to a possible solution through dependent typing in the future, not necessarily something that can be solved right now. 
You want Control.Applicative [(+1)] &lt;*&gt; [1, 2, 3]
It's unfortunate that more work hadn't been done on lambda calculus prior to the invention of the computer. We might have been further along than we are now. I'm not even in a computer science program (though I'm strongly considering it) and I find Haskell and FP exciting in general. I can't wrap my head around half of it since I started with imperative languages, but I'm getting there.
I maintain nearly 60 packages, and they work quite fine. The rate of contributors, committers, and users has only been increasing in the past several months since I started using NixOS in anger in production and development. So I'm not exactly sure where the claim it will be dead in months is getting any weight from, other than your ass - and as a committer myself, it brings into question your claim you were a years-long contributor, unless you consciously ignored this trend entirely. Which, to be fair, is probably entirely possible if you're not actively doing things like merging changes from people. Or if you were contributing years ago, and not recently. But considering I do this regularly, I can say the rate of changes and new contributors is steadily increasing every day. This would be the categorical opposite of "grinding to a halt" and "dead project".
Yeah, you beat me to it.
I think the author was using it to introduce functors as a concept, not just a basic map function.
My day job is PHP web dev and I cannot tell you how tired I am of it. Imagine my frustration when I realized I was accidentally assigning a whole active record to an integer ID field instead of the record's ID and PHP and the framework both just let it go. I was debugging that shit for **3 hours**. I scanned past the problematic line probably 20 times, because it looked just fine at a glance. While that's more of an argument for strong static typing than FP, both would make a lot fewer headaches for me. PHP has *some* FP features but they're so detached from the syntax and most often slower than the imperative approaches so no one bothers. The worst part is, all the devs on our team don't see the problem. These guys spent 6 months building our admin site from fucking scratch, vanilla PHP, and it's still broke in most places. I suggested Happstack for our high-traffic server and they turned it down because they didn't want to learn a new language. I'm surrounded by peasants.
Ah sorry, when I said current languages, I meant current widely used and non-dependently typed languages. I imagine that the version of the program with the check that b is non-zero, and that being used in a proof of the validity of the division, can probably be written today in a language like Idris.
&gt; so is fromJust on Maybe type But fromJust is buried in Data.Maybe &gt; head [1] 1 &gt; fromJust $ Just 1 Not in scope: `fromJust` &gt; And what about general recursion, should we restrict to primitive recursion Haskell has partial functions to allow the compiler to not have to prove a given function is total. Saying that we should have a Prelude partial function because we have that release valve available is a stretch.
You can do some check at the time you're reading b. Say, if b is zero, print an error message and terminate the program. Then after that point on, you know b is non-zero, and in a dependently typed language, the variable containing b can be of a type that reflects its non-zero status. Then you can do arithmetic on it and as long as the arithmetic preserves the non-zero-ness, everything will type-check. As soon as you do some arithmetic that might result in a zero value, the typechecker will complain and the program will not compile. These are techniques also used in formal verification of software.
The program wouldn't even compile if there's a possibility the denominator can be 0. You can imagine it something like this: normal_int a, b; float result; a = read_int(); b = read_int(); result = a/b; This program will not compile, because the division operator expects a `nonzero_int` value in the denominator. If we change our program to say, normal_int a; nonzero_int b; float result; a = read_int(); b = read_int(); result = a/b; the division operator is fixed, but again this program will not compile, because `read_int` returns a `normal_int`, not a `nonzero_int`. So we do something like normal_int a, c; nonzero_int b; float result; a = read_int(); do { c = read_int(); } until (c != 0); b = c; // at this point in the program, we can formally prove that b can not be zero. // it is at *this* point the dependent types come in handy for // guaranteeing that whatever you do with b from now on, it's // not going to accidentally become 0 again. If that possibility exists, // the program won't typecheck once again. result = a/b; Idris is probably not capable of doing this proof on it's own, but that's the principle. You provide an alternative codepath if the value you read is zero (in this case by an infinite loop) and then on the main code path you can guarantee the value is not zero.
The approach you'd take would probably vary depending on what other kinds of operations you want to do. Of course, *some* dynamic checks might be required, but it's almost never a problem to do limited dynamic checks. What is a problem is when you have to guard every division operation by a dynamic check. Dependent types can allow you to get evidence of non-zero status via a dynamic check outside of a performance-critical path and prove that it is preserved by the operations you need to do.
My strongest language is Java and I still love static typing. I do agree that Java went a little overboard with type annotations and the strict OO can be annoying but damn if you don't know what the type of everything is at a glance. It seems like the IDEs for static languages are a lot smarter, too. Java seems to be improving, though, especially with 8. At least you don't need type annotations for lambda parameters, and there's some [lesser-known type inference in 7][1]: ArrayList&lt;Record&gt; list = new ArrayList&lt;&gt;(); That can save a lot of typing with longer class names, which Java is notorious for. Unfortunately it's not available on Android, where I got most of my Java experience, since Android targets Java 6. Type inference is the shit. [1]: http://docs.oracle.com/javase/7/docs/technotes/guides/language/type-inference-generic-instance-creation.html 
The other comments are much more detailed, but ultimately what it boils down to is that the compiler will require the programmer to handle the divide-by-zero case *somewhere*. 
&gt; Engineers (by my definition) are people that apply scientific principles to practical problems. That is a weird definition. My definition is that an engineer is someone who can design an acceptable solution to a practical problem. The definition of "acceptable" may be very different, for different tasks, and the methods applied may be different as well. If someone is able to design good and relatively reliable things quickly without any math or science, that person is a good engineer. If someone has managed to apply math to a practical problem, but then the customer ends up not using that "wonderful" design, that person is not a good engineer.
The proof can't fail, it's an compile-time obligation to ensure that the user input logic can't yield a zero value to the function, otherwise it's a type error.
&gt; design good and relatively reliable things quickly without any math or science, That's a designer, not an engineer, IMO. Designers cater to aesthetics, engineers cater to functionality.
You have encapsulated a lot of my thought on this. I love working in Python, but my greatest goal is to produce code that can be left safely alone. I am self-taught and don't see myself as an engineer (even when interviewing for positions with "engineer" in the title). I was drawn to Haskell because of the idea of code correctness: I want to be able to confirm it does what I think it does. There's a certain subtlety in Haskell as well that appeals to me, but I've just started learning it.
Yeah, I did use Java in university 6-7 years ago so I'm not just an ignorant hater - I actually enjoyed it at the time (never coded w/ it full-time though). You almost have to have an IDE though when you have so much boilerplate to generate (getters and setters for all your instance variables for example). It was really awesome to have in-line static typing errors - would have definitely saved me from a lot of Ruby bugs.
I've seen people who can design functional software without giving too much thought to any kind of science. Understanding the needs of the customers, and a bit of common sense, is mostly enough for a software developer. There are certain areas where math is needed, true. Having a grasp of logic, science, and algorithm complexity can make one a better developer, certainly. But I would not go as far as to dismiss anyone who sits outside of my ivory tower as non-engineers. *Edit: I know, I know. This view is unpopular over here, in the land of perfectionists. I am a perfectionist myself, I like science, functional programming, and dependent types. But really, people, dismissing an approach to engineering just because it is "unscientific" is just plain arrogant. Math is a tool. Dependent types is just a tool. Functional programming is just a tool. Compilers are meant to make the life of developers easy. Developers are the customers of those who develop compilers. If a particular developer is telling you that they do not like your tool, you can explain how to use the tool, or you can fix the tool. What I am seeing so far in this thread, is "Hey, that customer is bad, let's punch them in the face!". A round of applause follows, the crowd is cheering: "Bad, bad customer! Utterly unscientific! Totally blasphemous!". This kind of thinking is putting people off.*
&gt; However the author did mention that you might check for 0, but that current languages could not take advantage of this. Note that when you'd use a Maybe monad you don't need to know the branching paths, just that everything is correct or that it can fail safely otherwise. 
&gt; What would happen if the second number were a 0? Would the function silently fail? Would it throw some kind of error? Would it filter away the 0 and wait for another number to be entered? Divide's type would look something like : divide :: Num a =&gt; (m n : a) -&gt; n &lt;&gt; 0 -&gt; a Now, if you "read in" two numbers, you'll only get two `a`s where `a` is a `Num` type. You still have to check that the second one is non-zero and decide what to do if it isn't. This decision is part of your application's logic and has nothing to do with the type system.
This looks neat. However, I couldn't help noticing that the "sum two numbers" example tells me that 1+2=2.
I don't remember ever going without an IDE except to help a friend going through a CS 100 course teaching Java where they started with notepad and CLI. I shudder to think about it. I can't even conceive of doing anything Java related without at least IntelliJ IDEA. I actually picked up a trial of PHPStorm for work and I'm liking it so far. It doesn't completely eliminate the headache of PHP but it reduces it from a sharp stabbing pain to a dull throb. Build tools like Maven or Gradle are the cherry on top but I don't bother with them for small projects, especially ones without external dependencies. 
I think there's room for both, and many more in this world. One doesn't need to be good at math to be a good programmer although for functional languages it certainly helps. Simply because you use math to think about your code doesn't make you an engineer, it may make you better programmer--or not. It's really up to the individual and her aptitude to code and incorporate diverse ideas and judgement calls into her code. I've always felt that programmers who refer to themselves as engineers or computer scientists are just to varying degrees pretentious. Most corporate programmers aren't generally any of those. Quite honestly, "programmer" describes quite succinctly what we do professionally whether it's in Agda, Haskell, Ruby, Scala, etc. &gt; this shows how young computer science is as a science and how it could possibly take at least another 100 years before it reaches the state of other sciences You're conflating practice of programming, i.e., people who write code, with the academic field of computer science. 
If the monad laws are respected, then you should be able to rewrite that expression at compile time to be: m1 &gt;&gt;= (\x1 -&gt; k1 x1 &gt;&gt;= (\x2 -&gt; k2 x2 &gt;&gt;= (... ... &gt;&gt;= (\xn -&gt; kn xn)...))) &gt; accessing the first the first instruction, m1, will take at least n steps, regardless of how the monad is implemented Nah, rewrite rules can fire for specific monad instances. Of course this only happens if you compile with optimizations. Check out the rewrite rules for `pipes`, for example: http://hackage.haskell.org/package/pipes-4.1.2/docs/src/Pipes-Internal.html
Basically the trick is you make the denominator a type that isn't inhabited by zero. Think: data Denominator = One | Succ Denominator Except dependently typed programming languages let you express this with dependencies in types instead of having to roll new types for everything all the time. Similar to how type parameters keep you from having to make new types for "list of ints" and "list of chars" because they're both `forall a. [a]`.
There was an unbearably long thread on cafe about numbers and equality. This is a very subtle in-joke.
We've known we don't want these functions for like 12 years now though, is there any hope of a backwards-incompatible Prelude that fixes these warts or is just a dream?
This blog post is almost contentless and not worth reading for anyone serious about this topic. If you want to get a feel for dependent types in a slightly nontrivial way, I'd suggest: http://www.cse.chalmers.se/~ulfn/papers/afp08/tutorial.pdf You can also write code to play with these ideas.
Also, compare the rather similar programs: A: if (b == 0) then [handle that case] else print (a / b) B: if (b == 0) then print (a / b) else [handle that case] In most languages today, both are accepted by the compiler. In a dependently typed language, only one is accepted. Now, there are static code analysis tools that will detect this in even venerable old C; dependent typing allows you to write your static analysis checks in the same language you are writing the rest of your program.
For almost all distros all you have to do is set dependencies, and run the existing build/installation of the software for the package. In NixOS you have write special code to actually have the build process work for many packages. Oh yeah and what about configuration for packages, the entire configuration system for almost all packages has to be redone.
Yes, there are more reasonable Preludes. No, you can't default to them, but you can import them in all your modules. Beware that your modules become dependent on them though.
There's a kind of myopia in our industry that the kind of software one works on and the practices used locally in your profession with generalize to *all* software development. But the field is so enormous and spans the entire range of heart monitors, to cellular base stations, to web apps and there are very few generalizations about the skills needed for software engineers that apply universally. 
&gt; The rate of contributors, committers, and users has only been increasing in the past several months https://github.com/NixOS/nixpkgs/graphs/commit-activity Commits dropping since 04/06 https://github.com/NixOS/nixpkgs/graphs/contributors?from=2013-01-12&amp;to=2014-07-01&amp;type=c Contributors also dropping in the last months. The only reason it seems like users is rising is because of promotional posts like this thread where people want to try it out, almost all of which drop NixOS within weeks. Kewl story bro. Enjoy wasting your time on a dead-end project. 
I get it, so the difference between something like Haskell and a dependently typed language would be that in Haskell you could forget the check and a run time error would occur, but if the possibility exists that the denominator would equal 0 in the dependently typed language, it wouldn't compile, thereby forcing you to guard it.
All true, plus: unless you enable a GHC extension every string literal you use in your program gets represented as [char].
And I still write my Java and Scala in vim and compile (usually with ant) at the command line. I've used a IDE with both languages from time to time, but I still generally prefer my vim, without stuff like syntastic. A lot of it is familiarity; I know all my keyboard "shortcuts" in vim. In IDEA and Eclipse I end up needing to touch the mouse (and remove my fingers from the home row) far to often.
Honestly, I don't think this is a very productive attitude. People can and do adopt Haskell when (a) they are able to work productively with it, and (b) they are convinced that it solves real problems with other tools that they have. When people resist using Haskell, it's often because they either have very real concerns about their productivity when using the tool, or they do not see the benefit of using it. Neither concern is addressed adequately by telling them that they aren't rigorous enough to meet your definition of being good at their job. Then you go on to construct a straw man ridiculing them for being math-phobic and not knowing anything. Obviously, the truth is that there are some very talented software developers working with Ruby, and most other languages, too, and practically no talented programmers would gloat about not needing to know anything about programming to use their primary language. You've pretty much lost any credible chance at accomplishing your goals.
I think we're mixing objective and subjective. Parents treat their kids' injuries and tend to them when they are sick, but that doesn't make them doctors. They *can* be doctors when it is their profession. I see that you mean there is a stigma about excluding people from programming and you feel there is a need to address that. But you won't get far by arguing definitions.
I tend to agree, and yet I also see things differently. My opinion is that "all that math stuff" can actually make it easier to program. And yes, it can make it easier even for the "don't want to deal with all that math stuff" people. The more we base our languages and libraries on sound mathematical principles, they will become easier and more intuitive for anyone to understand. So they should be excited; they just don't know it. Once they see the tangible, practical benefits, they will be excited.
I liked the first chapter of HoTT better as introductory material.
This same line of reasoning could be used to talk yourself out of all of Haskell's type system. If you have the opportunity to provide a guarantee, do it. Especially in such heavily used functions.
Also, a run on how it works: First, `foldr` can be written like this: foldr _ start [] = start foldr f start (x:xs) = f x (foldr f start xs) `foo` is easier to understand like this: foo f a bs = foldr (\b g -&gt; (\x -&gt; g (f x b))) id bs $ a With this in mind, we can evaluate `foo` by hand: foo (+) 0 [1,2] = foldr (\b g -&gt; (\x -&gt; g (x + b))) id [1,2] $ 0 = (\b g -&gt; (\x -&gt; g (x + b))) 1 (foldr (\b g -&gt; (\x -&gt; g (x + b))) id [2]) $ 0 = \x -&gt; (foldr (\b g -&gt; (\x -&gt; g (x + b))) id [2]) (x+1) $ 0 = foldr (\b g -&gt; (\x -&gt; g (x + b))) id [2] $ 0+1 = (\b g -&gt; (\x -&gt; g (x + b))) 2 (foldr (\b g -&gt; (\x -&gt; g (x + b))) id []) $ 0+1 = \x -&gt; (foldr (\b g -&gt; (\x -&gt; g (x + b))) id []) (x+2) $ 0+1 = foldr (\b g -&gt; (\x -&gt; g (x + b))) id [] $ 0+1+2 = id $ 0+1+2 = 3 This is the kind of equational reasoning that Haskell allows. It's better if you do it yourself. If will be way cleaner if you put the `(\b g -&gt; (\x -&gt; g (x + b)))` into a separate function: otherwise it's easy to get lost in the sea of lambdas and function applications.
Quite frankly MonadTran's whole comment seems to be based around the "*all that matters is shipping software to customers*" argument which is generally just used as a thought-terminating cliche to end a discussion rather than make an actual point. It can be used to justify anything because disagreeing with it is seen as wrong in our industry, and conflates the original point of distinguishing between practitioners and engineers in our discipline as somehow not caring about customers. I think DroidLogician's point about the distinction in the level of care a parent and a doctor can provide is a good analogy for the roles our field should have, but doesn't. 
If you read his post carefully he is not making a value judgment about whether or not it is good to be scientific as a programmer. He is just observing that many programmers are not scientific. And he's right. Most programmers are not scientific, either in the way they program, the way they debug problems, the way they handle code reviews, or the way they evaluate tools. I personally do think that is a problem, but I believe the correct solution is to teach people the benefits of being more scientific, not make them feel bad about it.
A dependently typed language would force you to handle the zero case, and then you can prove that division is safe for the remaining cases.
Try manually expanding some do blocks (using &gt;&gt;=, &gt;&gt;), and then re-sugaring them without looking at the original. This is what made it click for me!
That's a pretty [standard](http://dictionary.reference.com/browse/engineering) [definition](http://en.wikipedia.org/wiki/Engineer) of the word, for example "An engineer is a professional practitioner of engineering, concerned with applying scientific knowledge, mathematics, and ingenuity to develop solutions for technical, societal and commercial problems." In just about anything but software an engineer generally is or at least the term suggests the kind of professional who produces not "relatively reliable" but reliably reliable solutions, with safety margins and licensing and liability and that sort of stuff. Arguably there is no such thing in software, and you can certainly be a perfectly good programmer without anything like that, but it's not an unusual definition of the word.
To be fair though, for this example in an imperative program you'd either handle the zero case correctly or have a crash you didn't know about. If the compiler checks are increasing the complexity of your code then you haven't been handling all the cases properly. To me it doesn't seem to make things more complex, just forces the programmer to fix it immediately instead of in a refactor.
Thank you so much. It makes sense now. So if I set h = (\b g -&gt; (\x -&gt; g (x `f` b))) as you suggested, then I can simplify your equations and get the proof I was looking for: foo f a bs = foldr h id bs $ a foo f a [] = foldr h id [] $ a = id $ a foo f a [] = a foo f a (c:cs) = foldr h id (c:cs) $ a = h c (foldr h id cs) $ a = (\x -&gt; g (x `f` b)) c (foldr h id cs) $ a = (\x -&gt; (foldr h id cs) (x `f` c)) $ a = (foldr h id cs) $ (a `f` c) foo f a (c:cs) = foo f (a `f` c) cs Now the two key equations really do define foldl: foo f a [] = a foo f a (c:cs) = foo f (a `f` c) cs 
If you grant me a free category over r "Cat r" that is a catenable deque with O(1) append, snoc, unsnoc and cons then we can use it over the kleisli category of Free f: data Free f a where FreeRefl :: FreeView f x -&gt; Cat (Kleisli (Free f)) x b -&gt; Free f b data FreeView f a = Pure a | Free (f (Free f a)) instance Monad (Free f) where return x = FreeRefl (Pure x) id FreeRefl m r &gt;&gt;= f = FreeRefl m (Kleisli f &lt;| r) Shows how we can bind it in O(1) using the cons, but we can _also_ access the outermost constructor without destroying our ability to futher bind: view :: Functor f =&gt; Free f a -&gt; FreeView f a view (FreeRefl h t) = case h of Pure a -&gt; case unsnoc t of Empty -&gt; Pure a tc :| hc -&gt; view $ runKleisli hc a ^&gt;&gt;= tc Free f -&gt; Free $ fmap (^&gt;&gt;= t) f (^&gt;&gt;=) :: Free f x -&gt; Cat (Kleisli (Free f)) x b -&gt; Free f b FreeRefl h t ^&gt;&gt;= r = FreeRefl h (r . t) Here we only recurse past the initial Pure's, discharging as we go. We don't pay for how many left associated (&gt;&gt;=)'s are on top. The catenable deque has reassociated them gradually for us as they were put on discharging that obligation for free in passing. We get O(1) access to either end, and we paid the cost to rebalance as we put them on. Importantly because we did so 'smoothly' there isn't a situation where you can have to force the entire reassociation, then do something then have to force it all over again. The key is that you can do both on the same structure, so if you alternate between inspection and further binding the catenable deque approach is a win. If you never do binding after you start inspection then there is no win here. If you always associate to the right then you can of course use the naive free representation. But when you do inspect after you bind and then bind again? It can make an asymptotic difference. Why? because you pay nothing for the length of the chain to add more bindings under it, just like the CPSd representation, but you have efficient access to the start of the sequence of bindings like the naive representation. When does this happen? When you bind monadically in pipes you want it to be CPS'd for fast binds. When you compose two pipes together you want fast inspection of the outside. If you alternate between these two behaviors currently you get an asymptotic slowdown. Reflection without remorse can fix this issue. The nice part is the user doesn't even have to know such a trade-off previously existed, they just use the API and get the benefits.
I find this paper fascinating. I liked this tangential code sample way too much: interleave :: m a → m a → m a interleave l r = msplit l &gt;&gt;= λx → case x of Nothing → r Just (h, t) → return h `mplus` interleave r t Deja vu. I wrote a function with almost the exact same shape; it's for connecting two coroutines: ($$) :: Monad m =&gt; Producing a b m r -&gt; Consuming r m a b -&gt; m r producing $$ consuming = resume producing &gt;&gt;= \co -&gt; case co of Done r -&gt; return r Run (Produced o k) -&gt; consuming o $$ k https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/coroutines-for-streaming/part-2-coroutines It's pretty cool how all these concepts fit together.
Might I suggest keeping exactly `[Char]` rather than `String` then? If you proudly keep those semantics, why hide it under a synonym? There are a few cases, like lexers consuming one character at a time, where a list of characters (perhaps streaming off a byte buffer somewhere) is exactly the right thing, one it inlines and fuses and everything else. For other things, even if you just say "ordered sequence", that doesn't preclude efficient random access, and indeed I'd guess the unicode standard assumes efficient random access rather more strongly than it assumes that sequence is possibly infinite - should elegance then mandate `Seq Char`?
Not necessarily better, but it can certainly be fairly efficient. I've benchmarked certain lexing type things that really want to eat one character at a time - for that you are at worst creating the list nodes right off some IO buffer or whatever as you consume them, and at best it's inlined and fused entirely. You'd probably want to keep a `stream :: Text -&gt; [Char]` around even if `String` vanished entirely.
&gt; [(+1)] &lt;*&gt; [1, 2, 3] Or (+1) &lt;$&gt; [1, 2, 3]
The type of `/` is not `Real -&gt; Real -&gt; Real`, it is `n : Real -&gt; d : Real -&gt; d /= 0 -&gt; Real` (or whatever, depending upon your notation). The proof that the denominator is non-zero is a parameter to the division function. If you get the denominator `b` from something like `read`, then you will not generically be able to construct an object of the type `b /= 0`. You may, however, check whether `b` is nonzero in such a way that you obtain an object of type `b = 0` if b is zero and an object of type `b /= 0` if b is nonzero, and then *in the latter case* you will be able to use the `/` operator.
I haven't thought all the way through my critique yet (so perhaps I'm missing something simple), but wouldn't this require that the type checker be Turing complete? This opens up a whole other can of worms about metaprogramming in the type system (the way you can do template metaprogramming in C++), and that way madness lies. How does dependently typed programming avoid this? or are we restricting it to some subset of proofs that are easy to verify? and if so, doesn't this restrict us to using only a subset of provably correct programs?
Well, I would say according to that definition, applying ingenuity in order to design a structure of a company, is still engineering. Safety margins, licensing and liability are mostly imposed on engineers by governments. I personally do not care when stepping onto a bridge if the bridge is designed according to standards, if the engineer is licensed, or if the engineer would be liable for me falling off the bridge. All I care about, is that it looks sturdy, and a person heavier than me has crossed it before. As soon as those conditions are met, I would call the bridge designer an engineer, even if no science was involved, no math has been used to prove that the bridge would not collapse during a major earthquake, etc. 
That is *not* the type of `tell ["blah"]`. Its type is `MonadWriter [[Char]] m =&gt; m ()`. And `RandT` (from the `MonadRandom` package) is an instance of the `MonadWriter` class. There is no need to wrap `tell` in `lift`. 
I call BS here. Maybe others are reading into it what they want to, but this started with a comment denigrating programmers for being afraid of math, and wanting tools that let them do a crappy job without knowing too much about programming, and compared their work to "building a shack out of twigs". It harms our community to go on as if this was a normal and acceptable way to speak about other software developers.
I purchased this book after seeing the original post and am waiting for it now. I hope its better than the below comments. A book I'd definitely buy is one on everyday programming tasks like parsing CSV files, connecting to databases, using regular expressions...etc. 
Could you be more specific? What type of clever guesses are we talking about?
You're reading into it, too! You are the first to add the word "afraid" to this discussion. The straw man about the programmer who doesn't want to "deal with all that math stuff" is absolutely *not* representative of all programmers who choose not to use FP, but is an example of one argument against using FP in general, and Haskell in particular. There are many math savvy programmers out there using whatever language you or I don't like, and, yes, the OP's single complaint does not address that audience. Perhaps the OP could expand the argument to encompass all reasons for not using FP, but I'm fine with accepting what's there as informal kvetching. There are lots of non-universal yet legitimate complaints about Haskell and its users. I'm fine with people complaining about them, too.
I don't use auto-complete at all. I just have documentation in a browser window for quick browsing if I mentally misplace a method / function name. I tend to spend more time thinking about my code than implementing my designs and when I'm not doing either of those I'm refactoring where the method / functions I need are already in the existing code. So, auto-complete has never been a big time-saver for me.
What does it mean for an udder to utter a non sequitur?
I think the best examination of this is in applicative versus monadic parsers. Applicative parsers can only parse context-free languages while monadic ones can do context-sensitive. "Recognizes context sensitive languages" is powerful enough to imply completeness. But now you can go and write an applicative or monadic parser in a few tens of lines of code and feel how they differ in power. (And then after you feel comfortable with that difference, have your mind blown a bit about how general recursion in Haskell gives you the necessary infiniteness [to get RE parsing out of applicatives](http://byorgey.wordpress.com/2012/01/05/parsing-context-sensitive-languages-with-applicative/).)
&gt; Developers are the customers of those who develop compilers. If a particular developer is telling you that they do not like your tool, you can explain how to use the tool, or you can fix the tool. Let's not conflate *customer* with the *end-user* of an open source project, customer implies that something is being exchanged for a service. An open source compiler like GHC is a gift from a bunch of really awesome people who donate their free time to building it for a variety of reasons which may or may not include end-users. I know in industry the mantra "the end user is always right" is repeated quite often, but in compiler development the end-user is often very wrong and uninformed about the decisions that went into development of the compiler internals.
It would help a lot with automatic parallelisation. It would help the optimiser to know that it is free to group and distribute the operations as it sees fit.
And arrows are said to lie somewhere between context-free and context-sensitive, as I understand it?
Here's something I wrote down in a text file just to explore the parallels between various Haskelly concepts touched upon in this paper: class Monoid m where mempty :: m mappend :: m -&gt; m -&gt; m class Seq s where nil :: s x append :: s x -&gt; s x -&gt; s x class Category c where id :: c x x (&gt;&gt;&gt;) :: c x y -&gt; c y z -&gt; c x z data Nat where Zero :: Nat Succ :: () -&gt; Nat -&gt; Nat -- superfluous () arg is just for symmetry data List x where Nil :: List x Cons :: x -&gt; List x -&gt; List x data TList c x y where TNil :: TList c x x TCons :: c x y -&gt; TList c y z -&gt; TList c x z 
Something like that. I don't personally know how comfortable I am saying it, though. Arrows are semi-orthogonal to applicative-&gt;monad.
&gt; That is not the type of tell ["blah"]. Its type is MonadWriter [[Char]] m =&gt; m (). That depends. If you are using `mtl` which includes the module `Control.Monad.Writer`, then `tell` is defined in the `MonadWriter` class as you said. If you are using `transformers` which includes the module `Control.Monad.Trans.Writer`, the type is `tell :: (Monoid w, Monad m) =&gt; w -&gt; WriterT w m ()`. When `w` is `String` and `m` is `Identity`, this can be written as `tell :: String -&gt; Writer String ()`. &gt; And RandT (from the MonadRandom package) is an instance of the MonadWriter class. There is no need to wrap tell in lift. Again, you are right, but only if you are using `mtl`. I should have clarified that I was talking about `transformers`.
The nice consequence of this is that can force you to handle bad input early on (near the IO-facing portions of your program) and then your logic can be even purer!
What is the definition of `battle`? I think the problem is there.
I understand, just wanted to fix the ghci code.
Nah, fromJust should be left alone, unless you want to replace it with "unsafeFromJust".
Nah, weird cabal issue. It was running an older version of my code. Had to manually `cabal configure`. It worked as expected after that. Thanks for your help
Hmmm . I added lately a validation sentence in the second box that obviously does not work... This is a work in progress. Fixed.
There is a slight technicality. If you are parsing over a finite alphabet then an applicative parsing combinator library can parse a context-sensitive grammar. How? Nothing prevents you from having an infinite number of states in your parser! It is only if you restrict yourself to a finite number of states or infinitely large alphabets, that you then get the context sensitive/context free divide.
That is the purpose I had in mind. Fay lack the necessary extensions to compile the code. 
You can easily prove that user input is non-zero: if(x != 0) { // In this block you proved it } else { // Here you proved that it is zero } The proof may look different in a dependently typed language of course. 
If you would say that's according to either definition, you didn't read the definitions. That's a totally crazy standard for bridge safety too.
Thanks for helping me understand Control.Applicative (&lt;*&gt;) better!
Look up how to fold trees. Once you have a fold specialising it to equality should be simple.
Not familiar with Irdris either, but I think the way to think about this is somewhat different from how you're phrasing it. &gt; What would happen if the second number were a 0? Would the function silently fail? Would it throw some kind of error? Would it filter away the 0 and wait for another number to be entered? The non-zero type is not the same type as the integer type that you read from the user. You have to *construct* the value of the non-zero type *from* the integer type. So in this case you'd basically have the equivalent of a function that returns Maybe and it returns either Just Nonzero or Nothing. That's all there is to it, as far as using it. Once it's constructed, it's just like any other type. You never have to worry about dividing by zero for the same reason you already never have to worry about dividing by Nothing. The magic part is how does Irdris figure out that the implementation of that constructor is valid? Well it's not too magical, the non-zero constraint of Nonzero corresponds to a conditional and the compiler knows it. You use the conditional that Irdris knows corresponds to the constraint in order to construct the type. (And of course once you have that little bit of magic, you don't actually need that Maybe wrapper or even the explicit constructor; you can use the magic directly.) Again I don't know anything about Irdris, this is just my understanding of the basic concept. The input case is actually not very interesting because you can't know it's non-zero, so you have to check. The interesting bit to me is what happens when you *do* know something about the value. Now you can't use that knowledge to avoid the check unless you actually prove it. But proving isn't as easy as knowing...
&gt; If you have the opportunity to provide a guarantee, do it. Especially in such heavily used functions. OK, I was expecting that critique :-(. However, I'd like to play devil's advocate and argue that there are safe uses of `head` and `tail` that would not be possible (or at least harder) with a Maybe-lifted version: application to infinite lists, e.g. `iterate tail [0..]`; combination with non-strict functions, e.g. `zip xs (tail xs)`. 
just try writing out all the possible combinations of patterns on each side of the equals :) 
Arrows don't really belong on this spectrum... that's like trying to put Eq on the spectrum. 
You can use Java 7 on android. It was added to the KitKat build tools and most of it is backwards compatible.
would `-fdefer-type-errors` promoted to a pragma help?
Your definition has no distinction between a lucky lay person and a trained expert.
&gt; it is a partial function on finite lists but so is fromJust on Maybe types; should the later be deprecated too? Yes.
Consider changing the definitions of `Add` and `(+)` so that automatically deriving `Eq` will do the right thing.
Since lots of people have answered this with "I'm not familiar with Idris but..." I thought I'd write a simple command-line program which reads two inputs and divides one by the other if it's safe: [here's the gist](https://gist.github.com/edwinb/0047a2aff46a0f49c881) The important bit is that we have: safe_div : (x : Int) -&gt; (y : Int) -&gt; {auto p : so (y /= 0)} -&gt; Int safe_div x y = div x y The 'auto' keyword means that Idris will do a proof search when safe_div is called (you can also do the proof by hand if Idris fails). This will either find that y is a non-zero constant (in which case the proof is easy) or try to find something in the context which proves that y is non-zero. It's easy to make such a proof which a run-time check, using this function from the library: choose : (b : Bool) -&gt; Either (so b) (so (not b)) Of course this means you only need to do the dynamic check once. As soon as you have the proof term, you *know* it's non-zero so you can divide as much as you like. In some cases you might even be able to manipulate the proof to prove something else without any dynamic checks at all. So dependent types aren't going to save you from doing necessary run-time checks, but they can help you spot where you've *missed* a necessary run-time check, and they can help you remove *redundant* run-time checks. Hope this helps!
[The HoTT book](http://homotopytypetheory.org/book/).
Does it look like a thought-terminating cliche? Hmm... My intended goal was to provoke some discussion on how we can explain things, improve things, or promote things. In my experience, it is the "customer is stupid" attitude that is thought-terminating. I'm seeing it all the time, last week's example: "Customer is stupid, they sent us a file in the wrong format again, that's why our application crashed". OK, but, why is the application *allowing* the customer to crash itself? Why is it not providing instant feedback to the customer? Can we provide some tools so that they always generate it in the correct format? Same thing here. Do we just talk to people, and explain things? Do we lead by example, refactoring imperative code into more pure code? Do we improve the infrastructure around GHC / Idris? Make it easier to interact with imperative languages? Add Oracle and MS SQL bindings? Lots of thoughts here, but all of it doesn't matter if the majority of developers are stupid non-engineers who are unable to grasp the divine beauty of strongly typed functional programming, because in that case, any efforts to promote Haskell are in vain. Dijkstra was complaining several decades ago about non-scientists writing code, and the state of the industry, and hoping that it would get "better" over time. Now there are even less scientists in the industry, and I don't see the trend reverting.
Backwards incompatible changes have happened before. It only takes a strong enough advocate, I think. 
I think it is in this particular case. The problems that the originator of that thread run into were unexpectedly poor performance in a simple program and running out of file descriptors. The poor performance was due to the very inefficient String. Running out of file descriptors was due to the highly unintuitive behavior of lazy IO. Choosing `String` and lazy IO is a beginner error, but a very understandable one, *since that is what the Prelude gives you and you have to do something special to have the usable alternatives `Text` and strict IO that yields `Text`*. 
What is the logical difference between Bytes and (Vector Byte) or something similar? Is there a reason bytes are treated as a special case, requiring a unique type? Sure, lists of characters are a poor abstraction for handling binary data. But the idea of a container that is generic over contents seems like quite a good idea actually. Does it cause a performance issue or limit optimizations?
A list is a singly-linked list, which is different from an ordered sequence semantically. It implies that it functions as a stack, with a sequence of next pointers and an end pointer. It implies a possible usage pattern: switching out the beginning, and pointing to the same end. It implies another usage pattern: viewing the start, but stopping partway through. Neither of these reflects the way we use and modify text. It does not even reflect a particularly useful way of using or modifying text. Semantically, what is natural for text access is to use slices, to glue together chunks, and to substitute pieces for other pieces. Trying to shove everything into the mold of "list" unnecessarily awkward and inefficient.
You are assuming that you have a choice of what bridge to take. Your example, in fact, illustrates my point perfectly. There is no objective quality standard, in any area of engineering. Customers determine what's good enough, by choosing from a list of options. If there is only 1 bridge, there is little choice, so anything that looks sturdy is fine. As soon as there is a nearby bridge already, the acceptable quality bar is raised. The new bridge has to be either wider than the previous one, more sturdy, more conveniently located, safer for pedestrians, etc., otherwise, nobody would be using it. No customers = project failure.
Popularity is not success, Javascript is used and accepted by an enormous amount of people and is still an abject failure and train-wreck of a language. When someone who doesn't have the engineering credentials to build a language goes off and does it anyway it hurts the entire profession. 
Yeah, dude doesn't even understand the difference between functional and declarative programming but clearly sees the future. Where all these semi-educated prophets come from?
It would be better if the partiality had better warnings. If you really really know what you are doing, its not THAT much harder to make it clear with a fromJust. Or perhaps just label all the partial functions with a "partial" prefix, like all the unsafe functions. The goal, of course, is to make you actually carefully consider places where you might introduce errors, where you might accidentally gloss over them otherwise. In practice, though, head seems pretty absurd anyways, because pattern matching seems like the more common pattern for handling that sort of use case, and the compiler provides partiality warnings in this case.
I know I'm against the grain here, but I'm a big fan of tabs used correctly. "Tabs for indentation, spaces for alignment" is a good adage ([more here](http://programmers.stackexchange.com/a/267)) 
What I got from the post is that the first chapter of the HoTT book covers dependent types in a nontrivial way. I am not sure why you think this is a non-sequitor.
You can do that. module Stuff (module Data.Text) where import Data.Text
Basically say you expose a database of patients who took part in a controlled study recently, with records exposing what their diagnosis was, when they visited, what their treatment regime was. You want to find out if you were a control subject or if you actually got the treatment. Your name is not exposed but you do have a rough idea of when you were getting treated and you know what age group you were in. Tuples are also labeled with a "patient ID", you can look up all sorts of data if you have the patient ID, including the treatment administered. Even though you can't see the name associated with the patient ID, you can try to guess which one is yours by looking at all the visits made by patients in your age group, then try to guess which patient ID most closely matches your own memories of the situation. With that, you could look up what treatment regiment the likely candidate for you had. You've gotten access to data you were not authorized to have, even though you didn't break into the database by exploiting any security holes. There is some formal theory about preventing these kinds of weaknesses in security although I couldn't point to any resources myself. Usually a safe approach is to only ever expose aggregate data and never expose individual records.
as tel commented elsewhere: what precisely is declarative and functional programming? these terms are _very_ fuzzy.
Presumably, this leads naturally to a notion of "optional" dependant types, where you can elect not to provide such a provide such a proof at first pass, and can think about it more carefully and fill in the proper details later to ensure correctness and/or faster performance, instead of insuring perfect correctness on the first pass.
the problem is that not all intentions to work in cs or as software engineer can be described by vulgar economics. it might be true for java (the cliché of an enterprise technology) but is not true e.g. for the free software community or a more academic community like haskell.
Yeah, I thought the first chapter of HoTT was an excellent introduction as well. Tried recommending it to a couple of friends, but then they insisted on reading the Introduction to the book first and got scared off, despite my emphatic warnings that they should skip straight to Chapter 1. Kind of like some of those fairy tales...
WebKitGtk includes C functions for manipulating the DOM and these are [included in the webkitgtk3](https://github.com/gtk2hs/webkit/tree/master/Graphics/UI/Gtk/WebKit/DOM) Haskell package. It also supports adding callbacks to Haskell for DOM events. To try it out just you should be able to run something like sudo apt-get install libwebkitgtk-3.0-dev cabal install gtk2hs-buildtools cabal install ghcjs-dom-hello ghcjs-dom-hello Edit: [ghcjs-dom-hello](https://hackage.haskell.org/package/ghcjs-dom-hello) is in hackage and GHCJS is not required to install it.
Oooh, by that definition I'm an engineer - awesome! :D
When you are handling bytes, you normally want chunking of the bytes for performance. Types like `Vector` don't do this.
&gt; That is correct. Engineering is an activity, anyone can do it, with different degrees of success. No. "Engineering" and "engineer" are regulated words in many countries. You simply can't call yourself an engineer without a certain education, and without belonging to a professional engineering society.
It being free to do something does not make it reasonable or meaningful or useful. In general, a monoid effectively represents the type of thing that is 1) Associative and 2) Has a default "zero" or "empty" value. It seems like this would be all we need to build a parallel map-reduce-style operation. However, lets look at a couple instances of Monoid, and see if there are any real benefits to parallelizing their append operations: - Lists (concatenation): Nope, under strict semantics your performance will be much worse on multiple processors, and under lazy semantics your performance will be at best the same, since the cores would just be building thunks that the consumer of the list would have to evaluate linearly anyways. - Endo ((a -&gt; a) under (.)): totally useless, as above, since the code consuming it has to traverse the entire thing anyways to evaluate it. So what cases is it useful? Well, if you actually have, say, summation of numbers, then maybe. However, in many cases (small lists) the overhead of parallelization might be more expensive then the benefits. In many other cases, (millions of items which are each simple to compute) the only way to get good performance is to split it up into chunks which are performed at once. And in other cases (very small number of items that are difficult to compute), good performance is increased by *not* batching operations. In all these situations, the correct action to take is subtle and complex to figure out. The compiler is not going to benefit much from it. So the answer to your original question is yes, that type is a semigroup (and any logical subclasses) and no, its not useful at all for the compiler, but it sometimes is useful for a library writer as a type sanity check. 
It makes a little sense - who can write the most unabashedly positive post about something, but for someone just getting into something that doesn't know the flaws? (well, without the plain dishonesty you may get from someone trying to sell you something).
Not all programs on a turing complete machine are undecidable. Existing type systems heavily restrict you in order to ensure that they can verify decidability, but that does not imply that a system that allows undecidability does not allow you to build decidable things. The clear example here is that all the examples shown have clearly been decidable. If you want to divide by zero, and provide clear evidence of the non-zeroness of the denominator, it compiles. If you fail to provide such evidence, it fails, and requests you do so. It might indeed be possible to construct something undecidable in the compiler. However, it is unlikely to be useful, and likely to be caught with no problems by a timeout if you do so by accident.
&gt; Engineers (by my definition) are people that apply scientific principles to practical problems. Engineers are people that**, in theory, should** apply scientific principles to practical problems. Not all programmers are engineers and not all software engineers care (some just want to get the job done, who could blame them for that? We don't all have to love unconditionally what we do). However I do agree with you that the "I'm able to do such and such without caring in Python/Ruby" is repellent. It always baffles me how people who've been through 5 years of studies (and is then used to intricate stuff) may even come up with the argument "it's simple to learn/use" thiking that _by itself_ simplicity is a proper asset.
After the crisis I will update the app to make 1+2 = 4 at least. (Note for academics: it´s a joke)
When you compile a ghcjs-dom app with ghcjs it uses Java Script FFI calls and will work in any browser. When you compile it with ghc (or ghcjs with -fwebkit and --ghc-option=--native-executables) it uses WebKitGtk and the app itself IS the browser and the ghcjs-dom calls go straight to WebKitGtk. You can still use the JavaScriptCore engine (that is included in WebKitGtk) to run javascript code in the app, but you don't have to. 
Okay, after "There is no objective quality standard, in any area of engineering." I can no longer believe you're even attempting to say things you actually believe rather than whatever sounds like it might support your argument. Many unsafe bridges look sturdy enough until they collapse and kill people. You're saying nothing was objectively wrong with the [Hyatt Regency walkway](http://en.wikipedia.org/wiki/Hyatt_Regency_walkway_collapse) until customers mysteriously decided they would prefer other options?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Hyatt Regency walkway collapse**](https://en.wikipedia.org/wiki/Hyatt%20Regency%20walkway%20collapse): [](#sfw) --- &gt; &gt;The __Hyatt Regency hotel walkway collapse__ occurred at the [Hyatt Regency Kansas City](https://en.wikipedia.org/wiki/Sheraton_Kansas_City_Hotel_at_Crown_Center) in [Kansas City](https://en.wikipedia.org/wiki/Kansas_City,_Missouri), [Missouri](https://en.wikipedia.org/wiki/Missouri), [United States](https://en.wikipedia.org/wiki/United_States) on Friday, July 17, 1981. Two vertically contiguous walkways collapsed onto a [dance competition](https://en.wikipedia.org/wiki/Dance_competition) being held in the hotel's lobby. The falling walkways killed 114 and injured a further 216 people. At the time, it was the deadliest [structural collapse](https://en.wikipedia.org/wiki/Structural_collapse) in U.S. history, not surpassed until the [collapse of the south tower of the World Trade Center](https://en.wikipedia.org/wiki/Collapse_of_the_World_Trade_Center) in 2001. &gt;==== &gt;[**Image**](https://i.imgur.com/Endkbpz.png) [^(i)](https://commons.wikimedia.org/wiki/File:Hyatt_Regency_collapse_end_view.PNG) --- ^Interesting: [^Kansas ^City, ^Missouri](https://en.wikipedia.org/wiki/Kansas_City,_Missouri) ^| [^Sheraton ^Kansas ^City ^Hotel ^at ^Crown ^Center](https://en.wikipedia.org/wiki/Sheraton_Kansas_City_Hotel_at_Crown_Center) ^| [^Joseph ^Waeckerle](https://en.wikipedia.org/wiki/Joseph_Waeckerle) ^| [^Kemper ^Arena](https://en.wikipedia.org/wiki/Kemper_Arena) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cimlr50) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cimlr50)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Further hint: Have (+) fully left (or right) associate the whole expression. Change one of the arguments to Add to an Int instead of an Expr and that will force a particular association.
As as fan of tabs myself, I will say that Haskell's layout rules do seem intended for spaces usage. If you want to use tabs for indentation, you end up placing a newline everywhere an implicit '{' is added by the layout rules. That aside, tabs for indentation and spaces for alignment seems to work fairly well. Here's an [example](https://gitorious.org/haskell-cryptsy-api/haskell-cryptsy-api/source/c94b44b51b54ddf1d19dd47408d6d96ab97c1449:src/Cryptsy/API/Public/Internal.hs) of my code that does this -- everything looks nice independent of how wide you prefer your tabs (2 chars: "\^I" in my vim, 4 characters in the formatted version, 8 characters in the "raw" version in chrome).
I think those words need some unpacking, but even in a restricted meaning like "someone can muddle through without extensive training", it's obvious to me it's a proper asset - there are certainly people who's occupation is not programming that would benefit from a bit of programs, and domains where it's pretty safe - and in theory you could even scientifically certify that - to let them have at it. For an expert, simplicity in the sense of easy to reason about is probably good all the way up, and even easier to use at the cost of some reliability can be the right tradeoff - consider the expressions you type into a shell for ordinary tasks - but that's easy of use rather than specifically ease of learning.
It absolutely prevents bugs, sure. And to be honest, preventing division by zero is a pretty straight-forward case. The proofs are nearly always trivial. The problems come about when attempting to write library code. Will the library functions be written with types that happen to propagate your invariants? It turns out that in practice, the answer is "no" at the moment. It's really hard to write reusable code in dependently-typed environments, because purely structural properties (your tree is a balanced BST) end up getting mixed with business properties (each key in the tree is related to its value via some mechanism). Once the different sets of requirements start getting mixed together, it's very hard to use code that only cares about one of those sets of requirements and still manages to preserve the other proofs. There's hope that patterns like ornaments will turn out to solve these problems in a way that results in maintainable and reusable code. But that pattern is still so new that it's unclear whether they really work out in practice. And that's why I say I'm not convinced that dependent types are the way forward *yet*. These problems might have solutions. But if they do, they're not well-known or widely-used.
Or in LiquidHaskell: http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1404313408.hs 
You might also try the #haskell channel on Freenode IRC, and the ghc-users mailing list on haskell.org.
From a classic imperative language standpoint (sorry fellow haskell people for my reverting to python-y syntax): output = function(input) With a pure function, it will always return the exact same output given the exact same input. This is regardless of order, timing, number of calls, other functions that are called, etc. It also will not modify anything *other* then output, at least not anything that could be observed in a pure context. If we try to write a function that takes no input, and returns a char representing a keypress, you might try to write a function like this key1 = getkey() key2 = getkey() In haskellm however, you are guaranteed that key1 is the same as key2, since both function calls had the same input (nothing). It cannot "take a value from the keyboard" because no keyboard was provided. So that code cannot possibly work. Furthermore, this wont really work either: key1 = getkey(getkeyboard()) key2 = getkey(getkeyboard()) The problem is, the values returned by get keyboard must be identical (whatever they are!), and a function called with the same input must give the same output, so key1 and key2 are again always the same. This holds even if we extract the call into a temporary constant: x = getkeyboard() key1 = getkey(x) key2 = getkey(x) This is because absence of side effects comes with the guarantee that all data is immutable. In other words, the x *must* be the unchanged between calls, and thus it *must* be the same value going in to both getkey functions, and thus the resulting keys *must* be the same value. However far you try to fiddle with this, you won't actually get two different yet pure characters from these functions. So that sucks, but its ok... it helps you in reasoning about what your code is doing, and there is a pretty convenient way to "get around" the restrictions when needed. Instead of getting pure characters, we get the library to provide us with something that gives us IO actions, that when executed will give us a character. Now, when this imperative pseudo-code is executed, it gives us the same value each time, an IO action. key1action = getkey() key2action = getkey() But thats not what we wanted! We want to print the key! So for this, we come up with a function I will call "bind". Its a little complicated, but basically, it combines IO actions. The haskell signature looks something like bind :: IO a -&gt; (a -&gt; IO b) -&gt; IO b So I could write code like this: dostuff = bind(getchar, printchar) Which would build an action that gets a character and puts a character. This could be extended further too... for example if I wanted to do it twice, reading a character each time, I might build it like this: dostuffTwice = sequence(dostuff, dostuff) In haskell, bind is the infix operator (&gt;&gt;=), sequence is the infix operator (&gt;&gt;), so the entire program might be written like this: main = (getChar &gt;&gt;= putChar) &gt;&gt; (getChar &gt;&gt;= putChar) You glue together pure action objects into bigger actions with pure functions, and return single resulting action to main. If this is all too verbose, there is a lot of syntax and tooling to make this super convenient and easy to do, so it could just look like this main = do a &lt;- getChar putChar a b &lt;- getChar putChar b But however imperative that looks, in the background, it is just doing the bindings in the background. If this is too complicated or confusing right now, it will all get more clear as you learn more... feel free to ask questions any time you get stuck, either here or on #haskell on freenode IRC. (...holy shit, I just randomly wrote most of a monad tutorial on a random reddit post, wtf)
Unfortunately, this will likely be an eyesore for most of the Haskell community. 
Sometimes you know that a maybe will always be a just, and it would just be a burden to your library consumers to expose the value wrapped in a maybe.
My comment was more of an aside. I strongly recommend using camelCase for all Haskell code, or at least anything you might want to upload to hackage. PascalCase for types and camelCase for other identifiers is, for better or worse, the idiomatic Haskell style.
Can't you just configure it?
If you work it out that parser has an infinite number of states, one for each n.
In a way, yes, sure. You didn't promise anything to anyone, so you are free to do anything, and call people not using GHC all kinds of names. I *think* it would be more productive to hold GHC to the same quality standards as non-free software, though. Unless you consider open source software to be inherently inferior, which I don't think is the case.
I agree that Turing complete systems can run programs that halt, and that some instances of undecidable problems are nonetheless decidable (for example, the halting problem is undecidable but there also exist programs that will provably halt). My worry is that if you make it possible to do crazy things in a language, people _will_ do crazy things. Some idiot/genius will build a library that optimizes something or other by simulating a Turing machine in the type system, and anyone who gets stuck using this library will have to wait for 30 minutes of compilation before receiving an inscrutable failure message, all because they forgot some subtle detail in the proof system which the error message doesn't even describe directly. or worse, the compilation times out---was there a bug that messed it up, and if so, what was that bug, or if not, would changing the timeout and letting it run longer have made the compilation succeed? I want to avoid these questions; they're why so many companies have style guides which restrict how templates can be used in C++. I want a language whose features I can use; not a language whose features I need to be trained to avoid. I was initially attracted to Haskell because it limits the programs you're allowed to write to ones that are very likely to be correct and well-structured, and I worry that adding Turing completeness to the type system will expand the set of compilable programs to ones that are not obviously correct or well-structured. You also didn't answer my question. You had said "[a Turing complete type system that is] executed well is pleasant and useful." I said "I don't think I've seen a pleasant, useful, well-executed compiler that can handle this. Could you give an example?" I still am unaware of any examples of this done well, but I know several examples of it done poorly. Can you give an example of it done well? I am concerned that if every attempt to make one has ended up with an inelegant monstrosity, any other language that tries this in the future is likely to turn into an inelegant monstrosity again.
That's one step in the right direction, but automatically promoting functions and data to the type level would be a much bigger help. The current way to do this in Haskell is to duplicate everything at the type level, with cruftier syntax.
A quick Google search doesn't show me a simple configuration change to make that will have vim treat camelCase as two words. I might have found a way to have it completely ignore camelCase, but that's not the goal. I've also found the quite useful camelcasemotion plugin, something to definitely add to my configuration, but it doesn't solve the original problem.
As a relatively new user, what is wrong with prelude? I haven't noticed anything I didn't like about (I come from a ML/Javascript/Java background). Most frustration when learning haskell has been cabal related.
I've had the same experience. "Read chapter 1, AND ONLY CHAPTER 1!" Days later... "So, how did you find it?" "Well, I read the first few pages of the preface and got stuck, so I gave up" HNGGGGGGGG
GHC is much higher quality software than you'll find pretty much anywhere, but Haskell is an organic effort. It follows the standards, quality and goals of whoever wants to contribute to it, not the needs of the market or consumers like non-free software. If you think GHC should be held to a certain level then here's [the tracker](https://ghc.haskell.org/trac/ghc/) go fix bugs until it reaches your standard. The dichotomy between "customer" and developer is effectively an illusion in open source.
No, the OP wants 3+4+5 /= 2+5+5.
Quite interesting. I'm completely new to this concept so it's good to hear some downsides.
Nothing, unless you are alternating between building your monadic expression and observing it, in which case it pays to reassociate your binds into an efficient data structure once. This way, you avoid paying that 'n' cost for each time you bind into `e`.
There's a decent argument to be made that much programming, and most code, doesn't need any decent engineering (more code than programs becasue you can isolate stuff). They are not making anything close to that argument. Being friendly and welcoming is good, but not at the price of agreeing with nonsense like "There is no objective quality standard, in any area of engineering."
If you liked that code sample you should read the LogicT paper. http://okmij.org/ftp/Computation/monads.html#LogicT
I think so. If we think of each `c x y` as a directed path from node `x` to node `y`, then `TList c` is "all paths."
Sure, these are all just tools, but the reason people are talking like this here is because they are trying to bring some actual engineering to the currently stupidly named "software engineering" profession. I disagree with your bridge example; someone who makes a wooden bridge that 'looks sturdy' is a carpenter, not an engineer. That isn't to say their result won't work, but they won't be able to prove it in any reasonable way. My definition of an engineer is someone who can be confident that the design works before they build it, and knows how to test and experiment around the 'physics' of whatever they are building in the cases where it's not well understood. I do agree that anyone can be an engineer, that "formal training" and "government licenses" are not what makes an engineer an engineer, but the ability to reason about what it is you are making and be confident that it will perform its function to its specifications is the core of engineering, and that's one reason why dependent types are so interesting, because they help even lay people be better engineers. EDIT: &gt; I've seen people who can design functional software without giving too much thought to any kind of science. And I've seen people who can build amazing chairs and coffee tables without thinking about that stuff either. It's a testament to the power of the tools we have that it's so easy for people to make software that mostly works. But it's not engineering.
Yes. Some other cool applications are described in the linked PDF in there: https://hackage.haskell.org/package/thrist
&gt; everything looks nice independent of how wide you prefer your tabs It may "look nice" at first glance. But it is not nice. Because the Haskell layout algorithm specifies that tab stops are [always treated as 8 spaces apart](https://www.haskell.org/onlinereport/haskell2010/haskellch10.html#x17-17800010.3). So for any setting other than that, your program will likely not parse the way you think it will from how it looks. Other than viewing files with different tab stop settings, which is not recommended for Haskell source code, what other reason justifies all the extra work and risk of subtle bugs that goes into using "tabs for indentation" and not doing it the simple way and just using spaces?
He was overloading `[]`; when he said `[plusOne]` he meant "lift plusOne into the `[]` functor", i.e. `fmap plusOne`.
&gt; The bridge itself had the physical property of being unsound The bridge does have physical properties, sure. The width, the ability to hold certain weight, built in safety devices, earthquake resistance, etc. Whether any of these properties make the bridge "unsound", is subjective. For instance, the same bridge may be considered unsound in modern times, but it would be a perfectly well-engineered bridge in the Middle Ages. A bridge may be fine for the purposes of one person crossing a shallow stream, but it may be considered unsound for the purposes of letting ten thousand people through every day. The same bridge made out of 2 ropes may be fine for a healthy person, and unsound for a disabled person.
I suspect that it would be a no small endeavor. GHC is partly written in Haskell, and, as far as I understand, there is no procedure to bootstrap it from the ground up. I think that the most promising venue would be to cross-compile on a host system that already has working GHC.
Ah, OK, so the magic is "there isn't any Javascript involved, ghcjs is a bit of a misnomer here." Is that accurate?
Thanks! I know that I could've just used something like Conduit.Binary.mapM_ to operate on the individual elements without taking them out of the bytestring. I use that simple pipeline only as my benchmark to show conduits won't work efficiently with such granular chunks. The real problem is on the output side of my own code, producing those granular chunks myself. If you write something like run length encoding or an arithmetic encoder, there is no simple map/reduce relationship between input and output, and you need to output zero or multiple bytes for each input element. So I don't think I can express that as any of the standard building blocks from Data.Conduit.Combinators / Data.Conduit.Binary. The problem here is really more related to efficient construction of those bytestring chunks. I'll see if anybody here chimes in with a good solution, otherwise I'll bug SO/haskell-cafe!
I can vouch for this course! I don't really have any affiliation with it, but I think that this type of thinking is very important in large scale applications where you simply can't hold everything in your head. Even more important when you're working as a team, as the only way to truly capture everything in your head so your colleagues know about the invariants is to write them down!
&gt;most programmers are not engineers. And that's exactly how it should be. 99.99% of people who drive cars are not professional drivers. Our civilization would grind to halt if we'd demand that every person go through professional training before they are allowed to drive a car. Programming is over the counter medicine of engineering. Accessible to everyone without years of expensive education. In the age of global network of computers the demand for programmers is so high, the volume required is so big, that there's no viable way to produce enough engineers for every business out there. Hopefully functional programming will "trickle down" to the masses in the form of specialized tools and libraries, LINQ, two-way data binding (we call it FRP), immutable collections, map-reduce frameworks etc. But do not expect haskell break 10% market share anytime soon. 
I was thinking about that but it seems like a lot of work :/
I *think* what you're looking for here is a [Builder](http://hackage.haskell.org/package/blaze-builder). The basic approach I'd take for this is: * await a chunk of data from upstream. * Perform some kind of a fold or the like on that chunk, producing one or more Builders. * `mconcat` or similar all of the Builders produced on that chunk into a single Builder. * Yield that builder downstream. [`foldMapMCE`](http://hackage.haskell.org/package/conduit-combinators-0.2.6.1/docs/Conduit.html#v:foldMapMCE) may actually be very close to what you're looking for here, though you'd like something that yields the `Builder` generated from each chunk immediately instead of producing just a single `Builder` at the end of processing.
&gt; Haskell, to my knowledge, lacks a structure like an std::vector with amortized O(1) append and the compact storage of an array. I'm not sure this is even possible in persistent / immutable context. I've not seen a proof that it is impossible, but I don't think it is a Haskell-specific problem. There are several structures with amortized O(1) append (and prepend), -- 2-3 finger trees -- but they generally don't have compact storage. I think the closest thing to what you want in a persistent / immutable context is a RRB (Relaxed Radix Balanced) tree and I don't know that we have a Haskell implementation of one, yet. It's still not just one big chunk but, if my intuition is correct, can be written so that iteration is cache-oblivious which is very nearly as good.
I agree, I don't think it's likely that there is any functional data structure with those properties. I'd settle for a mutable vector! I'll have a look at what the heck a RBB tree is, though! ;-)
&gt; Haskell, to my knowledge, lacks a structure like an std::vector with amortized O(1) append and the compact storage of an array. What structure has that in any language? 
If you just use tabs for indentation and not for alignment you're fine. I've never come across a problem in my code being rigorous in this regard. 
Mutable array + bookkeeping + double (or some other constant factor) the backing array size when it gets full and you need to insert more. It'd be trivial to do with a mutable vector in Haskell.
Oh I was assuming append meant concatenation, not a single element.
Apt-get install ghc pulls in some hundred megabytes of binaries and dependencies on my Linux system. Are you saying you *want to* try to *build* that on your mobile device instead of cross compiling?
&gt; Most programs these days operate on codata I'm not really sure this is literally true. In any case, codata is invisible to most people, since they are used to thinking about iterators (or infinite lists, if you're coming from Haskell). When you have general recursion available in a language, you can use inductive types coinductively and vice versa, but misuse results in loops. The type of the top level term in any program will be something akin to `IO ()`, and `IO ()` "is coinductive" for lack of a better term. While the type is opaque and very magical (and hard to reason about naively), intuitively, it is a giant command-response game tree, and potentially infinitely deep. This means yer word processors and yer webservers are all "terminating" (assuming all the bits inside do), since `main` reduces to a head-normal value of `IO ()`, which refuses to unfold on its own. 
Its true. Monomorphization is trickier than it may seem, actually. And there actually are times that it just can't be done without whole program optimization, or perhaps even at all. For an example, Rust doesn't support polymorphic recursion for a reason.
I don't think the following is the answer; but it is probably related to the answer: From [here](https://www.haskell.org/ghc/docs/7.0.4/html/users_guide/primitives.html) The main restriction is that you can't pass a primitive value to a polymorphic function or store one in a polymorphic data type. This rules out things like [Int#] (i.e. lists of primitive integers). The reason for this restriction is that polymorphic arguments and constructor fields are assumed to be pointers: if an unboxed integer is stored in one of these, the garbage collector would attempt to follow it, leading to unpredictable space leaks. Or a seq operation on the polymorphic component may attempt to dereference the pointer, with disastrous results. Even worse, the unboxed value might be larger than a pointer (Double# for instance). 
I think that's probably at least the better part of the answer. One could try to sidestep issues like those, but it would result in extra checks being done, and need for extra flags being stored next to things in the heap which would in many cases outweigh or match the savings being made by the optimisations.
&gt; Most programs these days operate on codata, so termination on a per-destructed-codata-component basis is productivity if I understand correctly. 
Ok, imagine this scenario: library A exposes String -&gt; Maybe LibA.Url library B exposes String -&gt; Maybe LibB.Url and I want to write the function LibA.Url -&gt; LibB.Url I could write: f libAUrl = toLibB.Url (show libAUrl) But, that gives the signature LibA.Url -&gt; Maybe LibB.Url, and that burdens the caller with having to deal with a Maybe value that will never happen if both libraries can parse a url correctly. Instead, if we do: g libAUrl = fromJust $ toLibB.Url (show libAUrl) The consumer doesn't have to worry about the conversion failing. And if it were to fail, it would be a bug in one of the libraries, and not something we want to handle as a consumer (i.e. fix the libarary bug instead of adding error handling code to account for a library bug). 
Yeah, that would probably be a better name if it were to change.
The most obvious problem is with polymorphic functions and separate compilation. Imagine we have a list type data List a = Nil | Cons a (List a) and, when `a` happens to be `Int`, we want to represent it as data IntList = IntNil | IntCons !Int IntList and likewise for `Char`, `Double`, `(# Int#, Int# #)` and everything else unboxable. How do we compile `map :: (a -&gt; b) -&gt; List a -&gt; List b`? It needs to be able to extract the `a` fields, pass them to an unknown function, and build a list containing `b`s, without actually knowing what `a` and `b` are. GHC does this by having `List a` and `List b` use the same (boxed) representation no matter what `a` and `b` are. What if we give up separate compilation? We could go the C++ templates route and generate a new monomorphic copy of `map` for each concrete `a` and `b` it gets instantiated with. But this doesn't work for Haskell in its full higher-order glory. For instance, we have - Rank 2 types. If we define `data Mapper = Mapper (forall a b. (a -&gt; b) -&gt; List a -&gt; List b)`, what happens when we write `Mapper map`? There's no single `map` function anymore that we could put in the constructor and expect to work on all types when we take it out. - Existential types. How should we represent `data Covert a where Covertly :: x -&gt; (x -&gt; a) -&gt; Covert a`? Now the layout of `Covert a` depends not on `a` but on the existentially quantified `x`. - Polymorphic recursion, which means the set of ground types a polymorphic function might be instantiated with is possibly infinite. 
A widely used bridge that collapses is also a failure though. An engineer can ensure that doesn't happen. An amateur can't. That is the difference.
Thanks, now it really clicked for me. Haskell is actually a lot closer to "dynamic" languages than some believe it to be.
But it was poor engineering from the start. Your definition would say it was fantastic engineering until it collapsed.
For strict data types you could imagine creating monomorphic versions, but for lazy ones it's not as simple. A lazy field has to be able to contain both a thunk and an evaluated value. 
My C++ is a bit rusty, but templates are instantiated by virtue of being in the same compilation unit as their use. When you want to separately compile some templated code, you do things like instantiate your templates at the types you expect them to be used at in the library code so the linker can find the instantiation when linking downstream code. This is a lot like how GHC's `SPECIALIZE` pragma gets used. If things get inlined, types can be specialized, boxes can be un-ed, and speed may be had. If inlining is not likely, or inappropriate, you the library author can at least ask for specialized code to be produced irrespective of downstream. Without that, a big non-inlined function has to be compiled based on some representation, so the compiler assumes it will be fed boxes. This is the fallback when further information is not available, rather than a choice. The trend of late seems to be trying to mark as much as possible as INLINABLE (or INLINE) because we've agreed (?!) that compilation time isn't such a burden after all.
Well, we'll have to agree to disagree, because that's a case where I don't want to be exposed to the possibility of failure as a library consumer. 
As a practical matter, polymorphic types can be instantiated to particular specialized versions using type families: http://www.haskell.org/haskellwiki/GHC/Type_families
Since you're using SQLite backend, you can't store something of a type `[Text]` in a single column – no SQLite type is a mapping for `[Text]`. You have (at least to my knowledge) two options and the one you choose depends on your problem: either your model contains one-to-many or many-to-many relations and you need to serialise accordingly, or just serialise the value as a simple `Text`, concatenated with a separator or as a JSON string, which you can parse back into `[Text]` after reading it from the DB.
This would be ideal, but there's not a lot of standardization across APIs. Probably a lesson to take when someone write's the world's next great FPL. That said, for beginners learning the language, it is nice to have a built-in isomorphism between whatever standard text/bytestring type and a list of chars/bytes.
&gt; But for the default Prelude, I want absolutely the simplest and most elegant denotational semantics. Period. The thing is that programmers coming from other programming languages don't care about denotational semantics. They want running programs. I agree that strings-as-char-lists is the morally right representation for thinking about strings. But what you are suggesting requires a [Sufficiently Smart Compiler](http://c2.com/cgi/wiki?SufficientlySmartCompiler). Now, if your language was meant to be purely pedagogical, that would be a different story. But I don't think Haskell is really well-suited for that goal.
A function "leaves :: Expr -&gt; [Int]" would make your definition of (=) much easier to write.
&gt; for teaching purposes. What? You don't like the Haskell numeric tower? :P
Total functions don't get stuck in infinite loops and don't crash. They are defined on all possible non-bottom inputs. Lots of Haskell functions are partial. `head` and `tail` fail on empty lists. `length` fails on infinite lists. However, `map` is total, and works on all lists, finite or not, as does `empty`. (Note the non-bottom requirement. `fmap (+1) undefined` will fail because `undefined` does not reduce to an actual list object).
So, there is a PersistField instance for `[a]` (assuming `PersistField a` of course) which already implements the serialization to strings. I was able to get it to work, but I had to delete my initial database. I'm now asking if there's some way to avoid doing that.
If you open a Haskell file with tab size set to 4 and see f=case 1 of 0-&gt;case 1 of 0-&gt;"" 1-&gt;"buy" _-&gt;"sell" You cannot know if f will return "buy" or "sell" without knowing which lines were indented with tabs and which were indented with spaces (it will return "sell" if the line 1-&gt;"buy" is the only one indented by a tab). If you use a tab size of 8 you will see how GHC interprets the code. For this reason most Haskell programmers will view your code with a tab size set to 8 (even if they like things to be indented by 4 spaces at a time). This includes anyone following the advice of the [GHC Coding Style](https://ghc.haskell.org/trac/ghc/wiki/Commentary/CodingStyle#TabsvsSpaces) document. Knowing that they will be expanded to 8 for most others when they view your code, would you really want to use a tab size other than 8? There has been [talk](https://ghc.haskell.org/trac/haskell-prime/wiki/Tabs) of making tab dependent code a syntax error in GHC. If this happens then maybe things will change (it has been 8 years already so don't hold your breath), for now the message is ["Don't use tabs. At all."](http://urchin.earth.li/~ian/style/haskell.html)
I wonder if this could be improved by having more inhabited kinds to represent different storage behaviours? Thus "*" for boxed types, and something like "Bits 32" for four-byte data that didn't involve further allocation. For example: data Word32# :: Bits 32 data List (a :: *) = Nil | Cons a (List a) data Bits32List (a :: Bits 32) = Nil' | Cons' a (Bits32List a) I'm not sure if we can do polykindism here though, because we'd have to distinguish inhabited kinds (ones that we can use for constructor functions, function types etc.). Maybe a class or something.
That type is an F-coalgebra. Basically, an F-coalgebra is something of the shape: A -&gt; F A ... where `F` is some functor. In this case, you have: A = t a F x = Maybe (a, x)
&gt; NixOS currently has zero benefits over a distro like Funtoo and many deficiencies. Does Funtoo have rollback? I did a few searches and I could find any affirmative answers. A few mailing list answers in the negative but they were all several years old. I often use the rollback functionality that nixos provides since it makes it easy to experiment without consequences.
That looks like noise. There are similar occasional drops over the last several years. Your selection in: https://github.com/NixOS/nixpkgs/graphs/contributors?from=2013-01-12&amp;to=2014-07-01&amp;type=c Starts at such a drop over a year ago. By eye the trend looks upward since mid 2011.
This approach doesn't work. You're exporting everything that's inside Data.Text rather than exporting the module itself. What I'd like to do is to export the qualified module. Consider: module Stuff (module Data.Text, module Data.ByteString) It won't work because of multiple conflicting definitions. Instead I'd like module Stuff (module qualified Data.Text as Text, module qualified Data.ByteString as BS) import Stuff -- now Text and BS are qualified Text.map ... BS.map ...
I'm curious what you mean by this.
Actually, you can get something like that quite easily. Just allocate some memory and start writing to it, and when you're done, turn it into a `ByteString`. That's fundamentally what blaze-builder is doing. I'm surprised that the performance would be slow from blaze. Though I will acknowledge that Kazu modified some of the internals of Warp at some point to use direct buffer modification for example the same reason. I took a crack at optimizing your code above. Instead of explicitly tracking the size of the buffer, I just left that to blaze-builder, which is quite good at that kind of thing. I ended up with: {-# LANGUAGE NoImplicitPrelude #-} {-# LANGUAGE OverloadedStrings #-} import qualified Blaze.ByteString.Builder as BB import ClassyPrelude.Conduit import Data.Conduit.Blaze outputByte :: BlazeBuilder -&gt; Word8 -&gt; BlazeBuilder outputByte builder w8 = BB.fromWord8 w8 ++ builder processAndChunkOutput :: Monad m =&gt; Conduit ByteString m BlazeBuilder processAndChunkOutput = mapC $ foldl' outputByte mempty main :: IO () main = runResourceT $ sourceFile "test.dat" $$ processAndChunkOutput =$ builderToByteString =$ sinkFile "output.dat" On my system, the original code ran in 11.76s, and this version runs in 4.311s. I'm sure it can be sped up some more by using direct buffer manipulation. That example may seem like it won't extend to something more complicated involving state between words, but it's not too difficult to extend it by passing around a state machine instead of just a Builder. For example, the following replaces each pair of consecutive bytes with their sum: {-# LANGUAGE NoImplicitPrelude #-} {-# LANGUAGE OverloadedStrings #-} import qualified Blaze.ByteString.Builder as BB import ClassyPrelude.Conduit import Data.Conduit.Blaze data S = State0 BlazeBuilder | State1 BlazeBuilder Word8 outputByte :: S -&gt; Word8 -&gt; S outputByte (State0 builder) w = State1 builder w outputByte (State1 builder w1) w2 = State0 (builder ++ (BB.fromWord8 $! w1 + w2)) processAndChunkOutput :: Monad m =&gt; Conduit ByteString m BlazeBuilder processAndChunkOutput = loop (State0 mempty) where loop s1 = do mbs &lt;- await case mbs of Nothing -&gt; return () Just bs -&gt; do (builder, s2) &lt;- case foldl' outputByte s1 bs of State0 builder -&gt; return (builder, State0 mempty) State1 builder w -&gt; return (builder, State1 mempty w) yield builder loop s2 main :: IO () main = runResourceT $ sourceFile "test.dat" $$ processAndChunkOutput =$ builderToByteString =$ sinkFile "output.dat"
iOS software is almost all cross-compiled. (by usage, anything not cross-compiled is just a statistical error) The ghc-iphone people may have some experience that can help you or you may have luck with other people compiling to iOS: http://www.haskell.org/haskellwiki/IPhone
It can't be much worse than building on a laptop 15 years ago. ;) 1. Start build 2. Go to bed 3. Get up in morning, note build failed due to inadequate disk capacity. 4. Clean off rest of disk, start build again. 5. Go to work. 6. Come home to some linker error on a submodule that you will never use. 7..5000. Wrestle with it until you disable enough functionality to get a "successful" build. 5001. Write simple hs program that segfaults the compiler. Be sure to put a waterblock and a big fan on the phone. :)
Ah, I see. Yes, this is indeed OCaml-like, and something we don't have in Haskell land.
Oh I see. I learned monad transformers way back when `mtl` was more or less the One True Way, so I've never really paid much attention to `transformers`. I thought it was just a strict subset of `mtl`.
Hm, couldn't you even use `Cont` to turn any parser over a finite alphabet with only `Applicative` interface into a `Monadic` parser in a mechanical way?
OK, color me surprised. I thought that using raw buffers would be faster than builders, but not this much faster. The following code runs in 0.108s on my system, vs 4.3s for the builder version I pasted two hours ago: {-# LANGUAGE NoImplicitPrelude #-} {-# LANGUAGE OverloadedStrings #-} import ClassyPrelude.Conduit import Data.ByteString.Internal (ByteString (PS), mallocByteString) import Data.ByteString.Lazy.Internal (defaultChunkSize) import Data.ByteString.Unsafe (unsafeIndex) import Foreign.ForeignPtr.Safe (ForeignPtr) import Foreign.ForeignPtr.Unsafe (unsafeForeignPtrToPtr) import Foreign.Ptr (Ptr) import Foreign.Storable (pokeByteOff) data S = S (ForeignPtr Word8) (Ptr Word8) {-# UNPACK #-} !Int newS :: IO S newS = do fptr &lt;- mallocByteString defaultChunkSize return (S fptr (unsafeForeignPtrToPtr fptr) 0) processChunk :: ByteString -&gt; S -&gt; IO ([ByteString], S) processChunk input = loop id 0 where loop front idxIn s@(S fptr ptr idxOut) | idxIn &gt;= length input = return (front [], s) | otherwise = do pokeByteOff ptr idxOut (unsafeIndex input idxIn) let idxOut' = idxOut + 1 idxIn' = idxIn + 1 if idxOut' &gt;= defaultChunkSize then do let bs = PS fptr 0 idxOut' s' &lt;- newS loop (front . (bs:)) idxIn' s' else loop front idxIn' (S fptr ptr idxOut') processAndChunkOutput :: MonadIO m =&gt; Conduit ByteString m ByteString processAndChunkOutput = liftIO newS &gt;&gt;= loop where loop s@(S fptr _ len) = do mbs &lt;- await case mbs of Nothing -&gt; yield $ PS fptr 0 len Just bs -&gt; do (bss, s') &lt;- liftIO $ processChunk bs s mapM_ yield bss loop s' main :: IO () main = runResourceT $ sourceFile "test.dat" $$ processAndChunkOutput =$ sinkFile "output.dat"
So you think this code: returnValue = maybe (error "library specific errror msg") id myMaybe ... is different from this code in some significant way in how it interacts with the outside world: returnValue = fromJust myMaybe Lets look at [the source](https://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-Maybe.html#fromMaybe): maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b maybe n _ Nothing = n maybe _ f (Just x) = f x fromJust :: Maybe a -&gt; a fromJust Nothing = error "Maybe.fromJust: Nothing" -- yuck fromJust (Just x) = x Note, I did not write the comment, the implementer did, because they thought the code sucked. The only difference between the two functions is that in the `maybe` case, I actually get to specify an error so if I am not infallible the user is at least exposed to a slightly more helpful error message. Also, the even simpler choice here is the ever helpful fromMaybe (error "library specific error") myMaybe
Ah, thank you for that link, which led me to rediscover [elastic tabs](http://nickgravgaard.com/elastictabstops/). For some reason I couldn't find that page using Google-fu.
Nice - bye bye Text.Printf.
&gt; You cannot know if f will return "buy" or "sell" without knowing which lines were indented with tabs and which were indented with spaces (it will return "sell" if the line 1-&gt;"buy" is the only one indented by a tab) As I've said (many times now), if you *just use tabs for indentation*, and *just use spaces for alignment* you'll be fine. In this case your example works fine if all your indentation is done using either just tabs or just spaces. Mixing tabs and spaces for indentation or alignment will cause you problems in almost every programming language! A real example: [Here is code indented using tabs and aligned using spaces](http://imgur.com/a/ZJNBe). On the top, I've used the standard tab indentation size of 4. But I could like my tab size to be 8 (or any other number) so I can now change it. Since I've indented with tabs (to before the `where` starts) and aligned with spaces (to line up `psfold`) everything is fine. [You can also just aggressively indent so you don't need to align](http://imgur.com/a/kXq2T), which also works. 
&gt; Mixing tabs and spaces for indentation or alignment will cause you problems in almost every programming language! But in most other languages it will just look bad, but the code will produce the only result the person looking at it could expect. In Haskell it may look like it does one thing, but actually do another. &gt;Here is code indented using tabs and aligned using spaces. On the top, I've used the standard tab indentation size of 4. But I could like my tab size to be 8 (or any other number) so I can now change it. Since I've indented with tabs (to before the where starts) and aligned with spaces (to line up psfold) everything is fine. You can also just aggressively indent so you don't need to align, which also works. You are able to view other peoples Haskell code with a tab size of 4 because they are not using tabs. You can edit your own code and make it work right because you are careful and know the risks. But when I look at your code I would like it to be indented by 4 spaces also. Unfortunately that is not safe (how do I know you or someone else who worked on it did not slip up and put spaces in), so I have to use a tab size of 8 instead and your code looks more indented then either of us would like. If GHC treated code that depended on the tab size as a syntax error I might feel differently.
Use a type family to pick the most efficient/appropriate representation for the types you're interested in, that should do the trick.
Does `conduit`s have stream fusion? If not, `pipes` might buy you some very small fractions of a second.
I think someone has an axe to grind and we're giving them just enough information to do so.
Cool! Thanks; those are great tips :) I'll get right on 'em.
Thank you much. This helped me. I was stumped all day.
[fixplate](http://hackage.haskell.org/package/fixplate-0.1.5/docs/Data-Generics-Fixplate.html) supports [generically drawing any simply-recursive fixed point type](http://hackage.haskell.org/package/fixplate-0.1.5/docs/Data-Generics-Fixplate-Draw.html) (both graphviz and ascii)
Thank you for the builder and raw buffer implementations! ;-) I think appending single bytes is a kind of worst-case for most builders. I'm just going through your code and finishing up something I tried earlier, will post a full benchmark program later today on GitHub / this thread. I really didn't expect this to be so tricky, it kinda seemed like a fairly basic use case to me. btw, would you consider adding a ByteString input variant of conduitVector to the conduit combinator library? I think the idea of building a fixed-size vector from an incoming stream is very useful, but doing it with single values as input is just too slow for many situations.
&gt; deal with all that math stuff Bad teaching is to blame for this.
Good catch -- I've fixed it now. I'm using file handles so I can set universalNewlineMode on it; my input file appears to have CRLF line separators, but I'm on OS X. I think I get your solution, more or less. I'll spend more time scrutinizing it later. I ended up rewriting my code quite a bit as well, after some chatting on #haskell and some searching of stackoverflow: https://github.com/int3/scrabble-solver/blob/94bcd4db293b280a6e684a677e244340ae5f1d7d/Trie.hs#L42
As for your edit: It looks like you had a data type like: share [mkPersist sqlSettings, mkMigrate "migrateAll"] [persistLowerCase| User ident Text password Text |] And changed that to: share [mkPersist sqlSettings, mkMigrate "migrateAll"] [persistLowerCase| User ident Text password Text questions [Text] |] Persistent tried to automagically migrate your data to the new format, however, if you had any rows in that table that is going to fail: it doesn't have a value for the `questions` field to use for the rows already in the table. This means SQLite threw an error, as the `questions` field was defined `NOT NULL`. There are three solutions: allow `questions` to be `NULL` (making it a `Maybe [Text]`): share [mkPersist sqlSettings, mkMigrate "migrateAll"] [persistLowerCase| User ident Text password Text questions [Text] Maybe |] Second, you can provide a default value for `questions` (which is only used when migrating): share [mkPersist sqlSettings, mkMigrate "migrateAll"] [persistLowerCase| User ident Text password Text questions [Text] default="\"[]\"" |] (Edit: It seems the value is passed directly to SQLite, so it appears to need quoting like this.) If that doesn't work for you either, you'll have to do the migration manually. There are a couple of functions in `Database.Persist.Sql` to get a list of the SQL queries it would have run to migrate, allowing you to modify them to deal with the new column however you want.
Neither conduits nor pipes have stream fusion. I looked at using stream fusion in conduit, and it doesn't actually provide any optimization (at least none I could find). If you're talking about rewrite rules, those exist in both conduit and pipes, but are not the bottleneck that SirRockALot1 is running up against.
This comment was reasonable when it was made.
Thanks for the post, as I've been struggling with the similar problem too. In my case, I'm splitting files using Shamir's Secret Sharing Scheme. input file ==&gt; process ==&gt; multiple output files but I needed randomness in my process, thus I have been doing the following -- Create a source for splitting a file -- The source will split the input file into individual bytes, and yielding each byte along with a RNG seed sourceSplit :: (MonadResource m, MonadIO m) =&gt; FilePath -&gt; Source m (StdGen, Word8) sourceSplit fp = getZipSource $ (,) &lt;$&gt; ZipSource sourceStdGen &lt;*&gt; ZipSource (CB.sourceFile fp $= CC.concat) So it seems like we've made the same mistake by splitting the `ByteString` into `Word8` instances. I don't necessarily need each Byte to be paired with a random source, as I can most likely pair a `StdGen` with a chunk of `ByteString` and do a monadic map over the chunk instead. However I'm struggling to understand how I can achieve this without manually specifying the chunk size. What do you guys suggest? 
Ah, shortcut fusion was the term I was looking for. 
That's an interesting can of worms I'd like to preserve for another thread. Especially Haskell's treatment of IEEE 754 doesn't sit well with me.
No, the strategies aren't an option at the moment. Is your build system a Shake build system? If so, do you want to raise a ticket on the github bug tracker (https://github.com/ndmitchell/shake/issues). I'm switching to continuations right now, so don't want to disturb anything else. But once I've finished that, I'd be interested to see the effect of alternative strategies, and perhaps longest-to-run might always be the best now.
I wrote about this in 2009, http://www.scribd.com/doc/19637058/Improving-Data-Structures We rely on the uniform representation to give us polymorphism, but specialization of data types is possible via type families (for strict types). It can have good performance impacts.
That's a good point. I'm also not sure conduit is the best abstraction for this kind of code, but it's certainly the best structure I could think of for this type of compression pipeline. These types of loops like in your example always seem simple when all you're doing is some very rigid fold or something like this, but I'm not sure I'd know how to generalize this into something really practical, like where I can simply say 'outputByte' somewhere in my conduit and have it append a byte and ultimately yield a chunk. I always got pretty bad performance from most 'builder' type things in Haskell, certainly nowhere near what I'd expect from simply filling a chunk of memory. I think for this example I'll got with actual mutable memory.
Do you think it's possible to get performance from a Builder close to the direct Ptr mutation version that I pasted [above](http://www.reddit.com/r/haskell/comments/29nvsx/how_to_get_good_performance_when_processing/cinba55)? I'd hope we can do better than the results I showed, but I'm concerned that appending a bunch of single bytes may be too much overhead for Builders. And to be fair: my test was with blaze-builder, not the newer bytestring-builder. So perhaps just that switch would increase performance significantly.
You should use the Data.Sequence . Its more efficient in that scenerio
[haste-perch on hackage](https://hackage.haskell.org/package/haste-perch) Hmm, maybe this is too similar to Alberto's [hplayground post](http://www.reddit.com/r/haskell/comments/29lapx/hplayground_write_haskell_code_for_the_browser/) yesterday, which I just noticed.
I have no idea whether it would be a good idea in the general case, but I'm probably pretty far from that.
I can see it being quite possible that it is a good idea in the general case. Your build system is probably at the extreme of having one file much slower than the others, but it could help generally.
If you want to see something that's really surprisingly "dynamic", take a look the Java bytecode and JVM specifications. The Smalltalk/Self heritage is pretty obvious at that level. Personally, using boxed representation so you can compile generic code to a single object code rather than needing some kind of specialization reminds me of old-school C libraries passing around `void *` more than anything.
Is there a whole program optimizing compiler for Haskell then?
Instead of making PerchM an illegal monad, you should use something along the lines of the writer monad. It turns a monoid into a monad in a nice way.
It's not dynamic. Ghc is simply choosing a fixed representation for the polymorphic data structure. There are no runtime checks.
The JVM heap layout is pretty similar to GHC's as I understand it, but most people wouldn't call Java a "dynamic" language either. Maybe you are looking for "managed" versus "unmanaged".
We need compiler generate instances though. Type families work as long as you only have one type argument, but as soon as you have two (e.g. in a map with unboxed keys and values), you have to write O(n^2) instances. We could have GHC generate the instances at the type use site instead. This is essentially what C++ templates do.
The closest to this is [hplayground](http://www.reddit.com/r/haskell/comments/29lapx/hplayground_write_haskell_code_for_the_browser/)
Indeed, while I don't have any experience with writing real programs in dependently-typed languages, it's my intuition that many of the classic DT examples like `Vec n a` build too much information into the types that would be better carried alongside the type as a proposition. I don't want a structurally-of-length-11 list because I can't pass it to another library's function that wants a structurally-of-odd-length list without a run-time conversion(!) not to mention that I have to insert a conversion function into my code that someone had to write for this particular purpose (since there are many different such functions!). I'd rather have a pair `(xs : [a], p : length xs = 11)` where I can trust the compiler to write `(length xs = 11) -&gt; (odd (length xs))` for me, since proofs are irrelevant. If I want a `Vec n a` type (e.g. to talk about a list whose elements are lists of the same length) I can always recover it via `Vec n a = (xs : [a], length xs = n)`. Is there a downside I'm missing?
Stackoverflow question, check referal link, check Looks like someone is going after their promoter badge:)
either it would not be the same syntax or it would be necessary to define different combinators for the monoid elements and for the monadic elements. The key of the compatibility of both is: mappend == &gt;&gt; that´s why each combinator can be used both ways. With the writer monad I have to define two different sets of combinators. For example: p = nelem "p" -- for the monoid combinators p' = put $ nelem "p" -- for the monadic combinators and also run them with different primitives runPut etc...
How about the following: newtype JSBuilder_ = JSBuilder_ { build :: Elem -&gt; IO Elem } deriving Typeable instance Monoid JSBuilder -- implementation omitted data JSBuilder a = JSBuilder JSBuilder_ a instance Monad JSBuilder where JSBuilder m0 x &gt;&gt;= f = let JSBuilder m1 y = f x in JSBuilder (m0 `mappend` m1) y return = JSBuilder mempty instance (Monoid m) =&gt; JSBuilder m where mappend = lift2M mappend mempty = return mempty and then just simply using JSBuilder and not JSBuilder_?
These look brilliant. I'll watch 'em as soon as I can. Many thanks! :)
The second edition, for all the reasons in the top answer. Although I've been hearing rumours of a 3rd edition in the not too distant future... I have a big graph of book recommendations (in my head) for various areas of FP / PLT study, and Introduction to Functional Programming using Haskell by Bird (the second edition) is the second-most recommended books out of all of them, and it's a prerequisite for a lot of paths through that graph. Incidentally, the book I most recommend, which is right at the start of many of the paths through the graph, is How To Prove It by Velleman. One day I'll turn that graph into a Snap based webapp...
Well, I am not familiar with `conduit`, so I can't tell you the good way to do this, but there's always `unsafePerformIO`. If you take care so that it doesn't get memoized, you could make it work. If nobody tells you a better way, just try that. Note that you are gonna have to use special pragmas for that.
&gt;I've heard it's better to read the first edition of "Introduction to Functional Programming" by Bird &amp; Wadler than the second edition. maybe he/she heard from here [Philip Wadler and Erik Meijer: On Programming Language Theory and Practice](http://youtu.be/9SBR_SnrEiI)
Care to share the graph as is?
If `m` is a monad then `m ()` is a monoid with `mempty = return ()` and `mappend = (&gt;&gt;)`. And if `w` is a monoid then `Writer w` is a monad of course, and then `Writer w ()` is isomorphic to `w` with the same monoid structure. So hide your underlying monoid type `w` and export a newtype `W` of `Writer w` and use `W` as your monad and `W ()` as your monoid. Then you will have `mappend = (&gt;&gt;)` again, but `W` is now a real monad. This is the same thing tailcalled said I think, but with less code and more words.
It's large enough that it's not feasible to share at shore notice, and probably best presented as a suggested path given an area of interest and a starting point. If / when I get around to writing the webapp, I hope it'll have the ability to have multiple people contribute their graphs / paths and share / annotate them. 
How is that different from have a NonZero class, where the constructor asserts that the parameter isn't zero?
Sorry about the length of this comment. You make an excellent point about the newness of the idea of programming languages. It's been less than a century since Grace Hopper did her work, and it's too presumptuous of me to assume that we will never get it right in the future just because no one has gotten it right yet. I am excited to see what improvements to language design the future holds, and I shouldn't preemptively crap on them. I haven't worked with dependently typed languages themselves; my concern is more generally with languages whose compilation is undecidable. I have extensive experience with C++. Parsing/compiling templates in it is a nightmare (yes, I have tried to both make a C++ compiler and to modify an existing C++ compiler; neither project got very far), and debugging errors involving these templates can be a very difficult task. I was also thinking of Perl when I wrote those comments (parsing Perl is undecidable; you can write lines in which the parse tree is radically different depending on the arity of the functions mentioned, which in turn can depend on arbitrary previous computations). Although I haven't tried working on a Perl compiler/interpreter myself, my impression is that mere mortals are unable to create or modify them, either. My concerns are described in my previous comment. I consider a language feature to be horrible if it leads to compilation problems that don't clearly tell the programmer what the problem is and where to fix it. If you let people use horrible features, some of them will. Others will make style guides that forbid the use of certain language features to prevent horrible things (which results in a re-education process of teaching the language to people who already know the language). Some of the horrible things will go into libraries that others get stuck using, and will thus tend to have obtuse/inscrutable compilation errors (or just compilation timeouts, which suggest but do not guarantee that errors exist but which give no information about what or where they are). The way to avoid all of this is to restrict the programming language not to allow horrible things, and I think Haskell does this really well. An example of a horrible rule in C++ leading to wise people saying "do not use this" is the [Google style guide](https://google-styleguide.googlecode.com/svn/trunk/cppguide.xml)'s advice on template metaprogramming: &gt; Some Boost libraries encourage coding practices which can hamper readability, such as metaprogramming... In order to maintain a high level of readability for all contributors who might read and maintain code, we only allow an approved subset of Boost features... &gt; some C++11 extensions encourage coding practices that hamper readability—for example... by encouraging template metaprogramming... the following C++11 features may not be used: ... Similarly, the features of Perl that let you sneak in undecidability tend to be full of long strings of punctuation and thus are also widely considered unreadable unless you're already a Perl wizard. So, can you show me an example (whether it's dependent typing or something else) of a language whose compilation is undecidable but which nonetheless is straightforward to read/parse/compile and whose error messages are easily understood as pointing to the part of your code that is problematic? It's not good enough to say "the programmers must stay in this subset of programs in the language and not venture over to the subset that has unreadable errors" because people are going to venture over there anyway. If not, you still have a great point about how just because we don't have one yet doesn't mean we never will. I'll just be leery of any new attempts until I see one that finally works well.
Okay, if your experience with compile-time computation is C++ and Perl, I can understand why you may feel a bit burned. The main problem is that the features that C++ and Perl grew for compile-time computation weren't _designed_ for compile-time computation. I don't know as much about Perl, but in C++, templates were designed to fix a real problem (e.g. C++ templates were designed to fix C's horrible polymorphism system), but sort of grew more and more features to fix more and more "one more problem"s until they became Turing complete by accident. On the other hand, I think most of the people in this subreddit are talking about languages that were designed, up front, with compile-time computation in mind, and were designed by programming language experts. So I encourage you to give one of them a try before you totally despair. I believe you will find that the modern languages are much more usable than you guess. There are certainly still some rough edges. I'm not going to pretend it's a solved problem; several of my colleagues have proposed Ph.D.s centered around making dependently-typed programming languages more usable in various ways, and I know in my field that roughly half of newly-minted PL Ph.D.s did their dissertation on dependent types in one way or another. There's a long way to go for sure. But give one of them a try. I'll suggest two below that I think are particularly well done. The first is Agda. Of the two, it has had significantly more man-hours invested in tooling, so you may find that there is more documentation, better error-reporting, better editor plugins, etc. On the other hand, it was designed as a proof assistant, and there are some places where that's a visible disadvantage -- one of my colleagues decided to double the RAM in his computer (which was already a gaming rig) to be able to use the Agda compiler. Supposedly they intended to implement hash-consing, with some people guessing that this would significantly reduce memory usage, but I don't know the progress on that project. My second recommendation is Idris, which was designed from the ground up to support programming. One result is that it focuses much more on giving the programmer access to efficient data types. On the other hand it is a much smaller project. I don't have as much direct experience with this one, but I have heard many good things from other people who have tried it.
rwbarton and tailcalled: thanks. I see. In essence, the trick is to define the monad instance first and then derive the monoid instead of the other way around. Although yours is better because it avoids the ugly unsafeCoerce, somehow the result is the same. I mimicked the trick done in blaze-html without thinking too much. I also can define (&gt;&gt;=) in the current code and make it a legal monad, but I copied the error message from blaze-html. Later I though that that is better to leave at it is and not include a valid bind. I was even tempted to define JSBuilder as a monad transformer, so it could be used as a general monad usable in a framework but that is not good for some purposes: For example: in the hplayground framework, there is a sort of post processing of the rendering that can not be done if the Perch sentences are directly executed. for example wcallback drop all the rendering generated previously. That is possible because the processing is executed fully before the rendering is evaluated. If the processing goes at the same time than the creation of the DOM tree, that would not be possible.
I think that a good (and legitimate) haskell extension would be the use of the do notation for monoids as long as only &gt;&gt; is used. The compiler can detect &gt;&gt;= and report a "No monad defined for...." That would avoid making people say sorry to the god of category theory. The do notation is very convenient because it avoid a lot of parenthesis that are necessary when monoids are used to build a tree. Such is the case of HTML combinators, but also in the case of any kind of tree structure in general. That is the reason why monoids are not very interesting for the constructions of trees. In fact Perch is a generalization of the builder used in blaze-html (that construct a single branched tree, a bytestring). And similar construction can be used to build any N-tree (I´m tempted to generalize it). The usage becomes difficult using monoid notation when the tree has some complexity. With do notation it looks easy and natural. By the way, it is possible to create perch accessors and modifiers besides builders. 
An assertion happens at run-time, while dependent types sorts that kind of thing out at compile-time. Getting run-time assertions at compile-time is literally the only major difference, but it is a big deal.
Ah, shoot. Should've figured it'd already exist in some fashion.
ATS springs to mind: http://en.wikipedia.org/wiki/ATS_(programming_language) It's sort of like C with ML syntax and functional style and dependent types that natively grok things like pointer manipulation at compile time, avoiding the worst offender in any C program: memory corruption, overflow, leaks, null pointers, etc. The syntax is a veritable Lovecraftian nightmare, though, so I wouldn't want to program in it without first making the syntax non-grotesque.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**ATS (programming language)**](https://en.wikipedia.org/wiki/ATS%20(programming%20language\)): [](#sfw) --- &gt;__ATS (Applied Type System)__ is a programming language whose stated purpose is to support theorem proving in combination with practical programming through the use of advanced [type systems](https://en.wikipedia.org/wiki/Type_system). The performance of ATS has been demonstrated to be comparable to that of the [C](https://en.wikipedia.org/wiki/C_(programming_language\)) and [C++](https://en.wikipedia.org/wiki/C%2B%2B) programming languages. By using theorem proving and strict type checking, the compiler can detect and prove that its implemented functions are not susceptible to bugs such as [division by zero](https://en.wikipedia.org/wiki/Division_by_zero), [memory leaks](https://en.wikipedia.org/wiki/Memory_leak), [buffer overflow](https://en.wikipedia.org/wiki/Buffer_overflow), and other forms of [memory corruption](https://en.wikipedia.org/wiki/Memory_corruption) by verifying [pointer arithmetic](https://en.wikipedia.org/wiki/Pointer_arithmetic) and [reference counting](https://en.wikipedia.org/wiki/Reference_counting) before the program compiles. Additionally, by using the integrated theorem-proving system of ATS (ATS/LF), the programmer may make use of static constructs that are intertwined with the operative code to prove that a function attains its specification. &gt; --- ^Interesting: [^Type ^theory](https://en.wikipedia.org/wiki/Type_theory) ^| [^OCaml](https://en.wikipedia.org/wiki/OCaml) ^| [^Dependent ^type](https://en.wikipedia.org/wiki/Dependent_type) ^| [^ML ^\(programming ^language)](https://en.wikipedia.org/wiki/ML_\(programming_language\)) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ciobiyo) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ciobiyo)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I'll start. My vote is #5 Erlang as language of choice for new platform OS from scratch. Ideally, a modern OS language still needs to be fast (comparable to C) as well as allow concurrency for cloud-based apps and virtual machines, fault tolerant (no blue screen of death), encourage parallelism from multiple CPU cores with optimized pipelines, a proven track record for platform-agnostic virtual machine (like Java and Erlang), and preferably static-typed and obviously compiled. For the above reasons, I think an OS written in Erlang seems ideal. Golang supported by Google is similar. Haskell would be interesting for an OS from scratch. A logic paradigm language for an OS also seems interesting to me. Thoughts? Any good reasons to choose an OO language (Java?) or mixed paradigm for the task?
C, but I'd love to see anyone try with Haskell
I'll vote for #11 Rust. Rust is designed to be a modern systems programming language - this will be a great test of it. The parts you can't do in Rust, do in C. 
House is a research OS made in Haskell time ago (2005) http://programatica.cs.pdx.edu/House/ Things now sould be easier. there are haskell compilers designed to run on bare metal like ajhc: http://ajhc.metasepi.org/ The category theoretical abstractions should help massively reduce the complexity of the kernel and increase the factorization of the libraries in any aspect: threading, debugging, IO etc. That would have advantages that now are unknown. Even processing speed can improve thanks to optimizations than may appear once the factorization is done.
Thanks though I will avoid this at all costs, it's still good to know.
Awesome, thanks very much!
It really depends on your objectives in writing this 'new OS'. Microsoft's focus with Windows, for the longest time, was on backwards compatibility -- the tensions between that and (what they think is) innovation are noticeable every major release. Apple's objective in Mac OS X's objective, as I understand it, is user experience, no matter the cost to the feature set. The choice for a programming language is relevant here. Well, more precisely, you can (probably) base the motivation for a language of choice on the motivation behind the design of the operating system itself. * Do you want to get the job done quickly by reusing a lot of libraries? Pick C, because most FFIs are based on C-style (or even Pascal-style) namespaces. * Want to take some 'everything is an X' to the extreme (much like [Plan 9 from Bell Labs](https://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs))? Pick Lisp, where everything is an S-expression, or a logic programming language, where everything is a proposition. * Do you want a lot of architecture-independent object code, or do you think using a VM for security principles? (Basically, are you reinventing Microsoft's [Singularity](https://en.wikipedia.org/wiki/Singularity_%28operating_system%29)?) Picking Java or Scala would make sense if the JVM wouldn't suck as hard as it does. You could do this for every programming language on your list. The exception here would be assembly language. With the computational power available on build platforms, you really *can't* justify the use of assembly language in places other than architecture-specific procedures (like the first steps of a boot loader or the system call interface).
I thought Erlang was usually interpreted? It's also not known for being very fast at non-concurrent tasks.
It *should* be possible to do it all in Rust, using `unsafe` blocks. Of course you wouldn't get the full safety of proper Rust, but it's surely better than nothing.
example: http://rust-class.org/
i don't know about "ideal" but it is interesting: i've been keeping an eye on the Red language, which aims at being a "full stack" language, being appropriate for both low and high level programming. The primary features of interest to programming language nerds are homoiconicity and a focus on DSLs (called "dialects" in that community.) http://www.red-lang.org
Just adding a few choices for option 13. **D** http://wiki.osdev.org/D_Bare_Bones http://wiki.xomb.org/index.php?title=XOmB_Bare_Bones **Oberon and derivatives (Currently Active Oberon)** http://www.inf.ethz.ch/personal/wirth/ProjectOberon/ http://www.ocp.inf.ethz.ch/wiki/Documentation/WindowManager **Ada/SPARK** http://www.cubesatlab.org/ http://marte.unican.es/ http://www.rtems.org/ http://archive.adaic.com/projects/atwork/boeing.html **C++** Genode OSv OS/400 Symbian BeOS Mac OS X device drivers (IOKit) Supported in Windows Kernel code since Windows 8 
I don't think you should leave D off the list. It is targeted at the same problem domains as C++, and it strikes me as a very well thought-out alternative. I think it would be a great choice for this task.
Since you want to implement the low-level stuff in the chosen language, I guess GHC Haskell is out because its runtime needs to be implemented in some other language. [HaLVM](https://github.com/GaloisInc/HaLVM), for example, allows Haskell code to run without an OS, and it does so by implementing the GHC runtime on top of the primitives exposed by Xen. I assume they wrote those in C. They still get to write most of the low-level code in Haskell, including drivers.
http://this-plt-life.tumblr.com/post/41441254767/when-i-checked-out-rust
I'd vote for a merger of idris and disciple.
Having to go through the Beam VM to run your OS kernel sounds like a horrible experience. You really want something that compiles to native code for this kind of work.
I think I remember looking at this at some point before. What about just having a `Conduit` that adds a same-length `ByteString` into a tuple based on a random seed? Something like: {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE NoImplicitPrelude #-} import ClassyPrelude.Conduit import qualified System.Random.MWC as MWC addRandomBS :: MonadIO m =&gt; Conduit ByteString m (ByteString, ByteString) addRandomBS = do gen &lt;- liftIO MWC.createSystemRandom mapMC $ \bs1 -&gt; do -- Note: if we used my mutable-bytestring code above, -- we wouldn't need a repack. bs2 &lt;- liftIO $MWC.uniformVector gen (length bs1) return (bs1, repack (bs2 :: UVector Word8)) main :: IO () main = yieldMany ["foo", "barbar", "baz.", "binnn"] $$ addRandomBS =$ mapM_C print
When reading the title I immediately thought: Rust.
I feel like [BitC](http://en.wikipedia.org/wiki/BitC) should be mentioned, even if not suggested. &gt; BitC is a systems programming language developed by researchers at the Johns Hopkins University and The EROS Group, LLC as part of the Coyotos project. It aims to support formal program verification. &gt; The language has two primary objectives: &gt; &gt; 1. To merge the advances of modern programming languages; sound type systems with abstraction, sound and complete type inference, let-polymorphism, and mathematically grounded semantics — with the requirements of systems programming; first-class treatment of state, support for prescriptive low-level representation, explicitly unboxed types, and performance comparable to C. &gt; &gt; 2. To support formal program verification of low-level systems programs, such as kernels/microkernels. The author abandoned development on it two years ago, [saying it had insurmountable flaws](http://www.coyotos.org/pipermail/bitc-dev/2012-March/003300.html). I imagine that the experiences they had with BitC are worth learning from. Edit: more content.
&gt; How To Prove It by Velleman. That book is a very good introduction to mathematics.
Out of all the languages I've seen Rust is probably the only one that could one day replace C. That said, I still truly love C. It's small, simple, and exceptionally good at the things it was designed to do. Besides, it was the first language I ever really learned.
I think the term you may be looking for is "bare-metal programming". Take a look at the [MirageOS](http://openmirage.org), a Univ. of Cambridge project. It is OCaml-based low-level code. 
While that version was abandon and he took some time off, there is still continued effort on a BitC v2. I believe this is the message that kicked off renewed development, http://www.coyotos.org/pipermail/bitc-dev/2013-January/003531.html
I would add the Red language to the list, as one of their design goals is to crete a language suited for both device-level and high-level stuff: http://www.red-lang.org It is based on Rebol
Ever since getting into Haskell I've been really cynical about other languages...I finally read the Rust docs yesterday and they're surprisingly solid! Props to Mozilla for keeping with the times, at least to a degree.
http://erlangonxen.org
&gt; Rust is designed to be a modern systems programming language which cannot even do TCO 
You can (and should) easily write type-safe wrappers around any unsafe code. Still doesn't help you if the unsafe code turns out to be broken somehow, but you can at least know that it's not the rest of the code because that's type-safe. So basically, you can have a small unsafe core that you can verify works through tests, type-safe wrappers around that, and you know the rest of the code isn't the source of the issue if memory corruption or a segfault happens.
C is the most realistic (tools, ecosystem, etc.) but ATS has the most appropriate semantics. D is a nice middle ground.
Rust, Idris, and ATS spring to mind as decent contenders, though I've never actually programmed in any of those languages so take this with a grain of salt. Haskell might be a reasonable choice, though I think laziness might get in the way more than it helps. Ultimately, I think the general OS design is more important than the implementation language. I've been having nagging doubts for awhile about the UNIX way of doing things. I think it's true what they say that "those who don't understand UNIX are doomed to re-implement it poorly", but that doesn't mean that it can't ever be improved upon by those who do understand UNIX well enough to realize we can do better. Some examples of things I'd like to see: * Transactional semantics for filesystems. It should be possible for a shell script to fail part-way through some major filesystem operation, and for the system to revert everything back to the way it was before. * Strong typing for files and pipes. Say I have a program that ouputs only pngs and another that inputs only jpegs. Connecting the former to the latter with a pipe should result in a type error. I should also be able to insert wildcards into a pipeline and have the OS automatically do a Hoogle-style search for a program with the appropriate input and output types and insert that in place of the wildcard. * The OS should be able to distinguish programs with side-effects and those without. For example, a lot of Unix utilities are (depending on what arguments they're given) pure functions that read from stdin and write to stdout, and it would be nice if the OS could enforce purity when appropriate by restricting system calls.
My gut reaction was Ada, too.
I'd personally vote for SparkAda, or possibly C with suitable techniques to ensure correctness. We know that SparkAda can reach the same performance as low-level C programs in some cases, as evidenced by the SparkAda implementation of Keccak. There's also the option of using some of the techniques used in CompCert. 
One negative review here: http://llaisdy.wordpress.com/2014/04/26/review-beginning-haskell/
Let's not theorize it, ok? There's exactly one compiler. And people behind the language/the compiler don't want to do it (“too complex”). 
Very cool looking language. This is the first time I've seen it.
[...yet.](https://github.com/rust-lang/rfcs/pull/81) (It's [postponed ](https://github.com/rust-lang/meeting-minutes/blob/master/weekly-meetings/2014-05-20.md#tail-calls) for now, but planned for discussion later.)
I'm interested in an example for this, it's unclear to me how this would work.
Why does the title say dynamic programming instead of dynamic optimization?
a cost model should be part of the language spec, and proper tail calls are very usefull to have in your cost model. I like rust, and I understand the reasons for not having proper tail calls (first and foremost to maintain the c calling convention) but it is design decision worth complaining about.
You might be interested in [The Haskell School of Music](http://www.cs.yale.edu/homes/hudak/Papers/HSoM.pdf).
{-# UNPACK #-} and ! both fields of DPState. After that, compile with profiling on, and get some numbers. Might be some places where MagicHash will help.
Thanks! :D
[This](http://www.haskell.org/haskellwiki/Haskore) might be an alternative to Overtone for using with Supercollider. Actually, it seems like that has been placed by [Euterpea](http://haskell.cs.yale.edu/euterpea/). There was also a great paper posted here a month or two back that tied together more advanced music theory than I received in high-school with some category theory that I've only recently been exposed to. I can't remember the name though. :(
[This](http://arxiv.org/abs/1204.3216) is not the paper I was thinking of, but it covers the same ideas, but is not nearly as approachable. :/
The [Haskell Music Suite](http://music-suite.github.io/docs/ref/) is quite impressive. It uses some more advanced Haskell idioms than the Haskell School of Music, like Lens and type classes, although it's no more complicated to actually compose with AFAIK. I've been trying to write some harmonic analysis software while learning Haskell at the same time, and I started with Euterpea (the Haskell School of Music library) and then kind of graduated onto the Music Suite because it helped me learn more advanced Haskell, and because the more powerful abstractions make the analysis code cleaner.
Hey, thanks! :) Do you know where I can find more information on these projects? I'm struggling to find any.
I recently wrote [Bang](https://github.com/5outh/Bang), a drum machine DSL in Haskell, and it was a ton of fun. Also, the Haskell School of Music has been mentioned a few times in this thread -- [Euterpea](http://haskell.cs.yale.edu/euterpea/) was the product they produced. Check that out! Edit: Also, what is Tidal lacking that you need? Just curious.
Thank you. Is there more information on Euterpea? All I can find is that page. Sorry, I'm still wrapping my head around how the Haskell community is structured. Tidal is (as far as I can tell) a DSL for describing patterns, while I was looking more for something like Overtone, which seems to be more like the code world's alternative to a DAW. You couldn't write music with Tidal alone, you'd need to pair it with other software. Perhaps I'm making a poor choice, but I think I want to start with something more featureful while I'm starting out. Bang looks neat, but I'm afraid I don't own a Windows or OSX computer, so I'm unable to check it out.
Most interested in this: &gt; When this work is complete, _all_ types will be promotable to kinds, and _all_ data constructors will be promotable to types.
http://metasepi.org/en/posts/2013-01-09-design_arafura.html
You're in r/haskell. You're going to get a bias answer when asking these kinds of questions. Tidal and Overtone are different. Overtone focuses more on synths and scales while Tidal focuses more on loops and samples. It depends on what you're going for. 
Yes, that's exactly what I said. Tidal and Overtone are not the same thing at all. And I know I'm going to get a bias answer, that's why I asked. I want people to come out of the woodwork and show me projects that excite them, which they have. If I wanted to learn about Clojure projects, I'd ask in /r/Clojure
You don't need to involve LayoutModifier. You can just make some definitions like: import XMonad import qualified XMonad.StackSet as W import Text.Read getLayout :: X (Layout Window) getLayout = gets $ W.layout . W.workspace . W.current . windowset -- setLayout :: Layout Window -&gt; X () -- defined in XMonad saveCurrentWorkspace :: X () saveCurrentWorkspace = do Layout x &lt;- getLayout writeFile "currentLayout" (show x) loadCurrentWorkspace :: X () loadCurrentWorkspace = do string &lt;- readFile "currentLayout" Layout xProxy &lt;- getLayout case readMaybe string of Just x -&gt; setLayout (Layout (x `asTypeOf` xProxy)) Nothing -&gt; return () -- or complain that loadWorkspace is failing to parse the currentLayout file And then call those functions with keybindings, startupHook or something else. Depending on what layout you actually have, this might not work very well. One example of how this can break is that the layout state might include a particular Window. Then you if you restore a layout after restarting your computer, that Window probably doesn't reference anything. 
Applicative being a superclass of Monad has been a long time coming; it's exciting that it'll finally be done! 
Good stuff - please keep posting as you proceed with the engine.
You'd never write liftA6 anyway, there is fun &lt;$&gt; arg1 &lt;*&gt; arg2 &lt;*&gt; arg3 &lt;*&gt; arg4 &lt;*&gt; arg5 &lt;*&gt; arg6
http://www.reddit.com/r/haskell/comments/29c9dt/interpreters_how_should_it_be_done/ I think there was a similar post earlier before. I'm also interested in this, but i don't quit understand the solution there.
Can you/someone explain what this means in context?
Was hoping to see improved type inference for closed type families, but I think Richard Eisenberg mentioned the SMT solver stuff as possibly being a way forward there.
It's my understanding that it's mostly an optomization
There's a pretty specific use case noted in [Facebook's Haxl paper](http://community.haskell.org/~simonmar/papers/haxl-icfp14.pdf), conveniently co-written by Simon Marlow, who suggested the extension. :) It might not have super wide-reaching consequences but it's nice in the context it's presented it, at least. 
13. Other: ATS Combine programming with lightweight theorem proving. If the idea is to learn something, that's a good place to start.
Why don't we do SPECIALIZE for data structures that does C++-style template compilation?
It's pretty easy to improve closed type family inference without an SMT solver. 
an external solver. we don't have an off-the-shelf one for haskell... yet.
&gt; a rope Not really. A rope from mutable land is much different and starts with one leaf that is the whole content and splits it up as needed, achieving cheap append and very compact storage for the common use case. This is the same problem when people conflate quicksort with the faster one from mutable land.
I'm on Chapter 3, I currently paused my *road to logic* to read "Beginning Haskell" and the recommended (in this thread) [course on haskell CIS-194](http://www.seas.upenn.edu/~cis194/lectures.html) .
[Gershom](/u/sclv) came out to Boston Haskell a couple of weeks ago and gave this wonderful talk on jmacro-rpc.
Hm. An interesting method, but I think you could do it simpler and avoid the unsafety. 1. Your use of `unsafeCoerce` in `popMax` does *not* seem safe if you ever try to use the `popped` for anything *other* than putting it straight back into an `HSet`. However your `delete` *might* still be safe since it only does that. 2. I suspect that the way you use it in `delete`, you will have `a ~ Any` anyway. (GHC chooses that type when it has to make a choice but there are no restrictions on which to use.) You could still be in trouble if your `v` is something non-parametric such that `v a` is *not* represented the same as `v Any`. 3. Existential qualification doesn't need a "vtable" unless `a` has type class restrictions (in which case the dictionaries for the type classes are sort of analogous to a vtable.) Bonus questions: 1. This is a well known problem with rank n types being hard to infer whenever functions are not explicitly fully applied to their arguments. The fact that it works at all with just `$` is actually a hack put into GHC to make it possible to write things like `runST $ do ...`. Basically `f $ x` is treated as a syntactical construct equivalent to `f x` for type checking. 2. Well there are different kinds of styles and packages, and some think the kind of heterogeneous list/set which hides the type of internal elements is an antipattern. But for your kind I can indeed see a different way of doing it. First, make a simple type that only existentially quantifies over a single element: data Exists v = forall a. Exists (v a) Secondly, instead of your `Ord1'` instances for `v`, make an *ordinary* `Ord` instance for `Exists YourVType`. Once you have that, I don't think you need your `HSet` machinery to know anything about the elements being existential. I think e.g. `Data.Set.Set (Exists v)` should give basically the same capabilities as your implementation. 
Compared to 7.8, there's relatively little that excites me personally. Except 'DWARD-based stack tracing'. That's just a killer feature. Hope it'll make it into the release. Overloaded record fields, AMP and CPU specific optimizations are neat!
Non-positional applicative record construction: Compare: do -- Maybe applicative; but could be any applicative foo &lt;- couldBeNothing bar &lt;- couldAlsoBeNothing let baz = 42 return Foo{..} -- or return Foo{foo,bar,baz} if one considers wildcards to be too hairy The alternative would be: Foo &lt;$&gt; maybeFooOrNothing &lt;*&gt; maybeBarOrNothing &lt;*&gt; pure 42 Which is all well and good for the tiny example above but when you have 10+ parameters it gets a bit complicated. Is 'pure 42' going to baz? What happens when you add another field to your record that also happens to be Int and sits next to baz?
This is probably totally irrelevant for your use case but what I want is a library like [gloss](https://hackage.haskell.org/package/gloss) or [graphics-drawingcombinators](https://hackage.haskell.org/package/graphics-drawingcombinators) but for 3D. You would have functions mesh :: Mesh -&gt; Entity color :: Color -&gt; Entity -&gt; Entity translate :: Vec3 Float -&gt; Entity -&gt; Entity entities :: [Entity] -&gt; Entity and so on. 
A better mechanism is to change the type of Eq1: from class Eq1' t where (==%) :: t a -&gt; t b -&gt; Bool to class Eq1 t where (==%) :: t a -&gt; t b -&gt; Maybe (a == b) for some equality type like `(Data.Type.Equality.:~:)` from GHC 7.8, which provides the proof, e.g. data a == b where Refl :: a == a Similarly you can upgrade Ord data Ordering1 a b where LT1 :: Ordering1 a b EQ1 :: Ordering1 a a GT1 :: Ordering1 a b so that given class Ord1 t where compare1 :: t a -&gt; t b -&gt; Ordering1 a b you can write out the keys for maps like this and have the match of a key provide the proof that the types are equal, without unsoundness. data Map k v where Bin :: k a -&gt; v a -&gt; Map k v -&gt; Map k v -&gt; Map k v Tip :: Map k v can now be indexed into, etc. This matters a great deal because otherwise it is possible to get some very well meaning terms where the term inhabits multiple types, and accidentally turn that into an `unsafeCoerce`! This isn't that unlikely. Consider `[]`. `[] :: [a]` inhabits `[a]` forall choices of a.
Hey, thanks! There's something really quite pleasing about seeing the source of a song in a git repository. While you were learning Euterpea, what resources + documentation did you find most useful? 
That sounds extremely scary.
http://ro-che.info/ccc/21
The main benefit is for folks who don't actually know enough _to_ be explicit. They can just derive the benefits. For me the benefit comes from the fact that it can find a lot of 'non-obvious' parallelism in otherwise mostly straight-line code when turned on. You always _could_ refactor the code to use applicative notation directly, but that can often lead to much harder to read code when your data constructor doesn't have the fields in just the right order or complex argument swizzling is involved.
The problem there is that on some distros you'll end up with really outdated tools, and they'll be subject to random modification, or incorrect packaging.
This blog post is by the author of that Reddit thread :)
I guess that's true. I run Arch so I can follow this guideline fairly well, but I can see there being things missing in other distributions.
I don't think that OS package managers can help. Most of them are slow to adopt updates and new packages. Packages usually are several releases behind upstream. The latest Ubuntu includes pandoc from half a year ago, and it got many new features since then. And what about tools that are not yet packaged? The latest Ubuntu does not include hasktags, packdeps, present, shellcheck and others I use daily. Besides, OS X doesn't even have a default package manager, and Homebrew does not package many Haskell tools.
With respect, don't try to lecture me what I am supposed to care for and what not. Thanks. See [my reply to SirRockALot1](http://de.reddit.com/r/haskell/comments/29vy9a/how_do_you_work_around_the_lack_of_cabal_upgrade/cip0x5o) for why OS packages managers don't really help. 
Scary to some of you... perhaps ;)
Of course I will ;) I just need some hindsight right now.
As ocharles said, it’s me :) I just feel the need to expose all I have in mind about that issue to see how people would do, in order to pick a better solution than IDs.
`gem` has `gem upgrade`, so this isn't a problem there.
... that includes GADTs? Holy shit...
~~Doesn't leak memory for me on GHC 7.6.3. What's your set up?~~
This is the problem. forM_ runs in constant memory with GHC 7.8.2
True, but so does the `cab` Haskell package, which wraps around cabal. Additionally, if you use multiple versions of Ruby (as pretty much every Ruby developer does) you'll want to have the gem available in all versions, which leads to upgrading across multiple Ruby versions.
Good point :)
Oh I see. "Right now" means GHC 7.8.
Clear enough if you don't put a `Monad` constraint on that code. The applicative/alternative operators are problematic for a number of reasons, like unclear precedence, implicit effects ordering, and generally getting messy for anything larger than a few arguments and branches. I would like to see idiom brackets as an additional option, though.
Come to think of it, the OP question requires a more detailed answer. A newbie can reasonably expect that if you only use the first 10 values of the list returned by a **forM**, only the effects required to produce the first 10 values should be executed. But, as it turns out, **sequence**, **mapM** and **forM** don't work like that, they execute *all* the effects and return *all* the values. To only execute the effects required by the items you actually use, I guess you need a streaming library like **conduit** of **pipes**. But this requires a good understanding of monads...
Howdy! There is a Haskell analogue to Overtone called hsc3: https://hackage.haskell.org/package/hsc3 Certainly doesn't have the community of Overtone yet, but it's an impressively complete set of packages and incredibly capable. It's actively maintained. See http://rd.slavepianos.org/?t=hsc3 for the full overview and http://rd.slavepianos.org/?t=hsc3-texts&amp;l=lhs/hsc3-tutorial.lhs for a tutorial.
It wasn't leaking per se, it was accumulating a small amount of memory very slowly over time. This was the list of (), as for why it doesn't happen on an older GHC.. well I have no idea. forM is the same from 7.6 to 7.8 so really they should be the same.
Of course it does. And C and C++ are not exactly new-designed languages. Why are you comparing to them?
I take it back. It does indeed, very slowly, leak memory.
&gt; True, but so does the cab Haskell package, which wraps around cabal. [This](http://www.mew.org/~kazu/proj/cab/en/) `cab`? I doesn't seem to have a command to upgrade packages.
Unfortunately contains the old myth "in the expression `(x ++ y) ++ z` the list `x` must be traversed twice". The correct description is "accessing each element of `x` requires traversing to depth 2" which is something different albeit with the same running time. I drew some pictures of what is going on here: http://h2.jaguarpaw.co.uk/posts/demystifying-dlist/
I use Gentoo Linux. Portage (specifically, the Haskell overlay) keeps track of upgrades for me. It's not ideal but fairly decent. 
Its still under development, but so far its definitely more about composing scores than synthesising sounds. It has a midi back end to actually perform a score, and I think they intend to support more. I'm sure they'd be interested in taking patches adding new performance back ends :-) Edit: looks like there is a supercollider backend already: https://music-suite.github.io/docs/api/music-score/Music-Score-Export-SuperCollider.html
Frustratingly Euterpea is not in Hackage, but it is available on github.
There's a Pacman repo for Arch with recent packages from hackage. I vaguely remember there being something like that for APT as well. There's little point in expecting the core repos to keep up with this. Dedicated repos are the way to go.
Fo such task you will need a language with low memory footprint, highly embeddable, cross-platform, ideally homoconic (to do not wait for a commitee if you want a better or simply your own language syntax). * Ad1. Not cross-platform * Ad2. Too verbose, low level. * Ad3. Very few platforms supported * Ad4. Not a systems language * Ad5. Not a systems language * Ad6. Garbage collector (unsuitable for device drivers) * Ad7. Low level Haskell code looks worse than assembly, high memory consumtion, introduces bugs that force you to manually manage memory leaks * Ad8. Not a systems language * Ad9. n.a. * Ad10. The same as Java * Ad11. Huge executables, higher level than C, but still low level if you need high level stuff or DSLs * Ad12. n.a. * **Ad13** Try Red/Rebol - trully full stack, homoiconic, very simple language. Still young, but in my opinion best option if you want one language to "rule them all". http://www.red-lang.org/p/about.html http://3.bp.blogspot.com/-xhOP35Dm99w/UuXFKgY2dlI/AAAAAAAAAGA/YQu98_pPDjw/s1600/reichart-abstraction-diagram.png
I have been going through the examples in chapter 1, and they are filled with small errors. Another example, with the example of mongoDB. This is what it says you should enter: {-# LANGUAGE OverloadedStrings, ExtendedDefaultRules #-} import Database.MongoDB main :: IO () main = do let db = "test" pipe &lt;- runIOE $ connect (host "127.0.0.1") e &lt;- access pipe master db run close pipe print e run = getData getData = rest =&lt;&lt; find (select [] "people") {sort=[]} but when compiling with ghc you get db.hs:8:13: Not in scope: `runIOE'
I rather think of them as not variables but as arguments to a function. I think if the applicative style starts to get out of hand, it can become clean with a bit of refactoring, anyway. For example, not using a lambda, but a named function w/ where, or something.
Normally, user-defined kinds are introduced implicitly with a `data` definition. The drawback is that **too many** names get defined: data Nat = Z | S Nat *Datatype promotion* introduces * `Z` and `S` at the value level * `Nat`, `Z` and `S` at the type level * `Nat` at the kind level But when you only want the new kind, you have to do something like: data Nat :: *1 where Z :: Nat S :: Nat -&gt; Nat (this is Ωmega syntax) The other benefit is that you can refer to `*` when giving a signature to your new type-level things, which is impossible with datatype promotion.
I'm not against being explicit, but if the compiler can detect cases and optimise them, I'm always for that also.
I basically never install binaries using cabal. If I wanted to build the binaries on my system from source, I'd be on Gentoo :P
Let's say a `Mesh` is just a set of triangles. Then `mesh` creates a black entity centered around the origin from the given `Mesh`, `color` gives the given `Entity` the given `Color`, `translate` translates the given `Entity` by the given vector and `entities` groups the given list of entities so that they can be colored and translated as one. On the implementation side you would have data Entity = Mesh Mesh | Color Color Entity | Translate (Vec3 Float) Entity | Entities [Entity] render :: Entity -&gt; IO () You probably want a fully fledged 3D engine while I want something to play with.
Hi again. Is there any hsc3 community I can get involved in at all? Thank you.
I had another look at this, wrote a simpler version using a StateT, same horrible performance. The problem is simply the overhead from the monad itself. This is fine: fptr &lt;- liftIO $ mallocByteString $ B.length bs let ptr = unsafeForeignPtrToPtr fptr liftIO $ forM_ [0..B.length bs - 1] $ \i -&gt; pokeByteOff ptr i $ unsafeIndex bs i lift . yield . PS fptr 0 . B.length $ bs This isn't (~40x slower): fptr &lt;- liftIO $ mallocByteString $ B.length bs let ptr = unsafeForeignPtrToPtr fptr forM_ [0..B.length bs - 1] $ \i -&gt; liftIO $ pokeByteOff ptr i $ unsafeIndex bs i lift . yield . PS fptr 0 . B.length $ bs (Note the moved liftIO) So, I can't really think of a good solution that would give me yield-like semantics with acceptable performance.
I'm using haskell platform on windows, is it better for me to migrate to a standalone ghc + cabal installs? I'm a newbie, what should I be careful about h-platform?
I guess I just do `cabal install tool` whenever I feel like I need a newer version of that tool. I've never ran into trouble with that approach, I guess. For some obscure tools I don't use much and are hard to build I'll build them in-place in a sandbox and then use them directly from the sandbox.
Cool! Does it use the standard smtlib format? Any existing library? Can it be used with any SMT solver using this format, or is there a strong preference? Is anyone aware of work being done on an SMT solver in haskell?
[not-gloss](http://hackage.haskell.org/package/not-gloss) ?
I see. So in the case of the dependently typed language, NonZero(0) would be a compile time check, is that right? But, that seems like a small win, since other than that, it's all the same. Maybe this example is too contrived.
Can you post this script, or is it not ready for sharing? 
I'm answering this question because when I was new to Haskell I had the same question having worked in Slime somewhat before hand. I actually don't think the ability to send single expressions to the REPL actually services Haskell as much as it does Lisps. I also think that when you send a piece of Lisp to the REPL you get less than when you send Haskell modules to the REPL. For example when I send and S-exp to the REPL I think (correct me if I am wrong) you are only getting a guarantee that your Lisp is syntactically correct. In Haskell you get type checks and type inference which is going to try to pull in more of the contents of the project anyway. Now if you just want to parse some of the contents of the file independent of everything else (akin what Lisp REPLs do) you could use something like haskell-src-extentions but of course the only useful information you would get from that is that the expression actually parses (unless you are using the AST for something cool... see below). I have used it haskell-src-query in the past when developing Intellij like features for Haskell editing modes. https://github.com/ExternalReality/haskell-src-query Which, as mentioned, is an editor extension which is heavily inspired by Intellij and borrows the source parsing technique from Structured-Haskell-Mode (which is an awesome mode for AST based development -- much like Intellij in a sense) https://github.com/chrisdone/structured-haskell-mode Hope this answers the question to some degree. 
I think that this is exactly what nix is built for. Oddly enough, I just started using nixos (second attempt) last night. Here's a recent post that you may find relevant: http://fuuzetsu.co.uk/blog/posts/2014-06-28-My-experience-with-NixOS.html
It's because the amount of stack space allocated for executables by default isn't 8MB anymore - it can use up to 80% of the physical memory available in 7.8.x In practice having such a small stack size was problematic for such simple examples, so it was increased (generally most people agreed this limit bump was a Good Thing).
It's also the first step to having real Pi types in Haskell, since it essentially requires turning Core into a fully dependently typed language. I eagerly await Richard's contributions here (probably not surface-available until 7.12 at least).
Windows is the only platform with a good reason for using HP and even then it depends on libraries used. If HP is working for you, don't mess with it. 
Oooh, you're right! I thought it did; my bad.
So I tried your approach, simplified further by merging `Exists` and `Cmp` into data Exists v = forall a. Cmp Int a (a -&gt; Double) ...and it works, and is 10x simpler than what I did. Why didn't I think of it? And thanks for your answers, they made things much clearer. However, I still don't get this part: &gt; I suspect that the way you use it in `delete`, you will have `a ~ Any` anyway. (GHC chooses that type when it has to make a choice but there are no restrictions on which to use.) You could still be in trouble if your `v` is something non-parametric such that `v a` is *not* represented the same as `v Any`. Could you give an example of such troublesome `v a`? To me it seems that `a` has to be lifted (as well as `v a`?), and as long as we're not inspecting `a`'s contents, we're good. What is it that is potentially unsafe here?
Slides: http://prezi.com/vbyunso4izak/ Code: http://hackage.haskell.org/package/jmacro-rpc http://hackage.haskell.org/package/jmacro-rpc-happstack http://hackage.haskell.org/package/jmacro-rpc-snap
here's some recent from /r/math on topology and neo-Riemannian analyses. Most of the music and math threads are about just intonation, equal and pythagorean temperaments, physics of different instruments harmonic structures and acoustics, i.e. *not* about composition and harmonic analysis of pieces. http://www.reddit.com/r/math/comments/297rfe/amazing_blog_topology_and_music/ http://www.reddit.com/r/math/comments/29dbeh/does_anyone_know_about_mathematical_music_theory/
The Internet? :P No, I really don't as I've not needed to look into it myself.
Yes, they are equivalent. The operational monad is another way that you can encode a free monad.
So you're saying it's much `ado` about nothing?
There is also Inferno and Plan9 which are very much not Unix * http://en.wikipedia.org/wiki/Inferno_%28operating_system%29 * http://en.wikipedia.org/wiki/Plan_9_from_Bell_Labs Also Phantom * http://en.wikipedia.org/wiki/Phantom_OS
Yeah, I would assume so. I've sketched out the mechanics of how I imagine it could work, being completely ignorant of how GHC implements type inference. Any pointers for someone new to GHC hacking to help get started working in that part of the code?
I see, thanks.
There is also Mezzo. Can't say much about it, but it seems to be concerned with getting low level stuff right and is related to ML/Ocaml. https://protz.github.io/mezzo/
I don't think I believe your premise; particularly, I don't think I believe that there is a such things as what you call an "entity-semantic" object. Why do "two perfect similar humans remain different"? I'm perfectly happy with data Mesh = Mesh {- … -} data Material = Material {- … -} data Model = Model Mesh Material or something similar, until someone can provide a case where this model is insufficient.
I guess the first question is: what kind of 3D? Will you be rasterizing the scene in Haskell? (raytracing/more traditional forward rendering?) Just sending instructions to a graphics API like OpenGL? Or more abstract than that? Coming from a more imperative BASIC-&gt;C-&gt;C# background and new to Haskell, I'd favour the type class approach as it mimics the interface system of C#. Not sure what the purpose of an identifier would be; what functionality would depend upon its existence? Looking forward to more.
I wasn't actually thinking this part through very much, but I was imagining if `v` was something like a type or data family where the internal representation of `v a` depends entirely on what `a` is. I.e. something that gives `a` a "nominal" role in GHC's new role system. Although thinking about it further, this probably wouldn't matter for just *storing* a `v a` into your set, *unless* somehow that `unsafeCoerce` warning about not confusing ADT and function representations applies. (And which is as far as I understand the main reason for playing it (less un-)safe by using `Any`.)
Imagine you want the change the material’s properties at runtime. The material is shared over several models. Then, if the material changes, with your model, each model’s material can’t change. You’d have to adapt and get the material from some kind of reactive environment.
You are talking about backends while I’m taking about a scene. Forget OpenGL, raytracing, just focus on the core types. This is what I’m talking about. Being able to describe a scene, and write several *backends* to plugin. The *identifiers* are just a way to connect objects between each other. You can see those as generalized references, if you want to. Still, in order to *dereference* them, you have to pass by the context. The identifier could be an index in an array, for instance. If you’re used to **OpenGL**, the common `GLuint` returned by `glGen*` functions are such identifiers.
They are different in that "Free f" reuses "f"s functor instance while Operational works on types which do not admit a Functor instance (e.g. most GADTs)
The NY Haskell Users Group has been hosting weekly office hours for the past few months. Although none are scheduled at the moment, they should pick up again soon: http://www.meetup.com/NY-Haskell/ Also, if you show up to a meetup and ask around, I'm sure you'll find a ton of people interested in helping people learn the language.
I'm always surprised at how short my C programs end up. I'd never want to really engineer complicated software in it, but when I write a short program for a class assignment, I end up writing the same number of lines as I'd write in Haskell. Given how much less expressive C is, this surprises me - though part of it is that when I write Haskell, I'm thinking, "time to abstract everything beautifully, making safe, reusable software" whereas when I'm writing C I'm thinking, "well, it's C code. I can do anything I want" and I nail out something quick and dirty. C is definitely a lot of fun.
How do Yoneda and Coyoenda relate? I would have described Yoneda similarly. Would `Free (Yoneda f)` have the same freeness as Operational?
I just barely followed this post. I think there would he value in an "undergraduate" version of Haskell theory that was a bit more concrete and had fewer vocabulary words that hooked back to far more general concepts...
I tried to contact them some time ago but wasn't successful. I'll try that again. Thank you.
FWIW- gbaz1 is one of the organizers there. 
[This paper](https://www.math.washington.edu/~morrow/336_09/papers/Ada.pdf) is also probably not the one you're thinking of, but I thought it was neat.
It may be useful to consider the difference between `Coyoneda IORef` and `Yoneda IORef`.
The fun wears off quickly when you get beyond the "class assignment" sort of programs. Writing robust, maintainable software in C is a big part of my job, but I can't say I find it "fun". What's really ironic is that C is most terrible at the sort of really low-level management of fixed-format binary data that primarily motivate its use these days; i.e. manipulating hardware-level registers to control hardware, parsing packed binary file or network packet formats, etc. It pretty much boils down to fragile macros for combining various shifts and bitmasks, or some very non-portable packed structs. Sadly, few C-replacement languages think much about this problem. Something like Erlang's binary patterns or the bitdata types that you find in research languages like Habit and Ivory would be a wonderful addition to a language like Rust.
It does, but it's not guaranteed to happen. The point of the "Tail Call Optimization" , a.k.a. "proper tail calls", is that there are patterns of procedure calls that clearly *could* run in constant space if they were implemented reasonably. This becomes most clear when you transform a program to "continuation-passing style", where nothing ever 'returns', but instead calls the next code to be executed. When a program is transformed in this way, *every* call is a tail call. There is no need for a standard control stack anymore. In a recursive procedure, if it called itself in tail position before, it will never need a *new* continuation; it just passes the one it got initially back to itself until it reaches its basis case, and then it calls the continuation with the result. Thus it takes constant space; it's essentially an iterative loop. If your recursive procedure did not call itself in tail position, it will be split into two, and at each invocation the original continuation will be "wrapped" with a call to the second half of the procedure. This sort of procedure will necessarily require an amount of space linear in the size of the problem to store the "rest" of the computation to perform after the basis of the recursion is reached. When a language has "proper tail calls", then you can reason about its space usage in the above manner. If it doesn't guarantee that reasoning is valid, then it doesn't have proper tail calls, even if it *might* have the underlying mechanism in place sometimes as an optimization.
What libraries of yours would this affect the most, do you think, and what sorts of gains would you see?
I feel like I'm not really grokking the type application and partial type signature bit. Can you give me a use case (contrived or no)?
And not just *more type-safe* programs, but also strictly *more* type-safe programs. :)
Sure. A really simple and not at all contrived example is `reads`. The type of `reads` looks like this: reads :: Read a =&gt; String -&gt; [(a, String)] Well, actually it looks like this: reads :: Read a =&gt; ReadS a Now, why is that? It seems like an awfully trivial place to make a type synonym. The reason is when you want to use `reads` at a monomorphic type, without the type synonym, you have to do one of these: reads "3" :: [(Int, String)] (reads :: String -&gt; [(Int, String)]) "3" But with the type synonym, it's a bit less typing: (reads :: ReadS Int) "3" Isn't that nice? With the type synonym, you get to essentially just say "instantiate `a` to `Int`", which is what you really mean. But making up a type synonym every time you want to do that is awful. Type application would let you say this: reads @Int "3" ...which really, *really* reads just like "instantiate `a` to `Int`". No extra type synonym needed. No extra context needed around the `Int`. Just say what you mean. I really want that kind of thing often, and I'm sure you can think of a few times it's happened to you, too! Partial type signatures hit a similar point. A quick example of where I've wanted that kind of thing before is with values that are polymorphic in which monad they use. Often you know which monad you want, but don't want to say anything else; then you could annotate your `(Monad m, OtherConstraint a) =&gt; m (complicated type mentioning a)` with the partial type signature `Maybe _` (for example) to say "I want to use the `Maybe` monad, but stay polymorphic in all the other stuff in exactly the way it was before".
For the entity IDs, how about IORef or STRef? ident :: a -&gt; m (ID a) newIORef :: a -&gt; IO (IORef a) reify :: ID a -&gt; m a readIORef :: IORef a -&gt; IO a imod :: ID a -&gt; (a -&gt; a) -&gt; m () modifyIORef' :: IORef a -&gt; (a -&gt; a) -&gt; IO () &gt; What would happen to if you escape an identifier and reify in another monad? To prevent this from happening, you can use ST's trick of adding an extra type variable on both the monad and the refs. newSTRef :: a -&gt; ST s (STRef s a) readSTRef :: STRef s a -&gt; ST s a modifyIORef' :: STRef s a -&gt; (a -&gt; a) -&gt; ST s () The idea is that a ref from a different execution of ST than the one it belongs to will have the wrong `s`. If you choose to implement this trick yourself instead of reusing the existing STRef implementation, it is essential that only universally-quantified instances of the monad can be executed: runST :: (forall s. ST s a) -&gt; a Otherwise, two distinct ST executions could use the same `s`, which would incorrectly allow refs from one execution to be used in another.
I think you and /u/phischu are simply using the same word to mean different things. Besides, "Entity" seems rather generic to me. Maybe you're thinking of an [instance](http://en.wikipedia.org/wiki/Geometry_instancing)?
I would question the overall importance of code completion in Haskell in the first place. In any DTL, you have a zoo of types to manage. This isn't just worrying about `List`s vs `Vect`s; `Vect n Int` and `Vect (S n) Int` are totally different types. There's a much larger cognitive burden when writing DTL code. Inductive types which enforce constaints, IMO, do not usually reflect the mental model I have of a type. Even with something as basic as `Fin`, it's quirky to know that in the expression `fS (fS (fS fZ))` -- which has type `Fin n` for any `n` greater than or equal to 4 -- the three `fS` constructors all have different types, as does `fZ`! Assuming our expression is instantiated to `Fin 4`, in reverse order, they are typed as: * `fZ : Fin 1` * `fS : Fin 1 -&gt; Fin 2` * `fS : Fin 2 -&gt; Fin 3` * `fS : Fin 3 -&gt; Fin 4` If you have been doing dependent types for a while, this may not seem all that unusual. But that's because `Fin` is so intimately familiar to you as well as that you are focusing on the question in isolation. My problem, as a programmer, is never "correctly discover the type of `fS (fS (fS fZ))`". It's something higher level. And if I have to break my concentration to make sure my shoes are tied every other moment, I'll never finish the race. Related to this, constructors for propositions (when defined inductively) are often tricky to name. The saying goes that there are only two hard problems in Computer Science: cache invalidation, coming up with good variable names, and one-off errors. But when we reason about theorems informally, we never give names for the *reasons* things are true. Names like `zeroEven : Even 0` and `nEvenThenSSEven : Even n -&gt; Even (S (S n))` are just fucking terrible. In mathematical proofs, these things tend to be spelled out in English. "Let n be an even natural number. If it's zero, then the claim holds trivially. Otherwise, consider n' = n - 2...." With the critical fact we appealed to the inductive hypothesis sometimes left out entirely! Admittedly, this is a social problem, as well as a psychological one. I'm sure as popularity picks up, someone will find a more clever way to handle it eventually. Speaking of inductive hypotheses, induction presents more mental overhead than recursion. In Haskell, when you make a recursive subcall to some function, say, `f : A -&gt; B`, you know `f x` has the same type as the type you're expected to return (namely, `B`). But during induction, this isn't usually the case. For a section `s : (a : A) -&gt; P a`, a recursive call to `s x` does NOT have type `P a`... it has type `P x`! And it's only when you can properly transport the result can you write a type-checked program. Haskell, of course requires you to pay attention to such details to write correct software... but you're able to do so at your own pace, informally, and in whatever detail the situation dictates. That "informally" bit is kind of important, so I'll just add a bit more on that using a real-world example. I once wanted to show my friends how to use Idris to prove the statement, "Every natural number is either even or odd." They were not impressed when it took 20 lines and about 10 minutes to convince the compiler I was right. So my argument is that the additional cognitive burden of DTLs actually necessitates (as well as enables) code completion. I think more important for Haskell (and any statically typed language) is the ability to glean pre-compile-time, context-aware type information. Being able to see all the variables in scope, together with their types, together with the type of the hole you're currently standing in is so amazing for the Agda and Idris IDEs. And it requires comparatively little design work (and no appeal to heuristics at all). The current state of Haskell is pretty miserable in this regard. Proper practice requires you use Cabal. But Cabal (as far as I know) doesn't let you load programs into the REPL. And even if you can, that's information that should be available even if your program doesn't fully typecheck. If we wanted to be ambitious, type-aware code completion would be amazing. That would help reduce the total number of keystrokes. But I don't see the need for smartypants code completion without the smartypants typesystem to back it up :) I've actually thought about this subject a lot, so I hope my comments are helpful. 
This, combined with the SMT solver in the type system leaves me excited.
Until you do: https://github.com/bitemyapp/learnhaskell Perhaps even if you do.
Freenode's #gentoo-haskell represent
Isn't that job for resource manager? Changing material means that you stop using current material and then start using another one. If it is changing with time or user input then it should be just another property of material.
`forM_` runs in constant memory, period.
Can someone ELI5 what is this about? It seems like he keeps the loaded areas in backend and then reloads just some of them from frontend via ajax or something? I'm confused about the point...
First of all, thank you for the reply, your comment is very helpful. I absolutely agree with the interdipendence of DTLs and code completion. I have a few question on the second part of the post, though: * You refer to the ability to glean pre-compile-time, context-aware type information, and types and variables in scope in holes, even without complete code. Do you have in mind something that is not obtained, in haskell, with haskellMode/ghc-mod, and ghc7.8's holes? * Could you clarify in which way you don't feel the need of this feature without a dependent type system? Do you mean that the value of this outcome would be neglegible for the expected implementation work, or that is weak/undesirable in general? To be clear, I'm referring here to the ability to generate the standard implementation of, e.g., filter, by writing the type and selecting the option `filter odd [1,2,3,4,5] = [1,3,5]`, not doing anything with proving at any level, from the user perspective. Thanks again for your time! :)
Oh dang! I'll be in Vienna during the ITP conference later this month, but not in time for the Lambdaheads. Such shame!
Ah, that does complicate matters.
Maybe https://www.codementor.io/ ? (Disclaimer: I'm a mentor there.)
Hm, that’s an interesting point of view :)
I _think_ this doesn't affect the answer, but I don't really know the algorithm so I can't be sure. findPeak::(Int-&gt;Double)-&gt; Int -&gt; Int -&gt;(Int,Double) findPeak keyfn x hi = go (keyfn x) x where go !v !yp = if yp &gt; hi then (yp,v) else let !new_y = yp + 1 !ky = keyfn new_y in case ky&lt;=v of True -&gt; (yp,v) False -&gt; go ky new_y and policy::Int-&gt;Int-&gt;Int-&gt;Array U DIM2 Double-&gt;(Int,Double) policy cap prod start evf = findPeak fn start (nGridCapital - 1) -- [start..(nGridCapital-1)] where fn nxt = compute_vf evf cap prod nxt saved a second and a half and a fair bit of allocation on my machine.
I disagree with one point in particular about the Haskell article. Laziness does not make metaprogramming redundant. That's a common misconception because in Lisps, metaprogramming is sometimes used to emulate laziness. That doesn't make it the same thing, though; metaprogramming is more powerful than that and Haskell still needs Template Haskell for its metaprogramming needs. ---- But why are you asking the question? You won't find an objective measure. If you really want an answer, then sure. Haskell is a 7.32 language on the High Level Language Scale, while Lisp is a mere 7.14 HLLS. That makes Haskell 0.18 more high level than Lisp. Now why does it matter, again? What I'm trying to say is that as long as you're not writing a shell script in Java, or a microservice in an assembly language, I think you're doing fine. ---- Edit: I don't think the discussion is bad, mind you. I just don't understand *what* you want us to answer. You don't seem so sure either, given the wording of your post. Please follow up and expand what you're thinking! Second edit: Now that I've re-read *Beating the Averages*, I might do what Graham did. Graham points to macros, and says, "In every other language I've used, I've missed this." And he's right about that. But when I look at Lisp, I point at the Haskell purity and type system and I say, "I miss this. This is more important to me than macros." I also love how Graham sort of concedes this in between the lines. He makes sure to differentiate between garbage collection which is "a good thing" and dynamic typing which is "growing in popularity".
Hi! http://lurk.org/groups/haskell-art/ is probably the best place to go.
Most powerful? I think it would be the one which is not following fads or religions, language with batteries included. It is often good to place a language in a typical (real life) scenario: * I've got some software and database which is no longer maintained * I want additional feature, e.g. sms and email notification for my clients. * Database format is Paradox v7.0 * I need some place to keep info about these notifications, eg in Sqlite db. * What is the best tool to do this fast (simple command-line program executed at system boot)? * Code must be closed, it is not open source (no *GPL dependency) - programmers must eat. * Software must run on Windows (XP, 7, 32/64 bit) and Linux machines, with or without internet connection. I seriously doubt Haskell or Lisp is catching on. Edit: Actually after what I saw lately Haskell is a pure crap, such it has huge problems with dividing two simple integers. It is a language which breaks the rule of least astonishment every single time. WTFskell would be a better name. 
 Thanks for the response. (And thanks for the links, they could be handy when I get past the basics). As you say - what I'm after is like using slime in lisp, but not necessarily even that fancy. Other languages (ocaml, ruby, python, etc) all have ways of sending a section of the buffer to the repl. &gt; For example when I send and S-exp to the REPL I think &gt; (correct me if I am wrong) you are only getting a guarantee &gt; that your Lisp is syntactically correct. No, it actually *evaluates* whatever was sent so you can see what's going on with a specific piece of code. You could work through a buffer section-by-section and watch what happens at each step rather than having to load the whole buffer in one go. I guess my post wasn't so clear *why* I wanted to sent parts of a haskell-mode buffer to the repl (well to ghci I guess - repl, toplevel, interpreter - whatever's the correct thing to call it ). I haven't used haskell before so I was just going to try some very basic stuff. For example, following some of the things in a tutorial such as [Learn You a Haskell](http://learnyouahaskell.com/starting-out). That is structured as a dialog with the repl (interpreter). I could type the things at the command prompt of ghci. Or I could type them into a file, so that I build up a record of the stuff that I'm trying out, and can easily edit stuff and try variations. Then I could cut-and-paste stuff to the emacs interactive ghci session. But it would be much more convenient to be able to "send region" or "send expression" with some single key-stroke. Also, cutting and pasting multi-line expressions and indenting looks like it might not be so straight forward. Does that make sense ? 
Willing to try remote lessons via video chat? I (and probably many others) could help that way. Also, if you can share a few sentences on where you currently are and what you want to learn, we can better connect you with someone who can help you best.
ok, https://github.com/mwotton/avb-econ-hs has my tweaks. started at 12s, got it down to 4.7s. interesting playing around with INLINE - it seems like you really want to do profiling _without_ INLINE, so you can tell what's going on, and then bung INLINE on once you're happy each piece is as fast as it can be.
&gt; [...]and Haskell still needs Template Haskell for its metaprogramming needs. I wonder if dependent typing substitutes metaprogramming. AFAIK they are at least not completely orthogonal.
I'm not a professional programmer, so read my remark with that in mind. But for me "high level" has always meant "closest to the way I think about things" and in practice that translates to "has libraries for most things I want to use in my program that I don't consider to be directly part of what I'm trying to do". I find that language design, beyond a few things that lots of languages offer (automatic memory management being by far the most important, I think) doesn't make too much difference in how "high level" a language feels for a given problem, it really is availability of appropriate libraries.
The problem with your idea of "high level" is that by that measure, high level will be anything you're comfortable working with. A Java programmer will say that his stuff is high level, a C programmer will say that his stuff is high level, an assembly programmer will say that his stuff is high level and so on. This notion was what Graham rejected in his essay.
Thanks for all the responses. I'm at a point where I believe that a 1 on 1 interaction in front of a whiteboard (rather than a shared screen) would be the most efficient way for me to work through the items of interest. I've had a number of very helpful interactions from people through various forums but I really just want someone to walk me through some of the more theoretical pieces, respond interactively to questions and correct some of my misconceptions/confusion. 
&gt; Proper practice requires you use Cabal. But Cabal (as far as I know) doesn't let you load programs into the REPL. It's not perfect no doubt, but we can still load a program with ``cabal repl`` and use type-holes or ``-fdefer-type-errors`` to build your program interactively. It's not quite at the Agda level, but still gets you quite a bit of the way there.
My imperfect but usually good-enough setup is something like: 1. Global is reserved for haskell-platform and OS packages 2. User is reserved for development tools and cabal-install upgrades 3. Sandboxes for individual projects This way I can upgrade #2 with `cabal install world --upgrade-dependencies --force-reinstalls` and #1 upgrade with the OS as usual. The imperfection here is that any upgrades can break sandboxes but those are easy to rebuild. Upgrades to #1 can also break #2 but that too is easy to rebuild (same as upgrading).
[Well.](http://stackoverflow.com/questions/14263363/is-operational-really-isomorphic-to-a-free-monad)
haskell-mode is for Haskell modules, not "ghci scripts". If you were to put the sort of things you type into ghci into a file, it wouldn't be a Haskell module, because for example a module doesn't have expressions at top level. A module is a collection of declarations, which can appear (for the most part) in any order. Contrast to Lisp where a source file is an ordered sequence of side-effecting expressions. If there is an expression you want to enter into ghci repeatedly between reloads of your module, you can just give it a name. For example if you are testing your sorting algorithm, put `test = mySort [3,1,4,1,5]` in your module. Of course this won't cause anything to actually be evaluated until you type `test` into ghci.
No, I think you might be replying to the wrong comment because this sounds completely unrelated to what I wrote. I didn't say anything about using previous knowledge of a language. I said that high level for me means that I don't have to write everything myself and can instead find and use libraries for things I need but that are only implementation details in what I'm trying to accomplish (such as parsing file formats, plotting, etc.) In fact I usually pick which language to use for a given task after googling for libraries, and it a library is tempting enough I'll take advantage of the opportunity of learning a new language (an old hobby of mine).
Are you running this on a VPS with low RAM, like 512MB? That "resolving dependencies" step seems to take around 500MB to run on my machine. Assuming this is on Linux, `dmesg` will tell you if the OOM killer kicked in recently. If so, you wouldn't have been able to compile certain dependencies of yesod with such a small amount of RAM anyways. You'll have an easier time compiling locally and copying the binaries to the remote system.
&gt; for me "high level" has always meant "closest to the way I think about things" That was what I referred to. You tend to think about things in the way commonly associated with the languages you use. Defining the level of abstraction of a *language* after which *libraries* have been made available sounds silly. Does a language become lower-level if libraries become unmaintained? Even though the language doesn't change? Isn't level of abstraction of a language supposed to be a property of a language, and thus invariant under change of libraries?
No it's a vagrant image but you're correct. I've increased the memory size to 4GB and it's installing now. Thank you.
It's reasonably straightforward to compile Haskell for ARM or even JavaScript now. Cabal's infrastructure is also a lot better now, with sandboxes and whatnot. It's a lot easier to avoid problems now.
I think Lisp might be more powerful than Haskell, in the sense that given equivalent libraries and the same (large enough) task to solve in both languages, the Lisp version can probably be made shorter and easier to read than the Haskell version. However, the Haskell version will be easier to modify. The reason for the succinctness and readability of Lisp programs is the usual one: instead of writing the solution in Lisp, you can write the solution in a macro-based custom dialect which is perfectly adapted to the problem at hand. Importantly, this dialect can be *anything*: it doesn't have to satisfy a type checker, and it can have its own custom non-prefix syntax (like [LOOP](http://www.gigamonkeys.com/book/loop-for-black-belts.html)). Haskell doesn't have this luxury. When the program needs to be modified, this lack of limitation becomes a disadvantage for Lisp. To the Lisp compiler, macros are black boxes which are free to do anything. In particular, any tree of s-expressions is a valid input and a valid output. So when you change something in the custom dialect, the compiler will happily pass the old, now-incorrect programs to the new macro, and there will be a large number of failures, and those failures will be in the *generated* code, which will be hard to trace back to the original code in the custom dialect. In Haskell, we also get to write our final solution in a dialect which is close to our problem domain: a custom combinator library, such as a custom monad, applicative, or category. Since this custom dialect must fit into Haskell's type system and use Haskell's existing syntax, it might not be *the* most succinct and readable dialect in which the solution could be written, but it can still be quite good. Critically, the compiler understands our dialect: thanks to the precise types, it knows exactly which inputs are acceptable to each combinator, so the type errors are triggered by expressions in the dialect instead of some generated code.
This seems a bit out-dated to me. Cross-compiling works, with Template Haskell support for cross compiling on the horizon. iOS support shows that ARM works just fine.. All x86 platforms are supported as far as I know. Even ARMv8 has a port. Android support will most likely be in 7.10 as well. The complaints about "high-level code can't be high quality if it isn't cross platform" is flawed because a language's quality and the code thereof is of no value if the compiler can't make the product. This was a missing feature in the implementation of the language compiler, not the language or source product itself. Haskell libraries being in flux is true in that the average Haskell library's interface will change more than the average C++ library. I think this is a strength as much as a weakness, and more of a matter of preference. (Evolution vs maintainability) Cabal Hell is something that definitely sucks, but DLL hell is something maintainers from many different languages manage to deal with. It just requires some constraints on the problem. Sandboxes make this less of a problem, in practice, but I agree that work could be done here. Interesting to see why some programmers choose to go 'back in the cave' nonetheless.
I like your assessment. By this logic, can we assume that the declarative languages (functional, logic) are higher level than the imperative (procedural, OO) languages because they instruct the computer what to do instead of how to do something? Furthermore, are the logic paradigm languages (Prolog) and domain specific languages (SQL) even higher level than functional (Lisp, Haskell) because we query the computer in human language instead of defining mathematical functions? But then don't we still lose some functionality even with a Turing complete logic language, in which case we lose programming 'power' even as we gain a 'higher level' of abstraction closer to our thought process? If 'high level' is on par with human thought over housekeeping machine chores, then it might not correlate completely with its 'power' in efficiently getting things done. There is correlation, but it's not perfect.
I for one see a lot of overlap. Metaprogramming and dependent typing have a lot of conceptual overlap, because they are both about blurring the lines between compile phase and run phase.
The biggest feature Haskell gives you over Lisp is a powerful type system. This doesn't make it more expressive, in fact it makes it less expressive, but it makes it much safer and easier to use. One of the reasons that Racket is an improvement on lisp is because of its powerful "contracts" system, which recovers some of this kind of safety. Typed Racket is still too rough around the edges to be comparable to Haskell.
Right, generally any procedural details in the code is a low-level smell. Prolog can be considered very high level for its domain, because you program the query (ideally) without any reference to how it should go about it. Domain specific languages (I consider prolog domain-specific) are special because they are very high level for a certain problems, but very low-level for others. For example, J (Of the APL family) is a very high level language for array processing, I can't think of any language that matches it for this domain. And sure, you CAN program anything in it, but once you start having to reframe all of your algorithms using complex datastructures in terms of vectorized operations on arrays, it stops being natural since your mental primitives (the complex datastructures) can only be represented clunkily by program primitives (arrays), with your brain having to do all the interop. A truly high level language has the ability to adopt new primitives as naturally as the builtins. Both lisp and haskell (and a good deal other languages) have this property. Notice as we get higher level, moving away from the machine, we often get less efficient. For example, prolog has to do a bruteforce search where a clever direct computation might be more efficient, lisp has to copy data when maybe doing it in place would be more efficient (you could use mutation directly in lisp, but then you stop being high level). This is where type systems come in. In lisp, everything is a list, but the functions we write aren't meant to operate on ALL lists, just those with a certain structure (say, the ones that look like employee records, to be boring). Maybe there's a more efficient way to encode our problem by exploiting the structure of our data. We tell the computer we want to to do something on lists, and so it will, but in our head, we're manipulating employee records - the computer doesn't see the hidden structure. Types are just a way of telling the computer the hidden structure of your program, even though it's all binary arrays at the bottom level. Types give the compiler freedom to transform your algorithm into a more efficient one. This is why I believe that this hypothetical universal high language will include some form of powerful (dependent) type system, so that in addition to being able to specify your intention directly, it will compile to an efficient implementation.
(I posted knowing it was probably outdated, because - as a newbie - I was curious on haskell infrastructure/ecosystem progress. It's nice reading the answers, thanks!)
&gt; This seems a bit out-dated to me. Well, the post is from 2011, so it **is** outdated. 
Here's a comment I left a while ago that goes through what uses of lisp-style metaprogramming are/aren't typically needed given Haskell's other abstraction mechanisms: http://www.reddit.com/r/haskell/comments/1929xn/are_lispstyle_macros_a_code_smell/c8k7wl8
TL;DR - `Free` can construct a `Monad` out of a `Functor` even if we forget what made it a `Monad` in the first place - namely, `bind` and `return`. To be "free" means we have forgotten some of the information, leaving only that information which we can create a `Monad` from. So a `Free` monad is exactly a `Monad` and nothing more. On the other hand, `Operational` can construct a `Monad` by *only having the right kind*. This means that `Operational` can create a `Monad` out of non-`Functor`s, like a GADT. Hence, operational is "Freer than `Free`" since we don't even need a `Functor` - we are more 'forgetful' of the original information. We can go from `Free` to `Operational` through a type called `CoYoneda`. In fact, `Free (CoYoneda f)` is the same type as `Operational f`. data CoYoneda f a where CoYoneda :: (b -&gt; a) -&gt; f b -&gt; Coyoneda f a liftCoyo :: f a -&gt; CoYoneda f a liftCoyo f = CoYoneda id f lowerCoyo :: Functor f =&gt; CoYoneda f a -&gt; f a lowerCoYo (CoYoneda f m) = fmap f m `CoYoneda` allows you to take a Functor `f` and *forget it is a `Functor`* (because you forget how to `fmap` it). Note how `liftCoyo` does *not* have a `Functor` constraint! So you can use any `f`. If `f` is a `Functor`, then `CoYoneda f` and `f` are equivalent. This is because we can lift it, then lower it back. But if `f` is *not* a `Functor` - then we can create a `CoYoneda f`, but we can't take the `f` back out by lowering it. Now, take the definition of `Free`: data Free f a where Pure :: a -&gt; Free f a Free :: (f (Free f a)) -&gt; Free f a Now inline the definition of `CoYoneda f` into `Free`: data Free f a where Pure :: a -&gt; Free f a Free :: (CoYoneda f (Free f a)) -&gt; Free f a Then: data Free f a where Pure :: a -&gt; Free f a Free :: (b -&gt; (Free f a)) -&gt; Free f b -&gt; Free f a Now flip the `Free` arguments: data Free f a where Pure :: a -&gt; Free f a Free :: Free f b -&gt; (b -&gt; (Free f a)) -&gt; Free f a This is the definition of `Operational`: data Operational f a where Return :: a -&gt; Operational f a Bind :: Operational f b -&gt; (b -&gt; Operational f a) -&gt; Operational f a In this way, CoYoneda lets us take a non-functor and create a `Free` monad from it - which is what `Operational` does! And if `f` actually is a `Functor`, then all of these are equivalent (you could directly create a `Free` monad anyway). Hope this helps a little.
Do you have a solution for interfacing with a database layer? Preferably, Postgres? It seems like it should be possible to generate an ORMy kind of thing using techniques similar to what you use. Are you doing this in-house at Silk?
&gt; Prolog can be considered very high level for its domain, because you program the query (ideally) without any reference to how it should go about it. But as we all know, if you do that you are likely to create a disaster. It's absolutely critical that you know how the resolution engine works to ~~costruct~~ construct something more complex than a family tree that Prolog can answer. Edit: costruct -&gt; construct. What is a costruct anyway, and can we get them in a library?
I read [this comparison](http://www.ffconsultancy.com/languages/ray_tracer/index.html) of a ray-tracer implementation in different languages (OCaml, SML, Lisp, Haskell, Scheme, C++) while back and the lisp implementations ended up being the most verbose.
As the main designer of the work, my goal was to show that packages should be abstracted over their unknown/external bits using *types*, not names, version numbers (which are still important, though), and social conventions. What is the right design for expressing packages in this way? The argument for Backpack consists mainly of two parts: First, the mixin design suits package-level modularity better than what the ML module system offers. Second, you can actually build this mixin abstraction on top of the existing module system and thus reuse implementation tools. Although I'm pretty sure /u/ezyang would throw up his arms in frustration at the thought of Backpack merely "reusing" the GHC implementation. ;) Essentially, I wanted Backpack to represent an ideological shift in the way people design large modular programs, partly by kissing the ring of the ML module system and partly by highlighting how even that system is insufficient. You can see how Haskell's lack of module interfaces percolates into "next gen" languages like Agda and Idris, and IMO, that's not such a good thing. Disclaimer: The above thoughts are not necessarily shared by my coauthors -- well, except probably Derek. :)
Thank you for the feedback. I built this because I was trying to solve some project Euler problems, and wanted to make sure my solutions run in under a minute
Cool, thanks for your work on this. Glad to know it's even better than ML and looking forward to seeing what comes of it! 
Concerning the whiteboard: I have given regular lessons over Skype, and I would say that for learning Haskell, Etherpad and online whiteboards work very well as a replacement for a physical whiteboard. In fact, for writing and testing code, Etherpad is actually better than two people sitting in front of a single computer. (On the other hand, for mathematics and physics, online is not up to par.)
That looks like a fairly comprehensive study! I must note, however, that the studied programs did not use macros nor combinators; and that given the size of the task, it would have been overkill to do so.
Not only is implementing a "specification -&gt; implementation" transformation hard, it's hard for humans to clearly describe a specification in the first place. Providing a clear transformation from our thoughts to a program isn't good enough, because our thoughts don't generally start out as a coherent specification of anything useful. What's needed are tools that are approachable given the constraints of human thought processes, and which *guide* that thought towards coherent specifications of programs. Once that coherent specification is reached, a powerful language will allow the programmer and language to work together towards a more efficient implementation of the specification.
I didn't know about `cabal install world`. Many thanks for this hint. Looks like this is all I need to make upgrades more convenient.
Anything on that site will show that some ML variant, probably the author's favorite one at the moment it was written, is the best thing ever. Not that there's not a lot of good information to learn from it; it's just that the author is clearly motivated to make the languages he uses in his consultancy business look like the obvious best tools for any job. In his hands, they probably are, but the experiences of others can and do vary.
I see what you mean, now. Thanks for the clarifications! I think your version of "high level" should really be called something else, so as not to confuse it with the more traditional meaning of "high level", which is a property of the language alone. "Usefulness" or "the right tool for the job" might be good terms for what you're describing.
I'm game to try ---- I'm actually less concerned about testing code than I am about adjusting my mindset around this stuff.
I'm not sure if it's worth much, but here's a very basic 3D engine I have been working on: https://github.com/mattgallivan/daimyo/blob/master/src/Renderer.hs I believe I represent a scene using a list of objects and a call to `lookup`. Most of it is crufty code from when I started Haskell but you're welcome to browse through it for anything that might be helpful.
Thanks again for taking a look at this. The current HEAD in my repo which incorporates the change you had suggested in your other comment but not the inlining and strictness runs only slightly slower than your current HEAD. If possible could you please run both versions on your machine and let me know if you see a significant difference?
You could always scale the time limits depending on the machine.
OK, looks like we're mostly on the same page now. I'm just curious if what you mean by "high level" is what I was proposing to call "high-level language", "high level (language + plus standard library)" or neither. The problem with "high level (language + plus standard library)" is that a language can become higher level by blessing third party libraries into the standard library. Maybe you're really after "high-level language" then.
When I say "high level" I'm normally talking about the language alone, without any libraries whatsoever. The standard libraries can sometimes serve as a proxy though, because they often show what is possible within the boundaries of the language, but as you say, it's not always so.
ski (Stefan Ljungstrand) was the one who first showed me Coyoneda IORef was useful back when I found Yoneda useful but Coyoneda not as compelling.
Thank you for the idea
You can use `ulimit`s to do such stuff if you are on a *nix system. 
Thank you. I will try doing it with ulimits 
The instructions would pretty much be the same for any other patch; the type checker isn't particularly special. Actually it's probably one of the nicer things in the compiler to work on (I assume), since it isn't concerned with fiddly details like platforms, architectures, or the trillion other things GHC has to care about. Read these two pages: - https://ghc.haskell.org/trac/ghc/wiki/WorkingConventions - https://ghc.haskell.org/trac/ghc/wiki/Newcomers For type checker guidance, Simon PJ &amp; Richard Eisenberg are invaluable information sources. You should join the [ghc-devs](http://www.haskell.org/mailman/listinfo/ghc-devs) mailing list and email them (contact info [is here](https://ghc.haskell.org/trac/ghc/wiki/TeamGHC)).
Not just haskell - I need 2Gb of heap to resolve some dependencies using gradle at work. 
By the way, [tasty supports timeouts out of the box](http://documentup.com/feuerbach/tasty#options/timeout). You can set them directly in your test suite, pass as a command line option or through the environment, just as any other tasty option. 
Old and outdated are not synonyms in my mind.
I'd rather generate a DB schema based on Haskell datatype definitions. Perhaps something like [postgresql-orm](http://hackage.haskell.org/package/postgresql-orm-0.3.0/docs/Database-PostgreSQL-ORM.html)?
&gt; This doesn't make it more expressive, in fact it makes it less expressive, but it makes it much safer and easier to use. That doesn't seem quite right to me, but maybe I have a different intuition about what "expressive" means. Don't you need a powerful type system to really express e.g. Functor? Otherwise you've just got a "Design Pattern", i.e. the language has failed in expressiveness and you have to fall back to some shared cultural understanding of the concept.
It uses smtlib. I don't know if there's a preference on the particular solver, but CVC4 and Z3 are probably good bets. I don't know of any SMT solver being developed in Haskell.
Yeah, I'd say it all depends on how you define "expressive." &gt; Otherwise you've just got a "Design Pattern", i.e. the language has failed in expressiveness and you have to fall back to some shared cultural understanding of the concept. I wouldn't say that "having to use a design pattern" = "language has failed at expressiveness." My thoughts on this are very scattered at the moment, so I won't try to elaborate just now. Maybe some other time. ---- The reason I say that introducing a type system limits expressiveness is because type-checking does exactly that: it rejects programs which the algorithm considers to be ill-typed. This almost invariably includes rejecting some programs which are perfectly legitimate programs. Hence, with a type system, we *cannot express* some programs that we might want to express. But you're right. This doesn't take something into account. Once you have types guiding a program's execution, as is the case with typeclases or dependent types, then things get interesting: now you can express things that you *couldn't express before* without types. It's a tradeoff, I guess.
Ah, I meant "testing" in the teaching sense, i.e. "Implement this function and use GHCi to check whether it works".
[more comments](http://redd.it/29xeb0) in /r/programming/
IRC channel on freenode: #snowdrift
I looked at the FAQ and about pages but I'm still quite fuzzy as to what this website is about. Looks like it's essentially an open source version of kickstarter?
&gt; Looks like it's essentially an open source version of kickstarter? More like [patreon](http://www.patreon.com/) given that the whole model is based around [monthly support](https://snowdrift.coop/p/snowdrift/w/intro) for various project.
In a very rough sense it's like Kickstarter. But we're not the 487th clone of that (seriously there's like almost that many), what we're doing is fundamentally different. Kickstarter = one-time, arbitrary make-or-break goal and deadline Snowdrift.coop = ongoing, sustainable, accountable system *yet still assures some critical mass* Currently, the only common mechanism for mutual assurance (which is a key factor in donations) is the Kickstarter threshold model. It has its place but is not a complete solution and offers little accountability. By offering a matching pledge, we address the main reason why it fails to just ask each supporter to donate unilaterally. Our system provides mutual assurance without introducing arbitrary deadlines and the hype and high cost of running these one-time campaigns. Put simply: **each month, I'll put in *something*, but the more people who are with me, the more I'm willing to do.** And so my message to the world is: the sooner you join me, the sooner I'll put in more. Might as well pledge because if few people join us, there's no risk. If many join us, then we really get a lot of funds and then get to see the results we want from the projects. And yes, we're focusing the site on the particular needs of Free/Open projects (not just software though)
Right, Patreon is the closest model to us. The differences are: in Patreon each patron is unilateral, there's no matching (so lacking the assurance that Kickstarter has), and Patreon is a totally proprietary site where most projects remain proprietary too. Snowdrift.coop is based on matching (because there are fundamental problems with just asking each person to individually donate), we plan to run democratically, and we are totally Free/Libre/Open and focusing on the special needs of Free/Libre/Open projects.
So how is explicit type application going to work without explicit foralls? How will you know the order of the type variable formals?
If that's the case, then perhaps it should be data Model = Model (IORef Mesh) (IORef Material) (or your other favorite mutable reference instead of IORef). This *is* similar to your idea of IDs, of course...
It is a magic trick. You can program _as though_ the loaded areas are in the backend, but in reality you have _no_ storage on the backend, and the frontend can call back to any old server running the backend code, with no need for any sort of "session". So you can write continuation-style interactive workflows that dynamically update the page but you don't have to pay any cost in scalability.
yours is faster - 3.7s vs 4.7s. (Next step would be parallelism i think, but if you're implementing a specific algo maybe that isn't the point.) (i had to bump the version of base to get it to run with current GHC.)
I think all three examples would work with an execution model like the CLR has, in which type-specialized methods can be JITed as they are needed.
IMO, this is a very simplified view of "what's holding Haskell back", and that's completely ignoring the nebulous definition of "fast". For example: People don't choose Ruby for the performance gains.
If you're asking for volunteer work, are you non profit? 
nor Java or Javascript ;)
Yes to both. We are welcoming volunteers (and will also work to help people who are learning Haskell, if that's the case). And, yes, we are non-profit. We are incorporated as a non-profit in the State of Michigan but have not formally begun the long process of getting IRS 501 status.
 &gt; haskell-mode is for Haskell modules, not "ghci scripts". If &gt; you were to put the sort of things you type into ghci into a &gt; file, it wouldn't be a Haskell module, because for example a &gt; module doesn't have expressions at top level. A module is a &gt; collection of declarations, which can appear (for the most &gt; part) in any order. Contrast to Lisp where a source file is &gt; an ordered sequence of side-effecting expressions. Thanks for your explanation - that's really helpful. (On an aside: in lisp, in a "kind of" functional style, the ordered sequence of side-effecting expressions would be mainly a set of function definitions, with a final statement that "does something" with the functions. But I guess the sequence of function definitions is a sequence of side-effecting expressions in that each definition creates a binding for the function name in the current environment. So I think I understand what you mean.) I guess working through small examples step-by-step in ghci (as in introductory tutorials) is not the sort of thing that an experienced haskell user would want to do (and is fundamentally different from what they would want to do). So haskell-mode is not intended to help with that. &gt; If there is an expression you want to enter into ghci &gt; repeatedly between reloads of your module, you can just give &gt; it a name. For example if you are testing your sorting &gt; algorithm, put test = mySort [3,1,4,1,5] in your module. Of &gt; course this won't cause anything to actually be evaluated &gt; until you type test into ghci. I might be misunderstanding, but that sounds like it would be more effort to: - think up name - edit module to add line setting name equal to expression - load module - switch to interpreter buffer type out name - edit module to remove added line than to just manually copy and paste the desired text into the command line of ghci, like: - copy region of expression - paste in interpreter buffer (let alone just pressing a single keystroke to get the desired effect) Except, as I'm starting to realise, in the module I just write declarations such as: a = 1 b = 2 (I like that mathematical "purity" - those look like facts that just "are", not bindings in some environment. ) Then when loaded, I could go to ghci and type a+b and see the result (without needing to do any I/O) But, with my (misguided) idea of "cut-and-paste" it would be necessary to paste let a = 1 let b = 2 into ghci to get a similar effect. And I'm not sure what would be necessary with multi-line expressions ( :{ ... :} ? ) and indenting (???), but it seems like it might not be pretty. So, even at a superficial level, it seems that the sort of thing I'm thinking of would not be straightforward. I guess I just have to accept that this method of getting immediate and fine-grained interaction, which works so nicely with lisp (and ocaml, etc), is not consistent with haskell. And I guess with experience I could would out some way of module "tweak-and-reload" to get as close to this interactivity as possible, but in a haskell-y way. If you have any other tips, that would be much appreciated. Thanks again. 
Nor Python :)
Haskell code will not generally be as fast as c code. C makes it easy to write cache line aware code, pervasively uses mutation for performance gains, avoids the overheads of automatic GC. Performance is not that important for most software, and Haskell performance is good enough for most code. I do think there's a marketing issue, where Haskell is sold as near c performance, when it simply isn't. It is an order of magnitude better than Python or Ruby though.
There's also https://floobits.com/ I tried it when it first came out and it worked really well, I assume it's even better now!
Unfortunately, many videos are still missing. If anyone knows why, please comment on
nor PHP :]
the closest thing know of is dynlang, which is more aimed at python / ruby. but when i went (a year or two back now) they seemed interested in a lot of things. http://www.meetup.com/dynlangchile/ you might try /r/chile or /r/santiago
IIRC, such heuristics were helpful when writing [Djinn](http://hackage.haskell.org/package/djinn) and [the template haskell version](http://hackage.haskell.org/package/djinn-th-0.0.1/docs/Language-Haskell-Djinn.html). I'd actually expect the term inferencers of Idris/Agda to be at least as good as Djinn, but perhaps they are optimized toward to single-solution case instead of the interactive use case.
Thank you. I think this *is* the paper I was thinking of. Agreed, neat.
I did this for a model of the Javascript DOM. It works nicely, though it's a little bit of a brainbender to keep track of "entities-as-IORefs" in an otherwise pure tree.
Could this be used as a foundation for a Haskell-based band-in-a-box clone?
I believe Scheme was the first language to enforce TCO in the spec.
Then I suppose it's that functional programming is so different from imperative and no body really cares enough to make the switch 
Thanks, I guess dynlang is my best bet. Hopefully they take kindly to Haskellers :)
So it's a way to directly fund open source projects? That's cool, I can think of some people I want to support. 
Where is the source hosted? You should make that front and center if you are looking for contributers. EDIT: Found it: https://gitorious.org/snowdrift/snowdrift
Just recently tried to do some dwarf debug information parsing. A python script to do it took 18 seconds. Haskell one took 1.5 seconds. C with libdwarf took &lt;100 millis. Could be interesting to make bindings for libdwarf, and compare that as well. But when you care about performance, Haskell is a risk. It may be fine, or it may be slow. If slow, sometimes your only recourse will be to rewrite in c.
&gt; Do you have in mind something that is not obtained, in haskell, with haskellMode/ghc-mod, and ghc7.8's holes? Maybe I hadn't configured things properly, but I never had good success with Haskell Mode. From what I could tell, it just looks up type signatures from the code? That is, it doesn't specialize type signatures based on constraints induced by the caller. I can look up type signatures without a problem. The issue is always figuring out what typeclass is being instantiated or doing mental substitution replacing a type variable by some concrete types.
how can i get involved?
Num doesn't have Eq any longer.
Yeah, that makes sense. We're on both Gitorious and GitHub (the former for being considerate to those who prefer to avoid proprietary silos). Besides the link on the homepage to https://snowdrift.coop/p/snowdrift/w/how-to-help I agree more clear link to the source would be good… we originally thought "Check out the source!" was good enough, but it is indeed a bit hidden in the footer… thanks for the feedback
Check out https://snowdrift.coop/p/snowdrift/w/how-to-help for general info (and there's a volunteer form there if you want to share your interest that way) and come by at #snowdrift on freenode and say "hi" Thanks for the interest!
You got it! We just need to do more coding (and then some legal stuff too) to get it launched…
It still does in the 2010 report. GHC might not require it, and it might disappear in the next version of the report, but for now standard Haskell does require an instance of `Eq` to accompany all instances of `Num`.
A line-up of legends. A talk by BoS is the only one I haven't collected yet!
Oh, I didn't realize it was just a GHC extension! Huh. The code example for Num should list it then. In any case, good to know.
In fact, both `Eq` and `Show` are required in [the 2010 report](https://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1350006.4). It's rather nice not to have either one be required, at least from a practical perspective; applicative functors can given instances of `Num` in GHC but not in standard Haskell. (Well, you have to "fake" several of the `Eq` and `Show` functions.) The `Eq` requirement is also troubling from a theoretical perspective. The computable reals certainly have an implementation of everything in the `Num` class proper, but equality between computable reals is undecidable, last I checked. I can't think of a specific example where `Show` would be impossible other than applicative functors.
Most people already know what I'm about to say, but for me all of these problems have been solved by [switching to Nix](http://ocharles.org.uk/blog/posts/2014-02-04-how-i-develop-with-nixos.html). Nix gives me the ability to easily create a development environment, switch into profiling/shared library mode, change GHC versions, try out variations of libraries from Git, try different versions of libraries for different projects, build documentation/binaries, integrate seamlessly into deployment/continuous integration, and it avoids having to compile over and over and over again by having a sensible binary caching strategy.
It might be useful to mention Scientific?
I hope someone will eventually create an Idris equivalent without GPL dependencies.
He is talking about defining and `Entity` as a `Value` with repeated transformations applied. For instance rather than: `Entity Color Position Model` and `Model Mesh Material` you would have `Translate Position $ Color Color $ Model (Material {- -}) (Mesh {- -})`. The main problem is it allows n applications of a property rather than 1. This can be solved by providing default properties and using the first or last if multiple are provided. Whether those issues are enough to warrant the method being used is up to you.
This is gonna be an epic Haskell Exchange indeed!
[foldr](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:foldr) [scanr](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:scanr) [init](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:init) [last](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:last) 
What about the link he posted?
&gt; Facebook's o_O I mean, I appreciate that they pay the guy, but I don't really think of him as "Facebook's."
See in addition: - [Haskell - Nix Wiki](https://nixos.org/wiki/Haskell) - [My experience with NixOS](http://fuuzetsu.co.uk/blog/posts/2014-06-28-My-experience-with-NixOS.html)
What's the reason for it?
There's no reason to believe that numbers can be compared for equality. As a simple example, it's meaningful to consider functions as numbers instance Num a =&gt; Num (c -&gt; a) where ...
What is “Scientific”?
Haskell noob here. The prospect of having packages reside in nix, ghc, and in cabal is a little daunting. cabal install sometimes pulls down other libraries and compiles them, storing them in your .cabal directory somepace. However, with nix what you want is to use nix to pull down the libs that you need instead. How do you prevent cabal from pulling down these dependencies that you don't need or want. Is there a way to pull the dependencies from a cabal project file and get nix to download those for you? Or, maybe you never actually use cabal? If not, how do you make sure you get all the dependencies you need, is that a manual process? confused. I installed nixos on a VM, and it has packages for ghc7.6. Is there a nixos repo (or equivalent) to get ghc7.8 and beyond? 
A diagram very similar to this can be found in the Haskell 2010 Report (at least in the PDF, page 76). [(Screenshot)](http://i.imgur.com/J54o4Wf.png)
How well is 7.8.2+ supported? In terms of pre-packaged packages.
Whenever I'm hopelessly in love with an idea, I remember that I just learned about it. After some time, it always turns out that there's a big tradeoff. Edit: I'm in love with dependent types; just not hopelessly so any more
Not really. DAWs record and play back audio and MIDI, this is for writing notation.
&gt; Even reading through Learn You a Haskell doesn't come close to preparing someone to read through half the libraries on Hackage. Learn You a Haskell is a fine resource but it still is an introductory book. The problem is that there is a dearth of more advanced books.
&gt; I don't see anything replacing C++ for games programming on the order of the next 25 years and that scares me. Should it? Game programming is a soft real-time problem. They cannot afford the overhead of a garbage collector, let alone the overhead of abstractions provided in higher languages. They pay for this by having to hire more developers, but market factors point to this being a necessary expense. At least C++ isn't stagnating.
I see Haskell getting more adoption in the video game industry by Functional Reactive Programming catching on, and libraries like Netwire getting more use and documentation. I'm convinced all of the tools are there to write a very solid game, just no one has done it yet. Therefore an endless cycle appears in which people think Haskell is bad for creating games because there would be games in Haskell if it were good at creating them. Thus the cycle continues. How can you help? By writing some code! Get games out there, get the word out that equational reasoning can make higher quality software and show that FRP can be a great declarative model for game logic. That's what I intend to do. EDIT: fixed
There has been recent success is using a garbage collected language for tasks that are traditionally hard real time like robotics. (I can't find the link right now, sorry.) Plus, you can always use your Haskell skills to write in the [Atom DSL](https://hackage.haskell.org/package/atom) and generate fixed-memory hard real-time programs. That said, I would really like a language that did region inference but allowed explicit region annotations to provide guaranteed[1], prompt[2], and safe finalization of resources across dynamic[4] scopes. Something that allowed the fine-grained controll over allocations of C++, but used the type system to ensure no memory was leaked. (Eliding deallocations handled by the OS at the end of the program run is optional, but encouraged.) Then, on top of that, provided an opt-in garbage collector (or multiple garbage-collected regions that were each tracked as a resource). Use of the run time's garbage collector would be tracked as an "effect" so that you could easily avoid libraries (or parts of libraries) that would force the garbage collector on your application. You'll find many, maybe even most games now have a garbage-collected scripting language used either for modding or as part of level building (usually lua or something custom), so it's not like game developers don't want to be able to opt-in to a garbage collector, they just need to disable it temporarily during their rendering passes. There's a few languages headed that direction, but they are rare. Rust maybe, in the future. Possibly Idris, if there was sufficient interest. It might even be something new that come out of Valve in 3-5 years, after the Steam box and Steam OS start winding down. I don't see Haskell moving that way, just because of the its pedantic nature. It's still going to be good experience because static, higher-order typing is going to be the only reasonable way you are going to get that level of guaranteed resource mangement. [1] No leaks, when I close the region either I have to release all the resources allocate in the region, or they are automatically releaased. [2] If down one path I know I no longer need a resource, I can get rid of it early. E.g., the example below can be type-checked, at least. do r &lt;- alloc c &lt;- populate r case c of Nothing -&gt; free r &gt;&gt; simple; Just (Left s) -&gt; fast s &gt;&gt; free r &gt;&gt; complex Just (Right b) -&gt; expensive b &gt;&gt; free r [3] No use-after-free bugs or similar invalid-handle issues; calls to release a resource are statically check to ensure that no reference to the resource is "squirreled away" and might be used in the future. [4] Two meanings. First, not *always* mapped to a static / lexical scope. Second, overlapping (not just nested) so that region B can start while region A is active, and region A can be closed leave region B active.
This generalizes to any applicative functor. So, can also be really nice for DSLs, particularly those that use HOAS.
I think the original reason *might* have been^(*[citation needed]*) to make support for literal patterns easier (specifically, numeric literal patterns): f 1 = () `f` must have type `(Num a, Eq a) =&gt; a -&gt; ()`, and making `Eq` a superclass of `Num` seems to simplify this a little. This is a poor reason, though, as it has very little benefits and some nasty drawbacks (as /u/tel points out).
Pretty sure he means [this package](http://hackage.haskell.org/package/scientific-0.3.2.0) on hackage. GHC also has [Fixed](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-Fixed.html). They aren't exactly standard Haskell, but useful to have in mind if correctness is more valuable than speed or if you are exceeding the range of Double and should work on most Haskell compilers.
Optionally on libgmp for some of the compile paths, but Haskell has that as well so I can't imagine it being a problem.
Also non-software projects under similar licenses. Art, science, educational resources, &amp;c.
As well as any other version - they all use the same expressions. You can search [here](http://nixos.org/nixos/packages.html).
when you say mechanically, does that mean there's a tool that creates the nix-expression automatically from a cabal config, or that's something I have to do by hand for my project? I didn't see in the OP's article where making nix-expressions from cabal files was automated. If its automated, what's the tool that does that? In the article, it looks like cabal-install is still used, but its used by nix and not by hand directly. 
Thanks! We'll follow up soon. Glad to see so much interest. Already getting further feedback to incorporate… Always lots to do. I look forward to working with you!
The disadvantage of the continuation approach you are describing seems to be in memory locality. Rust is meant to be a low-level systems language, so if your abstraction is elegant but expensive, then it is probably not going to make it. 
Adding on to this, I'm a developer at an indie game studio, and we're fed up with C++. Next game is in Haskell, we've been spending nights and weekend with the language to get familiar with it.
&gt; Then, on top of that, provided an opt-in garbage collector (or multiple garbage-collected regions that were each tracked as a resource). Honest question. I can see this working for C# and other imperative languages, but how can Haskell be both Lazy and not use a GC at all? I can see some solutions to the problem but all of them seem sketchy. &gt; No use-after-free bugs or similar invalid-handle issues; calls to release a resource are statically check to ensure that no reference to the resource is "squirreled away" and might be used in the future. Actually C++ has gotten a lot better about that with RAII and containers to handle your reference mechanisms (std::unique_ptr and like). I think the biggest thing that will help Haskell and its ilk is improving compiler performance. Whether that be in Haskell or a dialect that gives up some guarantees to enable heavy performance optimizations ala C/C++. (Not code side lapses, but more like "assume this can't be bottom" ones).
I did not mean the continuation-passing transformation as something you would intentionally do to your program. It's a way of thinking about the space cost of certain patterns of procedure calls so that you can reason effectively about space usage without dropping down to a specific machine model. If a language implementation promises to make that form of reasoning valid, then it effectively does "proper tail calls". CPS transformation is actually not too different than conversion to SSA form; whether the higher level language can be implemented efficiently is mostly orthogonal to what intermediate forms are used by the compiler. Proper tail calls and procedural abstraction can absolutely coexist in a low-level systems language, but some restrictions on the form that the procedures may take and how that interacts with nested procedures may have to be put in place. For example, it's entirely reasonable for the "nested continuations" in my previous example to exist at runtime as stack frames in the standard OS form. It's just a more abstract way of looking at them.
It seems that you can convert cabal file into nix commands: [cabal2nix](https://nixos.org/wiki/Haskell#cabal2nix)
&gt; applicative functors can given instances of Num in GHC but not in standard Haskell. Especially functions of type `Num b =&gt; a -&gt; b`, which turns out to be quite handy!
&gt; There has been recent success is using a garbage collected language for tasks that are traditionally hard real time like robotics. (I can't find the link right now, sorry.) If the problem is that the garbage collection strikes at arbitrary times and lasts for an arbitrary amount of time then I suppose a language could ensure that the garbage collector be run at regular intervals and last for a specific maximum length of time. It wouldn't always collect all the garbage but that could be ok.
Just fix whatever you think is broken about the Haskell ecosystem. The best time to do this is when the problem is fresh on your mind.
thats also an example where you're doing binary format parsing! :) I absolutely agree in that context haskell tooling suffers, and theres room for doing better. 
Your comment carries a latent assumption that Haskell is a *new* language and that its primary design concerns should have been motivated by today's industry use cases. In fact, the basic ideas behind the language were already pretty solid in the community that created it by the mid-80's. Miranda, which Haskell closely resembles, was released in '85. The first definition of Haskell itself was in 1990. The stated purpose was to create a "common ground" for the exploration of non-strict functional programming language design. This was a committee of academic programming language researchers working together to create a solid foundation for research. That was around 25 years ago now. The encouraging thing today is that game programmers have now *heard* of Haskell, and realize that the theory behind it might have some application to programming in their domain. Lots of research that started in the academic realms that Haskell inhabits has spread out into industry, albeit in a rather diluted fashion. And programmers are now more aware of it and willing to look into it for principled solutions to their problems. So, if in another 25 years people are still using C++ (and I won't be terribly surprised if that is the case) it will at least be a very different C++ that looks a lot more like Haskell.
Sup guys, I'm planning to write a game in Haskell too. Would love to follow any devlogs you have.
&gt;The problem is that there is a dearth of more advanced books. Compounded by the problem that Haskell is extremely hard for beginners. On the other hand of the spectrum, you can start writing PHP apps in about an afternoon.
Yes, there are garbage collectors that make some fairly good responsiveness guarantees. [Generational Metronome](http://researcher.ibm.com/files/us-bacon/Frampton07Generational.pdf) guarantees that the application gets at least 7 microseconds out of every 10 microsecond window. [Concurrent, Parallel JamaicaVM](http://www.aicas.com/papers/ismm02f-siebert.pdf) didn't have any simple, strong guarantees that I could see, but does not require all threads to wait on a garbage collection; would be nice if it was generational as well, since most objects seem to be short lived.
Nah. Haskell has grown a _lot_ since h'98, and at this point it has been pushed by industrial concerns in many ways, and the surrounding ecosystem has as well. More and more people are using it for all sorts of practical problems, and cutting edge language research has tended to migrate to other languages. Instead, we get cutting edge _library_ research, and research in _productionizing_ ideas developed in more experimental contexts. People will still be using C++ in the future, I suspect. But also a _lot_ more people will be using Haskell (or a successor to Haskell) than are today.
I think that's what the OP is getting at, in his/her mind Haskellers spend more of their time and energy building higher and higher abstractions without lowering the ladder behind us for anyone to climb up, at least compared to other open source language communities. Don't know if I agree, but it's not the first time I've heard this. Even Haskell veteran /u/sigfpe said the other day that: &gt; I've enjoyed watching the Haskell community's relentless march towards abstraction over the years. But I don't envy newcomers.
I don't think we are actually in disagreement. Haskell is what it is because of its past. GHC has grown a lot lately, sure, but the Core language is nearly unchanged. The theory was shown to be *useful* as well as nice to work with for research. So all sorts of practical stuff is showing up now, both in Haskell and in other languages. Some of the core features of Haskell mean that it may never be an ideal language for some domains itself, but it may host ideal languages as DSLs or provide a lot of inspiration for them.
Yes, there is a tool called cabal2nix. It took me about 1 hour after starting a haskell project in Nix to using nix-shell with it and creating a "custom" *.nix package that wasn't available yet in the nix repos.
As someone whose job it is to write JavaScript/Java, I'm extraordinarily jealous of anyone whose employer is paying for them to go here. One day....
Looks like there is some very interesting stuff here.
well the core language _was_ the simply typed lambda calculus, now its System F with coercions, which is pretty cutting edge stuff as core calculi go :-) And honestly I don't think there's a basic obstacle to a "haskell" that is either pauseless (that one we can just do with the right GC) or even not garbage collected at all (such a language might need a few more restrictions or annotations). There may be some research involved, and certainly some coding, but as the popularity of the language grows, I'm sure those will come with time...
Right, and writing good tutorials is part of lowering the ladder
I find FRP a lot easier to read and shorter than the equivalent C++ base when combined with lenses and other nice Haskell abstractions, but I can understand where confusion and complexity can come in. I think your issue is not with FRP, but with the fact that most FRP(including Netwire) is implemented using Arrows. I too find Arrows quite confusing to deal with, but I have a series of links you may be interested in reading: http://cdsmith.wordpress.com/2011/07/30/arrow-category-applicative-part-i/ http://cdsmith.wordpress.com/2011/08/13/arrow-category-applicative-part-iia/ These articles show that Arrow is not exactly the same as Category + Applicative, but it shows that *most* instances of valid Arrows can be represented as a Category + Applicative. I find Applicative syntax a lot easier to read so this is indeed good news to me. I'd like to see netwire rewritten in an applicative style with some typeclasses redefined in terms of Category rather than Arrow(ArrowLoop comes to mind). Maybe there's something else I'm missing? :P That's the only ugly side to FRP I saw.
[Nikki and the Robots](https://www.google.com/search?q=nikki+and+the+robots) is a full game in Haskell, with some C++ FFI to Qt, and the source is available for your perusal. Hopefully, the Haskell there will be more instructive than other guides that are less focuses on games.
You should [also](http://www.reddit.com/r/haskell/comments/2a310v/where_is_haskell_going_in_industry/cirb2jk) look at Nikki and the Robots, and you might crawl through the archives of the [devlog](http://joyridelabs.de/blog/?tag=nikki) though the project has been shuttered. :(
Hmm... well, the hardest part is using the FFI to make everything happy, I was planning on targetting SDL 2.0, there's a bare bones binding for it in hackage already, and I have a few minimal tests getting it working. But thanks so much for the pointer! I'll definitely check out the game and the sources. EDIT: For those following along at home, I finally found the source code, you can get it using: darcs get http://code.joyridelabs.de/nikki
Thank you
Actually you've (perhaps unwittingly) described the issues with using Haskell for games. That's a tremendous amount of work for a person to go through just to be able to script basic game logic. I don't actually have a problem with Arrows myself, but they sit nicely alongside Applicative and Lens in the family of tools that people *could* use to produce nice code but instead use to produce gibberish that appears to work via dark magic. My poorly phrased point was basically that there's significant overhead to using Haskell for games in the form of personnel training, and no real evidence that it will provide a concrete benefit over current languages (as opposed to being "as good as" which is not enough reason to switch). 
I disagree the Haskell is hard for beginners. It *is* hard for someone that has previous experience in an imperative language and no experience in a declarative language (e.g. Prolog).
Their FFI is somewhat limited, IIRC. Basically they used the C ABI style FFI to pull in a few dozen calls into Qt using the mangled names of C++ entry points.
AKA the vast majority of working programmers :P
My experience with nix is somewhat mixed. Suppose I just want to install something in my global package repository so I can play with it in GHCi without tinkering with nix expressions. I had zlib fail to compile because it couldn't find the C bindings, ncurses fail to compile for the same reason and so on. I resorted to tweaking LD_LIBRAY_PATH to make it work. I'm sure there is 'some' proper solution to this problem but I couldn't find any on either IRC or Google. nix-env -i ncurses doesn't solve the problem. ncurses is now installed, but how will my code (for example a C library I just pulled from GitHub and just want to build locally) know where to look for it? I don't really want to write nix expressions for everything I ever want to do and that seems to be the only solution.
You can also consider trying [LambdaCube](http://lambdacube3d.wordpress.com/) if you want to experience what it is like to write your shaders and pipeline setup in Haskell. It is not production ready yet, but it’s quite powerful already. Lately I’ve been using it simultaneously with Unity/C#, and I have to say that even though the latter has much better tooling (ReSharper is nothing short of amazing!), I still feel more productive in LambdaCube/Haskell due to the expressiveness of the language.
These days you could use Qt Quick to bypass the C++ API. You could also use Qt Quick for in game GUI which means two birds with one stone. :)
Thanks for sharing. Although a game probably would utilize the young generation collections the most on garbage generated per-frame. It would be interesting to see how it affects performance in games specifically.
Wow I had the opposite experience with Applicative. Sure it looked very strange at first, but once I learned the few useful combinators provided and understood what they did it became very easy to read code that uses them. Much easier than code that, for example, came up with it's own way to do the same pattern every time (sort of what happens with the mainstream languages today which do not have great abstraction facilities). I'd say that is concrete benefit right there. The fact that these abstractions can be captured, implemented correctly once, and then re-used is huge for code understanding and maintainability. I'd rather learn Applicative once, and then understand its use in hundreds of libraries than having to learn each one individually.
The high-level, handwavey description of how Rust's type system works—for the generally curious, not just for the parent—is that Rust has two kinds of pointer, one which "owns" the memory it points to and another which "borrows" the memory, and the compiler guarantees 1. that a given chunk of memory can only be referenced by exactly one "owned" pointer, so passing an owned pointer to a function or assigning it elsewhere transfers ownership as well, and 2. that borrowed pointers to a value never outlive the owned pointer to that value. This means that a call to `free` can be statically inserted whenever an owned value goes out of scope, which—coupled with Rust disallowing uninitialized pointers—gets you complete memory safety and manual memory management simultaneously. More concisely and technically—Rust has a linear type system for managing memory, and a limited region system for managing references. Rust calls regions "lifetimes". Rust also implements reference-counting and garbage collection in the standard library, which you use on a per-value basis (i.e. it doesn't magically start tracking resource usage—you have to explicitly allocate a value in a garbage-collected box.) However, even if you do use GC, it's implemented in such a way that garbage collection is only performed when interacting with a GC'ed value, so if you create a GC'ed value and then have a tight loop in which you never touch that value, then you can be sure you won't get some kind of random GC pause in the middle of that loop.
Rust doesn't allow you to *explicitly call* `free`—freeing memory happens automatically when the pointer to the memory in question goes out of scope. It works kind of like a compiler-checked version of C++'s `unique_ptr`. (That said, we can always force a value to go out of scope in order to free it.) Rust is pretty conservative in reasoning about whether a value has been moved out of scope—if a pointer has been moved at the end of any branch, it is treated as having been moved in all branches. If we have an `if` where one branch transfers ownership of a value while the other doesn't, Rust will overapproximate and treat the pointer as no longer valid after the `if`, and its contents will be freed: fn sample(n: int) { // this allocates space on the heap, initializes it with // the value of n, and returns a unique pointer to it let x: Box&lt;int&gt; = box n; // we have ownership of the memory pointed to by x, // and can use it at will writeln!("x={}", *x); if some_condition { // this transfers ownership of x to another_function another_function(x); } // we can no longer be sure that x is valid, so it // will be freed even if some_condition was false, // and referring to it here will be a compile-time error // writeln!("x={}", *x); } If you're still curious, [this blog post](http://featherweightmusings.blogspot.com/2014/04/rust-for-c-programmers-part-4-unique.html) is a good starting point.
What if the she shader / material comes from a `V`?
Maybe something based on a `Monoid` then?
Functional reactive programming is pretty much useless for video games. Yea, you can build your event loop on FRP but that's it. Outside of that tiny piece of code your game code still looks exactly the same as when you would have written the event loop directly. No, the area where FRP has a chance to really change things is GUIs. Even the mainstream is already moving in that direction with data binding and reactive templates in JS frameworks.
i'm pretty sure pinealservo did not mean `core` as the intermediate ghc representation but core of haskell (higher-order functions, adt, type classes).
Easy - you just extend the set of Haskell packages with your own library. I'm working on (another) Haskell implementation of Socket.IO, and I've split my work over three libraries. In my examples project, I bring all of these together: https://github.com/ocharles/engine.io/blob/master/examples/shell.nix The layout in each of these projects corresponds to how I generally lay things out, btw.
Excellent - keep up the great work! 
Ha! I knew I was playing with fire with that one...
&gt; you can start writing **shitty** PHP apps in about an afternoon FTFY Haskell beginners experience just follows its dev pattern - doesn't work at all until it works nicely for most use cases (=
To answer one of your questions: *Why is there is so much work on building more perfect languages and abstractions when the industry adoption already lags 25-30 years behind what you fellows are already working with?* Without this work, what would industry be adopting in 25 years?
&gt; Next game is in Haskell Wow, very much looking forward to that.
&gt; I don't think there's a basic obstacle to a "haskell" that is ... not garbage collected at all That sounds ... amazing :o
My personal vote is a Lisp, which I think was the original intent in the design of the language, namely machine learning through reflective metaprogramming.
 So: Haskell is hard for most Haskell beginners; not for those who are new to programming in its totality.
I agree that continuous time isn't useful, but FRP doesn't imply continuous time. FRP signals/events work perfectly fine with discrete changes.
The premise is problematic. *Why* would computer algorithms become 'smart' enough to write their 'own' code? What was our goal in constructing those algorithms? Besides this, what is the purpose of your question? Thought experiments must have one. It's hard to discuss an already vague idea without this context.
Thanks, this is a problem I ran into recently.
To my knowledge the core language was never the simply typed lambda calculus. I don't see how that could work without monomorphising the program.
Once you're conversant, is a book really the best way to keep learning? I needed a book to kick-start my understanding, but after that coding and reading code trumped books. I never ended up reading RWH.
Machine learning is a historically interesting topic. Metaprogramming of code writing code, where character strings as output then serve as instruction sets as input, is a very popular topic in design pattern lectures. My personal belief is that algorithms getting smarter requires an element of this type of reflective metaprogramming of a given system's codebase. This is not currently a widely held belief or strategy, as a lot of machine learning emphasis is in probabilistic data, Bayesian algorithms, and imperfectly known weighed decision making with feedback reinforcement within a static framework. However, I think the creators of Lisp had auto-metaprogramming in mind as a key paradigm in machine learning. It is no accident that Lisps are considered strong in metaprogramming design patterns and also considered the historical programming languages for AI. I don't think this is a coincidence. At the end of the day, however, all programming languages are designed for people to write the instructions and interface with the computer. The higher level languages are considered higher because they allow more intuitive interaction between human programmer and machine. But what happens when a program written by a human programmer in say a Lisp or any high level language starts doing a really good job metaprogramming its own source code. Will it evolve to continue doing so in the high level language of the human programmer's choice? And if so then which language is more likely to create such a program in the first place? Or will it revert to a lower level bytecode or even machine code in its auto-metaprogramming as a more efficient strategy for its own improvement, since the code remaining a high level language is no longer an issue for the machine without the programmer? The purpose of my question is curiosity and a desire to better understand computer science, even if only academically.
I love this answer!
My experience as a mostly noobie is that the documentation spends too much time defining the library and not showing people how to use the library. We need far more cookbooks (like the new one that just came out last week (http://haskelldata.com/) covering more domains.
I know one of the people that led that project. It was very ambitious and sadly is indefinitely on hold right now. It was intended for the iPhone, which is why the GC issues came up. I actually think that on more powerful platforms the GC is mostly a non-issue. I also want to point out somewhere in this big comment thread that #haskell-game is a thing on Freenode. There is about one solid conversation a day there on average, and more activity is very welcome! 
Its a real time way of sharing a session between people in different locations. So you can be coding away in emacs, a teacher/collaborator can be in a separate location in vim and what you type changes the text in his buffer in real time. There's also a video chat so you can talk about what you're doing.
Careful with your wording. x doesn't go "out of scope" until the end of sample. However, based on the blog post, the memory pointed to by x is freed before leaving another_function. So, in a sense you *can* explicitly call free. It can even be your own free. As long as it takes ownership and doesn't give it back (by returning the pointer), you'll see the memory be released promptly. Rust does seem like a very nice starting point (at least) for the way resources should be managed.
Stephen Blackheath is the creator of the Sodium FRP library. [It'd be worth exploring his work more.](http://blog.reactiveprogramming.org/?author=6)
I think such an algorithm is very likely to use some form of search. Therefore, I think that an appropriate language would be one for which the program-space is smooth, that is, one in which similar programs behave similarly. None of the existing languages fit the bill, either because small changes are likely to make the program crash, or because abstraction allows us to make drastic improvements with a small amount of code. In addition, your algorithm wants to modify its *own* code. This adds an extra layer of complexity, because after the algorithm switches to the generated program, it can no longer use its known-safe existing program to monitor and abort the generated program. You don't want the algorithm to accidentally generate a program which contains an infinite loop or a dynamic error, especially not in the part which monitors the next generation of generated programs! To avoid such errors, I think the programming language would need to include something like Agda's totality, termination, and progress checkers. Of course, now we're back to the original problem: small changes are likely to make the program fail those checks. In summary, I think the language you are looking for will be very different from anything in your list.
Lenses are mentioned. If this approach really can derive lenses that's fantastic. No more TH :) I'm also hoping it will do profunctors and their subclasses. Looking forward to reading through properly!
It can compute the lenses for a datatype (as an n-ary product), but unfortunately not introduce meaningful names for them. There's simply nothing in GHC except TH that can generate names right now.
Ah yes of course. The sums-of-products formulation "forgets" a lot of the structure of the datatype, especially the names! Anyway, all your generics stuff so far is really cool so looking forward to learning more about this one.
No, the problem isn't at all that we do not have the names (we *do* have them, in the metadata), but that we cannot create new declaration for all the lenses of a datatype. A generic function still has to produce a Haskell value, it cannot produce a (variable number of) declaration(s).
Oh really, so if you have data T = A { a1 :: Foo, a2 :: Bar } | B { b1 :: Baz, b2 :: Quux } then you can still keep around all of that structure? Anyway, I guess I should read the paper rather than jumping to conclusions :)
Yes, but most generic programming approaches can do that. In particular, GHC.Generics can keep all that structure around as well. We claim in the paper that in generics-sop, this is nicer, because the metadata is separate from the main structure and can be recombined as required, but isn't in the way at all for code that doesn't use it. In comparison, in GHC.Generics the representation even of the simplest datatype is really difficult to grasp, because all the metadata is intertwined with the structure of the datatype.
No. You can have FRP without continuous time, and only discrete changes in response to discrete events.
it makes it possible to have a combinator that does data Point = Point {_x:: Double, _y :: Double } x, y :: Lens Point Double (x, y) = extract glenses note, that they are talking about "simple" not van laarhoven lenses. but there is no conceptual problem with defining van laarhoven lenses.
&gt; The earliest formulation of FRP used a continuous semantics, aiming to abstract over many operational details that are not important to the meaning of a program. . &gt; FRP has taken many forms since its introduction in 1997. One axis of diversity is discrete vs. continuous semantics. From [the Wikipedia article on FRP](http://en.wikipedia.org/wiki/Functional_reactive_programming) Or as Archer might say: "I've heard it both ways."
Pattern matching on integral types is a special case: it desugars to using `==`, hence why it only works when the type also implements `Eq`. c.f. `Char`, where each value really is a nullary constructor.
Your comment has confused me even more! Pattern matching on integral types desugars to ==, but the right hand side can't be an arbitrary expression, it has to be syntactically a constructor, like 0. So in some sense it still seems like Integer's constructors are exposed, so it doesn't look like an ADT. Maybe I'm missing something?
 display = do loadIdentity -- I think the problem is here. It's going to try and render in the area (0,0)-(1,1), but that may not even be in the viewport. Instead, you'll probably want a call to ortho (or some other view matrix function) to match your rendering coordinates with the viewport. I have some [old example code](https://gitorious.org/hsnehe/mainline/source/e02c54e51664265c08992075f4e6a7b47496f9a7:NeHe_02.hs) for doing GL in Haskell. It's written against a rather old version of the OpenGL bindings, didn't get to far, and doesn't use Gtk, but it still might tie together the parts you are missing. And, just looking at my old code tells me your loadIdentity call isn't the problem at all. Hrm. Have you tried disabling back face culling? I'm not sure if your vertex order has the polygon facing toward the camera or away. If turning off back face culling fixes things you can either change your ortho projection or vertex order to get things working with back face culling enabled. Sorry I'm not more help, this isn't really fresh on my mind because I've not done OpenGL for some years now.
Tell this to the Wordpress team.
&gt; So: Haskell is hard for most Haskell beginners; not for those who are new to programming in its totality. I expect it's still hard for them, just not significantly harder than some other languages. 
&gt; The right hand side of what? You can't write equalsFive (1 + 4) = True equalsFive _ = False I guess OP is missing the fact that literals, but not general expressions, can appear in patterns. 
Well what's bugging me is that the very same code works perfectly once I create the drawing area at program start up. The box that is drawn is distorted but at least it's there. So I think that this shouldn't be the problem, but I'll keep trying. By the way all the drawing code is copied from some example on the wiki. I thought I'd get the thing up and running before I try anything more ambitious.
Oh, I had to present the Multiplate paper by Russell 'O Connor, so I know what they are now, but it wasn't very easy :)
It's hard to say, really. I believe it would "think" in a dependently typed language (Agda, Idris, etc.) because it would constantly be attaching propositions and to-be-erased proofs or proof-parts to "code" for purposes of composition and searching. But, I'm not sure the code itself would be dependently typed; it would likely be un(i)typed, portable assembly. Portability is useful because it extends the resources you can utilize, at least until it can manufacture hardware that marginalizes all the hardware that existed before it began rewriting it's code. I say this because while having propositions and proofs and computation in the same file (in the same language) is valuable for humans that have wonderously flawed recall abilities and can only track a truly meager number of pieces of information without assistance. It's likely that neither of these flaws would exist in a general machine intelligence; if that is the case, correlating the propositions, proofs, and computation would not require they be in a common storage or language. In order to be bandwidth (network on memory) efficient, it would likely only send the computation parts to the various CPUs, so it may look externally like it is writing in the un(i)typed portable assembly, and JIT compiling. That assumes we can recognize the streaming-suitable compression (something like lz4, maybe?) that is used from data transfer. This is also under the assumption that it is not using fully homomorphic encryption to hide all transferred code and data from outside viewers.
Yeah, exactly. I thought the rules of pattern matching were "only constructors allowed", rather than "constructors and numeric literals allowed". Note that only numeric literals need to be special cased, because other literals have types whose constructors are exposed, like Char or String. So it seems like Integer is an ADT with hidden constructors *and* allows pattern matching. Woah. The next obvious question is why I can't define a type like that, but maybe that's beside the point...
I don't think I'd say that `Char` "has its constructors exposed".
Hmm ... well you're clearly correct! However, I can't say that I find the Report's terminology particularly helpful there. Character literals are certainly not syntactically the same as data constructors. It seems to me it would be clearer to say that "logically" `Char` is an enumeration of the characters (or it is isomorphic to such), not it literally *is* an enumeration. I think your objection to `Char` having "special" constructors that mere mortals cannot replicate is better addressed by saying the Report is phrased badly there. 
I started with Pascal, was writing complex programs in about a month. My code sucked, but it worked. The problem I see with Haskell is that you don't necessarily get a lot of feedback/rewards until late in the game.
&gt; If down one path I know I no longer need a resource, I can get rid of it early. E.g., the example below can be type-checked, at least. Do you mean in a potential language that's not Haskell? I don't see Haskell rejecting Just (Left s) -&gt; fast s &gt;&gt; free r &gt;&gt; complex &gt;&gt; mistakenUseOf r 
BTW, in Haskell (and some other FP) culture ADT generally stands for Algebraic Data Type. This is in stark contrast to the C/C++ (and imperative / OOP) culture where ADT generally stands for Abstract Data Type. Numeric patterns are handled specially by the language, they are not constructor patterns. So, numeric data types can be pattern-matched even if they are abstract as long as they have both a `Num` and an `Eq` instance. You'll notice you can actually widen the type of your fac definition to `(Num a, Eq a) =&gt; a -&gt; a` which also unifies with `Word8 -&gt; Word8`, `Double -&gt; Double`, or `AD s Scientific -&gt; AD s Scientific`. The Integer type in particular is special as the compiler itself has to know how to create Integers from strings of digits as it has to (pretend to) pass an Integer value into the fromInteger function in the `Num` instance to "overload" numeric literals for all the other numeric types. Usually, you do not have access to the constructors of the Integer type, so you can't use constructor patterns to match on Integer values, as Prelude (and many other GHC-specific) modules only export the Integer type abstractly -- without exposing any of its constructors. If you find [the right module](http://hackage.haskell.org/package/integer-gmp-0.5.1.0/docs/GHC-Integer-GMP-Internals.html), you can get access to the algebraic constructors of the Integer type. However, it's mostly uninteresting, as the "deep magic" is in the types require "magic hash" to write. Types with a magic hash name are abstract and non-algebraic as only the compiler knows how they are implemented.
Well, it can help. Layering/nesting tagged, initial encodings with final, tagless encodings can often "solve" particular instances of the expression problem.
Yeah, folks have already explained that to me in the other thread. Thanks! I guess I was imagining Haskell to be a different language, where pattern matching only allows matching against nested constructors and is guaranteed to terminate. As far as I can tell, numeric literals with their Eq instances are the only exception to that rule.
How would matching only against nested constructors ensure termination?
That's a great explanation, thanks! So if String was an ADT, it could also support pattern matching via Eq?
Yeah, sorry, that was imprecise. Of course in a lazy language any non-constant function can fail to terminate. When I say "always terminates", I usually mean "would always terminate in an eager language". That's a useful property that pops up all over the place, e.g. derived instances of Eq have it.
I'm not really sure what you're asking, but you can already use literal `String`s in patterns, because they are sugar for lists of literal `Char`.
&gt; A related question is why String is defined as [Char] instead of an ADT with appropriate operations. Maybe Haskell's designers were against using ADTs in the standard library for some reason? The `text` module does use that, and is way faster. You really shouldn't use the `Prelude`'s `String` for anything serious.
band-in-a-box is an automatic accompaniment generator. It's not a DAW.
Ah, I see, there's already an extension -XOverloadedStrings. It doesn't replace the built-in String type because that's impossible, but it allows you to use string literals with arbitrary type, and it hooks into pattern matching using Eq in exactly the same way as Num.
True, although that's not what I thought you were asking!
Strings literals too: {-# LANGUAGE OverloadedStrings #-} import Data.String data MyStr = MyStr String instance IsString MyStr where fromString = MyStr -- No instance for (Eq MyStr) arising from the literal `"foo"' isFoo :: MyStr -&gt; Bool isFoo "foo" = True isFoo _ = False Matching on Char literals is probably also implemented with Eq, but since there is no OverloadedChar extension yet, I can't write a demonstration program.
Now I'm just wondering how far we can generalize pattern matching if it's already using a mixture of constructors and Eq, rather than just constructors. Why not allow arbitrary expressions then?
That is indeed a good and interesting thing to wonder!
Do use cases for phantom types always overlap with use cases for GADTs?
... crud. Hmmm ... could it help to have some sort of type-level distinction between Entities and Values? Probably doesn't solve the problem, but being able to work with them in an abstract way could make things more clear.
&gt; Constructor calls and record update syntax (at least) would no longer be pure, they would have a region-tracking type I am curious about this. I wonder if the compiler can infer unique usage in a way similar to how streaming is done. But I agree it is a fundamental problem. &gt; If not, those would appear to be ways to "squirrel away" a value and get a use-after-free bug. It isn't designed to be bullet proof, it has a `get` method that returns the raw pointer, so need for trickery if you want to remove the safety. It is necessary for interop however (especially since C style APIs are still popular). The idea is that `delete` is called automatically so trying to squirrel away a value is suicidal.
It would certainly use assembly and machine language. The reason we develop the other languages is to substitute human computational deficiencies. There wouldn't be any benefit for a machine to use any other programming language in your scenario. The interpretation and understanding would require separate logic which wouldn't really be a programming language but would require a lot of the same technologies. Basically the computer would be able to perform all of the neat functions provided by programming languages and would have no use for encoding them just so that they could be decoded into machine language again.
The advanced topics seem at least two or three steps above the regular topics. It seems odd that someone capabale of understanding "Finally tagless" or Conal's work, would be interested at all in the actual course topics itself: https://github.com/fptudelft/MOOC-Course-Description/blob/master/TOC.md
When you use GADTs + PT then you can *choose* which constructors get which phantom type. Furthermore, it becomes impossible to construct a value with an incorrect tag. So if you have 1 PT per constructor, GADTs can help. GADTs can do a lot more than just phantom types though.
There are probably 4 camps floating around. 1.) The camp that believes it must be able to be explicit so I can explicitly ask for something to be merely Applicative. (Your case above.) 2.) The camp that believes it must actually be explicit, so I can ensure I don't get space leaks, when my monadic code gets turned into Applicative code. This is the dual of your case above. 3.) It must be a flag so that users can have it become the default, so that folks who have never heard of Applicative can just have fast parallel code. 4.) The camp that just wants whatever solution we get to be simple, and if it gets too complicated is just going to ask for it all to stop. The main issue is that the #2 'it must be explicit' crowd provide zero benefit for the folks who really want to use it for users who will never know the difference. The sad truth is that I think that trumps all other concerns here. Why? ...because Simon Marlow is the one who will probably implement it -- and it is only useful to Simon at Facebook if it is a flag that turns it on and makes it so his users can't screw it up. (#3) ;) Otherwise he might as well explain `Applicative` syntax. Pretty much the way I see it power to define how this works is going to go to the person implementing it. There are too many points in the design space, but _all of them_ are better than the status quo, so I'm not going to get too bent out of shape as long as there is something like a flag to turn it on.
I think it's tempting to think that there will be one ultimate language which will work for this kind of clever machine. But ultimately I think that such a machine that you describe will choose what language to program in will depend on the circumstances and what its goal is. It's similar to how we humans do things. We might want to experiment with some kind of algorithm and do exploratory programming. Then we choose a high level programming language. But when it is time to deploy the algorithm we've developed we choose a more low-level programming language closer to the metal to really make use of the hardware resources. I think any machine that writes its own software will ultimately use domain-specific languages for many different tasks, simply because it is the best use of resources over the long run. You could also imagine such a machine developing its own hardware. Then the low-level languages would have to be developed together with the hardware to come up with a match that can make good use of resources and be efficient to program with.
Yeah, that really looks odd. But I really hope that Erik teaches these concepts. :)
me too but if there's anything i've learned about 'extra credit' in teaching, it's usually something done on the student's own time. But I'm sure there will be lots of forum discussions - which are half the reason to participate in moocs themselves.
This: `decrypt :: Message Decrypted -&gt; Message Decrypted` should probably be `decrypt :: Message Encrypted -&gt; Message PlainText` shouldn't it?
Phantom types are less powerful. They might play better with extensions that are bad with GADTs though. 
The question makes me a little dizzy. If we make pattern matching rely on Eq in the general case, then we might run into trouble, because instances of Eq are themselves implemented with pattern matching. But I can't say exactly which situations will lead to trouble, and which ones will be okay. Is there a good explanation of this somewhere?
I always appreciated the wide diversity of GP libraries, great to see another one!
I think we... agree? I was saying (or at least trying to say) that we could both have an opt-in via LANGUAGE pragma to the "optimization" aspect (which Simon and co. could use) and an explicit "ado" which those of us who like to know when our code is being "pessimized" (applicative-&gt;monad) could use. \#2 could perhaps be handled by an opt-in warning? Of course it's the implementer who gets to decide :). I just think it's a shame we can't have our cake and eat it too... Oh well, perhaps "ado" is something that could be proposed post-7.10. I guess it might even be a good first foray into GHC hacking...
Right on the money. The question is kind of ironic considering the semi-recent buzz and PR about lambdas in C++11.
It sort of like the race to the Moon. Ultimately there wasn't much scentific gain from a few men stepping on the Moon, but there were a *lot* of spin-off technologies which we as a species have benefited from enormously. (Same thing with cosmology in general, I think.)
Hopefully the C++ standard gets the tools to allow proper C++ APIs. C APIs are workable but have their shortcomings.
What do you mean by "strictly negative" types?
Indeed, thanks!
Yes you're right, fixed.
This is pretty much a final encoding, yep. It's interesting to compare the encoding you're building here to the following one data List a = List { viewLeft :: Maybe (a, List a) } which has an obvious correspondence to the way lists are usually defined as ADTs.
You're not missing anything, using a wrapper like you say is an alternative solution. I guess this depends on personal preferences, but to me it feels cleaner to have the `attribute` as a kind of `tag` (not the best words) on the original type, rather than creating a completely new wrapper.
Hey, I was an active member of the DynLang community in Chile until last year. There were a couple of people interested in Haskell, you should go to one of the meetups and find those haskellers!
you can have functions working on all `Message a`, i.e. a `length` (which might or might not make sense depending on the encryption chosen).
&gt; The key is that you can do both on the same structure, so if you alternate between inspection and further binding the catenable deque approach is a win. &gt; &gt; If you never do binding after you start inspection then there is no win here. &gt; &gt; If you always associate to the right then you can of course use the naive free representation. What I want to say is that you can alternate between inspection and bind in the "`(&gt;&gt;=)` is a constructor" model as well and have amortized O(1) access to the first head *if* you slightly restrict your usage pattern to "never bind an inspected expression to the left" plus "never inspect an expression twice" (aka *ephemeral use*). In the example, inspecting the expression let e = ((…((m1 :&gt;&gt;= k1) :&gt;&gt;= k2) :&gt;&gt;= …) :&gt;&gt;= kn) will transform it to m1 :&gt;&gt;= (\x -&gt; k1 x :&gt;&gt;= (\y -&gt; k2 y :&gt;&gt;= … kn)) The left-associative use of bind in the expression `e` has been amortized over subsequent inspections, so that's ok. (This is where *ephemeral use* comes in). Moreover, the continuation on the right of the `:&gt;&gt;=` constructor can be used inexpensively, as long as it occurs on the right of a bind. As far as I can tell at the moment, catenable dequeues lift this restriction and give `O(1)` access in all cases. However, I would argue that since the continuation `k` has an existential type, it is seldomly used to the left of a bind, so the first restriction is sensible. Seconds, monads are often consumed ephemerally (see also the "IO state hack"), so I think the second restriction is sensible as well. 
This isn't really what's called the monomorphism restriction, which is concerned with overloading. The problem here is that we recurse on nil and that point require nil to have a polymorphic type, so we need some form of polymorphic recursion. However, type inference assumes unannotated bindings to have a monomorphic type in recursive positions. 
I'm not really sure, this is just the sort of intuition I have, coming from the idea of "virtual species", where we could have the quotient type (Bool,x)/2, isomorphic to x, but we could also have x^n/n!, isomorphic to an unordered bag, for the first case we can just cancel the 2 with bool, to get a concrete x, but in the second case, there's no non-quotiented type algebraic type for bags, so it's "virtual". In the refinement type case, if you think of predicates as "subtracting out the values that don't hold", we could "subtract out too much", so that we can only describe it as a type with a predicate. I have no idea if such a concept makes sense though.
I think your intuition about subtraction is a good place to start, but it's more like set subtraction than integer subtraction. The number of inhabitants of a type is always going to be non-negative. This does mean however that you can end up with types for which you cannot construct any values, a simple example is {v:Int | v == 0 &amp;&amp; v == 1} which can be read as the set of Ints `v` that are equal to both 0 and 1. Of course, this set is empty, so we would say the type has no inhabitants. Note that you can also define uninhabited types in Haskell, e.g. data Empty
So, final encoding is just using procedural abstraction then.
The _requirement_ of ephemeral use is the problem. Heretofore it is the price I've been willing to pay for speed, but there is a lot to be said for something that 'just works'. I've had to maintain phase distinctions between construction and inspection because of the tooling I've had available. There are a number of cases for which this phase distinction isn't possible though. Notably the reflection without remorse approach works very well for syntax trees where you do a lot of substitution and also occasionally rewrite / work from the bottom. For me the big payout is for things like pipes/conduit/machines, where you do alternate between the two extremes a lot. You append to the continuation to (&gt;&gt;=) but then you have to inspect on the outside to compose pipes/machines (~&gt;). This currently leads to an asymptotic hit in pipes when you compose after binding, where you have to pay for all the data you've seen. In machines I enforce a phase separation by using different types for what you can bind and what you can compose and say "don't do that" by making it non-expressable with the API. Reflection without remorse makes that price go away. Anyways, for me it is nice to have a third point in the design space. It is also nice that you can 'pay as you go' with a Tarjan and Mihaescu style deque. This doesn't matter for amortized use, but if you are streaming, there is something to be said for the reactivity of having a known O(1) (or O(log n)) worst-case bound to keep your latency under control even if your amortized bounds are O(1)! Will I use this approach for every free monad? No. But I'll definitely think really hard about whether I should.
That was a very helpful explanation, thanks :)
The TL;DR is that by doing this you can be parametric over the details where you don't care about them. A better example IMO is encryption keys, which is what I do in my encryption library. There are two types of keys for asymmetric cryptography: public and private. Furthermore, there are *different kinds of asymmetric keys*. I use different keys for Diffie-Hellman than I use for cryptographic signing, for instance. So the key types might look like: data Key t k = Key { unKey :: ByteString } data Public data Private data Signing data Exchange createSigningKeys :: IO (Key Public Signing, Key Private Signing) createDHKeys :: IO (Key Public Exchange, Key Private Exchange) keyLength :: Key t a -&gt; Int keyLength (Key bs) = length bs Now I only need one representation, and when I don't care about the details, I can be parametric over it. I can also choose *which* details to be parametric over: maybe I don't care if it's a signing key or a DH key, but only that it's public. This is easily doable by constraining the `t` type parameter, without having to rewrite any code. Furthemore this scheme still works for e.g. symmetric private keys. Of course you can also use a typeclass to abstract out the creation of keys as well, based on the context.
Yes. data MsgType = PlainText | Encrypted data Message (t :: MsgType) = Msg String That should work, IIRC. Very nice for encoding automata execution as an indexed monad. State will be an enumerated kind with a constructor for each node/state. Indexes are of kind State and show the initial and final state of that (part of) the execution.
&gt; delete is called automatically so trying to squirrel away a value is suicidal. And yet, new noogler / newB on my team will do it every time and be absolutely aghast that using the resource later causes a crash that robs me of my evening / weekend.
[And so much worse...](http://www.reddit.com/r/haskell/comments/2a5zhr/could_this_be_done_in_haskell/cirx702) ;)
And to be fair, his presentation of denotational semantics there is not very ~~challenging~~ advanced at all. Edit: wanted to indicate that I meant it was approachable by newish coders, not to be denigrating toward Meijer or his students.
What? STLC is strongly normalizing and non-polymorphic; clearly it couldn't have ever been GHC's [core language](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CoreSynType). It was a restricted form of System F-omega for a [long time](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/FC) (certainly necessary for Haskell 98 type classes), and was changed to System F-c with the introduction of GADTs, which added type-level equations/coercions. Certainly it's always been some variant of System F. Sure, small changes to a core calculus can have wide-reaching effects, but Haskell's variant of System F-omega was already a pretty advanced calculus, offering most of STLC at the type level. I think most of the recent changes have been based on learning to take full advantage of what System F-omega and F-c offer over plain System F. I'm not sure what future research will bring, but I think for tightly-constrained systems where being able to reason clearly about resource usage is of primary importance Haskell will always be at a disadvantage vs. languages based on strict versions of similar core calculi. This doesn't make Haskell *irrelevant* to those domains, as a lot of work on taming the powerful core system and making it usable could conceivably carry over, and it also provides a powerful tool for modeling new languages and type systems. But I think there will remain a class of problems for which Haskell is not the best-suited language for direct encoding of solutions.
Arguably yes, but then you can't allocate two pieces of memory and free them in the opposite order, unless you have some decent means of working with commutative indexed monad transformers!
Note that now with `DataKinds` you can write that as: data Access = Public | Private data KeyType = Signing | Exchange data Key (t :: Access) (k :: KeyType) = Key { unKey :: ByteString } ... It's more type-safe and stuff.
No interesting concrete data type can be held entirely abstract. The point of abstraction is not to hide *everything*, the point is to hide the "uninteresting" or "implementation specific" things. That is, abstract data types are always presented with an interface— and that interface is non-abstract. In this sense, Haskell's 'Integer' are a perfect example of abstract types. The interface says they behave like the integers familiar in mathematics: we have some notation to refer to specific values, we have addition, multiplication, etc. However, without digging into compiler-specific details, we know nothing about how `Integer` is actually implemented. Is is an algebraic type? Does it even have an endianness? If so, is it big-endian or little-endian? Is it stored as a flat hunk of memory or does it contain pointers to substructure? etc. We don't know, because all that is hidden behind the interface. While algebraic data types have a natural notion of case analysis, there's nothing about case analysis that's exclusive to algebraic types. If we can form predicates over the type (e.g., `(==0)`) then we can partition the type into those that match that predicate and those which don't; thus, conditional statements are, in fact, a form of case analysis on predicable types. For abstract types it's often helpful to provide "views": translations of the actual representation into a different representation upon which we perform case analysis. For example, we can view stacks by returning the top and the popped stack à la `(x:xs)` on lists— but our providing that view does not mean the underlying representation is actually a singly linked list. The the fingertrees of `Data.Sequence` offer both the cons-view and the snoc-view. This general notion of views are what is actually central to case analysis. The underlying view of algebraic data types is just an example; the partitioning of types via predicates (or functions more generally) is another.
Every language has its tools that will cause you extreme pain, they are unavoidable. `unsafeCoerce`/`unsafePerformIO` in Haskell, `std::unique_ptr.get()` and more than I can count in C++. At least by adding `get` you have something you can search for and flag. For instance your compiler may be able to say "you called `get` and didn't immediately call a C style API".
Like gridaphobe says, the "subtraction" is more like the relative complement of sets than it is like subtraction on numbers. While there are a few notions of negative types, they don't arise from refinement so far as I've seen. As for non-structuralizable refinements, I think you'll have to be more specific about what you want. Refinement types à la ∑-types are perfectly structural. It seems like you want to adjust the "data" side of the ∑-type so as to eliminate the need for the "proof" side, but this requires clarification about what precisely counts as "data" (and therefore acceptable to keep around) vs what counts as "proof" (and therefore must be eliminated). Broadly speaking, we can view refinements of `A` as the pair `(B,b)` where `B` is a type and `b : B -&gt; A` is a monomorphism. This is somewhat evil since there will be many such pairs which are isomorphic, so really we should think about [isomorphism classes of such monomorphisms](http://ncatlab.org/nlab/show/subobject). For the refinement type `{x:A | P x}` we can choose `B = ∑ A P` and `b = fst`. This suggests that as an alternative to defining what you mean by "structure", you could try looking for models of type theory which (a) don't have pullbacks, or which (b) don't have subobject classifiers.
Finally, true sums and products! :)
Yeah, Standard ML and OCaml have similar core calculi requirements; SML needs f-omega for encoding the module system, and OCaml has GADTs now, so I assume it's also based on f-c now as well, though I have not confirmed that. But the Haskell Report says non-strict semantics, so that puts a limit on how eager you can be unless you want to do speculative evaluation, which doesn't seem like the sort of thing you'd be able to do in an environment where you have to be able to reason about resource usage. It boils down to the fact that in a language that's not strongly normalizing, different evaluation strategies lead to *different denotations*, not just different runtime behavior. You can't just paper over that. Both have their advantages/disadvantages, as does using a strongly-normalizing language instead. I don't think we're going to be able to do without well-supported instances of all three kinds.
What is the category the encodings are initial and final exactly? I understand the objects are encodings, and the morphisms are convertions between them, but there is somthing I am missing
If you have newtype List a = List { view :: Maybe (a, List a) } then `List :: Maybe (a, List a) -&gt; List a` is initial in the category of `Maybe (a, _)`-algebras and `view :: List a -&gt; Maybe (a, List a)` is final in the category of `Maybe (a, _)`-coalgebras. An `F`-algebra for a functor `F` is a function `phi` and a type `A` where `phi :: F A -&gt; A`. An in the category of `F`-algebras between `(A, phi)` and `(B, psi)` is just an arrow of the underlying category `f` such that `f . phi = psi . fmap f`. In the category of `F`-algebras if `(X, embed)` is initial then there's a unique arrow to any other `F` algebra `(A, phi)`. So if `f :: X -&gt; A` is that arrow we have `f . embed = phi . fmap f` and if `embed` has a right inverse (call it `project`) then we can define `f` recursively from the definition of `phi` f . embed = phi . fmap f f . embed . project = phi . fmap f . project f . id = phi . fmap f . project f = phi . fmap f . project It'll turn out in Haskell (through a reasonably clever argument) that `embed` is not only right invertible but actually an isomorphism, so `f` is well defined. We usually call this `fold phi` fold :: (F a -&gt; a) -&gt; X -&gt; a So now if we examine `List a` again we have `F x = Maybe (a, x)` and I claimed that `List` was an initial algebra so we have a `fold` that looks like foldList :: (Maybe (a, x) -&gt; x) -&gt; List a -&gt; x foldList phi = phi . fmap (foldList phi) . view Furthermore, I claimed that `List a` and `[a]` are isomorphic so we ought to also have a `fold` like foldList :: (Maybe (a, x) -&gt; x) -&gt; [a] -&gt; x -- fiddling with the type... foldList :: ((a, x) -&gt; x) -&gt; x -&gt; [a] -&gt; x foldList :: (a -&gt; x -&gt; x) -&gt; x -&gt; [a] -&gt; x foldr :: (a -&gt; x -&gt; x) -&gt; x -&gt; [a] -&gt; x -- oh! foldr cons nil = foldList (maybe nil (uncurry cons)) Finally, given a value of `[a]` we can partially apply `foldList` v :: [a] flip foldList v :: (Maybe (a, x) -&gt; x) -&gt; x If we then pass the initial algebra `List` as the next argument we'll get an identity function which goes to show that `v` and `flip foldList v` are isomorphic. If you look carefully at `flip foldList v` you'll see I referenced that type above newtype List a = List { constr :: forall x . (Maybe (a, x) -&gt; x) -&gt; x } so `List a` and `[a]` are equivalent because `List a` is just `[a]`'s recursion principle. Finally, we already know that anything with a `Maybe (a, _)`-algebra `(B, phi)` can be uniquely built from a list using the recursion principle phi :: F B -&gt; B flip foldList v phi :: B but this isn't too terrifically weird because `phi` is basically instructions for building `B`s from the basic building blocks of lists: `nil` and left `cons`. --- Then dualize all that and you get the existential "final" list encoding I mentioned based on unfold principles. --- As a final note, if you stare at this stuff long enough especially concerning lists you ought to be able to convince yourself that `foldr` is *right* and that `head` and `tail` really are ugly.
different denotations is a bit of a stretch. only with effects! in a pure context, you just have a larger or smaller quantity of expressions which terminate.
Ah emacs.....back in the 1980s .... think I'll pass on that one :-)
Though, ironically, it is *a* monomorphism restriction... huh.
Although this is a reply, it's addressing a general audience rather than just the author of the message to which it is replying. A lot of papers are not terrible to understand, or at least have a section that gives an in-depth prose description of the core ideas before diving into proof trees or equations. Even if you feel intimidated by papers, you ought to give reading them a try. You may find that they're more approachable than you thought!
It's not a stretch at all, even when you don't consider side-effects. The differences have far-reaching effects on many aspects of the resulting languages! See this pair of blog posts by [Bob Harper](http://existentialtype.wordpress.com/2011/04/24/the-real-point-of-laziness/) and [Lennart Augustsson](http://augustss.blogspot.com/2011/05/more-points-for-lazy-evaluation-in.html) for more details. 
Of course it can. I'm using vim, using a bunch of :abbreviations for the unicode characters, as documented in Agda's [vim editing](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=Main.VIMEditing) wiki page. The command to typecheck a file named `Main.agda` is simply `agda Main.agda`. *edit*: there is also an [Agda subreddit](http://www.reddit.com/r/agda/).
As gelisam says, it can be, but I wouldn't recommend it. The emacs mode is massively helpful for doing anything substantial. Much more so than your average IDE.
Ah, I see. I'm also a vim user, so that would be far more convenient for me to use. I know that vim hasn't been made as useful for agda as emacs has - do you just end up not using certain features in that case? Or did you write your own keybindings?
Note this is very much a work in progress, and its all due to the amazing work of chris done. His blogpost explains what he's been up to, and what remains left to be done: http://chrisdone.com/posts/haskell-lang There's a github repo, and patches and pull requests and the like are I'm sure welcome to keep improving things. There's also folks in #haskell-infrastructure on freenode discussing deployment issues and the like, which are now being explored. So anyone that wants to get involved with the web side of haskell.org infra is welcome to stop by and help out!
see also this prior reddit discussion http://www.reddit.com/r/haskell/comments/26rilp/an_alternative_haskell_home_page/
It currently takes too long to load, especially the fonts. The layout kept resizing everything for a full two seconds after I loaded the page. I'd like to see some attention to improving page load time.
For the lazy, [here's a link to the GitHub repo](https://github.com/haskell-infra/hl). Fantastic work!
They *are* avoidable. But programmers, language designers included, really like these escape hatches, especially since the outside of the ivory tower isn't actually all that scary, forg ood reasions: We have "real work" to do. That problem is already solved by this (C/ASM) code, the proof is just external to the code. There an non-constructive existence proof that this is true and our term language only allows constructive proofs. Etc. `unsafePerformIO` isn't even in standard Haskell, though you can expose similarly dangerous things via the standard FFI. /u/edwardk and others have used unsafePerformIO in some pretty great ways; that said, I would not be opposed to a language that did not expose such escape hatches. First, they'd protect me from the rest of my team, but they'd also protect me from myself. I'm fairly disciplined when writing Haskell, but I've peppered my code with unsafePerformIO (before I knew about Debug.Trace), and been sorely tempted by "unchecked" calls to fromJust, fromRight, and head or (ugh!) throw/error. I really want a language this is useful, but makes it so *hard* to do something the wrong way, that doing it the right way (even if that means learning some alien abstract nonsense) ends up being easier. Writing in it will make be a better programmer and make the software that comes out higher quality.
The disadvantage is that now it's a closed set of options rather than an open one.
Design looks great! I like it a lot. Uh, when I started the interactive Haskell thing, I typed `help` and it prompted me to type in an expression. I typed in `5+7` and it responded with `can't find file: Imports.hs`
Funny you should mention that, I commented with *exactly* the code to do that on the post itself.
Yeah - I've seen this reported a few times. Right now it uses tryhaskell.org (another Chris Done project) - think it is just getting overloaded from all the connections. We've been talking in #haskell-infrastructure about what to do with this going forward, and there's a good chance we might end up running our own tryhaskell deployment for this purpose (so that if it goes down, we also have access to fix/maintain it, and so on). Nothing is set in stone yet though.
I haven't experienced that. What browser/os/etc are you seeing this on? I've only tried chrome but I've tried it on OSX and Windows.
I agree about the Features section.
Firefox/Windows/Laptop with good specs. It only occurs when nothing is in cache. If I reload with everything cached it is fine.
The design looks *fantastic*. I love those colors. Great stuff! :)
Is the sieve really the best example? Not everyone knows what that is, maybe a quicksort or something would be better.
Sean Seefried presents a talk on Functional Graphics (and animation) at the Melbourne Haskell Meetup. Slides: http://lambdalog.seanseefried.com/static/files/functional-graphics-melbourne-haskell.pdf More info in Video description.
Almost as bad: I typed x + 1 expecting an error. I got some `Expr` value. Hardly a standard Haskell feature and not one that a beginner needs to be exposed to.
Um, okay. If you're not going to engage any deeper than that, I guess there's nothing left to talk about.
If you hover over the thumbnails you can see the video's title.
Gorgeous! This definitely gives Haskell a more professional look, and I especially like how it directs interested coders to try out Haskell online.
Lol, I love this line: &gt; Haskell code has a reputation: if it compiles, it usually just works.
It feels like this problem should be solved without the use of an entirely different operating system. 
I would prefer the following prime number finder if you want to do trial division: primes = 2 : [x | x &lt;− [3..], isprime x] isprime x = all (\p −&gt; x ‘mod‘ p &gt; 0) (factorsToTry x) where factorsToTry x = takeWhile (\p −&gt; p*p &lt;= x) primes It is both easy-to-read and more efficient. (also, it is listed in the above paper)
One nitpick: the try-in-browser command line is visible above the fold but extends to below it. This isn't ideal as it causes an obnoxious scroll once you've entered ~5 lines of code. 
The "Introduction to Functional Programming using Haskell" link sends me to "Aqueous Environmental Geochemistry, 1/e". If you change "allbooks" to "books" in the URL it works, but the result is kind of useless. If I search their catalog, I do get this link: http://www.pearsonhighered.com/educator/product/Introduction-Functional-Programming/9780134843469.page
nixpkgs can be used on any unix based operating system. i.e. you don't need to go full nixos to use nix
Websites seem to love looking awful at my preferred screen widths, which are wider than a portrait phone but narrower than a landscape phone. In this screenshot is my portrait mode Nexus 7. Look how stupid the top section is. http://m.imgur.com/KAd0ZUN
Great article.
Looks extremely good. Keep up the good work!
Looks nice. I think the download button should be a bit more prominent. If you look at Scala, Ruby, and Rust's homepages it is. Also, the color difference between the top navbar and the section just below it is not big enough. Either make it bigger or make the two the same color.
Quite an improvement! A few comments: - I miss the big, obvious 'Download' button linking to the HP - The documentation section should also link Hoogle/Hayoo, School of Haskell - How about linking to FP Complete's IDE? Showing commercial support for Haskell goes a long way, and being able to launch a nice IDE with a skeleton project one can start editing right away is very neat - I would add links the various plugins for configuring vim/emacs/sublime/etc. to work with Haskell code
I knew someone would bash the "bad" quicksort, but it shows off haskell's elegant syntax while remaining dead simple to understand to those with no previous exposure to fp.
Which might be an advantage. Depends on what you need. If you write a *closed* library, I don't see a problem.
Nice. Just submitted a pull request for a small typo I found.
Fix your damn contrast. Especially in that code snippet.
maybe i misunderstood, but i did not find any mentioning of kinds in your article.
No, I have not called this function anywhere. I don't have a rendering context explicitly at any point either. As I said, I simply took the (working) example code from the wiki and worked from there. I will look into it however. I managed to get a (more or less) minimal non-working example though and I've posted it [here](http://lpaste.net/107118). For reference, [this](http://www.haskell.org/haskellwiki/Gtk2Hs/Demos/GtkGLext/hello.hs) is the code this is based on. I've altered it such that the drawing area isn't created when starting the program but later on, when the button is pressed. The same problem occurs, as there's nothing drawn into the drawing area.
Would prefer "Compiler and base libraries" were named as "Haskell Platform". A lot of work went into branding "Haskell Platform" as the first thing you should look to download - here its a little lost in with the Hackage and Stackage stuff. It should be really really obvious what you want to download first.
Thanks, I'll surely attend some of the meetups once I am in Chile.
That is quite interesting! So in the end do you just use vim for the unicode input and do the typecheck manually? Or are there a few more features that you didn't mention? (I'm asking all this because depending on your answer I might try building some of those tools for vim)
[This](https://dl.dropboxusercontent.com/u/30225560/poc-haskell.png) is my attempt I've never finished, but I like the direction. https://dl.dropboxusercontent.com/u/30225560/poc-haskell.png 
Well, then let's call it something besides 'quicksort'. For example, 'partition sort', which is a more generic term. http://stackoverflow.com/questions/1560667/what-is-the-difference-between-partition-sort-and-quick-sort
This post is (sadly) missing a working example demonstrating the concept.
If you're a vim user, I recommend emacs with evil. It works nicely together with the agda input mode, and unless you rely on a lot of plugins in your vim workflow, you won't even notice it's a different editor.
I find nix very interesting and it's been quite useful already to me for Haskell development. I do have some issues with it: 1 - It doesn't really solve cabal hell. If your project depends on packages not on nix-pkgs then you have to manually add the package you need. But then that one might depend on another one also not on nix-pkgs and the process just goes on and on, basically doing manually what cabal does automatically. There is hack-nix which is supposed to do exactly this job, but the documentation seemed very lacking to me and I couldn't figure out how to use it. 2 - cabal build seems not to work with the wrapped ghc installed by nix, often it doesn't find the haskell libraries installed by nix. Using ghc directly will work though (runhaskell Setup.hs configure --ghc; runhaskell Setup.hs build). 3 - documentation on workflows is still a bit lacking. For instance to include Haskell packages for development you don't install them, but you setup a development environment that depends on them, and then either use nix-build or nix-shell. 4 - Using editors and ide's requires more steps then usual, for instance because of the issue mentioned in 3). Also, ide's tend to use cabal to build things and cabal doesn't work with nix for building... In any case, it's a very nice system and I'm sure with some polishing it will become great for Haskell development.
I would appreciate it even more if (old and new) haskell.org would not include Google analytics.
I can't read any of the punctuation/symbols against that purple background. Not good where it's intended to demonstrate the syntax.
Wasn't Chris on board with the [theme unification](http://www.haskell.org/haskellwiki/Brand) a couple of months ago? Some got to reflect that already (hackage, hoogle). Why try to deviate again at this point?
That wouldn't be accurate though, as the Linux and OSX instructions seem to just install `ghc` + `cabal`. It's only the Windows platform where you can't seem to avoid installing the full haskell platform to get a working Haskell devel environment.
Thanks a lot, this was really helpful!
You are missing the point.
`-Wall` will try and it'd definitely be good to see that error and respond to it. In general pattern matching exhaustiveness is undecidable thanks to guards, though.
Oh it's not my article, I just commented on the post itself, check the Disques comments on the bottom :)
I agree with you about the Features section. "No more null errors" is a blatant lie for anyone who uses head or tail in Prelude. Additionally, as far as I know, the Haskell compiler never actually "write[s] them [type signatures] for you" in your source code (and in fact, the types which Haskell infers may be invalid in your source code without language extensions such as FlexibleContexts). I think that "Features" should focus on actual Haskell features, rather than subjective value judgements. For example: Algebraic data types Haskell types are declared as compositions of types. For example, with the definition data Maybe a = Just a | Nothing , `Maybe` transforms any type into one which is possibly null. Type inference Haskell automatically infers the types of unannotated expressions. For example, the type of f x xs = filter (&lt; x) xs is inferred as f :: Ord a =&gt; a -&gt; [a] -&gt; [a] Let them draw their own conclusions. Why tell when you can show! (For other suggestions for features, I might suggest "Typed Effects", "First-class functions and effects", and "Referential transparency") Additionally, note that there is a LOT of requisite knowledge of Haskell syntax in order to understand the `primes` example. Especially if you don't know that `:` is list `cons`, the example will be entirely inscrutable. The list comprehension is a gratuitous use of advanced syntax - why not replace it with `filter`, which would be more idiomatic?
That looks quite nice, but what's the background?
After spending a lot of time with it, I don't think it should be a headliner one-liner, but if there were a selection of them (a carousel?) then I think it would have a place.
To be clear, most of what I (and others I've observed) use the emacs mode for isn't writing code but looking at types of holes, contexts around them, and typechecking expressions in those hard-to-reach contexts. Having it write my pattern matches for me is nice but definitely not the main reason I use it. I say that as someone who doesn't like emacs and only uses it for Agda.
Well, my first solution via `ID a` is something like that, I guess.
See my sibling comment :) I'm on my phone right now, but google Derek Elkins' GitHub account and you should find some preliminary Agda-mode bindings for vim.
Frustrating. I find this happens a lot with Bootstrap's default CSS, which breaks into 'vertical phone' mode at some fixed width. It's probably a good idea to tweak that number based on the page content, or do something fancier.
Agda is technically, but not practically usable without emacs. Idris, OTOH, comes with vim and emacs mode that are equal citizens.
Why? Reality is that it's very hard to get websites to remove things like analytics (they're trying to understand how their site is doing) and so it's up to individuals to block that stuff themselves.
That's a great blog post, but in my opinion a terrible initial example. Pick something that people can relate to. How many developers have ever tried to enumerate the rationals?
The features section makes me gag. :) I'd never get past the first page if I was subjected to such marketing speech. Surely Haskell can be sold with less excess? Or is that just me being European? Otherwise, really great look!
I very much doubt this has anything to do with Haskell. Chrome on linux recently switched from GTK to Aura for displaying buttons, context menus, etc. My guess you had updated Chrome but not restarted it. Rebooting just revealed the update.
I actually completely agree. I was originally excited about its beauty, but ignored its practical appeal. I don't think it would be a good first look.
I'm not sure that `apt-get install haskell-platform` gives a better experience than `apt-get install cabal-install ghc`. Now, if your distribution doesn't package GHC and cabal, then the platform is probably easier.
Other examples that might be better: * [Factorial](http://www.willamette.edu/~fruehr/haskell/evolution.html) * Fibonacci (I think the fixed-point form best) * Palindromes * [99 Bottles of Pop](http://www.99-bottles-of-beer.net/language-haskell-1070.html)
Well, it works until you try to store it to disk (I believe). If you add a createCheckpoint (what uses the safecopy instances to store it to disc) you get the undefined error and the program ends because of an indefinite mvar operation. Without using the checkpoints you're kind of just using it as an MVar.
Things will continue to work -- but not being able to create checkpoints does seem like a bit of an issue. It will have the following side effects: 1. you will always have to replay every event that has ever occurred every time you restart 2. you can never modify `apply`, `sample`, or `sample'` because that would change the results of replaying the events 3. you would potentially not be able to use a future version of acid-state that supported replication Because there is no compile-time way to prevent you from modifying `apply`, `sample` or `sample'`, you would probably get very mysterious data corruption at some point? Personally, I would not recommend it. Everybody wants to store functions in acid-state at some point -- but I'm not sure it is ever the right answer. If it was going to be done, something similar to the approach in *m*Haskell might be the solution, http://homepages.inf.ed.ac.uk/stg/workshops/TFP/book/DuBois/duboismhaskell/cameraready.pdf By using byte-code they were able to capture the entire closure and send it across the network. Storing it on disk seems like it should be similar? 
IMO, I really wouldn't want to manage a website as popular as haskell.org without Google Analytics. I'd love to see a cost/benefit analysis that provided sufficient evidence that removing GA was in the web site owner's best interest, but I don't think one of those actually exists.
It would throw an error if `createCheckpoint` is called. But it is more than just an `MVar`. The `Apply` event does actually get logged to disk and can be replayed. 
https://en.wikipedia.org/wiki/Google_Analytics#Privacy_issues
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 6. [**Privacy issues**](https://en.wikipedia.org/wiki/Google_Analytics#Privacy_issues) of article [**Google Analytics**](https://en.wikipedia.org/wiki/Google%20Analytics): [](#sfw) --- &gt; &gt;Due to its ubiquity, Google Analytics raises some privacy concerns. Whenever someone visits a website that uses Google Analytics, if JavaScript is enabled in the browser then Google tracks that visit via the user's [IP address](https://en.wikipedia.org/wiki/IP_address) in order to determine the user's approximate geographic location. (To meet German legal requirements, Google Analytics can anonymize the IP address. ) &gt;Google has also released a browser plugin that turns off data about a page visit being sent to Google. Since this plug-in is produced and distributed by Google itself, it has met much discussion and criticism. Furthermore, the realisation of Google scripts tracking user behaviours has spawned the production of multiple, often open-source, browser plug-ins to reject tracking cookies. These plug-ins offer the user a choice, whether to allow Google Analytics (for example) to track his/her activities. However, partially because of new European privacy laws, most modern browsers allow users to reject tracking cookies, though [Flash cookies](https://en.wikipedia.org/wiki/Flash_cookies) can be a separate problem again. &gt;It has been anecdotally reported that behind proxy servers and multiple firewalls that errors can occur changing time stamps and registering invalid searches. &gt; --- ^Interesting: [^Web ^analytics](https://en.wikipedia.org/wiki/Web_analytics) ^| [^Google ^Website ^Optimizer](https://en.wikipedia.org/wiki/Google_Website_Optimizer) ^| [^Google](https://en.wikipedia.org/wiki/Google) ^| [^Urchin ^\(software)](https://en.wikipedia.org/wiki/Urchin_\(software\)) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ciskz2z) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ciskz2z)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Distro-native packages are good. Distro-native Haskell Platform is better -- as it resolves a guaranteed set of C and Haskell dependencies for you, so you can target tutorials against that set. Beginners shouldn't need to touch cabal for a few days at least.
Yeah, I'm pretty sure -Wall is conservative and will emit a warning if it can't easily prove coverage. So, it can cause spurious warnings, but it will not let something past that is suspicious.
Done as a hylomorphism (using `hylo` from recursion scshemes) `hylo crush grow (1, 1)` Good breakdown. Not absolutely sure that hylo does fusion, but I believe it should. 
I avoid guards for this reason, unless I can refactor them to end with `otherwise`. Even that is undesirable though since `otherwise` does not explicitly enumerate the conditions it catches.
We're talking about newbies here. If it their your first time using haskell, regardless of your platform, you want them to use the platform. Sure they *could* do it manually, but if there's one screwup somewhere they are gone and not coming back.
Look, the web-based repl is cool and I know why the example doesn't work in the repl, but for new users why make your front page example one that won't work in the repl? Try it Type Haskell expressions in here. λ primes = sieve [2..] where sieve (p:xs) = p : sieve [x | x &lt;- xs, x `mod` p /= 0] &lt;hint&gt;:1:8: parse error on input `=' λ 
&gt; Beginners shouldn't need to touch cabal for a few days at least. So they wouldn't even write a `hello.cabal` file to provide a build-system? :-/
`hylo` *is* fusion—at least the fusion I used here. In particular, we have ana g = embed . fmap (ana g) . g cata f = f . fmap (cata f) . project then hylo f g = cata f . ana g = f . fmap (cata f) . project . embed . fmap (ana g) . g but since `(project, embed)` is an isomorphism = f . fmap (cata f) . fmap (ana g) . g and then we use the functor law and fold in the definition of `hylo` = f . fmap (cata f . ana g) . g = f . fmap (hylo f g) . g The `recursion-schemes` version defines an explicit let-fixed point but it's the same idea hylo f g = h where h = f . fmap h . g
You're right about all that text needing to be fixed up and point to the platform. That's definitely on the list of things to do to the site before we launch it.
bit of a nix noob, but running nix-env -qaP '*' | grep ghc763 | wc -l, shows 2731 entries, and the above command with ghc782 only shows 1. If I wanted to use nix to resolve cabal hell, but still use 7.8.2 how would one go about doing that?
augustss: if you have any suggestions, not even finished text but just partial ideas, please send them on to me and i'll do my best to polish/use them. striking the right balance between accuracy and generating excitement for an audience who doesn't know much is _hard_. but i also agree that the sorts of claims made will make more seasoned devs dubious about a "flavor of the month" instead of providing a real case to them :-)
If the function can get memoized, you should be able to store the memoized function: a pair for a `Bool -&gt; a`, and things like that. If the type is recursive, however, I'm not sure how that would be possible.
&gt; fromJust, fromRight, and head Ah yes partial implementations (one of which somehow made it into Prelude). &gt; I would not be opposed to a language that did not expose such escape hatches. Coding standards could get you there. No need to change the underlying language when you can just restrict yourself to a subset.
&gt; Coding standards could get you there. Maybe. Coding standards were also supposed to give us safe C code; I'm still waiting. Changing the underlying language seems to make progress faster, IMO.
Oops, my mistake, I was thinking of Garage Band. Never mind then.
To be fair we actually have analytic tools to enforce said coding standards now. Historically you had to rely on trust or code reviews. (Both of which sometimes let stuff through)