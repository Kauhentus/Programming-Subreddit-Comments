Integration with Cabal so that the editor does not fail when it sees its CPP pragmas.
Vim bindings will stop sucking once [neovim](https://github.com/neovim/neovim) gets a bit farther and creates an easily embeddable vim... I can't wait for this to happen.
We don't want this. I'd rather TypeHoles taken to their completion - i.e, a type-aware completions system that allow me to fill in a type hole with a single value or a spine of a function call with holes for the arguments.
A programmable breakpoint similar to `Debug.Trace.trace`: trace :: String -&gt; a -&gt; a breakpoint :: Bool -&gt; a -&gt; a [Apparently](https://ghc.haskell.org/trac/ghc/ticket/8429) this used to exist?
Windows support is essential if you want someone to take you seriously.
Refactorings like rename, move etc. Or provide a good plugin interface so that plugins might fill that niche. Fuzzy completion, like in Visual Studio. Actually most of the C# editing feautures.
This and auto-gen signatures. It would also be nice to be able to search and pull stuff from Cabal into my sandbox. 
The vim bindings in yi aren't too bad.
Can't say why this downvoted. Perhaps the formulation should not include Windows but non-unix instead..
What about the recent situation with `mtl`/`transformers`, where any version will do except for one that happened to be broken? Will you percolate that "upper bound" back, too?
I think using a browser as the ui solves this today, like what github did.
The problem with this approach is the slowdown in performance we witness when loading heavy projects.
"Big data" integration so that compilation errors and in general pain points are aggregated across all developers globally so the community gets a good view of the pain points. This should be integrated with stackoverflow and some information retrieval library to make solving single problems simple.
the two good non-vim vim bindings are jvi (which was a straight port of the vim code to java, and really good when using netbeans), and emacs's evil mode, which has come a very long way.
The evaluation boxing that IPython notebook and MATLAB have is pretty cool. There was a thread on /r/programming about syntax highlighting by type... that would be pretty neat, too. Here it is... http://www.reddit.com/r/programming/comments/25p6v3/_/
I've never heard of this. Awesome! 
&gt; syntax highlighting by type That might give weird results when using type heavy libraries like lens or parsec. I think Haskell would benefit more from context highlighting. This lets you spot captured variables in closures and might make let-expressions more popular again.
The plugin interface is definitely key. Some amount of intelligent boilerplate generation might be good, as well (e.g., makeLenses -&gt; explicit lenses would be really cool).
Which hasn't stopped people from using Eclipse. 
Customize it using Haskell, rather than vimscript, elisp, Lua, JavaScript, etc.
Why, yes I rather think I am. Thanks.
That doesn't fly for things based on some native libraries like OpenGL.
&gt; Even better if it only suggests values that typecheck. Certainly an interesting suggestion, but seems hard in the face of incomplete expressions.
If by this you mean move a function from one module to another and adjust the imports and calls accordingly, that sounds great.
A great IDE for Haskell is something I would LOVE to contribute to. I don't feel experienced enough in Haskell to carry out a project like that all by myself, but if anyone is serious about building one, please PM me!
Have you tried Evil mode in Emacs?
Are you on OS X Mavericks? I actually just had a similar problem (though not exactly the same), that was fixed by doing this: http://justtesting.org/post/64947952690/the-glasgow-haskell-compiler-ghc-on-os-x-10-9 Worth a shot (takes 5 minutes, and is easy to revert). Download the wrapper, put it in /usr/local/bin, chmod a+x to make it executable, and change the GHC settings.
Hugh may be interested to know that `brew install cairo` will work.
While I don't recognize the "= does not exist" bit, I do recognize most of that stuff as commonplace difficulties for building a gtk package.
&gt; but, as an industrial practitioner that doesn't seem to me as if it can be right. Do you mean you think people in industry don't use emacs? Because that's very incorrect. 
I'm pretty sure I tried that and it failed, but as I said it was between about 11:30pm and 3am several days ago, so I don't remember.
Yeah but codemirror based editors are even slower than eclipse.
I want "Delphi" for Haskell, complete with the equivalent of the VCL so that I can just drop widgets on to a window and associate them with the appropriate event triggers, properties. That's how you'll get Haskell to the masses!
You may need to `brew install cairo --universal`. I can't remember, now.
tab completion: keep pressing tab until program is written. realistically, programmer writes type signature. and keep presses tab until correct algorithm is written. due to h-c isomorphism, this could be practical enough. 
I downvoted it because plenty of productivity happens on non-Windows environments. It isn't necessary to release something on Windows for it to be taken seriously. That's a quaint notion in the computer industry, an industry in which Windows is becoming less important every day. Windows may never go away completely, but that doesn't mean you always have to develop things for Windows for them "to be taken seriously." Programming on Linux is such a relief for me (personally) compared to developing on Windows or even Mac (which is Unix-like).
Oh god this is hard to read. Though from what I understood from your comment at the end powSame should just be `powSame n = n == (n^65)` \``mod`\` `1079` (you can do it in fancier ways if you want to, but I think those are unnecessarily complicated). The whole thing would just be something like this: main = print $ take 2 $ filter powSame [1..n] where powSame n = n == (n^65) `mod` 1079 n = --number to where you want to test I quickly ran it in `ghci` (so single core, single thread afaik; I removed the `take 2`), found a few numbers almost instantly (1, 83, 248, 333, 415, 416, 499, 580, 663, 664, 746, 831, 996, 1078) and now it's desperately trying to find bigger numbers (can't find any in a pretty long range), going through them in what I consider to be an acceptable speed.
Sweet! Thanks for the fast reply! And if you want to show me the unnecessarily complicated ways you can. If it is what a person might see in the real world, then yes. My only trepidation is that you are actually modding by 1079, not actually the whole thing that starts with 1079 (though the 65 is just 65, so no biggie). I'll try that. And lastly if you want to make a copy and format it how "real" haskell is written (you can leave comments out. I know the importance and this was going to be a quick afternoon project so I just didn't add them) I'd gladly take a look. I'm not sure what the conventions are yet, and am coming from a very C-family background. EDIT: Yeah I am guessing the 1079... looked like that was my modulo, but I meant the huge number that starts with those digits. Running it with the original huge number is not giving me very quick results.
&gt; if you want to show me the unnecessarily complicated ways you can. `powSame = (==) &lt;$&gt; id &lt;*&gt; (^65) . flip mod 1079…` This obviously goes with the numbers hardcoded. Oh, and don't do it, please. It might have it's uses at some places, but it's definitely completely unnecessary here. &gt; My only trepidation is that you are actually modding by 1079, not actually the whole thing that starts with 1079 Oh, didn't see the ellipsis &gt; And lastly if you want to make a copy and format it how "real" haskell is written […] I'd gladly take a look. I only recently (~1 month) got into Haskell myself, so I can't say that much about it. What I wrote in my initial comment is how I would write it in an actual program though. &gt; Also, since I was aiming at making b e and m all variables in powSame, would a powSame written as `powSame b e m = b == (b^e)` \``mod`\` `m` be acceptable? Yes, looks perfectly fine.
Now that's quite a feat! 
Also, mostly rhetorical, but how do you know that complicated way if you've only been into Haskell 1 month? I don't plan on using it because I have no idea how it works, but you've got some magic going on there that you seem to have picked up very quickly.
No I haven't, I should check it out though, I've got a lot of respect for Emacs even though I've only ever really been a vim guy. My real problem with vim modes in other editors is that they get (most) of the keybindings okay, it's just the behaviour is subtly wrong in such a way to really break my flow.
Parsers, for me. I tried building a little functional language that compiled down to javascript. It hardly did much, but the parsing bit gave me some exposure to some of the things monads, applicatives, etc. are best at, IMO.
Automatic type signature insertion and updating, as I write the function body (at stable points). Especially if I'm writing for a function that already is called somewhere, and the program doesn't pass the typecheker yet. I'm used in doing: finalFunction = someOtherFunction x y z -- initially someOtherFunction = undefined -- at this point I want type insertion (which would mean "partial compilation") --- starting to hack on it here while not yet adhering to the call type someOtherFunction = ... I usually comment `finalFunction` here to load the program in GHCi, but it gets hairy when `someOtherFunction` dependes on `finalFunction`. But I'd feel good if it work for the base case only, since most of the time each function is isolated. **Second feature** would be package suggestions (and imports management) based on the packages in my sandbox. If I type `map` I want it to suggest it from the modules (ex) `Prelude`, `Data.List`, `Data.Map`. Another feature, selecting blocks of code and generating a module where those should be moved (and minimal imports inserted in current file, as required). Also the option to select a block of code and "push" into the module. Integrated Hoogle search (default within sandbox) HLint integration and automatic issue resolution. Jump to definition (basically hasktags integration), but download the source code as well for packages inside the sandbox. Option to check for package updates and option to build with packages a couple of different package versions that match the ranges defined in dependencies. Maybe also have support to test under different ghc versions. For most functions, if their function body is short, upon mouse over where they are used alongside the type information (that should be a bubble text) the function body should be listed as well. And necessarily vim keybindings, which hopefully (as someone else said) neovim should make that easier to integrate.
&gt; due to h-c isomorphism, this could be practical enough. Proof Search is hard.
Learn You A Haskell (see sidebar) covers that way in the part about applicative functors
From a first glance looks clean and nice. It's a rather large program for me to assess, but I'd think there could be improvements in the code generator. Maybe things like removing `error` calls and handling failure via Either, to compacting `boilerplate` to `mapM line [list_of_strings_that_dont_span_so_many_lines]`. While the second is dirt cheap to do (and overall benefit low), the first would be harder to do but would help in the implementation of better error reporting. Also, consider generating a .cabal file, so it others interested would have an easy time in building it.
Not to be that guy, but Mac is unix and linux unix-like.
You may be interested in an IDE called [Leksah](http://leksah.org/)
In addition to all the great suggestions here I'd love an architecture like [fsharpbinding's](https://github.com/fsharp/fsharpbinding) or [OmniSharpServer](https://github.com/nosami/OmniSharpServer) which allows many editors and IDE's to use the cool features described and reduces the need to reinvent the wheel.
No, ``(b^e) `mod` m`` won't do the necessary optimization. I suspect your stack overflow has nothing to do with `powm`. Without actually testing this, I have a hunch that GHC might not optimize properly a list comprehension that throws away nearly all of an enormous number of values. Also, you won't get any parallelization automatically, either, you need to use a proper library for annotating how much to parallelize.
No, I got you, and I'm with you all the way. Mac puts all kinds of very un unix things on top of their unix, and sane linux distros don't. You're keen, mate, it's just me been drinking all day -- Atletico v. Barca was 11AM here.
We would love your help on yi!
Would you be able to take a look at the code and help me get there? If there is a way to do this without lists (I don't care about misses. I just need one number that passes the test) that might be faster. And I always thought Haskell, being functional and immutable, was parallelized for you. Can you help me get those annotations in to parallelize it (each thread checks a different number in the gigantic list) or point me to a resource that shows me how? Or is it something I can simply google? What are some reputable sites haskell specific that would have good documentation?
I've used Qt for several commercial products but the designer is not live like Delphi. I don't know about hsqml (I'll check) but i bet it's not a visual solution. Visual Basic, Delphi, and even Cocoa with Interface Builder make it essentially seamless to move between GUI and code development and Haskell needs something like that badly.
Smart, type-aware auto-completion. Also, auto-complete as I type (no need to hit Ctrl+Space). And this should work even if the file doesn't compile currently. (Basic support seems relatively easy to me: if you have 'f a b |' then autocomplete should suggest only values having the same type as the 3rd argument of f). In the same situation as above, show a popup with parameter types and function documentation. Like this: http://www.jetbrains.com/idea/features/screenshots/60/parameter_info.gif Auto-generate missing type signatures. Both for top-level and local definitions. On-hover type information + haddock (extract docs from source code automatically, if necessary). Suggest to import definitions from another module. (And install a new package if necessary.) http://www.jetbrains.com/idea/features/screenshots/60/import_assistant_optimizer.gif Ctrl+Click to go to definition, F1 to go to haddock docs, Ctrl+F1 (or something) to hoogle. Find usages. Rename (both for top-level and local definitions) Extract/inline variable/function (both for top-level and local definitions) Debugging Apply Hlint suggestions. Customizable code formatting. Integration with HUnit, QuickCheck. View generated Haskell Core. 
Keyboard shortcuts solve this. But if you're totally mouse adverse why not just use a text editor? 
Overall very nice. Some stuff I noticed: * In Lexer.hs, you can just match using a record wildcard on the generated lexers rather than manually unpack each one into an identically named binding. * In SSA.hs, you could make statement parameterised by the type of the ID, and then derive a Functor instance automatically and you get your mapIDs for free. Not only that, but you know from the types that mapIDs is correct. Then, your SSARegisters can use the same statement type but with Register as the type parameter rather than ID. * I would suggest sharing BinaryOperator and other similar basic types rather than duplicating their definitions (for example in AST.hs and SSA.hs) * I recommend rolling a little monad transformer stack for the Typechecker, containing at least a reader for the environments and an error monad for type errors. Don't use `error` for those things.
I'd like to be able to develop like this: http://youtu.be/L6iUm_Cqx2s?t=5m1s (the Aurora demo at StrangeLoop 2013) For most of the other suggestions in these comments, you could start with Visual Studio or Eclipse, and just fill-in-the-blanks for new language support and pretty much be done. Haskell would be just about as difficult to use as before, though.
Auto-parallelism is unfortunately not a thing. You might check this page out to get you on the right track: http://www.haskell.org/haskellwiki/Parallelism par/pseq are probably what you want, since you just want pure parallelism. 
We need closed type classes.
[HaRe](https://hackage.haskell.org/package/HaRe) can do some of the refactorings. It comes with emacs bindings out of the box. AFAIK there even are vim bindings. I never used it myself so i can't tell you how well it works in practice. But it might be a pleasant addition to emacs haskell-mode and ghci-mod.
Very good article! I recently encountered this problem myself, and I couldn't figure out how to make Parsec give me better error messages. In hindsight, it should have been obvious, but it wasn't. I'll keep this in mind for future parsers, for sure.
Woah nice job. I think that makes sense. As soon as you mentioned it was cylical it all rushed back and I remember doing all this stuff now. Thanks for the great explanation on it! Thats why it couldn't figure it out. It had to get all the way to the factors before finding one!
This is just a backtracking semantics, with a breadth-first implementation instead of depth-first. Of course backtracking is better-behaved semantically (the alternative commutes!); I'm not sure why people are excited about PEG's asymmetry as being a feature.
I think the [Utrecht parsing library](http://hackage.haskell.org/package/uu-parsinglib) deserves a mention here. It is a good point that a parser API should make it easy to write parsers that both work correctly and produce meaningful error messages. But that is a non-trivial research topic. Utrecht has gone quite far down that research road, and it's a shame that so far their work seems to have been overlooked for all except their own parser library.
I'm working on a Haskell GUI library called [Threepenny-gui][1] that uses the web browser as a display. At the moment, the GUI is built from source code only, but thanks to FRP, it's actually quite pleasant. In principle, it should be possible to build an interface like Delphi on top of this. However, I'm not knowledgeable enough about HTML to turn any web page into an editable interface. Also, Delphi has the habit of writing into your source code files, I'm not entirely sure how to separate IDE-produced code from your code in Haskell. [1]: http://hackage.haskell.org/package/threepenny-gui
&gt; This is just a backtracking semantics, with a breadth-first implementation instead of depth-first. Yes exactly. I think it's key to an implementation with good memory behavior though. &gt; I'm not sure why people are excited about PEG's asymmetry as being a feature. Semantically PEGs have one big attraction: they are deterministic. Each input string will be parsed in at most 1 way. Unlike natural language, we want programming languages syntax to be deterministic. We don't want an expression to parse in 2 different ways. So the semantics of PEGs are better aligned with what we want. It seems ugly to introduce non-determinism everywhere in our semantics, just to get some lookahead...
You just need to add `-optP-include -optPdist/build/autogen/cabal_macros.h` to you GHC settings. It would be easy to set that as the default if your inside a cabal project. 
The problem is that many users don't think at the operational level required to properly reason about asymmetric choice (and I think that's a problem with the model, which is too operational, rather than with the users). It's excellent that the PEG allow you to express deterministic languages with a convenient grammar, but that only works well when you have first done the work of coming up with a deterministic presentation (as we do with LR-style parsers when we massage the grammar to eliminate shift-reduce conflicts). What happens in practice is rather that people think of the choice as if it had a backtracking semantics, and only change their mind (locally) if that actually refuses valid input they want to write. The result is deterministic, but because of a mismatch between the intent and the behavior, instead of by proper design.
In the parser combinator library I made about 5 years before Haskell existed the backtracking alternative combinator (there was no `try`, just different alternative combinators) the parsing state kept track of the furthest location in the token stream it had seen. When reporting an error, this was the location used in the message. It worked quite nicely. The way Parsec does it is broken. 
I agree that PEGs are very operational, but I don't fully agree that this is a bad thing. If I understand your argument correctly it's that nondeterministic is the 'natural semantics', and only in the subset of grammars where PEG parsing happens to give the same result as full nondeterministic parsing they work well. In those cases where a PEG grammar would parse fewer strings than a nondeterministic grammar, that means there is an error in the PEG grammar. I've personally also had the opposite experience where I wrote a grammar, but it parsed *too much* because of nondeterminism because I was thinking with PEG mindset about a nondeterministic grammar. So I don't think that one is necessarily easier than the other for humans, what matters is that the semantics in your mind actually correspond to the semantics of the parsing library you're using ;-). Nondeterministic grammars have a simpler declarative semantics: the set of strings parsed by `a | b` is the union of the set parsed by `a` and the set parsed by `b`. PEGs have a simpler operational semantics: in `a | b`, the choice `b` will be tried precisely when `a` fails. In nondeterministic grammars, `a` may succeed, but then something else my fail down the line, which causes `b` to be tried. This makes stepping through nondeterministic grammars in your head very hard. PEGs on the other hand follow a stack discipline, and can be stepped through much like you can step through procedure calls in your head. To evaluate whether PEGs or nondeterministic grammars are more suitable, we'd have to look at examples parsed with PEG grammars and parsed with nondeterministic grammars, and see which one is simpler/easier to understand.
Thanks Simon! A few of us are also going to ZuriHac. :]
Yes, I've seen that GUI, it's a nice idea, but personally I'm not a fan of using a webbrowser for native applications. But it's not just about rendering....there's also the need to be able to design interfaces graphically as done by Delphi, VB, and (to some extent) Qt. Yes, Delphi has the habit of writing into source code files. So what? The benefit is that it actually lets you see what it did. I prefer that to the Xcode module where the designer produces a separate XML file with the state to be loaded. One doesn't learn anything about how to reproduce that behavior programmatically.
I'm missing just one thing from haskell-mode, real autocompletion just like python gets from jedi. 
PEGs' determinism can be problem when it hides grammar bugs. A well-written grammar should be deterministic in a non-deterministic interpretation, as that makes it easiest to understand when reading; that is, despite all branches being taken, all but one branch must find disparities in all cases. PEGs' asymmetric choice forces you to consider the context in which a production is used to understand what it means.
Shameless plug: Utrecht has a great undergraduate course on parser-combinators :) http://www.cs.uu.nl/wiki/TC/CourseMaterials I'm going to follow it next year. Edit: We apparently also maintain two haskell compilers: http://www.cs.uu.nl/wiki/UHC/WebHome (which has multiple backends like JS, which is pretty cool) and http://www.cs.uu.nl/wiki/Helium/WebHome (which is a subset of haskell with clearer error messaging). They're both built with uu-parsinglib and an attribute grammar system. Oh and I just discovered this today: http://www.cs.uu.nl/wiki/Center/CompilerConstructionInHaskell TIL: my uni does a lot more haskell than I thought
Shameless off-topic. I did this course, and failed.... keep up on the assignments! But it's pretty interesting!
At some point, I'd like to get rid of the web browser and use a native shell, namely XUL (on which Firefox and Thunderbird are built). The UI is still written using JavaScript and HTML, though. You can still distinguish it from a native Mac application, but it should be fine for most purposes. If I remember correctly, there's a lot of stuff that Delphi does not write into the source code, at least back in version 3. The UI elements are not generated programmatically and neither is the layout. However, Delphi does add new member functions to a class when you want to add event handlers, The problem is that Haskell is syntactically way more free-form than Pascal. I simply have no idea to add source code automatically without screwing up your source files.
What can we conclude looking at the *mojo* of testing packages? * QuickCheck 130 * HUnit 55 * test-framework 20 Is Haskell code well tested?
Ugh I wish I went to Utrecht. I live just a bit too far, now I'm stuck in a department that thinks logic programming is the future. Luckily I seem to have found an underground pro-Haskell movement so my thesis subject is safe. 
Well I think /u/dafdsafl was right you needed to use more math - the solutions are so rare you wouldn't have found them in a reasonable time even without the bug. Although I still find it somewhat eerie that it overflows the stack, when it intuitively shouldn't. Normally GHC should compile this kind of list use efficiently.
smart syntax highlighting, e.g. different color to distinguish terms from types, etc
A few years ago dons did a similar calculation and made a nice graphic out of it. Anyone remember where it is?
As pointed to by [gelisam](/u/gelisam), there is an implementation. But it is rather unsatisfactory, as there is no decent denotational semantics that goes along with that operational semantics. Amr and I have been working on new implementations. We have, oh, about 12 of them, all partial, and none yet fully satisfactory. We're still on it. Unsurprisingly, it is rather tricky. 
Parsec does that already, almost. Here is the relevant function from `Text.Parsec.Error`: mergeError :: ParseError -&gt; ParseError -&gt; ParseError mergeError e1@(ParseError pos1 msgs1) e2@(ParseError pos2 msgs2) -- prefer meaningful errors | null msgs2 &amp;&amp; not (null msgs1) = e1 | null msgs1 &amp;&amp; not (null msgs2) = e2 | otherwise = case pos1 `compare` pos2 of -- select the longest match EQ -&gt; ParseError pos1 (msgs1 ++ msgs2) GT -&gt; e1 LT -&gt; e2 The only difference is that Parsec prefers errors that explain themselves over errors that do not. Would removing that preference be a significant improvement? Would it solve the problem in the specific cases mentioned in ezyang's post?
Definitely check out the parsing library [linked in the original article](http://hackage.haskell.org/package/uu-parsinglib), as it offers the option to do exactly that.
&gt; What if instead of backtracking we did this: at every choice point, we split the parser into two, and run those two in lockstep on the rest of the input. This sounds like [parallel parsing processes](http://www.cse.chalmers.se/edu/course/afp/Papers/parser-claessen.pdf).
Note that usually packages have separately included test-suites that aren't part of the main library. This is good exactly because the test specific dependencies don't become dependencies of your library. Not sure what parts of the .cabal file have been included in this analysis, but I can imagine not the test-suites.
Not exactly what you meant, but maybe as a tiny substitute, I've somewhat recently written (https://github.com/quchen/hackage-graph). There's also [a list of (immediate) reverse dependencies](http://packdeps.haskellers.com/reverse).
Very nice. If anyone wants to make a hackage feature to provide this, that'd be cool.
I like parser combinators that can annotate their errors as they unwind. It can be done in Parsec even: expected ';' at 5, 7; in statement at 5, 1; in function at 1;1 
Oh, there it is! No, I only wrote the link to the paper on my to-read list. Today I was crazily looking where I got it from but couldn't find. Thanks!! Edit: also I just finished reading The Computational Content of Isomorphisms, I'm really impressed. Do you have any comments? Is it as groundbreaking as it looks for me?
The Z-Machine is a weird and wonderful little piece of technology (especially for its time) and certainly worth kicking around! Also you can play Zork!
&gt; and since this is a text adventure I didn't think efficiency would be a big problem. Well it is a *text* adventure game! ;) Linked lists of chars are quite slow and Text is quite fast *and* easier to work with! I think it'd be a nice upgrade.
Oh, for real? I didn't know the thing was so known. Why are you guys interested on it, particularly? Do you host repos?
to be honest I would have been fine finding the "obvious" answers, but finding them is what I was trying to do. If it can be written as better haskell that didn't crash and was maybe faster somewhere (but not at the cost of readability, as mentioned in another comment by /u/Regimardyl) those would be bonuses, since I was basically using this as a way to learn Haskell, not actually use it to solve any problems. My goals, somewhat achieved, were to write it without any loops. Coming from the C/C++/D (my favorite of the bunch) I was tempted to look up how to write for loops, if statements, etc. and did get rid of them with the giant list that wasn't evaluated completely (would use up probably all that memory, thanks for lazy evaluation!) and have haskell stop as soon as it found an answer other than 1. I think I generally succeeded at doing things "the functional way" so I'm happy with that, and was just here to see if there was something obvious I was missing as to why it overflowed. Sounds like other projects shouldn't cause that to happen so I was on the right track, just hit a strange obstacle which I would have to dive into the haskell language itself to figure out.
&gt; http://lpaste.net/104249 You'll see powm does do what you're talking about. /u/Regimardyl missed the ellipsis in a comment and thought that I was using 65 as the exponent (still very big) and modulo 1079. The real 1079... has (didn't count, so rough estimate) 50 digits in it? Anyway powm does use the approach of exponentiating by squares, taking the modulo all the way, making it much more manageable. 
I'm surprised array ranks so much higher than vector.
Yeah, powm is along the right lines. I was just commenting on the fact that when you do an exponent modulo something, you're usually not interested in calculating the entire exponent first, because in most situations that would be unfeasible.
http://bitemyapp.com/posts/2014-04-17-parsing-nondeterministic-data-with-aeson-and-sum-types.html http://github.com/bitemyapp/bloodhound/
FWIW haskell-mode does this. If GHC reports something not in scope, it hoogles that identifier and then gives a choice of modules to import, then prompts with an import string like "import Foo" that you can edit or leave as-is, then it inserts it in your module list.
In which case is this simple? 
[This](http://www.haskell.org/haskellwiki/Cabal-Install) page should explain how to use `cabal` to install a package. I'm on a nix-like, so it goes something like this $ cabal install arithmoi ... lots of stuff compiling ... $ ghci GHCi, version 7.8.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; import Math.NumberTheory.Powers as Pow Prelude Pow&gt; Pow.powerMod 10 1000 9 1 Prelude Pow&gt; edit: so for example, your program could be written import Math.NuberTheory.Powers as Pow A = ... big number ... main = mapM_ print $ take 1 $ filter (\n -&gt; n == Pow.powerMod n 65 A) [2..A-1]
OT: That's a pretty paper!
Never heard of it.
I second that, except it's "customizable Vim bindings". Having to use the insane "hjkl" to move would be worse than mouse-dragging for me.
How is hjkl insane? Wait...what do you use? Plz don't say the arrow keys...
YAML instead of JSON for configs.
Thanks that looks very interesting!
Nice, I guess this is one of the precursors to uu-parsinglib mentioned by dmwit?
I think that logic programming may well be a part of the future
Oh I mean, that is the thesis that explains the same thing by the same author. Pardon...
If you think this is awesome, you'll appreciate a pointer to the motherlode of lecture videos from whence this was mined: The [Oregon Programming Languages Summer School](https://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html) Change the `summer13` portion of the URL to a smaller number to see the curricula of previous summer schools.
Speaking of semantics, I wonder if the language could be given a deterministic (but still reversible) interpretation? In the paper, non-determinism is introduced at the end of page 7, by observing that if we have a fresh logic variable `α` of type `a1 + a2` going through a wire `c1 + c2`, then there are two rules which could fire: either `α` unifies with `left β1` and `c1` is applied to `β1`, or `α` unifies with `right β2` and `c2` is applied to `β2`. Alternatively, we could simply give up and say that the semantics of a fresh logic variable going through a `c1 + c2` wire is that the program (deterministically) aborts. But is this alternate interpretation reversible? That is, do programs which abort when evaluated from left-to-right also abort when evaluated from right-to-left? Consider the credit card example given on page 2: from top-to-bottom, the rightmost wire begins with the logical variable `α`, which is later instantiated to 20. But from bottom-to-top, instead, the rightmost wire begins with a concrete value, and never switches to containing a logic variable. So along that wire, any abort triggered by the logic variable from top-to-bottom would not trigger an abort from bottom-to-top. Of course, in this particular program, the abort would be triggered by the logic variable in the leftmost wire instead. Since the `1/α` value must be eliminated somehow, I think a similar situation must always occur, guaranteeing that aborts occur in both directions. Thus, for this particular deterministic interpretation, reversibility is probably preserved... but also, almost every program aborts. I wonder if more useful deterministic interpretations are possible. Perhaps by only aborting if `distrib` needs to pattern-match on a logic variable?
Is there more to multi-dimensional arrays than the baked-in use of `Ix` though? Since the Vector API is much nicer in general, I sometimes used `Vector` together with (manual) `Ix` instead of `Array`.
&gt; [...] guess at an upper bound for packages that don't have one. The heuristic is: if the lower bound is a.b.c.*, use "`&lt; a.b.(c+1)`" for the upper bound. So the packages would need to have a lower bound set, i.e. `=&gt; a.b.c`? But if those packages do have a lower-bound set why didn't the author just set it to `== a.b.c.*` in the first place? And what about build-deps that don't specify any bounds at all?
I didn't dive into your question, but since you haven't mentioned any of that, I must inform you that when dealing with streaming in Haskell you absolutely must be aware of the two major libraries: [**pipes**](http://hackage.haskell.org/package/pipes) and [**conduit**](http://hackage.haskell.org/package/conduit). Each one has a family of associated libraries specialised for different tasks. Personally whenever I identify a problem as streaming, no matter how trivial it may seem, I use "pipes". Lazy IO is an archaic and error-prone approach.
Constraint solvers and type logic, sure. But I believe the experiment with SAT solvers and expanding prolog has failed. It's not that the research was a waste or anything, my uni is just a bit too obsessed with it.
&gt; which depends on the length of bytes drawn from input `length`? Did you fall into [the classic trap](http://donsbot.wordpress.com/2008/05/06/write-haskell-as-fast-as-c-exploiting-strictness-laziness-and-recursion/) of evaluating a large stream twice, once for the main computation, and once for computing the length? &gt; changed the only fold in the code to a strict fold From what I think your algorithm is, I don't understand why you need a fold. Maybe an [unfold](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-List.html#v:unfoldr)? You're not consuming elements from a list, you are transforming a lazy ByteString into a list of bytes.
Thanks, but I anticipated that, being familiar with iterators from Python. I folded the input accumulating encoded data *and* the length of the data at once, so it should have been able to discard the inputs on the fly.
As answered to Tekmo, no: I accumulated the length while folding over the input, rather than using the length method on the input Bytestring.
I'll tidy up some code and share it: thanks for your help so far. Very informative in getting me into grokking Lazy-the-Haskell-way.
I have a suspicion that "Use Pipes" is exactly the sort of thing I was missing, we'll see! :) Thanks!
I would need to see the code then, to provide more detailed guidance.
Well if you do that for `average`, and don't think it through quite thoroughly enough, you get something like: average xs = s / n where (s,n) = foldl' (\(s,n) x -&gt; (s+x, n+1)) (0,0) xs and this *still* leaks space because it's not strict enough - the `foldl'` only evaluates the *outer constructor* of the `(s,n)` tuples, not either of the `s` or `n` inside. To fix this, add bang patterns (you can also use `seq` if you don't want to use an extension) to ensure everything that should be evaluated at each step is: {-# LANGUAGE BangPatterns #-} average xs = s / n where (s,n) = foldl' (\(!s,!n) x -&gt; (s+x, n+1)) (0,0) xs I have a little hunch that your real problem may be similar to this. 
I cannot profess to know much about Haskell....I'm spending all my spare time trying to learn it as I believe it has tremendous potential. I'm still trying to understand Monads (yes, I've read dozens of articles about them, it's still unclear!). I've been an advocate of strongly typed languages for 35 years, and often getting laughed at for making the claim that the app will probably work if it compiles. When I can drag a button and a text field on to a window, double-click on the button and just write code in Haskell that will cause the text field to "hello, world", THEN things will get exciting. I have no idea how such a thing would be done. Nevertheless, I think it's what's needed to propel Haskell into the mainstream. Until then, everyone interested in GUI apps still has to figure out (from first principles) which GUI widget library to install, figure out HOW to install it (none of them ever seen to just install the first time without grief) and figure out how to talk to it from Haskell, a serious non-trivial piece of work in itself. I (and many others) would pay real money for such a thing. Remember that the enterprise version of Delphi still costs over $4k and people pay for it! There are some great competitors like RemObjects Oxygene but Haskell with a Delphi GUI.....almost orgasmic!
Would it be possible to use operational semantics (i.e. monad reification) to move a `try` further in the grammar if the branches start out the same? This would make it clearer to program your grammar while it gets fixed like in the blog post but automatically, so, good error messages and better performance. 
Phil Wadler has [some lectures](https://www.youtube.com/watch?v=AOl2y5uW0mA) from University of Edinburgh online. They're roughly analogous to depth and scope of LYAH.
Thanks
Many of the features posted here are possible with Yi. [My Yi config](https://github.com/fiendfan1/Yi-Config) has support for auto-gen signatures (using GHCi), "ghc-mod check" output, running GHC/GHCi from commands, and vi(m) commands. It also uses the Solarized dark colorscheme. I am probably going to work on autocomplete suggestions, but I don't think Yi provides a way to display them [like you can with vim](https://camo.githubusercontent.com/2135f9b37963594325b304a7a57163b5b6ab8b11/68747470733a2f2f662e636c6f75642e6769746875622e636f6d2f6173736574732f3231343438382f3632333135312f32383461643836652d636635622d313165322d383238652d3235376433316266303537322e706e67).
If you are interesting in our experience with web enabled data processing and analysis using Haskell email me oleg.sidorkin at us.ibm.com. We are in Foster City / Bay Area.
I don't know why "like all CS slides" use it, but Simon Peyton Jones answered the question of why [he uses it](http://www.reddit.com/r/haskell/comments/1bd1ia/spj_and_comic_sans/).
http://channel9.msdn.com/Series/C9-Lectures-Erik-Meijer-Functional-Programming-Fundamentals
That works, until you get to products. They exist, but turn out not to be Functorial. That's where all the difficulties lie.
And previous years, too! Same URL just twiddle the year (including back to 09 or 08 I think?). 10 was the first Great OPLSS in my opinion.
Oh, if your search hard enough, you'll find a repo on github with this stuff. It is chock full of partial stuff, so I am not going to go out of my way to advertise it. We are interested because 'negative types' and 'fractional types' just sound too intriguing to not research. No idea if anything will come out of it, but it has been tremendous fun to explore.
It's very cool but unfortunate that the name conflicts in my brain with the Singleton pattern, which is considered harmful.
Very neat. Having started programming in Scala before Haskell, I hadn't realized that Haskell actually implemented type classes essentially using the same implicit parameter approach (of course, without the additional flexibility afforded by Scala's implicit system). 
We are trying to do just that. I don't like the non-deterministic interpretation. I do like the idea of reversibility, especially in its "information preservation" guise. No, the relational version is not actually reversible. But something really close to it seems like it ought to work. [Basically you have to restrict what you trace over to be of a certain shape]. There has been SO MANY blind alleys, that I have given up speculating on which exact one might work. But we're trying.
&gt; We are trying to do just that. I don't like the non-deterministic interpretation. +1!! &gt; No, the relational version is not actually reversible. I can't tell which version that is. &gt; There has been SO MANY blind alleys, that I have given up speculating on which exact one might work. But we're trying. I'd be quite curious to hear more about those blind alleys. I guess I'll have to look at that hidden github repo :)
the segfaulting is doubtless due to some sort of unsafety in how the cairo backend is bound, where the bug is subtle and triggered rarely. in such cases you can create another sort of "minimal" testcase sometimes by writing code that does a simple thing zillions of times as a stress test. If you do get that, or even if you're stuck with your original case, you can then run e.g. ktrace or dtrace against your binary, or valgrind it (although i think valgrind historically hasn't always worked right with ghc?) in order to see where things blow up. you can also look through all the source code in the pipeline (including diagrams itself and cairo) for any potential uses of unsafePerform or interleave IO that may need a "noinline" marker that's not added to them (hence why different optimization levels would cause the issue). Also, what's your monad stack that you're lifting through? It could well be something tricky going on in there...
Good to know. Thanks for the tip.
Two sensible lines of attack, thanks. &gt;Also, what's your monad stack that you're lifting through? It could well be something tricky going on in there... At that point of the application it boils down to RWST AppEnv AppLog AppState (ErrorT String UI) where `UI`, which comes from `threepenny-gui`, provides the `MonadIO` instance. 
Hi looks interesting and useful. It depends on ruby/rake however? I don't hate either but it does make me a little uneasy depending on those.
Haskell doesn't prescribe how type classes should be implemented. There are several ways being used in different compilers. 
Use JHC. AJHC was forked from JHC when JHC's maintainer went on hiatus. Now that he is back, he's merged most of the changes from AJHC back into JHC, with AJHC's maintainer noting that he would try to write a patch to get the remaining bits into JHC. References: * http://www.haskell.org/pipermail/jhc/2014-May/001079.html * http://ajhc.metasepi.org/ * http://repetae.net/computer/jhc
A while ago I wrote an article/tool explaining how to track down segfaults in GHC compiled programs: https://github.com/blitzcode/ghc-stack It's all a bit of a PITA (I guess your situation already is anyway), but it should quickly point you to the offending bit of Haskell / C code. Good luck!
Hmm, I never realised we could just help improve Hackage, which is nice as I finally have more time to hack on stuff. I see that reverse dependencies is listed as a partially completed feature on the [new features page](http://hackage.haskell.org/new-features) - is there currently any way to work out the transitive dependencies on Hackage? Is this something people would want? I know I would. It seems like an easier feature to get started with.
It was an excellent talk. Unfortunately it seems Zaki was a bit too focused on Dan's manly chest to catch the screen in the frame. ;) With the slides you can follow along pretty well though.
One precision about Magma : the operation must be close that means the result of applying the binary operation is in the same Set. (S, op) is a magma =&gt; forall a, b in S, a op b is also in S 
Sorry for the delay, had to sleep. Edited to include a link to the code I have right now, after tidying up and commenting functions for ease of reading.
Thanks for the suggestion; will look up Bang Patterns and see if this is the problem. Not really sure what you're getting at about the outer/inner constructors though; do you mean it's a scoping issue of re-using the same variable binding names within and without the lambda expression? Or something even more subtle to Haskell?
&gt; When I can drag a button and a text field on to a window, double-click on the button and just write code in Haskell that will cause the text field to "hello, world", THEN things will get exciting. That would be awesome, indeed. Note that with functional reactive programming (FRP), it will most likely be the other way round: you'll have to double-click on the text field and enter code that displays "hello world" depending on the button click. (A bit like an Excel spreadsheet.) &gt; Until then, everyone interested in GUI apps still has to figure out (from first principles) which GUI widget library to install, figure out HOW to install it (none of them ever seen to just install the first time without grief) and figure out how to talk to it from Haskell, a serious non-trivial piece of work in itself. My goal with the Threepenny project is to make a GUI library that installs the first time without grief -- hence the web browser thing. Give it a try? Granted, you still have to learn the API etc if you want to go beyond the code examples, but it's a start, at least. &gt; I'm spending all my spare time trying to learn it as I believe it has tremendous potential. I'm still trying to understand Monads (yes, I've read dozens of articles about them, it's still unclear!). Depending on how much of a priority it is to you, it may be a good idea to consider taking a course or working with a mentor. I have heard about a [Haskell online course form TU Delft][1]. [1]: http://www.tudelft.nl/en/current/latest-news/article/detail/tu-delft-kondigt-vijf-nieuwe-moocs-aan-voor-het-najaar/
This paper is a fun application of the ideas presented: http://ilyasergey.net/papers/graphs-mpc12.pdf I still haven't totally got my head around it. 
I don't think it would solve the problem described in my post, which has no explanations at all.
I'd love to take a course. Very few colleges are willing to allow mature students in their courses and those that do don't have haskell. I'll look into the online course you suggested. Much appreciated
Dylan already had singleton types back in 1992, and these were predated by EQL-specified generic-function arguments of Common Lisp. So the singleton "pattern" and the singleton types (latter stand for types that are inhabited by exactly one value) came up around the same type and none can claim priority IMO. And there are also singleton lists (and in general singleton collections) which contain exactly one element.
Dave Sands has recorded his lectures at Chalmers: &lt;http://www.cse.chalmers.se/edu/year/2013/course/TDA452/FPLectures/Vid/&gt;
Basically, foldl' is stricter than foldl, but doesn't necessarily evaluate the accumulator _fully_. What I mean is that, if you take the expression (s+x,n+1), then before you evaluate any of it at all, it is a thunk _. When fold' evaluates it, it evaluates it until it finds a constructor, which, in this case is the constructor for the tuple. ( _, _).
I did the exact same thing for [my Dart port of parsec](http://pub.dartlang.org/packages/parsers) (with the twist that &lt;|&gt; backtracks by default) and the error messages are usually what I would expect.
Your `dncodefile` function uses the input file twice. The offending line is `let outp = BString.snoc (fst outpWithLen) enc`. The input is used once for encoding and once for determining if it's RNA or DNA.
that is captured in the haskell definition.
You use `foldr'`. That's going to force the whole thing in memory. Just use `Prelude.foldr` instead. There may be other problems; I stopped reading when I got to the first one. The use of `pack` and `unpack` also looks suspicious, but not outright wrong in terms of memory usage.
Wat. I was having this problem *before* switching to foldr', and chalked it up to thunks.. but I said "I'll just recompile with foldr again to satisfy dmwit that it isn't the problem". And now it's working, streaming output fairly quickly. I don't know what I changed in the meantime to fix matters, but returning to lazy foldr instead of strict foldr did it, somehow. Thanks!
Somehow, switching back to foldr instead of foldr' fixed things, even though it wasn't working with foldr beforehand. Something I changed in the meanwhile fixed things. Thank you for your help, though, and for clearing a few things about folds up for me! :)
Aye, but it short-circuits on the first "U" or "T" character. Yes, on bad input it will consume the entire stream, but that was a problem I was going to address later. For now, I'm assuming that valid input, which is statistically improbable *not* to contain "T" or "U", is all that gets passed to the function. ..and if it isn't, it still bugs out on the first character whose ordinal value is not part of the set "ACGacg", so it's likely to fail quickly unless input is deliberately crafted to break it.
I'd like to suggest [clckwrks](http://www.clckwrks.com) as a starting point as it is an actively developed Haskell CMS. It already has modules that hook into an admin section. It even has the possibility of downloading and installing modules from hackage into an already running server with out restarting. (Though full implementation of that future is waiting on a currently accepted GSoC project). The other feature requests are inline with what clckwrks aims to provide as well -- though more resources to implement them would make it go faster :) 
Minor nit, the Monoid instance for lists is listing `(*)` and `e`, should be `(&lt;&gt;)` and `mempty`.
A very informative guide, thank you. 
I think talk of closure is a historical artifact. For example, Wikipedia says "a magma consists of a set M equipped with a single binary operation MxM-&gt;M." The next sentence "[t]he binary operation must be closed by definition but no other properties are imposed" is completely redundant. I think it's there for historical reasons. There was a time in the past when people didn't explicitly state the domain and range of their functions and so it was necessary to make closure part of the definition.
Where would I find this Boston Hac registration sheet? 
Neat-o, thanks!
 Prelude&gt; let sumAll = map sum Prelude&gt; :t sumAll sumAll :: Num b =&gt; [[b]] -&gt; [b] This is what I get from `ghci`, version 7.8.2
This is the standard, dreaded Monomorphism Restriction. The basic story is that whenever you define a top-level definition which isn't "obviously a function" Haskell will try to coerce it to a monomorphic type even if its most general type is polymorphic. In your example, `let sumAll = map sum` doesn't have argument binding occurring on the left-hand side and so GHC is monomorphizing it. However, `let sumAll' xs = map sum xs` is "obviously a function" (again the variable binding is key here) so GHC realizes that the polymorphic version is intended. When you write out the full let form the MR doesn't get invoked because it's not a top-level definition. You can avoid the MR by either (a) using the NoMonomorphismRestriction pragma or (b) by always writing type declaration for your top-level types. The latter is more common as it's just good Haskell style to do so. let { sumAll :: Num a =&gt; [a] -&gt; a; sumAll = map sum }
You've hit up against the [Monomorphism Restriction](https://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-930004.5.5). When you don't have a free variable on the LHS of the equation GHC will treat the expression as a constant and apply defaulting rules for Num instances.
As of 7.8.1. The monomorphism restriction is turned off by default in GHCi (but not in e.g. --make): http://www.haskell.org/ghc/docs/7.8.1/html/users_guide/release-7-8-1.html#idp5920480
Is there a reason we can't turn it off completely, except that we still want to conform to Haskell98? 
For me (on both machines, about 3 or 4 GHz) it did take maybe an hour (? estimate.) before it ran out of stack space. It takes it awhile. You'll see my edit about the 8 core machine still going strong and I was thinking it was going to continue that way, but it did eventually run out of stack space. If you don't mind, I sort of think of the =&gt; as "returns" and -&gt; as "input." Sort of like "int Maximum(int a, int b)" would become the type string "int -&gt; int =&gt; int". Can you tell me what your reasoning was for turning the = into a - in some arrows?
Ok. Thank you! This will be a great help taking that first step. I did see arithmoi and would have used it for this project if I had figured it out, and only resorted to no packages when I couldn't.
I think its for performance reasons. Problems caused by the monomorphism restriction only arise in GHCi or unfinished .hs programs anyways. It causes ambiguity, mostly with helper functions for math and parsers. That's usually not problem for programs that you want to --make. 
This is my bad, not Zaki's. This was setup as a screenshare of Dan's slides and I guess the screen changed on us when the projector was plugged in. Not sure, it was sort of a last minute thing. Anyway, the plan is to get the slides from Dan and edit them into the video. Too bad it was posted here first. 
Programs without the MR can run arbitrarily slower than when it's enabled, and it is usually non-obvious from the source code why this is so. For example fib x | x &lt; 2 = 1 fib x = fib (x-1) + fib (x-2) f = let x = fib 30 in \y -&gt; x + y main = mapM_ print $ map f [1..] Without the MR, `fib 30` gets computed on every call to f. With it, it's only computed once and shared across all calls.
MVars will probably be faster than TVars on most workloads for this application. Mutation at the leaves will result in you taking O(log n) locks down the path to your leaf and under contention it is very likely that the transaction will fail and need to be retried.
That's why I'd love to see data structures to be written with open recursion, that'll allow you to play with ideas like this 'for free.' So not: data Tree a = Leaf | Branch a (Tree a) (Tree a) but: data TreeF a f = Leaf | Branch a f f type Tree a = Fix (TreeF a) data TreeVar a f = TV (TVar (TreeF a f)) type TreeT a = Fix (TreeVar a) So many possibilities!
So what reading materials should I study to understand the stuff in that library?
&gt; I guess the screen change on us when the projector was plugged in I think that's it because I clearly saw you select Keynote as the source of video :-)
ah cool, I can fix things now.
We'll throw up a google form and link it on the site.
&gt; MVars will probably be faster than TVars on most workloads for this application. The sole intention of this proposal is to create a hash table, whose structure is maintained in STM, so I don't think `MVar`s are applicable here. &gt; Mutation at the leaves will result in you taking O(log n) locks down the path to your leaf and under contention it is very likely that the transaction will fail and need to be retried. I don't think so. It's STM, so there's no locks. The only var that's gonna be updated is the one that holds the node, others on the path to it will only be read from. 
Just google the terms. Bath in your ignorance. Get familiar with the terminology. I would start with catamorphisms and F-Algebras, because they are simple and fundamental. Make a recursive data type. Make an unifixed version of the same. Play about. Read Tim William's [slides](http://www.timphilipwilliams.com/slides.html). Read my blog [post](http://benstuff.so/posts/2014-05-16-dynamic-programming-histo.html). I think the biggest thing is, you need a problem that needs recursion schemes before you are really going to get it. Typically data type generic programming over a tree of some kind. Write an expression evaluator one way, write it another way. Try to express it as a fold. This is what builds intuition.
Definitey. This is more a shoutout to the awesome things the DPH folk are doing than it is a recommendation to use any of my code. Still, I have a lot of code just for myself whose bottleneck is distance calculation between vectors and this helps a bunch.
When I listened to the talk I thought a magma was a different name for a semigroup. But I see on wikipedia that a semigroup is an associative magma. I wonder is a magma just too trivial to put in the typeclassopedia?
Author here; my attempt at a new way of introducing practical ways of thinking about Functor and Monad and experimenting with some "new" pedagogy. I tried hard to not say anything too misleading, because there is enough misleading content out there already; but I definitely could hav e missed something! Let me know what you guys think and I'll make modifications to preserve the integrity of Haskell education. Also, it's a bit longer than I would have liked for an introductory article but &gt;_&gt;
The two reasons for wanting the MR are: i) performance, as stated below - calculations on different types take varying amounts of time ii) equivalence, it is not necessarily true that fib 1 == fib 1, for example The MR is partly my fault: when I showed John Hughes what could happen without it (in the very first Haskell compiler), he was horrified! The MR was the fix. 
&gt;data Maybe a = Just a | Nothing As an English sentence that reads like a cynical Goth beat poem about computer science.
It goes back to 05, though only the recent years have video. For the lazy: https://www.cs.uoregon.edu/research/summerschool/summer05/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer06/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer07/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer08/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer09/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer10/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer11/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html https://www.cs.uoregon.edu/research/summerschool/summer14/curriculum.html
Seems like he should *at least* begin by saying "Why the IO-Monad is evil" because he barely touched on any other monads, most of which are very, very different. I feel like I could have written an article called "Why white people are evil" and spend the majority of my post exclaiming why my white neighbor Chad is a huge bastard.
I like [http-conduit](http://hackage.haskell.org/package/http-conduit-2.1.2/docs/Network-HTTP-Conduit.html).
It could be nice if instead of the super confusing special case known as the monomorphism restriction, we'd get a warning about the definition being non memoized/shared. 
i like [http-client](https://hackage.haskell.org/package/http-client). many also like the lensy [wreq](https://hackage.haskell.org/package/wreq). note that all of them use http-client as backend.
Just read the comments on the post.
Oh. Well, if you called out to some simd c codes you'd have resulting routines that work whether or not fllvm is afoot. I have such a lib, need to polish it for hackage. Also the simd work was not by the Dph folks but rather by Geoff mainland. They both do work in the same space, but I'd rather not conflate their different works. 
Just a couple of projects that came to mind when reading the internship description (though I'm not suggesting that they necessarily be used together): &gt; optimized for “single page” sites (no reloads, but retrieve further content with AJAX and use the history API) * http://mflowdemo.herokuapp.com/ &gt; provides a method of writing JS-in-Haskell (Fay?) * http://haste-lang.org/ &gt; search as a first class citizen * https://github.com/bitemyapp/bloodhound/ I suppose this list comes from the "oooh, new and shiny" portion of my brain which saw each of these projects in recent (within the last 3 months) reddit posts. There are of course plenty of mature solutions that would fit the bill as well. [snap](http://snapframework.com/) with it's [snaplets](http://snapframework.com/snaplets) being a very natural fit for modular design. [clckwrks](http://clckwrks.com/page/view-page-slug/1/clckwrks-com) as already mentioned has a lot of what you'd expect from a CMS backed in already, such as [hot code reloading and a highly granular plugin system](http://clckwrks.com/page/view-page-slug/8/why). 
Yes! Often I just want to make a function overloaded for a few types using `DataKinds`, but I can’t tell GHC that I want my overloads to be exhaustive. I would also love a static typeclass (“trait”, to some) that will always be resolved at compile time and involve no dictionary passing.
I really adore your website, and I'm even happier that you're mostly using Haskell underneath. Did you write it yourself? Or are you using a web framework? I can't see anything that obviously points one way or another.
non-ideal, but simple and may work for this particular case: import System.Process main = do text &lt;- readProcess "curl" ["http://..../foo.json"] "" 
Interestingly enough, the "older article" he references at the beginning is titled "Real functional programming or "Why the IO-monad is evil".
&gt; It's STM, so there's no locks. Depends on what you want to call a "lock". There are no locks that block the thread (although a retry would) but updating a TVar definitely does a spinlock, the implementation is [here](https://github.com/ghc/ghc/blob/23fb7f3b7987d7b3fa445e54680a8a00a37e714d/rts/STM.c#L306). Under certain conditions, like a high read/write ratio, stm does quite well for itself and under others it struggles. For some workloads, e.g. small to medium sized binary trees, the rollback rate can get to the 30-40% range. (See http://www.bscmsrc.eu/sites/default/files/cf-final.pdf from 2008, although I'm not 100% sure the results haven't aged due to STM improvements in the meantime). Based on the results from that paper you would expect that a high-branching-factor tree like HAMT would do better in this regard, however. As dmwit pointed out, even if stm were slower than using MVars, it gives other guarantees and there are situations where you might prefer to use it anyways. The easiest thing to do here is lock striping: split the table into n=2^k parts, and use HashMap from unordered-containers, i.e. `newtype StmHashMap = StmHashMap (Vector (TMVar (HashMap k v)))`. On lookup you take `k` bits from the hash code and use it to decide which hashmap to do the lookup in. This is kind of a "cheap and cheerful" way to do it: theoretically you would get significantly lower overhead, especially in terms of indirections, by baking the locking into the data structure. It has the advantage of being easy to implement and performs fairly well under contention.
If you moved clckwrks to github I would hack on it. Public commitment. So sick of setting up php / drupal for people.
Can you explain what this does? &gt;type Tree a = Fix (TreeF a) I missed something. 
Why is it better than the original?
I will defenitley consider applying! I will of course first have to dicuss this with my study coordinator. What is the deadline for applying?
Usually Fix is defined as: newtype Fix f = Fix { unFix :: f (Fix f) } It's the type-level fixed point operator, with the exception of having some extra wrapping constructors.
I think points 2 and 5 of the "10+ things that you can do with MFlow and you can't with your Web framework" section cover the load-once-then-AJAX case: &gt; 2. Convert your application from an active, single page app to multiple pages and back with little code modifications. &gt; 5. Make an element of a page to refresh itself independently by adding a single statement. Specifically, I think the autorefresh function helps here. There is a [tutorial on FPComplete](https://www.fpcomplete.com/school/to-infinity-and-beyond/older-but-still-interesting/MFlowDSL1#single-page--monadic--autorefreshing-) that covers this as well as other examples. While I don't understand MFlow completely, my current understanding of it is that it's like parsec for making websites. That is, a web page is created by combining various formlet combinators in combination with some which modify its behavior (such as whether to autorefresh or load a new page). As [this](http://mflowdemo.herokuapp.com/noscript/monadicwidgets/combination) example seems to suggest that my "parsec" intuition is correct, as it shows how to combine separate pages into a single one. There are other AJAX examples on the original page that I linked before.
Instead of going directly into recursion in your datatype, you parametrize the type with the recursive positions. By parametrizing your datatype with its own fixpoint you get back the original type. While this doesn't buy you much on its own, you can now generically annotate the recursive positions with more data and functionality. Like in my example, you can now generically insert a `TVar` at every position. You get the same recursive structure, but now every node has a variable containing a reference to the next level. There are many other possible recursive annotations possible. (This happens to be the topic of my msc thesis, if interested you can read more about this in [this paper](https://github.com/downloads/sebastiaanvisser/msc-thesis/wgp10-genstorage.pdf) where we use the same technique to persist recursive datatypes on disk.) 
 withManager defaultManagerSettings $ \m -&gt; do req &lt;- parseUrl "http://haskell.org" responseBody &lt;$&gt; httpLbs req m will get you the response body using `http-client`. i don't think that's incredible difficult.
[wreq] seems great. Up until here I only used http-conduit that worked without any problem. [wreq]: http://www.serpentine.com/wreq/
We have no hard deadline. Next week and the week after we will line-up some interviews: if you know by then you can also be a part of this "round". If your coordinator cannot let you know before that, but responds approvingly, please let us know.
&gt; So every code we have inside such a do-block can have nearly arbitrary semantics. It's a bit similar to Lisp macros but the transformation happens at runtime. And because of the chaining of bind-functions, semantics of such a block only depends on the type of the first statement in the do-block. wut
I'm traveling this week and I've got 100 GB video of data to deal with from the weekend. Going to be hard to get into the cloud from the road. Will try to get this organized as soon as possible. 
Thank you kindly for your suggestions. Unfortunately neither wreq, http-conduit nor http-client work correctly (really anything which uses the tls package, since that package has a serious bug in handling wildcard names). My requirement for the library, the /only/ requirement, really, was that it actually works. Since the above mentioned libraries fail that requirement, this leaves spawning curl as the last option. Unfortunately I can't rely on 'curl' being available on the host.
Why not use a mutable hashtable of TVar elements? That way you have multithreaded access to the TVars in the container, without forcing frequent rollbacks, the downside of lock free. Yes that implies unsafeIOtoSTM somewhere for additions , deletions and lookups but these updates may be made idempotent, so there is no problem. TCache define TVars with keys associated, indexed in a mutable hashtable. the elements called DBRefs are heterogeneous and with a optional persistent instance so they can be read/written to/from another place . A database for example . but the hashtable is hidden behind the scene. you have this basic and clean interface: instance IResource a where key :: a -&gt; Key how to read from storage how to write to storage how to delete from storage getDBRef :: Key -&gt; DBRef a readDBRef :: DBRef a -&gt; STM (Maybe a) writeDBRef :: DBRef a -&gt; a -&gt; STM () delDBRef :: DBRef a -&gt; STM () To allow garbage collection of unused DBRefs, they are linked to the hastable by weak pointers with finalizers that delete themselves from the Hashtable. They also have a query language, triggers and indexation by other fields besides the key. https://hackage.haskell.org/package/TCache
I get the impression that the author doesn't understand Haskell too well yet. Looking forward to more complaints when (s)he discovers Applicative.
&gt; What the I/O monads does is creating a new sub-language with imperative semantics. Every code which runs 'in' the I/O-monad is in fact evaluated imperatively. That part is correct. Pretty much everything else is misinformed. The main misunderstanding is here: &gt; we can create a language with all the semantics of the Java language [...]. But Java is an imperative, object-oriented language so nobody would say that we're still writing code in a functional programming language. We can, indeed, use Haskell to represent and evaluate imperative programs. But we can also use Java classes to represent and evaluate functional expressions, and that doesn't mean that Java is secretly a functional language. An IO action is a value representing an imperative computation. The Haskell code which produces the IO action is purely functional, in the sense that the code always produce the same imperative program. This imperative program is in turn executed, and of course, since it is an imperative program, there is no reason to expect it to always produce the same output. Conal Elliott wrote a funny article entitled [The C language is purely functional](http://conal.net/blog/posts/the-c-language-is-purely-functional) which is relevant here. The C language has two parts: one them (the preprocessor) is purely functional, and the other (the actual C code) is imperative. The C code can be seen as pure data which the preprocessor manipulates in order to produce a pure value representing an imperative computation. The fact that this imperative computation eventually ends up being executed, with imperative semantics, doesn't make the preprocessor any less pure. Haskell is the same: it has two parts, one of which is purely functional, the other is imperative. At runtime, the IO action computed by the `main` function ends up being executed imperatively, but this fact does not taint the purity of Haskell functions, even those functions which return values of type `IO ()`.
The article is incoherent. The "other" model proposed is in fact a monad in disguise, just a less useful one (because you can't endow it with concurrency semantics).
Had a laugh when I saw this: main :: world -&gt; output Followed by "In a simple console application both 'world' and 'output' would simply be lists of characters." [That sounds vaguely familiar](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:interact). But here's the real joke: so `World` can represent the current state of the world, but what is `output`? I guess it's supposed to be a list of commands, but then we only ever have the one `World` value. How are our commands supposed to affect the `World` value if we're fixed to the singular `World` value? Any changes to the outside world (including our own!) would be invisible to our program. Really what we need is to produce a new `World` at every step to capture the idea that the world changes at every step, either by our hand or behind our backs: main :: World -&gt; World There we go, given the `World` we do some work and produce a new `World`. Okay, but some functions that take the `World` will produce a value (like `getLine`), so let's include an extra return value to bundle that up: getLine :: World -&gt; (String, World) putStrLn :: World -&gt; ((), World) main :: World -&gt; ((), World) We need to deal with all these `World` values because we can't just produce them from nothing, we have to ensure that we're always working with the most recent `World`. It's kind of a pain, though, maybe we can factor it out... type IO = State World -- (s -&gt; (a,s)) ~ State s main :: IO ()
http-client replaced http-conduit AFAIK. I just upgraded my library from the latter to the former and it was quite painless and pleasant.
Maybe you should file a bug report. Your chance of getting the bug fixed is really high since Michael is a very active maintainer. 
Np we'll get to it.
You can easily change the internals to use `TVar`, `STM`, whatever you want and since you've abstracted away how you recurse over the datatype all your old functions still work (I think).
Great idea. I remember examples and arguments like this got me very motivated to learn more about Haskell and what could be achieved with an advanced type system. Thanks
Of course dear sir. A certificate with CN=*.foo.so will not match the domain name sub.foo.so, the validation fails with NameMismatch.
You really should file a bug. Also, I am going to hazard a workaround that might be totally off, as your problem description is quite vague. You can replace the function that checks if the server cert is OK with yours. It might be possible that wildcard certs are not handled. For example, I *think* java default SSL implementation doesn't support them.
Works fine here. I'm using http-conduit for domains with wildcard certs all the time. What is the actual domain you're trying to hit? Sounds like an issue with the server configuration (possibly SNI-related.)
Almost every development environment of the last twenty years has been trying to get back to where Smalltalk was thirty years ago. And failing.
I am quite aware that wildcard certificates work with some domains. However my example included a specific hostname and specific CN. If it is not too much trouble for sir, I would suggest you try to generate a certificate with that exact CN and try it out. However, it might be easier to just call the name validation function with appropriate arguments and observe its return value.
The thought of disabling certificate validation had indeed crossed my mind. However, upon checking today's date and realizing the year reads 2014, I quickly dismissed that thought.
I think with wreq you write {-# LANGUAGE OverloadedStrings #-} import Network.Wreq import Control.Lens import Data.Aeson import Data.Aeson.Lens main = do r &lt;- getWith (defaults &amp; header "Accept" .~ ["application/json"]) "https://foo.com" print $ r ^? responseBody . to decode . _Just . (_Object :: Prism' Value Object) . at "key"
GHCi disables the monomorphism restriction by default, since it's not generally used for actually writing programs.
As it hasn't been mentioned yet, have you tried [`http-streams`](http://hackage.haskell.org/package/http-streams), which uses the OpenSSL bindings?
I upvoted this submission, but it definitely runs long. As a writing exercise, make a separate copy of this post and edit your writing heavily for length. Delete words, sentences, even entire paragraphs, sections or concepts if you have to, until only a single, focused, indivisible message remains. This exercise improves writing clarity.
Maybe, if I took the time. But it seems this post on the /r/haskell right now seems like a good starter: http://jozefg.bitbucket.org/posts/2014-05-19-like-recursion-but-cooler.html
"give Learn You a Haskell a quick read". Now there's a laugh.
How to deal with rollbacks/retries in this case? E.g., a transaction, which amongst other operations performs insert, fails and retries, wouldn't it result in possibly the same row being inserted multiple times?
&gt;&gt; It's STM, so there's no locks. &gt; Depends on what you want to call a "lock". `MVar` is a lock by nature. Only a single thread can access it at a time. This concerns both reading and writing. `TVar` only becomes a lock when its optimistic suppositions fail during a write, i.e. when a concurrent thread writes to it. This means that reading from a `TVar` should never result in locking and writing to it should only result in locking accidentally. &gt; As dmwit pointed out, even if stm were slower than using MVars, it gives other guarantees and there are situations where you might prefer to use it anyways. Yeah. I know. ) I think he was addressing his message to you. &gt; The easiest thing to do here is lock striping: split the table into n=2k parts, and use HashMap from unordered-containers, i.e. newtype StmHashMap = StmHashMap (Vector (TMVar (HashMap k v))). On lookup you take k bits from the hash code and use it to decide which hashmap to do the lookup in. Thanks for your suggestion. While this does seem easier, this solution introduces a set of other problems, like balancing and growing. That is, besides the obvious overhead issues concerning both memory consumption and performance.
&gt; It can hurt readability: A concrete monad is choosen by the return type of a function. For example a simple 'x &lt;- get' can switch the rest of the do-block into 'state-monad'-land. This...just isn't true. At worst, if you do nothing specific to the monad you're working in, using `x &lt;- get` will cause the monadic object you're working with to have type `MonadState s m =&gt; m a` or something similar, which would be immediately caught with either type annotation or using that object elsewhere.
Thanks for the correction. The FFI doesn't work very well with ByteArrays, though, does it?
Have you reported this bug to vincent?
Mostly two things: 1. Switched to the `Base + Foldable + Unfoldable` variant. This alone provided a huge boost in performance. 2. Changed several `cata (fmap too algebra) tree`s to `fmap too $ cats algebra tree`s. This is probably specific to my usecase as it produces some overhead. I expect the first pattern to be quite useful otherwise.
Ah thanks! It is 'step 1' on the 'get started' page but I have also added a link on the get involved page and on the front page as well.
no it works GREAT. in fact, "unsafe" C calls are actually totally safe to call on relocateable byte arrays (not just the pinned storable sort). note that you probably never want to write any code making an unsafe call that takes more than 1 millisecond (becauese unsafe calls BLOCK GCs from happening), but you could split up the vector codes to work in chunks per ffi call to make large inputs safe. the FFI overhead for an unsafe call is going to be &lt; 30ns (more like 5-10ns more typically) for most use cases, so as long as your doing &gt; 100ns of work per call, the overheads are negligible. you should hangon #numerical-haskell more, theres all sorts of lore :) 
Great, thanks! I think I missed that because it was in a code block, and I just was skimming for links...
Do you mean the insertion of a new TVar in the hash table? that duplication is not possible, because insertion or deletion is easily made idempotent: a repeated IO action of insertion can check if the insertion has already been made. getDBRef for example, lookup for the TVar with that key in the hashtable. if it is not there, it creates one and insert it. there is only one TVar with that key by construction. So externally, getDBRef is pure, unlike newTVarIO, which creates ever a new TVar. There are IO involved in the creation but the important thing is the properties that the design guarantees. 
http-streams is pretty nice - gives you a lot of control over the http call (can stream requests &amp; responses etc)
Indeed, if you've ever read SPJ's classic [Tackling the Awkward Squad](http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/mark.pdf), you'll see him go through pretty much this same process to get to the current monadic IO. It's what I find so funny about it.
That's smart incremental locking, the delayed transactions would have failed anyways. It doesn't really make the program sequential, that's what OP was talking about I guess.
What I mean is that you can define what 'ought to be' a (product) Bifunctor, which has the right signature, but is not actually a bifunctor. Really rather tricky, because it "looks right" until you try to verify some of the finicky details...
During compilation, not at runtime.
Why on earth should that be the case? Are you saying that if `fib` and `f` were declared polymorphically it would be recomputed, or are you saying that only due to the lack of a monomorphism restriction they get recomputed?
If it's a tls bug blocking you, use something based on HsOpenSSL like http-streams for now
`fib` is polymorphic even in the presence of the MR since it is 'obviously a function'. If `f` had a polymorphic type declaration, then yes, `fib 30` would get recomputed on every call to `f` (aside from potential smart compiler optimizations due to noticing the common subexpression in `main`; if the uses were further away from each other it is unlikely the compiler would do anything). The reason is that a polymorphic `f` is no longer "just a value"; it depends on what polymorphic type it is called at. I could imagine a compiler that somehow did a type-directed dispatch to a memoized table in this case, but that doesn't match the "dictionary-passing" strategy used by most current Haskell compilers.
Here's a GHCi session I just ran. Note that newer versions of GHCi disable monomorphism restriction by default, so it will be `f` that needs the explicit type signature instead of `g`. GHCi, version 7.4.2 Prelude&gt; let fib x = if x &lt; 2 then 1 else fib (x-1) + fib(x-2) Prelude&gt; :t fib fib :: (Num a, Num a1, Ord a) =&gt; a -&gt; a1 Prelude&gt; let f = let x = fib 30 in \y -&gt; x + y Prelude&gt; :t f f :: Integer -&gt; Integer Prelude&gt; let g :: Num a =&gt; a -&gt; a ; g = let x = fib 30 in \y -&gt; x + y Prelude&gt; :t g g :: Num a =&gt; a -&gt; a Prelude&gt; :set +s Prelude&gt; f 1 1346270 (3.20 secs, 334243784 bytes) -- slow, as expected Prelude&gt; f 2 1346271 (0.00 secs, 0 bytes) -- fast! Prelude&gt; g 1 1346270 (3.17 secs, 334324804 bytes) -- slow Prelude&gt; g 2 1346271 (3.17 secs, 334182400 bytes) -- still slow! Prelude&gt; let h = g :: Integer -&gt; Integer (0.00 secs, 0 bytes) Prelude&gt; :t h h :: Integer -&gt; Integer -- specialized! Prelude&gt; h 1 1346270 (3.17 secs, 334746376 bytes) -- slow again Prelude&gt; h 2 1346271 (0.00 secs, 0 bytes) -- fast now! Prelude&gt; g 2 1346271 (3.20 secs, 334881136 bytes) -- still slow, g doesn't have any way to know it was specialized into h. EDIT: Some additional bits to address your other comment: Prelude&gt; mapM_ print $ map g [1..5] 1346270 -- slow 1346271 -- instant, g specialized to Integer 1346272 1346273 1346274 (3.20 secs, 334695292 bytes) Prelude&gt; let g1 x = g x (0.00 secs, 0 bytes) Prelude&gt; mapM_ print $ map g1 [1..5] 1346270 -- slow 1346271 -- still slow 1346272 -- still slow 1346273 1346274 (15.80 secs, 1669531396 bytes) Prelude&gt; let f1 x = f x (0.00 secs, 0 bytes) Prelude&gt; mapM_ print $ map f1 [1..5] 1346270 -- instant since f's x is still memoized from before 1346271 1346272 1346273 1346274 (0.00 secs, 1241696 bytes)
ADTs can be recursive, self-referential. You can break down this self-reference by stripping it into layers in the same way that a fixed-point function works. data ListLayer a next = Nil | Cons a next type List a = ListLayer a (List a) -- invalid haskell, but close -- compare `fix f = f (fix f)` -- compare `data List a = Nil | Cons a (List a)` Since most data types have already been "sealed" by performing this fixed-point operation we are stuck with the "layers" as they are. If someone provides you the layer type and asks you to "fix it yourself" then you have the opportunity to slide layers of your own preference in between layers of the type. People can write some operations just the same over types or stacks of individually non-recursive type layers. So it's possible to provide those layers alone sometimes.
You're right, see my other comment for a example case in which the function isn't immediately specialized. EDIT: Also, the regained sharing due to specializing `f` can be trivially lost by eta expansion: fib x | x &lt; 2 = 1 fib x = fib (x-1) + fib (x-2) f = let x = fib 30 in \y -&gt; x + y g x = f x main = mapM_ print $ map g [1..] 
You miss the point. What about a transaction that performs insert, fails, and _doesn't_ retry? Or on the retry path doesn't try to insert?
I think an easy way to learn Haskell is to whip up a little compiler for Haskell.
http://perl.plover.com/yak/typing/notes.html slides 28-31 are a compelling example of H-M type inference's ability to help you locate bugs, which translates pretty directly from ML to Haskell.
There's either a big language barrier, or you're being an ass while expecting others to help you. Since you're doling out knowledge on SSL while asking questions about it, maybe this is a good time to point out that trusting your OS CA cert pool is *not* what you want to do in the year 2014 if you have the ability to include just your own certs in the client software.
Sir, thank you very much for suggesting this library. It works to my satisfaction. I was able to make that library successfully connect to the server and retrieve the document. If I meet you in this life or another, I shall buy you a beer (or fruit juice, depending on your preference).
I didn't see this when it popped up originally, but Jeff Polakow (the author) showed it to me this weekend at BayHac, and it's pretty cool.
Does SPECIALIZE help with this?
 for (int i = 0; i &lt; 1&lt;&lt;40; i++) { malloc(sizeof(double)); }
Another variant is to install with cabal install your-package --solver=topdown or cabal install your-package --reorder-goals
Maybe you should take at the Rosetta Stone paper. http://math.ucr.edu/home/baez/rosetta.pdf 
It's De Bruijn, not deBruijn :)
Thank you for pointing out that I had forgotten to convert my power of 10 to a power of 2. No thank you for missing the point entirely.
I'm in the same boat and I'm going through "write yourself a scheme in 48 hours". I'm enjoying learning how to write a compiler and learn Haskell at the same time. I highly recommend doing the exercises as well though, otherwise it is just copying and pasting.
I don't like this paper. It presents type theory in a really ugly way, which ends up missing one of the main points of type theory --- namely, that type theory gives a systematic account of the concept of variable. Anyway, categorical logic and type theory are closely related, but not the same thing, because the approach you end up taking is a little different. In categorical logic, the basic question is whether you can find a category of models and a calculus such that you can prove soundness and completeness of the calculus. In type theory, you try to design a calculus such that you can show properties like cut elimination and normalization. So categorical logic looks for an "external" notion of correctness, and type theory looks for "internal" notions of correctness. As a result, the theorems you try to prove and the techniques you try to use are different, though type theorists benefit from knowing categorical logic and vice versa.
I often use graphmod (http://hackage.haskell.org/package/graphmod). It generates a module import graph of given Haskell files in dot format.
My bad, I misunderstood your comment. If possible, I would like to avoid re-implementing functionality that is expected to be present in the libraries which I use. The particular problem can be best shown in the code: https://github.com/vincenthz/hs-certificate/blob/2303df0f42a9962dc70a179228a5292018f97e99/x509-validation/Data/X509/Validation.hs#L356-L357 The check is too generic (rejecting all wildcards which match *.???.??), and furthermore the comment, while technically still true, will be misleading after June 2014 (http://www.nominet.org.uk/news/press-releases/nominet-introduce-shorter-uk-domain-name-registrations). Please also see https://publicsuffix.org/.
Ahhhh, ok; so in some sense they are duals of eachother?
If you `cabal update`, do you still need to do this with the current version of `transformers-compat` I shipped the other day?
I just tried this with `cabal update` and a new sandbox, and still getting the issue.
That's sort of a weird division, in that if we use categorical logic to give semantics to type theory we're still "doing" type theory in a sense. From my stilted standpoint, it seems to me that the "internal" notion of type theory as related to cut elimination and normalization, etc. is really a direct descendant one of a number of traditional approaches to logic, and categorical semantics are just a different (model-theoretic derived) descendant of the same logical tradition?
i googled "github clckwrks" and found a git mirror of the supposed source code. 
Oh sweet! You are writing a compiler in scheme? I have had a go at one in C and it was a blast. All the best! Also, I am doing the exercises but my point is that they are disconnected. I was wondering if anyone has an idea of something cohesive and coherent that I could build alongside the book. Something that grows as the number of read pages grow. The examples show how to use the features but not in conjunction with a lot of other features that you might usually use them with in terms of an application. So there is a little bit of a disconnect. 
You need to give this technique a catchy name so that it will enter conversations and the community's consciousness.
&gt; Thanks for your suggestion. While this does seem easier, this solution introduces a set of other problems, like balancing and growing. That is, besides the obvious overhead issues concerning both memory consumption and performance. Balancing should not be an issue with reasonably good hash functions (which we have). As far as overhead, as I said: lock striping HashMaps in this way will be slower than building concurrency into the data structure, but it can serve as a reasonable baseline to benchmark against in the concurrent case. Space overhead is actually constant here, so that's not an issue except for very small maps.
Thanks :) I'll fix that. Yeah, I always found the term "context" a bit odd. But I think the biggest sin was saying "monads are a context" instead of "contexts can provide a monad interface"; it sort of pushes people to try to fit monads into some conceptual box instead of just recognizing them as a very general and useful common interface.
Well, you tracked down the problem ! Why don't you open an issue on github ? I am certain this will be corrected promptly.
I am teaching myself Haskell by writing a library for Clifford algebra. It really helps put typeclasses, polymorphism, folds, etc into perspective.
I'm not sure about this. There is a long model-theoretic tradition in type theory. Some of the best ideas to come out of the type theoretic approach (such as domain theory, lambda based realizability, logical relations, etc) are external not internal.
Thank. I shall have a look at it. Math isn't my strong suit so I will probably have to do a lot of reading to understand what I want to implement (w.r.t., Clifford algebra i.e.).
I used to be of the opinion that the "additional flexibility" of internalizing dictionary passing was an unmitigated plus, even if the Scala implementation of it was rather, er, un-perfect. But, I am now not nearly as convinced. Confluence of resolution is a huge win that you get in Haskell and which is hard to make work in systems with first class dictionaries. 
They both have the same goal, but approach it differently. I'll try to explain using a simpler example. Imagine you want to do group theory. The traditional approach is to define what a *group* is (a set with some structure satisfying certain properties), then look at the category of groups and study its properties. Another approach is to take some arbitrary set X, and consider all the possible sequences of elements of X and their "formal inverses", subject to the usual cancellation laws. The first approach is more abstract and more general (we can talk about groups that are not free). On the other hand, the second approach gives a better insight into the computational properties of these objects. The same holds for categorical logic and type theory. The underlying subject is the same (essentially, the theory of sufficiently nice Grothendieck fibrations), but the methodology differs. In categorical logic, one defines the notion of a fibration and works from there. In type theory, one explains how to generate contexts, types and terms symbolically and gives their reduction rules. The syntax can be recovered from the categorical approach by looking at (2-)initial objects of appropriate 2-categories. Conversely, one can define some notion of "model" of a type theory, and recover the corresponding semantic objects.
One of the most popular platforms today, the web, has pretty good support for LLVM, but limited support for C. Although AsmJS currently does not support SIMD, I think it will in a not too distant future. Relying on C is not future-proof, at least not for the web platform. Although LLVM is not directly exposed, AsmJS et al are leaky abstractions. AsmJS and similar are not toys. I recently implemented the post SHA3-candidate Blake2s in AsmJS and in Firefox, it outperformed the sha256sum linux utility. I think it is important to also consider the web as a viable platform for Haskell.
I may be wrong, but I think it's called "free functor". Anyway, definitely it is a basis for a ["free monad"](http://hackage.haskell.org/package/free).
Ok. Since the transaction is in the STM monad, it ever retries until it succeed. 
Okay. How about one transaction inserts a row and goes on doing something with an assumption that the row is there, but a concurrent transaction deletes it?
That is a good idea concerning a benchmark baseline.
How hard is it to convert between a type theoretic approach and a categorical logic approach while solving a problem? Some of the things I've been reading lately on recursion schemes seem to be written in a style such that each paragraph can be read from either a categorical view or a type theoretical view.
Related (despite being in Ocaml): Danvy, Keller and Puesch's [*Tagless and Typeful Normalization by Evaluation using Generalized Algebraic Data Types*](http://syntaxexclamation.wordpress.com/2013/10/29/new-draft-on-normalization-by-evaluation-using-gadts/).
Can somebody explain the significance in layman's terms.
Perhaps you might find useful the "[relation between type theory and category theory](http://ncatlab.org/nlab/show/relation%20between%20type%20theory%20and%20category%20theory)" page on the nLab wiki? i've certainly found the "computational trinitarianism" table in that page to be helpful ....
&gt; Is there any (good) reason not to use Default-Language: Haskell2010 for new code No.
Someone should let [/u/vincenthz](http://www.reddit.com/user/vincenthz) know about this issue.
Oh alright, cool. Sounds interesting. I might give it a shot after Learn you a haskell. You should have a look at [this](http://www.youtube.com/watch?v=CqhL-BDT8lg) if you are inclined towards writing interpreters/compilers. James Coglan builds a subset of Scheme in JavaScript in about 20min. Quite a fun (and short) talk.
Actually, it's de Bruijn.
I wrote on that in my blog post here: http://fho.f12n.de/posts/2014-05-07-dont-fear-the-cat.html Basically you can work on the fixpoint of a Functor. 
Given the negligible changes in Haskell2010, this seems fine to me.
THB I have no idea what your point is either.
I actually pedantically asked edwardk to fix his texts too. I know that many (if not all) English texts mentioning De Bruijn spell his name as "de Bruijn" (see for example TaPL), but this is not the proper way to render Dutch names. "de" is a verbindingswoord (roughly "connector word", these are words like "the", "from", "of", etc.) these connector words are normally rendered lowercase (i.e., Nicolaas Govert de Bruijn, N. de Bruijn, etc.). However, that rule is trumped by the rule that Dutch personal names must *always* start with a capital letter. Therefore, when referring to a name like De Bruijn without an initial or first name, the 'd' should always be capitalised. The confusion of the correct capitalisation in English texts seems to stem from non-Dutch authors seeing the name rendered with a lowercase 'd' in the full name and then just directly copying this. I'm mostly pendatically correcting people because, as a Dutch speaker, reading a name like De Bruijn rendered as "de Bruijn" trips up my mind. It's like trying to step on the last step of a stairs only to find out you miscalculated and there is no step, causing you to stumble in the air ;)
Thank you for the clarification!
Can you explain what the runtime layout of this is compared to the original? Does this introduce extra boxing?
Having been learning Haskell for the past couple of weeks, I'd like to say that I thought this article was great (where great = fix emphasize), I learnt a lot from it. Thank you so much, and please do continue sharing your knowledge.
There is a difference between a program running out of memory at run-time because the language deliberately gives you enough rope to hang yourself; and the compiler running out of memory at compile-time because it can't efficiently handle a short, albeit contrived, sequence of code. Minimal test cases sometimes appear contrived yet still expose bugs that afflict longer real world code. To be clear: I don't think this is a bug in Haskell; I think it is a bug in GHC. (Whether it is worth fixing is another matter.)
Do you think this code will be a problem for the C++ compiler at compile time? The (albeit unrealistic) code in the linked thread troubles the compiler, not the generated program.
There is indeed a difference, but it's an inconsequential one. Both are cases of being given rope. I wouldn't consider this a bug, either, just an implementation decision with funny results in corner cases like this.
Does it matter? I fail to see why we should care about problems that only arise in contrived settings. Regardless of whether its compile time or run time, a problem that has no actual chance of arising in actual code is not in fact a problem.
I know very little about this, but I suspect it is referencing this paper: [Functional Programming with Bananas, Lenses, Envelopes and Barbed Wire](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.125)
I plan to release a fixed cabal 1.20 in the next few days. The fixes are already in. We just need to do some more testing.
Simply: categorical logic is about models/denotations, whereas type theory is about syntactic objects/programs. There's a big difference, surely, between, say, the bit of code `\x -&gt; x` on the one hand, and the mathematical object that it corresponds to, whether thats viewed as a set of pairs, or some other equivalent. These are clearly distinct things. You can get your hands on code, at least to some extent, whereas mathematical abstracta exist only in Plato's realm.
Because a problem in non-contrived settings can be hidden and made visible by giving an extremely contrived example. Without looking at the code and how it runs it's impossible to tell what the impact would be at improving this. It could be that GHC runs 1% faster even for non-contrived code because it turns out you can shortcut some cases that occur in normal code too.
The way I think about it is how hard do I want to make it for new Haskell compilers to get off the ground.
And if that were so, it wouldn't be contrived! :) I'm not saying that this *couldn't* be useful to address, just that this example isn't a good reason to.
Ah, I see now, thanks.
Okay. I get it. Thanks. However I can't help but have the feeling that all those workarounds form a potential bug pit, and require a chain of solutions which are workarounds for workarounds. E.g., &gt; a repeated IO action of insertion can check if the insertion has already been made While being a workaround already, which induces an extra lookup, it also requires you to perform lookup and insert atomically to ensure that a conflicting insert does not happen in-between those operations. This than requires another workaround, like introducing a lock for those operations. I'm sure, there's a whole bunch of other problems like that. And all those workarounds inevitably degrade both the performance and memory consumption. That is besides the evident problem of conceptual correctness of the data structure and ability to reason about it. 
None of the extent Haskell to js compilers use Asm js, so that point is moot. That said, the moment that browser js implementations have decent simd, I'll be the first to test out invoking them with ghcjs :-)
Great!
On the other hand I'm not saying the example isn't contrived, but I'm arguing that dismissing the example just because it's contrived is also wrong. Especially when it's a question made to understand the issue as opposed to a bug report (where you could indeed say it has a low priority). No one is going to wonder why your original example takes a lot of memory, on the other hand I'd not expect the example to cause an explosion in memory.
What changes in Haskell2010 make this more difficult? Given that Haskell98 usually means Haskell98 + FFI + Hierarchical modules anyway, the changes are very minor. It even removes a feature (n+k patterns)!
Concise and (I think) an accurate description of the alternatives, nice! But if you don't want to use GHCJS because it pulls in lots of packages, how are you going to survive pulling in lens with haste? 
I don't think having a monad instance (free or not) would give you much here. Do-notation to construct the thing? Why would you? Also, just because the functor version of the original datatype could be a free functor, free monad or whatever doesn't automatically mean this technique should be called that way.
I usually call this fixpoint annotations, which is far from catchy ;)
If there are any 6.* users out there i'm pretty sure they are already suffering :) As it is now, if you support the latest three haskell platform releases you are doing a really good job.
Nope! edit: Yep! https://github.com/arianvp/Hom
I will probably run into the same problem yes. I'm going to check how "bad" the damage is later. Maybe it isn't really a con because it's kind of unavoidable.
I'm not the job poster, so you'll need to contact the company directly. I was just the messenger, since Haskell jobs are quite rare they deserve a bit of publicity.
Instructions: Quickly build a Haskell compiler. Note: If this tasks takes a long time, then you aren't following instructions.
I assume that most packages generate a lot of code, but I might be wrong about this! I recall Luite saying that just deriving Show instances (in GHCJS) can be very expensive in this regard. (Normal) Haskell library authors don't care about this because the size hardly matters when you compile a normal binary. My guess is that you'd have to fork libraries to tune the output size as these changes are unlikely to be accepted upstream. It would be super awesome if I'm wrong about this stuff :-) 
A nice way to deal with the length might be to put explanatory comments in expandable boxes. That way, if the code confuses me I can delve into the explanations, but if it doesn't I can go right ahead to the next part. p.s. this is one of the easiest explanations of monads I've seen.
I was referring to the exercises in "write yourself a scheme". [Here](http://jonathan.tang.name/files/scheme_in_48/tutorial/overview.html) is the link in case you want to try.
I was actually just working on a talk comparing Om and Elm. I may have to add a bit about Hom, while I'm at it.
You may find this book review useful: http://www.math.mcgill.ca/rags/jacobs.html. And here's the PDF: http://synrc.com/publications/cat/Category%20Theory/Categorical%20Logic/Jacobs%20B.%20Categorical%20Logic%20and%20Type%20Theory.pdf 
Agree. There is also already an efficient PublicSuffixList lib available on hackage.
Sadly I can't imagine my Haskell skills are good enough for this, despite living in NYC and currently working midtown....would be super convenient...
Are you talking about [purescript-contrib/purescript-react](https://github.com/purescript-contrib/purescript-react)?
Ah — I'm actually not. Checking it out now, though.
Last time I tried, Haste just could not compile lens due to some dependencies. In particular (and fundamentally), Haste can't seem to handle deps like bytestring or text. Edit: However, lens-family does compile, which is what we are using instead. 
Sadly looks like this company repeats the same mistake many others do (mine included). They want the Jinny in a bottle (a very capable, experienced haskell developer) yet will not accept the possibility of remote work. Good luck finding an experienced haskell dev who is either not employed already, or willing to terminate his most likely very well paying job and relocate to try his luck. 
Yes sure! Probably just going to use the github issue tracker as a TODO list. Feel free to add issues. I havent got a computer at the moment so at this point i can only do some basic design discussion. But ive got my iPad so im going to spend time reading the Om source code And React documentation thoroughly. Oh and what made you decide against purescript? Because perhaps it would be interesting to instead build a lightweight lens library for purescript and use purescript-react
Have you considered building it on top of [ghcjs-dom](https://github.com/ghcjs/ghcjs-dom)? Then we could also use it to make native apps without any JavaScript at all.
I'd be interested to hear how they compare! I suspect we'll be looking into Elm + React this summer, so it'd be great to start with a good cost-benefit of how things look today. If you want to share, the [Elm mailing list](https://groups.google.com/forum/#!forum/elm-discuss) would be really curious!
Thanks. Gonna have a look at it later tonight. 
&gt; Jinny Unless I'm missing something, you meant "genie". 
:) Yeah, in middle east they are called jinn. 
There is an issue with type classes that have more than one function (like Show). A reference to an instance of Show will currently bring in the code for show, showPrec and showList even if only one of them is used in the code. So if you do "show (1::Int)" you are also lumbered with the code needed for showPrec and showList (for Ints). Luckily lens uses mostly typeclasses with only one or two functions, so the damage may not be too bad.
Stackoverflow has multiple urls to the same question. The two used in this situation was the url with the title in it and the other was based on the question id.
that's only if the _only_ thing in the transaction is the hash table mutation. the moment something else wraps it all bets are off. e.g. http://hackage.haskell.org/package/stm-2.2.0.1/docs/Control-Monad-STM.html#v:orElse
Different uri, Reddit isn't to know that StackOverflow considers it to be the same page.
Thanks. Fixed.
Apply anyway. I doubt they are getting a ton of applicants.
ReactJS uses a virtual Dom that diffs against the real Dom so I think that wouldn't work.
A second recommendation for Write Yourself a Scheme, that's how I learned. If you want something deeper, try SPJ's book Implementing Functional Languages: A Tutorial. Some of the code is in Miranda but it easily translates to Haskell. The book is available [free](http://research.microsoft.com/en-us/um/people/simonpj/Papers/pj-lester-book/).
I see. Thank you.
No, there are two logical traditions in play. The model theoretic approach is the mainstream mathematical and philosophical approach; the idea is that you interpret a language with a mathematical semantics.
I'm actually working on this as my Google Summer of Code project right now! I'm using GHCJS, ghcjs-dom and sodium. I've only just started, but you can follow the progress [on Github](http://github.com/ghcjs/ghcjs-sodium).
Great work once again. I would not be surprised if this became Elm's killer feature
Oops, hit send too soon. The internal approach is much less mainstream: it originates out of a remark in Gentzen's paper on cut elimination that you can interpret a logical connective as being defined by its inference rules. The philosopher Michael Dummet observed that this is closely related to the philosophical position of antirealism, which is a minority view rejecting correspondence or truth functional theories of truth. If you're interested in this stuff, Dummet's book *The Logical Basis of Metaphysics* is really good.
I was planning to wrap it
Fun! This is quite close in concept to one of my project ideas for GSoC this year (https://github.com/ghcjs/ghcjs/wiki/GHCJS-Google-Summer-of-Code-project-ideas ). I hadn't heard of Om when I wrote that, but it just seems natural to exploit immutability here. My initial thought was to use stable names or low-level RTS tricks to compare identity of heap objects, match them to existing DOM fragments. This way, the structures would just be plain pure Haskell values that can be manipulated through any means, including lens of course. The trick is then to find some way to exploit this observable sharing when rendering the data, in a safe way, with ugly details hidden from sight (for example writing/deriving a 'diff' data type first, so that the black magic happens in some `A -&gt; A -&gt; DiffA` function, and the incremental updater then just processes pure `DiffA` values). Of course, it could be that I've just been blinded by doing too much low level RTS work and that this is not a good idea at all. Kyle Raftogianis submitted a proposal based on this, and has just started working on his project ( https://github.com/ghcjs/ghcjs-sodium ). Although it's based on an existing FRP library, I think there is quite a bit of overlap with Hom. The project is still in very early stage, and I'm not sure yet what direction Kyle wants to focus on, but there might be some good opportunities for discussion. None of the above mentioned low level tricks are in the code, but as mentor I've promised to help out with any runtime system issues and questions should the need for those arise. Oh I think it's fair to also mention the other GSoC projects this year. For GHCJS we have: - Ömer Sinan Ağacan is adding profiling support - https://github.com/osa1 - Nathan van Doorn is building a GUI debugger with stepping/tracing (possibly time travel!) - https://github.com/Taneb Full haskell.org GSoC 2014 project list here: http://www.reddit.com/r/haskell/comments/23mfwj/14_haskell_projects_accepted_for_google_summer_of/
By the way, GHCJS doesn't support all Hackage packages, and it doesn't really make sense to do so, since there are many platform specific packages. While it's possible to implement the functionality of platform specific foreign calls in JavaScript, it's often not the ideal solution. Also things like the new vector primops in GHC 7.8 are unsupported (but the native codegen also doesn't support them, only `-fllvm`). Even `lens` required a (one line) change to support GHCJS: https://github.com/ekmett/lens/commit/608bb5bff55eaa4424dae24c84aab27b5f43441f We have implemented support for some commonly used packages (like `ByteString` and `Text`), and more can be added through the shims repository (hopefully eventually directly through Hackage), but the fact that for example the `unix` or `Win32` (depending on the host platform) packages are built by GHCJS as part of the installation procedure is more an artefact of how we currently support Cabal and Template Haskell, rather than actual intentions to fully supporting them. The plan is to eliminate the dependency of Template Haskell (and Cabal) on native code, so we can be more flexible with different platform-specific dependencies. Then when (if?) the split [1] of platform specific parts out of `base` finally happens, the package situation for GHCJS will be a lot cleaner, hopefully breaking little compatibility with the hackage ecosystem. I can't really give a timeframe though, the change might require more upstream GHC patches, which would make 7.10 the earliest opportunity. [1] https://ghc.haskell.org/trac/ghc/wiki/SplitBase
Ah. In that case I'd rather new compilers support only Haskell2010 than only Haskell98. Stuck in the future seems better than stuck in the past. In practice I guess making a general purpose Haskell compiler is already a pretty daunting effort: for many people and libraries, Haskell equals GHC. To get a good batch of hackage compiling, I think you'd need quite a few non-standard and non-trivial extensions.
Nice to see Haskell jobs in Israel :) Intel Yakum was also looking. I know of a few more people working on Haskell commercially in Israel. Hopefully this list expands further..
What makes you think they won't consider remote work? It just says &gt; It is also a plus if you are local to the NYC area
I've been playing with this actually (not using React, just swiping some of the ideas)
Yeah, this is a much easier sell, since the UI components are not as important when working with WebGL 
Was this a regression? I think saw the same problem in 1.18.x.
You can contact me - Tim Chevalier at AlephCloud. tim at alephcloud dot com . (We're in Sunnyvale.)
If ever there's a time that proves that Haskell was the right choice of technology, it's when you go looking for programmers :-)
Aside from showing another tagless-final example, and explaining Oleg's tagless-final linear lambda calculus encoding, this tutorial expands upon what Oleg did in two directions. First, the tutorial shows how to make a higher-order abstract syntax LLC; Oleg's version used De Bruijn notation. Second, the tutorial implements a full LLC with multiplicatives, additives, and their units which significantly complicate type checking. It is interesting to see how (relatively) easy it is to transliterate type system judgments into Haskell type signatures; the types of the language constructs really are the judgments in the cited papers.
If I click "Open in IDE" on the last snippet, I get this error: Main.hs@17:8-19:53Couldn't match expected type `()' with actual type `Bool' Expected type: IO () Actual type: IO Bool In the expression: runNavigation "" . step $ do { m &lt;- page $ play (Game 20) "Hello. "; page $ toHtml m ++&gt; wlink () &lt;&lt; " Another game?" } In an equation for `main': main = runNavigation "" . step $ do { m &lt;- page $ play (Game 20) "Hello. "; page $ toHtml m ++&gt; wlink () &lt;&lt; " Another game?" }
Thanks! Shameless plug: Michael Snoyman, lead dev for Yesod, has put good Yesod support into [FP Haskell Center.](http://www.fpcomplete.com) Bartosz Milewski wrote [this very basic intro.](https://www.fpcomplete.com/school/starting-with-haskell/libraries-and-frameworks/basics-of-yesod) Michael Steele wrote [this more advanced tutorial.](https://www.fpcomplete.com/school/advanced-haskell/building-a-file-hosting-service-in-yesod)
&gt; main :: world -&gt; output &gt; How are our commands supposed to affect the World value if we're fixed to the singular World value? I assumed the world was like a lazy event queue: if there was nothing on the queue, it would block until there was. If you wanted to open a file, you would output a "get file" command, and then a file would just show up in the events instead of it blocking. The exact details are totally hazy to me, and I am pretty sure it would be much harder then this author thinks even if it is possible, but it would be interesting to at least see how an attempt would go.
Why doesn't he build a prototype of his "better IO" then? If his way is superior or even feasable, it would be very interesting, and if it isn't then it would increase our confidence that the current monadic strategy is superior. Less internet debate, more results.
Does IO actually force order, or is it possible for the compiler to reorder in those cases where order does not matter?
Contributing to an Apartheid system should avoided when possible. Many private companies in the tech sector collaborate with the military. This same military is enforcing an unjustified oppression toward an entire population based on religion (Palestinians in Gaza and West Bank). 
You might find useful ideas at http://bit.ly/devbeat13
It looks like the author is supporting uniqueness typing, which exists in some purely functional languages already (most notably Mercury and Clean -- Clean has the most resemblance to Haskell and would therefore be the most apt comparison, though it's very much a research language and seems to not have much active development done with it or on it).
You can use acid-state and the monad-par packages to simulate map-reduce style programming in RAM. There's also cloud-haskell (the distributed-process package). 
Some links: 1. http://www.haskell.org/haskellwiki/Guess_a_random_number 2. https://www.fpcomplete.com/school/starting-with-haskell/libraries-and-frameworks/randoms 
Nice! Would it be possible to write shaders in Elm?
Is this an announcement anticipating an upcoming announcement? :)
most programming languages do not offer you true randomness either. They are simply pulling values from some global state that manifests in hopefully unpredictable ways. "pure randomness" is the same deal. you give it some seed and it pulls out some unpredictable data from it, and a new seed. the same seed will give the same data every time, so all you have to do is initialize it with a seed found from some entropy somewhere, like the time. Haskell is not a language without state; it is a language with out *implicit* state. 
 Basically the same way it was done 50 years ago in Simula. :)
Simple suggestion: Use data Maybe a = Nothing | Just a instead of the other way around. This is how it is defined [in the actual source](http://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-Maybe.html#Maybe) and makes more sense semantically once you derive Ord. Nothing is smaller, Just is bigger. It seems simple, but seeing it written backwards somewhere messed with my intuition for a while, even on things like "should Left or Right contain the error message".
I'm sorry that I'm not fifty years old :)
Indeed. There is an additional problem. If your transaction has some control-flow, it may choose the path it takes based on inconsistent reads that it does not yet know were inconsistent. x &lt;- readTVar a y &lt;- readTVar b if x == y then ... else ... Even if all transactions will ensure that `x == y` is always true when they are committed the else branch of the code above can execute.
Yes, you can write raw GLSL. In addition, we parse shaders and give them type annotations at compile time, to make sure that the shaders are compatible with your meshes.
Is categorical logic constructive?
Don't be. That's all you missed.
&gt; Many private companies in the tech sector collaborate with the military. Wouldn't that argument apply to the US tech industry as well?
It gives the date.. That's basically all the information you get before the contest starts anyway.
Is there an example which shows more a single cube in the scene? I'm interested in seeing how it looks and performs with a scene which has a tens of thousands to millions vertices.
I'll be working on bigger examples in the coming weeks; I'll be presenting at NDC Oslo and do want some more impressive demos.
The number of civilian deaths induced by the most recent US invasion of Iraq is at least an order of magnitude greater than the sum total of humanity lost in Israel Palestine conflict since the 40's. That particular manifestation of US power led to the genocidal destruction or dispersal of a number ancient peoples like the astonishing Mandaeans, the Assyrian and Chaldean Christians -- each of which had managed to maintain its existence for 15-20 centuries of shifting imperial arrangements in Mesopotamia, but vanished forever at the near approach of US power; that this would happen was known to any minimally reflective person at the time of the invasion. Finally, the relation of the US and Israeli regimes clearly entails that anything the latter is guilty of, the former is guilty of as well; though the reverse is not generally the case. But somehow job postings in Chicago and Honolulu don't elicit this kind of knee-jerk reaction. 
The stuff on that level is not a categorical logic approach, although it is categorical. The types of categorical results used are pretty self-contained, and don't exist within all the machinery necessary to give a categorical exposition of the semantics of the entire language. Nor in fact are the results you're looking at on recursion schemes in the traditional type theoretic tradition! They don't involve, typically, a system of typing judgments and proofs of properties about them. So the answers you're getting are in a sense too "deep" for the sorts of papers that we typically look at in "applied FP" (as opposed to direct type systems research -- and even there the ones at CS conferences will tend to use plain old type theory and not come within a thousand yards of the term "Grothendieck fibrations"). The points of view we're talking about is really about using some basic category-theoretic concepts (such as "initial object") to provide some key results in data abstraction (recall also we're talking about semantics of actual _programs_, not just the behaviour of their types!).
It doesn't have to be. As a general approach it can be used to model _many_ logics.
Are you kidding me? They're not asking for someone who has written a compiler, or a core library, or published a paper, or even has a single library up on hackage. They just want someone who has the "ability to write clear, concise, and efficient Haskell code" -- i.e. not a beginner, and not a putz. The NY Haskell users group has over 650 people registered as members. I don't think the situation is quite as dire as you imagine...
Does it mean it'll use buffer objects underneath to render the scene efficiently?
650 people in a 8 million city. How many of them are employed? Most likely all of them. 
The blog post claims Om ended up being faster because it could bypass deep equality checks. But I don't see any references to benchmarks demonstrating this? (Also I'm not sure if in typical cases the speed here is really at issue? Also also the inefficiencies this addresses are caused by a silly model to begin with -- one that just passes a whole big object in each "tick" and then says "update where things changed", as opposed to a model that explicitly represents the deltas _to begin with_ and thus doesn't have to "back out" what they may have been)
Good programmers in a (for now) tech boom. How many do you think are employed? (Anyway, New York is the greatest city in the U.S. and if anyone doesn't think so they're not worth hiring anyway :-P)
Outstanding article!
OP link is dead?
This just sparked this question in me: are RNGs threadsafe? Dies it improve randomness when several threads access a global randomness source at once? Or are there even interactions that decrease entropy?
Dependent types? in MY Haskell? It's more likely than you think!
Are you talking about in Haskell, or in other languages? In Haskell, it is typically desired to pass explicit states/seeds, so there is nothing global going on. If you need two random integers, you get one from the seed, and get another from the second seed. RNG's are just normal functions randomInt :: Seed -&gt; (Int, Seed) So asking them if they are thread safe is like asking of integers and muliplication are thread-safe.
Thanks, I think this helps :) It's similar to seeing how the `Ord` instance for `Bool` makes sense. I never thought about that on `Left` or `Right` being the error; I always just saw it as `Either e a`, the left value is the error and the right value is the value...because the Functor/Applicative/Monad is `Either e`, so it must "contain" an `a`. But intuition does go a long way in these matters.
 stars' :: (Category cat, Applicative (cat i0), Applicative (cat i1), Applicative (cat (i0, i1))) =&gt; cat i0 o0 -&gt; cat i1 o1 -&gt; cat (i0, i1) (o0, o1) stars' f g = liftA2 (,) (f . arr' fst) (g . arr' snd) first = flip stars' id second = stars' id
Obscure category-theoretical project name, check! :-)
Yep it's a lot of work to do better here, we'd have to track for each function that uses a typeclass dictionary which methods it uses. Then the linker would be able to leave the unused part of the implementation out by looking at the places where the dictionary is passed in. Fortunately the problem is not as bad as it used to, since the generated code for initializing top-level data is a lot more compact now that we track the data separate from code in our object files (and it initializes all mutually recursive bindings correctly now, which was the actual reason for this change).
Can you explain more? Maybe I'm lacking imagination but I can't see how acid-state fits.
&gt; This story only really works for pure, total, intuitionistic calculi. Consider cbv languages with effects (like ML), where substitution only works for values, or substructural calculi like linear logic, where you won't have a display map (because linear contexts don't have weakening). In both of these settings, the straightforward fibrational account doesn't work, but you're still doing type theory. Fair enough. I did say you can usually do more in type theory, as we don't yet know how to represent certain constructs semantically. But that's just a limitation of our current understanding, not a fundamental issue. Can you clarify your comment about linear logic? Why can't we use fibrations over monoidal categories to model it? What you're saying is that the codomain fibration doesn't work, but that's not the only fibration. &gt; Producing syntax this way doesn't yield a finite, effective presentation of the theory Well, not necessarily. You can work in a constructive metatheory, for example, and you could ask, say, whether equality of contexts or terms is decidable, and recover most of the computational aspects semantically. See for example the recent work by Bezem, Coquand and Huber on the cubical set model of HoTT. This is a model of type theory defined in some (unspecified) constructive set theory, from which they extracted an actual computational model for (a variant of) HoTT. Note that no one has yet managed to achieve the same syntactically.
Hom + Haste with Haste.App would be interesting to see (hiding all explicit server-client communication)
thanks for the link to the random number game. It turned out that what I really needed was to see this random numbers used in context. 
I understand your point. but under my experience any solution that offer these functionalities with IO and STM have compromises under the hood. Tackling the awkward squad ,as SPJ called it, is inherently triky and haskell aim for purity and reason-ability does not change its nature. I hope that you or any else find something better. In fact, STM is full of IO, exceptions, locks and other compromises under the hood. What Haskell does well in this aspect is to hide them and present the good properties that the STM design assures by construction as a clean interface, despite what is inside.
No documentation at all.
The most triky thing is the insertion. the insertion with getDBRef does not perform any STM operation. However, if it is evaluated in a STM branch that is aborted, this TVar does not change things. a new getDBRef will use it if it is already. And there is the caveats that nikita mention. Concerning IO read-writes from storage, the writes are asynchronous. So they are never done in the middle of a abort-able transactions. They are done in a different STM process that return a consistent snapshot of all the changed TVars (that is assured by STM). then this snapshot is written using the instance definitions for that. (tosave, elems, size) ← atomically $ extract elems lastSync save tosave if the TVar is not filled from the database, readDBRef read synchronously, but like insertions, if the transaction is aborted the work is done and the effect is that the TVar is already filled. There is no launchMissiles call. the effects are local and idempotent. safeIOToSTM assures that the IO action finalizes by the way. 
Well, how about: How do you register? Who is running the contest this year?
I believe hackage-server itself uses acid-state, maybe a Dev from there can elaborate.
&gt; Persistent + esqueleto looks like it gets around the limitations of persistent It's not "getting around" anything - persistent is (nowadays) explicitly designed to be used together with esqueleto. Persistent and acid-state are your two high-level options if you don't want to use a direct binding to a specific database and hand-rolled marshalling between Haskell types and DB queries. Persistent and acid-state are complements of each other: In persistent, you describe your data schema with SQL-like concepts, and then persistent automatically generates code for the corresponding Haskell types. In acid-state you describe your data using Haskell types, and then acid-state automatically generates the corresponding DB schema for you. Both libraries automatically create the schema in the database and perform simple migrations when you make minor changes to the schema. We are using persistent in production and we are very happy with it. My understanding is that there are people happily using acid-state in production, too. (For one thing, the new Hackage 2 server uses acid-state.) Use the one whose conceptual approach best fits what you are trying to do. EDIT: In other comments, the developers of Hackage 2 and others using acid-state are reporting problems. I hope those will soon be fixed. EDIT: Another option is [groundhog](http://hackage.hackell.org/package/goundhog). It is somewhere midway between persistent and acid-state. In groundhog, you define your both your Haskell types and the schema manually. In the DSL for defining the schema you also specify how to marshal the data to your types. EDIT: Another important point that has become clear from other comments: Since persistent and acid-state are highly automated, they are designed to create and use their own database tables. It is difficult to retro-fit them to work with existing databases.
Whatever library you choose, I would encourage you go with a true and tested data store. Your data is the most precious thing you have, so there's a good reason to be conservative in the choice of storage. acid-state is giving us a bunch of headaches in Hackage 2. It takes too much space, can only support 1 client (!) so you cannot connect to the DB of the running server if you want to debug issues. There are no off-the-shelf tools to look at the data, the database takes minutes to start up even with our puny data set so hot-failovers won't work well, etc, etc.
Thanks! Any way to turn this long list of Applicative constraints into something like `forall a. Applicative (cat a)`? I tried that and it doesn't work as written.
Awesome, well written blog post. Great read Edsko!
It says explictly that you don't need to register. We could say who's running it though, good point.
Well, you can use acid-state in a client/server model using [Data.Acid.Remote](http://hackage.haskell.org/package/acid-state-0.12.2/docs/Data-Acid-Remote.html), but that means explicitly writing a server (not hard) and running it, which - to my mind - takes away one of the advantages of acid-state (that you can just run your application without having to worry about managing or running other processes).
Edit: this doesn't work... the identity arrow in each fibre in E would give you weakening... See [Categorical Models of Explicit Subsitutions](http://www.cs.bham.ac.uk/research/projects/xslam/wolam1/catmodels.ps) for a related approach. I think you can do all the context management using fibrations, by treating types/terms as fibred over pairs of intuitionistic/linear contexts, which are in turn fibred over intuitionistic contexts. Not saying that this is the 'best' way of doing it, but I'm pretty sure it would work, and would allow you to express things like, e.g., semantics of a type theory with linear functions, but without tensor products. You'd need two fibrations that compose: p : LCtxt -&gt; Ctxt to model the dependency of linear contexts on intuitionistic contexts, and q : E -&gt; LCtxt to model the dependency of types/terms on pairs of linear and intuitionistic contexts. In the term model, the category Ctxt would have intuitionistic contexts as objects, and simultaneous substitutions as arrows. The category LCtxt would have pairs (G, D) of intuitionistic contexts and linear contexts as objects (where D is well-formed in G), and pairs of simultaneous substitutions as arrows. The fibration functor p projects out the first component. For each intuitionistic context G, the fibre over G would presumably be symmetric monoidal. The category E would have objects (G, D, A), where A is a well-formed type in the context (G,D), and arrows are pairs of arrows \sigma of LCtxt (equivalence class of) terms G; D, x : A |- e : \sigma(B). (You might want another total category to model intuitionistic terms too, and a fibred adjunction between that and E). The fibration functor q projects out the first component. This is basically a generalisation of Bart Jacob's presentation of a categorical semantics of the simply-typed lambda calculus without product types, where he uses a fibration to handle the contexts instead of the domains of arrows. I think there is a discussion of this early on in his "Categorical Logic and Type Theory" book. &gt; [..] I've been working on a type theory integrating linear and dependent types, and if you're interested I'll send you a draft when it's ready. I'd be interested in a look at that too.
It seems that all this videos by SPJ is on youtube now: [Classes](http://www.youtube.com/watch?v=6COvD8oynmI) ([slides](http://www.cs.uoregon.edu/research/summerschool/summer13/lectures/ClassesJimOPLSS.pdf)), [Kinds and GADTs](http://www.youtube.com/watch?v=brE_dyedGm0) ([slides](http://www.cs.uoregon.edu/research/summerschool/summer13/lectures/Kinds_and_GADTs.pdf)), [System F in GHC](http://www.youtube.com/watch?v=2IZQx7WNOMs) ([slides](http://www.cs.uoregon.edu/research/summerschool/summer13/lectures/FC_in_GHC_July13.pdf)), [type functions](http://www.youtube.com/watch?v=XtogTwzcGcM) ([slides](http://www.cs.uoregon.edu/research/summerschool/summer13/lectures/FunWithTypeFuns.pdf)), [slides on type inference](http://www.cs.uoregon.edu/research/summerschool/summer13/lectures/Type_inference.pdf).
Those benchmarks demonstrate the performance vs. _backbone_, where the speedup is a results of not rendering on every change, but batching them up with `onAnimationFrame`. They do not demonstrate performance with react, which _does_ make this optimization (but pays the cost of structural equality comparisons).
Great post. I especially like the "further reading" part since I'm currently learning GHC RTS, execution model and memory layout(how heap objects/continuations represented etc.) :-)
I think postgresql-simple is much safer than Python + SQL. Sure you still need to be careful to write correct SQL queries but conversion to and from SQL values is automatic and convenient. As it's "simple" it really doesn't get in your way either, and is easy to understand (and fast too!). I usually isolate all of the database code into a separate `Model` module and develop unit tests for that module. Simple unit tests catch most trivial SQL &lt;-&gt; Haskell interaction problems. The rest of you app would use types and functions exported from your `Model` without dropping down to explicit SQL queries elsewhere. Unless you have a *lot* of database code, I don't think this arrangement is particularly bad. The rest of your app will be completely type-safe while your database access layer has full SQL freedom. 
This was a great and very detailed post! For the TLDR crowd, though, the culprit was this: pre-7.8 ghc had an issue where stack overflow occurred when asynchronous exceptions were masked -- It would always grow the stack rather than terminating when stack size exceeded a threshold value that would normally trigger the "stack overflow" termination. Tweaking the code changed whether the stack hit the limit inside hPutChar, which masks asynchronous exceptions. So, the non-terminating code would have terminated eventually, but it would have required exhausting the heap with continuation pointers on the stack, which I assume would take quite a while with a modern machine. Post-7.8 behavior in a similar situation is for the stack overflow exception to occur as soon as asynchronous exceptions are unmasked.
That's true. So, it may not be for general consumption but you do have the source and examples. Isn't that better given that it's not a large code base?
In theory you should now be able to just cabal update and install with the newer transformers-compat flag-free. We had to rename the flags to swap the order of transformers2/transformers3 lexicographically (to three/two respectively). This enables cabal's backtracker to solve them properly. ಠ_ಠ
Completely agree. We moved away from acid-state (happstack-state) at Silk for the same reasons. I wish we had communicated this experience more clearly back then, maybe that could've prevented a similar path with hackage.
Perhaps I'm missing something, but doesn't `getDBRef` call `H.insert` without any synchronization? It seems like `getDBRef` can result in a reference that doesn't end up in the cache at all as the hashtable underlying it all isn't thread safe.
This is where you and I disagree. Source code to me is the best documentation. It's not an issue of debate since it's just my personal preference. 
That's nice! It has a lot fewer dependencies, how big is the generated code? 
A little note - that's the Spineless Tagless G-machine. http://research.microsoft.com/~simonpj/papers/spineless-tagless-gmachine.ps.gz 
Heheh. Great!
The one issue that kept me away from persistent and moved me into groundhog (which has its own set of problems) is persistent's inflexibility. In groundhog all the knobs for the code generation are available but with reasonable defaults. I tend to play around with replacing existing (rails) apps with Haskell so persistent's lack of customization for mapping is a non starter. Also no joins on either one of the two last I checked.
For modern machines, I really wish the default stack size were much, much bigger. In fact - is there an RTS option to do exactly that, keep growing the stack until the heap is exhausted?
This is a fascinating tour deep into the internals of GHC for the uninitiated. Thanks for a wonderful post!
Sure, whatever.
&gt; It takes too much space, can only support 1 client (!) so you cannot connect to the DB of the running server if you want to debug issues. If you use the Data.Acid.Remote module you can support many clients, and also connect to the DB of the running server and inspect it w/ queries / updates. Running acid-state on the same box as the web server probably contributes to a lot of disk and RAM usage. The remote module let's you separate it out. &gt; There are no off-the-shelf tools to look at the data... Those tools can be built. Do you need more than an HTML grid that modifies the state w/ AJAX calls? &gt; the database takes minutes to start up even with our puny data How large is your on-state disk? I use your ekg package to inspect the amount of current bytes used in RAM and it's nice so far. The only issues I've run into is sending large bulk data from a remote acid-state. Like sending 50k+ records to the client. Implementing paging and searching (w/ prefix tries) would probably be the way to get around it. Why not port hackage to use the remote module? 
Joins for persistent come from the [esqueleto](http://hackage.haskell.org/package/esqueleto) library. Persistent doesn't map anything by design. It creates new Haskell types that exactly reflect the data structures you specify in your schema, and it marshals your data back and forth automatically. So all the knobs are in your hands - you can map those types to your heart's desire using pure Haskell functions. EDIT: Groundhog and persistent each have strengths and weaknesses. Here is a [cafe post](http://www.haskell.org/pipermail/haskell-cafe/2013-November/111475.html) by the author of groundhog, taken from the thread linked by /u/bergmark.
Sounds like it's in the works 
To clarify for anyone else who reads the first part but gets bogged down in the details in the middle: when the author says "it never crashes" and "it will run forever" when `THRESHOLD` is 24, what they really mean is that the stack grows much larger than the maximum stack size imposed by the computer. The program still continues to take up more and more memory, and the author later notes that "this program will continue growing the stack unpunished, until we run out of machine memory completely." So, the program does crash and doesn't run forever, but it takes much longer than you'd expect if you try to give it a maximum stack size.
 Maybe a is isomorphic to Either () a like this Isomorphism (maybe (Left ()) Right) (either (const Nothing) Just) In other words, Nothing is like an Either where the error is always (). It all matches up so nicely.
I don't think that Haskell Reddit is a good place to push your political agenda.
Whoops, quite right :) Fixed, thanks. 
The knobs I'm referring to are mapping to an existing database schema, especially one which may have a different column naming scheme (underscoring instead of camel case) than you'd want in your haskell program, or possibly even a different table naming scheme. For my work stuff, I'd have an easier time (though I may still fail due of this pervasive thought at my company that haskell is "too hard") replacing an existing Rails API with a *much* faster and extremely safe haskell implementation on existing data than just starting out from scratch. That's what I liked about groundhog: it gave you enough freedom to do this mapping without having to completely abandon all the savings you got with TH/QQ.
Those are good points. I can comment about a few of them: * The ability to declare foreign relations across different QQ blocks and verifying them with migrations. A common workflow for persistent is to declare the schema in a separate file and instantiate it all at once in a `Model` module. With that workflow, this is obviously not an issue. Why do you need to separate your model into separate QQ blocks that refer to each other? * There is no support for non-int primary keys The type of a key is `PersistValue`, so in principle it can be anything. For example, a postgresql uuid could be used via the `PersistDbSpecific` constructor. The culprits here are the PersistFieldSql and PersistPathPiece instances for `KeyBackend` `SqlBackend` `entity` - they fix the key type as Int64 for `SqlBackend`. It's not hard to roll your own version of `SqlBackend` with a different key type. But it would be nice if persistent had a built-in way to do that. If you want different primary key types for different tables within the same database, that is already a bigger feature to add to persistent. * It's not flexible in column types, Text maps to character varying (but not text), and serial to int8. That's only for the default instances of `PersistField` and `PersistFieldSql`. You can newtype them and define instances that render them however you'd like, including backend-specific SQL types.
Looks like John provides [`unsafeShader`](http://library.elm-lang.org/catalog/johnpmayer-elm-webgl/0.1/Graphics-WebGL#unsafeShader) for exactly this purpose. If you can create shaders dynamically from strings, making a safe EDSL should be possible.
Thanks for the comments! &gt; Why do you need to separate your model into separate QQ blocks that refer to each other? I'd say, for the same reason you don't put all your source code in one file. Different parts (packages) in our stack are responsible for different tables in the db, but they have dependencies on each other. I'd also want to be able to give fields names that are qualified import friendly, I added some settings for this to persistent but using this means you need to put code in different blocks. &gt; The type of a key is PersistValue, so in principle it can be anything One step forward! :) I don't think this was present last time i checked. &gt; If you want different primary key types for different tables within the same database, that is already a bigger feature to add to persistent. We have this. Ints, UUIDs, composites. We could of course change all atomic PKs to be ints, but we wouldn't be able to gradually introduce persistent. &gt; That's only for the default instances of PersistField and PersistFieldSql. You can newtype them and define instances that render them however you'd like, including backend-specific SQL types. This again unfortunately means big-bang changes to our code base, we'd have to add tons of wrapping and unwrapping code, ideally the db layer would have close to no effect on the rest of the code base. 
IFLAT looks nice! I shall wrestle it post LYAH and Write Yourself a Scheme. Thanks :)
For the past while I feel like we've had suboptimal status reports, and most of the time it ends up being me or Gershom running around telling everyone what's going on in the event something went wrong, or if something will happen. Now, the internet can tell you what's wrong automatically! With categorization! But seriously, this has been something I've been meaning to do for a while at this point since we've had some outages in recent memory. We're also doing a lot of various infrastructure improvements, so we can highlight any expected disturbances here. Thanks go to [status.io](https://status.io) for taking most of the boring work out of it because I also don't know how to HTML. PS: This thing also has a [Twitter account](http://twitter.com/HaskellStatus) and a bot (called `statusbot`) sits on `#haskell` on Freenode that are automatically updated upon events occurring, if you're into those kinds of things. Should help get the word out faster, I think. Expect other good news soon!
I really hate font APIs. Other than that: good job! This is really useful.
This one, slightly different, produce identical output than the console application, with all the history of previous interactions. The only difference is that it does not uses wcallback, which erases the previous entries. It uses a normal monadic bind (do notation). wcallback and &gt;&gt;= have identical signature. It also uses setTimeouts to end the session thread after a timeout. Otherwise , Mike told me, the GHCI interpreter, used by School of Haskell, has a bug that will not delete them and will stay even if the code is changed and recompiled. module Main where import MFlow.Wai.Blaze.Html.All import Game -- loop that actually plays the game. play :: Game -&gt; String -&gt; View Html IO String play g@(Game l) m = do x &lt;- toHtml (prompt m l) ++&gt; br ++&gt; getInt Nothing &lt;! [("autofocus","1")] &lt;++ br case move g x of g'@(Game _, _) -&gt; uncurry play g' (_, m') -&gt; return m' -- Main entry: play the game and announce the results main = runNavigation "" . step $ do setTimeouts 20 20 m &lt;- page $ play (Game 20) "Hello. " &lt;** pageFlow "s" (submitButton "send") page $ toHtml m ++&gt; wlink () &lt;&lt; " Another game?" A submitbutton has been also added
I am currently working on exactly the same project to learn Haskell. Looks like someone beat me to it.
Thanks Michael, glad you like it!
Might also be good to note this is using the remote module to allow acid-state to run in its own process. So you can execute this query on as many servers as you have states. So there's the "clustering" part. Hope that helps. 
Thanks, great post!
 {-# LANGUAGE DataKinds #-} {-# LANGUAGE TypeFamilies #-} module Test where data Nat = S Nat | Z type family Prev (x::Nat) (y::Nat) a b :: * type instance Prev (S x) y a b = Fancy x y a b type instance Prev Z y a b = Fancy (S y) (S y) b a data Fancy (x::Nat) (y::Nat) a b = Nil | Cons a (Prev x y a b) example :: Fancy Z Z Char Int example = Cons 'x' (Cons 1 (Cons 2 (Cons 'y' (Cons 'z' (Cons 'w' Nil)))))
 {-# LANGUAGE TypeFamilies, DataKinds, GADTs #-} import Data.Proxy data Nat = Z | S Nat data SNat (n :: Nat) where SZ :: SNat Z SS :: SNat n -&gt; SNat (S n) class SingN (n :: Nat) where singN :: Proxy n -&gt; SNat n instance SingN Z where singN _ = SZ instance SingN n =&gt; SingN (S n) where singN _ = SS (singN Proxy) type family Next a b left next where Next a b Z next = Fancy b a (S next) (S next) Next a b (S left) next = Fancy a b left next data Fancy a b (left :: Nat) (next :: Nat) = Nil | (:&gt;) a (Next a b left next) infixl 5 :&gt; fancyMap :: (SingN left, SingN next) =&gt; (a -&gt; a') -&gt; (b -&gt; b') -&gt; Fancy a b left next -&gt; Fancy a' b' left next fancyMap f g xs = go (singN Proxy) (singN Proxy) f g xs where go :: SNat left -&gt; SNat next -&gt; (a -&gt; a') -&gt; (b -&gt; b') -&gt; Fancy a b left next -&gt; Fancy a' b' left next go left next f g Nil = Nil go SZ next f g (a :&gt; xs) = f a :&gt; go (SS next) (SS next) g f xs go (SS left) next f g (a :&gt; xs) = f a :&gt; go left next f g xs fancyTail :: Fancy a b left next -&gt; Next a b left next fancyTail Nil = error "empty Fancy" fancyTail (a :&gt; xs) = xs EDIT : I removed the singletons TH boilerplate generation, because I believe this way it's less opaque to those unfamiliar with the library. Anyway, `SNat` and `SingN` could be readily TH-ed away with `Data.Singletons`.
You can even close the type family type family Prev (x::Nat) (y::Nat) a b :: * where Prev (S x) y a b = Fancy x y a b Prev Z y a b = Fancy (S y) (S y) b a 
I would not, no. My point was that if not every US tech company is Lockheed Martin or Raytheon then the same can be said of Israeli companies.
I know this is a little old, but hopefully you will be able to help me out, just because I'm curious. What specifically is the problem you are trying to solve? Are you hoping to develop some sort of type framework for writing PDE solvers? I work in the PDE/ODE solving world (I simulate ultrafast quantum mechanical systems as a physics graduate student), so I understand a lot of the PDE part, but I'm still new to Haskell, so I'm trying to understand the Haskell part. Thanks!
`ST`, for example, is a use of "untracked `IO`". I don't agree with this though: &gt; in a strict language, we could in theory define two versions of `map`, one which takes a strict function, and another which takes a non-strict function There would be no point defining two `map`s. In a strict language if you have a `[a]` then all the `a`s are already fully evaluated, so there's no point having a `map` which takes a non-strict function. Instead just pass `map` a function of type `a -&gt; () -&gt; b` or `a -&gt; T b` where `T` is some thunk type constructor.
If you're trying to do something non-trivial with randomness, you should have a look at [MonadRandom](http://hackage.haskell.org/package/MonadRandom-0.1.13). It helps you create random functions that are easily composed and saves you the trouble of passing the state of the generator around.
I'd say for large data, that is not very "relational", and that needs to be accessed from (potentially) many clients, Riak is a good contender as well. Maybe the `safecopy` (from `acid-state` fame) can be used together with Riak to allow on-load/on-the-fly migrations of you data whenever data types evolve.
If you want to have uncontrolled effects, `unsafePerformIO` is there for you. Though there are reasons people recommend not using it.
&gt; The only argument for this the author gives doesn't apply to file and network IO: you don't lose anything by tracking them, thanks to &gt;&gt;=. You're saying there is no overhead to making pure code monadic? Have you never written pure code, then later plumbed a monad through? &gt; but what super-secure process of fetching data from an URL guarantees that the external server won't be down? And no, fucking cosmic rays are not a serious issue in comparison with that, as the research paper he links to confirms. First of all--dude, relax. Second of all, the external site being down is just another form of partiality, 'in principle' no different than the bottom value that's already included in all pure types in Haskell! &gt; Frankly, I don't see the point of this article. The post was a thought experiment. Maybe try using your imagination a bit instead of immediately naysaying. :)
I see your point, but I still think what I said holds. There should at least be a readme saying "this is work in progress and only for myself" or something when it is linked (or linkable) publically. But even then, it's suffering from something that I've seen a bit too often with Haskell libraries: no documentation. The author himself won't be able to use this library in a couple of months because people forget these things, and then it's "reading *and understanding* a handful of source lines", which is slower to understand than a remark in a comment by orders of magnitude. I firmly believe documentation is a crucial part of writing code that is supposed to be used, just like trying to make it bug-free is a virtue. It is not something to be postponed or even left away entirely. (I have yet to see a single purposeful module that has "self-documenting code".)
Code documents nothing but the existence of code. It does not show purpose, necessity, motivation, tradeoffs, tricks, pitfalls. Sure, operationally it's all there, but by dissecting a cake I can hardly find out why it was made this way and whether it's what I want. Newtype record unwrappers can live without code, but what does `sort :: Ord a =&gt; [a] -&gt; [a]` plus implementation tell you? It's hardly more efficient to analyze (!) the source than to read "stable sort suitable for small to medium list lengths (1-10k elements)".
I don't think the academic community in r/haskell welcomes post embracing an apartheid country. Notable academics such as professor hawking and Chomsky decry that regime actions. Defending human rights is not a political agenda. So maybe you join(if not already done) hasbara so you learn better techniques to make better lame comments.
An investigation regarding the company background should be considered before hailing a "job opportunity" in an apartheid regime. Keep in mind that the country in question is militarised and unlike other countries, the apartheid culture is flourishing. contemplate the origins of the academic boycott in SA.
But what if status.haskell.org is down? Who watches the watchers!?
The turtles, obviously.
Hi Duncan, is it just those few of you running the contest or is Well-Typed involved at all? I guess my question is: are you getting paid for your time working on it?
It would be really nice if there was something as simple as clusterMap f data nodes and have it work. I have some stuff in the works where, at least for simple parallelism, it is easy to have some program populate jobs in some queue, perhaps using SQS, and have a client program which reads from the queue, does the work, submits results somewhere, and when there is nothing on the queue, exit. 
Is all that work necessary? Would readFile be bad here?
That would be awesome. Have you looked at lambdacube? It's gone on hackage recently. I think it has shader abstractions that you could use.
Make sure you show em to us!
I find it's easy to accidentally leak file descriptors with `readFile`. `NFData` probably mitigates the problem enough that it no longer matters, though. Alternatively, using `readFile` you can force the filehandle to close after the fact with `evaluate $ length s`, but that keeps the stream in memory even if `parser` consumes some of it.
Nice! But the title of the HTML version hasn't been updated.
Constant memory use is usually pursued using a streaming library. It doesn't make Haskell instantly appropriate for embedded use but makes it a damn sight leaner than most GC runtimes.
Don't, it could very well be abandonware. That or join forces.
Could it be implemented using an extension or a compiler plugin? How is it done with ST and Par? Also this makes me wonder, are there libraries that abstract over unsafePerformIO to make it a bit safer?
Done.
It already uses buffers under the hood, but the API to the users acts as if the mesh were triangles. Basically, if you construct your mesh as a set a triangles, where each vertex has 4 attributes, it is actually converted to 4 buffers, and cached on the GPU if the mesh data doesn't change.
`yield` is a special case of the `respond` command, which returns the argument of the next `request` from downstream. `await` is just a synonym for `request ()`(i.e. a request with an empty argument) so a `yield` is waiting for the `()` that `await` sends upstream. You can write a unidirectional variation on `pipes` where there is no upstream flow of information to allow asynchronous pipes, and if you did so you would reinvent `pipes-concurrency`.
&gt; what we track as an effect is very much dependent on the assumptions that we as programmers choose to make. This is an excellent point. It would be very inconvenient for a 404 error to be permanent even across multiple "attempts" to perform a pure http fetch. Likewise, it would be inconvenient for writes to be discarded due to some compiler optimization. Counterintuitively, the IO type exists for your *convenience*, and not the other way around.
&gt; are there libraries that abstract over unsafePerformIO to make it a bit safer? What about it do you imagine could be made safer?
Huh, haskell.org uses phabricator. Who knew!?
Got that. You can't really override Producer to allow this. I still feel like `pipes-concurrency` is too verbose for this case, and a little abstraction might be useful for small streaming programs. Could it be possible to do something like this? `main = runEffect $ factorialsFrom10000 &gt;-&gt; buffer &gt;-&gt; waitForEnter &gt;-&gt; P.stdoutLn` 
Every composition operator has at most one active pipe and therefore any chain of compositions will transitively have at most one active pipe. See [this Stack Overflow answer](http://stackoverflow.com/a/23187159/1026598) for more details. This property is completely independent of the underlying pipes you connect, so there is no `buffer` pipe you can write to circumvent this restriction. You would have to define an entirely new operator to get your desired behavior.
Yeah, I'm not actually advocating doing this in Haskell code, it really was more of a thought experiment. :) I am interested in thinking more about smarter I/O managers (with perhaps a different, more restricted model for I/O) that insulate programs from the finite supply of file handles and the like.
Bob: Don't you find those fictitious dialogs annoying? Alice: I sure do especially when one of them just rolls over and agrees to everything. But in this case, pchi's post is really and ad. for his Scala services and book. 
We just set that up today. In fact when I first posted this thread, it wasn't there! Don't use it yet though, we're experimenting with it, but so far we're pretty happy. Caveat emptor, everything may get deleted, etc. Like I said, more good news coming soon!
I learned very little of logic (only classical and first order logic). But from what I got from it it's (usually?) easier to use proof theory to prove a consequence, while it's much easier to use semantic arguments to disprove one. That is: to show that Γ ⊬ p you would need to prove that there is *no* proof tree with Γ open leaves and p as root (to prove Γ ⊢ p, in comparison, you only need to give an example of such tree). But you can prove Γ ⊭ p by just providing an example where Γ is true and p is false. But then I spent most of time in classical logic, where syntactic and semantic consequences are equivalent.
WebGL implements OpenGL ES, which only supports buffer objects for passing data to the GPU (no [immediate mode](http://stackoverflow.com/questions/6733934/what-does-immediate-mode-mean-in-opengl)).
Rudeness with a smile. 
This is really nice.
I don't know where all the hate is coming from. Nothing wrong with thought experiments, and nicely presented as well. 
I thought you wanted to make it safer. That sounds like ways to make it more dangerous.
`ST` *is* a library that abstracts over `unsafePerformIO`.
It's his website, which happens to contain both blog posts and a description of his job. What's the problem with that?
One of the guys behind lambdacube (Csaba) is actually at Prezi, so John (/u/vladley?) and Csaba and I have definitely talked about that approach. As I understand, it uses some of Haskell's fancier language extensions to make strong guarantee with types, but that is getting into type level integers which Elm does not have. So it'd be awesome to have, but I don't know if there is a practical path to getting such strong guarantees. Perhaps we can rephrase things in a way that gives acceptable guarantees though?
I still don't see it. If your stream type is data Stream a = T (Stream' a) data Stream' a = Nil | Done (T a) (Stream a) then `fmap` is fmap :: (a -&gt; b) -&gt; Stream a -&gt; Stream b fmap f xs = delay $ \() -&gt; case force xs of Nil -&gt; T Nil Done y ys -&gt; Done (fmap f y) (fmap f ys) What second `map` were you thinking of?
&gt; what we track as an effect is very much dependent on the assumptions that we as programmers choose to make. More formally I think you could say something like "what is considered pure is very much dependent on the denotational semantics we give our program". In Haskell memory usage is not part of the denotational semantics and thus does not need to be tracked as a typed effect. Likewise there is no denotational difference between forced values and thunks, and between code evaluated in sequence and in parallel. Partiality is a bit subtle because although *is* part of the denotational semantics its presence doesn't violate any of the conditions needed to be pure (I believe). Imprecise exceptions don't fit into this model so well, I don't think. You could say they genuinely are an example of not tracking an effect.
Good to know, thanks. Ricky has told me updates themselves are actually technically pretty smooth, but I've followed Phab for a while and yes, it seems to change quite rapidly. We wanted something to help plan and keep track of all our admin work (so we also needed something with RBAC), so we're trying it out. But so far I really like it.
When someone is trying to sell you something--Scala services and book--the actual content is secondary. You've just been duped.
status.status.status ... status.haskell.org
On hindsight, I *was* a bit of a dick. Sorry. I still don't see the point of this. &gt; You're saying there is no overhead to making pure code monadic? Have you never written pure code, then later plumbed a monad through? Yes, when I actually wanted the effects in the middle. With both file and network IO you were talking about reading from immutable sources, so the order of effects doesn't matter, so you can do `effectfulOperation &gt;&gt;= \a -&gt; whatever`, and write all things that require them functions on the file, because that's what they are. &gt; Second of all, the external site being down is just another form of partiality, 'in principle' no different than the bottom value that's already included in all pure types in Haskell! "In principle", but in practice *very* different. Modulo cosmic rays or other weird failures, if you run a pure program twice and it gives a proper result the first time, it will give a proper result the second. This is not the case with network IO, as the server could be down and you could be without internet connection. Also, if you are *really* sure that file and network IO reading are pure, you can already use it like that: it's called `unsafePerformIO`.
Paul has been part of the FP community round these parts longer than you have. The fact that he has a book doesn't diminish that. Anyway, if he did want to sell us anything, he sure wouldn't advance an opinion like that in _this_ post :-P
This is precluded by the assumptions made in post. Network with no partitions. Remote server always available. The bytestring lib is covered in 'unsafe' reads.
I think he is talking more about the situation, say in f#. You have Int vs Lazy&lt;Int&gt;. One needs to be explicitly forced before it can be used, the other not. If you wanted to handle both pure and lazy tracked in such a way you would need two functions or require all values to be passed in via a functor and use identity to wrap evaluated values.
&gt; yes i know lazy io is evil You're saying that in a code snippet that also uses `unsafePerformIO`? Anyway, lazy IO isn't evil if you understand its limitations. What's evil is when you say to beginners, "Here's our IO, it works just like IO in the other languages you know. And hey, look, it works well together with Haskell's laziness. Isn't that so cool?" and then walk away snickering quietly.
I still don't get it. If you want your values to be strictly evaluated you pass `map` a function of type `a -&gt; b`. If you want your values to be lazily evaluated you pass `map` a function of type `T a -&gt; T b`. (Here I'm using the notation `T Int` instead of `Lazy&lt;Int&gt;`.)
Again good points. In summary - highly automated libraries like persistent and acid-state are great if you are starting out from scratch, but difficult to use with pre-existing databases.
Oh, OK, that's right. To achieve their automation, persistent and acid-state are designed to create and use their own database tables. If you need to retro-fit to an existing database, they are not good options.
tls and most of its dependencies are not maintained by Michael BTW.
This is *really* sad that you're unable to be constructive and report the limitation to the issue tracker or to me directly. It would have taken probably 10 minutes to push upstream a workaround validation function that include your case, while a more generic solution can be planned from there.
Does the "Last updated 4 hours ago" mean that was the last time something changed or that that was the last time something checked if the services are available?
there are very different opinions regarding that conflict. please stop.
The shaders described in [LambdaCube 3D](http://lambdacube3d.wordpress.com/) EDSL are pure functions, so they are composable, but they are different from Spark's OOP approach.
&gt; Second of all, the external site being down is just another form of partiality, 'in principle' no different than the bottom value that's already included in all pure types in Haskell! I disagree. A partial function in Haskell is always either bottom or non-bottom for a fixed argument. But in code like `readUrl url + readUrl url` the server could go down in between, which breaks referential transparency. Also, tracking hardware failures of memory and CPU (like x-ray or power outage) would be very nice. It is not done, because it would make programming really cumbersome, the possibility is very low and there are no good ways to recover anyway. Harddisk and network failures on the other hand have a much higher probability and tracking them and reacting to them is a lot easier. Moreover, there are many examples of errors, which are the result of the programmer’s assumption, that a certain file “will always be there”, “will always be readable” etc. (especially in languages without exceptions). We should not make the same mistake, just because reading a file or fetching an URL would be pure under very narrow assumptions, especially when Haskell makes tracking of effects so easy.
I would have if I were satisfied, but on further inspection while it was working, the memory consumption grew to about twice the size of the input. Honestly, not bitching or anything, but while I enjoyed dabbling with Haskell and learned a lot, it's not for me. I rewrote the code in lua and ran with luajit: easier to maintain, tiny memory footprint, blisteringly fast. If I wanted this much pain with manual memory management I'd be coding in C! Learning Haskell basics has made me a better programmer, though, and given me an appreciation for functional idioms. Worth the hassle for that alone. :)
In both cases, Parsec is ignoring the second element of the pair (because it never got that far), and correctly reporting that one of two possible problems might be the cause of the error with the first element: either there aren't enough digits, or the digits it found are out of range. How could Parsec know that adding more digits would never fix the out of range problem? Here's a way to write it that might give you more what you expected: -- | Parse up to the given number of items upTo :: Int -&gt; Parser a -&gt; Parser [a] upTo 0 _ = pure [] upTo n p = ((:) &lt;$&gt; p &lt;*&gt; upTo (n-1) p) &lt;|&gt; pure [] parseWord16 :: Parser Word16 parseWord16 = do many $ char '0' n &lt;- read &lt;$&gt; upTo 5 digit guard (0 &lt;= n &amp;&amp; n &lt;= 0xffff) &lt;?&gt; ("Word16 literal in range 0..0xffff ("++show n++")") return $ fromIntegral (n :: Word32) This way, since out-of-range only happens with the maximum allowable number of digits, Parsec knows that adding more digits can't possibly help. &gt; parse (parseW16Pair &lt;* eof) "" "123456" Left (line 1, column 6): unexpected "6" expecting "#" &gt; parse (parseW16Pair &lt;* eof) "" "92345" Left (line 1, column 6): expecting Word16 literal in range 0..0xffff (92345) 
No, it's not up to date. It's almost completely the same as the November 2013 edition (which surprisingly now contains references to the ghc 7.8 release!) and most, if not all of the articles are old (e.g. the reactive banana article, which I remember reading a few months ago, references version 0.7.1.3 as the newest, but that version is almost a year old). Something went pear-shaped in the editorial process. 
I don't think anyone ever gets paid for running the ICFP programming contest. It's all for the glory! ;-)
The same would be said about apartheid in SA or slavery. This is no reason to "*stop*".
I like your blog post. I think questioning common assumptions is a valuable use of time. However, I don't think avoiding monadic style needs to be the goal here (and I don't think you intend it to be, but since it came up in this comment thread, I'm just stating this explicitly). &gt; You're saying there is no overhead to making pure code monadic? Have you never written pure code, then later plumbed a monad through? I see this argument against monads a lot, but it doesn't make sense to me. I have never really had to do this; just using the existing code with `liftM` is enough for me. The implication this argument makes is that it's commonplace to have a function that doesn't use `IO` and turn it into one that does; why would one ever want to do that? I have asked this question of others before, and their answers actually seemed convincing at the time, but the more I thought about them later the crazier it seemed again. Perhaps it's something peculiar about the way I write code or the kind of projects I work on. I think one of the answers I got was in the form of an example: you might want to add logging (not just `Debug.Trace` type stuff, but actual production logging) to an existing function that otherwise doesn't perform any IO. It made sense to me at first. However, after thinking about it more, because it didn't quite sit well with me, I realized that plumbing IO through the function is not what I would do, or at least not what I would think of myself as doing. I would instead split/invert/restructure the function so that all the IO can still sit on the outside and "drive" it. Usually this means just breaking it into a couple parts and using them in a *new* function that uses `IO`. Sometimes it means returning the data to log alongside the result, for somebody else to log elsewhere. Perhaps some would say these is effectively the same thing as "plumbing," that I'm just deluding myself into thinking I'm not plumbing. Perhaps they would even be right. However, my discomfort with thinking of it that way makes me believe that this is a pretty different way to approach the problem. It has some benefits, too, since it encourages me to decompose my code into smaller, more modular, more easily testable pieces.
&gt; Modulo cosmic rays or other weird failures, if you run a pure program twice and it gives a proper result the first time, it will give a proper result the second. This is not the case with network IO, as the server could be down and you could be without internet connection. Your argument is based on the premise that the server being down is not a "weird failure" but some sort of a "normal" failure, but "weirdness" is a subjective/situational judgement call, so I don't think it makes for a strong, logical argument here.
I reported this to /u/vincenthz here https://github.com/vincenthz/hs-tls/issues/69 and he was most grateful! 
Well, we hope that the contestants will have fun! We've certainly had a great time putting it together :-)
&gt; I disagree. A partial function in Haskell is always either bottom or non-bottom for a fixed argument. Not actually true. `pureFn blah` could fail with an out of memory error, or a stack overflow, or 'gamma ray corruption', the second time it is called. But we don't model this possibility explicitly of course. There is nothing in principle different about `readURL url + readUrl url`! &gt; But in code like readUrl url + readUrl url the server could go down in between, which breaks referential transparency. Actually, does it? Let's think about this: let x = readUrl url in ctx x x -- vs ctx (readUrl url) (readUrl url) Do these programs mean something different? We would need a `ctx` that does not produce bottom which produces different results in the first case vs the second. Can you construct such a `ctx`? 
Possibly related: normally, the HCAR editors give pretty fair warning to past contributors when they pick a deadline for a new HCAR; but this time, we got less than a week's notice. It's possible that many articles' authors just didn't find the time in one week to update their entry.
There is a "Code" link on the site. It points to the [github project](https://github.com/ehamberg/9m), where we see that this is a [scotty](http://hackage.haskell.org/package/scotty)/[acid-state](http://hackage.haskell.org/package/acid-state) site.
Sorry ... stupid me - thanks!
&gt; I like your blog post. I think questioning common assumptions is a valuable use of time. However, I don't think avoiding monadic style needs to be the goal here (and I don't think you intend it to be, but since it came up in this comment thread, I'm just stating this explicitly). Okay, I'm glad you get that 'avoiding monadic style' was not my goal. I probably should have made that more clear up front. &gt; I see this argument against monads a lot, but it doesn't make sense to me. I have never really had to do this; just using the existing code with liftM is enough for me. The implication this argument makes is that it's commonplace to have a function that doesn't use IO and turn it into one that does; why would one ever want to do that? I have asked this question of others before, and their answers actually seemed convincing at the time, but the more I thought about them later the crazier it seemed again. Perhaps it's something peculiar about the way I write code or the kind of projects I work on. This is a good point. I'm not sure I have a good way to characterize the situations where converting to monadic style requires cascading signature changes in other functions in your codebase, but it does happen sometimes. It's something like: even if you do not literally update the existing function 'in place', but call it from a wrapper, you may still have to update callers to use the wrapper instead, and make related changes to their signatures, etc. 
Nice. I'll have to start using this. 
In a hypothetical strict Haskell with optional laziness, they would be: fmap :: (T a -&gt; b) -&gt; Stream a -&gt; Stream b fmap :: (a -&gt; b) -&gt; Stream a -&gt; Stream b The first takes elements unevaluated, the second takes them evaluated. In (actual) Haskell, we don't need the first variant. In Scala, say, we get the second variant only, even though the first is still useful. Of course, we can add explicit thunking at the element level and have the function accept and/or return thunks, but that's extra work.
I love the issue open on Github
Haha quite. First thing that it made me think is that I wonder whether you could somehow get a fixed point in the minifier.
I blog about FP because I love the subject and enjoy thinking about it and writing about it. That's also why I've worked on the book. I do currently make a living doing FP consulting. Is there something wrong with the fact that the book and my consulting is mentioned on my site? Is it just that it comes off too pushy on the site?
&gt; It's possible that many articles' authors just didn't find the time in one week to update their entry. That was definitely the case for me (reactive-banana, threepenny-gui). 
Well, at least I still have reddit :-(
How can we write `mapPrev :: (a -&gt; c) -&gt; (b -&gt; d) -&gt; Prev x y a b -&gt; Prev x y c d`, a seemingly-necessary component of the analogous `map` for `Fancy`?
I'm of the opposite opinion. Why aren't we tracking partiality and allocation as effects too?
&gt; There is nothing in principle different about `readURL url + readUrl` url! There is nothing different in principle. What I tried to say is, that I would reason the other way round: it would be nice to also track memory error, stack overflow or gamma rays the same way we track other failures. That is not done for reasons I mentioned above. When programming, we have to make some assumptions about our environment, preferably as few as possible. Currently these assumptions are: memory and CPU will always work as expected. This is not always true in practice, but without assuming this, we could not even assume, that the machine code is executed correctly. Luckily, the possibility for a CPU or memory failure is very low, so we are not concerned for normal programs (in other applications, like programming a space probe, gamma rays are indeed a problem and need to be considered). However, the fact that we do make assumptions, which have a small chance of not being true, does not justify to freely add other assumptions with much higher probability, because there is “nothing in principle different”. Think about how often you probably had a bit flip, which was not corrected by the memory’s checksum algorithms, and how often your computer had no network connection. &gt; Can you construct such a `ctx`? ctx a b = print a &gt;&gt; print b 
Yeah that's pretty much what I got from it too. It's pretty much a design decision to allow these things. Groundhog seems to be more flexible in this regard, but no joins is also a no-go for us.
The server being down or the computer having no internet connection are much more likely than a cosmic ray, and more importantly, the behaviour whenever that happens is that you can still reason about what to do whenever any of those happen: if it's not possible to reach the server you can raise an exception which can be catched, if a cosmic ray flips a bit all bets are off. That's what I mean with "weird": it's no use representing the cosmic rays in the type system, because you won't be able to do anything about it when that happens(incidentally, we don't track things that allocate memory dynamically or may blow up the stack because of the same reason). File IO is more defensible, but you still have the problem of mutability of data source.
&gt;There is a difference between a program running out of memory at run-time because the language deliberately gives you enough rope to hang yourself; and the compiler running out of memory at compile-time because it can't efficiently handle a short, albeit contrived, sequence of code. The short piece of code triggers combinatory explosion. Yes it's short, but 2^N can exceed reasonable bounds for even relatively small N. 
The point (which I think is an important one) is that there is no God-given classification of side effects into Pure and Impure, into observable and unobservable. I can observe the space and time usage of a program with `top` as easily as I can observe when it does file IO with `strace`. We allow certain kinds of effects (and not others) from pure functions because they don't change the meaning of the program, whether the program is "correct"; but "correctness" depends on the purpose for which the program was written. Whether it affects correctness is not an absolute intrinsic property of a kind of effect. For example if I'm working on an Agda compiler and it needs 64 GB of RAM rather than 2 GB of RAM on my input files, it's no longer correct for my purposes. If it takes 10 hours to compile rather than 10 seconds, it's no longer correct. On the other hand I don't care exactly when in its execution my Agda compiler reads the input file, since compilers are not intended to deal with their input changing while they run. (If you're squeamish about leaking open file handles then just assume I am reading each file all at once with strict IO and then closing it.) I'm not gaining anything by being forced to track which functions can open input files and which functions can't. I'd much rather have some kind of tracking of time and space usage, if I could! So maybe I want to not bother tracking the `readFile` effect. I have to provide a promise that I don't care when exactly my program reads any particular file, which normally will come in the form of guaranteeing that the files in question won't change during the execution of the program. But this isn't qualitatively different than promising that I don't care when exactly my program allocates memory; and often the former promise will actually be true when the second is not. Of course there are programs for which the traditional choice of pure/impure classification works well, such as web servers. But there's more to life than web servers, and the article asks whether these particular effects such as reading files are *always* worth tracking. Maybe you're saying "well yeah, everyone already knows this, that's why `unsafePerformIO` exists after all". Well, yes. But Haskellers can be awfully dogmatic in asserting that Haskell's notion of purity is necessarily the best one, or the correct one, or even the only possible one; which is a position I disagree with, and I think what the article is trying to argue against as well.
*"We don’t typically track partiality as a typed effect (because this is unavoidable in a Turing-complete language)"* Partiality is unavoidable in a Turing-complete language, but effects are unavoidable in a language that accomplishes anything. This isn't a good explanation. So far as I'm aware, tracking partiality means partitioning your program into not-Turing-complete, non-partial portions and potentially-Turing-complete, potentially partial portions.
I don't understand what is stopping me from constructing an isomorphism for any a 0 &lt;-&gt; 0 x ((1/0) x a) &lt;-&gt; (0 x (1/0)) x a &lt;-&gt; 1 x a &lt;-&gt; a factor_0 ; assocl^x ; e^x ; unite Isn't that going to be a problem for any denotational semantics?
So, I guess you're using a hash function to generate the shortened urls. Can someone enlighten me what the possibility is for finding two urls with the same hash? (He writes there are 61229 printable chars, which makes 61229^2 combinations, is the possibility then 2 out of 61229^2?) Op: did you think about that case? :)
How does code mobility work in Haskell? Is it easy to send a function over the wire and execute it remotely?
We're considering a lot of things actually, and the admin team has a lot of ideas, and that's certainly crossed my mind - I'd actually be very +1 on moving GHC to Phab for example, although the sticky bit there is doing the migration of course. Right now we're treating it as a sort of closed-beta admin/planning tool, but I'd like to offer it to users in the future for projects if they'd like. We'll see.
When I wrote in the `README` that this was a quick hack, I really meant it. :-) It picks two random characters in the range ['A', '\128709'] as &lt;random key&gt; and then inserts &lt;random key&gt; → URL and URL → &lt;random key&gt; in a plain, good old-fashioned `Data.Map.Map Text Text` – collisions and birthdays be damned.
&gt; &gt; parse (parseW16Pair &lt;* eof) "" "92345" &gt; Left (line 1, column 6): &gt; expecting Word16 literal in range 0..0xffff (92345) Thanks! That's almost what I want, but is there a way to have the column-index point to the start location (i.e. `column 1`) of where the word16-token was expected?
&gt; Software is a mathematical object and is the same whether it is represented as bits on a computer, printed on paper, written on a blackboard, or carved into sandstone. Once we have our software in place we need to acquire a device to interpret it, and the details of that device do not matter as long as the interpretation is faithful. ...a scary thought just crossed my mind: What if the universe is just such an mathematical object, and it's being evaluated by some incomprehensible means (does a formula even require evaluation for its solution to exist?), and we are just deluding ourselves that we "exist" even though we're all just some intermediate terms in the grand computation?
https://xkcd.com/505/
[Image](http://imgs.xkcd.com/comics/a_bunch_of_rocks.png) **Title:** A Bunch of Rocks **Title-text:** I call Rule 34 on Wolfram's Rule 34. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=505#Explanation) **Stats:** This comic has been referenced 55 time(s), representing 0.2637% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)
http://9m.no/粪荀 That really isnt a shortened URL ;)
This sort of thing is why I tell people to use reusable components on infrastructure.
Ah, I see. That's my problem, I start to write a program and end up generalizing and complicating *everything*. At least that's how I get from a three day project to a three months project. :) Thanks for the explanation, though! I guess most users wouldn't care about a collision anyway, because it should happen only rarely and it's easy enough to generate a new string. One last question if you don't mind: Why 9m?
I'm sorry for not being careful. :) But thanks for submitting, got a great answer from the author as well.
I'll have to point that foldr' is almost never the right solution... You either want foldr when the combining function is lazy in its second argument or you'll want foldl' if the combining function can be made strict. foldr' by definition will always need the whole list... foldr' f z (x:xs) = let z' = (foldr' f z xs) in z' `seq` f x z' In other words, foldr' will start by computing strictly the right fold on the tail of the list then apply the combining function to the head and the result... Thus it need to look at the tail before coming back to the head of the list, it can't stream ! foldl' was probably what you wanted (streaming and left fold can be used interchangeably), with the caveat that the combining function has to be strict.
Ah, a new edition of "What Edward Kmett wrote this month" is out. Fantastic.
On a busy day, I feel as though I have been INLINE'd. 
&gt; One last question if you don't mind: Why 9m? There was some campaign where you could buy `.no` domains for NOK 10 (EUR ~1.25) last year, and I snapped up a few domains, just because they were cheap, and `9m` was one of the few two-letter domains that were available – so... no good reason. :-)
Mathematical objects lack a first-person experience.
See you there!
&gt; Of course, we can add explicit thunking at the element level and have the function accept and/or return thunks, but that's extra work. Right, if Scala only has the second I guess you use `a ~ T a'` and get fmap :: (T a' -&gt; b) -&gt; Stream (T a') -&gt; Stream b Personally I don't consider that "extra work", I just consider it "extra explicitness" but YMMV. Anyway, I get your point about thunks being invisible to the Haskell type system. It's certainly a good example for your point.
Why would we want a troll to join the Haskell community? Should've given them what they wanted and banned them. 
I don't see anything to protect from key collisions. I think there is a chance of overwriting a url in the database if the RNG generates the same key twice.
This is a very nice, short example! I learned a lot about Scotty and acid-state by reading the code. Thank you, ehamberg!
Yea, so I learned: I did rewrite it to use foldl' and a strict function. Memory consumption no longer "ballooned", but still grew unacceptably large. Some time later I ditched it, for reasons discussed above. Too much bother, code was becoming inelegant.
&gt; Those tools can be built. Do you need more than an HTML grid that modifies the state w/ AJAX calls? Right. But some folks went and built all these tools and solved all these problems. They ended up writing software that we refer to as "RDBMSs". 
&gt; Partiality is a bit subtle because although is part of the denotational semantics its presence doesn't violate any of the conditions needed to be pure Why do you think this? 
That's what [cloud haskell](http://haskell-distributed.github.io/) does. 
You can even you GHC.TypeLits so you can just use integers in your types instead of S/Z Nats
Right. And along w/ RDBMS's came the impedance mismatch between application and backend logic, doubling the amount of code a programmer has to keep track of.
Thanks, that looks like it does the job, and in a relatively clean manner. Besides Data.Proxy, do you think there are other features needed by a GHC version greater than 7.6.3 (that's the version in the Haskell Platform 2013.2.0.0 which appears to be the latest Windows release)? 
Ugh, I've been misspelling Van Laarhoven lenses all this time.
You'll notice that the linked 'paper' is not published. What you point out above is one of the reasons. I got to work with Amr after that, and we've been puzzling it out ever since.
This document comes to my mind: From Tim Berners Lee in person, naming Haskell for precisely such purpose: http://www.w3.org/2001/tag/doc/leastPower.html Although all have equivalent expressive power, functional languages such as Haskell and XSLT facilitate the creation of programs that may be easier to analyze than their imperative equivalents. Particularly when such languages are further subset to eliminate complex features (to eliminate recursion, perhaps, or to focus on template forms in XSLT), the resulting variants may be quite powerful yet easy to analyze. When publishing on the Web, you should usually choose the least powerful or most easily analyzed language variant that's suitable for the purpose.
Shoulda named it select*ah*
Because allocation is not an effect of your program, but from some runtime? I think.
That's great, starring for future reference.
Just wanted to mention, you could probably get some great semi-free speedups by using the text library instead of Strings.
Sorry for a possible stupid question. Is Haskell doing all of this app? Or is it integrated with a web stack, nodejs or LAMP for example, that does the displaying of the information?
Perhaps. Interesting you don't remember this paper though, because [I originally got the link from you](http://lambda-the-ultimate.org/node/4148#comment-63358). ;-)
Would be cool if it only generated nine meter long URLs.
The idea of `&lt;**&gt;` is that the values go the other way but the effects are still left-to-right.
Will Mcnettle be opensourced or sold as a product some time?
Well, he's running it behind nginx. But aside from that, it's a full Haskell application: Warp web server, Scotty web framework, acid-state database, and Shakespeare templates (those are all Haskell libraries). 
&gt; On hindsight, I was a bit of a dick. Ah, no worries. My response was a bit snarky as well, my apologies. I don't think it was clear, but I was really 'just playing' with this post. I wouldn't advocate actually doing file reads of any kind in Haskell outside of IO. I just find it interesting to think about. The idea of a smarter I/O manager and a possibly more restricted I/O model (for a more restricted type of data store, perhaps where all data is immutable, say) also interests me. 
Semigroup mentioned switching to using Text instead of Strings, which is what a friend recommended yesterday. I tried that that and just pushed it to master, but the speedup isn't as impressive as expected (or maybe my expectations were too high), any thoughts? Compare these two criterion reports: http://curtis.io/final-report.html, http://curtis.io/final-report-text.html. 
Only if you're in a crummy language with crummy libraries.
I chose just one function to look at. Hopefully it helps you. Looking at this code, there's a few things I would change. The first thing that bothers me is that the type signature, you are working on text but the code would look much nicer if it was using lists, and it is not clear that there would be an appreciable slowdown. Furthermore, this violates the principle that a type signature should use the most general feasible type, as more particular types are more fiddly. minMatchLength :: T.Text -&gt; T.Text -&gt; Int minMatchLength (T.uncons -&gt; Nothing) _ = 1 minMatchLength _ (T.uncons -&gt; Nothing) = 0 minMatchLength (T.uncons -&gt; Just (qHead, rest)) choice = let matchLengths = filter (&gt;0) $ map (\t -&gt; endMatch rest (T.drop 1 t) 1) $ filter ((== qHead) . T.head) $ filter (not . T.null) $ T.tails choice in if null matchLengths then 0 else minimum matchLengths where endMatch :: T.Text -&gt; T.Text -&gt; Int -&gt; Int endMatch (T.uncons -&gt; Nothing) _ lastIndex = lastIndex endMatch (T.uncons -&gt; Just (q, qs)) s lastIndex = case T.findIndex (== q) s of Just i -&gt; endMatch qs (T.drop (i + 1) s) (i + 1 + lastIndex) Nothing -&gt; 0 The below code is cleaned up. I commented my changes. Note the below code is untested, but it does typecheck. -- | Haddock documentation is nice, use it. Why does function exist what is -- it for? What are its arguments? minMatchLength :: Eq a =&gt; [a] -&gt; [a] -&gt; Int minMatchLength [] _ = 1 minMatchLength _ [] = 0 minMatchLength (q:qs) choice = -- It's more idiomatic to compose functions than to have strings of $ let matchLengths = map (endMatch qs . tail) -- drop 1 is just tail, we can move the constant and name it properly . filter (isPrefixOf [q]) -- isPrefixOf folds in the null check and the head check . tails $ choice -- sticking the 0: eliminated the need for the null check, the max 0 for the map in max 0 $ minimum (0:matchLengths) where lastIndex = 1 -- Not sure about the naming of this function. I cannot tell what it -- is supposed to do endMatch :: Eq a =&gt; [a] -&gt; [a] -&gt; Int endMatch qs s = maybe 0 sum $ endMatch' qs s -- I use Maybe to provide shortcircuiting for me. Essentially we build a list -- and sum it up, if we can endMatch' :: Eq a =&gt; [a] -&gt; [a] -&gt; Maybe [Int] endMatch' qs s = -- I was debating between doing something with unfoldr to simulate state -- or using the State Monad or ST, which I decided was more fun. runST $ do s'' &lt;- newSTRef s s' &lt;- readSTRef s'' let ms = forM qs $ \q -&gt; do i &lt;- elemIndex q s' return $ modifySTRef s'' (drop (i + 1)) return (i + 1) return (fmap (lastIndex:) ms) 
I'm sure there are some really nice ways to restructure this if someone takes some time to really dig into what you are trying to do. However, since M is fixed in range, I personally would think a good place to start is to ditch the lists and write a typeclass to take the place of int_listfn and then instance it for a data type that has the information you are working on for that particular size. You might find that the data in M2 can be broken down and passed into the M1 instance. Lists are nasty to work with when you have these "if statements" depending on the size, since lists are typed to have an arbitrary amount of elements. I would start there and see what happens to the code. Sometimes I find you just need to start restructuring different ways and then the proper structure becomes evident. 
I didn't submit anything this time around. I'll batch them up for the next one. =)
It's patently doable in 7.6.3. Instead of importing `Proxy` we can define it: {-# LANGUAGE PolyKinds #-} data Proxy k = Proxy We also have to replace the closed type family for `Next` with the open form, and we're done. 
Not a stupid question at all! Yes, everything is done in Haskell. As /u/gmfawcett wrote, the only non-Haskell component is nginx that sits in front of the server and sends incoming traffic to 9m.no:80/443 to the internal 9m server that runs on port 7000. This is done because other HTTP servers run on that machine as well and because nginx can terminate SSL connections and do some caching and rate-limiting. That said, however, if 9m were the only HTTP server running on the machine, it could in theory just be set to listen on port 80 instead of 7000 and talk to web browsers directly. This runs on the cheapest Digital Ocean account and even when it was #1 on hacker news with 600–700 live users at any time, it never even pushed 2% CPU utilization and 5% memory usage. (This was of course very much helped by the fact that everything except creating URLs is easily cacheable.) If you take a look at the source, you'll see that it's using Scotty for everything HTTP, acid-state is used to persist its state (just a `Data.Map.Map`), and then finally [Hamlet](http://www.yesodweb.com/book/shakespearean-templates) is used to generate HTML from some simple templates.
I have a couple of conditions in my head about what is needed to be pure. If you have some stricter conditions then please let me know! Specifically I'm thinking that let x = computation1 y = computation2 in (x, y) is the same as let y = computation2 x = computation1 in (x, y) and let x = computation y = computation in (x, y) is the same as let x = computation in (x, x) 
i really think new libraries should use `text`. apart from the pattern views it should look pretty similar.
you can always reuse the db schema. in fact, when migrating you should get data out of the db and not via the application in most cases. i still think that db-design-first leads to way cleaner data models that will in most cases limit you to not using persistent but still be worth it.
Ah ok thanks :)
there are very different opinions regarding whether these things have anything to do with israel, or whether just asking that question trivializes apartheid and slavery. so please stop.
Or helecta, if you want to make autocompletion easier.
Thank you for sharing. BTW, nice looking blog. What do you use?
Defined in the [Extra](http://hackage.haskell.org/package/Extra-1.46.1/docs/Extra-List.html) package. But I don't quite like those mixed utilities libraries. It would be nice to have ``partitionM`` added to [``Control.Monad``](http://hackage.haskell.org/package/base-4.7.0.0/docs/Control-Monad.html).
I agree that you should use `Text`, but use `Text` for storing/processing text. The algorithm you have is much more general than just text and I think the type should reflect that. Unless using lists in that function is an appreciable bottleneck, you should probably keep it (or go even more general and use a Foldable).
Could Data.Tuple.Curry help? https://hackage.haskell.org/package/tuple-0.2.0.1/docs/Data-Tuple-Curry.html
And what if *that* thing requiring evaluation is just a time-step in another grand simulation! You're not the first to think such thoughts.
A few people are of the opinion that Haskell strikes a perfect middle ground between tracking everything and tracking nothing.
I agree!
 &gt; partition odd [1..10] ([1,3,5,7,9],[2,4,6,8,10]) &gt; partitionM (return . odd) [1..10] ([9,7,5,3,1],[10,8,6,4,2]) I think it would be less surprising if the lists were not reversed.
Originally I posted because I thought there was a space leak, but I'm not so sure anymore.
Neato! This is the first real Haskell application I have seen in the wild I think!
I wouldn't use and it and would not recommend such a thing to anybody because it's better to improve your vim with little increments so that you know what is what &gt;After installing this configuration, your .vimrc and .vim will be under version control. Don't alter them, add your own settings to ~/.vimrc.local instead and your additions will be loaded. I already have .vimrc, why would I want this proposed by the author system? Let me install it just like any other plugin or multiple plugins with vundle.
It's possible to have a language that has no real penalty for tracking more things. Disciple and Frank are two research languages in that direction. The annoying thing about not tracking something is that it becomes impossible to rule it out when you really want to. Every untracked effect in your pure language can't be statically guarded against, and your denotation must always accommodate it.
The best indicator of the quality of a programming language is presence of a good IDE or editor written in this language.
The problem with Haskell is you'd need to have perfect vim or emacs support to get much programmer buy-in.
I would say the languages with the best tooling are those where such tooling is absolutely required for it to be usable. Doesn't sound like the greatest thing you can say about a language.
Dimensional typing and analysis is something I don't see used a lot in programming languages. Do any of the more "engineer"-ey languages (like R?) have them? Sounds pretty nice to have the compiler tell you you're adding a weight value to a time value.
Perhaps partitionM p l = sel l [] [] where sel [] ts fs = return (reverse ts, reverse fs) sel (x:xs) ts fs = do r &lt;- p x if r then sel xs (x:ts) fs else sel xs ts (x:fs) 
tripipie, you're completely delusional. Paul has more Scala work than he can handle and writing a programming book pays much MUCH less than minimum wage. 
Julia has the [SIUnits.jl](https://github.com/loladiro/SIUnits.jl) package, which checks your units at runtime(?). I also saw [Dimrod](https://github.com/ClementJnc/dimrod) for Nimrod, which provides unit checking at compile time.
Hah! Probably only the concept stuck in my memory but I forgot the source after 4 years :)
R has dynamic typing, so no, it can't really have this.
This is so important. The day I got serious about not putting anything into my .vimrc that I didn't understand way the day I started getting some real use out of it as an editor.
This confirms that using parallelism to actually speed things up (as opposed to just demonstrating the use of it) is a very subtle business, and not all problems are easily amenable to it -- even just writing correct and concise code is just one element of many considerations. Great writeup!
We are working with a company that does a lot of applied scientific computing, to create a framework for modeling a large class of dynamical systems problems in Haskell. This includes equation solving, but also includes other issues like efficiently representing problems, and using Haskell's powerful type systems to ensure program correctness. It's a really exciting project! Ultimately I think the company is going to make money and help lots of people with the results of these computations. It's interesting working on software that has a specific application, because often we find that it's more important to meet specific scenarios especially well, than to build a solution that does everything equally well.
The polyhedral model is full of tools to help you decide whether a given transformation, such as this overlapped tiling, is legal. There has also been a bunch of work to try to estimate when it is profitable.... And that's where most of the hard problems come from.
[FP Haskell Center](http://www.fpcomplete.com) is a complete Haskell IDE written mostly in Haskell -- though the front end uses some JavaScript and some Fay. It supports both emacs and Web front ends.
Yeah I noticed that too. Since this seems to be a fun side project it probably doesn't matter. But if this were getting the same amount of traffic as top link shorteners, there would definitely be many collisions.
Yet you're still mentioned a few dozen times. =)
 partitionM p l = sel l id id sel [] ts fs = return (ts [], fs []) sel (x:xs) ts fs = do r &lt;- p x let (ts', fs') = if r then (ts . (x:), fs) else (ts, fs . (x:)) sel xs ts' fs' I'm not sure this is maximally lazy, probably could get improved further.
While this cool, is it actually faster? More lazy?
Have you tried out pipes or conduit yet? Functions like mapM, partitionM, filterM, etc. are a bit hard to use because you can't do streaming processing on them. Using them is like coding in a strict functional language, they don't compose in nice ways. λ&gt; import Pipes ((&gt;-&gt;)) λ&gt; import qualified Pipes as P λ&gt; import qualified Pipes.Prelude as P λ&gt; let partition p = P.mapM (\x -&gt; p x &gt;&gt;= \b -&gt; return $ if b then Right x else Left x) λ&gt; P.toList $ (mapM_ P.yield [1..10]) &gt;-&gt; (partition (return . odd)) [Right 1,Left 2,Right 3,Left 4,Right 5,Left 6,Right 7,Left 8,Right 9,Left 10]
. $ and parens are all used by Haskellers. 
...and don't forget to be careful with how big your `Int` gets! Use [unbounded-delays](http://hackage.haskell.org/package/unbounded-delays) if you don't want to have to think about it.
F# has nice support for this.
acid-state does it by way of RPC calls
computation1 == computation1 Non deterministically returning bottom (because of something like readLine) seems to violate purity... do you think I'm off base? 
I've been trying to get my `.emacs` up to the point this Vim-Hs-IDE is at. Very hard work, and not near finished; especially since I want it to work with: * cabal sandboxes * Yesod * evil.el * structured-haskell-mode No intention to hi-jack the thread with some Emacs stuff; but I very much welcome pointers in the right direction. I read Chris Done's recent haskell-mode notes, and studied several people's `.emacs` files, but it does not speed up the process much. I do think project like this may help those who want to get an idea of what Haskell tooling can currently accomplish; which is quite a lot, but not very "turn key" :)
Yes I think that's a good one. Perhaps we want a condition like let _ = computation in () is the same as () 
In fact it seems that partiality would violate my first condition anyway, so you were right all along.
Is there any particular reason to specialise this to lists rather than any `Traversable` or similar? Edit: Maybe because you can't construct a traversable... I should really think my comments through before I post them.
Only issue I see with it is that 99% of the time you won't be able to type the url yourself. If you see http://9m.no/妮뎕 somewhere, you will have to know the first Chinese character and need a Korean keyboard for the second one if you type it out yourself, as opposed to e.g. http://goo.gl/EI7Me8 where you can type the URL with your default Latin keyboard.
Hi kvanberendonck, Care to elaborate? I don't think Data.Fixed allows to mix (lets say) microseconds and minutes the way this library allows...
Ah, nice catch!
Well, that _particular_ parallelization is not worth it -- the question is if there is another that is :-)
See Max Tegmark's [mathematical universe](http://arxiv.org/abs/0704.0646) for a serious exploration of this idea, or Greg Egan's *Permutation City* for a science fictional one.
It is important for wider adoption that Haskell have an IDE that works out-of-the-box. The advantage of such a system is it lowers upfront cost; a newbie can focus on learning Haskell and worry about fine-tuning her editor later. I thus applaud efforts such as this, and starred the repo to show my appreciation. 
Haskellutviklere i Norge : husk å delta i funksjonell programmering meetupene, det er en i Oslo og en i Stavanger i alle fall.
Thank you. I need to put my phone down now...
Maybe I am being a bit pedantic here, but why is the result not a monad?
I expect ghci is defaulting to `IO`, performing the effects (of which there are none), then printing the result of the computation.
Well, lets use Occam's razor and cut the BS. Real problem with Haskell is Haskell. Lets take Clojure for example. It has already got at least 3 decent editors, and it is 7 years old, Haskell is 24. 
Language is just an abstraction layer between human and machine. Tools built around language allow you automate things, e.g. check spelling, highlight syntax or visualize data (color instead of raw numbers). Tools allow creator to see complicated things from broader perspective. To allow better communication between human and machine. E.g. there are tools that do not need a textual language to tell computer what to do. Language + tools create ecosystem which as a whole ables one to create software. Very often combination of a crap language and decent tools overweights another ecosystem made of good language and crap tools. E.g. using JavaScript (crap language) and three.js (good library) is orders of magnitude easier than using Haskell and its best graphics library. Superior language by definition should have better tools available. If you say Haskell is superior why is it such a mess? Why I'm able to replace code of a program in a crap language while it is running, but I cannot do this in a superior language like e.g. Haskell? I will tel you why: because Haskell is actually inferior to JavaScript. Actually, the more primitive language the more primitive tools. VIM...
What about yi and leksah? I don't know many people that usr them, but most haskellers I know don't use any ide for any language.
This looks great. Thanks!
Except for Java. In my book, writing Java without a decent IDE is a form of masochism.
No, there's nothing wrong with it. In the same vein, I think it's an ad.
The best IDE for Haskell is probably EclipseFP. There are a number of relatively decent options for text editors, but I suppose that is not the debate. Could be a matter of Haskell being heavy on the engineers and light on the artists. Also I think the few individuals who have the technical chops to create an IDE for Haskell largely don't need it. It could also be telling of why so many self taught Haskellers languish around the intermediate level.
No one uses explicit recursion anymore. It's all combinators. &gt; let partitionA p = fmap partitionEithers . traverse (\x -&gt; bool Right Left &lt;$&gt; p x &lt;*&gt; pure x) in partitionA (pure . odd) [1..10] ([1,3,5,7,9],[2,4,6,8,10]) 
You can also have it generate [(1,2),(3,4),...] directly if you'd need that to build threes for example.
 Today it's a one order of magnitude window for CPUs and two for GPUs. That's compared to the 9 orders of magnitude of range in problem sizes that can be solved in under a second sequentially on modern computers. 
I think your thoughts echoes something from Plato, but I'm not sure how much.
hmm... it looks like you do some work twice. you might be able to get rid of it with a map, instead of set. also, why toList cs? why not just cells?
&gt; E.g. using JavaScript (crap language) and three.js (good library) is orders of magnitude easier than using Haskell and its best graphics library. Try harder please !
What is the benefit of this over just using Homebrew to install?
i don't recommend using brew for haskell. ever. (basically no one familiar with GHC actually babysits the haskell brew formulae, and thus it shouldn't be regarded as a vetted choice)
That's a shame, because the Homebrew Haskell formulae (including haskell-platform) have always worked great for me. Besides that, Homebrew is very popular these days, so it's likely that a large chunk of people who want to try Haskell on OS X will install it through Homebrew.
As far as I know brew doesn't have a formula for latest GHC 7.8. 
The formula begs to differ: https://github.com/Homebrew/homebrew/blob/master/Library/Formula/ghc.rb
The idea being your functions could carry some constraint i.e. HasResolution a =&gt; .. and then you just unwrap their actual values depending on whatever you get passed.
Thanks! Would be great to see some collaboration and a 64-bit Haskell platform in the near future.
It has always worked 100% fine for me out of the box. Actually, using official tar'd builds has always given me problems that just using brew never has ...
[This](http://research.microsoft.com/en-us/um/people/akenn/units/dimensiontypes.pdf) and [these](http://research.microsoft.com/en-us/um/people/akenn/units/) may be of interest, I see someone already mentioned F#, see top presentation.
I have wanted this for a long time. Thank you so much! 
I'm using one now - I've written a DSL for LLVM where the LLVM types are encoded into the haskell types of the DSL - this means it's impossible to write haskell code which compiles that creates invalid LLVM. I'm also making my source languages AST encode it's types at the type level after type inference - now there are type level functions between my source language AST's types and the LLVM's types. This is a game changer for me as most of the time when the program compiles it actually works (or at least produces valid LLVM). Since you practically have to ffi into some C++ to produce LLVM bit code producing invalid LLVM will abort your program (also kicks you out of a ghci session) usually with no helpful errors. This was the bane of my life when I was making my last compiler (in F#, using llvm-fs). I'm currently using the singletons library to make all this type level stuff easier, but explicit support for function promotion would be fantastic. 
I implemented this once on a Code Retreat event. Code may not be pretty but it works. [Here](https://gist.github.com/edofic/7971500)
Why not just use dependent types?
 {-# LANGUAGE TupleSections #-} import qualified Data.HashSet as S import qualified Data.HashMap.Strict as M import Control.Monad next :: S.HashSet (Int, Int) -&gt; S.HashSet (Int, Int) next cells = cells' where moore (x, y) = tail $ liftM2 (,) [x, x+1, x-1] [y, y+1, y-1] neighCounts = M.fromListWith (+) $ map (,1) $ moore =&lt;&lt; S.toList cells cells' = S.fromList [i | (i, n) &lt;- M.toList neighCounts, (n == 3) || (n == 2 &amp;&amp; S.member i cells)]
Because then you can't use haskell any more, and have to branch out to something like idris. Or at least every conversation I've had with SPJ has indicated that haskell isn't going to get full on dependent typing any time soon (and I agree with him). This (and their previous work) enables some limited form of dependently typed programming in haskell. 
Strong types are a form of contact (in the sense of a specification) and by building a bridge between values and types one can enforce this contract. The bridges come (e.g.) in form of GADTs. As soon as data is put into this form, no more "off-road" traveling is possible, transformations need to respect the type-dictated invariants. As McBride said once, the geometry of the value space is restricted so that constructions become (mostly) correct. Easier to write type-level functions reduce the initial investment into this infrastructure and make the value/type double-entry accounting a more pleasant experience.
I fail to see why you can't use Haskell any more. The changes to the core language would be quite small. In fact, they'd almost be non-existent, since the GHC implementation (iirc) doesn't have an AST-level distinction between terms and types.
Cool, it's nice to know the proper names for all this stuff :) I plan to release it once it's in a good state (currently only implement a subset of llvm instructions) - I have a full time job though, so I don't know when that will be. 
Please give credit where credit is due. Some parts of your .vimrc are clearly taken from other (publicly available) configurations.
I am in the same boat. Keep on ascending the ladder though, and congrats to your steep progress (in less then a month [from type indices to type-level wizardness](http://www.reddit.com/r/haskell/comments/246fos/ghc_status_report_may_2014/ch5252q) :-)
Yeah, last weekend I went a bit crazy and learnt about indexed monads, lens (+making lens work with indexed state monads), DataKinds and singletons through 2 days of solid haskelling. Can't really do that very often as it leaves me a bit burnt out inside! 
I didn't noticed that it's in the devel version
What about installing MSYS using the `mingwgetsetup.exe` installer? I usually use it and additionally install utis like wget, patch, etc. It's nice as it manages package updates as well. One thing to avoid is installing the second MinGW, which the installer allows.
I've read the whole dialogue and find it quite inane. Tying a language's type system into the state of the OS or some files on some server would be a terrible violation of modularity. Besides, the problem is not in the type system of any language at all, but in the implementation. The real criticism of the Haskell way of side effects control should be not that it's misguided, but that it's so crude and un-granular, as well as weak. Once you're in IO, all bets are off and it's like C-land again. Spooky actions at a distance, easy resource mismanagement, you name it. And even then there's no guarantee that at least the non-IO parts of your program are pure (since unsafePerformIO ruins everything). Haskell's type system cannot guarantee absence of IO effects in any code term, or their presence for that matter. What I would argue for is 1) ban unsafePerformIO and its kin, because they undermine one of the key goals of the Haskell type system 2) create a new type subsystem that would work exclusively inside the IO monad, making mutation and resource management explicit and checkable by the compiler.
You could have a dynamic version of this though.
Good question how I ended up with toList cs! Thanks for pointing that out.
As it is, in order to store a single live bit you are using a boxed`(Int,Int)` pair and a `Set` node, requiring dozens of bytes per bit. It would be much more compact to, say, store 8x8 tiles of bits in an `Int64`. You could also take advantage of bit operations. With tiles you could calculate more than one bit at a time. For instance, you can use a lookup table to map the binary 8-bit value 01110010 to a 32-bit value 0x12321111 representing the number of horizontal neighbors each cell has. If you really want your mind blown, though, look up [hashlife](http://en.wikipedia.org/wiki/Hashlife). 
I've read about hashlife, but I'm interested in simplicity here ;) Otherwise, I could have just used Repa for parallel convolution. What I like about using a set is that it accurately represents the infinite cell plane with complexity in the population rather than the area spanned (though this is useless for space-filling patterns).
Thansk much. You have single handedly made haskell that much more accessible for the all of us.
i keep reading 'cofree' as 'coffee'. %-}
I almost always use case too, I find the repetition annoying. My current favorite trick is: f :: a -&gt; b -&gt; c f = curry $ \case (a,b) -&gt; ... 
Before LambdaCase it was pretty annoying to have to introduce dummy names to match with case. 
Thanks, I just successfully installed it! :) When I try to install "brew install cabal-install" after installing ghc with "brew install --devel ghc", brew seems to insist on trying to install ghc 7.6.3. I wonder if there's a way around this.
I meant to suggest that you use a `Set` of tiles, omitting completely dead tiles. 
Maybe it's gotten better in the past 6 months. But I don't think anyone is maintaining the ghc formula per se
Highly disagree with most of your comments. The function is probably not meant to be used on list, might as well use Text. Encouraging the use of Text for Text is a good thing. And changing a function to use ST... is disgusting.
Thanks very much for this. It's very useful to have such examples and they are much harder to write up.
OK, I think I see what you mean then. I'll try it! Although, something unclear to me - with maps, how do you handle the cells at the edge of a tile? Your example has 0 at both ends of the key byte.
In order to calculate the next generation of a tile you also have to look up the 8 neighboring tiles. Except here you're doing 8 lookups for 64 cells instead of 8 lookups for 1 cell.
Shouldn't that be instance Monad Id a ...
The language forbids the use of type synonyms for instance declarations. Even if it didn't (e.g. using FlexibleInstances), an instance has to be associated with a type constructor, and you obviously don't have one here.
Actually, I really meant to say you should use a `Map (Int,Int) Tile`.
Firstly, the error you are getting is caused by the requirement for type aliases (type functions) to be fully applied. If this restriction wasn't in place, the type system would have to be able to prove that two partially-applied type functions are equal, which is not possible in general. Secondly, even if the type function was fully applied, you would still need to use FlexibleInstances.
You can only use saturated type synonyms (in case of `Id`, you can only use it like `Id Int` or `Id a`, but not like `Compose Id Id a` or `Functor Id`). To do so, you need the `LiberalTypeSynonyms` and `TypeSynonymInstances` extensions (and `FlexibleInstances`, apparently), but because the former screws up error diagnostics, you probably don't want to do this in the first place. Moreover, the class system needs some form of distinctness between instances to work, and this type synonym doesn't provide that.
No, because `Monad`s must have kind `* -&gt; *`.
Excellent! Thanks very much.
Is the formula broken or is it just not updated as fast as other options?
Reminds me of the joke about comathematicians: &gt; Comathematians are devices for turning cotheorems into ffee.
Haskell is good for beginners because they don't have any preconceptions to confuse them. Haskell is good for experts because they can quickly understand the benefits of what it has to offer. Haskell is good for all intermediate levels who have a strong sense of curiosity and drive to learn. Haskell is very bad for [expert beginners](http://www.daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner) who have no desire to learn anything new. It requires you to learn new things. It's not just a new skin for the same ideas.
Note that Id would not be of the right kind even if GHC would allow partially applied type synonyms. 
When I started reading this, I thought it was a joke. Now that I've finished, I'm still not sure.
I think Haskell is easier to learn when you don't have any experience with imperative languages like C or Python.
You should learn Haskell without any programming knowledge. Haskell is so different from anything else that the hardest thing is to unlearn the concepts you use in other languages. 
I've always wondered what programming would be like to me if I started with haskell. Someone should do this experiment. 
Doesn't matter: it's like a completely different "programming", so expertise in other languages will not add up a lot in Haskell.
no one maintains it, and it needs proactive baby sitting because of the nature of the Brew folks structure their dev cycle
Haskell takes some patience, and in particular, I think many people struggle with static typing when they first see huge compiler errors. But Python is a good introduction to Haskell, and encourages _some_ of the same paradigms in problem solving. In contrast, C-like languages encourage _few_ of the same paradigms as Haskell - but they share the intimidating static-typed compiler errors. Well, guess what? Haskell's errors are significantly better written and more sensical than C's (or C++'s - even the people who would argue my claim about C would probably agree on this point). "Reciting each native function and attribute" in Python isn't important. If you instead think you're "generally pretty quick at looking up what you need to know" or similar, you'll be fine. The core Haskell language and libraries are very well documented (see Hackage, Hoogle). One thing people find intimidating about Haskell is that it's "academic" (and generally mathy). But an aspect of this is that it's also pedagogical. Unlike many languages, it was written to be taught and learned easily. For me, the biggest barrier to really learning some of the language was not knowing what to write in it. I actually ended up shifting from Python to Haskell because I found the type system to be immensely helpful in formulating solutions to some bioinformatics problems for a class I was taking. Then, once I had problems to work on, further practice was easy to make time for. One factor of Haskell that might make it hard to get into is its reputation for IO being hard to do. It's not, really - maybe compared to Python, but you can write much terser code in Haskell to do basic IO than in C or Java. The lack of imperative features can also make it somewhat hard to implement algorithms as traditionally taught in CS curriculum, which tend to rely heavily on mutable state. So you might find it difficult to make the jump right to making, say, an interactive application or a complicated array-based data structure. But, as I'm sure you would guess, these things turn out to be nicely expressible in Haskell - but there are some things to learn about how to express them (namely, the type system goes further than just looking at the types of values at memory locations - Int, Bool, etc, and also keeps track of things like what kind of state is involved in a procedure, etc.). This can get in the way at first, but it's very nice in general. So go for it! It's a lot of fun. I won't go as far as to say that Haskell's easier when you don't know programming - at the very least, most resources are tailored to people who know at least some imperative programming. But some knowledge of Python should be totally sufficient to appreciate the language. If you don't have fun, go learn C (or better yet, C++ or Java) for a few months - not because they will help you with Haskell, but because the experience will make you desperately want to go back to a better-thought language with a more powerful type system: Haskell.
[Now!!!](http://learnyouahaskell.com/chapters) Learning is best done when there is fundamental interest in oneself, and you seem to exhibit this quality by asking this question. I'd say give it a shot and keep it going while it still interests you.
It would have kind * -&gt; * which is the right kind.
Doesn't this already work with monad transformers? E.g.in the mtl package where instead of the State class there's a MonadState class with more than one type implementing it?
Every language is good for beginners, because its easier to start from zero when you have no opinions on the subject. Haskell brings no benefits for experts. Experts by definition are able to write Haskell in C or Java. Lets not forget that Haskell is only a language, abstraction layer below the one which are libraries and tools. Tools and language create a communication layer between human and machine. "Haskell is very bad for expert beginners who have no desire to learn anything new." Thats wrong assumption. Expert beginners already know what they want. Language is not the problem. Actually language is almost never a problem. The problem are libraries written in that language and active (or not) community. And since libraries are derivative of complexity in a given language they reflect the language itself. If language is bad libraries are bad. In case of Haskell libraries are very bad (one man hacks, abandoned or simply unusable). I'm writing this from a cross-platform game developer PoV. Haskell is modern equivalent of snake oil. What kind of problems does it solve actually?
Maybe is a type function that is partially applied, and it's a monad instance. I don't get it.
&gt; One thing people find intimidating about Haskell is that it's "academic" (and generally mathy). But an aspect of this is that it's also pedagogical. Unlike many languages, it was written to be taught and learned easily. This seems like a bogus claim to me. Haskell's design had nothing to do with pedagogy and everything to do with the lack of a free research platform. In the late 80s, computer scientists wanted to mess around with programming in non-strict languages, but that entailed either designing your own language from scratch or paying royalties on Miranda. There are dozens of quirks in the language that actively get in the way of pedagogy. You have to use `negate` rather than a unary operator. The numeric typeclass tower is a complete clusterfuck. The fact that integers, your most used data type, are non-algebraic data types, and consequently don't support pattern-matching. This leads to unforgivable uses of `if then else` in programs.... only to discover that for a long time (maybe this has been fixed now?) the off-sides rule for `if then else` was broken. I should also throw in that you have infinite lists, but also no infinite `Integer`. And I'm not sure I entirely believe the error messages for Haskell are better than C's. While they should be theoretically trivial to track down, unification errors have always caused me a lot of grief. Although part of that may be a consequence of Hindley-Milner and the compiler keeping most of its precious generated typing information all to itself.
&gt; Unlike many languages, it was written to be taught and learned easily. What's also nice is that a lot of the academic research is implemented in easy-to-use libraries that abstract over the complex internals and allow any type of programmer to use them. Generic programming comes to mind. Of course, some of the extremely powerful (features of) libraries still require some academic background, [attribute grammars](http://www.cs.uu.nl/wiki/HUT/AttributeGrammarSystem) being a good example. Not incredibly hard to learn, but probably daunting for the average programmer.
Short answer: because typeclasses work on type constructors, not on functions from type to type. And now for the long answer. Since Haskell is not dependently-typed, it doesn't support arbitrary functions from type to type. For comparison, consider the dependently-typed language Agda. In Agda, you can easily write a function from type to type: Id : Set → Set Id a = a And then you can write a Monad instance for it by implementing functions with the appropriate types: Id-Monad : RawMonad Id Id-Monad = record { return = \x → x ; _&gt;&gt;=_ = \x f → f x } ([full code](https://gist.github.com/gelisam/0127e5918b9ce3e5c489)) In this system, however, you cannot rely on the type system to figure out which monad you want to use. If you write an expression such as `replicateM 3 [0, 1]`, for example, do you want to use the identity monad and obtain 3 copies of `[0, 1]`, or do you want to use the list monad and obtain all the binary strings of length 3? Because of this ambiguity, Agda requires typeclass instances to be passed explicitly, so you would write something like this instead: &gt; replicateM Id-Monad 3 (0 ∷ 1 ∷ []) (0 ∷ 1 ∷ []) ∷ (0 ∷ 1 ∷ []) ∷ (0 ∷ 1 ∷ []) ∷ [] Now, in Haskell, typeclass instances are always passed implicitly. So if there was an ambiguity as to which typeclass to use, we would have a big problem, because there is simply no syntax to specify which alternative to use. In particular, if Haskell had an identity monad, then there would be plenty of ambiguities like the `replicateM 3 [0,1]` example above. So such monads are ruled out. Now that we have a theoretical explanation for why Haskell cannot have a "real identity endofunctor", let's look at the syntactical reason why your program was rejected: `Id` is a type synonym. Since `Id` is a type synonym, `Id [Int]` is not a real type. In particular, there are no situations in which Haskell could look at an expression, see that it has type `Id [Int]`, and decide to use the monad instance for `Id`. Even if you explicitly write `replicateM 3 ([0,1] :: Id [Int])`, Haskell will first evaluate `Id [Int]`, see that it means `[Int]`, and conclude that you want the list monad instance. So it makes no sense to define instances for type synonyms. *** All right, now let's consider your motivation. You want to debug a pure functions with print statements? No problem, use [trace](http://hackage.haskell.org/package/base-4.7.0.0/docs/Debug-Trace.html#v:trace): it uses `unsafePerformIO` to print stuff, and I use it all the time. And finally, let's consider your other motivation. If you want to write code in monadic style, but you don't actually want to use any effects, use the `Identity` monad. Unlike `Id`, this one is a type constructor, so a function of type `Int -&gt; Int -&gt; Identity Int` is not the same as a function of type `Int -&gt; Int -&gt; Int`, but it's trivial to make a function of type `Int -&gt; Int -&gt; Int` out of it: fooM :: Int -&gt; Int -&gt; Identity Int fooM x 0 = return x fooM x n = do --print x -- a compile-time error if uncommented fooM (x*n) (n - 1) foo :: Int -&gt; Int -&gt; Int foo x n = runIdentity (fooM x n) main :: IO () main = print (5 + foo 5 1) 
Maybe is a type constructor, not a type function. 
I've answered this question on reddit [before](http://www.reddit.com/r/haskell/comments/urc75/complicating_conduit/c4xyqiz). The short version is that there needs to be at most one way to decompose any particular type into an application `m a`. Otherwise, you have no way to determine which instance `m` to pick. If you allow `m` to range over partially applied type synonyms then you lose that uniqueness property. So it's not a technical limitation of GHC, it's a fundamental property of the language.
This is very nice if there is a specific reason why you want a "self-contained relocatable GHC build" that is bare-bones, or if you need the latest cutting-edge version of GHC. Thanks for doing this! But for a default "Haskell for Mac OS X" for general use, I would recommend the Haskell Platform installer. Besides a fully automatic setup of GHC, you get a set of basic packages that all work together perfectly, all the paths set up in the way you would expect them to be on a Mac, and a simple uninstaller.
Learn it now. As a beginner who had only taken CP1 with Java (the hardest project was to print a diamond shape to the console), I was able write a simple but non-trivial GTK application in Haskell after about a month or so of learning the language, and that was without LYAH. It really isn't as hard as it seems, the biggest hurdles are wrapping your head around recursion, purity, and vocabulary. It seems daunting at first, but learning what functors, applicatives, and monads are isn't any worse than learning what objects, interfaces, encapsulation, and inheritance are all about. The language is more math-y, but the difficulty of concepts is the same.
Wow, thanks for such a complete answer. It's all clear now. &gt; No problem, use trace: it uses unsafePerformIO That **is** a problem, because in the presence of `unsafePerformIO` the type system cannot guarantee that "pure" functions are actually pure, even in the sense of absence of IO. Which pretty much defeats the "purely" in "purely-functional". &gt; but it's trivial to make a function of type Int -&gt; Int -&gt; Int out of it: Yet that requires explicit changes in the calling function. It has to know whether to expect a "pure" value or a monadic one. 
Tis a shame that "cofree" is making it into people's lexica. I don't have anything against puns, per se; but typesetting puns are an abomination. (For those who are unaware, "cofree" means "free"; there's no generally meaningful dual to the notion of freeness. The pun comes from the fact that just as the `Free` monad is the free monad, the `Cofree` comonad is the free comonad.)
Experts are aware of their potential mistakes. Why? Because they are experts. Haskell compiler catches only statically typed errors. It wont catch pattern match errors, it wont logic errors and these are runtime errors. Haskell has nothing to help you to test logic while program is running like Smalltalks, Lisps, JavaScript, and others do. If Haskell is so good why Mozilla is creating their own C-like language not even considering to use Haskell? Programming language does not make your program safe or correct, architecture does. When you design your program to be thread safe it will be thread safe regardless of language. Language is only a tool. If it is a good tool it will make transition from paper to machine easier, if not... you know the rest.
Is there a problem with dualizing this page? http://ncatlab.org/nlab/show/free+object I prefer 'fascist' functors and 'fascist' objects, personally~
Not really. Sure, I *could* program in Java or C, but I choose Haskell because I have to worry less about things going wrong.
Gee, thats exactly why I'm using Java instead of Haskell. My work requires certain minimum guarantees from libraries and tools. What kind of guarantees? Uhmm, hmmm... maybe those they just download, install successfully and compile?
Generally I consider pervasive use of threads and concurrent messaging to be an anti-pattern. You might want to take a look at my [`mvc` library](http://hackage.haskell.org/package/mvc) since it was designed to solve this kind of problem. I wrote about how to use `mvc` [here](http://www.haskellforall.com/2014/04/model-view-controller-haskell-style.html), including a small example using the `sdl` library. The big difference if you were to restructure your application to use `mvc` is that all your application's internal messages would be communicated using `State` instead of `STM` variables. External inputs would be received through `Controller`s and external outputs would be sent out through `View`s. I'd also highly recommend not using a automatic type conversion type class. This makes your code greatly more difficult to read and maintain. It's better to use explicit named conversions.
That's just type classes. When resolving which instance of MonadState should apply with a computation of type `m a`, instance resolution only needs to look at what `m` is, and this uniquely determines it. (Of course, there is a functional dependency which helps to determine which type `s` should be uniquely from the choice of `m`.) The problem here is that you could always choose to insert another occurrence of Id anywhere in a type, which means that whenever you have something of type `m a` for some monad `m`, it might also be regarded as something of type `Id (m a)`, to which a different monad instance applies. For a simple example, let's suppose that I have `foo :: IO (IO a)`, and I go to apply join :: (Monad m) =&gt; m (m a) -&gt; m a to that. Well, with the usual instance, this goes through with `m` = `IO`, and results in `join foo :: IO a`. However, with your type synonym, we also have that `foo :: Id (Id (IO (IO a)))`, and if your instance were reasonable, we could pick `m` = `Id`, to get `join foo :: Id (IO (IO a))` which of course is the same as `join foo :: IO (IO a)`. Now, which of these two is meant might be determined by the rest of the program, or it might not. If we were just writing some IO computation where we ignore the result of `join foo`, like: do ... ; join foo ; ... then both `join foo :: IO a` and `join foo :: IO (IO a)` would work, but would have different effects -- the first one having more effects than the second (because it runs the resulting `IO a` as well). Because of this ambiguity resulting in different behaviours with no way to determine which was meant by the programmer, any sensible typechecker would have to complain, and the only way for the programmer to fix it here would be to give an explicit type signature to join, like `(join :: IO (IO a) -&gt; IO a) foo`, which is pretty ugly. Of course, it's not just join which will have this problem, it's really most things which work using an arbitrary monad.
Algebraically speaking, a "free foo" is something which obeys all the laws required to be a foo while admitting no additional equalities that do not follow from being a foo. Whenever I think of freeness, this is the notion I think of. Importantly, —though well-defined— this is an informal notion. In category theory we observe that the construction of free algebraic structures coincides with the left-adjoints of forgetful functors. One could use this to try and formalize the notion of freeness (and that formalization does have a dual: the right-adjoints of forgetful functors), but I'm not sure that's really the best way to think about freeness per se. It's a bit like the Church–Turing thesis. The thesis is a bridge between formal and informal notions. It can never be proven, because doing so would require a perfect formalization of the informal notion. But focusing on its unprovability misses the point: the entire content of the thesis is in the fact that it bridges between these notions. But to understand and respect that content, we must recognize the need for a bridge, which means recognizing what makes the two shores different.
`brew install cabal-install --build-from-source` will work for this case, but I agree with your point. I inevitably build it myself.
imeant the default build ghc from source when theres nothing to be gained from it part too. 
Languages are all good for different things. I actually really like C++ for compile-time metaprogramming, but it's not a good language for beginners - especially given that most beginning C++ is taught as a weird mix of C and Java, instead of as a language with great generics/parametric polymorphism, for example.
Fair points. There are definitely a lot of complex things in Haskell. I might be mis-paraphrasing something. Maybe Haskell's not designed to be pedagogical. I think maybe C gets away with having sane errors because everything is simple. You're working with a pretty straightforward model of the von Neumann machines we use in practice, and the error messages have to be somewhat sensible - "you tried to put something here, but you can't put something here cause you haven't gotten space for it yet," "you tried to get something from here, but there's nothing there," and so forth are probably more intuitive than "No instance for StateT Identity YourState a arising from "put". One reason I like comparing Haskell to C++ or Java instead is that it accomplishes static-typing safety, a purported strength of those languages, and the same amazing polymorphism you can accomplish with generics/templates - but reveals far less complexity to the beginning programmer. If you ever see a C++ template error, then a similar Haskell error, you see how hard compilers work to make an expressive typesystem work. And Haskell's type system is both more expressive and a lot less brutally explicit than C++'s. You don't have to declare the type of every damn thing just to have the compiler spit out literally pages of output telling you that, essentially, you got one thing wrong.
Is there a formalization of what "forgetful" is? I've only ever seen them presented as "obvious" as forgetting some structure from a structured set. Is there a better definition?
+1 on MVC. I've been playing with it to rewrite my process monitor and I was really happy with how much logic I could remove from the impure parts of my code. Views still run in IO but purifying and testing those is pretty straightforward and mechanical using the `free` package and TH. In my use case, the controller layer generates events into the model for process signals and process starts/deaths. The model makes all decisions in a pure fashion and hands down directives to the view. The view launches processes, monitors and uses some stuff from `pipes-concurrency` to signal back up to the controllers. The pattern is general enough that I could see it being applied in lots of places, either as the whole application stack or for components. I do find that I sometimes have to reach for explicit threads, but I'm not sure if it qualifies as anti-pattern. The only parts where I need explicit threads involve registering a hook and casting an IO operation off to a thread that can block on that.
It's easiest if you don't know any programming at all yet.
What does that conversion look like and how would you specify new conversions?
I can see that being somewhat useful. Guessing that buildwrapper doesn't quite do what you want? 
This is wrong: "cofree" is not a pun, and "cofree" does *not* mean the same as "free". At least in the cases I've seen both words use they are always used as follows: There is some functor U : C → D from the category of C-things to the category of D-things that everyone agrees should be called a forgetful functor. * If U has a *left* adjoint, this adjoint is called the *free* C-thing functor * If U has a *right* adjoint, this adjoint is called the *cofree* C-thing functor If you know any examples that does not follow this pattern I'd be curious (and shocked!) to see them. If you believe that "cofree" and "free" should be denoted by the same word, you also probably believe that "left adjoint" and "right adjoint" should also be called by the same name and I can assure you that having different names for them is extremely useful! By the way, to find out things like the difference between free and cofree, both [Wikipedia](https://en.wikipedia.org/wiki/Cofree_coalgebra) and the [nLab](http://ncatlab.org/nlab/show/cofree+coalgebra) are adequate.
&gt; Haskell is good for experts because they can quickly understand the benefits of what it has to offer. Understanding the benefits, unfortunately, does not increase the speed at which it can be learned. It certainly motivates but there are a lot of challenges for someone coming from an imperative background. I found that the most effective way for a C/C++ programmer to learn Haskell involves a surprising amount of alcohol.
Thanks, I just wiped my macbook clean today and this worked really well!
Someone in the audience mentions that.
Your mileage will vary depending on what you want out of it obviously. I generally enjoy reading stuff like this regardless of its currency.
You know, there exist software developers that do not and don't need to care about Windows.
It sounds like you need to give the Haskell Platform a try.
There are differing interpretations. You can say that every functor is [forgetful](http://ncatlab.org/nlab/show/stuff%2C+structure%2C+property) in *some* sense. If you follow that view then all left adjoints are free and right adjoints are cofree. If you take a more conservative approach of just saying it takes and "obviously" discards some information you get a more conservative notion that doesn't abuse the name so much. Both cover the cofree comonad case adequately.
https://twitter.com/antiselfdual/status/464623022494990336/photo/1
This is a lot faster and doesn't have any dependencies. Download, unpack, set your PATH. Not everyone uses Homebrew.
Xcode 5's command line tools should be sufficient to use it.
The Haskell Platform is woefully out of date, stackage mostly eliminates the need for it (although not having to build everything is a huge time saver), and this build doesn't need an uninstaller because it doesn't install anything! Using $HOME/Library/Haskell instead of $HOME/{.ghc,.cabal} isn't really that much of a win, although it does make docs a bit easier to find.
Note that `do` notation doesn't require a `Monad` instance if you never use a bind, so both of your examples compile just fine: fa1 :: Int -&gt; Int -&gt; IO Int fa1 x 0 = do return x fa1 x n = do print x fa1 (x * n) (n - 1) fa2 :: Int -&gt; Int -&gt; Int fa2 x 0 = do x fa2 x n = do -- print x fa2 (x * n) (n - 1) The only change you have to make is to just remove the `return`. 
Well you could, but that would at best lose a lot of information. Which came first, the 1 or the 2? How would it work if there were an odd number of results? I think you're reading too much into the fact that `odd` was used to demonstrate the functionality.
Ah, good eye. Typo. Fixed now.
Yeah, I noticed and I'm curious too, I for one find it harder to read. You have and extra ";" on the first line. Maybe new commits are done in that style in order to keep some kind of per module homogenity, if there is already code like that there.
Excellent.
Thank you for your answer! I was actually already thinking about using a state monad for message passing, but it appeared to complex and difficult to implement to me. Another reason why I opted for threads is, that I can easily add new event sources later, e.g. for Dbus. But I'll definitively have a look at your `mvc` library. Do you have an opinion on the Component approach? From what I've [read](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/), I get the impression, that this is just another anti-pattern I'm following. 
why not just partitionA f = liftA (partition f)
trace should only be used for debugging. There it's very useful and in my experience has always behaved like I expect. Don't fret about taking shortcuts when debugging. 
I like it because when I rename the identifier before `do`, the code still compiles (no messed up indentation), and it is much easier to correctly re-indent it in Emacs. 
No way! Example of inline C, pretty please?
Hmm, that looks messier. Why was this change to TH necessary?
My advice is to move the code after the do to the next line. foo = do start your monadic code here Now you can rename and add parameters at will.
Use structured-haskell-mode which auto-re-indents: https://www.youtube.com/watch?v=vB8AXfmyjQc
The short story is that the old TH neither fully typed quasiquoted code fragments nor did it refrain from typing them. Hence, relying on types was fragile and required to understand the innards of TH. Moreover, this design made some useful things —specifically, pattern splices— difficult, which is why they hadn't been implement so far. The new design is actually simpler (and arguably cleaner): nothing gets typed until it gets spliced. That price to pay is that there is less type information in TH code. (This has been somewhat offset by adding a second form of MetaML-ish strongly typed quotations and syntax representations, but they are not relevant for foreign inline code.) The gory details are at https://ghc.haskell.org/trac/ghc/wiki/TemplateHaskell/BlogPostChanges
&gt; Using $HOME/Library/Haskell instead of $HOME/{.ghc,.cabal} isn't really that much of a win, although it does make docs a bit easier to find. There's a lot more to being Mac friendly than that one path. However, now I see from the post on the cafe that there is an additional [github project](https://github.com/ghcformacosx/ghc-dot-app) that does more of that stuff for your install. That's great! Does everything still work OK when you already have Haskell Platform installed? 
I suppose it's easier to see how the sugar gets removed? Otherwise I think it's just up to taste.
I once teached one of my friends Haskell (both I and him were electronics undergrad at that time, he had no prior experience with programming), and he was able to use haskell to write a simple practical code in a few months, and now using haskell + some other languages like C and python after a few years.
Not completely. Yes, sure, many cases can't be inferred due to dependency, but many cases *can* be inferred. And we have to be clear where type *inference* is needed -- top level definitions only. Embedded where clauses do *not* need type inference because the type the defined terms is given by its use site, and thus can be *checked*. I think the overall cost of losing type inference would be very small in Haskell.
This works if you only ever want to put `do` at the start of a RHS. Alternatively you can keep a consistent style of the parent preceding the children, as in function application, across the board. If you don't want to deal with re-indenting children, just put them below the parent. main = do hello world bob x = case foo of Just 123 -&gt; do hello world etc = do abc foo (do hello world) (do hello world) def $(do hello world) The `do hello world` is always the same. Good for both reading and moving.
&gt; ... no it doesn't? &gt; &gt; bob x = &gt; case foo of &gt; Just 123 -&gt; do &gt; hello &gt; world That's a RHS (“right-hand side”).
&gt; etc = do &gt; abc &gt; foo &gt; (do &gt; hello &gt; world) &gt; (do &gt; hello &gt; world) &gt; def &gt; &gt; Though IMO if you are going to do the latter you should be using let or where clauses anyway. In other words: &gt; This works if you only ever want to put do at the start of a RHS.
But those dos aren't at the start of the RHS, as you claimed. What was your point then?
Which `do` specifically? It's clear we have a different understanding of what an RHS is. I mean [this](http://hackage.haskell.org/package/haskell-src-exts-1.13.5/docs/Language-Haskell-Exts-Syntax.html#t:Rhs) and [this](http://hackage.haskell.org/package/haskell-src-exts-1.13.5/docs/Language-Haskell-Exts-Syntax.html#t:GuardedAlts).
... I don't see the connection. As far as I know, the RHS is the right side in an expresion like this: lhs = rhs right? Then putting moving the code after the do to the next line can be done in more situations than only the start of a RHS: in fact it can be done always when a `do` can be used. If it wasn't clear, with "though IMO if you are going to do the latter you should be using let or where clauses anyway" I meant "though IMO if you are going to put do expressions in parentheses to apply them to a functon you should be using let or where anywhere". It doesn't have anything to do with moving the code to the next line or not.
why?
Oh well, style is subjective, so whatever. I don't think (do hello world) looks silly at all.
I think using two styles is inconsistent (and that you pay for it with legibility and navigation), but that can be our impasse.
In the style I propose, the column of the parent always precedes the column of the children. If you want to visually find the parent, you look left. Indentation actually means something. maybe 0 (+1) foo foo = maybe 0 (+1) foo do boo bar foo = do boo bar I have to do a double-take when I see code like this: bar x y z = foo $ \x -&gt; mu $ do case x of bob -&gt; do apples $ bob okay $ if x then y else z Because sometimes the parent is left (`case`, `if`, `apples`—though `apples` might mislead—`okay`'s parent is `bob`), sometimes it's not (`bob`, `mu`, lambda) and you have to scan the preceding line (or lines) and the real parent is waving "hey, I'm up here!" And the reasons for it aren't predictable. They tend to be whims of the author at the time of writing. I don't think I have to emphasize that the code-samples I'm giving are contrived and that when assessing style you should take into account of the audience real world influences like unfamiliarity, searching, stress, distraction, manipulation, etc. All of which are impeded by inconsistency in style.
Resolving dependencies... Configuring sdl2-1.1.0... cabal: The pkg-config package sdl2 version &gt;=2.0.3 is required but it could not be found. Failed to install sdl2-1.1.0 cabal: Error: some packages failed to install: sdl2-1.1.0 failed during the configure step. The exception was: ExitFailure 1
Clojure? Thanks for your good words. Actually I'm a Prolog programmer. 
So... what does that mean?
Is it a new paper? The singleton library has been around for a while now.
Eh. There are also quirks in the language (or were) inserted specifically _for_ pedagogy -- such as infamous n+1 patterns. And certainly texts such as "calculating is better than scheming" advanced teaching lazy functional languages pedagogically.
A year is a bit longer than a few months. There should've been an update within days of the Xcode release that broke everything, installing Haskell Platform on Mac today is really not a good option.
I'm quite aware of what being Mac friendly is, and the Haskell Platform for Mac is not. That GitHub repository is the tool that's used to build the app bundle, so I'm not sure what you think the difference is. It shouldn't interfere with any other Haskell install, you just need to change your PATH appropriately to choose the one you want to use. 
List fusion (or stream fusion) lets the compiler cut out steps of building intermediate lists to get the result you want, saving time, memory, and GC while letting you write idiomatic Haskell. The more the compiler can do that, the easier it is to write (forgive the buzzword) performant Haskell; check out the link danbst provided above.
The flower backgrounds seem rather stock-photo-ish, I would recommend something more in the theme of software/programming/haskell. Other than that it looks quite nice.
I don't disagree that the image should be changed, but wanted to point out that it's the same image used on the current haskell platform page. To OP: I read the title and initially thought this had to do with the platform itself, not the website...
This cycle is an outlier. It took months for the community just to learn how to work around all the problems introduced by Xcode 5 - that's part of the delay, and it wasn't a problem only for HP. Another problem was a sudden bug fix release of GHC just when HP was almost released. The HP coming out now is built on a lot of new automation that will streamline the release process in the future. Hopefully, we'll be back at least to the regular release schedule now. A high-quality vetted platform release is a lot of work, but worth it.
&gt; It shouldn't interfere with any other Haskell install, you just need to change your PATH appropriately to choose the one you want to use. OK, great. I'm going to try this. Thanks!
**Positives** * Clear call to action * Easy to read typography * Minimal **Deltas** * Don't like the flower in the background, it's too distracting and doesn't really fit in with the rest of the design * Too little white space at the end of the page
Thanks for the feedback, I can certainly tweak the whitespace. As emarshall85 said, the cherry blossoms I pulled from the original website and I can certainly switch them out with different images or go without images entirely. 
Yes, I understand this cycle is an outlier, I've been following it very carefully. The issue is that Xcode 5 came out in September (more than 7 months ago). Workarounds were discovered, yet a new binary containing said workaround has still not been released. The current release is not high-quality for modern Mac users. I was a Haskell Platform user until this cycle, but now that Cabal has sandboxing I've found that the platform really doesn't do so much for me anymore. Given a manifest of precisely which packages are installed I could pretty easily build a version of this that includes the Platform, and I'll probably give that a shot when they finalize what they're going to include. The reason I would do this is because this style of distribution requires no installation and will happily coexist with other versions of GHC and/or platforms.
Work like this is one of my favorite things about GHC: it provides a place to put theory to practice, and see how it works out for a wide audience.
It did not take months. The current HP is significantly over a year old, and the community worked around the OS X problems in days. I appreciate the effort put into the HP, but I don't see much upside in making excuses. I'm hopeful it will be better going forward.
Relatedly some months back I started a relaxed effort to make a replacement web site for Haskell. I've kept it to myself so far but it seems apropos to share concurrent tracks. Two motivations: * Haskell.org is embarrassing to link people to. I realised this when at a local nerd meetup group, when asked about Haskell, I noticed myself automatically omitting haskell.org from places to look at. * The wiki's just blah. I wrote a little page on it called [Handling errors in Haskell](http://www.haskell.org/haskellwiki/Handling_errors_in_Haskell) because someone asked about the usual ways they're handled and I thought I'd be a good community member and contribute to the wiki. But it looks poor and the formatting is all over the place. Rather than climb over stonewalls and wade through bike sheds that would only make me abandon the effort, I thought I would simply create another domain for presenting Haskell to people. If I were to propose it as a replacement, and by some miracle it would be greenlit, the domain could be 301 redirected. * The domain is [haskell-lang.org](http://haskell-lang.org/). This design brings back the glorious purple colors to which I've always been partial, from the original Haskell logo. * Here is [the above wiki page](http://haskell-lang.org/wiki/Handling_errors_in_Haskell) with a cleaned up presentation. (There is a simple request sent to haskell.org for `/wiki/*` pages, it parses the Wiki syntax with pandoc and renders it to HTML, at least for the pages that MediaWiki is kind enough to serve.) * Here is [a comp](http://chrisdone.com/comp.png) I mocked up in Inkscape for the landing page. The layout's a bit nobby and may change, but the content is thought through. It answers the kind of questions a newbie will have when landing on a language home page: * Any particular brand/logo? [header] * In a few words, what is this thing? [header] * What does it look like? I want to see code immediately. [header] * Can I try it right now? [try haskell section] * I'm still interested. Is anyone using this thing? [community &amp; videos, events section] * What are the selling points, over, say, ML or C#? [features section] * Where do I download stuff/find community/docs/news? [the main menu at the top] Not looking for feedback, and it [doesn't matter if people don't like it](http://en.wiktionary.org/wiki/a_camel_is_a_horse_designed_by_committee). Just thought I'd share as it's on topic.
Ok, so maybe I'm missing something here, but you say that threads and concurrent messaging is an anti-pattern. Yet, your `mvc` library is built upon `pipes` which is implemented using `STM` and `async`. Maybe I misunderstood it completely, but where's the big difference here? Don't want to offend you, I'm just curious because I don't understand it.
**Pervasive** threads and concurrent messaging is an anti-pattern. `mvc` forces you to factor any concurrency into carefully constrained `Controller`s and uses the types to forbid you from mixing your `Model` logic with your `Controller` logic. The goal behind separating threading logic from your business logic is to make your business logic pure and testable.
* I like the version without the flowers, I don't think they add much * The GHC version that ships with the platform should be prominent and listed first, the compiler is the absolute most important part of the platform. * Documentation should be in the navigation and easy to find. The current website has "Documentation" and "Library Doc" on the right side, these should be more prominent not just a tiny thing in the sea of links at the bottom
I've seen SPJ and Manuel Chakravarty use this style, if I remember correctly. I think it was just about reducing ambiguity that could be introduced by the layout rules.
https://www.archlinux.org/ This is an example of the web site that is helpful both for newcomers and current users. It has package search right there. I use it all the time. For haskell though i always have to go to a dozen of different web sites. hoogle, hayoo, fpcomplete. It has installation instructions. I use it from time to time when i need to install new machine. Because i forget things. It has prominent news section right in the middle. It has easy access to forums. For questions and general discussions on haskell i have to again go to a dozen of different places: reddit, google groups, stackoverflow etc. It has the best and most useful wiki i have ever seen. Even users of other linux distros routinely refer to it. For haskell know-how i have to again scour the internet: LYAH, realworld haskell, wikibooks, stackoverflow, google groups, various blogs. 
I know you aren't looking for feedback but this looks wonderful on my phone. Do love. Do want.
I like it -- its a huge step fwd from the current design. &gt; Not looking for feedback &lt;swallowed&gt; 
The `Monoid` instance for `Controller`s is using `orElse` under the hood for you. That's how you can combine multiple `Controller`s into a single `Controller`. The `mvc` approach one kind of free monad approach (the `Model` is a `Pipe`, which is special case of a free monad). However, I believe you are probably referring to another way to use free monads, which is to use them to replace all external interactions. The issue with this is that you will then have to write a scheduler to multiplex free monad calls so that each call in the free monad doesn't block your entire program. `mvc` lets you reuse Haskell's scheduler while still preserving testability.
(I know you weren't looking for feedback, but...) I really like this color scheme and design. Looks very clean. I also think you're on the right track in regards to what people will be looking for on the site.
Could this be integrated with cabal sandboxes somehow? You could have a fully localized Haskell environment for every project that way.
Yeah, you definitely don't have to use `mvc` per se! As long as it gives you ideas for how to restructure your program then I'm happy. :)
There's no need to do any integration, it's fully relocatable. Just drop it where you want it and set the PATH appropriately.
Personally I think so. At any rate, I appreciate you working on this, as I think the site does need a little bit of a face lift. 
If you are /u/chrisdoner, stop reading now. I like this design. The main page is much improved, and the wiki is a nice change, too. I'm not sure about the downloads page, but I don't know what to do there. It's better than the [haskell.org](haskell.org) link that just goes to the HP, but it still seems to demand a bit more reading than one might hope. I wonder if a top-level split for Linux/Mac/Windows might help. We should probably take this opportunity to talk smack about Chris since he's not reading any of this. Dude does good work. Who's next?
Mostly because this seems too crazy to be real, and you see joke proposals all the time. It's definitely an awesome idea, though I'm not a fan of the `$(...)` syntax.
Interesting observation.
&gt; https://www.archlinux.org/ &gt; &gt; This is an example of the web site that is helpful both for newcomers and current users. Ironically, Archlinux is one of the Linux distros which promotes using just `ghc`+`cabal-install` without any HP baggage...
The flower is horrific - looks cheap and is totally irrelevant. Clean, flat, and simple is the way to go. Thanks for your work!
Typically SPJ adds the braces and semicolons, and then I have to find an excuse to rewrite the code so that I can get rid of them again. It's sort of a game we have.
I will. But it'll be an example, as it's more of a use case. I wanted to play with all examples in drawille (and add shapes and things like that).
Awesome! I'd love to be able to hook up Cocoa to my haskell programs, however it's not clear what the objc syntax would look like. Are there any examples? Also, I know the project is young, but is there any indication of time/space overhead (at least in theory)?
Oh no! But the cherry blossoms are so great!
Agree 100% except I prefer the version with the flowers on top.
Wow! Thanks for doing this! Regarding the color scheme and graphics, I think the lighter blue is more inviting and open than the dark blue which, I think, looks more industrial. Personally, I like the cherry blossoms, but without them, I think the lighter colors are even more important.
Also, from the third design, the bolded TOC headers in the footer are really useful.
Unfortunately we're still seeing this issue despite the attempts at fixes. One way to see it still is to try cabal install git-annex in a fresh cabal sandbox. Resolving dependencies... cabal: Could not resolve dependencies: trying: git-annex-5.20140517 (user goal) trying: git-annex-5.20140517:+webapp-secure trying: git-annex-5.20140517:+webapp trying: git-annex-5.20140517:+dns trying: dns-1.2.3 (dependency of git-annex-5.20140517:+dns) trying: resourcet-1.1.2.2 (dependency of dns-1.2.3) trying: transformers-0.4.1.0/installed-ac1... (dependency of resourcet-1.1.2.2) trying: warp-tls-2.0.5 (dependency of git-annex-5.20140517:+webapp-secure) trying: wai-2.1.0.3 (dependency of warp-tls-2.0.5) trying: hamlet-1.2.0 (dependency of git-annex-5.20140517:+webapp) next goal: yesod-core (dependency of git-annex-5.20140517:+webapp) rejecting: yesod-core-1.2.15.1, 1.2.15, 1.2.14, 1.2.13.1, 1.2.13, 1.2.12, 1.2.11.1, 1.2.11, 1.2.10, 1.2.9.2 (conflict: transformers==0.4.1.0/installed-ac1..., yesod-core =&gt; transformers&gt;=0.2.2 &amp;&amp; &lt;0.4) rejecting: yesod-core-1.2.9.1, 1.2.9, 1.2.8, 1.2.7, 1.2.6.7, 1.2.6.6, 1.2.6.5, 1.2.6.4, 1.2.6.3, 1.2.6.2, 1.2.6.1, 1.2.6 (conflict: hamlet==1.2.0, yesod-core =&gt; hamlet&gt;=1.1 &amp;&amp; &lt;1.2) rejecting: yesod-core-1.2.5, 1.2.4.5, 1.2.4.4, 1.2.4.3, 1.2.4.2, 1.2.4.1, 1.2.4, 1.2.3, 1.2.2, 1.2.1, 1.2.0.4, 1.2.0.3, 1.2.0.2, 1.2.0.1, 1.2.0 (conflict: wai==2.1.0.3, yesod-core =&gt; wai&gt;=1.4 &amp;&amp; &lt;1.5) rejecting: yesod-core-1.1.8.3, 1.1.8.2 (conflict: wai==2.1.0.3, yesod-core =&gt; wai&gt;=1.3 &amp;&amp; &lt;1.5) rejecting: yesod-core-1.1.8.1, 1.1.8, 1.1.7.2, 1.1.7.1, 1.1.7, 1.1.6.1, 1.1.6, 1.1.5, 1.1.4.2, 1.1.4.1, 1.1.4, 1.1.3.1, 1.1.3, .1.2.2, .1.2.1, 1.1.2, 1.1.1.2, 1.1.1.1, 1.1.1, 1.1.0.1, 1.1.0 (conflict: wai==2.1.0.3, yesod-core =&gt; wai&gt;=1.3 &amp;&amp; &lt;1.4) rejecting: yesod-core-1.0.1.3, 1.0.1.2, 1.0.1.1, 1.0.1, 1.0.0.2, 1.0.0.1, 1.0.0 (conflict: wai==2.1.0.3, yesod-core =&gt; wai&gt;=1.2 &amp;&amp; &lt;1.3) rejecting: yesod-core-0.10.3, 0.10.2.2, 0.10.2.1, 0.10.2, 0.10.1 (conflict: wai==2.1.0.3, yesod-core =&gt; wai&gt;=1.1 &amp;&amp; &lt;1.2) rejecting: yesod-core-0.9.4.1, 0.9.4, 0.9.3.6, 0.9.3.5, 0.9.3.4, 0.9.3.3, 0.9.3.2, 0.9.3.1, 0.9.3, 0.9.2, 0.9.1.1, 0.9.1, 0.8.3.2, .8.3.1, 0.8.3, 0.8.2, 0.8.1, 0.8.0.1, 0.8.0 (conflict: wai==2.1.0.3, yesod-core =&gt; wai&gt;=0.4 &amp;&amp; &lt;0.5) rejecting: yesod-core-0.7.0.2, 0.7.0.1, 0.7.0 (conflict: wai==2.1.0.3, yesod-core =&gt; wai&gt;=0.3 &amp;&amp; &lt;0.4) This is using Debian stable's haskell platform and cabal.
Forgive me, dumb mistake
Seems to only be for ObjC and not regular C :(
Not the right type.
Are you trolling? That's an amazing number of false statements. Of course you can negate with an operator, - Of course you can pattern match on numbers. Of course you can have "infinite" integers. 
Alas, once a post is submitted you can't change its title.
This one seems to be because you have transformers 0.4 installed, so subsequent packages are trying to run with it, but can't because of other constraints. trying: transformers-0.4.1.0/installed seems to indicate to me that you already have it installed either in the sandbox or elsewhere. I'm not 'there' though so it is hard to debug by remote.
&gt; That's an amazing number of false statements. Bullshit. &gt; Of course you can negate with an operator The default `(-)` operator is binary. You can't negate with it, unlike with every other language. This is precisely why `negate` is defined in the prelude. &gt; Of course you can pattern match on numbers. The point I mean to make here is that `Int` and `Integer` are non-algebraic. You end up with stupid definitions like this: fact :: Int -&gt; Int fact 0 = 1 fact n = n * fact (n-1) What you really want is to teach this: fact :: Nat -&gt; Nat fact 0 = 1 fact (S n) = (S n) * fact n The distinction is subtle, and if you can't appreciate it, you might also be jaded enough to think C++ is a good language choice. The difference is the latter is total and it demonstrates what a function is: a choice of a value for each possible case. &gt; Of course you can have "infinite" integers. let x :: Int; x = 1 + x in x == 0 No you can't.
It's kind of inexplicable on the existing page, as well...
I feel like I'm feeding the troll, but maybe you're just sadly misinformed. Prelude&gt; let x = 5 Prelude&gt; -x -5 The type `Int` behaves exactly like an enumerations type with a very large number of constructors (similarly with `Integer`) and you can pattern match on it just fine. Your new complaint seems to be that the standard integral types in Haskell are not inductively defined, and that's true. But still: Natural&gt; let fact 0 = 1; fact (S n) = (S n) * fact n Natural&gt; fact 6 720 The type `Int` is a type of finite size numbers, so there's no chance it will support infinity. Again, you need inductively defined numbers. Natural&gt; let x :: Natural; x = 1 + x in x == 0 False So, it's true that Haskell's default integral types are not inductively defined, but they are only an import away. BTW, how did C++ enter the discussion?
The Objective-C syntax is exactly the standard syntax also accepted by 'clang'. Currently, there are three forms to embed ObjC in Haskell: 'objc' for inline code in Haskell expressions as well as 'objc_interface' and 'objc_implementation' for top-level declarations (typically @interface and @implementation declarations). There are four examples at https://github.com/mchakravarty/language-c-inline/tree/master/tests/objc (One of the, 'app', is a complete Mac app, albeit a small one.) The runtime overhead is the same as for C bindings using the standard Haskell foreign function interface (because that is generated under the hood), which Haskell uses for all system calls etc. There is no extra space overhead. However, if you pass complex Haskell values to Objective-C, the system automatically uses stable pointers for that. You need to ensure that you explicitly free them, when Objective-C land doesn't need the reference anymore (e.g., in [-dealloc]). Otherwise, you will leak Haskell heap memory.
BTW, with the new pattern synonym extension (not fully implemented yet), you will be able to use the S notation for numeric types. Then we can get an efficient implementation of natural numbers that looks like the naïve ones. 
Objective-C is a proper superset of C. You can just not use the Objective-C features. Then, you are left with plain C. Having said that the set up of the library is currently biased towards Objective-C (which is what I needed right now) — e.g., marshalling 'String' to 'NSString' by default. I am planning to provide a second interface that exposes the C part only, but haven't gotten around to that yet.
I had no hope of converting you, but at least others reading your comment will not be led astray. 
I would claim that I was expert-level in ruby and python at the time I learned haskell, with earlier expert-level knowledge of java and, um, well, an ancient (pre-visual) version of basic. Haskell was quite a challenge to learn, when I got around to it - but I knew that its ideals aligned well with what I had determined led to maintainable software through many years of experience. That's the part that led me to keep working at it.
&gt;Ironically, Archlinux is one of the Linux distros which promotes using just ghc+cabal-install without any HP baggage... You can count me in that bucket: https://github.com/bitemyapp/learnhaskell
It has actually been done many times. Some good Haskell programmers started this way, e.g. Dylan Lukes started Haskell with no real previous experience, but most of us came in the back door from other languages.
I came into Haskell via Coq. Which door is that. ;)
I'm not sure but I'm pretty sure the writing on that door is in French. ;)
So honorable! I'm going to faint.
+1 for the HaskellWiki color one.
I am thinking that the new `coerce` mechanism would be useful for safely stripping away `Identity` from more complicated types.
From a design perspective, I definitely like the mockup. I think it's really nice and it is definitely how I try to lay out mocks for clients. Embedding Try Haskell is especially cool. It has the sort of content I would expect to motivate newcomers, and anything that will simplify getting to the goods of making Haskell "happen" for them is a boon. The reimagining of the wiki is especially motivating. From a "grumbly jerk" perspective, I think it makes Haskell out to be a Node.js plugin (to appeal to a kind of ugly /r/programmingcirclejerk attitude). 
I like the flowers. I don't like them with the blue color scheme. +1 for cutting the clutter on these pages, their current incarnation is way too jam packed with tons of links that nobody really cares about.
Here's the feedback you don't want, if not for you, then for others who may be thinking about some of the same issues: A) Haskell is a community effort and if this redesign will ever succeed, eventually the peanut gallery will want to weigh in. B) It is pretty C) It is vastly wasteful of screen real estate. D) I actually think the http://www.ocaml.org site is pretty nice these days. E) I don't care if we show users code immediately, honestly. F) Hiding most of the content behind those dropdown menus isn't very wise, in my book. I'm much less concerned with first-time user experience than with having a bunch of useful links upfront to help navigating to key sites and information. G) The "first impression" matters way less to me than good documentation as to how the Haskell ecosystem works and how to navigate it, and that's partially a design question, and partially lots of helpful text on various pages, all of which needs to be put together.
that you have to use template haskell is not so nice of course (bad for e.g. cross compiling). i don't think there is a way out without something integrated deep inside ghc or maybe a plugin though.
I've been pondering this question for quite some time now. I was on the fence until just recently when a friend asked me to help her with a C programming assignment for an online programming course she recently started taking. This person is a professional musician, but has always been fairly computer savvy and at times has contemplated the possibility of a career in tech. The assignment she was having trouble with asked her to print a pyramid of '#' characters on the console with a user-defined height. She had been struggling with the problem for several hours before she asked me for help. Here's what she was trying to do: printf("# * (count + 1)"); Now one could argue that the course she was using didn't present the concepts well. Or possibly that programming is just very different from the way she's used to thinking. But I just couldn't help thinking that she likely wouldn't have been stuck like this if she had been learning Haskell for the simple reason that types are front and center in Haskell. Yes, the argument to printf is of type char *, but nobody tells you that on your first day of programming C. With Haskell she would have learned types long before she got to a problem like this, and likely would never have had the flawed mental model that she had when struggling with this C assignment. That one anecdote has tilted the balance in favor of Haskell for teaching beginner programmers for me.
the flowers pic is too low quality.
I doubt that cross-compiling is a problem in conjunction with TH. It has just not been done yet. Of course metaprograms using IO may get hairy.
yes
Finally, Haskell is becoming mainstream!
Rather than the flowers, how about the infamous picture of the Haskell committee from the late 80s, with SPJ, Wadler et al. I can't find it on the web, but I've seen it appear occasionally in talks about Haskell's history. A greyscale version (i.e. unedited!) of that picture in place of the flower would pay homage to the language's origins and is perhaps more relevant than a pink flower, as nice as it is.
Thanks for the effort in making this! I'm going to be completely honest: I don't think it has the professional touch (look at sites like http://nodejs.org and https://www.ruby-lang.org), **however** it's great to know people acknowledge our current web page needs a little bit of remodel and I look forward to what we can come up with. I will also be trying my hand at soon (tm).
Can you take a screenshot?
This is great! Do you think you could upload the inkscape files so we can css-ify it? :)
&gt; Haskell's design had nothing to do with pedagogy and everything to do with the lack of a free research platform. In the late 80s, computer scientists wanted to mess around with programming in non-strict languages, but that entailed either designing your own language from scratch or paying royalties on Miranda. This is partially true. The reason Haskell came about was that everyone doing lazy functional languages had their own language, so we decided to have a single language instead. There was little worry about paying for Miranda from a research point of view; very few did. But early on in the Haskell design the need for a teaching language was also recognised. The pedagogical aspect is why `n+k` patterns were introduced.
If she wrote what you did then that would still compile in Haskell of course. But I imagine she at least got as far as `"#" * (count + 1)`, of course this would work in ruby (and presumably several similar languages), so I'm not sure that it's anything other than an indictment of C as a beginners language 
Which OS? For Debian, I am using http://deb.haskell.org
Sorry, OS X, 10.9
I heard that someone has just given you another [option](http://www.reddit.com/r/haskell/comments/26bx5n/haskell_for_mac_os_x_selfcontained_relocatable/). Even though I am longing for an official release.
I saw that option already (on reddit). That's what caused me to go look for updates, discovering the issue I mention here.
Yeah, I know about this --- but my thread is about the issue where, if one goes to just download the latest GHC, one is advised to download the platform instead.
I would say wait for the next Haskell Platform release, unless you have too much free time and love to torture yourself by installing things from source.
It was a while since I did this, but it's really not all that difficult to build it from source. Basically just download this bindist: http://www.haskell.org/ghc/download_ghc_7_8_2#macosx_x86_64 and do the standard ./configure make make install Although I have to say, there are several packages that are not built for 7.8 yet and if you want everything to work, wait until the Haskell Platform serves up 7.8, then the ecosystem should be fairly stable. GHC isn't really like the latest Rails version where you want to upgrade asap. There's nothing wrong with sticking on 7.6 for a while unless you want some specific 7.8 feature.
[This might be a good option,](https://github.com/begriffs/haskell-pair) especially if you're already using Vagrant.
Yeah, I was afraid that would be the answer. Would be nice if the platform was kept in sync with the GHC release. Thanks everyone for responding
Fair enough --- I'll wait :-)
The right answer is to wait the next Haskell Platform release. But, here is how I did with not so much pain: - download the latest GHC for OS X: http://www.haskell.org/ghc/download_ghc_7_8_2#macosx_x86_64 follow informations inside to install it - you have 7.6.3, so you have a cabal, you should upgrade it to the latest version: `cabal install cabal-install` - be sure the you use this latest version in your PATH (which cabal /Users/&lt;you&gt;/.cabal/bin/cabal ) to do so add "export PATH=$HOME/.cabal/bin:$PATH in your .bashrc file, or your .zshenv if you use zsh). And now you should be ok.
Online dictionary?
I am seriously considering starting to teach it to my niece, who's 12 and has absolutely no idea about programming, the next weekend. I think it should kinda answer your question. 
That's not possible - last I checked some things on hackage had issues with the new ghc - I rather wait some time for a stabilized lib than having state or the art with lot's of pain
&gt; Would be nice if the platform was kept in sync with the GHC release. It's the whole point of haskell-platform is that it gives a more stable experience; broader then only GHC, but also including the some common libraries. Those libraries are currently not upto that level of stability yet; so there is no new release of HP.
That being said, GHC is pretty straightforward to build (at least on Linux; I can't comment on OS X). If you have a burning desire to try the latest and greatest don''t let the fact that you need to build from source discourage you. On my Linux box (where I admittedly already have the build dependencies installed), $ git clone git://git.haskell.org/ghc $ cd ghc $ git checkout -b origin/ghc-7.8 $ ./sync-all get $ ./boot $ ./configure --prefix=/opt/ghc-head $ make -j4 &amp;&amp; make install [get some coffee] $ PATH=/opt/ghc-head/bin:$PATH $ ghc -V The Glorious Glasgow Haskell Compilation System, version 7.8.1 To upgrade, `./sync-all pull` and follow the same steps starting at `./boot`.
I rather wait. By the time it gets in the Haskell platform there are enough blogposts so I understand what the new features do and I can do more with it than just mess around with examples. If you're a noob you've probably have enough to discover first. You're really not falling behind if you're using the latest Platform version. Most current research is in the form of libraries anyways.
What's the point(s) you think I should concede?
http://ghcformacosx.github.io/ ships with the cabal-install binary, so you don't need to do any of that.
Correct my advice was applicable for all platforms. For the author, he can use that. Thanks for sharing, I did not even know it existed.
Note that the fold operations from my `foldl` library copy the trick from `Data.Foldable` to implement left folds in terms of right folds. They still run strictly in constant space but they get the benefit of list fusion.
nomeata points out that `foldl` is fused in GHC HEAD. I'm not sure if it's using that same trick or something else.
Pattern matching on numbers desugars to an equality test, which seems like cheating to me. I would expect real pattern matching to not incur an `Eq` constraint.
See http://www.reddit.com/r/haskell/comments/26fdxq/call_arity_pdf_a_new_analysis_implemented_in_ghc/
Pattern matching on numeric literals is indeed a bit strange. It's a definite irregularity in the language, but it's a convenience. An alternative would be to restrict pattern matching to concrete types, like Int.
Haskell does catch pattern match errors (forgot to match some patterns, or added new patterns and didn't match them everywhere). It catches logic errors. For example, if you encode red-black-tree's invariants in the tree's type, then logic errors that break the invariants are also type errors. But various logic errors manifest as type errors. e.g: Forgetting to flatten a list will yield a list of the wrong type, and is easily considered a logic error. Mozilla is not considering Haskell because Haskell is not a panacea. Haskell is great for a wide spectrum of programs, but not all of them. In a web browser, the trade-off between programmer costs and resulting performance leans far more towards the latter -- so it makes sense to use lower-level languages, to squeeze out more performance via more programmer effort. For many applications, it makes sense to spend 5 times less programming effort even if performance is 15% worse. Also, Rust is copying quite a few good ideas from Haskell. &gt; Programming language does not make your program safe or correct When the programming language rules out all runtime errors of a certain kind (for example, there can be no places where the programmer forgets to check for nil), it does not make the program safer or more correct? &gt; When you design your program to be thread safe So immutability guarantees given by the language everywhere, including all third party libraries, does not help you create thread safe programs? &gt; it will be thread safe regardless of language When your language allows mutability to hide everywhere, under the hood of every library, that doesn't make it harder to be thread-safe? &gt; Language is only a tool, If it is a good tool it will make transition from paper to machine easier, if not And a language that allows concise expression of your program invariants in a formally-verified way, while also making the program easy to express concisely and perform reasonably well, does not help the transition?
Well, it was the URL that started this thread. :)
I'm trying to write more, so I made a writeup of a little weekend project. I'd love any commentary on style or substance. Thanks!
Explicit and qualified imports for everything, or qualify only in case of conflict? And ouch, 59 lines of imports in the register allocator when using -ddump-minimal-imports. If I use a lot of functions from a package, should I avoid selecting each one explicitly and simply import all?
I'm pretty sure 7.8 is in homebrew as development version, you should be able to install from there seamlessly.
Thanks, I could clean up the code generator by making a helper function for creating a MIPS instruction. I took your advice and compacted boilerplate and generated a .cabal file. If I were to continue working on this, I would probably also wrap the results of the parser and type checker in Either like you suggest.
It refers to a series of automatic transformations that let you compose certain list functions (e.g. `take` followed by `foldr`) and avoid creating intermediate lists. These transformations are implemented with rewrite rules, and you can read about them in 7.21.4 here http://www.haskell.org/ghc/docs/7.8.2/html/users_guide/rewrite-rules.html
One thing is that MathJax seems to not work with iOS Safari. Sad :(
You might want to take a look at "Algebraic Dynamic Programming": https://web.archive.org/web/20140515002439/http://bibiserv.techfak.uni-bielefeld.de/adp/ Their approach takes the basic idea also found in your post much farther. Complete with a Haskell DSL for describing and solving rich dynamic programming problems in bioinformatics. The probably most accessible resource, including exposition of the Haskell implementation of the DSL, is https://web.archive.org/web/20120507015800/http://bibiserv.techfak.uni-bielefeld.de/adp/ps/adp_discipline.pdf 
Oh hey, that looks great. Didn't see it before. Especially because the constant factors of using my exact method don't seem to be too great :P. It'll probably be really useful when I finally implement and get around to tuning and optimizing the tree-edit-distance algorithm I mentioned.
Unless you **just** started learning Haskell, get GHC. If you are here, reading haskell news, many of the features you'll read about require the latest GHC. In addition, HP support is often an afterthought for new libraries. As for stability, you can handle it.
Site is currently offline: &gt; Internal Server Error &gt; &gt; static/markdown/home.md: openFile: resource exhausted (Too many open files)
Structures Haskell Mode is one of those rare beautiful pieces of software that suggests we might be living in the future after all.
It doesn't work with GHC 7.8.2 since light-haskell requires base == 4.6.*.
This is really a needed workflow that I still don't get how to do well in Haskell. Typically, the compilation step makes your "red-green-refactor" process suck. Without judging whether using tests like that is a good idea, a pain-free, fast testing cycle is a desirable thing.
Another approach is Varmo Vene's work with Kabanov on dynamorphisms. http://math.ut.ee/~eugene/kabanov-vene-mpc-06.pdf
I like them as well!
No, she didn't seem to have even gotten that far, which I think is an indictment of not having the types of things front and center in one's mind. She had no idea of the context of things inside quotes and outside them. Types give you that context.
Wow, 32 commits and 3 releases for about 20 lines of code. Anyways cool first project, mine was significantly less useful.
Yes. The compilation step makes the red-green-refactor process suck a little
That reads more like an explanation for a competent programmer who has never heard of the topic than an explanation for a 5 year old. You should be more condescending and ask where their parents are.
It's not necessary to build GHC yourself in this case. There are [binary releases for OS X](http://www.haskell.org/ghc/download_ghc_7_8_2#macosx_x86_64).
Actually I have a few things to fix before that vagrant config is ready.
Whoops, sorry!
Yes. You should ideally have no experience whatsoever, so that you don't get brain-damaged by other languages first. I remember one of my partners from college, who didn't have programming experience prior to the CS program, and who had an easier time learning the functional languages that stumped the other "smart" kids in her graduating year.
Wait...shadower itself doesn't use doctests? No eat-your-own-dogfood? Either way, cool concept.
Haha. Thank you! I'll add doctests
The docs use the same example twice in explaining what the function does. I'm guessing foldl was meant to be foldr in the working example?
Not for a newbie probably, but just want to tell you using nix is a good solution to have multiple version of ghc. You do not have to install NixOS on your system. One can install nix on any linux or darwin system ( from http://hydra.nixos.org/release/nix/nix-1.7 : I recommend to use distribution independent tarball. it's very simple to install). By default, it's ghc-7.6.3, but one can change in .nixpkgs/config.nix like ( this is an excerpt from my .nixpkgs/config) packageOverrides = pkgs: rec { ghc782env = let hsEnvGHCDev = with (pkgs // pkgs.haskellPackages_ghc782); ghcWithPackages (self : [ cabalInstall ]); in pkgs.myEnvFun { name = "ghc782-dev"; buildInputs = with pkgs; [ hsEnvGHCDev ]; extraCmds = '' export NIX_STORE=$NIX_STORE ''; }; } then inside ghc782env (which can be loaded by load-env-ghc782-dev), you can test fancy ghc 7.8.2, but outside, you're using ghc-7.6.3 safely. I hope that other nix users also share their tips in this thread. 
Imagine you have a list of integers `[Integer]`, say `[2,3,5,7,11]`. Now you wish to transform them by adding 1 to them: `map (+1) [2,3,5,7,11]` and in another place you want to negate the previous result. This amounts to doing `map negate (map (+1) [2,3,5,7,11])`. But it turns out that the intermediate result allocated five cons-cells in vain, because the above expression can be written as `map (negate . (+1)) [2,3,5,7,11]`. This transformation is called *fusion* (or *deforestation*) and can be automatically performed by the compiler whenever the two element transformers are visible to the it. This is because `map` is defined in terms of `foldr`. For `foldl`-based functions such transformations were not easily possible, for reasons described [in the paper](http://www.joachim-breitner.de/publications/CallArity-TFP.pdf).
&gt; The list never has to completely exist in memory That is not true in this case, because of the use of `length`
If something like syntastic can be run on every write without slowing me down, it seems like using something like dispatch.vim to run tests on every write and show the results should be tenable.
Recursion schemes getting quite a lot of love in the last couple of weeks. Me like.
 GHCi&gt; let listOfChars = "abcdefghijklm" GHCi&gt; listOfChars !! 0 'a' GHCi&gt; listOfChars !! 1 'b' GHCi&gt; listOfChars !! 2 'c' GHCi&gt; listOfChars !! 3 'd' GHCi&gt; listOfChars !! 4 'e' GHCi&gt; listOfChars !! 5 'f' GHCi&gt; listOfChars !! 6 'g'
Ah thanks, I forgot lists are indexed from zero. Problem solved
In the spirit of give a man a fish / teach a man to fish: you can [find out using Hoogle](http://www.haskell.org/hoogle/?hoogle=%21%21).
I didn't know this was a thing.You saved me so much time. Thanks 
I use doctests all the time, and I'm pretty sure it doesn't require a compilation step. In fact, my workflow is: edit and run doctest (on the current file only) until they pass, then compile (and run tests) on everything.
[This is the standard, dreaded Monomorphism Restriction](http://www.reddit.com/r/haskell/comments/25y1u5/why_do_these_two_definitions_have_different_types/chlshsf) which held as default in GHC prior to 7.8. The basic story is that whenever you define a top-level definition which isn't "obviously a function" Haskell will try to coerce it to a monomorphic type even if its most general type is polymorphic. In your example, `let g = groupBy (==)` doesn't have argument binding occurring on the left-hand side and so GHC is monomorphizing it. However, `let g xs = groupBy (==) xs` is "obviously a function" (again the variable binding is key here) so GHC realizes that the polymorphic version is intended. When you write out the full let form the MR doesn't get invoked because it's not a top-level definition. You can avoid the MR by either (a) using the NoMonomorphismRestriction pragma or (b) by always writing type declaration for your top-level types. The latter is more common as it's just good Haskell style to do so. let { groupBy :: Eq a =&gt; [a] -&gt; [[a]]; g = groupBy (==) } (Note: sorry for the repeat but this question does come up repeatedly. Maybe there should just be a sidebar link?)
I think a link to the [Haskell-wiki](http://www.haskell.org/haskellwiki/Monomorphism_restriction) might help here ;)
You could at least [link to the relevant XKCD](http://xkcd.com/1364/). :)
[Image](http://imgs.xkcd.com/comics/like_im_five.png) **Title:** Like I'm Five **Title-text:** 'Am I taking care of you? I have a thesis to write!' 'My parents are at their house; you visited last--' 'No, no, explain like you're five.' [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=1364#Explanation) **Stats:** This comic has been referenced 35 time(s), representing 0.1640% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)
Hey, it's no problem. It's a really frustrating and confusing event to run into the MR and that inconsistent behavior is part of why it's been disabled by default in 7.8. And yep, `NoMonomorphismRestriction` is exactly what 7.8 has on by default now.
Half of all type error questions here and on stack overflow can be answered by "the dreaded monomorphism restriction".
Or search through all of Stackage at: [https://www.fpcomplete.com/hoogle?q=!!](https://www.fpcomplete.com/hoogle?q=!!).
Yes. You can use it even in ghci h&gt; let g = groupBy (==) h&gt; :t g g :: [()] -&gt; [[()]] h&gt; :set -XNoMonomorphismRestriction h&gt; let g = groupBy (==) h&gt; :t g g :: Eq a =&gt; [a] -&gt; [[a]] 
That looks great! But does it support instant mode like Hoogle?
That's correct. I meant generally :) Doctests are neat. Hope you like shadower
I'd actually like to see this sort of thing in general, and beyond Haskell... I'd really like to be able to assert that a certain optimization has occurred and be warned/errored at compile time if not. Ideally the error would even tell me "why", though that obviously requires more work. It's not the sort of thing you should use all the time, but when you're trying to optimize something to the nth degree, it would be nice to have something like this. (Especially if it can give you the "why".) (If it sounds silly, don't just consider the static case where I'm making one change to the code. Consider an ongoing process. For instance, if I've got something fusing nicely but I'm still working with it, perhaps along with a lot of other things as I'm trying to optimize, I'd like to be able to assert that it stays fused even after performing other manipulations. Compiler optimizations are nice, but they're so fragile and opaque... one harmless-seeming little change and you can break an optimization you're depending on without even realizing it. I'm calling for a _power tool_, not something for routine use.)
System F should be easy if you've already implemented the simply typed lambda calculus; you just add type level lambdas. AFAIK that's all System F is, I don't think anything more is required. For example, consider the polymorphic identity. In the simply typed lambda calculus you must have a separate identity function for each type, despite them being "the same". In system F you do: Λ α -&gt; λ (a : α) -&gt; a Haskell hides this implicit type parameter from you, inferring it. In deptype languages like Agda you will need to pass it, although support for implicit arguments can make this transparent. The Λ operator is the type level lambda. Λ α -&gt; t has the type forall α, t, so your type structure is also slightly more complex. Martin Lof Type Theory is considerably more complex, though.
Excellent! I think function transport across ornaments is incredibly powerful , and have been wondering why there hasn't been more hubub about them. I'm glad they're being brought down to a wider community for "practicality testing". Sad it's not haskell, but shouldn't be too bad to port :P