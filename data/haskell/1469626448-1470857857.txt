The site is using `-webkit-filter` to decrease the brightness of the image and to apply a slight blur effect. Here's how it looks in webkit browsers: https://i.imgur.com/OTDqTXS.png It should work in FF with the unprefixed `filter` property. There's also a neat tool called [autoprefixer](https://github.com/postcss/autoprefixer) that will deal with a lot of this CSS nonsense for you, so you can just write it the "right" way and it will generate all the required browser prefixes for you. Or, well, "premultiplying" the image would not be unreasonable here.
I rescind my previous comment. It's much more pleasant to generate XML like a savage with Haskell than using the C# library.
Perhaps we should consider objects to be maps from strings in a programming language to turing machines, and morphisms to be maps between strings in two programming languages such that the triangle commutes ([The category of strings in programming languages over turing machines](https://en.wikipedia.org/wiki/Comma_category#Slice_category)). That would also capture equivalence in time/space requirements, and again only very close languages could be proven isomorphic.
Pfft, did _you_ make $300k on stat-arb trading high school? Did you even know that stat-arb stands for statistical arbitrage? I fuckin did. I fuckin did so hard.
&gt; As a personal note, I'ld like to note the irony of your no-fragmentation position considering you were the one silently forking cryptohash to cryptohash-sha256 I don't think that's ironic at all since you increased the number of dependencies in cryptohash, deprecated it, and pointed people at a cryptonite. The package was forked because it was being used by hackage-security and cabal-install where it is important to minimize dependencies both for ease of bootstrapping and to reduce unnecessary version number progression caused by the evolution of unused features. Adding dependencies can cause very real problems, as can be seen from [this issue](https://github.com/afcowie/http-streams/issues/96#issuecomment-234166078). Furthermore, fragmentation isn't even an issue here since cryptohash-sha256 isn't being marketed as an alternative to cryptonite or anything else. It exists solely to fit the needs of these low level packages.
Whatever, did *you* make a couple million with a dope Bitcoin mine like I did in high school? And did you drop out of **MIT** to pursue the virtuous goal of making spreadsheet software for the prestigious Wall Street? Also, I love Haskell (couldn't have made it this far without it...for some reason...) so just shut up and be thankful for the privilege of using a language that you are \#Passionate about. Also did I mention people on the team won the Math Olympiad?
Make a team of Java|PHP|Ruby|Python devs productive in Haskell in the same time that it would take me to make a team of Java|PHP|Ruby|Python-skilled devs productive in one of the other aforementioned languages. The [Haskell book](http://haskellbook.com/) is making a huge difference in bringing this time down. Thanks Chris and Julie (authors)! 
I'm not familiar with velocity, but usually template engine offer limited flow control whereas Haskell EDSL offers full Haskell power including maybe, pattern matching, monads and functions. With function you can define your own "block" and use then as if they were native edls keyword and redefine your own layer of EDSL (to generate tables, charts for example). I think velocity as some type of macro but would be surprised if they come as powerful as full Haskell.
This kind of thing is a double-edged sword. On the one hand: it allows us a lot of flexibility in specifying things. On the other hand: all users of this file will have to be much more sophisticated to use it correctly. I'd err on the side of too simple, at least initially.
Emotion becomes a factor like any other. That's why empty sentimental and emotionally charged posts so often beat out more down to Earth or objectively important links. Reddit, like any market would, places a high value on things which highten emotional state. That is closer to what markets in general optimize compared to some dated notion of rationality. 
I'm not the right person to ask, but I imagine something like Coq where even "hello, world" needs a team of 20 professors to convince to the type system that the program really does the right thing in even the face of disaster.
I have no clue what your papers are about, but based on your contributions around Reddit I definitely want to read them!
I'm talking about what the community likes to talk about. I make stuff and use Haskell in my day job. &gt;More making stuff and showing others. Libraries, personal projects, whatever. Code.
basic fact check: cryptonite has as many deps as cryptohash (abstracting away GHC core packages like deepseq and integer-gmp). hackage-security has about 4 time more deps than cryptonite, same goes for cabal-install (2*). but heh we all know how this goes; this isn't about facts; at least, stop pretending it is.
Oh, I get it. Thanks. :)
Sure, chatter is a relatively small collection and includes a lot of POS tagging/tokenizing helpers. My goal is to have this be much more explicit about the algorithms used (edit: more about the algorithms themselves, and less about the helpers for tagging/tokenizing/chunking) and have it complement chatter, not overlap edit: I also want to add that Chatter has a lot of working parts. I'd like to take a simpler approach and have each module reflect a different algorithm to essentially accomplish the same task
`x = record.field.field.field` and `record.field.field.field := x` with no imports, no libraries, and one obvious way to do it.
Hey, thanks for letting us know. The no-bullshit explanation: A lot of people value working with a bright team, and some people are impressed by "MIT," "dropout," or some combination of the two. If you don't mind sharing, what exactly made you hesitant about joining a team of 4 MIT dropouts? Is it the connotation of the word "dropout?"
Does remote contracting work sound interesting to you at all? If so, we're still happy to consider you.
I was referring to [this twitter conversation](https://twitter.com/kamatsu8/status/752488026022879232) and I regrettably have never had a POPL paper accepted, although it's not for lack of trying ;) I'm presenting my research at ICFP this year if you want to learn about what I do.
batteries included: having to import and chose libraries to be able to do anything
Seriously? I did check my facts. This was the state of cryptohash before the changes that motivated the fork: http://hackage.haskell.org/package/cryptohash-0.11.7 It has four transitive dependencies: base, byteable, bytestring, and ghc-prim After version 0.11.7 you added cryptonite as a dependency, bringing the transitive dependency list to: base, byteable, bytestring, ghc-prim, deepseq, integer-gmp, cryptonite, memory, and Win32 when building on Windows. The important new ones are cryptonite, memory, and Win32. So yes, it does indeed have more dependencies than the version of cryptohash that had been targeted. Also, cryptohash-sha256 eliminated unnecessary cruft, bringing the dependency list down to just base and bytestring. Comparing the hackage-security dependency tree to cryptonite is apples and oranges--they're not doing the same thing. But for cabal-install's purposes cryptonite and cryptohash-sha256 are.
Wow, that's a much bigger problem indeed! How do mathematicians manage to communicate with each other? Does it ever happen that a mathematician accidentally builds on top of proofs written by authors which used different definitions, and that as a consequence the combined proof is not correct?
To raise a number to a power, you multiply it by itself a number of times. Too see if a number is prime, list it factors. If the only factors are 1 and itself, it's prime.
Bindings. e.g. Google Cloud Speech needs grpc for streaming speech recognition. Official bindings exist for Python and Java, partial bindings for others, nothing for Haskell. 
I don't think factoring 10^74207281 -7 is feasible.
http://www.haskellforall.com/ by the author of `pipes`. [lens](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) [free monads](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) [shell scripts](http://www.haskellforall.com/2015/01/use-haskell-for-shell-scripting.html) 
&gt; Being able to throw a quick log statement (as in console.log etc.) anywhere without having to deal w/ monadic IO. Debug.Trace?
The dependencies doesn't add up: cryptonite replace cryptohash, memory replace byteable. Win32 is pretty much a given when building on windows, which is why it appear in cabal-install too. In any case, I'm not comparing apple and oranges; I'm well aware of the difference of functionality, but when I'm actually looking at the transitive list of cabal-install (or hackage-security) dependencies, this is easy to tell that number/size of dependencies wasn't such a concern after-all.
Regardless of what your opinion is on the dependency issue, your libraries were clearly changing significantly and cabal-install wanted stability and simplicity. Hence, the fork. https://github.com/well-typed/hackage-security/issues/152 https://github.com/well-typed/hackage-security/pull/153#issuecomment-209329628
Agreed! I also suggest each addition result in a blog post, so you have both RSS and background details. 
Here are some observations: it's base 10, no need to take a power; the exponent is a prime; and you're subtracting a prime. A little ground work with your fave search engine may help you.
This is an extremely large number! You will probably need to do some math in order to convert the problem into a form which can be solved by a computer. Hint: is this number divisible by 2? by 3?
It's not prime. 10^x mod 3 = (10 mod 3)^x = 1^x = 1 for all positive integer x. 7 mod 3 = 1. So 10^74207281 - 7 mod 3 = 0. It's divisible by 3, and is not prime. Also, even if it weren't trivially composite, you wouldn't find a prime that size by any means currently known. It's got 74,207,281 digits, more than three times that of the currently known largest prime. And it's not a Mersenne number, which makes it even harder.
In general: Short compile times. From C++, I miss, essentially, `KnownNat` inference; if `n` is an Int template argument, then I know that `2*n+1` is one as well, and can use it as such. With Haskell, you need a plugin to do that, and I wonder why it's not in HEAD.
Perhaps this kind of constraint could exist in the surrounding ecosystem, such as hackage, rather than in the package itself. After all, it is an emergent property of the collection of packages as a whole.
The ability to easily print any value, Show or not, when debugging. 
Yeah, where you're never sure if your debug statement will execute, because of laziness. 
It is not false. cryptohash versions 0.11.8 and 0.11.9 have a larger transitive dependency list than cryptohash-0.11.7. This is a fact, and is what I originally claimed. I'm not going to argue about whether the extra dependencies are significant or not.
How about a CVE field, given that this is the standard for identifying security vulnerabilities in the broader software world?
Given how amazingly great `lens` is, I'd import it a million times over before I settle for the weak accessors of other languages.
I think it's a great idea, though I may recommend making it an optional field still. I'd be interested in hearing arguments on whether or not it should be required.
I have been into Scala and FP professionally for 5 years, and as a hobbyist for 7 years. I also know Clojure and Haskell decently that I can jump on a project using them right away. How can I get in touch?
How are single method interfaces sufficiently different from some of the Haskell type classes in this regard?
Thank you for your comments :) I'm definitely going to take your advice on the corpus generation being incremental. As for the other stuff, I'm going to keep a list datatype for the [Label a], but FList is going to be made into a hash map. Error will be fixed, it was kind of just a placeholder - thank you!
If he wanted feasibility, he would have asked for it.
bang!
Unfortunately I did not enjoy reading the paper: it's does nothing but complain, it doesn't deliver a good story, and performs no analysis of the underlying causes. I was not familiar with Voevodsky's programme, but mechanically verifying papers does sound like a good idea, whether using HoTT or not. Wasn't there a CS conference which started to impose the requirement that all theorems stated in the papers they publish must be mechanically checked? I thought it might be POPL, but I couldn't find a reference corroborating it.
In my experience, I've never had trouble getting a CVE number for a security vulnerability in software I've written (in fact, most of the time other people do ti for me). But there is definitely a bit of a lag.
Sounds like your pro-Haskell points are largely shared by other functional programming languages.
The debug statement will be printed deterministicly when the associated value is forced. That's the best you can hope for without turning Haskell into another language entirely. I find that it's usually more than sufficient for debugging purposes. In a monadic context you can use traceM, which has stricter ordering guarantees.
I think Haskell needs a different kind of graphical debugger than strict languages. Rather than stepping through the code as such this debugger would show the relations in the data as a graph centered on the next piece to be evaluated: the path back to the main definition, unevaluated thunks (as source code) with their data references, and evaluated data. Attempting to step through Haskell source code seems likely to prove an exercise in frustration, since the evaluation model does not follow the order of the code. The debugger should instead attempt to visualize the process of evaluation.
Intuitive single-stepping, sensible stack traces, and impure debug logging are all more-or-less incompatible with laziness-by-default, unfortunately.
That could be something for a subreddit.
Here's the part that put me off: &gt; You'll be joining a team of 4 MIT dropouts (among them, one owned a multimillion-dollar Bitcoin mine in high school, two were USA Math Olympiad winners, and one made ~$300k on stat-arb trading in high school) At the risk of being excessively crass, what you’re doing is the equivalent of bragging about the size of your dicks. It will put people off for similar reasons. In a perfect world, under the ‘Who we are’ section of a job ad, I would find enough information to answer the question ‘What would working at $COMPANY be like?’ A list of accomplishments of the owners of $COPMANY gives me no useful information when those accomplishments are unrelated to running a business. I think a much better approach would be to talk about what you value as a company and what that means for your employees. As an illustrative example, thoughtbot does this pretty well (admittedly with the addition of a healthy amount of marketing jargon, which I could do without): https://thoughtbot.com/jobs At the end of that page, the reader has a fairly clear idea in their head of what working at thoughtbot would be like. 
Happy to help. It's not something anyone really wants, more an unfortunate side effect of the (otherwise quite elegant) way data types are represented in memory. You could of course special-case it and say that any `data` type automatically gains `newtype` semantics if it fulfils the conditions for a `newtype`; and a less academic language than Haskell might well have done so. But then you'd have a weird corner case where your data behaves differently depending on how many fields and constructors with which strictness annotation it has, which is also annoying.
I frequently do this with [`&amp;`](https://www.stackage.org/haddock/lts-6.9/base-4.8.2.0/Data-Function.html#v:-38-). It can get pretty silly due to the lack of record name spacing. a &amp; aB &amp; bC -- versus a.b.c Plus it doesn't help with updates, unless the fields have associated `setX` functions.
Maybe so! Other functional languages are good too :-)
We should absolutely have that! At the minimum, ghci should have it.
&gt; As a personal note, I'ld like to note the irony of your no-fragmentation position considering you were the one silently forking cryptohash to cryptohash-sha256, thus fragmenting further into some random excuse of lean package (or deprecated, or whatever random stuff you come up with) Please, everyone just walk away from all the "discussion" that sprang from this...
a rich scientific computing, numerics, data analytics ecosystem. hmatrix works ok, but i'd like more than a poor-man's matlab/numpy. haskell could be amazing in this domain if there were a critical mass of userbase. 
It may have worked out ok but no longer serves a compelling purpose and is basically deprecated. I think at one time it was very beneficial - particularly for users on Windows. It often lagged far behind compiler releases, and the anchoring benefit is now provided by Stackage.
&gt; and is basically deprecated. Oh? Interesting. Is there anything I can read somewhere to learn more about this? &gt; the anchoring benefit is now provided by Stackage. Just to confirm my understanding here; `stack` is similar to `cargo` or `bundler`, and so has a lockfile, unlike Cabal before it, which is what you are referring to with "anchoring"?
I think the only thing I really miss is how easy it was to build a simple graphics interface in Flash, but that's dead now anyway. Something I pseudo-miss... type safe plugins. If you want your program to be both extensible after compiled and type safe you basically have to build your own scripting language with compiler to extend it. Granted, that's easier to do here than literally anywhere else, but it's still a lot of overhead.
By anchoring I just mean that you have a core group of libraries that are compatible with each other at specific versions so you do not have conflicts between your transitive dependency version requirements. If you use library A and B, both use C but require different versions of it then you may be stuck. The Haskell platform helped with this somewhat but Stackage more or less completely solves it by requiring all its member packages (a self selected subset of the open source Haskell universe) to build together, and quickly resolve it when they don't. edit to answer your other questions: cabal-install can use the Stackage lock file, and it can (at least since the past year or so) also generate project-local lock files for its resolved dependencies like bundler. It doesn't manage the whole tool chain the way stack does though, and doesn't make it easy to add projects not on hackage to your project in a principled way. As far as deprecating the Haskell platform - officially it isn't - haskell.org lists it as one of three principal ways to get started (bare ghc and stack are the other two). But if you ask in IRC or reddit, most people are not using it and not recommending it.
They're not unavailable for local names, you just need qualification if you want to refer to the top level lens and you have something local that has the same name. Shadowing is allowed. I know some people have somewhat mixed feelings about this, but you might like [`makeClassy`](https://hackage.haskell.org/package/lens-4.14/docs/Control-Lens-TH.html#v:makeClassy) or [`makeFields`](https://hackage.haskell.org/package/lens-4.14/docs/Control-Lens-TH.html#v:makeFields). These make it a bit easier to reuse the same lens field name for different types. Both of these also exist in the `microlens-th` package, which goes with the [`microlens`](https://github.com/aelve/microlens#readme) package.
If you compare Haskell to say, Rust, I find there's a lot more people posting libraries, examples, and large projects that they've done in Rust. A lot more "show", a lot less "tell".
&gt; A lot more "show", a lot less "tell". That's because Rust hasn't adopted Haskell's [lazy execution model](http://www.theregister.co.uk/2016/01/13/stob_remember_the_monoids/) yet. Maybe one day. :)
Ah, I forgot `stack` also managed language versions, thanks. One of the reasons we don't do global caching of build artifacts is that compiler flags can change between projects; we cache source globally, but output locally.
The Haskell Platform has two purposes that should be considered separately. 1. An easy to install, fairly complete, multi-platform distribution. That is pretty uncontroversial I think, but stack has taken this role since it can do even more than the Platform e.g. install GHCJS. 2. It provides a set of recommended and curated packages that are known to work together. The distribution and the discovery of a working combination of package versions, is handled equally well, if not better and with a much broader scope by stack and Stackage now. The recommendation part is not provided by Stackage currently and is potentially still valuable. I don't think the choices on the Platform list are too good, but there is no reason they could not be better. However I think in practice there are just to much opinions and different situations to provide a meaningful official choice between competing packages (except for very few packages maybe, that could just as well be in the standard library). Though maybe something like [this](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md) could be official. I think it makes sense to organize a package ecosystem like this: 1. A package database similar to Hackage that basically just indexes packages and has as little requirements as possible in order to not turn people away, but gives the ability to specify dependencies with known-to-work and known-not-to-work version ranges. 2. A subset of those packages at pinned versions that actually build together and work together but other than that aren't subject to more requirements. The set should as inclusive as possible, technical correctness is the only criteria. That is basically Stackage. 3. More opinionated or restricted lists can provided as subsets of 2. Distributing package binaries as part of the compiler distribution is not really the best direction. Every package should be so easy to install as soon as the package management tool is installed that this should be unnecessary. Package endorsement should happen as part of documentation and not be intermingled with package ecosystem infrastructure.
How does literate Haskell syntax fail you for this?
Thank you! (In Rust, we would end up pulling in both B-1 and B-2, so it would play out a bit differently.)
You could literally call this a "cargo cult" lol 
turn back! the haskell platform was a huge mistake that turned away many users. I almost gave up the language because of it. If you want a model to emulate - see how stack does things. The key difference - instead of hand curating a fragile batteries included subset of the ecosystem that is never the right subset for any particular user and leaves users to fend for themselves when they step out of that subset, have a platform/architecture that "just works by default without breaking" for getting packages as needed.
Sorry. New to reddit the UI is tripping me. I meant to link it to a blog post http://cse.iitk.ac.in/users/ppk/posts/2016-07-11-First-release-of-Raaz.html
You could rule it out in short order with the Fermat Primarily Test. And at that magnitude, you dont even need to run other tests unless you need a primarily certificate.
It updates extremely rarely, but the [Haskell Cast](http://www.haskellcast.com) (podcast) is great.
They have a lot of code in GitHub https://github.com/GaloisInc
I agree that saying it was "huge mistake" might be a bit hyperbolic, but it (ultimately) has resulted in wasting a *lot* of (GHC/Cabal/package) developer time because it diverted effort from fixing the underlying problems (better Win32 support, cabal dependency hell, etc.).
The problem isn't mainly that you cannot swap out e.g. one map type for another, the problem is that if you're a library author you will have to commit to one map type in your APIs and force some users to do a O(n) conversion every time they call your API.
Thanks!
Without knowing the specifics of the problems you had, I know that my experience with the platform was poor mainly because of the underlying infrastructure (cabal and the global package repository), not the platform itself. For example, broken packages would require me to basically uninstall and reinstall everything - the platform couldn't do anything about that. I believe Rust doesn't suffer from the same infrastructural problems, so a platform isn't necessarily a bad idea; the Rust community might enjoy the benefits while avoiding the issues we had.
I understand their intrinsic differences. I just don't see how they're different enough in usage that one would be good and the other bad.
This is exactly what I end up doing. (edit: I specifically mean the naming convention of a ~&gt; aB ~&gt; bC etc.)
GHCi has `:print`
Interviewed. Are you applying as an intern? If not they seem to mostly take PhDs.
As someone who hardly programmed in Lisp outside of the SICP and briefly in Uni, can anyone explain to me what Lisp macros achieve that Haskell doesn't? I know Edward Yang put out that article of Racket's macro system having a better design than TH; yet, for the needs of Lisp macros, we get a lot of mileage out of laziness and monadic flow constructs.
Are you aware that using words like "ridiculous" or "poisonous" don't usually help if you're trying to convince your discussion partner that she may be wrong?
s/I hope I will see that day/I hope I will not see that day/ How could I make that mistake?
Why don't we emulate that in Haskell too then?
The details will be up on our website www.pneumainnovations.com It will be up by the end of today. 
Hi! current (though not longtime) maintainer of HP here. It is, as you can tell from this thread, modestly controversial, though it is still widely used by all our information and statistics. I'd say at the time it arrived it was _essential_. We had no standard story for getting Haskell on any platform but linux -- not even regular and reliable teams for getting out mac and windows builds. Furthermore, we had developed a packaging story (via cabal and hackage) but cabal-install which played the role of a tool to actually manage the downloads and installs for you came later, and to get it, you had to bootstrap up to it via manual installs of all its deps. So the initial platform resolved all that stuff -- suddenly the basics you needed on any platform were available. Furthermore, by tying together the version numbers of the various pieces, we could also provide a standard recommendation for downstream linux distros to package up -- which is still an important component to this day. As far as the grander dreams of tying together packages _designed_ to work together, I think tibbe's comments are correct -- authors write the packages they write. We can bundle them or not, but there's little room to lean on authors externally to make things more "integrated" or uniform. That vision can only come when packages develop together in a common way to begin with. A set of issues evolved with the platform having to do with global vs. local package databases as package dependencies grew more complex and intertwined -- in particular to resolve the issues with so-called "diamond dependencies" and related issues people started to use sandboxing. But having packages in the _global db_ as those are that ship with the platform means that they are in all the sandboxes too, which restricts the utility of sandboxes, since they're still pinned to the versions for the "core" packages. This is a very technical particularity that I hope rust doesn't run into too much. (And also related to the idea that the global db is historically _cross user_ which is an artifact of an era with lots of timeshared/usershared systems -- still true of course on machines for computer labs at schools, etc). So as it stands we now provide both the "full" platform with all the library batteries included, and the "minimal" platform which is more of a installer of just core tools. Even when users don't use the full platform (and many still want to, apparently, judging by download stats) those known-good versions of _acknowledged core_ packages provide a base that library authors can seek to target or packages distros can ensure are provided, etc. In any case, it sounds to me like the rust story is quite different on all the technical details. The main problems you have to solve are ones like are pointed to in https://github.com/rust-lang/cargo/issues/2064 The platform, whatever it may be, is two things. A) some way of recognizing the "broader blessed" world of packages. This seems very useful to me (but as a community grows there also develop a whole lot of resources that each have their own notions of "the right set of stuff" and that collective knowledge and discussion will for many supersede this). B) some way of packaging up some stuff to make installation easier. This also seems very handy. In my experience, trying to do more than that in the way of coordination (but it looks like this is not proposed for rust!) can lead to big headaches and little success. (Another lesson by the way -- sticking to compiler-driven release cycles rather than "when the stars align and all the packages say 'now please'" is very important to prevent stalling) The difficulty all comes in what it means for users to evolve and move forward as all those packages move around them. And here the problems aren't things that are fixed necessarily by any particular initial installer (though some make things more convenient than others) but by the broader choices on how dependency management, solving, interfaces, apis, etc. are built in the ecosystem as a whole.
It's difficult to change at this point. Also people might disagree with the changes (e.g. merging text into base).
You are welcome. Did you have a chat with some one from the Industrial Haskell group or for that matter FP complete guys. I do not know these guys but they might have some valuable suggestions. 
Great name!
Would you mind being more specific here? If you just mean the `import` keyword in general, I'm having trouble thinking of a language that doesn't require you to import what you want to use (e.g. python, Java, C#, C++, C, etc.). If you mean installing with cabal, would you mind being more specific about what kind of libraries you commonly use and consider "batteries"?
Haskell has a module system, no? It doesn't allow ML style "functors" but I don't think those are compatible with typeclassing? I was under the impression that the ML functors is one solution, typeclass is the other solution. EDIT: Nevermind, apparently SPJ once made some statement about the power/cost ratio for ML modules and ask for something with a better ratio. I believe that's what I was thinking of.
This was the nicest thing I could find to say about php :-). It is indeed specifically about web development. You make a request in your browser and, if there's a problem, you see it in your browser. I also use atom+ide-haskell and related plugins and I like it. I don't find it updates as fast as, say, C#'s IDE. But I think compiler performance will get better for 8.2.
Me and /u/snoyberg have been talking about this topic internally for a long time after I kept finding subtle bugs in our and other people's code. The main outcome was the creation of [safe-exceptions](http://github.com/fpco/safe-exceptions), but we thought a tutorial was also needed to explain the subtleties involved in a more coherent way.
Fascinating that you find Stack the best build toolchain. Haskell's come a long way in this area!
Theoretically, but it comes down to my last bullet: what does it mean to deprecate a package/version combo on Hackage? Without clear semantics on that, there's no guarantee that these two mechanisms should have the same packages in them.
Everyone else here seems so much smarter than me that it's intimidating to post code. ¯\\\_(ツ)_/¯ Maybe that's everyone else's problem too. Then again, probably not.
Oh without a doubt, Haskell could use a real IDE a la Visual Studio+Resharper. And it could go much further into refactoring and so on because the type system knows much more about the code. I was just surprised to see a dynamic language for fast feedback. It is very fast to compile/run but it can take so much longer to detect trivial errors.
Merging text into base would be a much easier sell if it used utf8. But who's willing to port it to utf8?
Thanks! Is there a useful link?
UTF8 thing was tried back then, https://jaspervdj.be/posts/2011-08-19-text-utf8-the-aftermath.html 
There is a bit of a balancing act between answering the call to do more from some, while responding to the conservative nature of much of the community and the call to disrupt less from others. Let's take one of your problems as an example: Lazy I/O is one of those areas where there are a lot of "easy"ish solution that can make headway. Out of the two you named it is the far more tractable. We're not terribly big on grossly and silently changing the semantics of existing code, so this more or less rules out silently changing readFile to be strict. The community would be rocked by a ton of fresh hard-to-track-down bugs in existing software. We could add strict versions of many combinators as an minimal entry point towards cleaning up this space. I'm pretty sure adding prime-marked strict versions of the combinators that read from files and the like wherever they don't exist today would meet with broad support. But to do more from there, would take trying to get broad community consensus, say, that it'd be a good idea to make the existing readFile harder to call by moving it out of the way. Less support. For something in the design space with even less achievable consensus: There is a pretty strong rift in the community when it comes to say, conduit vs. pipes, and I don't feel that it is the committee's place to force a decision there, and in fact not choosing at all has allows different solutions with different strengths to flourish. The string situation gets more thorny still.
What's the difference to Vincent Hanques' [cryptonite](https://hackage.haskell.org/package/cryptonite)?
Fwiw, there's still the desire among some of us to have a UTF8-backed `Text` type. Personally, I see little benefit over UTF32 or UTF8; in fact, I actually consider UTF16 as combining the disadvantages of UTF8 and UTF32.
&gt; The platform could occasionally cause "cabal hell" as bounds for packages drifted outside of what the platform caused. The platform, as it locked a slew of widely used packages at specific versions. Herbert and others are very close to getting it to where you'll be able to rebuild even the `base` package `ghc` ships with. Combined with the shiny `new-build` stuff, this would avoid lock-in even for the fragment of core packages that GHC needs internally that even `stack` can't concoct a build plan for once you need to mix in, say, the GHC API in order to get your `doctests` to work today. This will also go a long way towards making it easier for us to find ways to split up base into smaller pieces, that could revise at different rates.
I think GP is referring to having more in Prelude so you don't have to import or hunt for libraries.
Then I would want to see examples but suspect I would be against any such changes (as are all languages I can think of).
Sounds like a smart and solid company. Thanks for sharing.
Why is this not the case for single-method type classes?
Under what circumstances would the community want something in your list but not to stop people using it in general via hackage deprecation?
Is a Functor interface really that different from a Functor typeclass?
Coming from PowerShell I'd say an amazing pipeline...but PowerShell is OOP so that really doesn't apply...I can tell you what I don't miss...inferred types(which Haskell is a little better at anyway).
&gt; Better is just to make a recommended libs section of your website or tutorial. Rust has this already but it's problematic for them because a lot of the library authors haven't really grasped how semver works.
That's just the impression I gathered. From this reddit I think. I take it back.
&gt; Most of the world prefer not to use UTF8 Really? Do you have a source on this?
Personally, I've tried R many times but the absolute chaos of the language (i.e. I can't work out what their convention is for what would just be part of the "prelude", what it would be named, etc.) has been a large barrier. Most languages work similar to Haskell: everything lives in a "module" with a related name and you need to import it to use those related things (in fact, a lot of languages have a lot less in the "prelude" than Haskell does). I feel like these kinds of concerns are more about the lack of a really nice IDE than anything else. In C#, I just type the method/class I know I want and Resharper notices what I want and asks me if I want to import it. I never touch the import statements directly. So I get the best of all worlds: when reading, it's obvious where things are coming from by the import statements and with writing I don't have to burden myself to even remember what module has what, just use what I need and let the IDE work out what to import.
thanks
I'm not arguing in favor of R, I'm convinced about the "absolute chaos" bit. It's just an example of included batteries.
Would you say it's not mature enough yet ?
Only anecdotal. Our customers are many of the well-known global enterprises, and we work with large volumes of textual content they generate. Most of the content we see in languages where UTF8 does not work well is in UTF16. (By "does not work well in UTF8" I mean that most or all of the glyphs require 3 or more bytes in UTF8, but only 2 bytes in UTF16 and in language-specific encodings.) Since the majority of people in the world speak such languages, I think this is evidence that most content is not created in UTF8.
I agree with you. I just would like the haskell platform to do its job properly, and have only one "extended" prelude.
Is it? Strict evaluation is an effect, so if plain generic idiomatic Idris code is strict, Idris is not completely pure. 
Nice! Previously I think the best tutorial / explanation of Haskell exceptions was in P&amp;CP so it's great to have an updated look with safe-exceptions. 
I am afraid that the download stats might be highly misleading. I myself have downloaded the platform to try it out and uninstalled the next day. A more meaningful measure would be how long does the platform remain in use once installed. The uninstaller could open up a web form, submit installation date, and ask people for the reasons for uninstalling. Some programs do that. 
FYI all content on StackOverflow (including Documentation) is CC-BY-SA 3.0
The cabal new build functionality is closer to what rust supports, because it can handle multi version builds and caching of different build flag variants of the code. Stack can't
I've found the [here](https://hackage.haskell.org/package/here-1.2.7/docs/Data-String-Here-Uninterpolated.html) package to be an adequate solution for heredocs.
Thanks very much for this work. It's an important topic for the working programmer.
Basically the idea is to have the class ``` class Equality a where eq :: a -&gt; a -&gt; Result ``` The `Result` type is the result of comparison which is opaque so there is now way to peek into it. One can combine two such results using the monoid instances. This combination is like taking an and of the two result but the operation is timing independent. So for all cryptographically sensitive types like hashes, MAC's etc, we follow the rule that one has to first defining an instance of `Equality` type and then derive the `Eq` type out of it. There is no way this style can be enforced at the type level but while performing a code review, if for a potentially sensitive type this style was not followed, for example one sees a `deriving Eq`, we can be suspicious of the code. More details are in the Haddock documentation. https://hackage.haskell.org/package/raaz-0.0.1/docs/Raaz-Core-Types.html#t:Equality If something is confusing please let me know or better send be a pull request. We care a lot about documentation. 
Good point. I wasn't thinking in those terms because I don't have a bazillion libraries on Hackage :). I must say, though, that I actually don't typically run into APIs that use Map/Set and the like. Maybe it's just because of the general area of my interests doesn't overlap too much with such libraries.
See [cabal-install changelog](https://github.com/haskell/cabal/blob/master/cabal-install/changelog#L6). ``` * Deprecated 'cabal install --global' (#3356). ``` Sorry, I meant `cabal-install` not `cabal`.
A nice observation is that currying has a direct analogue in linear algebra: a -&gt; (b -&gt; c) ≅ (a,b) -&gt; c corresponds to Hom(X,Hom(Y,Z)) ≅ Hom(X⊗Y,Z) The meaning of this is that the tensor product encodes *bilinear* functions. 
[removed]
No I haven't contacted them yet. Will do so. Thanks for suggesting :)
There's a direct categorical correspondence which generalizes this all neatly: `Hom(_, _)` (taken as a space) is the exponential object of the category of linear mappings. Similarly, `_ -&gt; _` is the exponential object of the category Hask.
[removed]
It's definitely something I'd like to see happen. Another approach is just encoding all of the type-based detection into `Control.Exception` itself, which I'd ultimately like to propose. Michael Sloan made a proposal to make exceptions more extensible with metadata, with a number of motivating use cases. You can see the proposal at: https://mail.haskell.org/pipermail/libraries/2015-April/025471.html Unfortunately, it fell into the all-too-standard mire of someone claiming they had a better design they were working on, and then nothing ended up happening. (Does this count as POPL syndrome too?)
Have you looked at the [linear](http://hackage.haskell.org/package/linear) package? Parts of it require a familiarity with lens, though.
Yes, many of us Galwegians hang out here. I'll post a link on our internal chat and we'll see who else chimes in. Do you have any specific questions in the mean time? Yes we use Haskell quite a lot, yes we use ML languages too. Fun fact: Galois was the original host of hackage.haskell.org back in its infancy. I see comments, such as /u/recoveringhobo, about us only hiring PhDs. These are incorrect but we aren't always hiring for all positions - at certain times we might only be looking for people with particular qualifications or experience.
Given the book OP is referncing I'm not sure they're there yet. 
I would suggest first programming the algorithms in the book in order to make your understandings precise. Anything that would leverage your Haskell use would involve category theory, which is probably more abstract than you want given that you're using Axler right now.
Thanks for the reply and info. I have several questions: What kind of work is involved in formal verification/software correctness? What kind of work is involved in scientific computing (I read compiler and language expertise and got excited)? I was also curious as to what the roles of your developers are. Do you spend a lot of time researching and working with R&amp;D or is it more like the developer jobs found in other tech companies? As for hiring, does Galois hire often for developers/engineers or is it more of a demand-based thing? I'm curious since there are no open positions currently available and want to know if there's typically a spot in the near future I can apply for. As a more personal question, how are you liking your work and experience at Galois? Thanks again for sharing your thoughts!
Isn't that a package level `@deprecated` flag? That's a cool idea!
Would you hire (Did you already?) C++ engineer willing to move to Haskell? Or if you have C++ projects...
So it is a Mu job ;)
Thank you for this, it's extremely helpful.
&gt;I am a Haskell dev based in India (Delhi). Hope you don't mind some questions... Where do you work? What do you do with Haskell? How much experience did you have when you where hired as a Haskell dev? Is the job 100% Haskell, or do you have to write Java sometimes?
I encourage you to apply when the time and position is right for you. It's pretty much a fact that C++ is one of those languages you can't get away from - that's not trying to start a language war, rather an acknowledgement that C++ is ubiquitous and we do work in C++ on some projects. WRT Haskell: knowing Haskell is certainly not a prerequisite to applying or working here. People run the gamut, but being a language-ophile helps.
Good to hear about C++ :) While we're at it, do you sponsor work permits (H1B?) and relocation?
I've known too few people so far who admit to obtaining knowledge through osmosis.
What's `Hom`? 
Hom(X,Y) is the vector space of linear maps between X and Y. It corresponds directly to the Haskell notation `X -&gt; Y` (except that Haskell does not require functions to be linear).
It sounds very much like the definition of a dual space to X. Of course Haskell doesn't prescribe basis transformations, but more general ones. So I have trouble understanding how far we can stretch this comparison..
What is `timing independence`? Check runs in the same time across the set of data-type values?
I assume this refers to the requirement that the execution time of all operations involving private key data must not correlate in any predictable way with the private key data itself. Or else attackers can start figuring out private key data through evaluation time of eg. signature verification operations (a timing attack).
It would be good to add comments to the pages. Some years ago the PHP website allowed this and it was quite useful (don't know offhand if it still has this feature.) Also, the Real World Haskell Web book handled this well. Please do NOT wikify it--then it looks like the Arch wiki ("do this. NOTE - I did this, and it didn't work, I did that instead. NOTE - I did this, and it worked fine.") but if comments were allowed in a separate section and you checked them occasionally, that would be useful.
FP Complete is consistently tackling the biggest pain points of Haskell development. Exceptions are one of the more intimidating parts of Haskell development in the wild, and these packages help tremendously.
Link : https://github.com/runeksvendsen/restful-payment-channel-server
We've already tried EU timezones before, it wasn't an ideal fit for our size. At our scale you end up with a developer who's all by himself all day every day except for one or two hour overlap with the US. It's pretty lonely and it's hard for them to care. I suppose if someone wanted to live on PST hours while in EU it would work, but that doesn't seem too humane. Startups our size are about quick decision making and unblocked work.
&gt; The question is: if users add packages whose dependency bounds go outside of the Rust Platform's, what behavior should occur? The same behavior as if the dependency was added individually, of course. cargo already has a solution for this (its not a perfect solution, but improving it is orthogonal to this).
From a learning perspective, it has definitely changed the way I think about programming, and continues to do so. I enjoy programming a lot more now, too. Also, I got the interview for my first job cause I listed Haskell on my resume (kinda), so that's cool too. (Sadly, the job doesn't use haskell to my knowledge. XP)
Have you considered to insert it in https://algorithmia.com/ ?
I've looked at Algorithmia briefly before, but it appears to me to be about data generation/serving and transformation. This server allows the operator to receive bitcoin payments instantly (without having to wait for confirmations) and without paying the Bitcoin transaction fee (until the channel is closed). So the only API the server provides is the API that allows a client to send value to the server (http://paychandoc.runeks.me/). The point of this is that something needs to be sent in return, such that the server only acts to receive payments, while a different protocol takes care of content delivery (using the RESTful Bitcoin Payment Channel Protocol to receive value). So, as far as I can see, unless Algorithmia scripts want to send bitcoins to some server somewhere, it's not useful until people start creating protocols that use this payment channel protocol. You could use my server to create an API/service on Algorithmia that requires the user to pay per request. Like a RESTful email service, where you can POST emails to it for 0.01 cent per email (so we avoid spam abuse).
Related: something like F# type providers, to make working with loosely-typed tabular data easy.
Say `open2Files` does mask asynchronous exceptions. Okay, it won't receive an async exception until it's done, but it still must return from this `mask`. If the code that called `open2Files` is itself not masking async exceptions, then there is a chance for an exception to strike after `open2Files` returns. Therefore, the calling code should always put allocation functions under a mask, and therefore, the allocation functions themselves don't have to.
What’s nice is that because a lot of things follow the same algebraic structure, you can find nice little currying analogies all over the place. Exponents: (c ^ b) ^ a = c ^ (b × a) Logic: a implies (b implies c) = (a and b) implies c Code algebra: if (a) { if (b) { c(); } } = if (a &amp;&amp; b) { c(); } Cartesian closed categories: Hom(a × b, c) = Hom(a, c ^ b) 
Somewhere in our library stack we need to be able to encode/decode UTF-16 (e.g. for your database example) and other encodings. The question is what the Text type should use internally.
As the others have correctly pointed out, the timing of a comparison of a secret should not depend on the values being compared. Character by character comparison of a string is an example of an equality check that is _not_ timing safe. In raaz, each crypto sensitive data like hashes and macs are distinct types. This avoids confusion of the semarics: it is impossible to for example compare `HMAC SHA512` to a `SHA512` value and accept. These types are also not strings and their equality has to be defined in the way that we described.
I'm not sure, I haven't really been following the GHC proposals discussion. I know I should, but jumping into yet another contentious community issue about an established core group not accepting external input kind of makes me queasy 
[reflex-platform](https://github.com/reflex-frp/reflex-platform) gets you that today with reflex frp. I just went with threepenny in the talk because I was already familiar with reactive-banana and rather like its API; if you want to play around with the idea, I'd probably recommend reflex because reflex-platform makes it so easy to get started.
Can someone explain why an exception should be thrown instead of checking a return value? Mangling the control flow with invisible goto always seems awful to me even in languages built around them
I know some of these words.
You can definitely urge :). I will try to get to it, but the size of that discussion thread is daunting. 
I'm surprised that apparently GHC inlining/specialization trips up on MTL; my intuition would be that since transformer stacks must be concretely resolved when we run them, it should be easy to specialize them at that point. Even if we write recursive functions, surely they're not *polymorphically* recursive in the monad dictionaries 99.9% of the time, so they could be lifted out and specialized all the same. My impression is that fixing MTL specialization should be an easier job than getting the alternative effect systems (algebraic/extensible effects) up to speed. 
&gt; It's pretty lonely and it's hard for them to care. Everything has its trade-off's. If you paid "yet another EU wage" then yes, but mid-6-figures I think would make things much easier (for both sides)...
Even if you recurse using "already interpreted" actions?
Things learned : * The constraint entailment type operator `:-` * `:bro` macro in GHCi * Monads are monoids in the category of endofunctors: monads are monoids where the product is taken to be functor composition (@ 21:30) * Good MTL style (better to write constraints than working with synonyms): 25:00 * Good MTL style is inefficient: 26:00 * Distributive laws for `ReaderT`, `WriterT` and `StateT` (30:00) * Monad transformer homomorphisms * How somehow all of the above ties together (a huge pile of instances, up to 46:00) * `instance Profunctor (-&gt;)` (right arrow is a really a function of two arguments, i.e. the types of domain and range) * Two forms of `Lens`: Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t data Lens s t a b = Lens {get :: s -&gt; a, set :: s -&gt; b -&gt; t} which are equivalent for an appropriate choice of `f`. * An `Iso s t a b = forall p f. (Profunctor p, Functor f) =&gt; p a (f b) -&gt; p s (f t)` simplifies into a `Lens` if `p` is taken to be `(-&gt;)` * The existence of `Strong` profunctors and how they relate Lost the plot at: 1h 03 Questions: * Does the `exists x .` qualifier really exist in Haskell or is it just shorthand? * Why do the kind signatures in the `Tensor` class have to be given explicitly? * Is `Strong` related to https://hackage.haskell.org/package/data-lens-2.11.0.1/src/src/Control/Category/Product.hs ?
&gt; An Iso s t a b = forall p f. (Profunctor p, Functor f) =&gt; p a (f b) -&gt; p s (f t) simplifies into a Lens if p is taken to be (-&gt;) Last night I started thinking about generalising lenses to be work on profunctors instead of functions, in order to represent database queries. This might be the answer. I knew Ed would have done it already.
What about the following: import Linear ( Additive, sumV, (*^), basis, Metric(dot)) import Data.Traversable ( Traversable) newtype Dual v r = Dual { unDual :: v r -&gt; r } toDual :: (Metric v, Num r) =&gt; v r -&gt; Dual v r toDual x = Dual (dot x) fromDual :: (Additive v, Traversable v, Num r) =&gt; Dual v r -&gt; v r fromDual x = sumV (map (\b -&gt; unDual x b *^ b) basis) Sadly I am not sure if `toDual` and `fromDual` together form an isomorphism. EDIT: Or the following definition of `fromDual`: import Linear ( identity) fromDual :: (Applicative v, Traversable v, Num r) =&gt; Dual v r -&gt; v r fromDual x = fmap (unDual x) identity 
The Kmettoverse has a single inhabitant, the trivial Kmett
This is definitely the sort of situation where a JIT would shine.
Not at all. To elaborate, compile time specialization for MTL should be impossible only in outlandish cases that barely occur in production code, which I'll try to list here: - Polymorhpic `forall m. Monad m =&gt; t` or `forall m. MonadFoo m =&gt; t` fields inside data. - Existential `Monad` or `MonadFoo` dictionaries inside data. - Polymorphic recursive functions that change the choice of monad or monad stack on recursive calls. - Statically unknown functions taking as arguments polymorphic `forall m. Monad(Foo) m =&gt; t` values. 
Very nice, thank you! Does sum identity depends on order? E.g. are `type A = String + Int` and `type B = Int + String` identical? Also, does it support excluding a type from a sum? E.g. is it possible to define something like the next: `type A = String + Int` and `type B = A - String`? It is a must-have IMO to be useful for "checked" exceptions like in the example with `ExceptT`, e.g. `catch` should automatically remove already handled errors from the sum. (Probably I want too much? :) )
&gt; type B = A - String I'd love this. Now, I have to "poke a hole" with polymorphism, and "fill it in" with a few types, where "minus" is `Void`/`()` for sums/products.
&gt; Does sum identity depends on order? E.g. are type A = String + Int and type B = Int + String identical? It does. `A` and `B` will be different types. &gt; Also, does it support excluding a type from a sum? At least, not yet. I haven't explored that so far. &gt; It is a must-have IMO to be useful for "checked" exceptions like in the example with ExceptT, e.g. catch should automatically remove already handled errors from the sum. &gt; (Probably I want too much? :) ) I wouldn't say that you want too much, because I dream about the same :) However it needs to be explored. What the library already solves I would consider a step towards that goal.
Glad to hear it. Though, if I was to nitpick, I asked whether it might be hard and you answered that it wasn't impossible. Do you expect that GHC will soon do this automatically at -O1?
FYI, today we fixed the issue that revert on file buffers doesn't work. We use regular `basic-save-buffer` but just disable hooks (delete whitespace, flycheck, etc.) with dynamic binding. It interacts properly with magit, or external changes, now.
&gt; type Gender = Proxy "male" + Proxy "female" For millennia, binary gender has been *reified* into natural language. In the third millennium, our programming languages can now *reflect* boolean gender onto the type level. 
&gt; HTML is a poor argument Since the web is probably the biggest source of text anywhere, I'd say it just states something about how widespread it is, but agree with &gt;when putting non-latin text in a database, and how to have Haskell's Text support that well to be a more interesting case. I usually use UTF8 encoding for databases too though, but then again I usually only care that at least Æ, Ø and Å is kept sane, so UTF8 is a much better choice than UTF16, which'll in 99% of the data take up an extra byte for no gain at all.
Can GHC 8.X's anonymous sums (i.e. `|`) replace your `+`? Why use `Product3` over `(,,,)`? 
The closed type family `+` for un-nested sums, over nested `Either`s, is elegant. Whenever possible, I replace type classes with closed type families. 
&gt; Virtually there’s no difference for the lazy Product, however for the strict Product there’s no tuple alternative. Also the * operator syntax makes it consistent with the + of Sum. `(*)` can build lazy `Product&lt;n&gt;`s, and the `Product&lt;n&gt;`s can be aliases: type Product3 = (,,) and patterns: pattern Product3 a b c = (a,b,c) for switching imports. 
The economic assumption of "rationality" is that everyone is a genius sociopath. Most people are neither geniuses nor sociopaths. 
I think the best answer is: don't use type classes. In your particular example, those two instances do work and they're only a total of four lines of code. Sure, if you use that approach everywhere, you'll end up with lots of duplications. So don't use that approach. Instead, try modelling your problem with types and functions rather than classes. Sorry that this advice is a little abstract. Do you have an actual problem that you're trying to solve? If so, someone here can probably show how to model it just using types and functions. A good rule of thumb is to minimise the number of new type classes.
And how do you actually construct and destruct these?
There is a destructing example in [demo](https://github.com/nikita-volkov/compound-types/blob/master/demo/Main.hs): intCharBoolSumToString :: Int + Char + Bool -&gt; String intCharBoolSumToString = \case Sum3_1 int -&gt; "Int: " &lt;&gt; show int Sum3_2 char -&gt; "Char: " &lt;&gt; show char Sum3_3 bool -&gt; "Bool: " &lt;&gt; show bool which as far as syntax goes is basically a huge no-no. (If you later decide to change the number of types in the sum, you have to edit all constructors all over the place.) With `PatternSynonyms` and some type-level magic, you can make it much nicer work with like this: https://gist.github.com/PkmX/4371605e74f41f5afc17351b2d5fc17c EDIT: `TypeApplications` broke GitHub's syntax highlighting. :O
&gt; which as far as syntax goes is a basically huge no-no Absolutely.
Cool! Is there any way to get the next project error? With `ghci-ng`, I can reload `Main.hs` to get the project-wide errors. Couldn't figure out any way to have that work flow with intero. Also, intero's repl doesn't quite work with Lamdu (I import GLFW in modules with TH and that somehow breaks ghci). ghci-ng somehow works perfectly there. Would love to have the nice flycheck behavior too :)
This is really cool, but it could use some examples of how you actually implement functions accepting or returning these types. What does this look like on a value level? Do you just use calls to either everywhere instead of pattern matching? 
Every vector space is a 'free' vector space, which means you are really giving some sort of map from some set of basis vectors to scalar values. So a vector space V over R is really some function (Basis -&gt; R). A dual space is linear transformation from a vector space to the scalar field it is built over. (Basis -&gt; R) -&gt;_L R where I'm using -&gt;_L to indicate that the function is linear. `f (2 .* v) = 2 * f v`. This looks a lot like Cont! `(a -&gt; r) -&gt; r`, the difference is that callCC is sadly not a linear operator. The linear package contains this type as http://hackage.haskell.org/package/linear-1.20.5/docs/Linear-Covector.html The nice thing is that all of the monad operations, applicative, etc. things you can define all are well-defined things in linear algebra related to tensoring, currying, etc. Kleisli arrows `(a -&gt; Covector r b)` defines a linear map from a vector space with basis b to vector space with basis a or, more accurately, a linear map from the dual space of a vector space with basis a to the dual space of a vector space with basis b.
I did it because I think it can help make it feel a bit more approachable and make it stand out when it gets embedded on things like FB. I might replace it with a static image though, the animation doesn't add much. I'm sorry it annoyed you -- thanks for still reading the article!
I am not knowledgable enough to know how much of a big deal losing global type inference is. But dealing with the borrow checker is not anywhere nearly hard as dealing with the stuff you will come across in Haskell. Just having to deal with different String types and various language extensions alone makes working in Haskell considerably harder than dealing with Rust's borrow checker. I know that this difficulty goes down considerably as you grow familiar with the language, but so do the difficulty in dealing with the borrow checker. I am still not convinced that using Haskell over a imperative language with a powerful static type system (like Rust) is actually an overall superior choice for a use case like this. I know that there are stuff that you can enforce with Haskell's type system, that you cannot do with Rust's. But I am just wondering if there is a point beyond which the type system's power stop making a considerable difference for certain use cases..
Could you explain this "collapse", what the laws of (-) would be? Or at least point us to where you found this info? Thanks:)
I suspect it would not. The GHC inlining infrastructure is fragile and has many limitations. The main problem is that most of the time we actually want partial evaluation, of which the most important part is beta reduction, but there's no direct support for that. Instead (if I understand correctly) we get beta-reduction as a side effect of inlining fully syntactically applied functions. Non-fully applied functions just fail to inline, which means we already have to decide on the preferred inlining behavior when writing our functions. For example, we have the following ugly thing in [GHC-Base](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.9.0.0/src/GHC-Base.html#.): -- | Function composition. {-# INLINE (.) #-} -- Make sure it has TWO args only on the left, so that it inlines -- when applied to two functions, even if there is no final argument (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c (.) f g = \x -&gt; f (g x) If we define `f` as having two formal parameters, and we want to partially evaluate `f x`, we're out of luck. Class specialization is essentially partially applying a function to its dictionary arguments. We can use the SPECIALIZE pragma for that, and it works in simple cases, but apparently it breaks down for MTL-level indirections. It might be worth to try SPECIALIZE-ing the methods for the transformer stacks that concretely appear in our program, for example `{-# SPECIALIZE pure :: forall a. a -&gt; ExceptT String (StateT Int Identity) a #-}`. I don't know if this works, but even if it does I would rather like this specialization to happen out-of-the-box. What I would like to see is general support for partial evaluation; for example, function parameters could be annotated at definition-site with reduction hints, but also overridden or explicitly specified at use sites. However, for partial evaluation to generally make sense, we would have to have a well-defined semantics for compile-time strong reduction, i. e. reduction to full normal forms. Since at runtime we only ever need reduction to whnf, there hasn't been much developer focus on reduction to nf. If we had a dependent core language, then strong reduction would be one of the nice things that come with the starting package, but alas we don't have such core language.
Even with this formulation, GHC will _always_ emit a non-exhaustive pattern warning, correct? 
[removed]
For my team, this bug https://github.com/vincenthz/hs-tls/issues/124 caused us quite a bit of headache. EDIT: If it wasn't clear, I definitely appreciate all your hard work and fixes!
Yeah, this is essentially pattern matching with `Prism`s using `preview`. I wonder how the [total](http://www.haskellforall.com/2015/01/total-100-exhaustive-pattern-matching.html) library can be plugged into this (provided GHC also fixes the bogus exhaustiveness warnings from pattern synonyms).
Seems tricky to get working with separate compilation. You'd specialise at link time?
An open union is another solution: data Union :: [*] -&gt; * where Z :: x -&gt; Union (x : xs) S :: Union xs -&gt; Union (x : xs) class x :&lt; xs where inj :: x -&gt; Union xs instance {-# OVERLAPS #-} x :&lt; (x : xs) where inj = Z instance {-# OVERLAPS #-} (x :&lt; xs) =&gt; x :&lt; (y : xs) where inj = S . inj So, you wouldn't have to change the names of the constructors if you add new data types, but you would have this awful-looking pattern matching code: foo = Union [Int, Char, Bool] -&gt; () foo = \case Z x -&gt; ... S (Z x) -&gt; ... S (S (Z x)) -&gt; ...
I meant that if you take the dictionary above, that is, types corresponds to vector spaces and functions corresponds to linear maps, then the dual space does not have a Haskell analogue. Of course you can encode vector spaces in Haskell directly like you did, and then not surprisingly you can have a dual space. Though if you already have a dot product, then the whole concept of dual space becomes somewhat less interesting.
I've done most of the exercises in the LADR book and most of them are proof- rather than calculation-oriented. I don't see how Haskell is much help there. Of course people can find category theory analogies which might be interesting, but you'll probably get most done if you focus on the linear algebra itself.
Yeah. That was a headache for us as well. 
&gt;But dealing with the borrow checker is not anywhere nearly hard as dealing with the stuff you will come across in Haskell Developing in rust is much slower than in haskell. &gt;Just having to deal with different String types and various language extensions alone makes working in Haskell considerably harder than dealing with Rust's borrow checker That is ridiculous. Rust has several string types too, so do most languages. What is challenging about this? &gt;I am still not convinced that using Haskell over a imperative language with a powerful static type system (like Rust) Then go try it, that's the simplest way to be convinced.
Any chance you could send GHC HQ a pull request?
&gt;Developing in rust is much slower than in haskell. Let me put it another way. Suppose two similar developers spend six months, one learning Rust and other learning Haskell. After six months, the rust programmer will outperform the Haskell developer in building any non-trivial functionality. Say, you give them six more months. The rust dev's productivity will have more or less remained the same. But Haskell dev will be still getting better. But he will be still slower than the rust programmer. It might take six more months, or an year or may be even more, for the Haskell programmer to catch up with the Rust programmer in productivity. It might be possible that Haskell dev will continue to improve, vastly surpassing the rust dev in productivity. It is also possible that the Haskell dev will get fedup of having to read academic research papers, to keep on improving. So anyway, at the end of five years, you call them both and give them a complex program to implement. The Haskell programmer gets it done in 3 days time, while Rust programmer took two weeks. They both work well for all the inputs. But the rust program, while having a lot more LOC, have consistent performance, while the Haskell program takes a little more time and a lot more memory for some inputs. You send the Haskell programmer back with his program, and asks to him to try fix the issue. He goes back, and comes back after 5 days. The program now works well without taking too much memory. (Please don't take any of these too literally) So my question is, was the five years, that the Haskell programmer took to painfully learn all these abstractions, worth the trouble in the end? I am not saying it was not. Just that the answer does not appear clear cut to me. &gt;That is ridiculous. Rust has several string types too, so do most languages. What is challenging about this? I am not sure if things have improved now. The last time I checked, the preferred method for dealing with this issue was using the TextConversions package. In it's [Hackage page](https://hackage.haskell.org/package/text-conversions), it says, "This is a small library to *ease the pain* when converting between the many different string types in Haskell". This is also from my own experience. &gt;Then go try it, that's the simplest way to be convinced... I have. [this](https://hackage.haskell.org/package/hastily) and [this](https://bitbucket.org/sras/hastily.rs) are the same program that I have written in Haskell and Rust respectively. The Rust one implements a web interface. You can see it running [here](http://sras.me/subfixer).
I agree that literal rationality is a dated notion. I don't know the history, but my guess is it came into being before the modern understanding of psychology. But even if people do not act in their best interest, doesn't mean their actions defy logical reasoning. Junk food, for instance, is in no one's best interest from a health perspective. But it does relieve stress on a very short term scale. And while it had nothing to do with office supplies, they still sell it at the checkout line because it is a dependable source of side revenue. On the surface, though, we should see no "rational" reason that Staples should sell products completely outside their niche to customers who would be better off if they never bought such products at all. 
&gt; Is there any way to get the next project error? I have a patched flycheck which displays the errors from other files, but the flycheck maintainers are dragging their feet. &gt; Also, intero's repl doesn't quite work with Lamdu (I import GLFW in modules with TH and that somehow breaks ghci). ghci-ng somehow works perfectly there. I think perhaps intero's repl is not cd'd to the root dir of the project. I'll check that.
I think the `total` library combined with type classes for prisms for sum constructors analogous to the `Field{N}` classes for the fields of tuples in `lens` would give the desired behavior
I think you mean quick (x:xs) = quick (filter (&lt; x) xs) ++ [x] ++ quick (filter (&gt;= x) xs) quick [] = [] And I think that the much better-performing version quick (x:xs) = let (less, more) = partition (&lt; x) xs in quick less ++ [x] ++ quick more quick [] = [] Is barely longer. As for whether or not GHC is capable of sorting in-place, I have no idea.
No, it wasn't that one. It turned out to be a problem in a non-standard version of `bytestring`. It caused issues with SSL connections, so that's why we suspected the `tls` package. The bug was actually fixed in the `bytestring` version shipped with GHC, but an early, buggy version was released to Hackage with a higher version number. If you used `cabal install` in a certain way, that buggy version could be picked up and cause problems. That's why the bug was present is some builds, but not others. That was before Stack was released.
I really like Rust, but I think you're overselling it. Especially for writing web services. * Cargo is very nice, but Stack is getting there. * Rust is still lacking good libraries for many use cases. For example, there's still no obviously best way to write web services. In Haskell, you can pick Snap or Warp/Yesod and be productive from day one. * While Haskell's database libraries aren't super great, neither are Rusts. (There are some libraries with great potential, but they're not there, yet.) * Lightweight threads are a really good abstraction for web services, and Rust doesn't have a compelling story here. * Haskell's different types for pure functions do actually help. In Rust you'd have to enforce that by convention/code review/training people. The key advantage for Rust is more predictable performance and memory usage. In Haskell (well GHC's RTS, to be precise), a heap of 3GB can cause pauses of 300ms (or so). In our case that was just about acceptable, but it may not be in other cases. Of course, you can use both! It's trivial to export a Rust function to C, and it's easy to call C functions from Haskell. So, you can really choose the best tool for the job, without sacrificing productivity or safety.
It would have to recognize that `(filter (&lt;x) xs, filter (&gt;=x) xs) == partition (&lt;x) xs` and that in general requires deciding equality on functions, which is undecidable. There could be some heuristics that catch easy ones, but they'd be very limited, and that's not the kind of direction the GHC team seems to favour.
When talking about profunctor lenses, Edward shows this: class Profunctor p =&gt; Strong p where first' :: p a b -&gt; p (a,c) (b,c) and says that you can then dimap over this to get a profunctor between things isomorphic to the pairs. It seems a shame that you have to use both first' and dimap to get this; maybe you could add this to Strong: first'' : (d -&gt; (a,c)) -&gt; ((b,c) -&gt; e) -&gt; p a b -&gt; p d e with a default definition. Or is this not worthwhile? Is GHC always able to optimise this enough anyway?
Could it pretend all sums are 7-way (or whatever the limit is) and use `Void` for the excess slots? I can see that it would make `+` more complex :-/
(-) here is effectively a coexponential. See [CoCCC](https://hackage.haskell.org/package/category-extras-0.53.0/docs/Control-Category-Cartesian-Closed.html) coapply :: b -&gt; (b - a) + a cocurry :: (c -&gt; a + b) -&gt; (c - b) -&gt; a uncocurry :: ((c - b) -&gt; a) -&gt; c -&gt; a + b As for a CCC that is also a CoCCC collapsing to a poset, its something I tripped over along the way. I'm not sure where I first saw it.
I thought GHC 8 will only implement strict sum (via #l).
Thailand.
ditto
Yea, thank you for correction. 
Did you end up hiring many engineers without any Haskell experience? I'm interested in Haskell but I've never had a hobby project that the language would be good fit for. Applying for Haskell jobs for me would be a chicken and egg problem where I need experience to apply but can't get experience due to not being able to land a Haskell job.
&gt; That sounds good to me, as long as transitive "real package" dependencies override rust-platform ("meta package"?) dependencies, I think many issues can be resolved. Yes. The design hasn't been fleshed out yet, but this blog post already says that if you specify an explicit dependency, it uses that version instead of the version in a "metapackage."
Junk food is also cheaper, and easier.
Yes. It's optimized for cost in production. And it optimizes for appeal to our primitive human nature. So as much crap as others have given me in this thread, I still stand by my claim that the market will optimize. It's just that the aspects it optimizes don't align with what we *think* it should optimize.
On a somewhat related note, what the hell happened to the `record` library? When it was proposed, it seemed like it was the ultimate solution to the infamous record problem, increase everyone's productivity by 100x and cure cancer (after all it is the all time top post of /r/haskell as of now), but it seemed that the hype died down rather quickly. Did you discover something fundamentally wrong with the design or was it just too much to implement/maintain a separate GHC preprocessor?
Another physics (almost finished) PhD who learned haskell on the side checking in. I am looking forward to getting to use haskell more in my post PhD life.
Nice, I had no idea something like that existed!
[removed]
&gt; arbitrary (-)'s If you restrict exclusion to non-function types, does it still collapse?
You can write prisms for `Z` and `S`, and then your typeclass ends up acting like a combination of `inj` and `proj` from Data Types a la carte. class InUnion (x :: *) (xs :: [*]) where _Union :: Prism' (Union xs) x instance {-# OVERLAPPABLE #-} InUnion y xs =&gt; InUnion y (x ': xs) where _Union = _S . _Union instance {-# OVERLAPPING #-} InUnion x (x ': xs) where _Union = _Z I've been using something like that a little recently, and it's been handy so far. I've also got another version that prevents duplicates being added to the `Union`, which can be handy if you're building these things up across API boundaries (ie someone might hand you a `Union` that has a second `Char` in an unexpected place, making one of them unreachable via the typeclass).
That is saying if you can give me a function from c to a + b, then you could give me a function from c - b -&gt; a, ruling out the `b` case. Like I said, once you have these, if you have the '-' and _functions_ you're hosed. The analogy doesn't make sense in something like Set -- Set lacks coexponentials! but here if you read -&gt; as &lt;= everything translates. coapply states: b &lt;= b - a + a cocurry states: (c &lt;= a + b) |- (c - b) &lt;= a uncocurry states: (c - b &lt;= a) |- c &lt;= a + b Intuitively, that is why these feel like things you do on ordered numbers.
Another way to show that: that number is 10^74207281^ -7 = (10^74207281^ - 1) - 6 = 99...999 - 6, (74207281 9's over there) which is divisible by 3.
Check out http://stackoverflow.com/questions/22528534/why-is-type-inference-impractical-for-object-oriented-languages Now in trivial examples discussed already it isn't an issue. But in the general case type inference is downright nasty, to the point of very exponential behavior (having to check every / a substantial portion of ALL possible type combinations). Even for the apple + orange thing. You still have to consider that they are also both objects, and also both "objects that fit in my hand". The former you could say is "unnecessarily loose" but the latter is neither better nor worse than saying "fruits" in the sense that some objects belong to one but not the other, some to both, and some to neither. If you take that to its logical extreme you could apple + orange gives you an "apple + orange combination" object or something weird like that. And now you can hopefully start to see why exponential / combinatorial behavior is inevitable in the general case.
I was under the impression that we had enough tricks to build an EDSL with linear typing.
Well the idea being, if a user states, "I want rust-platform, and I also want say, `diesel = "x.y"`", then I think it's probably reasonable to allow the `diesel` package's transitive dependencies to override those in rust-platform. Otherwise rust-platform risks becoming an anti-pattern, something expert users advise novices to avoid because it will cause problems when they try to include packages that aren't updated as reliably, whose bounds don't align with the rust-platform's, and so on.
I'm sorry, what you're saying doesn't make any sense to me. I think you're missing that if your dependencies have version requirements that can't be unified, cargo will build multiple versions of the same crate. cargo will *never* attempt to build a library against a version of a dependency that conflicts with its version requirements.
As a Rust contributor, I agree with this assessment. We're pushing for a strong web story, but it's spotty so far. That said, stay tuned, the Rust community is one of the most productive and engaged open-source communities I've ever seen. 
Yeah. I agree. This should be implemented in future releases.
It looks like the word *primality* is missing from your dictionary. Evidently, it's missing from mine too (Chrome suggests *principality*).
And then you have lazy / strict versions for Text and Bytestring. But I agree, it's not as bad as people make sound after you've been doing it for a while
Haha :) I expected someone to bring "record" up in this thread. The preprocessor is a quite hard problem. Nevertheless it is implemented and it works. It just doesn't give you nice compilation errors. The library didn't die off, but it's far from the only project I have to maintain and I only have that much time. Unfortunately no guys from the community jumped in to work on the project. So it's basically just me whenever I get the time. In a sense, the library did solve it's most important goal: it made it clear that the Record problem, not dependent types or whatever, is the most important one to the community and how this solution fitted with the community's desires. The guys making decisions about the compiler are now well aware of all that and have been involved in discussions about integrating this into the compiler. However since the work on OverloadedRecords was already ongoing when "record" made its appearance, it's been decided that that thing should be implemented first. The final goal though is and has always been to make this a compiler extension.
It gave a parse error and yes [there is](https://ghc.haskell.org/trac/ghc/ticket/11350).
For the Enums part, why not do something like this: data Male = Male data Female = Female type Gender = Male + Female Would there be any overhead if enums were done like this?
The space complexity is mind bogging, I'd use `seq` ...
If your horse will compile, it will probably run.
In Unix world you want to have a newline character on the last line as well, so this is reasonable behavior.
So if I manage to find a Windows computer and run `unlines` there, will I get a different behaviour? I guess that instead of `\n`, `\r\n` would appear. Is it reasonable to accommodate to a specific OS world view in one of the Prelude functions? Also, that particular trailing newline at the end of the file was used to correctly predict, if a file was suddenly terminated or not. Is this even relevant nowadays?
So sad to see these things languish :(. Alright, sometimes there are good reasons, but... Maybe a restricted proposal for a simple boolean flag would have better luck? The proposal seemed a teensy bit "runtime-checky".
Thank you, this is a very constructive answer! :) I actually did not know that the Haskell standards even define the implementation of Prelude functions. Now the questions is: is this a correct design? I would totally expect `function` and `unfunction` to be `id`.
Wouldn't it break parametricity if you could match on types and do different things for different types? Or would this only be restricted so that all cases have to match on the same type (in which case, I'm not sure I see how it is useful here)?
I believe it is the latter case, and my code above is actually an example of such use case.
The implementation doesn't differ for the Windows version. I think it's a sensible definition when you view it as a utility function that takes a list of strings and puts each one in a new line instead of being a reversal of lines.
[removed]
I don't think _anybody_ interacts with mailing list servers after initial sign ups. Maybe I'm wrong? 
Really? I thought it was standard that rings without identity were called rngs.
There's certainly room for back and forth on the proposal itself. What was upsetting was to see a well thought out proposal killed with a simple "I already have plans for this." I'm the one who recommended Michael Sloan to just give up at that point based on my previous experiences, that may have been a mistake.
Well, `lines . unlines == id`. *whoops, not when any strings contain '\\n'.
&gt; A candidate that couldn't say how large a number 2^40 is (roughly) Is that the kind of question that you expect should be answered on the spot (e.g. under 10s) ? It took me a bit to remember the relationship between 2¹⁰ and 10³.
only when the strings don't contain newlines: `lines $ unlines ["a\nb"] == ["a", "b"]`
I think step "0" needs to be a little more complete A. assume cassandra B. Look for Haskell+cassandra C. Find cql-io on hackage immediately, which is the newest haskell+cassandra package in terms of recently uploaded. Making a proper step zero obviates the rest of the process - cql-io installs cleanly (at my last use) and has examples at the top of the haddocks.
Yeah, agreed. Then again, such is the nature of the beast, I suppose. Talk *is* cheap, but especially so when nobody understands your language. EDIT: It's basically about accountability. I'm all for accountability.
Firstly, Thanks for the feedback! Secondly, later on you'll see I don't assume Cassandra and use docker to spin one up. This wasn't written so that it could be a cookbook/tutorial, but also to give a "feel" for doing something real world in Haskell. I suppose that wouldn't be altered by having a step 0: setup section but I also don't see how it would add that much value. Given my response do you still feel that adding "step 0" would add value? If so, how?
As far as Haskell `String` is concerned, the only newline character is `'\n'`. The platform-dependent newline conversions will be handled when performing `IO`. See [`System.IO`](https://hackage.haskell.org/package/base-4.9.0.0/docs/System-IO.html#g:25).
That is matching on two *different* types (of kind `Nat`): `1` and `2`. As far as I'm aware, even languages with dependent types usually don't have that feature. It might be ok if you are only allowed to do it on promoted types (like `1` and `2`) though.
A lot of applications make more use of 'Dynamic' in reflex which is a behavior + event, so most applications use them heavily. They're used for handling (local) state in an FRP application.
You can simply split `Download` into `IncompleteDownload` and `CompletedDownload` and lose the `status` field: {-# LANGUAGE DuplicateRecordFields #-} data IncompleteDownload = { parent :: Maybe CompletedDownload, ... } data CompletedDownload = { parent :: Maybe CompletedDownload, ... } runDownload :: IncompleteDownload -&gt; IO (CompletedDownload, [IncompleteDownload]) runDownload incomp = error "implement download here" runDownloadRecursive :: IncompleteDownload -&gt; IO CompletedDownload runDownloadRecursive incomp = do (comp, children) &lt;- runDownload incomp forM_ children $ \c -&gt; runDownload ((c :: IncompleteDownload) { parent = Just comp }) return comp
The examples listed on that page look like they should be supported by [LiquidHaskell](https://wiki.haskell.org/Liquid_Haskell). It's a pretty cool project, check it out!
Awesome! This is the kind of stuff the haskell community really needs.
I think it's worth noting there are still other Cassandra packages out there. Some people will look at your post in its full generality (thanks for making it!) and some will be specifically looking to use cassandra and skim off the package name(s) - it happens.
Ha i think they just wish that it wasn't necessary because fewer were used. 99.9% of the time I just want to use strict Text, but some packages that i'm using will return a string for some functions, while other packages return a lazy bytestring for other functions, so you have to end up doing conversions. It's a small inconvenience
I have no idea _why_ this is the case, but it's not a bug: &gt; we do not infer the type of the argument to determine the datatype, or have any way of deferring the choice to the constraint solver (from http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#selector-functions) So, although `test (A 32)` is ambiguous, `test (A 32 :: A)` is not.
This is an equivalent example without confusing this issue with metapackages. [dependencies] regex = "2.7" a = "3.0" As I said: &gt; I think you're missing that if your dependencies have version requirements that can't be unified, cargo will build multiple versions of the same crate. This means the current behavior of cargo is to compile `a` against regex 3.0, and your library against regex 2.7. This behavior is totally orthogonal to 'metapackages,' an idea which I should remind you has no spec (as the blog post proposed it, though, I think you should think of it as a macro which expands to a set of dependencies). I don't know how much clearer I can be, and I feel like I am just repeating myself at this point.
That seems to contradict the blog post. To clarify, I am *only* talking about metapackages and the spec for overriding. I understand how transitive dependencies work, your explanation was satisfactory, I understand that it can cause multiple instances of the same package to be installed side-by-side within a build. *I get this*. What I am asking is whether the behavior for meta-package overrides will extend not just to explicit dependencies written as lines in `cargo.toml`, but whether that behavior will be *recursively applied* to transitive dependencies. Your equivalent example is ***not equivalent***, because the blog post explicitly says that the metapackage dependency will be overridden. That is, even if a metapackage dependency was `regex = "= 2.7"`, it seems the blog post suggests that a line in cargo.toml of `regex = "= 3.0"` would act as an override. What I'm asking is: will it apply that same behavior in overriding `rust-platform` recursively to all lines in `cargo.toml`, or just to the bounds you explicitly specify in the root `cargo.toml`?
Wow. Haskell for embedded applications is really taking off.
[removed]
It could very easily be that I'm not understanding, actually. I only know the basics of pattern synonyms and I'm not familiar with how `where` clauses work with them. That being said, what I'm thinking is that what you are suggesting could be analogous to something like: data Example a = Example a example :: forall a. [a] -&gt; Int example (Example @Bool _) = 0 example (Example @Char _) = 1 example _ = 2 which would violate parametricity. Like I said (briefly) earlier though, what you're suggesting might be ok, if that were an issue, because it is dealing with a promoted type (it has kind `Nat`, not kind `*`). As a result, it has a fixed "sort" of very specific types that inhabit its kind, and more types can never be added to that kind later on (unlike `*`).
According to the prime number theorem, the density of primes around $n$ tends towards proportionality with $1/log(n)$. That means that the odds that a random number of the magnitude $10^74207281 -7$ is about 1:170 million. But how to prove it? My first instinct, unless the number to be tested was of a special form, would be to just start with trial remainders. (Not trial division--calculating the actual quotient is both unnecessary and extremely expensive for very large numbers). So how to calculate $a^b + c mod p$ where a, b, and c may be very large numbers? First mod down $a$ and $b$ which will give you small integers. Unless the modded down $a$ equals $0$ (a case easily handled separately), the modded down $a$ will be a member of the cyclic multiplicative group of $Z_p$. The order of that group is $p-1$, so you can mod down $b$ by $p-1$ (Fermat's Little Theorem). So after the modding down you are left with an object consisting of a handful of numbers smaller than the $p$ you are trial-remainder-ing by. Such an object is trivially calculated. (As your test $p$ grows, you may want to employ binary exponentiation to increase performance and avoid the risk of intermediate result overflow.)
This is one of those "be careful what you wish for" moments. I tried to warn folks that this was going to be far less useful than things we already know how to do today. When I get functions in a functional language, I think I'd want to be able to, you know, compose them and call them with arguments and stuff.
I also don't know about "proper" deployment. But let me share the way I deployed my first Haskell web app a few weeks ago. I deployed application as a single docker container image on Heroku, so it requires a knowledge of docker ecosystem and a bit about Heroku. [Here](https://github.com/hi-ogawa/yesod-experiment) is a repository and I also wrote [an accompanying blog post](http://wp.hiogawa.net/2016/07/24/haskell-container-deployment/). Simpler version is also available [here](https://github.com/hi-ogawa/haskell-heroku-docker). I know it lacks some of normal Haskell practices (e.g. use cabal directly instead of stack), but I hope it will help someone.
Dude this is awesome! 
Yes, that is what is generally needed, as explained in several other posts. It would be annoying for the definition to be anything else. The names `lines` and `unlines` are the problem - they make you think that they are inverses of each other. It would be fun to have a bikeshedding session to think of better alternatives. But in reality, we are quite stuck with those names, since they have been part of the standard libraries since the beginning of time and are deeply hard-wired in code all over the ecosystem.
Part of the problem is that usually such advice is very opinionated. For example, I disagree with people who say that `String` is deprecated. I agree with `fail`, but there are those who disagree. For `foldl` I am sitting on the fence. On the one hand, it is very rare for `foldl` to be inherently the right choice. On the other hand, `foldl` is the classic left fold, and GHC is getting better and better at figuring out the right thing to do with it. And even when GHC gets it wrong, `foldl'` is also wrong just as often as it is right, so there really isn't any good natural alternative. Bottom line - don't take advice like that too seriously. Learn Haskell, have fun, write code, and you will soon formulate your own style.
For me, the promise of OverloadedRecordFields is not so much handling name collisions but being able to "reason" more at compile-time about the components of a record.
And managing database connections and credentials, and lots of other stuff.
What is there to configure? All I'm aware of is receiving all messages vs receiving summaries: which you configure when you sign up. Sure you might want to change it, but what else is there?
Yes I do. I think it's very important to be able to do rough estimations on cost, speed, size, complexity etc. quickly, so that's something I want to test for. Assume 2^40 is the probability of a hash collision in a particular type of design (birthday paradox result - another concept I'd like candidates to know about). Is that a problem or not? Well at least you'd *have* to have a feel for what 2^40 actually *means* in order to answer that. The difference is then between someone who will start designing a database layout ++ on the basis of some design where the properties of that design might be unsuitable for the initial problem, and someone who will just reject the design or tweak it right away.
Productive in what sense? I'm honestly fairly sceptical that someone could get truly productive in haskell within two weeks. Maybe if they were studying a lot outside of work too. But with no FP experience, no experience with types, etc. I do think 6 months is plenty of time to ramp up with Haskell, but I doubt most people learn all the basic abstractions in haskell within even a month, much less 2 weeks.
I cannot but agree with you --- we should co-operate on the low level primitives there is no other solution. If possible, we should just include implementations (with minor changes like argument order or something) from standard well reviewed sources like NaCl or Gladman etc. It is not always as easy as it is claimed but one should definitely try it.In fact it is no fun writing yet another bitsliced AES. My focus has been very different from what cryptonite is attempting (I think). Cryptonite has an enviable collection of primitives which can be used right now. I have very little to show on this front and I do not see raaz easily reaching the level of support for primitives cryptonite currently has. I am currently more keen on getting the api correct. Thanks for the message. I very much appreciate your efforts in getting cryto up in Haskell. 
I'm very fuzzy about the relationship between the ghc api and haskell-src-exts. Are they completely independent packages that don't share any data structures?
`cabal-install` architecture supports having multiple solvers. AFAIK SAT-solver based dependency solver was never discussed to replace the currently used default "Modular-solver". There was some experiments, and Z3 based solver is much faster to find *some* solution, and usually faster to say that there aren't solution (yet it doesn't say why there isn't). And for some use cases those would be enough, but not in general. There are other formalisms as well, e.g. answer set programming, and they are worth exploring too.
In the "building a working web app" sense.
This is a myth that Google has in the past tried very hard to pander, for its own reasons. But for that link to be relevant you would need to prove that most created content is open and freely available on websites, or at least visible to Google, and I do not believe that is the case. Actually, I noticed that Google has become much quieter on this issue lately, since they started investing efforts to increase their market share in China. As a professional working in the industry of content creation, I can testify to the fact a significant proportion of content, probably a majority, is created in 2-byte encodings, not UTF-8. &gt; by using something else you'll just make programming harder on yourself. That is exactly my point - since in fact most content is not UTF-8, why make it harder for ourselves?
&gt; What part of "most of the world" is that exactly? The part of the world whose languages require 3 or bytes per glyph if encoded in UTF-8. That includes the majority of people in the world. &gt; I'm under the impression that UTF-8 is more or less the standard that everyone uses That is so in countries whose languages are UTF-8-friendly, but not so in other countries. &gt; There's also the point about efficiency, unless you are heavily encoding Chinese characters, in which case UTF-16 might make more sense. There are a lot of people in China. &gt; HTML4 only supports UTF8 (not 16), HTML5 defaults to UTF8, Swift uses UTF8, Python moved to UTF8 etc etc etc. Those are only web design and programming languages. All standard content creation tools, designed for authoring books, technical documentation, and other content heavy in natural language, use the encoding that is best for the language of the content.
Filed https://github.com/nick8325/quickcheck/issues/106 -- hoping that Nick responds.
Please try it. I was not able to make it compose elegantly, even with that.
Good question! My best suggestions for lists: * What I wish I'd known when learning Haskell: http://dev.stephendiehl.com/hask/tutorial.pdf * The Commercial Haskell Group's Jump: https://github.com/commercialhaskell/jump These lists are somewhat opinionated and not everyone agrees. But they're expert opinion. I agree that Haskell Book is good but warn that it teaches Haskell as it is rather than a "Haskell the Good Parts". For example, its chapter on strings is based on `type String = [Char]`, not on `Data.Text`. Not that it could realistically teach Haskell without teaching `String`. But do compare that chapter with Stephen Diehl's. 
I've even started a bounty on http://stackoverflow.com/questions/38644779/how-to-use-quickcheck-to-test-database-related-functions
The best part is the [merchandise](http://www.monmouthparkstore.com/haskell-2016/). I'm getting the [shot glass](http://www.monmouthparkstore.com/haskell-2016-shot-glass/).
Tom deserves recognition for this work. This is exactly the sort of documentation that GHC needs to become a more welcoming codebase for newcomers. Thanks for your work, Tom!
It would help if the lazy versions of Text and Bytestring didn't exist, see https://www.reddit.com/r/haskell/comments/4uxgbl/the_rust_platform/d5udb42 Text and bytes are different things, so Text and Bytestring needs to be different. (It would be great to merge Bytestring and Vector Word8 though.)
Tried searching for [building a blog with Haskell](https://www.google.co.in/search?client=opera&amp;q=building+a+blog+with+haskell&amp;sourceid=opera&amp;ie=UTF-8&amp;oe=UTF-8) * http://yannesposito.com/Scratch/en/blog/Yesod-tutorial-for-newbies/ - deals with just one data model, Articles, so no question of associations * http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html - talks about infrastructure-level stuff, not actually data-modelling * http://www.janrain.com/blog/tutorial-building-a-sample-application-with-haskell-snap-postgresql-and-the-postgresql-simple-snaplet/ - again, just one data model * https://www.fpcomplete.com/blog/2012/10/yesod-tutorial-1-my-first-web-site - nope, nothing here either.
`haskell-src-exts` is a completely separate package with no relationship to GHC, and was written years after the GHC API had existed in at least two forms (it was essentially rewritten back in 2006). The only GHC developer who I believe is actively involved in both projects at all is Matthew Pickering.
It is also becoming important as we start treating tool writers as first class users of GHC.
I use nixops. Makes it trivial to deploy to a virtual box instance for testing and then deploy to the live site. Plus it is trivial to rollback a bad deployment.
Actually the main point of my post was NOT Esqueleto's verbosity. I can live with that. But what I'm not clear is, what to do with the `(p, mb)` that is returned in your example above. How does one write a large-scale code based on those building blocks?
Kind of worrying that it's not mentioned at all for web development work, instead.
If you're making complete Haskell tooling based on haskell-src-exts, this is almost certainly a mistake. GHC and HSE treat too many things differently, and the inconsistencies mean than for serious projects, HSE-based tools aren't very usable. HSE is only acceptable where the tool has a suitably narrow use case, or if its application is strictly optional and can degrade gracefully when it fails to parse.
Thanks (also to aseipp) this is the kind of orientation I was looking for.
Thanks I'll look into it. I work on a pretty small laptop so I've never noticed!
Thanks :) The driver is in really really early development, so you will find many things missing. If you face any problems, just let me know on github.
For me it's some overloading mechanism that isn't type classes or implicit params. Really hope whatever OverloadedRecordFields is desugaring to can generate good error messages and handle large numbers of record fields without compile time regressions. Once this lands it *will* become the most (ab)used extension in GHC.
Thanks for the vote of confidence, Luke. /u/saurabhnanda, if you are interested in trying Opaleye and you need any help please feel free to email me any time. My email address is linked from the Opaleye README on GitHub.
I mean, someone might be able to make basic changes, but I think in two weeks, there's no way you'd be programming the right way in Haskell.
Well if I ever get a time machine I'll be sure to go back in time and stop them for you.
Yes that's right. We like that Opaleye is psql specific as it allows use of some of the more special features. I view that as an advantage. 
Hi guys! I've been wondering for a while if it was possible to perform a breadth-first search on a graph using a lazy linked-list instead of a queue. Today I gave it a go, and it worked out *(EDIT: for a tree at least)*. I wish I had found the answer somewhere before, so here are some notes I took for others who might be wondering. I'm by no means a regular writer, and haven't done any algorithm analysis in a while, so... all constructive criticism is very welcome! Enjoy the read, I hope you learn something.
Sweet! What about using this same technique to perform a breadth-first search of a finite tree?
I'd strongly suggest removing the `unsafePerformIO` use in this code. It's a bad practice and unnecessary in this case. If we trace the use of the values gained from `unsafePerformIO`, we'll find that they're used in `getAuthEntity` function in the `YesodAuthPersist` instance for your app. According to the [docs](https://www.stackage.org/haddock/lts-6.10/yesod-auth-1.4.13.3/Yesod-Auth.html#t:YesodAuthPersist), that function runs in `HandlerT master IO`. There's a [bunch of functions in that monad](https://www.stackage.org/haddock/lts-6.10/yesod-core-1.4.22/Yesod-Core-Handler.html#g:1) listed in the docs, one of which is `getYesod` which can get your entire app configuration. Another is `getsYesod`, which takes a function that you can use to focus into a specific part of it. For example, you can use `getsYesod appAdminName` to get the admin name out of the config, in any `HandlerT` monad function. So now instead of `unsafePerformIO . loadConfig`, it'll be: getAdmin :: HandlerT App IO SiteAdmin getAdmin = do name &lt;- getsYesod appAdminName password &lt;- getsYesod appAdminPassword return (SiteAdmin name password) If you're just storing the password in the configuration, then the `validatePassword` function becomes: validatePassword username password = do SiteAdmin{..} &lt;- getAdmin return (adminUsername == username &amp;&amp; adminPassword == password) The other functions that rely on the unsafe IO can be similarly factored out. This is left as an exercise for the author ;)
Is your complaint with Esqueleto's verbosity due to having to write out all the join conditions by hand? If so, I made a small library which might help you. It uses entity's types to deduce relationships and then writes out the joins for you automatically: https://github.com/pseudonom/dovetail/blob/master/test/Spec.hs
You may enjoy the [fgl paper](http://web.engr.oregonstate.edu/~erwig/fgl/haskell/old/fgl0103.pdf), especially the section on breadth-first search (in arbitrary graphs).
Very cool application of lazyness you have there. Looking just at the given problem you can become significantly faster by searching in the other direction, starting from xf, as you can eliminate any branch that leads to a rational number. Using this for example the infeasability of 10^10 can be checked with less then 20 divisions.
Right now, you use lists to supply values to the prepared statements. The clear reason for this is that you automatically support any number of parameters. The downside is that lists must be homogeneous (without getting into existential stuff), so each parameter receives that "put" function. Can you instead find a way to support tuples? Because they are heterogeneous, users of this library won't have to decorate all of those parameters. It may involve a lot of code (or Template Haskell) to support all the tuples, but it would be cleaner for users of the library. As an aside, since the length and types of a tuple are known at compile time, you're closer to making static guarantees about the prepared statement.
Using tuples is an interesting idea, do you know if any of the existing Cassandra libraries do this?
Commented at https://www.reddit.com/r/haskell/comments/4vh4sg/recommended_way_of_dealing_with_db_associations/d5z2rvy about the same topic. Is this really the best way to model associations in Haskell? Why not nested records? Also how do these ad-hoc data structures scale to a large code-base? Any examples?
Is there a library which automatically does the last transformation in your code? Else writing these functions by hand can become a time-sink and source of unnecessary bugs. 
Hmm. I am beginning to see your point. Having spent years with Rails+ActiveRecord I can't think of a DB layer which doesn't marry DB-access and domain-logic. In your example, does `Order` or `UserA` have to parameterised? The way it's current written, it allows them to be parameterised over **any** data type. for example, I can do the following, which is, in all probability, incorrect: UserA{userName="Saurabh", userBlogPosts=[comments]} :: UserA [Comment]
Lol, I got the idea from the [article](http://codygman.github.io/posts/2016-07-30-real-world-haskell-cassandra-pt-1-connecting.html) you wrote, Mister! For saurabh, look at [cassandra-cql](http://hackage.haskell.org/package/cassandra-cql) for the technique they used, it's pretty clever.
If the tuple could guarantee at the type level that it was a result of a valid join, then this is all good, but how do you get that? That's at least something that the nested record approach deals with.
Definitely not for a beginner but there is this beautiful oneliner: powerset xs = filterM (const [True, False]) xs
Hi, [Wikipedias power set article](https://en.wikipedia.org/wiki/Power_set#Algorithms) shows a simple recursive power set algorithm, which can be easily translated to haskell. 
In this context the ring is a field, which makes everything *much* simpler. And the commonalities come from the fact that both categories have somewhat similar properties (eg. [Cartesian closed](https://en.wikipedia.org/wiki/Cartesian_closed_category), or whatever), though there are of course big differences, too.
If you understand that then congratulations! You definitely understand the List Monad :)
While `foldl'` is strict enough in your simple example, you might as well just use `foldl` there, because GHC will be able to figure out that the addition should be strict at each step. Whereas in even a slightly more complicated case, `foldl'` is not strict enough: mean :: Fractional a =&gt; [a] -&gt; a mean = uncurry (/) . foldl' (\(s, n) x -&gt; (s + x, n + 1)) (0, 0) The strictness of `foldl'` resolves the tuple constructor at each step, but inside the tuple you build up two big piles of thunks just as with `foldl`. Or perhaps GHC's strictness analyzer now figures out what to do here, but if that is the case, it probably figures it out for `foldl`, too. So there's actually not much advantage of `foldl'` over `foldl`. If you are just going to rely on the GHC strictness analyzer and hope for the best, or your calculation is small enough that you don't care (the usual case), then you might as well just use `foldl` or even `foldr`. If you want to make sure you are getting the right depth of strictness, you need to work that out in any case; you can't rely on `foldl'`.
[removed]
You definitely want to check if that works or not.
I would guess that reflex https://github.com/reflex-frp/reflex-platform is the most battle tested.
Actually the technique does generalize to graphs easily enough. It takes a little bit of work, but it's quite doable.
How does it work?
I use it in `distance`: ... go (t:ts) | value t == xf = Just $ depth t ... I don't believe we'd get significantly faster removing it anyway, since it is just "there". It's just a few extra bytes to copy (though I my be misjudging the cost). 
Good read, thanks
I'd recommend react-flux. Example: https://github.com/agrafix/react-flux-example
Jep that's what I get for just skimming your post :(
I think I disabled `hlint` because of inconsistencies. (some valid comments cause parse errors?).
I'm using `react-flux`. I've just added a separation between `Event` and `Cmd`. Event are the events that can modify the state purely (using a pure `update` function) and `Cmd` are events that can't modify directly the state but produce `Event`(s).
Lovely explanation, thank you!
This is really cool. Just a couple of weeks ago I tried to come up with exactly this, but I didn't have enough time and ended up using a queue instead. Thanks!
It uses the ConstraintKinds extension to define Constraints using type aliases and familiies. in the top-level GADT called `TypeSpec`. The constructors of `TypeSpec` `Valid` and `Invalid` call `Try` and `Donttry` respectively. For data types `It`, `ShouldBe`, ... instances of the open family `EvalExpectation` exist, which compare e.g. compare types and return `TypeError` `ErrorMessage`s when things are not ok. A type class `PrettyTypeSepc` allows the nice printing. 
This is sometimes called an Allison queue or a corecursive queue. 
I recommend transient. You can code the complete application (client and server) in a single expression and create reusable widgets https://github.com/agocorona/transient/wiki/Transient-tutorial#transient-in-the-web-browser
Like magic!
I'm wondering if that's a spurious match due to "embedded DSL", domain specific language, rather than use in embedded systems. I work in embedded systems and would love to use Haskell, but haven't put all the pieces together. I'm aware of Lee Pike's CoPilot project with Ivory and Tower languages and their use in UAVs, also Tom Hawkins Atom language and its use in hybrid garbage trucks, but those are a few years old and I haven't seen much news lately. And there's Anthony Cowley's robotics work. However, I can't name any other ongoing embedded uses of Haskell. Please, someone, prove me wrong! 
Nested records, as it relates to representing a relational database, doesn't make any sense because every relationship is a 2-way street. No table "owns" any other table. By having nested records, you're pigeonholing yourself into selecting `B` every time you select `A` from the database. Meanwhile, the database allows you to select any combination of `A` and/or `B`. Do you really need to have access to all of the blog posts whenever you select a user? To be quite honest, I can only think of a single instance where you would want to display blog posts that belong to a specific user: while browsing blog posts that belong to that user. So why would you make a record that's only suited for one particular way of "viewing" the data? Meanwhile, I can think of plenty of cases where I would want to select the user and not want to look at blog posts at all (editing a user, adding a user, listing all users, etc.). Using tuples to contain your types, rather than having nested types, gives you the greatest flexibility to represent data from the database because they can be composed together in an unlimited number of combinations.
Also an option: bfs f = head . head. filter (/= []) . map ($[]) . bfs' where bfs' Tip = [] bfs' (Branch a l r) = (if f a then (a:) else id) : zipWith (.) (bfs' f l) (bfs' f r) You represent each layer of the tree as a difference list. So to merge the in-order representation of the two children, you compose them pairwise.
At FP Complete, we build Docker images and deploy them to a Kubernetes cluster running on EC2. I actually just made a blog post that's more about our CI setup, but also covers some of our overall deployment approach: [Announce: public Jenkins CI server](https://www.fpcomplete.com/blog/2016/08/public-jenkins-server). Tim Dysinger also made this blog post: [Kubernetes for Haskell Services](https://www.fpcomplete.com/blog/2015/11/kubernetes).
This is indeed by design. While in this case it is obvious which type is meant, in general it is hard to specify how type inference should work in the presence of ambiguous field names. Thus we don't even try; we just do type checking and otherwise require a signature. DuplicateRecordFields could have ad hoc rules for cases like this, but such rules would inevitably be incomplete and harder to predict, so we chose the stupid simple option. DRF isn't really designed to be useful when you have lots of ambiguous selectors. The plan is to add OverloadedRecordFields proper in a future release, allowing names to be disambiguated by type inference, but requiring a syntactic indicator when this happens.
Can you say more about the reasoning you have in mind? It may help inform the design.
Calculating the depth for each node in a row seemed like overhead to me so I implemented a lazy list based version for this problem that works explicitly row based. Although the lists representing each row are still lazy ones. Currently seems to be about twice as slow as your solution though but I don't think that's because it's inherently worse to work on lists and probably because of pitfalls because of laziness. Code is here: https://gist.github.com/AndreasPK/107493d82adb17f22993a391b78d8be3
We normalize our data so the result of our queries are typically not a direct representation of a table but the result of a join. We use types that match the result of the join, not a tuple of the two tables that went into the join. 
as I said: I like it but I would prefer the other (there are links floating around here) versions - yes those might be 40 or so chars longer but as you said: does 40characters really matter? - anyway: just my opinion man ;)
You're underestimating the power of an ORM query interface. User.find(1) User.includes(:posts).find(1) User.includes(:posts =&gt; :comments).find(1)
OP is using a queue under the hood, too, it's just sneakily encoded in the return-to-caller addresses of his `(:)` thunks.
* I might want to use the *HasField* constraint with *DefaultSignatures*. "If the record has a field named foo of type X, here's a default implementation of the function..." * If I have a record in one branch of a sum type, how about making record selectors work automatically as traversals for the sum type? * If I have a list of records, how about making record selectors work automatically as traversals for the list? * If I have a tuple of records, and the selectors are different for each record, I would like to be able to use the selectors directly on the tuple. * In general, if I have a record as field of another record, and the selectors don't clash, I would like to be able to use the selectors for the inner record directly on the outer record. A bit like the "classyLenses" technique, but without the TH. * Making a lens for a subset of a record's fields is often a pain. Could there be some generic combinator that, given a type level list of field names, returned a lens for those fields? 
Please read https://www.reddit.com/r/haskell/comments/4vh4sg/recommended_way_of_dealing_with_db_associations/d5zqtpa . What would you say to the last paragraph in my longish comment? &gt; If you notice, in the last example, there's a loss of fidelity. The user who created the post, and the user who originally referred the post-creator (assuming it's some sort of multi-level marketing scam we're talking about), both, are "unlabelled" in the resulting tuple. Don't records solve this problem? 
&gt; What I would like to see is general support for partial evaluation; for example, function parameters could be annotated at definition-site with reduction hints, but also overridden or explicitly specified at use sites. However, for partial evaluation to generally make sense, we would have to have a well-defined semantics for compile-time strong reduction, i. e. reduction to full normal forms. What's the essential difference between this and supercompilation?
You can even eta-reduce it!
Thank you for replying to my post. I am new to Haskell so I am still learning this program. I just picked this number randomly to calculate the size. However, I am interested in another classes of prime numbers and not in Mersenne primes. I am having problems entering numbers into a power, I know how to enter integers and not int. The computers I have access to have the abilities to produce a number to trillion digits.. Are you in New York City? Yours truly George Remscrm
Nice, and also shows the algorithm. Note for next time: Google "corecursion" beforehand.
No, I live in Montreal, but there's no need to meet in person, I regularly help people on this subreddit by answering their questions. Did our comments help you to get the answer to your first question? Do you have a follow-up question? Otherwise, I have a few questions of my own. 1. You didn't tell us much about yourself, so we have no idea what kind of knowledge we can rely on in our answers. Do you know how to test that a number is prime in other languages, but you're having difficulties to do it in Haskell? Or are you asking if we know of an efficient method for verifying the primality of huge numbers? 1. In another comment you say that you have access to a computer which can produce numbers with trillions of digits. Storing such a number would take about a terabyte, so I doubt such a number is stored in RAM, is it? If the numbers are stored on the disk, you'll have to write your own implementation of addition etc., because `Integer` uses the RAM. 
Except that I never use ORMs. Why would I? When the majority of my time is spent on complex queries that ORMs can't possibly help with, using an ORM to deal with simple queries like that is a colossal waste of time and effort.
Great news that making this an extension is in the cards. Is there anything specific that you'd like another pair of hands to work on on the record library? I am just a Haskell dabbler and reddit lurker but have found `records` very ergonomic. 
Yeah, you're basically relying on the runtime to build up a queue of continuations from within bfsList. Similar to how you'd solve the problem with continuations, but even simpler as it's essentially done for you by the language.
Oh man those infix (`###⋯###` and `~~~⋯~~~`) operators are brilliant!
http://i.imgur.com/NCC6d4w.jpg
Great work /u/dxld!
Thanks, interesting. It is indeed reasonably clear, however it makes use of mutable global variables in IO. As far as I can tell getItem and setItem are some kind of global storage? This doesn't seem very composable to me... as soon as you want to put two "TodoMVC's" one beside the other you cannot because they're using a mutable global variable. 
Unfortunately, I'm not aware of any such papers. Can you provide a link, please?
Yeah, I intensively searched Hackage for a similar project and came across `should-not-typecheck`. Heres what's different: 1. `should-not-typecheck` is about stating what **shouldn't** work , `type-spec`'s about what **should** 2. `should-not-typecheck` works at **runtime** - `type-spec`s are validated at **compile time**. (1) This is achieved using deferred type checking and runtime unit tests. I strive to achieve something similar to `should-not-typecheck` with the `Invalid` constructor for `TypeSpecs`, but it's just not there yet :) I thought about linking to `should-not-typecheck` on the `README.md` but then realized it's Apples and Peaches. --update-- and I was wrong I actually had a link to it from the very first version of the README.
That's actually the reason I wanted to release this library :P
I'm using cassandra-cql but think I would use cql-io if it supported auth.
Having a way of checking that some things can't type check is extremely useful. After all, we use types to reject programs that do not make sense. 
Alas, no direct recursion and no list comprehension, so your lovely solution fails the exercise. Let's try this: powerset xs = let go 0 = [x | x &lt;- [filterM (const [True, False]) xs]]; go 1 = go 0 in go 1
Silly question: how do I install this version of `ghc-mod` using `stack`?
It fell out of stackage master a while ago because they moved to GHC 8, just submitted a PR to have it re-enabled: https://github.com/fpco/stackage/pull/1760 I don't remember the incantation to install something that's not from Stackage off hand unfortunately, maybe someone else does?
any mid/large-sized professional projects that use reflex? 
Hello /u/dxld, Speaking on behalf of the Cabal team here, were there any bad BC-breaking changes in 1.24 which you would have preferred if we hadn't done? Anything that you wish Cabal provided but doesn't? We'd love to coordinate with you and other tool authors to give ghc-mod and similar tools the API they need to operate smoothly.
I'm sure you've been asked why you think string shouldn't be deprecated before, can you link me to that answer?
But why? What is wrong with using Type Families to make type inference work perfectly fine? See my earlier post: https://www.reddit.com/r/haskell/comments/4tj0xa/are_type_families_a_suitable_solution_to_the/
Yeah that's definitely true. I actually had a link to `should-not-typecheck` in the REAMDE from day one (yesterday)
Wooohooo!!! Finally &lt;3 Thanks alot!
That... was my fault. We have better BC processes in place now. I swear it will be better!
Your suggestion is essentially equivalent to the [OverloadedRecordFields plan](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields) (modulo some details about type families vs. functional dependencies, etc.). So the question comes down to: why do we require a syntactic distinction between fields that get special overloading treatment (`#foo` with OverloadedLabels/OverloadedRecordFields) and bare record selectors that must be unambiguous (`foo` with DuplicateRecordFields)? The problem is that while the constraint-based approach works well in many cases, it doesn't work in all cases. For example, if you have a higher-rank field, bidirectional type inference depends on knowing the field type before running the constraint solver. Moreover, adding polymorphism can introduce ambiguous type variables and thereby break previously working code. We don't want to change the default behaviour of field selectors to something that will simply fail in some cases, hence we provide an opt-in syntax for users who want the special behaviour.
Oh my. Do they have aliases for varying numbers of symbols?
Yep. :) Here is a [fairly recent one](https://www.reddit.com/r/haskell/comments/4n5zmh/a_sticky_stringy_quandary/d426426).
I'm very curious. Coincidentally I have a project at hands with a Yesod backend that needs some minimal browser-side logic: managing associations (1:N, N:N) of the main model other models from the main form (possibly using autocompletion) and a page with a bunch of filters on the main model (where filters need no "submit" to be effected). I'm currently leaning towards: * Just add some good ol' jQuery to my Yesod-based templates * Add some React components instead of jQuery * Make the whole browser-side in React, resulting in: 1. Actually no need for Yesod, possibly Servant becomes more interesting then 2. Server-side prerendering as SEO is very important for the public part of the app * Do "all in Haskell" and since it requires server-side prerendering that means that I'll have to rely on an experimental feature like the one in your project. In that case Servant would suit better as well. In my opinion these 4 options are in order of increasing "time to market", therefor I'm now leaning towards the first or second option. Having a Haskell based app from bumper-to-bumper is obviously so cool that I really want to consider the last option :) Certainly I'm looking into you projects right now!
&gt; then code like &gt; &gt; let x = 6 :: Int &gt; x + 2.3 &gt; should give an error when evaluated in GHCi (it doesn't) Are you sure? Using GHCi 7.10.3 I get the following type error: ghci&gt; let x = 6 :: Int ghci&gt; :t x x :: Int ghci&gt; x + 2.3 &lt;interactive&gt;:4:5: No instance for (Fractional Int) arising from the literal ‘2.3’ In the second argument of ‘(+)’, namely ‘2.3’ In the expression: x + 2.3 In an equation for ‘it’: it = x + 2.3 ghci&gt; This intuitively makes sense to me as well, so I don't see how RankNTypes would alleviate this. There are lots of things that you can create Num instances for which can't reasonably be added to 2.3. (People will create Num instances for functions, infinite series of functions, etc).
&gt; Even for the apple + orange thing. You still have to consider that they are also both objects, and also both "objects that fit in my hand". Well treating `newtype` (or another keyword, `inherit` or whatever) as a type declaration would constrain this to only having one ancestral line, and I don't see why inferring directly up the line should have any wild computational cost. That being said, the above is certainly overly restrictive in the mathematical sense--`Apple` is a subtype of `Fruit` would mean it's *not* a subtype of `RedThings`; but Haskell already embraces this limitation in its typeclass system anyway: i.e., if I say `Nat` forms a `Monoid` under `+`, that entails that as far as Haskell is concerned, it is not a `Monoid` under `*`. As usual, the mathematically sensible thing is a pain in the ass to work with, and anything less feels claustrophobic.
Yeah... the last time there was an effort to implement supercompilation for GHC, I seem to recall it also took the approach of only activating where requested by annotations.
One solution might be to replay the appropriate portion of your GHCi command-line history. Unfortunately, if you were running GHCi inside your editor and the editor died, you probably have lost that part of the history. So perhaps a simple solution is to have either the editor, or GHCi itself, save the history periodically and not wait until the next graceful exit.
I hope you there will be open roles after I graduate (2 - 2,5 years from now) too! :) Actually one of the reasons I take finances as application area is increasing my small possibilty to work at Standard Chartered.
At this rate, you are going to dry up the Haskell dev pool! :)
Well done! Works really smooth.
Cheers.
Apparently, "TRex" supported a "lacks" constraint: https://wiki.haskell.org/CTRex Would it make sense to have something of the sort as part of the "magic" typeclases of OverloadedRecordFields?
Thx, already registered :) 
I'm a Stack user liking your work, yet I downvoted this. Upgrading to a newer cryptohash would have increased dependencies, though /u/mightybyte was unclear. And most other cabal dependencies range indeed from extremely stable to frozen. No real clue why it wasn't integrated, but it was used in two packages, so that might be a reason. I'm addressing the point on marketing upthread.
+1, last time I had to install from candidate url, but it went through @dxld: Thanks for all the heavy lifting ;-)
I upvoted this, but regarding toxicity, you should both stop trying to blame each other and discuss who started. I disapprove but understand Snoyberg's behavior. Each party should try to deescalate such conflicts, even though I understand it's hard. If you misspoke, saying it earlier helps, especially in a tense discussion.
Please correct me if I'm wrong, but only Neil seems to be doing publicly visible open source work these days, from what I can tell, and Shake could be considered a result of a previously internal tool.
Thanks, great explanation!
*curtseys*
These [CUFP notes](http://anil.recoil.org/papers/2011-cufp-scribe-preprint.pdf) have some information.
The design principle here is that GHC extensions should be conservative, i.e. (as far as possible) enabling the extension shouldn't break/change existing code. If we had an extension that generated constraints for all field selectors, not just those with a special syntax, then turning it on would break code in hard-to-understand ways. And the extension would make it impossible to use selectors for higher-rank fields. It is possible to work around these problems by doing constraint-based name resolution only for ambiguous fields, not for all fields. But then the typing rules for fields change radically depending on what is in scope, and that doesn't seem desirable either.
On a similar note I'd been thinking about measuring complexity of programs based on their types and using Kolmogorov complexity (thanks /u/kamatsu) as a way to rate things and counting/describing the number of cases possible in a type. E.g. * `data X` has zero complexity. * `data Y = Y` has identity complexity. * `data P = P | Q` has 2-complexity. * `data S = S S | Z` has infinite cases in the spine. * `data S = L S | R S | Z` is _more_ complex, even though you can use the previous to model this one, this type's definition permits "more" complexity as you might intuitively see as a programmer worried about combinations. Real world example: compare `Data.Tree.Tree` vs `Data.Vector.Vector` -- trees are just harder to think about than a vector, and more things can go wrong in manipulating them. * We define some notion of complexity for functions, e.g. `Y -&gt; Y` vs `P -&gt; P` (a function returning `Y` can't do anything complicated assuming that it's total, but OTOH that assumption isn't necessarily practical). Perhaps we just rate their bodies instead. Given a smart person who knows about these things to formalize that in a way that is (1) consistent, and (2) automated (i.e. take in a type and spit out a rating for it), we could get useful things out of it: * Determine that `data Family = IPv6 | IPv4` is less complex than `ipv4=2 :: Int; ipv6=10 :: Int`, because the latter permits infinite cases. * Determine that any case-expression involving `Family` is _obviously_ less prone to error than one involving `Int`, and so we could say that any expression of type `Family` will end up with a lower complexity than `Int` or `[Family]` or even `Either Family x`, for example. We could make tooling to consume Core (or, perhaps, Haskell) and output ratings. With this you could ideally write your really important code in an isolated module which defines all types up-front and aim to maintain a finite complexity, thus giving you big confidence in your code. Writing the code is one part, but then _changing_ the code is really going to yield benefits in code with a lower complexity count. There're just fewer possibilities.
&gt;No I did not. Why are you making up things I said when my posts are still here and can be easily verified? &gt; It looks like you are making the mistake of trying to shoehorn an OO anti-pattern into haskell. 
Getting emails is the entire purpose of basic contact forms for most SMBs. Why force users to login to a service to view their form records that are stored in a remote database when they can receive them in their inbox that they use day in day out already? It's exactly what they desire and expect from a service like this.
I think that post completely misses the point of mutability. Who cares about the difference between creating a changed copy vs simply changing the original? If your question is "how do I make this object different?" then neither approach is less valid than the other. The meaningful difference comes in when the question becomes "how should I handle values from outside this scope?" (Im)mutability matters when the values you're trying to change don't belong to you. The point of mutability is that other code can ask you to make changes, and you can do that without just hoping that they use the new returned value. The point of immutability is that this practice of changing objects that don't belong to you can be considered an anti-pattern. It also sorta bothers me that the writer chose to acknowledge object identity. There's almost zero cases where an object's identity is useful. You should never use identity for equality. You should never use it to track anything. There's just no case where identity is a reasonable metric. TL;DR: The difference between mutability and immutability is about managing how larger scopes see changes from smaller scopes. This crap about performance and identity is superficial.
Thanks, I'll update this when I'm preparing the deployment tutorial.
Thanks that's useful. I can probably sort a lot of this by using a bootstrap container instead of container-fluid.
I'm also leaving Standard Chartered in 2 weeks to go to Barclays, which will reduce that by 1. Fortunately I hope to be doing some open source work in my new job at Barclays.
I suggest emailing again and asking for a response. Corporate email systems use Outlook, which sucks for keeping track of these kind of things.
Check the FAQ on the kickstarter, it's there. Agreed, it should be added to the site
&gt; should give an error when evaluated in GHCi (it doesn't). You're probably thinking of `6 + 2.3`, which works because the type of `6` is not `Int`, like in most other programming languages, but `Num a =&gt; a`. &gt; The point is that we should not insist that both inputs actually have the same concrete type, only that they belong to the appropriate typeclasses (in this case, Num). You're probably thinking in terms of OOP-style inheritance where `Int` and `Double` could both be a subtype of a parent class named `Num`. Type classes work differently, a type may or many not have an instance for a particular type class, so it's more a "has a" relationship than an "is a" relationship. In other languages you can add an `Int` to a `Double` because numeric types are organized into a tower and it is always possible to find a numeric type to which both arguments can be implicitly coerced. The coercion functions are known by the compiler in advance, and this in turn is only possible because all the numeric types are also known in advance. In Haskell the `Num` typeclass is open, meaning that users can define their own numeric types, the compiler cannot know all the coercion functions in advance, and so we require explicit conversions using `fromIntegral` and friends. That being said, it is possible to implement an alternate addition function `add` such that `add (2 :: Int) (2.3 :: Double)` is well-defined, not for every type instantiating `Num`, but for a few pairs of types (here `Int` and `Double`) for which we specify the target type (here `Double`) and the coercion functions (here `fromIntegral` and `id`). I have implemented a proof of concept [here](https://github.com/gelisam/hs-promote#readme). &gt; I've read that this effect can be achieved using forall and the RankNTypes extension That doesn't sound right. Perhaps you are thinking of existential types, which also uses the `forall` keyword? It allows you to create a type which can hold a value of any type which has a `Num` instance. {-# LANGUAGE ExistentialQuantification, StandaloneDeriving #-} data SomeNum = forall a. (Show a, Num a) =&gt; SomeNum a deriving instance Show SomeNum -- | -- &gt;&gt;&gt; increment (SomeNum (6 :: Int)) -- SomeNum 7 -- &gt;&gt;&gt; increment (SomeNum (2.3 :: Double)) -- SomeNum 3.3 increment :: SomeNum -&gt; SomeNum increment (SomeNum x) = SomeNum (x + 1) The big downside of existential types is that you lose all the type information except for the fact that the type has a `Num` instance, so you can't do much with them. Here, I can add `1` because `1` can be converted to any type which has a `Num` instance including the type of `x`, but if I had a second argument `(SomeNum y)` I could not add `x + y` because the information of whether `x` and `y` have the same type has been lost. &gt; A constraint must be a monotype I have never seen that either. What does your type signature look like and which version of GHC are you using? 
Yeah, there's a lot of interesting stuff here. While writing the post I wondered whether my fragility metric would be more useful as a measurement of a language or a measurement of a program/project. I'm not sure what the answer is, but your type complexity seems to be more in the latter category. I think a related question is whether you want to include the test suite in the definition of "build". /u/BayesMind mentioned the ratio of the token based fragility to the character based fragility. I wonder if some kind of relationship between fragility and your type complexity would have any interesting meaning...
Goes along with 'compile time verification', but typed queries are also more robust to refactorings. At my job, we have a few queries that we're not able to express in `esqueleto` and have to use raw SQL. These queries are always the most troublesome during refactors. Someone forgets to rename or remove a column and the only way to find out is a runtime crash.
Sometimes "for sure next week" is better than "perhaps the next year". There is so many example of that...
Unfortunately, developers type characters, not syntax. Until we get structural editors, anyway.
Except you break then everything what haskell tries to teach you, and you make newcomers confused. 
I don't know how to turn it into a measure of languages rather than programs. Like, per language you could have programs with immense variation in that metric. I could build extremely fragile software in Agda or something even, just by deliberately not encoding anything important...
Anything that requires membership or authentication is always provided to a client in the event they wish to self-manage their services or pass management to another developer/webmaster. It might seem trivial to you and I, but it isn't to clients for any number of reasons, many of them completely asinine, but that's the burden *we* bear as service providers. I've had clients refuse to "sign in with google/facebook" to certain affiliated services they want to embed on their website because "I don't want them reading my email/profile." We already spend enough time overcoming their ignorance on *what it is we even do* (i.e. "Why can't you just add in this completely vague and potentially impossible functionality I'm requesting in a couple of days, it's easy."), that it's just not worth it to add "oh-just-one-more-thing" to the pile if there's an easier alternative. Again, to you and I it's trivial, but it's not, especially not when you compound it by 20-30 SMB clients a year. Just get your auth straight where it's a simple user/pass with no intermediary and I'll use it. Hell, do what formspree does and forward any POST that hits your servers, up to X submissions a month for free or whatever, and I'll even evangelize you to other shops. All said and done though, I wouldn't even really worry about it. We're just one shop and I'm certain you'll get *plenty* of business from shops/clients that aren't trifled with the github issue. I'm just speaking from my chair. Take it for what you will.
&gt; "if you have some spare CPU could you try evaluating it before it gets downstream but no worries if not." I know neither if this is possible or if the semantics actually make sense Ah, I hope some of the GHC boiler-room engineers will get back at us on this.. 
&gt; I could build extremely fragile software in Agda or something even, just by deliberately not encoding anything important... Yeah, but I wonder if most real world projects in a given language would tend to have a fairly consistent fragility score.
Ha, forgot that law school was listed on my profile. I did go to law school, but early on discovered that I didn't have any desire to practice law. I did finish though (and even passed the bar), but I never want to any of my classes. I spent all my time learning programming and when I graduated, I decided to pursue software development as a career. 
Others will comment on advantages of libraries like Persistent. I'll give you some of the disadvantages: 1. Db support. Persistent for example does not support MS Sql Server or Oracle. Opaleye is Postgres only. 2. You cannot reuse sql statements, you'll have to rewrite them all. 3. You also cannot copy paste and test those sql statements in any external tool. You'll have to resort to generating sql statements if you want to run them like that. 4. You will still have to write a lot of required boilerplate and repeat yourself. Though the libraries like HaskellDB allow you to generate boilerplate, i do not think this is the case with Persistent. And finally i found that simply writing raw sql and using text templating along with some to-from sql functions to cover sql injection, works best for me. This approach is simple. It does not force you learning yet another quite complex layer over the sql with its own syntax. You can reuse your sql statements, or test them easily. Boilerplate is an issue, you'll have to write it, but interestingly it is comparable to the amount of boilerplate you have to write anyway. Unfortunately i find [this article](https://blog.codinghorror.com/object-relational-mapping-is-the-vietnam-of-computer-science/) all too true, even in haskell land.
Then the followup article: http://www.yesodweb.com/blog/2014/05/wai-3-0-alpha
OK now I think I understand you. It is like `OverlappingInstances` where it isn't "hard" to allow you to do the things you want to do, it's just arguably too dangerous. So something like `foo mempty` would go from perfectly valid to not compiling if a second `foo` function / record accessor was imported. So you either have to make `foo mempty` never valid even when there is only one `foo` (non-injective `TypeFamilies` approach), or limit what the extension can do as far as type inference (your approach), or go the `OverlappingInstances` approach and decrease the amount of general safety you have.
10% slower for some of my client is 1 million euro / year...
I don't really care about newcomers. We need tool to be efficient at doing our work. If bringing more newcomers is a way to improve our tool, ok, but the newcomers are not the target. Please also think about the idea that some mutable algorithm may be more efficient, it may be a fact on current hardware architecture. If this is true, then no compiler optimization for immutable algorithms can beat mutable ones.
There are multiple startups that are essentially web interface wrappers for [pandoc](http://pandoc.org/). As for why? Stuff that becomes popular is a numbers game. More Haskellers, more projects, more rolls of the dice and it'll happen.
[Brisk Forms](http://briskforms.com/) is another free alternative.
The problem here is not that people don't realize that Haskell has or does not have potential. The problem is the big entrance barrier that Haskell has: Pure functional programming. Most of the developers nowadays program in an OO way, some of them even in an imperative style. FP was there for a long time, but it wasn't necessary during that long time, until now (well, not now, but some years ago). We have approached the era of the Big Data, and there is where FP shines IMO, in **data processing**. Languages like Haskell allow us to program in a Data First approach. Allowing efficient data processing, and using design patterns that could've never be used before. Also, with lots of data comes the problem of scalability. Another place where pure FP shines. If you don't have state, the concurrency complexity reduces to a minimum. We as Haskell developers don't need to prove that Haskell is really capable. It's being used in production by many big companies (Microsoft, Facebook, AT&amp;T...). It is indeed another really mature language. We need to show those impactful examples that you talk about, but with things where Haskell really shines. Things like: &gt; Look, instead of doing those *40 LoC* that you did, you can solve it with a `foldl`. Or &gt; Instead of having that complex DSL interpreter in your project for your config, you can do the same with Algebraic Data Types Also, sometimes you will have someone saying: &gt; But I'm on `insert JVM, CLR, JS, or whatever here` I need lots of libraries from that platform The most logical answer is: "There must be something like that, or you can build it yourself!", but that is not pragmatic, someone cannot be doing those *fun things* on production. The best of it is to point them to the most pure functional language in that platform that has good interop: - **JVM** : Frege - **JS**: Elm or PureScript - **CLR**: F# This will motivate them to learn functional patterns and start thinking in a functional way. Even better, they might start writing small pieces of their projects in production, and they'll say &gt; [Wow, 6 months in production and not a `RuntimeException`yet](https://forwardcourses.com/lectures/92) That is really powerful, **plant a functional seed in their hearts, but let them water and grow the majestic tree** One day they'll try Haskell, the day that they are gonna be ready for it, and the day they'll enjoy it the most.
Too bad almost nobody use it. Git won, because of the exact same reason the OP is talking about - there was a big and important project using it.
Laziness isn't the answer for bidirectional relationships because it makes it very hard to update and reason about the bidirectional structure. There is 'knot tying', as in your example, but it's hard (impossible?) to update knots without recreating the entire structure from scratch.
I agree with most of what you said, but I don't agree that a showcase project is useless. I still think that showing in practice that this project can be done in a better way in Haskell rather than language X, will motivate many. This is a natural question to ask that if Haskell is so wonderful for data processing, etc., why isn't there a popular open source project that does that?
I don't think that's Kolmogorov complexity. There was a lot of talk about Kolmogorov complexity on /r/programming a few years back that was based on a really misleading definition of it, which might have muddied the waters. The difference comes down to where the quantifiers are, and means that with Kolmogorov complexity you don't get to specify or inspect the encoding. One result of this is that there is no computable function to determine the Kolmogorov complexity of something. It's actually quite a useful tool when you put the various quantifiers to work - there's a really nice proof of the density of the primes that uses it, for instance - but I always cringe a bit at the misuse of terminology whenever it comes up as some kind of programming language metric.
In this case, you shouldn't spend that much money on Haskell programmers. Hire some C programmers and use the money on machines instead.
Nice! I too really miss SML modules. The other obvious SML comparison these days is with Rust
What do you mean? You mean a Spark clone in Haskell?
Yeah, since you mentioned data processing as a strength. It would be reinvention, but maybe that's okay?
I agree there's unrealized potential. I'm preparing something that will hopefully come out in the next year that should have some decent visibility to it. There is also stuff that should be bigger news than it is. Like why isn't there more press and hype around Haskell in VR? http://store.steampowered.com/app/458200/ As far as I can tell that's the first thing in it's class and breaking some new ground.
Continuation passing style
&gt; Additionally, SML doesn’t have significant indentation which means that occasionally awkward parenthesis is necessary. The lack of the significant indentation isn't the reason for this. Braindead grammar design is. It's one of the few things about SML where I think "Well, they didn't think this through".
Slightly unrelated, but would you like to weigh-in on http://programmers.stackexchange.com/questions/326453/examples-of-mid-large-scale-web-apps-built-without-an-orm
Non ORM approach is simply ad hoc queries. There's nothing wrong with it. I've successfully built quite large systems with thousands and thousands ad hoc queries and without any orm libraries. It has its problems. But they are all workable. There's also another, absolutely alien and crazy approach: CQRS + Event Sourcing. But it is so different from classical mainstream sql rdbms that i do not think you can accept it as a real alternative. Anyway, you can read about it and see if you like it (let alone can afford it) 
Thanks! Just to clarify your second paragraph, you're referring to the dependencies that the custom setup script is built with?
I think if you have an SQL database set up, you probably don't need a SAAS to provide a contact form. The idea is to not have to host a backend at all, though maybe integration with backends-as-a-service would be appropriate.
Its immutable, you only make sure, that certain kind of optimization will take place. 
You should read ElvishJerricco answer. 
"Killer app theory" is so 1990s.
This is correct. To make it slightly more concrete, here's a (really dumb) program which returns a file. (Exercise for the reader: do this the smart way with `responseFile` instead.) #!/usr/bin/env stack {- stack --resolver lts-6.10 runghc --package warp --package wai-conduit --package conduit-extra -- -Wall -Werror -} import Data.ByteString.Builder (byteString) import Data.Conduit (Flush (Chunk), ($=)) import Data.Conduit.Binary (sourceHandle) import qualified Data.Conduit.List as CL import Network.HTTP.Types (status200) import Network.Wai.Conduit (responseSource) import Network.Wai.Handler.Warp (run) import System.IO (IOMode (ReadMode), withBinaryFile) main :: IO () main = run 3000 $ \_req send -&gt; withBinaryFile "Main.hs" ReadMode $ \h -&gt; send $ responseSource status200 [] $ sourceHandle h $= CL.map (Chunk . byteString)
Why? Haskell is not only still being maintained, it's actively advancing. The more people we have using Haskell the harder it's going to be to fix things that really, really need to be fixed (e.g. Num). You talk of killer apps, but I think about opportunity. Is your goal to work at some megacorp and write Haskell for them all day? Let me warn you up front that corporations could make even secks boring and tedious. Personally, I'd rather leverage my secret weapon to be able to develop software that would take a whole team to do in e.g. Java and get into the marketplace with it. In the past you were pretty limited in what software you could use to make products but those barriers are coming down in every field. Undiscovered potential is exactly that: potential. Once it's discovered it's not potential anymore and most every opportunity will have already been exploited by someone else.
That's why our critical paths are written in C++... My point was that nothing is black or white and I wanted to mitigate the point of the OP.
&gt; Non ORM approach is simply ad hoc queries. You mean strings that are fed to a database driver? Are you saying your code base with thousands of naked strings that will be interpreted as SQL at some point? 
If I understand it correctly, {hmatrix, repa, accelerate} are not nearly as fast as python's numpy nor are many of the more esoteric computer vision, machine learning, etc. algorithms implemented in Haskell as one might find in scikit.
But numpy, and the python scientific library are just calling the fortran functions (BLAS). If the pure Haskell code can't be made comparable speed we can always just do the same trick (and I believe I've seen several Haskell BLAS bindings already).
I agree we it would be nice to have an impactful app in Haskell. But I somehow believe we will get there eventually. I'm positive about it, for instance: 1. Maybe it is Servant? 2. Maybe it is Servant (1) + some yet-to-be-developed GHCJS-thingy that provides typesafety over the API barrier. 3. Maybe it is (2) + some totally awesome FRP thing 4. Or (2) + something that does typesafe CSS 5. Or (2) + server-side prerendering
I think it's closed source. McNettle stands for "Multicore Nettle" and only older "single core" version is available on Hackage http://hackage.haskell.org/package/nettle-openflow http://hackage.haskell.org/package/nettle-frp
I mean, if it was to promote haskell, it would have done it already.
Regarding the first one, I think that is what par does. Check this: https://downloads.haskell.org/~ghc/7.0.3/docs/html/users_guide/lang-parallel.html 
Hi, any news on the open source release?
which is why your book could be one of the most important contributions to the haskell ecosystem. it's also why battles to improve the new user experience need to be taken on. Influx of new users is important for everyone who has a stake in the technology.
But again, I'm suspicious of what you're calling "ease-of-use". Is it safe or will I need loads of tests and defensive code to detect situations that a type system would be able to simply prohibit? There was some Haskell project that tried to duplicate numpy's Timeseries behavior by just accepting anything you put in and trying to divine what you intend, etc. but I would personally never use such a library.
As a side note, I expected this function to be as fast as if I used mutable array, but it wasn't. Does somebody know why? myMap fn ar = inner min fn ar where (min, max) = bounds ar inner i fn !ar | i &lt;= max = inner (i+2) fn (ar // [(i, fn (ar ! i))]) -- apply fn each even index | True = ar
Parametrised queries like those in e.g. HDBC don't suffer from SQL injection vulnerabilities, but the queries are still just strings. In postgresql-simple queries are newtyped to make concatenation difficult, intended to prevent another class of vulnerabilities, but also preventing reuse, it seems to me. So there are lots of points in the desing space here, it's not just Opaleye on one side and vulnerable string concatenation on the other side.
&gt; Personally, I would love to see a Haskell-based CAD system, and I think it would also be loads of fun to try to re-create something like a modern version of Pov-RAY. Other people here could probably name other things they would like to see. You may be interested in [ImplicitCAD](http://www.implicitcad.org/).
&gt; when people have a tiling manager, it's XMonad more often than not. Really? I have only seen one XMonad in the wild (plus me, so two) so far, but plenty of dwm, i3 or awesome. I do think we need an equivalent on Wayland, though. The playing field for tiling Wayland compositors is still very young, so there are plenty of opportunities.
Sorry, didn't mean to be rude even if my comment might appear as rude. Just wanted to clarify that we're really calling JVM code there, to make Spark available in Haskell land, as opposed to something that "works like Spark". I've been thinking about what a "haskelly" Spark would look like, and what it could do better/worse, since my work on sparkle. There are a few points in the design space there that I'm eager to explore whenever I get a chance to.
Cloud Haskell is quite low-level; it provides a totally different degree of abstraction from what Spark provides. I really wish we had a compelling open-source replacement for Spark; Cloud Haskell would be a great substrate on which to build such a thing.
&gt; There are a few points in the design space there that I'm eager to explore whenever I get a chance to. Indeed this would be fun!
--no-load
Exactly. There are lots of trade-offs here. Other points in the design space may also be interesting. :-)
I would post this as a new question, it'll get more attention and response.
Unless you have a reason to use Array I also find [Vectors to have a better Interface](https://hackage.haskell.org/package/vector): Edit: Missread something.
FYI -- seems like I just missed this feature in the docs...
Of course but my point is that if you're selling something, a tool that collects the form content and makes the collected data reusable, that's more helpful than a basic email message with the form contents
Run `stack ghci --no-load`
Who is "we"? _I_ certainly don't need an impactful haskell application.
Good, but it doesn't solve the acutal problem which is that emacs does not send that. Is it possible to add that to some project local .stack config file or similar? EDIT stack.yaml has ghc-options, but no ghci-options: https://github.com/commercialhaskell/stack/blob/master/doc/yaml_configuration.md#ghc-options
For sure, luckily one advantage the "overlapping instance" approach has over true overlapping instances is that a new import can only ever change the code from compiling to not compiling. Which is much less bad than changing the meaning of the code potentially without you noticing. So I would personally still be ok with using the slightly more dangerous approach. But not everyone is going to agree with me on that understandably. I kind of think it would be useful to have extensions categorized into "safety groupings", although that might be too subjective: e.g "IncoherentInstances" and "ImpredicativeTypes" are both very dangerous, but not quite for the same reasons, incoherence and weird behavior vs. just being broken. Also perhaps for extensions that sort of rely on a not totally open world to be safe, maybe some sort of warning when a new import introduced a potential behavior change, compared to the last successful compile. 
Hmm that's strange. I had very few issues with it to be honest. I user it to this day when I'm on my Windows 10 laptop
`hmatrix` is just a wrapper around BLAS/LAPACK so it should be as fast as NumPy (which is also a wrapper around BLAS/LAPACK)
That looks pretty nice, I haven't seen it mentioned before.
&gt; But shouldn't category theory make formal specification easier and more concise? How so? The only Haskell-related use of category theory I'm aware of is as an inspiration for various type classes, but since those are expressed within the language, they would not be part of a language specification.
&gt; Also tera means 1e10^12 While I actually agree with you, that hasn't affected *de jure* use of 1TB of RAM to mean 2^40 bytes of RAM. (Using proper SI units this would be 1 Tio = 1 Tebioctet of RAM.)
I dunno, but you could try the Anaconda Python distribution -- it comes with a metric shit ton of libraries and might "just work"
It took a bit of searching but, I think I found the answer on [StackOverflow](http://stackoverflow.com/a/1990580/2008899). I also forgot a caveat. The paper assumes strict evaluation. Lazy evaluation has referentially transparent mutation (where a thunk/closure is replaced with its value), so it's unclear if this logarithmic slowdown is forced in a lazy language like Haskell.
I'm sorry, but this is awful design. I wouldn't personally want to work with someone who felt this was an acceptable way to engineer software. For such a project, do you at least keep the ad hoc queries in one layer or will a maintainer be faced with random SQL showing up pretty much anywhere at random? If you change your schema, how are you detecting what all has to be updated? How do you test that these naked strings are right, since you have no possible way of type checking them? And no, that is not "what's happening" with ORMs. An ORM is creating a type safe DSL, it's pre-preparing common queries, it's introducing caching, etc., to improve performance and many other things. It may even use some non-string protocol for database communication, if available.
No haskell library has the features you described. No caching, no pre-preparing common queries. And almost no haskell db library uses non-string communication protocol with db. There's one i think for postgres and it is not the most popular. People mostly use postgres-simple which uses strings. So let's not pretend that features available in other orm tools apply to those available in haskell. 
&gt;I use Sql Server, and none of them supports it. Well that's a fair point, but not all Haskell libraries are tied to one implementation. Groundhog, at least, appears to support different drivers. All that would be needed is a generic ODBC-type driver to get the same level of support you're probably getting now but with type safety. If you're so deeply invested as to have thousands of ad hoc queries, maybe working the the maintainer of Groundhog to develop this functionality would be a possibility?
Maybe you could start an initiative. List what features you feel need to exist (just speaking of linear algebra and coherent numerical ecosystem) and people who have time can begin to address them.
&gt;You have to account for historical decisions and cost of rewrite. Sure, I agree about rewrites but you rarely actually have to rewrite. You can introduce e.g. Groundhog for new code and migrate old code as you refactor. I don't know what your development processes look like but everywhere I've worked, the software was in a *constant* state of modification and refactoring. There was just a general rule that any new code and any old code being refactored must always follow current best practices. Since best practices are updated regularly, the whole code base was never fully compliant but eventually the oldest practices would get phased out.
Finally, a good analogy and a byline! An answer by [@pigworker](https://twitter.com/pigworker/status/760444487944601600).
There's also a Slack channel which has been great: #haskell on fpchat http://fpchat.com/
What's a better way to do nested cases if you aren't using significant whitespace? OCaml has begin-end keywords which I think are marginally better but ultimately not very different from awkward parens.
Whether or not it is an "awful design" is irrelevant in cases where the SQL I need to express is not possible within the chosen library. I am fully aware of the risks that come with modifying the schema. I am also aware of unit testing libraries that allow you to test functions within your application. At the end of the day, both methods send a query in the form of a raw string to the database, you're just choosing to hide it behind a layer of abstraction. None of that other stuff you mentioned is unique to ORMs or any other library that generates queries for you. I'm not aware of any "non-string protocol for database communication" that involves querying the database, so I can't comment on that.
&gt; 1) The barrier to entry is quite high It sounds like you have the skills to build a github "resume" that should be able to get you a job without too much trouble. That obviously takes time, but this is where my points about collaborating with other people can make up for it. If you go to some Haskell gatherings and get to know people they should be able to assess your knowledge pretty quickly. That's a really good way to find opportunities. I have had multiple job opportunities in the past from people I met at Haskell events. &gt; 2) There is no Haskell shop in my area and I can't / don't want to relocate. This point impacts both sides of the fence. Companies looking to hire Haskell programmers have a lot more options if they open themselves up to remote workers. Programmers looking for Haskell jobs have a lot more options if they open themselves up to moving. Going back to my point about collaboration, you're going to be in a MUCH better situation if you move to a place where there is a lot of Haskell activity (New York, London, Singapore, etc) and people that you can meet and collaborate with in person. Obviously that's a very personal tradeoff that only you can make. But there are definitely remote Haskell opportunities out there.
bounds :: Array i e -&gt; (i, i) -- you cannot map over tuple
My bad, I meant for there to be a range call in there. I'll fix it.
Thanks, that is a useful tip.
&gt; the SQL I need to express is not possible within the chosen library. Then use a stored proc. &gt; I am also aware of unit testing libraries that allow you to test functions within your application. It's important to understand that, while you may be using a so-called "unit test", the kind of test you're describing is *not* a unit test. It's an integration test and there are a lot of caveats involved in integration testing functions that call out to a database. &gt;At the end of the day, both methods send a query in the form of a raw string to the database, you're just choosing to hide it behind a layer of abstraction. This is an oversimplification that misses the point of ORM's, etc., entirely. It's exactly that "layer of abstraction" that is required to engineer a proper solution. It adds typing, it opens up the possibility of mocking and unit testing, etc., etc.
Did someone downvote my comment? Wonder why.
&gt; In the current state they are, available haskell ORM libraries are not accessible to me for one very simple reason: I use Sql Server, and none of them supports it. I'm not sure what you are saying. Are you saying that if Opaleye/esqueleto/whatever supported Sql Server then you would use them? Or do you have other reasons for avoiding them at the moment? It should be a couple of days' work for a junior Haskeller to convert Opaleye to Sql Server. 
No clue, but I recommend not complaining about it. These things normally rectify themselves in short order.
My code is updating only each even index. You map over **all** elements.
No, just that it is one most obvious obstacle. But the real reason is of course that the projects were started many years ago when none of those libs were available. And no one is going to pay to go back and do a major rewrite of stable and complex systems. That would entail so much testing i shudder to even think about it. &gt;It should be a couple of days' work for a junior Haskeller to convert Opaleye to Sql Server. Awesome! Eagerly awaiting for that junior Haskeller to materialize and spend his unpaid personal time for my gain :) 
Essentially yes. This would be more like solving the 100k problem though, on most hardware.
Yes, you don't have to use epoll directly as you would normally do in C/C++. Just spawn with forkIO one greenthread for "reading" on the socket and another one for "writing" on it. The haskell RTS will handle the rest for you and it will be performant. So relax and enjoy :) P.s: One major pain will be how you choose to comunicate with those threads. Think carefully before rushing to a solution 
for a summary, please take a look at the issue I filed on Esqueleto 's github repo -- https://github.com/prowdsponsor/esqueleto/issues/145#issuecomment-237294781
I'm exceptionally skeptical about the value of paid courses that don't result in attaining an industry relevant certificate of some kind. There are so many high quality free resources out there already. There's also a disadvantage with these "all-in-one" hand-holdy courses in that they remove a lot of the trial and error of learning the caveats and intricacies of the ecosystem, and removes the massive advantage of *needing* to get plugged into the Haskell community to move ahead when "stuck." What does your product offer that can't be replicated by free resources? Why should an aspiring Haskeller drop $50 on a hand-holdy course instead of getting connected with the wider Haskell community through IRC, mailing-lists, reddit, SO, etc etc and self-teach themselves what Haskell and the ecosystem are all about?
I'm not sure how much being an Haskell expert by itself helps with finding a job. I was lucky enough, I didn't spend too much time looking for jobs.. But I feel like you also need some good domain knowledge to be employable. As an example, I check job threads in HN etc. regularly and I never see "expert / N years of experience in Haskell" without some other buzz words also thrown in, like "big data", "databases", "learning", "micro services" etc. Personally I have zero experience in these fields, and so I think I have no chance at all getting hired by these companies. Another thing that I don't really understand is, I feel like most people would be OK with writing _anything_ in Haskell, which is exactly the opposite of how I feel about jobs. For example, in the past I did web-related programming in Haskell, and compiler programming in Java. These languages are at the opposite ends of "fun programming languages" spectrum, yet I enjoyed the compiler work much more than web programming in Haskell. Since that experience I focus on the domain first. Bonus points if it's also a Haskell job. 
Is Philippa is getting back into haskell? That'd be good news. 
It now has automatic record conversion! Checkout the new example.
What's wrong with 1990s? Forrest Gump, The Matrix, Titanic, Pulp Fiction, ..., Doom, Quake. That was the best decade ever :-)
I relate to your last paragraph. Part of the reason I don't want to relocate is that moving abroad to work for a financial institution does not appeal to me. The opportunity of working with an innovative startup might make me reconsider.
Cool! Good job. In the example, try formatting the record in a more conventional style.
or even better as in python (a,)
I guess the need for two separate threads stems from the way you communicate - if the thread is doing just some reading and writing to a different network node, you don't need two separate green threads. Or is there some reason for sticking to one file descriptor per green thread? (Like some sort of a design pattern that Haskell's RTS recognises).
The guys that created HList didn't consider documentation useful enough to write one :( 
Conflicts with `-XTupleSections`. `(a,)` is `\x -&gt; (a,x)`.
Wow, my bad. I was searching hackage/hlist but was nowhere to find. Didn't realize I need to google "haskell hlist pdf"...
You would need to do some further modification to type system. 
Correct: and in this case the type is `*`, the type of types. So the `[Int,Bool,String]` in `HCons 42 (HCons True (HCons "foo" HNil)) :: HList '[Int,Bool,String]` is indeed a list whose values all have the same type, even though the `HCons 42 (HCons True (HCons "foo" HNil))` is a list-like structure which contains values of different types.
What are you talking about? I see a lot of documentation on the [hackage page](https://hackage.haskell.org/package/HList-0.4.1.0/docs/Data-HList-HList.html). Perhaps you have only looked at the [latest version](https://hackage.haskell.org/package/HList-0.4.2.0) of that hackage page? It's a bit annoying, but many packages are like that, the documentation wasn't automatically-generated for a while (is it back on yet?) so you often have to look at an earlier version in order to read it.
Although not in Haskell I've recently watched a nice video about free monads, which contains a straightforward explanation of the concept. [Here it is](https://www.youtube.com/watch?v=U0lK0hnbc4U)
they are both monoids in the endofunctor category, with different monoidal structures. that define n-ary functions by iterating their multiplication. one being FX * FY -&gt; F (X * Y) the other is (M . M) X -&gt; M X (imprecise as tensors are really defined in the functor category - and make both categories monoids in Cat) not sure bits : Since applicative value come from left kan extension, I think "there exist" a (cat / external) product they are *internal* product of
Having ported Ruby to IBM's Blue Gene/L, I think you should be fine by just using Haskell MPI bindings.
not sure what you mean. both have to be endofunctor on monoidal categories, as that's how you can define them 
I think Haskell has crossed the point where the killer app is necessary. Just 5 years ago the prevailing mindset was that an "easier" version of Haskell would capture mind-share while Haskell itself remained an obscure language used for academic papers. The very opposite has happened: Haskell has gotten way more complicated *and* people are adopting it for business. I have trouble reconciling this but I'm not complaining. I see about 3 Haskell job postings a week which is about the same as Clojure (although much less than Scala). I think that Haskell is at the brink of mass adoption has been for the last year or so. With the current focus on developer tooling (Intero, haskell-ide-engine) and OverloadedRecordFields (hopefully) I predict huge changes. Once the latter lands, for better or worse, I think the community can expect a tsunami of new users.
To my understanding, you can have a monad on *any* category, but you can only have an applicative on *monoidal* categories. Is this incorrect? Or am I misunderstanding what is meant by "monoidal category"?
Ok, how is it different from `(Int, Maybe Int, Maybe Int) -&gt; [Int]`?
Verbosity is a problem with readability. At this point I'd like to point out that `enumFrom 1` (idiomatic Haskell) is way more readable than some arcane `range (1,)` (your proposal).
Data.Text is hardly some random small package. It's a core package to the ecosystem at this point, as the de facto way to represent text in any nontrivial system.
&gt; Or am i just lazy and should figure out how to write these functions myself? Never write a function yourself if you can help it!
Just curious, how would you wait for events on multiple fds in a single thread?
But shouldn't Data.Text then be included in the default installation of Haskell. Do I really need to download extra packages just to do standard operations on text? For reference, i did as the book recommended, and installed the [Haskell Platform](https://www.haskell.org/platform/) which is advertised as "Haskell with batteries". It seems awfully inconvenient that i should have to download even more "batteries" for Haskell just to be able to work with text.
`words` will split a String on whitespace. Also available for ByteString and Text. For the reverse operation have a look at `unwords`. There is also `lines` and `unlines` for breaking on newlines. 
It is an instrument that can only "hit" on odd beats of length 3+, so I turned it into a 4 state state machine, just to model it. Then modeled n players simply as tracking the number of instruments in each of the states. A vector of length 4 where the numbers sum up to the number of players, and we now have a non-deterministic transition function, where assuming there is a 'beat' in the current timestep one or more players have to transition from idle or the 'can hit' state. There are only C(n+3,3) possible states you could be in at any beat of the music, and this collapses out the symmetries between all the player. (Number of ways to place n unlabeled players in 4 labeled bins). He was effectively modeling each player independently, but didn't have the ability to collapse states back. The idealized form of his previous computation would be to model n labeled players in 4 labeled bins. But if you're just concerned with existential 'can this many players play this rhythm' then there is no need to assign labels to players. It was rather fun to calculate it out and to generate some basic graphviz visualizations of the symmetries.
Both `split` and `text` [are a part of Haskell Platform](https://www.haskell.org/platform/contents.html). Perhaps you chose minmal install?
We have a Haskell Platform, which includes the `split` package by default. Some folks really don't like it, so now you may or may not get it, depending on how you install Haskell.
left-pad was a management issue. I would still use left-pad, but I would never rely on npm
This is and was the whole point of the Haskell Platform, which contains `text` and `split`. The batteries you asked for are in the box. You may have just grabbed the wrong box.
making a huge impact is incredibly hard. you need a lot of luck, a lot of dedication and a lot of focus on visibility and usability. Most of the projects I mentioned are great but require haskell knowledge to use so will definitely not have the same impact as anything with an interface for a mainstream language. But these projects have great potential for haskellers, maybe shake is the best build system out there? maybe servant can let you build rest API with the least effort? maybe gpipe2 and luminance give you the best experience writing graphics code? maybe project-m36 is the most reasonable relational engine out there? maybe turtle will give you the best experience for maintaining shell scripts? maybe reflex-dom is the most expressive way to build GUIs today? I don't know the answer to these questions, but I'm sure these projects are much better than their marketing suggests. If we make these projects better or more visible, I'm sure great applications using them will start to pop up.
You normally wouldn't do that — you'd just `forkIO` a thread for doing your blocking operations and if you wanted to actually handle events in a single place then you'd push the events into a channel or something similar. However, there *are* bindings to kqueue, poll, epoll, and even libev if you really want to handle the IO multiplexing manually. Seems like some of those are fairly old so I'm not sure how well maintained they are.
So you need a free account to get a free account of free monads?
If I may say so, I would prefer using generics to make the record conversion automatic. But sweet!
I think there are pockets of users around the place: many from academic backgrounds or in firms with strong academic ties. I have to say I would never have persisted with it had I not worked at a company where it was more-or-less the desktop of choice (this was ~2 years ago and before I got into Haskell). It is not dissimilar to Haskell itself, both in that the learning curve (and initial productivity burn) was way higher than I had experienced before, and in that it was worth every bit of it :)
There's nobody currently working on any real idiom bracket implementation in GHC. (I can't even find a direct ticket or feature request for adding them, with a quick search.) There's also the tidbit that `(|` and `|)` are already stolen by Arrow syntax. So, that detail needs to be sorted out.
I write commercial web applications and our web applications will typically depend on hundreds of packages from hackage. Almost any Haskell package of significant size is going to depend on a whole lot of packages from hackage. There is a brief period when you first learn Haskell where it can seem annoying that more things are not bundled with the standard GHC install. But, ultimately you are going to have to start depending on other libraries anyway. Whether your use nix, cabal-install, or stack, I think it is testament to the wild success of hackage that we are pretty ok with not bundling much with the minimal install because depending on packages from hackage is relatively painless. Of course it took a while to get to this point. But it is a far cry from when I first started using Haskell and you'd try to build some package but it would fail because it imported some library, but didn't tell you what the library was. So you would have to search for the module name on google and hope to find the source code on some homepage. And, since we didn't have hierarchical modules, you had to hope that you got the right 'Parser.hs' and not some other library. And, then after you'd downloaded and extracted the .zip or .tar.gz file, you'd find that the library did not install because the Makefile included hardcoded paths to the author's home directory. And after you fixed that you'd find out this library depended on yet another library you didn't know about. And then you'd download that one and it would require cmake or some other build tool you didn't have installed. Things like 'The Haskell Platform' have attempted to create a packaging of GHC with commonly used libraries. The fact that not everyone uses the Haskell Platform is perhaps another indication of how successful hackage is. Things like stackage have organized developers better so that commonly used packages are more likely to all build against the latest dependencies. Even if you don't use stackage itself, you benefit from this because the fixed packages are first uploaded to hackage. Recent and upcoming improvements to cabal-install have made it quite reliable for installing packages at random from hackage. Things like nix or stack have made it easier to develop using fixed, controlled sets of dependencies. The fact that it is relatively seamless and easy to mix together a hundred different libraries in your Haskell apps is a testament to functional programming I think. 
Yeah, but it would probably be less surprising if it preserved `1.`, `2.`, etc. but converted `n.` to the right number for the position in the list.
-1) People can start numbering from whatever number they like. 0) You're welcome to your own personal opinion. π) This subthread was a waste of everyone's time.
I'm currently doing Haskell professionally. For me, I acquired the job by getting a beer with one of the other Haskellers once and being good at frontend work as well.
Excellent advise. I consider 4 to be the best way to do it, and starting your own project works too - different people prefer either to help out or start afresh. If following 5, be very careful - if the job isn't Haskell, it had better be very Haskell adjacent. If you love Haskell, getting a Java job at the same company doesn't give you much leverage to move across, unless the company is small. 
I didn't think I'd want to work at a financial institution, but I've now been doing it 8 years and soon to be 3 banks. It is pretty special to have a large pool of users who care a lot, which seems to be what finance has above all else. 
SHE had something like this, but I have no idea if it's been kept up to date with GHC. https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/ https://hackage.haskell.org/package/she
I wrote [a proposal for an alternative](https://www.reddit.com/r/haskell/comments/4dsb2b/thoughts_on_an_inlinedobind_extension/) a few months ago, but to my knowledge, neither I nor anyone else has actually worked on it.
Not quite, the category of endofunctors is itself a monoidal category. A monad is a monoid object in this category, not the original category (which needn't be monoidal). Horizontal composition of endofunctors is the monoidal product. However, for certain kinds of categories, there is another monoidal product you can define on endofunctors, Day convolution, and applicatives are monoid objects in that monoidal category.
&gt; interacting with experienced Haskell programmers is by far the most important thing to do Absolutely agree. &lt;complaining&gt; At the same time, I find that there's really a problem with hiring Haskell devs. Companies just want experienced people and nobody, it seems, is willing to offer internships or apprenticeships to people who want to learn. People that write Haskell seem to be either really smart (PhDs) or really lucky (drink beers with PhDs). This makes the gap between beginners and pros the largest I've seen in any community. Knowledge transfer is very limited. &lt;/complaining&gt; At the same time, I believe that it's all up to oneself. If you read books and write code, sooner or later you'll get there.
A monad on Hask, is a monoid object _in_ [Hask, Hask] viewed as a monoidal category with the choice of functor composition as the tensor. Functor composition always exists in [C,C] and makes ([C,C],Compose,Identity) a monoidal category. Compose is a bifunctor, given natural transformations (f -&gt; f') and (g -&gt; g') you can map over Compose f g to get Compose f' g', it is associative, you have a unit (Identity), Mac Lane's pentagon holds, etc. In addition to that choice of tensor for your monoidal category, in general any category [C,D] borrows almost all of the interesting structure of D. If you have products there [C,D] has products, etc. This gives you more structure to play with. Applicative is interesting because it comes from Day convolution and Day convolution borrows some monoidal structure from C, not D in [C,D]. The form that gives rise to applicative / alternative / divisible /decidable uses D being self-enriched and C being monoidal. This is a pretty limited scenario. [Hask, Hask] works, [Hask^op, Hask] similarly works, etc. This is not the most powerful form of Day convolution, however. We use a strange-to-a-category-theorist form of covariant Day convolution to make Applicative, but Day himself originally formulated them in promonoidal categories. The monoidal category stuff is just a simplified, comparatively easy case. We can phrase Applicative a bunch of other ways than 'strong lax monoidal endofunctor'. That formulation only gets you something you can convert into the form we have, and we can do that because we have both products and exponentials, and they are forced to have a pretty close relationship. A 'closed functor' is perhaps a better way to think of them as it captures the nature of (&lt;*&gt;) mapping exponentials to exponentials, and this way it doesn't require the existence of products.
If you believe that than fine, but please keep in mind that what you claim is solely belief and has not yet been shown to be factual. Know also that I once believed as you do but after many years of Haskell even getting to work on a few large projects, I have to be honest with myself here, I don't believe such any more. Yet, I still like Haskell. I also think that static analysis (strong types) are not confined to Haskell or FP and that static analysis is not confined to types. Which means that lessons from Haskell can and likely will (and has) make it into languages of all flavors and shapes. So again I claim that I feel no need to see Haskell conquer the world. 
Lets not forget data loss which were reported on several occasions aside from what is posted below: https://www.mercurial-scm.org/pipermail/mercurial-devel/2008-January/004493.html and The hidden conflicts lead to data loss. The problem is most acute with the darcs rollback command as rollback is designed to create the inverse patches. However, it is not limited to rollback. As explained above, the inverses can be created by accident or worse yet, maliciously. Also, the pretension that a "theory of patches" made the software more suited to carry out it responsibilities than competing software didn't help much either. As a community we have to be more honest with ourselves. The "compiles to code faster than C" days didn't do much to help the community's reputation. Honesty, inclusion, simplicity, would get Haskell a higher adoptions rate but there seems to be divided interests in the community. On the one hand there are the academics who use Haskell for their own purposes and there are those who want to see a core of Haskell move more toward the main stream. When is that last time a divided community achieved anything. To me it seems that Haskell should probably remain as is, that is, half experimental with some willing to use it commercially. The language seems to do well in that niche. 
Thanks for the reply. Looks like I've got a lot of reading to do to fully grok this comment =P
No. This was such a mistake. The number of bugs because of this is just absolutely miserable. Granted, the compiler should catch it most of the time in Haskell, so maybe it won't be so bad, but I'm still scarred.
Data loss is 100% the reason I switched to github from darcs in the first place. A site named patchtag was hosting darcs repositories, but used acid-state in the backend. Regardless, it ate something like 13 of my repositories completely, right as I switched machines, and I was left with no copy of my data except the most recent things I'd pushed to hackage, and had to recover a fake repository history from the tagged versions. In that case it had nothing to do with darcs. I think the idea of the theory of patches was more attractive than the particular implementation. As it is, I think we're doing a fair job of managing the move forward, and I agree that we seem to be in a pretty good niche. Some might like to see an instant overnight adoption of 'whatever is practical, lets get something out the door, ship, ship, ship' mentality, but a lot of the community seems content to exploit the benefits of the language to keep building better and better tooling, while using the last generation, because upgrading in a world where the types are so strong is so much more painless than it is in other languages. I've yet to meet someone who built a startup on Haskell that then abandoned the language because they couldn't serve their core business needs on it. I've seen places that abandoned Haskell out of fear they couldn't replace their developers if they left, but not any place that did so because they couldn't actually replace those developers, just because they held a nebulous fear. The fear Haskell engenders far overpowers any actual negative effect Haskell has on the enterprise. 
I'm always amused and impressed with how consistently diplomatic your comments around this issue are. It's like we're not even on the internet.
Yes. This is why we need a migration to something like text in base.
With `OverloadedLabels`: https://gist.github.com/PkmX/bfb2c5af4317c96282795f8c588fda1c type User = ( "login" := String, "id" := Integer ) user :: User user = ( #login := "themoritz", #id := 3522732 ) mentioned :: ( "url" := String, "title" := String, "user" := User ) mentioned = ( #url := "https://api.github.com/repos/commercialhaskell/intero/issues/64" , #title := "Support GHCJS" , #user := user ) #url mentioned -- "https://api.github.com/repos/commercialhaskell/intero/issues/64" #login (#user mentioned) -- "themoritz" (#id . #user) mentioned -- 3522732 Unfortunately, this also means that the label names will be subject to Haskell's identifier rules. You can write `$("lisp-case")` but not `#lisp-case`. Can we make `#"lisp-case"` desugar to `fromLabel @"lisp-case" @a proxy#`, PLEASE? ---- Also, &gt; Using type-level naturals, overloaded strings, and splices to make named tuples You mean type-level symbols and not overloaded strings?
Maybe this gets another shot with the ghc-proposal process?
split raises a bunch of questions about whether it should take multi-character delimiters, what do do with ones on the ends, etc. These are captured fairly nicely by the `split` package, in which Brent has given a great deal of thought to these concerns. Heck, even `unwords` and `unlines` differ with respect to behavior in non-trivial ways. Note that they aren't simply split / join with a different choice of delimeter!
(un)`lines` and (un)`words` aren't exactly the same operation with a slightly different delimeter. The (un)`lines` puts a newline at the end, (un)`words` doesn't drop in a trailing space. The `split` package captures enough of these differences into the notion of a Splitter to handle both scenarios. Since its more complex than folks thought, a library that captures that nuance was created and standardized. It is in the Haskell platform, but doesn't pass the current test to be considered mandatorily a 'core library': https://wiki.haskell.org/Library_submissions#The_Libraries That said, if Brent wanted to he could readily subscribe formally to the process in that document. Nothing would likely change in practice with this designation though, as the only change in the last 4 years was the addition of a single combinator.
As long as you don't misinterpret code in other languages, it's ok.
I find that it makes me lazy to write code in other languages.
I think it was always that hard. I just didn't realize it was hard until I discovered languages that don't *make* it hard.
&gt; &lt;https://haskell-lang.org/libraries&gt; Is it an oversight that the page features `attoparsec` but not `parsec`? Seems like those two should be in their own "parsing" category rather than having atto in the "core" category and suggesting that it's "regularly used in most Haskell programs".
I find this to be true to some degree and also false in others. True: Haskell types provide a lot of context. A type often tells me what a function can/cannot do, so I enter the reading phase with greater and more constrained context. This makes reading a function easier. Compare this to an impure for loop, which provides very little context other than iteration. False: Sometimes Haskell code can be fairly abstract and overly terse, often when long point free expressions are used. This can require keeping a fair amount of type information in your head, exactly what I hate about dynamic types. These can be difficult passages to read, when more top level functions, naming or an "imperative" style might be more illustrative. Well written Haskell is a joy to read.
Honestly, Haskell makes it hard for me to read code written in Haskell.
I agree with you and certainly don't expect anything out of the blue. But things you've mentioned work in almost any community, nothing unique to Haskell there. Yet the steep learning curve certainly requires being around the masters more than with other languages. And as much as IRC is great, for me that's not a substitution for face to face discussion. Meetups and hackathons are great, I wish there were more of them.
Ooo, I wasn't aware that OverloadedLabels was a thing. That's awesome. The use-cases for this... I'm so excited. Named-parameters for functions. No more "order-blindness". Handling external data in an R-like way without declaring types up-front. I don't mind being subject to identifier rules, although I suppose it is cool to be able to have static look-up into some data structure, like you might do for working with a type-driver on a CSV file in an impromptu way. So `$("foo")` serves nicely for that. &gt; You mean type-level symbols and not overloaded strings? Sorry, I meant type-level literals. It uses both strings at the type-level and for the $("foo") it needs the overloaded strings instance. 
&gt; demo = id @ "foo" • Expected a type, but ‘"foo"’ has kind ‘ghc-prim-0.5.0.0:GHC.Types.Symbol’ Naive attempt not working. What's the minimal way to get `@"foo"` to do something?
People can always do what ever they like, including being wrong as you are. The subthread was not a waste of anyone's time except those who actually read it and didn't get anything useful from from it, as you have.
It seems to me that observable sharing is isomorphic to let bindings. Does anyone know any reason that I'm wrong?
&gt; If you believe that than fine If I believe what, that cost is king? That's non-controversially true. That haskell will win eventually? That's obviously an opinion of mine that has not yet (and may never) come to fruition. &gt; I also think that static analysis (strong types) are not confined to Haskell or FP and that static analysis is not confined to types. Sure, Haskell has traditionally been a research ground so the result of it being a new language or big improvements to other languages is fine.
&gt;Yes, you did. "You stated in this sub-thread that nested records are an OO anti-pattern". Again, you need to learn what the word "lied" means. I interpreted your statement as I stated here. You later claimed that you did not say this. That means my interpretation of your statement was *wrong* but that's different than *a lie*. Maybe in your culture you don't make much difference between "I said something incorrect" and "I lied" but in my culture there is a very large difference: one is something everyone does from time to time, one is purposely dishonest.
He gave me a plastic fork. I was only repaying his kindness.
I was under the impression that you could always define the day convolution. Anyway the standard type classes in Haskell are in Hask, if I am not mistaken. Anything else need to be encoded I guess?
Nice overview thanks, I am sure many of us don't know those ideas
That thread seems to be a question about persistent. What kind of information are you looking for exactly?
Ahh, nice hack!
This is one of the reasons why ApplicativeDo is not really an extension I support. The other being it relies on a very inelegant blessing of particular functions (return and pure) to properly desugar into applicative style. I'd much rather -XIdiomBrackets than -XApplicativeDo.
Haskell but Quickly submitted by someone called Node or Bust. I'm so confused...
[If your target is web development, you should probably start by creating a website that fits within the browser!](http://i.imgur.com/xZ7eLJ3.png)
Disclaimer: I've never shipped a game in any language, every game I've made is a toy project thus far. Saying that, I personally use Haskell where a lot of people would use Python, and Rust where a lot of people would use C/C++. There's nothing like the excellent PyGame out there for Haskell (although maybe this would fit the bill, I'm planning to try it out for my next project) but for game programming I've yet to find something quite like Yampa for pure coding bliss, at the cost of a steep learning curve.
I think it's made it easier for me to read code in other languages. Haskell makes me think of state and types a lot more, even in other languages, so as long as I know how those behave, I can read them mostly fine. Still rather have Haskell, but I'd be waiting a lifetime for the industry to catch up.
I gave up and wrote my own wrapper around ghcid.
I don't like how socialising is used and suggested by many here as a way to get a job. I find that this attitude poisons human relationships. That applies also to using the IRC channel as a way to get a job. I find getting a job over a beer kind of insulting of the people who work hard and have better merits. I get that for local companies, politics are important and controlling who gets jobs is one of the best ways to keep or grow someone's personal power, but do we really want this also in online communities? Please not. Let's try to be disinterested there at least. I have been looking for an Haskell job for months if not years now but i will not accept a job offered over a beer, i will not go to a conference to look cool, i will not contribute to an open source project for the interest of being hired. Putting personal relationships over intellectual rigour, and acting for the sake of showing off instead that for sincere interest, this is bringing a lot of bad habits in tech companies and this is the reason why we are stuck in an industry full of bad software and cargo cults
Can you elaborate? EDIT: new to Haskell
What i like about the idea is that it relates to a very common experience for most programmers: a typo. So this metric can be calculated automatically, and at the same time it can be explained very simply: how much damage can a typo cause you?
I didn't find it offensive or derogatory, so it seems to be a rather subjective thing. I can certainly relate to the feeling that the Haskell community doesn't seem to be interested in *creating software with Haskell* as much as it likes to tinker with the language itself and explore its capabilities / inventing new abstractions / etc. I assume that “just another useless Fibbonacci exercise” was simply a manifestation of that feeling. Also, Fibonacci is darn easy to misspell so I can't blame the author for that :)
My point is about not pretending and not having a second interest in doing things. Let's say that two new Haskell projects, A and B, are created in order to resolve the same, or a very similar problem. Let's say that project A is better than project B under any metric which can lead to open source contributions and overall success, but project B is backed by an employer who explicitly has the policy of hiring among contributors. Under these conditions, project B could have more community traction and eventually lead to project A being abandoned, even though A was better structured and, in some sense, less tied to some private interests. A loss for the community. This is why i mention intellectual rigour. It is such a fragile thing, and so important! I think that avoiding "success at all cost" made a lot of the quality we get in the Haskell ecosystem today, and this applies also to the personal journeys of who is trying to make Haskell their profession
Also because git was better than darcs in many important ways. Darcs is better than git in less important ways, IMO. Git was faster. Git has reset and in-place branches. The snapshot model is also better than the patch model if your diffs are heuristic (and they are).
OK, but you don't have to pretend. 
I'm not sure it's been made clear that neither `text` nor `split` will actually help with this problem, since you need `String` to get the most natural `read` function. The core of the program is something like `sum . map read . lines :: String -&gt; Integer`. The rest of it is `take 10 . show :: String -&gt; String` So everything is in `Prelude`.
Not sure what you expected from someone self-named "reddit_is_gayest"
I don't know if you're already familiar with this paper but it's a terrific one. [Notions of Computation as Monoids](https://arxiv.org/pdf/1406.4823.pdf) - Exequiel Rivas, Mauro Jaskelioff Also another classic one for bridging Haskell with category theory is [Kan Extensions for Program Optimisation](http://s3.amazonaws.com/academia.edu.documents/30913151/Kan.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&amp;Expires=1470321945&amp;Signature=lqmWhYcgvX3u0IsbA9pIJRXgFeM%3D&amp;response-content-disposition=inline%3B%20filename%3DKan_extensions_for_program_optimisation.pdf) - Ralf Hinze
The sense I've been getting is that anyone who might implement it thinks that the effort is not worth the benefit, and that the benefits are questionable, since it leads to yet another syntax extension that tools must support.
&gt; A candidate that couldn't say how large a number 2^40 is (roughly) Hm. I don't even know what an answer to that question would look like (the correct answer seems to be: `2^40` but surely that's not what's sought). Is the idea to express it as a power of 10?
Hello newbie, here is a gift from /u/nikita-volkov to you: * https://hackage.haskell.org/package/rebase Related: * https://hackage.haskell.org/package/base-prelude (These don't actually include `split` but you'll still thank me. Or at least you'll thank Nikita.)
Or we can add more constraint kinds to make the name fit
I hope the author gets that feedback because I doubt he's the one actually posting this. OP posted this first to /r/programmingcirclejerk (who are currently having a party with this one) before xposting it here hours later.
Interesting, any particular reasons for leaving (and joining B)? (PM if you prefer).
[removed]
I don't think so. Many languages are designed to maximize readability at the cost of a lot of safety. If anything I find Haskell to be one of the most difficult languages to read, especially when lots of arcane infix operators pop up. I will say, though, that while I find Ruby or Python code very easy to read through, it also feels like Cowboy coding in the wild west. Virtually anything could go wrong, no matter how reasonable the code looks. This is something you become very accustomed to when you use those languages daily (like I still do). But the more you use Haskell, the less comfortable you become with that feeling. I feel like I have to almost go overboard writing tests because I can have so little confidence in the code itself. I find that I *really* miss type signatures as well. I love being able to see if a function is pure or has side effects just by glancing at its type (and being able to actually trust that the code matches the type signature). I really wish more programmers could experience this without having to go to the trouble of learning Haskell in depth. I think if more people were aware of how nice it is, the development of programming languages over the next 10 years would be dramatically affected.
&gt;If stored procedures are an option, why do you need a library to generate SQL for you? It depends on the application, but generally speaking: you don't. It's just that if you use an ORM (or FP equivalent) then you get some checks between your serialized model and the database model. Of course, that depends on how your project is structured. The best large project I've ever worked on had the DDL, etc. stored in the revision control system in such a way that integration testing was accomplished by applying all DDL, SQL data loading, etc. to an empty database to generate the entire database from scratch for the integration environment. The NHiberate schemas were verified from these DDL as well, so any mismatch was detected during the build phase. If you can't do something like this, then yes, you might be getting more bang for the buck by just going stored procs everywhere you can because mismatches will have to be detected at runtime in any case.
Reading Conor McBride's code is what made me want the feature.
I'm not a big fan of the overlapping instances at the base of the whole thing, and type applications make you care about the order of types in a forall in ways you never had to care before.
Yea I don't think O(1) cons is the important part of the point I'm trying to get at. The important part is being able to reason about tuples with type level lists without losing much of the efficiency of tuples.
&gt; If you need to be an attendee to ICFP to participate or anyone around the world can join in the competition? You don't need to be an attendee. It's possible to do it in teams; I personally think it's more fun to do in a small team than alone. Also, there is one every year, and not every contest will interest you. It's perfectly fine if you decide at the beginning that this year's topic doesn't feel so fun to you, and to do something else with your time.
was this supposed to be sarcastic, or did I just get scammed into signing up for some nonsense. 
&gt; But things you've mentioned work in almost any community, nothing unique to Haskell there. Absolutely. I thought about writing it as a more general post, but I decided that since I get this question so much about Haskell specifically, I would gear it towards the special case of Haskell so it would be more searchable and include a concrete list of Haskell-specific resources.
Yea. Plus, Haskell's a little far gone to try and change tuples this way =P I think a young language like Idris that already has type level lists as standard could maybe make a change like this.
Since most people can't be convinced to use Haskell in production, I wish there was a language with opt in purity at the typelevel, it would have to be designed from the ground up to do so, but it would be pretty nice imo. The function must only contain pure calls, and the moment you use IO, you lose purity.
It is quite possible to write readable code in Haskell. Unfortunately, neither the currently-popular style nor the syntax and semantics of the language particularly encourage it. It is really easy to end up with relatively unstructured mega-expressions that do...things...that it practically takes a degree in Haskell to understand. This is the point at which I should provide an example, but I need to rush off. I'm sure other Redditors can fill in some good ones.
You don't need to sign up in advance. The contest is open for anybody (except those too close to the organizers, usually), and for teams of any size. All future information will be posted on the [blog/site](https://icfpc2016.blogspot.hu/) linked above (I linked the first post originally as that was the most informative), probably when the contest starts (UTC 00:00, about 6.5 hours from now), and also on [twitter](https://twitter.com/icfpcontest2016). The contest runs for 72 hours, and there is a "lightning round" for the first 24 hours. Hopefully the task will have something to do with functional programming this year.
This has been available for about 6-7 years now: https://hackage.haskell.org/package/applicative-quoters-0.1.0.8
He's just a troll. Probably a troll that had a hard time learning Haskell and feels so infuriated by it that he decided that a more worthwhile pursuit is to spend all his time on making fun of the language and people that use it. I prefer to spend my time on things that I enjoy, not things that I dislike. But to each his own, I guess.
Wait what? You need ambiguous types for the vector approach (if you want to use `@` for keys anyway), but I don't think you need actually overlapping types.
8 . Apply to Myrtle Software! https://www.myrtlesoftware.com/vacancies/
&gt; Try to address my pet problem Lol! It really is your pet problem. &gt; how do you deal with DB associations in Haskell. Pick a complex domain problem with lots of associations between the tables, and lots of ways that data is being represented in the UI. And show us the code. Can you tell us how any other language deals with it? Why can you not do the same in Haskell?
I was a Java programmer for 10+ years and always thought that Java's Input/OutputStreams have a wonderful design, until last year that I started working on a compression format that is inter-operable in both Haskell and Java. While Lazy/Strict ByteStrings in Haskell could be less confusing for beginners, after implementing the same logic in both languages, I found the Haskell one much much shorter, less buggy, and far more clearer. I really praise Haskell for that. I agree that learning Haskell makes it difficult to write code in imperative languages. It's mostly because once you know there exists a better approach, you don't like to use the bad one anymore. I suppose you can have the same sense when reading code. There is not other reason to make reading code in other languages more difficult, I think.
And me :-) Here currently!
I'm looking for teammates! Ping me either here or on Freenode ..
This could be fun! I think it might be a good idea to try and form teams of people with similar coding styles/aptitudes though, otherwise you're stepping on each others toes, or one person is doing most of the work. I say this having been by far the least Ruby-knowledgeable person on a Ruby angel hack team. It sucked for me, and it sucked for the rest of the team.
I think cause and effect are getting mixed around here. I wrote this post in response to a question about how how to get the skills needed to get a job programming Haskell. Becoming skilled is the cause, getting a job is the effect. If you go to a Haskell meetup or hackathon and are buying people drinks and trying weasel a job offer, I can almost guarantee that people will see right through you. It works both ways. When recruiters come around talking about hiring, we can see right through them too--especially the ones who are trying to sell you on working on their Python app. Don't pursue the job. Pursue knowledge and improving your Haskell skills. Then when you're out to dinner or chatting on IRC people will recognize that, and THAT is what can get you a job. I chose a title that focused on the job because that seemed like the question I tend to see people asking most frequently.
Yes. This is a whole different idea. I think that this kind of message has to be carefully tuned though, because i might not be the only one misunderstanding it. Maybe for you it sounds natural that a job will be the result of learning, deepening and being passionate about the contents, but this is not how it works in many, many other areas, where networking, showing off and pleasing the boss (or the future boss) is all that matters
I did try my hand at last year's comp (hexagonal Tetris, Cthulhu-inspired cryptographic thingie). I don't know how to gauge others' coding styles and aptitude, but if someone wants to see my work, it's up on [github](https://github.com/ocramz). It would be best for a team to be sitting all in the same room or within shouting range; anyway, I'm on CET.
I saw this one. it's awesome to see the detailed connection with programs worked out. However I am not too fan of it from a categorical perspective. It would bring a lot of clarity to go in 2-cat and have high level view. I don't know why it's not more widespread. I still have to read Hinze, it's on my todo list. I will probably still miss nice simple pretty 3D picture I imagine :) In that vein, have you seen jaskelliof's nice paper about [second order representation](http://arxiv.org/abs/1402.1699) 
FYI the github issue for this is https://github.com/SublimeHaskell/SublimeHaskell/issues/275
One of the main advantages of immutability is safety / general code reasoning, and that kind of object annotated for optimizations would absolutely maintain that safety / general code reasoning. Which is the important part. So it is just as immutable as before, you are just restricted in how what you can do to it.
I might be quite biased, since I'm mainly a web dev, but I feel like the majority of developers out there are doing web related stuff - I mean, one of the easiest companies to start is making web sites for people (at least from my experience). Therefore, I feel there is quite a lack in ready-to-use things like a CMS to compete with Wordpress and something for eCommerce to compete with Magento. I personally, a long time ago, toyed with my own one, HsCMS[0], while learning Yesod and Haskell, and later on there was also another attempt, LambdaCMS[1] which I get the feel is not being actively developed anymore. We basically need something that is an easy sell to clients, since the website creation market is enormous (I mean, if you don't have a website, you basically don't exist outside your very local area). My main problem with not doing this myself, is that I haven't quite figured out yet how to support some quite essential stuff if you want to be taken serious: plug-and-play extensions and themes. I had a small attempt at themes in HsCMS, with some TH and then you could set it in your configuration file. I've also been thinking about just making what is basically a frontend for Jekyll, which I've outlined in a repo on github[2]. [0] https://github.com/Tehnix/HsCMS [1] https://github.com/lambdacms/lambdacms [2] https://github.com/Tehnix/hakyll-frontend
Thanks. You said all I wanted to say in my rant. But you got 18 points at this moment, while I would have had -18. Can you teach me how to do it?
Yeah, you make a very good point. I'll keep that in mind in future discussions on this topic. For the record, here is the initial comment that led to this post: https://news.ycombinator.com/item?id=12190952 Notice how the whole comment is talking about skills/knowledge/learning, but then in the last sentence the wording switches to "getting a haskell job".
2-categories are a great way to think about monoidal categories and monads. A monoidal category is simply a 2-category with a unique object in the same way as a monoid is a category with a unique object. This lets you generalize the notion of monad to endomorphisms in a 2-category :-) Hinze's paper, if my hazy recollection isn't wrong, gives hints of a 2-categorical flavor by discussing string diagrams. I haven't seen that other paper, thanks for the link!
There are instance heads for (x, l := a) and (l := a, x) so (l := a, l := a) is ambiguous. In the HList case, you have to pick out whether something in the head of the list matches the label you have against 'everything else' where you continue rummaging through the HList looking for the next match. The 'everything else' instance is overlapped by the special case.
Nice writeup.
Heh, I'm on EST, so that's that then.
I'm unsure. Overloading ! seems like it could be quite confusing.
parentheses actually have nothing at all to do with tuples in python, merely the commas. `x = a, b, c` is a valid tuple. leaving a comma after a value is also a valid tuple literal. ie: `return foo,` returns a 1 element tuple. i've seen a number of bugs relating to this (due to edits that left a comma) in production code. &gt;&gt;&gt; x = 1, &gt;&gt;&gt; x (1,) 
If you think about it its like saying 'we're parametric here... almost, except for this one special case in particular, screw that case' So until you have enough detail to know if you are in the special case or not things get 'stuck'.
Haskell has quite a few partial functions. Then there're also exceptions with all their nasty behavouirs. Runtime errors because of unitialized record fields or incomplete pattern matches still seem quite strange to me for such a safe language. I somehow like the error handling of Rust, that exceptions aren't used for it. Sure, there can be unreadable Rust code, but I also saw quite a bit of unreadable Haskell code, so this IMHO mostly boils down how the code is written and is less a "quality" of the language itself. There can be quite beautiful Haskell code, like parsing code written in an applicative style or every kind of code that can be written in a declarative style, here does Haskell really shine, but sometimes a simple loop is the easiest solution and having to use e.g. some nested folds doesn't increase the readability. In every language I'm missing features of an other language. :/ 
Since you haven't actually written your rant, I will just highlight phrases in your *other* comments that make me dislike said comments – if your goal is to actually get your point across, and not alienate the community, then becoming aware of -what phrasing tends to be alienating- might help. &gt; I doubt that this would aggrandize me. I'm as bitter as the **reality** of the haskell community is, and this is the only way to break the circle of **auto complacency**. **Truth makes you free.** *[This just sounds pretentious, really.]* Next: &gt; The best contribution of Haskell to other languages are not the lambdas or the monads, but that Haskell has attracted **most of the people who enjoy self promotion and complication.** *[Strong statements need either authority or strong evidence. Cf. /u/bitemyapp's [comment](https://www.reddit.com/r/haskell/comments/4uv3f8/what_are_the_top_things_you_miss_from_other/d5t1b73) about navelgazing – it worked out for Chris because Chris is well-known in the community.]* They are now busy assembling monad transformers in Haskell and the **good programmers** can work in other languages uncluttered. Next: &gt; PS: I love haskell, **but I don't love you.** *[Why should anyone care whether you love them or not?]* I don't want to make you angry. Only **give you the opportunity** *[this sounds pretty patronising]* to be **more humble (no, really humble, not the false humbleness)** and really contribute to the success of Haskell Next: &gt; But the Haskell language is wonderful. Is the Haskell community -that is **misguided by wannabe academicians that hate industry**- *[more strongly worded, attackish claims without evidence/authority]* the one that makes Haskell a mess. Overall, I think that if your goal is to get more people think about unpleasant questions (e.g. “how come there are only five or so well-known, widely used apps written in Haskell – pandoc, xmonad, git-annex, postgrest – while there are tens of thousands of such apps written in any mainstream language?”), you should stop ranting (this should be easy since you actually are aware that you are ranting) and start thinking about the reasons. * Is it that the Haskell community doesn't have as much manpower as other communities? * Or is it that Haskell itself is poorly suited for “tinkering” and so it doesn't attract people who like tinkering, while e.g. those people are the ones who end up starting most software projects? (Not saying it's true, just saying it's something you should investigate.) * Or is it indeed that Haskell is poorly managed – but if so, what exactly does it mean, and how could it be managed/guided better? What would you change to make it easier to create software with Haskell? (Even if the answer is “throw away academic rubbish”, you'd still have to elaborate *what* rubbish should be thrown away; how exactly it is detrimental; and where did the reasoning of -people who introduced that rubbish into the language- go wrong. Hey, [Chesterton's fence](https://en.wikipedia.org/wiki/Wikipedia:Chesterton's_fence).) After that you can start telling people your insights. Getting some authority in the process (e.g. by writing something successful in Haskell) would help. Being non-confrontational would help too because people perceive information better when they don't have to battle the desire to downvote you in the process. (As for me, I never made claims as strong as the ones you wish to make, so, correspondingly, I didn't get downvoted.)
Bummer! May the best win then!
&gt; thought that Java's Input/OutputStreams have a wonderful design, The new Stream api looks very well designed as well.
"organized by the University of Electro-Communications" http://www.icfpconference.org/contest.html
That initial comment is interesting. That person does not have the possibility to learn during work hours. They study in the extra hours but that is obviously a lot of personal stress and not a sustainable situation. The same could apply to Haskell like to anything else. This is a common problem in the industry, i would say, and it brings a lot of insecurity to programmers, because while they work they feel like getting worse, not better. I at least had this feeling and so i decided to work part-time and use the rest of the time for learning. This is not a new problem. Learning has a cost and many companies do not have the interest in paying that cost
When trying to understand what `(|` and `|)` do in Arrow syntax I once downloaded the whole of Hackage and grepped it. If memory serves me right I found zero uses of this syntax! I'm pretty much one of the biggest Arrow fans there is and even I don't think `(|` and `|)` should be considered taken. I don't think I ever managed to understand them.
But... stuck how? I hacked together a basic version so that I could write: test = (+) &lt;$&gt; lookupT @"The" &lt;*&gt; lookupT @"Boot" . lookupT @"Discontinuous" And GHC infers that: test :: (Lookup "Boot" a1 ~ Lookup "The" a, Lookup "Discontinuous" a ~ Arr a1, KnownNat (Index "The" a), KnownNat (Index "Boot" a1), KnownNat (Index "Discontinuous" a), Num (Lookup "The" a)) =&gt; Arr a -&gt; Lookup "The" a And it correctly figures out: &gt;&gt; test $ toArr @["The","Discontinuous"] (Cons 3 $ Cons (toArr @'["Boot"] $ Cons 4 Nil) Nil) 7 it :: Num a =&gt; a Which generally satisfies what I want from inference. So what's something we *can't* do?
I'm quite happy that duplicate fields leads to ambiguity when trying to access the field. Anything else would be weird.
Yeah, I think this applies everywhere in almost all fields. Things would be so much more efficient if the barrier to switching careers was lower. But given our high level of technology and specialization that seems almost like a contradiction.
Me me 
To be clear, the problem description is found here: http://icfpc2016.blogspot.hu/2016/08/task-description.html
No, but I had a moment today with Java where I thought "those parentheses are really ugly, there should be a $ there....oh.".
&gt;it worked out for Chris because Chris is well-known in the community. Possibly also because I don't shy away from or put down teaching unfamiliar/scary things, even if I think the value of some advanced topics is over-stated and distracts beginners at times. I like and appreciate Haskell's origins because we wouldn't have all these nice and interesting things were it not for that. I wouldn't trade what we've got in terms of community, history, or language for any others, even if I am often critical of where we're at relative to what things could be. My comment was critiquing threads like the one I said it in. I don't think constantly comparing ourselves to larger communities is healthy. I wasn't criticizing Haskell being "too academic" or some nonsense like that. From experience I can say that if I said something equally silly, I would've gotten downvoted into oblivion.
Awesome! Are you going to write up a ghc-proposal?
I've heard similar stories about Scala. Like most developers I have experience with a variety of mostly imperative languages. Have been looking for a new language for a big upcoming project. Stories make Scala sound like C++ meets Java - each developer has her favorite subset of the language and often results in ridiculously terse and obfuscated code. For my big new project, have been evaluating lots of languages, of which golang and Haskell are current favorites. Project is a game server that's network and computationally intensive. No graphics, just a back-end MMO server. The much-advertised benefits of golang seem to directly address problems like "unreadable Haskell" discussed in this thread. golang was designed for simplicity, maintenance, etc. I'm 200 pages into Haskell from First Principles, really enjoying it. EDIT: have to agree, that typechecker code you linked is unreadable. A lot of Haskell code I've seen seems similarly unreadable.
Well, I *was* interested. This sounds... tedious.
I kind of like them. Although I don't have anything released since the literature does use them for something completely different how about relinquishing `(| ... |)` and adopting `&lt;| .. |&gt;`?
The bug is when you don't notice that it's not there. This is particularly damning in python because `()` *is* a tuple. So `()` and `(a, b)` are both OK, but `(a)` is not. `(a) == a`. &gt;&gt;&gt; cats = ('bartholomew', 'Emily Kittenson') &gt;&gt;&gt; dogs = ('barktholomew') &gt;&gt;&gt; for c in cats: ... print(c + ' says, "Meow."') ... bartholomew says, "Meow." Emily Kittenson says, "Meow." &gt;&gt;&gt; for d in dogs: ... print(d + ' says, "Bark."') ... b says, "Bark." a says, "Bark." r says, "Bark." k says, "Bark." t says, "Bark." h says, "Bark." o says, "Bark." l says, "Bark." o says, "Bark." m says, "Bark." e says, "Bark." w says, "Bark." &gt;&gt;&gt;
Yes. This is just way too easy to get wrong and benefits no one.
https://www.reddit.com/r/haskellquestions/comments/46iwen/how_to_actually_use_duplicaterecordfields_ghc_81/ 
You could always have them in separate modules, similar to how you can have the same function in separate modules. You might be looking for `{-# LANGUAGE DuplicateRecordFields #-}` to allow the above module to compile. See also: https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields
You can run ghci from the shell and can load the files created in editor using :l command. It works just like a locally running ghci...
I love the interactive display as an aside window
The goal is never to recruit a person. The goal is to build a product or service that brings some kind of value. Personally I don't even have *time* to go to any of the Haskell meetings where I live. I do have time to recruit people who work remotely as consultants. The checks that I do are relative to the task at hand. Recruting takes incredible amounts of time to do right. I've had my team spend 30% of their time on recruiting - talking to people trying to find those persons that would be excellent to work with. And it pays off - using more time gives better results, but you're still spending 20-30% of excellent engineers' time on advanced estimates of other peoples' performance. It should be pretty clear that there's a trade-off here. You can't spend 100% of a company's time on recruiting in order to get that last percentage point of accuracy in the job performance predictions. Think of it like this: Imagine that you're employed at a place called Bell Labs. You're hanging out with this person called Ken, and with some back and forth, you figure out that it would be cool to create an operating system based on a few simple ideas. Whether Ken is an employee or not is irrelevant at this point. You have a conversation. But instead of starting up a project with Ken, you think. "Should I really work with Ken?" Working with Ken could be a superficial choice. He might actually not be the best qualified for this work. I should really talk to Bell Labs Recruitment and run him through some vetting process along with a public call for applicants that want to work on a new interesting operating system. But before I do so, I should sit down and make some really great interview questions, and also check with Bell Labs Recruiting to see if they have some good statistics on whether I can predict the performance of a candidate relative to the gut feeling I have regarding Ken. ... after 3 months ... Oh darn, Bell Labs Recruiting crunched the numbers and it really seems like it's a coin flip whether minor differences in academic credentials lead to significant job performance differences. Surely there's a better way.. Oh, and they are asking me what I've been doing lately as nothing seems to be happening. Shit, I hope I'm not losing my job over this. 1. You can't predict job performance from academic performance 2. Interview scores correlate badly with job performance. 3. The more you work with someone, the better you can predict job performance. Why not just get someone in quickly and see how it goes - i.e. do a very casual screening. 
The answer as mentioned is that 2^10 is roughly 1000, so this would be roughly 1000 billions. You can relate this to other numbers such as the population of the world, the number of requests facebook probably has during a day etc.
The "twisted functors" stuff sounds interesting as well! https://byorgey.wordpress.com/2016/08/04/new-haskell-symposium-paper-on-twisted-functors/
I wouldn't consider a typechecker to be that simple code, so you certainly need some domain knowledge to be able to comprehend the code. In this case even longer names wouldn't help you that much if you don't understand what they mean. 
I prefer Sublime, not looking to switch editors. I've been coding in Haskell without an IDE since I started so I can get by without it.
Well in my experience specialisation is perceived more as an asset by clients (for instance "I need a Java Spring programmer") than it is an asset in real work life, where being open and capable of operating on a wide range of technologies and problems is way more valuable. This depends on the size of the company though
Fermat was a lawyer.
Yes and the quality of the project has declined steadly. Bloated Kernel, bloated userland, low code quality, community fragmentation (hundred of distros), man pages made useless... Now it's basically a worse windows, without the good bits (games). Yet, even if everyone is using Linux, OpenBSD is not in danger of dying because it has found its niche in the market, while retaining all the good qualities I spoken before. What I'm trying to say is that I don't want to see Haskell dies, but at the same time you don't need Haskell to be used by everyone to ensure its surviving, especially if this could bring objectionable compromises that will lower the quality of the project. On the other hand, focusing on quality will naturally bring interest in the project/language, yet without the need of the aforementioned objectionable compromises. edit: I see this as a similar formulation of Simon Peyton Jones thinking: you want a lot of user, but not too many to impede flexibility/change (research on the language); I say I want Haskell used by a lot of users, but not too many to bring a droop in quality.
What you are talking about is the "secretary problem". You are saying that a too bureaucratic process is not effective. I agree with you. As far as i see it, the solution is finding the right trade-off, not throwing the process away &gt; The goal is never to recruit a person. The goal is to build a product or service that brings some kind of value. But companies pivot. So i would argue that yes, sometimes the goal is actually to recruit a person, and that person can be helpful in a number of different ways
&gt; Networking isn't about weaseling your way into a job For you it is not, but different people have different behaviours and beliefs. So i am just warning about possible misunderstandings and their consequences. Honestly i see many tech conferences where the interesting content is close to zero ... yet the conference price is very high and it gets sold out. Why do people go there, given that there is nothing to learn? Let's be honest and admit that our industry in not immune from a number of social behaviours like favoritism which are dysfunctional. Let's be careful about what we want to become as a community
I think I'll go buy one of those plastic fork 100-packs now.
So the idea isn't to express it as a power of 10, but to name something of similar magnitude.
University of forall b. (Electricity -&gt; Signals Binary b) * ((Electricity -&gt; Signals Binary b) -&gt; Brain (Idea a))
Paging /u/_jgt
While these are all valid paths to a coveted Haskell job, one should also keep in mind that soft skills are important to develop also as being a good team player makes everyone happy =)
So your post caused me to revisit that thought. I wondered if we could represent such a Constraint in types. Obviously we need at the very least: * A function that marks a value as "update in place" `return (Annotated m) =&gt; o -&gt; m o` * A way to enforce that the object can't be copied while it is marked. We can do this by hiding the Constructor of Annotation which makes `breakOptimization (Annotated a) =&gt; a o -&gt; o` impossible. * Because we might still want to use the value normally we need a way to unmark a value `unwrap :: (Annotated m) =&gt; m o -&gt; o` * We need to prevent unwrapping at different places `evil ao -&gt; (unwrap (f1 ao), unwrap (f2 ao))` which would also break the optimization. So it's pretty clear that we end up with a Monad of some kind. There seem to be some candidates from more functional (Cont Monad) to a middle ground (StateT) to barely functional (ST Monad). In the end I didn't really know about the Cont Monad and it seems to be very close what /u/Ford_O wanted which also keeps safety/reasoning so maybe that is something I will use in the future. But Monads in general look to me like mutable programming in a functional dress so I would still say the only way to achieve this is by treating the Value in question like it's mutable.
`postgresql-simple` is certainly actively maintained.
"In the future, with things like GUI frameworks, if we ever want to get a decent story in that space for something other than the web, I feel we're likely going to need to get back to something like the platform, as they are almost non-installable by mere mortals." I'd like to point out real quick that [FLTKHS](http://hackage.haskell.org/package/fltkhs) is easily installable and integrated with the Fluid GUI builder, even on Windows.
Note that you can score points by generating and submitting new problems.
There aren't really any specific tickets but I have been working on a basic tutorial that uses type families to help you set up your types: https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/Tutorial/TutorialBasicTypeFamilies.lhs If you're interested in helping out that would be great! Feel free to email me at the address listed in the Opaleye README and I can help you get started.
You may already know all that, but in case you need an explanation for the ByteString vs. Text... As the name says, ByteString holdd binary data. If you want to explicitly encode your text into a UTF-8 binary, this is the type you'll output. It's file IO will write in a file exactly the bits you put into it. Also as the name says, Text holds text. It will give you all functions for encoding and decoding it correctly during IO. The problem of manipulating text as ByteString data is that most of the functionality isn't there. You'll have to manually create most functions, or relay on half-baked conversions. A few times you'll want that, but it is not common.
True I guess I didn't pay enough attention to the types. Although I could imagine you could get away with a wrapper that restricts you functions of the form (a -&gt; a) but didn't think that one through yet. But then that would make dealing with anything that contains unique values a pain. I will have to think that through. I like your input here a lot! Thanks for the feedback.
For anyone interested, I made a small package for encoding-aware `readFile`/`writeFile`, which can be used with `String`/`Text`: [encoding-io](https://hackage.haskell.org/package/encoding-io)
Hmm, even then, I think most people's primary purpose in going is to have fun and interact with many people that they already know online.
In do notation: foo = do list &lt;- getArgs return $ do may &lt;- listToMaybe list readMaybe may Or equivalently: foo = (\args -&gt; listToMaybe args &gt;&gt;= readMaybe) &lt;$&gt; getArgs Or, avoiding lambdas: import Control.Monad (&gt;=&gt;) foo = (listToMaybe &gt;=&gt; readMaybe) &lt;$&gt; getArgs The `&gt;=&gt;` operator composes monadic functions and is called Kleisli composition or fish operator, depending on your taste. This can be made more natural to read using a reversed `&lt;$&gt;` operator `&lt;#&gt;`: import Control.Monad (&gt;=&gt;) (&lt;#&gt;) = flip (&lt;$&gt;) foo = getArgs &lt;#&gt; (listToMaybe &gt;=&gt; readMaybe) or a reversed Kleisli composition: import Control.Monad (&lt;=&lt;) foo = (readMaybe &lt;=&lt; listToMaybe) &lt;$&gt; getArgs
Something like this, maybe? foo :: Read a =&gt; IO (Maybe a) foo = join . fmap readMaybe . listToMaybe &lt;$&gt; getArgs However, if you aren't really comfortable with Haskell then I wouldn't recommend code golfing and trying to make stuff point-free. There really isn't an advantage and it can be considerably harder to read. You also often have to jump through hoops and write unidiomatic code to make it work.
I've been playing with the "overloaded-records" package and it seems that, using {-# OVERLAPPABLE #-} pragmas, one record can "inherit" the accessors of a component: https://gist.github.com/danidiaz/ceaf518b833f760e778dce7675d1f588 I'm unsure about how "evil" is that use of OverlappingInstances, however. Also, it only seems to work for one component.
It would. I believe Spark is built upon Akka, the Scala take on Erlang.
The `a -&gt; a` thing reminded me a bit of `IO`, which has type `IO a = RealWorld -&gt; (RealWorld, a)`. And `IO` safely mutates `RealWorld` in place (I realize it is just a token, not the literal real world, but the point stands). The reason why `IO` can do such a scary thing is because `RealWorld` is not accessible / duplicatable / createable / destroyable etc. If you create two separate `IO` types then you eventually have to sequence them with `&gt;&gt;` or `&gt;&gt;=` bringing back the idea of one unique token flowing through everything, unless you use `unsafePerformIO` at which point all bets are off. So in the end exactly one `RealWorld` is created by `GHC` and then it is sort of "consumed" by the runtime itself, again exactly once. So I think a uniqueness type is required for any sort of safe mutation. However the way `IO` and `ST` and such are utilized is generally through their monadic (and functor / applicative) interface. Because it permits the indirect use of `RealWorld` without ever actually exposing it (explicit usage of the inner `IO` tuple would expose it for example). So I think we need some sort of uniqueness type and instead of exposing a monadic interface (as at that point we just have another `ST`, and we are now dealing with fairly explicitly mutable values which is what we are trying to avoid), we expose the "raw token" per-say, either directly or implicitly, and then we have the compiler throw an error whenever it cannot define a total order for all of the mutations of said token that is consistent with Haskell's semantics. So lets say we want to use vectors in a pure context using uniqueness to make them fast: a = [1, 2, 3, 4] :: UniqueVector Int b = a |&gt; 5 c = a |&gt; 6 Would raise a compilation error as it cannot sequence the 3 operations in a way that fits Haskell's semantics. As it will either make `b` use `c`'s output token (the output token being the value itself, as we are mutating `UniqueVector Int` not `RealWorld`), or vice-versa, both of which will change the semantics of the program. I think you are correct in thinking that if you have two `a -&gt; a` functions and you compose them with `.` then the compiler should safely be able to convert it into an in-place mutation, although the input value would have to be deep-copied before doing such a thing unless the compiler can guarantee uniqueness in another way (such as with uniqueness types). Which may not be worth it (e.g duplicating a large set and then mutating it could be much more expensive then inserting a couple nodes into it). And you are welcome, this stuff is really interesting and thanks for discussing it with me! I have often thought about "automatic mutation as an optimization" and how practical it could be. It seems like that kind of thing could really make Haskell's performance pretty impressive, as I am guessing in an average program a substantial portion (perhaps even the majority) of the data structures are used linearly.
Haskell could provide a much nicer typed API. Spark appears to rely on Scala string interpolation to generate the necessary typed code. E.g from their docs: case class University(name: String, numStudents: Long, yearFounded: Long) val schools = sqlContext.read.json("/schools.json").as[University] schools.map(s =&gt; s"${s.name} is ${2015 – s.yearFounded} years old") GHC8 allows some reasonable syntax for structural typing and anonymous records. Spark also appears very monolithic and coupled as far as I can tell from a brief look at the code. A Spark query seems to require a cluster to construct and test. It looks like a lot could be improved, but it would be a lot of work. Spark started off in academia, so maybe the author of the next big thing will pick Haskell next time. 
&gt; # Myths &gt; **Exposing the database destroys encapsulation** &gt; PostgREST does versioning through database schemas. This allows you to expose tables and views without making the app brittle. Underlying tables can be superseded and hidden behind public facing views. The chapter about versioning shows how to do this.
&gt; Meetups and hackathons are great, I wish there were more of them. You don't have to know how to write Haskell to organize these kinds of events =)
I just wanted to get this off my chest and see what people think. Can anyone help me close off that GHC ticket? Makefile magic needed.
You should open a ticket for the hdevtools maintainers 😊
Unfortunately, no. (As far as 'excuses' go: It's highly nontrivial unless you build it in from the start, etc. etc. These *are* excuses.)
Can you name an instance where the Documentation has failed you? I for one have always found what I needed on Hackage or the Hoogle CLI. Surely finding the package which does what you want might not always be as easy. But once I've found it, the documentation has always been sufficient.
Return value of `getResponseBody` function depends on Request type parameter using `getRequestBS`, so after that you have to convert ByteString to unicode Text type with `decodeUtf8`. Consider simpleHTTP type `simpleHTTP :: HStream ty =&gt; Request ty -&gt; IO (Result (Response ty))`, so `ty` is type of response output, and so if we look to `HStream` instance we found in there `String`, `ByteString`, `ByteString.Lazy`, so by default you can choose only out of these return types. module Main where import Network.HTTP import GHC.IO.Encoding import qualified Data.ByteString as BS import qualified Data.Text as T import qualified Data.Text.Encoding as T import qualified Data.Text.IO as TIO import Network.URI ( parseURI ) zastupnici :: String zastupnici = "http://www.sabor.hr/zastupnici" main :: IO () main = get zastupnici &gt;&gt;= TIO.writeFile "zastupnici.html" . T.decodeUtf8 get :: String -&gt; IO BS.ByteString get url = simpleHTTP (getRequestBS url) &gt;&gt;= getResponseBody getRequestBS :: String -&gt; Request BS.ByteString getRequestBS urlString = case parseURI urlString of Nothing -&gt; error ("getRequest: Not a valid URL - " ++ urlString) Just u -&gt; mkRequest GET u 
If anything learning Haskell has made me aware of the intractible nature of reasoning about untyped or mutable memory programs. I've also adapted to this and still review and write a fair bit of Java with a large degree of optimism about correctness. The initial "there is no ground to stand on outside Haskell" feeling fades. I still prefer Haskell, but I don't expect it.
`--explain` has helped me more than once, but I think that's because it extricated the explanation of the type error from the usual compiler noise.
I can't say the documentation has failed me, but having examples of function usage can save me time.
`hasql` appears to be actively maintained.
I remember not understanding how liftIO worked and hoogling it. I got https://www.haskell.org/hoogle/?hoogle=liftio. No quick example. It's not the most complicated type but you get my point. It's on my TODO list to add an example :)
Sure; they should also come with a couple in-line examples though.
I think the term "MonadWriter" or "writer monad" is pretty intuitive. It represents some sort of write operation within a monadic type (very simplified).
&gt; class Monad m =&gt; MonadIO m &gt; Monads in which IO computations may be embedded. Any monad built by applying a sequence of monad transformers to the IO monad will be an instance of this class. Not sure how one can make this any clearer.
Apparently so.
This looks like a remarkable effort. In particular, focusing on concrete ways to help on the problem is what was really needed, I think. Looking at how other communities are doing it is also a very interesting part of your approach -- I felt there could be more details on what are the processes in place in these initiatives you describe as "thriving".
I use documentation testing, but not doctest itself - I basically wrote my own customised to the extra library. 
Thanks! I [added a paragraph](https://github.com/lwm/lwm.github.io/commit/831abda493836163b25e75a13251ec76c2c4cd31#diff-b7a04853caeedab8c9702fd1b6ee8bbdR55) to try to add more clarity.
There's nothing wrong with papers. Having papers is good. But papers are not a substitute for beginner friendly tutorial-like documentation, because that is not what most papers intend to be.
Hopefully after taking Jezen's course they won't be median for much longer.
&gt; Probably a troll that had a hard time learning Haskell and feels so infuriated by it Oh. Poor troll then. Hey troll come to our subreddit and ask us nicely and we'll help you learn Haskell.
By adding an example.
Exactly. Most helpful community I have ever encountered. Anyway, that they had a hard time learning Haskell is the only plausible reason I can come up with for someone spending so much time and energy on something so ridiculous. 
TIL: instead of monoid actions one can have [actions of a category](https://ncatlab.org/nlab/show/action#actions_of_a_category) which match the well-known concept of a `Functor`. An example in Hask would be a distributive action of Hask ((.),($)), since it satisfies the laws `id $ a = a` and `f . g $ a = f $ (g $ a)`. Fun times!
As noted by Mordlib, just a simple example would help! λ import Control.Monad.IO.Class λ liftIO $ print "Hello, World!" "Hello, World!" People learn differently. A wall of text with 'monad' written 4 times in it is bound to scare somebody starting off! 
The `base` library is no longer a submodule, so yes, you really do need the entire `ghc` repository to submit and test this sort of patch. However, getting GHC built is in my experience a rather straightforward exercise after you have the dependencies (e.g. `autoconf`, `ghc`, `happy`, `alex`, and `gmp`). Do let us know if you have trouble.
If it’s so intuitive, it should be simple to write a few sentences that convey that intuition, right?
I think currently the strategy in GHC is to never update in place but [shortlived allocations are never copied onto the Heap.](https://wiki.haskell.org/GHC/Memory_Management) Which means there is always overhead for the allocation but shortlived ones have relative little overhead which should apply to most compositions of functions `f :: a -&gt; a`unless a is really big. However I'm not too familiar with the GHC implementation details so maybe I'm wrong on that front. If such a optimization is actually possible is dependent on the implementation of the GC among other things so I can't speak to that effect. **However** anything that could be updated in place in a tight loop should also help with performance since it exploits the reduced overhead of short lived allocations. So maybe it would be useful to create such a wrapper even without the possibility of in place mutation. ***** I think the only way to make your example a compile error that fits the semantics of Haskell is by using a Monadic approach and I can't think of a way to make your (breaking) example a Type error in Haskell without coming back to a at least a Monadic interface. A semantic sensible implementation of Unique types requires the unique reference to go out of scope after we used it once. The only sensible way to do that in Haskell is by creating a Context for each unique value that informs us if it has been used already. Which brings me straight back to Monads already. Although I guess it would be more of a simulation of linear types? Not sure. Sadly I don't see a way to achieve a **guarantee** for the possibility of in place mutation optimization being applicable without wrapping the whole thing in something similar to a Monad no matter how you turn it without changing the basics of the language. ***** &gt; I think you are correct in thinking that if you have two a -&gt; a functions and you compose them with . then the compiler should safely be able to convert it into an in-place mutation, although the input value would have to be deep-copied before doing such a thing unless the compiler can guarantee uniqueness in another way (such as with uniqueness types). Which may not be worth it (e.g duplicating a large set and then mutating it could be much more expensive then inserting a couple nodes into it). Thinking about it I don't think a deep copy is neccesary data Foo = Foo a b c calculateResult :: Foo -&gt; Foo calculateResult = f1 . f2 . f3 . f4 a b c are all immutable. If you don't change them they can continue to point to the same chunk. If you change them you create a new Thunk, change the reference in Foo to the new thunk and the old one can get garbage collected. The cost you DO pay is a shallow copy of Foo but you would pay that one either way if you change Foo. ***** The big questions to me are * Is it possible to make this wrapper nice enough warrant using it over ST and explicit mutability * Is it possible to implement this optimization in GHC/Does GHC do such a optimization. (Probably hard/no) 
Wasn't there this thing that you can set up on GitHub to test any changes you make? People could submit pull requests to that, and be told automatically whether their patch works.
To use this, you'd have to link in the Haskell runtime, right?
The runtime is already linked in. Once [built](https://github.com/deech/fltkhs-altimeter#installation) the executable (and the PNG files) can probably be shipped as is to any x64 Linux box and run without installing anything else.
I'm considering writing a haskell program that operates on gnupg secret keys, including AES encrypting them. So this is very timely and encouraging! BTW, I opened this issue a while back requesting the same thing: https://github.com/vincenthz/hs-securemem/issues/1
Somehow I find this ironic in a way that I can't cleanly express in words.
Perhaps it won't be necessary. Running doctest on the file your wrote your documentation patch would suffice. Travis will catch anything else, I suppose.
I would like to know about your progress. However remember the following 1. If you read a pure value out of the secure memory it can potentially end up in the swap memory. 2. My understanding is that for the `Integer` type, the integer-gmp package does not give an easy access to the underlying buffer. This makes implementing RSA for example difficult under this framework. Idealy I would want to allocate secure memory for the private key. I believe that all the primitives I have set as target for Version 1 can indeed be handled with our interface. The first test is the X25519 Diffie-Helman key exchange. https://github.com/raaz-crypto/raaz/milestone/2 
Pretty much this here is why base and associated packages don't have way more docs contribution. Perhaps if someone setup a machine that just builds docs with GHC already setup or a CI thing like that it could grease the wheels a bit.
Are you sure this single-inheritance behavior is actually problematic in real-world use cases? Because Rust has it, and I've never seen anyone complaining about exponentially-exploding type checking due to inheritance there. I know there are languages which do have problems with this (Swift), but even then it's almost exclusively in cases with overloaded literals, and afaik Haskell would have the same problem via `OverloadedStrings`. Although even in this case, usually the solution is just to write "see if the type String actually works for this String literal, if not, then go do the fancy inheritance check." EDIT: On second thought, I'm not sure this is even what I was originally asking for anyway. I do *not* want `1 Apple + 1 Apple = 2 Fruits`, I want `1 Apple + 1 Apple = 2 Apples` and `1 Orange + 1 Apple = 2 Fruits`. So maybe `inheritance` isn't really the right word, at least not in the sense that it's used in OO languages.
Cabal is one project that'd gladly accept PRs improving documentation. You can start by looking at the following issues: * https://github.com/haskell/cabal/labels/component%3A%20user-guide * https://github.com/haskell/cabal/labels/documentation Another thing Cabal sorely needs is an updated website. The website sources live at https://github.com/haskell/cabal-website ;here's a list of relevant open tickets: https://github.com/haskell/cabal/labels/website.
Nice. Do you mind that your altimeter is not working properly? The 100 ft needle is counting up but the digital counter is counting down.
Totally mis-spoke. The widget and bindings are cross-platform so the executable you build can be run without dependencies on any machine with the same OS/Arch not just x64 Linux.
One thing to remember though is that a `Monad` is just an abstraction, there is nothing magical that a `Monad` can do that raw values and functions and what not cannot. It only indirectly allows you to safely mutate an object (`RealWorld`) by providing an interface that never directly exposes `RealWorld`. That mechanism absolutely does not require a `Monad`. So that means that using a `Monad` to make an immutable interface for mutable values is not possible without uniqueness types of some sort. (Like `RealWorld` for `IO`). So we either have to: a) Change / add to the core language itself, perhaps through an extension. Which if it wasn't clear that actually is what I was suggesting with my code example. I know that it is not currently possible to make that example fail at compile time. b) Not use an immutable interface, but then you just have `ST`, at which point you kind of throw away any safety benefits you get from immutability. So in order for us to obtain anything new that doesn't already exist, we basically HAVE to change the core language. And I think you are correct in saying you do not need a deep copy, just a shallow one. But for my `Set` example, that really doesn't help you all that much, it still will very frequently decrease performance by shallow copying first. And sets are actually one of the prime candidates for this kind of optimization. As using uniqueness types to make performant `HashSet` and `HashMap` implementations without `ST` or `IO` would be really really cool. Now such a change would be absolutely massive, but it could also be absolutely amazing in terms of what it would allow you to do in terms of performance. Like I seriously think this could be an absolutely fantastic performance improvement. I have thought about it a little and I think I have a fairly cohesive idea of what could be done to implement this, but there is probably a lot more to think about, as I am sure this is insufficient. Types of functions that could potentially mutate in place could like something like this: union :: Ord a =&gt; Set a -&gt; Set a -&gt; Set a union :: Ord a =&gt; a * Set a -&gt; Set a -&gt; a * Set a -- Mutate first arg in place union :: Ord a =&gt; Set a -&gt; a * Set a -&gt; a * Set a -- Mutate second arg in place (:) :: a -&gt; [a] -&gt; [a] (:) :: a -&gt; a * [a] -&gt; a * [a] And then types of functions that create a brand new object that is totally safe to attempt to linearize / later mutate could look something like this: fromList :: Ord a =&gt; [a] -&gt; Set a fromList :: Ord a =&gt; [a] -&gt; a * Set a mempty :: Monoid a =&gt; a mempty :: Monoid a =&gt; a * a Then there will also be hybrids where an entire data structure is touched and thus the structure can either be mutated, or if it is not safe the mutate it, then it can be used as a starting point for later mutation. map :: (a -&gt; b) -&gt; a -&gt; b map :: (a -&gt; b) -&gt; a -&gt; a * b map :: (a -&gt; b) -&gt; a * a -&gt; a * b Note that the letters before the `*` and the other letters are unrelated and do not need to be named the same. Now if the compiler is able to provide a total order for an object's usage, such that those mutation tokens are never duplicated, and such that the semantics of the code is unchanged. Then it will mutate that value in place, starting from when it is first "created" (that is the first instance of a function of the form `a -&gt; a * a`). Now as long as we are only operating on one "layer" per-say, this should actually buy us quite a lot. Particularly with things like maps, sets and vector, which are probably the top use cases for such an optimizations. As currently immutable vectors, hash-sets, and hash-maps, have some pretty annoying pain points due to them needing to be copied on edit. For working on multiple layers you might need something like `**` or `***` which represents how many levels are safe to mutate. So then something like `(:)` might actually have the more complex type: (:) :: a * a -&gt; a ** [a] -&gt; a ** [a] Or: (:) :: a (n)* a -&gt; a (n + 1)* [a] -&gt; a (n + 1)* [a] Obviously the exact notation should not be this, and should be thoroughly thought through to get something elegant and pretty. But the general idea I think is pretty cool and not too impossible to do, and I honestly think this could be pretty huge for vectors, and hash-maps/sets that are currently not very fun to work with. Of course this still does not describe the exact way in which the compiler actually does the mutation itself, nor does it explain how you would write your own function that mutates in place, but I think that should be the easier part honestly. Although if it could be done in a way that allowed for inferred mutability of functions based on the mutability of the components that would be really really cool. There is so much stuff to consider that I think making a formal proposal of exactly how this would work and some sort of proof that it is valid might be a good idea.
Cool, but, in their scoring, it sounds like everyone who approximates a solution shares a score equal to roughly what *one* team gets for an exact solution. I wonder how well the trivial approximation of no folds would score if you apply it to every possible problem.
I'm very late to the game, but I have written such a library. See e.g. Section 3.2 of this http://web.cecs.pdx.edu/~ntc2/haskell-decorator-paper.pdf for examples of logging and memoization decorators that work on pure functions using `unsafePerformIO` behind the scenes. The implementation is here: https://github.com/ntc2/haskell-call-trace. I think these are nice uses of `unsafePerformIO`, since a misuse of `unsafePerformIO` related bug here just means lost log messages or cache results, not incorrect output from the functions being logged or memoized.
First update in a long long time. Hope to see more :)
Author here. I was hoping to polish this a little more and check my proofs before writing up a proper blog post. The point of the tweet was really to talk about the improved typed holes feature in the latest PureScript release. But for anyone interested, it's best to start by reading the comments at https://github.com/paf31/purescript-comonad-ui/blob/a644d0d39c39c5c58a0f55804e956e133f237dec/src/Main.purs#L40.
Hope I didn't release anything too soon for you! I love the idea of different Comonads representing different UI paradigms. A lot of the react-like programming models are all very similar and perhaps substituting different comonads is a way of comparing their differences. Still mentally digesting :)
&gt; A lot of the react-like programming models are all very similar and perhaps substituting different comonads is a way of comparing their differences. Yes, the goal with this was really to try to come up with some way of unifying all of the different approaches to UI design that we have on top of React. I suspect it applies to others, by considering other comonads too, which is nice.
These sort of posts seem to consistently argue the point in terms of `undefined` and/or `seq`, and most use cases of `undefined` only seem to be due to the lack of visible type application in ghc versions &lt;8...
Whatever happened to "morally correct"? 
The Haskell Platform doesn't actually work, though, correct? It's deprecated, not updated, and Stack is the recommended route for people learning.
Those sound interesting, but I don't see how they'd actually be used in a program. Also their purpose is a little murky to me. Are they intended to be Prelude replacements?
Please stop spreading FUD
Not exactly. It is conventional to treat random access to memory on the currently popular hardware architectures to be O(1). That is actually wrong - from a purely algorithmic standpoint, the fastest possible random access is O(log n). But all computer memory includes low-level highly parallelized hardware which simulates O(1) access for the finite set of available physical memory locations. Some imperatively-specified algorithms leverage that hidden assumption to shave off a factor of O(log n) from the complexity. I don't know of any mutability-based algorithms where the analogous mutable algorithm could not achieve analogous complexity on a level playing field. E.g., if provided, say, with a binary tree oracle that can be assumed to have O(1) access and insertion. Assuming such an oracle is not unreasonable; it is not hard to envision a hardware implementation of a specialized binary tree which would provide O(1) performance up to a fixed finite size. EDIT: For example, the [Pippenger paper](http://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf) linked elsewhere in this thread via SO, explicitly treats memory random access as O(1), and the extra log n factor required for functional programs explicitly comes from a binary tree which it treats as O(log n). So if either we pay the theoretically required log n cost for the memory access or we grant the functional program an O(1) binary tree oracle, this paper no longer demonstrates any advantage of mutability-based algorithms over pure functional ones.
If it's not practical to implement secure memory for `Integer` for this purpose, it might not be impractical to re-invent just the small part of the large integer wheel that you need for RSA. For the key sizes of a few thousand bits typically used in RSA, or even a few tens of thousands of bits, simple Karatsuba multiplication should be fine. 
&gt; Added higher rank classes: `ToJSON1`, `ToJSON2`, `FromJSON1`, and `FromJSON2`. This seems to be a common pattern. I feel like it must be possible to generalize it. But the only thing I can think of is something like this: class ToJSON (f :: k) where toJson :: Rep k f -&gt; Value type family Rep k (f :: k) :: * type instance Rep * a = a type instance Rep (* -&gt; *) f = forall a. f a type instance Rep (* -&gt; * -&gt; *) f = forall a b. f a b ... *(On mobile; not sure that's exactly correct)* Just some kind of way to universally represent fully quantified type constructors without requiring these "ClassName1" style classes.
A DCPO is a perfectly cromulent category. `undefined` isn't the problem. Being able to distinguish `undefined` from `undefined . id` by `seq` is.
What alternatives are there for glade, if I want to use a RAD tool for my app development process?
True. Timing attacks might make it harder. But you can't rely on gmp either for that, can you? Also - if you use `Integer` you can't even know that you have gmp - you might have a GHC compiled with an alternative bignum library. In what way is GHC8 better? GHC8 is released already, so it is a fact, not a rumor and no waiting. :)
I haven't written any GUI apps in Haskell, so if anyone has a better answer on what's most practical right now, I'd love to hear it. But I'll just point out [this post](http://keera.co.uk/blog/2014/05/23/state-gui-programming-haskell/) and the [GUI Libraries](https://wiki.haskell.org/Applications_and_libraries/GUI_libraries) page on the Haskell wiki to see what options you have.
How you write instance `ToJSON Bool`, or `ToJSON(2) (,)` ?
I once heard a story that the problem was not `seq` itself, but the fact of it being polymorphic, and that a monomorphic `seq` wound't have the same problem, is it true?
If `seq` was constrained by a typeclass you would get much nicer properties. I don't know exactly what the implication is for this Hask category business though.
FINALLY it's stable! /s :)
Which new GHC feature are you referring to?
I really have to check that out! Thanks :)
The allocation of memory for gmp was complicated it was resolved in https://ghc.haskell.org/trac/ghc/ticket/9281 Looks like now it should be simpler to work with integers
This looks like an [inappropriate use of type classes](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) to me. I say that as someone who sometimes uses existential types and thinks that sometimes people have too much of a knee-jerk reaction against them. When you use a type class like this that also requires that it is essentially always packed up in an existential type, that is a real sign of a problem. The ultimate reason that people don't like things like this is that it makes things much more difficult to work with and you don't gain anything over the `data` approach. Why not just go with this? data Parseable = Parseable { name :: String , parser :: Parser } New `Parseable`s can still be made any time and this is much simpler to work with. Also, if I'm understanding correctly, you could provide a function like this to get the "match the parser to the language" functionality you want: parseLanguage :: [Parseable] -&gt; String -&gt; Parser parseLanguage parseables langName = ... Doing it based on what instances are in scope would obfuscate things unnecessarily. Instances can come in every time you import something (it is actually impossible to *not* export an instance from a module). As a result, tracking down where the code is coming from, if you wanted to do that for debugging or just reading and understanding the code, could become much more difficult. I don't see a downside to using the sort of table approach I suggested above as an alternative to that.
Thanks!
The old one I think (the one in https://izbicki.me/blog/markov-networks-monoids-and-futurama). I'd love to use it in production but we really need more time and developers to learn it thoroughly.
I had this exact same misunderstanding before. There are no "invisible parens" though that's what it seems is happening at first (especially for `$`). `$` and `.` are just infix functions and it's `$` that was really tripping me up (because it's usually used to replace parens). To make that clear, [look at `$`'s docs][0]. This example is what made it click for me: zipWith ($) fs xs As for `.` &amp; `$`, [play with "The Owl"][1] (`((.)$(.))`). It helped them "click" for me... especially after I realized that my understanding of `$` (which I thought was syntax for a while) was wrong: &gt; ((.)$(.)) (==) 1 (1+) 0 True [0]: http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:-36- [1]: https://wiki.haskell.org/Pointfree#The_owl
Ha ha, that's funny! :-)
There are problems even without `seq`, mostly due to lack of any sort of official operational semantics (but I am now told there is such a thing, so perhaps we can work things out). Your comment about "you cannot computationally determine such equality" is irrelevant, I am not talking about that (nor would I ever).
If you know Haskell then the functional side will be a piece of cake (F# has a very similar syntax and a similar mindset - it's just strict and not pure and overall quite a bit easier) - so I guess you'll struggle most with the .net stuff. Sadly I know no resource coming from this side but I would try Expert F# which covers much of this. Check out [FSharp.org](http://fsharp.org/learn.html) for more learning resources. For the rest you'll probably have to deal with C# (which can be easily translated into F#) 
This would be a great time to adopt semantic versioning and call it 1.0.0. Judging from the changelog, Aeson was already doing this before, only with a ‘0.’ prefix.
It's really not a good idea to use docker for sandboxing things.
Keep going. I met a guy recently that said it took him 5 years.
I like the way each graphic in http://138.68.50.128/ becomes the controller for the next. Clever. 
I should add that the system is all very much untyped right now. For which Haskell type is the `Schema`? How can we guarantee all the fields of a record appear in the HashMap? How about the invariants about union? There are legitimate answers within the Haskell (or GHC) type system to these questions, but that does not make them the right engineering trade-off, so I punted.
Thank you!
I've been onerous about this for a long time. The use of "Hask" seems to highlight the distinction between the Haskell community's academic-sounding discourse and actual reasoning about functional programming. It's a vaguely defined construction. Even the term "Haskell" itself is unspecified. (Something like Haskell's core language would be a better choice). The community loves to use reason by analogy. But to borrow the phrase from Bradon Smith, the price of metaphor is eternal vigilance. It began with monads as burritos in space suits becoming a meme in the community. You have today still people who talk about functors as containers. I personally don't even feel comfortable with `Traversable`, since it seems to be defined in terms of its own analogy. (It's like a less clusterfucked version of `Num`). It is a quick quip that people can use to communicate the basic idea of a category. But like most of the use of category theory in the community, it comes off to me as dishonestly shallow. It's a throw-away example without much depth. And it does not give you any sense for the universality of the categorical perspective. 
You might be interested in the related thread https://www.reddit.com/r/haskell/comments/4wmqji/first_release_haskell_avro_library_with_encoding/
No. What's even more interesting than Haskell not being a category, in my opinion, is that so many category theoretical concepts apply even if it isn't one.
It would fix the first of the two issues -- that you're not a category at all, because you don't have real identities. This is because `seq` as a typeclass wouldn't apply to functions, and so we could keep eta, and there would be no distinction between "undefined" and "undefined . id".
I wasn’t aware that there exists a package versioning policy for Haskell. ([Link](https://wiki.haskell.org/Package_versioning_policy) for those who didn’t know either.) Some packages using three-number versions and some four-number versions confused me a lot, I thought those three-number versions were semantic versions. Thanks for pointing me to this.
Don't worry. Most of the code written in Haskell is either terrible or follows the fashion of the moment and will look terrible in a few years from now. That happens in any language. Just try to get things done and nothing more. Whatever is your style, you have the strong type system helping you, the maintainer of your code and your client. How to abuse do and return? If a piece of code is algorithmic in the sense that is inherently imperative, it is imperative. Haskell MUST be an imperative language to execute programs. "do" is not a sin. IO is not a sin. A sin is wasting time in fashionable prejudices. For God shake! forget all these stylistic complexes. Let's make Haskell a programming language that solve software problems. Not a religious sect. You probably call "good style" to using all the conventions and libraries on fashion today. Don´t do that. It will look ridiculous and contrived in a few years. Make it simple. 
### Low Level Issues For the implementation the biggest issue would probably be the way GHC handles mutable code. Afaik active mutable objects add a cost to every garbage collection cycle. So having mutable objects might not be an advantage without making the GC strategy more friendly towards mutable objects. I think it's not useful to think about GC optimizations before there is a framework that can be used to organize computations in a way that could enable the optimization though so it's still useful to have this discussion before that. **** ### Implicit mutation Imo for a piece of code being able to profit from mutation it needs to satisfy certain semantics in regards to data dependencies, parallel access and so forth which I will just call mutable semantics here. I think while it's possible for a piece of code to happen to fulfill these "by accident" I think that this would be the exception and not the norm. So one of my basic assumptions here is that it makes sense to **explicitly** write code to be compatible with these semantics. Now the question becomes how can you write code that is both pure and satisfies mutable semantics. Since if you throw out the first ones you might as well use STRefs and similar solutions. &gt; Now if the compiler is able to provide a total order for an object's usage, such that those mutation tokens are never duplicated, and such **that the semantics of the code is unchanged.** It sounds to me like you want to be able to tag a value and have the compiler try to figure out a way to optimize it's processing through mutability and if not possible fall back to default semantics or give a compiler error. Imo once there is a reliable well defined way to describe mutable semantics in a pure way it should be possible to find code parts compatible with these and transform them so that they satisfy these. However I don't think tackling this issue first is helpful or the way to go. If you want to find a way to enable **implicit** mutable semantics than we are talking past each other for now so keep that in mind for the rest of the post. **** ### How safe is the ST Monad? &gt; but then you just have ST, at which point you kind of throw away any safety benefits you get from immutability. I don't see how the ST Monad itself throws away safety benefits when used without STRefs. All it then does is provide order of execution and encapsulation of the computation within runST. The ST Monad already uses the State# primitive to avoid duplication and provide order of computation and I don't see a reason why that couldn't be reused to simulate Unique Values/Linear Types (By wrapping it in a monad or creating another kind of wrapper. **** ### Can we capture mutable semantics in types? I want to know if/how we can a describe a structure that can be used to enable in place mutation while preserving pure semantics similar to what IO/ST do. If it turns out there is a straight forward way to represent this structure then it should also be possible to check if a given program can be transformed into such a representation automatically. (Probably relying on pragmas to give the compiler hints in that regard). However first we need to know how such a optimization compitable structure looks! ## I want to be able to express for a given value that functions operating on the value should semantically be compatible with updating the value in place instead of creating a new one based on the old one. Even if the compiler might not actually perform the optimization at this point. I will use `m*` instead of `a*` as annotation for "mutate this parameter" because I think that is less confusing. Let's ignore the issue of `m* a -&gt; m* b` and nested data structures for now and try to figure out the semantics of what `:: m* a` actually means. Also let's for now assume we have the functions: return :: a -&gt; m* a extract :: m* a -&gt; a Further let's assume we know how to write functions operating on mutable values with type `m* a -&gt; m* a` 1. Can a result `m* a` be passed to a regular function taking a type of `a`? 2. Can a function of taking a type `m* a` be passed a value of type a? 3. If the answer to the two above would be yes then would `m*` be just a kind of compiler Pragma? 4. If 1/2 are no then why should it be impossible? (Typechecking Error, Compile time, Runtime?) Imo the answer to 1-3 is no and 4 should be a type error. Lets say we have a bunch of functions which could use in place mutation. f1 :: m* a -&gt; m* a f2 :: m* a -&gt; m* a f3 :: .... Great now we can use composition between any of them in order to avoid recreating the Data Structure between them : `f = f1 . f2 .... fi` If `m* a` could be passed to a regular function taking a type of `a` we could easily break the optimization by including a function in the middle of the composition that takes a, creates a persistent copy, does whatever with that one and then returns a new m* a which might or might not be the same one. This would break our requirement for f that `for a given value that functions operating on the value should semantically be compatible with updating the value in place` The same holds if we can pass `a` instead of `m* a` only now we could include any function returning a new immutable a in the middle and break the optimization that way. However we can simply make `m* a` a type DIFFERENT from a. This solves our requirement that we can chain in place optimizing functions without breaking the optimization. Making cases 1/2 a type error and making a PRAGMA unnecessary. Once we consider `m* a` to be different from `a`we must specify within a functions type if and which argument should be mutated in place. So you could implement three types of union function (pure, mutate first, mutate second) but only under different names. I assume this is what you wanted to express with your union example to begin with though. **** ### Using Monads to provide explicit order of computation &gt; Now if the compiler is able to provide a total order for an object's usage, such that those mutation tokens are never duplicated, and such that the semantics of the code is unchanged. Then it will mutate that value in place, starting from when it is first "created" (that is the first instance of a function of the form `a -&gt; a * a`. I don't think it would be better to make the ordering requirement implicit. Especially not when you want to consider of incompatibility with the optimization an error which was the starting point and which makes sense for a starting point. And once you require a objects usage to be ordered why not wrap it into a Monad and make the ordering of computation performed on the mutated value explicit that way? Sure any other type of combinator could be used and maybe another one fits better. But Monads seem to be as good a starting place as any. ## Why writing code using mutable semantics doesn't equal using STRef I think the biggest difference is that using Pointers the result of your computation is implicit. All you get is () and the result of the computation is stored in the STRef. While with mutable pure semantics [buzzword bingo] you can to a degree pretend the value you get back is a new pure immutable value. Only that in truth when executed the return value might be the same pointer you passed as argument pointing to the same address, saving you the allocation. **** &gt; Now as long as we are only operating on one "layer" per-say, this should actually buy us quite a lot. Particularly with things like maps, sets and vector, which are probably the top use cases for such an optimizations. As currently immutable vectors, hash-sets, and hash-maps, have some pretty annoying pain points due to them needing to be copied on edit. I had similar thoughts in regards to the "layer" approach and issues with lazyness and I think it's a good starting point. What I wonder is if one could capture the constraints a data type requires for this kind of optimization in a type class based approach. But I'm interested what you think of my other points, especially if you think it makes sense to capture the semantics required for mutation explicitly in a typesystem focused way. If yes feel free to shoot me your contact info via PM and we might actually get somewhere. Even if it ends up only being a place where we have learned that we were in way over our head. 
&gt; Avro Someone a fan of abandoned Canadian military aircraft? 
I suggest you ask Apache! (and they might point you to the actual origin)
Yes, that's right. But, perhaps surprisingly, there's a `ComonadTrans` instance too. So you have a comonad transformer for every comonad. `Day` seems to make the category of comonads into a symmetric monoidal category.
On the topic of the zipper - it's actually both a monad and a comonad, which is kind of neat. I'm still struggling to get a lawful zipper comonad transformer together to do an interesting take on the Game of Life, where a) it's done with several layers of monad transformers and b) there's a free monad / cofree comonad as the interface to the grid, so that you can write the update rule for a cell with the free monad and apply it everywhere in the grid via the comonad.
&gt; Is your criticism that there is no example of how to define fromJsonKey or it is that there is no example of how to use it? (Or both?) Both. &gt; As an end user, you seldom use toJsonKey or fromJsonKey directly since they just get called for you when you serialize or deserialize a map-like container. I did not realize that, everything is sort of jumbled together in the root `Data.Aeson` module.
Thanks! This other project has much broader and deeper coverage of the GEOS API. I will see if there's anything I can contribute to it.
I think `Hoid` has potential for this. https://reddit.com/r/haskell/comments/4rw1qk The examples I've given so far don't provide any info about the arguments, but it's feasible to do so. Workable or not, it'll probably be pretty dense/subtle... https://ghc.haskell.org/trac/ghc/ticket/9269 is probably the cleanest path forward.
Reading the headline, I was hoping for something involving the [C64 windowing environment of the same name](https://en.wikipedia.org/wiki/GEOS_(8-bit_operating_system\)).
My hope was for the implicit "just works" kind of mutation, thinking mainly about repeated, linear and not too convoluted mutations of vectors, hash maps / sets and so on. Sort of like how strictness analysis and the like sometimes just works, switching `foldl` for `foldl'` and what not. Admittedly laziness complicates this, potentially quite substantially, without some compile time guarantees of what will and will not be strict I think you will be fairly limited in what you can do. I don't fully understand what you mean with just using `ST` to order things without using `STRef`, the whole point of `ST` is to use `STRef` IIRC. Because something like: main = do let a = undefined let b = 6 print b Works just fine, there is no ordering between `a` and `b`, as let binding is "outside" the scope of the `IO` monad. Just like with `ST`. So would you be looking for something like: a &lt;- fromList [1, 2, 3, 4] b &lt;- a |&gt; 5 -- At this point `a` is "out of scope" and thus can't be referenced let z = length b -- b is still in scope as it was merely viewed, not modified c &lt;- copy b -- again b is still in scope as it was merely viewed, not modified Because this actually could be quite interesting, although it would still absolutely require an extension to the type system to force things "out of scope". With that said I still think a hybrid approach where you explicitly tag when you want something to be mutated in place, perhaps combined with explicitly tagged functions, but outside of any `Monad` could be possible, perhaps with the compiler telling you to insert a few `seq` calls before it will compile your code. Also specifically for the whole "can you pass an `m* a` into a function taking an `a`" I thought about it a bit and the answer is sort of "it depends". In order to make performance acceptable Haskell users use a ton of sharing, such as with `Set` only mutating `log n` nodes (so like a "super shallow copy"), rather than making a true "shallow copy". Passing in `m* [a]` to `length :: [a] -&gt; Int` is safe as long as you evaluate that `length` to `WHNF`before doing anything further to your `m* [a]`. But on the other hand passing in `m* Set a` into `insert :: Ord a =&gt; a -&gt; Set a -&gt; Set a` is extremely dangerous. I will PM you my contact info now!
Would that include forbidding terms of the type `forall a. c a =&gt; a` for some Constraint `c`?
I always enjoyed the Haskell community as one of the more friendly and civilized programming communities but sadly things seems to be changing. It's really disheartening to read something like this here. PS: I was obviously never even a beginning Haskeller as my brain refuses to allocate storage for `Category` classes in the first place (well maybe I don't remember the lobotomy) PPS: Yes I get it - it's supposed to be a funny joke - I would probably have laughed if it had some kind of helpful content also - but all I am getting from it (as a non-english speaker) is contempt for others 
I'm a big fan of Snap, and have been looking forward to the 1.0 release for quite a while. Congratulations to all involved!
Great work! :)
Thanks, this seems like great advice that I will start putting to use. Do you have any general tips for ghci you can share? 
Just curious, how long did it take you?
Can you please push it to hackage? That takes only a few seconds, and it adds a huge amount towards ease of use for other people. Thanks.
Ha, I didn't know you were in that department. :)
Not at all. It is a way to make it easier for others to use your code. Currently you indicate stability/maturity with version numbers and with your package comment/`README.md`. The original plan was to add features to Hackage for authors to indicate stability in a machine-readable way, and social features for the community to assess stability, quality, and popularity. And Haskell Platform was to become the reference set of libraries accepted as stable and widely used. Unfortunately, none of that has happened. So all we have is an informal version number convention (&lt; 1.0 as opposed to &gt;= 1.0) and human readable statements in the README.md.
Ah, glad they finally fixed that. This enables you to link gmp independently, even for GHC builds that use gmp for the `Integer` type.
I'm using both at the moment - Servant for a REST API and Snap to serve that API, some static files and to handle websockets - although it's currently fairly simple / the scaffolding for some FRP stuff I'm exploring. I'm also a fan of snaplets, but I'm not really making full use of them in my current project.
I guess the use of concrete types with IO and compiled templates probably makes it fast. (Benchmarks?) Snap advertises itself as simple but I guess that doesn't apply if you are effectively building servant on top.
I use this "trick": `$` and `.` are like an arrow pointing to the left: `&lt;-` So you can read `fire . prepare . arm $ buildMissileFrom metal` like `fire &lt;- prepare &lt;- arm &lt;- buildRocketFrom metal` The only restriction is that you cannot put a `.` before a value, for example `buildMissileFrom metal` is a value, the missile itself, thats why we use `$`
I recommend forcing yourself to use it more, especially GHCi. My workflow is to have Sublime Text 3 for my code with `stack build --test --fast --file-watch` so compilation is constant so I can see the effect of each change I make on file-save. You get a quick feedback loop at this point. Also, trying building something bigger than you currently do, as this reuses skills you know and reaffirms them as skills, so you'll concentrate on what you don't know.
Learn Haskell proper by doing a real project with it, go way beyond the "Hello world!" stage. Next think about your future goals. You most certainly need to look beyond functional languages if you want a job as a programmer, because jobs for functional languages are really rare. If programming is just a hobby, then you can pick whatever interests you. Progression from Haskell to Java is probably more natural than to .NET, because despite of Mono and .NET Core, .NET is rather minor thing beyond Windows. But if Windows platform development is of interest, I would suggest learning C# first and then F#. Makes the process much smoother and gets you C# dev jobs, which are plentiful. So, basically you could pick from Java or .NET platform as step 2 after Haskell. Both camps have very large number of languages, but here are some of the most commonly used ones. Java Scala Clojure There is also Frege, but it hasn't really taken off, yet at least. C# F# PowerShell Either way, learning Java or C# will make it easier to get into the other languages that use same platform. There is also Python which could be useful to learn, a lot of Python jobs available.
why not [Winter 2016](http://www.scs.stanford.edu/16wi-cs240h/)?
The two aren't mutually exclusive. [This project](https://github.com/haskell-servant/servant-snap) allows you to use Servant with Snap. It requires 1.0, so I expect it will get more activity now that 1.0 is on hackage.
At the lowest level Snap is a web server. It also includes the Heist template system and the higher level snaplet system for composing applications from reusable pieces. [Servant](http://hackage.haskell.org/package/servant) (the core package) is none of these things--it is a library for describing REST APIs at the type level. The `Server` monad you mention [is defined in the servant-server package](http://hackage.haskell.org/package/servant-server-0.7.1/docs/Servant-Server.html#t:ServerT) which facilitates serving Servant APIs with the warp web server. The [servant-snap package](https://github.com/haskell-servant/servant-snap) is the analogous package for serving Servant APIs with Snap. The main reason servant-snap is not on Hackage yet is because it depends on Snap 1.0. Now that 1.0 has been released I expect we'll see servant-snap on hackage in the near future.
Are there any benchmarks around comparing Snap 1.0 to previous versions, and/or to Yesod?
The problem is that being overloaded with work is not just true for anyone developing haskell tooling, but for GHC developers themselves. What you're proposing will only shift the work of keeping those things up to date to the developers of GHC, and I don't think that they have the capabilities right now. In my opinion, we should tools on top of the GHC API, but the GHC API really needs a lot of polish. 
We've been using io-streams for high performance networking and have been super happy with it. Excited to have snap-server with io-streams finally released! I'm a huge fan of snap. Doug and all thanks for making the final push!
Is there a link to functional reactive programming?
I read a post about this somewhere in on the internet (written by the designers of Haskell) but I cannot find it anymore. Can someone provide it? I completely understand the position they're taking. After all @matijapretnar and I first defined the denotational semantics of [Eff](http://www.eff-lang.org/) and implemented it, and only then came up with an operational semantics. But it seems to me that a language can always benefit from a precise operational semantics. After all, GHC does *something* (graph rewriting) and there can be no harm in describing what it does.
Friend, I think you have the wrong subreddit.
No, but you would have to forbid `forall a. a` from being a valid instance. Also, just to make sure I'm being clear, I don't think this is something we should actually do.
&gt; There are no "invisible parens" Well, `$` is so low-precedence, that there's effectively "invisible parentheses" on *both sides* of it, which is what allows `$` to be used in lieu of parentheses sometimes.
Well there are "invisible parens" in that you can translate everything to prefix with parens. Are you trying to get at the idea that translating back and forth in your head is the wrong way to go? 
This isn't necessarily good advice (at least, my reading of it, which is "typeclass all the things!"). Typeclasses _can_ be a good abstraction mechanism, but they can also be heavily abused. Roughly speaking, typeclasses without laws can often be replaced with good ol' higher-order functions. Defaulting to "multiple thingies have this interface so it's obviously a typeclass" has not worked well for me in the past.
&gt; stack test --fast --file-watch Saved you a word.
I think worrying about overusing typeclasses before the learner has got a 'feel' for how typeclasses work is a case of the tail wagging the dog, to be honest. I say let's get lots of practice creating and using them without worrying about laws and abuse and such; the sense of how to use them appropriately will evolve with practice, much like with anything else.
oh, I didn't mean to imply there was some error, just that it went from 0-60 like _that_
&gt; the sense of how to use them appropriately will evolve with practice, much like with anything else. Definitely, but broadly speaking I think there's too much of a mental association between Haskell typeclasses and OOP interfaces in the community at large. It's non-obvious to a beginner and even to many experts when a typeclass is or is not appropriate, and in the end it's entirely subjective anyway. So, sure, typeclass all the things, curry all the things, GADT all the things, and on and on. But in the end, it doesn't hurt to at least be aware of the suggestion that typeclasses are a uniquely abuseable (sp?) feature of the language.
Nice! I was really looking for that. May I ask you why you use `IO` for almost everything? For example intersection :: Geometry -&gt; Geometry -&gt; IO (Maybe Geometry) isEmpty :: Geometry -&gt; IO (Maybe Bool) Conceptually, both are pure functions.
The second counit law (and associativity too) needs a parametricity condition which allows us to identify terms in `Day f g a` with different internal structure. How this corresponds to `Day` as a coend, I'm not sure. I was planning on writing up my proofs in a second post later this evening, but I'll ping you before I do.
We could definitely stand to be a lot more clear as a community that, in general, applications of category theory to Haskell are talking about the total fragment. Whether or not a Hask category with partial terms can be defined, it remains the case that most of Haskell's library of categorical type classes have instances which only satisfy their laws for total terms. (And correspondingly, we can be more clear when, in specific instances, we might mean something more than the total fragment.)
But Scala's still favored to win
Hi gelisam I know how to test for primality but I have no idea about how to program this into Haskell. I got a book about learning Haskell for beginners; yet, it didn't a full program from beginning to end. The mathematical end of what I am trying to do I already know but the programming I need help with. Is it possible to teach me this Haskell programming from opening the program to typing every keystroke that is needed to complete a fully operational program. Thank you, George Remscrm P.S. Don't forget to run a number from one power to another
cool!
&gt; comonad instances for Day we'd love to hear more about this!
^_^ thanks :)
On HackageDB: https://hackage.haskell.org/package/x86-64bit
I sometimes define a similar function that I haven't found in any commonly used package: unfoldWithState :: (b -&gt; Maybe (a, b)) -&gt; b -&gt; [(a, b)] unfoldWithState f = unfoldr $ fmap (fmap (\t@(_, b) -&gt; (t, b))) f
What can this be used for? What does it generate machine code from?
[The Essence of Dataflow Programming](http://cs.ioc.ee/~tarmo/papers/cefp05.pdf) by Uustalu and Vene uses comonads to model dataflow computing, which is closely related to FRP. Essentially, if we define `Hist` to mean values which have varied over time, implemented as a non-empty list, then functions `Hist a -&gt; b` are roughly isomorphic to automaton arrows `Auto a b`, which are the basis of arrow-style FRP. Unfortunately, comonads are not useful for *implementing* FRP, because they can't share values between computations: calculating a value at time T+1 can't generally re-use anything from the computation at time T. (Introducing a memo table saves time, but then you have to decide when you can free old entries.)
If you know haskell F# is something of a subset. I learned F# before haskell (around 2008). If you want to really understand programming do haskell. You will be able to be hired to write F# (or scala or other FP languages) because you know haskell.
Hmmm. The inverse of `Split` does pass the injectivity checker. type family Zip (as :: [a]) (bs :: [b]) = (r :: [(a,b)]) | r -&gt; as bs where Zip '[] '[] = '[] Zip (a ': as) (b ': bs) = '(a,b) ': Zip as bs I wonder if that should show me the way forward.
Keep in mind that your RHS is not a type family here, but rather, a list constructor. This suggests using Fists and Seconds type families, and packing them into a tuple. The problem is that those can't be injective. Interesting problem!
When I say "invisible parens" I mean specific syntax that supports putting `$` in lieu of `(...)`. `$` is just a plain old function ([with special impredicative type support but let's not worry about that now][0]). Moreover, I don't agree with you that you can freely translate *everything* between `$` and the "invisible parens". This translation, which is the normal way `$` is used, is valid as the `(..)` are necessary: foo $ bar x foo (bar x) But I don't think there's a valid translation in this case (without redundant `(..)`): zipWith ($) fs xs zipWith (\f x -&gt; f x) fs xs This one *really* gets at `$`'s nature. It's function application... that also allows us to not have to write a lot of parens. Sure, you almost always can translate, but that's my point in posting here: because of how `$` is commonly used, having the wrong intuition is pretty easy when you're learning. At least, this happened to me. [0]: https://ghc.haskell.org/trac/ghc/wiki/ImpredicativePolymorphism
It behaves itself just fine, but I wanted QuickCheck for all of the instances involved. Pretty much everything checks out except for one of the transformer laws, which I got stuck on for a while, after which I got distracted by other things. Thanks for the link - when I have time I'll take a look and dust off my version and see if that unsticks me.
Thanks for the responses! It's interesting that all of you have seemed to address point 3 on this list (and I suppose partially point 4). Any opinions on the first 2?
So this isn't the subreddit for people named Haskell?
I don't use heroku, but that's not a solution for everyone :). We build on Jenkins, using agents that match the arch of our target boxes (we target Windows and Linux), and then we're deploying the Linux binaries using mesos+marathon. There's really not a lot Haskell going on there, the builds are just producing a binary, and then marathon knows how to grab a binary from a uri and keep it running. 
I've not used Heroku with Haskell before, so can't comment there, also haven't used halcyon. I've a preference for plain cabal sandboxes or using a script around cabal like [mafia](https://github.com/ambiata/mafia). At work we build binaries using some CI software we wrote (in Haskell) which builds the binaries on the right architecture/OS in this case x86/Linux. It then uploads it to a permanent storage area, like say S3. Then the binaries are pulled down onto EC2 instances based on their specific configuration, i.e. what software should they be running. There's a number of custom bits we have but really it boils down to build a binary and copy it to a machine. 
It's working very nicely! (I've been using yasnippets for that so far.) 
A DCPO is a category. I didn't say that Hask equipped with (.) as its composition operator was that category! You can get a bit closer with: f . g = f `seq` g `seq` \x -&gt; f (g x) yielding undefined . id = undefined id . undefined = undefined Our products are certainly a mess due to the extra `_|_` as well.
I've been feeling for a while it'd be fun to rewrite PROJ4 in Haskell, but then remember it's probably filled with FORTRAN from the 70's and no one knows how it works any more. This probably isn't true, but the business case for us hasn't been there yet.
oh I get this, it's just not obvious to me what the real problem is with the inequality. it seems to be that if we take the composition of the category to be Haskell's `(.)` then there is damage, but we probably shouldn't be doing that anyway — composition in the category is substitution in Haskell, not Haskell's `(.)`. we would want this to be true: Γ ⊢ M :: A x :: A ⊢ undefined :: B -------------------------------------- Γ ⊢ [M/x]undefined = undefined :: B and also Γ ⊢ undefined :: A x :: A ⊢ N :: B -------------------------------------- Γ ⊢ [undefined/x]N = undefined :: B The first is obviously true, the second is really where the issue arises, I assume? The second looks like a strictness criterion. But in general do DCPOs have to be strict? I've only ever seen that presented as an option, not as if its some absolute requirement for categoriness. ***EDIT*** Scratch this, obviously I wrote the wrong thing; it should be x :: A ⊢ x :: A y :: A ⊢ undefined :: B -------------------------------------- x :: A ⊢ [x/y]undefined = undefined :: B and Γ ⊢ undefined :: B x :: B ⊢ x :: B -------------------------------------- Γ ⊢ [undefined/x]x = undefined :: B Which is trivially true. So I'm truly at a loss for what the issue is :(
Is there a way to get `hlint` to work with you implementation of flycheck to get nice improvement ideas like the one here https://s10.postimg.org/5m5nkvneh/hdevtools.png For me that's one quite important thing I'm missing. Edit: corrected package name
how long did it take you to be productive with haskell? I have started to learn haskell but it seems very daunting.
That looks like hlint suggestions, which I believe are on by default in intero (via flycheck).
Upps, true you're right. That's hlint. But I don't get any when I use the intero checker. Do I need to install something for that to work? I have `haskell-hlint` listed in `flycheck-checkers`.
So if you (/u/paf31) have a way to symmetrically combine two comonads and /u/edwardkmett has a way to create a monad (`newtype Co ...`) for each comonad, do we then have a way to create symmetric monad combinations?
Sadly, no. Not every monad is given rise to by a comonad in this manner. You'll note that the nicely behaved comonads he ends with are the one that when run through `Co`, give the well behaved heart of the MTL: reader, writer, state, which all commute with one another. Likely due to how nice the behaviors of their associated comonads are.
Okay, so if comonads are spaces and monads are (e.g.) movement instructions in spaces, two of the latter won't generally commute. The analogy would be translations and rotations in euclidean space, which generally also don't commute (they do when both are infinitesimal, though).
Thanks! I'll use that too.
thanks
The intention is to create JIT compilers. See also [harpy](http://hackage.haskell.org/package/harpy) for a similar project for 32-bit x86 code. It creates machine code from an assembly EDSL. In theory you could also use LLVM (like [synthesizer-llvm](http://hackage.haskell.org/package/synthesizer-llvm) does), but that's a really very heavy dependency, while this is lightweight.
I also recommend his book: https://gumroad.com/l/maybe-haskell There's more detail on Applicative (as well as Functor and Monad). 
I would suggest a book like Learn You a Haskell, that teaches Haskell, before you jump into the standard library.
So is this something you want to do to learn, or because you need it? If it's just because you need it, use postgrest, which turns any postgres DB into a rest api. 
The Haskell is at the bottom. Not obvious from the post whether this is the implementation language, but I assume so. 
I fail reading comprehension forever. Thanks! 
Eh. I dislike that he describes functors as "containers." That's far from accurate, and is very misleading to those who don't know much about functors. Parsers, Reader, and State are all not containers.
When we started writing Snap WAI didn't exist, so no.
Honestly not trying to do that. I'm speaking from (1) my personal experience of trying to get the Platform running, but it being broken, and (2) [posts in this thread](https://www.reddit.com/r/haskell/comments/4uxgbl/the_rust_platform/d5u7utr) and [here](https://www.reddit.com/r/haskell/comments/4uxgbl/the_rust_platform/d5tu8fn) and [here](https://www.reddit.com/r/haskell/comments/4uxgbl/the_rust_platform/d5tokjq) and more... My point is just that Haskell Platform is not something we can simply recommend to people without caveats and reservations.
[Problems with the Haskell Platform](https://www.reddit.com/r/haskell/comments/4uxgbl/the_rust_platform/d5u7utr) Personally, I tried for days to get it running on Mac OS X, and could not.
So when is next year going to come? Summer of Haskell 2017 should be interesting. 
&gt; [http://futhark-lang.org](my own compiler) You got the two components backwards. You need (my own compiler) [http://futhark-lang.org] (with no space).
How come it is faster than the bindings? Maybe the bindings have a performance bug?
I think you could probably get significant performance gains by using [store](http://hackage.haskell.org/package/store) to parse the binary protocol. In everything I've done so far it uses far less memory and time. In your [MySQLValue](https://github.com/winterland1989/mysql-haskell/blob/master/Database/MySQL/Protocol/MySQLValue.hs#L76) type, you could probably get away with using more efficient time types as Day, TimeOfDay, LocalTime, aren't the most efficient of types. Likewise the `MySQLText` constructor you could switch to `ByteString` and decode when needed, otherwise you're probably wasting time doing an O(n) algorithm to create a new copy of a string twice the size. With the `MySQLValue` type normalized to something flat you could change [`getTextRow`](https://github.com/winterland1989/mysql-haskell/blob/master/Database/MySQL/Protocol/MySQLValue.hs#L243) to return unboxed or storable vectors, instead of lists which have a higher memory foot-print and access cost.
I've seen applicative syntax contrasted with concatenative syntax. In that context, it means what happens when two things are put next to each other. For example, `x y` or `foo 3`. In an applicative language, it means function application. Lambda calculus is applicative in this sense. In regular maths, juxtaposition means multiplication - 3x means 3 times x. You might say it has multiplicative syntax. If you meant anything to do with applicative functors, please ignore me ;-)
Assuming you mean a syntax for working with applicative functors, let's go right to the source: Conor McBride's [*Idioms: applicative programming with effects*](http://strictlypositive.org/Idiom.pdf). If not, please clarify what you mean by "applicative syntax".
[removed]
I've used `bound` in a couple of "production" compilers. I wrote the library because of the pain we were having in the ermine compiler in Scala due to Barengredt convention (unique names) bugs. We started a Haskell implementation of Ermine, but due to reasons entirely unrelated to name capture we've stopped working on it. We use `bound` for terms, types, kinds, and our core language, but switch out when we reach STG as it doesn't have that nice substitution based monadic structure. There are some situations where the code gets hairy, but they are er.. precisely the hairy situations where you can mess things up, dealing with type checking a function in a haskell-style binding group where you need to take into account the names in the where clause, all of the patterns, and everything else in the binding group as well as otherwise free variables, means you have a local situation where there are 4 kinds of variables to deal with. Bound makes you deal with the order of them and which ones can reference which ones -- and won't let it typecheck until you get it right. The Barendregt convention there means you just have to think real hard, and it'll typecheck no matter what you do! In general my experience is that using `bound`forces you to think about names when and only when something tricky can go wrong. However, when you need a 'typed bound' so that your syntax tree is a GADT, etc. I find that things get complicated fast, just because Hskell is bad at working 'one index up' in `k -&gt; *` with our current typeclasses. In practice unless your language is a crippled subset of Haskell it is hard to do that anyways though. Here's the bit of code in Haskell for dealing with typechecking Ermine binding groups: https://github.com/ermine-language/ermine/blob/master/src/Ermine/Inference/Type.hs#L201 has code for extracting strongly connected components worth of statements from a list of implicit and explicit bindings. and once you've identified the layering of hte binding groups in topological order, https://github.com/ermine-language/ermine/blob/master/src/Ermine/Inference/Type.hs#L109 goes through and figures out their types. This is the hairiest type checking situation in that codebase.
And how to read backwards.
Experience with SML shows that having a standard mathematical definition of a programming language is immensily useful as a point of reference for implementors, as a base on top of which we can build new mathematical theories, and prove theorems about the language, and so on. It is symptomatic of Haskell community to come up with reasons *against* mathematical precision, while at the same time they profess allegiance to category theory, but practice it only as long as it is fun to code.
The existing bindings do a have a serious performance bug regarding concurrency. A PR has existed for a year.
Here's an applicative idiom in the wild: std r = (\s ss -&gt; sqrt (ss - s**2)) &lt;$&gt; ma r &lt;*&gt; sqma r The usual standard deviation calculation, except adjust the mean term. It leads to highly dense, readable code, that's easy to refactor. Often combines well with zipWiths and &lt;$&gt;, to do array maths that you can do in your head.
Is this compatible with credstash? Can I use the same DB for both, and can I create a secret with one and retrieve it with the other? Besides Haskell vs. Python, and possibly compatibility as above, are there any other differences between this and credstash?
This is dope! I spent a brief few months with JavaScript as a first language before coming to Haskell and it was a nice exercise thinking of how I'd implement these as well. Still a beginner in both languages, but I love seeing essays like this as they help me understand what's fascinating about Haskell. Sometimes, because I'm learning Haskell without expertise in any other languages, I don't understand how different things are in this language or why that's useful / cool. Hope to read more :) +1
https://github.com/bos/mysql/pull/15
From my testing, FFI version has seriously bug under multiple threaded environment, it segment fault a lot. The mysql package on hackage is also not installable on ghc 8, there is a pr solved it though, So I had to include the patch version into my benchmark by hand.
[Stack](https://haskellstack.org/) irons out these concerns, by design. I've had really good results with colleagues working on macbooks while I was running Debian; I basically gave them the git URL, and told them to 1) install stack, 2) clone the git repo, 3) `stack install`, and it worked without a hiccup. And because stack pins dependency versions down, you can also rely on things still working in the (near) future, which should answer part 2. If you don't want to rely on stack, using cabal sandboxes and a hand-written cabal.config that pins down dependency versions serves the same purpose, though with a lot more manual labor. Alternatively, the brute force approach is to provide compiled binaries along with the source code (although this may mean that you will have to compile for multiple platforms), or, if your project is a web application, just give access to a live demo install.
Any resources out there for building an intuition for Day Convolutions? I am coming from a traditional undergrad CS background and enthusiast understanding of the Haskell type class hierarchy.
Cool, finally seriously haskell user in china.
&gt; However, cabal needs the package name. It took me an excessively long to find out that the package name [...] Since you said that, it looks like you've got a workflow problem over there. It seems that you are doing this: (Paste code into fantastic-integration-tool.hs) cabal install cubicspline integration # Here's your problem I suppose ghc -o fantastic-integration-tool fantastic-integration-tool.hs ./fantastic-integration-tool However, you *should* be using Cabal for dependency management too. **You should create a proper Cabal package for your program**, so that users can just git clone https://path.to/your/repo.git cd repo cabal install --only-dependencies cabal build cabal run fantastic-integration-tool cabal install # If you want it installed to ~/.local/bin **A stack project usually contains a Cabal package**, and what users do with `stack` is basically a front-end thing. Given that you've written a stack project too, users can do this, which will even get GHC if it's not there yet (Install stack) cd fantastic-integration-tool stack setup stack build stack install # If you want it installed to ~/.local/bin
The javascript blocks don't display on firefox 46.01. The last relevant looking things in console: NetworkError: A network error occurred. RemoteAddonsParent.jsm:762:0 NetworkError: A network error occurred. &lt;unknown&gt;
This makes sense, and seems like the route I'll probably end up going with my next Haskell application. My main point here though - this is definitely a bit more work then just a plain "git push" deployment like one can do with Heroku. What your describing is fairly simple, but things like that are generally quite a bit of work (comparatively) to get set up just right and maintain, in my experience. 
This is SO helpful! PLEASE PLEASE PLEASE do more articles like this!!!
The git push setup for Ruby on Heroku is rather more complicated behind the scenes than it appears from the outside (which you may already know). They hide that complexity well. Having been responsible for a high traffic Ruby/Rails site before, I found the deployment story for Rails more complicated than what it is for Haskell, getting the various gems installed and needing to do it on the server itself, rather than a nice self contained binary. On the scale of things, in my personal experience, Ruby is towards the more complicated end, while Haskell is somewhat similar to Java and Scala, and possibly simpler.
I built a small example application using [airship](http://github.com/helium/airship) which is a Webmachine style framework written in Haskell. We're using it for all our REST api things at work and we're pretty happy with it. The example code doesn't actually use a RDBMs like Postgres but that'd be fairly simple to plug in using something like postgres-simple. [https://github.com/ambiata/airship-tutorial](https://github.com/ambiata/airship-tutorial)
The best intuition for Day convolution is as a generalization of `Applicative`. It can be argued that the most important function `Applicative` gives you is liftA2 :: Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c Day convolution is a generalization of this type. data Day f g c = forall a b. Day (a -&gt; b -&gt; c) (f a) (g b) There's a joke in the Haskell community that goes "a monad is just a monoid in the category of endofunctors, what's the big deal?" Well, Day convolution gives a different kind of "monoidal product" and its corresponding "monoid objects" are applicative functors. A "monoid object" has a multiplication and unit, (like Monoid), in this case class DayMonoid f where unit :: a -&gt; f a multiply :: Day f f c -&gt; f c The most important thing to notice is that `multiply` is equivalent to `liftA2`. liftA2 abc fa fb = multiply (Day abc fa fb)
Is there a story on Nix integration for Haskell dependencies yet? (Not system ones, Stack already does that.)
I second all of the people saying that you should package your software. But if you really do not want to use Cabal, then your best choice is to NOT use any external libraries: stick to only the basic libraries that ship with GHC.
Thank you, this is a most helpful reply. It can be wonderful getting pushed in the right direction when one is able to formulate only unspecific questions. It seems not too much effort and quite sensible to add a [sandbox](http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html) step to the second workflow. Since that would allow me to specify dependencies without worrying about the Cabal ecosystem of the local user. Without these worries, the cabal procedure could go into a Makefile or shell script. Am I on the right path here? (This appears to be also what /u/tdammers kindly suggested.) Side by side, I cannot see much advantage to *me* by using Stack. Getting GHC and cabal-install could be done by the system's package manager. (I suppose these are not only in arch's package manager a dependency of Stack in the first place.) I suppose it has its use in cases where the users cannot be expected not sudoers on *nixes. Other informational things I've stumbled over while reading for this: [Cabal is not a Package Manager](https://ivanmiljenovic.wordpress.com/2010/03/15/repeat-after-me-cabal-is-not-a-package-manager/) (not clear to me, how would I do without cabal? yet interesting. ps: someone seriously would like xml configuration files?! he must be joking.) [Creating a package](https://downloads.haskell.org/~ghc/7.0.4/docs/html/Cabal/authors.html) (with examples!) *edit:* highlited 'me' to make clear I am not speaking about a general case; added missing line-break.
Now that I know it, it's obvious. Perhaps I'm just a bit blind there, but identifying which is the package cabal-install needs was an obstacle. After all, the page I land upon was the module page, in this case: https://hackage.haskell.org/package/integration-0.2.1/docs/Numeric-Integration-TanhSinh.html From there one has to click the tiny *contents* link in the upper right corner to get to the main package page. I remember that I didn't notice much of that bar at all, and also looked long for a link to source code. However, this really is only an aside.
I was diving a bit into parser and parser combinator in ELM languge(functional programming for front end application) so I have asked the following question about parser [here](https://www.reddit.com/r/haskell/comments/4v5qi6/elmhaskell_understand_parser_combinator/) and I got an answer that explains parser I will cote the answer part &gt;Which, in a nutshell, means that our Parser type is in fact a Monad, and the andThen combinator aligns with &gt;&gt;, which also gives us do notation for free, as well as applicative syntax, so you can write stuff like parenthesizedExpression = openingParens *&gt; expression &lt;* closingParens (which parses an opening parens, expression, and closing parens, returning only the expression itself). I have understand the parser part but the applicative syntax makes me confused
do you mean that parser is a functor also ? as I have understand that parser is a monad!!
[A tutorial for servant + persistent](http://www.parsonsmatt.org/2016/07/08/servant-persistent_updated.html)
Yep, I'm at my third attempt to install, learn, and use Haskell — this time successfully. A clean Stack environment is what's making it work. Stack actually makes for a best-in-class experience. It even beats out Ruby, which to me has excellent tooling. Through the single `stack` command, I ... * create new projects * install packages "globally" * compile a project (which also installs packages) * run a project's executable * run the ghci REPL with the Project's dependencies available * install binaries And the commands are all simple and intuitive. 
Comment parsing munged that definition rather badly in your quote. Here it is again, for clarity: parenthesizedExpression expression = openingParen *&gt; expression &lt;* closingParen Some of your confusion is caused by a small misuse of terminology: this isn't "applicative syntax", it's just normal Haskell syntax. `(*&gt;)` and `(&lt;*)` are normal, user-defined Haskell operators like `(+)` or `(==)`. `(*&gt;)` is equivalent to `(&gt;&gt;)`. It sequences two actions and keeps the result of the second one. `(&lt;*)` sequences two actions and keeps the result of the *first* one. The arrow points to the action whose result is kept. The result of the other action is discarded. Translating them into do-notation equivalents might be instructive: first *&gt; second = do first value &lt;- second return value -- *1* first &lt;* second = do value &lt;- first second return value The example parses an opening paren and throws the result away, parses an inner expression and holds on to the result, parses a closing paren and throws the result away, and then returns the result of the inner expression as the result of the entire action. In do notation, this is equivalent to do openingParen value &lt;- expression closingParen return value --- 1. The definition of `(*&gt;)` could really just be first *&gt; second = do first second but I've added the return to make the symmetry with `(&lt;*)` more apparent.
Because with 10M random integers from 0-999, with lists I run out of memory and I specifically need a parser for this sort of problem. In fact, the next best immutable solution that I found, using a [`persistent-vector`](https://hackage.haskell.org/package/persistent-vector) evaluated in WHNF is still 5x worse than boxed `Vector`s. From a performance perspective, lists aren't that good at growing, they are quite horrible in fact. Roughly, they are about as efficient as a `Vector` with only two elements max allowed. The reason why they are used so ubiquitously in Haskell is because the language gives them a first rate syntax. This is actually a huge pity as lists could be replaced with immutable arrays in more than 95% of use cases and it would make the language much faster overall. I would add the `[||]` array syntax and having an intrinsic array type a core part of the language to my wish list for Haskell definitely. Be it `fold`,`map` or `filter`, they work the same regardless of the structure. And well, the reason why it was so painful is because it took me a while to realize that I actually needed an modified `unfoldr` that returned state, which was exacerbated by the fact that Attoparsec uses continuation passing for control flow internally making it difficult to connect the two ideas as I was thinking how to adapt the library to use `Vector`s. And then I had no idea how to use the ST monad to make the function.
Yes, but if that's frequent enough, what's the benefit? In my mind, I kind of have a "code complexity budget" that I spend on statically preventing certain classes of errors. While I have a decent grasp on the kind of errors that systems like `bound` prevent, I still don't have a good intuition for how they affect the complexity of the code. I am particularly concerned about what design constraints they might force - the hoops you have to go through when using HOAS is a good example of such a situation.
i suspect its more that they coerce things to be truthy or falsey rather than raise a runtime error
yes a functor is defined as having fmap (with the appropirate signature) where fmap id = id
By definition, a Functor is any type for which `fmap` can be defined with the given signature. Some clarifications: "any type" - well, the signature of `fmap` only makes sense for a type like `Maybe`, where to use it you need to provide another type, as in "`Maybe Int`" or "`Maybe String`" or whatever. So not really *any* type. In Haskell this is called a "type of kind * -&gt; *". In other languages this kind of type is sometimes not called a type at all; it is a "template type", or a "generic type", or a "higher-order type", or whatever. "can be defined" - we mean "can be defined in a natural and meaningful way". You can always define `fmap = undefined`, but we're not talking about that kind of thing.
thank you, very useful answer 
Could you share your benchmark code? Consing an element onto a list should be fast. It's a single allocation of three words: one for the constructor tag and one each for the two pointers inside the constructor. That means for a list of 10M 64-bit integers you should see a memory footprint of ~320MB, with no garbage beyond that generated by the actual parser (if you're doing it right). Nothing to sniff at but certainly small enough to fit in memory on a laptop. Now, of course a growable array in `ST` would be more efficient because it allows you pack the integers into a contiguous block of memory, but you pay all the usual costs of imperative code. Preallocating an array with spare capacity, mutating it to add elements, and doubling it when it fills up is a serious chore compared to the simplicity of just consing an element onto a list. If you really need serious performance then this could be worth it, but you should at least make sure you've exhausted all the purely functional options first.
Here is the [list example](https://github.com/mrakgr/futhark/blob/parser_attempts/list_parser.hs). Here is the [F# script](https://github.com/mrakgr/futhark/blob/parser_attempts/generate.fsx) I used to generate 10M random integers, though I do not think it would be difficult to write one in Haskell. Actually, now that you put it that way, what you just said makes perfect sense to me. So then the reason why it blows up cannot be anything other than the `many` function. The `many` function in Attoparsec is defined exactly the same as in the base library. many v = many_v where many_v = some_v &lt;|&gt; pure [] some_v = (:) &lt;$&gt; v &lt;*&gt; many_v {-# INLINE many #-} The above has to be the reason why it is dog slow. That having said, I have no idea how I would modify the `many` function in Attoparsec as it uses continuations for control flow. Edit: Of course, I tested it with `many'` as well which forces it to evaluate strictly the first argument `v`.
Sometimes `A` stands for `Arrow`. In fact, most of the instances of it that I know of offhand are `Arrow` not `Applicative`, though that may just indicate that I don't get out enough.
GHC uses [non-shadowing names](http://research.microsoft.com/en-us/um/people/simonpj/Papers/inlining/inline-jfp.ps.gz) rather than unique names for substitution. As soon as you have any sort of substitution, unique names don't work well (as explained e. g. in the link above). I think the best solution is to use abstract machine representations/closures for substitutions, for example as in [this paper](http://www.cse.chalmers.se/~bengt/papers/GKminiTT.pdf). It doesn't have variable renaming (and doesn't even necessarily need unique name supply) and it's massively faster than any syntactic substitution method (including `bound`). 
The library supports OS X now too. Please test it if you are interested in it.
Yes we still use ES architecture and quite happy about it :). Sure, happy to discuss, thanks!
[removed]
[removed]
I would say that if you want to write a Haskell program that attacks its own runtime system, there's nothing stopping you. It is not a security barrier in that sense. You might be interested in Safe Haskell but even that's not a guarantee. Are there any inherent weaknesses in the RTS that an external attacker could take advantage of, even if the Haskell program was otherwise correct? Not that I know of. Does Haskell generally help with correctness? It sure does. 
I'm really glad Stack is working for you, and that you're successfully using Haskell now. This comment is awesome, thank you for the positive feedback!
A package management system is the ideal way to distribute binaries. Have your users `sudo apt-get install cabal-install` or `brew install cabal-install` or `choco install cabal` or `emerge cabal-install`, there are like a dozen supported OS's for getting Cabal installed quite easily. Then just submit you package to Hackage and ask your users to `cabal install yourpackagehere`.
An easy way to subvert a program X would be to put code into a library that X uses and does bad things only when X calls it;ala Stuxnet. A Haskell program is no more vulnerable than a compiled C program. I don't see great vulnerability with the RTS interface. But any time any program invokes a library bad things can happen; to stop this you have to find a way to ensure the integrity of the library. A dynamic link to a library is even more horrendous - there is no way to proof it is the same library used during compile and testing. If you want to do bad things; subvert popular dynamically linked libraries.
&gt; You are dramatically overestimating the security of the linux kernel. I wouldn't try to estimate the security of the Linux kernel; I don't know enough about it. That's why I'm asking here instead. &gt; Lots of eyes doesn't actually do anything when those eyes don't know what they are looking for. Plenty of eyes are looking and have been looking at the Linux kernel source code with the intent of finding exploitable bugs. Linux kernel exploits pay good money on the black market, so I can promise you that the number of eyes having looked at the Linux kernel, after exploits, is many orders of magnitudes higher than for the GHC RTS. Of course, this doesn't mean that the Linux kernel is more secure than the GHC RTS. It just means that, for the Linux kernel, all the low hanging fruit, exploit-wise, has been picked, such that you need to make a substantial effort to find a new exploit. The same is not the case for GHC. It's primarily the low-hanging fruit I'm worried about. Most systems can be subverted if you spend enough money on it. My goal is just to ensure that the cost of a GHC exploit is reasonably high.
As far as I know, you are correct that there has been less scrutiny of the security of GHC's RTS than the Linux kernel. However, if you choose to write your software in Haskell---and there are good reasons to do so---it might be better to have to trust just Xen and the HaLVM, then Xen, Linux, and GHC's RTS. In the end, it all depends on the context: a Linux system is much more complex, and you need to get its full configuration right to get security but, of course, it also provides a lot more functionality than a HaLVM instance.
When thinking about GHC vs C from a security perspective, there's a couple things I think about: 1. Haskell has this lovely type system and type safety guarantee that does away with all sorts of silly mistakes we make in C. 1. The fact that Haskell is higher level means that we have significantly fewer lines of code, so that when we multiply out our defect rates, we're likely to find fewer bugs per program. **[citation needed]** 1. The fact that Haskell is lazy by default makes timing attacks harder to reason about and protect against. 1. No one has ever done an analysis of whether the execution model used by GHC makes it more or less easy to form ROP (or similar control flow) attacks. On the one hand, it's weird enough that it probably surprises people, on the other hand, there's a lot of opportunities for potentially-exploitable indirect jumps. There are probably some other points to think about, as well. To some extent, right now, GHC provides some protection simply based on its obscurity. At least for non-targeted attacks, your adversaries going to look for software with wide deployment, and there aren't that many widely-deployed Haskell programs. So it doesn't make sense for an attacker to spend all that much time finding flaws in the GHC RTS. With regard to the HaLVM, you mention most of the points I mention in HaLVM and security talks, with two exceptions: 1. The HaLVM makes aggressive use of GHC's "-split-objs" feature, and aggressively pushes as much into Haskell as possible. What this means is that we end up throwing out any code that isn't used in the final binary (to a first approximation), down to the level of thunks, I believe. So it's not just that if you don't use your disk driver, it won't be compiled in, it's that if you don't use the write function of the disk driver, *that* won't be compiled in. The end result of this is (a) a high degree of variance from HaLVM to HaLVM, hopefully meaning that an exploit against one won't work on a different one, and (b) a great deal of limitation in what services are easily available to the attacker once they get in. 1. I dispute your implication that drivers don't matter. :) Drivers are where the bugs are, and all the drivers in Linux are written in C. If you happen to use very well-distributed hardware then you're right, there's been a lot of testing on it, but I wouldn't count on it. So when comparing the HaLVM and Linux, I do include drivers, and point out that the choice is between a much smaller but relatively untested code base versus a much larger and better-tested one. With regard to your other questions, I don't know of any documented *remote* attacks against Haskell programs, although I'd assume there are weaknesses. Besides obvious mis-use of C APIs, there's also quite a bit of Storable(..) strewn around the code base, and any time you're pulling data off the open wire is a chance for catastrophic failure. So I assume there are problems, just not weaponized ones.
If I could ask for something it would be to spacemacsify it. It's a more featureful although opinionated base. I think it makes a better default.
IMHO it's just easier to add an `ann` paramater to ast nodes, whether they are `Fix (ExprF ann)` or `Expr ann`. You can always plug `()` there, if you don't want annotations. And almost always you want some, or you don't care. --- FWIW, unfortunately machinery in `recursion-schemes` doesn't scale to support bifunctors, it would be cool to have `Fix ExprF ann ~ Expr ann`, see https://github.com/ekmett/recursion-schemes/pull/23 for discussion. ... 
Thanks. I'm not sure I like the fixpoint idea but I'll keep this in mind.
Thank you. This looks good, albeit a bit ad-hoc and it doesn't seem right to include metadata in the AST type. Do you think there's a way to abstract that away?
The mentioned type checker plugin can be found on hackage: http://hackage.haskell.org/package/ghc-typelits-knownnat
&gt; it doesn't seem right to include metadata in the AST type I am not sure what you mean. You need a location data in your AST. How would you not include it? I guess you could do something like date Ast a = Number Int | Add Ast Ast | MetaData a Ast type AstWithLocationInfo = Ast LocationInfo type AstWithType = Ast Type But it isn't a big difference. 
You can use free applicatives, you can get away with few well understood extensions (`GADTs`, `RankNTypes`). E.g. `optparse-applicative` has something like that in its core. data ParserF a = ParserF { pFieldName :: FieldName , pMatch :: String -&gt; Maybe a } deriving Functor type Parser = Ap ParserF runParser :: Parser a -&gt; [(String, String)] -- ^ list of tokens: (fieldname, data) fields -&gt; Either String a runParser (Pure x) [] = Right x runParser (Pure _) ts = Left $ "Unconsumed input " ++ show ts runParser p@(Ap _ _) [] = Left $ "End-of-input, missing-fields: " ++ intercalate ", " (collectFields p) -- Find a field parser in `p`, use it, and then proceed with next token runParser p ((name, input) : rest) = go p &gt;&gt;= flip runParser rest where -- Here we use cool feature: polymorphic recursion! go :: forall a. Parser a -&gt; Either String (Parser a) go (Pure _) = Left $ "No parser for field: " ++ name go (Ap f@(ParserF name' fp) x) | name == name' = case fp input of Nothing -&gt; Left $ "Cannot parse field: " ++ name ++ " from " ++ input Just value -&gt; Right $ flip id &lt;$&gt; pure value &lt;*&gt; x | otherwise = -- recurse! Ap f &lt;$&gt; go x For full example see: https://gist.github.com/phadej/41d41c624a8586852ed3e1940fdc1b06
... or just build `HashMap FieldName Token` and lookup fields when done (found end of record token). Simple approach is neat when it fits!
I would also like to mention that you can mechanically transform the example into its do notation equivalent by equational reasoning using the definitions I've given for `(*&gt;)` and `(&lt;*)` and the Monad laws. 
This micro-service specific one might be helpful depending on which Scala libraries you're looking at in comparison. https://github.com/k-bx/owlcloud I also think this is a nice extension of Servant used with an Elm frontend: https://github.com/mattjbray/servant-elm-example-app
Oops, thanks for notifying me!
hetero-dict is also an option.