Fixed. Thanks!
Also, don't forget that Haskell has excellent support for building ASTs: the [free monad](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html).
"in the style of aeson" means that the API is in the style of aeson i.e. an approach to parsing that uses type classes to map a generic representation (i.e. `Record` and `NamedRecord`) into user defined data types. Contrast this with other CSV libraries on Hackage, which typically just return `[[ByteString]]` and let you do deal with converting the raw bytes into something useful. The intention with that remark is that if you've used aeson, you know what to expect from this API. aeson is not the only library that works this way. Others include: json, binary, mysql-simple, and probably a bunch more.
Ooooh, I see. Well, in that case you can definitely structure that using a `Pipe`, but not using `pipes-attoparsec`. If you can wait a month or so, this feature will be available in `pipes`. I already have a general parsing mechanism that lets you stream output, too, but I'm busy merging `pipes` with the `pipes-core` suite before I release. If you ever need this behavior before then, I can show you how to set it up in your own code by hand, because it's not that difficult.
As I wrote in my reply to aristidb you can use the parsers exported from `Data.Csv.Parser` to do streaming parsing already today. When I add a more convient streaming API it will have per record failure information.
I used this GADT compose and type-check the combinators data T t where S :: T ((a -&gt; b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)) K :: T (a -&gt; b -&gt; a) I :: T (a -&gt; a) C :: T ((a -&gt; b -&gt; c) -&gt; (b -&gt; a -&gt; c)) B :: T ((b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)) (:$) :: T (a -&gt; b) -&gt; T a -&gt; T b The letters in "Hello world" are embedded in the program using Church encoding type Church a = (a -&gt; a) -&gt; (a -&gt; a) encodeCharT :: Char -&gt; T (Church a) encodeCharT '\n' = C :$ K encodeCharT c = S :$ B :$ encodeCharT (pred c) So that '\n' becomes the function \f x -&gt; x and for example '\r' (ascii code 13) is encoded as the function \f x -&gt; f (f (f x)) The whole string is encoded as a function that takes in a folding step, the initial accumulator and the two parameters needed to decode characters, so the type of the hello function is hello :: (a -&gt; b -&gt; b) -&gt; b -&gt; (a -&gt; a) -&gt; a -&gt; b The part that folds over a single letter I assembled by hand and then I automatically generated the code to fold over all letters by repeating that piece. encodeStringT :: String -&gt; T ((b -&gt; a -&gt; a) -&gt; a -&gt; (b -&gt; b) -&gt; b -&gt; a) encodeStringT "" = K :$ (S :$ (K :$ (S :$ (K :$ K))) :$ (S :$ (K :$ K) :$ I)) encodeStringT (c:cs) = S :$ (S :$ (K :$ S) :$ (S :$ (K :$ K) :$ (S :$ (K :$ S) :$ (S :$ (K :$ (S :$ (K :$ S))) :$ (S :$ (S :$ (K :$ S) :$ (S :$ (K :$ K) :$ (S :$ (K :$ B) :$ I))) :$ (K :$ a)))))) :$ b where a = encodeCharT c b = encodeStringT cs The T-type has a show instance which outputs the final source code. 
Thanks for this explanation, sounds good. You could also consider deriving instances automatically using [GHC generics](http://www.haskell.org/haskellwiki/GHC.Generics)
Ok, thanks. That clears things up :-) by the way, it looks like your FromField/ToField stuff is almost strictly generalized by goerzen's convertible package? http://hackage.haskell.org/cgi-bin/hackage-scripts/package/convertible
In past years he has discussed extensively the approaches they have taken internally to software quality. Most he has discussed static analysis and restricting developers to a safer subset of c++. He has said that while he would like to use a functional language like haskell or OCaml it doesn't seem practical to retrain an entire company and throw out the codebase and all the experience. He has also noted that Haskells laziness makes it much more difficult to reason about performance.
Is anyone planning on updating the look'n'feel to match the "Ocean" style for Haddock?
Have you profiled your program? You need to compile with `-prof -auto-all`, and the run with `+RTS -p -RTS`. See [the chapter on profiling](http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/profiling.html) in the GHC user's guide for more info.
For speed, you might try using Data.IntMap, consistently indexing into a list is pretty slow.
Nice, I hadn't known about IntMap. Unfortunately, while it probably did speed up my program, it looks like it's still too slow for CodeChef. =(
Hmm, I just thought of this, but I had a ton of speed issues with the generic readLine I think it has something to do with converting to Strings. Try using Data.Text instead, should be a ton faster. Check out http://hackage.haskell.org/package/text. For the alternative to getLine, you should find it in Data.Text.IO and then the number conversions are in Data.Text.Read.
Nice!
Add -rtsopts when compiling, as suggested. (It's a security feature that was added when someone noticed +RTS is a vulnerability for Haskell CGI programs.)
your original solutions runs in O(n^2). you need to use ByteString to handle IO and STArray to solve this problem.
Worse than Brainf*ck...
https://gist.github.com/3456868 passes the time limit, but says wrong answer. I may have to revisit my logic.
Slightly offtopic: Are you benchmarking your work to make sure it's not performance-regressing when compared with conduits?
Answering that requires explaining what my plan is for the next release. So basically, I've managed to decompose the conduit features (and several other features not in conduit) into composable extensions which I call pipe transformers (or in the more general case, pipe morphisms). You can see an example of this in [this hpaste](http://hpaste.org/73647) where I implement a parsing just by layering two extensions, one which provides pipe-local state and the other which provides error handling. These extensions automatically derive a correct category instance, making it much easier for me to prove the laws for each one individually and just layer them to get whatever features I desire without having to reprove the laws from scratch. Besides making my proofs easier, the other important advantage of this approach is performance, for two reasons. The first reason it performs faster is that you only pay for the extensions you actually use. Using the monad transformer analogy, conduit would be like one giant hard-coded monad transformer stack with every monad transformer baked in. Pipes will be like monad transformers proper, where you pick and choose which ones to incorporate into your stack. So, for example, if all you need is parsing, you only pay for parsing. However, I still plan on duplicating the entire conduit stack and seeing how that compares performance-wise to see if it can be used as a drop-in replacement for conduit-based libraries or not. Right now I'm still writing up the most demanding extension which is finalization, so I can't say for sure how that will pan out. I can tell you write now that pipes with the simple extensions still out-perform conduit by a decent margin, however my guess is that once I add finalization that lead will vanish and might even be slower, so I can't really say yet. The second optimization opportunity is far more important, though, which is that each extension is defined as a functor from the lower pipe to the higher extended pipe and the functor laws are correct by construction for each extension. I'll use the same example of a parsing-enriched pipe. There are four categories at play, which in order of fastest (least features) to slowest (most features) are: * Haskell functions * Kleisli arrows * Pipes * Parsing-enriched pipes I can define a functor from each of those categories to the next one: (return .) :: (a -&gt; b) -&gt; (a -&gt; m b) mapMP :: (a -&gt; m b) -&gt; Pipe m a b r liftP :: Pipe m a b r -&gt; ParseP (Pipe m) a b r Or you can skip the Kleisli step and jump straight from Haskell functions to Pipes using `pipe`: pipe :: (a -&gt; b) -&gt; Pipe m a b r The importance of this is that for any pipe-line, you can use the functor laws to fuse operations in the faster categories together to get optimal performance. Again, using the monad transformer analogy, this would be exactly analogous to using the monad transformer laws to join steps in the base monad together to avoid binds in the higher monad: do x &lt;- lift m1 lift $ f x =&gt; lift $ do x &lt;- m1 f x The user can do this by hand or using rewrite rules (and I plan on writing up all these rewrite rules, but not in the next release). In some cases, you can even fuse away the entire pipe-line machinery and rewrite it into the hand-written loop. The latter trick is where I expect to make the biggest speed gains against conduits. Anyway, I will do a thorough benchmarking when I'm ready to go on the offensive against conduit, but right now my primary concern is releasing these extensions and merging with `pipes-core` to provide a full standard-library of pipes utilities. Edit: Oh, and it will all be implemented using ordinary monads now and not indexed monads, so that will be a huge speed win, too.
The changelog feature is nice. Here's to hoping more packages will include changelogs for sake of Hackage2 (simply list it in data-files or extra-source-files). Maybe we could even standardize on a format and have the changelogs displayed prettily by version?
This sounds great, and I applaud your work. However, I am *extremely* skeptical of the claims that it should/will perform much better without benchmarks. Especially as Snoyman had put quite a bit of effort into optimizing/benchmarking his work. At least as of a while ago, monad transformers were known for not being well optimized by GHC and performing relatively badly. That might mean Snoyman's more monolithic/less composable approach can perform better than the "pay for what you use" approach.
You know what it really needs? Automatic password reset. Who knows how long it's been since I pushed out my toy package as a student, but emailing somebody to do that seems... quaint.
Duncan has most of the support for cabal ghci done I believe.
This means that you can specify loose bounds in the .cabal file and exact versions in the package environment file. See the wiki page for [package environments](http://hackage.haskell.org/trac/hackage/wiki/PackageEnvironments) for more details.
How about this? http://hackage.haskell.org/packages/archive/text-format/0.3.0.7/doc/html/Data-Text-Buildable.html
He hasn't committed it yet, only the GhcOptions refactoring part.
Sorry to post ancient artifact. I only read the date it was submitted to arXiv.
thanks! we've been avoiding renderPrimitive because it's out of date/deprecated/inefficient/something like that.
Indeed, a final product should use vector buffers. renderPrimitive is quite inefficient. But, the tutorial should give you the basic system and upgrading to vector buffer should be easier if most of other details work. I had difficulties to find an example of Haskell vector buffer on the web. May be some OpenGL Haskell game source on github exist. 
most of the examples i found utilize haskell opengl raw which is sad. i already have glfw so the next thing i'm looking for is how to get a lovely triangle rendering from vertex buffers, but alas i cannot :&lt;
Congrats on a, as I perceive it, focused and very fruitful GSoc project!
Yeah, increasing was a much more interesting question and headline. I am disappointed.
Step 1. Make your code a little bit more type-safe. Step 2. Find a bug immediately. Step 3. Profit!
Will the repository be put on [the Haskell Github organization](https://github.com/haskell)? There will be a lot more contribution.
&gt; Hence ∇M(T) = M(T), i.e. M(T) - M(T-1) = M(T), but here I am a bit stuck. Trying to solve this in a similar manner as before yields M(T-1) = 0, which seems bogus. 0 is certainly not a solution, since M(0) = 1. I think in this case we are actually not justified in subtracting from both sides, though I’d be hard-pressed to explain exactly why. It's because M(T) is aleph_0 for (finite) T &gt; 0. Doing integer algebra on the cardinality of infinite sets as if they were finite is a recipe for disaster. S(T) is finite (for finite T, of course), so algebra works fine on it.
i think we did that (got glfw-b), and unfortunately we're looking for the full 3D or life would be tidier, thanks!
Thanks!
Can you give some more examples? I do not see how this works for other monads. (Btw, [monoid actions are finally in a package on hackage](http://hackage.haskell.org/package/monoid-extras)!)
Ancient artifacts are great, I just wasnt sure I was interpreting things correctly.
I like your levels explanation. I pretty much got the definition, though. It's the knowledge of practical uses and monad theory that I miss. Lot's of packages to look through. That's for October.
When I started out, I read all kinds of descriptions, including ones like yours. Nothing really helped until I really started using a variety of monads in my code, and analyzed how some of them work in detail.
'Running' state is just the identity. If you have M = GF, where F -| G, then M -&gt; M = GF -&gt; GF ~ FGF -&gt; F = FM -&gt; F Because of the adjunction. Also, W = FG is a comonad, and we have extract: W -&gt; I, and if we map with G, we have GFG -&gt; G = MG -&gt; G. So there is always one of those, too. Since every monad is the composition of two adjoint functors in at least one way, the only question is whether you can find a pair of adjoint functors that doesn't look trivial, and for which the intermediate category is close enough to look like a Haskell function.
One of the definitions of F -| G is that there is a natural isomorphism: FA -&gt; B ~ A -&gt; GB If we set A = GFX, and B = FX, we get that there is a natural isomorphism: FGFX -&gt; FX ~ GFX -&gt; GFX There are other ways to get this, too, depending on the definition of adjunction you use. For instance, instead of taking FG -&gt; I and putting G on the left to get GFG -&gt; G, we can put an F on the right to get FGF -&gt; F.
Can someone explain the greater significance of this discovery ? I understand what's written above in technical terms but not sure where this is supposed to take me. Also in the first statement: run . fmap join = run . run does it follow that run = fmap join ?
Please explain the difference.
Indeed, this is part of the problem, too. Anyone who explains his intuition will have another explaining how misguided that explanation is, leaving the newbie questioning anyone's explanation and developing “there must be more to it” syndrome.
It is easy to implement run for the composition of two adjoint Hask-functors.
The functors do not have to be adjoint. The simplest example of that is the degenerate case of "running" using a constant functor. A more sophisticated example is that you can use a "bigger" monad than `m` as `f`.
Actually, it's a very practical way he's right, it just sounds more confusing than it really is. Here is a short explanation: a monoid is basically being able to combine things, so a slightly reduced version of what he says is "a monad is just a way to combine things in the category of endofunctors". Now the confusing thing is "the category of endofunctors". "endofunctor" is the name for every type constructor that is an instance of `Functor`. All endofunctors allow you to do stuff that can be interpreted as side-effects. For example, `State s` gives you access to global state. In other words, "category of endofunctors" basically means "effects", which means that we can reduce "monads are just monoids in the category of endofunctors" to "monads are just ways to combine effects". This intuition arguably misses a few outliers, but it works most of the time.
Ah, right. F.e. the continuation monad is `(-&gt; r)` composed with itself, so `f = (-&gt; r)`, but the intermediate category is `Hask^Op` so the arrow of `run` flips and you get `run :: (a -&gt; r) -&gt; (Cont r a -&gt; r)`. Nice!
&gt; Indeed, a final product should use vector buffers. renderPrimitive is quite inefficient. Actually it's not Haskell-specific, right? Even using raw C OpenGL you should avoid primitives and prefer vector buffers, shouldn't you?
How would GPipe relate to graphics-drawingcombinators? Would GPipe be the prefered way? Are there other available solutions?
Whoops, you're right.
Another solution -- as OpenGL is extremely imperative -- if you want to stick with it (and not use a higher-level solution as it's been suggested on this thread) would be to leave the OpenGL code in C (you can then benefit from all the resources that exist on the web re. OpenGL tricks) and use FFI (and tools like Data.Vector.Storable to pass data for instance), but I'd really like to have some input from people who had chosen this solution to realize a game (or similar) in Haskell...
Thanks! And agreed re: a cabal repl. See also https://github.com/haskell/cabal/pull/1014 .
I suspect you probably want a further condition: run . fmap (fmap k) = fmap k . run (the first and third `fmap`s are for `f` and the second one is for `m`). This is a compatibility condition from which you can deduce that `f` factors through the Kleisli category of `m`. If you define `lift k = run . fmap k` then `lift` is a functor (not a `Functor`!) from the Kleisli category of `m` to `f` which satisfies lift (h &lt;=&lt; k) = lift h . lift k lift return = id This makes it much clearer what's going on, in my opinion. In fact, many confusing things to do with monads and applicatives become much clearer when explained in terms of their `Category` instances. 
I believe that the condition you are proposing follows from parametricity.
Since we're nitpicking... it should be *assigning a unique natural to each structure* (though I'm not sure why say "structure" instead of simply "each element in a given set"). You can enumerate B = {false, true} by assigning 0 and 1 resp., but obviously you cannot assign a unique element of B to each natural.
It would be excellent if it did! Could you tell me how I can see that? I'm not familiar with that kind of thing yet.
Nice. I'd also thought that cabal should emit a warning when an install plan is not using the latest version of a package. Also, we should look into using more colour like this.
Basically, as far as I can see, your condition says that `run` is a [natural transformation](http://en.wikipedia.org/wiki/Natural_transformation) `f . m -&gt; f`. If that is the case then it follows from parametricity (as long as `seq` isn't used, technically).
Yes, you're right, that's exactly what it says. Very nice, thanks.
Yeah, that definition ignores finite data types, but "assigning a unique natural to each structure" means you can skip numbers.
My big thing is that I just figured out some part of using Parsec applicatively: eg. parseOptions :: Parsec String () Operation parseOptions = choice [setup, create, use, rename, delete, other] where setup = Setup &lt;$ (string "setup" *&gt; spaces *&gt; eof) create = Create &lt;$&gt; (string "create" *&gt; many1 space *&gt; many1 letter &lt;* spaces &lt;* eof) use = Use &lt;$&gt; (string "use" *&gt; many1 space *&gt; many1 letter &lt;* spaces &lt;* eof) rename = Rename &lt;$&gt; (string "rename" *&gt; many1 space *&gt; many1 letter &lt;* spaces) &lt;*&gt; (many1 letter &lt;* spaces &lt;* eof) delete = Delete &lt;$&gt; (string "delete" *&gt; many1 space *&gt; many1 letter &lt;* spaces &lt;* eof) other = pure NoOperation (data structure: ) data Operation = Setup | Create String | Use String | Rename String String | Delete String | NoOperation deriving (Show) and it allowed me to now do things like this: s &lt;- (intercalate " ") &lt;$&gt; getArgs I would not have figured that one out before this.
this could also work. i haven't seen options using this yet, but i haven't been looking. more commonly i've seen people just using openglraw, which seems to work pretty well even if it's abusing haskell with imperative-ness.
This is why I don't write monad tutorials. I just link people [here](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html).
The article is wishy-washy handwaving. It doesn't even characterize the distinction between "positive" and "negative" types, and it makes elementary mistakes. Laziness is not a property of types. Most of Robert Harper's posts are like this.
Recall that we have comonadic extract :: F (G a) -&gt; a Then run :: F (G (F a)) -&gt; F a run = extract 
That was fast.
He's saying that if F is the left adjoint of an adjunction that gives rise to the monad M, then we get run for free. The insinuation is that all the interesting runs are obtained in similar ways. He did the first instance so let me do the second: instance LMonadAct ((,) s) ((-&gt;) s) where run (s, r) = (s, r s) Suppose F -| G and F is itself a comonad and G a monad. Then FG -&gt; FFG -&gt; F where the first arrow is cojoin on F and the second fmap_F extract on FG. Said instance has F = product comonad, G = reader monad, GF = state monad, and FG = store comonad. 
&gt; Coinductive types (possibly infinite) are by their nature lazy while inductive types are naturally eager. Danger Will Robinson: If I weren't so perfectly sure of Harper's impartiality, I'd say that this is exactly the trap that the focusing work is designed to snare the unsuspecting in. See Rob Simmons' response to Dan Doel on this. In an ideal language you'd mark both eager and lazy types, instead of privileging one with mere whitespace. 
&gt; Coinductive types (possibly infinite) are by their nature lazy while inductive types are naturally eager. Except they aren't, as I pointed out in the comments. It's (conceivably) useful to have a type of inductive, finite lists to reason about by ordinary set-like induction, and to fold over without worrying about infinite lists. But it can also be useful to have those same lists be lazily evaluated, because the lists are large, but can be produced and consumed incrementally, despite being finite. In a total language, for instance, you can have this, because evaluation strategy has no bearing on the inhabitants of types. I don't know if that is a necessary condition, but it is sufficient. And the only response I got was, "just use possibly infinite lists if you want lazy evaluation," which is a complete non-answer. Recursive coinductive types cannot, of course, be eager, due to the possibility of infinite structures. And sometimes during dependent type checking, we must 'compare normal forms' of inductive types, except that doesn't imply anything about how we normalize, and really one can do something like evaluating to weak head normal form, compare constructors, and then continue. And all of the above doesn't require anything about how _runtime_ evaluation relates to _compile time_ evaluation. Presumably Harper likes the focusing stuff because it fits his "ML gets it all right and Haskell gets it all wrong" sort of narrative, but the reality is more like "ML makes a different poor trade-off from the ideal than Haskell does."
I would like to see the proof of the first one.
This is a temporary UI. We'll most likely change it to be a flag (to configure) in the future.
That sounds more like an implementation than a proof that it follows the laws.
&gt; Except they aren't, as I pointed out in the comments. And well done. It's essential to not conflate the oppositions: inductive-vs-cointuctive, positive-vs-negative, eager-vs-lazy, strict-vs-nonstrict. Life would be so much easier if we could conflate them, but they are not the same and must not be treated the same. Agda once fell into the trap of trying to conflate (co)inductive with positive-vs-negative, but they realized the error of their ways. Unfortunately, Bob seems not to have been following that work.
However, a list can be specified both inductively and coinductively, so there is no reason that the existence of an eager list must exclude a lazy list. Presumably you would distinguish those two types of lists in a language that distinguished inductive and coinductive types. I'm definitely NOT an expert at this sort of thing, but as a casual observer it makes superficial sense to me that when you want eager behavior you need only prove that the type is inductive, and when you want lazy behavior you need only prove that the type is coinductive.
 I = SKK Also: I = SKS 
Really, `I = SKy` for any choice of `y`: SKyx = Kx(yx) = x
&gt; However, as a personal preference, I like the Haskell typesystem better. I almost never notice the typesystem. Yeah, not really buying that. 
It will be great if there is a online judge designed for Haskellers. So people can use it as a learning platform. People who solve the problem can see other people's solutions. I would make one if only I know enough Haskell + yesod to produce such a thing. I see a potential in making this a start up, it will help the adaptation of Haskell to the industry.
Wow, I'd never heard about GPipe. [For the lazy](http://www.haskell.org/haskellwiki/GPipe/Tutorial)
Or you could just focus one the real issue at hand, the monad reader is "resting" for long periods of time to return with a few articles.
https://gist.github.com/3486644
I don't disapprove of comparisons. But this is kind of fluff and inaccurate. Downvoting.
Without fromJust, there's no way to get the value inside the Maybe. You can't just do a case statement. But even elimMaybe that takes a default nothing value is useless without the rest of the tools. The other reason that Maybe is useless in C++ is because it already exists. Therefore, reimplementing it is stupid. Instead, we should skip that implement fmap and &gt;&gt;=. 
That's why i'm making fun of it at the begining of the article. std::unique_ptr is what i recomend. You also have to keep in mind that the default-constructed x may be a valid value, so to say that a Maybe holding Nothing would be a lie--the implementation doesn't preserve the concept Just x != Nothing. 
You could define finite lazy lists in a dependent version of Rob's system: codata SizedLazyList {n : Nat} a where Nil : SizedLazyList O a Cons : a -&gt; Thunk (SizedLazyList n a) -&gt; SizedLazyList (S n) a type FiniteLazyList a = Sigma {n : Nat}. Thunk (SizedLazyList n a) (My intent in writing `{n : Nat}` is to suggest that an irrelevant/erasable/squashed type could be considered here, in the hope of keeping the usual economical memory representation. Not sure if it works, though.). I furthermore suspect that you could define a least-fixed-point of the functor `F(A)(X) = Thunk(1 + A * X)` and that, in a total language¹, such a type would not accept infinite lists. This is how I retrospectively understand Harper's rather mysterious answer to your comment: "As soon as you admit partiality, the situation changes.". ¹: the dependent size index in the above version could be understood as a technique to locally enforce totality, in some wishy-washy sense
&gt; positive (the bird is in hand) and negative (it's behind a barrier, viz. computation) Just putting it like this suddenly makes it much more clear to me what Harper is talking about. Thank you.
Perhaps he meant that he almost never gets annoyed at having to jump through hoops just for the sake of the typesystem as in some other languages.
&gt; (My intent in writing {n : Nat} is to suggest that an irrelevant/erasable/squashed type could be considered here, in the hope of keeping the usual economical memory representation. Not sure if it works, though.). I'm skeptical that it does. If you want to define a fold on such lists, in a total language it will be justified by recursion on the natural number. In a partial language, the only thing keeping the following definitions from working (that I can see): cons x xs = (fst xs + 1, delay (Cons x (force (snd xs))) foo = cons 1 foo is the eager evaluation of the first component, which will diverge. But that won't happen if it's gone by runtime, unless perhaps we use it in a type somewhere, causing type checking to diverge (if then, even), or if various values just get aggressively evaluated at compile time before erasure. Or if we don't really erase it, but turn it into a unit token that we evaluate all over the place. You could probably do it in Haskell that last way: data FiniteList a = FL !() [a] cons x ~(FL tok xs) = tok `seq` FL tok (x:xs) ...
You're doing it wrong. If I were you (and I was, I did a bit but left it) I'd take OpenGLRaw, wrap it for my needs for instance make code that creates buffers, puts data in it. There are several things that can use such managers: textures, vertex buffers, fonts and so on. gDebugger is very useful. As far as I remember Haskell "OpenGL" package is quite useless, it has too many constructors, too many types and doesn't achieve anything in little code. There are already existing libraries like FTGLand such but they don't integrate with buffer/shader-based code properly or at all. GPipe and things like that are research toys, in best case you'll make small demo and will have enormous problems with integrating it with anything. GLFW-b and OpenGLRaw are good starting points, stb-truetype seems quite easy to integrate (if you have your texture manager) with a bit of work. "vect" library is very good. It's enough make Haskell more preferable to bare C. That was all from perspective of a person that knows a bit of Haskell already. Learning Haskell by writing OpenGL might be masochistic experience for your colleague.
I totally disagree. C/C++ tutorials are really easy to translate into Haskell. Functions that allocate are easy to use once you grasp alloca/temporary string allocation and such. You have to learn Haskell FFI but that's not that much of a pain to stop using Haskell. OpenGL thread issues can be solved by running code GL code from 1 specific thread. "runInBoundThread" and you're done.
I have some code that is almost working, you'll have to tweak few things to put data in vertex buffers and point to them from main loop: http://hpaste.org/73779 http://hpaste.org/73780 I'm not saying that this code is nice or anything. :)
Using `fromJust` on a `Maybe` is like dereferencing a pointer without checking if is is `null`. &gt; there's no way to get the value inside the `Maybe` This is true for nullable pointers, too. There's no safe way to get the value out because there might not be a value there. You handle a `Maybe` in Haskell the exact same way you handle a `null` pointer. You provide two code paths: one for when the `Maybe` is a `Just`, and one for when the `Maybe` is a `Nothing`: case myMaybe of Just a -&gt; ... -- Use the 'a' Nothing -&gt; ... -- Handle the missing value However, throwing an asynchronous `error` on a `Nothing` (i.e. what `fromJust` does) does not constitute a valid way to handle it. Instead you should either: a) Provide a default value b) Provide a default behavior in the absence of a value c) Propagate the error forward (using `fmap` or `(&gt;&gt;=)`) d) Transform it to a more descriptive error (by converting it to an `Either` and working in the `Either` monad instead)
It's more readable to you when it comes to learning what it means. Readability for long term use is completely different thing. I hate whenever someone writes if (x == true), it's idiotic waste of key presses, screen space and brain power to me. A lot of Haskell Prelude was written as not general enough instead of making it general and adding training wheel library on top of it. More generally what you have problems with are things that you're learning and you want to be able to comprehend them as easily and quickly as in C++. Sorry, that's different language, different abstractions and you won't be able to do it. TH and HaskellDB are different topic though.
And when one thinks about &lt;$&gt; or &lt;*&gt; one thinks about slightly more abstract idea than addition.
Gee thanks -- writing is how I'm going to earn my bread, so your upvote means a lot to me. 
&gt; I totally disagree. C/C++ tutorials are really easy to translate into Haskell. If you already know Haskell. And learning Haskell though OpenGL is a bad thing because of its imperativeness, as it has already been told on the thread. So either your partners learn Haskell _and then_ come back to code the OpenGL part (as I said, that's the solution if they want to learn Haskell), or they just stick to C and leave the binding through FFI to you. I've not recommended to stop using Haskell _for all_, I've told to stop using it for the irrelevant parts (the parts that will _remain_ imperative whatever language they're coded in).
That bait really caught me. He is obviously referring to Haskell, but the language itself is not lazy. It is *non-strict*, and there is a subtle but crucial difference between the two.
I'm curious to hear more details about the problems with garbage collection.
I'd be surprised if this stuff really can only work for call by value. Presumably it means it will not be able to be extended to full ML with reference cells, since once you have reference cells you can simulate call by need.
Okay, so that's an interesting feedback. Can you give more details concerning what were those projects and the troubles you ran into? So you think you cannot in the same project merge low-level C code (developped by people wont don't know about functional) and higher-level Haskell code?
It should work for strict data structures in haskell, I spent a bit of time mulling over this very topic of GC + stuff = cache optimality this summer
Forgive me if this is a silly question: what is Haskell-Type-Exts?
wow, someone is actually working on darcsden! That is so exciting! :) is dev activity happening on #darcs? also, it looks like you are using john macfarlanes highlighting library now? a year or two I hacked that into a local branch and it was MUCH faster (which you probably know). Like, exponential time vs linear... :). anyhow, excited for activity, and sorry for the strange place to comment.
Haskell *the language* has "non-strict semantics". However, the language specification **does not say** that you must "only evaluate when the value is needed". However, laziness *is* how GHC is implemented, which is the most commonly used compiler &amp; interpreter for Haskell. Implementations of Haskell are free to implement it "mostly" strictly, as long as they preserve *non-strict semantics*, meaning that a computation that turns out to be an infinite loop or an error should not cause an infinite loop or error if its value is never forced. Also of note, Haskell provides the `seq` primitive, which allows you to insert strictness wherever you want.
http://cleantypecheck.wordpress.com/2012/05/03/kickoff/
Let us introduce modalities to make eagerness/laziness explicit: * `!a` is the type `a` evaluated eagerly * `?a` is the type `a` evaluated lazily Without bothering to exemplify all the semantics of these modalities, consider the following list types for any arbitrary type `a`: * `μx. 1 + a * !x` --- least fixpoint + eager recursion = eager finite lists * `μx. 1 + a * ?x` --- least fixpoint + lazy recursion = lazy finite lists * `νx. 1 + a * !x` --- greatest fixpoint + eager recursion = eager finite lists * `νx. 1 + a * ?x` --- greatest fixpoint + lazy recursion = lazy semi-infinite lists You recognize the importance of distinguishing the first type from the fourth type, but you (and numerous others) fail to recognize the importance of distinguishing the first type from the second type. Just because I want a guarantee that my lists are of finite length (i.e., are inductively defined) does *not* mean that I want to eagerly evaluate the entirety of those lists. Part of what makes laziness great is the fact that we can define streaming algorithms etc. But streaming algorithms aren't restricted to working on coinductive (i.e., semi-infinite) datatypes. Streaming works for data just as well as it does for codata. Coinduction works for data just as well as it does for codata. Only induction is limited to operate on merely finite structures. The ideas of inductive-vs-coinductive and the ideas of eager-vs-lazy must be distinguished. The only reason why they are not completely unrelated is the peculiarity of the fact that the third type above turns out to describe the same set of values as the first type. This peculiarity allows us to get away with using greatest fixpoints everywhere in Haskell, because we can still make the crucial distinction between the first and fourth types (by faking it, since we actually distinguish between the third and fourth types). But, Haskell still fails to identify the second type. This is because in Haskell we've conflated the idea of lazy evaluation with the idea of partial evaluation. Just because something is lazy does not mean that divergence must be a possibility. Laziness just means not doing things right away. It has nothing to do with (co)induction.
I think it's more that mathematics is traditionally inductive. Laziness is one of those "weird" new things from computer science, like coinduction and bisimulation--- things that arise naturally as soon as you view abstract structures computationally rather than set-theoretically. Mathematicians have long been blinded by the foundationalistic biases of ZF/ZFC set-theory. Aczel ran into similar biases by daring to refute the axiom of foundation. Constructivists ran into similar biases by daring to question the set-theoretic interpretation of logical connectives. It's easier to acknowledge your enemy when you can safely push them out of your sphere (e.g., conflating laziness into coinduction, which is already enemy territory); it's harder when you're forced to acknowledge that these strange new ideas have been here all along (e.g., allowing laziness into your induction).
[Persistent union-find](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.8494) has been done. Albeit, it still uses mutation internally, and they don't prove that it has identical lower bounds (i.e., it's unclear whether the small amount of empirical slowdown is due to asymptotic or constant factors). A lot of the mutation for backtracking/persistence can be eliminated by using a State monad instead of global state. The trick is the in-place update of the primitive arrays, since `O(log n)` access/update isn't acceptable in this context. And the laziness aspect is still open AFAIA.
yayayayayay! thanks! another suggestion for raw, so i think i'll just do that...at least at first.
I wish I could make it. That would be really, really cool.
One note. Induction can (I think) work on infinite structures if you [generalize your induction](https://personal.cis.strath.ac.uk/neil.ghani/papers/ghani-csl10.pdf) sufficiently. You just start having more cases. For instance if we're talking about Haskell data types, they should be initial algebras in a domain category, so in addition to the normal constructor cases, if you prove: P _|_ if {x_i} is an ascending chain, and P x_i, then P (lub {x_i}) then you get forall x. P x. This isn't set induction, but it lets you do induction-like reasoning on infinite structures. You need to generalize in this way anyway, of course, because even ML has non-set-theoretic data types mentioned in that article. This also assumes you have a _|_, so you're partial, in which case lazy is mixed up with infinite, and we can't make the distinctions above. Edit: one more: I'm not sure how #3 can work unless eager evaluation has partiality as an effect. I can write a coalgebra: f () -&gt; Just (1, ()) and the above says that there is a finite list `l = unfold f` such that `uncons l = Just (1, l)`. But there is no such finite list, so it must be that `unfold f` (or something else, possibly) just diverges.
Laziness is not a property of types. Laziness is a property of the interpretation for a language. For example, consider: foo :: Foo -- lazy, by hypothesis seq foo :: Foo -- strict, by forcing If an interpretation for an expression terminates at all, then the interpretation will be the same whether the interpretation was strict or lazy. If laziness was a property *of types*, then these expressions would be distinguishable by the type system. They are not. That does not mean that you can't make a type to enforce strictness: data Strict a = Strict !a My post included justification. In particular, he did not even characterize the subjects (positivity and negativity) of his thesis. He also made elementary mistakes, such as the obviously demonstrable "strictness is not a property of types".
This is not an answer to your question, but do you know about [More haste, less speed: lazy versus eager evaluation](http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=44113) by Bird, Jones, and de Moor where they provide a solution to Pippenger's problem in a call-by need language that runs in linear time?
Imagine that you have a `Behavior` that counts the number of clicks. bCount = accumB 0 $ (+1) &lt;$&gt; eClick When you have dynamic event switching, it may happen that this behavior is used for some time and then discarded. But even when it is not used, it will still accumulate click events. The library has to work with the garbage collector to detect this kind of situation and manually delete the unused behavior, so that it no longer performs unnecessary computations.
I don't really care about templating languages, but I'll ask the obvious questions for those who do: * Why yet another one? * How does it compare to the existing ones?
All right. First of all, I wanted something that had syntax more or less compatible with HTML, so that I could take existing HTML and simply adjust it to my needs, I also wanted something compatible with Blaze, which you can easily integrate with (or it's already integrated) various Haskell web frameworks and other libraries. I also liked what [Bravo](http://hackage.haskell.org/package/Bravo) did with the template arguments, as it allows you to create pretty much self-contained templates (you do not have to rely that much on the Haskell names in scope and it also allows you to use very little amount of Haskell in your template other than "splicing" the template arguments). **Why not Hamlet?** * Syntax. The syntax of Hamlet feels really unintuitive, since sometimes you have to close tags (inline) and sometimes you can not... and if you forget to close a tag, it still compiles and you have a broken HTML. This point also includes the syntax for conditional attributes and probably other little things I can not remember right now. **Why not Heist?** * The templates are compiled at run time and last time I checked, they were compiled every time you requested them (I think there were/are plans to compile them only once at start-up). * Since splices are just nodes, it silently "fails" if you forget to bind them (maybe this changed since I last checked). * Sometimes you want more power inside the template than what splices can offer (`maybe` or pattern match on a constructor can often go a long way). This point is a bit debatable, since often less is more (in this case less Haskell in your templates). So, these are my opinions/preferences. Also thanks for asking, I should have probably included this bit in the blog itself. 
&gt; “Simple” is when you dumb down concepts so much, they are less efficient again. That's not what *my* dictionary says...
I guess you're right, but that doesn't cover everything example I have.
&gt; Although I will say that your first two points against Heist will probably not be true of the next release. I'm glad to hear that since these were on top of the virtual list of the "issues" I had back then. 
So, how does the HTML escaping work? Does it use the same escaping rule in all three of these contexts? &lt;li&gt;{h|userName|}&lt;/li&gt; &lt;a href={h|link}&gt;link&lt;/a&gt; &lt;a href="window.location('{h|location}')"&gt;JS link&lt;/a&gt;
&gt; When you have dynamic event switching, it may happen that this behavior is used for some time and then discarded. I fail to see why it is not the case when you *don't* have dynamic event switching.
Well, my concern here is whether the same HTML escaping rules are safe to use in all syntactic contexts in template output. For example, the [OWASP recommendations for anti-XSS output escaping](https://www.owasp.org/index.php/XSS_(Cross_Site_Scripting\)_Prevention_Cheat_Sheet) recommend different escaping rules for text inserted into the body of an HTML tag, text inserted into the value of a tag attribute, text inserted into the content of a Javascript string literal, etc. Your answer tells me what code in the underlying `BlazeHtml` library the templates translate to, but not how the escaping is done. I had a quick look and it doesn't follow those guidelines I linked; the escaping in `Text.Blaze.Renderer.Text` works like this: -- | Escape predefined XML entities in a text value -- escapeMarkupEntities :: Text -- ^ Text to escape -&gt; Builder -- ^ Resulting text builder escapeMarkupEntities = T.foldr escape mempty where escape :: Char -&gt; Builder -&gt; Builder escape '&lt;' b = B.fromText "&amp;lt;" `mappend` b escape '&gt;' b = B.fromText "&amp;gt;" `mappend` b escape '&amp;' b = B.fromText "&amp;amp;" `mappend` b escape '"' b = B.fromText "&amp;quot;" `mappend` b escape '\'' b = B.fromText "&amp;#39;" `mappend` b escape x b = B.singleton x `mappend` b This is similar to OWASP rule #1, but doesn't include forward slash '/'. If `BlazeHtml` guarantees that tag attribute values are always quoted it might be possible to relax OWASP rule #2, so it might be possible to ignore that one. On #3, however, BlazeHtml appears to treat Javascript-valued attributes as being outside of its scope, so if a template tries to do something like your version of my third example (which I repeat here), all bets are off: &lt;a href={h| "window.location(\'" &lt;&gt; location &lt;&gt; "\'")|}&gt;JS link&lt;/a&gt; Here `location` might be `"'); alert('mwahaha'); window.location('"`. &gt; If you do not want them HTML escaped you can turn the expressions into B.Markup or B.AttributeValue by using B.preEscapedToMarkup or B.preEscapedToValue respectively. Oh, with the abuse I've seen of templates in my life, I would much prefer if breaking free from the escaping were much, much harder. (Heck, I once saw a case where a developer implemented a feature by performing an XSS attack against his own application.) &gt; Also, what do you think would be a sensible default behaviour for your third example? Ideally, the template language parser has Javascript support, it forbids expanding into any Javascript context that's not a string literal, and it escapes the text expanded into the string literal using a rule appropriate for that context. Yeah, this is hard, but that's because the web is crap: HTML tags with attributes inside of them that have Javascript inside of them that has string manipulation of HTML fragments that then gets put back into the DOM using `element.innerHTML = "&lt;p&gt;" + userInput + "&lt;/p&gt;"`. And there's a fourth case: what if somebody expands into CSS? I figure this is one of the reasons why Yesod has Cassius and Julius in addition to Hamlet, but I haven't really looked yet. **TL;DR:** I was charged with fixing XSS in a 500,000+ line Java application and now I'm a bitter old man. 
That's a lot of duplication! Time for a type class with instances for `E`, `T` and `K`?
How would such a startup be funded?
Related interview: http://www.infoq.com/interviews/haskell-erlang-p2p-implementation "Contrasting Haskell &amp; Erlang in peer-to-peer protocol implementation"
I'm not trying to argue that a pointer to an X is exactly Haskell's Maybe X, i'm trying to argue that fmap( f, x ) makes sense when X is a pointer, boost::optional, or whatever. That C++ programmers shouldn't try to implement Maybe in C++, or rather: reinvent the wheel. This requires (in C++) thinking of Maybe as a concept. fmap(f,ptr) should work the same way as fmap f (Just x) does in Haskell. A generic version of fmap that works on standard C++ objects is a lot more valuable **in c++** than a new type with one implementation of fmap.
As a tool for universities to grade students. Or as a way to recruit Haskell people like interviewstreet.
It's not really true that GHC implements lazy evaluation. Strictness analysis can make things strict, and parallelism may cause duplication of work. 
I've realized that maybe what people are responding to is that i called Maybe useless. I did not call it useless without context. Imagine if Haskell had only these types: Int, Char, Float, and Maybe. Imagine if Haskell had no &gt;&gt;= or &gt;&gt; fmap or (.) or anything. That's what C++ programmers are doing right now. They're writing Maybe implementations, but not any of the tools to make use of it. I do not mean that Maybe is useless when used with fmap and like operatins, i mean that it's useless if all you got is the class. It's not something that works in a vaccum.
There is a lot of duplication between the ADTs, and unfortunately they differ in style. 2.8.0.0 does unify kinds and types, though. For example, "A B" in Exp, Type, and Pat: a = mkName "A" b = mkName "B" e = AppE (ConE a) (ConE b) t = AppT (ConT a) (ConT b) p = ConP a [ConP b []] Here, e and t are at least very similar. What about "(A, B)"? e = TupE [ConE a, ConE b] t = AppT (AppT (TupT 2) (ConT a)) (ConT b) p = TupP [ConP a, ConP b] Now e and p are the similar ones! I have written some unpublished code that attempts to use a single ADT to represent Exp, Pat, and Type, with partial conversions from / to each. It's pretty WIP - somewhat because I'm not sure if it's actually a good idea - having an abstract type class might be a better approach. This can be a simpler way of constructing TH, but also allows things like converting most varieties of patterns into expressions.
Ooh, more great work by Milan. Thanks for the pointer.
I realised the other day that a templating system that worked with all monoids (not just stringlike monoids) would make it easier to keep track of what the Vary and Last-Modified headers of a http response should be.
Are there any opinions here about the way he implemented his cross-thread signalling? Are asynchronous exceptions the right tool for the job?
http://neilmitchell.blogspot.co.uk/2010/12/new-version-of-hoogle-41.html - this page describes some of it, but it isn't as clear as it should be by a long way.
--local should work - are the local docs installed? If they are, where are they? I use heuristics to try and find them, which may not cover your case.
**This is a massive case of Not Invented Here syndrome.** It doesn’t support Haskell *at all*, but just does [the same thing that Firefox* could always do](http://kb.mozillazine.org/Using_keyword_searches), only that you have to input it in a search field for additional redirection first. __ * And probably other browsers (like Krusader) too.
No. I meant that swapBuffers in GLFW will update keyboard/mouse state when needed and I'm the one controlling main loop (beacause I wrote it). It only does a single update and none of IoC retardation.
&gt; This is very reminiscent of the problem of ensuring resource safety for streaming IO libraries, and the solution there is to be able to introduce scopes with well-defined lifetimes and within which resources can be allocated and freed at scope close. The problem is similar, indeed. But in this case, managing resources by hand (i.e. managing scope) is very unwieldy; in fact, I don't know how to do that in FRP. Garbage collection is alright here.
Without laws, typeclasses are just overloading with the advantage that you automagically get implementations of derived functions.
I understand you're very invested in this discovery of yours and feel rather protective over it. That can sometimes backfire when you leave money on the table by not seeing every variation. The one I'm alluding to is when F is a functor and M is a monad, but you don't have any run symmetries.
That's an excellent analogy. You discovered monads, say, could you have discovered applicative functors too? What are you missing with left monad actions?
Impressive! Good work.
&gt; but you don't have any run symmetries. By run symmetries, I assumed that you meant that the laws didn't hold?
In this example, it's sufficient to have more fine-grained combinators. `switchB` is a little too general. You want a combinator whose meaning is _only_ to switch from one behavior to another at the first occurrence of some event, and never to switch back (maybe `latchB`), rather than `switchB`, which can pick an arbitrary interleaving of the two behaviors. This doesn't require any Oleg-style tricks (and in fact, it's more flexible because scoping need not be static). I kind of suspect this sort of thing could be done for the vast majority of use cases that might otherwise require the full-blown dynamic event switching that causes problems for tracking resource lifetimes. Basically, you just need to reify some additional information at the value level to allow the framework to figure out resource lifetimes. In some ways, you have an advantage over using the GC since you know everything about your domain and can make lots of assumptions.
Hi, at the time of your asking it was not possible, I have now added the option for the whole attribute to be an expression, it would look like this: opt :: Monoid a =&gt; Bool -&gt; a -&gt; a opt c m = if c then m else mempty test :: Bool -&gt; B.Attribute -&gt; B.Markup test cond attribute = [m| &lt;div {h| opt cond attribute |}&gt; Lorem ipsum dolor sit amet. &lt;/div&gt; |] For now the type of the expression has to be [`Text.Blaze.Attribute`](http://hackage.haskell.org/packages/archive/blaze-markup/latest/doc/html/Text-Blaze.html#t:Attribute), this might be fairly limiting, if you use `OverloadedString` and use the default options for the templates (there will be ambiguity). I could allow it to be something else, like `(String, AttributeValue)` as well as `Attribute` (or possibly `ToValue a =&gt; (String, a)` if some template option is set).
This looks really interesting and like a nice option to [ACID State](http://hackage.haskell.org/package/acid-state) that in not in-memory only.
I haven't had any Mac users, and finding the docs is all heuristics based, so now I have your details I'll see what I can do!
That is fucking awesome.
Here is how ACID would apply currently: Transactions are executed sequentially, and Perdure does not support the declaration of constraints, so Isolation and Consistency are implicit. Durabality is provided (to a greater extent than usual given the graph of digests). Atomicity is implicit when using a single PVar (which is the main intended usage scenario). Thing will get more interesting on that front once we support more concurrency.
well, don't take it bad, but you're a pig with that bool File::read_next(const bool, const bool) I wouldn't trust even Agda function if it were 250 lines long :D 
I'm really pleased to you released the code on github. I came across your blog plots while trying to figure out some attoparsec problems a few months ago. There is a lack of documentation on attoparsec. It is supposedly filled by the claim that is just like parsec. That only helps until your run into a brick wall of a difference, it is really helpful to see some more non-trivial usage examples in the world.
Don't you love it when the best solution is *more* laziness? :)
Looks promising. Apache 2 license, similar in nature to acid state. Can keep much in persistent store. But they'll need to fix the issue with in-memory refs (e.g. using GHC's weak refs) before I could use this.
This is rad - though it's a little weird that it shows spaces as a '.' with a background color...
Not a big deal in terms of Perdure's API, it contains a single lens I think. We could also provide the Control.Lens version without adding a dependency, which is quite nice. We are considering Control.Lens for internals and for apps, what would you say is the main reason to adopt it?
First Hackage upload :) I stumbled on a couple of issues. 0.1.2 should be fine but is still in the queue.
You mean the ability to keep references to old states ? Note that you can work around that by having your new states refer to some or all older states. You are then managing your root set yourself.
I'm going to respond against my better judgement ... *DISCLAIMER* This is NOT real world code. Using max instead of min just saves a final reverse which is a walk through the list. This doesn't matter for an O( n^2 ) worst case algorithm. Saying "this sort is inefficient" and then recommending that change is really silly. Yes, the list versions are worst case O( n^2 ) BUT really ~n^2 (instead of ~n^2 / 2 like the imperative version) because of the list access and delete which are each n per iteration. If you were looking to pedantically optimize the example(which it really seems you were based on your comments) then you would be more concerned with doing so inside the recursion instead of a single reverse call. Yes a list is "persistent" like Data.Sequence but not efficiently so for accesses or updates. My bad. As for "modify" vs. "update", I'm not even going to argue this. Overly pedantic. Yes, we know persistent types are immutable even when using scary words like that. Of course "mutate" would mean something very different in CS terms. Again, anyone who knows even a little about CS knows that selection sort is often a bad real world choice. No need to re-iterate that. Besides, it's still highly valuable studying non-optimal algorithms. The comparison between the imperative version and the functional version was to demonstrate the difference in implementation and NOT the performance. Relax.
&gt; Saying "this sort is inefficient" and then recommending that change is really silly. Well, not only is it inefficient (constant factors matter too), but it's simpler to implement the algorithm without the reverse. &gt; Yes, the list versions are worst case O(n2) BUT really ~2n2 (instead of ~n2 / 2 like the imperative version) because of the list access and delete which are each n per iteration. If you were looking to pedantically optimize the example(which it really seems you were based on your comments) then you would be more concerned with doing so inside the recursion instead of a single reverse call. It's true if I was looking to save a few machine cycles then this would give bigger bang for my buck, but my reason for presenting the correction is quite different. Removing the reverse gives a simpler presentation of the algorithm (and makes it quite a bit easier to prove correct). Reversing it makes no sense, semantically. It's like the author wrote a selection sort and then realised that it was coming out the wrong way, and rather than fix the problem, just reversed the result post facto. &gt; Yes a list is "persistent" like Data.Sequence but not efficiently so for accesses or updates. This directly contradicts the article, at least, before it was edited. Now it says "efficient persistent" where before it just said "persistent". The persistence of the data structure has no bearing on it's time complexity, save that actual O(1) update is essentially impossible, and you always have log factors. &gt; My bad. As for "modify" vs. "update", I'm not even going to argue this. Overly pedantic. No, it's actually a very important distinction. Modify implies *modification*, i.e *destructive* update. In FP we use the term "update" instead because this leaves semantic room in the term for a *non-destructive* update. &gt; 
sounds pretty cool, but for some reason, it doesnt install ghc-mod correctly for me :(
&gt; You're just nit-picking again. In this specific case reverse may not be required but it's only a minor detail compared to your recommended 'optimization'. What? Removing the reverse was my recommended optimisation. I don't understand what you're trying to say. &gt; You contradict yourself, you started out saying it was inefficient and now it's to do with what you call 'semantics'. I didn't contradict myself. It is certainly inefficient, and I said so, but my reason for presenting the clarified version is that there is no reason for the inefficiency. In the case of the optimisation you presented, it makes the code substantially less clear, so there's an argument for leaving it as is. In the case of the reverse, there is simply no need for it, so why not make the code clearer, and get an optimisation to boot? The change is of uniform benefit, I don't understand how it has upset you so much. &gt; You are extending your experience(with only Haskell ?) to all of FP Actually, I started functional programming in Scheme, moved to Haskell and Agda recreationally and I work primarily in SML, Scala and Isabelle. I'd hardly say my experience is limited to just Haskell. &gt; In Clojure for example, the word 'modified' is used in terms of it's immutable and persistent collections by the author himself. I'm not familiar with much of Rich Hickey's work, but if he's chosen to use terms differently then that's fine. It's just an aside in my original comment anyway. &gt; Stop being anal, grow up and/or get laid. I don't see the need for such ridiculousness. Kindly take that attitude back to where you found it, I don't think it's welcome here. &gt; Ok, so you've been mentally abused by academia to jump through arbitrary hoops. That's where the pedantic scrutinizing is coming from and the inability to concede on minor insignificant details. I'm just trying to engage in polite discussion. I'm surprised by your defensiveness and angry attitude.
Normally it only shows this while typing and it is meant as an aid for indenting things correctly. You can turn it off in .vimrc.local if it bothers you though.
&gt; In Clojure for example, the word 'modified' is used in terms of it's immutable and persistent collections by the author himself. Hickey puts 'modified' in quotes when referring to immutable data, every time. I suspect he shares kamatsu's understanding of the word.
I'm not sure why they are using continuations, when their abstraction really just a free monad transformer in disguise: data TraceF e x = Exit | Ret | Yield x | Fork x x | Watch (e -&gt; Maybe v) (v -&gt; x) -- Sprinkle 'forall' somewhere | Signal (e -&gt; x) type Task e = FreeT (Trace e) exit = liftF Exit ret = liftF Ret yield = liftF $ Yield () fork' = liftF $ Fork False True fork t = do b &lt;- fork' case b of False -&gt; return () True -&gt; t &gt;&gt; ret watch f = liftF $ Watch f id -- I probably wrote this one wrong signal e = liftF $ Signal e () In other words reinversion of control is just a fancy name for an ordinary abstract syntax tree with effects. The whole `ContT` stuff is just obscuring that fact. I don't necessarily mind it being implemented with `ContT` (Maybe it's faster that way? I don't know), but I never see them or the original authors ever make the connection to free monads, which is the actual meat of the abstraction. Edit: And if you're not sure when to use free monads, a really good rule of thumb is: If you are writing an interpreter function, you probably have a free monad. Also, if you want to use free monad transformers, check out the `free` package, especially now that Edward finished merging in my free monad transformer code from `transformers-free`.
I'm stuck on just proving the identity laws. Here's the reason why. Let's start with the left identity law: 0 = join $ run [] x + y = join $ run [x, y] 0 + x = join $ run [join (run []), x] Now, we need to resolve that last equation to `x`, which means eliminating the outer `run`, however your `run` laws only have one way to reduce 1 `run` to 0 `run`s, which is: -- 1 run on left side, 0 runs on right side run . fmap return = id ... which means that you'd have to reduce the right-hand side of the equation to the form: -- There exists some 'a' and 'b', such that: join $ run [return a, return b] However, you can't guarantee there exists a `b` such that `x = return b`, which is the part where I get stuck.
I guess it all depends on how you define lazy. Also, lazy doesn't necessarily mean bad. I've only started tinkering around with Haskell lately, but the more I learn, the more it impresses me.
Looks like I made a mistake in my "proof" of the left identity law, sorry.
Is it possible to have non-strict denotational semantics with a non-lazy implementation? If not, I don't see the distinction...
as I pointed out, the GHC compiler generates eager (i.e. call-by-value) code for functions that it can determine to be strict
this is a very useful technique. I have used this continuation/coroutine technique for inverting control when implementing hoodle (previously hxournal) program, which has a fairly large bit of codes. Business logic can be expressed in a much clearer way than just using bare event handler with all IORef or MVar exposed to every part of program as typical GUI programming in haskell. With continuation/coroutine, after purifying IO action into a functor using Free monad, states of a program, previously accessed by using IO monad, can be transformed to a captured state in closure so that you can program in pure state monad. This can be further bridged to FRP with more abstraction. 
I see you can do it by typing "vi". That's not bad. Thanks again! 
I certainly have overlooked other abstraction model since it was adapted from some old code that I wrote in 2008 after reading Peng Li's paper. But your rule of thumb is interesting, and actually reminds me of the continuation semantics for interpreters, which, not surprisingly, can also be given a more direct semantics, which perhaps falls in the free monad category.
I can’t discern your intentions WRT the linked submission. You very briefly describe selection sort as an in-place algorithm, followed by a C version alongside two trivial implementations of a superficially similar algorithm. At no point do you describe why this comparison should seem interesting; you don’t actually do any *comparison*. You don’t talk about implementation of the algorithm, or its complexity guarantees (aside from a blurb quoted from Wikipedia), or the differences between functional and imperative versions. Or is the reader supposed to be impressed by the differences in code size? Or something? Yes, yes, the concise “functional quicksort” is one of the first Haskell examples everyone sees; a selection sort variant is unnecessary. (That’s not even to mention the sloppy editing.)
All that shows is the relationship between `ContT` and `Codensity`. That still uses the free monad as the base monad.
I wasn't asking about the difference non-strict and strict, but what difference does separating non-strictness and laziness make to the pragmatic programmer who is just trying to solve a problem at hand?
You can edit the text of the post by the way.
So why does it matter that infinity messes up A+x=B+x implies A=B if your code will never be able to calculate A+infinity or B+infinity? What could you actually do to the actual infinity value that would cause problems?
Every graphics program (even those using FRP) is basically "loop on the event queue", but the differences are: 1. whether the framework forces inversion of control upon you. (GLUT does, SDL does not, and GLFW sort of is in the middle) 2. whether your program is a big state automata. (the tutorial goes from one to one that is not) 
I think this is implicit in what was said anyway, but to make it explicit... &gt; Such cyclic values can lead to undesirable consequences. For example, they break some familiar algebraic laws: from A+x=B+x we can no longer infer that A=B. One way to view the problem is that infinity is not a natural number, or a rational, or a real. [Infinity is not a number](http://nrich.maths.org/2756). Therefore, trying to tell Haskell that it's a member of those types leads to problems. If you feed Haskell a fallacy, you take responsibility for the consequences, and for ensuring that the results aren't fallacious. You're manipulating the expression evaluation mechanism to achieve an effect, not doing "equational reasoning". You're using a hack. Another argument IMO is that that definition of infinity isn't really a definition of infinity. Defining a value to be it's own successor is a logical contradiction, not a way to define infinity. I'm probably on dangerous ground with that last claim, though - too many people (mathematicians included) have claimed that infinity plus one is infinity (ie that infinity is its own successor) - just because I'm ignorant of their reasons doesn't mean I'm right and they're wrong. I don't know how, but my guess is that the axiom of choice may be relevant. Though maybe they just mean something that is not finite plus one is also not finite, without meaning to imply that they're both the same thing. Anyway, the safe version is probably that defining a value that is both (1) a natural (or other finite) number, and (2) it's own successor is a contradiction, and though that data definition is polymorphic, when you use it at some point you always end up constraining the own-successor definition to some particular numeric type and thus end up with a contradiction. 
IIRC, lazy is only ever considered bad in efficiency terms and maybe in following-what-the-debugger-is-doing terms. It's good in computation terms, because (I *think* I'm remembering accurately) if it's possible to compute a result using a deterministic evaluation order at all, some lazy evaluation will always give a result, whereas other deterministic evaluation orders (including strict) may not. Avoiding a division by zero where the result of that division isn't needed is a common example. However, there's more than one "lazy" evaluation order (I'm probably abusing terminology here - don't trust my definitions, but I think you'll get the point). The outermost-first thing is important, but left-to-right vs. right-to-left doesn't have the same significance, yet it can still decide whether some expressions can be evaluated - e.g. due to "shortcut" logic expressions. However, there's no one best order for all expressions - unless you can reliably determine which subexpressions will fail to evaluate (which in general you can't) there's no perfect evaluation order. Even so, if innermost left-to-right succeeds then outermost left-to-right succeeds, but not necessarily visa versa. Or else I've got this all confused and I'd appreciate an explanation if that's the case. 
Pedantry is never important until it is. *EDIT* - that looks really arrogant even to me within seconds of saying it. I don't really mean that everyone everywhere should obsess on every pedantic detail or else they're bad programmers, only that the discussions should happen somewhere so I can Google them when everything goes horribly wrong. 
Definitions need to be made at some point. The question I think is do they matter in the daily life of a haskell programmer.
My point is that one should be aware of the consequences of allowing such values. As others have pointed out, if it doesn't make sense then we should just as well rule it out by construction. Thinking about strictness of your data types allows just that.
It isn't inconsistent to say that inf = 1+inf (in fact, this is just the Haskell definition I gave); the point is that then you have to mind the consequences of losing some familiar algebraic laws. Mathematicians rely on intelligent humans interpreting symbols in the proper context; computer scientists and programmers have to deal with much less "forgiving" machines.
If your code happens to be strict, the compiler can generate code for eager (not lazy) evaluation. And you can force data types and functions to be strict if you want to. 
This what meant about "flogging a dead horse" -- I read you post some time ago and fully agree with your point. I merely wanted to emphasize that the Haskell programmer isn't totally at the mercy of a "magical" compiler/strictness analysis, you can actually declare strictness if you need/want it. 
Well, call-by-name is also an evaluation strategy which provides non-strict semantics. I guess one could call it just a matter of "optimization", but I think it's rather important for the programmer to know whether `double $ double $ ... $ double x` runs in linear time or exponential time... and situations like this where there is a *large* asymptotic difference in running time between lazy and call-by-name evaluation are common.
There are two main differences. For one, this can affect your space usage. In particular, GHC performs [strictness analysis](http://en.wikipedia.org/wiki/Strictness_analysis) in an attempt to optimize certain code by evaluating earlier than needed. This can often make code you would expect to have a space leak perform fine. However, this also makes the exact space behavior of code less predictable--an issue fundamental to any sufficiently complex optimizing compiler. In most cases--ideally every case--this analysis should *improve* your space performance, so it is a good compromise. Additionally, this distinction can come up if you use multiple Haskell compilers. Haskell is a standardized language (modulo language extensions), so writing portable code is completely reasonable. For example, you may wish to use [UHC](http://www.cs.uu.nl/wiki/UHC/) to compile your code down to [JavaScript](http://uu-computerscience.github.com/uhc-js/). This is particularly neat because [Reactive-Banana](http://apfelmus.nfshost.com/blog/2012/05/15-frp-banana-0-6.html) now supports UHC, meaning you should be able to write reactive JavaScript UIs. Coincidentally, you can also use [Elm](http://elm-lang.org/) to do this, which is a very cool project that bears following. However, if all you're doing is using GHC without too much worry about space leaks and the like, you're probably fine not being concerned about the distinction. The semantics of your code cannot change just because the language is lazy or not quite lazy--that's the whole point of non-strict *semantics*! So every correct implementation of Haskell will give you the same results for a program--they just might have drastically different performance. It is very much an implementation detail that *can* be important but doesn't *have* to be. It's also nice to learn about exactly how Haskell works and what programs mean. Studying both semantics and language implementation is a great way to become a better programmer, but is also fairly advanced. It's great fun too :).
Yes.
awesome work. This is exactly what I needed. Hopefully you will have the fay integrated posted soon. Can't wait.
It depends on the problem the programmer is trying to solve. Fixing unwanted space leaks is an example of where the distinction is necessary.
Non-strict is useful because it allows you to define stuff like infinite lists. The easiest way to get non-strict semantics is via lazy evaluation (which is what GHC does). So lazy evaluation is the mechanism behind non-strictness. However lazy evaluation has some unfortunate side effects. Like making it tricky to reason about the performance characteristics. Another problem is that it outright hurts performance in many cases. If we could have non-strict without general lazy evaluation it'd probably be a good thing overall. Though sometimes lazy evaluation is really useful as well. Overall I think it'd be nice to be able to turn it on and off without resorting to seq.
I know and have a lot of those plugins, but nice job automatizing everything from a bare vim installation with just one script ;-)
That's a neat trick, although I like to use `gq` for formatting comments. vim2hs provides a `:PointFree` command that you can set up a custom mapping for instead. *edit:* Actually I tend to use `gw` for comments which doesn't use `formatprg`. Maybe I should make vim2hs use your trick instead...
`plusish` and `zeroish` aren't even well typed.
To get it to work I had to fork ``pointless`` so that it read from stdin instead of taking an argument. Did the same with Lambdabot's monad desugarer and djinn and they work pretty well with formatprg. Pretty trivial changes, but if you want the source I'd be happy to put it up.
Take the church encoding of your data structure and watch what happens...
Whoops, I forgot a return.
Not to start a religious war, but I prefer Data.Lense, so I think it may be a matter of taste here.
Standards implementations are a pretty interesting area for Haskell, I think. One thing I find attractive about some of the syntax wrangling you can do with it, is that with some creativity you can often come up with a kind of declarative DSL that almost reads like a file format or protocol definition. The idea of "executable specifications" is pretty cool, I think, even if it only leads to a reference implementation that can be made more efficient in another language.
Among all the things that haskell can do, with relatively less knowledge, I think that haskell is really shining when you need to write your own parser. Since parsing is common in every task, starting to write a parser is a good start. Even if you have some nice parsers in other language, I strongly suggest to write a haskell version. Parser written in Haskell shows the power of denotational semantics very well and therefore very intuitve. It will reward back very soon. 
Deactivating auto-compile can be done by removing the following line from .vimrc.local: autocmd BufWritePost *.hs GhcModCheckAndLintAsync HLint should also be deactivated then.
Thanks for pointing it out loud. I could have done: 1. Add a `Pure r` and define `Trace` to be instances of `Monad` and `MonadTrans`, or 2. Wrap `FreeT` over `Trace`, and derive `Monad` and `MonadTrans` instance. Or just like what I did: `3`. Wrap `ContT` over `Trace`, and derive `Monad` and `MonadTrans`. Your points are well taken. I'll modify the tutorial to reflect this discussion.
Oh, there's no need to modify it. I was just pointing out that syntax trees don't get enough love when they actually appear all over the place in advanced Haskell libraries. One thing I want to note is that with `ContT` you are actually not deriving the `Monad`/`MonadTrans` instance, at least not for `Trace`. All it does is push the definition of your actual monad instance into the builder functions you define. As a trivial example, consider the [codensity transformation](http://hackage.haskell.org/packages/archive/kan-extensions/2.7/doc/html/Control-Monad-Codensity.html) (which is basically what you were doing with `ConT`) also generates a monad: instance Monad (Codensity m) where ... But notice that it has no constraints at all, not even `(Monad m) =&gt; ...`. This is because CPS style transformations don't require any information about the base monad at all to form their monad because they don't actually do anything other than defer the monad instance to the construction functions (i.e. your `fork`, `signal`, `yield`, functions). What this means for you practically is that you are inlining your `Trace` `Monad` instance into every new primitive you define, thus not saving you any work. The only way you actually get a monad instance for free is to use the `FreeT` types to define your type. This then automatically derives the correct `Monad` and `MonadTrans` instances.
Well, making the `instance Monad m =&gt; Monad (Trace m e)` declaration is actually fairly verbose. The use of `ContT` certainly helps by removing this burden. 
Sorry for the content-free post—but isn't this a right monad action? I would certainly say that "`f` is a right `m`-module" or "`m` acts on `f` from the right", and Google results for phrases like "left monad action" or "right monoid action" suggest that "right action" = "action from the right". Or maybe you've given End(Hask) the monoidal structure sending (F, G) to F;G (aka G ∘ F), but that also seems quite non-standard.
For the first time in a long while, I wrote some C code. I have often thought of C as a nasty language; a dangerous tool that makes it all too easy to shoot yourself in the foot. However, my experience was refreshingly just the opposite! The significant Haskell experience that I have gained since the last time I wrote C code has taught me how to properly decompose problems in a safe and pure way. I quickly learned about how to use `const` and `restrict` annotations, and the project I was working on really came together quite nicely. I'm very glad I chose to start learning Haskell (about 2 years ago), and I can definitely reaffirm that it has made me a much better C programmer.
You also learn lots of tricks to make C more type-safe. e.g: * Using "newtype" in C (struct with one member) to separate various types * Representing sum types via catamorphisms (since C has no tagged unions) * Passing type-class-style method dictionaries rather than OO-style object-vtable-ptr dictionaries 
Example: It means that usually, he can avoid worrying about foldl vs. foldl', because GHC will use eager evaluation because it can detect non-strictness doesn't matter in that case.
Very interesting to see more real-world users of Haskell. I like the fact that there is honest criticism on things that could be improved --- this is much more valuable than just blind support. Any chance of a follow up? 
How does the catamorphism of the data structure encode the data structure itself? I mean, you still need to implement this somehow. Could you give an example?
Example, instead of: struct my_data { struct the_large_tagged_union_above member; } You would have: struct my_data { void (*member) ( void *arg , void (*A)(void*, int), , void (*B)(void*, bool, bool), , void (*C)(void*, char, int) ); } Then, instead of: x.member.tag = A; x.member.a = 1; You have something like: void my_a_maker( void *arg, void (*A)(void*, int), void (*B)(void*, bool, bool), void (*C)(void*, char, int))) { A(arg, 1); } x.member = my_a_maker; Defining the function is a bit crufty in cases such as this, but all of the crufty parts are redundant so are compiler-checked anyway.
I just read the Li &amp; Zdancewic 2007 paper. Fascinating work. There it seemed to be all about multi-tasking, but when I read the source code of monad-task, I didn't see a forkIO or MVar or... anywhere. I realize you are modelling application threads, but at some point don't you have to fire off real ones for concurrency? Maybe this is apparent from the graphics code in your blog, but [and much as I've done a tonne of GTK work] perhaps it wasn't obvious to me, not speaking GLFW / GLUT.
In those cases, you usually don't have to write a parser. Instead you can often: * Use a standard parser * Embed the configuration as a script or part of the program if recompiling upon changes is not a problem 
Interestig that you use gdb with Haskell. I've not before heard of a useful reason to use a "debugger" with a high-level language.
I really recommend this. Printed one in pocket format.
/Slightly offtopic/ On the [screenshot](http://adinapoli.github.com/cumino/img/cumino_session.png): * is that awesome wm? * I like your vim statusbar config - would you share? Btw thanks for sharing Cumino, will play with it but a little bit later.
Possibly, I didn't look it up. I assumed it was a left action, but I guess I was wrong.
Not the first company to roll a new time library or C++-oriented build system ... 
Seems a bit circular to me: Kleisli is defined in terms of Monad, and vice versa?
 $ tmux new-session -s cumino -t ghci "ghci" usage: new-session [-d] [-n window-name] [-s session-name] [-t target-session] [-x width] [-y height] [command]
monad-task is designed specifically as a transformer that goes on top of other monads. You can't have that with GHC's forkIO or any true concurrency model because suddenly you have to worry about race condition updating a state monad, or similar things. monad-task is only co-operative threads, and hence not true concurrency. There is also a package called forkable-monad that tries to generalize concurrency as a monad transformer, but it specifically says: &gt; StateT makes a copy of the parent thread's state available in the new thread. The states in the two threads are not linked. 
Definitely playing with Cumino a little later. Another (sorry) off-topic question, what colour scheme are you using? I've been looking to replace my current theme and this one looks rather tasty.
I had another go at a [time library](https://github.com/alphaHeavy/time-cube) but it didn't seem quite right (and it's far from complete). We're still using the old one... I'm going to try again after we upgrade to 7.6.1. Do you guys have any plans to open source other projects besides shake?
&gt; Also, that page is just HTML for me. Like, it's not just written in HTML, it displays as HTML source. Sorry about that. Try again now!
Every monad has an associated Kleisli category, and to every Kleisli-like category (that is one with a bijection between `K a b` and `a -&gt; K () b`) one can associate a unique monad (`K ()`). This correspondence itself is bijective.
I understand the relationship -- but I don't think it's a good answer to the question "What is a Monad, really?" because if you then ask "What is a Kleisli Category, really?" you end up at square 1.
You're not back at square one, because the definition of Kleisli-like category is much simpler, in my opinion, than the definition of `Monad`: it's just one where `K a b` bijects with `a -&gt; K () b`. 
Replace the undefined words with normal human words, and the statement actually is obvious and makes sense. The sad part is that some losers always want to seem bigger, by using quargelgarx words that make no irresporxil sense to anyone but them and other sreboklaxonic intashoolaqshishs, and then accuse the neotrafilotasnifloptic person of being quasi-plasflapsontoric and stupid. And it’s even worse, when honestly you try to look them up, and the definitions of all those words are formed from the other words you don’t know, resulting in a huge unsolvable mess of circular dependencies. (A huge problem in Wikipedia’s medical section for example.) I, for one rasskalamate this with assklimarotory passperlipidaeii!
All the pieces were there. He says a monad is a Kleisli category, and he defines the type signatures of the arrows in the category (`a -&gt; m b`).
I applaud your desire to find a simpler English word and do believe in time we will find a word that better fits it. However, saying that there is a problem isn't the same thing as fixing it, and I don't think your descriptions accurately characterize the full breadth of what monads can do.
A remote control is something specific and concrete that is defined by its special purpose, it's easy to describe. A monad is a general, abstract mathematical structure without a specific or special purpose. It is much harder to describe it in generality.
Yes. We didn't spend the time to drive GHC itself which should absolutely be done. However we have almost 100 packages so we get good enough parallelism. 
So in PLINQ I could write something like this (replaced PLINQ functions with haskell equivalents): MyCollection. AsParallel(). Map (some lambda). Filter (some filter). GroupBy (some predicate). Map (something else).ToList() I can easily build complex queries that run in parallel with a tool like that. To me parallelism tends to by more important than concurrency just because of the applications I work on. I tend to write applications that process large amounts of data. I did try to build an equivalent in haskell last fall but I had a space leak that occurred maybe one time in ten that would blow out the memory on the machine in a few second. My guess was something to do with threading since I could run it over and over again on the same data and occasionally get the blow up. This afternoon I am working on trying it on the most recent version of monad-par so if that works I will post the code in our github.
Well you should do it less often, most of the time there are probably better ways to fix your problems. And when you do, removing the .cabal should not be necessary.
You seem to be forgetting about categories of algebras, which is what monads are *really* all about...
I'd like some details on what you didn't like about Haskell's time library. Myself, I like 'time' quite a bit, the only thing I don't like is that the parsing/formatting is rather slow. But I imagine our needs differ.
&gt; OpenGL is basically just a state machine... In the pre-OpenGL 3 world, yes.
I believe these words exist out of a drive for precision about a very abstract concept. Once we concretize on an implementation of a specific monad, it becomes very simple to describe in 'normal human' words what is going on, but those words just can't suffice in the abstract. Every and any attempt I have seen to make the definition of monads more intuitive via analogy or some similar mechanism fail because they say *too much*. It is just as much what the categorical descriptions do not say as what they do say that is important about monads. 
&gt; However, there's no one best order for all expressions Depending on your definition of "best", normal order is the most likely candidate. If any evaluation path terminates, then normal order evaluation will find it; also, normal order is truly confluent, whereas the typical evaluation orders compilers use aren't (except in a weak/bisimulation sense). Unfortunately, it's hard/impossible to implement normal order efficiently. And even if you could, evaluating under lambdas is something we've been trained to believe doesn't exist (e.g., ML's hack for side effects wouldn't work).
Problem with monads is usually terminology. [This image](https://s3.amazonaws.com/VincentToupsScreencasts/monadic-bind.png) helped me.
Download stats; tags; write-your-own stats plugins
I believe removing `.ghc` suffices. Also, you can be even less heavy-handed by just removing the specific ghc version subdirectory inside of there.
Awesome library name :)
I'm surprised at the number of people in this thread trying to give simplistic explanations. It says for experts right there in the title! Thanks for the construction, I'd always meant to look into the correspondence, but hadn't gotten to it.
A sentence on what it is might be useful, so I don't have to wonder or go elsewhere :-). Also [a link](http://hackage.haskell.org/package/aws). 
Its usually just easier and quicker to remove everything.
Well, at least cabal-uninstall seems to work. (Granted it seems a little rough around the edges.)
 ghc-pkg unregister blah should do the trick
I found the image on [this](http://koweycode.blogspot.com/2007/01/think-of-monad.html) page more useful.
You're welcome.
:-)
Awesome! Do you know if it is all historical download stats, or just downloads since hackage2 turned on?
Yes, I really felt like I stepped into the middle of a conversation with this one.
`darkblue` is the One True Color Scheme.
Thank you for this. A couple of things that are probably typos: &gt; The easiest use case for this quantified type is the `observeE` combinator &gt; &gt; observeE :: Event t (forall s. Moment s a) -&gt; Event a Either this doesn't kind-check, or there is some advanced type magic going on. Perhaps it should be: &gt; observeE :: Event t (forall s. Moment s a) -&gt; Event t a Later you say &gt; The point of all this is that in reactive-banana, we may only switch between Events and Behaviors that have variable start times. In particular, the switching combinators have the types &gt; &gt; switchE :: Event t (forall s. Moment s (Event s a)) -&gt; Event s a `s` appears to escape from its scope. Should this instead be &gt; switchE :: Event t (forall s. Moment s (Event s a)) -&gt; Event t a 
If anyone else has this issue, I'd advise them to symlimk rather than copy the binary.
Links: * [Hackage](http://hackage.haskell.org/package/aws) * [Github](https://github.com/aristidb/aws)
You're right on both accounts. Fixed, thanks! (I like how the type checker catches typos.)
Nice work Aristid! I'm looking forward to upgrading (which I'm probably going to do next week).
Concretely, could Monomorphism Restriction change the run-time behaviour of the code? Or was it there only because in: a = sum [1..200000] f :: Integer -&gt; Integer f x = x + a 'a' would rather be, for performance reasons, an Integer than a (Num a) =&gt; a which gets recomputed each time we need it?
\^\^ Actually I knew I hadn't coined the term "the mother of all monads", which comes from the fact you can re-code every monad with Cont. I saw it before, just did not remember where. (personnaly, I prefer "Arch Monad" ;) ) *EDIT:* Yep, I already saw this post from Dan's blog. A long ago, certainly when I was trying to implement monads in Python. But it doesn't mention "interpreted" monads or in any way the dynamicity scale and the way the different monad kinds *all* stem from basic CPS principles, which was my key idea.
The second. If code typechecks both with and without the MR, its behavior will be identical. http://www.haskell.org/haskellwiki/Monomorphism_restriction has lots more info and discussion.
If haskell makes you mad, just don't use it. I wouldn't.
 known' [] = Nothing known' (first:_) = Just ( cast' (valueAt "time" first) , cast' (maybe 0.0 (valueAt "amount" first)) Then a `catMaybes . map known'`
&gt; . I have a few examples of tight loops that get better with the new code gen, but of course for complex low-level loopy code LLVM will still be necessary to get really good code. Why do a new codegen when LLVM works well and has the advantage that it gets patches from a broader community?
Now in GVim with Cumino configured for gnome-terminal, it says "starting new cumino session" and then nothing happens.
The usual \`\`getting things done'' fallacy. Nothing to see here.
This is actually an interesting idea in that the monad seems to be "deferring" or "outsourcing" part of its implementation to an external entity. Witness how many monad need some sort of `runXXXX`. The free monad is the canonical example of this where most the meaning of the monad lies primarily in the interpreter. Same idea with `Cont` except even more powerful than the tree monad where the entire meaning of the monad lies in the interpreter. Some things that don't quite fit the mold so cleanly are the list monad (which is closely related to the `Maybe` monad). So I guess the way I understand your notion of dynamicity as the monad late-binding some aspect of its implementation.
"the "new code generator" is a replacement for the STG-&gt;Cmm phase of the compiler" "After Cmm has been generated, we have a choice of targets to compile to: ... The LLVM code generator ..."
AFAIK, the "new codegen" replaces the STG to C-- stage of the pipeline, so you can still use LLVM with it since that's a later stage.
Works with xterm. One issue I discovered is that the documentation says `g:cumino_default_buffer` and the source says `g:cumino_default_terminal`, but even setting the latter to `'gnome-terminal'` it just does nothing except saying "starting new session...". I'm thinking maybe the issue is that `gnome-terminal -e ghci` doesn't "block" if there's an existing gnome-terminal running, but even with no terminals open I can't get it to work.
[WTF?](https://skillsmatter.com/custom/images/haskell-exchange-800x215px.png)
*g:cumino_default_buffer* is the location of the cumino buffer. cumino is a sort of intermediary between tmux and vim, and he writes commands to submit to ghci in the buffer specified at location *g:cumino_default_buffer*. *g:cumino_default_terminal* does what it claims, set the terminal to use for the cumino session. Cumino should work with *mlterm* and *urxvt* too. I guess the problem is the way *gnome-terminal* handles the *-e* command, so definitely is not a Cumino problem :)
I showed this to someone and he thought it was for an anime convention... 
Sorry, there was a typo in the Wiki, the correct variable and semantics are the one I've mentioned above. The thing I don't understand is: does gnome-terminal work if no other gnome-terminal processes are running? Bye, A.
So did the designer who produced this…
It doesn't work with Cumino, but it does behave like xterm if I call it myself.
So, one common way to do this is to have your program actually be a library, and have the user's config file / scripts be the client application of that library (this is the approach taken by Xmonad, I believe). But it does indeed require the user to have a Haskell compiler.
http://www.haskell.org/haskellwiki/GHC/As_a_library may be of interest, but in general, it's just easiest to rely on a locally installed copy of GHC.
If you are in a hurry, this is okay, but please do not do this in production code. The correct thing to do is handle the error, just like you would in any other language: case someOperation of Just a -&gt; ... -- use the a Nothing -&gt; putStrLn "Nothing to see here. Move along" The pure equivalent of this is to provide a default value: maybe (defaultValue :: t) id :: Maybe t -&gt; t None of this is any different from the way you'd handle it in any other language. The only thing Haskell does differently is that it allows you to compress all your `Maybe` checks into a single check by using the `Maybe` monad. However, you still need a final top-level handler to decide what to do if that single check still returns a 'Nothing'.
That doesn’t seem to work with interactive scripting though…
Haskell? Must be a new anime
I’m also very much interested in this. I’m thinking of something like this: 1. I import a module. 2. I create a interpreter “object”. 3. I add some APIs to communicate with my core program to that interpreter. (Think DOM in your browser.) 4. There is some function that I can call with a string I obtained from my command line. (Or defining stdin in another way.) 5. I get stdout and can print it to my built-in console. 6. I can also load whole modules. (As in: Import statements work.) 6. There is an init / autostart mechanism and several other event triggers (defined in my APIs) that (can) call script code.
The scenario is quite different. Consider the REPL like coding the body of 'main'. Now, the REPL gives you answers as you are programming along. This means that defaulting kicks in *considerably earlier* than it would otherwise kick in. This leads to bad error messages in code that is written across multiple prompts in the repl that would be perfectly acceptable if it were smashed together into one statement. This is counter-intuitive behavior, and leads new users of the language to believe it is less compositional than they thought. I am wholeheartedly in support of the change. This isn't an Internet Explorer worse-is-better argument. Its just that type checking is running in the REPL with a lot less to go on and working on smaller chunks.
They use that anime style for all their conferences. Haskell is currently sharing the Scala branding.
They also do the free F# user group once a month, which I recommend if you need a fix of functional programming.
With `hint` you could try something a bit different. (I haven't tried this yet, but I've been thinking about it today.) Define a function that a script must export as its "entry point" which will take whatever useful arguments. Import the module inside a `hint` interpreter, and extract out that function and return it. Then, your program can call the script by calling this entry point function and passing it whatever hooks you've defined.
The Java-Haskell interop talk sounds very interesting.
This strategy seems compatible with `plugins` as well.
&gt; at the cost of it being possibly less efficient Yes, but only when running under GHC**i**, where you already don't expect your code to be executed efficiently. IMO it's a good call.
Don't worry, now you know they deal with different sames.
I would pay twice the entry fee to go to this, unfortunately tickets to London from the U.S. are a bit spendy for a 1 day conference :( Hopefully videos are made available online.
&gt; But it does indeed require the user to have a Haskell compiler. I don't think you can find an approach that doesn't require your end user to have the compiler installed.
Hmm, that package looks interesting, but on my system `eval` always fails because it uses `-fglasgow-exts` (which is deprecated) and sees the deprecation warning as a failure.
Sometimes the correct handling of an error is for the script to crash. Especially if the actual error is somewhere other than the script.
Thanks! I didn't know about fromJust, this will make things easier.
Admit it: You did that on purpose, to rake in even more karma. ;)
Since I have no clue what the Reddit "karma" system is based on, and the link to the GHC blog I posted above has been featured on /r/haskell a couple of weeks ago, I won't admit anything at all.
monad is actually quick-and-clean (even though not-the-best always) way of making complex modularized system. it separates concerns of pure algorithm from side effects. If you already noticed haskell is awesome for some tasks, why don't you give a chance to investigate monad and what is really for? Haskellers are very strict and considerate in what's good and what's bad, and there are enough reasons why monad is so popular in haskell world. 
&gt; to allow Haskell-based scripting of my application (itself written in Haskell) What sort of "scripting" do you have in mind? What sort of "application" is this that we are talking about?
I don't know about "normal," but you can look at how I do it in gitit: * [Interface.hs](https://github.com/jgm/gitit/blob/master/Network/Gitit/Interface.hs) * [Plugins.hs](https://github.com/jgm/gitit/blob/master/Network/Gitit/Plugins.hs) This approach does not require that the user have GHC installed. 
See also http://hackage.haskell.org/package/dyre.
Xmonad does this. I use it. Works great.
It's already forked, and there have been some things Simon seems to have only fixed in HEAD, so I doubt it. That's fine though, that means there will be a full release cycle of regression testing beforehand.
Our idea for solving this problem in cabal itself it to use an immutable package DB which can support any number of compiled versions of the same package versions e.g. you can have two versions of containers-0.5 installed, each compiled against a different set of dependencies. This is the idea used in NixOS. The semantics of the system would be as if you reinstalled everything from scratch every time, without the associated inefficiencies. 
Ok, now it should be fixed. I've personally tested it with gnome-terminal and GVim under Gnome. Please let me know, Bye, Alfredo
I have implemented part of this over Summer of Code. You can see a short and very technical summary at the top of the page [here](http://hackage.haskell.org/trac/ghc/wiki/Commentary/GSoCMultipleInstances). There will also be a talk at the Haskell Implementors Workshop.
I've been following Edward Kmett's [indexed](https://github.com/ekmett/indexed) work on GitHub, which only works with 7.6 and wanted to see what was so magical about 7.6. A bit of Googling later, and I came across this great talk!
"Monads are the ultimate Dad’s shiny red corvette that he lets you drive around the parking lot, but you must always put it back in the garage and never drive anywhere with it." Here you go: http://www.allproducts.com/manufacture100/kobecoltd/product1.jpg You're too good a developer to be spouting such nonsense swizec. The time it took you to write your rant could have been spent in #haskell getting the answers you needed.
I think they've changed it slightly - that finger used to look even more like a penis than it does now!
Hey Neitz, I work at Skills Matter and we will be recording all talks. They'll be online pretty much on the day, say, an hour or so after they happen. Free to watch for all regardless of where you find yourself. Though it of course is no match to actually being there... ;)
I am not convinced you need to _retrict_ sharing, at least not in the way you are maybe thinking, you simply need to track sharing explicitly so the framework can make use of this info. I don't think this means going all-out arrowized either. For instance, Ed Kmett has a very interesting new library, [machines](http://hackage.haskell.org/package/machines-0.1.1) that lets you write in a very straightforward "pull-oriented" style, at least locally, while still retaining enough sharing info to be able to do deterministic resource deallocation. This is something that pipes also tries to do. &gt; This reminds me of arrows in the FRP world. They did solve some of the implementation issues, but there, the notational burden is already too much for my taste. My problem with arrows is more that they aren't expressive enough to encode certain computations. There are also claims that they aren't as 'natural', though I wonder if that's more a function of what you're used to. I think the important thing is that whatever model you are using composes, lets you remove code duplication, and gives you reasoning tools.
 -- | Derpendency projection. (Work around) herp :: (Fst ij ~ i, Snd ij ~ j) =&gt; p '(i,j) -&gt; p ij herp = unsafeCoerce -- | Derpendency injection. (Work around) derp :: (Fst ij ~ i, Snd ij ~ j) =&gt; p ij -&gt; p '(i,j) derp = unsafeCoerce Heh.
Sorry, forgot to italicize that bit.
Could you expand a little on that? I'm interested.
You can actually track sharing explicitly if you are willing to put up with monadic syntax or some other DSL that lets you talk explicitly about variable bindings. As a straw man: do x &lt;- blah return (x, x + 2) There are lots of directions you can go with this. Maybe you think this syntactic overhead isn't worth it? You can even use [observable sharing using stable names](http://www.ittc.ku.edu/~andygill/paper.php?label=DSLExtract09), though I think that approach is a little to sledge-hammer-y.
Well, my phrasing was an ironic reflection on the "for experts" and "that's what it's all about" tone of the discussion, and I'm very rusty, but let's have a go. First of all, one view of a monad is as an engine for constructing terms from free variables for a particular language, so let me try and expand on this a bit: 1. If `X` is a set, which we can interpret as a set of names for variables, then `T X` (where `T` is our monad) is to be understood as the set of all possible terms in our language with free variables drawn from `X`. For example, if our language has one binary operation (and nothing else) then `T X` is the set of binary trees with leaves drawn from `X`. 2. The unit of `T`, a function `X -&gt; T X` takes each variable to the term consisting of just that variable. 3. The counit of `T`, a function `T T X -&gt; T X`, performs substitution: a term in `T (T X)` can be thought of as a term in `T Y` for some convenient set of variables `Y` together with an assignment (substitution) of a term in `T X` to each variable in `Y`, and the counit constructs the term in `T X` obtained by performing all the substitutions. That can be thought of as the universal algebra view of a monad, in a sense a monad is a language, and arrows in the Kleisli category are substitutions. However, a language is no use without models. A model of our language, or to say the same thing, an algebra of a monad, is quite simply a set `X` (the carrier of the model) together with an "interpretation" function `h: T X -&gt; X` which specifies how each term in the language is evaluated in the model. This must satisfy a couple of consistency equations (h x = x when x is a variable, and h respects substitution). Naturally morphisms between algebras are easily defined (functions on carriers which respect the interpretation), and the category of algebras turns out to nicely recover the original monad. I'm now thinking of chapter VI (Monads and Algebras) of [CWM]. For a final bit of context, some reminders of some relevant category theory: 1. Every adjunction gives rise to a monad 2. Every monad has an associated 2-category (I guess) of adjunctions which all give rise to the same monad. 3. The category of algebras is a terminal object in this 2-category. 4. The Kleisli category is an initial object in this 2-category. Sorry, this doesn't have much to do with Haskell. [CWM] Saunders Mac Lane, Categories for the Working Mathematician
That makes sense, thanks. This has been an interesting discussion. I have some more ideas and I may ping you sometime to talk more about this. :)
It works, thanks!
That was very clear and illuminating. Thanks.
You're welcome! I hope you will find Cumino useful! Don't hesitate to open tickets on github too!
Sure. :)
Ah good, more recent haskell talks. Any other good ones recently? 
In my post I only wanted to address a characterisation (and in my opinion the most transparent characterisation) of a Haskell typeclass, not a category-theoretic construction. This is why I was careful to write "`Monad`" rather than "monad" (although I missed one -- oops!). I agree that the tone of the article was somewhat provocative, but I am frustrated by seeing so many misleading `Monad` tutorials, and so many misleading diagrams. I don't think even acknowledging the existence of a similar concept in mathematics would have been helpful to the exposition. The correct formulation of the equivalent more general result would not be particularly enlightening for Haskell programmers. `Monad`s are quite special monads, not least because all `Category`s are enriched in Hask. I agree with you that the it is the connection to universal algebra that leads to monads' significance in Haskell, although following Hyland and Power [The Category Theoretic Understanding of Universal Algebra: Lawvere Theories and Monads] I am inclined to think that the monadic approach to universal algebra in general and in the semantics of effects in particular is an accident although I am far from expert in such matters. [I have one minor correction to what you wrote: the function `T T X -&gt; T X` is typically called the "multiplication". The "counit" is actually the natural transformation from the identity functor to the comonad for the adjunction.] 
With your free monad expertise I'm surprised to learn you didn't already know that, Tekmo!
aka liftDerpDerp
Actually, in looking at the code you're complaining about in the article, your problem isn't with Maybe *as a monad*, after all, you hardly even mentioned return or (&gt;&gt;=); your problem is with Maybe *as an ordinary datatype*, and a solution to the problem you're having is ironically provided by making use of the Monad instance. The Monad instance for Maybe lets you locally treat the results of the operations as if those operations were successful, and if any of them happens to fail, the whole thing fails. For example, let's say we have this list: prices = [("apple",1.05), ("blueberries",2.50), ("pear",1.75)] and now we want to look up the value corresponding to an apple and a pear and add them. Presumably we'd like to use lookup :: Eq a =&gt; a -&gt; [(a,b)] -&gt; Maybe b But oh, there's that Maybe result, so we pattern match: case lookup "apple" prices of Nothing -&gt; ... Just a -&gt; case lookup "pear" prices of Nothing -&gt; ... Just p -&gt; ... a + p ... Okay, well, the lookups might fail (imagining that the list is our totally webscale MongoDB), and we're meant to use Maybe to encode that, I guess, so we write: case lookup "apple" prices of Nothing -&gt; Nothing Just a -&gt; case lookup "pear" prices of Nothing -&gt; Nothing Just p -&gt; Just (a + p) and we're starting to hate Maybe at this point, with all the noise regarding Nothing and Just. Monads must be terrible! Well, hang on, we haven't actually used the fact that Maybe is a monad. What if we actually used the Maybe monad? This code becomes: do a &lt;- lookup "apple" prices p &lt;- lookup "pear" prices return (a + p) Or if you're a little more advanced, you might even write: liftM2 (+) (lookup "apple" prices) (lookup "pear" prices) Much nicer, no? Of course, if the Maybe values are really the results of IO actions, you may need to name those results before doing this (unless you're super-fancy and use liftM2 twice), but you should start to get the idea here. The Monad instance is there to save you the trouble of handling all these potential failures individually, not to get in your way. :) Some other people suggested using fromJust, but in my opinion, this is usually the wrong suggestion. You *can* use fromJust in the case where you just don't care if your program crashes at runtime because the data is bad, or in cases where you can actually prove that Nothing is impossible. But usually in the former case, you'd be much happier presenting the user with a more reasonable error than "*** Exception: Maybe.fromJust: Nothing", in which case, you can use the Maybe monad to combine results of possibly-failing things up to a point, and then react to the failure all at once by pattern matching and producing a nicer message.
It's default now :) Only a few days ago.
If you haven't read it already, I highly recommend his [monad tutorial](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html).
I came across it when I first started learning haskell. Back then I think I was a little overwhelmed by everything. Now that I am getting comfortable, I will have to take another look at it.
http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/profiling.html
Good to hear this is being done! Nice work. I wanted to do something similar as I believe it will solve the vast majority of issues with cabal...
Also, why use '.' for yet another thing? We could have used '_' instead. 
That's not what Amdahl's law is talking about.
Exactly. Thanks for bringing this up. I think it is especially relevant for commonly used modules, that you import qualified. Say I would prefer to write import qualified Map over import qualified Data.Map as Map
&gt; Free to watch for all regardless of where you find yourself. You've just made my day! This is truly awesome and I can't thank you enough. 
Thinking about this, it may be a good idea to avoid the phrase "tracking sharing" altogether, as it is easily confused with "observable sharing". A better term would be "tracking life time". The conclusion so far is that only the garbage collector can track the life time of a value that can be shared freely.
An advantage of your proposal is that Java programmers can now write toString :: Show a =&gt; a -&gt; String toString = show {-# PRAGMA SpecializeNow! #-} toString :: String -&gt; String toString = id and feel at home. But do we want to allow something like foo :: (Monad f, Monoid m) =&gt; m -&gt; f m -&gt; f m foo = {- something generic -} {-# PRAGMA SpecializeNow! #-} foo :: [a] -&gt; [[a]] -&gt; [[a]] foo = {- something specific -} or do we want to restrict it to specialising a single type parameter at a time?
I wouldn't. E.g. in an oral conversation saying Data.Map clearly implies that you're talking about the module, while Map by itself could refer to anything — a function, a type, or a module (given your proposal is accepted). import qualified Map is already long enough to have it bound to an editor macro.
&gt; Reactive-Banana &gt; Haskell and higher education &gt; Algebraic Dynamic Programming &gt; XenClient for the Xen Virtualizer &gt; They are in German, though. :(
English-speaking person: “Ohh, so that's what it feels like.”
Eh, I think Java's approach is superior at least in that packages are namespaced by organization rather than some abstract notion of intent. Then again, having to tie a package to a domain name is obnoxious and overly verbose. Really, namespacing packes by `package_name.module.submodule...` or perhaps `organization/author.package_name.module.submodule...` seems like the best approach. Though Haskell doesn't prevent you from doing this (nor does Java), it's not convention and I'm wary of doing so.
Hierarchical modules can make it easier to browse modules on your system. So you can look under Codec.Binary and see what is useful there. If the module were just named UTF8 then the codec modules would be scattered all over the hierarchy. One counterargument to this though is that you should browse package by package rather than looking at the whole hierarchy.
 zipWith :: (zipWith -&gt; map -&gt; concatMap) -&gt; [zipWith] -&gt; [map] -&gt; [concatMap]
org.haskell is not likely to be a module itself. Also, import on demand opens up the possibility of preventing compilation if a conflicting symbol is introduced.
Fantastic! mixing regular function names for extra incoherence!!
`foo` is looking a lot like an automatic typeclass to me. So we can just reuse all the typeclass rules and extensions to answer your question. Except now we've pulled functions out of their typeclass to make typeclasses out of them. I'm not sure we're getting anywhere here.
The dot in qualified names means something entirely different from the dot separating parts of a hierarchical module name. There is no reason they should look the same. 
Overall I think it's a good idea to have minimal complete specifications (MCS) in a machine-checkable form. However, there is one glitch in your proposed scheme... Namely, what the default instances should be can depend on which MCS is being used. We don't get this for Eq, Ord, or YinYang, but it does show up for more complex classes. So really, the default implementations should be tied to the individual MCS, rather than being tied to the whole class. Of course, at that point, what we really have are "instance constructors" and there's no specific reason why such constructors should be tied to the class itself. E.g., given a Monad instance we can construct a Functor instance; or given a Traversable instance we can construct a Foldable instance. However, in both of these cases, the more powerful interface is a subclass of the less powerful one. There's no reason the less powerful class needs to know that it's being implemented by the subclass; and if we declared instance constructors as part of the class specification, we'd introduce a circularity which could become problematic down the line.
&gt; circumstances where the memory cost of laziness is higher than the time cost of recomputing things Oleg wrote to me in May about how memory performance, being the bottleneck in High Performance Computing (HPC), is the very loud squeaky wheel that gets all the oil unlike raw compute power. "Memoization is really bad" and communication with main memory absolutely needs to be minimized. 
Thank you :)
This also solves another problem: documenting which package a module comes from. I always hate having to google for the origin of a module or commenting in a tutorial where to find a module.
How does that help?
I wouldn't say this is the *only* reason. I for one would prefer a Monad typeclass defined like so: class Functor m =&gt; Monad m where return :: a -&gt; m a (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b join :: m (m a) -&gt; m a m &gt;&gt;= f = join (fmap f m) join mm = mm &gt;&gt;= id That way I could choose whether I felt like defining a monad instance in terms of `join` or in terms of `&gt;&gt;=`, or whether I wanted to provide both for performance.
Are you aware of ghc-pkg find-module? Obviously it only helps for things you've already installed, but still.
That's a bit restictive! Shouldn't it be (&gt;&gt;=) :: (Monad warmfuzzy) =&gt; warmfuzzy thing -&gt; (thing -&gt; warmfuzzy thing') -&gt; warmfuzzy thing' instead?
What you are asking is closely related to this blog post. http://blog.sigfpe.com/2008/12/mother-of-all-monads.html 
I am now! Thanks.
Sorry. Just typing too fast, you know? (You may moan now.) (&gt;&gt;=) :: (Monad warmfuzzy) =&gt; warmfuzzy thing -&gt; (thing -&gt; warmfuzzy thingy) -&gt; warmfuzzy thingy
Actually, I believe the culprit in that example is `Any`, not `herp` or `derp`. In order to implement a type family that can match on `Any`, you need to import `GHC.Exts`, which is not a `Trustworthy` module, so you've taken correctness into your own hands.
Also relevant: [We show that any monad whose unit and extension operations are expressible as purely functional terms can be embedded in a call-by-value language with "composable continuations". ](http://www.diku.dk/~andrzej/papers/RM-abstract.html)
That's how I got to it in the first place! I also noticed that the type of `callCC`, (MonadCont m) =&gt; ((a -&gt; mb) -&gt; m a) -&gt; m a can similarly also be expressed as: (MonadCont m) =&gt; ContT a m (a -&gt; m b) ^(Hey, I heard you like continuations, ...)
I think it is an *awful* restriction that type variables can't be symbols or unicode. (&gt;&gt;=) :: (Monad €) =&gt; € § -&gt; (§ -&gt; € $) -&gt; € $ 
This is so clear. I've read a few articles on Monad Transformers and I think that this is really clear due specifically to the lack of type definitions. Idk about everyone else, but a lot of times, I feel that those end up overshadowing what is actually happening in the code when I try to understand the type of the function. Using this and :t in GHCI makes this a concise and easily understandable little post. Thanks!
We're so arrogant.
I prefer to think of TDD as "Type **Directed** Development". In TDD, you write your value's type signature first, and then you refine that signature until you get an implementation. Okay, so in Haskell, we don't actually have any editor support for refining types into suitable terms like in Agda or Epigram. Also, I'm not sure how refactoring fits into the TDD framework yet.
&gt; To prevent that you should, whenever possible, use tail recursion Not quite true in Haskell. It depends on whether you're writing an eager or a lazy computation. And even these more specific circumstances, it's more nuanced.
I like that some of the emphasis is placed on learning to *read*, watching for significant whitespace or capitalisation. Training for perception is mission-critical. Meanwhile, a pedant writes: that which is named "currying" is not currying but partial application. (I know, yawn.)
That's a good informal attempt, but the article still needs significant editing to make it more succinct and easier to follow. My first, high-level recommendation would be to split it in two; the dependent types part should be spun into a "Part 2" article.
Only scrolled through the article for now, but instance Bool where struck me.
Exactly! I was writing from the point of view of a newcomer for whom such an error message is completely cryptic.
Well, any error message will be cryptic if you don't know what it's talking about. There's cryptic and then there's cryptic. If you didn't know what a type was, or what a function was, or what a pointer was, you'd be mystified by error messages about those things too. That doesn't make the error message cryptic, it makes your knowledge incomplete.
But of course it *shouldn't* tell you that doing Y is wrong, because Y is *not* wrong. Not objectively. It's only with the baggage of prior expectations that you've convinced yourself there is such a thing as right and wrong. You can, for instance, perfectly well define some sort of number instance for strings (128-imal or whatever you like) and there you have it. And so on. And it's *not* crazy to do such things, it's just unlikely (for now). Who knows. Adding strings, or at least lists of symbols, is certainly sensible. It gives you a way to handle arbitrary bases, for instance. You could have a general type class for numbers with carrying operations, define that on the symbols, and then lift that to lists of symbols and use that for your Num instance, and hey presto. And if you're really keen you could figure out some way to make it super generic so that all you need to do is give a set of symbols and an ordering over them and you can bootstrap yourself into general numeric types. Which really all goes to the point: "crazy" is subjective, or at the very least, task specific.
How the... How? EDIT: I assume it has something to do with number bases but I haven't yet figured out what base the interpreter thinks I'm working in. 0o60 == 48, but 0o59 fails to evaluate.
Hey man, have you been drinking? Terminal sigma, please. (&gt;&gt;=) :: Monad μυθος =&gt; μυθος λογος -&gt; (λογος -&gt; μυθος παθος) -&gt; μυθος παθος 
Octal, I think. 0o60 is a valid octal literal, but 0o59 is not.
The more general response to this is that you're missing the point, because you're having a hard time looking at it from the perspective of a newcomer. The more concrete answers are: 1. Seriously, most of the time GHC tells me that it doesn't have a `Whatever` instance for `a -&gt; b`, its suggested "possible fix" that I define such an instance is **wrong**. What happened most likely is that I made some dumb error like forgetting a `+`. So the "possible fix" suggested by the compiler is not helpful to experienced users, and the opposite of helpful to inexperienced users. 2. I know it's easy to define a `Num b =&gt; Num (a -&gt; b)` instance (or maybe a `Num ((a -&gt; a) -&gt; a -&gt; a)` instance?). It even sounds useful to do so, because it allows nifty tricks like `average = sum / (fromInt . length)` (with a `Fractional b =&gt; Fractional (a -&gt; b)` instance). The problem is that it just compounds the error message problem, because now `1 2` is not an error; so when you type that instead of what you meant, either you get an error somewhere else, or you get no error at all.
Well, I've made three related arguments here. I'll call the first one the "scare quote" argument: applying a number to an argument is "wrong" (with scare quotes), and the idea of defining a `Num` instance for functions is "crazy" (again with scare quotes). You're answering that argument, but there's a reason I call it the "scare quote" argument. The second argument is that the "Possible fix: add a Num (a -&gt; b) instance" part of GHC's error messages is generally unhelpful or misleading, even if you know the language. The premise of this argument is that most of the time the suggested fix misidentifies the nature of the error. By this argument, we can improve the error message simply by deleting that line. The third argument is that even if the functionality afforded by some sort of `Num (a -&gt; b)` instance, actual experienced Haskell programmers will refrain from it because it would lead to confusing error messages—or to a program being accepted that doesn't mean what they think it means. You'd have a `newtype` in this case.
Does that mean that, in Scala, `callCC` should have the signature callCC[M[+_]: Monad, A](f: ((A) =&gt; M[Any]) =&gt; M[A] ): M[A] in order to get the same effect? EDIT: Got my `A`s and `B`s mixed up.
Regarding the second argument: it's only unhelpful or misleading *if you didn't intend to do that*. If you *did* intend to do that, then it's certainly very helpful: it's telling you you forgot to do something! Again, it depends on your goals. Hence, you shouldn't remove that line at all, but rather keep it, and if it weren't there, you should add it. For the third argument: you're acting as if people don't create Num instances for things. Sure, they'll rarely create one for `a -&gt; b' but you might create one for, say, `(Int, Int)` or something isomorphic to that, like, oh, an implementation of `Rational`. And then you would definitely want this kind of error message. You can't say an error message is bad or cryptic just because *you* can't imagine why it could be useful. Lack of imagination and a handful of bad examples is not an argument.
Currently i'm using https://github.com/eagletmt/ghcmod-vim, but it is quite slow. I'd like to try hdevtools but I get this when I try to build it: src/Info.hs:53:23: Not in scope: `GHC.printException' src/Info.hs:120:13: Not in scope: data constructor `GHC.RealSrcSpan' 
More like: callCC[M[+_]: Monad, A](f: ((A) =&gt; M[Nothing]) =&gt; M[A]) : M[A] The type basically says that the reified continuation (of type `(A) =&gt; M[Nothing]`) does not return when called (because [undelimited continuations are not functions](http://okmij.org/ftp/continuations/undelimited.html))
Accepting your earlier argument that it's most important that the error be clear if you do understand type classes, it would still be thoroughly obvious to anyone who knows the first thing about type classes that you could respond to "No instance Foo ..." by making an instance. Do you imagine someone who intends to make (Int,Int) an instance of Num staring dumbly at the "No instance Num (Int,Int)" part, and going "Aha, that's what I forgot to do" at the suggestion to make an instance? And if so, at anywhere near the rate the people just starting look at the suggestion and set off to learn how to define instances when that's not at all the problem?
&gt; What I'm saying is that someone who knows what a type class is (i.e. anyone who's programmed in Haskell for more than a few days) will understand that error. But the target audience of this post doesn't seem to be people who have programmed in Haskell for more than a few days. Did you read the next couple sections? They describe the syntax of function application and function definitions. Type classes aren't mentioned anywhere in the entire document. No one is saying it's not a sensible error message. The author is just saying it's non-obvious to new users, which it is.
&gt; Regarding the second argument: it's only unhelpful or misleading if you didn't intend to do that. If you did intend to do that, then it's certainly very helpful: it's telling you you forgot to do something! Answer: most of the time you don't intend to do that, and if you did, you'd easily figure from "No instance for (Num (a0 -&gt; t0)) arising from the literal `1'" &gt; For the third argument: you're acting as if people don't create Num instances for things. I know people create `Num` instances for all sorts of things. I don't believe, however, that they do so for function types unless they're hidden behind `newtype`. I.e., we'd see this: import Control.Applicative newtype NumFunction a b = NF { runNumFunction :: a -&gt; b } liftNF op (NF f) = NF $ liftA op f liftNF2 op (NF f) (NF g) = NF $ liftA2 op f g instance Num b =&gt; Num (NumFunction a b) where (+) = liftNF2 (+) (-) = liftNF2 (-) (*) = liftNF2 (*) negate = liftNF negate abs = liftNF abs signum = liftNF signum fromInteger = NF . const . fromInteger ...but not this: import Control.Applicative instance Num b =&gt; Num (a -&gt; b) where (+) = liftA2 (+) (-) = liftA2 (-) (*) = liftA2 (*) negate = liftA negate abs = liftA abs signum = liftA signum fromInteger = const . fromInteger **EDIT:** Also not this: {-# LANGUAGE FlexibleInstances #-} import Control.Applicative instance (Applicative f, Num b) =&gt; Num (f b) where (+) = liftA2 (+) (-) = liftA2 (-) (*) = liftA2 (*) negate = liftA negate abs = liftA abs signum = liftA signum fromInteger = pure . fromInteger 
Sorry, I mean the "No instance for (Num [Char]) arising from a use of `f'" part (and I completely agree about removing the "possible fix" suggestion).
Yes of course they don't do it for function types (usually), but so what? Now you're complaining about how *some particular* error messages aren't very useful, which is completely irrelevant to the point. You've lost track of what we're talking about.
Yes, and my point is that an error message that a newbie doesn't understand simply because the language is new to them does not make the error message cryptic. Why is this so hard to understand? The error message isn't cryptic at all. Period. End of story.
That's certainly true. The type class is pointlessly suggesting a solution. *That's irrelevant to my point.*
This appears to do basically the same as ghcmod-vim, but faster and more reliably for me. Nice!
Ok, if we're not disagreeing about whether or not it's cryptic, then it's a separate matter that I have no strong opinions on.
&gt; Regarding the second argument: it's only unhelpful or misleading if you didn't intend to do that. If you did intend to do that, then it's certainly very helpful: it's telling you you forgot to do something! Again, it depends on your goals. Unfortunately GHC doens't know your goals; it can only guess. Whether or not it's worth it make "possible fix" or "probable cause" suggestions depends on *how likely* they are to be correct, *how useful* they are when they're right, and *how confusing* they are when they're wrong. The existence of a case where the suggestion is what you want to do isn't sufficient reason to always make that suggestion, or else we'd get error messages like Not in scope: `typo' Possible fix: add a definition for `typo' In the case of the "add an instance" suggestion, I think that not giving the message when the suggested instance would be an orphan would drastically cut down on the number of spurious suggestions at very little cost. 
Since this has bothered me for quite some time, I [filed a ticket](http://hackage.haskell.org/trac/ghc/ticket/7222).
Awesome!!! I think you really nailed the parts of Haskell that can be initially off-putting. When you're used to each new language being a small set of syntactic and semantic deltas it can really seem quite bizarre. Something else that can take a while to get used to is the proliferation of operators, and parsing the precedence. Definitely a bit of a learning curve associated with each additional introduction of combinators. One thing to learn is that there's a bit of a tradition of using layout to make precedence clear - breaking in such a way that the operators contiguous in a line have higher precedence (binding tighter) - and also lining up logically related combinators (often ($) and (.) for example). These layout habits can also seem pretty bizarre to the uninitiated, but greatly aid reading (and writing, even, because you can do line-level manipulations that have semantic significance). In general, after becoming proficient with reading and understanding the language, there's the learning curve of style. Style is of course multi-faceted, but since we're mostly talking about syntax and layout, I like this guide: https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md
For smaller projects, I love that the system imports do have dots, and my imports don't have dots. Same distinction as #include &lt;file&gt; or #include "file" in C, only prettier to look at.
That sounds like a deficiency in the grammar more than anything? :(
One suggestion though... Because of Haskell's evaluation strategy, explicit recursion is error-prone and easily leads to accidental stack overflows and the like. Combinators like foldr, foldl', map, etc. really are preferred, both because they're better style, and because it's harder for a beginner to get accidental overflows out of.
Me, too. The new version also has much clearer semantics. None of that reawait stuff, which I got lots of complaints about (rightly so).
Since the type for composition is `(a -&gt; Proxy ...)` does that mean one *must* use `&lt;-&lt;` instead of `Control.Category..` for `Proxy` composition? Or, some newtype.
Hmm, that was the case for `Pipe` as well :(
Nice! I really like how this work is progressing. Are you going to start writing libraries on top of pipes? E.g. http-conduit/network-conduit like things? I'd love to use pipes, but I don't want to write plumbing when I don't have to (so I'm left using conduit).
I'm not wedded to the syntax, though it might look better using layout instead of explicit `{;}`. Using a pragma would avoid introducing a new keyword. Can we use layout inside a pragma, or do they have their own rules?
Yes, I will. It will take a couple of months before I release a standard library, since I want to get out the two extension releases first, but it will happen.
I haven't looked at the code yet, but this is lovely.
A shame, I would almost like to see Jon Harrop give a talk at a Haskell conference :)
Basically, all iteratee (or consumer), generator (or producer) and pipes are simple coroutine type with suspension functor data Request a b = Request a (b -&gt; x) like for iteratee, use Request () b , for generator, use Request a (). One interesting problem is how to support multiple interface coroutine generally, i.e. having several types of Request. I used a signature approach, which is a GADT definining argument and result type of each request, but using existential type, only signature is passed as type variable. In monad, this is only partially type-safe. (only for client, not server.) server has a session until it finishes its responding operation. I think we need indexed monad for server for full type-safely, but I just gave up at the point by just allowing partiality. I would like to see how this problem is nicely solved. What I described here is desolved in Control.Monad.Trans.Crtn and Control.Monad.Trans.Crtn.Object in my coroutine-object. (without much documentation unfortunately now)
It's okay, I can understand your code, which is very clear and easy to follow. There are two issues I encountered when trying out what you just discussed. The first issue is that, as far as I can tell, you cannot define a correct bidirectional composition operator unless you introduce the extra input parameter, i.e.: a -&gt; FreeT ... If you remove that parameter there is no sensible definition for composition or identity and you see this pretty quickly if you try to define the identity `Proxy` without it. The second issue is that even if you find a type-safe way to define multiple interfaces you also need a way to automatically derive composition correctly. The original reason I created the `pipes` library was to combat the culture of ad-hoc coroutine frameworks and to simplify the number of ways to combine coroutines.
And then you don't gain anything from the ResourceT and close.
The goal is that you want to finalize resources exactly when you are done with them: no sooner, no later. With lazy `IO` you have no control over when you are done with a resource, so you never know when to finalize it. If you finalize too early, you get an error. If you finalize too late you waste a scarce resource.
What when you write your code in a way, that the action that does the very last IO also triggers the close at its end? Unless the actions are out of order (which is the exact thing a monad is there to prevent), that should do it, no?
The problem is that with lazy IO there might be no final read, such as when you take just the first line of a file.
Yes, I have taken a look now. The biggest difference seems to be the type of composition, since the arguments have to take their first requests as arguments now.
Right.
I agree, type operators starting with `~` should have been reserved for type variables.
@psygnisfive: I've been following this thread with interest. You are right that the error message is logical. And yet it's totally useless. It reminds me of this joke that my 9-year old brought from school: A programmer goes shopping and his wife is giving him instructions: "Get me 5 apples and if there are eggs, get a dozen." The programmer comes back with a dozen apples "because there were eggs at the store." The "add an instance" suggestion from GHC has *never* worked for me. It was *always* triggered by a type mismatch. The only thing that could make this piece of advice more annoying would be if it came from an animated paper clip.
I feel like making library APIs look neater at the expense of making some library code slightly messier is a reasonable tradeoff. Maybe I'm not considering some use cases for type variable operators though?
I've been yearning for something like this lately, great work!
I believe Tekmo is planning on writing a solution &amp; tutorial regarding this. The basic idea is, when you send requests or responses, you also provide a finalizer. (response', finUp) &lt;- request (myRequest, finMeAndDown) (arg', finDown) &lt;- respond (myResponse, finMeAndUp) Then you can bundle the finalizer with the result type, and create a light wrapper around `runSession` that runs the finalizer before it retrieves the final value. finalizeSession :: Monad m =&gt; (() -&gt; Session m (r, m ())) -&gt; m r finalizeSession s = runSession s &gt;&gt;= \(r, fin) -&gt; fin &gt;&gt; return r If you want to finalize upstream or downstream early (e.g. when you know that you will never again communicate with them) you can just call the appropriate finalizer with `lift`.
 GHCi, version 7.4.1 &gt;&gt;&gt; :k (-&gt;) (-&gt;) :: * -&gt; * -&gt; *
Lots of TODOs on that page.
Fair enough. I've had the opposite experience.
 class Arrow a where first :: b `a` c -&gt; (b, d) `a` (c, d) 
How do you respond to this quote, from the MetaHDBC draft paper in reference to the HaskellDB library: &gt;HaskellDB is restricted as it neither include any RDMS speci c features, nor do it support all of the standard SQL features. The latter is unfortunately, but avoiding RDMS speci c features is a two edged sword, as it do preclude the programmer from useful (RDMS 9 speci c) features, but also shields the programmer from accidently becoming dependent on one particular RDMS. (sic) I'm currently using MetaHDBC in a project and really like the programming flow - all of my queries are bounced against the database at compile time.
Replying to my own post... I've tried. My library that compiled with 7.4 doesn't with 7.6. Any chance we could get an overview of the API changes?
Looks awesome. Bonus points for not using TH. Can existing db tables be turned into that Persistent definition thing? I didn't find it in docs. 
It seems neat. If it provides the same level of type checking of ghcmod, I could make the switch. In know, the linting capabilities of ghcmod are hard to beat, but it's so damn slow..
I suddenly feel like I'm looking at a different language.
oops
I am curious about the use of two functions, `select` and `from`. Is that just to stay close to SQL syntax? Is there any time where you wouldn't use `selectFrom = select . from`?
cheers I did touch on laziness in the first article in August, though only implicitly (the fizz buzz example), and it will be in the background for several of the future planned articles as something that supports a particular style of programming 
I did agonize over this. It was rather longer than I planned (and took a while to write...). But, in the end I thought it better to try to give a high-level view of everything rather than splitting it over two months. Future articles will not be that long!
Relying on the GC is the whole problem. It results in nondeterministic finilization, which is fine in many programs, but not in servers getting many requests per second.
Sorry I don't get it. How is it "in French"?
It's unrelated. In fact, if `Proxy` could be made an `Arrow`, then you could make `Pipe` an `Arrow` too, by specialization.
&gt; So this combines all the shortcomings of persistent No, it definitely doesn't combine persistent's difficulties with joins. Your snarky summary is definitely wrong. (In other ways too, this is just sufficient to prove the point.) The entire point here is to harness strong type systems and get good query composability. Using raw SQL brings neither. This may not _succeed_ at those lofty goals, but it's probably at least on a right track. (There are multiple right tracks here.)
Yes, you can, but for now we have just one: [countRows](http://hackage.haskell.org/packages/archive/esqueleto/0.2.4/doc/html/Database-Esqueleto.html#v:countRows), which is COUNT(*). You can see it in action [on the test suite](https://github.com/meteficha/esqueleto/commit/df63cd864b477da4d83b91fcc4b680548a9ec869). EDIT: By the way, please file issues or send pull requests for any other aggregate function you may need =).
Why would it be interesting in it's own right?
Staying close to SQL syntax is one of the reasons. The other reason is composability. fooByName :: SqlExpr Text -&gt; SqlQuery (SqlExpr (Entity Foo)) fooByName name = from $ \foo -&gt; do where_ (foo ^. FooName ==. name) return foo The function fooByName may be used as part of any other query, for example: foosFromBarByEmail :: (...) =&gt; Text -&gt; SqlPersist m [Entity Foo] foosFromBarByEmail email = select $ from $ \bar -&gt; do foo &lt;- fooByName (bar ^. BarName) where_ (bar ^. BarEmail ==. val email) return foo However, foosFromBarByEmail may not be used on another query: it already has flesh (if you would pardon my poor pun :). So select is how you get out of the SqlQuery monad. Of course, we could do something such as renaming "select" into "run" and "from" to "selectFrom". The point is that having two different functions does serve a purpose. =)
We're looking forward to it (or at least I am) :-)
I just wanted to thank you for your lucid article in the Monad Reader that pretty much inspired all my work.
&gt;The entire point here is to harness strong type systems and get good query composability. Using raw SQL brings neither. Why would you make such an absurdly misleading statement? Try reading the post you are replying to. Let me try to make it clearer for you: &gt;Why not just write actual SQL and **use hssqlppp to type check it against the actual database**
We can't define `first`, which doesn't work even for the restricted case of the `Pipe` type because there is no synchronization between the two interfaces. I remember that we also tried using sums instead of products and I don't remember exactly why we stopped but I think it was because it still violated the arrow laws.
hssqlppp has some tools that may someday be combined into a composable query language, but it is not itself composable. (Or if it is, no examples were given.) Just having a syntax tree for SQL is not a composable interface. And raw SQL strings are even less composable, which is what your comment appears to be calling for when you say "actual SQL". Your proposed solution is not the same thing, and certainly won't be close to the same experience in practice.
Why wouldn't it be? Under an F-functor, we can discard a *Monad m*. By itself, without appeal to additional laws, that's already interesting because that can only happen in unusual circumstances. 
What if it were made an extension? Then anyone who enables `-XTypeLambdas` should know what they are getting into, and anyone who doesn't want to deal with that can still have proper inference.
If I had to guess, I believe they might talking about an arrow over different variables. Perhaps they mean: newtype N f a b = N (a -&gt; f b) instance (Applicative f) =&gt; Arrow (N f) where arr f = N (pure . f) first f = ???
Personally, I'd want a clarification of the statement, because PostgreSQL does report types. And Sqlite is dynamically typed, so such a statement is at the fringes of sensibility on that count.
&gt;I feel pretty safe in assuming that by "actual SQL" You don't need to assume anything, since I was completely explicit. Just read. Yes, SQL. Using hssqlppp to type check it. This means that when comparing it to esqueleto, "but it isn't type checked" is not a valid complaint. My point, which I made quite clear from the start, is that you can already get type safe SQL, without all the limitations imposed by poorly thought out libraries like esqueleto. Hence my question, why not just do that instead? A question which despite replying many times, you have not attempted to answer.
I think if you specialize it to the `FreeF f r` functor you basically get the free monad transformer, however I may be grossly mistaken as I have only had a chance to briefly skim through it.
Are you referring to something like what was in `pipes-1.0`: data Pipe a b m r = Pure r | M (m (Pipe a b m r)) | Await (a -&gt; Pipe a b m r ) | Yield (b, Pipe a b m r )
What happened to the Mac `.pkg` installers? 7.4.2 had them, but it looks like 7.6.1 only has bindists.
We are looking forward to it as well. :-)
[This comment of mine](http://www.reddit.com/r/haskell/comments/z4inb/invert_the_inversion_of_control/c62rslj) posted some days ago exposes the way I see the relation between CPS and monads. **tl;dr**: Yes, the're equivalent: if you're doing monads, you're doing CPS in the &gt;&gt;=. If you're doing Cont/ContT, you're doing CPS into every action. Same principle, different level of dynamicity: ContT enables you to determine how the next continuation will be called depending on the result of the current action. Regular monads don't: they're static in their way to call what happens next.
&gt; My understanding is that they represent tuples where the fields are not necessarily put next to each other in memory at all It depends on your unboxed tuple's elements being themselves unboxed or not.
I don't think we are on the same page, here. I'll explain it the way I understand it. Here's a boxed tuple, as it would be referenced from a register: pointer | | tuple (contains two pointers) /\ / \ A B Here's a tuple of unboxed values (e.g. `data Pair = Pair {-# UNPACK #-} !Int {-# UNPACK #-} !Double }`) pointer | | tuple (contains A and B directly) Here's an unboxed tuple of boxed values. Note that there is nothing actually associating the two pointers. Each pointer would be in a *separate* register: pointer pointer | | | | A B Finally, here's an unboxed tuple containing unboxed values: A B Again, note the complete lack of anything resembling a struct.
The "arrow laws" (actually, premonoidal category laws) are actually satisfied by the various instances using sums as the underlying monoidal structure. I think you just didn't like the fact that the interchange law doesn't hold (`first f &gt;&gt;&gt; second g /= second g &gt;&gt;&gt; first f`), i.e. that the category is only premonoidal, and not monoidal.
Yeah, I'm not really sure what that part means. We have both source and binary package systems -- commercial components should be able to use either of those...
Just a follow up, I wound up switching everything over to this about a month ago.
The constructivists amongst us may wonder whether the question might be backwards.
Yes, and the big focus of the next release is the use of functors to promote compatibility between code written to varying feature specs. I'm going to write a lot more about it in an upcoming post, but the basic idea is that I formulate all pipe extensions as functors from a one pipe type to another. This means that code written to a simpler feature set can be automatically be promoted to be compatible with code written to a more complex feature set. The advantage of this is that you don't have to standardize on a particular monolithic type and you can write code targeted to just the features you need and rest assured that it's future-proof. Using the specific example you mentioned, you can write a function `liftPipe` with the type: -- The old Pipe type in Control.Pipe, not the Proxy type synonym liftPipe :: Pipe a b m r -&gt; Proxy () a () b m r ... and you can prove that this function satisfies the following laws: liftPipe idP = idT liftPipe (p1 &lt;+&lt; p2) = liftPipe p1 &lt;-&lt; liftPipe p2 ... i.e. the functor laws. The main contributions of the next release are going to be: * A standard interface for pipe morphisms and pipe transformers (i.e. functors between pipe types) * A suite of extensions that use this interface, most notably ones that extend pipes with error handling and parsing * (If time permits): Tools for optimizing away the overhead of these transformations at compile time Now, the really interesting thing is that this interface is not limited to the pipe type. Any other iteratee library can conceivably implement it and then automatically take advantage of the extensions I write.
I would argue that programming in general is simply applied mathematics, with varying levels of obfuscation.
Mathematics is programs, but with various levels of erasure :-)
Indeed, as I was thinking about the issue, I also didn't see a way for Proxys to provide type-safe end-closing without stepping outside of the monad. For what I am about to say, consider a proxy as a box where the client interface is on the "right" side of the box, and the server interface is on the "left" side: client server ---- ---- | | -&gt; outr inl -&gt; | | | | &lt;- inr outl &lt;- | | ---- ---- I believe you can provide something like: closeRight :: (inl -&gt; Server inl outl m r) -&gt; (inl -&gt; Proxy outr inr inl outl m r) closeRight next arg = ... closeLeft :: (() -&gt; Client outr inr m r) -&gt; (inl -&gt; Proxy outr inr inl outl m r) closeLeft next arg = ... (And also, a similar thing for closing the Left side.) But then the code must indent to a new block when used. do proxyFoo proxyBar closeRight $ do serverBaz I suppose since there are only two ends to close, this might not be such a horrible sacrifice. Indexed monads are currently quite cumbersome to deal with imho (especially when mixing them with regular monads, you have to deal with explicit upgrades/downgrades), though I really do love the concept.
Now write some Template Haskell to generate the instance automatically. Shouldn't be too hard. See [lens-family-core](http://hackage.haskell.org/package/lens-family-th) for some *fairly* simple TH that scans a single-constructor record type and pulls out its field names. It looks daunting at first but once you take a harder look you'll realize that it's just because TH, and my variable names, are rather verbose and tedious.
I like a "sociological" approach that I found in a book about philosophy of science: what is a scientist? It's someone that does the same thing a scientist do: think about stuff, validate ideas, write peer-reviewed papers about them and goes to conferences. If that's your way of life, then by this definition you're a scientist. (Of course that's not the only possible one and probably not the definitive one, but that's an interesting definition that I like to think about.) With the same approach: are functional programming researchers mathematicians? I would claim that some of them are, they work as mathematicians, sometimes. On a paper-by-paper basis I could say that someone wrote "a math paper" or rather "a comp. science paper" (It would be better to have also seen the researcher working on the paper, but I usually don't have that chance). (Note that I haven't defined what "a math paper" is. I can't do that (yet), but I have an intuition and I have heard it from other people ("this really is a math paper") in ways that matched this intuition, so my claim is that there could be a rough consensus on that.) Lars Birkedaal writes math-like papers. Neel Krishnaswami, one user on the MathOverflow thread, has written math-like papers. On the other hand, my best guess would be that most of Simon Peyton-Jones papers are *not* math papers (because they have a strong "language design" component); of course that doesn't mean they're bad, they're usually great, but they don't feel like math, they're something else that is very interesting as well. I also don't have the feeling that Conor Mcride's paper qualify as "math paper" according to this intuition. They have this intangible quality of being written, somehow, "from a programmer perspective", even if discussing rather theoretical subjects. And that's good! There is another scale that may be related is whether a given work is more or less "theoretical" in nature. I also have an intuition about that. The big picture is: "does it lives in the platonistic universe of mathematical facts?". There are some pieces of knowledge that we feel are "universally true", that have an infinite time span, while other don't. Proving a normalization result on Pure Type Systems is a "universally true" result, while discovering a convenient way to formalize the grammar of C is probably not (while some ideas involved may be), though it can be equally hard, challenging and useful as a research result. Note that there is no necessary coincidence between "very theoretical" and "maths as mathematicians do it". In particular along the "constructivism" idea that both computer scientists and mathematicians are interesting in, you'll find work that are very theoretical but not math-like, work that is extremely math-like, and interesting connections drawn between the two. (A last point: it is not completely accidental, I think, that "logic" and "abstract details of category theory" are two fields that have a hard time gaining acceptance from everyday mathematicians; some partly-ill-informed people will even contend that "logic" (including the more math-like aspect of it such as ordinal theory and model theory) is not really math, for example. To the general mathematical student they don't have the glory of "noble subjects" such as formal probabilities, algebraic geometry or complex analysis. Mentioning the parts of math that we use in computer science is not going to impress the adversarial freshman. Of course impressing a freshman is not the goal of research and that's a rather vain discussion, but I suspect there might still be something about perception of science in this invisible line here. The new links between type theory and algebraic topology might be an opportunity to "conquer the mainstream" (... of mathematicians)). 
Yes, I have something like that and although it's based on GHC Generics rather than Template Haskell, the purpose is the same. However, we also need access to the combinator library, because we need some customization in the extract field for some datatypes (in the unsimplified version, extractors are parameterized over a monad, i.e. `fieldExtract :: t -&gt; m a`, which allows for cool customizations).
What you want is to read [this](http://lambdor.net/?p=171). Entity framework is an OOP-thing. We use (A)FRP instead. For example: netwire, yampa, reactive. Google "Yampa arcade", it's a 2D shooter implemented in yampa. (Although both AFRP and CBD can be used in imperative and functional languages.)
If we consider the APIs, I find sodium to be the nicest, due to its simple and non-intrusive nature (it limits the IoC/framework effect). Elerea does quite fine, too. (Note that I haven't yet assessed reactive-banana). If we consider only AFRP, it generally results in a more complicated API but netwire does a fine job too. The problem is that all those frameworks still have open issues (efficient memory management for instance impacts the complexity of the API). Concerning the performances I've never assessed them. I don't know the state of their "mixability" with concurrency or parallelism.
I like to think that reactive-banana is simpler than sodium. (Or at least, that's my goal.)
Yes, I still have to read your last blog post regarding the API changes due to dynamic switching \^\^. BTW, why did you prefer a classic FRP approach to the AFRP one? Simplicity, perfs, ease to implement? From what I read, AFRP reduces the complexity of signals sharing/memory handling, but at the cost of an Arrow syntax which is more complex to use.
&gt; BTW, why did you prefer a classic FRP approach to the AFRP one? Syntax. To be honest, I find the arrow syntax to be unbearably complex. Events should be first-class and any difficulties with their implementation are the problem for the library author, not the library user.
That's because System F_omega is capable of encoding GADTs, and the 'finally tagless' style is using that encoding, with type class dictionaries for the algebra.
The late Bill Thurston once characterised mathematics as the least fixed point of: (a) what mathematicians study (b) mathematicians are humans who advance human understanding of mathematics (c) mathematics include arithmetic and plane geometry. I think similar definitions work well for many things (not just data types!)
I'm super impressed by this. I want functional programming to take over the world, and GPU programming is a nice step in the right direction, haha. Never heard about LambdaCube until now, but it looks awesome!
One thing I like about this is that, since `table` is a data structure (rather than an action built from combinators), you can define additional traversals. This makes it easier to support queries like: SELECT * FROM foo, bar; Granted, matching up column names to columns is tricky here, since the database probably won't let you get the table name of a column in a result set. But what you could do is replace `*` with schema-qualified column names: SELECT foo.a, foo.b, bar.a, bar.b FROM foo, bar; Thanks to multiple traversals, we can have one pass look up column names (to generate the SELECT expressions) and another pass read the result.
&gt; Mathematics is programs with omniscience Or programs with exceptions :)
I think so, yes, if the adventurous is willing to put up with the pain that is the extremely ugly transitional ‘exposed typed AST rather than a carefully designed EDSL’ API. There’s plenty of running examples to check out, but we put absolutely no effort into making the code look nice, since it’s destined to be thrown away when the language starts its new life as a proper DSL.
Mathematics is not actually just the act of proving theorems.
If you pass the `Pair` type you declared above to a strict function the difference will so away though, as the `Pair` will be completely eliminated and the two `Int#` that are the result of the unpacking will be passed in registers (if available).
Beware though, that there is quite a learning curve, and finding information can be quite hard. The texts that introduces the subject are written like academic papers(probably because they are) and texts like the yampa arcade, while being easier(not directly easy, imo) to read are not a complete guide to FRP or Yampa for that matter, but just an example of how to use it.
Doesn't that depend on whether the pair is shared, and perhaps even many other things?
That's awesome! When reading Haskell articles, I feel just dumb enough to not be able to participate. I understand why this stuff is interesting and relevant to computer science, but not how to produce meaningful research of my own. I understand that Haskell code, or code in general, is isomorphic to mathematical proof, but that knowledge doesn't empower me at all. Maybe that's an indication that I *don't* really understand the connection.
In the real world your system and my system could be isomorphic and this is might be provable. Upon concluding this, all the mathematicians I know then go to lunch. If we merge companies, actually writing down the morphism that converts your system to my system still costs $125 an hour, requiring a programmer and a business analyst working together. So, at present I would have to argue no. However, the existence of theorem provers (type systems?) and functional programming gives me hope that the wage goes up, that the total cost goes down, and the programmer goes to lunch with the mathematician. 
Right, MetaHDBC has you write raw queries (I really want to see how it handles stored procedures). I just think it's interesting how different the two approaches are. I wonder if they are orthogonal enough to be used together.
I haven't had time to do this, and likely won't until after ICFP. However, I did notice that if you do: hoogle data --local=~/Library/Haskell/ghc-7.4.1/bin it _might_ work, that helps it find more docs by adding your path to the list. I've raised a bug so I don't forget: http://code.google.com/p/ndmitchell/issues/detail?id=559
There's been a spate of exciting category-theoretic analyses lately. Hinze's Kan extensions and Paterson's Constructive applicative functors also deserve reddit entries of their own. But I smell delicious low-lying fruit extending worker/wrapper theory into the domain of [interleaved effects](http://www.reddit.com/r/haskell/comments/zguyw/new_paper_on_interleaving_data_and_effects_fandm/). Doing so would yield immediate practical optimizations for iteratees, pipes, and conduits. 
:)
Are you speaking of the ArrowSyntax extension? I use arrows extensively, but do not use ArrowSyntax. Plain use of &gt;&gt;&gt;, &amp;&amp;&amp;, ***, first, second, etc. is very simple and (with proper use of named abstractions) not too onerous. First class events aren't incompatible with arrowized reactive models. You'll need something like ArrowApply, then you can model a switching behavior. This more structurally expresses and enforces the property you achieve with Moment in time: it is clear when the program begins observing the event source. (It can also be clearer when observation halts.) If you consider problems of resource management, e.g. enabling an event sensor only when there is an observer for it, it is quite advantageous to have first class *bidirectional* events (or behaviors), a sort of observer effect. I do not know how to express this information through the type system, which is why I use arrows. 
&gt; The architecture is not about objects, it’s about data (components) and sub-routines that operate on that data (systems). This translates to Haskell by "The architecture is not about objects, it’s about data (datatypes) and sub-routines that operate on that data (functions)." _Et voila_, you have your basic architecture. You still need a way to group your components to indicate they belong to the same entity, though. To sum up: type Position = Vector2D type Speed = Vector2D data Sprite = .... data RenderInfo = .... newtype Entity = E Int -- The position of this entity's components into each Component list (the lists handled by Systems) data Component = MoveComponent Position Speed | RenderComponent Sprite RenderInfos | ... type System = [Component] -&gt; [Component] (Arrays or Vectors would be more suited that lists) This can be made type-safe, e.g. by making Component a GADT and by parameterizing System by the type of Component it handles (so that you can be sure you don't give to a system components it can't process). You can also "split" your Component type, by transforming it into a class or a type family, which will enable you to add new components without modifying the datatype (because doing that would require a lot of your code to be re-compiled, as much of it will depend on the type Component) but be warned you'll have to use ExistentialQuantification and even maybe Data.Typeable when you regroup your components into an Entity. As it's been proposed, you can replace Systems by Signals through FRP (but it requires some learning if you don't know FRP). I still don't get the point of the separation between nodes and components: to me entities should be directly linked to their node, just like the systems, that's simpler. At least that's the way the game company I worked at was doing (just with different names).
I would do anything for a real beginners tutorial. Learn you an FRP, or something of that sorts. I think I 'get' why it is useful, but I don't get how any of them work or are supposed to be used.
Sounds like a smart move, if they want to keep end users happy with a well-known environment and still want to integrate with MS tech.
If you haven't read it yet, Heinrich Apfelmus' blog provides a few good insights (like http://apfelmus.nfshost.com/blog/2011/05/06-frp-why-functors.html).
The nested type-synonym expression `Simple (IndexedLens Time) SystemState a` expands to (something isomorphic to) `forall f. (Functor f) =&gt; (Time -&gt; a -&gt; f a) -&gt; SystemState -&gt; f SystemState`. The idea here is that, given an alteration, parametrized by the current time, of some part of the system's state `a`, you can construct a corresponding alteration of the whole system's state that is valid at any arbitrary time. The functor `f` can be chosen to be anything; in particular, it can be chosen as `Const a` to get the value of `a` at the supplied point in time, or it can be chosen to be `Identity` to actually perform the modification.
I was waiting for this reply. I rethinked this, and I agree, I was wrong. (Fortunately I wrote later that both can be implemented in both type of languages, hee-hee :)) &gt; components hold only data (there is one of the comments that says that), they don't update themselves or others In fact, this is how I've implemented once a component-based system in Python.
[SkillSwap Blurb](http://skillswap.scottylabs.org/workshop/haskell.html)
In case you have not noticed: http://cleantypecheck.wordpress.com/2012/09/05/hte-v-haskell-2010-at-github/
It's the same connection as between any sort of programming and math, really.
This seems useful in terms of error handling when doing, say, a bunch of data validations: runCollectLeft (SomeConstructor &lt;$&gt; collectEither tryToParse1 &lt;*&gt; collectEither tryToParse2 &lt;*&gt; ...)
Yes, say Hi - come share a beer and steal a business card if you are interested in applying.
I believe this is a subset of `MaybeT (Writer e)`. The difference is that `MaybeT (Writer e)` can succeed and still log an `e`. However, I believe they have the same `Applicative` instance for the subset they coincide on.
I am really pulling for this effort; it sounds like this design has its head screwed on right. I am very wary of efforts that sacrifice performance or API coverage, because you are already limiting the utility of available documentation, tutorials, and forum threads by doing things differently. That is unavoidable if we want change -- which we most assuredly do -- but a successful effort will not seriously limit what the user _can_ do when compared to the traditional approach. Charge onward!
I haven't thought about whether this is actually a monad, but if so, this is more general than `Either`'s current instance, since its behavior can be recovered with `Either (First e)`.
I've noticed that one of the Haskell benchmarks game programs [that worked fine with ghc-7.4.2](http://anonscm.debian.org/viewvc/shootout/shootout/website/websites/u64q/code/fannkuchredux.3.ghc.log?revision=1.15&amp;view=markup) no longer works fine with ghc-7.6.1? Strangely that fannkuch-redux program does work fine with ghc-7.6.1 on x86 Ubuntu -- but [on x64 Ubuntu the program keeps 4-cores 100% busy until it's timed out after an hour](http://shootout.alioth.debian.org/u64q/program.php?test=fannkuchredux&amp;lang=ghc&amp;id=3#log). My guess is that I've done something wrong, but so far I haven't been able to find any differences in the compile or run commands -- so *fyi* there is a very very slight possibility that the program's hit some wierd bug ;-)
Harder. Better. Faster. Stronger.
I don't see why you should be copying the function style in the first place. Semantically, it doesn't make sense that you terminate the whole computation when any thread terminates. If you follow mypetclone's suggestion, you also get the `empty` placeholder for free.
I'm not sure exactly what you mean, but I do essentially this inside of [postgresql-simple](http://hackage.haskell.org/packages/archive/postgresql-simple/0.2.4.1/doc/html/src/Database-PostgreSQL-Simple-Ok.html)
Recall that with type families and constraint kinds you can actually make typeclasses that are parameterized on other typeclasses. e.g. an `Ordered Monoid a`. At some point I plan to go back and see what I can do to retrofit this style into `algebra`, and see how much more succinct I can make that mess of classes.
`type String = [Char]`, i.e. `String` is a list. `Text` has its own internal representation of, well, text which is not a simple list of `Char`. Therefore, if we have `Prelude.length :: [a] -&gt; Int` `Data.Text.length :: Text -&gt; Int` it is clear which function should be used on λ&gt; length "hi" because `"hi" :: [Char]`. Just as it would be clear for λ&gt; length $ Data.Text.pack "hi" because `Data.Text.pack "hi" :: Text`. This only gets problematic if you enable `-XOverloadedStrings`, because in that case in the first example `"hi"` would be of type `IsString a =&gt; a` and could either be `Text` or `String`. The "ambiguous occurence" error would be justified here.
Yes.
Many thanks! I guess that I'll have to learn how type checking and name resolution interact in GHC. I'm a few months away from that level of understanding but that link at least tells me where to look.
Here's how to do this with the generics code I posted a few days ago: https://gist.github.com/3684853 `fromRecord` and `toRecord` are each just 2 lines of code. I assumed it's only used for data types with one constructor (that's what `head` is for.) There's a small difference with the code from the blogpost, since I needed access to the selector names. The final code in the generics library should give access to constructor names too.
Partly in response to this article, I wrote an article on Free `Applicatives`: http://www.reddit.com/r/haskell/comments/zlr0l/towards_free_applicatives/
What is the type-safe equivalent of memo in Sodium and reactive-banana you’re alluding to?
What does "free" mean in this context?
The same as "free" in free monad: the functor that sends some object in some category to the "simplest" object in another category based on the original object. [This can be formalised.](http://ncatlab.org/nlab/show/free+functor)
I feel like it would work better as this data KList' k t where KNil :: KList' k () KCons :: k a -&gt; KList' k b -&gt; KList' k (a, b) data FreeApplic f a where MkFreeApplic :: KList' f e -&gt; (e -&gt; a) -&gt; FreeApplic f a but the applicative operations may be hard (they should not be impossible) to implement, but when implemented, they should quite obviously be correct. EDIT: Also, here is the folding combinator: freeApplicFold :: (Applicative g) =&gt; (forall a. f a -&gt; g a) -&gt; FreeApplic f b -&gt; g b freeApplicFold nt (MkFreeApplic lst f) = fmap f (klistFold nt lst) klistFold :: (Applicative g) =&gt; (forall a. f a -&gt; g a) -&gt; Klist' f b -&gt; g b klistFold nt KNil = pure () klistFold nt (KCons a n) = (,) &lt;$&gt; nt a &lt;*&gt; klistFold nt n
Both of you guys are assuming that finalization will be the same semantics as Frames, which it will not be. First, pipes register two kinds of routines: a finalization routine and a suspension routine. The finalization routine is only called when composition terminates (when you can prove the pipe will never regain control). The suspension routine is optional and is only invoked in response to some upstream or downstream pipe indicating that it will probably not request or respond for a while by issuing a suspendU or suspendD command, respectively. The suspension routine need not be a finalization routine. It's merely an optional hook that lets a pipe respond to the suspension signal, although the most common use case will be to free a resource if it's more expensive to keep the resource open than to simply free it and reacquire it later when it is needed again. However, if a pipe does register a suspension routine then it's responsible for checking to see if the resource needs to be reopened every time it regains control. The deterministic finalization is more straightforward than the suspend/reawaken stuff. The main issue is that there are no clear semantics for how suspend/reawaken should behave as it is purely a signal. For example, if a file source opted to support the suspend hook, it's clear what it should do when it reawakens: should it seek back to where it left off? Ok, I guess that is pretty clear, but you can imagine that for other resources it might not be so obvious what is the correct solution for reawakening. Also, anything can simply choose to ignore the suspend signal (especially for resources that cannot be reinitialized) and rely solely on the deterministic finalization at the end of composition. However, without an indexed type you cannot permanently and promptly finalize upstream/downstream before composition terminates, for the same reason you can't provide that guarantee in any non-indexed monad. However, while deterministic finalization is definitely in for that release, I'm still mulling around how to implement suspend/reinitialization properly. I may delay that by another release because it is possible I could define it as part of a broader extension that simply emits and receives arbitrary signals and let the users settle on their own protocols and semantics. Anyway, I hope that gives some insight into what I have in mind.
Do you possibly plan on splitting the package, i.e. separating the rendering pipeline from the engine itself?
The above comment is wrong, since all of the effects of the first argument of `&lt;*&gt;` should come before all effects from the second argument. Here is a better version: data Free2 f a where Pure2 :: a -&gt; Free2 f a Ap2 :: f a -&gt; Free2 f (a -&gt; b) -&gt; Free2 f b instance Functor f =&gt; Functor (Free2 f) where fmap f (Pure2 x) = Pure2 (f x) fmap f (Ap2 x y) = Ap2 x (fmap (f .) y) instance Functor f =&gt; Applicative (Free2 f) where pure = Pure2 Pure2 f &lt;*&gt; y = fmap f y Ap2 x f &lt;*&gt; y = Ap2 x (flip &lt;$&gt; f &lt;*&gt; y) 
I've been playing with this to provide lazy reading of bytes from a file handle: &lt;https://gist.github.com/3686732&gt; (using `ResourceT` for cleanup). This works quite well for `ByteString`, for example: runSession (debugPrint &lt;-&lt; getChunks 10 &lt;-&lt; PipeUtil.readFile "/dev/urandom" hGet) But I think a slightly different strategy is needed for `Text` or `String`, since in that case I cannot just read "n bytes" but must read "n chars"... but with many encodings "n chars" does not translate to a fixed number of bytes, and so the `Server` would in that case need to be encoding-aware.
Concerning sodium, I think it's integrated to the Event and Behaviors definitions, you don't have a direct equivalent to 'memo', I believe it's just you can't compose Events and Behaviours without having sharing. Or I misunderstood the point of an Event containing an action in the Reactive monad (FRP.Sodium.Plain).
Note that [ClassyPrelude](http://hackage.haskell.org/package/classy-prelude) was created to solve this inconvenience.
This looks very nice. Perhaps my comment about needing to separate `FreeA` from `ChainA` to prevent denormalisation is not valid. Your method seems to avoid that problem completely and still have only one data type. I prefer that. I also like that you have defined `&lt;*&gt;` in terms of itself.
Nice, this is very similar to [twanvl's comment above](http://www.reddit.com/r/haskell/comments/zlr0l/towards_free_applicatives/c65s0zj). It's not actually a general free applicative though. It's a free applicative over `Option`. You can easily replace `Option` with a type variable to get the free version of course. Is Paolo's example generally known to be a free applicative, or is this something only you have noticed, Tekmo? I have found very few mentions of "free applicative" when searching for it.
From the article: &gt; These instances make me uncomfortable, as they do not satisfy the required Functor laws and Applicative laws up to equality of the GADT terms. For example in the Functor instance, fmap id (Pure ()) is Map id (Pure ()) instead of Pure (). Are those laws really supposed to hold for some structural equality over the GADT? Can't we just say those laws should hold for some interpretation over the functor. So that we don't necessarily expect `fmap id f == f`, but we expect `interpretation (fmap id f) == interpretation f`. This seems very reasonable to me and I've used this kind of laws in several occasions.
In fact it's the opposite. I know what the mathematical construction is and what the mathematical meaning of "free" is. That's straightforward enough. What I don't know is whether free applicatives have found their way into the Haskell consciousness. I don't think so.
Have [the paper I most like to quote](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html).
Yeah, you have to modify it slightly to get the free applicative, but it's all there. When I noticed it was similar to a free applicative I asked him about it and he said he intentionally wrote it that way but he didn't call it as such because he wasn't entirely sure if it truly was a free applicative or not. As far as I know you and Paolo are the only examples of free applicatives I've seen either in code or in the literature.
Monoidal functor.
Sounds like a [SAT](http://en.wikipedia.org/wiki/Boolean_satisfiability_problem) issue? Is this any different from [TDNR](http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution) BTW?
&gt; Yes, this was what I understood. And you need memoization so that you don't keep recomputing that state, don't you? No. State is always shared, since it is stored in dedicated IORefs. What’s not shared by default is the stateless manipulation of data. E.g. if you fmap over a stateful signal, the fmapped function will be applied separately at each reader, unless you thread it through memo. &gt; Doesn't it reveal in the end that Applicatives don't provide a good abstraction for signals composition? Why not switch to AFRP? Or something even different than arrows... This problem is orthogonal to the choice of abstraction. Ultimately, you don’t want every node to be memoised implicitly, only those that are shared by many readers and perform complex enough logic to make it worth. It’s purely a performance issue; with Elerea, you should basically write your program without any memos, then insert them at the bottlenecks revealed by profiling. I don’t see this as something a compiler can do optimally, although it can make educated guesses.
Conal Elliott has taken this approach in his [Denotational design with type class morphisms](http://conal.net/papers/type-class-morphisms/).
Then I shall write a definition and proof that this is indeed what a free applicative is.
However, then I would be tempted to write the `Applicative` instance for the interpretation itself, although perhaps this is not always possible.
&gt; To understand what free means formally you need to understand adjunctions. A functor F is left adjoint to a functor U when (F a -&gt; b) is isomorphic to (a -&gt; U b). We write this F -| U ~~What this means in the applicative case is that (for an `Applicative` `g`) there is a unique way of lifting `f a -&gt; g b` to `FreeA f a -&gt; g b` in a way that preserves the `Applicative` laws, and that every such `FreeA f a -&gt; g b` arises in such a way.~~ [EDIT: what I wrote did not make sense. See my reply to pcapriotti's reply] 
&gt; What’s not shared by default is the stateless manipulation of data. E.g. if you fmap over a stateful signal, the fmapped function will be applied separately at each reader Okay, I mixed things up, sorry about that.
No, none of it is online. Apart from this paper, there are also slides from the talk I gave about this system. The talk was rather basic (since it didn't assume either Haskell or financial domain knowledge), but still might be of interest. http://ro-che.info/docs/2011-05-26-Belgrade-FPF.pdf
It would be a bug for it to assume this. However, once the instance is known, for example, if you specialise the type to Integer, then it will reduce at compile time.
 ghc -ddump-simpl ...or cabal install ghc-core you can also have a look at the rule firings etc, just have a look at the manual.
For us mere mortals: why would it be a bug to assume that?
I could be wrong here, but... When you specifically make a `Num` instance, you make your own `+` function. So if you define `+` to be something other than what `+` normally is for a number, wouldn't it be something else? For example, if your `+` operation was defined as `(+) x y = succ $ x + y` (this is incorrect notation, but you get the idea), GHC making this assumption would be very much a bug, as `1 + 1` should evaluate to `3`. Is that what you're asking, or am I off?
since by providing your own Num type instance you can redefine fromInteger and (+) to whatever you want.
I think it may interact very poorly with type inference. It seems like you could fairly easily end up with combinatorial explosions of possibilities for type directed name resolution to try. Of course, HM already has terrible worst case behaviour, but it seems like just a few things which needed type directed name resolution in the same module would start to cause headaches for a type checker that was required to then try all possibilities. (Assuming someone left out the signatures at the top-level, and definitions depended on one another.) The right solution, if you need the polymorphism, is to use a type class. We just don't happen to have a type class for length or size generically, I suppose because it's not a piece of information which you can usually make good use of on its own.
The other people have already covered it. You can provide your own instances in which fromInteger is not a homomorphism. Obviously, it's not very friendly to do so, but you may have your reasons, and it's probably better for the compiler to behave consistently in the face of it. It's better in terms of being able to understand what your program is doing for it to wait until it knows which type the fromInteger is being applied at (perhaps after inlining), and then start simplifying the expression, so that it can guarantee that things behave as they would have behaved had it not done the simplification. Obviously, you may miss some potential simplifications this way, if things aren't inlined and you're running polymorphic code, but I think it's fairly rare that you have complicated fully polymorphic constant expressions which are going to waste a significant amount of time at runtime because they weren't constant-folded. Here's an example (incomplete) Num instance where the simplification would change the behaviour of the program: data Foo = A | B deriving Show instance Num Foo where A + A = A _ + _ = B fromInteger 1 = A fromInteger 2 = B main = print (1 + 1 :: Foo) This program prints A, but would print B if fromInteger were treated as a homomorphism and the implicit occurrences of it pushed through the (+).
I test it, and GHC make the rigth thing: https://gist.github.com/3691872
I can express that functionality. My model lacks `accumB` (or any internal accumulation of state) and would require external state, e.g. an abstract register to count reds and an abstract register to count blues. I could dynamically switch between displaying the red count or blue count based on a time source. I wonder if that seems like cheating to you? ;) I excluded use of internal state from my design to better support live programming and orthogonal persistence. I push every state to external state. Rather than modeling ad-hoc "fresh" state within my model (via `accumE` or `accumB` or `integralB`, etc.), I can securely partition an abstract set of registers by using monotonic (no `..`) composition of path names. 
&gt; We can build our entity architecture using an object-oriented language but, on a fundamental level, this is not object-oriented programming. The architecture is not about objects, it’s about data (components) and sub-routines that operate on that data (systems). That seems to be the main point. And it agrees with you perfectly. Programming in Haskell is already primarily about separating data and operations on that data. It seems like that fundamental design principle is already encouraged.
Ah, so it's "deep" TDNR more or less.
If you're just looking for fun haskell tricks, make your leftOf, rightOf ... functions be of type [Square]. (I think) you'll pass in whole rows and split on this specific face. Once you can have variable length definitions, you should be able to generalize to any size cube.
that'd be a cool plugin to see someday :) 
http://www.ittc.ku.edu/~neil/papers_and_talks/workIt.pdf or if you'd prefer the extended version: http://www.ittc.ku.edu/~neil/papers_and_talks/workIt-extended.pdf
Oh I see "using Functor f" was ambiguous. You need f, you just don't need it to be a Functor.
It seems like you could have a unicode decoding proxy - it takes in the number of requested characters. It then guesses the needed number of bytes, optimistically assuming ascii, and requests that number of the bytes from the dumb file server, which returns exactly that many bytes. The proxy then decodes the bytes and checks how many characters it actually has, and possibly issues another request to the server until it has the character count requested by the client. I was going to say it could even improve its guesses over time, but without the ability to push values back onto the stream, that might not work. So perhaps it always has to guess a lower bound on the number of characters needed, or else it might sometimes have to return more characters than requested. 
Yeah, that seems like a reasonable implementation. And yeah, without "unget" it'll need to always try one-byte-per-character first, and also needs a character set decoder that will return something like `(Text, [Word8])` for the decoded stuff and the remaining bytes, and if it's not going to request one byte at a time on decoding failure, it'll actually need to have the decoder built in (so it knows how many bytes to request next, etc).
I would not recommend /r/haskellquestions (it's dead). The best resources for beginners are: * [Learn you a Haskell for great good](http://learnyouahaskell.com/) - An introductory tutorial for beginning programmers * [#haskell on freenode](http://webchat.freenode.net/) - Use this to ask really dumb questions without fear of being downvoted. * [Stack Overflow - questions tagged "haskell"](http://stackoverflow.com/questions/tagged/haskell) - Use this to ask more sophisticated questions
&gt; A good programmer also knows when NOT to use programming to solve a problem. If this is a memorization of different parts, I think you would benefit more from an SRS tool such as [anki](ankisrs.net/). It is very easy to use and surprisingly efficient. By the least, it should give you ideas for your own program.
I think I'll stick to calling them lenses or at worst, lens families. ;) A lenticuloid sounds like a growth you'd want to have removed.
The problem with this approach is it does lead to a combinatorial explosion when you have multiple ambiguous symbols in an expression. You would in theory have to check all of them together, If you have `n` 2-way ambiguous symbols, the complexity of the type checking problem seems to explode by a factor of `2^n`.
Ah, yes, by providing a 3rd length it can conflict with. ;)
Mostly I prefer just giving genuinely different operations different names, but that's just me. I even kinda think of the module system as a last resort to rescue us in the case where we've accidentally given different things the same name.
If your aim is simply to help you/your friends in your studies and if you don't want to spend a lot of time on it (I guess the point is to *save* time in the end, right?), I can guess a simple terminal interface would be sufficient: you open a window just to display an image (that should be straightforward with wxWidgets, GTK+ I don't know but both have functions to load images), and then display the choices in the terminal, the universal GUI ;). That's the way I'd go, and then I'd make a nicer interface if I had the time to. *EDIT:* Ok, actually Anki would be the way I'd start.
no.
3rd vote for anki, unless you also want to do it in Haskell as a learning experience. However as someone very knowledgable about having limited Haskell knowledge, doing it in Haskell will not help with your limited free time in the slightest!
You can ask sophisticated questions in #haskell too, we won't mind. It's too bad that /r/haskellquestions seems dead. I'd answer questions there, whereas I have a mostly irrational dislike of stackoverflow (I dislike the UI and karma nonsense, and all the restrictions seem not to actually help anyone. Just a plain wiki format seems like it would serve the purpose better.)
In general that's probably true, but in this case the structure of the cube is known in advance.
An `a -&gt; b -&gt; c` would be a binary operation, wouldn't it? I'm looking for a term that means the `a`s in `a -&gt; a -&gt; a` are all the same type.
Closed binary operation. I don't think CT has a special name for `A*A -&gt; A`
Technically this is a function that generates an endomorphism in a.
Yes but this is specifically a function that generates an endomorphism in a from an a.
The mistake is assuming that smartness is a total order.
Ah, but my friend! You simply exhibit parts of the Dunning–Kruger effect! _Of course_ you think you're not really smart _because_ you're really smart. Ok, I'll go and get my things.
I also have some anecdotal evidence against: I have a co-worker that has this mindset. It's rather strange. Whenever haskell comes up in discussion, he mentions that he's not smart enough to do it. And yet whenever we read some haskell (that was not specifically obfuscated or over-optimized) he can keep up just fine.
Maybe slightly better to say "binary operation closed in 'a'"?
I've read papers on FRP and arrows, but I barely ever use them. So that can't be required reading. And kinds are less fancy than types unless you get into the really new GHC stuff which 1) doesn't work very well yet and 2) hasn't existed until very recently. You don't need to know everything that has yet to even become the cutting edge to be a competent Haskell (or any other language) programmer.
I found I understood Haskell better when I started implementing its concepts in C++. Try that. Or, if you're more familiar, try doing so in Python. 
I agree, but when you don't have anything to say about `a`, such as when it can be any type, it's probably not worth mentioning it.
&gt;Actual competence may weaken self-confidence, as competent individuals may falsely assume that others have an equivalent understanding.
I don't know how rich the naming scheme is, but you could always deal with [multicategories](http://en.wikipedia.org/wiki/Multicategory).
Perhaps I didn't make my meaning clear before. I was commenting on Larry's accusation that Haskell requires intelligent programmers. As I stated before, I believe this to be an exaggerated issue (because Haskell code is extremely easy to read, and most programmers are plenty intelligent to write it too). However, I am conceding that Haskell is less friendly to VERY low end (particularly new programmers with little affinity to math) programmers. I was absolutely NOT making any statement about the intelligence of someone who doesn't grok Haskell. I totally agree that 95+% of all difficulty with Haskell is due to paradigm shift it requires.
Ok, well, then can I suggest that instead of "untrained hack" you say something like "novice programmers with no mathematical background", if that's what you mean. It's more precise and less...gratuitously insulting. _EDIT:_ a polite, constructive suggestion collecting downvotes. Well done, everyone.
Haskell is a very different beast than almost every other programming language, so, while not unlearnable by anyone, a certain degree of mental adaptability that typically comes with high intelligence can certainly help in learning Haskell. The paradigm shift between procedural programming and functional programming is pretty large and is usually eased by the fact that many functional languages have facilities to be written somewhat procedurally; Haskell has almost none of that and forces pure functional methods, so there's no real bridge to aid that transition. Thus, switching from any other language to Haskell is quite literally a mental leap.
I would dispute several thing which you have said. In understanding pure code vs. IO there is very little on getting started and how you should go about designing an application. I don't think it has much to do with discipline at all. Having been an "untrained hack" learning how to develop real software using haskell was a considerable challenge. Laziness is the real deal breaker for someone trying to build software in haskell. While its virtues are regularly extolled if one is serious about performance then haskell would not be involved. It would be C/C++ on the intel compiler with the latest chips. I have been told that many things in haskell are not possible without laziness but I have never gotten a good explanation as to why. I do think laziness could work for the masses if the tools were better.
Most libraries I use actually have pretty good haddocks, but I do agree that linking to PDFs of academic papers should not be considered an acceptable way to tell people to learn new things. The typeclassopedia is pretty good for most common things like this, though. The only paper I tried to read was the SYB paper, and I got far enough in to realise that it's not something I'll ever want.
&gt; segregation of IO from pure code requires that the programmer has at least some minimal notion of how they want to structure their code before they write it I have not found this to be true. You can write terrible hacks that live entirely in IO if you just want something to work and refactor later, just like in any other language :)
Well it's a matter of convention but I don't think so. See for example http://ncatlab.org/nlab/show/magma. `a -&gt; a -&gt; b` is just "a function from a x a to b"; it would not usually be called an "operation".
I think if you're referring to a "binary operation on a set" as the ncatlab page does, then that's fine. But we also can talk about "a boolean-valued function on R x R" for example, where "on" doesn't imply closure.
on a to a
Certainly. To me the key word is "operation". I usually expect an operation to be a function of the form f : S^n -&gt; S for some S and n, n being the arity of the operation. A "function on R x R" could have anything as its codomain. But of course context is everything. If you say "a boolean-valued binary operation on R", I might think it slightly odd but I would have no trouble understanding what was meant. Or when talking about models for multi-sorted theories, an operation might relate values of several sorts. But without a context that provides evidence to the contrary, I will expect "binary operation on `a`" to mean `a -&gt; a -&gt; a`. I don't think there is any totally unambiguous term for functions `a -&gt; a -&gt; a` that's better than "a function `a -&gt; a -&gt; a`".
Yes, laziness is the real challenge. The obvious sign of this is the commonality of 'seq' or !patterns in real code. Haskell programmers often must consider evaluation strategies, while eager-language programmers don't need to even know what "lazy" means. The perks of laziness are generally in more advanced usages. For example, it's by far the simplest way to create cyclic data-structures in "pure" code. It also improves deforestation, which is really important for idiomatic Haskell.
I've heard comments that there's *too many* tutorials [1], they need to be prioritized, and that there's lots of high quality explanations here, stackoverflow, blogosphere, café etc, but they need to be easier to google for. Booklist [2] will be updated for School of Music soon, [1] http://www.haskell.org/haskellwiki/Tutorials [2] http://alexott.net/en/fp/books/#sec7
To add to that, there's even much more too many *beginner* tutorials. Most other tutorials are advanced. There are not a lot of tutorials to fill the gap (only good one I know is learn you a haskell and real world haskell. They also cover beginner stuff) and you might not find the right ones right away as a new learner.
A bimorphism is either a morphism between morphisms (2-morphism), or an epimonomorphism.
I just think the tutorials are too abstract right from the get-go. LYAHFGG and RWH both jump into the language using syntactic sugar and abstract concepts in order to help the transition from an imperative language. I think this hurts in the end. Haskell, like other functional languages, are all about building blocks. abstractions upon abstractions. I honestly think most of the LYAHFGG chapters should be presented in reverse with simpler examples and comparisons.
Yeah, the only tutorial I used was LYAH, and then I just went out and wrote code.
FRP is becoming its own branch of abstraction, though. There are new languages like Elm that make this "design pattern" a thing of the syntax. But you're right, it took waaay to long before I realized that a "monad" wasn't even anything special. The only thing holding me back to learn what it was all the explanations people tried to give. That's quite ironic to me.
&gt; I suspect many people picked up this use of "closed" in an introductory abstract algebra course I picked it up from algebra, so that's very possible.
&gt; The actual type forall a. a -&gt; a -&gt; a is the type of booleans Booleans have this type, but not all functions of this type are booleans, so I'm not sure it's really relevant.
I don't understand what is wrong with hiring smarter programmers. Yes, they will cost more but presumably they are also more productive.
Well, to be fair, if you have a lambda construct then your language is not quite "all eager", you just have to be a bit verbose and annotate when you want lazyness.
I keep hearing about RWH, and I think I used a segment of it as reference once, but in general after LYAH I never saw the need. That's probably just me.
I think you'd be interested in [call-by-push-value](http://www.cs.bham.ac.uk/~pbl/cbpv.html) and the [levy language](http://hg.andrej.com/plzoo/file/7d2506fba92d/levy).
On a huge tangent... &gt; And kinds are less fancy than types unless you get into the really new GHC stuff which 1) doesn't work very well yet and 2) hasn't existed until very recently. Regarding #1, the new kind extensions are now "officially supported" in ghc 7.6.1, and are considered to "work well" now, I believe. Even then, with all of the -X\*Kind\* extensions turned on, the kind system is still just a simplified version of the type system for types.
Well, I think "magma" counts as such an unambiguous term. Though it's a rather obscure term, no doubt because there's no substance to it. 
&gt;if one is serious about performance then haskell would not be involved. Look around, count the number of python, ruby, perl, php, asp, jsp, javascript, java, c# programmers. Now compare it to number of c/c++ programmers. Now ask yourself, are ALL those programers who do not use c/c++ (more than 99% of ALL programmers) not "serious about performance" ? Or is haskell (only 1.5 times slower than c unlike python, ruby etc) absolutely adequate for 99% of tasks performance wise ? 
Let's be fair, chalk boards don't *need* maths either; it's the humans who feel the need to put the maths there. It's all about the humans.
&gt; while eager-language programmers don't need to even know what "lazy" means. They may not need to know what the term means, but they use it daily nevertheless. Any form of codata requires laziness. Thus, every time you use an iterator or generator in your favorite OO language, you're using laziness--- you are explicitly depending on the fact that you can generate the sequence incrementally rather than needing it all in one go. This is identical to Haskell's use of lazy lists in lieu of for-loops.
Not all projects need smarter programmers. Sometimes it's like hiring Disney's landscapers to mow my lawn, when the kid next door does fine.
&gt; Larry's intention was to warn project managers about the difficulties of finding replacement for a haskell developer who suddenly left the company. I find this extremely ironic given Perl's reputation with respect to maintainable code.
OT, but you don't really need a to be a subtype of b, having an f:a-&gt;b is enough. Let g be the binary op on b. Then f is closed under g if there is a binary op on a, h, with g(f(—),f(—))=f(h(—,—)).
In my experience, this is usually a sign that you're doing something that a smarter programmer can automate somehow -- similar to how all programmers make small scripts to automate simple tasks that normals spend hours doing manually. (And if that's not possible, that is usually a sign that the system was designed poorly.) I'm sure there are exceptions.
Actually they do. Because it's a polymorphic type, there's only two definitions for this (ignoring undefinedness and non-termination): left :: forall a. a -&gt; a -&gt; a left x y = x right :: forall a. a -&gt; a -&gt; a right x y = y These are the definitions of the standard pure lambda encodings for booleans. Essentially, these are the case functions for booleans when already applied to booleans: boolCase :: forall a. Bool -&gt; a -&gt; a -&gt; a boolCase b x y = case b of True -&gt; x False -&gt; y left x y == boolCase True x y right x y == boolCase False x y All of the information you need about booleans is encoded here. Any time you need to use a boolean, you can use `left` and `right`. Consider the definition of `if` as a function: type NewBool = forall a. a -&gt; a -&gt; a newTrue = left newFalse = right if' :: forall a. NewBool -&gt; a -&gt; a -&gt; a if' b x y = b x y if' newTrue x y == newTrue x y = left x y = x if' newFalse x y == newFalse x y = right x y = y Normal boolean operations can be defined as well. not :: NewBool -&gt; NewBool not b = if b newFalse newTrue not newTrue == if' newTrue newFalse newTrue == newTrue newFalse newTrue == left newFalse newTrue == newFalse not newFalse == if' newFalse newFalse newTrue == newFalse newFalse newTrue == right newFalse newTrue == newTrue and :: NewBool -&gt; NewBool -&gt; NewBool and b b' = if' b b' newFalse and newTrue newTrue == if' newTrue newTrue newFalse == newTrue newTrue newFalse == left newTrue newFalse == newTrue and newFalse newTrue == if' newFalse newTrue newFalse == newFalse newTrue newFalse == right newTrue newFalse == newFalse It's called a Church encoding, which is essentially just a hard coding of constructors as a function that takes the args and returns the recursion schema (a fold) pre-applied. So in general it looks roughly like Constructor :: ... -&gt; Type ~ \ ... -&gt; \f -&gt; foldType f (Constructor ...) only without actual constructors. The fold, normally, is just going to immediately case on the constructor anyway and apply `f` as appropriate, so why not just bypass that whole case business?
&gt;I program in Python ... do you count me in Yes of course. What other languages you use is irrelevant. I use java and delphi. Yet i am a haskeller because i also use a haskell. Obviously tasks you implement in python have no performance problems. Otherwise you wouldn't chose python. And saying that for critical parts you use C is irrelevant. You use C, because python makes it trivial. But so does haskell. Therefore i see no performance problem for haskell. In fact yesod web server i am using runs circles around apache web server (written in c btw). And don't even mention node.js :)) 
&gt; There's some other social pretext going on here that leads to this self-debasement. People who've tried repeatedly to learn C++ without making sense of it don't go around saying they're "not smart enough" for C++. So what's the difference? It's okay to be too dumb to make sense of Haskell, because "only really smart people do."
If one were to carefully, objectively translate the statement into predicate logic, then that's what it means. But natural language doesn't work that way. As several other commentators point out. _EDIT:_ update for context.
Yeah. Natural language is not a serialisation protocol by which the author's idea is transmitted into the readers' heads. The probability of the “underhanded” use _might_ be lower on /r/haskell. But on the other hand, [/r/haskell](/r/haskell) has over the years featured plenty of material which quite openly invites the conclusion that programmers either rush to embrace Haskell, spurning inferior languages (which is all of them) or else are mathematically demonstrable idiots with a perverse set of concerns and a retarded approach to satisfying them.
Rather than using `membero` and the like, I would suggest just exporting `member` and recommending that users solve clashes via a qualified import, or by hiding Prelude stuff.
To be fair, if you were a compiler you'd probably panic when edwardk started coding, too.
Personally, I tend to define `type BinOp a = a -&gt; a -&gt; a` and call it a day. It's clear enough from context and makes type signatures more readable when used appropriately.
Straying from the discussion, but I think &gt; That's not the way to win friends and influence people. sort of lost track of politeness. Beside, I readily admit that, in the context of other languages like Scala or Clojure, I'm an untrained hack. I just think you blew it out of proportion dude; untrained hack is pretty bland.
To me, this "reactive" idea is just a part of the "functional" idea. To not understand reactive programming is to not understand functional programming. It feels like a great way to answer the question "how does functional programming work?" for typically imperative thinkers, but doesn't seem to offer anything new.
How do you really define "smart"? 
Oh, natural language is far better at transmitting ideas than many people imagine--especially people inclined to the kinds of thinking necessary to be even more precise than what natural language does allow. If the overwhelming majority of readers would perceive something as a veiled insult, then that's quite simply *what it means*, no matter the intent behind the statement. Communicating accurately and honestly with natural language is about transmitting ideas as accurately and honestly as possible, which often has little to do with the strict denotation of what's said.
Sure, natural language can be used to communicate ideas very well—it just doesn't do that by the mechanism that a lot of people seem to want to believe that it does. 
If you hire Ernest Hemingway to write a phone book, will he be more productive than just some clerk ? 
False analogy. Ernest Hemingway's talent is very narrowly applicable (writing fiction) whereas Haskell is reasonably generally applicable to many programming problems, at least comparable in scope to Larry's favorite language Perl. I imagine most problems Perl was designed to solve would benefit from a Haskell programmer.
The analogy was not about Ernest Hemingway. It was about a phone book :) 90% of all the tasks programmers do are boring, primitive operations. CRUD, GUI, reports etc. Hiring haskell programmer will not boost productivity much, but will significantly affect the cost. Perhaps the right approach would be to have one-two haskell programmers that work on a core functionality and 10-20 java/vb programmers that work on various GUIs/Reports/Data entry etc. Just like a hospital can have 2 doctors and 20 nurses. 
That sounds reasonable and I will defer to you on this since you have more experience on this than me.
&gt; Flattering, but exaggerated Hey now, don't ruin my little ego boost over here :P
It looks like this is a prolog-style unification engine, whereas logict is just the nondeterminism monad with more fair disjunctions. Probably there are programs in this system that fail that would work better if it were implemented on top of logict instead of [].
This is a very nice recommendation. Thanks.
but magma is the algebraic object, not the operation itself.
I learned just fine before RWH, but it involved reading a bunch of papers. And on the "efficient code" side it involved hanging out on irc and following the mailing lists. I think it's the best way, but obviously it won't scale :-)
Glad to see this! I was just only wondering why we didn't have a user-friendly kanren-type thing in Haskell (and I know this is just a start in that direction) given the interest in logic-libraries elsewhere and the fine research done on them in Haskell and ML.
The good stuff on denotational semantics _per se_ is from the 70s, and v. little is actually online, which bothers me to no end.
I still don't know what _truly_ OO code is... any examples?
Somebody (Conal Elliot, I think) calls it "Haskell's imperative ghetto" .
You're right. `sth / TAny` were totally broken. As you suggested I got rid of `TAny` too. Now: *Main&gt; run $ do { x &lt;- fresh; y &lt;- fresh; heado x y; return [x,y] } [[?_0, [?_0, ?_2...]]] *Main&gt; run $ do { x &lt;- fresh; y &lt;- fresh; tailo x y; return [x,y] } [[?_0, [?_2, ?_0...]]] Maybe I add `sth` back when I figure out how to do it right.
It's early and I'd like to respond to this properly as I get time today (a good reply deserves another :) ), but I'll take on one point now. &gt; By talking about "undisciplined programmers" when you mean programmers with little mathematical training That was not the context initially, nor was it the context in the followup. An undisciplined programmer is someone who will rush in and start writing code without considering the larger architecture. They'll just keep changing things and poking things until it works. We've all been there and worked with others who do the same. You can get something working (although perhaps unreliably and difficult to maintain) this way in some languages, such as JS where you can do a lot of things and it'll still run. You can quickly change what you're passing where, and the runtime will not even care if you pass the wrong number of arguments into a function. This is one of the reasons it's so quick to prototype in. Haskell is not so forgiving here, which can be a blessing and a curse. Minor mistakes will mean you can't compile, even if it doesn't affect the bit you care about at the moment. It also causes problems when you hit a point deep inside a call stack and think "Right, now I just need to read a file *here*" if you've not considered that before. It requires more discipline to just get something working. This is hugely frustrating when you come to the language (I'm still quite new), and can be a big blocker (I left and came back a few times because of this). &gt; First of all, natural language entailment is far more subtle than logically-inclined people are willing to admit. Just because you intend it to be read one way does not mean that that's the only reading possible. Yes, which is why you call people out on it, and ask clarification, not accuse them of trying to insult others.
&gt; Hiring haskell programmer will not boost productivity much, but will significantly affect the cost. Why would it? It's not because you know Haskell that you can claim higher incomes. It depends on the experience you have, but same applies to every language. So the question should be "Why hire an expert in language X when only a freshly graduated who knows X is sufficient?". Haskell may make me more efficient, but I will be less efficient than a Java expert whose been in the race for 15 or 20 years. So no, if you want to be able to use Haskell at work, it would be a big mistake IMHO to use it as a pretext to pretend to higher wages, because companies will prefer to hire experts in mainstream languages if you're as pricey as they are!
I think what we lack is not an alternative to Real World Haskell, but an "Efficient Haskell", which supposes already a certain amount of knowledge of the language. I mean, it's that kind of what exists for other languages for people who want to master their craft. Another "Haskell Patterns" or "Architecture Design in Haskell" book would be neat too: what are iteratees/conduits/..., what is FRP, how do you make type-safe stuff [1]. Just like imperative developers have books about MVC and so on. [1] The Yesod book presents it a little, but it's focused on nice tricks and in the big picture web-development, not on general Haskell architecture design. Still this book is very intersting to read even for people who are not into web development.
I think Haskell is simple to learn if you have a good mentor. It's easier to adapt your teaching to the reactions of your student when you're not... well... a book.
&gt; It's called a Church encoding, which is essentially just a hard coding of constructors as a function that takes the args and returns the recursion schema (a fold) pre-applied. I've heard this as described as 'types are encoded as their elimination forms' which also coincides with the constructors in F-algebra models (well given some currying).
FRP also adds continuous values over reactive programming's events. Reactive programming is just a way to gain some of the declarative advantages of dataflow languages. The continuous part is what's causing all the issues from a research POV.
&gt; I still don't get the point of the separation between nodes and components: It's possible, though arguably a bad idea, to have systems that do the processing of several components per-entity. Or more commonly you may have a system that processes one component but requires the presence of other components to function. The node is really just a tuple of those components. For example you may have a RenderComponent and a PhysicsComponent both have the idea of a position and orientation. You could try and factor out the common transform information into a TransformComponent and change the RenderComponent and PhysicsComponent to require an entity to have a TransformComponent. I'm not sure whether this is a good idea though.
I'm posting this to elicit discussion, but as the maintainer of EclipseFP, I fail to see why Michael is not putting his considerable expertise in enhancing one of the existing IDE, instead of building a new one. You want to get developers from other languages to come to Haskell? Contribute to EclipseFP or to the Visual Studio/IntelliJ plugins. You want a Haskell-written IDE to show you eat your own dogfood? Contribute to Leksah. Oh, and the "IDE is for newcomers" mantra is wrong. I'm 15 years writing Java, and I still use an IDE. An IDE should make simple things easy and hard things manageable.
That's a huge project to undertake. I feel a bit bad for whoever's going to spend the next few years reinventing all those wheels. I do hope that they work to improve a few existing components like scion and hoogle, which would be work the rest of us can benefit from. Having tried Leksah, I can see why they didn't use it. Less sure of the story with EclipseFP.
I had briefly talked to FPComplete about them helping with EclipseFP, but the discussion died and it looks like they're going some other way. They've never told me why they won't use it and they think rewriting from scratch is a better option for them. I think scion is not maintained any more. My own library, buildwrapper, is what EclipseFP uses to hide the cabal and GHC API interaction behind a hopefully sane API.
Yi has been and still does look like the best idea in this context. It just makes so much sense.
Actually it's already optional: if it's installed then you got indentation for free, otherwise you get the "default" one, whatever it is. Do you want also a flag to enforce Cumino NOT to indent with stylish, even if is it installed? As regards for the plugin scope maybe you are right, but i'm only trying to incorporate nice features to make my haskell coding experience smoother :)
I have had some problems with haskell-mode too a while back, but I now work off master from haskell's github organization (using emacs prelude). Chris recently started using stylish-haskell and I definitely haven't had any problems since then. Perhaps try upgrading?
here's my config https://github.com/boothead/prelude. It's not bang up to date as I only do haskell in my spare time - I'll update it later when I get home.
They did take in a lot of feedback... fwiw my answers went something like 'Must work in OSX, have vi keybindings, not be Eclipse'.
I'm not familiar with indent, It's like tidy for html (or the pep8 tool for python). But I must be happy with haskell-stylish-on-save because: bford@..:~/.emacs.d|master⚡ ⇒ grin stylish personal personal/custom.el: 14 : '(haskell-stylish-on-save t) personal/customisation.el: 11 : '(haskell-stylish-on-save t) personal/haskell.el: 37 : ;; To enable stylish on save. 38 : '(haskell-stylish-on-save t)) 
Does it automatically update the buffer with the result? Why do you configure that setting in three different files? The first two seem redundant (without contenxt).
My preference in editors is just that: a preference. I've used Eclipse enough in the past and don't feel a strong desire to use it again. Luckily for the internet, opinions are ~~cheap~~free. They were soliciting feedback and I gave them some of mine. Am I their target user? Probably not. I do have a vested interest in Haskell being more commercially viable and hope they are successful.
&gt;Hiring haskell programmer will not boost productivity much, but will significantly affect the cost. I found both of those to be false.
What OS this new IDE will be running on ? windows ? linux ? OSX ? all 3 ?
http://xkcd.com/927/
Seems like `vi` satisfies those requirements. :)
Definitely, however I agree with [cigonsal's](http://www.reddit.com/r/haskell/comments/zpff3/larry_wall_you_should_probably_know_about_it/c66s9vm) comment. Too often in Haskell tutorials you read something like: *"Dont worry about this for now, it will make sense later"* 
Yes, Emacs like Yi and Vim has a well thought out model which allows you to create __your__ editor based on the extendable core with usable defaults. What's worked for Vim and Emacs sure works well for Yi too. From what I saw how to extend the OSGi based Eclipse it sure didn't look as approachable.
Thanks, feedback are very important for me. As regards the stylish-haskell question, I'll dig more deeply in the equalprg (I'm still a beginner Viml scripter). I was thinking to simply add an option in cumino, something like: let g:cumino_use_stylish = 0 (** will be the default **) Then with an "and" I can enable it only if is installed AND the user wants to. Does it makes sense? :)
Well in either case I hope it works out. I don't know who their target market for an IDE is and they may have their own reasons for building from scratch (if they are). I am sure they have someone to charge some $$ once they have built it.
https://github.com/bibanon/bibanon/wiki/The-Curse-of-the-Text-Editor
Alternate title: compile with `-Wall` and listen to it.
Or another one: having ghc-mod linting inside Vim saves a lot of time (both computational and of development) :)
I honestly think the solution to these problems is [Lisk](https://github.com/chrisdone/lisk), but I don't think many Haskellers will be excited to switch to Lisp-y syntax.
&gt; Philip's GSOC project to be presented this week in Denmark? Interesting, are the any more details on this?
&gt; I always felt what if language needs and IDE to be at least bearable to write (like java) then this language sucks. Or your code sucks. Or both. I agree in spirit, but to some degree it's a question of scale an familiarity. The usefulness of an IDE* is more apparent dealing 5 Million lines of code you inherited, as opposed to 3K lines you wrote yourself. *or pseudo-ide features (e.g. ctags etc).
&gt;And linux kernel is fine without IDE at 15m+ loc. I doubt anyone who hacks the kernel without an IDE isn't using at least ctags.
I agree - some pseudo-ide features are good to have. But I would never use "can do anything, but not wery well" monsters like eclipse. And in the context of the OP post - haskell newcomers don't need these either.
Just a note: `(\x -&gt; f x)` is equivalent to `f`.
Good rule. Bytestring does the same thing. Compile time literals are underappreciated.
I love asynchronous programming, but some people have a very hard time keeping it in their heads.
When I considered getting into programming, I tried learning C++ and couldn't make heads or tails of it and gave up. This happened two or three times before I finally tried another language, which I had no difficulty learning. It was only after years of programming experience that I could come back and start to grok C++, and it is only then that I started criticizing it. I've surely made up for it since then, but I don't think my case is that unusual. The most unusual aspect is that I was not entirely discouraged from programming due to the experience, whereas many people would have given up after the first or second bafflement.
JPMoresmau says above that he had some discussion with them, but conversation died without adequate explanation.
[Haskell Implementors Workshop, September 14th 2012, Copenhagen](http://www.haskell.org/haskellwiki/HaskellImplementorsWorkshop/2012): [Philipp Schuster and Andres Löh: Making cabal-install non-destructive](http://www.haskell.org/haskellwiki/HaskellImplementorsWorkshop/2012/Schuster) btw, I'm *really* looking forward to the videos :-) yay!
Agree. I've been using EclipseFP recently, and find it extremely useful. Especially as I'm also developing C++ code in parallel. I'd struggle to see how writing another IDE from scratch is a good idea. As to what the authors said about new developers feeling comfortable with an IDE, I don't exactly agree - unless it is an extremely smooth experience. Otherwise it just adds another level of configuration and tools which can screw up (and in the IDE case .. be left wondering why). 
Eclipse is subtly incredibly irritating. Every time I try it, thinking "it's been n years, it has to be better!", it seems just as laggy, just as micro-stuttery. It just feels _bad_.
I'm overjoyed to see more discussion about Haskell IDE's. As versatile as vim and emacs are I agree that there is probably some severe selection bias going on in the Haskell community. If FPComplete are going to be making a new IDE from scratch, however, I'd like to see it take advantage of a new beginning. For example: * I'd like to see it make full use of a powerful and static type system. Light Table and Sublime Text are **very** impressive projects with lots of potential, and the only argument I think I can agree with for not contributing to them is that they were originally designed for dynamic languages. * I'd like to see it side strongly with the 'convention over configuration' camp **particularly** if it is trying to be noob friendly. It should only be one click to create a functioning yesod project. As long as configuration is still available somewhere you're only going to improve the experience. * On the topic of configuration, I'd like to see that every UI configuration option maps directly to a text based configuration language (something like json or the xmonad EDSL). Being a noob friendly IDE is a valuable goal, but the project will have no future if all of its users graduate to 'adult' IDEs once they stop noobing it up. A high customization ceiling is important and as with program correctness I believe the best way to achieve this is to accommodate for it from the start. * I'd like to see the functionality be developed in a modular fashion such that it can be a healthy open source project, sharing its successes with other projects (like FPEclipse and Sublime Text) * I'd like to see it not re-invent the wheel in places where it doesn't intend to bring improvements. A new Haskell IDE aimed at being noob friendly is trying to make improvements in project refactoring and autocompletion, etc, for Haskell -- it should **not** be trying to re-write a UI library from scratch when they already exist * On the topic of UI, I'd like to see it use webkit or something like it. Light Table is demonstrating that a smooth UI can be achieved with the web platform (html/css/js) and I strongly believe the web platform is the platform of the future. Use something like [code mirror](http://codemirror.net/) and move on, don't waste time on what's been done. Finally remember that if you're going to make a new IDE, *make it different*. 
It doesn't help in dynamic languages like js, where you have to remember the types of objects. In a static language you know what your callback is being called on but if you have to run the code just to check what the type is things can get hellish.
No, that's the mysterious *secret* of space. The two are easily confused.
[This](http://i.imgur.com/AXQCh.jpg) I saw. *EDIT:* Hey, would be a nice name for the IDE...
Yes. I just had the impression that this was probably more of a technical discussion and wanted to point out that personal skill sets are also a deciding factor. For instance, hiring the lead contributor of a project is the easiest way to contribute to that project.
Argh! Hopefully these will find their way onto Vimeo at some point. Youtube is blocked from work :-(
I'm not sure that the concept of a "beginners IDE" even makes sense. Beginners are precisely the people who should be using Emacs and GHC. Universities are teaching students to write code this way so they understand how the basic tools actually work. IDEs are something you use when you are beyond toy examples and learning.
Me too, I saw this on my frontpage all day thinking it was a science submission.
&gt; If you put a database programmer hat on, it's pretty much exactly what you'd get Mmh, I don't see why. It also pretty much what you get when you make simple aggregations in any language with structured data. &gt; Well they're elegant up until you need a sum type Haw, haw!
So, this is probably explained in the git pages of one of the plugins, but I have no idea where to start looking.. When editing a line with nested parentheses, if I delete a closing parenthesis inside the nesting, because I want to move it, and then want to type a closing parenthesis, it jumps to the nearest existing closing parenthesis instead. How do I change this / get vim to actually insert the parenthesis. 
&gt; Youtube is blocked from work :-( Harsh!
These are excellent points, and the issues you raise are important to us. Remember that driving commercial adoption of Haskell is our priority and we do this because we think the Haskell open source community has developed something truly valuable to commercial users. The things we are doing are driven by customer needs and what people are telling us. In essence, Michael is sharing our design goals based on what we have heard and is looking for feedback. We haven't said we are reinventing the wheel. Our goal is to leverage the outstanding work the community is producing. As Michael mentioned, he is not downplaying the importance of Leksah and EclipseFP. We believe these projects have great value and intend to leverage this work as much as possible. Michael is discussing design goals here, not implementation. As you mention, the online documentation and educational resources are good and demonstrate great community contribution, but we need to do better. Education and documentation are central to the broader adoption of Haskell. Our goal is to work with the community to make this even better. 
That looks like exactly the sort of thing I want :)
I'm more asking about turning TH-based code into a standalone code generator than how to do things in TH itself :)
SSH proxy?
Still no luck. It's kind of bizarre because the error message looks kind of like a server error, but if I strip the path off the URL and just go to www.serpentine.com I get a spyware removal message from my web filter. This behavior persists even after I explicitly whitelist the domain, so something strange is going on.
Is that really the only feasible way?
http://www.google-melange.com/gsoc/project/google/gsoc2012/phischu/19001
I don't know how much hlint and ghc-mod overlaps, but the latter already gives useful linting suggestion. I guess I can give hlint a shot, though :)
Tantalising, but this is a blog post that acts as a long way to say "I am going to write some blog posts!"
This is awesome! cool!
&gt; We're also hoping to provide some "humanization" of error messages to make them less intimidating (with the full error message still available for those who want to see it). This would be a very good idea to provide it as a separate tool. I think the whole community would benefit from that, esp. the beginners.
&gt; Oh, and the "IDE is for newcomers" mantra is wrong I would exactly say the reverse: IDE is _not_ for newcomers.
Heh. "IDEs are for people who are beyond learning."
I'd suggest either dividing by 10 before the numbers get to those charts or making it very clear that those numbers are for 10 operations.
So, I actually built an "unget" anyway, but I'm not using it for this. That and my first attempt at a decoding proxy are up at &lt;https://gist.github.com/3686732&gt;. It works pretty well, but has one small problem: if you ask for more characters than there are left in the file, that will trigger the proxy to ask the upstream for more bytes after the very last bytes have been returned. On all of my existing upstreams, this terminates the session, which means the Session ends and those last few characters are lost.
I really just want something like [typerex](http://www.typerex.org/) for Haskell, I think.
Let's do a many-to-many join benchmark and include esqueleto on the mix =P. EDIT: In all seriousness, though, it would be interesting to see how esqueleto fares with simple queries such as select $ from $ \p -&gt; where_ (p ^. PersonId ==. val pid) return p which is basically just a complicated way of saying "get pid".
Note that if your define something like newtype IOT m a = IOT (World -&gt; m (World, a)) you'll be able (with, say, m ~ []) to write code that discards the world, or produce multiple copies of it. This might make sense in GHC, but that's because GHC's definition of IO has nothing to do with semantics; its only a trick to force sequencing of the unsafe IO primitives. This is important to stress: *you can see IO's semantics as a state monad, but that's not what the GHC definition does*. Why? because GHC's IO is defined in terms of impure functions and you don't want these in your semantic domain. If you want to see IO as a *real* state monad, then an IO action takes the whole state of the world as an input value, and returns the new state of the world alongside its result. And if two actions take the same world as input, then only one of them can possibly have its effect reflected in *our* reality.
&gt; They've never told me why they won't use it and they think rewriting from scratch is a better option for them. Actually, if they wanted to sell the IDE commercially, then it would make sense to start from scratch, due to licensing.
Here's [a gem by edwardkmett](http://www.reddit.com/r/haskell/comments/vuty0/what_is_the_advantage_of_monad_transformers_over/c57yeyw) on why the common monad transformers exist.
A bit heavy in CT but I'll try :)
I guess so. "get pid" is just how you'd write the code to get the person who has ID pid in persistent. 
But what is the cause of the problem really in these examples? Is it just the existence of `IOT`, or rather the existence of `runIOT`? What if we had `IOT` but no `runIOT`, we had `instance Monad m =&gt; MonadIO (IOT m)`, and `main :: MonadIO m =&gt; m ()`?
There the problem would be that `MonadIO m =&gt; m ()` is less powerful that `IO`. IIRC, exceptions cause nasty problems in the former.
&gt; Function signatures would be easier on novices if there weren't so much "left to right" versus "right to left" tension in how functions are described and applied. Can you elaborate?
So where's the problem in API docs actually adding information?
I don't think I understand the problem you're describing.
If we just restrict ourselves to `ST`, the reason there is no `STT` is that the purpose of `ST` is to ensure linear use of a resource, the heap. However, if we parameterize by another monad, there is no way to guarantee that that property is preserved. Filling in `Maybe` allows the resource to be discarded, and filling in `[]` allows the resource to be discarded or duplicated arbitrarily. You can, of course, implement an `STT` via means that just clone the heap in the above situations (that is, use a persistent map as the heap, and then use `STT m = StateT Heap m`), but then the optimizations to use mutation in practice are invalid, and that is the entire point of `ST`. This all applies to `IO`, too, inasmuch as it overlaps `ST`'s functionality, but it has other stuff that wouldn't make a lot of sense in a parameterized scenario.
Function application associates to the left (`f x y` is the same as `(f x) y`), while the function type constructor associates to the right (`a -&gt; b -&gt; c` is the same as `a -&gt; (b -&gt; c)`).
Ah. So this program both destroys the world and doesn't: main :: IOT [] () main = do mwahaha &lt;- lift [True, False] case mwahaha of True -&gt; launchMissiles False -&gt; return () 
&gt; However, this does increase cognitive load, putting novices over threshold on seeing how type signatures compose. In my experience, type signature confusion rarely arises because of the right-associativity of `-&gt;` in type signatures. Rather, it is often because novices don't understand how parametric polymorphism works. (e.g. `id :: a -&gt; a` means it spits out the same type of thing you put in. Novices often think `a -&gt; a` means `Any -&gt; Any`, which is completely different.) And parametric polymorphism is littered all over the basic Prelude functions and introductory Haskell material.
Basing on my Java experience, there are usually tens of statements for one transaction, so I expected that the one transaction per statement would give cost much higher than typical, and disabling them got benchmark closer to the real applications. I tuned the benchmark to measure operations with the transaction costs. The database operations made difference in libraries overhead less drastic. http://lykahb.github.com/groundhog/SqliteBenchTransaction.html http://lykahb.github.com/groundhog/PostgreSQLBenchTransaction.html
At least one video has no audio for me: e.g. HIW 2012. Niklas Broberg: haskell-suite 
There are some arguments in favor of this reverse arrow style. One is that the type of function composition makes a bit more sense. I never thought about what currying would look like though.
Your mental model is basically the 'free monad implementation of IO'. Check out this series: http://comonad.com/reader/2011/free-monads-for-less/ -- [part 3](http://comonad.com/reader/2011/free-monads-for-less-3/) talks about using this to implement IO on top of an FFI functor which represents the pure value "runtime system, please call a low-level machine function and process the result with a pure function"
by the way, for continuous string appending, I think blaze-builder is more useful. Internally, blaze-builder is essentially the same as DList
You can add things to the end in constant time, but the tradeoff is that accessing individual elements takes linear time. In practise we often build and transform entire lists, eventually folding them into a summary value, this implementation will perform roughly the same as lists for that common use case, except that adding items to the end will be faster. In other cases this is not an improvement.
I prefer that model as well.
I've gone through these posts a while ago, and I have to say I can't see the difference between this and - let's say - yampa. Although it was interesting. Took a while to wrap my mind around it.
(2) is implicit in (1), I think. I would imagine you also want a function liftM :: Monad m =&gt; M a -&gt; MT m a
Hm is it automatically true that whenever MT is a monad transformer, there exists a function of type (Monad m) =&gt; MT Identity a -&gt; MT m a ?
Parsec (not AttoParsec) has a Perms module derived from this paper. I think Ross Paterson has a general (depends only on Applicative/Alternative) Perm library on Hackage. 
There's [permute](http://hackage.haskell.org/packages/archive/permute/1.0/doc/html/Text-ParserCombinators-Perm.html) on hackage. "This module implements permutation parsers, and is a generalisation of Text.Parsec.Perm that will work with any parser combinator library. The algorithm is described in *Parsing Permutation Phrases* ..." It does work with Attoparsec, but you'll notice that the approach will restrict how you structure your parsers. If memory serves right, it's insufficient that you find an appropriate permutation matching the data, but you also have to be able to match a terminating parser outside the permutation, because otherwise Attoparsec will just return a Partial value because you haven't forced the remaining permutations to fail. Another limitation also dealing with the way that permutation phrases work, is that I believe the permutations have to be on a single 'level', you can't have inner permutations parsers *contributing* to an outer permutation parser which would build up a tree of permutation parsers (if an inner level doesn't contribute to the outer one and is just disconnected it should work, but you might run into the first caveat I mentioned above, which is that you actually have to be able to terminate the inner one for the outer one to terminate as well). Out of order parsing in a specific instead of general fashion is also implemented in [aeson](http://hackage.haskell.org/package/aeson), which uses an ADT intermediate representation and parses the results into a map ([Object](http://hackage.haskell.org/packages/archive/aeson/0.6.0.2/doc/html/Data-Aeson-Types.html#t:Object)).
&gt;redditor for 3 hours Meh.
This shitpost can be made better by providing pull request or at least bug report. Get back with your unpopular opinion once you learn how opensource community developed projects work. Maybe then people will care about it a bit.
Really hope that this will succeed well! Given that darcs user base and haskell user base are much overlapped, just choose haskell as a prime choice of language and support all haskell-oriented functions, which github does not do yet. For example, supporting cabal more clearly and automatic integration with haddock would be great (i.e, every cabal project has its own website with haddock documentation like hackageDB ) Once setting up, other language correspondent would be followed by contributions from inspired people who wants to have similar service in their beloved language.
All of the UI buttons are cut-off on the left edge of my browser. This makes it hard to use.
If they're doing language comparison I hope they have the source in a github repo. 
Thanks for the link, and all feedback. I've since announced: [darcsden 1.0](http://thread.gmane.org/gmane.comp.version-control.darcs.user/26556) and [hub.darcs.net](http://thread.gmane.org/gmane.comp.version-control.darcs.user/26557). I very much wanted to see if it was feasible to run a reliable, useful repo hosting site based on darcs, and I have got close enough to see the answer is yes. Hurrah! So darcs hub is now usable, hackable, and ready to deploy your patches; its future depends very much on you. Join us in #darcs if you have questions! 
That paper is from Utrecht University, so naturally, [permutation parsing](http://hackage.haskell.org/packages/archive/uu-parsinglib/2.7.3.4/doc/html/Text-ParserCombinators-UU-Demo-MergeAndPermute.html) is included in their excellent [uu-parsinglib](http://hackage.haskell.org/package/uu-parsinglib).
I'll just leave this here: at·trac·tive adjective 1. providing pleasure or delight, especially in appearance or manner; pleasing; charming; alluring: an attractive personality. 2. arousing interest or engaging one's thought, consideration, etc.: an attractive idea; an attractive price. 
I believe this misses the point the article made. Just because it is a compliment does not mean it isn't sexist. The article specifically addresses this by noting that it subtly implies that women are only valued for their looks. If you truly believe it is not sexist, then ask yourself why nobody comments on handsomeness as a virtue for male programmers.
Or maybe he was just making a joke, and everyone should lighten up? Or was that also sexist? I've never found CS crowds particularly sexist. Quite the opposite, actually. Yes, men look at women. We can hide this and not talk about it, or we can act like grownups, admit it, and joke about it.
I am going to criticize this post, but before I do, let me just say that I do recognize that sexism is still a very real problem, and I admire this person for taking the time to raise awareness by blogging about it. I hope that my criticism (and it's gonna be fairly harsh) does not detract from the effort to eradicate sexism from our society. [edit] disclaimer: This was my first post of many in this discussion. My opinion has changed somewhat since the time I posted this. &gt; During the "Future of Haskell" discussion, Doaitse Swierstra (a professor of computer science at the University of Utrecht), suggested that a good way to increase the number of Haskell programmers would be to recruit one woman for every man in the room and that this would be a good thing because it would "make the meetings more attractive". This is the central example that the post is based on. There were several interpretations of this statement that I disagree with. The fundamental point that I disagree with is that what Swierstra said was inappropriate or "sexist". &gt; * tells women that a space is unsafe for them. * saying that women are valued for how they look, not for what they do. ("objectification" or "hypersexualization") * potentially alienating to any non-heterosexual men who were present, as they reflected an assumption that he was speaking to an audience of people who found women, and only women, "attractive". * talking about women who have cissexual bodies, are thin, aren't disabled, and are in a particular, narrow age range... if you're a woman and not all of those descriptors apply to you, maybe you shouldn't think about learning Haskell * what matters is not his intent, but the effect of his words. That is extrapolating an *awful lot* out of very few words. The author even acknowledges that Swierstra probably didn't *intend* to communicate any of these things, but still insists that the audience and/or moderators should have corrected him. I feel that targetting this particular comment will prove counterproductive: people will be scared to even touch on this subject for fear of their good intentions going awry and accidentally driving women away. If you want to raise awareness of this problem, then you should *encourage* discussion about it, rather than telling people to walk on eggshells. The "one woman for every man" comment is fairly innocent, and although it might be interpreted as referring to heterosexual relationships, I don't think this is the part of the comment that the author is targeting. The entire post is centered on the usage of the word "attractive", and the immediate and unfair assumption is that this exclusively means "sexually attractive". There are other forms of attraction, and the word was spoken in a broader sense. Men and women are pre-programmed differently. Men cannot breast-feed babies, which one simple interpretation for why men's personalities are stereotypically rougher and less pleasant than women's, because women *must* take care of newborn babies in an intimate fashion, and women *often* take care of children as they grow older. The company of women is "attractive" to men not only because they might be aesthetically pleasing, but also because they typically have gentler personalities. I watched the video of Grace Hopper posted to /r/programming recently, that woman has a certain charm that was very "attractive"; where a woman like her goes, I'd be more likely to go, too. Why did the audience give a hearty laugh in response to this comment? Was it because those men were thinking "yeah, I want to get laid at the next Haskell Symposium?" Of course not. They laughed because it was *self-deprecating humor*. If anything, it was sexist *against men*, saying that men are less pleasant company than women. No one objected to the comment because in most of their minds, it was not objectionable. It was not sexist. It was merely a light-hearted reminder that there are too few women in computer science. &gt; tells women that a space is unsafe for them. The sad reality is that rape is rampant in our society. It is a disgusting truth that today's women simply have to deal with. A comment that reminds you that a large majority of heterosexual men that are sexually attracted to eligible young women will be present at a programming conference should not be news to anyone. ---- [edit] ~~I say a few things that follow which I am told could come off as very condescending towards women. So if you are a woman and reading this, then please realize that what I am about to say is probably already very obvious to you. I'm leaving the text, though, in the unlikely event that it proves enlightening to *someone*.~~ ---- [edit2] I've deleted some of this section because it was tangential, condescending, and overall just a bad idea to include here in the first place. I've left the anecdote mostly intact to supply context for a particular subthread.] ---- I served as a Mormon missionary for two years, and I noticed something about the female missionaries. These women *dressed a little frumpy* on purpose. They didn't want to get invited into someone's home just because they were pretty; they wanted to engage with people that were sincerely interested in talking about the gospel. To this end, they just didn't pretty themselves up as much as they used to. They wore clothes that *didn't* showcase their curves, and makeup and hairstyles that *wouldn't* attract unwanted attention. [edit: maybe I'm just really thick or very sheltered but as a male the concept of a woman intentionally dressing to deflect attention had never really crossed my mind.]
You do a fantastic job motivating why concepts from category theory solve Real World problems.
Looks like there's an issue for that. [Layout of links and navigation places them offscreen.](http://hub.darcs.net/simon/darcsden/issue/16)
I think the main thing the post is missing are more concrete examples of the cross-language or cross-framework functors that I mentioned rather than the contrived ones I give. The main reason I wrote this post was that the next pipes release provides an interop mechanism between pipes code and other Iteratee libraries and also an interop mechanism between various pipes extensions and I was trying to lay the conceptual groundwork for all of that. So hopefully when people see all those examples they will have a more concrete handle on what I am talking about.
What about SSE / AVX? If you're not using it you're leaving a lot of parallelism on the floor. It would also be interesting to compare these solutions with good implementations of Fortran and OpenCL, or even handwritten SSE / AVX code that uses threads manually.
&gt; That being said, the comment "they usually don't program, they leave it to us." is the more damning one. He also said that most women come from the math department. So it is possible it means "mathematicians usually don't program, ..."
I just want to give a big heads up to any guys who want to have an opinion (and oh boy do guys always want to have opinions about gender issues) about this: Imagine instead of this being an isolated incident that your entire professional and personal life has a constant background noise of little sexist things like these comments constantly devaluing you in tiny decrements and making you feel as if you were born defective. Men never see this stuff in totality which means they can't piece together the experience of being a woman in a man-dominated world. All we see is isolated phenomena in small subsets of personal experience. P.S. The worst thing you can say is "It's not there." because you don't get to decide how people are affected by these words. If these words make people feel uncomfortable, unwelcome, and disrespected, then they do. P. P. S. No. I take it back. I read the comments. The worst thing you can say is that women should change their behavior to avoid being mistreated. You know who you are and you can rightly fuck off.
&gt; we can act like grownups, admit it, and joke about it You mean ignore the interests of others who feel as if that behavior is hostile and completely unwelcome? That sounds more like a selfish child than an adult. I was of the opinion that adults try to accommodate their peers on mutually respectful terms.
And to a significant extent, what he *meant* doesn't really matter if many (and likely most) people interpret it according to the first definition in the context.
Wow. *Fantastic* job proving the article's point, there. Good work!
This is legitimate criticism, but any attempt at responding to it runs the risk of being a back-and-forth dialogue of "no, *you* take responsibility". I will therefore take care in my response to avoid that thread. &gt; #2. Oh but it's alright though, girls can bear the responsibility by altering their dress and making sure not to be to hot and tempting to our week little souls. Your use of "our" is very insulting, and is in fact sexist, overcharacterizing all men as rapists. Do not group me with rapists. I am not a weak little soul, and will never rape anyone, no matter how attractively dressed. I was offering that tidbit of advice because I honestly and sincerely hope that it will help someone to not get raped. I hope that male readers will also spread this advice to women they know. If you are surrounded by many friends that can protect you, then this sort of precaution is probably unnecessary. &gt; It does not matter what is intended by ones words, what matters is what is communicated. This is both entirely true and entirely false. If you are the one speaking, it is entirely true. If you are the one listening, then it is entirely false. The reason I dislike this is that it discourages speakers from speaking about the issue, for fear of *communicating* something that they did not *intend*. Should we all think a little more carefully about what we say? Of course. But your point #2 is a perfect example of what I am talking about. I had no intention of communicating that men have no responsibility in this matter, and it is a gross misinterpretation on the part of the listener to extrapolate that. I make no apology for offering advice that might help someone. So let me make my intent and communication clear: sexism and rape are very real problems, and no matter how much any individual man bears responsibility for preventing it, he can't mitigate all the risks presented by a plethora of other men being present, not because tons of men are ready to rape you, but because odds are in a large crowd there are a rare few that might. I plead with the women in computer science to both take responsibility, and to encourage the men around them to also take responsibility to solve these problems. There *are* things that *you* can do to protect yourself; please make sure that you do. And please, do not make enemies out of allies. As a final note, may I mention that the links in the article are indeed an excellent read for men that are interested in being more cautious about what they say.
I agree with you on this instance. The intended meaning was unclear and therefore has little relevance to whether what he said was damaging. I just think that if we're going to have this discussion we should realize that there *is* another possible meaning. Also, I don't get why people are downvoting you. Does no one follow reddiquette? 
Yes, it's particularly worth considering for anyone who might speak to Prof. Swierstra himself about it. But in cases like this, even if it *was* unintended, there's a common tendency for people to focus on questions of intent (nobody wants to think badly of the speaker, I'm sure) so much that it displaces any consideration of why the actual remark was a problem to begin with. For anyone who doesn't know Swierstra personally and isn't likely to speak with him about the subject, the intent is essentially irrelevant.
&gt; Your use of "our" is very insulting, and is in fact sexist, overcharacterizing all men as rapists. That was sarcasm. &gt; The reason I dislike this is that it discourages speakers from speaking about the issue, for fear of communicating something that they did not intend. Should we all think a little more carefully about what we say? Of course. Then we should be able to take criticism and learn and grow from it. It is not the end of the world to be called out on sexism. You can handle it quite gracefully, in fact. "Thank you for pointing that out, I had never thought about that before. I will be sure to be more mindful in the future." Or if you disagree "Could you go into more detail about that, I am not sure that I see your point." &gt; There are things that you can do to protect yourself; please make sure that you do. And please, do not make enemies out of allies. When I hear words like this, I tend to translate them as "I am ready to listen, but only if your words fit into my preconceived notions and don't upset me too much." I believe this is an issue that requires a lot of compassion and listening on our part, and if we are not prepared to work through our own feelings of being offended by what is said, we will just get stuck and never make progress. EDIT: it is very fascinating to me how men feel very threatened by the possibility of being called out on sexism. We tend to turn the tables around and get very huffy and entitled that other's are "stepping on our freedom of speech" or some other nonesense when infact they are meerly practicing theirs.
&gt; But in cases like this, even if it was unintended, there's a common tendency for people to focus on questions of intent (nobody wants to think badly of the speaker, I'm sure) so much that it displaces any consideration of why the actual remark was a problem to begin with. 100% agree. &gt; For anyone who doesn't know Swierstra personally and isn't likely to speak with him about the subject, the intent is essentially irrelevant. I hadn't thought of it that way. It's definitely a good point. I do hope that someone brings this up with him. The welcoming nature of the Haskell community was a big part of what got me into Haskell. I would absolutely hate to think that anyone is turned away because things like this are considered OK.
When I read that line “make the meetings more attractive”, that's the interpretation I read. Seems dependent on the audience (and in this case Tim's) to read a sexist comment out of it. Kind of an odd interpretation given that the speaker is dealing with a delicate issue and should be assumed to be sensitive to the issue, not completely ignorant to say something like “women are pretty herp derp.” If any people in the audience took that as a joke, shame on them. This reminds me of a stand-up set that Bill Hicks did about gay people. He was on Letterman. He made some satirical comments deriding gay people, and the audience, rather than laughing, cheered and jeered without a shadow of irony. The look of horror on his face… I imagine Doaitse felt the same. Seems a bit ridiculous, this long post by Tim seems predicated upon this negative assumption about Doaitse. It's quite a big one, and I'd expect a strong, explicit apology from Tim for this kind of character assassination if inaccurate. It's a pity because the message is good, but the catalyst seems grey area.
It has an interesting implementation, but do you know of any advantages compared to DLists?
Yeah, shows like Family Guy try to take the stance that because something is framed as a joke, it's not really offensive or demeaning. There's a guy who comes to my friend's Magic: The Gathering games and always rifles off a bunch of "women are bad at Magic" lines. He's a major alpha-male type person, so there's no getting into his head that what he's doing is wrong. I want to be proactive, but I fear we can only wait for the culture to change.
The reason I felt the need to speak out is because I don't want women to think it's OK to be hypersensitive about a comment that *could* be interpreted as sexist. There is a tendency to overcorrect when it comes to sexism, and I feel that this is harmful to the end goal of gender equality. Nobody decries the phrase "black is beautiful" as racist, and yet in this case, someone says that more women would make a conference more attractive, and now he is sexist? The kicker for me is that the comment's original intent was *clearly* one of raising awareness of the gender gap. This is why I am not buying into the intent/communcicated bit. This is the part that bothers me most. The message that *intent doesn't matter*. We should not let people think that it is OK to interpret things in a way that was not intended. Swierstra's comment was, if anything, intended as a *compliment* to women. I find it counterintuitive to decry it as sexist. The author expected someone to correct him, to point out that what he said was sexist, as if it were completely obvious that what he said was inappropriate. But the only reason I see to take this comment as inappropriate is by completely ignoring the intent with which it was delivered. &gt; I am ready to listen, but only if your words fit into my preconceived notions and don't upset me too much. Well to be frank, this is how everyone behaves naturally. It's just a matter of how flexible your preconceived notions are, and how easy it is to upset you. I believe I was "willing to listen" to this article, but in the end my preconceived notions about intent and sexism did not agree with the author's, at which point I produced a few walls of text in response.
&gt; we should be encouraging discussion Absolutely! That is why we should encourage people to be vocal about their experiences instead of telling them to shut up and stop rocking the boat and hurting our feelings and making us second guess ourselves. Pretending that it is too much of a burden to risk being accused of sexism is to dismiss the omnipresent threat of accusations that ~~women~~ the oppressed face anytime they choose to open their mouths and talk about these things. 
&gt; No. I take it back. I read the comments. The worst thing you can say is that women should change their behavior to avoid being mistreated. You know who you are and you can rightly fuck off. I assume this is directed at me. Let me just say that any advice I gave for women changing their behavior is not because I expect women to change *for me*, but rather, because it is a cruel world and I want them to have the best chance that they can. We should not spread the misconception that women can simply go places alone and dress in such a way that they become noticed by the wrong people. I wish that stupid guys would stop being stupid too, but cultural shifts are slower than that. You are the master of your fate, and especially when it comes to rape, women need to make sure they take fate into their own hands as much as possible. Downvote me all you like but if I have helped even one woman remember to be more cautious about preventing rape then I will be satisfied.
&gt; I think that is a pretty crappy way to look at things, laughing it off and normalizing the behavior. [...] Wise words, but I thought I'd add this: very often, these "jokes" are no less than the tool a harasser uses for testing potential victims' boundaries. Unassertive responses can lead to escalation.
I will concede that it is not desirable to encourage people taking great liberties in their interpretation and running away with them just because it suits their fancy. Gods know we've all seen plenty of this in politics etc. However, I do not feel this is what is happening here. Words carry weight. They effect people. We build very strong relationships with words. Words are magical. With words, I am able to take ideas that are in my head and (through an admittedly lossy process) put them in your head. When someone has gone through the world experiencing words very differently from how we experience them, they are going to have very different associations to them. I merely posit that we should listen to what they have to say, and not go on with this "well I don't feel that, what are you talking about," charade. It is dismissive and it is insulting and if you truly want to fight *ism you should be prepared to take people at their word that, yes indeed, these things do exist and are problems. Because frankly, you have no experience to judge these things by, and they do. I've never been to the Amazon, Europe, Japan, the Moon. If I wanted to I could sit here and shake my head at anyone who said these places exist, but I don't, because the outstanding evidence and testimony from people who have tells me otherwise. So we should treat people who are victims of *ism. They have been there, they are there, and damned if they don't have stories to tell. 
Thanks for the feedback. I fixed the order and put monad morphisms last since it is the most difficult section.
&gt; EDIT: try asking any of your female friends some time what happens if they tell a guy who's been chatting them up at a bar that they aren't interested. Likely they feel threatened and try to use excuses like "I have a boyfriend" or some such because of the reactions they have gotten from assertively saying no. Behaviour like this from both sexes is what keeps me out of bars.
&gt; I think this is what aggravates me the most when these kinds of things get brought up. "I don't see what the big deal is. You should take it as a compliment and lighten up." Really? I've found most of the time it's the opposite. When these things get brought up usually all the men apologize or also go out for blood on any man who might have been sexist (though they often assume that neither they, nor any woman, ever are, which is of course the real problem).
Absolutely right; my mistake. What I meant by that is that it *affects* women, although male rape is also a very real problem. It is, of course, everyone's responsibility to help prevent and eliminate these abhorrent behaviors from our society. However, no one controls your fate more than you do, so in the short term, it is up to each person to take necessary precautions to protect himself or herself.
My experience too. As an aside, I often like to remind those who struggle with this notation that Visual Basic has similar, with the return type specified to the right. In practice, this was never the real issue anyway; the student has misidentified what was really causing them difficulty. The challenge is in coaching the student to overcoming the misidentification of the misunderstanding, not the misunderstanding itself.
I also was present. You can see me on the video (top left) looking really uncomfortable just after those remarks. I agree totally with your "Firstly" and your "Thirdly". I disagree with your "Secondly": it's clear that he's cracking a joke, not just making a point that people found funny accidentally, and that the construction of the joke depends on the presence of both meanings (a double meaning set up by the "woman for every man" opening). I was thinking "defeat snatched from jaws of victory: the innuendo alienates the very people he wants us to encourage". I wasn't thinking "PITCHFORKS NOW!". I just thought it was a sad irony and an indicator of the scale of the problem. It's the collective drip-drip of remarks like these, which are clearly without malice but which reinforce a toxic culture, that's the systemic problem that we all need to tackle. I wish I'd heckled, making a joke at the expense of that irony, but I didn't get past shocked and awkward quickly enough.
The hard to hear bit is "more attractive to come to".
Any such comparison without a C/C++ version tells me very little. 
`fmap :: (a -&gt; b) -&gt; ([a] -&gt; [b])` `arr :: (a -&gt; b) -&gt; Kleisli [] a b`, which expands to `arr :: (a -&gt; b) -&gt; (a -&gt; [b])` and from the source: instance Monad m =&gt; Arrow (Kleisli m) where arr f = Kleisli (return . f) That's not to say that there isn't a functor from a monad's base category to its Kleisli one, but it doesn't really fit into the Haskell `Functor` class. Edit: I think what you're thinking of is `arr` on the `SP` `Arrow` instance.
Thank you for the clarification.
Depends on the crowd. With really divisive issues, and in a context where nobody wants to spark a huge argument, if people think their opinion is a minority in a group they're likely to stay quiet. Having someone speak up with an opposing opinion and not get challenged reinforces this. Once a particular side has established itself that way, people with very strong views on that side will feel safe making their feelings known, and so on. End result is an appearance of uniform opinion tending to the extremes even if many people there are ambivalent or have opposing views. And the moral of this story is: Speak up! The sooner, the better!
That's not what the paragraph said. Tim didn't leave Haskell because of the university. He left Haskell because, as the post says, "I have a job that doesn't involve Haskell, and lack the privilege of having spare time and energy left to do programming projects when I'm done with paying work." The very next sentence notes that this is something that he often regrets. I'm glad this post was written. Would I have written it the same way? No. But then I'm not the person who took the time to write the post, and the post is not the problem, it just does us a service by calling attention to the problem.
If someone had told me that we need more gay (as in happy) people in the Haskell community, you bet people would be upset.
&gt; I will denote our "source" category's identity as idA You used 'id' instead of 'idA' for the right identity. Great post though.
&gt; replace ♀ with whatever word indicating a subject of an individual of the Homo Sapiens (sapiens) species that happens to have two X chromosomes. I just wanted to point out that not everybody with two X chromosomes is a woman, and not everybody who is a woman has two X chromosomes.
If you think that even one woman needs to be condescended to in such a treacly way about the basic facts of her daily life, you need to have more conversations with actual women.
Men deal with it. yes. But when is the last time, as a man, you were walking in a parking garage alone late at night, and saw another dude and worried "He wants to rape me" instead of "He wants my wallet". A woman tends to worry about both things.
It is true that offense is subjective, BUT if you want to encourage women to join the Haskell community then we cannot be offensive, even unintentionally. We can't say that we care about them being part of our community and then turn around and completely disregard their criticisms of our community's culture. If they are truly to be equal members of this community then their voice, subjective or not, must matter as much as ours.
I just watched the video and wanted to add to the comments so far, that on the positive side that before Doaitse and the audience went off the rails by adding "more attractive" and laughing at that, he called for recruiting more women, and the audience cheered, I'm guessing mostly because lots of the audience at least subconsciously recognizes the total gender imbalance as problematic for all. On the negative side, I found Contorer's followup, that he supported more women programmers because he married one he worked with, way more crass and disturbing. Hopefully in the future Doaitse or someone wanting to make his comment, will instead say something along the lines of "There aren't many women in this community, relative to the industry [the talk was largely about use in industry]. That's a big problem for the community and for the women who aren't part of it, but should be. We've got to make sure we're completely welcoming to all, and any sexist or other discriminatory behavior is considered intolerable." Or something else, but something unambiguous.
I'd like to think that we could all firmly agree that offending and alienating potential Haskell programmers for reasons that have absolutely nothing to do with programming is undesirable and should be avoided at all times (sadly, I don't think we can enforce this via the type system).
&gt; And a functor isn't necessarily a transformation, it's just a collection of correspondences, i.e. a mapping, which is to say that it isn't necessarily something we can code up (although in Haskell of course we can/must). I'm somewhat inclined to forgive this one, at least in the context of programming--a functor is a "transformation" about as much as a function is, and emphasizing that similarity (a function can be seen as a mapping as well, after all) seems like a net positive for intuition-building. Basically, I don't think it's a problem to implicitly limit things to programming languages (which tend to be "sufficiently constructivist") for the sake of introducing the concepts, as long as it's truly a reduction in scope, rather than anything *strictly inaccurate*. Unfortunately the terminology is getting a bit muddled in places, which is *terribly* counterproductive and does create such inaccuracies, as in several examples you mentioned. The use of jargon in general could be improved, I think; it feels like an awkward attempt to mix mainstream programmer lingo with category theory terminology without thorough consideration of what use of jargon is helpful vs. just a distraction. That said, I personally think the basic idea here is fine, and that with some editing based on sclv's critique the result would be a much stronger and more persuasive article.
&gt; These are the realities. But sadly, the dude who gets his car stolen isn't catch nearly as much flack as the woman who was raped, even if he left it in a bad part of the town. But at the end of the day, the person's who car is stolen will catch less blame for his actions than the woman who was raped. You know what? These are the worst people (Those who blame the rape victim) in this situation. Because of them anyone who tries to point out this: &gt;But we don't live in a perfect world. Gets vilified because suddenly we're blaming the victim too. You know what? I'm fucking tired of it. I'm tired of them giving rational people a bad name, and I'm tired of more irrational people making more generalizations due to that. Edit: There is another vocal group of white knights/whatever that suggest that women (or men, men get raped too, does anyone remember that? Kids too) should be able to go wherever they want and not be at risk of anything.. Which is completely naive, even if it /should/ be that way.
&gt; Seems a bit ridiculous, this long post by Tim seems predicated upon this negative assumption about Doaitse. Tim did not make any kind of negative assumption about Doaitse, in fact quite the opposite: "*So while Prof. Swierstra may have meant no harm -- may indeed have meant to do good by encouraging efforts to increase women's participation in the Haskell community [...]*" In fact Tim's whole point is that *even assuming that Doaitse didn't mean to make a sexist remark*, it was still somewhere between unhelpful and damaging.
Assuming you've read the comments in this thread, and the number of rhetorical contortions people are making to ensure that they don't have to stray past their pre-existing comfort zones, I'm surprised you can take issue with a statement like "oh boy do guys always want to have opinions about gender issues". Of course, "Suggesting that all men are X" is itself a classic "here is a picture of a cat; your argument is invalid" trope for not engaging.
For what it's worth, what caused *me* difficulty at first was simply the *default currying itself*, specifically the lack of an obvious visual marker to distinguish the return type from the arguments, other than "it's at the end". Combined with polymorphism allowing a "return type" to unify with a function type and change the apparent arity, this confused me to no end for at least a week or two until I got used to it.
&gt; This is Haskell and it is an equal-opportunities mindfuck of a language. Well said!
You can, of course, supply RULE pragmas telling GHC to perform a given optimization which the programmer knows to be a Good Idea.
Well, nothing. But just because a word can have a certain meaning doesn't mean that it's appropriate for that meaning in all contexts. If he hadn't just been talking about women, then 'attractive' would be a more appropriate word to use (i.e., we need to make the haskell build toolchain more attractive).
As someone else who was there (not that makes me an authority) your first instinct was not how it came across for me. I saw it as inappropriate but he appeared to mean well. Here is my take. During an impromptu QA by an outsider who's words and presence were starting to cause conflict, Swierstra made probably the most universally applauded comment: the Haskell community should recruit more women. So with the audience on his side, he took a stab at joke he thought would go over well with the almost entirely male audience. Which turned out to be ironic in an unintended way. 
Hm. As camccann said, it depends on the crowd. Most of the men I've been around for an extended period of time think that women just "make a big deal out of it". Many times, I don't even bring this up, because everyone just gets awkward since "oh no... Gigliorosa is blowing it out of proportion", when it's a totally appropriate comment to make. If I were a male making the exact same statement, their reaction would be completely different. Yet another anecdote, I play D&amp;D. Things that the other players (all men) say, in character/out of character may seem harmless to all of them but are pretty freaking sexist. Countless times I've heard, "What's her charisma score? Is she hot? That bitch doesn't know anything. I'm going to carouse in the tavern and get bitches." They single out the women, too. When one guy says "I cast fireball in this area to target these guys.", the DM's like "Okay that's cool". When a woman says it, it's "Well this only covers so much of an area. Which direction are you pointing? Your mini isn't facing that direction, so it hits the wall. That was dumb, your attack fails." And if you argue the point, your seen as making a big deal out of it. Also, patronizing voices are a *big* thing. "Don't make such a big deal out of it. It's not an issue. It's okay... You didn't hurt anything." All in that "comforting" tone of voice that you use for a young child. D:&lt;
&gt; And the moral of this story is: Speak up! The sooner, the better! Agreed! I waited a summer to bring this up with one of the professors in the department. They had no idea that something like that had even happened, but couldn't do anything so late other than keep an eye on the guy. I spent several semesters just finding ways to avoid him so I wouldn't allow myself to be in situations alone with him. &gt; End result is an appearance of uniform opinion... Yep. That pretty much sums it up. Men don't experience what women do, because other men tend not to comment around men. Riding a bike in the summer, for instance: Alone - "Damn, girl. Look at that ass. Nice cleavage, keep up the good work!"; With a male friend - "I like your bikes!" So men never have the opportunity to experience what women do since they're not around when it happens. Therefore, "Well I've never seen it... so you must be exaggerating."
I wholeheartedly agree. &gt; "You left the door unlocked, you obviously wanted your shit to get stolen" is rarely used to justify the crime, compared to "You were showing off your goods, you were asking for it" I find neither of these to be satisfactory justification of the crime. While both exhibit an instance where preventative action *might* have averted the crime, in both cases the criminal is still fully to blame for the misdeed. While I advocate that people (especially women) take control of their own fate as much as possible, if someone ever said something like "You were showing off your goods, you were asking for it" to a rape victim in my presense, I would *very* clearly let them know that they are way out of line.
&gt; And to a significant extent, what he meant doesn't really matter if many (and likely most) people interpret it according to the first definition in the context. I think that it matters because it changes the nature of the response. The feedback you give to and the response you expect from someone who made a sexist remark in a talk is different from someone who made an intrinsically non-sexist remark that was accidentally interpreted as a sexist remark due to a (retrospectively) poor choice of wording. This matters because if you beat too much on someone who did everything basically right but just didn't realize that people would misinterpret a word in his speech then eventually you'll only convince that person that they will get jumped on for something no matter what so there is no point in trying to address gender issues at all.
So, I think that the discussion about this is weird because seems to be a false dichotomy: making Doaitse immune to any criticism because he was just misunderstood and criticizing him for making sexist remarks regardless of his intent. I can understand why people don't want the first option because many people were apparently offended, and I can understand why many people don't want the second option because it seems unfair to lay so much on Doaitse if he had just made a word choice that led to misinterpretations. So how about a third option: we acknowledge that the remarks can be read as a sexist way *but* since they can just as easily be read in a non-sexist way we presume that the author meant the latter (given that we have no reason to believe otherwise) and suggest to him that he be more careful about using the word "attractive" in this context it triggers people to read it in a particular way due to their unfortunate experiences of sexism. This way we are making progress towards fixing the problem while not beating unreasonably on the author.
&gt; Agreed! I waited a summer to bring this up with one of the professors in the department. They had no idea that something like that had even happened, but couldn't do anything so late other than keep an eye on the guy. I spent several semesters just finding ways to avoid him so I wouldn't allow myself to be in situations alone with him. True enough, but it also shouldn't fall solely to the target of such nonsense to be responsible for taking action. :T Ostensibly uninvolved bystanders implicitly condone such behavior by their silence--just a dirty look and a "dude, what the fuck"-type comment goes a long way to changing the atmosphere and perception of what's acceptable, and that's really what I was trying to encourage. Sudden disapproval from third-party observers is a *powerful* social force, far moreso than any amount of protest from the victim, I'm afraid. I'm not surprised the professor had no idea about the guy's behavior, either. People who act like that are often very good at not doing so in front of authority figures. &gt; Men don't experience what women do, because other men tend not to comment around men. Yes, well... I have several younger sisters. Suffice it to say that I'm well acquainted with being a walking harassment shield.
Well, we tend to hold our programs and our language to a higher standard than most other languages. Is it so surprising that many of us would hold the community itself to a higher standard, as well? :]
&gt; I'm sure that women have already been educated about such things from a far more enlightening source than myself. Yes, it's the school of "make it from one end of the month to the other without being assaulted, intimidated, or made to feel uncomfortable simply for who you are". Be glad you didn't have to go to that school. If you're interested, it's really easy to learn by reading about what it means to not be a well-off white male in a place run by, and for the benefit of, well-off white males. I think there are few better things you could do with your time. It'll really make you uncomfortable, be warned.
I've thought about it, and I think eventually I will do it, but I don't want to rush it or force it. If I accumulate enough of these kinds of posts then I will probably polish them, focus a bit more on correctness and collect them into a book. However, it will take time, though, since I don't think my own understanding of a lot of category theory concepts has completely matured yet.
Yeah. I think a lot of the disappointment isn't about what Doaitse said, but about how the community has reacted to it. Everyone can make foot-in-mouth or ignorant comments about matters relating to privilege, especially if they're spontaneous. What's telling is what you do when called out on them. It might even be natural for Doaitse to attempt to argue against a characterization of his statement as sexist, because people don't like to admit to things like that, and I'm sure he didn't intend it that way. Intent is a whole other can of worms though... More disturbing, to me at least, is that people with no "ego stake" in the matter (i.e., not Doaitse, who at least to some degree might have an understandable desire not to admit fault) are stepping up and attempting to tell people to lighten up about what he said. What is X's motivation when he steps up in public to tell Y to lighten up about what Z said? But on that matter, what's my motivation to tell X he's wrong? This shit is too complicated.
&gt; And as for "you'll only convince that person that they will get jumped on", I should bloody well hope for precisely that outcome. Good, then that is one person you have successfully turned off from possibly wanting to do anything about sexism. Well done. I hope you are proud of yourself for that. Keep this up and soon you will successfully kill any chance of anything being done by anybody, as it will be well established that trying to do anything about the problem will just get you jumped on for *something* so you might as well not even bother.
Yeah, I already got it from the other, less passive-aggressive responses.
A good way to not have people assume you're creepy is to do the work it takes to not sound creepy. It sounds here like you want to shift that work onto other people, so that they have to do the work of divining your true nature, rather than you doing the work of writing clearly. That's hella creepy.
It was a joke but it was also true and insightful. Who better to encourage women to become involved in FP than actual FP programmers and researchers? If not us, then who? And everyone would benefit. Let's not let the way he said it ~~distract us from~~ lead us to forget his good and important and much needed suggestion. [edited]
I can only speak for myself, but my motivation was when the blogger wrote he thought it was too 101 to crosspost to another blog, and I thought, no. You, Mr. blogger, are a little out of touch. It is not an open and shut matter at all. So I took the opposite side on Twitter. He blocked me within the space of a few tweets, incidentally. And called me a jackass. I think he was a little oversensitive.
&gt; Yes, it does seem like the reaction of the audence after he said "attractive" seemed like some in the audience it took it as a joke to perhaps imply the latter If he had actually meant "attractive to [both men and] women", he could have responded to the laughter by saying, "I'm not sure why people are laughing. I meant 'attractive to both men and women', if you think we should have more women so you can look at them, you should be ashamed". But he didn't say that.
I quite agree that people taking offense is generally bad, whatever our intention originally was. But taking *anyone's* offense seriously, disregarding the reason for their offense (which in this case is borderline hysterical imho), is not good either.
There were plenty of people actually in the room; their understanding is not affected by the recording quality. Based on the laudable prefix of his remarks, I'm confident that it's a real possibility that he could produce a serious apology that would save his reputation and advance the community as well. (I'd like to think cognitive whiplash from the sudden pivot from saying something completely laudable is why the audience didn't respond appropriately.)
Darn, even my honest attempt at devising a neutral way to address the issue is open to malinterpretation. (not that you did, tough. You're entirely correct that the matter of one's personal gender is subjective too)
~~Why are men standing up for hypothetical women so sensitive?~~ **EDIT**: [I'm an idiot](http://www.greenrd.org/2012/09/women-computing-apology/).
&gt; I wish that stupid guys would stop being stupid too You know what I wish? I wish you hadn't just dismissed *raping people* as just being "stupid".
Yes, it tells me which orange is tastiest, but not if it's tasty enough that I should bother getting it instead of the apple. 
I don't know, you'd have to ask one.
It's hard for me to understand why people read my post as being an attack on Doaitse unless they are hyper-inclined to empathize with people in a position of power and disinclined to empathize with marginalized people. It's also hard for me to understand why people who are preoccupied with intent don't care about my intent (I didn't intend to "criticize" Doaitse as a person or to "lay so much on Doaitse", but I suppose that doesn't matter to you, because I'm not someone whose intent is likely to count.)
&gt; If he had actually meant "attractive to [both men and] women" This would have been funny and not discriminatory.
&gt; You know who you are and you can rightly fuck off. We keep our community to a higher level with regards to sexism; let's do the same with regards to discussion. No need to get offensive.
But the much bigger problem is the lack of women in CS, and obsessing and being oversensitive about offhand remarks like this is unlikely to be helpful in drawing women into CS. In my personal opinion. If 100 women showed up to this thread right now and disagreed with me I might change my mind, but to the guys: your opinion on this does not matter.
no, the gender ratios in industry are low too. we should aim higher.
i find the use of that four letter word in this context highly inappropriate and I demand you folllow this speech code I've just dreamed up on the spur of the moment.
so, are women bad at magic?
&gt; While you're entitled to your opinion, I reserve the right to politely disagree. No, you're actually reserving the right to rudely and obnoxiously disagree: &gt; which in this case is borderline hysterical imho 
&gt; In my personal opinion. ... to the guys: your opinion on this does not matter. Please take your own advice.
A programming book? People still buy those? Or you just mean a reall big website organised like a book? Heh.
&gt; I didn't intend to "criticize" Doaitse as a person or to "lay so much on Doaitse", but I suppose that doesn't matter to you, because I'm not someone whose intent is likely to count. Where exactly did I say in my post that you did *either* of those things? In fact, where in my post did I refer to you at all? You really should not be calling me a hypocrite when you clearly aren't bothering to read what I actually wrote but rather some kind of caricaturize of it. But since you have brought us to this subject, assuming that you are the author of the OP as you seem to be acting like it, let me clarify what I think of your article directly since there seems to be a misunderstanding on this. First, let me say that your primary intention was clearly to point out behavior that drives women away from technical communities and in particular it *not* to assassinate Doaitse's character. For this reason, you will note that in none of my posts have I accused you of such things. What I do think, however, is that you were nonetheless unfair to him by criticizing him for making an inherently sexist remark when in fact he did no such thing. It was almost as if he said "Haskell is a very unhappy community so we really need to draw in more *gay* people!" In this context the word "gay" clearly means "happy" rather than "homosexual", so it is unfair to criticize the author of making an genderest comment merely because some people read the wrong meaning for the word "gay"; it would be fair, however, to criticize the author for having made a confusing word choice. Mind you, I do agree that there exists another kind of remark which is intrinsically offensive *even though the speaker ment no offense*, it's just that there is a big difference to me between a remark which caused offense because its actual meaning was misunderstood and a remark which caused offense because it was intrinsically offensive when fully understood *despite* the fact that no offense had been intended. So in short, I think that the criticism that you make in your article missed the mark and was of a much harsher nature than it needed to be because of that. And I am not saying this because I empathize with people in power more than marginalized people; frankly your post is the *first* post on the subject of sexism at technical conferences where I have had more sympathy for the subject of criticism then for the receivers of the offense. I hope that in the future when you ask other people to understand another's perspective (which I agree is an important and laudable goal, especially in the context of battling the sexism on a small scale that adds up to sexism on a big scale) that you will do the same yourself, removing the plank from your own eye before telling the rest of us how to remove the plank from ours.
Why do you think everything is about you?
Can't we all just get along?
Is *attractive* really that unambiguous in this context (I haven't watched the video)? Okay, people were laughing, so it was probably perceived as slightly out of line by the audience (but not so much that someone stood up and complained). The strange thing about programming (or IT in general, or science) is that it's often not a welcoming environment (which is a problem, although right now, probably more men suffer from the frat boy atmosphere than women), but there is little hard, direct discrimination. If a woman wants to join the IT workforce, she has access to training in various forms, and her gender is unlikely to make a difference in hiring decisions (or if it does, it's in her favor). Compare this to becoming a professional musician: even today, there are reputable organizations in the Western world which, as a general rule, simply do not hire female musicians (except as harpists).
It is entirely plausible. He wasn't suggesting they recruit "bimbos" as PhD students; he said it would make the meetings "more attractive". Now he could have simply meant that it would make the meetings more interesting and engaging for both genders, or - what most of these discussions are centered around - he meant that having women at the meetings would make them more aesthetically attractive (i.e. "Now we have a bunch of pretty girls to look at while we work on code). Without getting a statement from the man himself as to his intentions, it's really a moot point to continue arguing his meaning. The point of all of this is that people need to be made aware of the frequency of demeaning statements toward women in these fields. There are plenty of others who make very sexist comments who are oblivious as to how much it offends women.
Oh I'm well aware the target shouldn't be the only one taking action; the main reason I waited so long was that when I told some other peers of mine, they didn't believe me. They were convinced it was "just an accident" and I was being oversensitive. I insisted that I checked by asking the guy "dude, what the hell was that?" and it took several months to convince anybody that it had actually happened. Unfortunately the guy was involved in a lot of extracurriculars I was in, so I had to find ways to stay away from him as he kept walking directly behind me when we'd all go somewhere. So that's another aspect... "It must have just been an accident. You're overreacting." Accident or not, if someone says they've experienced a situation that makes them uncomfortable around an individual, you should respect their feelings and either try to understand what happened or at least try to help them be more comfortable. I worked in a lab at a different university last summer and had to sit in the same room as a guy who was known notoriously as a pervert. Some of the other women pulled me to the side and told me he had grabbed their chests, believed women who were raped deserved it, and generally invaded much of their space. None of them wanted to report him to the department or his faculty advisor because they thought, "Well maybe it only happened to me. I don't want to be seen as a bitch for reporting it without proof." So this asshole was just frolicking around the department, making everybody uncomfortable, and nobody would do anything about it because they would then be embroiled in a lot of arguments as to the validity of their claims.
Wow, that's really useful advice. I might try this out. I really like the ability to update it, too. However, I think I will still submit most posts to /r/haskell first to get feedback as a precaution before I publish them since you guys give great commentary on how to improve them. I might also still wait until I have written more posts, though, because I can only think of about 6-ish posts I've written that I'd like to include in such a book.
A swift response to things that, compared to other communities, may be relatively minor, will serve us in good stead.
There are no links; just first-hand experience. Here's a case study, off the top of my head. You want to implement free monads (forget about the category stuff; it's just a stand in for a parameterized data type). In Haskell this looks like: data Free f a = Pure a | Roll (f (Free f a)) instance Functor f =&gt; Monad (Free f) where ... Now, the first thing you'll run into is that it is vitally important to put as many variance annotations as possible on your types in Scala, because type inference doesn't work if you don't have variance (and sometimes not even then). So our stuff looks like: trait Free[+F[+_], +A] { def flatMap[G[+X] &gt;: F[X], B](f: A =&gt; Free[G, B]): Free[G,B] ... } except, that's wrong, because we have no evidence that `F` is functorial there. So, how do we add that? Well, I know of two ways. As an aside, you won't find your solution in the standard libraries. Every functor will have a map method that properly returns the same type, but you can't abstract over that fact in the standard library. They do have an abstraction for, 'things with a map method,' but it is not functor; the generalized map is actually based on iterating through something and building up something else with a `CanBuildFrom` and the transformed elements. You can also abstract over the implementation details of your list/sequence type, but not over functors. Anyhow, one way to pass the evidence is the way Haskell does it: type classes. Scala has a similar mechanism, so we can write: trait Functor[+F[+_]] { def map[A,B](f: A =&gt; B)(v: F[A]): F[B] } Oops, that signature is a bad idea. Scala's type inference is left-to-right biased. So if you tried to write: map(x =&gt; ...)(v) where `v` had known type `F[A]`, scala would not be able to figure out that `x : A` without your annotating it. It also can't figure it out if you write: def map[A, B](v: F[A], f: A =&gt; B) because that propagation only works between tupled sets of arguments, not within them. So the best way is: def map[A, B](v: F[A])(f: A =&gt; B) We have lots of code that chooses a strange (to us) argument order for our functions because of this left-to-right behavior. Anyhow, then the signature of `flatMap` becomes: flatMap[G[+X] &gt;: F[X], B](f: A =&gt; Free[G, B])(implicit Functor[F]): Free[G, B] This works okay, except you'll find that it sucks to make `Functor[F]`s for a lot of choices of `F`. In Haskell, every type is curried, but in Scala, such currying is impossible. So for instance `Free` should be functorial for functorial `F`, so we write: implicit def freeFunctor[F[+_]](implicit Functor[F]): Functor[({ type T[+A] = Free[F, A] })#T] = new Functor[({ type T[+A] = Free[F, A] })#T] { ... } Where that `({ ... })#T` is building an implicit class and projecting out the type; it's a type lambda with terrible syntax (and there is no better syntax). This works for this part, but it's awful to have to type all the time. The other problem is that there are a lot of types you might want that have more than one parameter, like: Free[F, A] Either[E, A] State[S, A] .... And Scala's inference isn't so good about figuring out what needs to be partially applied in these examples, so it's easy to get into situations where you have to write those type lambdas all over the place. There is a library (scalaz) which has some constructions I don't understand for helping Scala figure this out better through various intermediaries, but it's not very obvious. Note: the reason Scala isn't good at this is because no argument is really privileged over any other, and you can partially apply any of them with the lambda technique I showed above. That's a very hard (impossible) problem to solve. But it is known to be hard, and it is a failure in itself to choose to tackle that problem when one could do something else (i.e. what Haskell does) that would at least be useful in a lot of cases. So, I recently went looking for another way to abstract over functors. Some folks like to say that Scala isn't Haskell so you shouldn't use Haskell solutions; you should play to Scala's strengths. So, the above stuff is pretty Haskelly, and maybe an OO solution would look better. I believe the OO solution is to make a trait of functors. It should look something like this: trait Functorial[+F[+_], +X] { def map[Y](f: X =&gt; Y): F[Y] } Now our signatures look like: trait Free[+F[+X] &lt;: Functorial[F, X], +A] extends Functorial[..., A] { def flatMap[G[+X] &gt;: F[X] &lt;: Functorial[G, X], B](f: A =&gt; Free[G,B]): Free[G, B] } I elided the partial application of `Free` in its own implementing of `Functorial`; we still need that. So, that looks awful, but it works so far. Then you go to write one of the constructors: case class Roll[+F[+X] &lt;: Functorial[F, X], +A](r: F[Free[F,A]]) extends Free[F, A] { ... } And this [doesn't work](https://issues.scala-lang.org/browse/SI-4758). It's a compiler bug. I know it's marked as fixed (there at least), but it was marked fixed nine months ago, and I encountered it two weeks ago. So I guess you can't use case classes (algebraic data types) for this; you have to implement them by hand because the compiler blows up on this example. So, there's a sampling of some of the problems you'll run into working on something simple. It's 10 lines of code, maybe, in Haskell. Nothing advanced. And I haven't even mentioned that using the above in Scala will be prone to stack overflows, so you need to add another constructor: case class FlatMap[+F[+_], A, +B](f: Free[F, A], g: A =&gt; Free[F, B]) extends Free[F, B] And then use an explicit stack to shift pressure over to the heap. This is due to a lack of tail call optimization. There's also other problems in other examples I could mention, but I won't get into it.
Rather than attempt to explain why I had legitimate need to access the database, etc (I don't think anyone is interested in listening to me ramble on justifying why I am not creepy), I just deleted some of the details from that paragraph (including the first 3 quotes bos pointed out). Better? :)
Glad to know that people in the Haskell community are willing and eager* to help me out in this way. *no pun intended
I'm not following. What is this adjunction you mention? Since the naturals are a free monoid on 1 element, there is a monoid isomorphism Nat =~ List (), and under this isomorphism, `length` is just `fmap (const ()) :: List a -&gt; List ()`. Recasting everything in terms of categories, monoids become categories, and homomorphisms become functors. I don't see why it's a bad example. Trivial, maybe, but correct, and it shows how very familiar concepts can also be described as functors. &gt; And if you don't deal with objects, then how can you even say "a category with a single object!?" etc. Ah, you certainly have to deal with objects in some way or another. I agree with you there.
What is an "inherently sexist remark"? I'm guessing you think it's a remark that a heterosexual cis man thinks is sexist? If so, why?
Why do you think you're entitled to my attention?
I'm fine with telling sexists to fuck off. They *should* fuck off.
&gt; However, no one controls your fate more than you do Another brief point that I meant to cover is that this is a very neat and elegant worldview to hold, but isn't really true. Prejudiced hiring managers, policemen, and so on, hold a lot of power over everyone. A lot of it can certainly be overcome, but what that means is that any minority you're likely to interact with in a familiar environment has likely put a hell of a lot more work into getting there than you have. The reward for getting there will likely be to be seen as an example/paragon for the entire minority, where any shortcoming automatically applies to the entire minority. "Nuh uh, we aren't sexist! Look, we have Mary working right over there. And one of my best friends is gay, so I'm totally not a homophobe!" or "women are just so bad at indenting their code!!"
I'm wondering what you do when your code doesn't compile, since there's no way to argue with the Haskell typechecker to convince it that you really wrote well-typed code.
thank you for the detailed explanation!
I'll stick to offending scientologists next time then. Oh, I almost forgot - racism is completely unrelated to sexism now?
It seems to me that your comment isn't very constructive. Could you tell me more precisely what I'm doing wrong?
I fix it, because the Haskell compiler is able to tell me what I did wrong, but please, I would like to keep this civil, so let's stop using ad hominem "arguments". This isn't SRS.
You took the words out of my mouth.
I can't second this advice enough. Thanks for putting it so well.
&gt; because the Haskell compiler is able to tell me what I did wrong Clearly we aren't using the same compiler.
&gt; It is my opinion that we should encourage the oppressed to take as much control over their situation as possible, because I believe that no one can effect a person's fate more than that very person. I believe the overarching point being made by bos and others is that there really is no need to say this, and in fact it is quite insulting. &gt; Or they can go out of their way to give the oppressed a hand. While the third option sounds like the best one, I feel that such is not universally the case; sometimes handling the oppressed with kiddie gloves works against the cultural shift towards equality. Considering these people spend nearly their whole lives being treated unfairly, even by *allies*, I don't consider it a burden to do my best to help create spaces wear they feel safe and welcome. If I would like them to do everything in their power to change society, and I (a member of society), am not ready to meet their effort, I might as well say "I'm not going to stop leaning no this rock. That would be to inconveniencing for me. You just need to try pushing harder on your side so all these other people who are leaning over here don't crush you against all those spikes. What's the problem, you aren't pushing hard enough?" I think it is unfortunate to make this single comment by Daoitse out to be the crux of Tim's post. I think it is fairer to say that it was a straw on the camels back that spawned a howl of pain, and focusing on how light that one straw was isn't doing anybody any good. EDIT: OK so it is not actually correct to say that Daoitse's comment is not the crux of Tim's post. I've derailed myself thinking too much about some comments in these threads and had to go back and re-read the post again. Mia culpa. 
You'd think Haskell programmers would be more open to understanding other real people when they speak up about their concerns. After all they are willing to put up with the recursive riddles raised by the unthinking, unfeeling automaton that is GHC.
Why is something so seemingly irrelevant so important to you?
GHC is saying that it doesn't know how to treat string-functions as numbers.
Please learn what "ad hominem" means. Also, "I didn't do anything wrong" is a funny way to say "would you please mind explaining to me in more detail what I did wrong?"
&gt; Could we, please, at least give people time to understand why something is problematic and a chance to improve first? I've been doing this for about the past 21 years.
No, because it's your responsibility to educate yourself.
&gt; This whole thing has escalated in a really unpleasant way I feel like this is not the clearest way to put it. The "escalation" happened a long time ago, when Western society started to be a patriarchy. I think what happened was that some men got upset about having a social order in which they get to dominate women questioned, and chose to take out their anger, resentment, and insecurity on everyone available. 
If it's my own responsibility to find information, then what's the point in communicating?
The work required to switch everybody over to lisk syntax is a bigger effort than coming up with a working haskell syntax indenter. Don't you think so?
You posted this comment a few minutes ago, and at that point over 180 comments on this matter had been exchanged. Did you read them before posting your comment? I think there are plenty of good explanations about what the issues are on this thread.
Yes, same here. As a woman in IT, I hate the fact that I can hardly even read these kinds of articles or comment threads because the vitriol I tend to see in response is so discouraging. It seems that much like it is in many other ways, Haskell and its community are setting themselves apart here. And that makes me happy :)
I haven't read them all, but those I've read are either interpretations or a public bashing festival.
Then you aren't asking that question in good faith, especially since you already seem to have an opinion on the matter, as stated in your second sentence. 
One last comment. Why is it OK for Tim to broadcast publicly on Twitter that he is attracted, sexually or non-sexually, to many of the people in the Haskell community - and this point is crucial - _a community which includes some women_ - but not OK for the professor he is criticising to imply that unspecified males would find unspecified females attractive? It seems like a double standard to me. Help me understand. Because I would never talk about my work colleagues that way in public. That would be way over the line for me.
By all means take comfort in the fact that the people you've driven away are the ones who are in the wrong rather than yourself if you wish, but the fact is still that you've driven away potential allies for your cause and that is probably not what you want to do.
Spot the difference: 1. I say that I find a community appealing because it shares many of the values I appreciate. 2. Someone says that women should join the community because they'd make it more attractive, and everyone applauds. If you can't spot the difference, it's that in one case, I have stated a preference of mine. In the other case, that someone has simultaneously suggested that women's attractiveness (to whom?) is relevant when discussing this, which implicitly carries the idea (to some degree or another) that women are there to please the men. Think of a large male-only mining colony way back in the day, and the kinds of things people might have said when discussing bringing women out to keep the men company: "having women here [to keep us company] would make this community significantly more attractive, *wink*". Do you see what I mean? As another example, in a workplace, let's say you have a female colleague, and you do peer evaluations. Would you ever consider saying "yeah, she's really good at low-level hacking, and she's pretty hot too!" in that review? Whether she is hot or not (even if it's a compliment!) not relevant or professional to bring it up, and carries the subtext that the woman is somehow there to please your eyes or senses in some way or another. Even if it's true that as a straight male you find her attractive, that does not make it relevant, and the utterance carries many connotations. What exacerbated this whole thing, as Tim has pointed out many times both on twitter and in the blog post, was the response from the audience, who did not call Doaitse out on the remark. Although there is friction due to the extreme awkwardness that would come from doing that, it would have certainly helped significantly.
Okay, I see your point that the context here does not disambiguate the meaning of the remark, even if we have agreed that we are using definition 2 of attractive, though I still ultimately view this as a communication problem rather than as a sexism problem.
&gt; A programming book? People still buy those? Yep. Paper doesn't need electricity and is the support which offers the most flexibility. (And e-books are pretty neat too) And having a compendium explaining the state of the art is much nicer than having to crawn from link to link to blog post to paper to etc.
&gt; it would be nice to have people read my ideas and respond to them intelligently That does sound nice. Fortunately, that appears to be what most people responding to you have done. I'm honestly not sure what sort of timid criticism you'd consider acceptable. At the moment, I'm not convinced that such a thing is possible.
I too believe that the functional code is easier to parallelize, but if the fastest functional parallel code is slower than sequential C then it doesn't help. I'm not saying that they should have written parallel C code; I just want a calibration point. 
I'm only aware of one ghc transformation that relies on parametricity and that's foldr/build. (Also the zonking in the typechecker.)
True, but my concern is that we at least want people to be sympathetic to the cause rather than apathetic to it or worse be against it. There is a disturbingly large number of people who consider the whole program of changing social norms regarding sex and gender to be nothing more than a "political correctness brigade" that feeds peoples' need to feel self-righteous while not actually accomplishing anything good, and the last thing that you want to do is to provide fodder for them. P.S.: Thank you, your reply was reasonable and responded to the point that I made rather than a caricature of it, which has not been my general experience here.
The only way to never offend anyone is by never saying anything.
&gt; There is a disturbingly large number of people who consider the whole program of changing social norms regarding sex and gender to be nothing more than a "political correctness brigade" that feeds peoples' need to feel self-righteous while not actually accomplishing anything good, and the last thing that you want to do is to provide fodder for them. The key point here is that *any attempt to push for change will provide that fodder*. Seriously. Which isn't to say that everyone promoting a social cause does so in the best, most effective way possible, but being excessively accommodating to the opposition cripples a cause to very little gain. This point can be particularly hard to get across to programmer-ish people, who tend to be far more honest about their reactions. In society at large, most people who complain about "political correctness brigade" are neither voicing their true objections nor participating in good faith in any sort of discussion. It's a code phrase for "stop pushing me out of my comfort zone", not "please be friendlier". I'm happy to give you the benefit of the doubt for your own intentions, but realize that a lot of the irritation directed at you is colored by the above reality.
&gt; I'm honestly not sure what sort of timid criticism you'd consider acceptable. Did you read the comment to which I responded? I pointed out exactly an example of this. I was criticized for two things. First, for turning away from "the cause" so easily, and second, for lying about this since the author believed that I hadn't actually left "the cause" but was claiming I had done so anyway for rhetorical reasons. The problem is that I never claimed that I personally was leaving "the cause" and in fact I think that it is important which is partly why I feel frustrated that the people in it are being so unreasonable by responding to invented versions of my posts. I mean, didn't you notice or care about that? Do you really see no problem at all with criticizing someone for something that he or she never did? Incidentally, if you want an example of criticism that I thought was reasonable, see http://www.reddit.com/r/haskell/comments/zxmzv/how_to_exclude_women_from_your_technical/c694ldr. This comment responded to what I actually said and only to what I said, and it focused on the intellectual content of the post rather than elevating it to a personal criticism.
Yes, but we do have standards for what is acceptable. For example, consider [this comment](http://www.reddit.com/r/haskell/comments/zrm9z/yet_another_haskell_ide_in_the_works/c676uaq?context=3) by Bryan where he puts some commenter in their place. Although the post is deleted, I'm pretty sure that it was offensive by many people's standards. Bryan spoke up and most people agreed with him because he voiced the standards for our community. Now, consider what kind of message it sends when a woman speaks up at something she believes is offensive in a manner similar to Bryan, but the community doesn't agree with her. It suggests that she really doesn't belong to that community and that she should go somewhere else where she can find people that agree with her sentiments. So if you want to make women feel truly a part of our community, you have to compromise a bit on your existing culture and be willing to embrace their culture, too, otherwise it's a completely one-sided relationship. That means growing to slowly accept their standards for what is offensive, too, and perhaps finding a compromise somewhere in between. A community that is not willing to change its own standards to embrace change is a community that doesn't really have a future. It's not enough to say that everything is potentially offensive, because I'm sure nobody would have used that defense for that anonymous commenter that Bryan called out. We clearly do have standards for acceptable behavior, but now we're learning that perhaps those standards weren't as high as they needed to be.
While I may be somewhat more charitable than godofpumpkins in assuming good faith, I have to say that every time I've seen this discussion (and it consistently plays out exactly the same way every time) the people who take the position you're arguing almost always turn out to be disingenuous about their objections in pretty much the way godofpumpkins is suggesting you are. The only difference between my response (that is, the comment you thought was reasonable) and the others is that I didn't outright conclude that you would follow the well-established pattern I'd seen before, even though (empirically speaking) it's not an unreasonable conclusion. People who aren't used to dealing with logically-minded folks like programmers are unlikely to be even as generous as godofpumpkins has been. The well has been thoroughly poisoned for raising even reasonable objections of that sort, and there's not much you or I can do about it.
Consider any recursive function where you're building up a list from the front/head to the end/tail. Hence, you have an accumulator and you're constantly appending things to the end of it. Assuming the core of the function itself is trivial: using a plain list for the accumulator makes the algorithm `O(n^2)`, whereas using difference lists makes the algorithm `O(n)`--- which is about as good as you can get since you need to enumerate the `n` elements of the list eventually. The canonical example of this is when defining instances for `Show` (since `String ~ [Char]`).
I did get the information from actual sister missionaries. The conversation went something like this (I was actually a bystander): I was present when a male presented the hypothesis that sister missionaries dressed to avoid unwanted attention from men (that was a very awkward moment, thankfully the sisters didn't take offense); the sisters agreed but clarified that they were more interested in filtering out people that only wanted to talk to them because of their looks. This may not be universally characteristic of sister missionaries, or even of sisters in my mission, but I believe that was the consensus among 4 particular sisters.
Without dwelling on it, the isomorphism isn't just a monoid isomorphism, because you can e.g. multiply nats, and similarly "multiply" lists in various ways, etc. The full identification between Nat and List () means that lots more than just a single monoidal operation translates! So then to look at it just through the lens of that operation is weird. The adjunction is basically what you've spelled out -- the forgetful functor from List a to Nat and the adjoint functor from Nat to List a which can be constructed for any a where you can pick at least one element. This is in fact a galois connection, even.
I see what you're trying to explain, but bear in mind that my point wasn't about how one "can" present CT in the abstract, it's that this particular presentation doesn't use either a standard treatment of objects or a proper object-free treatment, and so runs into problems. You could write an article that does what you describe, but it just also wouldn't be this article.
&gt; It is certainly nice, I must rather dryly note, to have the option of simply avoiding communities whose culture makes one uncomfortable. Not really. I mean, even setting aside the marginalized groups making your note so dry, what about those who feel a moral imperative to confront injustice and to make the world a better place? And I do mean "imperative" rather in the sense of Kant, not in the sense of mere indignation. If one is to have an impact then she cannot always choose to avoid such toxicity.
Er, yes, of course. But I'm sure you realize that any sort of deontological compulsion is of a rather different character than what I was alluding to with the remark about having the option. 
&gt; it saddens me that saying something so critically important and true would be perceived as insulting. Are you familiar with the word "patronizing"? Contrary to it's phonemic content, which often leads people to believe it is rooted from the word "patron", in actuality it comes from the Latin *pater*. Cognate with "paternal", it conveys a sense of condescending authority. In short, you are infantelizing the target of your words. When you are imparting what to you seems such sage wisdom, in fact you are alerting someone who is by definition an expert about something you have absolutely no experience in. So, no, it's not really unfortunate at all, because your intent is misguided, your effect a net negative, and your goal already attained. That is *they already know*. I apologize for being a little patronizing myself in this comment. I have tried to be very civil with you, and appreciate your effort to grow in this whole experience, but considering how many times this theme has been repeated to you, I saw no other tac. Perhaps it seems critically important and true to you, but to everyone else it is just obvious.
Which means he left the *university* over the university. If circumstances improve, I'd like to think that maybe the _Haskell_ community will get Tim back some day :-)
&gt; Then don't say things that cause people to associate you with [rapists]. You missed both my point and efrey's point so completely it is astounding. What part of "women please protect yourselves" says "I am a rapist"? &gt; You do not get to dictate what other people think about you. I was referring to what was *said*; think whatever you like. &gt; once your communication has failed, you need to step back and consider the fact that any attempt to repair that communication is going to be filtered through our original interpretation of your first words. I would advise against clinging to first impressions. I'm tempted to elaborate but I think I'll just leave it at that. &gt; You need to look inside and reinterpret your opinions before anyone else's going to consider recalibrating theirs. You apparently dislike something that I originally said, but you have not indicated what that is. (Or, you're just trolling me.) Please expound on that point specifically; what part of my opinions should I "look inside and reinterpret"?
Thanks, and that is a really helpful answer. I will be more careful and explain my terms upfront in the future and whenever possible emphasize when I am referring to objects or morphisms.
&gt; a good opportunity to insert the tangential comments of "if you are a woman, please take precautions" Do you have any idea how many times a day women hear that comment? Especially the women who take an active role in combating sexism? Being constantly inundated with these "helpful reminders" only reminds that we really don't get any control over our bodies and our lives. If anything, it only serves to reinforce rape culture by ensuring that women are constantly on edge. Oh, and for bonus points, why don't you turn that edge around and get upset when women don't give you the benefit of the doubt? Making tangential remarks in a discussion of this sort is, by definition, derailing. Yes, at critical junctures you're free to propose changing the topic of any discussion; but the key words here are "critical juncture". When the participants are committed to and engaged with a particular topic, there are rails. When you try to force the conversation to a different topic ---whether because of disinterest, or because the current topic makes you uncomfortable--- even the folks who are just along for the ride are going to get pissed at you.
You have no need to apologize. I have taken a lot of flac in this thread and most of it is probably well deserved. I may later disagree with things I said in this thread, but I cannot apologize for saying them. I needed to work through and discover my opinions about some of these things, and the opportunity to talk it out with people just doesn't come around very often.
&gt; Do you have any idea how many times a day women hear that comment? This I honestly *was* ignorant of. Many people in this thread have given me good food for thought in this regard. &gt; Oh, and for bonus points, why don't you turn that edge around and get upset when women don't give you the benefit of the doubt? I have no idea what this is referring to; I am only aware of one particular respondent to my comments that identified herself as a woman, and there was no "getting upset" between either of us. I am assuming that this is specifically referring to me and something I said. Can you please point out what it was and what is wrong with it in plainer terms?
Oh yes. I've got scores of paper books on CS topics
If it helps, I ran my code on a similar problem just now: ANSI C, gcc 4.6.3, 16k particles in all-pairs simulation, 4-core 2.5 GHz Phenom, **2D** gravitational n-body code (so 10 FLOPs per interaction instead of 20, so double these times to match the right half of Table 1): * single-threaded x87: 1.38s * single-threaded SSE2: 0.45s * OpenMP, SSE2: 0.12s
Really, do we want people back who poison the community with discussions about assumed sexism and cisgender drama?
At this point the article has substantially more upvotes than downvotes, so it seems like people appreciate it for the most part. Meanwhile you are a sockpuppet account with only one post. So I suppose that you fear the community would probably not take well to your sentiments if you expressed them under your usual handle. I guess that says it all...
I'm guessing something like: class (Category cat1, Category cat2) =&gt; Functor cat1 cat2 where fmap :: cat1 a b -&gt; cat2 a b
 &gt; a remark where the idea behind the remark is sexist So my problem with this definition is that it requires me to read _intent_ into the original remark, and you rightly point out that making assumptions about people is bad. You're distinguishing between "sexist as designed" and "sexist as conveyed," and that might be a useful distinction if you're trying to figure out if _Doaitse Swierstra_ is a sexist _person_, it's not relevant when you're trying to figure out if _the thing he said_ was a sexist _thing_. When you keep the focus on _the thing and its effects_ and off of the _person_, when you make it a ["what they did" conversation and not a "what they are" conversation](http://www.youtube.com/watch?v=b0Ti-gkJiXc), you avoid having to judge intent. You can just say, "that struck me as a sexist thing to say," and the person who said it can figure out whether that means "oh, I need to change the way I think about things" or "oh, I need to be more careful how I phrase things."
&gt; There is a disturbingly large number of people who consider the whole program of changing social norms regarding sex and gender to be nothing more than a "political correctness brigade" that feeds peoples' need to feel self-righteous while not actually accomplishing anything good, and the last thing that you want to do is to provide fodder for them. This is called "concern trolling". It's counterproductive. Don't do it. Do *you* feel that the program of changing social norms is just a political-correctness brigade and so forth? If you do, then pull up your big-boy pants and *say so*. Own it. If you don't feel that way, then why bring up hypothetical people who do? You're only responsible for your own behavior, so if *you* in fact are not making a decision to not support human rights because you don't like what one pro-human-rights person said (and why would you do that, anyway?), then there's not really any need for you to point out that other people might. Other people, indeed, might. People do a very great number of silly things. But every minute you spend concern-trolling people on the side you *say* you're on is a minute you're *not* spending being the voice of reason to guys who don't understand that they're being sexist.
&gt; It feels like in the department of gender studies or political science: every man is a sexist, rapist, misogynist, racist a.s.o. and of course he's totally unaware of his privileged position, when he reproduces his hetero-normative system of suppression. Allow me to tell you a story. I was raised in a quiet liberal college town full of feminists. Growing up, I certainly got the feeling that I was destined to become some awful rapist angry dumb lug, and I certainly had a lot of guilt. It was never an overt action that gave me this idea, but a general subtle malaise that permeated a lot of the conversations I absorbed and stories I heard. This guilt and anger built and built, and some years later I moved with it to another atmosphere entirely. This knew place of residence was the exact opposite of what I had grown up in. Everyone was conservative, men ruled. Women made babies and lived at home with no purpose outside of raising children, completely enslaved to their husbands. Men and women went through life with a schism between themselves. They would conspire separately about each other and play the boy-girl team game and god was it awful and caused lots of problems for them and they all laughed it off as some inevitable part of life. All of a sudden I started realizing just how much freedom and liberty I had in the past. People began to judge me for the way I held my body, the way I spoke. I was "queer" and a "sissy". That ball of self-hate bloomed in a way I had never know before, until all of a sudden it hit me. That feeling of subtle attack I had as a child was very small, and largely a product of my own self-absorbed adolescence. Now I was experience a much more serious form of discrimination, and I was not at all prepared to endure it. And I am a ciss male, for Christ's sake. Imagine, I thought, what this must feel like to women who live here. It was then that I realized why the women I had grown up with said the things they did. They had all migrated to where they lived, few were natives. They all new this other place, and they had been enduring this bullshit their whole lives. I don't want to excuse all of their actions, certainly some were out of line, but frankly you have no idea what you are talking about. Tim's words are not some attack on your liberties or some form of "reverse sexism". I have seen reverse sexism*, and let me tell you, it does not sound like Tim. *Some will disagree with my use of sexism here, sighting an academic interpretation of the word. I acknowledge the utility of this nuanced interpretation, but here go for the common tongue meaning.
&gt; You're distinguishing between "sexist as designed" and "sexist as conveyed," and that might be a useful distinction if you're trying to figure out if Doaitse Swierstra is a sexist person, it's not relevant when you're trying to figure out if the thing he said was a sexist thing. Agreed, though it does matter if one is going to do things like insisting that Doaitse make an apology about the remark. Furthermore, it is hard to make criticisms that are completely detached from the person whose actions they are directed at. **But**, having said all of that... &gt; When you keep the focus on the thing and its effects and off of the person [...] That is certainly a fair point. To the extent that the most important thing is making sure that our words don't cause unnecessary offense and drive people away and so we are focusing the conversation on this issue alone rather than blaming the speaker for the effect of his or her words, I agree that the distinction I have been drawing is not an important one --- though it does help to clarify where went things wrong, which is useful in figuring out what one would need to do next time to avoid falling into the same trap. P.S.: Thank you for writing a reasoned response to my comment that addressed what I actually said, as this was not how people generally reacted to my remarks.
Absurd or not, that's pretty much how it works in practice. This is the same thing I said in previous comments. You're welcome to dislike this fact, but it doesn't change the reality of the matter.
I do not believe any of these apply to me. Since you are claiming some do, it would be helpful if you could tell me which, so I know what I'm accused of.
Like I said, the well has been poisoned. Even if you have only the *absolute best* of intentions, you're still going to sound exactly like someone attempting to undermine the cause by bogging any discussion down with endless quibbling over minutia. Honestly, I also suspect you severely overestimate the actual negative impact of the sort of behavior in question. Most of the people it's going to "turn away" were never going to accept the ideas no matter what. But that's another issue, and I think that if you can manage to get past your own perceptions of it and observe how most people actually respond when social norms are challenged, you'll see what I mean.
Well, PDF is not in the scope of Juicy.Pixels which is aimed at raster image formats.
I was also there, and my Twitter feed shortly after the incident probably made me look like I was sharpening my pitchfork. I do not apologize for my anger. But after some time to recover from travel, I think I can better articulate what upset me. This post is mainly about why the remarks affected me so deeply, and is not meant to treat the more important issue of *why* the remarks are harmful. For that, see Tim's original post and the thoughtful words of others on this page. This is for the sake of additional perspective. In my undergrad CS career, I'd often wind up in informal settings before class, during a lab, at social events, etc., where the topic of gender disparity would arise. The overwhelming majority of the time, the trajectory of these comments was a direct descent from an admirable starting point ("we should do something about this!") to the same ugly depths: wishing for a solution not for equity's sake, but for the pleasure of their eyes and for easier dating. I was never comfortable with this, but at the time it wasn't in a way I could articulate or turn to protest. I moved on to grad school, and along the way engaged in some of the terrifying, difficult self-examination that efrey so eloquently describes elsewhere on this page. I realized it was not moral of me to simply know in my heart that I rejected such words. Conscience demanded that I put in work to make my new community more inclusive. In practical terms as an instructor, this meant keeping an eye out for sexist (and racist) behavior, and letting my students know in no uncertain terms that it was unacceptable. Fast forward to (my first) ICFP, where despite travel mishaps and pre-talk anxiety, I managed to have a pretty awesome time. I spent the week connecting and reconnecting with folks I usually "see" only electronically, and was really on a high, feeling lucky to be welcomed by such a great community. Then I hear, from someone who bears respect and prestige from that same community, make the exact same sort of well-intentioned yet ill-conceived remark that made me cringe those years ago. And worse, I hear my peers and superiors laugh approvingly in just the manner of a group of sophomore men in the corner of a basement computer lab. It was a punch in the gut, to instantly be reminded that no matter what little good I might have done in correcting my students, they need look no further than the elite of their chosen field to see their original behavior echoed and lauded. No careful parsing of words nor divining of intent will change the fact that these remarks and the nature of their reception normalize harmful behavior. This is just not the way we make progress against exclusion. 
&gt; My objective has not necessarily been to convince everyone to see things my way but to at least consider my points seriously for a moment before rejecting them, though it would seem that I have largely failed miserably in this goal. Oh, don't get me wrong; I don't think you've said anything that's worth rejecting summarily. Most people _do_ distinguish heavily between accidental offense and intentional offense, and if people like me want the world to work differently it's going to be an uphill battle.
I'm suprised about how many people find arrow syntax weird. It felt so natural to me the first time I saw it... Also, about classic FRP and AFRP, you might want to read [this](https://lukepalmer.wordpress.com/2010/01/17/beliefs-and-the-unimpossibility-of-frp/).
&gt; Consensus has already been broadly reached: Doaitse's words were not intended in a sexist way Consensus on /r/php is that PHP is a good language to use for web development. Doesn't mean that it is. ;)
Which signatures are "a bit hairy"? If some are too confusing, I can try to simplify them for ease of use. StateT is used internally, EitherT is used through cereal, Reader is not the best thing for bit level access. I don't understand the remark about put (Int, Int), could you clarify?
I'm not sure what the point is that you're trying to make.
I agree, a slingle threaded optimal C implementation is a great base line.
OK, and?
This is hilarious! You're a funny guy!
I think we can hope to get a *lot* closer to C level performance than we do now. They got a 14.5 times speed up when using strict fields and UNPACK pragmas in Haskell for example, could the compiler have done at least some of that for us? 
For those like me who were unfamiliar with it, the Haskell Wiki has [a helpful discussion of the the `foldr`/`build` rule](http://www.haskell.org/haskellwiki/Correctness_of_short_cut_fusion), and related issues.
Unfortunately, I don't believe this would actually be conveniently useful for the current common use case of `fmap`. -- "list functor" instance Functor (-&gt;) (???) where fmap = map -- "maybe functor" instance functor (-&gt;) (???) where fmap f Nothing = Nothing fmap f (Just x) = Just (f x) -- pseudo "io functor" instance functor (-&gt;) (???) where fmap f (world, x) = (world, f x) Can the `???` be filled in without changing these implementations? The only way I can think to do it is by modifying these implementations with a newtype for each sort of "lifted" function. And that is just a pain to deal with for the library user. -- derive the obvious Category instances for these newtype ListFunc a b = ListFunc ([a] -&gt; [b]) newtype MaybeFunc a b = MaybeFunc (Maybe a -&gt; Maybe b) instance Functor (-&gt;) ListFunc where fmap = ListFunc . map instance Functor (-&gt;) MaybeFunc where fmap f = MaybeFunc $ \case Nothing -&gt; Nothing Just x -&gt; Just (f x) 
Well, you can define such a category: type FCat f a b = f a -&gt; f b However, I still agree that it is not a good solution. For example, pipes will soon introduce a functor that transforms 4 separate categories, whose morphisms all overlap in the same type. There is no sane type class that can encompass that behavior.
State: generateFoldImage :: (Pixel a) =&gt; (Int -&gt; Int -&gt; State acc a) -&gt; Int -&gt; Int -&gt; State acc (Image a) (Int,Int) datatype: data Position = Position {x :: {-# UNPACK #-} !Int, y :: {-# UNPACK #-} !Int } type Size = Position -- x = width, y = height generateFoldImage :: (Pixel a) =&gt; (Position -&gt; State acc a) -&gt; Size -&gt; State acc (Image a) EitherT: readBitmap :: FilePath -&gt; EitherT String IO DynamicImage Reader: readPixel :: Position -&gt; ReaderT (MutableImage s a) (ST s) a writePixel :: Position -&gt; a -&gt; ReaderT (MutableImage s a) (ST s) ()
Use XPolymorphicComponents, which is even milder than XRank2Types.
I had a post with a chart of the performance of lists vs. different lists on comonad.com, but it was lost when the server crashed 3-4 summers ago. The difference in asymptotics is quite noticeable. (I didn't use criterion, just hand rolled some benchmarking stuff, not sure if criterion was out yet then.)
Generality for one. The trick from `fmlist` generalizes, the one from `dlist` does not. newtype Free p a = Free { runFree :: forall r. p r =&gt; (a -&gt; r) -&gt; r } then type List = Free Monoid type Maybe = Free Default and the classic free monad can be rederived with class Algebra f a where phi :: f a -&gt; a type FreeMonad f = Free (Algebra f)
Modules, but not parameterized. Some stuff that's a little bit fancy, if you're willing to build it. But yeah, annotations aplenty once you move to standard .NET classes. http://fssnip.net/3S Word of warning, this is generally considered bad practice in F#. 
The main problem I had with the Haskell LLVM bindings a while back was that, as far as I could tell, they made it impossible to emit a function call with an arbitrary dynamically chosen number of arguments. What happened was the "call" function's type depended on the number of arguments the eventual LLVM "call" instruction needed. So if you only know the latter at runtime (say based on the program you're compiling), there was no possible way to get the original Haskell program to statically typecheck. I suppose template Haskell could have given me the equivalent of a switch on the length of the argument list up to some fixed limit, but that was too ugly so I gave up. What it really needed was some kind of "retval &lt;- callWithArgList myArgList".
&gt; You don't think that women find that attractive, do you? Au contraire! It has been my experience that by focusing heavily on communication, I have had the blessing to explore relationships with some of the most legitimate, passionate, creative, open-minded, loving souls I've ever had the pleasure to meet. It has allowed me to explore parts of their psyche and my own that I would never have known otherwise, and experience pleasures beyond description. Some times I have had multiple lovers, and on occasion they have loved each other as well. It's been all quite lovely. So no, I have no fucking clue what you are talking about. EDIT: So, it's possible I focussed to much on the sexual connotations of "attractive" in this comment. 
I don't believe they were referring to anything specific in this thread. If I'm reading the comment correctly, I believe this is directed more towards rape culture, and how women are constantly flooded with tips on what they can do to protect from rape, and on the other hand are berated for looking at all strange men as potential rapists. It's exhausting.
So I don't understand what bringing up this hypothetical consensus has to do with anything.
How about you don't write a rebuttal to anything, because I didn't call you any names, I just said your post was derailing, which I think it was, and I explained why *in the body of my post*. I also gave a link with some broader stuff, some of which you jumped right into, starting with "If You Won't Educate Me How Can I Learn" and into "If You Cared About These Matters You'd Be Willing To Educate Me". Some relevant but not exact matches to yr. original post are "Unless You Can Prove Your Experience Is Widespread I Won't Believe It" and "You Have A False Consciousness". What you actually did was slightly more subtle, which was to accuse danharaj of telling you you can't have an opinion, which was *not* what the post said. All it said was that you should second-guess your opinion in light of certain likely limits on your knowledge and experience. And this idea that maybe you should chill out and learn a bit instead of holding forth because maybe your experience is partial, that apparently freaked you out enough that you needed to write a whole post defending your 'right' to have an opinion. Said right was never under attack. This is basically a discussion you didn't need to take a stake in, and you decided to weigh in not on the discussion itself, but on the strawman premise that somebody wanted to take away your 'right' to an opinion on something that you still haven't clearly bothered to take a stand on. Outside, of course of "your comment sounds more sexist than Doaitse's comment". Which is precisely "You're As Bad As They Are!" in the link I posted, which you seem to have studiously ignored.
I think you are oversimplifying the problem. The problem isn't `(l ++ l) ++ l`, it's with general operations like data Exp a = BinOp Exp String Exp | Atom String showExp (BinOp l op r) = showExp l ++ op ++ showExp r showExp (Atom s) = s In this case, for large expressions, the left nesting costs a ton. When it's trivial to replace your data structure with a good one, why not just do so? `show` works this way internally already!
LLVM can't possibly be "too slow for a JIT compiler," since it has one built in! (`lli`)
Right. I hope. I wrote that page. :-) Unfortunately I never got around to update that page to also cover a later part of the story, told here: http://dx.doi.org/10.1007/s00236-011-0136-9 Also relevant in this context, and hopefully helpful to get a quick impression of when and how seq impacts free theorems, is the accompanying online tool here: http://www-ps.iai.uni-bonn.de/cgi-bin/polyseq.cgi 
Yeah, sure, that's a thing that happens. But personally, I'm very happy to *aggressively discourage* such idiotic behavior in this or any other worthwhile community. I care not in the slightest that a depressing number of men want to act like children, no matter how much they might whine about being called out on it.
You cannot do without the newtype. I believe I've seen a proof somewhere that System F cannot Scott-encode lists (among other things). You need to already have recursion on the type level.
To establish that there is little need for further debate, when agreement has already been reached.
Because people responding to unexpected laughter are expected to have the most effective stinging response ready within seconds of said laughter occurring.
Also just re-built my F90 treecode, though this can't be an apples-to-apples comparison, as there are a number of parameters that have a significant effect on treecode performance. That said: Fortran 90, gfortran/gcc 4.6.3, no explicit SSE2, 80k gravitational particles in 3D with compact exponential kernel, modified Barnes-Hut traversal, same computer as before (4 cores, 2.5 GHz, Phenom): * serial: 7.29s * OpenMP (4 cores): 2.02s
Well, it should be "too slow for a high-performance JIT compiler" then ;-) The purpose of the LLVM JIT is an different one: Shifting static compiling to execution-time. For example: You can build your application, say, with llvm 2.5, but using it with llvm 3.1 now. The advantage is clearly, that you can now use they hopefully better JIT compiler, which produces faster code (but it doesn't necessarily compiles faster). In practice this is a real problem: Some companies have only binary access to some application, and can't recompile it with a newer compiler (in order to gain better performance). Hence there're projects like Dynamo or DynamoRIO. A solution for this is LLVM JIT, or any other system where a VM is involved (like Java).
It seems like with `Free Monoid` the forall basically eats any opportunity for the compiler to optimize since it can't make any assumptions about the monoid being used. So why not just go directly to the simple version? data FMList a = MLeaf a | MEmpty | MAppend (FreeM a) (FreeM a) At least then you can manually optimize by removing MEmpty in your constructors and/or have `MSeq [a]` as a constructor, and write the obvious functions: foldFMList :: Monoid m =&gt; (a -&gt; m) -&gt; FMList a -&gt; m foldFMList k (MLeaf a) = k a foldFMList _ MEmpty = mempty foldFMList k (MAppend x y) = foldFMList k x `mappend` foldFMList k y -- convert to DList for lists to avoid left-associative mappend toList :: FMList a -&gt; [a] toList xs = appEndo (foldFMList go xs) [] where go a = Endo (a:) 
'better', 'diverse', 'awesome'?
&gt; The difference in asymptotics is quite noticeable. They are indeed. For this particular case I'm wholly convinced of the cost and of the analysis, having done a few different benchmarks over the years. When I said it's good to have benchmarks, I was meaning more for the general case.
No worries. FWIW, if you haven't seen [Tim's new post](http://geekfeminism.org/2012/09/17/how-to-exclude-women-without-really-trying/), I find that version far less problematic.
Poison? What poison? Just because *you* do not like to discuss the problem of sexism, does not mean that it should not be discussed. Just because something is unsavory, does not mean that it ought not be discussed. And as far as such discussions go, this is one of the healthiest ones I've seen in any online community.
&gt; I've ever had the pleasure to *meat.* Um, please say you didn't mean that ;)
My point is, if women are indeed to take precautions against rapacious men, that means that they must end up taking precautions against all men--- because, a priori, there is no way to distinguish between those men who are dangerous and those who are not. Due to the nature of defense, this means assuming the worst until provided with evidence to the contrary. Bear in mind that the overwhelming majority of rapes are committed by acquaintances and family members--- people conventionally assumed to be safe to be around. However, you then get upset when people do not treat you as a special and unique snowflake, getting to know you before passing any judgement. You cannot have it both ways. Let us assume for the moment that you are indeed innocent as a lamb. If this is the case, then surely she will realize it in time and will come to treat you kindly. You must only "suffer" to wait until your actions make your nature apparent to those around you. Now, let us assume that you are not so innocent. If she were to trust you, then her defenses will be down and you could take advantage of that--- exactly the situation you have warned her that she must protect herself against! This belief that you are a special snowflake and that people should give you the benefit of the doubt is a sign of privilege. What makes you so special? Why should people assume the best of you, whereas they should assume the worst of other people? The belief that you are "suffering" to wait until people realize your true nature is another sign of privilege. To rage against this "suffering" is to declare that your need to be validated is more significant than her need to protect herself.
They are not asymptotically faster for everything. If all you care about is `nil`, `cons`, `map`, `concatMap` and `foldr` (down to some other type), Church encoding will probably be asymptotically faster. Scott encoding is better at `tail` and such. There is a paper, Asymptotic Improvement of Computations over Free Monads, about how if `T` is a free monad, then `Codensity T` is a more efficient version of `T`. But the two are actually isomorphic, so actually `Codensity T` _is_ a free monad (with the same generator as `T`). And in fact, implementing free monads using Church encoding also gives you the same asymptotic benefits as `Codensity T` is presumed to have. So really, it comes down to implementation strategy, not whether a monad is free or not, and while Scott encoding (or, the expected implementation of data types in Haskell) is asymptotically faster for some operations, it is slower for others.
There is something strange with the code involved. I tried comparing their version of the code (`haskell/bh.hs`) with the sequential Haskell version submitted for the computer language shootout. At 100k iterations they both terminate quite quickly, but for 1M iterations their parallelizable version runs in approximately 4 seconds on my machine, while the (sequential) shootout code runs in 0.24 seconds. Given the difference in performance, I don't quite see how improved parallelism could ever become profitable. The [sequential code from the Shootout](http://hpaste.org/74898) ghc --make -O2 shootout.hs -fforce-recomp -fllvm -XBangPatterns -rtsopts time ./shootout 1000000 +RTS -K100M -0.169075164 -0.169086185 real 0m0.249s user 0m0.248s sys 0m0.000s The [parallelizable code from the article](http://hpaste.org/74897), slightly modified to replicate the Shootout calculations with data from the solar system, and run sequentially: ghc --make -O2 bh.hs -fforce-recomp -fllvm -rtsopts time ./bh 1000000 +RTS -K100M -0.16907628426213958 -0.16499965884754134 time taken: 4.64s real 0m4.647s user 0m4.464s sys 0m0.176s The -K100M is necessary for the article code, which uses up to 10 megabytes of stack on this input data and therefore overflows the default settings. I suspect the performance problem might be related to a memory leak issue rather than a fundamental problem with the parallelization approach. The `-fllvm` flag was suggested by the Shootout website, and does make a difference (I suppose on float computations) on larger number of iterations for the shootout code (for N=5_000_000 it is approximately 60% faster with -fllvm than without). 
You're right, this needs an equivalence relation. I think you can use the one generated by: App ((.) &lt;$&gt; g &lt;*&gt; f) x == App g (f &lt;$&gt; x) I'll try to work out the details.
Tim adds another story in that post which is interesting and a bit sad (it caught my attention because I organised the event in question). Tim describes that the atmosphere was good and he felt included -- “one of the boys” as Tim puts it -- but that at dinner one comment from one person shattered the otherwise positive experience. To me, this illustrates how fragile things are once we end up in a highly male dominated environment. Even though the vast majority of the men don't have a sexist bone in their body (and are also aware that it's not just intentions but perceptions that matter), it just takes one or two people with silly comments to make people feel uncomfortable or excluded. And that's even when that minority are not really sexist but perhaps grew up with a different norm from the children of the babyboomers. I don't recall the particular comment but if I had, I probably would not have said anything. I've certainly intervened in the past with worse transgressions but I'm sure I'd have considered this one too minor and let it slip. This goes to illustrate the difficulty I think: once we are in a very male dominated group it's very hard to sustain an ideal atmosphere. There's two bad effects: firstly the minority is very aware of the imbalance and my guess is that increases the percieved significance of silly comments (at least that's how I imagine it would feel for me), and secondly the minority who might be more gaurded in mixed company probably feel more able to say silly things -- even when everyone else there doesn't share their attitude. The majority let things slide because we don't like to make a fuss. We can do better on that, but as this story shows it's even quite minor things that can sour an experience, and I worry we cannot police things sufficiently once we're down in that rut of 5%/95% imbalance.
There's a collective action problem: many of us cringed but we didn't say anything to each other, so we don't know "oh you thought that too?" so that we can agree "ok, next time it really is ok to make a fuss and say something". As you say, it doesn't have to be pitchforks and confrontation but a friendly heckle or giving someone a second opportunity to clarify their remarks. Perhaps that's something we can take from this discussion: that if your cringe detector goes off then it probably has for many other people too and while we can all give the benefit of the doubt on the intention, it's ok to say something just to clarify it.
btw, am I the only one confused by the keyword-filter on cufp.org? E.g. when I select the "Haskell" keyword (-&gt;http://cufp.org/videos/keyword/55) , I only get presented 4 videos, although there are definitely more videos tagged with the "Haskell" tag...
Spoiler alert! newtype C m a = C { unC :: forall r. (a -&gt; m r) -&gt; m r } instance FreeLike f m =&gt; FreeLike f (C m) where wrap f = C (\h -&gt; wrap $ (\p -&gt; unC p h) &lt;$&gt; f) I was pretty weirded out by this solution too in particular, and I got it the same way you did I imagine: I concurrently smashed things together and did unification in my head until the types were kosher. It really made me yearn for an equivalent to Agda-style holes in GHC. Of course this IS reminiscient of other `FreeLike` instances for other various transformers (as seen [here](http://comonad.com/reader/2011/free-monads-for-less/),) but yeah, I didn't get a super intuitive grip. I think I honestly had the biggest problems with the `Monad` instances for `C m`, as opposed to the definition of `wrap` (which is funny, since they're weirdly similar.) Granted, I did all of this without reading Janis' paper. That would probably help now that I've done the exercises.
...so SPJ seems to be an Emacs user =)
But there's still hope for him, he may repent!
Actually, I'm not sure that you could use other examples. I'm no expert on this library, but at a glance the API seems very simple and clean. That one function has a slightly hairy type, but not enough so that it seems worth introducing monad transformers into the API that are only useful for the one function. It took me a lot longer to decode your `XYEnv` type than it did to understand `generateFoldImageSource`, and I could do it without looking up several new definitions. It's trivial to introduce the monad transformers in your own code if you're using them enough to justify the cognitive overhead, and apply the appropriate run functions to match the given API. (As a nitpick, even if you did "simplify" the interface, it would be a huge mistake to use the same `XY` type to represent x and y coordinates, and also image width and height. That *would* be a confusing interface.) *Edit*: As a further comment, part of the reason that `generateFoldImageSource` looks hairy is that it's, well, just a pretty messy function. It's underspecified (what order are the pixels visited? That matters if you're threading state) and probably not useful anyway (if I need to thread state, what are the chances I need to do it in whatever order the library chooses?) I can see how it would be useful, perhaps, where the state is some kind of a semantically transparent cache so that the order doesn't matter...
This looks very nice! However, it looks like both `readJpeg` and `decodeJpeg` assure me that I can convert to RGB using `colorSpaceConversion`, which is not exported anywhere. Did you miss an export?
Just as an aside, to me "attractive" has never meant anything about physical appearance unless used with words that imply it in context. "Make X more attractive", where X is not a person, to me always implies a non-sexual definition. Obviously people (who were actually there) interpret it another way, but it wouldn't have been my interpretation. E.g If I said "Adding more women to our CS programme would make it more attractive" I'm not saying that it would consist of better-looking students, but that it would be more appealing to a wider group of people because it promotes a gender-equal environment. I think that's what Daoitse intended to say, but as has been made clear here, intent isn't as important as the message that is actually communicated. The only time I interpret "attractive" to mean "physically attractive" is if it's applied directly to a person, e.g "Mr. X is very attractive" or "Ms Y is not attractive". edit: Is there a reason I'm being downvoted? If I have said something offensive or inaccurate, I'd like to know.
I had not made a connection between monadic regions and ST's phantom type. Clever. I think the name 'region' makes the point of the "esoteric" 's' parameter easier to understand for beginners.
I'm updating the HIW page with all the slides and videos. I will post a link on the Haskell reddit once I have all of them done.
See, that's the problem with people like you. You're on a mission, you're easily offended, you're aggressive and annoying. 
This isn't a discussion about sexism. This is an accusation party based on what people might have said and felt.
Thanks for pointing the missing documentation, will be fixed in the next version of Juicy Pixels.
It's the same thing. IIRC Oleg &amp; co even refer to ST in the paper.
You're right, the Haskell tag's busted. And [just as I feared](http://www.reddit.com/r/haskell/comments/yvfpa/2day_introductory_haskell_training_at_cufp_next/) the Gibbons and Marlow session doesn't appear recorded. 