For the record, I can see precisely what MaybeJustNothing's Haskell snippet does, but I can't parse the meaning of either the english or the javascript in eckyputr's example.
&gt; * '`addGregorianMonthsClip :: Integer -&gt; Day -&gt; Day` computes a date that is simultaneously "in a month" and "in the next month"; the two are not necessarily equivalent' - I couldn't understand this, can you rewrite ? Does this not become more clear after you've read about the function later in the tutorial? I changed "in a month" to "a month from now" – does that make it slightly better? &gt; * I think you should not conflate ZonedTime and LocalTime the way you've done. There are cases where the zone-less LocalTime is the correct abstraction, which is why the time library includes it, and users will soon notice it. It would be less confusing to explain both types. Could you give an example of when it is a good idea to throw away the information about which timezone a local time applies to? &gt; * The discussion of realToFrac will help every newcomer. I agree it's unintuitive, should the time lib add a more easily discoverable alias like `toNominalDiffTime`? I'm pretty sure this is a general problem whenever a library introduces a new type that is an instance of Num et al. but don't make it clear that you get some conversion functionality that way. &gt; * You helpfully mention formatTime - it would be nice to also cover time parsing with `parseTime` I knew there was something I had forgotten! I'll make a mental note of adding a bit about parsing.
moment.js is *not* user friendly. Libraries like this that mix the jQuery popularised chained idiom with mutating methods give me PTSD. 
&gt; Does this not become more clear after you've read about the function later in the tutorial? Ah, yes it does, I missed that. Still, the first mention is unclear. Maybe something more like the haddock, or "Move a date some number of months forward or backward, adjusting it when necessary to stay within short months." ? &gt; Could you give an example of when it is a good idea to throw away the information about which timezone a local time applies to? Someone else can do better, but eg modelling a business rule which says "work ends at 5pm" (not 5pm GMT). Note I haven't suggested throwing away a timezone, sometimes there is no timezone involved.
Wow. This is amazing! the time package is much much less confusing for me now :D Just like the other comment said, I too was looking for time parsing. Although it's quite clear from the docs, but I think it's worth mentioning. One example use case that I can think of is when your haskell server parse date input from client. Thanks for writing the tutorial!
I like your wording. I'll steal it! &gt; Someone else can do better, but eg modelling a business rule which says "work ends at 5pm" (not 5pm GMT). Note I haven't suggested throwing away a timezone, sometimes there is no timezone involved. Ah, I see what you mean. I'll have to think about that. I'd like to not make the tutorial much longer than it already is, but perhaps there's some way of rewording it to make it clearer that in the rare case where you have a local time rule that's timezone independent there is a separate type to deal with that.
&gt; I think the requirement for writing .cabal files, with their non-standard syntax and infuriating redundancy, is still holding the UX of Haskell project management way back. That's interesting. cabal files *are* a bit of a pain these days. But how does stack help with that - it still requires cabal files ? [hpack](http://hackage.haskell.org/package/hpack) solves the redundancy of cabal files, and provides a simpler easier-to-edit syntax (YAML), though it doesn't yet support all cabal file constructs (flags, conditionals..). I'd love to see it be incorporated into Cabal as an alternative file format and perhaps eventually displace the old one.
Great. And if you want to cover all common Haskell time confusion, you could add a note about system time (getClocktime) and when it's appropriate to use one or the other. It's your call but a high-quality tutorial like this covering a vital topic is welcome to be as long as likes IMHO. :)
`--dry-run` makes it a bit more tolerable but isn't enough, as opt-out is the wrong default for me. I'm used to tooling which adheres to opt-in style, rather than automatically downloading stuff and launching rockets without prior notice. I want to stay in command and be involved in any additional work the tool is doing. This could either be like with `cabal` which has different sub-commands or by telling me what's going to happen and interactively asking me whether I want to proceed with that.
Don't thank me; thank Thomas Tuegel and Ryan Trinkle (the mentors of the project), and also everyone else (esp. Duncan Coutts) involved in designing this feature.
We hope that GHC is deterministic, but we don't rely on it. Currently, Cabal register will refuse to install a package if there is already an existing one with the same IPID in the database. You have to unregister it first. (Story is better with cabal remove, since now it can also get rid of anything that depends on it.)
general problem is still a problem!
&gt; FD produces architectures that are much closure to being robustful [..] Is `closure` a gag or a typo? 
&gt; started hacking together a tool to create TAGS Please make it available as a stack plugin. Like `stack create-tags ` :)
So.. no.
My experience is that dealing with time and timestamps in haskell is terrible. 
When you use mutation, State monad has terrible performance (5-15 times worse than tail recursion). So in those cases it's better to use ST or tail recursion.
What do you mean by mutation?
But isn't that the only thing the state monad does?
Yes. But the state monad is hardly the only way to do it.
I'm not at the moment interested in watching the video, but what does "counterexample" mean here? What's the proposition being countered?
But if the only thing the state monad does is mutation, and there are better ways to do mutation than the state monad, that leaves no use case for the state monad. Or the state monad needs to be re-implemented in terms of some more efficient mutation mechanism.
if you're talking about being downvoted, for the record it wasn't me :) 
I think it's more about whether this state is actually "mutable" or not. If I find most of my types in a module to be something like `s -&gt; ... -&gt; (s, a)` with `s` being some primitive type, I'd be inclined to just use the state monad. It's simply a cleaner way to do it, because it makes for tidier type signatures and it's not like the state monad is some obscure pattern.
By not letting you thread the state manually. It does that for you.
Because the final version of the state is always the one returned by runStateT. If you are passing the state as a parameter to a bunch of functions that return modified versions it is difficult to spot places where you might have lost the modified state and returned the original state instead.
Mutability is a different thing than how you pass arguments. There's not even a function call in there, so it can't be pass-by-reference vs pass-by-value! Or are you trying to say the `state` variable in my C program *isn't* mutable...?? If so, we have a very different definition of what "mutable" means.
Since `State` is the composition of two functors, you would have to navigate two layers instead of one.
Putting clear words to my loose intuition. Nice!
I see `Reader` used often, but I agree that `Writer` is not seen that much. However I think the ideas behind `Writer` are very important, which I think is why it is discussed a lot.
interesting.... but you would still need some sort of internal state to keep track of valid sessions right? 
A Stackoverflow answer to [my question](http://stackoverflow.com/questions/31281655/index-contents-of-a-list-a-int-a), with 5 favs, said &gt; ... state is almost never necessary and most problems can be solved by combining standard functions. State really should be one of your last resorts.
There's no semantic change[*], it's just syntactic sugar. Which aside from being easier to work with, eliminates a big source of mistakes in cabal files. I find it so much nicer to work with I've switched even though I have to apply manual fixups. [*] yet, though hpack's author seems not averse to that.
If you desugar a state computation by substituting the bind for his definition in the state monad, you will see that your monadic expression is just ordinary pure chaining of functions with an extra parameter. No impure sin is committed using the state monad. Go in peace and use it anytime brother. 
Yes, but because of https://phabricator.haskell.org/D1086, not the improved pattern matching. The improved pattern matching has to do with exhaustiveness checking, not haddocks.
Writer is where you feel the pain of unfortunately defined Monoid instances. (I'm looking at you, Map...)
If you find yourself adding to state before each function call, and removing each return, don't use state, just pass a parameter. 
I feel like Map's monoidal problems are really Maybe's monoidal problems.
If you're actually modifying the state during the course of your computation, then the State monad may be the right thing. If you're not modifying it, then you should probably use Reader instead of State. And if you're using Reader, then you could also just pass the environment object as a parameter to your functions. Reader is simply convenience so you don't have to keep passing the environment around.
Not really, the session is always valid. You simply delete the cookie to "log out" the user. It's one of the downsides of storing sessions in encrypted cookies :/
Interesting. I prefer to think of applicatives as closed functors, which makes `&lt;*&gt;` a theorem that `f` preserves internal homs.
First of all, you could participate in [this Codewars Kata](http://www.codewars.com/kata/befunge-interpreter/discuss/haskell). Otherwise there is really not much to criticize and it depends on what you are really trying to achieve: * Does it really have to be in IO? * Updating a `Vector` is not really efficient. (Only bad if your program runs through many `p`s.) * Writing a `Random` instance for `Direction` is probably better.
But you could still forget to put it back in.
relatedly, someone proposed something like: insert :: Monoid v =&gt; k -&gt; v -&gt; Map k v -&gt; Map k v with explicit update strategy. then you recover "keep the last value inserted" with (Last v).
&gt; the only thing the state monad does is mutation It actually doesn't. It provides a *substitute* for mutation, but is functionally pure. The following are basically equivalent under the hood: f :: x -&gt; s -&gt; (x,s) f :: x -&gt; State x Basically, if you're passing that `s` around to keep track of it, `State` does so in a well-defined, generic, and proven correct way. You don't have to handle the `s` on its way from input to output, and therefore you can't *mishandle* it and create a bug that needs to be tracked down. The justification here is maintainability, not performance. As a bonus, `State`-based code is guaranteed safe for parallel execution: because that `s` is really a function argument, it's copied per application, which means two simultaneous "mutations" aren't actually affecting the same data, and can't create race conditions. In the rare (but real) cases where `State`'s extra machinery does turn out to be a performance bottleneck, we've got faster alternatives. But we'd rather not use them for exactly the reason we're programming in Haskell in the first place: they create more places for bugs to hide. When something goes wrong, it's nice to be able to rule out large sections of code as *definitely* irrelevant. With mutation in the mix, the best you can generally do is *probably* irrelevant, and that nugget of doubt can suck up many hours.
but replacing put with writeSTRef and get with readSTRef also prints "1 2 3".
What are the pain points exactly at the syntax level? And what makes YAML so much better to justify the cost of migration? I have only been using YAML for Travis so far and it's been more painful for me to write (despite Emacs) than editing cabal files. 
So this isn't about whether you have the desired level of control, but whether the tool adheres to the defaults you want, right?
You could do that with `adjust k (&lt;&gt; v)`, right?
I found myself using State a () and then realizing that you can often achieve the same using a fold. A fold takes an accumulator and applies values from a list to update the accumulator using some function. The simplest example perhaps is summing a list [1,2,3,4] with initial value 0 and the function being addition. However another way to look at folds is there is an initial state, e.g. 0, a state modifier (the function) and a list of inputs used to help modify the state [1,2,3,4] I have used this in a game to convert a list of moves and an initial position into a current position. You could so something like that in Chess for example. I try to avoid using State if I can, not because it is bad but often there is a simpler way to achieve the same effect, which is more like functional programming than using State which with the do sugar and get/put functions feels like an imperative crutch.
Could you add a README with an example run? This helps users understand what your program does.
Isn't that just `insertWith mappend`? Making that the default could be interesting, though it hasn't yet bothered me. I certainly use `insertWith mappend` sometimes, but in contexts where I can just name it explicitly. The problem with the Monoid instance is that Last is usually *not* the monoid I want, and it's a non-local change to move away from the incorrect default.
I don't follow you. If memory serves, singleton k v1 &lt;&gt; singleton k v2 = singleton k v1 while Just m1 &lt;&gt; Just m2 = Just (m1 &lt;&gt; m2) To the extent each have problems I would expect them to be different. Could you clarify? 
for example, the Scotty Web framework uses State internally to keep track of middleware, routes, etc 
Run a local WebKit and web server, and use html. 
&gt; I have never ever seen anyone refer to those things as a bug, and never seen a definition that includes that. I kindly disagree. In my view, I see the operation of configuring, making and deploying an application as either being part of the application itself, or as a separate application (that you could call an "*application X* builder"). Let me give an example: if I download foobar's source, I expect it to properly compile, assuming that the requirements are properly met. If it does not compile, I will go to their bug tracker and file a bug that says "version x-y-z does not compile". Plain and simple. (of course, I'll ask around on the IRC channel and the mailing list, first) Similarly, an application accepts a malformed configuration file and exhibits undefined behaviour at runtime, I see two bugs: the configuration file is not properly validated (i.e. static validation), and it shows another more subtle underlying defect, that is the code is written in a way that does not prevent undefined behaviour at runtime. Many projects will have categories in their bug tracker system that include "Configuration", "Packaging", "Testing" and even "Documentation". IMO they are all forms of bugs.
I don't think you need to manually thread state between calls, you just need the ability to run IO.
What's the best place to learn how to use reactive-banana right now? 
yeah, but calling it insert might force the user to be use the right one. along with everything built on insert, like union (or if it's more efficient, still uses insert-like merging values). why not have a Monoid-constrained insert, with the "implicit argument" for the dictionary; and an insertBy for the "explicit argument" of appending. consistent with sort/Ord versus sortBy. also, i guess it only needs a Semigroup constraint. you're combing values, not introducing a default one.
Popular, well-regarded definitions of bug do not include compile-time errors (such as the Wikipedia's). The original thread was how some languages detect errors at compile-time and some others at run-time. An error raised at run-time very often produces incorrect behaviour- a bug. Errors which result in non-execution (due to compilation errors) do not produce incorrect resullt. 
Popular, well-regarded definitions of bug do not include compile-time errors (such as the Wikipedia's). The original thread was how some languages detect errors at compile-time and some others at run-time. An error raised at run-time very often produces incorrect behaviour- a bug. Errors which result in non-execution (due to compilation errors) do not produce incorrect resullt. 
Thanks for sharing your experiences, but I'm wondering if you read the article. The reason I've used "FD", instead of "FP", is that I'm focusing on what I believe is a top-down, disciplined, architectural process, that considers multiple functions before it converges on a result of critical functions and types. Similar to how one has to be precise over a number of calculations in say, a calculus problem, where results may be accumulated, and used later to calculate an outcome, hence *mathematical discipline*. One can understand substitution and do as you've described at the end there- without consideration of a larger robust and re-usable goal.
oh I thought it made sense when I read it. (64 -&gt; 64) is ((-&gt;) 64) 64) is a (proxy 64). but I don't know the type system well. 
Ah, the "problem" just being "there are many to choose from" - not anything about the particular choices. I follow you now.
I share Snoyman's thoughts on documentation being versioned files that can be improved via pull requests. I'm building something that takes this logic to the extreme. Stay tuned :)
Sorry, my first post on reddit and I mixed it up. It should be fixed now. It is a true streaming library. Aeson streaming libraries incrementally consume the data and then produce a result. This library takes a grammer and yields data as soon as it is available. E.g. if you have a huge JSON array, aeson streaming library would consume incrementally the input and then produce the big array. Json-stream can incrementally consume the input and produce the items in the array as they become available.
Of those three, I think Mikhail Glushenkov (23Skidoo) is most likely to actively contribute. You could try making a pull request out of that unicode issue diff and see what happens. Was there something specific you wanted changed? 
It has an interface usual for incremental parsing (Done unparsed_data/Yield value next/MoreData continuation/Failed error). Internally it uses C tokenizer - it turned out to be significantly faster. 
Cliffhanger :)
A while back I created the time-recurrence library and put it up on hackage (https://github.com/hellertime/time-recurrence). I haven't looked closely at moment.js but I think time-recurrence is comparable in expressivity. I haven't updated time-recurrence in a while but I'd be happy to answer questions if you find it useful.
I know, I don't like it either, but it's short enough to be workable. I couldn't use attoparsec as it's not quite incremental - it will yield complete result, not partial results. So I had to build my own and in the end it just did millions of allocations which unfortunately counts.
I agree with this sentiment. In the Python world, a tool called Sphinx (http://www.pocoo.org/projects/sphinx/) is often used to write in-tree documentation, and render it (to HTML, LaTeX/PDF,...),... Note Sphinx is not Python-specific, though it has more built-in support for it. I prefer the ReST format it uses over Markdown myself because ReST provides some more semantical markup support, and is more extensible. Tools like ReadTheDocs (https://readthedocs.org/) can automate and centralize these documents, in different formats for different branches etc.
&gt; This is common in some types of game, where your semantics really is "the object changes like this, and then like that, and then like this." Actually, no, I don't think this is as common as you do. I think that the perception that this is common stems from an imperative programming mindset. In the real world you can never look at a ball and say, "hey, at this point gravity has applied, but not yet friction", or "friction gets applied before gravity does", or "when the friction is applied, it does have access to the gravity from the previous point in time, but the y velocity from the new point in time". You can't say that because everything happens at the same time. The real world isn't modifying an object one parameter at a time, it's modifying the entire thing at the same time. Changes are atomic. The closest programming model to this is building a completely new object entirely from *the old* object each frame, not step by step modifying the old object to look more and more like the new object. For building a new object from the old one, `Reader OldObject NewObject` is far more applicable than `State`.
If you like you can use my C-unescape string from my highjson package: https://github.com/agrafix/highjson/blob/master/cbits/unescape_string.c
Indeed. A pull request is a suggested edit that you know will be reviewed, so I think it's psychologically easier to make do that than unilaterally edit a wiki. 
Yes and no. Of course, using reactive-banana for the FRP infrastructure of Threepenny was always a goal. However, to achieve it, I had structured the reactive-banana source code in a way that allows me to easily implement multiple APIs on top of a common engine (the `Reactive.Banana.Prim` module). This way, I would be able to use it for Threepenny, even though the public APIs were different. So, the semantic changes are independent of the needs of the Threepenny-GUI library. Of course, practical experience with the latter does inform the former. 
Is the content same as in Channel 9 lectures several years ago? Any plans for more advanced course?
Isn't this basically what Haddock does for Haskell code?
No. Whilst Sphinx can be used for purely API documentation, it's also useful for more high-level package, or even product, documentation. Haddock is mainly an API documentation tool (there's even a 1-to-1 mapping between Haddock pages, and Haskell modules, which leads to Foo.Tutorial modules).
What is stopping us to say C is a pure language that just doesn't have an opt-out the ST monad?
State or ST? The most obvious answer is that neither allow I/O.
It was very clear and I enjoyed reading it. Thank you /u/kqr !
This is amazing! This is the kind of stuff I eventually want to make when I'm good enough with Haskell.
Yeah, from the constraints library. Essentialy `a :- b` says that the constraint `a` implies the constraint `b`, so in my example I'm saying the empty constraint implies multiplication is commutative. You can't do `(a * b) :- (b * a)` since the (*) type family returns a Nat, not a constraint. And you can't tell GHC that this is true in general, so whenever you need to rely on this fact you need to pass in two proxies for the things you need to commute. If you try using GHC.TypeLits.Nat you are going to end up needing to explicitly tell GHC a *lot* of trivial arithmetic facts whenever you're code needs to rely on them, though a lot of this could be avoided if we get an SMT solver as part of GHC. I don't know if much work is being done on that though, though I really hope so. I would have given an example that didn't use constraints, but that was the only one that came to mind.
&gt; We can think of the time library as having three different kinds of data: &gt; &gt; * Date values, which indicate a specific date (like "25 april, 2009") and are timezone independent. These are represented in Haskell with the Day type. &gt; &gt; * Universal time values, represented with two types: the UTCTime type represents a date and time combination ("25 april, 2009, 13:52:31") and the NominalDiffTime type represents a difference between two UTCTimes. &gt; &gt; * Local time values, represented by the ZonedTime type which links a local time with the relevant timezone. This looks like a good outline for handling dates/times in any language. In a C# project at work, we recently ran into an issue with storing dates as date-times (e.g. Christmas 2015 as 2015-12-25 00:00:00). When converting to/from UTC for presentation to the user, it can produce different dates for people in different timezones. The logical flaw here is that a date is not an instant in time.
Nice tutorial! In the part on converting `Integer` to `NominalDiffTime`, you could also use `fromIntegral`, which is a bit more intuitive (but only works for integral types, so not e.g. `Double`).
The tutorial requests idea is great! As for collecting tutorials, it is worth mentioning there already is a large collection of links to learning resources: [dohaskell](http://www.dohaskell.com/). It is also collaborative, though it works in a different way.
I have since learned that in some situations it's also useful to have a timezone independent non-universal time value, such as for a rule that "the office closes at 17:00" which is true regardless of timezone, and does not reference a specific point in time either. The time library does have a type for that, I just gotta figure out a way of mentioning it in the tutorial without belabouring it. Edit: Although the example of the office closing at 17:00 is not really day-dependent either... it applies to every weekday that is not a holiday. So encoding it as "2005-08-19 17:00:00" isn't particularly useful. You could use the TimeOfDay type alone, but that wouldn't cover the "weekday" aspect of it. I'm sure there's also some sort of similar use-case for a bare LocalTime. Time management is generally just really complicated, and I probably shouldn't include every weird aspect of it in a starting tutorial on the time library so I might just not do it at all. Maybe just a quick note that this tutorial doesn't cover every use case, but should do well with the most basic ones. Second edit: quick note on this added.
Nice, never even heard of this
Before I criticise, I'll just say that I think this is awesome. Swiftly onwards! * Comment your code! * Structure your code with titles! I also found multiple lines like this: interpret '`' = do a &lt;- pop b &lt;- pop if b &gt; a then push 1 else push 0 That could be reduced to: interpret '`' = push . if' 1 0 =&lt;&lt; (&gt;) &lt;$&gt; pop &lt;*&gt; pop if you have if' already defined as: if' :: a -&gt; a -&gt; Bool -&gt; a if' a b bool = if bool then a else b which would greatly shrink and tidy your code.
Wow, this is great! Thanks for the link!
In the previous thread, there were some suggestions about performance issues. In particular the [`thyme`](https://hackage.haskell.org/package/thyme) lib was mentioned. Perhaps adding a note on that or other alternatives would be beneficial. But overall, a great tutorial!
&gt; I have since learned that in some situations it's also useful to have a timezone independent non-universal time value, such as for a rule that "the office closes at 17:00" which is true regardless of timezone, and does not reference a specific point in time either. Shouldn't this be a time with a timezone *location*, e.g. "The stock exchange closes at 17:00 London time"?
Not necessarily. Imagine a multi-national corporation with offices in several cities. Company policy states that the office in each city closes at 17:00 local time, regardless of which city you are in. The example you present would be more usefully stated as "The stock exchange closes at 16:00 UTC" (...during summer. This is where timezone-series makes sense.)
 interpret '`' = push . if' 1 0 =&lt;&lt; (&gt;) &lt;$&gt; pop &lt;*&gt; pop This code is shorter, but hardly strikes me as more readable or "tidier"---and if I'm reading it correctly, it is also not equivalent to the longer, more transparent code in the original.
So the obvious next step is to get this valuable introduction into the time library's haddocks, right?
&gt; Halcyon also seems to trigger more dependency rebuilds than necessary. I'm not sure why. Author here. Can you provide a build log to demonstrate this problem? 
Thanks, I've used it.
Distributed source control really changed the equation by giving open source projects an automatic commit workflow. I can write something on my own system, produce a concrete commit, upload to \*Hub, make a pull request or equivalent, and there's no hassle with permissions, really. In the SVN era you had an impedance mismatch between patches made by a non-committer, which existed as, what, patches floating around in email (and _that_ was often nontrivial to get right too), which had to be manually converted to actual commits. I've often pondered a Wiki that had a similar workflow, but if you start working through the problem the social problems are non-trivial.
Let's generalise this to countIf :: Foldable f =&gt; (a -&gt; Bool) -&gt; f a -&gt; Int countIf cond = length . filter cond . toList 
Heh, I was asking about this just the [other week](http://ircbrowse.net/browse/haskell?id=21263870&amp;timestamp=1439830904#t1439830904). I really find the number of array-like types on Hackage to be a wart. Certainly in the days before `bytestring` existed there was an obvious need. Now, though, we have an over-abundance of semi-overlapping container types with cludgy conversions.
You are missing a constraint.
Alternatively, less generic but more in line with the original function countOf :: (Foldable f, Eq a) =&gt; a -&gt; f a -&gt; Int countOf value = length . filter (==value) . toList
So I get why there exists gregorian and Julian calendars etc, but would it have been too much to ask for a function called fromYearMonthDate You know..something obvious and trivially findable by search!
By the way, great write up. 
Perhaps you are looking for the IO monad + channels. It meets all of your requirements. For a success callback, use the monadic bind (&gt;&gt;=), for errors use Control.Exception.catch, and for last use Control.Exception.finally. To trigger an event just write to the Chan. Is there a particular abstraction you're trying to build. Haskell doesn't need to have the nodejs callback mess if you're just doing IO, since GHC's run-time does it all for you. If you're making something more complicated (like a GUI or advanced networking stuff), perhaps you can use FRP via reactive-banana, sodium, elerea, or netwire?
Great :). I tried to make a PR but my implementation consisted of `non = to . fromMaybe`, and I had a hunch that wouldn't quite cover it. Thanks.
&gt; Smushing unlike concepts into the same implementation is not a valuable thing in itself, it needs to be motivated by a practical desire (type safety, performance, understandability, use patterns). A key observation. More histrionically, I would add: Haskell is a type-centric language, in that code thrives on having many different types to capture subtle yet important distinctions. We &lt;3 types!
&gt;We &lt;3 types! [I love me some types. Please, ask me about types.](https://github.com/bitemyapp/bloodhound/blob/master/src/Database/Bloodhound/Types.hs#L576-L722)
while I agree, I think the focus should be on Haddock. I read most of the documentation I do on hackage or the repo, not the wiki. that's just where following links and googling takes me, and is more obviously related to whatever package I'm trying to learn to use.
Here's what Spock does for SID generation: https://github.com/agrafix/Spock/blob/master/src/Web/Spock/Internal/SessionManager.hs#L339-L343
And with the stack.yaml committed to version control system, will transparently propagate to all the people using it. Including the necessary changes in code for the update, if any.
Congrats on your first package! I've quickly skipped over the code - the most important thing about session management is that it should be safe, and next is has to be fast - because it's executed on every request. Your session management will be very slow due to two reasons: * You access the database to load session information on each hit. This will not perform well - I'd recommend loading all session information into memory and syncing that with a database in the background. This will work fine for most applications (e.g. 50kB per open session * 100 000 users = 5GB) * You don't use connection pooling. If you follow the first suggestion, then this will not be needed, but otherwise every request will first need to open the database before doing anything. Keep in mind that sqlite does not support concurrent reads/writes to well... To address some of your questions: * Housekeeping (removing expired sessions) will have to be implemented something like you described. An alternative would be to lazily delete sessions on requests, but as I mentioned above, you want to be fast so this may not be a good idea. Another approach would be using a different storage backend that supports expiration out of the box, like redis. This would have the advantage that everything already lives in memory. * As persistent abstracts away which database backend is used, you should go that path, too. You should probably store the `Pool a` that each backend gives you after connection and the use runSqlPool function on top of that 
Sometimes `State a ()` is useful just to compose with other computations or combinators that expect a `State` argument.
They are inferred, but they don't match the type he gave. Thats the problem.
Why would you want to write down an incorrect type signature? By supplying a type signature you're saying "this is what it *should* be like" and the compiler will tell you if you're wrong. Constraints are just a part of that.
&gt; I wanted to ask why, as a general rule, the compiler does not automatically set the constraints. It does. But if you try to tell it otherwise, as he has done, it will listen, and complain if necessary. 
cool. is there a good tutorial for "finally tagless" stuff? i hear about it a lot. or should I just read the paper again, which was hard?
Right. Thank you.
next step, a haskell-made English-grammar checker! no but no worries, I love reading your content :) 
I was eagerly waiting for this course last year and in preparation I went through the Graham Huttom book. To my surprise his course turned out to be Graham Hutton light. 
We're still in stealth mode, but thanks for the interest. I'm pretty sure I'll be able to satisfy it in person if you join next year's ZuriHac.
they make good points, but syb is not the best way to deal with nested data boilerplate. lens is. it even has Plated, which subsumed syb afaik. 
&gt; right now, I'm working fulltime on GHC-mod all right!!
What "block" a unicode character belongs to is more or less an accident of history, and is not normally considered to be an intrinsic property of the character, unlike the "category", which in this case is "Letter, Lowercase [Ll]"; in [the GHC dialect of] Haskell (and Java, and many other languages) all characters in this category are considered normal letters which can be part of identifiers, and indeed, the letters in this block are typically used in mathematics as names of variables or objects. The categories "Letter, Uppercase [Lu]" and "Symbol, Math [Sm]" are already reserved as the first character of Haskell identifiers (for constructors and operators respectively), but currently letters which are neither upper- nor lowercase are not used (in fact a strict reading of Haskell'98 forbids them from appearing anywhere in Haskell source)... so how about we change the lambda calculus to the lamed calculus?
Funnily enough I always thought of Stackoverflow as a pretty unfriendly place since half of the google results for questions I search for that lead there have those snotty "we think this is not a worthwhile question so we closed this" markers at the top, not to mention all the hoops you have to jump through to start contributing there. Can't really say anything about Wikipedia though as I have no direct experience attempting to contribute there.
Open question: in light of this discussion, what do you think of the [plans by Stack Overflow](http://meta.stackoverflow.com/q/303865/2751851) to extend their scope to collaborative documentation?
I'm a fan of [reactive-banana](https://wiki.haskell.org/Reactive-banana). It's an [FRP (Functional Reactive Programming)](https://wiki.haskell.org/FRP) library, so you can write your interface more declaratively, and it can be used with various back ends. I use it with wxHaskell, which gives you a native look-and-feel on each platform.
The mathematical lambda is not likely going to find its way into everyday correspondence among Greek speaking peoples, so snatching it up as an alternate to \ is not going to create much of a problem for most folks.
/u/hamishmack Would, I'm sure, welcome your comments and enhancement requests, given his track record of enthusiasm for hearing ways to improve leksah. ([leksah github issues page](https://github.com/leksah/leksah/issues))
Are the margins backwards in this PDF? Or is there supposed to be a smaller margin towards the centre of the booklet? (Yes, I just printed this :p)
The problem is not people using mathematical lambda for Greek, but for mathematics: it could be used for wavelength, for example. Edit: but I agree it's pretty unlikely
Hmm interesting, I guess it didn't exist last time I asked? I'll take a look, hope it does it for now :) * 
The answer to this isn't necessarily more tutorials (though I won't say they're a bad idea), but rather better tooling. Fortunately people are working on it a lot: stack exists now (as others have pointed out), and Cabal is getting features like [immutable package databases](https://www.reddit.com/r/haskell/comments/3ite8n/noreinstall_cabal_a_project_to_move_cabal_to_a/) (finally almost here!) and [Backpack](http://plv.mpi-sws.org/backpack/) (more of a core module system change than just a tooling change, but it'll be nice). The Haskell ecosystem is improving! Yay! :-)
Great video, thanks for bringing djinn to my attention again.
For anyone interested, here's a worked example using, let's say, 50% of the stuff described in this great resource: [scope- and typechecker for STLC that turns "raw" STLC terms with no Haskell-type-level information into a statically-typed representation](http://unsafePerform.IO/blog/2015-02-05-typed_embedding_of_stlc_into_haskell/). In other words, it turns this: data Type = Base | Arrow Type Type type Sym = String data Term = Var Sym | App Expr Expr | Lam Sym Type Expr into this: data TVar (ts :: [Type]) (a :: Type) where Here :: TVar (t ': ts) t There :: TVar ts a -&gt; TVar (t ': ts) a data TTerm (ctx :: [Type]) (a :: Type) where TConst :: TTerm ctx Base TVar :: TVar ctx a -&gt; TTerm ctx a TLam :: TTerm (a ': ctx) b -&gt; TTerm ctx (Arrow a b) TApp :: TTerm ctx (Arrow a b) -&gt; TTerm ctx a -&gt; TTerm ctx b in less than 100 lines (not counting imports / `LANGUAGE` pragmas)
This is amazing. I've spent the last month or two puzzling through stuff like this - googling for mostly-correctly information, stitching it together, occasionally reinventing some things that are either in this paper or are off by some subtle but fatal detail - in order to come up with a nice variant of products / sums / pairings of functors for some things I'm playing around with. I'm happy that I went through that process, but I'm _much_ happier to see this paper. Now I just need to go through and redo a handful of draft blog posts and supporting code to remove the various slightly-off techniques...
It would help if you told us what kind of problems you are having exactly and how to reproduce. I must be doing something differently as I don't experience all those problems people seem to be having with `cabal`. If you're referring to `cabal install &lt;somepackage&gt;` running into a compile error, then please file an issue [here](https://github.com/haskell-infra/hackage-trustees/issues), as then it's most likely an incorrect or missing lower or upper bound somewhere, and we can fix that instance once and for all.
Version bounds are an essential ingredient of course, but they don't need to be *very* restrictive, only as restrictive as mandated by the PVP is more than enough to avoid the majority of compile errors. Sandboxing is currently being abused to elude package reinstalls breaking the package db. But once the no-reinstall package-db is available, we can start focusing on using sandboxes exclusively for their intended use (and tuning the sandbox UI for that), like e.g. for - installing unreleased package snapshots in order to not contaminate your user-global persistent package db with versions not matching their official releases on Hackage - installing packages with special flag settings - testing with locally applied patches/modifications - throw-away experiments - co-developing multiple packages (`cabal sandbox add-source`) - build isolation for buildbots - (maybe) for preparing bindists (relocatable sandboxes) 
Yet the base is full of very generic types. Bool? In a real world you never have Bool, rather you have MyDecision = EitherThis | OrThat. ShouldLaunchMissiles = YesPlease | NOWAY. There's also no Int, instead you have MyStringLength and MyCarWheelRimWidth and PopulationCount. And instead of String you have EmailAddress and FirstName and LastName. Why does base have those types? Having these types in base only encourages new Haskell programers to write badly structured code. 
Nice! And then you can [implement renaming, substitution, printing with names, normalisation by evaluation, etc.](https://github.com/gallais/potpourri/blob/master/papers/models/haskell/Models.hs) in a type and scope preserving manner. :)
After one week of unsuccessful tries with aeson and pipes, I used your library and it worked fine. Thanks! 
I might have selection bias, because quality of questions and answers is much higher for tags [haskell], [elm], [racket], [emacs] and so on, comparing to [html], [css], [php], [javascript], [jquery]. Therefore the friendliness of community around tags that are less popular is also higher, because people aren't irritated by constant flux of low-effort "give me teh codez" questions. I don't search StackOverflow for questions concerning JavaScript, CSS and similar popular technologies exactly because these topics are inundated with lots and lots of similar very low-effort questions. In other words, I don't use StackOverflow (for popular tags) not because it is unfriendly place, but exactly because it doesn't moderate itself aggressively *enough*. That's why when people criticize StackOverflow's elitism, I usually defend StackOverflow exactly from the same positions, as these Reddit comments[[1]](https://www.reddit.com/r/programming/comments/3hqii7/stackoverflow_celebrates_ten_million_questions/cub4ggr)[[2]](https://www.reddit.com/r/programming/comments/3hqii7/stackoverflow_celebrates_ten_million_questions/cuaicep)[[3]](https://www.reddit.com/r/programming/comments/3hqii7/stackoverflow_celebrates_ten_million_questions/cuajxhn)[[4]](https://www.reddit.com/r/programming/comments/3hqii7/stackoverflow_celebrates_ten_million_questions/cua9wsk). But again, my perception is skewed, because less popular tags have both quality and friendly community (seriously, try to ask any question about Haskell or Emacs, you'll get amazing answers), and I would want to preserve that and not have 10 almost exactly the same Haskell questions, where people ask about syntax errors or how to use function composition. I agree that moderatorship can go horribly wrong. I, personally, really dislike [Philosophy.SE](http://philosophy.stackexchange.com), because moderators often butcher your questions by heavily re-editing according to their own personal styles and tastes. They are also very obnoxious. If I only knew the secret of perfect moderatorship.
&lt;plug&gt; You really ought to use my [timezone-series](http://hackage.haskell.org/package/timezone-series) and [timezone-olson](http://hackage.haskell.org/package/timezone-olson) libraries for that. &lt;/plug&gt;
And then what will happen when I search for Gregorian? I don't think including an alias for every possible way to name a function is the right way to fix search. The Gregorian thing fits in best with the conceptual approach of the library. The great new tutorial of /u/kqr now makes everything you want to know about the time library easily searchable on Google :).
There is a huge amount of misinformation about cabal floating around out there. The right way to counteract that is to increase the amount of correct information. The title of this post is a case in point. No one gets "rekt by Cabal" nowadays. They get rekt by wrong information about how to use it.
This is all true, but precisely specified version bounds are far less critical to cabal than they used to be. Nowadays cabal does far better than it used to in the presence of poorly specified bounds. And when cabal is unable to overcome that, it gives you the information you need to figure out what went wrong, and plenty of options to nudge it in the right direction.
re &gt; gives you the information you need to figure out what went wrong Are you talking about solver failures or compile errors? In case of the latter consider e.g. if you get thrown something like the following in your face when you need to use `warp &lt; 2`: Network/Wai/Handler/Warp/Run.hs:286:123: Not in scope: data constructor `KnownLength' cabal: Error: some packages failed to install: warp-1.3.10.2 failed during the building phase. it's not so obvious to know which package I need to nudge to a different version (my wild guess would be a missing lower bound on `wai`, but is this something you expect an untrained user to investigate?) to get this to compile again. 
&gt; I would never dare to make publishable research utilising Haskell. Do you mean because of your personal ability/knowledge level in Haskell, or because of how it would be perceived by others in your scientific community?
Sensible, though it would feel weird writing something like `(x =)`, given the usual Haskell meaning of `=`. By following you I'd end up using `(≡)`.
A little bit of both to be honest. It's a Catch 22 scenario: Haskell does not have great and grounded reputation, because it was not used in any highly regarded research (Say NASA, ESA, Fermilab or CERN grade, at least I don't know about it)… therefore no-one wants to risk it and use for priority research. As far as my confidence: I was audited and allowed to run computations in C99 on various supercomputers before I have turned twenty. It might be purely psychological, I do acknowledge that, but I am far from being strongly confident about my Haskell skills. After three years I am discovering new amazing stuff on a daily basis, meanwhile it happens rarely in C or C++. If it is of any merit to you, I am developing a prototyping codebase for heat dispersal problems in Haskell. When it is going to be ready for a Github release, could I count on some feedback / corrections from people here? Let's say that I can take care of the maths, but my code will likely require all the help it can get :P.
This looks very interesting. Are there slides available somewhere?
I'm going to bring this out again: [from my friend who tried to code a project in Haskell many times but couldn't get cabal-install to work](http://i.imgur.com/194Bb0c.png). I also had huge problems with the user interface and semantics of cabal-install when starting until I eventually learnt to deal with it. Tools we expect beginners to use should have walk up usability. Stack move way more towards this by installing ghc for them, ensuring packages install &amp; work together and only requiring one command to get started (rather than many).
I really wish the GUI ecosystem in Haskell was more mature. From the research I've done, it seems like high quality GUI toolkits are successful when a large organization dedicates tens of thousands of man hours to create it. As an aside, does anyone have any experience using haskell as a *backend* to a desktop GUI, with the UI code written in another language (like Python or C++)? I currently use Python with PyQt and RxPY to make GUIs, but it would be cool to use Haskell with a FRP library along with PyQt instead.
It's also not a good idea when you store more than very little data in the cookie, because it always needs to be sent to your server by the client for every request. 
If people are still constantly messing up their usage of cabal-install, then it needs a better interface and/or defaults.
The canonical [lecture notes](http://okmij.org/ftp/tagless-final/course/) are on Oleg's website.
The last time I read anything truly _academic_ about psychology it was a different century, so perhaps I should have said "socially" rather than "psychologically". No I don't think it's about recognition it's about social confidence. I have permission to edit the Haskell wiki and Wikipedia but I've made very very few edits and every time I feel like I'm not sure I have the right to do so, even though I know that I've been given write permission because my contribution is welcomed. When I make a github pull request, I feel more confident. I'm confident that I have a good idea, but crucially I'm confident that because my idea can be rejected if it's unsuitable for some reason, I can't spoil something that's largely someone else's work. It feels much more polite to make a request than to make an edit. 
I mentioned this on twitter, but in case it's missed: the "Logging" section compares hslogger to Python's "ConfigParser", but it should be mentioning the Python "logging" module instead.
[Yeller](https://yellerapp.com/) may be relevant under the Debugging section. It's a production error collection webservice (compare to Airbrake or Sentry) that has native support for GHC 7.10 stack traces, as mentioned on the [Yeller Blog: How To Debug Haskell in Production](http://yellerapp.com/posts/2015-02-24-haskell.html). Their client library is on Hackage: [yeller](http://hackage.haskell.org/package/yeller) I've never used Yeller, I just think it's pretty cool that a service like this supports Haskell.
I wrote a library called [Threepenny-GUI][1] which uses Haskell on the "backend" and a browser window (JavaScript) on the frontend. It's sort of a JavaScript FFI. It also has some FRP support out of the box. It's experimental, but people are already doing things with it (see the [Gallery][2]). I'm sure it can be hooked up with Electron, but I haven't put in the time to do that yet. [1]: https://wiki.haskell.org/Threepenny-gui [2]: https://wiki.haskell.org/Threepenny-gui#Gallery
I like the mention of RxJava in this article. If you are someone like me who loves Haskell but programs in other languages for work, take a look at http://reactivex.io/. It is a reactive programming library that is implemented in many languages. Right now I am working on PyQt-based scientific software, and I am retrofitting it with RxPY. It is amazing how much signal-based and concurrent code I can *delete* from the codebase because of how simple RxPY makes things. I think that if Haskell doesn't catch on soon, then the next best thing is libraries in other languages (or built-in language features) that are inspired by Haskell. Of course, I would rather use Haskell itself :)
I'm posting this link here because despite being written in OCaml, Nebula borrows (through its own implementation) a lot of idioms that are common in Haskell-land: - Free monad based DSL for describing interaction with the processor and memory - _IO_ type for tracking effects (also built on the free monad) - Immutable data structures Comments, feedback, pull requests, and pitchforks are very welcome.
Let me rephrase that. When you do PR you aren't afraid that you are “stepping on someone else's turf”. You shift responsibility of figuring out whether your contribution is good to other people (maintainers). Even if your contribution is crap, there will be less flak from other people, so proactively contributing feels safer. Do I understand right?
I'd definitely check out your heat dispersal problem in Haskell. That sounds very interesting!
You can also, with `-XPartialTypeSignatures`, write count :: _ =&gt; a -&gt; [a] -&gt; Int which tells the compiler to use inferred constraints but lets you write only the main part of the signature.
I tend to use `State` when I find that I'm writing something in explicit state-passing style; that is to say, if I find myself writing exactly the boilerplate that `State` is intended to eliminate, especially throughout a large swath of my application, I use it. The more I program in Haskell, the less frequently I find myself trying to use state-passing (favoring things like monoidal operations or convenient folds).
I'm one of those cavemen software types. I started from a hardware background and built up to C, so I love C. Recently I started checking out the functional paradigm and I love it. It actually is changing how I go about C. I might be naive, but I really like the Haskell approach over a lot of the others like Go. With C, I feel like I can look at it and see down to the processor, which is where I feel at home. Haskell does the opposite, and I feel like I can see up to mathematics and logic pretty easily. I'm not expecting it to all be flowers and puppies in Haskell, but I just like the way it feels. I'm with you on the treating programmers like they can't learn Haskell. It might have tough patches, but despite that just learning it is a good didactic tool for programming. I'm already thinking of how it might be fun to play with this and the likes of Verilog!
Not extensive experience or a working application, but I tried gtk2hs and it was actually pretty nice. Separates out the GUI code into an XML file which you can quickly design in [Glade](https://glade.gnome.org/), and you write a short imperative `main` that hooks up widgets to functions. [Tutorial here](http://projects.haskell.org/gtk2hs/docs/tutorial/glade/). FRP is probably better in the long run but I haven't tried it.
&gt; we should not be putting such barriers in front of new users. I switched from cabal sandboxes to stack a month ago and I've been happy with the switch. But I was never an expert at using cabal or ghc, and there's still a ton of options that confuse me when I'm using stack. I'm hoping some more suggested examples of using stack will make it into the guide eventually, from experienced users of all the internal tools (ghc and cabal) that can help new Haskellers not only avoid the pitfalls of the haskell ecosystem, but invoke stack for different tasks in order to "automagically" use the tools underneath stack *effectively.* For instance, if I want my binary to be optimized, what should I pass into `stack build`? `--no-library-profiling` and `--no-executable-profiling` (or are they the default with `stack build`)? `--optimizations` (or is that also the default)? The new guide helped me understand more about how stack handles some GHC options with the `stack build --ghc-options=-O0` example, but I still don't really know how it interacts/deconflicts with GHC options specified in my project's cabal file; e.g., if I have`ghc-options: -Wall -threaded -rtsopts -with-rtsopts=-N` (there's no `-O2`) and invoke `stack build --optimizations` then what is GHC actually invoked with? I know with patience I could probably use `-v` to find out the answers to each of my questions above, but that's not actually my point :) What I think would be useful, is a list of examples of how to use stack to accomplish different things "efficiently", e.g., * All the suggested flags to optimize your final binary for speed * All the suggested flags to speed up the test-edit-recompile loop: here I have similar questions to my above examples, e.g., does using `--file-watch`imply `--no-haddock` (and maybe `--no-optimizations`) so that stack automatically does everything possible to speed up the recompiliation step, or should I manually pass in those flags as well? * Suggested flags for cross-compiling a binary for a different platform * Useful stack flags for benchmarking, for a CI server, etc... 
Yeah, I'm crazy about Rust, but these lies need to stop.
Yes, 100%. You're right: we need an examples section in this guide. Could I ask a favor? Please make a pull request with such a section, and describe some things you'd want to see in there. If you don't know how to make them work yet, that's fine, others (myself included) can help fill them in. What's invaluable is getting feedback from newer users telling us what is not clear. (I think the optimizations flag was actually an accidental holdover from an old version, and isn't being used. Thanks for bringing it up. __EDIT__: https://github.com/commercialhaskell/stack/pull/892)
Agreed. Can you open [a github issue](https://github.com/commercialhaskell/stack/issues/new) about this?
It is frustrating to see decisions that language and library designers make, especially after using a language that gets it right! The sentiment expressed on the lambda-dev mailing list is alarming &gt; We are not trying to shoe-horn in an option monad. and I cannot help but remember similar sentiments expressed on es-discuss regarding JavaScript Promises. Why are designers eschewing proven patterns? Why ship code that * Breaks parametricity * Conflates map, flatMap, etc. Functors, monads, and applicatives are too political?
Wasn't possible when I interviewed. Java, python, C, C++, all of which I'm mediocre at, so I didn't bother. You also have to do it in a google doc.
How does that ownership model for memory safety factor in when two threads are accessing the same memory?
Great question. The borrow checker is somehow able to reason about who has access to that memory. (The computer science behind this is above me, I'm afraid, but I believe it uses regions?) It will complain when you have concurrent writes, that is multiple mutable references (`&amp;mut ref`) to some memory, as it cannot guarantee this to be safe. Concurrent reads alone are, of course, not problematic. (Feel free to correct me if I'm wrong about this, /u/steveklabnik1)
The [Rust Book chapter on Concurrency](https://doc.rust-lang.org/book/concurrency.html) describes Send (the trait for things that can be transferred across threads) and Sync (the trait for things that can be shared across threads), and shows how they play out. The [Sync trait documentation](https://doc.rust-lang.org/std/marker/trait.Sync.html) has some more in-depth details of why some of this works in particular cases you may not expect.
Minor nitpick: Nullability annotations and tools that check them predate Java 8 by quite a bit.
Oh nice I needed rxpy about a year ago but it was not released yet, or at least I couldn't find it on pypi. Will have to go back and fix my threading mess now.
Port of https://github.com/I3ck/lib_2d I'm currently working on Feel free to tell me about coding-style-mishaps and whatnot, so I can improve
This is close but not _exactly_ true. Rust-the-language has no concept of threading. However, the end result is what you state: at compile time, you cannot have two mutable references to a single thing, from one thread or from many threads. See also /u/mypetclone 's comment about `Send`/`Sync`, as well.
You are basically right. The "borrow checker" ensures that two threads cannot both have access to a mutable reference and thus cannot write to the same memory space. This solves the usual concurrency problem of shared mutable state, which Haskell solves by not having any mutable state, or hiding it behind `stm`.
Why don't we hear about them? And why do we see so many "undefined method foo for Null" error messages in the wild? Are these tools just not very popular?
Haskell mode already supports Stack to some extent (see https://github.com/haskell/haskell-mode/issues/744). Or are you wanting something else?
Oops. Misread the topic. If not using ghc-mod, then what do you expect of stack integration?
Ok, thanks!
This looks pretty nifty! I'll have to give it a look through.
I think your Haskell code needs a few checks for undefined. After all, it inhabitates all Haskell types.
Thx, i'll check it out. 
Something I really like about Haskell (well, GHC at least) is that it's also not that hard to see all the way down to the machine. Once you learn a little about the way the GHC pipeline works and how to dump the intermediate representations, you can pretty easily trace all the way down to assembly and even make effective optimizations, usually without making your code much or any less high level.
I like the parsing / prettyprinting category, it's amazing how many languages SUCK at this. JavaScript, I'm staring at you.
Some kind person issued a pull request for that, so there should be rss and atom feeds available when you visit [here] (http://dlaing.org/cofun). There's quite a few more posts in that queue, but I also have a couple of other projects I want to blog about. Hopefully I'll work out how to aggregate them before they go live :) 
I really like the sound of that. 
I **haven't** seen "so many" in the wild terribly much in the three years I've worked at a large company that uses Java for just about everything. I **have** seen tons of them in frontend JavaScript where such tooling is lacking (should be using Flow or TypeScript, but inertia is highish), despite there being much less frontend code. Concerning tool popularity: If you're working in Java, your IDE almost certainly has good support for null inference and potential NPE warnings. IntelliJ IDEA has had support since [2006](http://blog.jetbrains.com/idea/2006/03/detecting-probably-npes/) with that maturing over time in various ways. Eclipse supports them as well but I have no clue about the history of them.
Can anyone show a concrete application of this result? o.O
How would you use Haskell to implement something like that?
Thank you.
Yes, that is what the algorithm is based on. I managed to mess up the names though.
yay animated gif documentation 
I think this quote says more about the expectations of the software industry than about Haskell. Can you think of another engineering field where a solution that is recognizably better than current practice would be dismissed solely because of lack of familiarity...?
&gt; When you do PR you aren't afraid that you are “stepping on someone else's turf”. Yup. &gt; You shift responsibility of figuring out whether your contribution is good to other people (maintainers). No, I'm confident my contribution is good, but the fact that the maintainers get to say no means I don't have to worry. It's like asking if you want me to power wash your drive rather than just doing it. Even though I can't imagine you preferring your drive grubby or turning down a free clean-up, I still don't want do just go and fix it without your OK. &gt; Even if your contribution is crap, there will be less flak from other people, so proactively contributing feels safer. I don't fear flak, because I think it's unlikely, but I would nevertheless like to avoid causing silent irritation. Maybe it's just a British thing to want to avoid doing something in case someone doesn't like it _and does nothing whatsoever_ to express that!
I have a hard time imagining this being useful in the general case. In the specific case of records, I think a helper constructor function (or several) is easier, more explicit, and more usefully scoped: data Address = Addr { street :: String , city :: String , state :: String , zip :: String } addrLA :: String -&gt; Address addrLA street = Addr street "Los Angeles" "CA" "90066"
ok, got it. thanks.
...power generation?
Cool. Have you seen Sunday's Quick Search algorithm? http://www.researchgate.net/publication/220424762_A_Very_Fast_Substring_Search_Algorithm What is your average case complexity?
been using doctest for my project. parsing comments / interpreting tests is slow, and typos in a setup comment make all the other tests fail weirdly. but I can't give up checked documentation :) 
&gt; When the Java 8 library team was designing Optional there was some opposition to the idea that it should contain some useful methods (essentially Optional.map and Optional.flatMap) on the somewhat spurious grounds that they didn't want their Optional to be a Monad. When I pointed out that it is anyway whether they liked it or not, they relented and put them back. Wow...&lt;shakes head&gt; Functor, monads, etc are the *physics* of programming...fundamental structures that come up all over the place in our universe. Somehow we need to figure out how to communicate this to non-Haskellers.
Aeronautics?
[Sad but true.](http://www.smh.com.au/content/dam/images/g/h/m/1/a/q/image.related.articleLeadNarrow.300x0.ghlhxq.png/1434023422014.jpg) (that's a reference to Australia's current prime minister, by the way)
 if (foo == null) AFAICT you don't need this because Optional isn't allowed to contain `null`.
Nope.
But `null` isn't undefined. `null` has an identity -- indeed, that's *all* it has, since asking for its identity is all you can do with it. It's the unit value that thinks it's a member of the bottom type.
Metric
God I hate Java. 
I'm pretty sure it's implemented in Clojure, since most of the articles on the Yeller blog are about Clojure.
I still think the mutpocalypse went the wrong way...
Say more! A comment like this provides no insight.
I stopped reading when I got that this was the gist of the article. Haskell is really not all that difficult, and it is weird that people successfully learned it and made something useful with it and still drop that the learning curve theme over and over. If they can use it, so can you.
As an embedded programmer who writes C for a living, learning Haskell made me realize how crappy C is.
Nothing like keeping it simple. I like it.
O(length needle + length haystack) if not found, O(length needle + indexOf needle haystack) if found. I'm not sure it makes a whole lot of sense to talk about averages here.
Eh, I use metric for anything that matters, but the rest of my daily life is a silly mix. “Add about 100 grams of flour and a couple tablespoons of butter. Hm? The library? Yeah, it’s a half-mile down the road, just a few meters from the post office.”
Have you spent any time learning some of the Haskell EDSLs for generating and verifying C code?
Learning Haskell definitely improved my C(++) fu. I didn’t grok concurrency in imperative-land until it clicked in Haskell and I smuggled it across the border. What’s especially neat is that Haskell and C are quite similar in terms of code organisation: it’s just data and functions, with a light touch of encapsulation—modules in Haskell, `static` in C.
Is there some paper with the details and semantics behind the implementation of borrow checker?
Facebook also uses OCaml and D (to name some niche languages) and they wrote their own language or two. I haven't heard about any major projects in Go, Scala or Rust but I wouldn't be surprised if they had some. Why is having *a* project in Haskell a big deal? Extrapolating from a single data point isn't a great predictor of the future! IMHO "Facebook's New Spam-Killer hints at the Future of Coding, and that future is functional" would be a better article. (and not necessarily in Haskell; functional and sometimes pure libraries are popping up in pretty much every language)
Aren't those completely opposing definitions of useful, i mean assuming we found the perfect language for each purpose the one seen as most useful by those who want to get work done and the one seen as most useful by those wanting to find a maximum amount of work in it would be on completely opposite sides of the spectrum. I agree that Java is very good at turning something into a lot of work that could be done in less work in other languages but that hardly makes it a good language.
This looks really cool. Waiting for the tutorials! Will this be usable to someone who has very little OpenGL background? Is learning some OpenGL first a prerequisite?
My point was that a language creating work is not necessarily the language anyone who needs to get work done wants to use. They are conflicting goals. That means the language likely to have lots of jobs is likely to be among the least efficient languages. I would disagree on the maturity of the Java ecosystem too. Yes, there are lots of libraries, but a lot of them are completely over engineered in some ways and broken in others. Java is certainly better there than some other languages, e.g. PHP or Javascript.
How much does GPipe rely on ContextHandle internally? Can I mix SDL with GPipe and avoid the context entirely? Or do I have to provide a SDL ContextFactory and ContextHandle? What I want is to manage the OpenGL context with something like SDL, and substitute my OpenGL code with GPipe.
s/ambivalence/ambiguity/ So so sorry couldn't stop myself. 
I didn't really learn concurrency from my brief dip into haskell (that just came from do a lot of multi-threading in Python... because threads.), but it did teach me to think functionally. I used to have a really hard time with the idea of recursion as well. Anymore, I stand by the idea that even if you plan to stick with OOP, learn you some haskell for the greater good, because it will teach you a thing or two even if you only scratch the surface.
I'm familiar with that. My point was that 1. an `Optional&lt;Foo&gt;` itself can be `null`; 2. after I've checked that the `Optional&lt;Foo&gt;` contains something, which of course by construction is non-null, I have no way of preserving this information of non-nullness, because if I extract that value it again is a nullable reference of `Foo`. `@Nonnull` annotations should help in those cases, though. As said before, I did not knew about them previously. Ideally, I'd like to have types for non-null-references.
See my answer to [this comment](https://www.reddit.com/r/haskell/comments/3j8n1s/why_javautiloptional_is_broken/cuny3co). `foo` is still a nullable reference, and in the `…` code in between someone could have nulled my reference.
[frisby](http://hackage.haskell.org/package/frisby-0.2/docs/Text-Parsers-Frisby.html) also makes use of mfix for defining PEGs. It's a bit dated now. John Meacham did a great job documenting both the API and the source though.
NixOS seems to support Haskell pretty well, how does this work? Is it because Nix hashes the Nix derivation and not the binary?
Maybe they meant Erlang? Rust is certainly shockingly fast...
&gt; When the Java 8 library team was designing Optional there was some opposition to the idea that it should contain some useful methods (essentially Optional.map and Optional.flatMap) on the somewhat spurious grounds that they didn't want their Optional to be a Monad. When I pointed out that it is anyway whether they liked it or not, they relented and put them back. What kind of reasoning is that? What were they thinking, "Let's not be unpopular like Haskell?" Is it really that case, or are people scared of making Haskell popular? I read the rest of the emails, and eventually found this: &gt; The goal is not to create a functionally-oriented Java. I found this funny. Java will never become completely 'functionally-oriented' because it is, at it's core, an imperative language. Besides, they've already added lambdas! Why not do more?
Not if you make it final, which you should
JPL uses Haskell internally.
Photoshop? Pro Tools? Auto CAD? Maya? Premiere? A proper GUI toolkit is still needed for a lot of serious applications, and it's sad that the Haskell ecosystem is weak in that area.
Why the limitation on URL rendering? Maybe I've misunderstood something but it looks like Hamlet.RT can do that, no?
The Rust language provides [atomics with different memory ordering semantics that are specified in terms of threads](https://doc.rust-lang.org/std/sync/atomic/), so I don't see how it's possible to claim that Rust has no concept of threading.
Hamlet.RT can do lots of more complicated things, but I didn't think there was much of a purpose to type-safe URLs in runtime Hamlet. The equivalent can be achieved with: Map.singleton "urlName" $ toHamletData $ renderUrl url I'm not opposed to adding back some kind of type-safe URL handling, but it will complicate the API, which I was trying to make as simple as possible here.
Ah, that's a good point! Second question: how do I get my local project to depend on the GitHub version of this? I would have thought the following in stack.yaml (under `packages`) would have sufficed: - location: git: git@github.com:yesodweb/shakespeare.git commit: 1271773aa6d283e38ef6a86ef833a5eff5a10f9b valid-wanted: false but when I've run `stack build` and then attempt to `stack ghci` it throws a wall of C preprocessor errors longer than my terminal scrollback at me. A taste: In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:178:0: warning: "CURRENT_PACKAGE_KEY" redefined #define CURRENT_PACKAGE_KEY "shake_6IffMW0Y15T5GJtesyz7Nf" ^ In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:213:0: note: this is the location of the previous definition #define CURRENT_PACKAGE_KEY "twowr_5oHO9FBrLshIPESwIds0fh" ^ In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:178:0: warning: "CURRENT_PACKAGE_KEY" redefined #define CURRENT_PACKAGE_KEY "shake_6IffMW0Y15T5GJtesyz7Nf" ^ In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:213:0: note: this is the location of the previous definition #define CURRENT_PACKAGE_KEY "twowr_5oHO9FBrLshIPESwIds0fh" ^ In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:178:0: warning: "CURRENT_PACKAGE_KEY" redefined #define CURRENT_PACKAGE_KEY "shake_6IffMW0Y15T5GJtesyz7Nf" ^ In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:213:0: note: this is the location of the previous definition #define CURRENT_PACKAGE_KEY "twowr_5oHO9FBrLshIPESwIds0fh" ^ In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:178:0: warning: "CURRENT_PACKAGE_KEY" redefined #define CURRENT_PACKAGE_KEY "shake_6IffMW0Y15T5GJtesyz7Nf" ^ In file included from &lt;command-line&gt;:10:0: /home/kqr/documents/haskell/two-wrongs-st/.stack-work/dist/x86_64-linux/Cabal-1.22.2.0/build/autogen/cabal_macros.h:213:0: note: this is the location of the previous definition #define CURRENT_PACKAGE_KEY "twowr_5oHO9FBrLshIPESwIds0fh" ^ [ 1 of 26] Compiling HamletTestTypes ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/test/HamletTestTypes.hs, interpreted ) [ 2 of 26] Compiling Text.IndentToBrace ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/IndentToBrace.hs, interpreted ) [ 3 of 26] Compiling Text.MkSizeType ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/MkSizeType.hs, interpreted ) [ 4 of 26] Compiling Text.Shakespeare.Base ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Shakespeare/Base.hs, interpreted ) [ 5 of 26] Compiling Text.Shakespeare ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Shakespeare.hs, interpreted ) [ 6 of 26] Compiling Text.Hamlet.Parse ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Hamlet/Parse.hs, interpreted ) /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Hamlet/Parse.hs:24:1: Warning: The import of ‘Control.Applicative’ is redundant except perhaps to import instances from ‘Control.Applicative’ To import instances alone, use: import Control.Applicative() [ 7 of 26] Compiling Text.Css ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Css.hs, interpreted ) /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Css.hs:15:1: Warning: The import of ‘Data.Monoid’ is redundant except perhaps to import instances from ‘Data.Monoid’ To import instances alone, use: import Data.Monoid() /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Css.hs:23:1: Warning: The import of ‘Control.Applicative’ is redundant except perhaps to import instances from ‘Control.Applicative’ To import instances alone, use: import Control.Applicative() /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Css.hs:349:40: Warning: Defined but not used: ‘subblocks’ /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/Css.hs:367:5: Warning: Defined but not used: ‘subGo’ [ 8 of 26] Compiling Text.CssCommon ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/Text/CssCommon.hs, interpreted ) [ 9 of 26] Compiling Text.Shakespeare.BaseSpec ( /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/test/Text/Shakespeare/BaseSpec.hs, interpreted ) /home/kqr/documents/haskell/two-wrongs-st/.stack-work/downloaded/d1b49062c73fc55a8219f57d3bfd7ba01af3cb8a5ff810d53e90a03ed64baf09.git/test/Text/Shakespeare/BaseSpec.hs:43:26: Exception when trying to run compile-time code: test/reload.txt: openFile: does not exist (No such file or directory) Code: do { shakespeareFileReload (defaultShakespeareSettings {toBuilder = Language.Haskell.TH.Syntax.VarE 'pack, wrap = Language.Haskell.TH.Syntax.VarE 'toLazyText, unwrap = Language.Haskell.TH.Syntax.VarE 'fromLazyText}) "test/reload.txt" } In the splice: $(do { shakespeareFileReload (defaultShakespeareSettings {toBuilder = VarE 'pack, wrap = VarE 'toLazyText, unwrap = VarE 'fromLazyText}) "test/reload.txt" }) Failed, modules loaded: Text.Shakespeare.Base, Text.Shakespeare, Text.Hamlet.Parse, Text.Css, Text.MkSizeType, Text.IndentToBrace, Text.CssCommon, HamletTestTypes. *Text.Css&gt;
Quick Search (and similar algorithms like Boyer-Moore, in which the idea originated) are interesting because they have an average case time complexity of O(n/m). In other words, the longer the pattern you're searching for, the faster your search is expected to run!
These both look like `stack ghci` issues, I can't speak to either of them confidently, but: 1. The first looks like some funny business around fixes `cabal_macros.h` due to changes in how Cabal 1.22 works 2. The latter looks like a GHC limitation around working directory for TH code I'm surprised that `stack ghci` is trying to load these like that though. That's probably worth a bug report (extra-dep: true should prevent that I think). What happens if you just run `stack build`?
No. The *compilers* are mature, proven and battle-tested. I cannot seriously call Java "mature".
That is exactly what I`m looking for ! 
Seems to work with `stack build` and then running the executable.
Why do you need him to build the tool? Can't you just provide some binary? I'm assuming the configuration file is read at runtime.
Personally i have had a lot of issues with everything on Windows. Now i just use vagrant and work in Linux.
[What next standard](https://www.reddit.com/r/haskell/comments/3bnz7a/whats_taking_the_new_standard_so_long/)? Despite [Edward Kmett's comments](https://www.reddit.com/r/haskell/comments/3bnz7a/whats_taking_the_new_standard_so_long/csnw2tw), I fail to see any evidence of a conversation which hints at a new committee having formed let alone a new standard being in the works. A lack of reference material/conversations makes it difficult to take what you say as more than speculation, despite your prominence in this community.
Nix actually does hash the binary, and GHC's non deterministic output does bite sometimes.. It works because Hydra builds all of the packages and supplies a binary cache, but unfortunately, if some libraries are built from source on a user's pc and others installed from the hydra cache, mismatches in the expected hashes will cause later builds to fail to install. The only thing that can be done about it in the present is remove all of the Haskell packages and run the garbage collector, then either be careful to build everything locally or only install caches for Haskell programs/libraries. It's a painful and unfortunate situation, but until GHC's output is deterministic there isn't anything that can be done about it other than the described workarounds. Hoping GHC 7.12.1 will be a step in the right direction!
AJHC, UHC, and LHC.
Just add the extensions you want by default to your .cabal file.
It's a bit difficult for me to provide proof here, because there is no committee yet, and a lot of what I know was provided to me in private conversations, so I'm not sure whether I would rain on someone's parade by disclosing their personal heavily-work-in-progress pipe dreams. Herbert in particular mentioned a couple of times that he would like a new standard developed, and there is a "roadmap/wishlist" (on someone's personal Google Docs account if I remember correctly) of things left to do before we can officially announce Haskell 2020. The name is a combination of not dating the standard too early and a pun on 20/20 hindsight, so take it with a grain of salt.
That first example of using `fix` to guess numbers blew my mind.
Not built in, no, but it's just a matter of getting all of the packages in the snapshot and running `stack build $(cat packages-in-snapshot)`. I think providing some means of getting the list of all packages in a snapshot could be useful. That said: this likely isn't what you want, the number of system libraries you'd need available is very high. If you're still curious: https://github.com/fpco/stackage/blob/master/debian-bootstrap.sh
Math.
For a CLI tool that doesn't have tricky dependencies (extra C libraries), you can build suitable binaries in Wine on a non-Windows machine. Otherwise yes, stack would be a good solution. There doesn't need to be much knowledge of the windows command line from either of you - just how to open it, then standard stack commands. Note on windows stack installs git as well as ghc. It probably installs a bash too, so `stack exec bash` might get you a more familiar shell prompt.
I'm torn.
That's repeating the guarantees that those intrinsics give, but threads aren't part of Rust's type system, and we don't do any reasoning about threads, specifically.
That's truly insightful, thank you for the response! :)
* Haskell: https://chocolatey.org/packages/ghc/ * (Git) Bash: https://chocolatey.org/packages/git * Console 2, a tabbed terminal: https://chocolatey.org/packages/Console2 Or just VirtualBox + Ubuntu, `apt-get install ghc cabal-install`
Does the file sharing between Virtual box and Windows works ?
It's great to see that there are Haskell positions in Italy. I hope that in the future someone from Create-Net could deliver a talk at a Haskell ITA meetup.
&gt; It is indeed very rarely that one needs MonadFix; and for that reason, non-contrived cases where MonadFix is needed are quite interesting to consider. I don't know that this is a "needs" case, but I've used MonadFix in the past to compose reactive GUIs from widgets of type `Behavior output -&gt; UI (Event input)` in situations where (1) there's not a linear cascade of inputs to outputs, or (2) the `UI` actions need to occur in a different order than the flow of inputs to outputs, because the framework uses monadic ordering to build the layout. I don't think I have any examples lying around, but it can result in really nicely organized code. Unfortunately, you have to be very careful about laziness, and it's difficult to debug situations where you're evaluating the recursive values at the wrong time.
The `turtle` and `pipes` package are two examples. They both keep the tutorial in a separate module. For shorter examples see the `foldl`, `total` and `mmorph` packages.
If your program has no C library dependencies, using `stack` from the command prompt should work perfectly. The only "technical" thing is setting up the %PATH% variable and copying `stack.exe` to the correct location; otherwise your instructions will be "open this zip file; shift-right-click on the folder and select 'open command prompt here'; type `stack setup &amp;&amp; stack install`." If installing `stack` is a problem, you can try [the installer I wrote for it](https://github.com/conklech/stack-installer), which is a bit outdated now (I haven't made a new build with the current version). If you *do* have C library dependencies (e.g. you use `text-icu`), it'll be less easy, but still doable.
Yessss, I hit this lib some weeks ago when I was looking for an elegant GL wrapper, but was a little disappointed by the fact that it seemed a little out of date and it was not using ekmetts `gl` library (which turns out to be just an implementation detail for the toy game clone I was planning to do). I stuck with [`Gloss`](https://hackage.haskell.org/package/gloss-1.9.4.1) and it's enough for my Tile game at the moment, but I will definitely look into `GPipe` if need be.
I don't think it was random schedules. I think it was more along the lines of QuickCheck, where it generated some specific schedules for you that covered a large portion of the possible schedule space.
I think that dismissing Coq because it is tactics-based is a bit unfair given that the natural language-style of the proofs is quite tactic-y in a sense: * apply induction * Base case - assert [0] - use [append nil] with arguments ([]). * Inductive case - assert [1] - etc. I was going to translate this to coq, with a nice pattern-matching definition using Sozeau's [equations](http://www.pps.univ-paris-diderot.fr/~sozeau/research/coq/equations.en.html) plugin but couldn't install coq, equations and coqide so gave up. Apparently: &gt; Incomplete LablGtk2 (via ocamlfind): no /home/gallais/.opam/system/lib/lablgtk2/gSourceView2.cmi. :( Edit: so, after quite a bit of a struggle, I have finally managed to get everything working. Here is the script mimicking the proof in the blog post with fancy `Case` construct included: Require Import Equations. Require Import List. Require String. Open Scope string_scope. Ltac Case str := let val := eval compute in (String.append "case " str) in idtac val. Equations append {A : Type} (xs ys : list A) : list A := append A [] ys := ys ; append A (cons x xs) ys := cons x (append xs ys). Theorem append_neutral : forall {A} (xs : list A), append xs nil = xs. Proof. induction 0 as [|hd tl]. - Case "[]". assert (H0 : @append A [] [] = []) by apply append_equation_1. exact H0. - Case "hd :: tl". assert (H1 : append (hd :: tl) [] = hd :: append tl []) by apply append_equation_2. assert (H2 : append (hd :: tl) [] = hd :: tl) by (rewrite H1, IHtl; trivial). exact H2. Qed. `Equations` lets you write defining equations à la Haskell and provides you with various lemmas (`append_equation_1` and `append_equation_2` here) stating that the corresponding reductions hold propositionally.
Based on interaction on this thread and elsewhere, I wrote [a brainstorming post](http://meta.stackoverflow.com/a/304112/596361) about StackOverflow documentation proposal. As a big fan (but who only knows a little yet) of cognitive psychology, I want to understand the motivations and psychological processes, which go through somebody's mind, when they decide to contribute or not contribute.
I started out using OpenGLRaw but changed to ekmetts gl library when I saw that OpenGLRaw wasn't included in the latest LTS on stackage. Ironically, now hackage is unable to build GPipe due to the dependency on gl instead...
Yeah, what the hell? I've tried so many times to get a handle on what `fix` does and how (and why) to use it. Why isn't stuff like that a more common example? Edit: Okay, I've calmed down a little and got a handle on my excitement. The type of `fix` is `(a -&gt; a) -&gt; a`, which specialises in the case of the example to (IO () -&gt; IO ()) -&gt; IO (). Clearly the argument is the right type. The definition of fix is `let x = f x in x` which is a bit wtf for me. But we can just start by asking: what is x? Well, x is `(f x)` which is `(f (f x))` and so on. In an eager language, that construction would never make sense, *but* we are lazy so we evaluate outward in. So to evaluate `f (f (f (f ...)))` we start by expanding the outer `f`. That becomes: do putStrLn "Enter a guess" n &lt;- readMaybe &lt;$&gt; getLine if n == Just m then putStrLn "You guessed it!" else do putStrLn "You guessed wrong; try again" f (f (f (f ...))) Those are some legit IO actions that are `&gt;&gt;` together. We don't need to expand further for the runtime to start performing them. So that's what it does. If n == Just m, we never get to the other `f`s and all is well. Otherwise we can expand again. You know, I think I might just, finally, have understood `fix`. A little.
I haven't heard of any progress on that front since my comment.
I try in my own haddocks to be reasonably consistent to use ≡ for writing out laws that talk about equality, so that folks don't mistake the left hand side for the function being defined and the right as its definition.
I'd really like syntax for `import qualified Data.Array (Array, (!), (//)) as Array`, because a one-liner would be much nicer given the myriad of imports in some of my modules.
There's a short two-part series (part 1 [here](https://www.youtube.com/watch?v=ScS8Q32lMxA&amp;index=1&amp;list=PL5Pk7H4lYdc9BQmCmXYokcpw1QsUnNjmo)) about representing a chess board.
&gt;For instance, I'll consider `(+ 3)` and `(3 +)` both as equivalent under handwavey `=` and expect that it's good enough for you, without proof, for whatever reasons you should choose. It is certainly good enough for me! :) &gt;Equality is always hairy around infinite objects. For countably infinite objects, can we justify the handwaving by appealing to some correspondence with the naturals, as in `[a]`&lt;==&gt; `Nat -&gt; a`? (Disclaimer: most of the math I know was picked from hearsay.) 
Console 2 is excellent. I have used Haskell + Mercurial + Console 2 + PowerShell + Vim on occasion, and the environment was acceptably usable.
I think what Steve was trying to say is that the Rust type system doesn't incorporate any special treatment of threads.
No. You are not being overly critical. That is a perfectly good question. Now mind you, this is my understanding from reading something about it somewhere and seeing a job posting on their website. But from what I remember, they were using it on a team for mining data sent back from their probes. So not like they were using it for critical, real-time, stuff-in-space parts, but that there was/is a select few using it for their day job, mostly to build graphs and interfaces to the streams of data they are receiving.
&gt; there is a "roadmap/wishlist" (on someone's personal Google Docs account if I remember correctly) Wasn't that the point of the Prime' mailing list? I don't think being on someone's private Google Drive account does much for moving the standard forward. I guess I'd rather see work-in-progress than no work at all. Note: I know this isn't *your* fault. I also know that no one owes me anything, let alone a language standard. I just grow weary of all the mentions of the mythical unicorn called the next Haskell language standard that has yet to materialize. I'd rather we were realistic in saying that GHC IS the standard, rather than try to convince ourselves that its just a playground with ideas that will make it into Haskell proper.
Interesting. May I ask of you to send me something about it if you would remember / happen to re-discover sometime later? PM or additional post here are both just as fine as far as I am concerned. Thanks in advance :D.
What made you choose OCaml then?
Ooh. Thanks for the tip. I'm really excited to see such a serious push to improve our beloved Parsec.
In case it was even needed, I can personally vouch for two of them (Guide to the type system &amp; Advanced Haskell). They were tremendously well delivered by Edsko &amp; Andres, and I learned a lot! They come with high quality slides I find myself consulting every now and then in my everyday, professional life.
s/Pandoc/Parsec/
The page at https://www.haskell.org/documentation lists four tutorials that are recommended. Have you looked at those yet? * http://dev.stephendiehl.com/hask/#cabal * http://coldwa.st/e/blog/2013-08-20-Cabal-sandbox.html * http://www.vex.net/~trebla/haskell/sicp.xhtml * http://www.vex.net/~trebla/haskell/cabal-cabal.xhtml
I totally agree! However, I am cautious to confer a message of absolute truth with the advocacy of these abstractions as opposed to that of a useful perspective. There are many point-of-views vying to be considered the fabric of programming methodology.
I'm not sure if this will help at all, but maybe it would be clearer with different naming: define = fix ones = define (\this -&gt; 1 : this) factorial = define (\fac n -&gt; if n == 0 then 1 else n * fac (n-1))
Well, sure, if you allow me god-like powers of inspection! The trick is that if we're purely extensional then we're always in the situation where for all `n` less than some `M` we've got equality but... it could always fail for some other `n` less than `M+1`. This is semidecidability: we can detect inequality in finite time if they are genuinely unequal but we cannot convince ourselves of equality. You'll note that whenever you see a proof that two sequences are equal you are actually witnessing an intensional argument where we start to argue that the sequences are equal due to a property of how they are generated. If not that then you probably are looking at an argument which presupposes some external property (usually something to do with convergence) and uses it to derive equality. Lacking anything but extensional observation, though, you truly are asking for god-like powers.
Yeah, metaphors usually break down at some point. Stating it this way was just too good to pass up though. :)
Y̶o̶̲̓ū ẗ̨͇́h̞įn̩k̼͐͝ t͕͠h̖e p͉a̵̫c̗̍̕ḱ̬a͒́g̙͋e̟ ̤n̥͌a̓m̴ͪẻ̗ ͢i͚ͭ̀s ̊͢a͔͐ ̭͜c̲̄̕õi͎n̰cͩi̧̪d͐e̪͆̕n͖ċe͚?͔
&gt; GHC's massive internal rehaul hadn't heard about that
P̤͈̝̺͍͔͝h̡̟̣̰̫̗'̙̗̞͕n̼̱̲̭g̢̺͔̣͕̗l̲͎͓ͅu̡̟̖͎̭̳̞i͢ ̷̞̘̣͍̪m̯͙̳̭̩g҉͈l̩̱̪͓͈͍͡w̥̯͕̞͉͉'̳͍̲̭n̝͇͍̱̺̱ͅa̛̠͔f̰̺h̥̻̝͍̲̥͟ ̢͔̰C͚͉̭̲ͅt͎͍̭h̡̦͇u̧̫̙̰̝̤̼l҉̰̬͎h̻͙̣ù͓̺̩ ͕̰̙̳Ṛ'̘̯̱̰l̖͝y̜͙̠͕͙̗̗e̞̩͈̖͢h͇̞̩ͅ ̙̳͟w̧̱̳̼g̹͉̩͕̫ą̙̙̣̬h̯̬̼̮̫'̧͇̭͓ṇ̷a҉͓̱̼̝̜̰̗g̡̜͓̠̼̥l͚̹͜ ̡̳̙̩̣͖f͔̫̗͎ḩṭa̧̜̠̗͖̩̼g̷̩̺n͟
Right. "Tactic-based" is just another way of saying subgoal-directed, which is the style of proof being employed here. The reasoning within each subgoal is forward in nature, but Coq supports that so I'm not sure why the author feels the system is incomparable. On a related note, given the author's design goals, I'm not entirely convinced that Agda and Coq are the best comparisons in the first place. They seem to desire a proof system with a foundational logic that is equational in nature. Maybe PVS or HOL4 (or Isabelle, which the article mentions but never actually addresses) would be better? I know that both of these systems have been used to teach similar material at other universities with great success.
A few reasons: - I wanted to try out the language and ecosystem given developments in both (opam, etc). I also wanted to write a "big" project in ML, taking advantage of the module system - Since these abstractions are not "baked into" the language like Haskell, implementing them in OCaml (and understanding both enough to do so) seemed like an excellent learning opportunity as well as a chance to see if such a thing was viable in the first place. I came away with a perfectly good _IO_ type with custom exception handling, a free monad implementation, and the knowledge that pervasive use of monads in OCaml is entirely reasonable.
how can some people be that hostile to extensibility? and Haskell2010 comes with better language features than any mainstream language. "out of the box".
There probably *is* a case to be made that there should be a "mathematical operator lambda" in addition to the "mathematical letter lambda"; *that* should be what Haskell uses for it's lambda operator.
their tone is gross. how unfortunate we are that our benevolent Haskell designers didn't ban unsafePerformIO, because us contemptible programmers might abuse it to write better code.
Seems the page was updated and the link changed. It is now https://skillsmatter.com/conferences/7316-haskell-hackathon-2015
could we make that the default?
yes. Im putting upper bounds on my project, as PVP compliant as I can. but "checking signatures" is really the right way to do this. version bounds are a bad manual proxy for signatures. just like "reads a file and returns a string" is a bad manual proxy for `readFile :: FilePath -&gt; IO String` 
The calculation environment in the new lean prover (see section 4.3 of the tutorial) enables writing proofs in the very direct fashion as well. It's really neat!
It definitely helps me *understand* applicatives to think of them as preserving the closed structure of Hask. I'm not sure that it helps me *apply* them. :)
Mu
That makes intuitive sense from reading the code - I was trying to come at it from the other direction of `fix` defining the 'fixed point' of the function where `f x == x`. So presumably in this case the 'fixed point' is `f (putStrLn "guessed") == (putStrLn "guessed")`? And the resulting action that it returned to whoever wants to run it is therefore `putStrLn "guessed"`. This seems really interesting as a general recursion pattern - I could see myself using it, I _think_, in cases like [this](https://github.com/eightyeight/space-sim/blob/move-modules/lib/Spacerace/Main.hs#L45-L49) where I've been explicitly tail-looping to wait for some condition. Does that seem reasonable? I'm going to give it a go when I next get back to tinkering with that project.
If Ghc output isn't deterministic, then how does Stack achieve determinism? (Not asking you specifically, but anyone who might know, e.g. /u/snoyberg.)
Maybe this is missing some subtle details, but I think of `fix` as the **ability to refer to yourself inside your definition** factored out into a function. Instead of supporting self-reference in our language, we could just provide `fix` and would be able to express all the same things¹. With this in mind, we can go from a recursive definition to a `fix` version systematically. Take this ugly `fib` function, for example: fib n = if n == 0 || n == 1 then n else fib (n - 2) + fib (n - 1) We can write a non-recursive version of this logic by replacing each recursive reference to `fib` with an extra parameter: nonRec n recurse = if n == 0 || n == 1 then n else recurse (n - 2) + recurse (n - 1) In a sense, we have the same function definition, except we punted on how to recurse, leaving it as a parameter. We can then let `fix` do the recursion for us: fib' n = fix (nonRec n) The important insight is that we can use this same procedure for all simple examples of self-reference. Give it a try! So: `fix` is **recursion factored out into a function**. With this in mind, the definition of `fix` makes more sense. It takes in a function whose parameter is supposed to refer to the function itself—and it does this by passing in a recursive call to `fix f` into the function. let fix f = f (fix f) Okay, maybe that wasn't the most rigorous explanation, but it helped me understand `fix` better. **footnotes** ¹ Actually, we might need some additional combinators to cover more complex cases like mutual recursion and [polymorphic recursion]. I'm not entirely sure if we can cover these cases with just plain `fix`. Dealing with these would distract from building up your intuition. Besides, I don't want to think too hard about the more complex versions right now :P. [polymorphic recursion]: https://en.wikipedia.org/wiki/Polymorphic_recursion
That clears it up, thanks! I don't think I had reason to be alarmed after all: even though the problem is semidecidable in the general case we can at least talk meaningfully about equality of infinite sequences. For instance, we still can, in the context of some problem, assume `xs = ys` *ex hypothesi*, as in that case the source of our knowledge might as well have been Athena.
Oleg Kiselyov and SPJ showed how to do something similar [0], but this feels much more intuitive. Great job! [0] https://wiki.haskell.org/GHC/AdvancedOverlap
1. Omg yes 2. I just discovered http://reviewable.io/ today, which seems to solve all the annoyances I have with the github PR process (which, don't get me wrong, I already way prefer to things like phabricator or Gerrit) Not that I am likely to contribute nontrivially to GHC, but I'm way more likely to make trivial contributions if it is hosted on github or another more friendly system (say, gitlab).
Check out reviewable.io. It seems to solve most of those problems.
Well...
While I agree with you, the whole arc experience puts off me to even try contributing trivial patches.
And if you're willing to pass in a witness as an argument, you can also use ekmett's constraints package. Example [0]. [0] https://gist.github.com/jkarni/8d24a13b4be4cbbb1373
Actually, with defer-type-errors you don't even need the witness! https://gist.github.com/jkarni/0872c83bc4264a23fec1
Thanks, fixed!
I agree that I wish something like this had a nice to use syntax, because it'd be useful in so many places.
Side note: if your needs are that simple you might want to consider [Hakyll](http://jaspervdj.be/hakyll/). I used it to dePHPise my (mostly static) personal pages, with great success.
I've made a note to test that this weekend. Thanks.
Both would be okay I guess, but sticking with github as a single system has the huge problem of requiring people to use nonfree code to fully use the system and working under github's terms and providing information to github. There's suggestions to use more nonfree software to solve github's short comings which isn't great either. The problem I forsee is if both are done then there'll be two systems to worry about and to save effort the nonfree one will be chosen.
Cool, thanks. Just to clarify, since we already cut the next release, the code is now merged to master.
Thanks, I don't know why I thought it could do source installs. Almost all package manager *programs* are cross-platform to some extent (most are written in scripting languages). But they can't install anything, because their package repo isn't cross-platform. In theory, you could have an army of people making binary packages for every platform, but in practice the only way a package repo will be cross-platform is if you build everything from source. The scripting-language package managers get that check mark for free because they "build from source" when they copy stuff onto your system. 
I may need to subscribe to GHC-dev just so I can point out reviewable.io in that thread...
An easy and intuitive way to convince yourself that a given use of `mfix` will converge is to use the fact that normal evaluation order is the "most normalizing", in the sense that if an expression has a normal form, normal evaluation order will find it. Combined with the fact that lazy evaluation is basically just normal order with memoization, this implies that if you can come up with any just-so story about evaluation order that converges, so will the one you actually get from Haskell. I've learned the above trick from [Russell O’Connor's article in issue 6 of The Monad Reader](https://wiki.haskell.org/wikiupload/1/14/TMR-Issue6.pdf).
Spark? Twitter?
There's also precedent for `cxt` in [Template Haskell](https://hackage.haskell.org/package/template-haskell-2.8.0.0/docs/Language-Haskell-TH-Syntax.html#t:Cxt). But if we're talking GHC internals, `ctxt` is used much more than either `ctx` or `cxt`. ➜ ghc git:(master) git grep -i '\&lt;ctxt\&gt;' | wc -l 711 ➜ ghc git:(master) git grep -i '\&lt;cxt\&gt;' | wc -l 146 ➜ ghc git:(master) git grep -i '\&lt;ctx\&gt;' | wc -l 171
Is there a point, when adding language features to be able to "program" with constraints, that these constraints (or Haskell in general) actually lose some of the useful properties they have now? For instance, if you could pattern match on contexts, and these could be used as first-class values, and you had some kind of dependent types on Haskell (at least on constraints only), maybe you could define `cxtShow` with a simple case analysis on the context, like: cxtShow :: ctx -&gt; (ctx =&gt; a) -&gt; String cxtShow Show = show cxtShow _ = const "&lt;&lt;unshowable&gt;&gt;" Similarly, `cxtNub `: cxtNub :: Eq a =&gt; ctx -&gt; (ctx =&gt; a) -&gt; [a] -&gt; [a] cxtNub Ord = nubOrd cxtNub _ = nub Would adding this language feature "break" any existing functionality? Are there any other kinds of language features (regarding constraints), that when added, would do so? Maybe adding a feature for passing the context above implicitely and automatically so the programmer wouldn't have to pass it explicitely all the time, etc.
I'm not going to try to counter each point, but I completely disagree with what I think is your central thesis. I absolutely do contribute patches to projects on GitHub that I otherwise would not contribute towards. Moreover, the ease of contribution is a substantial vote in favor of using the project. I'm not going to belabor the point because I think it's pretty simple: I am more likely to use, and vastly more likely to contribute to, projects hosted on GH. Reasons: fork, PR, comments, push, comments, rebase, merge. I totally sympathize with the complaints about code review on GH, and its being closed source. Those are valid, important arguments. But you are choosing them at the expense of some of your users (users of an OS project).
Looks like this is a dynamic check though. OP's example looks pretty much static to me. EDIT: No, I'm wrong. defer-type-errors will just statically write in that error, won't it? You might still lose something here since you need to throw an IO exception, but I don't think that's terribly expensive in Haskell?
So this is a proof that Haskell with overlapping instances, rank N types, and constraint kinds doesn't preserve parametricity? mkIfCxtInstances ''Monoid fake_id :: a -&gt; a fake_id x = ifCxt (Proxy :: Proxy (Monoid a)) (mappend x x) --oops! x Was this already known? **This is wrong, look at duplode's answer**
The trade-off I see is: evaluating too much, and having to deal with exceptions, in the defer-type-errors approach; having to declare a lot of instances (possibly with TH), and more importantly, having no guarantee at use-site that *all instances (of e.g. Show) are in fact considered* in the function, in the OP's and AdvancedOverlap implementations. This latter issue is what put me off AdvancedOverlap, more so than the boilerplate. (I haven't really thought very much about the defer-type-erros approach though - there may be other issues besides.) EDIT: Oh, and it seems right now defer-type-errors leaks to client modules. That's pretty bad. I need to think about workarounds.
Is this a Haskell implementation of the techniques described in the [Generic programming must go](https://www.reddit.com/r/programming/comments/3cjr4v/generic_programming_must_go/) presentation?
That the JVM is a damn impressive compiler, but Java itself is nothing spectacular.
If publishing to Phabricator is too much of a hassle, opening up a Trac ticket with an URL pointing to your commit or branch in your GHC GitHub fork would be equally accepted as a contribution and creates less confusion. And we require an associated Trac ticket anyway for improvements and non-trivial fixes.
How should `arc` behave in order to improve the experience to the point of being at least tolerable?
You usually don't need PHP (or any other dynamic page generation) to just share things like headers and footers. You can create a Haskell program that runs Heist (if that's the templating language you like) on your site content and plops out HTML files, which you then just serve completely statically with something like nginx. This is what I'm doing here: https://github.com/kqr/two-wrongs-st/tree/master/src (of interest are Main.hs (loads the site, generates with Heist and writes to HTML files), Views.hs (contains all the views for the site, which are filled with content) and Html.hs (helper functions for creating/rendering views with Heist)) Site in action here: http://two-wrongs.com/ . When the user requests "two-wrongs.com/foobar" I serve them "two-wrongs.com/foobar.html" – simple rewrite in nginx. Code might not be optimal because I'm figuring out how to transition to Hamlet while staying sane, so I haven't bothered polishing it to a shine, but it should be workable.
The mentioned wishlist is just a strawman draft list of items consisting of updating/consolidating the library part of the report to account for the recent changes to the base libraries (AMP, FTP, MonadFail, ...) as well as integrating a couple of language-extensions which are enabled by a majority of packages on Hackage anyway and/or have stood the test of time and have little risk of being in conflict with future extensions (things like `BangPatterns`, `DeriveFunctor`, `LambdaCase`, `MultiParamTypeClasses` etc). But all these changes need to be decided by the committee on a case by case basis. The official roadmap will materialize on the Haskell' wiki/issue tracker once the new committee has been officially appointed.
We'll probably start seeking nominations for the new committee soon. There's just a few organisational issues to sort out first.
I use [minGHC](https://github.com/fpco/minghc). It just works.
For mutual recursion I don't think you need anything special, you just pack the inputs/outputs into a tuple where necessary. For example, with the classic example of mutual recursion iseven 0 = True iseven n = isodd (n - 1) isodd 0 = False isodd n = iseven (n - 1) you can define the non-recursive function g rec 0 = (True, False) g rec n = rec (n - 1) and then define iseven' = fst . fix g isodd' = snd . fix g If the function inputs aren't of the same type then you have to pull a similar trick with the inputs, packing them into an Either to deal with the two (or more) possible cases.
I vouched for `&amp;mut` at the time, but I think I was wrong.
I actually think this saves the day here. You're explicitly saying that you know enough about "all types" to tell test if that thing is monoid or not. So it was a dumb error on my part. Thanks for catching it. PS: Worth noting then that if this feature is rolled into GHC itself, it should still require some additional constraint in the type. An actual construction like this would otherwise allow you to break parametricity again.
&gt; I'm not going to belabor the point because I think it's pretty simple: I am more likely to use, and vastly more likely to contribute to, projects hosted on GH. I understand why this is the case, the above post is more of an extended version of my Twitter. It's all besides the point - the *real* question to be answered here is: does GitHub solve the problems we need it to solve, is it an actual solution, and *are those problems the important ones*. We need to be sure it solves the root problem. People will keep using and hacking on GHC regardless, there's no question of that. The above is just musing on the bizarro aspects of how we think about 'convenience'. &gt; But you are choosing them at the expense of some of your users (users of an OS project). Really, I'm not choosing anything. I think. Like I said, Phabricator had the luck of being the 'first child', but that wasn't *because it's unequivocally better* - it's because the *cost was low* and it didn't complicate any existing infrastructure, basically, because we didn't have another tool. So we needed a solution, it seemed to work OK. I've come to like it a lot, and I think it's a lot better suited for a project of our size. The same could not be said of GitHub, and all the downsides we didn't like about it then are still true now. If everyone votes on GHC moving to GitHub, I'm not going to stonewall it, although I will accept it begrudgingly. :) But it needs to be clear a decision based on the acceptance of existing developers, *because we are certain it is the root solution to a problem we clearly have*. Not because we have a feeling, or because of the nebulous "other contributors" that will magically appear (unless nebulous "other contributors" is the tiebreaker of course!) That's not a sensible bet to hedge yourself on, at the expense of downsides and a lot of added cost, migration, and surface area. It's a huge waste of effort we could arguably spend just making GHC better.
THANK YOU!
&gt;More generally, I'd prefer a non-abbreviated word like `Possibly` What crossed my mind was: cxtNub :: forall a. (Eq a, MightBe (Ord a)) =&gt; [a] -&gt; [a] A case could be made that this trick feels more like `Maybe` than `Bool`. In the `ctxNub` example, you might stumble upon the `Ord` type class with its additional methods, or there might be nothing else to play with.
Thank you very much to everyone involved in recording and making these available! I'm looking forward to when the videos from the "Type checking" session start going up, as I had to leave early and was disappointed to have missed those talks. For reference, [here](http://icfpconference.org/icfp2015/program.html) is the full ICFP program, breaking the talks down into sessions (categories) and providing references for the connected papers.
Yeah, the `arc diff` thing is unfortunate. I tend to use `arc which` and most of the time `arc diff HEAD^` to denote I just want to create a review out of the top-commit. As to bundling Arcanist as a submodule (as well as bundling it with a linux distribution unless we also use the same distribution's Phabricator version), I think this is the wrong approach as it conflicts with how Arcanist's updating mechanism (`arc upgrade`) works with Phabricator being a rolling release project. We'd have to frequently update the two submodules each time we update Phabricator contaminating the GHC development history with commits related to the GHC phabricator infrastructure... and you'd still have to install `php` somehow. Finally, you may end up having Arcanist being rolled back to an older state when you checkout an older GHC commit or a different branch, as we'd tie the arcanist version to to the Git development history, even though this is unrelated to the devel history but rather correlated with the deployment history of our Phabricator instance. TLDR; You're supposed to install Arcanist exactly as described in the [Arcanist Quick Start Guide](https://secure.phabricator.com/book/phabricator/article/arcanist_quick_start/) otherwise you'll run into subtle problems sooner or later.
I agree. The thing with voluntary open source is that tolerance for faffing about is much lower, there are always a dozen other things I could be working on and a dozen other things I could be doing off the computer. This includes tolerance for wasting time discussing with people why convenience is important. There are other projects I can put my time into.
Alternatively you could install [KOReader](https://github.com/koreader/koreader) which offers many more features than the stock reader app, including on-the-fly column splitting and configurable text reflow. Also compatible with jailbroken Kindle devices.
Alternative proposal: automatically import Github pull requests in to Phabricator. This is what Facebook's HHVM project does https://github.com/facebook/hhvm/wiki/What-is-Phabricator I don't know if the bot they use for this is currently open source, but it shouldn't be hard to hook up the appropriate Github and Phabricator APIs.
Contrary to popular belief, Haste is still very much alive, which a big list of new fun stuff since the 0.4 era. In particular: * GHC 7.10 support; * proper Cabal support, including sandboxes; * better portable binaries, with user friendly installers; * better FFI; * new libraries for animation frames, audio support, fast string manipulation, regexes, etc.; * overhauled DOM libraries (and others); * faster generated code * *much* faster compile times; * architectural improvements that will result in faster turnaround time for new releases. One thing that is still a major issue, however, is the pointer story, preventing bytestring and text from working properly which in turn is pretty bad for compatibility with a lot of things on Hackage. These kinds of compatibility issues are going to be the focus of the next release (together with some super secret and totally exciting new work on Haste.App - compatibility doesn't generate publications after all), so things will only get better from here.
Apparently it has [k2pdfopt built-in](http://miscool.blogspot.be/2015/01/koreader-turn-your-android-phone-and.html). I might install this on my smartphone, thanks. Can't seem to find a good guide for the Kobo, though.
[Does this help?](https://github.com/koreader/koreader/wiki/Installation-on-Kobo-devices)
HHVM use such a bot. It seems to work really well and is IMHO the best way forwards. https://github.com/facebook/hhvm/wiki/What-is-Phabricator
Conf videos usually take a while to appear, I think for good reason because producing them is a PITA so someone has clearly put some work in here. THANKS
My first contact with `MonadFix` was through FRP. It is indeed a nice illustration, so here is a toy example to begin with. With the current version of reactive-banana you can define events and behaviours in a mutually recursive way without even noticing it: let ePow = bPow &lt;@ eClick bPow = accumB 1 ((+) &lt;$&gt; ePow) As `bPow` has an initial value, 1, the recursion bottoms out, and this tiny event network will happily churn out powers of two one click at a time. If your chosen FRP library has a monadic interface (like threepenny-gui, or the [forthcoming 1.0 version of reactive-banana](https://www.reddit.com/r/haskell/comments/3ivbht/frp_api_redesign_for_reactivebanana_10_feedback/)), however, `accumB` and friends produce behaviours in some appropriate monad. That means mutually recursive definitions run into the same issue demonstrated by Roman with the compiling of `Rep`... and the solution is the same as well: rec let ePow = bPow' &lt;@ eClick bPow &lt;- accumB 1 ((+) &lt;$&gt; ePow) 
Some of the other side event talks are already uploaded (HOPE, WGP) so I would assume (and hope :)) that they also appear soonish.
did you get any reply from /u/vsts15 ?
I asked the GHC team a while back if there's a way to remove the `IfCxt (Monoid a) =&gt; ` constraint, and [they said no](https://ghc.haskell.org/trac/ghc/ticket/10532).
That's a very interesting example. I wish someone would make a blog post (or long post on reddit!) expanding on it because I'm having a hard time seeing how this would look. 
It depends on what you want. If you are ok with the application looking like a linux port, than it's fine. But for a native look and feel, wxHaskell is a better option.
Thanks! Can't wait for all the videos to appear.
How someone answers that question can be revealing though, whether they just repeat what's been asked for or provide evidence that demonstrates their skills...
Some nitpicking: in denotational semantics it is usual to signal out a special bottom value (an upside down `T`) which represents undefined (non-terminating) computations and that inhabits every type. The unit value is () and inhabits only the unity type -- which in Haskell is also written (). The "bottom type" is `forall a. a`, i.e. the intersection of all types; the only inhabitant of this type is bottom (not the unit value).
He probably won't need much, but I do everything with the command line, so I'm not sure I want to learn PowerShell just for this occasion.
Isnt 8.0 supposed to be the next stable, that is, they just skip 7.12?
A terminal emulator with tabs, support for multiple command shells (cmd, PowerShell, Cygwin-bash, etc.), sensible copy-pasting and several other basic features neglected by Windows' standard tools. There is a link in mcandre's parent post up above.
Is 7.12 not supposed to be a stable release? I am confused.
It would be useful if list-dependencies had some kind of snapshot-only option here, or an option to exclude local packages and packages in the global DB like base, template-haskell,... I did that via grep -v now but it would be smoother that way. Other than that it seems to be working so far (right in the middle of building the docker image now). Thanks for your help.
Congratulations to ICFP team.
Thanks for the recommendation. I just tried it out, and I think I'll be going with this. It seems like it's built for exactly the kind of thing that I'm trying to do. As a minor aside, I will say that I do prefer the templating offered by `heist`, and I think that a `hakyll-heist` library should be possible. You know, just in case anyone wants a random thing to work on.
&gt; The "bottom type" is `forall a. a`, i.e. the intersection of all types; the only inhabitant of this type is bottom (not the unit value). That's why `null` is wrong. `null` is a member of every (reference) type, so it pretends to be an inhabitant of `⊥` -- but it isn't actually one, since its mere existence doesn't indicate a divergence. In fact, you can do something with it -- precisely one thing -- ask for its identity.
I haven't read the proposal completely but one problem I have with Type family is type inference can't go backward, which stops you to "pull" types from the return type. Please correct me If I'm wrong but I think type families will solve this problem. Let's say you want to some types A,B, and you want to convert them respectively to A' and B' data A = A deriving Show data B = B deriving Show data A' = A' deriving Show data B' = B' deriving Show You can define a type family add a `prime` to the data type family Prime a where Prime A = A' Prime B = B' You can check that `:i Prime A`gives `A'`. You also need a conversion function class ToPrime where prime :: a -&gt; Prime a instance ToPrime A where prime A = A' Now, I can do &gt; prime A A' Everything works fine, now let's say I add a Default instance to A class Default a where def a instance Default A where def a = A I would like to be able to do prime def :: A' (which is what I call *pulling*, The compiler should infer the type of `def` from the expected returned type `A'`). That doesn't work at the moment because GHC can't reverse the type family and go from `A'` to `A`. With injective type families, normally we can tell the compiler that there is only one `a` such as `Prime a` = `A'` (which is `A`) allowing `prime def :: A'` to typecheck. Of course to work you need the family to be injective , ie you can have also `Prime AA = A'` otherwise GHC won't be able to decide between `A` and `AA`. 
That'd be super nifty!
I think a lot of effort can be shared between reducing barriers to new contributors and reducing annoyance to current/frequent contributors. Win/win!
 ghci &gt; 7.12 == 8.0 True
Reminds me of SFINAE in C++ or `static if` in D. Which if my limited experience is any judge can quickly explode with conditions. Each instance also having to be unambiguous (with SFINAE) makes adding more than one or two very simple constraints unwieldy.
Oh, I read that like `ghci` was a variable: `((ghci :: Float) &gt; 7.12) :: Bool)` 
And that's a very good thing!
You could start with the documentation for the `base` package on Hackage. Note, however, that relying on third party libraries is extremely common, much more than it is in Java. For a curated list of commonly used packages, look at Haskell-platform, and probably also stackage, although the latter also contains many non-essential libraries.
I really hope that one day type families will be able to have kind signatures separate from the naming of variables. I really don't like how they're all cluttered. That said, this GHC release is looking to be absolutely amazing. Paging /u/goldfirere, have you given any thought to how this will work with term level functions being promoted to the type level?
&gt; I apologise, for it is just not polite to ask for so much of your time. [...] I don't think I'm actually supposed to write a serious essay on collaborative writing, so make of it what you will. No need for apologies, I explicitly asked for a discussion. :) Seriously, thank you so much for taking the time and writing, you weren't supposed to write a serious essay, but you wrote something very insightful anyway, that's very cool of you! Btw, you don't have to respond to my comments, but I would love to keep this conversation going. &gt; A tutorial should be a commentary on a reference text, and not the reference itself. Good phrasing. This concisely describes what I was intuitively getting at. I'll use this insight from now on. &gt; Firstly, such forms encourage, and even demand, that the content gets infused with a strong dose of personal vision and flair. [...] That being so, author numbers tend to be small. I agree, this is practically ineliminable. At least at this stage of cognitive psychology, when we still don't know how to reliably avoid ego and vision clashes, or how to make people happily submit to somebody else's vision. &gt; Secondly, the tools and workflows used to produce the materials do not encourage collaboration. YouTube is merely a publishing platform rather than an authoring one, and most blogging platforms are geared towards the single-author use case (GitHub Pages being the exception that confirms the rule). I think that should be the point, that we (the free software community) should attack first to solve the problem of “bad documentation”. That's what Michael Snoyman was getting at in the OP blogpost, the way wikis are implemented technically discourage contributions, while GitHub PRs, while being a very roundabout way of contributing, do encourage. As I see it from reading Snoyman and your comments, there should be a web software that gravitates towards small core of maintainers with a very flexible way of contributing small atomic pull requests. For example, there is an openly collaborative Haskell book. It is maintained by a few main authors, who have direct commit access to the documentation (or tutorial, or whatever) source. Whenever I see a paragraph with a typo, an source code example with an error, an exercise missing, a section not yet written, I contribute a patch via PR, that is tied specifically to that paragraph/example/section. The maintainers regularly review contributions, edit them at will to align with their vision, and merge with the source. To preserve ego, if maintainers reject my patch, it can end up in a special section “we rejected this contribution, but other people might still find it useful”. This is a good way to provide exercises, I think. For example a Haskell book can have exercises for list functions, and I want to add 10 more exercises, but book maintainers don't like them for some reason, so my 10 exercises are still accessible and associated with the book, but they exist separately. Just like there are community-contributed packages for many programming languages, that everyone is free to use or not use. This, I believe, prevents fragmentation, people are encouraged to dump all their knowledge about Haskell in one place, but the result Haskell book is coherent and clean from unintegrated messy dumps, while whatever was rejected still lives in the form of “community packages”. These packages can be as small as “additional exercises on lists and folds” and as big as “completely alternative take on the monad chapter”. The bottom line: let people contribute in an easy-to-entry, atomic fashion, where their contributions are visible, measurable, and attributable. But let the maintainers be the ones who keep the coherent vision, literary style etc. &gt; The arguments above do not really work for Stack Overflow [...] if there is worth in pursuing a different strategy than that used by an existing answer, writing a new one can be easier than editing the existing one or arguing with its author. There is also another factor that tilts the balance towards writing new answers: gamification, i.e. teh repz. Do you largely agree with my 5 reasons why it might be easier for people to contribute to SO and not Wikipedia, that I mentioned in [Warlords of Documentation proposal](http://meta.stackoverflow.com/a/304112/596361)? Or is there anything to change/add? It might be that I extrapolate from my own feelings, that may differ from other people's. Gamification is king, although it is kinda broken on SO right now. On SO it motivates people to write quick very short answers to very new questions on mainstream topics to get loads of karma, than writing long answers on obscure topics. I write long answers for obscure topics anyway, though, for 2 reasons: 1. I have this itch that if I haven't coherently explained a topic to somebody, I don't understand it, so I recently started writing answers on things like folds, function composition, type mismatches under [Haskell] tag. IOW, learning thru teaching + having canonical sources for my own knowledge. 2. I want Haskell to be popular. To clarify, I don't want “success at all costs”, but I do want more people knowing Haskell and knowing that Haskell is not hard/mysterious, so that these people might become valuable computer science researches one day. So I believe that providing verbose newbie-friendly answers on very simple topics like “what is a higher-order function” or “how to use function composition” is a good idea. I do admit that it might not be a very effective way of teaching Haskell, I'm open to suggestions on the most effective way to spend my efforts on that goal. Also, my answers might not be that good at all, and I don't know how to write brilliant insightful newbie-friendly answers on Haskell yet. Am I rare in this respect? If not, how can we exploit these 2 desires in other people, so more people would be willing to contribute long high-effort explanations on relatively obscure topics? Also, if ever build a system for collaborative tutorial writing, what is the best way to tie gamification with it and not create bad incentives? &gt; Because writing open-source books through online collaboration is a beautiful idea I completely agree that open-source books is a beautiful idea, and we should put more effort into it. I'm not against Wikibooks existence and not against other people already participating. But criteria you outlined — book, collaboration, online, open source — while necessary, aren't sufficient. Maybe the problem is strictly technical: take the best of both worlds of wiki and GitHub PRs, and voila. But we do not leverage human psychology, and even though SO implements gamification, we do not leverage it well enough. &gt; From the argument about the small number of authors it follows that there is an ineliminable tension between book (a coherent whole, strongly infused by personal vision) and collaboration (between multiple parties with potentially conflicting visions and motivations). Successful collaborative book writing requires a great deal of sensibility for dealing with this tension. Maintainers can be neither dictatorial (which alienates outsiders, who bring useful input) nor too lax editorially (which leads to the degeneration of the book into a disjointed hodgepodge of text fragments). Contributors can be neither too shy (which causes useful contributions not to be made) nor too assertive (which leads to ego clashes, drama and sub-optimal results). And, as you implied elsewhere, everyone involvement needs a to do a little self-effacement, given the need for tolerance, the necessary freedom in editing someone else's text at will and the relative anonymity of contributions to a wiki. Agreed. But that's the problem. Let's assume for a moment, that human nature is immutable. (This is true in practice, as science of human motivation, cognitive and social psychology, are in their infancy. It doesn't matter if in theory human nature is very much mutable at this point). We can't make a maintainer, who is an asshole, less of an asshole. We can't make shy contributors less shy. And so on. What we can do is to create technical and social workarounds for human nature, just like government, laws, police, ideology, morality are workarounds for humans being horrible animals. SO is such a technical workaround, it leverages human psyche without trying to change it, and SO is a great success (depending on what you believe are its goals). We need the same workaround for creating tutorials, and it should leverage our current understanding of what motivates people to contribute. What I wrote deliberately sounds futuristic. I'm interested in how to implement all this in principle, and maybe even influence SO-documentation proposal. As for now, I personally am happy to contribute to Haskell Wikibooks, SO answers, or even my own tutorials on my own blog.
Well that's a bit harsh...
Snap-static-pages will invoke a heist template first if it is found, the markdown side is optional.
The markdown aspect of it is completely optional. Heist has a &lt;markdown&gt; splice that uses pandoc to convert markdown to html. But you don't have to use that. All you have to do is take out the markdown splice and switch to HTML.
&gt; 15 years ago the heavily concurrent MLDonkey peer-to-peer file sharing client had hundreds of thousands of users (that's probably more users than all Haskell programs ever written combined). That doesn't mean anything about Haskell or its concurrency. &gt; Haskell got equivalent support for concurrent programming but with few users so it is nowhere near as mature. orly http://haskell.cs.yale.edu/wp-content/uploads/2013/08/hask035-voellmy.pdf
Right. Implement it by tracking already-seen items in a Set. This makes it stable, and also removes the ordering requirement (meaning Hashable could be the constraint instead). This is how LINQ's Distinct works.
There is a hakyll-heist library at https://github.com/pjones/hakyll-heist. It might not be up to date, but it probably wouldn't be too difficult to get it updated.
The tone of the article alone don't particularly glow of fairness.
This is comparable to black devil magic. And it's awesome. Though I wonder why it's so hard. We really need some kind of dependant type system magic to make this more easy.
Arrogant and wrong, a deadly combination.
Harrop makes a number of good points (and a few bad ones), but what a profoundly irritating and obnoxious post.
Oh, god, I haven't seen a Jon Harrop post in forever.
Looking forward to the [FARM](http://functional-art.org/2015/) talks showing up soon too!
You guys are awesome, all of you. Thanks!
This looked too good to be true. Sadly, it kinda is. I wish it didn't rely on a bazillion instances to work, but that's what makes it work. 
Harrop is a complete joke when it comes to objectivity. He is a circus barker looking to stir up trouble where none exists. He is another one of those 'look at me' folks who lives to make noise. He loves to pick things that may need improvement and then conflates them with some imaginary disaster. I find that his criticism is little more than shrill noise that can be readily dismissed.
Great work, though admittedly I haven't checked out haste yet. I hope to get time to check it out soon.
You may be correct. Thus YAL is born!
As I tell my children, "Consider the source." Harrop's exaggerations make his opinions highly suspect.
Add `discrimination` and you can drop further to `O(n)`. https://github.com/ekmett/discrimination/blob/master/src/Data/Discrimination/Grouping.hs#L233
Haskell is designed assuming an "open world". _This is a good thing._. This means that normally adding new information monotonically just lets you do new things, adding an instance just accumulates knowledge! It also means that once I compile something it can _stay compiled_. On the other hand, if we require global coherence of instance resolution (also a good thing!) then we have a problem if you can start asking questions using the closedness of the world about if something is actually an instance of something else. When you can make closed world assumptions then code that is compiled in one context, isn't correct given instances outside of that context. With certain diamond import patterns you just won't see instances and you lose coherence. The same code put in different places in your program will yield different answers. Even "fixed", this would make it harder to talk about dynamically linking in new code which can just add new instances and the like, because doing so could change the proper semantics of already compiled code. This is hard because it goes against the fundamental assumption that allows separate compilation in Haskell. It does something the language is specifically designed against in many ways.
Submit a PR. But the main way to fix that would be to make it the default in the `.cabal/config` distributed with Haskell Platform and other Haskell distributions.
Cool. Thanks for pointing this out.
Which are the good ones and which are the bad ones? I can't tell since the tone is so rough.
Yes, because we all know abusive ad hominem is the best way to make your argument. 
&gt; You cannot predict performance. You cannot predict memory requirements. Fancy compiler optimisations break under the most bizarre circumstances. This is fairly accurate due to the huge performance differences certain compiler optimizations can make. I would not give as much weight to it as he does as compiler optimizations always are painful to deal with but have proven to be invaluable in industry use. I would take the real complaint to be that Haskell changes a lot more when you deal with these kinds of performance problems compared to other languages. Or in other words he is overstating the important a bit.
The platform is a one-click installer on Windows, works well for me. But other people seem to like minghc (I haven't tried it yet). For the command line, I use Far Manager; it's great if you are used to the basic key-bindings. For a text editor I use either notepad++ or Far's builtin one (the latter is probably easier for just modifying a config file).
I remember back when Harrop was hating on OCaml in comparison to F#. http://flyingfrogblog.blogspot.com/2010/08/rise-and-fall-of-ocaml.html It didn't really make me want to try out F# or abandon OCaml. Based on that I don't think this will turn away too many potential Haskell users.
Not `do`, since you don’t have `pure` or `return`, but a *comprehension* with only a single binder could be desugared to `Functor`: [f x y | x &lt;- m, let y = succ x] =&gt; fmap (\x -&gt; let y = succ x in f x y) m 
Google's Chris Colah looks at deep-neural-networks functionally: * **Learned Vector** *Constant* * **Embedding Layer** *List Indexing* * **Encoding RNN** *Fold* * **Generating RNN** *Unfold* * **General RNN** *Accumulating Map* * **Bidirectional RNN** *Zipped Left/Right Accumulating Maps* * **Conv Layer** *“Window Map”* * **TreeNet** *Catamorphism* * **Inverse TreeNet** *Anamorphism* 
indeed
As Colbert has demonstrated, amusement is directly proportional to the amount of truthiness. And this posting was highly amusing.
On location. I still live here, but work remote for FP Complete.
Inversely?
I've never seen one before, and it seems like I wasn't exactly missing out on anything.
Is there potential here for a nice monadic interface for building deep learning networks?
Even if supporting multiple backends leads to supporting the least-common-denominator of the databases, I don't see how this causes poor performance like the OP is talking about. 
/u/acow's link to the [standard libraries documentation](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/index.html) is the first thing I would recommend. But you're likely to want to use packages from hackage pretty quickly. Some people would recommend using Hoogle or Hayoo to search for docs on these packages, but I prefer a different approach. I prime my browser's autocomplete by pasting the URL "hackage.haskell.org/package/" into the browser location bar 10 or 20 times. Then all I have to do is open a new tab, hit 'h', then type the name of the package I'm looking for at the end and I go straight to the package's hackage page for the most recent version. It seems a little absurd, but is actually quite efficient. :)
Yes, that should be reasonably easy to make. I don't know how its performance will be, though.
I get (kinda) the connection between functional programming and logic (types are propositions, values are proofs). what are the mentioned relations between FP and topology or geometry?
This page lists all of the packages included in LTS Haskell with synopses and an API search function. It also links to a list of all modules provided by those packages: http://www.stackage.org/lts-3.4
&gt; At least he's up-front about the fact that he's an F# and OCaml consultant, so people might be clued in to the fact that he has a vested interest in making F#/OCaml look better than Haskell. To be fair, he could probably be a Haskell consultant too, if he liked the language. Or maybe he just really can't install GHC.
all PR is good PR. Jon Harrop's a smart guy: if I ever become a consultant, I'll make sure to loudly say wrong things about other languages. relatedly, it might make your teammates angry/curious enough to talk to other people about haskell. 
See https://github.com/ajtulloch/dnngraph for essentially this idea.
Rust looks super cool, but when i heard about "v1.0", I didn't get why they didn't wait until things were more formalized. aren't you providing backwards compatibility guarantees now? what if the new implementation has subtle differences (but still better) in behavior? that might be inaccurate. but when I heard about Rust it was like "fast code with linear types, also ADTs" and I was like "no way!". I just wouldn't want Rust to prematurely stop making breaking changes to its type system. it's the language I want to learn after I get a hold on haskell.
Cool project! I more meant to ask if the analogies brought up in the article could be leveraged somehow; it looks like the project you linked uses the traditional NN description of operations.
For reference, the *main* reason I contribute to the Wikibook is because it's so GREAT already. I'm motivated to clean up things and improve stuff when it already has value. I hate trying to make new things just because it's *my* thing. I want to figure out how to make the stuff I already like better. For me, the Haskell Wikibook was the best intro to Haskell of all the things I read, and I felt great making it that much better. I then feel frustrated reading all sorts of other books and blog posts because I can't fix things easily.
&gt; probably more users than all Haskell programs ever written combined "[IMVU has over 50 million registered users...](http://www.imvu.com/about/faq.php)" Which is to say: lolno. :)
Well now I'm double-posting since I fished this up for a comment higher on the page... but IMVU claims [50 million](http://www.imvu.com/about/faq.php).
&gt;At least at this stage of cognitive psychology, when we still don't know how to reliably avoid ego and vision clashes, or how to make people happily submit to somebody else's vision. Indeed, though I would like to add a few extra remarks. To me, before speculating on the posibility of "avoid[ing] ego and vision clashes" or "mak[ing] people happily submit to somebody else's vision", there is the question of whether that should be done, under any circumstances. Another conceivable speculative solution would be eliminating vision from the equation. That is not an option: long-form works entirely devoid of vision would be pointless -- we might as well stick with automatically generated API docs. Even Wikipedia has an overall guiding project vision that makes it worthwhile, in spite of most articles being written so dryly. Another illustration: among the many awesome Haskell answerers at SO, my personal favourite is [Conor McBride](http://stackoverflow.com/users/828361), who writes brilliant long-form answers in a very idiosyncratic style. (And I'm quite sure he doesn't care at all about rep!) &gt;That's what Michael Snoyman was getting at in the OP blogpost, the way wikis are implemented technically discourage contributions, while GitHub PRs, while being a very roundabout way of contributing, do encourage. Indeed. I sense lots of potential in the hypothesis in points #2 and #3 of your meta-SO answer, and will raise the issue at Wikibooks to gauge the reactions. By the way, MediaWiki does have a mechanism quite similar to pull requests: pending changes. en.wikipedia decided against using it, Wikibooks doesn't use it in all places, and I'm not certain of the extent casual contributors are aware of it, but it is there. I have never considered the possibility that pending changes might encourage contribution rather than restricting it, that would be something very interesring to investigate. &gt;Do you largely agree with my 5 reasons why it might be easier for people to contribute to SO and not Wikipedia, that I mentioned in Warlords of Documentation proposal? The issues do certainly exist in pretty much the ways you describe, even though I feel differently about them. Also, they are not felt equally across all wikis. For instance, Wikibooks is far less affect by them than Wikipedia. In any case, I would say that at least a small measure of selflessness is needed for being a wiki contributor. In detail: * Ownership: Loss of ownership is an integral part of the Wikimedia experience, and I would say it is a necessary one. Wikibooks books, and even Wikipedia articles, are not atomic, they must be integrated into a whole made of cohrerent parts. As such, you should indeed expect your text to be modified. That being said, the experience is far more brutal at Wikipedia than at Wikibooks, not only because the rules are stricter but also because the lack of individual flair in the articles makes it harder to see the rules there as meaningful, rather than arbitrary. A lot also depends on the tact of the wiki mantainers, and on how much they care about respecting the person behind the text, regardless of whatever must be done with the text. * Persistence: Contributions do have a longer half-life at Stack Overflow, due to basically the same issues related to ownership. Psychologically, one way to make peace with that is adopting a longer term view. Unless your wiki contributions are reverted outright, at least a few people will read and use them as conceived, even if the contributions are deeply modified later on. Furthermore, even if your text doesn't survive in the long run, your intermediate revision might have been a necessary step for someone else to think of an even better version. The reverse of the disappointment in having your contributions reverted or replaced with something worse is the satisfaction at seeing them being substantially improved on by someone else. Trust me, it happens. * Atomicity: Most of what I said for the previous two issues also applies here. Getting over the loss of atomicity involves evaluating your contributions as part of the big picture and in the long run, while viewing other contributors not as competitors but as collaborators. (I don't mean to say it is an easy step to make, of course.) * Visualisation: Same goes for visualisation, though unlike most other issues it is rather easy to alleviate: there just be some way of viewing interesting stats. Even Wikimedia has a (quite well-hidden) [stats page](http://stats.wikimedia.org/) that ranks of contributors by edit counts. As for gamification, specially heavy-handed gamification *a la* SO, I feel it is ultimately antithetical to the collaborative, rather than competitive, spirit of wiki writing, not only as a point of principle but also as a practical one. * Recognition: That is definitely a factor. Here is one way of putting it: while you might not be unreasonable to point to your richly decorated SO profile in your CV, the same thing would be far more difficult in a Wikimedia wiki, as a result of the different model of contribution and collaboration. That being said, mitigating factors include user pages within the wiki that you are, within reason, free to use what you wish, including for highlighting your contributions (a common practice at Wikipedia) and author lists within the books (while such a concept doesn't make sense in Wikipedia, Wikibooks has no policy on the matter, and moderators won't interfere with author lists unless they become too cliqueish). For those contributors who once learned from the book, there is the silent recognition of seeing others actively using your work, and thus feeling that you managed to pay it forward the effort of those contributors that came before you. Divulgation is another way to counter the loss of recognition: by speaking in name of your project, you make others aware that you play a part in it. (By the way, I haven't done much of that for the Wikibook yet because only now I feel that it is ripe for wider exposition. So expect changes in that front over the next few weeks.) &gt;Gamification is king, although it is kinda broken on SO right now. On SO it motivates people to write quick very short answers to very new questions on mainstream topics to get loads of karma, than writing long answers on obscure topics. I write long answers for obscure topics anyway, though, for 2 reasons: I fully agree with your two reasons, and try to do the same as well. "Gamification is king, although it is kinda broken on SO right now" is also true, I see it as a reflection of the old adage that technical solutions can't solve interpersonal problems. Fully collaborative writing under explicit rules (Wikipedia) and collaborative-competitive writing under implicit rules (Stack Overflow) both break down at some point, only the failure modes are different. &gt;Also, if ever build a system for collaborative tutorial writing, what is the best way to tie gamification with it and not create bad incentives? I haven't analysed my instinctive stance on this deeply, but I sense that gamification works on too short a time span to help producing larger bodies of work, such as tutorials and book chapters. Long-term efforts depend more on intrinsic motivation. Also, any bad incentives (which I believe will persist in any gamification system -- even in very good ones such as Stack Overflow's) will be far more damaging to the overall results if the end product is not atomic like SO answers. I belive it is ultimately a people problem, with few obvious solutions, other than having level-headed moderators capable of sympathy and keeping the rules loose and flexible. As a bonus, if the community atmosphere is positive it might happen that those lightweight incentives (i.e. with no consequences other than psychological satisfaction) that don't matter anymore at Wikipedia because everyone is too nervous (Barnstars, The Teahouse, etc.) might actually make a difference. &gt;Maybe the problem is strictly technical: take the best of both worlds of wiki and GitHub PRs, and voila. But we do not leverage human psychology, and even though SO implements gamification, we do not leverage it well enough. As of now I do not know how to intervene other than by being the best moderator I can, and hopefully leading by example. I probably need to think more often in psychological, as that might well lead to actionable knowledge. There is one thing about Wikibooks you managed to convince me of in this discussion, though: just saying "Please contribute!!" is not enough. Well-defined channels for contribution and feedback are necessary. I will start looking for ways to improve the Haskell Wikibook experience in that respect. &gt;SO is such a technical workaround, it leverages human psyche without trying to change it, and SO is a great success (depending on what you believe are its goals). We need the same workaround for creating tutorials, and it should leverage our current understanding of what motivates people to contribute. SO is certainly a great success. However, I believe that finding workarounds for producing longer-form content is a far more difficult problem, non-linearly more difficult. That doesn't make SO any less worthy, or the solution it provides less useful -- there is no value judgement here. In any case, what I wrote when commenting your five points probably reflects which sort of things I believe can be *alternative* sources of motivation in a collaborative environment. 
&gt;So of course I want Wikibooks (or anything similar) to succeed. I would love to participate in writing Haskell wikibooks, however small my contributions could be. That is great to hear! :) And the ethos there is that all contributions matter, regardless of how small. On your project plans: I would have to bake my ideas further to make concrete suggestions, but overall the general idea sounds awesome. Game-making is both cool and engaging, and FRP is fantastic technology which is often shrouded by a completely unnecessary cloud of mystery. It is also a very positive sign that you are explictly concerned about learning strategies. Way too many writers in science and technology do not give pedagogy its due value. If you allow me, I will now make two digressions. Firstly: a while ago I scribbled in my Wikibooks userspace [a few brief notes](https://en.wikibooks.org/wiki/User:Duplode/Haskell_preface) attempting to state the teaching choices myself and others have made while working on the Wikibook. While they may be quite opinionated, I believe most of what is said there can be extended beyond the writing of textbooks, and so you might find it interesting to critique them.) Secondly: At this point, I sense a few words on my Haskell background might be appropriate, as part of a fair answer to your enthusiasm. I am very much an outsider in this community, with essentially no formal education in math or CS, who started programming out of necessity (in a scientifc computing context that now belongs to a very remote past), and who learned Haskell out of curiosity. I'm not a big player: [the most substantial body of Haskell code I have made public](https://bitbucket.org/duplode/stunts-cartography) is ultimately a cute toy or, if you are feeeling very generous, a piece of digital art. However, one thing that I know is that, in spite of all the typos, I can write well. That helps explaining the extent to that I focused on Wikibook from early on: that was a way I knew I could contribute. By now, however, I feel that I have learned well enough to take on more ambitious challenges. I will also be rather busy in the near-future, mostly with non-programming related stuff, and there are a few tiny projects in my hard drive that I want to polish and publish; but anyway I am on the lookout for interesting, longer-term Haskell-related projects to take part on. In any case, the takeaway is that, for all of the resons alluded to above, I will be most interested in hearing (and talking about, and spreading) any news about your plans! P.S.: I noticed that my two-part reply to you actually became a nice summary of how I see Wikibooks, so much that I am thinking about publishing an edited version of it there, in userspace and/or metaspace. Are you okay with that?
The intended usage, so far as I can tell, is to make use of the `map`, `flatMap`, `orElse`, `orElseGet`, and `ifPresent` methods, rather than calling `isPresent` and `get` directly. That is, you would write your example: Optional&lt;Foo&gt; optFoo = Optional.ofNullable(myFoo); Bar result = optFoo.map(nonNullFoo -&gt; ...).orElseGet(() -&gt; ...); But this is awkward for purely side-effecting computations (`ifPresent` can't have an else clause in the same way `map` can). Further, without do-notation, the nesting can obscure code pretty quickly, and it's more difficult to refer to multiple previous results simultaneously.
After eight years of development, its time to stop breaking stuff. Rust is meant to be a practical language, not a research language. All practical languages have warts, and very few have any degree of formalization. You can always be chasing be next thing, but at some point, you have to ship.
&gt; The developer tool stack sucks, the defacto standard compiler is grindingly slow and horribly badly written and virtually impossible to build much less develop, the REPL is a toy, the libraries are atrocious and **the community consists almost entirely of smug weenies.** And here were have /u/jdh30, in a wonderful display of self awareness.
If you use DuckDuckGo as your search engine, you can do "!hoogle func", and it does the same thing. It also works nicely for !youtube, !wikipedia and !arch.
I don't know how nice they are in Scala (might be very nice), but I think I fake this is haskell with GADTs (and ConstraintKinds, for clarity). that is, I had a type with two phantom parameters, each of which lived in he Bool kind, so &gt; data T a b where T1 :: T True b T2 :: T False b T3 :: T a True T4 :: T a False T a b had four variants; T True b, T False b, T a True, T b False each had three variants; T True True, T True False, T False True, T False False each have two variants. thus, given a function of type f :: T False False we only have to pattern match f T2 = f T4 = or i think mine was actually: &gt; data T a b where T1 :: T True True T2 :: T False True T3 :: T True False T4 :: T False False T a b had four variants; T True b, T False b, T a True, T b False each had two variants; T True True, T True False, T False True, T False False each have one variant. this made it easy to manage these related values, without introducing multiple Either types. is this what you mean?
My understanding was that they're not necessarily moving old code to Haskell, but it's become preferred for new features. The most recent post in their [engineering blog](http://engineering.imvu.com/) is about Haskell, and it's well over a year since the [postmortem post](http://engineering.imvu.com/2014/03/24/what-its-like-to-use-haskell/) about the pilot project you're talking about.
The example with the interpreter reminds me of Prolog! Looking forward to trying this.
Hoogle is the search engine to *find* links to the documentation of a function. Those links point to hackage, which holds the documentation for every released package, including, of course, the standard library. The main package of the standard library is called "base", and its main module is Prelude. In addition to base, there are also a few ubiquitous packages which cover really basic stuff and can be considered part of the standard library: bytestring, containers, random, text, time, transformers, and probably others I forget. 
I found that part to be the most exaggerated. Haskell's parallelism and concurrency are pretty fantastic. He seems to indicate in the post that Haskell's parallelism is limited solely to Repa, which is absurd, though he does seem to make all his criticisms from the scientific computing end of the spectrum so it would make sense for Repa to be the most relevant thing to him.
See "[Physics, Topology, Logic and Computation: A Rosetta Stone](http://math.ucr.edu/home/baez/rosetta.pdf)" for a generalization of "program : type" and "proposition : type" to "physical process : physical system", "morphism : object", and "cobordism : manifold".
Facebook's new spam filter is also built with Haskell, using Haxl for concurrency.
The ones OP is talking about: https://www.techempower.com/benchmarks/
What happened to 7.11?
I didn't make it explicit but I did mention one way: libpq, the C binding for PostgreSQL, has textual and binary interchange formats. If you're supporting multiple backends, you'll probably be missing such huge performance gains unless you're putting in a lot, lot more development time into optimizing each backend than one would expect for a generic library.
The version numbering is arbitrary, and whether we call it 7.12 or 8.0 in the scheme of things doesn't matter much, it's just a feeling. We bump it "when it feels appropriate", and in this case there are a few things that are probably appropriate. If types and kinds are merged, I think it's worthy of a super major bump.
Aha! I never watched Colbert, so I missed out on his new word. What a wonderful concept. It's perfect for our time. EDIT: I created a new word once, "bunge." I used it to describe my son, who was a very fussy infant. When he got to middle school, he wanted to use the word in an essay. He searched the dictionary to be sure how to spell it correctly, but it wasn't there! He asked my wife how to spell it, and she told him that I had made the word up. My son was furious!
What's better about it?
Isn't the conv layer/window map thing he's talking about here really a comonad?
The odd versions are "internal" releases. So 7.7, 7.9, 7.11 are all dealine releases that are made to pick out exactly which features will make it into the public releases: 7.8, 7.10, 7.12, respectively.
It's okay. English is not my mother tongue, so I am thankful for all corrections.
I've hardly ever seen a reddit user with negative karma. especially a 9-year user.
The same can be said about any language; Haskell is kind of unique in how far you can go in making abstractions explicit and formalizing them into libraries, so you typically end up with dozens of dependencies even for a fairly small project. Other languages tend to either bake these abstractions into the language itself (making it more opinionated and less flexible), or they don't formalize them at all (making the language more "pragmatic", but putting a burden on conventions, documentation, and the developers' mental capacity).
I've had this in my `~/.ghci` for so long that I forgot what the default looks like: :set prompt "\ESC[1;36mλ -&gt; \ESC[m" 
I think so, yes. You could even introduce some sensible type aliases. Quite ingenious, I must say!
Go ahead and make a pull request, or at least file an issue: https://github.com/haskell-infra/hl
That benchmark is not very good though, it wildly mixes frameworks and raw PHP and similar things with very few features with relative heavyweights like Yesod or Symfony which makes the graphs/tables confusing and misleading at best.
Haste is a great project! I want to give it another try in the future. I think that it would help having a section about what Haste is *not* for. When approaching a technology in a domain as complex as front end development in Haskell, the risk of taking the wrong direction is high and expensive. I believe that the different solutions available now have different tradeoff. It would be nice to know clearly what is the intended goal of a tool, its use case, and what is the anti-use case for it. For example i remember that i read somewhere that Haste is not a good choice if one wants to integrate with existing Javascript libraries, like embedding a jQuery plugin, a React component, a D3 chart, or interacting with PouchDB
I agree, I don't consider this a big problem with Haskell. But having some guidance when you don't know the ecosystem well enough yet would be great.
I haven't read your code yet, but here's a response to 1: http://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Reader-Class.html , specifically the 4th instance `MonadReader r ((-&gt;) r)`.. 
a quick summary rather than throwing a whole textbook on the table?
If you consider the input data as comonadic (e.g. pixel in a context instead of structure of pixels), the kernel can be turned into a function of type **W a -&gt; b** that, when extended over the comonad, becomes the convolution. I don't know if that made sense, but it is how i see it! 
That sounds interesting. Do you have some resources where that idea is elaborated? In what category are you studying taking the F-(co)algebras? Smooth manifolds, smooth sets, Cahiers topos?
So did anyone figure out why Yesod is so slow?? ;)
A nice application that was created with a combination of UR/Web and Haskell is [BazQux Reader](https://bazqux.com), a replacement for Google Reader that I've been using for a while. There was [nice article](https://www.reddit.com/r/haskell/comments/1ve1jp/urweb_in_production/) in /r/haskell a while back that's definitely worth reading.
It very clearly specifies which is which. And how would that complaint be relevant anyways? Does having raw PHP in the benchmarks somehow make yesod slow? No, yesod is slow for some other reason, which we should be concerned about rather than blaming database libs or PHP or saying "I don't like those benchmarks because haskell does poorly on them".
I think the *Borat* explanation is correct. If you check out [the author's twitter feed](https://twitter.com/bonus500) (not really touched since the book's release) you'll see that it – hmm… – fits his humour. :)
Of the examples you mention, I think that "Write Yourself a Scheme in 48 hours" is the oldest. It [dates back to *at least* 2006][1] when it was imported into the Wikibooks project. The tutorial and its title is likely even older. [1]: https://en.wikibooks.org/w/index.php?title=Write_Yourself_a_Scheme_in_48_Hours&amp;oldid=515989
The event sourced view of the world makes fantastic sense in Haskell. It's what we did at http://fynder.io and Haskell is a perfect fit. I don't have time right now, but happy to describe what it looks like in Haskell if anyone's interested?
Thanks for pointing this out. I will remove `MonadIO`. The only reason it is still there is because of one place where I get the current time, so I will add `MonadCurrentTime` too.
I'm also curious about that, especially considering that it contradicts [this old benchmark](http://www.yesodweb.com/blog/2011/03/preliminary-warp-cross-language-benchmarks).
(Warning: unorthodox POV below.) But that way you are exposing internal implementation details. Caller should not know that you are using current time internaly. The next time you'll add logging to your implementation. Will you add `MonadLoger` to the context breaking all the client code? See also http://blog.haskell-exists.com/yuras/posts/effects-encoded-in-types-break-encapsulation.html
Good point. What would suggest as an alternative. Just use `IO`? The original reason I used `MonadHTTP`, for example, was for testing. It meant in the tests I could use the `MockServer` instance of `MonadHTTP` instead of `IO`. Is there another way of mocking `IO`? https://github.com/pusher-community/pusher-http-haskell/blob/master/test/HTTP.hs#L59
WYAS is grammatical though isn't it? so maybe not quite the same
Excellent article. Though one question, how will you implement `fetchWeatherIO` ? If you are gonna just do `fetchWeatherIO = fetchWeather` then you have to define `IO` instance for `MonadHtpp` and `MonadFS` which just defeats the purpose of encapsulation.
This seems less elegant than the type family solution while still not allowing it to be used in regular do-notation (I assume, because no example was given)
Category theory isn't really my home turf, I'm mostly coming at it from the haskell side. So when i say parameterized differentiable F-algebra i mean something like type PAlgebra f p a :: (p, f a) ~&gt; a where (a ~&gt; b) is a differentiable function in the sense of Conals "beautiful differentiation": a function that can give a linear transformation from a to b (the gradient) at all points of type a. So in the end you can get something like: cata :: PAlgebra f p a -&gt; (p, Fix f) ~&gt; a or cata :: PAlgebra f p a -&gt; Fix f -&gt; (p ~&gt; a) As you probably don't care about the gradient w.r.t. the structure being collapsed. I haven't really found others doing similar things (until now), but as I don't really have the terminology or background it's hard to look for. 
If you weren't using `MonadIO` this wouldn't matter, because, for example, a function with type signature f :: (MonadError e m, MonadReader r m) =&gt; m Text can still be used as something like f :: ExceptT e (r -&gt; Text)
Actually where I disagree with you is that I think it's OK if the library makes `IO` an instance of this type class. That way the caller does not need to be upgraded when the typeclass is added. For example if `IO` is an instance of `MonadLogger` (using `putStrLn`).
Except that claim is incorrect, as the benchmarks in question show yesod does *worse* in the benchmark that doesn't involve the database at all.
Once you've finished debugging unicode, we are interested!
Right. But exposing anything other then `IO` API is a bad idea (except probably from some internal module) because you can't provide reasonable API stability.
I'd implement it in `IO` from the very begining without `MonadHttp` and `MonadFS`. I don't like purity at all cost, but that is just a personal opinion, nothing more.
I don't want to bring attention to superficial details, but the author should seriously consider redesigning his site (http://www.impredicative.com/ur/) to look more modern. It can be called more or less responsive, but the design looks as if it was done in the previous decade. [The homepage](http://www.impredicative.com/) is analogous to a butt naked clothing salesman. Also, [the grid example](http://www.impredicative.com/ur/more/grid1.html) does not look user friendly at all. I get it's supposed to be minimalistic and that you most likely would never force users to use UI like this in a real application, but the end result looks like an abortion. And when some people look at examples like this they are more interested in how the code would look like if the UI were to be usable. EDIT: I hope you're not disliking this just because I was brutally critical. If I were to write a domain specific web language and create a terrible looking page for that language, I hope that other people would point that out instead of trying not to sound offensive. There have been plenty of studies done on this subject, but to give you the summary of just [one of them](http://www.researchgate.net/publication/221516871_Trust_and_mistrust_of_online_health_sites): "Of all the factors that were mentioned for rejecting or mistrusting a website, 94% were design related; only 6% were content related." Even if you're the most competent developer out there, you're still not immune to this. I can't comprehend how you can have a homepage asking "Interested in hiring folks to build a web application?" when that homepage look like this. Sure, they were interested in hiring you based on your credentials, but now after they've seen this homepage they are not interested anymore.
How about you take over the debugging unicode and I'll write it up? I believe that's called "taking one for the team" :-)
Yes, I've seen that article before. The "new", nice FFI, with efficient, automatic marshalling for everything under the sun, including HOFs and arbitrarily nested complex data types, has replaced the old FFI since some time mid-2014, but the example code in the Haste repo (and even the docs!) were still referring to the old, awful method until just a few months back, so I can't really blame the author for not realizing that he was using an obsolete interface all along - after all, I pretty much told him to do just that in all the available learning material. I really ought to write a tutorial on that some time, to try to clear up the confusion I caused by leaving obsolete docs in place for way too long.
Those titles always remind me of [English As She Is Spoke](https://en.wikipedia.org/wiki/English_As_She_Is_Spoke).
It seems this could be used to provide a Functor / Monad instance for Repa and Accelerate arrays, which I've been interested in, so thanks!
Apparently stack is well suited for Travis as it supports sandbox reuse across builds and now even reuse of binaries across snapshots. So only the first build will be slow.
So you mean that it can only be an instance if the function from `r` is at the bottom of the monad transformer stack? I guess that won't work for me because my functions perform IO. I guess it could only work if I took the functions to perform IO into the library as dependencies.
Your bald assertion that his 'cause' is helped by my post is a logical fallacy. Please provide proof that it is the case.
Nice! I am happy to learn that this is cleared! Looking forward to try the new Haste!
&gt; portion of the code. the thing here is that clojure was designed upfront as a practical real-life language, and not an academica language. in real-life you simply need a way to do some side-effect operations, or even spawning some threads without expecting any result, hence i would not consider flexibility a real impediment, or a bad practice to have a declarative approach for a "fire-and-forget" approach(if that's the case). They are there(doseq or doall), that does not mean they should be heavily used. Imperative can be seen also through the perpsective of being more interested about the "how" not the "what"; extrapolating, these actors(doseq...) have a very good signature... they are more like a command (doXxx) which i am expecting to do some mutations.
I agree, though perhaps it's from my experience of seeing single page layouts lead to projects with barely any documentation.
in category theory, we like to spend a lot of time looking for universal constructions. things that are somehow "the best" or "the most primitive" way of representing something in your category. this paper goes through a whole bunch of different fields showing how we can unify their notation by just talking about these universal constructions. this can help us to transport intuition back and forth between these fields by recognizing their common structure you can think of plain old categories (those that just support composition and identity) as being at the bottom of a lattice in the same way that [magma](https://en.wikipedia.org/wiki/Magma_(algebra\)) is for abstract algebra. you can attach universal constructions to your category in the same way that you would attach structure to your magma, getting more and more fancy categories as you go. this paper spends time talking about some of this lattice, and what it is like to ask ourselves where we are in that lattice in whatever category we happen to be working in 
I don't know if it counts, but Dorai Sitaram's "Teach yourself scheme in fixnum days" is even older, from 1998.
I found /u/hvr's [multi-ghc-travis writeup](https://github.com/hvr/multi-ghc-travis) very useful.
I'm pretty sure there's an older Scheme- or Lisp-oriented version with a specific number of days, too, but I can't recall or find it.
My colleague pointed that's a parody of "All your base are belong to us"
Wouldn't symlinks be better? Why hard links? (ignoring the Windows issues)
Stylistically, the first text I saw of this ilk was "why's (poignant) Guide to Ruby". The sense of humor, the drawings, all suggest that it was the inspiration. But of course that doesn't say anything about the pattern of the title.
well, with hard links you can bulk delete stuff in a simple way, and not break other usages, because the kernel refcounts everything. E.g. in this example, `rm -rf dir1` could be the equivalent of deleting a stack snapshot. with hard links, dir2 doesn't care at all. With symlinks it breaks [anders@gurney:~/foo] $ mkdir dir1 dir2 [anders@gurney:~/foo] $ echo foo &gt; dir1/foo [anders@gurney:~/foo] $ echo bar &gt; dir1/bar [anders@gurney:~/foo] $ ln dir1/foo dir2/foo [anders@gurney:~/foo] $ ln -s dir1/bar dir2/bar [anders@gurney:~/foo] $ rm -rf dir1 [anders@gurney:~/foo] $ cat dir2/foo foo [anders@gurney:~/foo] $ cat dir2/bar cat: dir2/bar: No such file or directory [anders@gurney:~/foo] $ ls dir2 bar foo [anders@gurney:~/foo] $ file dir2/* dir2/bar: broken symbolic link to dir1/bar dir2/foo: ASCII text 
The "for great good" bit, perhaps, but it doesn't explain where "Learn You a..." came from.
x-post from haskell-cafe I'd like to post results of my small experiment to evaluate how awesome the reuse of binaries between snapshots is. I did the following: 1. Clean the C:\Users\&lt;username&gt;\AppData\Roaming\stack 2. Build my project for LTS-3.3 - 6 minutes 3. Switch to LTS-3.4 and build - 20 seconds! The reason? Here it is: cmdargs-0.10.13: copying precompiled package dlist-0.7.1.2: copying precompiled package executable-path-0.0.3: copying precompiled package nats-1: copying precompiled package mtl-2.2.1: copying precompiled package network-2.6.2.1: copying precompiled package old-locale-1.0.0.7: copying precompiled package base-orphans-0.4.4: copying precompiled package parallel-3.2.0.6: copying precompiled package prelude-extras-0.4: copying precompiled package primitive-0.6: copying precompiled package reflection-2: copying precompiled package mtl-compat-0.2.1.3: copying precompiled package split-0.2.2: copying precompiled package old-time-1.1.0.3: copying precompiled package stm-2.4.4: copying precompiled package syb-0.5.1: copying precompiled package tagged-0.8.1: copying precompiled package text-1.2.1.3: copying precompiled package transformers-compat-0.4.0.4: copying precompiled package utf8-string-1.0.1.1: copying precompiled package vector-0.10.12.3: copying precompiled package StateVar-1.1.0.1: copying precompiled package blaze-builder-0.4.0.1: copying precompiled package hashable-1.2.3.3: copying precompiled package parsec-3.1.9: copying precompiled package distributive-0.4.4: copying precompiled package exceptions-0.8.0.2: copying precompiled package string-conversions-0.4: copying precompiled package blaze-textual-0.2.1.0: copying precompiled package unordered-containers-0.2.5.1: copying precompiled package scientific-0.3.3.8: copying precompiled package network-uri-2.6.0.3: copying precompiled package semigroups-0.16.2.2: copying precompiled package json-builder-0.3: copying precompiled package attoparsec-0.12.1.6: copying precompiled package HTTP-4000.2.20: copying precompiled package bifunctors-5: copying precompiled package void-0.7: copying precompiled package aeson-0.8.0.2: copying precompiled package contravariant-1.3.2: copying precompiled package comonad-4.2.7.2: copying precompiled package profunctors-5.1.1: copying precompiled package semigroupoids-5.0.0.4: copying precompiled package free-4.12.1: copying precompiled package adjunctions-4.2.1: copying precompiled package kan-extensions-4.2.2: copying precompiled package lens-4.12.3: copying precompiled package lens-aeson-1.0.0.4: copying precompiled package Not a single of my dependencies was actually rebuilt! How awesome is that!
I've taken this as hint to extend that README section a little bit :-) Btw, [`lens`](https://travis-ci.org/ekmett/lens) was one of the first guinea pigs for testing the caching feature.
Just did another experiment. After switching from LTS-3.3 to 3.4, I manually removed the snapshot for LTS-3.3 and then did `stack clean` and `stack build`. I got a series of Could not find module ?Network.HTTP? There are files missing in the ?HTTP-4000.2.20@HTTP_5XqAraNG0KY3iQ8yoFqqWa? package, So, what now? Just manually remove the snapshot for LTS-3.4 and rebuild... But I think it's all worth the benefits - it makes upgrading a snapshot completely painless. And you can keep them all because most stuff is reused anyway.
I clicked your fun button and it turned all my datatypes into type synonyms for `String`. I am *not* a happy camper.
That certainly clears some things up. Thanks for commenting, particularly since it looks like you may have originated this whole business. By the way, WYAS was the first Haskell project I took on after finishing LYAH, and it taught me a lot – though, as you said, I think it took me rather more than 48 hours. So thanks for a great resource.
That is indeed a weak point in the ecosystem at this point.
As a Haskell newbie, this is the kind of blog posts that I'd like to read more often. I'm fine with the fact that the Haskell has a more theorical approach to teaching (I learned a lot since I started picking up Haskell because of this), but we noobs also need concrete and well explained practical examples such as this one. Thank you for this post.
I want is as well. If someone tells me what the ideal Hoogle command line workflow for stack would be, I'll implement it. 
My guess is something in Yesod is grabbing a lock. We have a bare-bones WAI/Warp implementation for one of the tests and we would expect it to perform about the same as the Yesod implementation for the same test, but they aren't even close. [See here](https://www.techempower.com/benchmarks/#section=data-r10&amp;hw=peak&amp;test=json&amp;l=1s). On the plus side, the plain WAI numbers are quite good, so it should be possible to get Yesod much higher.
It is not so much deceptive in that the data it presents is wrong, the presentation merely makes it very misleading, e.g. the millions of responses in some of the tests in the lead clearly point to highly optimized code for that particular benchmark, i.e. not regular, old code you would write in the framework in question. Essentially they are overwhelming the reader with the amount of data (and for this purpose it is useful to throw it all in one big table/graph), expecting people not to notice that it is the same synthetic old benchmark crap that has given benchmarks a bad name since forever.
What's wrong with NTFS symlinks?
I've done something alike in my hackmanager [1], but the real solution is that hopefully travis will enhance the GHC (stack based?) support... CircleCI does a better job at this, maybe I'll write a small post on that. [1] https://github.com/agrafix/hackmanager
Looks like the HIW 2015 talks are up on the [ICFP youtube channel](https://www.youtube.com/channel/UCwRL68qZFfub1Ep1EScfmBw) as of a few hours ago. :) Edit: Unfortunately, the audio is not great when the speaker is not at the podium (SPJ's talk, and all talks with audience questions). Perhaps a future improvement would be to use a podium mic mixed with a condenser mic that is pointed toward the audience.
I bought the lifetime subscription for it.
Mutlibulk messages can be nested but only a subset of Redis commands use it in practice.
The only thing that bothered me a bit about WYAS was that the later chapters have few exercises. I found it helpful to see if I could write the functions that the author describes before reading how it's done to make up for this. Another thing you can do to accomplish the same goal is to look through the R5RS specification and code up some things that are left out of WYAS. Good luck! It was an invaluable resource for me, and I hope you enjoy the experience.
Don't get discouraged if the IO syntax seems pointlessly weird for a while! For one thing it is weird. :D But also people often try the "imperative style" first because it reminds them of other languages, while in Haskell it's more of an intermediate topic. Everything makes sense if you stick to simple "pipeline-like" programs at first and get used to thinking in pure functions. Your `main` can often just be `main = interact yourPureMainFunction`.
It's a nice exercise, but storing the whole db in a single tvar will probably result in poor concurrency. Two simple solutions that will probably beat this one are a write lock and atomicModifyIORef. A more advanced solution would be to arrange multiple tvars in a tree or a hashtable to prevent contention. Even then, it's not clear why you'd use a tvar instead of IORefs or MVars. TVars are good for interaction of multiple references or waiting for a condition, but you don't need either. It would be interesting to look at a proper benchmark, though.
I'd try to keep the triangle function pure: tri :: Int -&gt; String tri 0 = "" tri x = (take x ats) ++ '\n':tri (x - 1) where ats = '@':ats main :: IO () main = putStr $ tri 5 
I thought the author was just a Czech and found amusement in using Czech grammar to structure an English sentence.
Please bring questions to `#haskell-beginners` on freenode
Mostly I am dismissive of benchmarks and benchmarks like this one comparing lots and lots of languages and frameworks in particular because we had things like this in the past (the language shootout comes to mind), comparisons that are so broad in contestants and narrow in scope as to be irrelevant but people waste entirely too much time and effort on improving results in them as opposed to improving productivity for real world applications. What is your use case where you need to serialize millions of identical JSON objects, each of them one key long? What is the use case for other small scale examples like those in the benchmark performed that often? You could probably come up with one or two...among all the projects in the world. And even if you do...what is the chance that you will be able to use some obscure framework just for that single, rare project, instead of using something where the learning effort can be used in all the other projects done in your organisation as well? Reality and benchmarks rarely if ever mix in any relevant way and when they do it is usually in some benchmark profiling existing real world code against alternate implementations.
This seems to plague DSLs (of the non-embedded kind) used for tasks where many general purpose programming constructs are needed in general. Build tools (e.g. cmake) and config management tools (e.g. puppet) come to mind here and to a lesser degree templating languages (they can avoid that fate by concentrating purely on view logic which does not need a lot of constructs).
what's sigma lcp of s?
&gt; If we were to sort our strings to put them in order and look at the length of the longest common prefixes of neighbors ΣLCP(S) Sort your inputs. You have to look at all the bits worth of information in their longest common prefixes, because otherwise in the bits you don't look at, I could change them, giving you a wrong answer. This much is true even in the cell probe model, so it is a very robust lower bound. So to sort correctly you have to store the sum (Σ) of the lengths of the longest common prefixes (LCP) of your set of inputs S, even before we actually consider anything about the act of comparison itself at all.
Oops, that's what I get for not reading the code =P
What is eHealth Africa? Where are you guys located? What is the mission? Thanks!
Functional paradigms really have a lot to offer in the DB area. For example, today, I just got a prototype of a database working that uses snapshots and a commit log like acid-state, but can (so far) process up to 300,000 transactions per second on my cheapo $350 laptop (with an HDD, not SSD). I expect that number to drop as I test the reliability though.
Yes, I've given up on complaining about this, but it is truly moronic. This two-column format dates to phone-book CS conference proceedings that simply don't exist anymore, but the editors haven't retired yet. Worse, there is no tradition to post tex source on http://arxiv.org/, in this community. I view this as "conservation of originality": Haskellers are so bleeding edge on their choice of language, they're incapable of original thought in other matters such as publication format. This starts at the top, if our zombie masters would reform their habits, others would follow.
Again, making up silly nonsense excuses helps nobody. If you have nothing constructive to add, then just stop. You are only making the haskell community look bad.
Write Yourself a "Write Yourself a Scheme in 48 hours" in 3 months
It's not *that* hard to do if you're a loudmouthed racist asshole. 
Well to be honest recursion can be rather impractical in languages without TCO.
They are there, but nothing expects them, so in practice they are somewhat painful. Lots of programs choke on them.
The GHCJS project allows you to compile pretty much any non-C reliant Haskell to JS. Example: [markup.rocks](markup.rocks), which is Pandoc compiled down to JS.
Consider opening an issue on the stack repo for this. I don't imagine that manually removing the snapshot is recommended usage, but it surprises me that stack doesn't handle this gracefully already.
I believe `stack` prints the documentation path at the end of running `stack haddock`
&gt; Even then, it's not clear why you'd use a tvar instead of IORefs or MVars Because that way he can make use of STM? IORef/MVar take place under the IO monad.
There's also Haste, Fay, PureScript, Sunroof and probably one or two more projects I forgot about that tackle the same problem in different ways, but I think OP is more about the intellectual challenge than about trying to replace any of them.
`Freer` is the [`Operational`](http://apfelmus.nfshost.com/articles/operational-monad.html) monad. The `Lan` mentioned in the paper is [`Coyoneda`](http://hackage.haskell.org/package/kan-extensions-4.2.2/docs/Data-Functor-Coyoneda.html) -- which is the left Kan extension along the identity functor, so while it is indeed a left Kan extension, it is a particular one.
Currently it seems that all Objects are mutable all the time. This is pretty bad as they're going to sit on the mutable list and [slow down GC wholesale](http://stackoverflow.com/questions/23462004/code-becomes-slower-as-more-boxed-arrays-are-allocated). It might be just better to wrap writes with `unsafeThawSmallArray#` and `unsafeFreezeSmallArray#` (essentially a write barrier!), although it has its own overhead (essentially, two extra writes to the info tag of the object and thawing pushes the object to the mutable list if it's not already there and not in generation 0. I admit this could be painful in hot parts of the code). This should be benchmarked, of course, but I expect that it's better to have slower mutating operations on Objects than to slow down the entire program. Also, one could (and probably should) fine-tune thawing, so that multiple writes to the same object are wrapped in a single thaw-freeze block. Another thing that is a bit irksome to me is the double initialization on `alloc` of Objects. Basically, we first allocate the object, initialize it with some junk value, then overwrite it again with our data. This could be remedied with some foreign Cmm primops for Object construction, but I found that it's something of a performance issue that foreign primops are never inline. On small objects the call overhead would offset the benefits. So I tried a criminally insane solution to this, which involved allocating uninitialized memory with `newByteArray#`, then copying stuff inside it (including info pointers, yeah) with array or address primops. It's pretty fast, but of course it's safe only if our dirty work is uninterruptible, so it must not contain heap or stack checks. I don't know whether this actually works (in the sense of never crashing; it just haven't yet crashed for me) or is a thing that anyone should do. Inline foreign primops would be much nicer in any case. 
I'm a bit puzzled why there's an indirection IORef (Maybe DLL) ~&gt; MutVar# RealWorld (Maybe DLL) I don't understand why the `MutVar#` can't be "UNPACK"ed into the `IORef`. The definition of `IORef` is newtype IORef a = IORef (STRef RealWorld a) So far so good. This is a `newtype` so we don't get an indirection to the `STRef`. But the definition of `STRef` is data STRef s a = STRef (MutVar# s a) This is a `data` so we *do* get an indirection. It seems that `data` is required because you can't have `newtype` of an unlifted type. So am I right in thinking, /u/edwardkmett, that your work here is using a `MutableArrayArray#` to basically do the unpacking "by hand" (or if not "by hand" at least "in a different way")? 
IMHO EIBTI from The Zen of Python is exactly the answer to this claim
&gt; it's fairly obvious that from zero, in 5 mins, you won't get far ahead. Not it's not. When I tried the other day, I was naively expecting something like cabal install --only-dependencies cabal build or even stack build and that was the amount of effort I was ready to put in at that time. Now, I understand it's a bit more complicated that than, hence my giving up. Anyway I was just saying that I'm sure the OP is right and "Hacking on GHC is not that hard" but it's still harder than "stack build".
You make it sound like "making use of STM" is a goal. It only is if you're learning STM and looking for exercises to try it out, but then this is not a well-chosen exercise.
Huggaloo! Couldn't resist, I'll see myself out.
Islam is not a race, and calling out Muslim crimes is not racist.
SPJ shouted from the back of the room saying that he'd change the typechecker to make it illegal. ;) (All in good fun... I hope.) He seemed quite amused at it all when we sat down later to figure out ways to get some new heap objects worked out to make it all faster, however.
As you noticed as you worked it through MutVar# lives in # not * and so can't be moved directly into the object. Here what I'm doing is getting something like * -&gt; # &lt;-&gt; # &lt;-&gt; # &lt;-&gt; # Now, when I go right, I manufacture a new wrapper in *. But if I do so twice or access something in the thing I just wrapped then GHC can see that it is making up a wrapper around something in #, then immediately taking it off. This enables all of the optimizations that GHC has to avoid boxing integers unnecessarily work to avoid boxing up whole data structures unnecessarily. The * wrappers get elided in straight line code.
As a side node, what I was implementing was a new extension I've tentatively called `ArgumentDo`, but maybe should really be called `ArgumentBlock` or something along those lines. See the [trac ticket](https://ghc.haskell.org/trac/ghc/ticket/10843#ticket) and let me know if you think this is a good idea, a bad idea, unnecessary, etc. It would allow code like this: main :: IO () main = withSocket \sock -&gt; do open &lt;- isSocketOpen sock when (not open) do throwIO "Socket still not open!" let timeouts = [1 .. 10] forM timeouts \timeout -&gt; do putStrLn $ "Waiting ... " + show timeout wait timeout The difference from current Haskell is that we don't require `$` when `do` blocks or lambda functions are passed as arguments to functions. 
What about `TrailingDo`?
The reason I don't like `ArgumentDo` is that that I envision this also allows lambdas: forM timeouts \timeout -&gt; .... which doesn't have any `do` in it.
It's an arrow diagram. It translates directly into code. 
In that case `TrailingArguments`, `ArgumentBlock`, `ArgumentBody` or some combination thereof might work. This is a worthwhile extension, I vaguely recall a comment by u/augustss saying that this would be a preferable way of parsing but I cannot find it.
Yes please do go for this! Agda does this (with λ; it doesn't have builtin `do` notation), and it's something I wish Haskell could have too. Edit: If you are going ahead with this, you might want to do something about the pretty printer: &gt; ArgumentDo.hs:8:7: error: &gt; Unexpected do block in function application: &gt; when True (do { return () }) &gt; Perhaps you meant to use ArgumentDo? Because of the inserted parens, this error message doesn't actually contain the error. :p
In .Net they got so frustrated with the compiler they essentially stopped adding features to the language to do a full re-write of the compiler, and in the process modularize things. I remember seeing a paulp tweet (scala guy) being envious of the roslyn design decisions and hooks compared to scala's. https://roslyn.codeplex.com/wikipage?title=Overview&amp;referringTitle=Documentation
&gt; ...hardware-assisted backtracking and choice operations in the presence of effects. Memory effects, yes, but limited and not focused. The hardware does not distinguish accesses that you may statically know are immutable or monotonic or thread local. Also, there are no guarantees. Transactions can spuriously abort. The `TStruct` is about getting the size down to where HTM can work most of the time.
&gt; Also, I really dislike how GHC includes all sorts of internal libraries and functionalities that could (and thus should) have been extracted to 3rd party libraries. From FastString, to the parser, to the IO engine that handles the epoll loop. Could you please open tickets for this, and paste the links here? Or at least elaborate a little bit. It could be a great way for someone reading this to start contributing to GHC. Ripping out code and replacing it by a simple library call is very satisfying. 
Yes that would be helpful. The page I mentioned talks about `brew`, but I don't know if the commands there are correct. There are actually 2 Mac specific pages: https://ghc.haskell.org/trac/ghc/wiki/Building/Preparation/MacOSX https://ghc.haskell.org/trac/ghc/wiki/Building/MacOSX If you could combine them into one page, that would be best. Don't hesitate to delete old stuff that is no longer relevant. Basically delete anything that isn't required to build GHC HEAD. Alternatively, move some sections to a new page with legacy instructions at https://ghc.haskell.org/trac/ghc/wiki/Building/Preparation/MacOSX/Legacy, but I wouldn't worry too much about that.
Wow, that is Best Comment ever ! 
But when I talked to GHC devs, they were against this, because they said that changing these libraries and then rebuilding GHC against them would be more difficult than changing it inline within GHC. Which seems to me like an argument against dependencies in general. You should always fork/inline them for this purpose according to this premise!
&gt; I don't know if "making use of STM" is the goal, but wouldn't it be the recommended way to write such applications? No, that's kinda the point of my original comment.
The big difference I'm aware of is that with stack, you get reproducible build plans, meaning that you're far more likely to get a cache hit than with cabal.
Could you exemplify some applications/design where STM would be suitable/recommended? You said "multiple references or waiting for a condition", but for me that's pretty vague in terms of how that makes them apply to different cases than MVars/IORef
If you find yourself in this situation, please swing by #ghc on Freenode or file a bug with us or something. It's very hard to know what's wrong unless people report it, and it's likely we could have helped find out what is wrong in a few minutes otherwise. I think OS X should work pretty well and IIRC, you don't need much stuff OOTB other than the default XCode tools and some extras for documentation and maybe a newer autoconf etc. Either way, it would be really super to get reports of failures. This page looks pretty up to date AFAICT: https://ghc.haskell.org/trac/ghc/wiki/Building/Preparation/MacOSX
How do I get it to handle the stream closing gracefully, instead of erroring in case of `Fail`. I know it happens in its own thread so it's okay, but I think there should be a better way. Just wrapping it, instead of hGetReplies returning IO a, return IO (Either ParseFailure Reply), and then in commandProcessor do case reply of (Right rep) -&gt; parseAndRun &gt;&gt; commandProcessor handle db (Left err) -&gt; print err -- which will exit this thread ???
thanks for all that info! I think exposing a parser (and the name resolution, and the typechecker...) in a "nice way" would help with IDEs. ghc-mod, ide-backend, etc all are implemented differently and support different behaviors. fewer bugs, less work for dev tools authors. but of course, it's up to you all. also, I am curious why things like parsing or name resolution can't be made pure. even typechecking, though when impure, you can support plugins to external solvers. been using the GHC api, and it's weird to run parseModule and typecheckModule all in a monad. where it seems to me (naively) the only impure stuff is asking for GHC to chase a module's deps on the file system, which can then be given to a more pure pipeline.
Thanks for taking the time to write such a detailed response. I am only familiar with parts of GHC, not the whole of it, and I have some conception of how I imagine a compiler would look like, and that's where my bar currently is. &gt; unless there's a perceivable benefit to doing so I think the benefit would be hugely reduced compile times when hacking on ghc. I don't mind dependency build times (which are often amortized anyway). But if every change on ghc requires 20 minutes of rebuild time, it takes out all the fun and I forget what I was even doing! &gt; The build system has to build all the boot dependency libraries and the compiler, so it is obviously going to need some glue. Why does a compiler need specialized "boot dependencies"? If you take the view that a compiler is a pure function -- is the problem here just that the names being emitted in the compiled output have to match the names in libraries the compiler was built with? &gt; But, we also have to build those libraries multiple ways, and install them properly for both in-place development and actual installations on the system, that have to use different paths, etc `cabal` already does this for profiling/non-profiling builds. If these libraries were dependencies, this wouldn't be an issue, right? &gt; autoconf and make may be considered grotesque but they are proven, In my view, they are proven much the same way that PHP is proven. They "work" but break in cryptic ways. You have to `make clean` to get rid of a cryptic bug now and again. &gt; so we have to manually generate parts of the compiler dependencies during the build phase, using other tools, like a form of metaprogramming We have a lot of this in our project at work too. We just use code generation scripts (and a build system more powerful than make). I think even if `ghc` had a "code_gen" script you had to run manually to do all the code generation, and had to re-run it manually when stuff change, but `ghc` had a simple `cabal` build system, it'd be much nicer to hack on. &gt; We could definitely go about replacing FastString. I don't mean replace it. It isn't actually like `ByteString`. It's an interned string. It should be extracted to its own hackage library. &gt; There is a small bit of tomfoolery in the RTS startup to invoke the IO manager thread and set up default exception handler, but it is otherwise 'in a library'. Do you think this library should be split out of base entirely, to be lower on the stack? I'd use an external C library for the `epoll` management. Currently ghc has the #ifdef-itis to handle `epoll` instead of reusing something like `libev` or `libuv` or some fork thereof. &gt; If you want to use the Parser, it's already in a library: it's called ghc. What do you want to user it for, is the question? `ghc` is so monolithic it is very unpleasant to use it for something like parsing Haskell. You mention name resolution and various analysis phases. If `ghc` were modular, then these phases would all output exportable data-types that I can actually use -- and each could have been in its own reusable library. This would encourage cleaner design, and many more people to hack on these phases not only because of the cleaner design, but also because they're using it in more contexts. &gt; but also whether it's reasonably actionable, and serves a real need For me, the real need is putting the fun into ghc hacking. It was a very unpleasant experience for me because of the bad build system, bad build time, and the unapproachability of such a monolithic blob. A lot of the ghc code I looked at had code quality issues, and I think those are much harder to address in a monolithic megarepo than in many small libraries. Small components tend to be "held in your head". The quality can go way up simply by having many more people (who cannot spend the time to learn how to hack on ghc) hack on those libraries and improve the overall quality. 
For future reference, `brew install docbook2x` will resolve the DocBook issue you were having.
I like /u/Platz's link to Microsoft's Roslyn. It seems to me that they have analyzed various use cases to come up with their modularization. I don't know much about the main part of GHC. My only experience is with the linker in RTS. That module is a monster! At the very least it would be helpful to separate out the code for the different operating systems into separate modules that each use a well-defined interface. The purpose of this would be ease of understanding. It wouldn't change the semantics. To me that's a big win in itself. Reducing the barrier to entry for new developers is worth a lot in an open source codebase.
I think Captain Category has a better ring to it. 
Ok, I found the [newcomers](https://ghc.haskell.org/trac/ghc/wiki/Newcomers) page which explain how to build. I didn't use the `--recursive` option when doing the git clone. I followed the other instuctions and when typing `boot` I got a nice error message telling to do `git submodule update --init`. When I do it get the following maxigit &gt; boot Error: utils/hsc2hs/LICENSE doesn't exist. Maybe you haven't done 'git submodule update --init'? at boot line 75, &lt;PACKAGES&gt; line 42 maxigit &gt; git submodule update --init Cloning into 'libraries/Cabal'... fatal: remote error: ghc/packages/Cabal is not a valid repository name Email support@github.com for help Clone of 'git@github.com:ghc/packages/Cabal.git' into submodule path 'libraries/Cabal' failed At that point I have no clue what to do. Edit ---- Ok I missed the first line which says # needed only once, URL rewrite rule is persisted in ${HOME}/.gitconfig git config --global url."git://github.com/ghc/packages-".insteadOf git://github.com/ghc/packages/ I did it and then tried to reupdate the submodules but it didn't work. However, deleting the project and recloning seems to work. Instructions are clear indeed and workes, however, it's not obvious that the quick build instruction are under `newcomers` 
Basically, have a fixed-size array of refs; map keys to refs via a hash function. Each ref would contain an atomically modifiable map from its keys to values like in the original article. This will prevent keys mapped to different refs from blocking each other. Or use a tree (say, a trie) instead of a vector. An interesting limiting case is an infinite lazy trie where every key has its own ref. These are just my naive suggestions; I'm sure people working on this kind of systems have come up with much more sophisticated algorithms.
&gt; every change on ghc requires 20 minutes of rebuild time Today I added a section called 'Fast rebuilding' to the Newcomers page here, in response to the OP's post: https://ghc.haskell.org/trac/ghc/wiki/Newcomers#Fastrebuilding Can you please try that next time you start GHC hacking, and file a ticket if something isn't working? Also, do you think the current defaults should be changed? The defaults are (if you just do `./boot &amp;&amp; ./configure &amp;&amp; make &amp;&amp; &lt;hack&gt; &amp;&amp; make`): * build a compiler ready for release, with full optimizations, which takes a long time. * track all dependencies. So making a change anywhere, also in the libraries, causes everything that depends on it to be rebuild: first rebuild the stage1 compiler, then rebuild the stage2 compiler and the libraries. This of course also takes a long time.
If we want handlers to see a set, rather than a list, don't we have to work exclusively with commutative effects? That removes flexibility rather than adds to it.
I believe you cloned from GitHub. GitHub requires some 'special' accommodations because the namespace rules are different (they do not allow '/' in their names.) So there's an extra 'step' needed to fix that. (Personally, I suggest people use GitHub for code browsing or a mirror of your patches if anything, and the GHC repository for all actual development) Try using `git clone --recursive git://git.haskell.org/ghc.git` instead, and you should be able to `./boot &amp;&amp; ./configure` out of the box, providing you have `autoconf`. The official repository has plenty of bandwidth anyway so it should still be fast. Perhaps we should really put a notice directing people to always use the official repository, and say GitHub is currently more of a 'back up'.
This is what I just did to build on OS X: 1. making sure I had ghc (I have 7.10.2), alex and happy (I cabal installed them) 2. `git clone --recursive git://git.haskell.org/ghc.git` 3. `cp mk/build.mk.sample mk/build.mk` 4. editing `mk/build.mk` by uncommenting the line `# BuildFlavour = devel2` 5. `./boot` 6. `./configure` 7. `make` 8. ~wait wait wait~ 9. `inplace/bin/ghc-stage2 --version` ==&gt; `The Glorious Glasgow Haskell Compilation System, version 7.11.20150905` 10. profit!
Why are steps 3-5 needed at all?
I don't know much about the main part of GHC. My only experience is with the linker in RTS. That module is a monster! Indeed it is. &gt; At the very least it would be helpful to separate out the code for the different operating systems into separate modules that each use a well-defined interface. I totally agree and would be very pleased if someone picked up the project of cleaning/splitting up this particular file. This is another case of there being too few hands to bring light to the many dark corners of the project.
Your description sounds right on :D. The risk isn't so bad that you can't do anything. I successfully execute Haskell level transactions in hardware, but there is some factor of cost I have to pay for. Interestingly lazy evaluation can be helpful by allowing you to move some execution out of a transaction or minimize the amount that ends up in the transaction. These transformations are hard in the C world. There are also cases where we can commit a hardware transaction and keep updates to thunks before we do any `TVar` updates, but after we know that we have seen a `TVar` level inconsistency or run to an effect we cannot handle in hardware. But note something very interesting: if you have a thunk evaluation race that kills off your hardware transaction, the hardware is telling you that the data is in some other cache. Ideally you might want to keep it there. Restructuring your program to avoid HTM conflicts is good for your program in non-HTM situations. Non-transactional reads and writes are certainly something I would love to have in hardware, but they are difficult in at least two ways. The cost can be quite high. Do you skip the L1? Do you have more tag bits in the L1 to distinguish non-transactional data? The semantics are also much more difficult. When do writes happen? How do you debug these transactions? What is a bug and what is allowed? It is pretty easy to see that TSX picks a nice safe corner of the design space that still gives excellent performance benefits.
Have you considered using Asciidoc instead of DocBook? IMHO it can be used to great effect and is quite easy to install and use, especially with `asciidoctor`. I think /u/thomie has already fixed many of the wiki pages in ways that would lead other newcomers to less confusion (although I did not go astray too much).
I think it would help me (and I'd guess others like me) if you more explicitly recognized that you wear (at least) two hats in these conversations: developer and maintainer. As a core developer, sweeping suggestions dump a load of bricks on your head, and I'm positive none of the regulars want to suggest that. The *maintainer* role, however, is much more strategic. I think a lot of suggestions for GHC development should be met first by answering the question of, if this patch existed and functioned well, would you accept it. That way others can jump in without worrying that their work will be dismissed by upstream because it wasn't strategically aligned. So, as a maintainer, you can say, "I'd merge that!" And, as a developer, you can say, "That's a giant hairball I don't want to touch." If all you say is the latter, nobody will work on it even if they were originally inclined to do so. 
Yes, but there were obvious benefits for doing this. For years a third party vendor had the best refactoring tools as they had much better AST access, the existing (C++) compiler couldn't export the detail required. As Microsoft sell their IDE it's a huge benefit to them to improve their system. The old C++ compiler used global state so could only do single threaded builds, but the new one is based on immutable structures so is easily threaded. Plus now it's self hosted, which is fun. 
`Member (Writer ...) r` is list-membership. You are right, it means that `tell` is modular in that it simply describes "an effect that takes some value and returns no value". But you aren't limited to just a single `tell` in your stack! Each individual `tell` can be typed differently: -- This type is inferred: -- ts22 :: (Member (Reader Int) r, Member (Writer Int) r, -- Member (Reader String) r, Member (Writer String) r) =&gt; Eff r Int ts22 = do x &lt;- ask tell ("bar") tell ("baz") y &lt;- ask tell ((y + 1)) tell (y * y) return (read x + y) -- EITHER: -- ts22r requires (19::Int) -- OR: -- ts22 needs to have a specific type signature -- (the inferred type is Num a, Member (Reader (Num a)) r...) ts22r :: ((Int, [String]), Int) ts22r = run . flip runStateR (19::Int) . flip runReader "23" . runWriter $ ts22 ts22r' :: ((Int, [String]), [Int]) ts22r' = run . flip runReader (19::Int) . flip runReader "23" . runWriter . runWriter $ ts22 ts22r'' :: ((Int, String), Int) ts22r'' = run . flip runStateR (19::Int) . flip runStateR "23" $ ts22 Now, observe the results of running 3 different interpreter stacks over `ts22`: *Eff.Internal.Eff1&gt; ts22r ((42,["bar","baz"]),361) *Eff.Internal.Eff1&gt; ts22r' ((42,["bar","baz"]),[20,361]) *Eff.Internal.Eff1&gt; ts22r'' ((42,"baz"),361) In the first, we treat the int parameter as a state and the string as a reader/writer, in the second, we treat both parameters as reader/writers, and in the third we treat both as states.
If I understood the talk the function we use to lookup keys in the graph has a type like `lookup :: Key -&gt; Graph -&gt; (Maybe Node, Graph)`, where the returned graph doesn't contain the node that we looked up (so we can recurse on a smaller graph). I'm curious if the `Graph -&gt; (Maybe Node, Graph)` part could be factored to use a State monad, while still using ViewPatterns or PatternSynonyms to imitate pattern matching on a true inductive datatype. I'm not sure this idea makes sense, but can we do something like: pattern Lookup key maybeNode = ? foo :: State Graph Bool foo (Lookup 0 (Just _)) = pure True -- node 0 is removed from the graph foo (Lookup 0 _ = pure False -- graph is returned unchanged Essentially, can ViewPatterns or PatternSynonyms be used so that a successful pattern match causes some sort of monadic effect (in this case the removal of the node from the graph?
I'm using Leksah to hack on Leksah and make it a better IDE. The next release will include some important fixes, improvements to the user interface and better integration with external tools. So stay tuned :)!
&gt; Why are steps 3-5 needed at all? 3+4 because the default is to build a compiler ready for release, fully optimised, including (profiling) libraries built with -O2, which takes a long time and isn't suitable for hacking. There are different build flavours you can select, devel2 is one of them. It makes (re)building much faster, see https://ghc.haskell.org/trac/ghc/wiki/Building/Using#Fastrebuilding for details. Maybe selecting between these build flavours could be done with configure flags, but that's just not how it currently works. And it doesn't really matter either, does it? ./boot runs autoreconf in all directories that have a configure.ac file.
You know I've been wondering about this recently, and maybe the default *shouldn't be* a release build, but instead a developer build. It stands to reason if you're going to clone the compiler, you can probably take a small performance hit for the large build-time gain, if you're going to be playing with it. Plus, we answer this question semi-regularly, and the amount of people doing 'release' vs 'developer' builds is small. So it stands to reason we should optimize for the *real* common case...
I'm using Sublime Text 3 (unregistered, installed SublimeREPL(modified) and FluentREPL plugins) with stack. I edit/save a module and then those REPL plugins automatically reload it (with the cabal package's config and other modules in it) into GHCi in another column. It even rebuilds the package if necessary or shows compile errors without halting the interpreter. Installing stack and those plugins is simple and then you only need to modify the SublimeREPL plugin's config file for Haskell, stored in `..\Packages\SublimeREPL\config\Haskell\Main.sublime-menu`, `"cmd": ["ghci"]` to `"cmd": ["stack", "ghci"]` so that the GHCi can load other modules under the package. If you need some further auto-complete functionality, the SublimeHaskell plugin sounds providing it (not sure). About testing, I believe that it's done automatically every time you edit/save if any testing method is specified in the cabal package's config (not sure, maybe another modification is needed in the plugin's config).
I would totally give patreon-style funding to someone who dedicated time every month to writing great docs for Haskell libraries/tools.
Speaking as a Vim diehard, if OP chose emacs, I'd be quite happy.
I guess this would work fine, though the proposed syntax is, IMHO, less readable that the current $ usage.
Also, I really apologize if I sounded accusatory or something or whatever in that last post. It's been something that's been on my mind a lot during my vacation, about the nature of how people 'think about' software development, of how people work on GHC, and what the expectations are for free software that is community driven. It's a complex interaction of human labor, technological constraints, human desires... Working on GHC has taught me many things. I have a lot of 'goals' that I try to abide by for 'leading' such a project (I more organize things), but these aren't written anywhere else other than inside my head. I use them to guide my decisions, so it's hard to see the reasoning sometimes. I apologize about that, and I'm sorry if you feel like I'm "boxing you out" or something. I can do that. Ultimately I want GHC to be the best project it can be while I'm working on it. Sometimes that means I have to be ready to say "no" or "this isn't a realistic result" or "the wins aren't clear" or just "I have no intent of doing this", so that I can direct my efforts better, and not waste the time of others. Sometimes being a maintainer isn't about telling people something can be done, but also what can't or shouldn't be done. So again, I'm sorry if I come off as blunt and unamused, or something. I really do think this is important.
It's redundant but it's consistent. This works, too when True (do putStrLn "works") This way, you can put multiple do-blocks as arguments for a function. 
And of course, for people who'd like a more Vim-like editing environment in Emacs, there's the [`Evil`](https://bitbucket.org/lyro/evil/wiki/Home) package, which can be enabled globally or per-buffer. i spend much of my time in Emacs, but regularly use Vim for sysadmin stuff, and sometimes my fingers 'want' to use Vim commands whilst in Emacs - for that, `evil-local-mode` is perfect. :-)
but if you're thinking of a standard parser, then the `do` keyword should be enough to signal that you're now in a do block, right? expr ::= number | symbol | doBlock And if you want to put an expression after the `do` block then you need parens, but otherwise feel like `do` + indent and non-indent (replaced with brackets in the lexer stage) should be sufficient to unambiguously deal with this case. Obviously the reality is probably more complex, because this fix is needed.
I use chris done's [haskell config](https://github.com/chrisdone/emacs-haskell-config) basically verbatim. Though I use Emac's `evil` mode , which has some binding conflicts, it works well enough for me. Emacs is slightly intimidating, so I strongly suggest printing out a cheat sheet and using it. I've tried using IntelliJ/PyCharm's haskell plugins, and while it works for syntax highlighting/ indentation, the symbol search basically doesn't work. Pretty frustrating...
*Sometimes?* :)
This was a good read. Your tone isn't really accusatory, but it shows you've had previous experience of making decisions that likely weren't the correct ones. It's hard to explain experience like that other than 'this could be a really bad idea if we aren't 100% sure it's going to work better than our current system'. It's easy to look at a codebase for the first time and see gears that could / should be moved, but it's hard to know all the repercussions of those changes. I'm not sure what role you play with GHC (I just saw this on my front page), but one thing I've learned is that there's no replacement for prototyping. It sounds like you guys are pretty torn between pushing forward as GHC experts who understand the rough edges of the system, and refactoring to smooth out the edges. Obviously the first one is more fun, interesting, and rewarding. But, that vision may be isolating the next wave of GHC contributors who could give you the bandwidth to prototype more ambitious code changes. Your rate of 'productive GHC contributors' may increase significantly if you invest a small amount of your GHC experts time into refactoring and providing an easier entry point to hacking on the compiler. Then you may have the bandwidth to try more things that are perpetually low on the priority list
To recover from this situation, you'll need to remove the other snapshot directory too. We discussed intelligent recovery in the issue, but [I recommended against doing it](https://github.com/commercialhaskell/stack/issues/878#issuecomment-136400546) due to performance concerns.
I would bet more people get confused by the current way than they would be with the proposed changes. I never expected the do keyword to behave like a function.
Emacs, haskell-mode, ghci and Cabal is my setup. I've been using Emacs for C and Python for a long time, and the support for Haskell is great. I still haven't done much with ghc-mod, but I just need to work at integrating it into my workflow.
FYI this is a totally standard location for Linux systems. Many other tools use it. I'm still boggled that it isn't included by default in most distributions.
I use vim, and bind keys to "build my project", "run my tests", &amp;c. I would say learn vim or emacs. As for deciding which, the advice I've given in the past has been "which will you have better mentorship for?" You *can* learn either without a mentor, but having good mentorship is probably the single biggest help for either. If you do go with vim, I also recommend vim-adventures.com - fun game and should help you learn lots.
vim + hasktags + stack ghci for REPL. And Xmonad to rule them all.
Here's an argument against. This saves a character here and there, but it seems inconsistent and unpleasantly full of special cases... something that you almost never want in your parsing rules. For example, why does this apply to do and lambda, but not to let or case? Is it just because you tried and got parser ambiguities with those? Or am I missing some simple and logical reason why I should expect the syntax to work this way?
&gt;What about Haskell itself? In the long run, could it evolve to the point where it becomes the norm? Evolve? All the other languages have been stealing from Haskell for 15 years!
Out of curiosity, why link to Phillip Wadler's content-free blog post, rather than the the article on WIRED itself?
Currently `do`, `case`, `let` and more are allowed as infix parameters but not as prefix parameters. I'd say that is more confusing.
I agree that it should apply to all the other keywords as well and not just `do`. For me it's not about saving a character, but about: * Not having to explain `$` to newbies I show Haskell to whenever I want to use a `do` block as a first-class argument to a function. * Eliminate one of the most common uses of the `$` operator, cleaning up the code from line-noise. I'd expect the parser to work this way, because there's no ambiguity or confusion when I write `forever do ...`, I think it's as clear as it gets.
I've been using [spacemacs]( https://github.com/syl20bnr/spacemacs/) for a bit.
That's a great idea. Does anyone know if anything like this been attempted before? (Not in the Haskell community, obviously) And if it worked long term or fizzled out?
To paraphrase /u/gfixler's comment: Speaking as a serious Emacs user, if OP chose Vim, I'd be quite happy. :-) Both are powerful editors - *really* getting to know *either* one can pay off in spades. 
I would recommend Vim or Emacs. I'm a hardcore vim user but I couldn't recommend one over the other. However which ever tool you are using, I highly recommend learning to touch type if you don't: all keyboard shortcuts are useless if you have to spend 2s to find the keys.
To those interested, the academic name for scoped continuations is "delimited continuations with multiple (named) prompts". The post provides some references to treatments by Phil Wadler and Oleg Kiselyov.
This is a monadic (or pure) encoding of the impure [scoped continuations](http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/) ("delimited continuations with multiple prompts")
I agree with you about debugger, you can still use an external debugger when you really need one ( for example develop using vim and debug with leksha )
1) Do you mean "linear" as in "linear types"? Then the answer is no. The graph is immutable, stored as a persistent map internally. When we " remove " a node and its edges (a context), we're semantically creating a new graph data structure that doesn't have the node or any edges to or from it. It's just like removing an entry from a Map. So the graphs behave just like other immutable, persistent values in Haskell. There's no need to always only access the last version, at least from the point of view of correctness. The reason we rely on ViewPatterns is because the graph type itself is abstract and we can't match directly on it. 2) Again, the graph is immutable, so you're not changing the old graph at all. If you have references to nodes from the old graph (for example, entries in the stack from our depth-first traversal code), then you can just rely on the fact that it isn't in the graph any more and the match function returns Nothing. Did that make sense? I think the main insight you need is to think of graphs just like any other functional data type in Haskell (ie Data.Map or, better yet, Data.Sequence which relies in view patterns itself).
Also for tool-development itself? At one point, I actually considered Patreon for funding development time for my [Threepenny-GUI][1] library. [1]: https://wiki.haskell.org/Threepenny-gui
I had a few drinks and I think you're right to a good extent. Wearing both hats is difficult and tiresome. And it's hard to separate "I think this is a long term viable strategy" from "I think this is cool" *when I really want both*, and more importantly it's hard to translate that language to potential contributors. It's really easy to take away "your idea is not good" when the actual reasoning is *so much more complex than that*! Still, I feel like - if something just *isn't going to happen realistically soon*, it feels wrong to say something other than that, you know? It's kind of like 'too many cooks'... You have a lot of people who want many things. I hope I don't come off as overly negative. I guess I just don't want to come off as a 'yes-man', because it's something that's dangerous IME and something that will sour faith in GHC as a project. I've gotten the impression many people are already soured, despite the needs we have to meet. It always comes back to the home plate. I just don't want to preemptively commit to things that just *don't seem doable* and get people's expectations up. The real world is fucking difficult, yo. Is being an academic any easier? Implying that's not the real world, I have no idea? Plz say yes.
When I start writing a function name in Atom I get a list of names that match from everything in scope, along with their respective types. It also works with qualified imports so when I write Map. i get all the map functions. Does haskell-mode on emacs offer this feature?
Which is a fantastic outcome for a research language, though I'm still disappointed Haskell is the only (usable?) purely functional and lazy language around.
*Request for brainstorm.* I realized there's another way wikis curb motivation to contribute. Take for example [Haskell wiki page on Threpenny-gui](https://wiki.haskell.org/Threepenny-gui). Is it complete? Is it not complete? It's hard to argue either way. When I look at a wiki page, I don't know how should a complete page look like. A wiki page is always “work in progress”, there's always ways to add new stuff, and polish, and edit, and revamp, and refactor. And the “open-endedness” of this wiki-page doesn't feel *right*, psychologically. Therefore, when I plan to add 10 examples of how to use Threepenny-gui, it doesn't feel like I started approaching completeness. On the contrary, if I add 10 examples as a SO-answer, or a separate blog-post, or some repository of examples, it feels like atomic, finished contribution. A contribution for a body of knowledge that is *supposed* to be incomplete. When I go to SO and don't find some knowledge, I don't feel “Oh, what a terribly *incomplete* web-resource”, I just ask a question. On a wiki, what do I do? Request a contribution? But there's no facility for requesting a specific contribution on a wiki, and people feel much less motivated to specifically contribute to answer your question on a wiki, than on SO or mailing list. When I go to https://wiki.haskell.org/Threepenny-gui, I see a page that could be super-fancy but isn't. But I have no idea, how would a super-fancy page look like. I don't see a path from the current state to a “complete” page. Should I add 10 examples? Should I add links to alternative tutorials? Should I add a section on the problem that I encountered? Adding to a wiki page feels like constant deliberation between things that belong and don't belong on a page. You could say: “Hey, a wiki-page for Threepenny-gui should just be a high-level overview, so it's *obvious* that you should put some general, widely-applicable descriptions in it, and link to more specific stuff”. But then wiki becomes a duplicate effort, because the most appropriate place for a high-level overview is `README.md` on GitHub repository for this particular software. Now, GitHub is a proprietary service that has nothing to do with Haskell. But then, ideally, there should be a dedicated repository for project homepages and high-level overviews of all Haskell packages, and it shouldn't be on the wiki. Maybe ideally Hackage should be upgraded to support project homepages. In other words, I find it strange to copy-paste something, anything, to a wiki. Original text is supposed to *originate* on a wiki, otherwise if there are better place to put this information, then it shouldn't be on a wiki. Otherwise it's fragmentation and duplication of effort. Right? (You could say: “Hey, you can create all sorts of articles, like *Examples of Threepenny-GUI usage* and *How to create a button in Threepenny-GUI*, just like there already is [*FRP explanation using reactive-banana*](https://wiki.haskell.org/FRP_explanation_using_reactive-banana)!” But these are articles written by one person, so we go back to the problem of small core of maintainers and how PRs are better for contributors than wiki). This all is rambling, so I'll continue to flesh out this thought.
All because Haskellers are allergic to parentheses.
Clean
I use plain vim, no plugins other than ftplugin; plus zsh, xmonad, urxvt, cabal, and ghci. Firefox as my main browser, chromium for testing client-side stuff.
That would be useful, but certainly less easy-to-read in some cases.
I find point-free style much easier to write with curried functions. Also, removing unnecessary parentheses is always nice. For instance, I have a gag reaction when I see code like this: f(a, b, g(c, d)) which is, in my mind, far inferior to: f a b (g c d) And in the end, Haskell is named after the guy who currying is *named* after. It feels wholly against the spirit of Haskell to *not* use currying.
In short, curry is great , uncurry has no advantage whatsoever. There are real of advantages to curry, mainly partial application, less to write, easier to read when you are use to it ;-) There are no real advantages of uncurrying functions. All the drawbacks you are mentioning are just about subjective readability, which can change over time. I personally find `(A -&gt; B) -&gt; C -&gt; D -&gt; (E, F)` much easier to read that `(A -&gt; B, C, D) -&gt; (D, E)`. I have for example no idea, what could be an object with the `(A -&gt; B, C, D)` signature. Moreover there are disadvantages to unccuried functions. Of course, partial implication becomes really hard without having to add extra syntactic sugar. What you suggest also implies creating tuples before calling a function. The compiler could probably do some optimization to not have to call create them on the heap, but that's unnecessary. Also, `(a, b)` is just syntactic sugar for `Pair a b`. Haskell could work without the notion of tuple built-in, so why should we make it necessary ? The only good argument about uncurry is when you have function with LOTS of argument. In that case, the consensus is to create a dedicated type (often a `Monoid`) allowing also to deal with default values. 
Commas are syntactic, not semantic. Maybe the language should have commas, maybe not, but changing the semantics of your program to gain a syntax seems... odd. Like the people who abuse do-notation for non-monadic ends.
&gt; I find point-free style much easier to write with curried functions. Yes, but possibly not when you are dealing with those imperative stuffs. &gt; Haskell is named after the guy who currying is named after. It feels wholly against the spirit of Haskell to not use currying. I know. We should use curry, but that doesn't mean we should always use currying when possible.
&gt; Commas are syntactic, not semantic. Yes. But currying is semantic. &gt; but changing the semantics of your program to gain a syntax seems... odd. It is not only about syntax. When you curry something procedural, it doesn't really make sense.
Do you integrate evil and SHM in any way?
Why are you making the distinction about "imperative" stuff? I don't know which package your popen example is from, but isn't there a similar read function that it would make sense to partially apply the handle to?
&gt; It is not only about syntax. When you curry something procedural, it doesn't really make sense. Sure it can. Let's say you have a `generalHttpGet :: RequestOptions -&gt; String -&gt; Result a` that you got form a library. You can easily make a more specific version for whatever your purposes are via `myHttpGet = generalHttpGet myOptions` (I like to put these sorts of things in as `where` clauses). And you don't really lose anything from having all your functions curried and ready for partial application so why not? You don't have to partially apply a curried function.
&gt; Try the latest Leksah. It's maybe not "mature", but it just works out of the box with zero need for bizarre rituals and incantations. That's still not my experience, or I have wrong expectations. The 0.15.1.4 download gave me an IDE with syntax highlighting, auto-compiling, inline error highlighting, completion of haskell keywords, and linting (which is pretty great). But no completion of haskell module names or names in scope, jump to definition, show usages, display type signature/haddock of selected thing, refactoring, or browsing of available modules/packages (Modules pane is empty). Enabling Debug -&gt; GHCI doesn't seem to do much, I don't see a REPL or how to set a breakpoint in the source pane. It hangs on CMD-q. (OSX, ghc 7.10 installed with brew). [Also, venturing a little into the bizarre rituals: the ghc-pkg recache recommended by the manual didn't help. Nor did removing ~/.leksah* and running the leksah-server invocation to rebuild metadata ($ /Applications/Leksah.app/Contents/Resources/bin/leksah-server -sbo +RTS -N2 -&gt; No entry for "cross compiling" in "/usr/local/Cellar/ghc/7.10.1/lib/ghc-7.10.1/settings").] 
Ah, the Lancer/Deuteragonist.
If you set the layer variable in the `.spacemacs` file like so (haskell :variables haskell-enable-shm-support t) You get the bindings listed [here](https://github.com/syl20bnr/spacemacs/tree/master/contrib/!lang/haskell#structured-haskell-mode). The way I use it, anyway, everything else is either transparent or the binding is how I'd probably want to do it in Vim, anyway. Roll in `evil-god-state` while you're at it and you get Chris Done's `god-mode` as an Evil state, so you can do all the fancy Emacs keybindings without ever reaching for a chord.
It's not `printErr` and `printOut` that doesn't make sense. It's the expression `hPrint stdout` that doesn't make better sense than `\s -&gt; hPrint stdout s`, because there is no good reason that `a` must be put after `Handle`. That would vary between different libs.
&gt; Why do I need a function to generate a function like FileMode -&gt; IO Handle? That function would be odd, but a function &gt; openReadOnly :: String -&gt; IO Handle Would totally make sense, in my opinion. Perhaps the order of the parameters in popen should be: &gt; popen :: FileMode -&gt; String -&gt; IO Handle 
&gt; Perhaps the compiler could be improved to take the definitional arity of curried functions into account when generatnig error messages? It does that already, see https://git.haskell.org/ghc.git/commitdiff/bc2289e13d9586be087bd8136943dc35a0130c88#patch4 (Note the "maybe you haven't applied enough arguments to a function?" part of the error message.)
When you work with a language such as Haskell, you really expect a lot from your editor tool set (at least I do). Unfortunately IMHO there is nothing that is really up to the task. However the situation seems to improve slightly over time. AFAIK vim has a bad story about async messages, so I don't see how it would fit (maybe `neovim` would do). I personally cannot bear vanillia`emacs` (crazy default key bindings) but I do enjoy using `spacemacs` or `evil`. Still for a rather large project, `spacemacs/emacs` is slow as hell. This is probably not `emacs/spacemacs`fault but rather the combination of plugins and tools I use (`ghc-mod`, ...). `Atom` might be an option but I have found the vim plugin to be quite lacking. If you like a modal editor, you are out of luck with Atom. If you are not ready to invest too much on "Vim/Emacs", I would first go and try Leksah. The last time I have checked it was unstable but I have heard the situation is improving. 
If `printErr` makes sense, then partial application make sense. If partial application makes sense, then curry does. `hPrint stdout` is shorter and less noisy than `\s -&gt; hPrint stdout s`. (I could also write `\a -&gt; hPrint stdout a`, `\foobar -&gt; hPrint stoud foobar` etc .. there is a redundancy is unnecessary and therefore noise ). Now, you are right, we are lucky that the argument of `hPrint` are in the right order. Had they been in the other order, I could have build other meaningful example `hPrintHello = hPrint "hello"`, or just flip it using `flip` ... However I still prefer to have 50% change of being partialize a function than 0% (with the uncurry version).
Nice writeup. You may be interested in mentioning [`ShortByteString`](https://hackage.haskell.org/package/bytestring-0.10.4.0/docs/Data-ByteString-Short.html#g:2) as well, as those avoid heap-fragmentation (which `ByteString`s cause due to their use of pinned memory).
Fwiw, I've made [an attempt at reimplementing `arc` in Haskell](https://mail.haskell.org/pipermail/ghc-devs/2015-September/009827.html)
You can avoid having to remember or look up all those conversion functions by using the [string-conv package](http://hackage.haskell.org/package/string-conv), which handles all monomorphic conversions with one simple `toS` function.
Better, but still not as good as the message for uncurried functions.
`Operational` is the first reference. So I guess the authors know that ? As a tl;ds I would like to understand what's the proposed improvement over what we already know or have. 
Awesome! :D I am noticing some potential overlap between your repository and [this one](https://github.com/parsonsmatt/hasktuts). Mine is specifically focused on tutorials while yours seems to have documentation as a focus. Yours has the backing of an actual user's group at the moment, so if you envision your repository as encompassing the scope of mine, then I don't mind directing traffic to y'all.
Is the ineffecency of `String` really the reason we have `ByteString`? Even if that is the case, is it worth using those words? The reason I'm worried is because people are already having problems when they think `ByteString` is for textual data, which it's not. That's `Text`. `ByteString`s are for binary data; arrays of 0s and 1s. Edit: I guess `ByteString` works if your idea of human language is that it's something that fits within 7 bits per character.. heh.
Not really. But it does seem it's the kind of thing that one could make a large impact with, if they applied themselves to documenting all sorts of different libraries and tools. 
One thing that came up in the solution-to-the-record-problems thread is you could use anonymous records to have functions like: foo :: { field1 :: type1 , field2 :: type2 } -&gt; returnType which would be useful especially if the function has multiple arguments of the same type, but without an obvious ordering. But a lot of functions do have significant ordering, which makes them work better with `(.)`, `(&gt;=&gt;`), `(&lt;*&gt;)`, `(Pipes.&gt;-&gt;)`, and lots of other combinators. I wouldn't want to trade syntactic noise for functions which take records (which already exist and can be used without difficulty) for syntactic noise for all other functions. And tuples themselves simply aren't that useful, especially large tuples, when records are more semantically meaningful.
What's the point in not using folds?
I would appreciate it if this proposal included a formal statement of the new syntax, so we know precisely what the new rules would be.
This is an excellent statement of why and when emacs is appropriate. I'm a lawyer in my day job; I always have emacs running for Org (hello, billable hours!) and for a variety of text manipulation crap that's easier to do there than in Word. Not having to spin up special editors for various other tasks, e.g. programming, is a big bonus. Shared knowledge is key. (It's a shame that the emacs ecosystem as a whole is, in my experience, shockingly unstable, inconsistent, and confusing for something that's had like forty years to stabilize. But that's just my desire for meaningful documentation and static types.)
One very important part of my development setup is [sensei](https://github.com/hspec/sensei). It loads modules I work on into a ghci session and -- if they contain an hspec test-suite -- executes the test-suite. Both test failures and compile errors and warnings are reported. Then I use *seito* (included in *sensei*) to load the output into a quickfix list in vim.
Hm, what about the opposite? I'm mainly trying to get enough requests for documentation together so that everyone has something to work on, which your repository helps solve. Absent some concerted effort by me to make this a regular thing, my repository probably won't live very long, whereas you're already maintaining yours long-term.
It seems unwise to fragment the community along function argument style. We have a good thing going right now where everybody agrees on a uniform argument style, so why ruin it?
Why is the haskell users group meetup on a weekday and not on a weekend? People that are interested to come but cannot due to work. If it was on a weekend, it wouldn't have been a problem.
I don't think I'd really call `ByteString` a string type...it has no semantics for being a string, and really is supposed to encapsulate nothing more meaningful than a list or array (depending on lazy or strict) of bytes. "Converting a string to a bytestring" doesn't really make much sense...what encoding are you encoding it in? The choice is ambiguous and comes down to weird implementation quirks and hacks and doesn't actually make sense in the context of what a String is...and doesn't pay any respect to the semantics of String and so is more or less a meaningless conversion. Text is actually designed to be a semantic *encoding-agnostic* container (following unicode standards). It's supposed to be able to understand text as text, regardless of encoding. So, that means that from the encoding functions in `Data.Text.Encoding`, it can *decode encoded bytestrings* from different encodings (utf8, utf16, etc., anything you'll probably encounter in the real world) and it can *encode to bytestrings* from different encodings, as well. Text operations deal with text and are an abstraction over whatever encoding they might be in. Think of it this way. I have a program that uses a lot of `Int`s. Should I go in and replace every `4` with `"4" :: String`? No, that's pretty silly. `4` the `Int` gives me a powerful API to deal with `Int`s *as `Int`s* -- they can be added, subtracted, multiplied, converted to different numerical formats...if I want to work with integers, I'd use `4` and not `"4"`. `"4"` to do arithmetic is just silly. Can you imagine `fib :: String -&gt; String`, which takes a String representing an Int and returns another String representing an Int? It's the same thing for ByteString and Text. Text represents text in a meaningful way and has an API for manipulating text. But using ByteString to store text is like using `"4"` to store numbers. It doesn't even make sense to store numbers as strings...there's more than one way! What if you wanted to encode 4 as `"four"` instead? And there's no API in bytestring to deal with text. It just doesn't represent text...it represents a string of bytes. And `ByteString` can really mangle your text if you don't pay attention to the implementation detail and exact quirks of the hack you are doing.
That works for me :) 
That's interesting, I'd always wondered how exactly you wrote the JS FFI. Thanks!
It's just an interesting exercise in polymorphism. We all know that we could use folds for many of these functions (save maybe printf), but that's not the point. It's fun to make the compiler work for you.
Usually because the arguments don't fit in a list because they have different types. Like if you wanted to abstract this pattern: concat [show a, show b, show c, show d] into one concatShow function, it won't work without using the polyvariadic trick. Personally though, I think I prefer defining operators for this sort of thing, like: let a +: b = a ++ show b in "" +: a +: b +: c +: d
I was going to remark on the "Text is for Unicode" stuff too but then I realised that I had, as it seems you also have, confused Unicode with various UTF encodings. They are not the same thing. Text by and large follows the Unicode standard for handling of characters, which is related to, but just like Text independent from, the UTF encodings.
thank you for this correction :)
What encoding does string-conv use for "converting" String to ByteString?
Care to show us what `f` is? I think it's a pretty important part of your question.
I get the same results if I replace 'f' with 'fib 30'.
Pardon the naive question: why not just putting the .stack in the CACHE_DIR instead of copying? (or symlinking?)
Assuming that /u/Jookia is right about what `f` is (and that does seem to be the case), that happens because the [blessed monomorphism restriction](https://wiki.haskell.org/Monomorphism_restriction) does not kick in by default in GHCi [since 7.8](https://downloads.haskell.org/~ghc/7.8.1/docs/html/users_guide/release-7-8-1.html), presumably to make playing around in the interpreter more convenient. Beyond turning on the restriction again, you can avoid recomputation by specifying a monomorphic type for `f`: GHCi&gt; let f = fib 30 :: Integer 
whoops, forgot to put that in
&gt; come with very strong alignment conventions You're saying that's a bad thing?
Basically the proposal is to use the operational monad as a basis for reflection without remorse rather than the 'Free' monad. The result is a benchmarkable win when you don't need any effects that don't fit into this framework, and if you need multiple 'inspections' of the monad like with reflection without remorse. You can think of this as "Operational Reflection without Remorse".
And just to clarify, if you type `:t f` or, before entering the commands, `:set +t` (which shows type information), GHCI will helpfully tell you: &gt; f :: Num a =&gt; a Because `f` is polymorphic, its value is not shared. Well, that's a little bit incorrect. When you have ghci print out the value of `f`, it evaluates `show f`, which evaluates `f` at a monomorphic type based on the default rules. The result of that computation is *not* the same as `f`--which is polymorphic--so it's not helpful in future computations.
`f` is polymorphic, so you can't really cache anything. Every time you ask for it, it has to be recompiled for the type you want. Do you want it as an Int this time? as an Integer? as a Double? a Complex Rational? It's infeasible to memorize every single possible type you could ask for it as, because there are no limits to how many Num instances you can write or call. the easiest fix would be to specify that you want `f` to be one type, like `Int`, so ghc knows to cache it. 
I agree that `FileMode -&gt; IO Handle` is not a terribly useful function, but we're not encountering it (only) "because of currying", but also because of the argument order for popen was chosen poorly. We are far more likely to want to open a bunch of files with the same mode than we are to want to open one file with a pile of different modes. `String -&gt; IO FileHandle` is a far more useful function. If you *are* interested in marshaling all the arguments at once, I would prefer creating a data type so the argument names are clear, default values can be provided, and so on. Uncurrying into a tuple seems rarely the right call, absent a specific need. 
https://www.codementor.io/haskell-experts
For one thing, Idris is a strict language. Some people think that Haskell should also become a strict language in a future version, but it may make more sense to support both strict and lazy types as first-class citizens, via polarity and focusing. The gist of it is to introduce a distinction between single-constructor datatypes (strict) vs. tuples/records (lazy), and so on. Moreover, Idris has support for proof construction for which there's simply no equivalent in Haskell. Future versions of Idris will probably also include some sort of totality checking, something which, again, Haskell does not focus on.
Yes, any time there's one way to communicate something to a compiler, and it requires a different way to communicate the same thing to a human reader, which is unchecked and can be wrong, that's definitely a disadvantage.
thanks for this suggestion, i've edited the comment :)
Once you have proper dependent types, a lot of the motivation for coherent type classes just goes away. For instance, you can just parameterize things like `Set` and `Map` by some comparison dictionary. Overall, this improves code modularity compared to typeclass hacks.
FWIW, Idris has had a totality checker for a long time, AFAIK as long as it's existed.
Good man.
The question is: why? Do the GHC devs test against ARM before release?
I'm not familiar with the effects library, but I'd assumed that it was possible precisely because of dependent types.
The effects system is really a library - you can see it as an EDSL for composing effectful computations. But I don't think that it's fair to say that it's more powerful than monad transformers, because each abstraction supports things that the other does not. For example, re-ordering the effects list doesn't change the meaning of the effects, while reordering a monad transformer stack does change the meaning - this ordering is expressive power. Really, this is a matter of "right tool for the job" rather than "one wins clearly". As far as the future goes, there are many more programs yet to be written than have ever been written to this day. There's plenty of space for a robust community of functional languages, taking inspiration and good ideas from each other. No need to overshadow - we can code in both!
title aside, I appreciated the post and hearing about what works and where the trouble areas still are. actually motivating for me to give it a try and report any problems I see to the team - or even fix them.
1. Why do you call coherence a "type class hack"? How is this a hack? 2. Parametrizing `Set` sounds good at first, until you think too hard about it, then it doesn't. 3. `Set`/`Map` is just a very specific case of a general problem. I don't want to play broken telephone here, so I'll redirect you to the relevant talk by Edward Kmett: [Type Classes vs. the World](https://www.youtube.com/watch?v=hIZxTQP1ifo). If it doesn't convince you, nothing will. It certainly convinced me.
There is no plugin support yet, so this could be a very useful area to explore. I think /u/hamishmack had some ideas about GHCJS based scripting, but other ideas are of course also welcome. Don't hesitate to come by at #leksah to discuss or start a discussion on the github issue tracker.
What's wrong with parametrizing Set?
Just curious, why would you want to pay for a mentor if you can teach yourself?
There's even more to a language than its type system and RTS. We also need to think about programming tools and environments, as well as the way that those things influence the creative process. I don't think that we yet have a good idea how Idris's RTS will look in 10 years - right now it's a pretty open question, and new hackers showing up have a good chance of molding it to their wishes :-)
By no means I wanted to sound condescending (or, worse, hurtful!). Sorry for this. I like many things about Idris and it makes me sad to see the language crippled (in my eyes) by incoherent type classes. &gt; Should I be able to assume that any two dictionaries for a type class at a particular type are judgmentally equal? Yes, and no proofs required. This is a job for the compiler. I like to think of a type class as an open function of type `⋆ → Dict`. A function should not have different outputs for same inputs dependending on where I call it. I don't know how to answer your other questions, but I'm sure the answers can be found (or someone else already has them).
I am just guessing here what they meant but the first thing that comes to mind is the fact that you suddenly have lots of incompatible variants of Set.
Yeah, for posterity, as I just did this 2 days ago on my first MacBook ever (from work): 1. install homebrew 1. install Xcode (I used `$ xcode-select --install`¹) 1. open Xcode and go through its licensing stuff², then close it 1. `$ brew install macvim --override-system-vim`³ 1. `$ vim` or `mvim -v` for terminal MacVim 1. `$ mvim` for graphical MacVim, similar to Gvim on Windows/Linux ¹ you need the full install for step 4, not just the command line tools ² if you don't do this, the brew install fights you impossibly about root/sudo ³ that flag is provided by a Vim brew formula
I think that using the term "crippled" here in this way has a reasonable potential to be unpleasant for some people with disabilities, which is perhaps not what you're looking for either. That aside, when you say: &gt; This is a job for the compiler. you're saying that the compiler needs to enable this kind of reasoning for free. This is a great thing to have! I like my compiler to do work so that I don't have to! But for the compiler to do it, we need to know how to implement it. In Idris, this means that the restriction must be made either during translation to the core language, or in the core language itself. Given that dictionaries are values, I think that the reasoning really needs to be available in the core language, because a dependent type system lets users talk about the identity of values inside of the types. If dictionaries were not values, then we'd be in a situation where proofs had no way of reasoning about their identity, which would mean that we probably wouldn't be in a position to do things like prove that an implementation of `Applicative` is lawful (because it relies on the `pure` and `&lt;*&gt;` coming from the same dictionary). So, at least at first glance, it seems that global instance uniqueness would require special support in the core. And then we're down to needing to be sure that we haven't broken the type safety of the core language by this addition, which (as far as I know) nobody has demonstrated how to do without having other undesirable effects on the system. &gt; I don't know how to answer your other questions, but I'm sure the answers can be found (or someone else already has them). The thing about doing research is that this isn't always the case. There are still things that humanity as a whole doesn't know. In fact, I don't think that there are known answers to all those questions, but I'd be very happy to be proved otherwise if anyone reading this has good references. But what I was really going for was to underline for you that this isn't a simple, easy question, that the design space for programming languages is large and fraught with unintended interactions of features, and that it's not so useful to stand on the sidelines taking pot-shots. If Idris not having global instance uniqueness means that you don't want to use it, I can totally support that. It makes sense - we want the tools that we're used to. But having a different collection of tools does not necessarily make a language bad. 
My one thought here is that indeed yesod is a fairly heavy web framework for minimal stuff, and if I were doing ARM work I'd look for something with less need for _stuff_. (And indeed, if I had something without TH, [i.e. not yesod] I might even have a chance to get cross-compile to work). There are much more lightweight frameworks that also run over wai, such as scotty (http://hackage.haskell.org/package/scotty). Also, the "minimal core" of other frameworks such as snap-core or happstack-server might themselves also be lighter weight... And failing that, writing to a CGI interface is feasible for when one wants to go _really_ lightweight :-)
yep, in a way its impressive that as much 'stuff' compiles successfully, even if it does fail at the link step. I had a positive experience playing around with scotty for a few days, and it does seem like a more reasonable choice for a minimal server. Unfortunately it had the [same link-time problems as yesod](https://ghc.haskell.org/trac/ghc/attachment/ticket/10801/epic.fail). 
Could you expand a bit on "ad hoc combinators", or provide some examples?
I agree with the point about other keywords. At the same time, I feel that this behavior is *more* consistent and in-line with my expectations otherwise. Particularly, I think of `do` as denoting an entirely self-contained expression and it's weird that I need to wrap it in parentheses (or use `$`). That feels extraneous. In my head, `do` inherently groups things. Perhaps a good way to express my point is that I think of `do` as having braces semantically even though they're syntactically optional. And I think everyone would agree that requiring parentheses around foo (do { x; y; x }) does not make too much sense! I would love automatic grouping like this, not just for lambdas and `do` but for all the syntactically special kinds of expressions.
Well, a lot of people can get off work by 6/7 but probably wouldn't want to interrupt weekend plans for something like this, so it's not clear a weekend would actually be better. That's definitely true for me, at the very least.
&gt; I think that using the term "crippled" here in this way has a reasonable potential to be unpleasant for some people with disabilities Woah. Let's just say that I'm bad with words, I don't want to offend anyone. &gt; It makes sense - we want the tools that we're used to. I don't want the tools that I'm used to. If I did, I wouldn't discover Haskell (or Idris) in the first place. &gt; If dictionaries were not values, then we'd be in a situation where proofs had no way of reasoning about their identity, which would mean that we probably wouldn't be in a position to do things like prove that an implementation of `Applicative` is lawful (because it relies on the `pure` and `&lt;*&gt;` coming from the same dictionary). I don't see how this follows. The proofs should be *in* the dictionary. &gt; I like my compiler to do work so that I don't have to! But for the compiler to do it, we need to know how to implement it. You implement it by turning type classes into open functions of type `⋆ → Dict`. You don't have to prove that a function gives you the same result for the same arguments. That's a part of what a function is.
do you elaborate on "consistent interface with consistent keybindings"? 
Good idea; I added a link in the post. For convenience here it is: http://www.meetup.com/Bay-Area-Haskell-Users-Group/events/224644473/?a=uc1_vm&amp;read=1&amp;_af=event&amp;_af_eid=224644473 Starts at 6 PM
you must be a hardcore lawyer to use Emacs :)
IMHO, figuring out the concepts and solving problems on one's own is the quickest way to really learn a language. For the occasional problem where someone /really/ gets stuck, they can ask for advice on this subreddit or some IRC channel. A mentor could be useful, but in the end you still have to do the work.
&gt; You don't have to prove that a function gives you the same result for the same arguments. There is a problem with "same arguments". Propositional equality for the arguments might not be decidable. 
what's propositional versus judgmental equality?
Not a serious problem for Haskell. GHC currently works pretty well in practice for reasoning about properties of programs because software developers are not adversarial and tend to avoid writing spurious proofs with `undefined` and infinite loops. `* : *` opens up new ways to use types to talk about programs, which is a win. Of course, it is inconsistent, but compared to existing methods to prove falsehood it's rather difficult to use `* : *` to do so, and such proofs aren't likely to show up in production code.
I don't mind supplying an equality proof for the arguments, if it's what it takes.
Surely it's not innately inconsistent. It's inconsistent with *something*. Is that *something* currently present in Haskell?
I totally understand that people feel that way. But you asked for reasons, so there ya go. I don't even necessarily agree with them, but they at least have some merit.
Wait, no, rereading your [other comment](https://www.reddit.com/r/haskell/comments/3jw3xm/dependent_haskell_vs_idris/cusvunr) helped me to see the problem. We're not interested in equality proofs, we're interested in all arguments *not* being equal. Now that's some food for thought. Thank you.
It means that we consider two functions equal if they take equal inputs to equal outputs. The alternative would be "intensional equality", which would roughly mean that two functions are equal if they have the same implementation.
D'oh; I didn't even think about pattern matching allowing us to not name the original input. That makes sense.
Don't want to hijack this but a working vimrc would be great too! EDIT: Some people use [Leksah](http://leksah.org/). I've heard both good and bad things.
Ahh his site has been so helpful to me!
Dependent Haskell was announced at ICFP this week. A lot of people, myself included, are wondering if it's worth the effort to learn Idris or Agda, now that Haskell's getting DT. But, the comments on this thread make it clear that it is!
The reason `MonadWriter` has `pass` and `listen` is because a finer-grained decomposition of `MonadWriter` comes with no laws at all. For that matter the same thing happens with a finer grained `MonadReader` sans `local`.
I use Spacemacs with this .dir-locals.el in my project's root. For current stable Spacemacs, I had to disable ghc and company-ghc packages due to my use of stack and not ghc-mod/cabal sandboxes, otherwise Emacs would hang. However, in the next major Spacemacs release, you can just disable the use of ghc-mod and not have to do the above. ((haskell-mode . ((haskell-indent-spaces . 2) (hindent-style . "gibiansky") (haskell-process-type . stack-ghci) (haskell-process-path-ghci . "stack") (haskell-process-args-ghci . ("ghci")))) (haskell-cabal-mode . ((haskell-process-type . stack-ghci) (haskell-process-path-ghci . "stack") (haskell-process-args-ghci . ("ghci"))))) 
FWIW I ran into the same issue. Is it possible that the code that was released to Hackage doesn't have the changes you wrote for stack support?
Yeah! I am just talking about the basic text manipulation commands. Emacs has commands to move the cursor and edit based on words, lines, functions, etc. I know that know matter what language I am programming, I can use the same shortcuts to quickly edit the source code. I hear Vim is better in this regard, but emacs is still very efficient when it comes to text editing. Also, a lot of IDEs have a Vim or emacs keybinding mode, but I prefer just using the original tool. 
I'm having the same error on Ubuntu 14.04. I'm very new to Haskell, but let me know if there is anything I can do to help diagnose the issue
Thank you for your comment. I've changed the post to move focus from ByteString to Text.
Thank you for your comment. I've changed the post to move focus from ByteString to Text.
Thank you for your comment. I've changed the post to move focus from ByteString to Text.
The official GHC devs support x86 and x86_64 on the three main OSes. Everything else is supported by whoever is willing to take on the task. I've been doing some work on Arm (mostly raising and fixing bugs as well as doing build system fixes), but I personally do not consider either native compiling or cross compiling for Arm to be production ready yet. Arm64 is even futher behind. 
It should not be "considered harmful" it should be "not fully working yet". The official GHC devs support x86 and x86_64 on the three main OSes. Everything else is supported by whoever is willing to take on the task.
Oh! One more thing: **don't** do `stack exec hdevtools check ...` , **just** do, `hdevtools check ...`.
Sorry for the excessive glibness. I don't mean to dismiss or minimize the considerable volunteer effort that has been made on behalf of ghc on arm - the post is just to point out the current state of affairs for those who might be considering writing a web server for arm or the like. 
So something that would actually *help* is trying the latest 7.10.2 release and logding tickets on the GHC trac for things that don't work.
 (add-hook 'haskell-mode-hook 'turn-on-haskell-doc-mode) (add-hook 'haskell-mode-hook 'turn-on-haskell-indentation) (add-hook 'haskell-mode-hook 'font-lock-mode) I use haskell-mode version 20150306.229 with Emacs 24.3.1.
&gt; I think that using the term "crippled" here in this way has a reasonable potential to be unpleasant I don't think there's a good synonym to use in this context. "Hobbled" and "hamstrung" are archaic or obscure to my ears; "impaired" and similar are weaker and more general, and "damaged," "defective," "broken," etc. are more insulting and less specific. There's room in technical discourse for the the idea "this thing is, for a specific reason, incapable of fulfilling a role or performing a function that should and could be within its capacity."
Care to mention a few examples? The ones I can think of (aeson, attoparsec and others) do not use it as a string type, but as a type for binary data, ready to be sent out on a pipe or into a file.
I do know that leksah's maintainer is very interested in user feedback, very responsive, and friendly to boot.
The full explanation is [here](http://www.cs.cmu.edu/~kw/scans/hurkens95tlca.pdf). Section 5.4 explains the construction. 
You tease me with a partial explanation and then send me off to chase references. I see how it is :)
Mostly it's because I tried to condense the explanation and I got myself confused. There's no simple derivation of False, it's annoying and involved.
Thanks, that package looks promising. I'll give it a shot!
I use spacemacs with ghc-mod which works great with cabal. Stack support is a bit fiddly, but I have it working pretty well now. My spacemacs configuration is pretty simple - (haskell :variables haskell-enable-company-mode t ) And I also added `(setq ghc-debug t)` to the `dotspacemacs/config ()` function. This creates a buffer called "GHC Messages" which contains all the error messages from ghc-mod. Some notes on stack support - 1. I recommend using Emacs 24.5 or later. If on Debian/Ubuntu, remove the older emacs that comes with the system and install emacs from source (quite easy to do). Emacs 25 is preferable but not released yet at the time of this writing. 2. I use ghc-7.10.2 and cabal-1.22 from hvr ppa. I've added their bin paths to $PATH in .bashrc. 3. You need to build ghc-mod from sources as stack support is not officially released yet. 4. There are issues with stack's logging of messages on stderr. To fix that you can either build stack from sources with the logging commented out (as I did), or use a script to redirect ghc-mod output to /dev/null. See https://github.com/kazu-yamamoto/ghc-mod/issues/575 for more details on the workaround. 5. You can use stack to build stack (if building stack from sources). But you must use cabal to build ghc-mod and cabal-helper. As I ran into multiple problems with using stack to build. I think mostly to do with libexec path etc. 6. Before building ghc-mod I had to `touch` the Changelog file (which is created automatically by their release scripts I guess). 
There's something fishy going on here anyway. `Compose f g` shouldn't even be a data type, it should just be `f . g`. Then the type class instance for `f . g` should just be whatever that is, and not derived from the instances for `f` and `g` by user defined instances, which are not guaranteed to match with the instance for `f . g`. *That* is the real coherence that you want, not just coherence among `Compose f g` instances.
&gt;It's infeasible to memorize every single possible type you could ask for it as, because there are no limits to how many Num instances you can write or call. Why not just memoize the values that have actually been computed? ie: the Int value is requested, then the Int value is computed and cached. If you try to get it as an Int again then you get the cached value, but if you try to get it as some other Num it has to "recompute" as that type. My mental model of polymorphic values is that they are effectively dictionaries that map from types to monomorphic values of that type. Is there some reason an implementation couldn't memoize the values rather than recomputing every time?
I wish we didn't have the strict and lazy varients of Text and Bytestring. It's easy to remember that Text is for textual data and Bytestring is more like a "blob" of binary data. However strict vs lazy is a source of neverending confusion to me as some API's expect lazy, while others accept strict varients. For my own API's also also there is no good rule of thumb for when to use strict Bytestring vs Lazy bytestring etc as there is no sematic difference between the two. 
I didn't down vote your comment if that's your question. In fact I even upvoted your initial post as I think it's a genuine an interesting question. Some people might see it as a troll, as the answer seems pretty obvious (at least within the community), which can explain the downvotes.
https://github.com/chrisdone/emacs-haskell-config
However then it's impossible to say what the instance for `f . g` would be without at least knowing what `f` is. The compiler needs to know that (Functor f, Functor g) =&gt; Functor (f . g) so that it can work with arbitrary functors where the concrete type is not known. Note that (.) is not fully applied here, so the best you'd get is Functor (/\x. f (g x)), and that's looking mighty hard to infer.
Here's mine using use-package and bind-keys packages: (use-package haskell-mode :mode "\\.hs\\'" :init (add-hook 'haskell-mode-hook (defun my-haskell-setup () (interactive) (my-enable-modes '(subword-mode flycheck-mode wrap-region-mode electric-pair-mode haskell-doc-mode interactive-haskell-mode haskell-indentation-mode)))) :config (setq haskell-process-suggest-remove-import-lines t haskell-process-auto-import-loaded-modules t haskell-process-log t haskell-stylish-on-save t) (bind-keys :map haskell-mode-map ("C-," . haskell-move-nested-left) ("C-." . haskell-move-nested-right) ("C-c C-." . haskell-mode-format-imports) ("s-i" . haskell-navigate-imports) ("C-c C-l" . haskell-process-load-or-reload) ("C-`" . haskell-interactive-bring) ("C-c C-t" . haskell-process-do-type) ("C-c C-i" . haskell-process-do-info) ("C-c C-c" . haskell-process-cabal-build) ("C-c C-k" . haskell-interactive-mode-clear) ("C-c c" . haskell-process-cabal) ("SPC" . haskell-mode-contextual-space)))
I understand that criticism, but it seems to assume, that you would always think of e.g. a function with signature `A -&gt; B -&gt; C` as “a function taking an `A` and returning a function `A -&gt; C`”. I don't think, that this is the case. Many Haskellers would just think of it as a function taking two arguments. You can read the `-&gt;` as argument separators, like `,` in other languages. The only practical difference is, that such a function can conveniently be partially applied. I personally do not agree that commas are easier to read than spaces in a function call. You get used to it pretty quickly.
It's not like a mentor/tutor learns something instead of you – you still have to figure out concepts and solve problems on your own. But where someone who learns on their own will solve problems that are way too easy and way too hard, [a tutor will throw you at problems that are just within that sweet spot where you learn something without getting confused](http://jorendorff.github.io/hackday/2013/tutoring/). A tutor will also closely monitor your engagement/motivation level and select problems which will increase that. The stuff a tutor does is impossible to do without already being intimately familiar with the thing being learnt, so it's not something you can do when learning on your own.
I think that this open-endedness issue can also be framed in a second, complementary way. Given that laziness and lack of time are two very powerful demotivating factors, if I visit a project as a potential contributor and can only discern monolithic goals that would take a long time to achieve I will likely be scared away. &gt;When I go to SO and don't find some knowledge, I don't feel “Oh, what a terribly incomplete web-resource”, I just ask a question. On a wiki, what do I do? Request a contribution? But there's no facility for requesting a specific contribution on a wiki Wikipedia has several channels that direct contributors to atomic tasks (some of them are advertised through the [community portal](https://en.wikipedia.org/wiki/Wikipedia:Community_portal)). Such channels do need a lot of manpower to be effective though. There are also simpler ways to introduce some atomicity to a wiki. Even something as prosaic as a section stub notice or a red link in the table of contents can be seen as an invitation to do an atomic task (in the Wikibook, it does occasionally happen that a new contributor completes one of those). Of course, if there are too many red links the wiki risks running into the broken windows problem. &gt;You could say: “Hey, a wiki-page for Threepenny-gui should just be a high-level overview, so it's obvious that you should put some general, widely-applicable descriptions in it, and link to more specific stuff”. But then wiki becomes a duplicate effort, because the most appropriate place for a high-level overview is README.md on GitHub repository for this particular software. Unstructured wikis like Haskell Wiki (or, say, [C2 Wiki](http://c2.com/cgi/wiki)) are valuable as a way to preserve information. By throwing brain dumps, scattered notes and interesting conversations into a wiki you at least help ensuring they won't be lost, and make them somewhat easier to find than e.g. in a mailing list archive. For goals more sophisticated than that, though, more structure is definitely needed. 
True...but very few people care about the ordering with Set since it is only an implementation detail. You would have more of a point with some other data structures, e.g. a priority queue. With those you do not perform merge operations as often as with sets though so the incompatibility of different variants does not matter as much.
It's the company i am currently working for. EHealth uses new technologies and software in order to help with health development projects in Africa. Some offices are in Nigeria, Sierra Leone, Liberia, California and in Berlin, and some of us work from remote. This is the home page http://www.ehealthafrica.org/ and here some job offers https://github.com/eHealthAfrica/jobs. We do not use any Haskell yet, unfortunately, but we use Couch a lot
Do "lots of programs" need to understand them? It's not like you'd need to open the symlinks with an obscure editor. I'd expect that at most only Explorer needs to be able to handle those, for the (rare and inexplicable) case that you want to manually delete them.
My point is that Haskell is at an equal disadvantage with the parameterised variants of Set. You get an equal number of different Set types -- one for every ordering you're using. Haskell doesn't solve this problem, it just makes it more inconvenient to use different orderings in different sets.
I think `stack exec --no-ghc-package-path hdevtools check src/Main.hs` may work
I for one make pretty effective use of sharing :: MonadWriter Any m =&gt; a -&gt; m a -&gt; m a sharing a m = do (b, Any modified) &lt;- listen m return $! if modified then b else a {-# INLINE sharing #-} to reduce the amount of unnecessary copying done in my unifier to almost nothing. Writer isn't useful for most things that people "want" it to be useful for. Accumulating a Monoid fully lazily is a rather peculiar effect, and while every once in a while it is indispensible, most of the time you are actually looking for something else.
This fundamentally restricts the flexibility of said library, however. In the parameterised approach, the library could also be parametric on the ordering (ML functors help a lot here) and then the library user gets to decide what ordering to use, not the library writer.
Is there an issue number for this? It sounds like the metadata collection is failing for some reason. Please send us some logs. /Applications/Leksah.app/Contents/MacOS/Leksah --verbosity=DEBUG and cat ~/.leksah-0.15/collectSystem.report
The intersection of the heterogeneous world of emacs configuration and the heterogeneous world of haskell tooling is both wonderful and terrifying.
How do you want to implement that open function? It's not particularly clear what that would be and how it would fit together with the rest of the type system. It's all well and good to declare that it will be a function, but it's not a function that can be written in Idris. So it requires a non-trivial extension to the theory, if you want the theory to represent the fact that it's a function (which one must do, if it's to be useful when proving things). Nobody's actually demonstrated how that extension will work, so far as I know.
To expand on /u/kamatsu's answer in the context of instance uniqueness: if we have judgmental instance uniqueness (that is, that any two inhabitants of, say, `Monad List`are judgmentally equal to one another), then we are justified in considering that anywhere we encounter `return` or `&gt;&gt;=` used with `List`, it will be precisely the same thing. If we only had some kind of propositional equality, then we might need to call a function to get ahold of the fact that they are the same.
I'm not an R user myself, but some of my department co-workers are; would you say this is aimed at R users who need some general-purpose programming language or at Haskell users who need want do some statistics/data analysis?
&gt; requires a non-trivial extension to the theory I don't think that the extension must be non-trivial. Andres Löh has demonstrated how open functions can be incorporated into Haskell: [Open Data Types and Open Functions](http://www.andres-loeh.de/OpenDatatypes.pdf). I believe something similar can be done for Idris. After translation to the core language, open functions become closed, and I believe such functions can already be expressed in Idris. I doubt, however, that the best-fit strategy proposed in the paper is the best one. I'd prefer the clauses to be unordered and provably non-intersecting, with the ability to order intersecting clauses explicitly (in the style of [Instance Chains](http://web.cecs.pdx.edu/~mpj/pubs/instancechains.pdf)).
Argh! Just as I was finishing the blog post to announce this and uploading to Hackage/Stackage... Notice that there hasn't been a release yet. This is still just the in-development Github repo. Oh well. Cat's out of the bag now. Stay tuned for further announcements this evening. :)
One of the major differences is that this is a direct embedding of the R interepreter into the Haskell program's address space. Whereas Rlang-QQ communicates with the R interpreter using a pipe for simplicity. The advantage of a direct embedding is that the overhead of calling an R function and passing data to it is much smaller. It becomes possible to pass much larger bits of data to R much more frequently, which in turn enables keeping as much code as possible in Haskell, rather than calling out once to a monolithic piece of R code.
`Delay` (augmented with a suitable productivity checker) gets you productive corecursion, but not general recursion. For something comparable to `IO`, try the partiality type constructor (or “partiality monad”, as it's known). Otherwise, the situation was similar with IO purity a few years ago. It was easy to dismiss Haskell because it supposedly couldn't do anything other than make your computer warm. Even the stupidest programming languages are Turing equivalent, so people have become accustomed to assuming that it's necessary everywhere. Changing that perception might take some time.
&gt; I like many things about Idris and it makes me sad to see the language crippled (in my eyes) by incoherent type classes. Many think Haskell's global coherence constraint is crippling, so don't assume there's a consensus that agrees with your opinion on this topic. Idris' approach sounds perfectly sensible, particularly for theorem providing, even if it doesn't match your preferred method of developing programs.
This is sad... :-(
Heh... I should have been more precise. Upload to Hackage, add to Stackage.
Thank you.
You shouldn't.
Idris doesn't allow pattern matching on types, so it wouldn't work to implement type class resolution as an incrementally defined function. At least not without doing the changes to the core theory that are necessary to get typecase working properly without destroying the ability to make parametricity arguments. This is better understood, though. Even if instances are expressed as the output of a function that defines the resolution algorithm, it would still make sense in the core theory (where the dictionary is a record structure) to construct new dictionaries. So we would need to replace that simple semantics with something more complicated in order to make it clear to the theory that the _only_ source of instance dictionaries was the resolution function. This is also nontrivial.
And the fact that `Bytes` is shorter is a nice bonus.
Actually I didn't mean that you did down vote, I just didn't know who to talk to...
`Num` is definitely one of the biggest warts we have in `base` - good, but could be a lot better. You're not the first to bring this up though, three other projects to compare against are [`numeric-prelude`](https://hackage.haskell.org/package/numeric-prelude), [`algebra`](https://hackage.haskell.org/package/algebra) and [`subhask`](https://github.com/mikeizbicki/subhask). I imagine there are many more possible designs. An argument against those three projects are "they are too big, we'll never fit that into `base`!" which is a fair argument. That said, your proposal which is much smaller is already *bigger* than a current on-going refactoring that you need, which is for `Semigroup a =&gt; Monoid a`. This worries me a bit in that we might never end up solving the `Num` problem :(
I have a lot of difficulty reading ~~long~~ paragraphs. It makes me wish I could I had a tutor when learning new things. For some reason, I'm having much more difficulty reading through Haskell tutorials than those for other languages. Haskell _is_ more difficult to learn, but it seems like most of the tutorials I've come across are more wordy. I'm also having a hard time navigating the documentation.
Are we taking about the same thing? (I checked, and I meant `Lazy`, which has value constructor `Delay`). If so, give a proof.
&gt; Idris doesn't allow pattern matching on types So Idris doesn't support anything like Haskell type families (closed or open)?
This looks really nice. Is there a particular reason you went with R over Python? Presumably CRAN? Could this be adapted to work with python? Having (almost) no overhead to easily call into the various numpy-based scientific libraries in python would be swell.
PS a frequent question I have when trying any of these growing IDEs is, what's supposed to work ? Since they do a lot and depend on a lot of complex subsystems and different system configurations and so tend to be fragile, I'd be happy (and more likely to report bugs) if the first thing I see on startup is an easy checklist or guided walk-through telling what to try and what results I should see.
Yeah, the cat *is* out of the bag. I've been playing around with it a fair bit, and I can say it isa really awesome library For instance, do you want the power of plotting ggplot in haskell? Here is a little [gist](https://gist.github.com/drwebb/a926d42685c737085b76) that demonstrates using HaskellR to use GGplot2 to plot some data coming from Haskell. The rough parts of HaskellR is that you are rather limited in what you can pass in, though this could probably be fixed. My ultimate usecase would be answered if I could pass in data from the Frames library into HaskellR as data frames. The interopt between Haskell and R is very tight, I've seen other libraries try to do the same with calling Python from Haskell but they are missing the most important parts such as Shared Memory, and a way to pattern match on values coming back from the R interpreter.
This should be fixed in latest HaskellR. Have you had another try since https://github.com/tweag/HaskellR/pull/192 was merged last month? If you still have to resort to ugly hacks, then that's a bug, so looking forward to a new ticket from you. :)
No I haven't had a try those changes yet. It's been on my Todo list though.
But it still doesn't work in your `app` project (with a `stack.yaml` ?) 
&gt; At least not without doing the changes to the core theory that are necessary to get typecase working properly without destroying the ability to make parametricity arguments. This is trivial. You just add both parametric and non-parametric quantification to the language. Dependent Haskell will have those, if I'm not mistaken. Anyway, the idea is described [here (StackOverflow answer)](http://stackoverflow.com/a/26012264/1486400) by Conor McBride. &gt; make it clear to the theory that the only source of instance dictionaries was the resolution function Ok, this is not so trivial, but still shouldn't be a drastic change. Maybe you can just attach a coherence tag, or something.
The second you integrate stack I will make the switch :) 
Are there slides (there don't appear to be)? In searching for them (in the youtube comments), I ran across [this](http://stackoverflow.com/questions/23220884/why-is-typecase-a-bad-thing/26012264#26012264) interesting suggestion by Conor McBride to have _quantifiers_ (what a surprise, Conor wants more quantifiers) be the thing that distinguishes erase-able from typecase-able arguments.
Yeah. We use rabbitmq quite a bit.
Oh hell yeah! Finally an excuse to use Haskell in my actual work. Would anyone be interested in a series of amateurish blog posts about trying to fit them together for bioinformatics?
Please do! I use python for most of my bioinformatics work, and I would love some guides on how to use haskell instead.
How about having an extra layer: class Semigroup a where mappend :: a -&gt; a -&gt; a class Semigroup a =&gt; CommutativeSemigroup a where (+) = mappend I also don't understand your second point. Can you elaborate?
When installing with stack I had to follow these instructions: While constructing the BuildPlan the following exceptions were encountered: -- Failure when adding dependencies: inline-r: needed (-any), not present in build plan (latest is 0.7.0.0) needed for package: H-0.7.0.0 This is using Stack 1.4.1 and lts-3.4. I thought you might want to add it to the instructions.
Cool! Me too, depending on the problem. I don't know SciPy so I use R for the visualization stuff. Most of my code ends up being a makefile with random R, Python, and bash scripts for the different steps. But I spend a lot of time debugging them so I'd love to write the overall pipeline in Haskell and have types to make sure everything stays sane. I know there are Haskell wrappers for shell scripts so that's covered. Not sure about Python but I tend to use it mostly for parsing anyway, and that can be done directly in Haskell.
[Would this convince you?](https://gist.github.com/gallais/303cfcfe053fbc63eb61)
&gt; 2. By making Group a superclass of Ring in this way, you're forcing every Ring to be a Monoid in addition, when they could just as easily be a Monoid in multiplication. But a ring is by definition already a group under addition, but there are some rings without a multiplicative identity. If you need to exploit the monoid instance for the multiplication, you could always use the newtype trick as we currently do for monoid instances for numeric types. 
Why is it too big? How much space is in `base`? As long as the gains outweigh the costs, why not?
That's my YouTube comment, look at the author name (: So yeah, I like this idea too.
it's not just ihaskell it's the worksheet metaphor. It only works for simple linear analyses. Once you explore charts and analyses with multiple inputs, data transformation, and modeling options interacting, you get into a space that's better suited to webapp and FRP territory. From personal experience it doesn't take much complexity before a data analysis project transitions from the worksheet to the webapp regime.
I think it's not so much the size as the fact that changing all the `Num` functions in `base` would break a **lot** of packages. Backwards compatibility is not something to break lightly... 
&gt; but there are some rings without a multiplicative identity. The common definition in my circles is that rings must have a multiplicative identity. I might call a "ring" without 1 a rng. Regardless, even if you don't want all Rings to have a 1, I can just modify my complaint to say that someone may want their Ring to be a Semigroup under multiplication, or to say that people whose Rings *do* have a 1 may want their Ring to be a Monoid under multiplication. The newtype trick for Monoid instances of numeric types currently uses a newtype for Sum and a newtype for Product, which I think is fine. `Int` is not a `Monoid`.
&gt; Writer isn't useful for most things that people "want" it to be useful for. Accumulating a Monoid fully lazily is a rather peculiar effect, and while every once in a while it is indispensible, most of the time you are actually looking for something else. Isn't that the problem? Writer isn't useful for most of the things people want it to be useful for, and it's not communicated well that isn't. Moreover, monad tutorials use it (and Reader, State) as the primary examples of "Useful Monads"(TM), almost never mentioning `listen` or `pass`, and these tutorials convince users that yes, in fact, Writer is the thing they're looking for. There's poor communication going on at many layers here, but it starts with the API. You said it yourself: *Writer isn't useful for most things that people "want" it to be useful for.* If I was designing an API for something and I could say that about what I've written, then I would know I've designed the wrong thing or failed to advertise what problems it's intended to solve.
&gt; How about having an extra layer: I like that. &gt; I also don't understand your second point. Can you elaborate? OK, look at the situation currently in Haskell, using the existing `Monoid` class. Suppose we want `Int` to be a `Monoid`. We could do instance Monoid Int where mappend = (+) mempty = 0 or we could do instance Monoid Int where mappend = (*) mempty = 1 They are both valid, and it would be kind of presumptuous for the library author to impose one on everybody. So instead they do a newtype trick to make both available. This proposal essentially makes the decision for you, by making all `Ring`s into `Monoid`s using `(+)`.
The problem with your proposal is that it skips a lot of levels What about non-associative right seminearrings? In all seriousness I use them when writing parsers all the time. `Num` including `abs` and `signum` is nonsensical for all sorts of numeric types. They are a particularly terrible choice for types like `Complex` or modular arithmetic. For `Complex a` you'd really like abs to give back an `a`, that we can then multiply back out using the algebra by the `signum` to get the original `Complex a`. `div` and `mod` vs. `quot` and `rem` are just two points in the space of possible Euclidean domains, and their current structure rule out all sorts of useful forms of pseudodivision that lets you run gcd in more interesting settings. There is a question of where and how you can stop. Also there is a big blocking problem that Haskell is very bad at dealing with typeclasses chopped up to their finest pieces. If you add laws separately from adding operations you don't get to use those laws when defining the operations due to the fact that typeclasses in Haskell unlike classes in OOP can't refine the definition of the methods in your superclasses. In the end this means that in haskell the language we have today, we are very bad at dealing with fine-grained class hierarchies. `Num` is an unprincipled mess, but it an unprincipled mess for a lot of reasons. There are several packages out there that provide much more fine-grained mathematical hierarchies for you to choose from. My advice, from experience building the `algebra` package, is to build what you want out in user-land and then beat the ever-loving crap out of it by throwing problems at it, things outside of what you originally thought it was good for, and in the end you'll have a better sense of just how awkward this domain can be.
A new edition of the [Introduction to Functional programming](https://www.edx.org/course/introduction-functional-programming-delftx-fp101x-0) course taught by Erik Meijer starts October 15.
pity then I cannot check if I understand the things learnyouahaskell tries me to learn. 
https://github.com/bitemyapp/learnhaskell
These are really exciting news! I really much prefer the new api over the old one. Thank you for all your hard work! I plan to use this library soon :)
I don't agree at all. In a language with linear types persistent data structures should only be used when you need nonlinearity (i.e. virtually never).
Here's a nice tutorial by [Alejandro Serras](https://github.com/serras/emacs-haskell-tutorial/blob/master/tutorial.md)
Given that 〈run〉 and 〈∞run〉 don't pass the termination checker, no.
What's the haskell-game group? Is it related to /r/haskellgamedev?
&gt; it would change my life I understand and definitely agree with revamping `Num` in a way that makes sense, but what are the biggest practical outcomes?
It's behind the [#haskell-game](http://ircbrowse.net/haskell-game) IRC channel and the [haskell-game](https://github.com/haskell-game/) group on Github. The ultimate goal is to improve the landscape for making games using Haskell.
EDIT (Updated to `stack` version 0.1.4.0.) I am using Windows. For me, the install fails with the message: Configuring inline-r-0.7.0.0... setup-Simple-Cabal-1.22.4.0-x86_64-windows-ghc-7.10.2.exe: Missing dependency on a foreign library: * Missing (or bad) header file: cbits/missing_r.h * Missing C library: R This problem can usually be solved by installing the system package that provides this library (you may need the "-dev" version). If the library is already installed but in a non-standard location then you can use the flags --extra-include-dirs= and --extra-lib-dirs= to specify where it is. If the header file does exist, it may contain errors that are caught by the C compiler at the preprocessing stage. In this case you can re-run configure with the verbosity flag -v3 to see the error messages. ghc: warning: _tzset from msvcrt is linked instead of __imp__tzset Any assistance as to what that means and how I can resolve it would be much appreciated.
&gt; The common definition in my circles is that rings must have a multiplicative identity. That might be so, but not in all circles. There could be good reasons for not including the existence of a multiplicative identity into the ring axioms. See [this mathoverflow question](https://mathoverflow.net/questions/22579/what-are-the-reasons-for-considering-rings-without-identity).
Haskell Wikibook is one of the best references of Haskell I have ever read. However, how is it? Is it still active? I saw a lot of different efforts that are very positive (such as School of Haskell and Intermediate Haskell of Commercial Haskell Group), but none as organized as this Wikibook, in my opinion. By the way, does anybody know if there is a way to connect the [inline IDE of School of Haskell](https://imgur.com/FEuBs60) to the Wikibooks? That would be awesome!
Well, interpreters are another thing. But I agree. I mentioned general recursion as part of dismissing the specific idea that Turing equivalence could be achieved with just total functions. Of course, the difficulty is in getting the totality checker to infer termination or productivity, where we could see some advances. I'd like some method to provide termination proofs via syntactically separate measure functions (and measures of the output for productivity proofs). And maybe productivity (constructivity?) should be expressible in the type system, so a fixpoint combinator can be defined only on constructor-like functions.
Are you an absolute beginner to Haskell, or an absolute beginner to computer science/programming? If it's the latter, consider taking a look through more introductory computer science books. [Structure and Interpretation of Computer Programs](http://sarabander.github.io/sicp/) is a great book to start with and will introduce you to a lot of topics which will help you understand the foundations of (functional) programming (recursion, abstraction, etc...). There are a lot of exercises, and although the book uses a Lisp, you could attempt them in Haskell instead.
I'm using `commit da1d503f901a05f70e936ea1181a603387c47917` (seems I haven't updated since mid-August). It should be accepted because all the recursive calls are guarded by `later`. Your problem may come from the fact that `copatterns` are a fairly new feature and were not well supported until now (they will be turned on by default in the upcoming 2.4.2.4).
&gt; In short, curry is great , uncurry has no advantage whatsoever. What nonsense. Currying is convenient, but it is hardly a unilateral win. One point where currying is notably *inconvenient* is when you make a mistake and forget an argument (or parentheses). The error message you get is usually misleading. The problem is compounded when the function is parametrized because the error you get partial unification, removing any hope at all that the error message will have anything more enlightening than the line number. (And even then, if you wrote the function yourself, you probably have to work out for yourself whether the actual problem is at the function definition or at the call site). I'm not saying currying should or shouldn't be the default. But to make a claim that currying is superior because you find it "easier to read" or because "pairs don't have to be built-in" is just fodder for frivolous opinion-driven debate.
It is active in that there are at least two people, one of them being me, doing maintenance work (reviewing edits, answering to reader comments, softening the remaining rough edges, etc.) and adding new content. Recent work has focused on matching current Haskell practice. For instance, in July we added the long-missing Foldable and Traversable chapters and vastly improved the one about Applicative, partly as an answer to the changes brought by GHC 7.10. Other key contemporary Haskell topics, such as lenses and effectful streaming, are also on the road map.
Looks nice. Good job!
(removed paste of code)
I'm sure that's true of the basics of writing functions, but if it's anything like my experience with Agda, it takes more than a few hours to learn how to program actual proofs in the system. Don't get me wrong, it was rewarding and worth it, but there's a pretty big gap from strong types to dependent types in terms of learning curve. Maybe Agda's lack of tactics was a big part of this, I haven't tried Idris at all.
Yeah, powerful is really what I meant. Like, it would be really awkward if you tried constructing a list comprehension manually, but it's pretty painless when you use the list monad. So I guess I'm wondering if there are certain constructs which are really cumbersome with monads that are perhaps more elegant and concise with something else?
Graham Hutton's book (http://www.cs.nott.ac.uk/~gmh/book.html) comes with videos by Erik Meijer and includes downloadable slides for each chapter and a downloadable solution set for the exercises. I have not used it. I have used, and strongly recommend, "Haskell, the Craft of Functional Programming" by Simon Thompson. You should be able to find a copy at a major city library, or be able to borrow from your in-state library program if you're not near a library that has it. Or, you can just take the plunge like I did and buy a used copy online. It's not cheap but the good news is if you want to resell it when you're done, it'll hold its value.
easiest is to use MaybeT from tranformers, probably.
one of the best haskell blog series I've read! I'm still slowly working through the third, trying to cover up the screen and guess a lot. since there's an insight like every few lines. some definitions are tricky, while others are obvious from the type. it would be nice if you gave some sign to "pause and think" before you drop a bomb. it's easy for me to peek ahead and stop at code blocks, and write them myself. less easy when there's a huge idea buried in the middle of a paragraph that could send me on hours of thought trying/failing to rederive it. but I don't know if I should keep reading to get an extra clue. for example, there was a leap of intuition (for me) between the "problem of managing languages and interpreters", and the "solution of the Pairing class". I feel like I could spend days on trying to come up with "a class with a method that helps us out here" and not being able to come close. but even if I just got a few hints about he general shape of things, it would force me to come up with a lot of cool stuff, so it seems like a nice place to slow down at. anyways, keep doing what you're doing!
To take a different tack, I'd say the primary virtue of purity/immutability is reduced cognitive overhead. In most languages, I really love having the debuggers show me the current value of all the variables in scope, etc. In Haskell, I rarely need that kind of stateful view. In large part, this is an artifact of the way Haskell programs tend to be written; but it's also a product of working with bindings not variables. I don't have to maintain mental state; I don't have to track any changes. Just relate expressions to each other. When I write `f x = ...`, I have some sense what `x` is, and that knowledge stays constant throughout the whole body of the operation. Likewise when working with data constructors: I can think about what a value *is*, rather than *what operations* I'm conducting on it.
thanks for the info!
Thanks! I'll definitely keep going :) I can't take credit for Pairing though - I stumbled on that through blog posts from Ed Kmett and Dan Piponi. I've considered putting some "exercise for the reader" blocks in there, and given your comment I might bump that up the list a little. If you want a fun challenge, I can point you down a path that is a little spoiler-y for the next post. Although the challenge itself is kind of implicit in the 3rd post :) We want coproducts and products, such that: * the components are unique * the components are all functors * we can pair the coproducts and products with good type inference I've got something at the moment that does this by keeping track of the functors involved via a type-level list. These [lecture notes](https://github.com/kosmikus/SSGEP/) probably give you most of what you need. Bonus challenge: * can you change your solution to the above from a type-level list to a type-level size-balanced tree? * what else do you need to change to keep the usability up? what are the tradeoffs that you need to make? 
You'll need MaybeT. Specifically, the MaybeT constructor, because you already have it's contents. If x is your value and f is your function, then you want `runMaybeT $ MaybeT x &gt;&gt;= MaybeT . f`which will give you an IO (Maybe b). If you want to keep it as a MaybeT IO b so you can chain it more, just leave off the `runMaybeT`.
thanks!!!
Yes, as far as more powerful meaning more expressive. [Parameterised monads](http://blog.sigfpe.com/2009/02/beyond-monads.html). A Parameterised State Monad can, for example, change the *type of the state* through out the parameterised-monadic computation. You can put an `Int`, read the `Int` and put a `String`, then read the `String` and put a `Bool`, and this is all properly checked. Is it fundamentally more *useful*? Haven't the slightest idea.
The `filepath` package has any support for the tilde character in paths, which I think may be the issue. Try using `$HOME/...` instead and see what happens.
Are you and/or neelk referring to the partiality monad or to the thunk-delay constructor ∞ used in some codata implementations like e.g Agda? 
Looks like you are missing the R library headers, HaskellR would need it to call out and link to it. There is a [extra-include-dirs/extra-lib-dirs](https://github.com/commercialhaskell/stack/wiki/stack.yaml#extra-include-dirsextra-lib-dirs) field you want to set in your `stack.yaml` when you can point to any built libraries or include dirs. I've only ever used this package on Linux, so no more ideas.
Another use of free monads is do delimit groups in an effectful stream, like pipes-group does. Perhaps pairing with some type of cofree comonad makes sense there as well.
http://blog.higher-order.com/blog/2014/12/21/maximally-powerful/ More power from the *users* perspective always takes away power from somewhere else. (This "power" comes at a cost.)
If MaybeT is an overkill this should work : import Data.Maybe (maybe) test :: IO (Maybe a) -&gt; (a -&gt; IO (Maybe b)) -&gt; IO (Maybe b) test v f = v &gt;&gt;= maybe (return Nothing) f 
&gt; The common definition in my circles is that rings must have a multiplicative identity. OK. I don't have strong views on this - I was once amused as an undergraduate (many years ago) to hear two lecturers pass me in the corridor saying "Let R be a commutative ring with 1..." "No! Let R be a NON-commutative ring WITHOUT a 1". I have no intention on having a similarly absurd argument with you! :) &gt; I might call a "ring" without 1 a rng. I like it. I like it a lot. &gt; Regardless, even if you don't want all Rings to have a 1, I can just modify my complaint to say that someone may want their Ring to be a Semigroup under multiplication, or to say that people whose Rings *do* have a 1 may want their Ring to be a Monoid under multiplication. Yup. &gt; The newtype trick for Monoid instances of numeric types currently uses a newtype for Sum and a newtype for Product, which I think is fine. `Int` is not a `Monoid`. ... which stops people from accidentally using the Monoid instance without realising they're using a different operation than they were thinking. Good point. It's certainly an advantage. I suppose I was thinking that in ring theory terms, the addition is more of a winner than the multiplication (even so in a field), and I've always felt that the Num typeclass is ugly from a mathematical point of view for not having the usual hierarchy, even if I've got used to it over the years and it doesn't bother me much these days. Can we agree at least that it's a mistake not to include the Semigroup superclass for Monoid? 
EclipseFP. It can be a little tricky for the initial setup but once you have that you get sandboxes out of the box, it manages cable config for you, manages your test configurations. It compiles on save which means you get instant feedback on type errors.
Don't feed the troll. If you read the Hacker News discussion linked to in this blog post, it's clear that this person is trolling (whether on purpose or not). His argument amounts to "obviously Pure Functional programming is bad because it's not popular". It ignores years of other languages successfully incorporating features from PFP into their language as critical enabling features (e.g. Linq). But the author may not realise this because he seems to be a Java programmer that apparently doesn't realise how unbearably under-expressive that language is.
I've already been playing with a manually installed version of this from git, it's been nice to work with as far as I have explored. 
oh yes, I’ve wanted this for a long time. could the author talk more about how it works? I see happy and alex as dependencies. so (i hope) the user really can modify GHC’s Haskell grammar, which should make it easier to make a preprocessor into an extension. 
&gt; One thing I typically like to do is add some smart constructors to get around it Worth noting that we have arguably better support for this via pattern synonyms now: pattern Zero = Fix ZeroF pattern Succ n = Fix (SuccF n) pattern Nil = Fix NilF pattern Cons x xs = Fix (ConsF x xs) This lets you pattern match and construct data in the "normal" fashion. Nice post!
Congrats! A little meta question: is SDL still the goto solution for cross platform low level game related stuff? GLFW for example seems to be another good solution for that.
Well, eh, GHC? Unless you turn `IncoherentInstances` on.
Good on them. I encourage successes in all languages with powerful type systems, not just Haskell. Hopefully Haskell libraries can borrow some nice tricks from Rust as well :)
So this is the equivalent of saying type Zero = Fix ZeroF But also introducing Zero as a kind of constructor? Pretty cool!
It doesn't really introduce a type, but basically creates a "fake" data constructor that will desugar to a view pattern (pattern matching) or function application (construction). They are great :)
So are you using these more than ViewPatterns now would you say?
You can access the documentation for the layers with `SPC f e h`. Search the haskell layer, the instructions are there. Or you can access the documentation for the haskell layer here: https://github.com/syl20bnr/spacemacs/tree/master/contrib/!lang/haskell
Not that I know of. What's the problem with that package, though?
Hi. Please see if instructions at https://github.com/tweag/HaskellR/pull/199 work for you. In short, you should be able to just run `stack exec H` now.
I don't use either particularly often. In `sdl2` we are using the same as patterns around `newtype Scancode = Scancode Int32`. We have things like `pattern ScancodeEscape = Scancode 42`, which cuts out some marshalling, but retains readability. I'm starting to feel that when you have big multi-branch `if` statements, they would be better replaced with `case` analysis and pattern synonyms.
Cool paper. I love formalizing the states of failure such that those states are expected and have defined behaviors. I would love to see more development toward something much more Dryad like. 
Doesn't seem to build. It loads fine in GHCI, though.
Have you done any functional programming before? Because recursion over data structures, ie a list, is a very fundamental skill needed for all functional programming languages. You could do project Euler problems to practice "thinking functionally" and compare your answer to this: https://wiki.haskell.org/Euler_problems
If the bare link to `Control.Arrow` is too cryptic for anyone, the point is the instance `Monad m =&gt; Arrow (Kleisli m)`. The Kleisli arrow is basically a monadic function `a -&gt; m b` with the input type stuffed into a type parameter as well, `Kleisli m a b`.
I hope that the layout you suggested works, that is *exactly* what I'd like to write!
I think the more common name for these is *Indexed* monads. They can be used to fake (or implement) linear types, among other things. I like to think of indexing like this: When comparing a &lt;foo&gt; and an indexed &lt;foo&gt;, the &lt;foo&gt; forgets the types of the arrows/functions and lets them all compose. A category is an indexed monoid; with mempty corresponding to id, and mappend corresponding to function composition. (Remember how monads are monoids in the category of endofunctors? Well, indexed monads are categories in the category of endofuntors!)
Saying "80% faster compared to Attoparsec" seems a bit misleading to me. The code in question takes 0.8s while attoparsec takes 1.45s. To me that makes it 55% faster than attoparsec. I wouldn't have complained if he had said "Attoparsec is 80% slower". English is bad with this kind of thing.
Since Rust is very much about borrowing, I'm sure they won't mind. ;-)
[Here's mine](https://github.com/patrickt/emacs/blob/master/init.el#L180-L212). Some highlights: * Enables decl-scan mode (I bound imenu+helm to a shortcut key, so I can jump to anything very quickly) * Enabes doc-mode (except for built-in syntax, as it is too noisy) * Uses stack-ghci, since I live entirely in stack * turns on Unicode symbol rendering with `haskell-font-lock-symbols` * Uses helm for haskell-mode's module insertion (with that ugly defalias at the bottom)
It's not super clear, though, at least.
Huh? No problem.
GLFW is a lot more minimal in scope. I like it because it gives me OpenGL contexts and input handling with minimal fuss, but it doesn't really provide anything else. SDL, meanwhile, has 2D rendering stuff, better controller/joystick support, audio support, etc.
Yeah I see. It depends on utf8-string &lt; 1.0 but doesn't say so (upper bounds lol). I added utf8-string-0.3.8 to extra-deps in my stack.yaml and it built.
"Powerful" here sort of just means "constrains more and more features about its members"; groups are more powerful than monoids, fields are more powerful than rings, etc. In this sense then every actual data type (Maybe, Identity, IO, Either e) is "more powerful" than Monad...because you can say more stuff about a `Maybe a` than you can about a `m a` for `Monad m` :) You can do stuff with a `Maybe a` that you could *never* do with a general/polymorphic `m a` :O
by "higher order" you mean like HFix in http://www.timphilipwilliams.com/posts/2013-01-16-fixing-gadts.html ? 
The features I'm most interested in: * [Strict Haskell](https://ghc.haskell.org/trac/ghc/wiki/StrictPragma) - Because for some modules, you just want everything to be strict. * [Injective type families](https://ghc.haskell.org/trac/ghc/wiki/InjectiveTypeFamilies) - Because it was a little ridiculous we could only go one way. * [Overloaded record fields](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields?redirectedfrom=OverloadedRecordFields) - Hands down the most exciting feature in this version. * [The improved pattern matching for GADTS](http://research.microsoft.com/en-us/um/people/simonpj/papers/pattern-matching/gadtpm.pdf) - Amazing work there, the constraint solver can even be extended with plugins.
Yeah, that's a nice example of a pretty useful thing. In Standard ML, you can (verbosely) implement a generic Fix constructor using a module functor pretty easily, but implementing anything like HFix seems out of reach in Standard ML.
Such a well written introduction article! I anticipate wanting an "introduction to the R Libraries" aimed at someone coming from Haskell. I'd this feasible? Any volunteers? Thanks for the great work.
Here's a [link](https://www.reddit.com/r/haskell/comments/2z2f6f/benchmark_rustnom_vs_haskellattoparsec_vs_chammer/) to some discussion of older nom vs attoparsec benchmarks.
Why blame english? It could be put to better use. In general, these formulations are better: - Throughput for benchmark Y is X% better/higher. - Runtime on benchmark Y is X% less.
If I have time to play later, I might be able to contribute some examples.
this works: &gt;&gt;&gt; :set -XNoImplicitPrelude -XRebindableSyntax &gt;&gt;&gt; let ifThenElse () t e = e &gt;&gt;&gt; if () then 't' else 0 0 but a Boolean class would be nice. 
&gt; Why blame english? Because the English word "faster" is so ubiquitous for this sort of thing. I guess your question really cuts to whether by "English" we mean the words, or the way people commonly use them.
This is very nice. I'll have to dig out that old book on R I got a while back...
pron98 is not a troll. He seems to have quite some expertise on the JVM and some good ideas. However, he thinks he knows something about Haskell and is quite happy to make sweeping pronouncements about it when in fact is he knows very little about Haskell. I wish he would shut up about Haskell and just say interesting things about Java, like I shut up about Java and just say interesting things about Haskell. It's very distracting to have otherwise interesting discussion threads polluted by "not even wrong" claims about Haskell. 
More specifically, an Arrow is just [an Applicative along with a Category](https://cdsmith.wordpress.com/2011/07/30/arrow-category-applicative-part-i/).
oh, that reminds me. Since nom 0.4.0 got a big speed up to match that HTTP benchmark, I should update the MP4 benchmark :D
A paper preprint was presented at IFL 2014 with more of the detailed technical bits on how this was all done, for those (like me) who are into such things: https://ifl2014.github.io/submissions/ifl2014_submission_16.pdf
Wouldn't be a bad idea! I'll add a link soon - thanks :)
https://gist.github.com/NicolasT/e8edde87a4f18dc082a9
The R-Haskell interface has been the sole factor keeping me from using Haskell in a number of previous projects, leading me instead to use languages that make me want to poke my eyes out. I can't tell you how exciting it will be to not want to poke my eyes out in the future. This work could be a serious quality of life improver. Thanks for that!
I think the real question to ask would be 'what holds us back the most?'. What aspects of ghc do you think impose the largest cost on new developments and maintenance?
Once in a thread on the brazilian programming community, some people were talking about all the cool features Haskell had, and how it was worth a try. An skilled programmer there got interested, so she downloaded GHC and made some tests by herself. A few minutes later, she comes and asks: why this doesn't compile? Something like that - data Employee = Employee { name :: String, age :: Int } data Company = Company { name :: String, employees :: [Employee] } (...) We carefully told her how the `data` syntax works, in that it just creates top-level functions, whose names conflicted. All she said was: &gt; LOL. Call me again when Haskell ceases to be a prototype. I guess the time has come? Edit: sp
but but `fibs = 0 : 1 : zipWith (+) fibs (tail fibs)` who even needs records with the same name come *on*
I could see the future complaint that something so obvious isn't enabled by default. 
Yeah, I'm kind of new to Haskell and this just boggles my mind. Why isn't this already in the language? Are there some difficulties with implementation for Haskell? Or is there some ideological stance on this? Haskell is not exactly a new language.
The solution we came up with is rather general and quite powerful. The issue is that saying "names should work between records" is fine if you always know what your records concretely _are_ (or if you are in an OO language and your records themselves know what they are at runtime :-P). But translating this to a proper haskell type that plays nicely with everything else in our type system, where often indeed we _don't_ know much about what we're getting handed outside of a typeclass constraint at best, that's a more difficult task. So in the presence of all our other neat features, this becomes hard to specify a uniform behavior for that both does what users expect and _also_ makes sense to implement such that there are no weird corner cases. The new solution will probably not be perfect either, but its the closest we've come to something workable.
How about isToken w = all [w &lt; 128, w &gt; 32, w &lt;= 57, w &gt;= 65, w &lt;= 90, w &gt;= 94, w /= 9, ...] Looks better, right?
&gt; Why isn't this already in the language? Are there some difficulties with implementation for Haskell? Or is there some ideological stance on this? The short version is that it wasn't entirely obvious what the best way to go about it was. For example, what type should *name* have in that example? There were a few competing ideas, and the consensus was that it's better to wait for a good solution most people can agree on than to rush forward and have to break backwards compatibility later or get stuck with an old solution nobody liked. [This page](https://ghc.haskell.org/trac/ghc/wiki/Records) has a decent overview of the problem and some of the proposed solutions. [This](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Redesign) seems to be what they are adding into GHC. 
He certainly has an unnecessarily combative manner.
Ok thanks for the report. We're tracking Windows support as a Github issue here: https://github.com/tweag/HaskellR/issues/201.
Thnx! Yes the applicative way is how my code looks at the moment ;) just wanted to be explicit in the example above. Also looked at json-sop - gave me more understanding, but my mind is still struggeling a bit with the juming between types and kinds :) Looking fwd to the to the generic Binary code!
(Wouldn't SO be better suited for this?) Here's roughly how you would do it using generics-sop: gparse :: (Generic a, All2 HasParser (Code a)) =&gt; Parser a gparse = do n &lt;- parseTag x &lt;- hsequence $ (apInjs_POP (hcpure (Proxy :: Proxy HasParser) parse) !! n) return $ to $ x where parseTag :: Parser Int Using `hcpure`, you generate a two-dimensional table of parser calls (for all constructors, for all arguments). Using `apInjs_POP`, you apply the corresponding constructor functions to each row of the table. You then select the one that corresponds to the tag you parsed and "run" it in the parser monad. The `json-sop` package should contain a similar function, and the generics-sop paper does as well.
Yes! I think that is more or less (haven't tried out yet) the thing I was looking for, thnx! My mind was struggleing a little bit with the type- vs value-level lists an i was not sure where an how to apply the (!!) :) Will try that.
&gt; The new solution will probably not be perfect either, but its the closest we've come to something workable. What are its caveats?
Here's a relevant and very interesting blog post (from few days ago from r/haskell) about Scoped Continuations, which argues that they are better (at least in some situations) than Monads: http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/
Are there people actually still using miranda?
Well, ambiguity becomes possible more easily, requiring annotation. In practice, it's probably a trivial complaint. But it would be nice with a mind-reading compiler!
[I suppose that has worked well for them...](http://www.i-programmer.info/news/216-python/7179-python-27-to-be-maintained-until-2020.html)
At least this is considered a bug, not the expected behavior.
&gt; English is bad with this kind of thing. On présume que le même problème existe dans toutes les langues humaines naturelles.
Well, you still need to turn `FlexibleInstances` on or define orphan instances (about which there are warnings). I agree, though, that it's a weak justification for this implementation deficiency. I wish GHC was better at enforcing coherence. I wonder if UHC is better in this regard.
Indeed, I think `bracket` is another good example, but even `runReaderT` works nicely: runReaderT do foo bar baz env
No, `FlexibleInstances` is only needed to confuse GHC enough to not emit orphan warnings if orphan warnings were activated. By default, the code compiles fine without any warning at all: $ ghc *.hs [1 of 4] Compiling B0 ( B0.hs, B0.o ) [2 of 4] Compiling B1 ( B1.hs, B1.o ) [3 of 4] Compiling B2 ( B2.hs, B2.o ) [4 of 4] Compiling Main ( Main.hs, Main.o ) Linking Main ...
Slightly off-topic but how close do Overloaded record fields get us to an extensible records implementation? TypeScript just got intersection types in the 1.6 beta and man.. it feels bad to be missing a TS feature when you're writing Haskell.
I'm hoping GHC is smart enough to make these transformations itself, but I wonder what the performance of using [PrimBool](https://ghc.haskell.org/trac/ghc/wiki/PrimBool) is like - it should be equivalent to the code generated by Rust if GHC's isn't already. I also wonder if there's a more efficient implementation using Karnaugh maps. Probably not, it's more something that's useful for hardware, but it might be fun to find out.
What's wrong with this comment? Too short? I don't see the point that this comment is not constructive. Why are people here so easily get trolled? Maybe there really should be a down vote for the down votes that are not constructive.
What I'm saying is that the Python devs made the wrong decisions and they are trying to fix them but people keep using the wrong solutions simply because they were there first so they have to extend support for the wrong solutions and that's the situation we don't want to be in.
Ok, I see. But, IMO, this is why Simon Jones would even say "avoid success at all cost", rather than "avoid defects before it's too late".
I am sorry you feel that way. It was never my intention to disrespect you. While I can answer each of your concerns at length -- as you have misunderstood my claims -- I don't feel welcome here, so let me say something that pretty much sums up my position in the thread you're quoting from and others, and if you think it's "ridiculous" or "trolling" or at odds with your ideas on "how we view math", well, so be it. My opinions and arguments are not based on theoretical logic but on over twenty years of professional software development, both as a programmer and later a manager, in and of teams both small and large, in projects ranging from mobile apps to defense (the first language I used professionally was Business Basic...). Still, feel free to disagree. I fully realize that completely different yet not-any-less valid and well-supported conclusions may be reached. Also, as /u/tomejaguar doesn't like me talking about Haskell, I won't mention Haskell; in fact, I'll do better -- I won't even mention PFP. How's that? I believe programming languages can serve two roles: a formal specification of an algorithm and a UI for programming a physical computer. We'll start with formal specification, which serves one main purpose -- proving correctness (or, alternatively, assuring correctness with some probability). Such a language *must* have a mental cost, namely require the programmer to concede some convenience to the verifier. Why? Because if it didn't -- meaning the programmer could write however she thinks or normally expresses programs -- then either many useful properties could not be proven, or we have a contradiction with the halting theorem. So this will be a language where the programmer makes concessions to the tool -- pays a price -- and in return, the programmer gets a proof that her algorithm is correct (for some definition of correct). To be useful, it is *my opinion* that such a language must be worth its price, so it must be able to prove non-trivial properties. At the very least it must be able to verify concurrent/distributed algorithms -- those are the hardest to get right -- preferably with something at least as powerful as temporal logic (session types have similar expressiveness to regular expression and fail at this); it should be able to verify most sorting algorithms (as an example of a "functional" algorithm), and preferably, it should verify time and space complexity (which are especially important to safety-critical systems). My *non-requirements* are speed and generality. Speed because correctness proofs are most valuable when the algorithms are subtle, complicated or critical, and such algorithms don't change often. Lack of generality means that it is not too crucial that the verifier applies to every language features and every library used, as developers may restrict their use of unverified features in correctness-critical code. Such features are nice-to-have but are not deal breakers because correctness has a price but its utility is not uniform throughout the code, so limiting correctness to portions of the code is acceptable. Although, to be more precise, correctness isn't even binary, so ideally I'd like different degrees of correctness -- each with its respective price -- for different portions of the code. Now on to the other role: a programming UI. A programming language whose role it is to produce general-purpose executables should minimize the cost of their production. Production of what? Of software that answers various requirements of functionality, correctness and performance. Those requirements are almost never uniformly absolute, BTW. Just as not every functionality needs to be as performant, not every functionality needs to be correct (especially as the halting theorem dictates that correctness must have a cost). *In my opinion* this entails being easy to learn, write and read so that programmers can move from one project to another, share work, maintain old code etc.. It should also produce executables which are performant and easily monitorable. Unfortunately -- as with any UI -- we have no way to *apriori* determine how well a given language performs this role. The only way is through empirical study. It doesn't matter how simple, elegant or beautiful a UI seems to its designer. It doesn't even matter how expressive it objectively is. A UI either works well or doesn't, and you only know after you've put it in the field. A language that proposes a completely new programming paradigm must therefore prove that it is better at this role -- namely reducing costs -- than its predecessors. Moreover, it is not enough for it to be provably better, it must be significantly better to offset the switching costs (training in the language, tools and libraries, rewriting of old libraries, rewriting of tooling etc.). In either case, the burden of proof is *solely* on the new UI. At this point I would add that my requirements from a programming languages for these two roles come from my own relatively extensive but certainly far from universal experience. Other people's experience may well be very different. And, naturally, my requirements are relevant for languages that are intended for use in the industry; research languages may have very different requirements (like easy extensibility). However, requirements can only be justified by *some* experience, and each requirement has its own price and relative utility. You can't just say, property X is good so we need property X! You must also consider the price of property X, its relative importance to property Y (which might be at odds with X), and your total budget. Both roles are *generally* (i.e. not at every point) at odds with one another, because to prove correctness the programmer must make concessions to the language, and to reduce costs the language must make concessions to the programmer or the team's workflow. So one approach is to use two separate languages. For example, Amazon write most of their AWS services in Java (I believe), but verify them with TLA+. Another approach is to pick a language in the second category and a verifier that works with it (and some additional annotations) as well as possible. When I wrote safety-critical hard-realtime systems in (realtime) Java, we used NASA's JPF model checker to verify the crucial bits; there are newer, possibly more sophisticated tools like that today. Another approach is to have one language try to fill both roles at once. The greatest success in doing that, IMO, is Esterel, which is successfully used by the industry to this day (maybe not Esterel per se anymore, but one of its descendant languages, some of them graphical) to write provably-correct safety-critical hard-realtime control systems, mostly in aviation (I had the pleasure of using it many years ago, so it could be that my views on it are strongly tainted by nostalgia). It is clear, quite easy to learn, produces provably correct programs verified with temporal logic, and also verifies state and time complexity. It can't verify a general sorting algorithm, but it's not intended to code such algorithms anyway (I'm pretty sure you can't). The price it exacts is restricting the computational model (it is not Turing complete), but that's fine for this kind of systems. I *believe* that a general purpose language that serves both roles well is not easily achievable, and I *think* it will be easier to achieve by reducing the first roles' cost (with more advanced verifier) than the second (change how people respond to UIs). In any case, I am *pretty sure* a general-purpose language that does both reasonably well does not yet exist (I have some ideas of what it can look like) -- I *know* I haven't seen it -- and those languages that try might end up being inadequate in both. I could go on and in more detail into explicit vs. implicit types and more, but I think that as it is, it's a much more respectful response than your comment deserves (although I appreciate you taking the time to do a background check on me). Anyway, that's the gist of it and it's already waaay longer than I'd hoped...
GHC is smarter than I am. Reading my mind would make it dumber.
Same could be said of any language. C strings are null terminated? Call me again when C ceases to be a prototype. Haskell isn't perfect (yet), but neither are other languages.
&gt; Can we agree at least that it's a mistake not to include the Semigroup superclass for Monoid? Yes.
&gt; IDE's for C could also display the corresponding parameter name of an argument permanently. Would that still be considered C? Or will someone looking at it will wonder: "what language is this?"?
If you have lots of `IO (Maybe a)`s, by all means, use MaybeT. But for `IO` and real world cases, there is something to be said for being explicit. tryAndGetA :: IO (Maybe A) tryAndGetB :: IO (Maybe B) tryAndGetB = tryAndGetA &gt;&gt;= \case Nothing -&gt; do -- Tell the user now because Nothing carries very little -- information about what really went wrong. logWarn "Oh no, we couldn't do the thing at the place!" return Nothing Just a -&gt; return (Just a) As a beginner it took me a while to realise I could just do stuff like this without the functional programming police coming for me because I didn't use every possible combinator.
I'm not sure how the firewall works, but ssh and openvpn, for example, have been identified and selectively blocked for quite some time now, a few years?
It.. it just works.. I've been wanting this for ages. Thanks, thanks, thanks! Some drink$ on the way.
Learning a language (especially one as "foreign" as Haskell) is a huge investment. Making that investment on the wrong thing is a big mistake. Thus, people prefer to err on the side of caution and avoid investing a huge chunk of time learning something that sucks. Thus, psychologically, people are inclined to look for reasons (excuses?) to rule out the learning investment. I found a missing feature that I think is rather basic? Got my reason to learn something else. I think I first got interested in Haskell in 2002 but found some silly code I thought was ugly in the standard library as my excuse to avoid learning it. Years later did I renew my interest and saw the silliness of the excuse I used earlier.
Hmm... I guess we've got some catching up to do.
Some type signatures will probably become a bit harder to read, since the functions defined are necessarily more general :-)
As I understand it, it should be possible to take the raw overloaded fields tools and use them in conjunction with an existing extensible records implementation like `vinyl` through defining a few typeclass instances. But I await people trying this in practice to see how well it really works :-)
&gt; I would say the language should simply be abandoned. You can believe that, but you clearly have never been a CTO entrusted to pick a technology stack for a critical, large piece of software, that has to be maintained at least for the next 10-15 years, and that people's lives -- or certainly a shit-ton of money -- depend on. I have (more than once), and I can tell you that the actual problem is that Java has *too few* viable alternatives (ranging from none to one, depending on the requirements). Other languages suffer either from lack of maturity, lack of stability, lack of assured prospects, or lack of extra-linguistic features -- e.g. performance, observability, hot code swapping, backwards compatibility, availability of libraries, availability of professional support -- that are about, oh, 10-100x more important than expressiveness or any other syntactic language feature (they're not all always required, of course). To give just one example, a language that doesn't have a strong commitment to backwards compatibility simply *cannot be chosen* by a responsible CTO for a critical project (the lifetime of the average codebase in the industry is over a decade). You can dislike Java all you want -- I know there are valid reasons to, as there are valid reason to dislike any other language I've ever seen; there are also plenty of reasons to like it, as I do -- but if you think it can be replaced today, then you are simply unaware of the actual state of the industry. You're wrong about the facts. Some of the blame, BTW, lies with maintainers of other languages that are somehow misguided about what actually matters for large, important projects, and instead concentrate on linguistic features. Unfortunately, I've seen this happen with some of my favorite languages (like Clojure and OCaml, although the latter has suffered from a bit of a general neglect for some time), but so far I have to say I'm hopeful about Kotlin (Rust, too -- for the C/C++ niche, where I spent my time for many years; Go is too oriented towards the way Google works). It's understandable, though, as today people can't make too much money off of programming languages, so maintainers do what they like instead of what's required. Languages developed in academia suffer an even bigger problems: their designers often don't have much experience in the industry, so they just *don't know* what features are important; even when they do, they have good reasons not to do them because they're often at odds with the developer's own academic interests. Which is why all successful languages *designed for usage in large, "serious" applications* have been sponsored by large corporations or organization: Fortran, COBOL, C, Ada, C++ and Java. So far there has never ever been an exception to this rule, and the reasons are clear, I think, from my description of the problem.
I think only if you are constructing, you don't need it if you're just pattern matching. But I forget the exact details, my memory is just that "sometimes you need it". I know that for using them in SDL2 so far (which is just pattern matching) I haven't needed the extension.
&gt; Ideally we should be able to have just records in the language, with a construction syntax light enough to be able to pass a record instead of a series of (named) arguments without much overhead. Actually this is pretty much exactly what we do in Lamdu. The editor sometimes displays it slightly differently (or very differently in the case of infix operators), but in the underlying AST it is just an application of a function with a record as its argument.
I do, I really do...
I tried this on GHC version 7.10.2 (which is the version I have used to run the tests in the post) and it takes the same time to parse as the version which uses `notInClass`. :( I also tried to use pattern-matching since it [optimizes even better in Rust](https://www.reddit.com/r/rust/comments/3k0d0d/parser_combinator_experiments_part_3_performance/cuu5c8r), hoping that GHC would be able to optimize it into a bitset just like LLVM does. Sadly this yielded the same performance as the other two options. I do not know if GHC already does this transformation or if it is something it cannot do just yet.
&gt; Pervasive laziness has its advantages, but it has disadvantages too, and that’s why it isn’t a common feature in programming languages. It's true that non-strictness and strictness have relative advantages and disadvantages, but those are not the reason why non-strictness isn't common. The first computers were based on an imperative approach to the foundations of computer science, led by Alan Turing, Max Newman, and John von Neumann. For the first few decades of the computer age, non-imperative approaches were completely unknown to practitioners and to most theorists, even though those other approaches actually predated the imperative approach. By the time the other approaches became better known, there was a huge amount of industrial and intellectual momentum to imperativism and strict programming languages. It's interesting to speculate what things would have been like if instead the first computers had been built based on the work of Church, Curry, Schoenfinkel, or Babbage.
Since we're talking IDEs already, has anyone else considered writing a haskell IDE that allows displaying and manipulating haskell programs in a flow chart/graph way? I think that fits functional programming really nicely in terms of "reading" comprehension and would make for a great addition. I was thinking of something like nodes are functions, constants and parameters, and types are represented by jigsaw-puzzle-like patterns which connect the return value of a statement with the return value of the function you're making or a missing parameter of a function you're calling. Arrows could handle data flow if it's non-trivial (think let-in or multiple usages of the same parameter). I haven't thought this out nearly enough, but I for one am tired of entirely text-based programs. The options for debugging and profiling I haven't even considered yet.
&gt; It is better make a wrong decision and fix rather than hesitate and stall. Strongly disagree, as apparently do the GHC maintainers.
&gt; the "formatting with spaces" craze! What is your preferred alternative?
There was someone on here a while ago who tried making a graphical representation of functional programming. He had trouble fitting all the desired semantics into graphical symbols though. Perhaps someone else remembers the name.
&gt; We avoid the laborious type declaration ceremony. In a language which has a structual type system with type inference, these type declarations are not necessary How does Lamdu provide the second advantage of tuples?
&gt;(Also, I think we should replace `.~`, `+~`, `/~` by `:=`, `+=`, `/=`, etc.? I find that for some reason novices respond much, much better to `+=` than to `+~` - which obligates me to use the former, which brings a lot of tricky concepts I'd like to leave for another moment.) A third option would be dropping the operators (at least at first) and focusing on `set`, `over`, etc. instead.
That's an interesting question. Do popular computer architectures put functional languages at an unfair disadvantage? Hardware engineer Yossi Kreinin argues [here](http://yosefk.com/blog/the-high-level-cpu-challenge.html) and [here](http://yosefk.com/blog/high-level-cpu-follow-up.html) that things are not so simple, because functional techniques inherently use more dynamic allocation and levels of indirection than imperative techniques, no matter what kind of computer you have. I imagine that would apply even more strongly to laziness.
And what should tuples use?
You're right, IMO. Just like syntax highlighting does not change the fact that it's C, fancier decorations don't also.
There are some very smart low-level optimizations. [See this page for details](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/GeneratedCode).
Something like [ghc-reskin](https://github.com/gibiansky/ghc-reskin) ?
And apparently, non-empty lists will be there [too](https://www.reddit.com/r/haskell/comments/3ipsc6/ghc_712_release_plan/cuk7pe9).
Isn't this what Lamdu is?
That's excellent to hear. And thanks for your awesome libraries. 
By using structural record types. You just create a record "on the spot", just like you create a tuple. You do choose names for the fields (or they are inferred from the type, if a record type is inferred), but that is unceremonious :) 
`+=`, `*=` and others already have meaning in the library? Or am I missing something?
If `(.)` wasn't used for function composition then you wouldn't be able to write `foo.bar.baz += 12` anyways, as it'd only be usable for field access not change. You can wish for all the things you want, but that still doesn't empower me to make the typechecker make them work.
&gt; There should be one-- and preferably only one --obvious way to do it. This viewpoint is one of many reasons why I don't work in Python. Let a thousand approaches prevail, then pick the winner if you must. Lens isn't the classic ["Haskell-y way"](https://ro-che.info/articles/2014-04-24-lens-unidiomatic) of doing anything. In Python it would have been slaughtered in its crib and we wouldn't even be having this discussion.
That's quite a bit closer. Now make jigsaw puzzles patterns represent types when joining boxes together and add some functional programming related stuff. The idea is that you can use visual cognition to match types and such and that you can see a lot quicker how data moves around your program. Maybe the program could also display suggested functions to use somewhere. I'm imagining the workflow to be somewhat like making each function a graph. So a module would be a folder of several graphs. Now, for your function you're working on you first define a interface. This gives you your parameters as nodes up top and a jigsaw pattern for your return type on the bottom. Then you pick auxiliary functions etc. (Suggestions come in here, as does a quick way to GREP the imported modules to quickly summon up functions) and place them accordingly. If you need to explicitly control data flow (as for example when you use let-in, as that violates the strict tree structure and makes your program a directed graph by piping one data source into multiple drains), you use arrows/graph edges. I think this would work incredibly well with FP.
There's no known way to break coherence using standard Haskell (without extensions) without orphan-instances (easily disabled by `-Wall -Werror`). So, ahem, Haskell people *can* implement the scheme in their own compiler.
Aside from the fact that this notation collides with about 12 other things.
Cale manages to avoid even mentioning the operators when he teaches folks `lens`, at least until I mention them and he groans in pain.
"Better to novices" isn't the most important concern in language/API design. In my mind this is part of the core spirit of Haskell. We don't do things just because that's the way they've been done. We do the Right Thing^TM. After that I think it's fine to try to make things better for novices, but not if you have to sacrifice a bunch of stuff in the process.
I fully support that approach!
&gt; In Python it would have been slaughtered in its crib In Haskell it doesn't get welcomed with open arms either... - https://www.reddit.com/r/haskell/comments/23uzpg/lens_is_unidiomatic_haskell/ - https://www.reddit.com/r/haskell/comments/1o6iqs/why_i_dont_like_the_lens_library/
what these are?
For the HdpH fib, you'd calling v &lt;- spawn $(mkClosure [| fibRemote (x-1) |]) And then fibRemote looks like fibRemote :: Int -&gt; Par (Closure Integer) fibRemote n = fib (n-1) &gt;&gt;= force . toClosure Are you subtracting "1" one time too many, or am I misunderstanding something?
Well spotted! I'll fix that. The (n-1) should happen on the remote side in `fibRemote` .
What's the TL;DR of this compared to other default superclass proposals? Why is this one better than the other proposals we've heard so far?
&gt; It is not about what a novice wants, it is about how much simultaneous information you can throw at them. Then do what is suggested elsewhere and don't teach them operators at all. Just stick with view, set, over, and composition.
Where in this classification are [Elm records](http://elm-lang.org/docs/records) and why Haskell can't implement them?