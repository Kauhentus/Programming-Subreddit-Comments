This is quite cool. But why have you adopted the obnoxious syntactic accoutrements of C? Why not just let the compiler infer `{`, `;`, and `}` like it usually does? C's syntax is one of my least favorite parts of the language. (Do I even *need* to explicitly mention how brain-dead, say, declaration-reflects-use is?) Providing less clunky tools for imperative programming in Haskell is probably a good idea. Documenting your efforts towards that goal by using an awkward and unconventional Haskell code style is probably not.
[My old question should be relevant](http://www.reddit.com/r/haskell/comments/qi0tu/is_this_possible_in_haskell/) The video talks about the developing benefits of live updates. Really exciting IMHO.
Your first two signatures aren't the way effects are represented in eff. They're the signatures of the operations on some non-free carrier algebra, `s -&gt; r`. The free algebra of the signature the eff paper uses for state looks like: data FreeState s a = Var a | Lookup (s -&gt; FreeState s a) | Update s (FreeState s a) But this can be inspected in ways that `State s a` cannot. For instance: Update 5 (Lookup $ \s -&gt; Var s) /= Update 5 (Var 5) :: FreeState Int Int But: put 5 &gt;&gt;= get = put 5 &gt;&gt; return 5 eff will let you write handlers that observe the freely generated structure of the state algebra, unquotiented by the rules one might expect for state. `State s` incorporates the rules, by contrast.
My biggest complaint with text-icu is the nastiness of the icu API. The external dependency issues on OSX are another wart. I've written some of my own encoders for ByteStrings to handle the most important cases (UTF-8, ISO-8859-1, and ISO-8859-9), but I don't relish maintaining them. The encoding package looks like a nice alternative for that sort of thing; I'll have to see how it compares with my hand-optimized versions.
TIL typeclasses too are just syntactic sugar! I'm constantly amazed by how nearly most of the features of the language are implemented using already existing features.
I completely agree - writing Javascript is like being whipped with vinegar covered thorns! Coffeescript takes some of the pain off - largely because there's very little mental effort required to transpose the JS back to the point in the source code you need to look at. I think to make this viable you have to have the ability to decompile back to whatever you wrote in the first place. Wether that is very readable generated source (and a reasonably close mapping between what you write and the JS that results as in the case of coffeescript) or something like source maps [1] and IDE integration I don't know. [1] http://www.html5rocks.com/en/tutorials/developertools/sourcemaps/
I'm not the OP, but I'm sure that it's intended for the browser. After all, we can always compile with GHC for the backend.
The UHC compiler has a Javascript backend, and includes an [FFI for Javascipt](http://utrechthaskellcompiler.wordpress.com/2010/10/29/a-haskell-ffi-calling-convention-for-javascript/).
"HJ" could return some (un)intended search results...
Handlers in eff are homomorphisms from the free algebra of the set of operations to some other algebra thereof, which corresponds to a fold over my above `FreeState` datatype. The handler there extracts in a way that is consistent with the expected behavior of state (I think), but there is nothing stopping someone from writing a handler that _doesn't_, for instance, one where `update` does nothing, and `lookup` returns a constant value, or where `update` keeps a stack, and `lookup` pops, so updating twice isn't the same as updating once. The difference is: monad algebras of the `State s` monad have (effectively) `lookup` and `update` actions that interact in the way one would expect of a mutable cell. `State s a` is the free such algebra over `a`, so it automatically follows those rules. Algebras of `FreeState s` have `update` and `lookup` operations, but there is no necessary connection between them. It's the difference between saying: &gt; A ring is a set R with two monoid structures (R, +, 0) and (R, *, 1) such that (a + b) * c = a * c + b * c and saying: &gt; A ring is a set R with two binary operations +, * and elements 0, 1.
If I started a project using a web framework today, and it was only me working on it, I would absolutely choose Yesod over Rails. That's partly just about choosing Haskell over Ruby, but also from all I know right now, my guess would be that the Yesod project would be much lower maintenance in the long-term (as I mention on the last slide). Realistically, for a typical developer who doesn't already know Haskell, needing to learn Haskell (and not just simple Haskell -- Haskell with all the bells and whistles) to use Yesod is a noticeable upfront cost. But if you already know Haskell well, Yesod is very nice.
Vaguely based on it, but the actual language is called *Core*. There's an interesting paper about it [here](http://community.haskell.org/~simonmar/papers/aos.pdf), the relevant section is 3.1
I don't currently understand what the use of indentation gains you in Hamlet beyond not worrying about closing tags -- but what it loses is surely the ability to copy in existing views from Rails, PHP, and all the rest. It just seems like a barrier to transferring from existing frameworks, for no huge gain to the developer. As for the minification, by default in Rails 2.3 it's not minified. I suspect there is probably a plugin to do it. I think it's also not minified in Yesod.
The major complaint I heard about ghcjs is that it include a huge amount of js code (representing the ghc runtime I think) by default. Are you compiling to google closure advanced mode and getting dead code elimination?
I think you have a unique perspective here, as you are tasked with Rails &lt;-&gt; Yesod conversions. Generally Yesod users are not transferring views from existing projects, but building them from the ground up. I found the very little difficulty in converting from normal html to Hamlet: all you have to do is delete the closing tags. If there is extra hassle, it can be because the html is not following best practice of indenting tags, which explains the need for Hamlet's indentation. That said, I have done very little converting. To give you a little more perspective on that, every Rails project I was on that uses ERB always ended up with invalid closing tags somewhere. It can get difficult in a large template to figure out the closing tags. The Rails community is the original embracer of HAML, which started these whitespace principles for template languages. Maybe we need an automatic converter though? I certainly understand that someone wants their template to work perfectly in a certain editor without writing any plugin code though, in that case a different template language (maybe Heist) is a better choice, and should work fine with Yesod. BTW, Great slides!
&gt; If you only know C, the less foreign the syntax the better for baby steps. This mode of thinking has hampered language design for decades, IMO. C is pretty much the only reason that, say, Python is statement-based rather than expression-based. It's also one of the main reasons that most languages don't have tagged unions or pattern matching. Only recently have these foolish trends been reversed. Sadly, if you bring up any of these issues to a C programmer, they'll invariably say that C is statement-based for reasons of performance and ease of compilation. Statements, they claim, are "closer to the metal." (If C is supposed to be that close to the metal, then why does it provide such abstracted, high-level features as compound, multi-op expressions and structured programming constructs?) Likewise with tagged unions. (The tagged union is one of those fundamental assembly language idioms — see Knuth, Vol. 1, Ch. 2.) Quite frankly, I have no idea where people get these views from, as they're all completely false. Also, `NULL` and `'\0'`-terminated strings: WTF? Sorry for ranting. This is just something I... feel strongly about.
The video on the page does not load for me and if I go directly to Vimeo, I am told it is a private video. Anyone else have this issue ?
Free things were, I think, traditionally described as satisfying only the required equations and no others (and only having the required 'elements' and no others). Maybe it works if you state it in a suitably general way, if that's even possible (i.e., [()] is commutative, but ¬(∀a. [a] is commutative)). But really, this is probably an example of why it's good that category theory can provide a firmer definition of free objects.
Pipes core is a fork maintained by a friend of mine who has his own ideas. I still prefer my pipes package and maintain that. He is definitely going to use either Free or FreeT, and I think I've convinced him to use FreeT.
There isn't much evidence backing the claims of the author, they just say that it's "hard", "unmaintainable", etc. without any proof. &gt;Perhaps I’m further away from the pure object-oriented approach than I’d thought. I’m certainly building smaller, cleaner and better structured software than I ever was before. Maybe for certain problems OO was a bad choice for them. Maybe they're bad at designing OO software. Maybe their software now is actually worse or no better than it was before in terms of maintainability, readability, etc. I thought there were some interesting points, but I think it needed more detail and some of the points seemed weak, for example, the arguments against design patterns. &gt;Some patterns even turned out to be anti-patterns Actually, the author just mentions one, so it may be true that one may not be that good or used too much, that doesn't mean the others aren't useful. &gt;We soon learnt to use patterns judiciously. Shouldn't that be for anything? I don't know what this is arguing against other than a strawman who uses patterns too much.
I had this problem on Firefox, but the video worked when I used Chrome (my OS is Ubuntu)
Do you mean the Hitler-Jugend?
http://conal.net/blog/posts/the-c-language-is-purely-functional
Ah, didn't know about this one. But the last post was 5 months ago and the subreddit doesn't have much traffic...so I think this is still worth doing.
with me that happens because i have a flash blocker on by default that vimeo does not play nice with. funny thought that the 'private' flic can be publicly consumed only when embedded.
Perhaps it should be added to the "other subreddits" list in the sidebar.
GitHub: [https://github.com/mikeplus64/hotswap](https://github.com/mikeplus64/hotswap) Example therein: [https://github.com/mikeplus64/hotswap/blob/master/examples/Main.hs](https://github.com/mikeplus64/hotswap/blob/master/examples/Main.hs)
I would suggest four things, all of which I've been meaning to work on a while, but haven't: The first is tiny - get the dbmigrations package to build with current libraries! it was really useful, but the last time I tried to build it it wouldn't work. Second: (and largest) a system automation system similar to puppet/chef, but that is flexible AND easy to use. There is a lot you can do with those libraries, but they are a beast to even figure out the beginnings of. This is a big project! I basically want to be able to say, I have a debian based system that should be running this software and these services, and it lives at this IP - set it up! Third (small to medium size): a backup system, for databases and files, that is flexible, declarative, and ideally can be scripted (has library access, in other words). Fourth (medium size): a deployment system (for web applications or really anything that needs to be put on a server). something along the same lines as capistrano, but with haskell goodness. again, the key in doing in really well are exploiting declarativeness, so that you can write that these applications should be running on these servers, and tell how they should be built and deployed, so that deploying is as simple as "bring everything to current". So think less "deploy" and more "sync". These are all things that I personally would love - they all have analogues on other languages, but each has their own issues, and I think they could be done better in haskell, and it would also, as a side-effect, make haskell a more attractive platform to build real systems with. They are also things that I think would all work really well as haskell libraries, as they are all more or less based on declarative ideas (albeit it an imperative world!).
No, this is not a communication channel for libraries. This is a communication channel between the user and any code inside a program (even in libraries). This is more composable, because the library just specify the command line flags and the main doesn't have to contain an explicitly maintained list of all the modules (or flags) used. I mention withArgs for testing, it's not related to composability. If you run initHFlags inside withArgs, then you can change the command line args before the library parses them. So just the standard usage of withArgs.
I found that objects break down exactly when you use them as metaphors for something, instead of what they express semantically: Bundeled closures. If you have closures to bundle, use them, if you don't, don't.
From the comments it looks like there is some collision detection. Generally I think commands should be separated to different modules based on modes. Does hflags support modal arguments?
It could be.. the plugins-auto library provides similar functionality, but includes file-watching support that automatically reloads the code. Unfortunately, the file-watching code is hardcoded to use inotify and therefore, requires linux. The yesod project has (from what I understand) developed some general purpose file watching code. This project includes a simple interface to the plugins library for loading and unloading, with out requiring any sort of filewatching code. So, perhaps another library could be built on-top of the two which would replace plugins-auto in a cleaner, more portable fashion. Another advantage of plugins-auto over hotswap is that it includes some TH code which automatically extracts the type-signature of the plugin at compile-time. That can serve to provide greater type-safety in many cases. Though, clearly the plugin can change its type-signature after the fact and start failing. I like a number of the ideas in plugins-auto, but a cleaner,more modular, and more portable implementation would be great. (I am one of several key contributors to what eventually became plugins-auto). In my opinion, the GSoC is basically plugins-auto done right. That includes not only support for loading via plugins, but also rebuilding and restarting via cabal, and maybe loading via hint, and maybe reloading via GHCi. We have a lot of 'prototypes' for the GSoC project to draw on. Hopefully we can now get the one-true code reloader.
Gtk2hs needs love. I know that web development is all the rage, but ordinary applications still matter and gtk2hs would really benefit from more maintainers.
That could still be the case .. there was talk about trying to merge things so that platform specific watching could be used where available (inotify, etc) and then fall back to busy-loop in the worst-case. I am pretty sure linux, OS X, and Windows all have notification libraries -- so most places would have real file watching.
If you avoid mutable state in Java you can modularize code even when acting on one data type. But maybe you call that non-OOP style Java. And in C++ even mutable state mutators can be modularized by using friend classes.
Change the first letter to H fir haskell and to make a fun name for it like ... oh, nevermind.
&gt; is it easily possible to share state at the application level? (without the need for memcache, or a DBMS) Not sure what you mean here. Share between what? Our snaplet system makes it really easy to manage in-memory state for request processing. &gt; are there alternatives to Heist? Because to me, it seems that one of haskell strength would be that you can could use Haskell to build statically typed HTML, without the need of a specific template engine. Yes, you can use Snap with pretty much any Haskell template system. The core Snap developers tend to be more interested in Heist though. I used to share your attraction to statically typed HTML generation until I built a site that way. I created Heist specifically because of the frustrations I encountered with that site generating HTML from Haskell. &gt; is there an abstraction for routes? As in, can I rename a route, without having to update my templates? Not built in, but there are a number of ways you could go about it. You could do something along these lines with Heist alone, or you could use a separate package like web-routes which was designed for this purpose. &gt; why aren't models encapsulated? It seems the database support prefers raw SQL. I would rather specify my scheme in Haskell, and have it type-check my queries? Interfacing with a database is a very complex problem and we felt it would distract us from the problems we wanted to work on. In my opinion Haskell's superior power and flexibility give us the luxury of being able to let other libraries handle database support. There are a number of different database libraries that each have a different approach to the problem. Our snaplet system provides a great way to integrate support for any of these. In fact, there are already seven different [snaplets](http://snapframework.com/snaplets) providing support for different database packages. I'd much rather use a framework that can easily work with anything than one that provides out-of-the-box support for something that ultimately boxes me in. There are at least three libraries on hackage (haskelldb, persistent, and groundhog) that provide type-safe database querying to some degree. With Snap you're free to use any (or none) of them. Maybe a database backed blog is overkill. The filesystem is a perfectly good data store for a number of applications. With Heist, we are able to store our pages in markdown on disk in an easily greppable form and have them served from memory to the user. For our purposes, this is perfectly adequate and almost definitely easier. It also means our site content is backed up transparently and git gives us a complete history of our changes for free. Snap's goal is to provide a simple and flexible way to serve web pages. This is orthogonal to interfacing with databases, and the power of Haskell allows us to keep it that way.
Then why are being taught that OOP solves exactly this in school? Or do we just need to wait for non-OOP methodologies to catch up in terms of education? 
If an object is stateless, wouldn't that make it into a function in the mathematical sense?
I meant handjob. Sex is a sin, not nationalsocialism.
Firefox on freebsd user here. I have [this extension](http://mp4downloader.mozdev.org/drupal/) installed which creates a download button underneath.
&gt; a tool that can generate C wrappers for existing C++ libraries Surely something like this already exists somewhere in the C world? If not I'm sure they would greatly appreciate such a thing as well.
CSS selectors for whatever parsing library to simply extract nodes from a NodeList Probably some jQuery sauce for node operations will be good to introduce lots of newbies from the WWW world. class NodeList a where getNodes :: a -&gt; [Node] instance String NodeList where ... instance ByteString NodeList where ... instance Text NodeList where ... instance [SomeXMLLibraryNode] NodeList where ... instance [AnotherHTMLLibraryNode] NodeList where ... Also, a SOAP client would be very nice, however this is real hard and messy business.
This remembers me the opposite way that was taken in [DTC](http://hackage.haskell.org/packages/archive/DTC/1.1.0/doc/html/Language-Haskell-DTC-Class.html) (Data To Class). Though it was just a theoretical entertainment.
Why would I want to do the compiler's work? Passing the instances around manually is very cumbersome. Also, type-classes give a *guarantee* that the same type will always have the same instance. For example, when using a (Map k), we can know for sure that k has one true Ord relationship. If we passed around the Ord instance to different Map operations, we could have nasty bugs where we passed different Ord instances in different cases. If we stored the Ord instance value inside the Map, that's somewhat wasteful, and does not allow operations to compose two maps (as the ord instances are not comparable to each other) efficiently.
That's exactly how type classes work under the hood. A value-level "type class" is usually called "dictionary". The extra syntactic sugar is so that the dictionary can be automatically passed around to functions that need it. I tend to agree that type classes, although powerful, have several shortcomings, and maybe a different approach could be devised, but simply removing the feature is not a solution. In scala, for example, type classes are also implemented as dictionaries, but the automatic wiring is taken care of by the implicit parameter system. This looks a lot more powerful than Haskell's mechanism, because it has all the flexibility of the manual approach, with all the convenience of traditional type classes. I suspect, however, that it's *too* powerful, in the sense that you can run into coherence problems, since no particular instance is tied to a type, and resolution is scope-directed, rather than type-directed. I think that there's room for a new approach here, based on a more disciplined form of implicit parameter resolution.
Right, Scala and C++'s concepts have the option to have the first-class dictionary. It's powerful, but has the caveat you and Peaker brought up about coherence.
Especially for monads, this style has been around for over 10 years (Tim Sheard - Generic Unification via Two Level Types and Parameterized Modules). It is perhaps not surprising though, that it hasn't caught on. 
I think your second request is more an application than a Haskell library. Have you checked out the features of [MetaConfig](http://metaconfig.com) (disclosure: it's written by fellow students; I'm not sure of its current status). Your third request may already be under development, depending on what features you want. As our CS master thesis project, I and a fellow student did a prototype for a backup system/framework in Haskell. Key features include a clear Haskell API for creating backups (which is what our file system client uses) and a simple backend API. We plan on continuing our work, but we have *just* defended the project, and need to decide on a roadmap.
I had to look at the calendar to verify that today is not April's Fools =).
Well... yes, that's the very principle of the circular inertia that governs this domain: industries use technologies, so schools tend to teach those technologies, so the outgoing students know only those technologies... so the industries stick with them!
I recently saw a solution proposed for the coherence problem for the Rust language, which has type classes with scoped instances: https://mail.mozilla.org/pipermail/rust-dev/2011-December/001036.html (Note that in Rust terminology, class -&gt; iface, instance -&gt; impl, and impls can be named.) Basically as far as I understand it, the idea is that when declaring a type variable for a type, you can also declare some type classes associated with that type variable. Then the type of the thing-using-the-type-variable will include not just the type the variable is instantiated with, but also the instances chosen for the type classes associated with the type variable (which are defaulted to the instances in scope where the type for the variable is supplied, but can be specified manually). So (going with the Map example, and again, if I'm understanding this right) it doesn't involve storing the Ord instance inside the Map, the only change is at the type level, where the chosen instance becomes part of the type of the Map. So two Maps are composable if they use the same instance, otherwise, you get a type error. 
I think the diferrence (from his point of view) is that with objects you have dependency injection, mixins, inheritance, etc. Its all about the programming environment.
&gt;Not sure what you mean here. Share between what? Our snaplet system makes it really easy to manage in-memory state for request processing. No, i'm talking about say, the current number of logged-in users. State at the level of the application, not the request. Just imagine implementing a chatroom in Snap. &gt;I created Heist specifically because of the frustrations I encountered with that site generating HTML from Haskell. .. and .. &gt;Interfacing with a database is a very complex problem and we felt it would distract us from the problems we wanted to work on. So, for the declarative areas of web-development, you guys promote traditional approaches with a template system and manual sql queries. So, what is left, what is actually being done in Haskell itself is a tiny bit of routing/controller logic? I understand that your use-cases are likely different from mine, and that my perspective is a selfish one. But I don't really see a clear advantage in any area compared to other ecosystems .. That tiny bit of controller logic is such a small unimportant part, I don't really see any value, of not having those 10 lines in PHP or hell, C for that matter. And in many cases, all those controllers share the same code, that just exposes a model/view as a restfull resource, with some authentication thrown in. &gt;Maybe a database backed blog is overkill. Well, yes and no. What we want from the database is concurrency, persistence and atomicity. Things, Haskell could in theory provide, without the need for a full DBMS. &gt;The filesystem is a perfectly good data store for a number of applications. No, not really. For starters it doesn't deal with concurrency well (depending on your platform, files will or will not be locked during write). On windows, if you have multiple requests per seconds, I don't think you will even be able to delete the file. It doesn't deal with atomicity very well either: on linux a partially written file will be served up, and a visitor could temporarily see garbage. And let's not forget about the security implication: everyone that has to interact with the data needs commit rights. I wonder how you could even enable comment support with such an approach. I can understand that a single or few author blog, that is already in source control, prefer to use this approach. But it's almost never a good general policy. Not to mention the fact, that most blogs don't use a git commit to send a blog post; they use an administrative section of the website. The person operating the blog, aka the customer, isn't as technically skilled. &gt; I'd much rather use a framework that can easily work with anything than one that provides out-of-the-box support for something that ultimately boxes me in There, we agree. But considering the different domains involved, people have, for good reasons, come to expect default "setups" of frameworks as the shared wisdom of the crowd. And if you want to aim, which i'm not sure you guys want, on the average PHP-based, Django or Rails developer, their first application will be a blog with an administrative section. But, there's nothing wrong, when it's easy to turn off certain modules or replace their use-cases with alternatives. &gt;Snap's goal is to provide a simple and flexible way to serve web pages. This is orthogonal to interfacing with databases, and the power of Haskell allows us to keep it that way. So, your goal is to replace Apache? Because if you want those 'web-pages' you serve to be customized in any way, you're going to need to maintain state at the application level. And in most cases, that state is important enough for persistence and atomicity to matter. Now, again, i'm not arguing the need for a DBMS per se. But to have some vision, some default policy, to optimize the framework on the specific use-case, of maintaining persistent state at the application level. &gt;and we felt it would distract us from the problems we wanted to work on. To play the devils advocate: this is how Haskell's avoid success at all costs. Nobody wants to focus on what, for most projects in this world is the most front and center issue: managing some kind persistent state. From document editors, to websites. The actual algorithms that have to operate on the state are often trivially simple, but to correctly model and maintain the state: that's the challenge. And when you have a framework, that doesn't even standarize on one particial interface to manipulate state, it's hard to imagine the kind of plug-and-play extensions, that you can, for example, find in the Rails world. Act-as-list. Paperclip. Thinking-sphinx. If such things would exists in the Snaplet world, without a common abstraction for state, we would need a separate version of each of those, for every persistent-state-providing 'snaplet'. There may be many faults to find with ActiveRecord, but at least it provides a common ground. Standardization is not some kind of disease. 
Responding to your last point. I don't think there is any reason why a given snaplet can't depend on another, so if someone built a really wonderful data abstraction into one, all sorts of other extensions could build on top of that. More likely (based on how things in haskell seem to work), there will be interfaces that will be implemented, so that you can still pick and choose what combination you want, but they will all interoperate. For example: I implemented the bare minimum to connect the auth snaplet to the authentication scheme I had from a postgres system (a legacy system), and then I was able to use the auth snaplet, as is, only writing a few lines of code. So I could continue to use postgres (and I think it is a good thing to be able to actually use the features of my database), but I could also use the authentication "extension". I think you are attributing way too much to the snap teams decision to implement a blog with a filesystem. There is NOTHING in snap that says you have to do this - none of the snap applications I've build (4 with &gt;2000 lines of haskell web code) have used this paradigm, but for a blog that is written primarily by programmers, and when the whole website is version controlled, this makes total sense. It's the right tool for the job. I'm not sure what most of the web applications you deal with do, but honestly the most complicated part I usually have to reason about is the logic of different requests, permissions, what checking has to be done, dealing with forms, etc. All of that stuff is clearly within the domain that Snap has tackled. The database part is usually just pulling an entry out of the database (by a primary key), pulling a lot of entries out of the database (by some other restriction), putting a new entry in the database, updating an existing entry, and deleting an entry. It's pretty easy to enforce static invariants - for user created data, if the only way to create a datatype is through a form that does validations, then there is no way of getting bad data in the database, because the only input vector is sanitized. You can also enforce a lot of static guarantees with the type system. Once you do that, dealing with the database is usually the easiest part - it's all the rest of the system that is complicated.
I thought I mentioned the overloading issue, even if not by that name. I run into this as a problem all the time because I use type class methods gratuitously. For example, I'll use `fmap` instead of `map` and `return` instead of `singleton` or `flip ($)` in my own programs. I know this is not a consensus coding style, but as I mentioned in another comment, type safety shouldn't be a matter of coding style.
I meant that overloading is a feature, not a bug. If you don't want overloading, then you don't need type classes. But we want to overload common names like `(+)` or `(&gt;&gt;=)` and that's why we need type classes.
This is a lot like some research being done in the ML community on combining type classes with modules. The [basic idea [pdf]](http://www.cs.ox.ac.uk/ralf.hinze/WG2.8/24/slides/derek.pdf) is to infer functor arguments (e.g. which `ORD` your `MkSet` functor was supplied) based on local defaults or abstract over them when no default applies (i.e. infer contexts on types). The usual (applicative) module equivalence rules apply, so `MkSet(OrdIntLt).t /= MkSet(OrdIntGt).t` even if you originally wrote them both as `MkSet.t` with different defaults in scope. 
Yes, I actually considered using this style and I prefer it, but I thought you couldn't bind parameters using operator symbols. You don't even have to use type synonyms to use this trick if you know the order of the type's fields, which you have to know for the tuple version anyway.
I know what overloading is for. The argument I make in my post is that overloading, while convenient, is not safe, and the type class implementation of it also deprives you of very powerful value-level capabilities that are not implementable within the type class system. One of the reasons I like Haskell is that on most issues it takes a principled stand and forces you to solve problems functionally and within the language, such as its State monad to model imperative code. I feel that with type classes the community took the exact opposite approach and decided that overloading was hard, so let's create an out-of-band solution that does not compose well with any of our existing functional tools. This is the exact same mistake that the ML family of languages made with IO by treating it as a an "out-of-band" language feature, making it difficult to compose and manipulate effects. Haskell has now reinvented this class of mistake by applying the same attitude to overloading and as a result we are stuck with type classes that are brittle, non-composable, and not functional.
What tazjin said is correct, the implementation is called Core, but it's not an implementation of System F but System FC (which is a super-set of System F). The Core language was initially based on lambda calculus, but was upgraded to a polymorphic lambda calculus System Fω to be able to decorate it with types. Core was further extended to System FC to support type equality constraints and safe coercions. GHC converts the Core language to another intermediate representation called STG (Spineless Tagless G-machine), before doing code generation into either C or native code. Relevant papers: * [the GHC Core Language](http://www.haskell.org/ghc/docs/7.4.1/core.pdf) * [System FC](http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/tldi22-sulzmann-with-appendix.pdf) * [Spineless Tagless G-machine](http://dl.acm.org/citation.cfm?id=99385)
I wanted a CSS selectors library too, so I wrote one for HXT called [HandsomeSoup](http://egonschiele.github.com/HandsomeSoup/). Is that what you were looking for?
&gt; No, i'm talking about say, the current number of logged-in users. State at the level of the application, not the request. Just imagine implementing a chatroom in Snap. We provide this capability with snaplets. This state is local to each request, but you can easily use any of Haskell's concurrency constructs like MVars, TVars, etc to achieve shared state. Also, a Snap chatroom application [already exists](https://github.com/snapframework/cufp2011) along with a [tutorial presentation](http://gregorycollins.net/posts/2011/10/01/cufp-tutorial-slides). &gt; .. and .. The margin is too small to contain... &gt; So, for the declarative areas of web-development, you guys promote traditional approaches with a template system and manual sql queries. Most people I have talked to seem to think our template system is rather innovative. We don't promote any approach to persistence. I was just talking about what we did for snap-website and why we did it. Nothing more. &gt; Well, yes and no. What we want from the database is concurrency, persistence and atomicity. Things, Haskell could in theory provide, without the need for a full DBMS. I wasn't talking about your needs. I was talking about our needs, and only for the Snap website. If you want a Haskell approach to this, check out acid-state. Oh, and there is also a snaplet for using it with Snap. &gt; No, not really. For starters it doesn't deal with concurrency well... I said "a number" of applications. Not *all* applications or even *most* applications. I'm aware of the shortcomings of the filesystem. &gt; So, your goal is to replace Apache? I said "flexible" meaning "programmable". I think it's pretty clear that Snap and Apache occupy distinctly different places in the design space. &gt; Now, again, i'm not arguing the need for a DBMS per se. But to have some vision, some default policy, to optimize the framework on the specific use-case, of maintaining persistent state at the application level. Our vision and default policy for state is called snaplets, and it addresses much more than just the persistence problem. &gt; To play the devils advocate: this is how Haskell's avoid success at all costs. Nobody wants to focus on what, for most projects in this world is the most front and center issue: managing some kind persistent state. There are definetely people working on this. It just happens to not be me right now. &gt; And when you have a framework, that doesn't even standarize on one particial interface to manipulate state, it's hard to imagine the kind of plug-and-play extensions... Actually, we've already done this with the auth snaplet. The auth snaplet defines the interface that it needs for persisting its data. This allows us to provide auth functionality to anyone who wants to implement the interface for their persistence mechanism of choice. We provide a default implementation that uses flat files for dead-simple experimentation without the need to have a database running. The snaplet-hdbc package provides another back end that works with any database supported by hdbc. If you want to use something else entirely, you can easily do that. I can easily imagine any of the rails plugins you mention being written as snaplets. They would do one of two things. Either they would depend on a certain persistence snaplet like snaplet-hdbc or they would use the pattern we used with auth and define their own persistence interface to be implemented by your back end of choice. The world you describe locks people into the former. Our system transcends the persistence discussion and allows you to use either approach as appropriate. This has gotten a bit long for reddit comments. I'm more than happy to continue the discussion in another venue like email or IRC if you have any more questions. 
Wow, hindsight looks awesome (from the docs on github), congratulations! (and I'll be excited when you release it).
There is, it's on Hackage and it's called cgen. It needs a lot of work though to get it to a truly usable state.
So, one way to handle the Map case would be similar to how `ST` works with `MVector`s. The phantom parameter `s` in `ST s a` becomes part of any `MVector`s it contains - `thaw :: Vector a -&gt; ST s (MVector s a)`. This prevents using the `MVector s a` being mixed with `MVector t a`. So for explicit dictionaries to be universal, the `Ord` dictionary that was being used with `Map` would have to become part of the *type* of the `Map`. This prevents mixing `Map`s with different `Ord` dictionaries without the overhead of storing the dictionary along with the `Map`. But I doubt dictionary kinds will be coming to Haskell any time soon, given that we just got symbol and natural kinds.
I'd like to have a game library like Python's PyGame or Lua's Löve. We have SDL bindings(even though it's not complete, for example it doesn't have binding for SDL_GetKeyState) and some libraries like HaskGame but none of them are complete, and most of them updated about two years ago.
Does anyone know when these features (i.e. the new "static" keyword for serializing top-level code) will be available in GHC?
It was fun and educational last year. I think it will be even better this year since my Haskell-fu is a bit stronger this time.
You can make a record with fields called return and &gt;&gt;= and just write sequence Monad{..} = ... 
This is the best possible way to do Tony Morris' problems.
Oh, good to know. I was going by half-remembered bits of the inliner paper, but it looks like some clever things are being done.
I can see how Constraint Kinds could let you parameterize over an interface, like for a `Set`-compatible `Monoid`: class NewMonoid (c :: Constraint) (m :: *) where mempty :: c =&gt; m mappend :: c =&gt; m -&gt; m -&gt; m instance NewMonoid () [a] where mempty = [] mappend = (++) instance NewMonoid Ord (Set a) where mempty = empty mappend = union But not how it would let you put the implementation of an interface (that is, a dictionary value) in the type.
Or you can just use the tools available to you out of the box: ghci&gt; :m + Data.Typeable ghci&gt; showsTypeRep (typeOf (undefined :: (Int,[Maybe Double]))) "" "(Int,[Maybe Double])" 
If only there were some way to integrate this with haskell-mode in emacs..
Currently the tab key inserts a tab character. This makes editing code with this tool very difficult, as you get weird syntax problems due to code not having the correct indent (even if it appears correct on screen). It would be much better to have tab insert 2 spaces instead.
&gt; Verbosity. Every instance has to be named and passed around. No problem, just use the Reader monad. Oh, wait...
&gt; Instances actually already can be created at runtime, via polymorphic recursion. ...which requires a load of (potentially dangerous) extensions, I guess? &gt; There are semantics to be worked out Yes, that's my principal concern.
Correct. I wanted a short name though, one that would describe the purpose rather than the implementation.
I have updated the editor to insert 4 spaces when you press Tab.
Objects are more than bunded closures though; they take a self argument, and sometimes this also involves "tying the knot" in order to implement inheritance. I have yet to see a compelling use case for these features, though. ADTs (implemented through ∃-types) seem to be enough in practice.
This works: eitherf :: (f a -&gt; b) -&gt; (g a -&gt; b) -&gt; EitherF f g a -&gt; b eitherf f g (InL fa) = f fa eitherf f g (InR ga) = g ga handleL :: (Functor f, Functor g) =&gt; (f (Free g r) -&gt; Free g r) -&gt; (a -&gt; Free g r) -&gt; Free (EitherF f g) a -&gt; Free g r handleL t = handler (eitherf t Free) So the handler for the inner effect can use the outer effect.
Just newtype the keys: newtype IntWithNaturalOrder = IWNO Int newtype IntWithWeirdOrder = IWWO Int 
You should also see Oleg's minification of type class: http://okmij.org/ftp/Haskell/TypeClass.html#Haskell1 "Haskell with only one typeclass" "... a single, pre-defined typeclass with only one method can express all of Haskell98 typeclass programming idioms..."
Ctrl+Right causes the type error panel to (dis)appear? That might be a bit inconvenient went someone wants to use Ctrl+Right to skip to the next word in the source code. Other than that, this is awesome. :)
ioToST. (Which is safe, but you have to run you run your top ST expression with stToIO and no longer runST)
Awesome! Any chance that we can take a look at the source code? This might be of help for Shae's project for this years GSoC, a multi-user browser-based GHCi.
 I just read this paper. The main point of contention is that automatic instance generation like instance Eq a =&gt; Eq [a] where ... requires a method for proof-searching (program-searching), i.e. a sublangage that can do logic programming.
Temko's only example of 'unsafeness' is a programmer writing one of these and meaning the other: main = encodeFile "test.dat" (2, 3) main = encodeFile "test.dat" (2, [3]) But in fact neither typechecks, each gets a list of errors that couldn't be more lucid and correct. The confusion may have arisen because ghci *will* run them; I assume this is because it defaults to reading the literals as naming items in `Integer` The charge of unprincipled-ness is 100% groundless. The idea of a type class is completely coherent theoretically; even if we had no representation of type classes inside the language, they would still exist. Maybe they wouldn't be that interesting theoretically, and for sure their practical utility is limited. But everyone knows this. The objection is an engineering objection masquerading as theoretical.
&gt; ...which requires a load of (potentially dangerous) extensions, I guess? This builds a `Show` dictionary for an arbitrarily nested tuple, in haskell98: poly :: Show a =&gt; Int -&gt; a -&gt; String poly 0 a = show a poly n a = poly (n - 1) (a, a) The instance declaration `(Show a, Show b) =&gt; instance Show (a, b)` corresponds to a function that builds the dictionary, and it gets called to discharge the `Show (a, a)` obligation of of the recursive call. 
Your criterion for safety are too strong. I can write `add x y = x * y + 24`. No language feature can prevent this. I agree that explicit dictionaries are sometimes correct. I just disagree that they *always* are. And I think they are correct only when you want/need more power than typeclasses give you. Two examples off the top of my head: I had a nice type family based representation of encoding layers for a parametric data storage framework. It worked great in Haskell. But it was very tricky to guide the type system to the right instances when I generated these representations on the fly from passing a file. An explicit dictionary representation cleared things right up. Another example: I was trying to code up an algebra of ring homomorphisms and their compositions. I started with the typeclass driven approach, and realized that it handled simple cases but couldn't deal with all the multiple ways the same bits could be glued together. I ended up moving to GADTs and things were much better. In both cases, switching to dictionaries provided more power. It also made the code much uglier in other ways. Typeclasses are great for lots of things, including (especially) certain patterns of recursive decomposition of semantics. But when there's more than one way to glue the dictionaries together, or when you're generating these dictionaries on the fly, you do need to move to more explicit representations. I don't think you can then turn around and argue that this means type classes should be junked. It does mean that it would be nice to explore ways to make them more expressive.
Well, an alternate solution I would be satisfied with would be to keep the value level representations and provide a mechanism for the compiler to infer the wiring when the solution is decidable and unambiguous, and let the human step in when the compiler fails.
Agda here comes close to having your cake and eating it too. All your classes are record types, and your instances are values (giving all the same advantages as above), but instances can be automatically searched for via instance arguments (or provided if the instance to choose is ambiguous). You also have a lot of nice sugar to open a particular instance in a particular scope so you lose all of the verbosity problems of doing this in Haskell.
 foo : Show a =&gt; Integer -&gt; a -&gt; String foo n x | n &lt; 0 = foo (abs n) x | n == 0 = show x | otherwise = foo (n-1) (x,x) Read in `n` from the command line, and the `Show` instance for a complete tree of pairs `n` levels deep must be constructed at runtime.
Syb or any other Generics library?
All awesome suggestions! I would personally love to see #3 and #4 too. And I think #2 and #4 would make Haskell more attractive as a development platform.
But I think that's worthwhile. While we all know the weaknesses of objects, people dedicated to functional programming talk about them with excessive zeal and people dedicated to OO programming don't discuss them in polite company. Having someone from the OO world step up and say, explicitly, "this is what we promised, and this is why we all know those promises were unfulfilled" is an important step in getting everyone to talk about the way the world actually is today. Neither hiding one's dirty secrets nor airing other's dirty laundry is productive or helpful. 
Actually I have had the luxury of first-class type classes, just not in Haskell. I agree it would be nice to have in Haskell, but there's a big difference between "first-class type classes" and passing records. I've also worked in languages where record passing was the answer to lacking type classes; those languages aren't any fun to use, and the overhead of making all the boilerplate explicit far outweighs the issues of second-classness that I've run into in Haskell. Moreover, the silent import issue is not a "small" issue. It's an issue that causes great consternation in the Haskell community (just google for "orphan instances") and led to the rejection of type classes in BitC (just google for "BitC retrospective"). Indeed, this is a *fundamental* issue which must necessarily be resolved in order to make type classes first-class.
There is no dependent typing going on here. Can we as a community please try to use proper terminology when the jargon has clear, well-defined, and widely accepted meanings?
If you want to go up a level, then to satisfy the name "dependently typed", you would have to have kinds that depend on types. Saying that GADTs let types depend on constructors is not quite accurate: though the constructors do have different types, the types themselves do not depend on the constructors in the usual sense of the word "depend"; that is, there is no function whose argument is a constructor and which returns a type. You're right that GHC is adding more and more type features that step in the direction of full-spectrum dependency, though!
The author probably intended the reader to work with Hugs, not GHCi. You may wish to send a bug report to the author, and to suggest adding support for GHCi.
Oops, hit save by mistake and I can't edit it on my phone. I will continue it in a third reply
I've wondered for a while how possible it would be to have an Ada - Haskell FFI
This is a great demonstration of the power of GADTs with an example I think a lot of people can understand. Jeremy: would you be interested in contributing this to MezzoHaskell?
I posted the link here and mostly kept the author title (haskell was replaced by GHC because of the specific extensions involved). I found his experiment worth sharing, but I am not qualified to know if it is dependent typing or not. Your position seems to be shared by others, so I apologize for having propagated a misleading terminology. I don't see an obvious way to edit the title, but if a moderator can do it, please go ahead.
I'm interested in this too. We could give it a go? 
That would make this make more sense: import Data.IntMap (IntMap) import qualified Data.IntMap.Lazy as Lazy import qualified Data.IntMap.Strict as Strict vs. choosing to import IntMap from Lazy/Strict, even though it may be used with Strict/Lazy.
Dear pedants: the blog entry you're all whining about is by Wolfgang Jeltsch. Aka, this dude: https://wolfgang.jeltsch.info/. He's written this software: https://wolfgang.jeltsch.info/software/haskell He knows what dependent types are. He probably knows more than you. His use of dependent typing is slightly jargony in that yes, you don't have types arbitrarily depending on values. However, it is not outlandish, in the sense that intermixing value and type-level computation has been known as simulated dependent types, or fake dependent types, or lightweight dependent types for a long time. One classic paper on abuse of Haskell's type system, now over ten years old, for example, is Conor McBride's "Faking It: Simulating Dependent Types in Haskell" (http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.2636) I suppose that type-level program has become so normalized that people don't consider it related to dependently typed langages at all. But let's not be so rigid, please! *Edit*: also, with appropriate CPS transforms, you *can* make values depend on types, within a restricted context.' *Edit 2*: It also occurs to me that the title varies in meaning depending on whether you believe that "and" or "with" binds more tightly.
See I'm not in the position where I actually understand what dependent typed programming is, but I know (or have read plenty of times) that Haskell can't do it. If the title had said fake/simulated/lightweight I would understand that it's a best-effort attempt without the full support of the language. But the title just confuses me; and the content just appears to describe what many others refer to as type-level programming. Maybe what I really need is a good intro to what is and isn't dependent types.
Can std::map efficiently compose two maps? If so, how can they make sure both maps use the same comparison function?
Idris now supports named instances, which seem like an interesting point in the design space: https://raw.github.com/edwinb/Idris-dev/master/samples/named_instance.lidr
This may be a good moment to express my old gripe: Namely, I wanted, several times in my life, to use Int64 indexed IntMap/IntSet-s on a 32 bit system. But there are no Int64 or Int32 versions... Once I even made a local copy and just search/replaced Int-s to Int64-s, but that's not ideal. Another small comment is that it is quite hard to find these used-to-be-base-but-not-anymore (~~containers~~, mtl, transformers, OpenGL, etc) libraries on Hackage (for quick docs), since ctrl-f will find a lot of related packages first. It would be nice to have a separate category for these (say, there could be a category for the packages included in the Haskell platform). And/or, even better, compiled docs for the Haskell platform, similarly as GHC has them for the base (or does this already exist?)
Cool! Unfortunately, though, Haskell's logo has always reminded me of an [airline logo](https://en.wikipedia.org/wiki/File:Alitalia_logo.png)... so I probably won't be stealing your wallpaper. It's good work though. :-)
I always think of the old [Amtrak](https://en.wikipedia.org/wiki/File:AmtrakLogo.png) logo. 
+1 More importantly containers and unordered-containers should provide the same API.
haskell's logo is just the bind operation of the monad (&gt;&gt;=) with the second &gt; replaced by a lambda.
Yes and McBride states very clearly that you're faking, simulating dependent types. Needless to say the argument from authority is silly. The definition of "dependent type" is very simple and precise, and I just pointed out that the title is misleading, which I think is unquestionably true - proper dependent types in Haskell would be big news.
People who know stuff can be wrong, too. Appealing to authority is fallacy. I do however agree that while the title is slightly misleading, as there are no dependent types in Haskell, the post itself makes quite clear that it is showing how a lot of the things dependently typed languages can do, can be done in Haskell as well, using new type level features.
Unfortunately, that is one mistake the type-checker can't catch for me yet..
I've always liked [this one](http://imageshack.us/f/156/swahili.png/)
Upload a new version and mark as **unmaintained**?
Since I maintain unordered-containers and co-maintain containers that's definitely possible. I think we're definitely closer to a point where that would make sense. Up to now I intentionally kept them separate as unordered-containers needed to move at a faster pace than containers; I recently completely rewrote the unordered-containers implementation and if the API would have already been the size of e.g. `Data.Map` at that point it would have been a much harder task. Edit: Wow! From a grammatical perspective, this must be the worst comment I've ever written.
It's a nice looking wallpaper. I was looking for a haskell wallpaper a while ago and didn't find anything I liked so I made [this one](http://i.imgur.com/2guFt.jpg).
That's been my wallpaper on all my computers for years. It plays very nicely on the eye with my transparent terminals. 
What's wrong with airline logos?
Awesome! What is the app showing CPU % and Mem usage?
The main program is [conky](http://conky.sourceforge.net/), and the rings are drawn by a lua script that the conky file calls. It's fairly simple, and it's for linux, so if you use linux then I'd be happy to give you my files and/or a quick guide to using it if you want. If you're using windows, then maybe I can suggest rainmeter? I don't know anything about it, but I've seen some cool things done with it. That's my disclaimer so that if you use it, and it breaks, it's not my fault ;-)
https://github.com/kallisti-dev/cond/blob/master/src/Control/Cond.hs is fairly readable
The mixfix ternary conditional expression might be another one to add. a &gt; b ? c ! d I've seen that in various forms since around 2002 or so (I think due to dave snowdon)
Wow, I didn't know it was so easy to get an .au link to blogspot. But traceroute indicates that it gets the same IP and goes to the same server as the US version: http://parfunk.blogspot.com/2012/05/how-to-write-hybrid-cpugpu-programs.html Which makes sense of course...
*if'* is overdue but I don't know that I'd use the other functions...
They're just a type family combined with a type class. You can fairly mechanically translate it to a separate type family.
That's ridiculous, again the term [dependent type](http://en.wikipedia.org/wiki/Dependent_type) has a very simple and well established meaning, and I don't see why you would want to "violently" change its definition to something that doesn't carry the core idea any more (the fact that you depend on **values**). If you take that part out of "dependent types" the term doesn't mean much any more. And more in general I don't understand why is it desirable for precise mathematical term to "evolve". Can you give me an example of a mathematical definition term whose meaning was "violently" changed at some point?
This and Reactive Extensions from the .net world. Are these examples of ideas from FRP gaining a mainstream foothold or am I over estimating their similarity?
Ah, same here really. Shame.
&gt; (2) type-directed inference of function parameters Yes, THIS is the whole point of typeclasses.
Haskell is primarily developed by Microsoft, and SPJ has pointed out a lot how Haskell is a testing ground for many things that then migrate into more mainstream .NET related languages.
I think you're right. The mainstream spin-offs are called [Reactive Programming][1]. [1]: https://en.wikipedia.org/wiki/Reactive_programming The main difference between FRP and reactive programming is that the former should have a well-defined semantics by restricting the use of `IO`. This usually shows up when you merge two event streams. In most mainstream approaches to reactive programming, the order in which you merge simultaneously occurring events is non-deterministic. In FRP, `union` orders the events deterministically.
If I'm not mistaken none of the lifted function need Monad, Applicative would be enough. And `select` is just a specialized version of `ifM`. You can take look at [bool-extras](http://hackage.haskell.org/package/bool-extras) for some other conditional operators.
You can use `otherwise`, as in the example.
Right! :)
&gt; Haskell is primarily developed by Microsoft And yet, Windows is a second class citizen for Haskell
I use a different ifM: ifM :: Monad m =&gt; m Bool -&gt; (m a, m a) -&gt; m a In the not uncommon case that I have do blocks to run, I feel this lays out nicer than the untupled version: ifM foo ( do ... , do ... ) Compare with: ifM foo ( do ... ) ( do ... ) I am happy to see &lt;&amp;&amp;&gt; and &lt;||&gt; , they can be quite handy!
Agree, I dislike cond being partial.
&gt; will continue to exist for the foreseeable future I meant past this foreseeable future, instead of removing it completely.
My Haskell side says: Dependency injection can be done with parameter passing or the reader monad, and mixins are convenient but (like multiple inheritance in C++) can be abused for implementation inheritance. You can get around the latter by coding to interfaces, but that's still a bit of a pain in OO languages (it's not quite as convenient as it is in Haskell) My Scala side says: Inheritance and the implicit "this" of Java and Scala are extremely convenient, especially for interoperating with existing OO code and existing OO programmers. Innovation is still going on in the dependency injection space so I think it's too early to declare it a solved problem.
Would the scope of this library include `while`: while :: Monad m =&gt; m Bool -&gt; m a -&gt; m () while p m = do p' &lt;- p when p' (m &gt;&gt; while p m) I know it's not idiomatic Haskell, but beginners ask about it all the time.
I agree that specifying an "else" as a separate argument is a nice approach, but I think cond's primary purpose is to act as an "anonymous guard". Since there's no requirement that guards have a completely safe else condition, I didn't see a reason for cond to have one either. The otherwise form matches nicely with the syntax of guards. Of course, there's also condPlus, which is intended as the well-defined alternative to cond. To create a conditional with a well-defined else case, you could write fromMaybe 2 . condPlus $ [...]
When I was first toying with the code I started working on ternary operators, but I couldn't find a syntax and representation that seemed to fit all use cases. On one hand, you want the ternary operators to look nice syntactically, since they exist as an alternative to the if-then-else form. However, you also want them to have a representation that allows you to leverage partial application. At first I had something like: data ThenElse a = a :| a (?) :: Bool -&gt; ThenElse a -&gt; a This would be nice in some situations, you could, for example, write code like this: [(cond1 ?), (cond2 ?), (cond3 ?)] &lt;*&gt; [a1 :| b1, a2 :| b2, ...] In essence the ThenElse structure represents an unresolved condition, with the ? operator providing the condition that extracts the correct value. But how often does this kind of use case arise? Would it be more likely that you want to reify the if-then section of the conditional? Such as: data IfThen a = Bool :? a (!) :: IfThen a -&gt; a -&gt; a At this point, I toyed around with the idea of having 2 forms of ternary conditional, which, as you can imagine, turned into quite a mess. But perhaps I'm overthinking this; maybe ternary operators should just be a nice syntactic trick, with intense curry magic being delegated to if', (??), bool, and the other combinators. I'm interested in ideas and thoughts on this.
That's certainly within scope; I simply hadn't considered adding it. The nice consequence of this package is that all of these various useful operators with no standard home can be put somewhere. I had hoped that once I uploaded this to hackage others would step forward and say "it would be nice if X were in some library somewhere." You could probably take a similar approach with other categories of functions. Pointfree combinators? That's scary to think about...
Nah they're my files, but I've had them on my pastebin for a while. [conky_start](http://pastebin.com/gpQKnGde) is in my home directory and is set to run at startup. That's only necessary if you have more than one conky file that you want to run; otherwise you can just run your one conky file. [conky_time](http://pastebin.com/LZ8LgKHF) is what handles the date and time in the center of the screen, but the one you expressed interest would be [conky_rings](http://pastebin.com/VSsSGNxq), which calls [this](http://pastebin.com/SGGdzFiy) lua script. The script isn't mine -- credit goes to [londonali1010](http://londonali1010.deviantart.com/art/quot-Rings-quot-Meters-for-Conky-141961783) -- but the only part that should concern you is in the conky_widgets function where you make the function calls with whatever parameters you want to draw your rings. So if you just want the rings, then all you have to do is have the conky_rings file set to run at startup, something like "conky /path/conkyrc_rings". And the desktop is linux mint 12, so it's gnome 3, which I believe uses mutter for the file manager. I don't know much about it though lol. There are about a million different conky setups, and some of them are really, really gorgeous. I encourage you to google it to get some ideas :-)
Oh haha nah I don't know much about OS X, though it does look shiny. I posted my files above, and you're free to cannibalize whatever you want. If anyone has any questions feel free to post them, and I'll do my best to answer them. I'm no expert, but I remember some of these things took me quite a bit of trial and error to get right ;-)
Less confusion which library to use. It's good to have a standard.
Thanks for all the suggestions! I've now added some new conditional operators for Categories and Monoids, using a nice sugary syntax. In addition there's now while and do-while constructs. The latest changes haven't been released on Hackage yet, but are available at https://github.com/kallisti-dev/cond
Well, sure. Reactive programming looks like reactive programming.
Oh, indeed. I guess there's no need to duplicate effort then.
What kind of features are you looking for in a backup system. In particular, how do you integrate both filesystem backups with database backups? I'm curious because this may be the kind of project I could work on, and I've been thinking about a nice and simple rotating log library that might share some code.
This seems apropos: http://zenzike.com/posts/2011-08-01-the-conditional-choice-operator 
&gt; Namely, I wanted, several times in my life, to use Int64 indexed IntMap/IntSet-s on a 32 bit system. Currently you can use bytestring-trie, with the obvious (i.e., immediate) representation of IntX as a ByteString. Unfortunately, at present, I haven't had a chance to keep bytestring-trie up to date with some of the more recent performance enhancements in containers and unordered-containers. This is something I hope to rectify over the summer. In addition to the low-level bit bashing optimizations, one thing I'd like to do is make the datatype more HAMT-like (or rather, AMT-like :) in order to reduce the overhead of large trees and to remove issues about choosing the size of the key/branch (presently, bytestring-trie branches on Word8, which is appropriate for strings of bytes but can be slower than using the native Word size). Strictly speaking that won't give you an Int64Map because it would allow both smaller and larger keys than just Int64, but the performance should be comparable.
Er, Haskell is being developed by a lot of people and most of them aren't involved with MSR (e.g., Utrecht, Penn,...). It's only GHC which is largely being developed by folks at MSR. 
You could also use a `HashMap`.
In my mind the foreseeable future means forever, or at least until no one cares anymore.
Fair point, but GHC is the most popular compiler, and MSR is probably providing the most funding to the development of Haskell. Perhaps I should've said something other than primarily, tho -- that implies majority, when what I mean is that MSR is probably the largest of the groups working on Haskell.
I highly recommend this package. Your mileage may vary of course, but we measured a 20-30% increase in performance (reduction in latency and increase in throughput) across the board compared to HDBC, and are now using postgresql-simple in production. 
I am curious if you'll see more performance improvements from the new FromRow implementation, courtesy of fewer intermediate data structures. I believe that you should see at least a small improvement, but since my current applications are not performance sensitive in that way, I didn't take the time to write a benchmark.
It's a bit frustrating. What used to be a single URL nowadays seems to depend on the country of whoever is currently looking at the page. If I visit a foo.blogspot.com article, I instead get foo.blogspot.co.uk. Same article, just wobbly identification Edit: [some more info](http://support.google.com/blogger/bin/answer.py?hl=en&amp;answer=2402711) on why this happens. Messy universe, where nice clean techie things have to be messed up to keep up with complicated bits of the universe like the existence of different laws in different countries 
I would guess that this non determinism and possibility of side effects makes it more difficult to do things the FPR provides, like integrate over an event stream. Does the lack of well-defined semantics have any other implications that would make FRP more powerful in a functional setting than an imperative one? 
A: take Lock A, then B, release B, then A. B: take lock B, then A, release A, then B A doesn't deadlock. B doesn't deadlock. A | B deadlocks (for example, if running in lock-step). I guess it depends on what your composition operator is.
A port of pygobject, but probably more compile-time than runtime. This would provide support for GTK+ 3.0 but also *any* GObject based library, including any written in Vala.
Well, having a semantics means that you can reason about your code and be certain of its correctness. For instance, graphical user interfaces tend to be very finicky, you always need to smooth out edge cases that confuse the user. This is a lot of work for the programmer and this is never going to become easy unless the programmer can get a precise understanding of what happens. I once heard about a bug in Microsoft Word concerning paragraph formatting that has exited for ages. In fact, nobody can fix it, because the source code has been lost and only exists in binary form. At a certain amount of complexity, you just don't know what's going to happen anymore and the only way to push that further back is well-defined semantics.
I wrote mysql-simple (which inspired postgresql-simple) speculatively, in the hope that I'd see some kind of improvement, and the result was even more dramatic - something like a 50% speedup. Not to mention the cleaner APIs. HDBC was good for its time, and remains the only game on town for people who care about API portability, but we've since come a long way as a community in our ability to write fast, clean code.
Yes, that's what I mean. Though I'm not sure whether in the concrete cases I wanted an ordered map or just a map. edit: I think in at least one case I definitely wanted an ordered map. 
&gt; I wrote mysql-simple (which inspired postgresql-simple) More than inspired at this point, there still is an awful lot of mysql-simple code and documentation in postgresql-simple. You are named as one of the authors of the package. &gt; HDBC was good for its time, and remains the only game on town for people who care about API portability I think the Haskell community will need to revisit HDBC at some point, and give its interface and implementation a massive overhaul and perhaps release it as HDBC-3 or something. But part of the problem is figuring out what such an interface should look like in the first place... so I hope the *-simple efforts eventually offer some real insight into that. I would think that the database-devel mailing list would be a good place to hash some of that out.
so jelly... you have a course with Haskell... You realize 6 days are 144 hours right? I feel this take 10 hours tops.
I feel completely overwhelmed by the project. Just 7 weeks ago we were learning about lists and how to understand Haskell. I feel like this course is going way too fast.
I'll do that, thanks. It just sucks because the first homework was only worth 50 points (and was a perfect fit for how far along we were in the course) , and this one is worth 140 points. I understand that the class should be about what I learn from it and what I take away from it, but I just hate knowing how this is going to effect my grade if I don't get it done correctly, and that just adds on to the stress.
I agree this looks fairly involved considering what you're telling us about the level of this class. On the other hand, after a cursory read, it seems to me the assignment is very, very nicely detailed, so I don't think you will ever wonder what exactly you're supposed to do. Good luck, I think you'll learn a lot from it, even if you don't complete it. 
Thanks.
1. You are incredibly lucky to be at a school where a class like this exists. 2. If the assignment was really created on May 2, the prof might not know how hard it is for noobs. Get to work, do your best, and talk the prof or TAs after you make an honest first pass at it. If the homework really is too much for the class, the prof might change the deadline or make part of it extra credit or (I always hated this on graded work) give out huge hints and guidance to people who show up for office hours.
I know I am. I took programming language concepts and we did some stuff with Haskell. When I saw that there was a specialized class for it this quarter I had to take it. I talked with someone else in my class and he said that the last specialty class we had was on Go, Google's programming language. I was kind of upset I missed that one. This is what I always do. I plan on seeing my professor for office hours twice during the week after I try to get some code written. We'll see what happens after that.
Just go for it. Fluet is not going to punish you for doing your best.
&gt; You will use the **Text**.QuickCheck module to write specifications and to test and debug the library
Fluet does awesome work! I've enjoyed a number of his papers.
The optimal strategy is actually to learn as much as possible as early as possible and to not worry about grades. Knowledge has exponential rewards: the more you know, the faster you learn new stuff. Then, if you know everything, a perfect grade is easy. Concerning the homework, it looks like a lot, but it's really not that involved. The first three tasks are an exercise in rote typing. Only the last task is actually difficult, but you just have to spend some time mulling over it. 
Muphry's Law as applied to programming languages? "Any reference to a test framework will contain an error."
What is rote typing?
This is pretty common for RIT's Programming Skills/Haskell course. I took it a few years ago with Schreiner, and he ended up curving the final forty two points (if I remember correctly). Make a best effort, working through these on your own will make you feel so intelligent. When you get stuck, here are my tips. * These are all set operations but on a funky representation, right? So take a look at library implementations on set operations for a more traditional data structure. * Part of your grade is based on the efficiency of your implementation. *Part*. * There are a lot of small pieces here, most of which are trivial. Don't be overwhelmed with the number of problems, as their solutions are mechanical.
Ah ok. Thanks.
Please ask your teacher / sys admin how many downloads this assignment has compared to another assignment.
I don't mean to sound like an asshole, but I think you're not getting my initial comment. Either that or you are responding to all of the comments directly from your inbox and not reading the context. All I am asking is that you try to find out how many random redditors downloaded this homework assignment.
&gt; However, I don't completely agree with the approach used in the package, which creates an instance for (Monad m) =&gt; m a. Yeah, I'm not sure what the best story is there. The problem, of course, is that we often have `m Bool` and we want to do conditional dispatch on the result, but doing so requires introducing an intermediate name which isn't used elsewhere, thus polluting the local namespace. In [Habit](http://lambda-the-ultimate.org/node/4205) they have syntactic sugar of the form: if&lt;- mBool then whenTrue else whenFalse and similarly for `case&lt;-`. I like those because they are simultaneously explicit about the fact that monadic actions are going on, while removing the need for an intermediate name or other boilerplate. Perhaps I should spend some time this summer seeing if I can hack this into a language extension for GHC (from which it could be proposed for haskell'). Though some would argue that it'd be better to have `if_then_else_` be rebindable syntax just like do-notation, numeric literals, and (with extensions) string literals. That way it'd be easier to support EDSLs with booleans and conditionals.
The history of science is rife with these things. One minor, but fascinating example is "momentum": http://en.wikipedia.org/wiki/Momentum#History_of_the_concept The linked wikipedia entry doesn't do justice to how much things transformed and shifted, but it does give a flavor and I don't feel like digging up something deeper. One of the canonical examples is Kuhn's discussion of a "paradigm shift" with regards to the concept of motion. There are lots of other, less known, but equally if not more interesting examples all over the place. The history of science is endlessly fascinating. Another great example is the contention over not the process of speciation but the definition itself of the term "species". From analysis, "infinitesimal" is another term with a storied history. In algebraic geometry, of which I know very little, see the confusion over the term correspondence: http://en.wikipedia.org/wiki/Correspondence_(mathematics)
&gt; I think this could be a useful typeclass to have lying around. Me too, though I'm not sure of the value in separating the Boolean class and the BoolValue class. If you put those together, then you get a Boolean algebra. From which it becomes obvious that we should offer bounded lattices as a superclass, lattices as a further superclass,... A better approach to the type classes IMO would be to follow the approach of [lub](http://hackage.haskell.org/package/lub), but abstracting the lattice work out from the [unamb](http://hackage.haskell.org/package/unamb) stuff. class JoinSemilattice a where lub :: a -&gt; a -&gt; a lubs1 :: a -&gt; [a] -&gt; a (||) :: JoinSemilattice a =&gt; a -&gt; a -&gt; a (||) = lub -- Alas, the Prelude doesn't distinguish upper/lower boundedness class JoinSemilattice a =&gt; BoundedJoinSemilattice a where bottom :: a lubs :: [a] -&gt; a false :: BoundedJoinSemilattice a =&gt; a false = bottom or :: BoundedJoinSemilattice a =&gt; [a] -&gt; a or = lubs class MeetSemilattice a where glb :: a -&gt; a -&gt; a glbs1 :: a -&gt; [a] -&gt; a (&amp;&amp;) :: MeetSemilattice a =&gt; a -&gt; a -&gt; a (&amp;&amp;) = glb -- Alas, the Prelude doesn't distinguish upper/lower boundedness class MeetSemilattice a =&gt; BoundedMeetSemilattice a where top :: a glbs :: [a] -&gt; a true :: BoundedMeetSemilattice a =&gt; a true = top and :: BoundedMeetSemilattice a =&gt; [a] -&gt; a and = glbs class (JoinSemilattice a, MeetSemilattice a) =&gt; Lattice a class (BoundedJoinSemilattice a, BoundedMeetSemilattice a) =&gt; BoundedLattice a instance BoundedLattice a =&gt; Bounded a where minBound = bottom maxBound = top class BoundedLattice a =&gt; Boolean a where not :: a -&gt; a In general, it'd be worth distinguishing the finer points about (pseudo)complementation, De Morgan's laws, LEM, noncontradiction, etc., but alas, since we can't attach proofs of laws to type classes I think that wouldn't really buy us a whole lot. Of course, we should also have a PartialOrd/Poset class instead of just having Ord.
It's also true that postgresql-simple could be made a lot faster, at least for some applications, by better supporting prepared statements, protocol-level parameters, binary parameters and results, etc. And it may be worthwhile to move towards using libpq asynchronously, using GHC's runtime to achieve concurrent IO rather than using the kernel's threads. But doing this is not a priority for me at the moment; postgresql-simple is plenty fast enough for my current projects. I am curious to learn about the ODBC improvements in more detail though. 
Those are very interesting suggestions. I feel system administration could really benefit from more programs in a strongly typed language like Haskell as the languages traditionally used there are all more towards the dynamic end of the spectrum and not terribly safe for fully unsupervised automated use (I mean in contexts where dynamic runtime warnings are easily lost unless wrapper scripts are written very defensively).
&gt;high-performance compiled language This isn't really true. It's no longer a remit of the language to provide low level access. Even google have taken to calling it a general purpose programming language rather than a system language successor. Its value lies in it's attitude towards concurrency.
If you get stuck on any of the function definitions, remember that you can stub things out with `functionName = undefined`, where the `undefined` keeps the typechecker happy. That way you can shift focus for a bit and come back to it later, without worrying that one troublesome compiler error will lose you credit on the whole thing.
I'm one of the students he's supervising for graduate work, and I've had two classes with him (three if you count my independent study). Don't hesitate to go to *him* and ask him for help. He loves explaining and seeing people understand.
Actually, I think there's only two ABIs you would have to pay attention to: - the Itanium C++ ABI, which has been adopted (not just on Itanium) by GCC and most other compilers on various Unixes; - whatever Microsoft's compilers use on Windows.
Well, there's the `fgl` package, however I think that an elegant functional graph solution is still an open problem. The `fgl` is nice in that it has an inductive approach to graphs which meshes very nicely with functional algorithms, but it feels like it's not a final solution.
So use mutable data types - immutability is the default, but it's not illegal.
Yes, there are a lot of books for imperative algorithms good in competitions. But none for functional ones, and as you said mutation may be hard in haskell... The closest book is the fantastic Chris Osaki "Purely Functional Data Structures", quoting a part of the book: "Time after time, I found myself wanting to use destructive updates, which are discouraged in Standard ML and forbidden in many other functional languages. I sought advice in the existing literature, but found only a handful of papers. Gradually, I realized that this was unexplored territory, and began to search for new ways of doing things."
What the typewriter did.
The public wants to know! We redditbombed his homework.
Start early and do your best. As some of the people have commented, most of the functions are not hard to implement, just take the time to think through things. If a majority in class thinks this is hard, can you ask the prof for an extension ? Good luck!
Interesting. I did not know about these systems, thanks for sharing. The idea of weights on arguments seems wrong to me. Shouldn't there be a concept of conditional weights like conditional probability so weights depend on the set of inputs presented? 
can't get simplest example to work...for xz &lt;- query conn "select 2+2" i get pgs.hs:20:9: Couldn't match expected type `IO t0' with actual type `q0 -&gt; IO [r0]' In the return type of a call of `query' Probable cause: `query' is applied to too few arguments In a stmt of a 'do' block: xz &lt;- query conn "select 2 + 2" In the expression: do { conn &lt;- connect myConnectInfo; xz &lt;- query conn "select 2 + 2"; return () } 
That might be true in terms of GB days stored but certainly not in terms of number of programs or programmers.
Yes, that works, though it might a little more idiomatic to remove the FromRow instance for Int and do this instead: main = do conn &lt;- connect myConnectInfo xs &lt;- query conn "select 2 + 2 as r" () forM_ xs $ \(Only r) -&gt; putStrLn $ show (r :: Int) return () Also, you can use `query_`, which doesn't use parameter substitution. Also, in this example the column name is entirely ignored, so you don't really need the `as r`. I updated [the changelog](https://github.com/lpsmith/postgresql-simple/blob/master/CHANGES) for 0.1.1, it just adds bit of documentation, a minor enhancement, and a small bugfix.
&gt; I am curious to learn about the ODBC improvements in more detail though. You can find all the details if you diff HDBC-odbc version 2.2.3.2 with version 2.3.1.0. The main difference was in how query results are collected. Instead of calling a ODBC API function to get the data for each field of each row as a bytestring (and then doing the deserialisation on the Haskell side), we use some ODBC API functions to bind buffers to columns in the query result. Then we just need to call an ODBC API function to fetch all the data for a row, putting it into the pre-defined buffers. This firstly means far fewer ODBC API calls and secondly that ODBC is doing more of the deserialisation work for simple types (int, float etc). For queries with several columns in the result set this makes a big difference. A similar approach would almost certainly be possible for postgresql. At the moment I would not be at all surprised if it's faster to use HDBC-odbc for postgres rather than its direct HDBC-postgres backend.
`Text.QuickCheck`??
I had a different, O( N^4 ), algorithm. My `dynprog`'s array keys had N^2 keys for positions in the two queues. You and sakai seem to have had more keys in your maps, for positions in the queues with addition to how much items were used in this position, which I'm not sure how much options there are for so it's hard for me to tell what the complexity is.. But anyhow, doesn't matter now :) Congrats for advancing to round 2!
As one of the authors of bool-extras, I can say it's not so much theory-focussed as it is humor-focussed ;) That said, a fold/destructor for bools (bool :: a -&gt; a -&gt; Bool -&gt; a) is very useful to use partially applied.
Gahh, I only just failed to get into Round 2... Thanks for the tip on parallelization. I was trying to do that yesterday, but I didn't realise at the time I needed rdeepseq instead of rseq. Makes complete sense, of course.
I'm not sure what is funnier, the fact that you actually think only startups are bright enough to use a free database or the fact that you think db2 is used anywhere close to as much as the "big" two.
It seems the surface has been scratched, but this glass is deceptively thick. It might even have infinite thickness. Good luck and have a lot of fun while learning! And don't forget to come join us in #haskell on freenode to ask questions.
I've been in to see him more than I have for any other class, he's a great teacher.
s/map/parMap/g
&gt; A similar approach would almost certainly be possible for postgresql. At the moment I would not be at all surprised if it's faster to use HDBC-odbc for postgres rather than its direct HDBC-postgres backend. Well, I don't know too much about odbc at this point, but it doesn't sound like libpq offers comparable functionality. It might be interesting to compare the performance of odbc versus libpq, though I'm not sure exactly how to do this "fairly". Also, at this point postgresql-simple doesn't make use of libpq any more intelligently than HDBC-postgresql; I suspect that most of the speedup is simply being more intelligent on the Haskell side of things. So any such improvements to HDBC-postgresql should easily translate to postgresql-simple too.
That is a great idea. One possible option would be to extend [redcarpet](https://github.com/tanoku/redcarpet) and then use [console_renderer](https://github.com/egonSchiele/console_renderer) to output to terminals.
If that is all the `Only` type does, you should consider using the existing `One` type to avoid writing another one: http://hackage.haskell.org/packages/archive/OneTuple/0.2.1/doc/html/Data-Tuple-OneTuple.html 
You could enforce that `typeOf` doesn't evaluate its argument by changing its signature to typeOf :: Tag a -&gt; Type using data Tag a = Tag or using the existing `[Data.Proxy](http://hackage.haskell.org/packages/archive/tagged/0.4.2/doc/html/Data-Proxy.html)` which is the same thing except it has a rather unintuitive name :)
I think ifM is a decent approach to this, even if it doesn't look as nice as a special if&lt;- construct. It removes the explicit temporary variable created by binding the m Bool, which is the main issue I had when writing conditionals in monadic code. I'm honestly not sure why it isn't part of Control.Monad
&gt; It works great, but there’s only one problem: it doesn’t nest... &gt; I assume this is a performance vs. power tradeoff It's not a problem. It's a design decision in Unix itself, not just in inotify. It gives you performance **and** power. Power comes from the ability to build complex things out of simple things. You don't get notified about changes to subdirectories because those are not the directories you are watching. The directory you are watching didn't change. In Unix, a directory is a table of pointers. The pointers can point to files, other directories, and various other kinds of things. But the directory itself is just a table of pointers. Simple operations on directories act on that table, not on anything that its pointers point to. Operations that act recursively on subdirectories are complex. They can involve not only performance issues, but also infinite recursion, remote network operations, and a host of other things. Sometimes it's convenient to have such operations, but it wouldn't make sense for the simplest building block tool, like inotify, to do that by default. 
&gt; The pointers can point to files, other directories, and various other kinds of things.... Operations that act recursively on subdirectories are complex. They can involve not only performance issues, but also infinite recursion, ... Just to make it concrete, as root on a Linux system try this: mkdir tmp; mount -o bind / tmp You now have a subdirectory tmp, under which is the root directory. (There are other ways to do this... numerous other ways. I know. This one makes the fewest assumptions about where you are in the filesystem at the time you execute that, I think.) Modern filesystems are in a very uncomfortable place where almost all the primitives for manipulating them assume they are trees, where users assume they are trees, where programs and programmers assume they are trees... but they are in fact directed graphs with no other restrictions (in particular they are not acyclic). It's the worst of both worlds.
are you a rwth student? i am doing the same module :]
I don't think they are more at odds than OO and relational databases. In fact SQL is a declarative language, making it closer to a purely functional language than to an OO language. It looks more like a lack of library/framework maturity in some FP communities with regards to database interaction.
I'd say more, and say that the relational algebra is a functional language, and thus the pure querying parts of SQL are clearly functional too. Take the most common clauses of a SQL query: * `SELECT` is `map` * `FROM` is Cartesian product, as in, e.g., `sequence` for the list monad * `WHERE` is `filter` There's also the fact that while the "flavor" of OOP all too often involves dictating how problem domain data is to be represented, functional programming's "flavor" is usually about how to compute, not how to represent data. Functional programming can cope pretty well with the idea that data is to be represented in terms of tuples, relations and declarative constraints on them. You tell this to an OOP weenie, and you're basically telling them to abandon their paradigm.
Object oriented design creates an impedance mismatch with everything else. Functional programming doesn't force a notion of one single way to hold data on you like OOD does, so there is no problem. You can very directly hold data from a relational database in whatever way makes sense for the context you are currently using that data in. The question is based on the misconception that functional programming is all about getting rid of state. Once you realize functional programming is actually about making state explicit, then the answer to the question is obviously "nope, functional programming and relational databases work well together".
I would say persistent is a bad example to use. It takes the approach of "all databases are dumb hash tables and nothing more". Totally throwing away one of the two paradigms involved doesn't demonstrate that the two paradigms work well together.
No. At length: [not](http://db.inf.uni-tuebingen.de/research/ferry) [in](http://trac.haskell.org/haskelldb/) [the](http://msdn.microsoft.com/en-us/library/hh225374.aspx) [least](http://www.haskell.org/haskellwiki/Libraries_and_tools/Database_interfaces/CoddFish).
From the persistent section of the yesod book: &gt;For example, if we're dealing with SQL, a key must be an integer. I have no idea how one would come to such a conclusion, but doing so completely breaks even the most trivial of correct databases. Simple things like lookup tables and cross-reference tables don't work for example. Beyond that, it provides no support for simple patterns in relational data storage. Relationships are effectively useless in persistent. Obviously from the name it is clear that relations are the fundamental concept of relational data modelling. Being able to interact with individual tables isn't helpful, as virtually any use of the system will involve relationships between tables.
I don't know much about Scala to compare. But in Haskell when you use fmap (which generalizes map) over an instance of Functor it returns a collection of the same type. This is enforced by the type signature: class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b For more advanced functionality, you can use Foldable and Traversable type classes. I think the filter example is trickier but doable, see: http://www.haskell.org/haskellwiki/Foldable_and_Traversable Maybe his comment meant the only language that had this by default in its standard collection's API. 
Oh, that's a good point! I added it into the code, it's just a simple diff to the directory watch adder. Thanks :)
&gt; It never, in my experience, works out beautifully with exactly the right type. Unfortunately, the compiler doesn't have a PhD.
Since we are on /r/haskell, I would ask what are the Scala collections, and if possible how do they differ from commons structures such as monoidal functors.
The functional-relational mismatch is far smaller than the OO one, and easier to think about, though perhaps in reality not any easier to deal with. Fundamentally, in things like Haskell we have inductively defined, *free* algebras for data types. Any time we want to impose equivalence relations or constraints on this, life gets *interesting*. Unfortunately, the whole point of a relational DB is to define the narrowest, or most constrained data type possible, applied to the problem domain. It is unclear that there can be any generic way to deal with this.
Semi-related; I've found that Maybe tends to be a bad solution in some cases, especially as fields of record data types. With data Foo = Foo { bar :: Int, thing :: Maybe Boo } you have to handle all cases of Nothing in runtime code. It is likely that the Nothing case should never happen given other conditions, so you will be checking Just/Nothing and writing exception throws for no reason. The multiple constructors approach is also problematic, data Foo = Foo { bar :: Int, thing :: Boo } | Mu { bar :: Int } you end up risking big time if you use `thing' as a function, and actually if your record isn't stupidly simple like this, you duplicate all the fields, creating a maintenance cost. A nicer approach for this particular problem is to parametrize the type: data Foo thing = Foo { bar :: Int, thing :: thing } and then instantiating it, e.g. withFooBoo :: Foo Boo → … or putting unit in there to indicate no value withFooNoBoo :: Foo () → … The advantage gained is that the optionality choice is evident at the type level, and in fact makes it a distinct type which can't be freely mixed with others, and you don't have to write any pointless runtime error checking code; if Boo is in the type, you can just *use it*.
we see this a little in haskell...or any strong type system that is old. over time you have to accomodate the changing world....probably the situation that impacts me the most is strings. Data.Text is a decent solution and i am thankful for it...but i can't be the first person to wish that there was no need for OverLoadedStrings
I finally watched the video, some interesting stuff. I liked the final question, about a type system for effects. But I am not sure I understand Martin Odersky's justification against monads? It did not make sense to me. Does anyone know what he meant?
We've had (effectively) OverloadedNums for decades. What's wrong with OverloadedStrings?
Because scala is a OOP language, it has traits grouping different kinds of operations together. You could probably do something similar in Haskell with type classes if you wanted to. cf http://stackoverflow.com/questions/3623612/in-haskell-why-isnt-there-a-typeclass-for-things-that-can-act-like-lists Whereas scala does define traits and abstract classes for "list-y" things and "map-y" things and "traversable" things, etc. I mean, to give a comparable example, you would need to perhaps compare the structure and layout of the Scala collection classes with Data.Sequence, which does a similair job of breaking up the Haskell collections into groups of typeclasses containing related functionality. So collections are built out of these traits which then define certain invariants on their behavior. Exactly how you'd use typeclasses in Haskell, ( cf Data.Sequence ), and looking at some of the trait names in the scala collection classes, they share a lot of the same nomeclature. 
Then you wouldn't be able to use head, tail, etc on string literals.
Those functions could easily be put into a larger typeclass that works across different types. It's a battle of one convenience vs another convenience, really. I'd say though that in the longer run it would be much more beneficial if [Char] where deprecated to prevent people tripping on the performance implications of using [Char] for string processing, which although it is incredibly easy to use Text (or anything IsString) with OverloadedStrings, definitely still happens (of course mostly due to the fact that the current Haskell ecosystem uses [Char] all over the place!).
This question was done in Haskell-Cafe: http://www.haskell.org/pipermail/haskell-cafe/2011-November/097165.html. But I'm still see it like a good idea.
&gt; and union would re-order everything That introduces inefficiency, which is very bad for a fundamental data structure. &gt; or do some StablePtr magic I'm not sure what you're thinking of, but if you mean checking at runtime that the two comparisons are pointer-equal, that is ok if you're fine with dynamic checking (you essentially use dynamic typing instead of static typing), but potentially not expressive enough -- you could want `(empty cmp)` and `(empty (dual $ dual $ cmp))` to be compatible. The maximally expressive solution is to use contract-like dynamic wrapping to test equivalence on each input, but that is again inefficient. The "good solution" (assuming infinite complexity budget) is to have a dependently typed language where the `Map` type is indexed by the comparison operation chosen. If you want to combine two maps, you have to prove that their comparison functions are observationally equivalent; this should be easy in the easy cases.
&gt; cabal: dependencies conflict: ghc-7.0.4 requires bytestring ==0.9.1.10 however &gt; bytestring-0.9.1.10 was excluded because ghc-7.0.4 requires bytestring &gt; ==0.9.2.1 If I recall correctly this means … actually I don't recall. I knew once. Perhaps there are two ghc packages? At any rate, it would be nice to add a special case to Cabal to highlight the difference/tweak the error message in the case that it seemingly contradicts itself.
:set -XNoMonomorphismRestriction for a saner ghci experience :-) 
Nice ghci prompt =p
Well, I've just started to study compilers, and I think it's like a best practice for writing compilers. Since parsing phase is already complex enough, some pre-processing may come in handy. Lexers are doing this pre-processing and groups the input to tokens. Then you can just work on this tokens in your parser instead of characters/bytes. It's generally easier to work on tokens than to work on characters since you can remove unnecessary details from the input while generating tokens, like whitespace. I think it's possible to do parsing with parsec once you generate tokens with Alex. I'm also trying to solve Coursera's Compilers course's assignments in Haskell, but I stuck at lexing because of this error..
It's not just Windows. I've had similar issues -- a bucketload of them -- in Ubuntu. The problem is that Cabal's dependency management system is (no nicer way to say this, I'm afraid) simply broken. I understand the issue is currently being worked on. Michael Snoyman has [blogged extensively](http://www.yesodweb.com/blog/2012/04/cabal-meta) about the dependency issues with Cabal. This makes sense, as any attempt to install Yesod normally through cabal has for months been a sure-fire way into dependency hell. He has made several commendable attempts to help alleviate the problems, and even more commendable attempts to educate new Haskellers about the issues involved. The best way to work around the issues right now is to use sandboxed environments with either cabal-dev (which I've found a clumsy to use, especially if you work with GHCi -- though your mileage may vary if you use the [instructions found here](http://www.reddit.com/r/haskell/comments/f3ykj/psa_use_cabaldev_to_solve_dependency_problems/)), or virthualenv (which is awesome, though it unfortunately doesn't work in Windows). Barring that, your best bet will be to try to get cabal to install your packages all at the same time: cabal install jmacro diagrams yi The dependency checker seems to have better luck if it can see all the packages together up front. Meanwhile, the standard advice applies: try to stick with the Haskell Platform if you can, run `ghc-pkg check` from time to time to see what dependencies are broken, and every now and then you may need to blow your cabal repository away and start from scratch. Best of luck to you.
The monomorphism restriction is the answer to 8 out of 10 Haskell questions by baffled Haskellers. ☺
Concerning what you said about development environments: Have you looked at EclipseFP for Haskell? I don't really like Leksah either (if you're used to OS X's default UI quality GTK apps are just *not okay*), and while Eclipse also falls into that category I at least found EclipseFP to be very feature rich.
Thanks for the advice, A few tutorials seem to require one package or another which is annoying specially the more practical ones which are what I need as I learn by doing, currently working through *Write Yourself a Scheme in 48 Hours* I'm a little afraid I'll get dependency hell again later on, and there's no list of dependencies, I'm hoping Parsec is all I need. &gt; (no nicer way to say this, I'm afraid) simply broken. Sometimes being polite gets neither party anywhere, but saying that, *simply broken* is reasonably polite. It's probably the reason many mailing lists to new comers feel hostile.
Parsec can also be used as a lexer.
Yes, typeclasses solve the problem of enforcing that various operations return collections of the same type, and Scala could have just done that. Martin seems to think though that monotypic containers like `BitSet` (essentially a `Set Int`, with more efficient representation) should also have a function called `map`, which automagically promotes the `BitSet` to a regular Set if you provide a function `Int -&gt; String`, but returns a `BitSet` if you supply a function `Int -&gt; Int`. Personally, I think this was a bad idea. But that was one of the major motivators.
That looks a lot like [instance chains](https://encrypted.google.com/url?sa=t&amp;rct=j&amp;q=instance%20chains%20haskell&amp;source=web&amp;cd=1&amp;ved=0CGkQFjAA&amp;url=http%3A%2F%2Fweb.cecs.pdx.edu%2F~jgmorris%2Fpubs%2Fmorris-icfp2010-instances.pdf&amp;ei=IXKqT9OeJ-Hl4QSz8aC5CQ&amp;usg=AFQjCNFfVix3dEVcvvZZMddewxTrfW-7bw&amp;sig2=Fb3putDsIJyHpJL_36Rt3Q&amp;cad=rja). Still, for non-* kinds you can define it yourself, e.g. for kind `Maybe *`: type instance Equal (Just a) (Just b) = Equal a b type instance Equal (Just a) Nothing = False type instance Equal Nothing (Just b) = False type instance Equal Nothing Nothing = True However, for kinds `Nat` and `Symbol` I can't define these myself, since there are an infinite number of types in them. However, since they're treated in a special way anyway, I was hoping the instances could be defined inside GHC itself somehow. Perhaps I should send an email to ghc-users.
What are the chances that we can ever get rid of the monomorphism restriction?
Saner experience, except when you try to use ghci as you might use R or Matlab... and then ghci decides to redo your expensive computations every time you want to use the result. ;-)
&gt; The "good solution" (assuming infinite complexity budget) is to have a dependently typed language where the Map type is indexed by the comparison operation chosen You don't need dependent types for this. In Haskell you could write: data MyMap wrap a b = Map (wrap a) b class Newtype1 f where wrap :: a -&gt; f a unwrap :: f a -&gt; a fromList :: (Newtype1 wrap, Ord (wrap a)) =&gt; [(a,b)] -&gt; MyMap wrap a b union :: Ord (wrap a) =&gt; MyMap wrap a b -&gt; MyMap wrap a b -&gt; MyMap wrap a b -- etc. 
http://www.haskell.org/cabal/FAQ.html#dependencies-conflict. Usually running cabal in verbose mode can point to where the actual problem is.
&gt; It's not unusual to have unix and posix dependencies on packages, since a lot of Haskell hackers just do not think or care about Windows compatibility. I totally understand, I use Linux sometimes but been using Windows lately, it's when the package says it supports windows and a series of errors happen, like diagrams for example I can compile the example code but I get dll errors. I guess I've been spoiled with apt and pacman over the years :)
Why the downvote? Defaulting *is* at work here.
You can also try a virtualbox host for Linux on your Windows box. VirtualBox is slick. Arch is a popular distro for Haskellers. 
My guess is that it was around before the various parsing DSLs, and they imitated the GNU lexing/parsing toolchain. Also, it is generally used in conjunction with Happy, which is a “real” parser generator.
I don't understand how this solves the problem. Could you elaborate? If I understand correctly, `wrap` is a reification of an equality coercion. In Haskell's core language we now already have equality coercions; so this would be useless, once such coercions will have made their way to the surface type system (and real equality coercions bring implicit casting goodness). Are you not just paraphrasing the `newtype` trick of definition a newtype with a different `Ord` definition? This is still not maximally expressive: we would want the ability to say that, if two different comparisons are observationally equivalent, then the corresponding Map types are compatible. With newtype (or the `Map.Make` functor in SML/OCaml), you can't regain compatibility after the fact.
What you might want to parameterize it by is not the underlying value type (as that doesn't change), but some functions indicating whether that value exists or not. data None a = None data Some a = Some a data Foo f = Foo { bar :: Int, thing :: f Boo } Now the value contained is clear in the type of `Foo`, we're just parameterizing over the container, which lets us use `Foo Maybe`, `Foo Some`, and `Foo None` depending on whether we've checked. We could even make `Monad` instances for `Some a` (just the identity monad) and `None a`.
As others have said, you can use Parsec as a lexer. In fact you don't really need separate "lexing" and "parsing" phases. A few years ago I wrote a compiler in Haskell using Parsec. It's not the nicest code, but it might be helpful to you. The "lexer" is in Compiler/Scanner.hs, and the "parser" is in Compiler/Parser.hs: https://github.com/michaelmelanson/cminus-compiler
Works for me, I think I prefer this.
The first volume was pretty good. I got a copy off Lulu. Print quality was mostly fine but there were a couple of errors - in some places, the string "ll" got elided by accident. The content was very variable, as you'd expect from a book with many authors. I think I enjoyed the Berkely DB chapter most.
True. The zlib error is obviously a missing library. I missed that on my first reading. I was looking at the ByteString dependency conundrum, which does seem to be a dependency issue. I've come across situations where Cabal fails to find a library by its hash value even though the library with the appropriate hash is clearly present. More often are problems with the packages themselves -- not updated for the most recent GHC, or relying on packages that themselves have not been updated, etc. I've had good luck with sandboxing lately, so it's less of an issue for me than it was a few months ago. &gt; What will really help here is more extensive build reporting on hackage, so we can get build results for a number of platforms. Agreed. A smaller-scale version of what WineHQ does with their app database would go a long way here. 
The other two answers are the layout rule (especially tabs vs spaces) and 'no, you can't, except by using `unsafePerformIO` which is evil'.
What is a better alternative? E.g. UHC can't compile smallcheck because it doesn't support the monomorphism restriction.
&gt; nor even as a reactive element (unless we ensure exclusive control, linearity). Why is exclusive control of the database by an application so implausible? And what do you mean by linearity?
I saw it a while ago and wondered if anyone was using it, because it looks interesting. I've actually tried porting it to Java w/ Functional Java and, later, Scala, and I got stuck both times.
&gt; I don't see the point of two different types being compatible. How is that ever possible? Are you saying that you want extensional equality? If two libraries define the type of integers sorted in decreasing order, you would like them to be able to define it independently, while remaining compatible (because they're both sorting integers by the same, well defined, order). With your trick, they have to share the same instance of `Newtype1` (so either one must depend on the other, or they must both depend on a common library providing it). That is the same kind of issues you have with object-oriented type systems that only have nominal subtyping: if you want a type `Foo` to implement an interface `Bar`, then the developer of `Foo` must have had knowledge of the code defining `Bar`, you can't say after the fact that, for example, all references having `read` and `write` methods also are `read`-only references (covariant "sources"). `newtype` is a nominal feature in this respect; the structural counterpart is to expose at the type level the reason for (in)compatibility, the comparison operation.
Don't put weird corner cases in your code that rely on certain functions having their type defaulted in order to compile?
In Scala: trait Monoid[M] { def append(a: =&gt; M, b: =&gt; M): M } trait FMList[A] { def foldMap[M: Monoid](f: A =&gt; M): M } Scala doesn't have by-default lazyness.
It is used by the [level-monad package](http://hackage.haskell.org/package/level-monad), but I don't know if that is used much. I don't know Scala very well, but I'd expect that it should be possible to implement FMList. What was the problem?
Not enough laziness, and the difficulties of integrating with the collections APIs, in both Java and Scala.
That's essentially what I tried, but I couldn't get it to integrate into the existing collections system, and a lot of the core methods ended up as stack overflows, even though Haskell has no trouble with them.
Scala's existing collections system probably won't work with this. I think the strictness is placed in many weird places here.
You can work around the stack overflows, but you need to use a trampoline like the one supplied by Runar Bjarnason in scalaz, which slows it down a couple of orders of magnitude. Also, many of the fold/traversal combinators supplied by scalaz are lazier than their core collection alternatives explicitly to support things like this.
I was already trying to integrate with scalaz (that's where I was getting the necessary Monoid type class), but I didn't think of using a trampoline. Thanks!
Lulu publishes ebooks. It's just not available quite yet. &gt; When the print version is available we will also make the content &gt; available on the website [...], and shortly &gt; thereafter we'll release the ebook and Kindle versions.
Oh, I thought the interesting part was that it started with FML....*sigh* fml
While I haven't used it in any rigorous environment, I did some exploration on the command line and it seems like a good enough package for what it's designed for
Was there ever a Volume 1 of Architecture of Open Source Applications?
If you're not concerned with performance, yes.
&gt; Now I'm working on this for hours and I'm starting to lose my interest in using Alex. Am I doing something wrong? I see you've already got your problem solved, but for what it's worth, Alex is a pain in the ass to figure out how to use. Most lexer generators manage to avoid requiring you to pick one of a dozen wrappers or, as I always resort to doing for various reasons, writing the various alex* functions by hand. There's really no reason Alex couldn't provide a sane interface by default, but it just doesn't. That said, once you get the hang of it, it is a good way to handle lexing. Unlike something knocked together with parser combinators, the specification of your lexing rules will be short and clean and your performance will be solid. Similarly, you're usually much better off using something like Happy over something like Parsec as Happy will give you good performance, allow for clear specification of rules, allow you to use left recursion, verify that your grammar is unambiguous without forcing you into ordered choices, et cetera. Like Alex, Happy isn't great, but it's "okay" and currently the best tool for most jobs if you're dealing with Haskell. A lot of people will try to sell you on things like Parsec. If you're trying to parse a programming language, please at least know how to use a parser generator like Happy so you have a benchmark when evaluating a combinator-based approach. I've tried to do non-trivial things with various parser generators and always regretted it, especially when it came to refactoring grammars. The validation that you get from a parser generator that your grammar specified in terms of unordered choices is unambiguous and can be parsed efficiently is not worth giving up just to avoid dealing with some awkward code generation. 
&gt; A structural record system, so we can make a record type that exactly matches our data without parameterizing it over every field's potential type. It's not at all clear that this is a win. You would tend, I think, to end up with large types, irritating type error messages, et cetera. Just look at how things tend to go with polymorphic variants (and structural objects) in OCaml. They're also quite hard to compile efficiently. &gt; Make partial record field getters return a Maybe value. This is a concept that shouldn't exist in the first place. Haskell's approach to records is frankly just bizarre and ill-conceived. There's a reason no one had the same approach before and no one has done the same since.
Ah, okay. It's the chapter list starting with Asterisk.
&gt; It did not make sense to me. You're not alone.
new theme?
ObOpenSourceResponse: patches welcome! 
Isn't this FAQ answer outdated? I think the current GHC/Cabal system does track ABI hashes (or something similar at least), and cabal-install warns you when installing a package would break another.
What the hell, dude? The question was, in part: &gt; is Alex best way to write lexers in Haskell? I cannot offer an objective opinion, but I offer my subjective two cents, which is: &gt; I tend to just use parsec for everything.
I noticed that main = print $ FML.length $ FML.fromList [1..10^8] uses much memory, while the equivalent using regular lists uses constant memory. Is this fixable? edit: using "foldl (const (1+)) 1" does the job. I wonder why using foldl' (strict version) is twice as slow as using the regular foldl, for FMLists and also for normal lists. edit2: disregard last edit. const (1+) is not the same as (\a _ -&gt; a+1)
Wouldn't writing an Applicative parser, rather than a monadic one, yield the same efficiency benefits (up to initial O(1) compilation of the parser at runtime)? You'd also know about ambiguity when compiling the parser, rather than when running it, which is not as good as knowing at compile-time, but perhaps worth the removal of awkward code generation and executable dependencies.
LLVM has has support for the PTX backend for several months. This whole thing isn't really an announcement of anything new at all, and I haven't been able to figure out why nvidia are making a fuss about it.
Yes, LLVM has had a PTX backend for some time now. But it's going to be deprecated and replaced with a new backend contributed by NVIDIA. NVIDIA has basically open sourced and upstreamed the backend of the CUDA compiler.
What then exactly are stateless objects? If you allow it to access arbitrary other objects, then it won't necessarily be stateless, even if it does not personally store state (for example it might access a database). Perhaps the essential property is that an input given to the object can only influence the direct output, and not outputs of subsequent operations. This is indeed a compositional property: you can't make a stateful object out of stateless ones. Is this the kind of object your interested in, or any object that does not personally store state? I'd say that the first kind of stateless object is not expressive enough to do anything interesting, and the second kind is just a stateful object in disguise.
They have to wait for AMD to build a LLVM backend for their GPUs?
Do you mean a CUDA port onto AMD GPUs ? CUDA does not run on ATI GPUs does it ? 
&gt; Can you give an example? An example of what? The claims I made for Happy are statements of fact. I don't really want to put together examples of why the ability to use left recursion makes things more pleasant or how the use of ordered choice allows for subtle bugs with parser combinators though. As for Alex, you're free to look at the examples in the Alex manual and come to your own conclusion. &gt; The combinator approach is so much more elegant and cleaner. If you don't see the elegance of specifying a grammar in terms of unordered choice and having it checked for ambiguities, then you don't see it. Personally, I view it as another form of static checking. It's something I'm grateful to have when dealing with large, complex grammars. The code generation aspect of parser generators doesn't need to be a big deal. For example, the Menhir parser generator for OCaml can type check your code before it ejects the final result so that you get good type error error messages during your development process. It's a very pleasant way to work.
No one's asking for an "objective opinion", but you can use objective statements of fact to justify your opinion as I did elsewhere when arguing for Happy in this thread.
Note, I am not arguing for Parsec. I am stating what I use. I am not making a statement on the objective quality of Happy/Alex nor Parsec. All I am providing is a data point. And there's nothing wrong with that. There are (or will be), obviously, plenty of other answers with more detailed and constructive statements. I am not required to produce similar responses. I am not beholden to reddit, required to answer every question with a detailed explanation and justification. I am not required to answer at all. If your argument is that I should avoid answering at all rather than providing an (admittedly) less helpful answer than others (I argue, however, that it's still helpful), then you have yet to substantiate that argument. My argument for my comment's helpfulness is simply that the original poster may not have known that Parsec can be used for lexing (as, it turns out, he/she didn't), therefore my comment would lead the original poster to evaluate Parsec as a possible alternative. I should stress that I am not out to persuade anyone of anything, I am just pointing out Parsec as an option. Furthermore, comments *discouraging* participation in a discussion, such as yours, work against a healthy community. 
I find the noise of "I like X" to just distract from the conversation. No one's asking you for a dissertation, but even saying "You may want to consider Parsec for lexing as an alternative to Alex" would've been more direct. As was made clear by the original poster's response to you -- "I think it's possible to do parsing with parsec once you generate tokens with Alex" -- it's clear that you did *not* at all impart the knowledge you claim you were attempting to impart with that post. As such, all you managed was "I like X". I'm not looking to be rude here, but it should be clear that you neither helped the original poster with that first comment nor imparted useful knowledge. &gt; Furthermore, comments discouraging participation in a discussion, such as yours, work against a healthy community. Communities work better when members make an active effort to help people asking questions by providing actual information.
See http://stackoverflow.com/questions/7751647/is-there-an-haskell-edsl-for-writing-lexers for an alternative to alex.
&gt; I find the noise of "I like X" to just distract from the conversation. Perhaps it does, perhaps it doesn't, but this is an entirely subjective argument - something you're criticising *me* for presenting. How ironic. (Furthermore, that's not even what I said) &gt; No one's asking you for a dissertation, but even saying "You may want to consider Parsec for lexing as an alternative to Alex" would've been more direct. Thanks for your writing critique. Once again I stress that my goal was simply to engage and initiate a discussion, not provide a well-rounded, complete answer. &gt; I'm not looking to be rude here, but it should be clear that you neither helped the original poster with that first comment nor imparted useful knowledge. And yet, the useful knowledge of which you speak was revealed in the pursuant discussion, in which I participated. This is such a weak argument, I don't know why you're making it. &gt;Communities work better when members make an active effort to help people asking questions by providing actual information. Which I did. Thank you for your completely non-constructive comments that only sour the tone of the otherwise polite discussion that usually takes place here.
Now if they'd only release the source for their video drivers. Rage.
He's talking about dependencies on the parser or lexer generators, in order to build your program that uses the parser/lexers.
There already is an LLVM backend for AMD IL
Parsec as a Char parser is great for parsing small files. Once you want to parse larger more complex stuff you do really need to use Parsec as a Token parser and then you need Alex for the tokenizer it you want to tokenizer to be *fast*. Check out the DDC compiler. It uses Alex and Parsec. 
Great. Now I'll be wondering which one of you is "CptCancer". See you in class...
I'll try to put one together in the next couple weeks.
&gt; `do return` wat
Could just be that no one got around to writing the press release until now. That happens.
You appear to have inverted his statement. If AMD wants in, they should contribute a backend to LLVM as NVidia has done. According to squirrel5978, they have already done this.
What about the fixed function stages in the pipeline? You'll [lose a lot of performance](http://www.tml.tkk.fi/~samuli/publications/laine2011hpg_paper.pdf) without them.
On the plus side, you can completely change the renderer at any stage to achieve effects that aren't possible on the fixed hardware. This is what the dream of Larrabee was supposed to be: a general purpose vector machine powerful enough to replace special purpose hardware. I think it's inevitable that we'll see such a development, the only question is when.
&gt; But in Haskell when you use fmap (which generalizes map) over an instance of Functor it returns a collection of the same type. […] For more advanced functionality, you can use Foldable and Traversable type classes. One issue that doesn’t arise with Scala collections is that, at least in Haskell 98, Set can’t instantiate Functor (due to its Ord constraints); Text can’t instantiate Foldable (it’s of the wrong kind); and so on. This limits the amount of achievable genericity without rolling one’s own variants using e.g. type/constraint families. Scala’s collections just do the right thing, or at least are intended to.
Jesus Christ, I wish my projects had been this well-defined. Strike that, I wish I had had this class.
&gt; There should be some reason for GHC authors to choose Alex/Happy over Parsec. Maybe because it was written more than a decade after the first GHC ? You have to take into account history for this kind of thing, if you have a working parser, you won't rewrite it with the latest technology just because it's shiny... Not that Parsec doesn't have its own share of problem, but really most parser/lexer recently have been written in Parsec and co (attoparsec, cereal, etc), Alex/Happy still work and are being improved (unicode support especially) but they're not used that much in modern Haskell programs. (Especially, for performance oriented parsing, Parsec is not a really good choice, that said some of the alternative in the combinator parsing crowd are fast enough for most tasks)
Looks like it's up now.
Ah, if that's what you mean by stateless objects I'm not entirely convinced of their practical advantages. If you encapsulate a mutable reference in an object then all objects are trivially stateless. I guess we'll see when you implement them and RDP.
looks great... can anyone explain why existing tools (like vlad and capistrano) are not fit for the job? (i see the numerous differences between them and keter, but i still wonder) i mean within yesod the all encompassing reason to DIY is to make sure type safety is not compromised on; but type safety stops at the boundary of the "app" -- keter is not using types from inside a yesod app i guess. 
This is neither here, nor there, but the word keter was introduced to me as a series of terrifying internet horror monsters catalogued here: http://www.scp-wiki.net/keter
indeed when db migrations are needed downtime is usually favored over the effort needed to take a different route.
Well there are (mandatory) exercises in which you have to apply the things discussed in the lecture. There is also an exercise course once a week in case you have further questions or did not understand an exercise, so I think it all works out pretty well.
I appreciate the feedback, and your point is absolutely correct. However, I think my wording is still accurate: &gt; __Allows__ updating your app with zero downtime. This approach gives the possibility to release without taking down the site, though other issues (such as DB migrations) could still cause problems. For that matter, a sudden power outage or meteor strike could also cause downtime... That said, I've added a note to the page explaining this point more clearly.
I've personally not used either of those tools. If others with experience with them are able to put together a demonstration of how to use them to achieve the same goals that Keter is doing, I'd be very interested. However, from a brief look, it seems to me that they really perform orthogonal functions: Keter is going to maintain your Nginx configuration, create PostgreSQL databases, and monitor processes. Capistrano is a "tool for running scripts on multiple servers". It seems like for those interested, Capistrano could be a great way to configure multiple servers with Keter and then deploy a bundle to it. But even if the two tools completely overlapped, there's one very simple reason for Keter to exist: Haskellers will be much more adept at modifying Haskell code than Ruby code. It's true that this reasoning falls into the NIH category a bit, but it doesn't mean it's not true.
Thanks for adding that comment. You're right that your wording was accurate. I somehow missed the "Allows" part. I guess I'm just sensitive to overreaching claims in the web community. (Speaking of meteors... latency compensation...)
Funny, I was about to report that I had effected that trivial labor. It seems a nice use of DataKinds, constraining the types to exclude nonsense. https://gist.github.com/2660297 As I said in a note to rampions gist, it is surprising how well the Show deriving works even with the added type fanciness; you can zip and unzip in ghc just fine. I added `deriving instance Show` for a couple of types that were unShown.
I like it. I'll fork off that when I get around to implementing delete.
It represents the fictional horrors that are capable of destroying all humanity (at a minimum). It's probably a reference to the Kabbalahistic meaning. The early founders of that site and some of the earlier entries indicate a certain familiarity with that area of expertise, quickly diluted by the incomers. Careful with that wiki. Like Wikipedia or TVTropes it can absorb mandays from "susceptible" (for lack of a better term) minds.
From what I know it should be easy to generalize. I think that process will be undergone lazily though. Michael originally announced this project on web-devel because that is his intention.
&gt; implement a variant of breadth first search You might find [control-monad-queue](http://hackage.haskell.org/package/control-monad-queue) and/or the first section or two of the associated paper useful. I must admit, the practical applications of that package are few, but breadth first search is right up its alley. (Well, might be, assuming you don't need to do any IO in the process) It should perform quite well, perhaps even favorably compared to other queues, though I haven't tested it out on more recent versions of GHC. It might be worth trying to implement Allison's Queue yourself and see if that's any faster; but at least at one point (with GHC ~6.8) there was almost no abstraction penalty with control-monad-queue. If you are looking for a fast queue, dlists (and presumably FMLists) can't deliver. It'd be better to go with Data.Sequence, or maybe the traditional two-stack queue or Chris Okasaki's persistent real-time queue.
well, what about tipharet &amp; malkuth?
Thanks for the clarification. I certainly understand that you've got better things to be doing than optimizing the user interface on a parser generator. All of your hard work on GHC is much appreciated.
First, thank you **so much** for taking the time to type this up. It makes my previous appreciate of Haskell pale in comparison, firstly because I had never been exposed to GADT's and they're such a beautiful concept, and secondly because I hadn't before considered another aspect that makes Haskell beautiful: It's community. Secondly, if I understand it right, B :: Node cL n a -&gt; a -&gt; Node cR n a -&gt; Node Black (Succ n) a enforces that the natural number "n" be equal in both, because it's not actually an integer, it's a type-level arithmetic? ie, if you pass it nodes with different "integer values" of n, it will fail at compile time? I assume, then, that the really complex type that eventually results (With 50 Succ's chained together) gets optimized down to almost nothing by the compiler? If so, that's absolutely gorgeous and brilliant!! I can't wait to see the rest of your writeup!!
That's huge! 
Not to mention the fact that Ruby is not quite widespread enough that you can assume it to be there on any server so it does add extra dependencies.
You mean an instance of `Storable` for a type defined as a GADT? That sounds suspicious to me. I think doing it in the straightforward way will usually be unsafe and generally require `unsafeCoerce`. The best way I can think of is to make a `Storable` instance for a plain ADT and then convert to a GADT in a type safe way (returning `Nothing` if the value can't fit the required type, for example).
Next we have our second GADT, `Context`. It doesn't use any language features we haven't already seen, but one still might be confused as to its purpose. Why have a whole separate type for incomplete trees? -- one-hole context for red-black trees data Context :: Nat -&gt; RedBlack -&gt; Nat -&gt; * -&gt; * where -- if we're at the root, the hole is a black node Root :: Context n Black n a -- we can go left or right from a red node hole, creating a hole for a black node BC :: Bool -&gt; a -&gt; Node Black n a -&gt; Context m Red n a -&gt; Context m Black n a -- we can go left or right from a black node hole, creating a hole for either EC :: Bool -&gt; a -&gt; Node cY n a -&gt; Context m Black (Succ n) a -&gt; Context m cX n a deriving instance Show a =&gt; Show (Context m c n a) (Some example values, for playing around in ghci) a = B Leaf 'a' Leaf b = B a 'b' a c = EC True 'c' a Root z :: Zipper ('Succ ('Succ One)) Char z = Zipper a c The key use of a `Context` is as part of a `Zipper`. If you're unfamiliar with [zippers](http://www.haskell.org/haskellwiki/Zipper), they're a technique for efficiently editing functional data structures. By keeping track of the path we used to traverse down a `RedBlackTree` to get to a particular `Node` (using a `Context`) we can view and modify the value at that `Node`, and then rebuild a new `RedBlackTree` using the `Context`. Think of a `Context m c n a` as a `RedBlackTree a` (rooted at a `Node Black m a`) with a hole in it where a `Node c n a` should go. The `Zipper`, then stores the tree-with-a-hole and a proposed value to fill that hole. So the constructors for `Context` are designed to maintain the same constraints as for `Node`. The `BC` constructor creates a partial `Red` node to fill a `Context m Red n a` giving a value and one of the child - all that is missing is another `Black` node for the other child (`BC` for "black color"). Similarly, the `EC` constructor creates a partial `Black` node to fill a `Context m Black (Succ n) a` - all that's missing is the other child, which may be of either color (`EC` for "either color"). This counts as one of the `Black` nodes between the root and the leaves, so any value that fills that hole must have one less `Black` node on each of its paths to the leaves. data Zipper m a = forall c n. Zipper (Node c n a) (Context m c n a) deriving instance Show a =&gt; Show (Zipper m a) Note how `Zipper` uses existential quantification to eliminate the color `c` and the black node count `n`. They're bound to be the same for the `Node` and the `Context`, but this allows a value of `Zipper m a` can be any point under a tree rooted at a `Node Black m a` - we don't want to limit it to being for a particular hole type. Creating a `Zipper n a` from a `Node Black n a` is easy - we just use the given node as the proposed fill and record that we're at the root of the tree. -- create a zipper unZip :: Node Black n a -&gt; Zipper n a unZip = flip Zipper Root Restoring a `Zipper m a` to a `Node Black m a` just requires replaying our history, fitting in the given `Node` into the `Context` to get a new `Node` and `Context` until we get to the root of the tree. -- destroy a zipper zipUp :: Zipper m a -&gt; Node Black m a zipUp (Zipper x Root) = x zipUp (Zipper x (BC goLeft a y c)) = zipUp $ Zipper (if goLeft then R x a y else R y a x) c zipUp (Zipper x (EC goLeft a y c)) = zipUp $ Zipper (if goLeft then B x a y else B y a x) c Now we finally get to some code that uses our `RedBlackTree` as a [binary search tree](http://en.wikipedia.org/wiki/Binary_search_tree). For each value `a`, there's exactly one position in a tree that will hold `a`. If `a` is currently in the tree, this will be an interior `Node` (using the `R` or `B` constructor). If not, this will be a `Leaf` node. Since this has us traverse through the tree, we use a `Zipper`, so that once we find this location, we can modify the tree to either insert or delete the given value. It's `zipTo`'s job to create the `Context` chains `zipUp` consumes. -- locate the node that should contain a in the red-black tree zipTo :: Ord a =&gt; a -&gt; Zipper n a -&gt; Zipper n a zipTo _ z@(Zipper Leaf _) = z zipTo a z@(Zipper (R l a' r) c) = case compare a a' of EQ -&gt; z LT -&gt; zipTo a $ Zipper l (BC True a' r c) GT -&gt; zipTo a $ Zipper r (BC False a' l c) zipTo a z@(Zipper (B l a' r) c) = case compare a a' of EQ -&gt; z LT -&gt; zipTo a $ Zipper l (EC True a' r c) GT -&gt; zipTo a $ Zipper r (EC False a' l c) When this becomes a full library, it probably won't export the `Node` type, since that's an implementation detail, and too involved to expect library users to deal with. So we'll need functions to create `RedBlackTree`s, since the library users won't have access to the constructors. -- create a red-black tree empty :: RedBlackTree a empty = T Leaf -- insert a node into a red-black tree -- (see http://en.wikipedia.org/wiki/Red%E2%80%93black_tree#Insertion) insert :: Ord a =&gt; a -&gt; RedBlackTree a -&gt; RedBlackTree a insert a t@(T root) = case zipTo a (unZip root) of -- find matching leaf and replace with red node (pointing to two leaves) Zipper Leaf c -&gt; insertAt (R Leaf a Leaf) c -- if it's already in the tree, there's no need to modify it _ -&gt; t The classic description of the red-black tree insertion algorithm uses eventual consistency. You insert a red node at the proper point in the tree, then recolor and rebalance the tree until the it fits the constraints of a red-black tree. That option's not available to us, however, because as we've defined it, any `RedBlackTree a` *must* fit the constraints. So we'll need a different approach. Rather than actually construct an inconsistent tree (for which we'd need a whole *other* data type), we can use the current `Context`. If all we wanted was to represent a hole in the tree, we could have defined the `Context m c n a` as just an alias for `Node c n a -&gt; Node Black m a`. But by defining it the way we did, we can inspect the values of the parent, uncle, and grandparent nodes for the various cases that need to be handled. insertAt :: Node Red n a -&gt; Context m c n a -&gt; RedBlackTree a -- 1) new node is root =&gt; paint it black and done insertAt (R l a r) Root = T $ B l a r -- 2) new node's parent is black =&gt; done insertAt x (EC b a y c) = T . zipUp $ Zipper x (EC b a y c) Note that `insertAt` returns a `RedBlackTree a`, rather than a `Zipper`. The reason for this can be seen in the first two cases. In case 1, we add a new black node at the root, incrementing the number of black nodes on all paths from the root to the leaves, so it would have to return a `Zipper (Succ m) a`. But in case 2, we just insert a red node, leaving the black node count as it was, so that would return an `Zipper m a`. We use the existential quantification provided by the `RedBlackTree a` here to mask that changing type. -- 3) uncle is red =&gt; paint parent/uncle black, g'parent red. recurse on g'parent insertAt x (BC pb pa py (EC gb ga (R ul ua ur) gc)) = insertAt g gc where p = if pb then B x pa py else B py pa x u = B ul ua ur g = if gb then R p ga u else R u ga p -- 4) node is between parent and g'parent =&gt; inner rotation insertAt (R l a r) (BC False pa py pc@(EC True _ _ _)) = insertAt (R py pa l) (BC True a r pc) insertAt (R l a r) (BC True pa py pc@(EC False _ _ _)) = insertAt (R r pa py) (BC False a l pc) -- 5) otherwise =&gt; outer rotation -- XXX: GHC seems unable to infer that gy is Black so I have to do both cases -- explicitly, rather than -- insertAt x (BC True pa py (EC True ga gy gc)) = -- T . zipUp $ Zipper (B x pa $ R py ga gy) gc -- insertAt x (BC False pa py (EC False ga gy gc)) = -- T . zipUp $ Zipper (B (R gy ga py) pa x) gc insertAt x (BC True pa py (EC True ga gy@Leaf gc)) = T . zipUp $ Zipper (B x pa $ R py ga gy) gc insertAt x (BC True pa py (EC True ga gy@(B _ _ _) gc)) = T . zipUp $ Zipper (B x pa $ R py ga gy) gc insertAt x (BC False pa py (EC False ga gy@Leaf gc)) = T . zipUp $ Zipper (B (R gy ga py) pa x) gc insertAt x (BC False pa py (EC False ga gy@(B _ _ _) gc)) = T . zipUp $ Zipper (B (R gy ga py) pa x) gc Lastly, we define equality for `RedBlackTree`s and `Node`s. We can't just derive `Eq` for these types since they use existential quantification to mask some types, so we manually define equality as structural equality with the same values at each location. This may not be the best definition of equality for `RedBlackTree`s, as two trees can hold the same values, but have different structure. For example, both `T $ Node Leaf 3 (Node Leaf 4 Leaf)` and `T $ Node (Node Leaf 3 Leaf) 4 Leaf` are valid `RedBlackTree Int`s holding just the values `3` and `4`, but since they have different structure, they would be marked as non-equal. -- can't derive, since we abstract over n, so we have to manually -- check for identical structure instance Eq a =&gt; Eq (RedBlackTree a) where T Leaf == T Leaf = True T (B l@(B _ _ _) a r@(B _ _ _)) == T (B l'@(B _ _ _) a' r'@(B _ _ _)) = a == a' &amp;&amp; T l == T l' &amp;&amp; T r == T r' (ran out of room, so I'll just trim the definitions) _ == _ = False -- can't derive, since B abstracts over child node colors, so -- manually check for identical structure instance (Eq a) =&gt; Eq (Node c n a) where Leaf == Leaf = True R l a r == R l' a' r' = a == a' &amp;&amp; l == l' &amp;&amp; r == r' b@(B _ _ _) == b'@(B _ _ _) = T b == T b' _ == _ = False 
re:type-level arithmetic. That's it exactly.
getLine returns IO String. String is a list of Char, so we can say getLine returns IO [Char]. (++) has signature [a] -&gt; [a] -&gt; [a], where in this context 'a' is Char, so [Char] -&gt; [Char] -&gt; [Char] Your change would have ++ expecting [Char] -&gt; (IO [Char]) -&gt; [Char], which isn't the same thing. Note that: do text &lt;- getLine return text ... is an expression of type IO String,.. just like getLine. That's why it is awkard; you could always just use getLine directly.
Glad to help!
You can use greetingForUser = do name &lt;- getLine return $ "Hello, " ++ name ++ "!" though.
I'm not too worried. Identifying when the flag is being turned on is pretty easy, and you do have to have some amount of knowledge to convince Cabal to apply it by default, etc.
Compared to unsafePerformIO, I don't think this flag is risky. If someone uses unsafePerformIO a lot, they might not be aware of the fact that they're doing it wrong. Code that uses unsaferPerformIO can appear to work fine: people try it, their code runs, the answers it correct, they think they're done. With -fdefer-type-errors, you don't even get the illusion that it works. If you try to run any part that had a deferred type error, it will always throw an exception. Sure, you might be able to run some parts, but it is not the same way as C where a type error means the compiler will do a best-guess at how to cast it.
It's not particularly *risky*, but it defeats many of the benefits of static type checking, benefits you might not realise you wanted until you've spent some time with them. The same goes for unsafePerformIO. I know many who tried Haskell and would without a doubt use unsafePerformIO and -fdefer-type-errors given the chance, just because the IO monad and type errors are so daunting at first. They see static type checking for the barricade it is, but they don't realise [it could be put in front of lion cages instead of supermarkets](http://labs.scrive.com/2011/12/why-monads/). Hell, I, too, was close to using unsafePerformIO back in the day, but thanks to the name and all the people and documentation shouting "NO" I didn't. And I'm glad for that today. I'm not saying it's easy; I'm saying it's worth it.
Minor detail: return isn't a keyword, but just a function (exported by the Prelude) with type Monad m =&gt; a -&gt; m a. It's one of the two functions you have to implement when making your own monad.
Cool, [our implementations are pretty similar](http://www.reddit.com/r/haskell/comments/fsvli/typeenforced_balanced_trees/). Note that after trying to implement "delete" I decided that maybe AVL trees are simpler. [So here's an implementation for AVL trees](https://github.com/yairchu/red-black-tree/blob/master/AvlTree.hs). 
You can replace: randomCardIndex &lt;- randomRIO (0, length unshuffled - 1) let randomCard = unshuffled !! randomCardIndex unshuffledBefore = take randomCardIndex unshuffled unshuffledAfter = drop (randomCardIndex + 1) unshuffled with: let (unshuffledBefore, randomCard : unshuffledAfter) = splitAt randomCardIndex unshuffled 
too late...
Argh! Still nothing in Texas, where I want to settle down. Is Texas just a Haskell wasteland?
I've written the patch, now I just have to work out how to get all my accumulated patches from darcs into the new git repo...
Perhaps another way of putting this is that what I actually would like to see is for the `-Werror` flag to turn into a more flexible ability to set the line between warnings and errors. There's a whole continuum, where one side is "never fail; even if you have to generate code that defines no symbols", then there's "only fail on parse errors where it's truly impossible to make sense of the file; turn everything else into runtime errors", and then there is "accept valid Haskell programs according to the language spec" and the other extreme is "turn all warnings into errors".
I like the idea of doing something similar for not-in-scope errors.
What makes a traditional lexer/parser faster than Parsec? I've heard this before, and I've always been curious what the main limiting factors are.
Nice idea. However, the same issue exists for normal lists: main = print $ foldl (const (1+)) 1 $ [1..10^8] foldl' is twice as slow for me.
Really? I think I just took it.
What is the point?
no point, just seeing that haskell is picking up a lot of following lately.
&gt; I remember a couple of months ago it was in the 6xxxies..! A couple of months ago it was in the 8400s I believe. It's growing , but not _that_ fast :)
See also [Good Math, Bad Math blog: Advanced Haskell Data Structures: Red-Black Trees](http://scienceblogs.com/goodmath/2009/11/advanced_haskell_data_structur.php) 
I'm definitely excited for these; it means that I can play around with functions that are in a file that doesn't typecheck without having to comment out the definition of the buggy function and replace it with undefined.
I believe blackboard and forcing students to copy stuff off it are the worst in the absence of transparent teachers since they force students to divide their attention between the part of the blackboard they can see and the part the teacher is saying and writing simultaneously.
We are also all eagerly awaiting GHC to support holes: http://hackage.haskell.org/trac/ghc/wiki/Holes
I, for one, really hope the new concurrency substrate will make it in. Finer control over thread priority would help me out a lot.
Very interesting! This effectful fold seems similar to what I've been working on for my MSc thesis and accompanying paper[1]. In the paper we show how to use a effectful fold to store binary trees on disk. It uses the same kind inversion of control as enumerators, but for non-list like data structures. The work of Johann and Ghani on recursion schemes was a big inspiration for my thesis. [1] https://github.com/downloads/sebastiaanvisser/msc-thesis/wgp10-genstorage.pdf
If this general pattern interests people, I just wanted to let you guys know that I'm working with Ross on releasing a "free monad transformer" as part of the `transformers` package that implements exactly this functionality. You can see the prototype implementation at: https://github.com/Gabriel439/FreeT/blob/master/Control/Monad/Trans/Free.hs So, for example: type Stream m a = FreeT ((,) a) m () type Reader a m b = FreeT ((-&gt;) (Maybe a)) m b -- or type Reader a = FreeT ((-&gt;) (Maybe a) It's called a "Free monad transformer" because when the monad is `Identity`, it becomes a free monad. The only other implementation on Hackage that implements the `MonadTrans` instance correctly for a free monad transformer is `monad-coroutine`, where the `Coroutine` data type is identical to `FreeT`. The reason I've been working on this data type is that I also plan on using it in my next release of `pipes`, where I will formulate the `Pipe` data type as: data PipeF a b x = Await (a -&gt; x) | Yield (b, x) deriving (Functor) type Pipe a b = FreeT (PipeF a b) This dramatically simplifies a lot of my library's code and fixes what used to be a broken `MonadTrans` instance.
I'm not really understanding what's the point here, but anyway, recently there was [a post on Haskell-cafe](http://www.haskell.org/pipermail/haskell-cafe/2012-May/101017.html) which includes a comparison of the available bit-array libraries on Hackage. Disclaimer: I'm the author of [bitstring](http://hackage.haskell.org/package/bitstring)
This library is for parsing binary data at the bit level. Libraries like Attoparsec allow you to parse binary data, but you don't get the granularity of working with individual bits. This library maintains its position into the byte string as well as into the current byte. You'd use this library for parsing binary protocols which need finer granularity than the byte level. An example might be a TCP packet, or the one that prompted me to write this was the LZ4 compression scheme, which uses a byte where the first 4 bits tell you the length of literal section to follow, and the last 4 bits tell you the length of the previous match to emit.
I missed the fact that almost all code is in Util.hs, at first sight it looked like an empty skeleton project. It doesn't help that I find github pages terribly uninformative... The bitstring library, providing lazy bitstrings similar to (and based on) lazy bytestrings was also born for such a purpose (finding the chunks of a bzip2 file), though it provides a lower level interface, as that level was enough for the given task.
I don't see that data D m f = D (m (f (D m f)) or the explicitly staged version data D m f = D (m (Step m f)) data Step m f = Step (f (D m f)) is the same as data FreeT f m r = FreeT ( m (FreeF f r (FreeT f m r)) ) data FreeF f r x = Pure r | Wrap (f x) Or maybe I'm missing something in FreeT. In any case, the 'functionality' Atkey is looking for is the associated proof principle, which rests on properly constructed instances of 'EMAlgebra'. It seems you are just trying to finesse the addition of a `Pure` constructor in the base functor by sinking it into FreeF. This seems to muddy the conceptual waters considerably, but I'm not sure.
Hi, I'm the author of this blog post. Many thanks for your feedback. I need to read it slowly and think =)
If your `f` is replaced with `FreeF f r`, you get the same data type Keep in mind that my approach has the goal of being a monad transformer as part of the `transformers` package. The more general F-algebra described in the blog post is not necessarily a monad, nor a monad transformer. You are correct that my approach doesn't give you the benefit of the proof principles, but his approach is a more awkward way to formulate the monad transformers that he describes in his examples. They are completely isomorphic, though, and I myself find that the F-algebra approach simplifies many proofs that I do for the `pipes` library, but the free monad transformer simplifies the code. Both approaches have their uses. 
I think your CPS style encoding of pipes is incorrect. Shouldn't it be: newtype Stream a b m r = Stream { unStream :: forall s . (a -&gt; m s) -&gt; (b, m s) -&gt; m r -&gt; s } Pipe composition is rather trivial once you think of it as just merging two streams. If the F-algebra is just a stream of functors, then pipe composition just merges two streams of functors, giving downstream priority when it's not awaiting, fusing await and yield, and otherwise giving upstream priority. This makes it easy to prove the category laws.
&gt; Second, there's the forall n. notation in front of the constructor T. This is something we get from the RankNTypes language extension. Really? I think you get this from GADTs (which implicitly enables ExistentialQuantification, which is the feature you are actually using here, right?)
&gt; an example might be a TCP packet Which is byte aligned or greater. There's definitely value in bit addressing within certain fields, but in general maintaining bit positions across fields will likely be inefficient with no clear tangible gain. Or this is cool, but not for those examples. 
Because then you get the `Monad` and `MonadTrans` instances. The whole point of the free monad is the monad! If you don't need the monad, then you are correct and then the F-algebra is more appropriate.
The plan is 1. get it working, 2. get it integrated into the GHC mainline. I don't want to get into too much infrastructure. (Git gives me gyp.) This is a project of iPwn Studios. We're still working on the video game "BloodKnight". No plans for commercializing the compiler, but GHC has a BSD licence, so anyone can do that if they want.
&gt; (Git gives me gyp.) Other than a plausible plan to get it integrated into the mainline. And the ability to easily rebase your patches on top of current mainline as the mainline changes. Well, that and the ability to ability to share your patches with other interested programmers who may be interested in helping. Oh, and a nicer viewing interface on github for casual examiners (for github specifically, not git). But other than that, what have the Romans done for us?
I am proud to say that I use git without knowing how it works. (I use a GUI. Click, click!)
Sounds a bit like a script we use at Silk. It creates sandboxes based on the current git branch (and you can override them using an environment variable, if you want).
Read the papers about nix.
man these startups go to such great lengths to avoid using mature tools. these guys were already bitten by their naive desire to follow the new-shiny toy, and now they are trying to build a commit-mechanism in haskell to make the next-shiny toy act like a mature db. i love haskell. haskell has the tools to make this happen...but to make it happen with 1/10 the performance of using postgresql, and with all the future debugging involved with mating a consistency layer at the edge of another tool. in twelve months i expect them to tell us they have just gone with postgresql since it provides a mature, high performance implementation of the frankentstein stack they built its very troubling when startups pick their technologies based on what gets voted to the top of hacker news. notice how you never see posts from startups expressing disappointment with postgres with the result being a return to a nosql db
The people writing their own crappy database on top of key-value stores were never going to consider making effective use of a relational(-ish) database in the first place. If they hadn't used mongo or riak or insert-nosql-fad-crap-here they would have just used mysql as a dumb key-value store anyways. The upside to the nosql fad is that as people foolishly re-invent the database wheel on top of these key-value stores, they might finally get a clue that their ignorance of relational modelling was the problem, not relational databases. I think at some point, some of these people are going to decide to take the time to learn some basic computer science and maybe the state of DB backed application development will be slightly less terrible. Terrible mongo/rails apps are just the logical progression of terrible mysql/php apps, but hopefully it is getting terrible enough that the people making the messes can actually see that they are messes, so they'll have a motivation to learn to do things right.
Not sure about this, but I think that just checking if dependencies are solvable is basically integer linear programming (which is NP-complete).
You need a couple of extra caveats in particular you usually need to ensure that you have a finite grammar. It is easy to generate infinite grammars applicatively if you aren't careful, but if you have a finite grammar, you can use observable sharing to build pretty much any kind of parser you want. I had a project for doing this, but it was named 'parsimony'. Sadly, before I got around to releasing it, someone else came along and dumped a now-unmaintained parsec clone out there on hackage with the same name, and I lost interest in it, and went on to research other approaches.
Stephen, Git should keep your local changes as separate commits. So you basically have two options: * Continually merge from head into your branch by using something like git pull origin master (whilst on your branch) This will have the effect of applying any changesets from master on top of yours * Adopt a rebase based workflow. git rebase master This is a bit more fiddly but has the effect of keeping your commits "on top" of master, largely eliminating merges and keeping your repository looking more linear. This is much like removing a set of patches, pulling and then re-applying said patches. I'll try and dig out some more informative links for you tomorrow or I'll try and implement this in my github account to give you a demonstration. Cheers, Ben
I have a camera, so I should be able to record the session. That said, I recently moved, and I haven't attempted to get together all of my recording gear since I moved. I do try to make as many of these as possible available through youtube, here: http://www.youtube.com/user/edwardkmett I've managed to record about half of the sessions lately. While I won't promise to record it, I will definitely make an effort. =)
Will try 'rebase' - much appreciated
&gt; syntax between the REPL and source code files is slightly different Not really. The only difference is that the REPL has a few additional commands (the things that start with a colon), and the REPL basically treats each line you write as part of a big do block in the IO monad. You can write ghci&gt; x &lt;- getLine And then it will wait for you to type in a line of text, and store that in variable `x`. Or you can use the usual `let` syntax within a do block to bind the value of pure computations ghci&gt; let x = 5 One other important difference is that if all you write is an expression, e.g. ghci&gt; 2 + 2 the REPL will automatically try to 1) perform it if the expression evaluates to an IO action, or 2) wrap it in `print` and then perform it.
Wow, there are some grumpy people in the Haskell reddit these days.
Yes, package constraint solving is NP complete. I'm afraid I don't know enough about integer linear programming to comment on that aspect. Apparently SUSE linux now uses a SAT solver for their package manager. The tricky thing about going with SAT is how to construct good error messages. The new cabal-install solver (as of 0.14) uses a more traditional solver approach.
Thanks! Some questions: * By doing this, am I pushing the changes I got from ghc-master from my local repo into ghc-iphone? * How do I examine my own work relative to ghc-master? 
You really should take some time to read [Pro Git](http://git-scm.com/book), or at least browse through [GitHub:help](http://help.github.com/).
If you grant 'the-real-blackh' write access, then I'll use those repos. psed: will do
Access granted.
v5 is now [on GitHub](https://github.com/ghc-ios/ghc/wiki).
Very helpful, thanks - and by the way - I've done a simple OpenGL example using the Haskell OpenGL package. I'll release that tomorrow.
&gt;(b/c, let's face it, basically everyone at scale treats RDBMes as k/v systems anyway). No, we don't. That's the whole point. The nosql fad is not based on a reasoned rejection of relational databases, it is based on ignorance of relational databases. You are talking about having 50 million users as if that is some sort of issue, or requires you to treat a relational DB as a hash table. That is absurd nonsense. Note that the top TPC-C spot right now is oracle pushing 500,000 transactions per second. That that isn't database transactions, it TPC "order processed" transactions. The system is also running 4 or 5 other kinds of transactions in a pre-determined ratio at the same time. Relational databases scale very well. &gt;I'm not really sure what else to tell you sky-is-falling folks about using something other than Postgres Nobody is claiming the sky is falling. We're simply lamenting the fact that so many people embrace ignorance. You are spending time writing code to do a bad job of what a relational database already does, just to avoid an imaginary scalability problem. &gt;it does strike me as ironic that the Haskell programming language subreddit is coming down hard on the "databases are solved, they cannot be improved" That is a bizarre attempt to misrepresent the criticism. Nobody said databases can not be improved, but rather that having thousands of different people writing ad-hoc partial databases on top of unreliable hash tables isn't improving anything. Why would it seem ironic that some portion of the haskell community thinks math and computer science are actually good things? Why would people using a language known for its links to category theory not also appreciate relational theory?
I second Chrisdoner on the matter, if you have figures, please share. I am a big fan of relational databases (postgresql specifically) mainly because my data is relational and very structured. PG happens to be amazingly fast when correctly used, so I am very happy with that. However, this very much depends on data and use case. For example, there is something in your design that can not be done with PostgreSQL : multi-master with automatic replication and partitioning. This high availability is THE point of Riak in my opinion.
&gt; No, we don't. That's the whole point. The nosql fad is not based on a reasoned rejection of relational databases, it is based on ignorance of relational databases. You are talking about having 50 million users as if that is some sort of issue, or requires you to treat a relational DB as a hash table. That is absurd nonsense. Okay, I'll bite. We were maxing out the IOPS a 24-way x25-e cluster could give us on a single master setup with a traditional database. What do we do next? My stock answer is shard on some key--but if I shard, I can no longer really join across tables b/c related keys may be split. I cannot really run aggregates like SUM and COUNT etc. I cannot really do transactions b/c the data I might want to operate on atomically is residing on several hosts. I *can* do all this (sort of) if I roll my aggregates up in app code, or I use some lock service, but then I'm, again, inventing a database on top of a database. So what's the point of an RDBMS if I can use any relationships? It has typed columns. Okay, that's good I guess. I"m genuinely curious here to discover what unexplored facet of these systems I'm missing. The scaling challenge is *moving beyond a single master*. How do I do `SELECT a.k, b.d FROM users a INNER JOIN posts b where a.id = b.id ..` in a multimaster scenario? And if I can't, what is the point? Relationships and good normalization and transactions and aggregates and etc are the whole neat trick these systems do, which is very neat--but if I can't use it, who cares? So is my ignorance: 1. I really don't need multiple masters? 2. You can do all this with multiple masters? Instead of just accusing me, and individual you've never met on the Internet, of being ignorant, it would be great if you could *actually* support your claim by telling me what it is I'm missing.
Well, Coq and Haskell are imho well beyond the realm of "research languages", at least in the sense that the term applies to Ur/Web.
well thank you for recognizing that my comment tried to not be too offensive or make too many assumptions or make any personal attacks. contrary to what dons thinks above, i'm not out to rip you a new one for no reason. i've personally used key/value stores since the 90s on sites that are in the top-ten of all traffic, i'm aware of the massive performance gains, but i remain a skeptic as to their ultimate value over a mature sql product...even if one is required to obtain higher-cost hardware and gin up a caching layer on your own. to me, the loss of transactions is simply too much. i've "been there" too with writing my own access layers (in c in my case). &gt; basically everyone at scale treats RDBMes as k/v systems anyway this is only partly true. indeed at the tens of billions of rows, k/v will fall apart and you probably need to go column-oriented (and yes i have dealt with a db that went into the tens of Billions of rows) 
You could also use my [operational package][1] which is a bit more general than free monads. data PipeI a b c where Await :: PipeI a b a Yield :: b -&gt; PipeI a b () type Pipe a b = Program (PipeI a b) The `Pipe` type is automatically a monad. [1]: http://hackage.haskell.org/package/operational
&gt;Okay, I'll bite. We were maxing out the IOPS a 24-way x25-e cluster could give us on a single master setup with a traditional database. What do we do next? The first step would be to find out why you are maxing out (assuming you really were). You don't appear to be anywhere close to big enough to be hitting the limits of a mid-range SSD array. I am interpreting "24-way" as meaning your server had 24 drives. How were they configured? How much RAM did the server have? What was the read:write ratio for all this IO? The first step is resolving configuration issues in hardware and software. Second is looking at queries that are creating more IO than necessary, often due to naive attempts to "optimize" by people who think "joins are slow". If a decent chunk of that IO is reads, then obviously improved caching is the cheapest course of action. Also obvious: add more drives (and controllers). I'm already pushing the limits of credibility in calling 24 drives "mid range" (feel free not to jump on me and point out how entry level that is dear readers). You don't need to add more servers to add more drives. &gt;My stock answer is shard on some key That's a pretty bad stock answer. Splitting individual tables is a last resort, not the first thing to consider. If you actually do need to move some data onto different servers, start with whole tables. Move individual tables off to separate servers, replicating the tables that need to exist on both sides. Joins, aggregates, etc all work just fine this way and you can easily scale your write load by an order of magnitude. You should have almost no reads happening already due to caching. If you have complex reports that pull lots of information (and your database is too big to fit in RAM, not sure how many TB you are dealing with) then you want a separate replicated server to run those reports on so you don't trash the cache on the main server. &gt;So is my ignorance: Definitely 2, but likely also 1. Your complaints are of the form: "if I arbitrarily decide the best way to scale is by throwing away relational modelling, then relational modelling isn't helping me any more." That is certainly true, but my point is not that it still helps you when you throw it away, but rather that you shouldn't be throwing it away. Instead of guessing at how to scale, hire a DBA. That's what we're for. &gt;Instead of just accusing me .. of being ignorant The only thing I accused you of was misrepresenting the criticisms of the database hating fad. &gt;it would be great if you could actually support your claim by telling me what it is I'm missing. I'd need a ton more information to give you specific advice relevant to your situation, the above is obviously just general "how do I shot database" basics. The best advice I can give anyone in this situation is "learn". Casually tossing aside 40 years of research by some incredibly brilliant people should trigger warning alarms in your head. It is almost certainly not going to be a case of "this shit is simple, I can write in myself in a weekend". And a warning: I have no idea what you guys do. For all I know you could be only storing a single piece of information and a key-value store is all you should be using. I didn't intend to specifically point a finger at you, the post just sounds identical to the millions of other bubble 2.0 startups screwing themselves over with mongo, and my comments were directed at the general trend of "lets do things wrong because its trendy!", not you specifically.
There are no fully baked systems for this ready to be used (that I'm aware of at least). A number of people have all implemented a blog system to some degree; you can get an idea from the [list of Yesod powered sites](https://github.com/yesodweb/yesod/wiki/Powered-by-Yesod). The Yesod site itself has a blog also (code [available on Github](https://github.com/yesodweb/yesodweb.com)). All that said, it would be nice to have a nice off-the-shelf subsite that provided this kind of functionality. Anyone interested in working on this?
I think Coq is used for software verification in industry.
In case anyone else is wondering where the blog post went to, it's available at [http://devblog.bu.mp/from-mongodb-to-riak-7138](http://devblog.bu.mp/from-mongodb-to-riak-7138).
Your advice is precisely why people are deciding to use key-value stores instead of relational databases. You suggest pretending your DB is a key-value store anyways, at which point it is just a slower (and far more expensive if you are using oracle) key-value store. &gt;our relational db keeps us from polluting the db with garbage or mangling things. When you can't even have a foreign key constraint or unique constraint, that is a pretty silly claim to be making. If all you are getting is "it checks that a number is a number", surely there must be a key-value store that offers types?
Best place to work on this would probably be the Yesod mailing list. Lots of people there have implemented blogs in Yesod, and can give far more insightful input than I can alone. There are also lots of questions of scope for a project like this: * Do we support multiple authors, or just a single author? In other words, is it a single blog, or a blog site? * How do people edit content? For various reasons, yesodweb.com stores all of the blog content in a Github repo, and updates are performed with a commit hook. Another approach is to store Markdown content in a database and update from an interface. * Should commenting be included, or should we defer to a system like disqus? (Personally, I took all comments off of the Yesod blog since I thought it just fractured the discussion between Reddit and the site.)
&gt;i still have constraints on what a row can contain. most key/value stores don't....with a modern k/v store, i can mangle a row (item, etc) at will. If you've already decided "I can let my app handle data consistency" once, it is a pretty obvious progression to decide that again. You are drawing an arbitrary line and saying "I need my database to enforce consistency, but only for this particular little subset of consistency problems, and the rest I will deal with in an ad-hoc fashion in my app". I find the nosql fad to be more logical than this, they are at least making a consistent decision of "I'll do all my data consistency in my app". You are both throwing away relational databases, the only difference is that you are arguing against it while doing it.
this is more or less a blog http://www.pbrisbin.com/ run on this code: https://github.com/pbrisbin/devsite Also, http://yannesposito.com/Scratch/en/blog/Yesod-tutorial-for-newbies/ has you build a super basic blog, as well, there is a blog example in the yesod documentation
My thoughts, in order presented above. * I think for a first try, single blog is the way to go. It seems easier, and once something is working, it may reveal what would work (and what wouldn't) for somthing that could make and manage multiple blogs. * I'd like to use the store-Markdown-in-a-database approach. This meets one of my personal goals of what I would like to learn how to do. * At first, let's allow the use of services like disqus. Then, make the next step of our own commenting mechanism. In the end, the user has a choice of what to use. This is in line with the Yesod Way. I can take a stab at this next weekend, this one is booked. In the meantime, I expect I won't be able to keep myself from doodling a bit. 
You can look at what we've done for the snapframework.com blog. The code for the website itself is [here](https://github.com/snapframework/snap-website), but the real work happens in the snaplet we have called [static-pages](https://github.com/snapframework/snap-static-pages). I have to give the caveat that snap-static-pages was a quick and dirty thing we hacked up for blog functionality way back in the beginning of Snap. Eventually we plan to completely overhaul it because it is nowhere close to meeting the quality standards we aim for with Snap Framework projects. It also probably stretches the definition of "blog system", but is is something that we are actually using right now, and it allows you to work in markdown. So take it for what it's worth. But like Yesod, Snap has a bunch of the pieces, but not a fully baked solution. If someone wants to take this on as a project, we'd be happy to help. Come to the #snapframework IRC channel and we can chat.
Hakyll?
&gt; I need my database to enforce consistency, but only for this particular little subset of consistency problems, and the rest I will deal with in an ad-hoc fashion in my app thats exactly what i am doing. until it breaks me or my data, i will continue moving in this direction, well aware of the risks, issues, and downsides &gt; You are both throwing away relational databases, the only difference is that you are arguing against it while doing it no, i use the features i want to use, not the features CJ Date tells me to
This does seem very similar in spirit to the pipes library as `Temko` now has it. Writing the separate evaluation function that is needed in each case, is in the present case writing `runPipe`. (Even the pleasant tirade against `mtl` at the bottom http://hackage.haskell.org/packages/archive/operational/0.2.0.3/doc/html/src/Control-Monad-Operational.html is very similar to `Temko`'s) `Monad` and `MonadTrans` fall out immediately, if everything works out -- but you don't export the constructors for `ProgramT` so it's hard to see how to write the composition operator `&lt;+&lt;`. I'm looking at some of the examples for a clue. 
Well, I find operational to be even simpler than the free monad approach, but ok.
I agree that the jsif stuff is hideous. Yeosd already has a nice, clean templating system with hamlet (and julius), maybe it could be integrated there? Building a general-purpose javascript-generating framework does not seem like a good long term option unless you go the GWT-route of implementing a haskell-to-js compiler. Essentially, the current implementation always reloads the list content when submitting - is there a way to generate the list html server-side with a hamlet widget (ie, without the "wrapTag "ul" $ jsjoin $ jsfor todos etc" stuff) and load it into the page with js? Ie, have widgets with dynamic loading?
Deifnitely - I already have a large chunk of a chapter on Cloud Haskell written, it's been really fun writing some examples and playing with it. I think you're right about needing some material on how the abstractions work internally too. Thanks for the suggestions, and the compliments!
I'm getting a copy for sure!
Indeed, to construct a monadic value from a primitive instruction, you have to use the `singleton` function. await = singleton Await yield = singleton . Yield So, you first analyze the two arguments with `viewT` and then construct a new value by using `singleton`. It's similar to how you would use `Data.Sequence`: `viewL` to analyze it, `&lt;|` to build it. 
Well, this looks like a problem that would have anyway, no matter the design of our pipe data type. The point is that in order to combine the two pipes, you have to specify in which order you want perform any lifted effects. Moreover, this effect has to be put into the `ProgramT` type via the `lift` function. So, the result will look something like this: combine p q = lift (viewT p) &gt;&gt;= \x -&gt; case x of Await :&gt;&gt;= p1 -&gt; await &gt;&gt;= combine p1 q Yield c :&gt;&gt;= p1 -&gt; lift (viewT q) &gt;&gt;= ... Return a -&gt; ... In other words: the result pipe will have to perform the effect and the pattern matching. &gt; All of this hiding removes the programmer very far from reality. It won't be any easier to guarantee the lifting laws by hand.
I wonder if DPH will be in a working state by then. 
yi is the way to go as an emacs replacement and should make it easier working with haskell code by reusing the parser and other GHC functionality.
There is an argument that Yi could be for Haskellers what Emacs is for Lispers, indeed. I tried out Yi recently. It works, builds fine. Along with Subtext I plan to sit down and try extending it (that's the acid test). Have you tried hacking on Yi? Also, if so, what features does it have from my list above?
siodine added: &gt; I have no doubt that Sublime isn't nearly as popular with the people that would answer a poll in the Haskell community. However, I'm not so confident with people wanting to try out haskell, or new users. The largest potential source of Haskellers is Windows users, and they typically use IDEs. And that's partly why I think Sublime will eventually win out--it acts as those users expect. That is, Sublime is smart enough to allow emac and vim users to use it as they like with extra effort, rather than forcing the largest body of users to use it in an unfamiliar way and have them use it as they like with extra effort. This may be a point of interest. Haskell adoption of course does include tool support, at least to some degree. The barrier to entry exists. Maybe others have something to add on this point.
Does this mean you don't have a need for any of the other features listed above in “Notable features of interest to consider”? All you want is indentation? Perhaps you don't consider yourself an “industrial” (if such a word makes sense) hacker and merely editing text is sufficient? Perhaps this is common in the Haskell community and hence why the IDE feature set is barren. Thanks for writing the long reply, by the way.
&gt;It works, builds fine Not on Windows; just tried installing it again with `cabal install yi -f-vty -fpango` (and *wow* a lot of shit had to be installed; problem building dyre). Didn't work months ago, either.
Correct, I don't have a *need* for any of the other things listed. I would like a couple of them (ghci integration, error highlighting) but I can certainly live without them. I'm not sure what you mean by "industrial", I'm the guy that programs in C because I like C, and does so using tmux, nvi, cscope and ksh for my "IDE". Hopefully that gives an accurate impression of my preferences. I am quite happy with that setup for C, and even for my own haskell code where I indent my code pretty much the same as I would with any other language. But when I am forking something on github to make changes to it and want them to get accepted upstream, it just becomes a pain in the ass dealing with the crazy spacing. All I really *need* is something that solves that problem, without breaking my existing vi finger memory.
This pretty much mirrors my experience, but Sublime has several packages for alignment (http://wbond.net/sublime_packages/community -&gt; search "align"). I think ElasticTabstops is what you want (but it has limitations, not as good as haskell-mode); you highlight code and press tab or shift+tab until it goes where you want.
I added that to a CON on Yi.
There's two sides to the coin, the newbies, and the experienced hackers who are productive but still have problems or ideas. I agree, for newbies, you have to watch them. That's user-experience testing 101. But hard to do from a distance. :-)
Writing programs for humans, not telephony switches ;) Seriously though, there is no clear answer. They're very different tools. You might use Erlang more for distributed programming problems, but without static typing and aggressive optimization, I wouldn't use it for desktop software or individual high-performance nodes. Go and write some programs, and you'll get a sense soon enough. Being expert at both would make you very valuable.
There are two sides of the coin, but I thought the problem was that there's been too much focus on the experienced users. The experienced users are likely not trying to install yi on windows and then ending up in dependency hell, and having to delete cabal and ghc directories in their hidden AppData folder, and then having to reinstall the haskell platform and all their dependencies (only to have it happen again when reinstalling the missing dependencies for their projects!). The experienced users are mostly on linux using emacs or vim, and they're improving upon that usecase. What happens then? You scare away potential new experienced users from other platforms/environments that might improve upon their own usecase.
You should add this to http://www.haskell.org/haskellwiki/IDE when you're done. Also, that page says HIDE is orphaned.
I'd prefer Haskell for almost all tasks. The lack of static typing in Erlang is a big drawback, IMO. 
Practical use is defined by available libraries and supported platforms. I'd say as of now haskell has more libraries than erlang, much more active community and momentum. It is ready for general programming in: - web development (Snap, Yesod, Happstack) - db access (HDBC or native Postgress/Mysql libraries) - GUI applications (Gtk2Hs, wxHaskell) 
One practical advantage is that the type checker keeps telling you what nonsense you are talking -- and suggesting what hope there might be for making sense. The intense beauty of Haskell is another practical advantage; this is very foolishly discounted by people who imagine that they are 'practical'. If I were arranging a workshop I would want the tools that move and astound my workers and take possession of their imaginations.
Refactoring! Even something as basic as function renaming would be a great start. I'm always a bit dismayed when someone claims that an advantage of Haskell is that you can rename a function then let the compiler point you to all the other locations you have to change. That was the technique I used 25 years ago when I was starting out with C! (Maybe those comments are aimed at Ruby or Python users, but still...)
Perhaps. Though it's easier to know Haskell and pick up Rails etc. for a job, than vice versa.
I came here to say this. Reddit is great for discussion but the interface doesn't make a decent job of remembering discussions and conclusions reached. Also in glad this issue has been raised. It doesn't matter how good a language is if it has no tool chain. 
Leksah uses haddock not hoogle for meta data (I suspect the same is true of eclipseFP). If you are using one of the Leksah windows binary installers, make sure you pick the one that matches your GHC compiler (if you are not seeing metadata this could be why). I have tried to make starting a project a bit easier, but it could still be better. We are using the standard Gtk file dialog to pick a directory and I think that might be the source of some confusion (especially for windows users). Here are the current steps I use * Package -&gt; New... * Click on the button in the top left of the dialog to show the location box (if it is not already visible) * Navigate the the directory where I want the project (the parent directory that is) * Type in the name of the project in the location box (this a subdirectory of this name will be created) * Click open * The package editor will now show up * Change any settings I want to customize * Click Save (in the package editor) 
I know it is not a REPL, but if anyone is interested Leksah has a scratch pad (Panes-&gt;Debugger). You can type anything you want in it, press Ctrl+Enter and the current line (or selected text) is sent to GHCi. The results come back in the output pane. It is persisted between sessions and you can easily sent multiple lines of code to GHCi by selecting them before pressing Ctrl+Enter (it uses wraps them in :{ and :} for you). This also works in Leksah's edit panes. You can put samples in {- -} comments inline with the code. Someone reading the code can execute it in GHCi just by moving the cursor to the line and pressing Ctrl+Enter.
Programming Haskell may be a niche (for now), but knowing Haskell, and knowing what you learn from learning Haskell is universally useful.
If you are using the latest Haskell Platform (2011.4.0.0) download. Then you need Leksah compiled with ghc 7.0.4 (http://leksah.org/download.html). If you are using the one built with ghc 7.4.1 then the metadata will not work (this is the most likely cause of the browser issues). Here is a cheat sheet for Visual Studio users that might be useful if you are familiar with that... * Solution =&gt; Workspace * Project =&gt; Package * File/Namespace =&gt; Module Package and Module are the Haskell names for these things and I think Workspace might come from Eclipse. Once the browser is working you should be able to view your modules by clicking on them in there. You can ad new ones by right clicking and choosing Add Module (or you can use File -&gt; New Module).
I find this kind of stuff absolutely fascinating, even though I don't really have the formal background in category theory to understand all of the implications. 
Right or wrong I see a lot of similarities with the Haskell IDE ecosystem as I saw with Python a couple years back. (Python may, probably is still this way, but I've not looked in a while). While there are obviously a lot of good choices, to be able to accomplish a task, especially due to what has already been accomplished by the talented folks around here. We just need something that works. I know I spent a lot of time wrestling with the platforms in python and never really found one I liked for all the flavors that existed. My primary OS is windows and I've kinda gotten EclipseFP to work at one time or another but after an update it is broken again. Leskah, I have gotten to install and work, while I think it is a fabulous effort and probably monumental to pull off, it just doesn't grab me. The integration isn't tight enough to say that it is better for me to wait out the load time instead of just opening up browsers, text editors and a few shells. I never really saw it in action, but I wish I could have seen visual haskell. Seems to me like it could've been the way to go on Windows. Dunno how feasible Xcode is, if at all for Mac. Cross platform is undoubtedly nice, but what about some small divergences for stuff like native look and feel GUI on windows and mac, could attract more bright minds and as a natural progression of a growing community wanting to interact would continue on something like a Leskah and make it as good or better than Visual Studio for Haskell, but cross platform compatible. 
vim has many of the features you mentioned for emacs (works on windows, tag-based completion and jumps, syntax highlighting), and other things are available through plugins.
Hoogle will install on Windows 7. It might even be part of Platform . EclipseFP integration is flaky though. 
I added "works in terminal" to the list of PROS. I agree, that's a nice feature.
&gt; Having just (hopefully) passed the most hated subject of my educational career so far, What exactly did this have to do with the rest of your post?
Vim is a text editor, as is Emacs. They both have extensions and integration which makes them IDEs in practice. I'm not sure what this pedantry adds to the discussion.
The compilation happens from Vim with syntastic when I save a file. I usually just use the REPL to run / test single functions.
Sounds sensible; I'll try to replicate it at some point.
I think you mean "writing programs for compilers, not telephony switches". 
There are probably not enough Haskell jobs in all the world for you to stumble across them without looking but I've certainly noticed that mentioning it on my CV has been an important talking point during interviews --- who's to say it wasn't that which made these companies call me in for interview?
Luite may have some ideas regarding this, both for dealing with sandboxed Haskells on the server-side, and using GHCJS. UHC is a fairly viable option. I had some fundamental problems with it not evaluating thunks properly. My little Haskell could also be used. It's a subset of Haskell. I'm still writing unit tests now. But I plan to whip up a little server that you send Haskell code to and it sends back JavaScript of roughly equal size, this evening. I think the "running it on your own machine" aspect would be cool. Especially as JS; you can play around with the browser which is a lot of fun, especially as a learning environment. With multiple people that could be a lot of fun. For what it's worth, TryHaskell's service could be used to share environments with a bit of JS and use of [the JSON API.](http://chrisdone.com/posts/2010-04-05-haskell-json-service-tryhaskell.html) (See the code loading example, this blog post predates my knowledge of jQuery's built-in JSONP support via `$.ajax`.) In the sense that it is a referentially transparent API, each time you evaluate something you can send along with it a file. See [example here](http://haskell.handcraft.com/). I'm not suggesting this to be used for a serious implementation, but certainly it can be used as a back-end to whip up a trivial prototype while you're thinking of ideas.
I specifically mentionned Hayoo, not Hoogle, because I thought searching by type signature was working in Hoogle only. I've just checked, and I stand corrected. I'm curious, was it working right from the beginning (in Hayoo I mean) ? 
happy with ubuntu, don't care about the web interface, but a build server and ubuntu PPA system would be utterly fantastic for distribution.
It doesn't have very good vi-input emulation, and I am very accustomed to vi so it breaks my finger memory. I end up making weird messes as vim moves your cursor around when it shouldn't. Vim's "vi compatible" mode doesn't make it more like vi, it just disables a bunch of vim features. It doesn't remap keys to be correct or anything like that.
Just today I installed the latest Hoogle on Windows 7. It required installation of curl and wget via cygwin, but after that everything went smooth.
I used to hand maintain my project and .cabal file. I recently decided to move my project into EclipseFP. It was literally a matter of copying it in although I used sensible names (not calling the cabal file for foo project bar.cabal). I've been reasonably impressed so far. It has lots of "don't do this! Do this instead!" messages which is useful for me because I don't know what I'm doing. (Eta conversion, what is this?) Outright error reporting could be better but really that comes back to GHC having a good error cryptography module.
Not sure why you are downvoted. It is much more sensible to import Vi into an IDE than to use Vim as an IDE.
I think just running locally is fine at first. My use-case would be more as a private code notebook with integrated graphics. Eventually, more work could be done on security architecture to make it possible for it to serve as a reliable hosted service. But that stuff is boring and complicated and doesn't look awesome. So better to do the awesome stuff first, and then figure out how to open it up for other use-cases down the line.
Yes, non-devs are potential users as well. And there are are other factors besides just being a dev; some devs can be ruled out *a priori* because they would never have any reason to learn haskell, but that doesn't negate my point. Once excluding the non-likely devs, you're still going to have more potential devs on the Windows platform--that's just how enormous its user base is relative to linux's. About your non-devs point: Devs are the important potential users for the growth of the language (it gets us things like yesod and snap that draw in other users), and devs are much more likely potential users for a language like haskell (rather than python because of the learning curve). Ignoring the whole dev/not-dev thing, the most users exist on Windows, too. The point is the largest collection of potential haskell users exist on the windows platform, and that hasn't been well exploited.
 At SeeReason partners, we use the autobuilder to automatically build Debian packages for all the libraries and apps we care about: http://src.seereason.com/autobuilder/ It can pull source from existing apt repos, darcs repos, hackage, etc. It can build against multiple distributions, can automatically apply local patches to upstream source code, and a bunch of other useful stuff. The packages can be published in your own apt repository. It includes support for public and private repositories. When getting source from hackage, it will automatically use cabal-debian to debianize the package: http://src.seereason.com/cabal-debian/ For Happstack based servers we used happstack-debianization to make the packaging simple. You just set a few variables and call happstack-debianization-install in your debian/rules. The install scripts take care of configuring apache, etc. http://src.seereason.com/happstack-debianization/ Not sure if you'd be interested in leveraging any of this. I saw you mentioned a desire to build real distribution packages.
&gt; What does the Haskell community think? Do you think that this is a path worth pursuing? Definitely 3, with an intent on 4. Compare [tryRuby](tryruby.org) with [replit](repl.it). The user experience is incomparable. Add to that the removal of needing a charity server to run the code and it's no contest. It would also help to structure it as a web app, if you weren't going to do that already. Web apps have much lower barriers of entry then other software. As for how to execute the haskell that is beyond me, given that even though there's an llvm-&gt;javascript compiler, ghc compiled code requires a sophisticated runtime environment. There's ghcjs and uhc, as chris mentioned. The ultimate solution is ghc implemented in js but I don't really know if that is possible. It might be feasible to get the runtime system in the web browser as a plugin/browser extension that one could download. From the [mozzila](https://developer.mozilla.org/en/Plugins) website on plugins: &gt; Plugins can be written completely from scratch using C APIs (usually in C or C++) or they may be built on a plugin framework such as Firebreath, JUCE, or QtBrowserPlugin. There are also some code generation tools that may be helpful. Being able to use C could mean you can use the full haskell run time environment. Also, although plugins are traditionally renowned for never being downloaded (adobe flash being the exception), given the target audience are programmers they're probably more likely to download a browser plugin to do some code then the average person when trying to view a video on the web. I've never developed a web browser plugin, but apparently NPAPI (Netscapre Plugin API) is supported by [many web browsers](http://en.wikipedia.org/wiki/NPAPI#Browser_support): * Web * Google Chrome * Safari * Konqueror * WebOS Isis browser * Mozilla project applications, including + Camino + Firefox + Mozilla Application Suite +SeaMonkey * Netscape Navigator and Communicator * Opera * Odyssey Web Browser on MorphOS * Internet Explorer up to 5.5SP2 Chrome, Safari and Firefox being the most important ones there. Maybe you do have a shot at full haskell in the browser, I don't know. It may be more then you signed up for though.
Thank you for the links. I did start trying out cabal-debian a bit already, but was not aware of the other two tools you linked to. I had some trouble building cabal-debian, but I haven't spent much time on it yet. I'll give it another crack next week.
The stylistic warnings come from HLint (http://community.haskell.org/~ndm/hlint/), which is automatically integrated with EclipseFP but very easy to use with anything else as well, or standalone. P.S.: Eta-conversion means converting "\x -&gt; f x" to "f" (eta-reduction) or back (eta-expansion).
just writing this here so hopefully you will see it - i recommend putting a paypal link or something on the cc/html page for the book. i want to make sure you get some $$$ out of this, and i would much rather paypal you the $39 than pay oreilly and then hope you get a sliver. 
It is said there will be another one then it will be over.
Aye I worked out what eta-conversion was. When I first started using EclipseFP it was confusing.
That's incredibly odd. [POSIX says](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/vi.html#tag_20_152_13_69) this is the correct behavior for the named registers but not the unnamed register, yet both vim and [original vi](http://ex-vi.sourceforge.net/) move to the end of the pasted text for both named and unnamed registers (unless the register's buffer is in character mode but contains multiple lines) on my system. I could see that getting annoying very quickly though.
Leksah is built on top of Yi, is it not?
But option 3 would let you do that, right? You'd just need to configure your own security policy on accounts? Set up a VM and serve the site from there (under an account with appropriately restricted permissions), and then give logins to users, with the understanding that malicious enough ones could cause some trouble on said system... Basically no more dangerous than handing out shell accounts, right? And the worst case, some talented griefer (unlikely if you're giving accounts to kids and teachers) manages to screw up the VM.
Well sure, but having done so, you could serve webpages to anyone else you feel like anyway, right? My reading of Option 3 is just "ignore the security issues for now and tell people not to allow remote access unless they're Real Sure they know what they're doing".
implement url module. I want to do things like import http://someblog.com/some/literate/haskell.html as Foo the page won't be in .lhs but human readable literate haskell (humans copy-paste to .lhs file). the server would export some url as haskell module, too. given that I was working on /some/haskell/file.hs in the browser connecting to localhost, i want that module to be automatically available to the public. others will import http://myhost/some/haskell/file 
tryruby is like tryhaskell, it takes your string command, networks it back to a server, executes it in a sandbox, then sends the string results back. repl.it converts ruby to llvm and llvm to javascript, executing it on the client in the browser. If you were to run a massive computation the server code would execute faster, but the bulk of commands, even at a 10x the execution speed, will have finished well before a ping has bounced from the server. The result is that repl.it is snappy and immediate and doesn't require a server while tryruby/tryhaskell takes at least a second for something as trivial as 'x+1' and consumes server resources that somebody has to pay for. The constant factor of network communication delays the feedback.
&gt; The result is that repl.it is snappy and immediate and doesn't require a server while tryruby/tryhaskell takes at least a second for something as trivial as 'x+1' That depends on your connection (see below). On a normal broadband connection TryHaskell feels immediate (TryRuby is a lot slower than TryHaskell). I agree with the point that network latency is a factor, though. $ ping tryhaskell.org PING tryhaskell.org (109.74.197.248) 56(84) bytes of data. 64 bytes from wiki.chrisdone.com (109.74.197.248): icmp_seq=1 ttl=50 time=49.6 ms 64 bytes from wiki.chrisdone.com (109.74.197.248): icmp_seq=2 ttl=50 time=50.2 ms 64 bytes from wiki.chrisdone.com (109.74.197.248): icmp_seq=3 ttl=50 time=49.9 ms $ time curl 'http://tryhaskell.org/haskell.json?method=eval&amp;expr=1' {"result":"1","type":"(Num t) =&gt; t","expr":"1"} real 0m0.134s user 0m0.008s sys 0m0.000s (Seems to be 80ms~ of constant factor + whatever the network latency (ping) time is.)
As an Emacs user, I'd also consider Yi an eventual Emacs replacement. Would Yi unify Emacs and Vim users at last? ;-)
Added. And substantially refactored some of the ide/editor pages, esp. Emacs's.
Wow, repl.it is really cool. So that would option 4, which is beginning to sound less wacky than I thought. Browser plugins are an interesting idea, but trying to bundle GHC as a browser plugin is probably beyond the scope of this project.
Yes, I'm definitely looking into the UHC approach most closely myself... so it may turn into a long-term goal to merge these things together, but we'll perhaps be going in different directions in the short term. Running the animations and games via streaming from the server works, though as you say, it's rather slow. But I will say that this year, anyway, it worked fine in the sense that students were able to get a good feel for their games and how they played; I did then go and build them with the OpenGL backend for Gloss to demonstrate at the final end-of-year show.
&gt; users should be able to restrict domains they allow to load from, say, hackage.haskell.org, localhost That's what I meant. Otherwise, we might end up with inscrutable dependency chains like "http://hackage.org/foo" imports "http://hpaste.org/bar" imports "http://shady.com/baz" imports "http://evil.xxx/botz". &gt; Now, haskell file is like a hypermedia document with link to external resources! No more writing dependencies in .cabal file manually. The more I think about this, the more I like it. We'll definitely take that into consideration, thanks a lot! 
&gt; But that stuff is boring and complicated and doesn't look awesome. Indeed. Then again, part of the summer of code is to make boring stuff happen. Still, we want to aim for maximum utility.
&gt; But I plan to whip up a little server that you send Haskell code to and it sends back JavaScript of roughly equal size, this evening. Nice! Any luck yet? We'll definitely look into supporting multiple backends (local machine, TryHaskell, ...)
what i miss is one that truly leverages the fact that GHC is so pluggable that it can be used for IDE making.. i guess the IDE needs to be written in haskell for that, but then a lot of GHC routines can be used to make IDE features i guess.. (stuff like syntax highlighting, refactoring, debugging, etc.
From their faq: "All code processed by repl.it runs entirely on your computer, with no server-side evaluation. It uses language interpreters written in JavaScript to execute your code and keep track of state. Some of the interpreters (like BiwaScheme) are originally implemented in JavaScript, while others (like Python) were compiled from their original lower-level implementations using Emscripten."
The problem is that it does, e.g., python via emscripten which is just llvm -&gt; js. So even assuming you can get all of ghci to llvm to js, then you just have a boring ghci repl session in browser. Adding the cool interactive toys to it is then much harder than if you're writing an entirely different frontend to the ghc api. (and suppose you just compile the ghc api to llvm to javascript -- emscripten still doesn't expose a good way to, e.g., call it as a library from js. if you could do that, then potentially that route would be pretty awesome :-))
This was already posted to this reddit [here](http://hackage.haskell.org/trac/ghc/wiki/Status/May12).
I didn't understand until you mentioned a REPL. And now I do: so many times I have to comment out half of my module to figure out which parts actually cause the type errors, and then having to test everything with `:t` in GHCi. So thanks :)
Oh, I wish I could've been there. Too bad there're no slides available...
If you've ever commented out something and put "undefined" to get it to typecheck while you debug another part of your program, then instead you can use -fdefer-type-errors
&gt; This is what the bottom value undefined is meant for. Precisely this. But it is annoying to have to comment out swaths of code and replace them with `undefined`. Or sometimes I will change a type synonym, and fix up a few functions accordingly, but not all. It's annoying to have to comment out the other functions I haven't fixed yet just so that I can check that what I have done so far is well-typed.
_Twice._
Sounds about right. Of course, in the browser, even for AJAX, browsers may keep the connection open with a Keep-Alive if supported. I haven't checked that, but that would reduce overhead of further requests. Not a big deal, but still, interesting.
*Defer Haskell type errors now su*`Exception: Deferred compile error:` `top-level definition already defined elsewhere in Reddit.Haskell.hs:2:0`
Sounds like they're replacing wrongly typed expressions with undefined (or some error).
It wouldn't run. When you use -fdefer-type-errors, your code is still type-checked at compile-time as usual, but when GHC encounters a type error it replaces the erroneous term with a dummy term that always raises the error. In your case, you can imagine your example as being transformed to foo :: (Ord a) =&gt; a -&gt; a -&gt; IO () foo a b | a &lt; b = do error "Main.hs line 2: no instance for (Show a) in ..." print "left" | otherwise = print "right" main = do foo 24 32 (That's not quite how it [actually works](http://dreixel.net/research/pdf/epdtecp.pdf), but you get the idea.) To be able to notice at runtime that a particular type a is an instance of Show you'd need a lot more apparatus. You'd have to * carry types along with values at runtime * keep a global table of all the instances in scope * add a dependency solver for class constraints to the RTS none of which is likely to ever be done in GHC.
Where might I see the pushout/pullback categories expressed using haskell?
Sunroof looks like a nicer HJScript, but is it that different from it? Someone remarked about HJScript that they kept trying to do things like `map f xs` which generated code like `[f x1, fxN..]` rather than an actual loop... I admit I only skimmed the linked article though.
Oh, you're right. Only one of these calls to print is an error. Edited.
This is quite nice.
Could we get a bit of a writeup on how it works? A quick perusal reveals that it's basically a core -&gt; stg -&gt; js pass + external libs for support.
Quite cool! This assumes that that "monad" you want to reify obeys the monad laws. This is often the case, but not always. Still, it it looks like an excellent trick. I will put it in my bag. :)
I saw the question and thought, it might be possible, but it's too much work to figure out. :)
I wonder if this could be implemented using the continuation monad. Something like: js_call :: Sunroof a =&gt; String -&gt; [JSValue] -&gt; Cont (CompM String) a js_call nm args = cont $ \k -&gt; do a &lt;- newVar code &lt;- k a return $ assignVar a ++ nm ++ "(" ++ showArgs args ++ ");" ++ code compile :: Sunroof a =&gt; Cont (CompM String) a -&gt; CompM String compile m = runCont m (return . showVar) The only reason I can think of to want to use `operational` is if you want to do complex inspection of the `ProgramView`, f.e. eval (Prim1 :&gt;&gt;= Prim2 :&gt;&gt;= g) = do ... Are there other reasons?
Agda/Epigram-style sheds nicely subsume both the flag and this use of `undefined`. Based on the mailinglist discussions, I'm hoping this flag is just a waypoint for implementing sheds in GHC.
The main difference is HJScript is for *generating* Javascript, and Sunroof is for *executing* Javascript, aka a FFI to a browser session. It may be possible to build a version of Sunroof on top of HJScript; we're looking into this.
Have you looked into jmacro for generating js? Tools like sunroof were exactly one application I had in mind when writing it. 
Would it make more sense to compile Haskell to LLVM and then Emscripten the byte code to JavaScript? It might be a more focused use of our collective efforts. 
You're not writing Haskell as browser code. It's Haskell to Javascript. You're writing haskell converted into javascript for your browser code. I'm trying to understand why someone would spend the time to support javascript in this way, when our collective efforts should be spent moving *away* from javascript.
are these tools anything more than interesting parser demos? is the code that comes out actually deployable?
You misunderstand the situation. This is not "supporting JavaScript". This is "supporting Haskell" in a JavaScript-infested world.
My, arn't we a bit cranky today?
[See the “Others” section](http://www.haskell.org/haskellwiki/The_JavaScript_Problem).
It's basically the same as GHCJS. Luite tried it out. It has some differences, but has the same general way of compiling and the resulting huge output.
I'd be interested on seeing your progress on this, let me know when it's ready for release.
That's just not going to happen.
No, Word32 is kind of broken when compiling on a 64 bit system ATM.
Last time I tried GHCJS, (which was several months ago, as the stand alone build instructions currently produce a binary that dies with a "can't find ~/.ghcjs/ghc-7.4.1somethingsomething/settings" error message - any suggestions on how best to get around that?) the output was in the ballpark of 10 megabytes of JS for the example and its dependencies. For my small example (https://github.com/valderman/glosie/tree/hsglosie), the Haste output is 53 kilobytes, 25% of which is the RTS and the vast majority of the rest is dependencies, so I don't know if I'd call that huge, particularly considering that UHC (the only other HS -&gt; JS compiler I've actually been able to test lately) does the same "several megabytes of dependencies" thing GHCJS used to(?) do.
Ah, I'm just going off what Luite said. He got a 10MB output from haste, but may have had some trouble getting it to behave. Look forward to the write-up.
Made a start, but I had a bit of a “weekend off” this weekend after merging and supporting my various open sauce projects. I'll post on Reddit when I have a service to play with. ☺
My [operational][1] package takes care of this by adding an explicit `bind` constructor internally. That's why the `Program` type is abstract. (This is similar to adding an explicit `++` constructor to lists.) [1]: http://hackage.haskell.org/package/operational
I don't really see why one is easier than the other. All the concepts map one-to-one. Converting one to the other is almost completely mechanical. operational has the advantage of having handling of return together with the handling of operations, and the continuation monad has the advantage of having less code (no extra data-type and creating singletons). So I guess it is just what you're used to.
Functional Programming, taught by Stefan Kahrs and Olaf Chitil. Good lecturers, horrible, horrible language.
Hej! Very cool work. Will you give a presentation of your thesis at some point?
think of js as the assembly language of the browser vm. the quality of js will then be as good as that of the source code you compile to get it.
The plan is to hold the presentation before midsummer, before the entire audience disappears on vacation, unless Koen decides that "this is crap, you'll have to do it all over" the next time I see him. :)
Please create bug reports if you have issues with EclipseFP, I'll look into them. The installation page http://eclipsefp.github.com/install.html does mention that you need extra tools for hoogle to work.
&gt; So I guess it is just what you're used to. Yes and no. It takes significantly longer to come with the `Cont` version from scratch, that's why I choose the other route. As a case in point, try implementing the example you mentioned: a monad with two operations `idempotentInc` and `flush` such that idempotentInc &gt;&gt; idempotentInc = idempotentInc but executing `flush` will allow you to increment a counter again. I can mechanically write a `Cont` version once I have come up with a version using operational, but not the other way round. 
GHCJS does support FFI, the Test example uses it: https://github.com/ghcjs/ghcjs/blob/master/examples/main/Test.hs the scheduler does not use explicit stacks, it does use an array of waiting threads (function closures): https://github.com/ghcjs/ghcjs/blob/master/rts/rts-trampoline.js I haven't done much with threading yet since i've only been contributing to GHCJS for a short time, but the goal is of course to support as much of the Haskell forkIO lightweight threading as possible (for example when accessing remote files on the server, where under the hood the RTS uses asynchronous javascript calls with callbacks, but to the user it looks like a regular blocking haskell call that can be forkIO'd).
&gt; GHCJS does support FFI, the Test example uses it: That's why I was surprised to see the readme explicitly state it doesn't. Keeping documentation in sync with code is great fun, isn't it? :) &gt; the scheduler does not use explicit stacks, it does use an array of waiting threads (function closures): I see. I'm a bit concerned about what continuation returns do to the JS call stack though, at least for code that runs for a long time. Regardless, it's a great feature!
Cool! Thanks for posting.
Yes and no. It looks like ji is focused on controlling the client from the server side. We have a library for querying/manipulating the DOM like ji: Kansas Comet (a comet plugin we wrote for Scotty). Sunroof is built on top of Kansas Comet, so instead of roundtripping to the server whenever we need a client-side value to continue the computation (which has obvious latency issues), we can offload the whole computation to the client via compilation.
Is that in the tail recursive or the general case? With tail recursion, everything is obviously quite fine; what I'm worried about is the call stack growing with every return from a call not in tail position.
I think it works in both cases, but I'll check later since I'm not 100% sure. If it doesn't, it's obviously a bug that needs to be fixed. With the GHCJS RTS being much more complex than Haste's I expect there to be more bugs, and probably some performance problems in the beginning, so getting that test suite up and running is really important to me :) 
&gt; It needs to happen soon. Excellent! I take it you've nominated yourself to undertake this endeavor? I'll be looking forward to your contributions. Seeing how Google, Mozilla, Microsoft, and Apple are all the best of friends and love coming to a consensus, I'm sure your project will be a shoo in. In the mean time, I'll be halting all of my client-side development until you feel comfortable with your first release. Take your time. We don't want to rush you the same way Netscape rushed Brendan.
I guess I'd do it like this: idempotentInc = cont $ \k c b -&gt; k () (if b then c else c + 1) True flush = cont $ \k c _ -&gt; k () c False runCounter m = runCont m (\a _ _ -&gt; a) 0 False Not very readable, CPS style rarely is, and without compiling it I don't trust that it actually works.But does the operational version improve that much?
Great question and great answer. I've been wondering this myself and it makes a lot of sense.
The main problem with melting OOP and statically typed FP is that OOP is inherently type unsafe. Inheritance, in particular, permits unsafe down-casting (see the ArrayList class before java generics). Generics/Functional polymorphism is the way to go for function polymorphism. OOP polymorphism is fine for unusual cases like heterogeneous lists (even if FP techniques exists for this), not for function polymorphism/re-usability. Interface/type-classes are good and mixins (see Scala, Smalltalk or Ruby) are a good alternative to class inheritance (while not really different from interfaces and type-classes). What I see in your project is Haskell with a class syntax (object.method, this is often fine) mixed with the ugly and verbose Java syntax. I know that you need to build a really terrible, oppressive and stupid syntax to attract Java programmers (otherwise they will not take you seriously, cause more lines is more security/power in their brains), but I don't think it's good.
I haven't fully understood how this works yet, but I must thank you already. I admit I've been waiting for you to solve the finalisation problem :) This seems to be a very clean and elegant implementation. And it comes with real documentation! Very nice.
By all means let me know what you had trouble understanding when you finish. I revise the documentation each month based on feedback I get from you guys.
horrible, horrible how?
The metadata thing reminds me: I kind of wish Hakyll and Gitit would use the same convention for metadata headers. I think Haykll uses --- on both top and bottom.
Thanks for the great documentation! One minor nitpick, clicked links on your page are pretty close in color to the rest of the text so it's hard to find them again.
I was wondering if that fixed the asymptotics or not. Hence why I used the weasel word 'often'. ;) I should have guessed (or calculated) that it yielded the same speedup. We actually use (more or less) the operational approach in scalaz for the Free and Trampolines types you need to use to build up decent monads in scala. There, if you don't trampoline a monad you wind up blowing the stack, because flatMap can't make a self tail call, no matter what you do, so you wind up accumulating frames and blowing the stack the moment you try to write non-trivial monadic code. This led to a raft of problems in our compiler until we figured it out. https://github.com/scalaz/scalaz/blob/master/core/src/main/scala/scalaz/Free.scala
Gitit follows YAML here. YAML uses `---` as a prefix and `...` as a suffix. 
I have a few other posts to finish up first but it's in the works.
Nice job! Is it slow because of the server connection or because it's actually doing work? I suspect a bit of both. I know Gitit supports a cache.
I believe it was the fact we were thrown in the deep-end of it. I just didn't find it a very flowing language, perhaps it was me but it took forever to get the syntax and overall feel for it.
Good job. Now somebody should port the front-end too. I personally like the front-end of the official Haskell wiki. 
I think "how do you solve the finalization problem" and "how do you implement it" are separate questions. It's not clear the solution would be any different if it were implemented "directly"; put another way, maybe it just happened to be convenient or elegant to implement the new in terms of the old. I think the theoretical work on how to handle finalization while obeying the category laws is going to be interesting more widely than just this implementation and is definitely a valuable contribution. That said, like you I'm missing a rationale for why it makes sense to expose both Pipe and Frame. Which doesn't mean there isn't one, but I don't know what it is. Programming with Pipes might be somewhat simpler, but does that alone justify it? Are there real-world use cases where you don't need to handle finalization or the rest of the "awkward squad" but benefit in a meaningful way from the simplicity? I'm just guessing, but maybe one of Tekmo's goals is that the implementation itself should be simple elegant and understandable, and in fact that users should understand it; that it's in some way embarrassing or at least nonideal if users of your library can't understand how it works. And maybe keeping Pipe around as a simpler base to build upon and the copious documentation of what sometimes feels like implementation detail is meant to further this aim. I'm not sure I agree with it though. It's very important for a library to have a clear and simple and elegant and understandable \*semantics*, so that you know what it does and can reason about it and have an intuitive conceptual model to think about it with, but the actual implementation being simple feels like something merely nice to have. If implementing an elegant semantics requires an ugly implementation, I think you just say "so be it, that's life" and suck it up. That's what libraries are for, after all: to hide complexity behind an approachable interface so that users don't have to think about it. That's all a longwinded way of saying that unless there's a compelling reason otherwise, I would also like to see the interface unified around a single Pipe type (which would be the current Frame, though the naming isn't very significant) with the implementation remaining abstract. But I'm open to arguments why that's wrong.
...and it seems Tekmo just answered my question while I was writing this :)
I was going to ask about parametrized monads maybe being a step too far, being another advanced concept for people to learn and even causing the loss of do-notation barring inconvenient GHC-specific workarounds, so I'm glad you're already taking that into consideration. If the choice really is between the current situation on one hand and parametrized monads on the other (you being the expert in your own library I have no reason to doubt it) then I'm not at all sure I would choose the latter. Just because something is more firmly grounded theoretically doesn't automatically imply that it's going to be easier for people to understand and use... just the phrase "parametrized monads" would probably scare a lot of people away. I'd be interested in some examples of other things you could potentially do with pipes besides IO if you don't mind listing them.
It feels pretty fast here in the Netherlands. The homepage's initial HTML is delivered in around 170ms. After 900ms `DOMContentLoaded` fires, and after 1.4s the load event. Most other pages feel a bit faster.
Great news, thanks!
niceone! i heard some sounds from within mediawiki that did not speak very favorable of php -- maybe they can port it to haskell, using gitit as the basis :)
Probably, using the same tricks for finalization, but I want to try my hand at it first before giving a definitive yes.
Thanks for the explanation. I can see the promise here: especially with your reply to my other comment regarding close/bindClosed/reopen the shortcomings of the current approach are apparent. But I also see the peril. I have in fact encountered the coroutine library before (I'm subscribed to the Hackage RSS feed) and had filed it away under "another of those interesting not-so-easily-comprehended libraries with too many type variables in it". Maybe better design and copious documentation can alleviate that, I don't know. It definitely feels like there's a big tradeoff to be made between friendliness and (type) safety, and parametrized monads and session types feel like a pretty decisive step toward the latter. ...it occurs to me that different problems are being solved. The big hole in the Haskell ecosystem right now wanting to be filled is how do you do I/O with resource handling in a convenient, intuitive, composable and reasonable (pun intended) way. Session types help with that a little (expressing properties about open/closed input and output ends), but mostly they're solving the separate problem of "how can I prove more things about my I/O handling code at compile time so that I can be sure it won't go wrong at runtime". You acknowledge more or less the same thing so I'm not adding anything new. Have you considered separate libraries for the two use cases?
That's amazing. I'd love to see more on that sort of LLVM / GHC-primop hacking. Especially so we can figure out a good way to make it less hacky.
Neat. However, I would like to see numbers, to see how much faster this trick is than a standard FFI call...
Hooray! Thank-you! Haha, this opens up a tiny bit more seamlessness. Makes you wonder if Gitit could be repurposed into an online blog editor
Only the Haskell using subsets of those users. They'll probably be burned as heretics by their respective clans.
&gt;That's nonsense. Ask some of the most profitable companies out there. What "most profitable company" thinks javascript is a good language? Google is the only big profit maker I can think of that makes extensive use of javascript, and they sure don't like it. They wrote their own java-&gt;javascript compiler, and are trying to get a replacement language (dart) added to browsers because javascript is so bad. &gt;Perhaps you should take a deep breath and actually learn JS. Then you might see that it's not the language itself that is the root of your problems. Perhaps it is you that needs to do the learning. Javascript is on par with PHP, it is an absolutely terrible language.
Looks fun! Does seem similar to polymorphic variants, though. Also kind of defeats the purpose of exhaustivity cheching and such.
Depends what you are trying to do. The core reform library contains nothing HTML specific. The 'view' type can be whatever you want -- even (). But, the model of 'collect all the values and then try to validate them' doesn't seem to be that useful in many places aside from HTML. If you can explain your use case more, I could try to tell you if it might be a good fit or not...
[Done](https://github.com/dag/vim2hs/blob/master/plugin/offside.vim).
Yeah, my usual approach is to use the Church encoded version while it can be kept ephemeral, and then convert to Free when I need repeated access, for say interpretation at the end.
Well, I was suggesting that it is useful to have a library provide a part of the form, and the user to provide another part. For example, an authentication library needs to check that the username is not in use, that the password confirmation matches the original password, etc. But the person using the library wants to have a lot of flexibility over how the login form is actually displayed on the page. In reform, the authentication library can provide all the validation code, and require that the user provide a Form that satisfies the proofs. So, the user gets complete control over the appearance of the form, but doesn't have to worry about how the username is actually validated, etc. The problem with: validDate :: Date -&gt; Bool is that there is no record of that function being applied. So if the user gives you a, SimpleForm Date, you have no idea if that form is really generating a valid Date or not. If not sure what your point about the javascript changing in the date-picker is. It is true that when you change the javascript portion, you need to change the Form to match. But those two pieces would be provided as a single unit by the date-picker author. Anyway, this is exactly the sort of thing I want to explore in detail later. (By, in detail, I mean, working code examples). There is too much hand waving and places for misinterpretation at the moment. But if we try to nail things down it code, it should become much clearer. I want to do the comparison because I don't fully understand what the differences are, and what will actually work in practice. The only way to find out is to really do it.
Yes but for instance : newtype State s1 s2 a = State (s1 -&gt; (s2, a)) is very akin to the type Arrow arr =&gt; arr s1 (s2, a) So why couldn't Arrows be a replacement for parametrized monads?
&gt;In any case, besides Google, companies that certainly make money on it include Facebook, Github, &lt;insert whatever webapp money maker here&gt;. It may not be the perfect language - but certainly isn't worthless. Those companies don't make money on javascript, they use javascript incidentally in the process of making money. That is like saying whatever brand of floor cleaner they use is of high quality by virtue of being used by profitable companies. If any other language were embedded in browsers they would be using it, they didn't choose javascript, they are stuck with it. That is not an endorsement. Google is the only one in that list that genuinely needs some kind of browser embedded language in order to do portions of their business, and they obviously aren't thrilled about it. I don't think characterizing one of the worst languages ever created, with absolutely no redeeming qualities whatsoever as "worthless" is particularly outrageous. Just because people are forced to use it doesn't make it worthwhile. Rather than take the typical php/mysql route of "it is possible to have a successful company in spite of using php/mysql/javascript, therefore they are just as good as anything else", how about expanding on what exactly makes javascript good?
You might check your work against: http://www.haskell.org/haskellwiki/Prime_numbers 
Roughtly twice as good performance is expected, as you halve the amount of comparisons. If you had carried that thought a little longer, you would've found that you can actually make that the square root instead of half, and so you have the performance squared as good!
That is really neat. I googled around to see if somebody made something similar for natural deduction instead of the sequent calculus, and the closest I could I find is [this "Domino on Acid"](http://www.winterdrache.de/freeware/domino/index.html) page that links to a Java applet implementing a game for natural deduction of the implicational fragment of propositional logic with bottom. But it looks quite a bit clunkier than this sequent calculus page, and doesn't do the quantifiers. Which leads me to wonder if one could adapt the sequent calculus game's presentation to natural deduction—and from there to the lambda calculus.
Another tip: instead of `if-then` expressions, learn about pattern matching. If you're serious about learning Haskell, a great place to start is [Learn You a Haskell for Great Good](http://learnyouahaskell.com/).
I was very glad that I didn't have to implement alpha-conversion and handling of binders in this demo ;-)
 factorial :: Integer -&gt; Integer factorial n = if n &gt; 0 then n * factorial (n-1) else 1 main = do print "Enter a Number" numS &lt;- getLine print (factorial (read numS)) 1. Replace the first `print` with `putStrLn` --- you already have a string, you don't really want it output with extra double quotes. putStrLn "Enter a Number" 2. loudnclear suggests you use `$` instead of so many parentheses. So you might replace the last line with print $ factorial $ read numS or print (factorial $ read numS) or print . factorial . read $ numS *if* you think it looks clearer. (Look into what the `.` operator does as well.) 3. seepeeyou suggests you learn about pattern matching (and guards, I presume) instead of if-then-else. (There's nothing wrong with if-then-else, but pattern matching and/or guards are often clearer.) factorial n | n &gt; 0 = n * factorial (n-1) | otherwise = 1 Although in this case there is a non-recursive alternative that is clearer still. factorial n = product [1..n] 
Specifically: http://codereview.stackexchange.com/questions/tagged/haskell
There's lots of (apparently) ad-hoc weirdness there ---e.g., "open abstract" vs "empty extensible variants"--- but other than that it looks like fun. I'd love it if Haskell had a consistent way of distinguishing open vs closed types, type functions, type classes,...
You do know they're essentially the same, right? Sequent calculus just uses some extra syntax to ensure well-formedness of natural deduction proofs (namely that assumptions are discharged appropriately and not used after being discharged). I've always found sequent calculus a lot easier to understand and to teach because it doesn't rely on meta-theoretic notions of well-formedness or metavariables to capture "any proof goes in here". With sequent calculus the rules tell the whole story.
This post was very timely for me, so I really appreciate the effort that went into this. I'm exploring Kohlenbach's Applied Proof Theory book when I can, and while I'm still in the early chapters, one of the most difficult parts for getting started was just learning all the notation. This kind of thing is definitely not in my comfort zone mathematically speaking.
Finally someone who shares my teaching philosophy when it comes to monads. It is so easy to teach people when you start with practical examples, not theoretical, opaque models. You don't learn calculus by starting with the Fundamental Theorem of Calculus and then working towards the practical. You learn by summing up rectangles under a curve then later abstracting to definitions. Similarly, a student doesn't learn monads by starting with category theory and definitions of bind and return. They learn by starting with some practical examples of maybe, either, and lists such that by the time you introduce them to the definition of a monad, they've practically invented monads themselves. Edit: Grammar
Angry Axioms! Squash the bottom sequent by hitting it with axioms, which causes more sequents to appear on top of it. Once you get just the flat bar to appear over a sequent, an anvil drops on it, and once you have anvils on top of all sequents stacked over the bottom one, you squash it! But you have to hit the sequents in the right way, or your axioms won't be able to squash them properly. Now re-read the above, replacing the word "axiom" with "bird" and "sequent" with "pig". ;) Sounds like a compelling game to me.
Formal definitions have the illusion of being easier to understand by being more precise &amp; compact. Also, understanding the formal definition at some point will aid your learning tremendously.
Many Haskellers have an academic background and are used to compacted definitions and explanations.
I support this approach -- after all, it's how I learned about monads (doing IO and parsing).
Because it's really hard? Teaching is incredibly difficult and a lot of people can only flail ineffectually when it comes to concepts like these which require a deep understanding of both the subject and how people learn. Not to mention that many people come to the notion of monads with the firmly-entrenched "ivory tower academic nonsense just making everything sound harder than it actually is" belief that there's nothing worthwhile, or that they're a cheat that Haskellers use to do IO.
It is strange you have not been advised to use operational monads (like the packages 'operational' and 'monad-prompt') or even free monads. They are simpler to reason with than Cont.
...is the curly-bracket syntax for block structuring optional, in order to leave the user a choice, as it is with Haskell?
&gt; And will look into . operator. I seems like piping in unix Very much so. Many unix programs even use streaming IO processing, which is similar to laziness in Haskell function evaluation. 
As Wadler re-tweeted (in his own way) today, [“The original monad paper is *far* more comprehensible than any of the other 36 monad tutorials out there.”](http://2.bp.blogspot.com/-0Yo8KNrjMhE/T7yjHPBNVcI/AAAAAAAAARI/Sqt9AOpVq-A/s1600/original-monad.png) This makes complete sense; because it is (was) a new idea, they have to explain it as a new idea, not something that is assumed and being explained within a culture of it being assumed knowledge. That also means a reader of [these papers](http://homepages.inf.ed.ac.uk/wadler/topics/monads.html) can know with certainty that she's not “missing something”, because it's written in a “guys, we normally write code like this, how about we write code like this?” kind of way. Reading a tutorial by the inventors of the things also helps feel the tutorial is credible. (IMO monad confusion comes from bad tutorials/scare-culture combined with a bad understanding of the flexibility of types and kinds in combination with type classes. Ignore the former, fix the latter, and you'll experience no mystery.) The same can be said of the type-classes papers. If you don't understand type classes, why they're useful, how they might be implemented, read the original [“How to make ad-hoc polymorphism less ad hoc”](http://homepages.inf.ed.ac.uk/wadler/topics/type-classes.html). You can't really beat the original papers by the people who came up with it.
Ertugrul's actually does what he talks about: http://ertes.de/articles/monads.html
If the language is in every way simply a worse version of another language, then in what way does the language have worth? You are upset at javascript being characterized as worthless, but admit that it has no value. That is what worthless means.
Hi Haskellers, Updated my post as per your feedback. And now I am feeling much haskelly. Please have a look at it and also let me know if my understanding of things is correct or I am still stuck in JavaWorld. Link :: http://firstlineofcode.blogspot.in/2012/05/my-1st-haskell-program.html
I think (at the risk of sounding extremely cheezy) that everyone is "winning", since there is a lot of cross-pollination between these libraries as they continue to pick up good ideas from each other while maintaining their own emphasis.
There has been a bit of renewed discussion recently in the Scheme community whether `call/cc` should be retained in the language, due to [Oleg's proposal to do so](http://lists.scheme-reports.org/pipermail/scheme-reports/2012-February/001824.html). The argument is that other abstractions, such as delimited continuations, are a good enough replacement.
&gt; In other words, pipes let you decompose loops into modular components, which promotes loose coupling and allows you to freely mix and match those components. This is a pretty straightforward thing, and it's quite nifty that you can do this with pipes. But my brain keeps telling me that this is extremely important for some reason, that there's more depth or generality to this aspect of pipes than meets the eye.
Oh, believe me, there is. I briefly touch on it when I talk about "stack frames". People sometimes use the term "inversion of control", but I prefer to think of it as "turning the call stack sideways". We tend to program call stacks vertically, by specifying which functions call which. With pipes, you instead program horizontally, by specifying the behavior of each stack frame across multiple calls to or from other anonymized stack frames. If you generalize pipes to transmit and receive heterogeneous and finite inputs, then the analogy to call stacks is exact. The category of pipes is the category of stack frames, where composition of two frames stacks one on top of the other and the identity frame is the transparent stack which relays all requests and results.
You already posted that [here](http://www.reddit.com/r/haskell/comments/tzr01/start_with_programming_my_1st_haskell_program/).
So this is what http://blog.ezyang.com/2012/05/what-happens-when-you-mix-three-research-programming-languages-together/ was about, right? I feel a bit cheated, because this looks like a personal hobby project, whereas I assumed (from the dialogue at the beginning) that you actually had to go and convince some manager and a team that using Haskell + Ur/Web + Coq was the (or a) right solution.
I'm a Haskell novice and holy shit that code you linked to is awesome. I can't imagine how much C code I'd have to write to do the same thing.
If more experienced people have thoughts on whether a paper on this sort of thing would be a publication worth pursing, I'd greatly appreciate that feedback. So far, the only other real application of quasiquoter pattern matching I've found is regex, but I feel like there's a great deal of potential here and I'd like to try and get it used more widely. I've abstracted the actual pattern matching work to my [Pattern Power!](https://github.com/colah/pattern-power) library and am thinking of making a library of useful pattern matching quasiquoters...
That's awesome! I think it's essentially equivalent to [View Patterns](http://hackage.haskell.org/trac/ghc/wiki/ViewPatterns) (which are ultimately the back end for this :) ) except with object-oriented integration. It's a clever idea. That said, I feel like building complicated patterns would be really tedious. That's where quasiquoters really shine through. I suppose you could handle it with having your object constructed by parsing a string...
Of course Schemers could always write amb using the list monad too...
Good project, I see that you have already worked with Sage.There was a proposed google summer of code project for haskell to provide a mathematical environment for haskell you can check it here. http://hackage.haskell.org/trac/summer-of-code/ticket/1600 Starting with this symbolic algebra library may be one possible extension can be in his direction. 
Well, my point was that for a motivated audience (either highly motivated or with a modicum of background in programming) natural deduction isn't any simpler :) But since you insist. Yes, natural deduction could be gamified in just about the same way. The main points about taking apart logical constructors and putting them back together again, about there being different rules for each constructor, and about fitting those rules together until every (non-assumption) statement has a line over it--- those parts are all the same. The only differences are that the rules don't involve moving things back and forth across the turnstile (only the right-side of the turnstile exists in ND), and when introducing the game you'll also have to introduce the metarules about when and how you're allowed to introduce local assumptions, use them, and how they must be discharged/eliminated. As far as the lambda calculus goes, the bit about ND vs sequent calculus (as Edward presented it) is just a ruse. There is an important difference for the full sequent calculus, but not the version he talked about. In typed lambda calculi there are two major things going on, there are value-level terms and there are type-level terms. In natural deduction there are two major things going on, there's the object-level theory (i.e., statements in FOL etc) and then there's the metatheory (i.e., the proof trees). In the full sequent calculus there are three things going on, there are two object-level theories ---one of which Edward elided in his game--- and there's the metatheory (still proof trees). Given these seven different (meta)theories, here's how the Curry--Howard correspondence works and how ND is related to sequent calculus: * The value-level terms of lambda calculus are isomorphic to the metatheory terms of ND; i.e., lambda-terms "are" ND proof trees. * The first of the object-level theories of sequent calculus is(exactly) the value-level terms of lambda calculus, or the proof trees of ND. {N.B., the ND proof trees are metatheoretical in the context of ND itself, but they're only (object-)theoretical in the context of sequent calculus.} This is the part of sequent calculus that Edward elided--- and rightly so, since he wasn't talking about type inference for lambda-terms nor about proving the well-formedness of ND proof trees. * The type-level terms of lambda calculus are isomorphic to the object-theory terms in ND; i.e., types "are" statements in FOL (or rather any particular logic of choice, depending on the type theory). * The second of the object-level theories of sequent calculus is(exactly) the type-level terms of lambda calculus, or the statements of FOL(etc). * The role of the metatheory in ND is to "prove" some statement in FOL(etc); correspondingly we could think of the role of lambda-terms as "proving" that their type is inhabited. But just because someone claims to prove something, you can't simply trust them. The role of the metatheory in sequent calculus is to relate the ND "proof" with the statement in FOL(etc) in order to prove that the ND proof does indeed prove what it says it does; aka, to relate the lambda-value with its purported type in order to prove that the value does indeed have that type. * Proof search in ND is like trying to make up a lambda-value for some given type. Proof search in sequent calculus is like type checking/inference. If you're doing the full version of sequent calculus with both object-languages, then things do get a bit more complicated. But the one object-language version of sequent calculus that Edward presented is, IMO, just a cleaner way of talking about the same thing that ND is talking about. If we call the full version 2-SC and Edward's version 1-SC, do note that only 2-SC can do the job of proving that a given proof is sound, mentioned above. With 1-SC you don't have on hand the proof about which soundness would be shown. Thus, in 1-SC you're just building up proof trees over a single object-language--- which is *exactly* what ND does! The only difference between 1-SC and ND is the manner of presentation. In ND you have metarules to bear in mind about introducing, using, and eliminating local assumptions; in 1-SC all those metarules are encoded directly by the left-hand side of the sequent. There are two reasons why I think 1-SC is superior to ND on purely technical grounds. The first is the bit about making sure local assumptions are handled properly, which I've been harping on about. The second is that in 1-SC you must have the turnstile there, whereas in ND because there is only one side to the turnstile people often elide it. This may seem like a fiddly detail, and it is, but it's also a detail of great technical importance. There is a very big difference between simply writing out a statement in FOL(etc), vs declaring that the statement is true. Writing out the statement with a turnstile in front of it is the latter; and eliding the turnstile in ND proofs is erasing a very important distinction, namely that the whole point of the game is to learn something about which statements are true and which are not! Just manipulating statements of FOL(etc) cannot be talking about truth; there must be a metalogic in which one declares that a statement of the object-logic is "true". While the technicalities of all this are far beyond what any introduction to formal logic and proof systems should discuss, I do think it's important to train people to 'accidentally' do the right thing, rather than being forced to retrain those who wish to delve deeper.
pretty sure somebody hooked djinn up to template haskell to handle just such gimmicks.
I think, if you really want to explain Curry--Howard, then you're going to have to get into all the gritty detail of the full sequent calculus. You kinda need both proofs/values and also propositions/types in order for Curry--Howard to make much sense. If you're only doing ND, then you're just programming in a different language; not learning about what programming is. If you do full sequent calculus then you're showing how the lambda-values are related to their types, as well as showing (the idea of) how type checking/inference works, both of which Haskell newcomers would be interested in. I definitely agree that there should be simpler more interactive explanations of all this stuff. There's very little out there for the people who don't necessarily want to do type theory professionally; and even among the professional stuff, it's all hints and whispers when it comes to finding introductory material. There's definitely a secret society aspect, where budding young professionals must be indoctrinated into the mystical ways by one-on-one interaction with an old wizard. I don't know how many type-theory papers I read before I ever found out what the heck ND had to do with anything, let alone what the difference between ND and sequent calculus was.
The second talk is about grammatical formalisms for (computationally oriented) linguistics. In particular, ACG is a kind of categorial grammar (CG), where TLG and CCG are other popular CGs. One nice thing about the CG framework is that both syntax and semantics happen together--- sort of like how types and lambda-values happen together. This sets CG apart from the other major grammars frameworks (e.g., constituent-based grammars like CFG or TAG, or dependency grammars) which believe in doing syntax alone and then handing the final syntactic tree/graph off to a separate unit for doing semantics. While the simultaneous pairing of syntax and semantics has been a part of CG from the beginning, and is often espoused when teaching CG to students, in practice most CG researchers tend to only focus on the syntactic (type-level) side rather than the semantic (value-level) side. Like Oleg and Ken, I aim to fix this state of affairs; the synchrony of syntax and semantics is of major importance, and shouldn't be ignored. Whisking away from grammatical formalisms, one of the big things these days in formal semantics is dynamic logics; i.e., logics with a notion of "time", where the truth of statements can vary as a function of "time". One of the most classic issues in semantics is trying to figure out the scope of how quantifiers bind things. For example, in "John met his mother" can "his" refer to John? must it? (Now consider other sentences with pronouns and proper names in them.) Or consider the classic sentence of ill repute: "every farmer who owns a donkey beats it." How exactly should we formalize the meaning of that sentence? More particularly, how can we do so in a way that's harmonious with the usual treatment of "every", "a", and "it"? While familiar with CGs in general (and CCG in particular) I'm not especially familiar with ACG in particular. But apparently, the ACG literature on how to handle these various scope and binding issues cannot handle them without resorting to type lifting. All "type lifting" means is reversing the function--argument relationship. So if we have: (f :: A -&gt; B) (a :: A) then `f` is the one "doing the work" and `a` is just sitting there like a lump. But we can reverse this relationship by making `a` the active one: (($a) :: (A -&gt; B) -&gt; B) (f :: A -&gt; B) Now `a` (well, actually `($a)`) is doing the work and `f` is just sitting there. In the context of CG, we actually have (at least) two kinds of functions: those which take their arguments on the left, and those which take their argument on the right; so in CG, we can chose to do the original forward-application, or we can do type-lifting (without re-ordering the `f` and `a`) and then do the reverse-application. Also, the choice of how to combine atomic expressions via application, composition, type-lifting followed by application, type-lifting followed by composition, etc--- that's all left up to the parser, rather than being given like it is in programming. While it's sound, there are big problems with type lifting. One problem is that if you're not careful it can lead to the grammar breaking down and just accepting any old sequence of words; i.e., you can lose the formal power to recognize certain classes of languages (CCG can recognize mildly context-sensitive languages, but with unrestricted type-lifting it collapses to context-free languages or even lower). Another problem is that if you're aiming to implement this all on a computer, then type-lifting increases the algorithmic complexity, because it increases the space of all possible parses you must consider when deciding which is the correct one. So in this talk, Oleg is presenting a generalization of ACG which can handle all those scope and binding issues. In particular, doing so in a way that uses the standard dynamic-logic analysis (i.e., there's no new semantic analysis being done) and doing so in a way that makes use of the applicative functors we know and love in Haskell. To get a bit more of an idea about the flavor of all this, you may want to look at the [NASSLLI class](http://okmij.org/ftp/gengo/NASSLLI10/) they taught in 2010, and are offering again this year. The grammar formalism there is CFG, but the ideas about mapping between natural language, formal language, and semantics are similar.
Perhaps you're weird, but you're not alone. Whether it's easier to learn things in a bottom-up manner or in a top-down manner is very much a personality trait of individuals, and neither is especially more common in the populace. (And certainly neither is "better" than the other.) The reason it's problematic is that as a teacher one must address a group of people, some of whom learn best the one way, some of whom learn best the other way, and try to get all of them to understand it. Unfortunately, doing so is a very difficult skill to learn, and most teachers ---both in the professional sense, and just those trying to educate one another on blogs--- fall back to teaching however it is that *they* learn best, regardless of what the learners need.
I've done a lot of work on this sort of thing recently--- not the quasi-quotation stuff, the algebra stuff. Adding the pattern matching is really nice :) I have a few thoughts, but I have to head to bed. I'll try to post them tomorrow or friday.
&gt; and most teachers ---both in the professional sense, and just those trying to educate one another on blogs--- fall back to teaching however it is that they learn best, regardless of what the learners need. Which explains the 36 monad tutorials, esp. those comparing them to burritos.
Symbolic algebra is one area where you would think Haskell would be be a great choice, but where libraries seem to be lacking. This proof of concept is a good step in the right direction.
I would prefer to be commented if something is wrong instead of being silently downvoted when I try to be constructive. That is the point of posting it here: be taught, and teach others that might come here afterwards.
This is neat, have you given thought to how you'd implement different term rewriting strategies on expressions? 
So here's an idea: people who aren't skilled teachers should refrain from attempting to teach such concepts. 
Yes, _at some point_. But that point should come after the concept has been made familiar through examples.
True. But why don't the authors of monad tutorials of the more traditional type (especially given how unsuccessful most of them are) not stop and think about why their communication strategies are not working so well with all those other folks?
That's pretty cool. A [link](http://hackage.haskell.org/packages/archive/djinn-th/0.0.1/doc/html/Language-Haskell-Djinn.html) for those of us that are lazy -- scroll down for some examples.
Yeah, I've been wanting to use them for type level strings ("Symbols"). Sadly, I haven't been able to get it working on my computer... The real problem, unfortunatly, remains getting the type level sets...
An expression datatype with a pretty show instance is rather easy to make (just copy the way Text.Show works). Even operations like differentiation are quite easy (ok, integration is hard); what is *very hard* is symbolic simplification, especially if your variables are in the complex domain; and handling assumptions about your variables and subexpressions. And then we are still talking about expressions which evaluate to just numbers...
Congratulations! I hope you continue to make a good impression and give us Haskellers a good name.
This sounds like an awesome project! Very exciting, especially since it's going to be done with Haskell :) Good luck!
Yes, I know, but I just need for an example interactive session in a blog post. So that has to be something that others can easily try themselves, so it should be available from Hackage.
GHC HEAD lets you do a lot of cool stuff: *Main&gt; type Or = FoldR (||) False *Main&gt; type Any = (Compose1 Or) `Compose2` Partial1 Map *Main&gt; T::T (Any ((==) 1) '[3,2,1]) True http://code.atnnn.com/projects/type-prelude/repository/entry/Prelude/Type.hs
Speaking for myself at least, I'm generally only willing to fund *very* well backed OSS efforts, because there's no real accountability for the people that receive the funds to continue working on it. I'm not saying that I won't give out of gratitude, but the concept of supporting somebody I don't know personally or have any sort of contract with for work that is yet to be done makes me reluctant to give my money. 
Excellent summaries! I might add that LSA is also called LSI "indexing" and is/was under patent, as a specific application of SVD. So first paper, good backgrounder is the chapter from freely available text by Manning, Raghavan, Schütze. http://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf For 2nd, his previous linguistics papers: http://okmij.org/ftp/gengo/ 
[Ardour has an interesting bug bounty system](http://ardour.org/bugbounty). Is what you are describing different from [FOSS factory](http://www.fossfactory.org/)?
We will be releasing it early next month; Scotty is already available.
Congrats! &gt; I've heard that it's difficult to get paid work on Haskell. (Oddly, I've heard this from both haskell programmers, and people worried that, if they use a Haskell program now, they'll later be unable to hire haskell programmers..) Both are somewhat true, and it's not that odd: programmers are not fungible, and neither are jobs. That said, right now it should be much easier for companies to find Haskell programmers than it is for Haskell programmers to find Haskell jobs.
They actually taught us calculus in school by showing us the fundamental theorem first. Of course we had learnt about limits beforehand... This matches my intuition in that its easier to understand monads if you understand functors, monoids, and applicative functors first, which I feel are a lot simpler.
You're even going out for dinner, I see. Great work!
Very cool!
Isn't that just `instance Unsafe () =&gt; Unsafe ()`? Edit: Oh, but you probably want something that can satisfy any class constraint, just like `undefined` can be any type.
Well, note that dictionary construction is strict, so you can't actually pass that dictionary in to any method.
The :t command in GHCi tells all: Prelude&gt; :t (.) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c Prelude&gt; :t ($) ($) :: (a -&gt; b) -&gt; a -&gt; b '.' actually comes from function composition in mathematical notation. so f(g(x)) can be represented (f.g)(x). '$' does the exact same as ' ' (an empty space is also a function) but with lowest precedence in order of operation, so it allows it to be used in the stead of parantheses, so it can make writing code faster and cleaner.
Not true. People with academic background are used to get useful explanations on their own and after they understood the necessary basics they can go on using compacted definitions. The question why it is difficult to explain scientific stuff for many people, however, is another issue.
I don't know, but this seems to be related to the new flag -fdefer-type-errors, which takes unsatisfied constraints and gives `undefined` as their witnesses.
I haven't looked too deeply at the code, but the first thing I noticed is that you're using lists to represent the collection of arguments. A more efficient way ---especially when it comes to large expressions--- is to use a data structure which encodes the normalization of the expressions. (I'm assuming in the sequel that all you care about is normal forms, not the concrete syntax used by the programmer.) A free commutative monoid is a multiset, since sets abstract over all the different choices of ordering and grouping. Thus, you can efficiently represent an arbitrary finite summation with a `Map` (or `IntMap` or `HashMap`) from the summands to their multiplicity, or just a `Set` of the summands paired with their multiplicity. The only requirement here is that you can provide an arbitrary but fixed ordering on the summand expressions (or a bijection to `Int` or a hash function). But this is usually easy to arrange by using the natural ordering on natural/integer coefficients, and by choosing variables which support some ordering (e.g., `String`, or a newtype of `Int` for "randomly" generated unique names). Similarly, you can represent an arbitrary finite commutative product by a map from the multiplicand to its power. This is especially helpful for optimizing the representation of powers of variables. For non-commutative monoids, the free one is a sequence, since sequences abstract over the possible associativity trees. Lists are a decent representation here, but if you want to have cheap access to both ends then you should use something like a fingertree (cf., `Data.Seq`) and if you want to have cheap random access then you should use something like a map from positions to the multiplicand at that position. Usually you only need fast appending (and fast access to both ends in order to resolve inverse elements) but not random access; though I'm not sure if pattern matching makes random access more important. So basically, for a non-commutative semiring over variables `a` I'd suggest using a structure like: type Polynomial a = Set -- the adds ( Natural -- the constant factor , Seq -- the products; empty sequence denotes a pure constant ( a -- variables , Natural -- power of the variable ) ) If it's commutative, then replace the `Seq (a,Natural)` by `Map a Natural`. If it's a ring, then replace the constant-factor `Natural` by `Integer`. If it's a division (semi)ring then replace the power-of-variable `Natural` by `Integer`. And naturally, you'll have to adopt this to your data vs pattern way of doing things. Then, once you have that representation, you can just implement `Num` and similar classes such that they automatically normalize at each step. [augustss](http://augustss.blogspot.com/2007/04/overloading-haskell-numbers-part-1.html) has some discussion about doing this; and I have a local extension to that idea which handles all the other normalization stuff that comes up. In my version I do sometimes care about unnormalized polynomials, so I just normalize the AST rather than using the above type; though I make no claims to my AST normalization being close to optimal, it's just sufficient to get the job done. I can email you a copy of the code as it stands, if you'd like.
Another thing I wasn't quite clear on is: what exactly do you mean by "variables"? On the one hand we have Haskell's variables, which would serve as metavariables over the (semi)ring expressions. But on the other hand, I assume you're wanting to deal with polynomials, so we'd have object-level variables which are properly considered part of the (semi)ring expressions. In my post below about representations, I'm talking about the latter. But since you're doing pattern matching, it's not clear whether you intend to be talking about the former. If you're interested in having metavariables, then you should look into "associative commutative unification". There are a few packages on Hackage which already target this sort of thing; e.g., [agum](http://hackage.haskell.org/package/agum) and [cmu](http://hackage.haskell.org/package/cmu). Even if you don't want to use them for some reason, you should check them out for ideas. I also forgot to mention before, there's also the [polynomial](http://hackage.haskell.org/package/polynomial) package which is in a similar ballpark to all of this. *Edit:* this question gets at the stumbling block you mentioned about how to handle variables in the types. There's been a lot of work here, particularly for dealing with lambda calculi which are a lot more complicated since they have binders as well as variables. But without knowing exactly what it is you're after, I'm hesitant to provide pointers which may be off-track.
Finally, I noticed that you're in Toronto. Depending on how deep your interests run, you may want to try getting in touch with [Jacques Carette](http://www.cas.mcmaster.ca/~carette/) who's just a bit south of you. He's a fan of Haskell but, more importantly, he was for a long time one of the main developers at MapleSoft--- Maple is renowned for its use of symbolic methods as opposed to the majority of other algebra systems which only offer numerical methods, so he's well versed in all the issues you'll run into with this sort of thing.
Won't GHC remove the `Unsafe ()` dictionary from any function that uses it? What I mean is that, just as the type `(Num Int, Eq Int) =&gt; Int -&gt; Int` would get simplified to `Int -&gt; Int`, the type `Unsafe () =&gt; IO a -&gt; a` would get simplified to `IO a -&gt; a`. What's to stop GHC from optimizing this whole infrastructure away?
I also find that understanding more abstract mathematical ideas much easier given a definition rather than a bunch of examples. This definitely held for monads, but also worked for more basic concepts as well. In high school, I had a fairly vague idea of what a function was; I had seen plenty of examples in my various classes, and had at ton of "intuitional" descriptions like the vertical line rule, but never a straight-up definition. When I was finally told that a function was a mapping between two sets, everything suddenly made a lot more sense. Similarly, I only really understood monads after seeing the map/return/join definition. Later, I read some simple category theory primers and they helped even more. One of these days, I'm probably going to have to sit down with a category theory book and learn it properly. 
I get beer money from flattr, but that's about all. Compare with now 200% funded kickstarter. (BTW, I flattred reactive-bannana recently, although I have not done enough with reactive programming to grok it. I'd probably flattr any haskell library on flattr. :)
Types get optimized away :-)
I've been using fabric to deploy stuff to my two sites, but this looks interesting. The only problem right now is that I'm using apache as my webserver, but this just might motivate me to switch to nginx.
That's really weird. This doesn't seem to work though: data Proxy a = Proxy class Asdf a where asdf :: Proxy a -&gt; Int; asdf = const 9 main = print (asdf (Proxy :: Proxy Any)) 
He emailed me!!! So, yes, we'll hopefully be talking. :)
I considered some structures that encode normalization, though not as thoroughly as you have here, some of these suggestions are very clever! The main reason I didn't do something like this was that an AST approach gave me low hanging fruit to demonstrate the pattern matching. It also lent itself to being extensible to a larger range of mathematical expressions. If I was trying to write something more serious, I would definitly represent polynomials/rational expressions in one of the clever ways you suggest. :)
No, and the error message doesn't even mention Any: Ambiguous type variable `a0' in the constraint Neither does the type of this expression: &gt;&gt;&gt; :t flip c . (:) 2 flip c . (:) 2 :: Num a =&gt; [a] -&gt; c -&gt; c
Are you sure this whole thing doesn't actually qualify as a bug?
You're hackage page seems to be somewhat lacking in documentation. http://hackage.haskell.org/packages/archive/Elm/0.1.1.4/doc/html/Language-Elm.html I understand you have a home page, but this is the first I look. I suggest at least pointing people to the homepage from here.
Looks awesome, very impressive! I like the apparent auto-Show instance.
&gt; Maybe it's the jet-lag talking but I'm just so enormously disappointed with the world that such an interesting and useful idea has fallen into the hands of so many people who seem so congenitally bad at expressing their thoughts about it. Before allowing yourself to fall too hard into a pit of despondency, you should try to understand why monads *seem* so hard when a subject like quantum mechanics can be taught successfully. Viz., that both the audience and the environment differ. On the one hand we have dedicated university students, having developed some measure of mathematical and scientific maturity, taking semester-long courses taught by skilled professors. On the other hand we have impatient but well-meaning dilettantes who largely *don’t* meet the prereqs, expecting to understand something in five minutes from web tutorials, themselves written by impatient but well-meaning dilettantes. The barriers to teaching and to learning are too low. There’s no one to blame, really; that’s just the way it is. I guarantee you that if all of a sudden a great number of working programmers decided they needed to learn quantum mechanics, you’d see a thousand QM tutorials like “Atoms are Burritos”.
elm-lang.org is currently down. It says: &gt; An Error Occured: Better Messages to come.
Elm compiles into Blaze's Html type, Yesod and Happstack already use this so it can already be integrated (almost) flawlessly. Happstack: someHandler :: ServerPart Response someHandler = ok $ toResponse $ compileToHtml "/" "Some page" $ [| .... |] Yesod: someHandler :: Handler RepHtml someHandler = defaultLayout $ toWidget $ compileToHtml "/" "Some page" $ [| ... |] The issue that arises in Yesod is how they use a defaultLayout, as far as I could tell elm currently generates the entire page while Yesod uses a template that contains the rough page layout and places to insert the page title, page head-tag and body tag. Currently I suppose you could use an empty layout (and throw away a lot of Yesod's widget options, like dynamically setting the page title and splicing in more CSS, JS and so on). An easy way to get around this is to add some kind of compileToBody function that leaves out everything before and after the body-tag. wheatBread's idea for a typed data protocol between Elm and the framework also sounds pretty cool!
I had a look at the generated source for a couple of the more document-like pages. Summary: body consists of one div tag, everything is generated with JavaScript. As expected, this doesn't work with lynx even a little. blank page. I see basically one major problem with all this: you're treating html/css as part of your "assembly language" - i.e. as code - But they are in fact intended to be *data*. Web pages are not just GUIs, they have semantic content, which is intended to be analyzable by user-agents for arbitrary purpose. All manner of decidability issues make doing anything but displaying a GUI impossible if your page is a giant ball of JavaScript. If you want to do something which really can't be done without resorting to js, fine. But if your site reasonably fits into a document model, then by doing this you unnecessarily make things difficult for: - The blind, and people with other disabilities, who rely on their browser's ability to analyze the document so that it can be presented to them in a way that works for them. - Any nonhuman users of your site, such as search-engine indexing bots. I would really like to see this project make some effort to output machine-readable pages. Let's not destroy the last shred of good design that went into the web. 
Does tanakh relate to the Hebrew word "תנ"ך" (meaning bible)? And is saizans the same as Saizan in IRC?
Especially the correlation between being talked about and being used is interesting, we could use that to weed hackage out a fair bit.
I think there's a problem with that in that stable packages tend to be taken for granted and not discussed as much as novel packages. For example, Yesod has a very active mail list and the most populated IRC channel; Happstack and Snap are relatively quiet by comparison - but does that make them irrelevant?
I was reading RWH for the 3rd time and needed to be writing a real world program to go with it, and this was something I didn't mind spending 10 times as long writing as it would have taken in perl, due to learning curve.
Nice, I hadn't seen http://hackage.haskell.org/package/GTALib before. 
&gt; I think that they're just less-used, and there's probably a reason for that. Haskell is less-used than a whole lot of other languages, and there are reasons for that as well. :) &gt; explicit voting has the same problem, probably worse Sure.
I added that you can also write unsafe instances instance (Unsafe (), Show a) =&gt; Show (IORef a) where show = show . unsafePerformIO . readIORef
I am aware of this problem and have plans to fix it. In a previous iteration I generated no-script tags for every page that contained most of the text on the page in roughly the correct order. This was taken out because some of the internals of the compiler were getting changed pretty drastically, but it is an important part of the compiler and will be added back in! Ultimately, I'd like to generate all of the initial HTML and CSS statically, along with a no-script tag. Chapter 3 of [my thesis](http://www.testblogpleaseignore.com/wp-content/uploads/2012/04/thesis.pdf) shows that it is possible to compile every terminating Elm program down to its initial state statically. This saves the client from doing a bunch of work *and* makes everything indexable. This will all become possible as Elm optimization passes become more robust. In any case, I should add this information to the site because this is a very important concern. edit: I just uploaded a new version that generates noscript tags.
Very well said. Thank you! I have only seriously used HappStack, so I wasn't aware of Yesod's `defaultLayout` conveniences. I'll try to better accommodate these in a future release. I just wanted to make one comment on your code though: `"/"` should probably be something like `"/elm-mini.js"` or `"/resources/elm-mini.js"` or wherever you happen to serve that file from. Otherwise it looks great :)
I think that package quality should be determined by doing a beauty pageant on the logo. [My contestant here][1]. [1]: http://www.haskell.org/haskellwiki/Reactive-banana
I think a better criterion would be how often they are imported. If a package is talked about a lot it could be because it has lots of bugs.
AFAICT, cross compiling isn't possible [ref](http://hackage.haskell.org/trac/ghc/wiki/CrossCompilation). But perhaps it would be possible to create a virtual machine with qemu and allocate it lots of memory.
&gt; I think that they're just less-used, and there's probably a reason for that. `bytestring` is a very widely used library. While suggestions to use ByteStrings in lieu of Strings are common, how often do you actually hear questions about how to do things with them? I'd wager that the number of questions and overall level of discussion are quite low. Similarly, until the recent era of performance hacking, there wasn't a whole lot of discussion on `containers` nor questions about how to get something done with them. Even now, excluding the libraries list there's not much discussion or questioning. Does that mean `bytestring` or `containers` are "less-used" or "irrelevant" or "should be weeded out"? The answer is clear, but it highlights something important. The dynamics of discussion on libraries varies wildly and is almost unrelated to the popularity of the library, unless you hold the domain and other factors constant. Things like GUI, web, and I/O frameworks have a lot of complicated bits and moving parts, and therefore generate a lot of discussion. Things like data structures generate much less discussion, and typically only do so when they are (a) new, or (b) unusual--- as examples of "unusual" I mean things like bloom filters or lock-free concurrent collections. Which, again, moving parts.
But aggregates of aggregates! Meta everything for great profit!
You're completely right, actual usage should definitively count in favour of not weeding out (I was thinking of semi-hiding) things on hackage. Discussion is only one measure of possibly a multitude, there. The article deals largely with the issue of linking discussions to the API documentation proper, though, and that definitely can't hurt, especially when links go to encyclopaedic or near-so sources like stackoverflow.
&gt; The blind For what it's worth, blind people have told me that they prefer “blind people”, rather than “the blind”, which sounds like “the lame”, “the undead”, etc. &gt; who rely on their browser's ability to analyze the document so that it can be presented to them in a way that works for them. Screenreaders work from the DOM, not the original HTML. So whether it's generated from the server or the client is irrelevant. The important question is whether it generates proper headings (which I see it does generate a h1), which are useful for skipping which is what blind users do, spending time sitting around listening to the whole page isn't fun. &gt; Any nonhuman users of your site, such as search-engine indexing bots. &gt; I would really like to see this project make some effort to output machine-readable pages. Let's not destroy the last shred of good design that went into the web. To be fair, the behavior you're describing, which is machines reading HTML, which is by design a human-readable representation of a concept, is not the “good design” that went into the web. REST is good design. The search engine should ask for a machine-readable concept (`Accept` header) and get proper meta data about it in XML or something, not have to sift through an incomplete sample of human-readable turge. But we don't have that because we suck. Let's not pretend it's good only because it's all we have. :-P
I really like the idea. A Haskell like language + FRP compiled to JS. Last week I tried to create a little game, but was missing keyboard input or a tutorial on how to integrate abitrary IO functionality myself. You seem really ambitious. As a developer I'd like to know how it may fit into a complete web toolset and see a roadmap where the project is going and not being abandoned next month. I also tried UHC und Emscripten the other day and so far I think Elm is the closest we can get with Haskell to Javascript :)
∃ a trac ticket for this?
No, it's Tanaka Hide, last name/first, by convention http://tanakh.jp/pub/haskell-day-2012-05-27.html#1 and this on GTA: http://qiita.com/items/a372458d171e373285b1 event wiki http://wiki.haskell.jp/Workshop/HaskellDay/2012 and a vid http://www.justin.tv/kiwamu -------------- unfortunately, I can't read Kanji at all anymore and my Dad's translations of programming blogs are... creative
incidentally, i'm getting URL's off this twitter feed http://jetsli.de/?hits=24&amp;time=last24&amp;query=haskell
This would require type universes above the value level then? I really should brush up on my agda and idris in the mean time. 
Yeah! At the last Hac-PDX there was a lot of Hackage 2.0 hacking, and I took a (unfortunately brief) swing at proposing / implementing stuff like this. A few ideas I had for it: * The user-created content should be given prominence when documentation is absent. Otherwise, there should be some visual queue about the presence and magnitude of the community-created documentation. Clicking on this queue would unfold to the docs / info. * It seems like there are two modes of community-driven documentation: appending, correcting/re-writing. Appending is more like leaving a comment (which may contain, e.g. example code, or links to good examples). Correcting is more like wiki editing / applying a patch to the code. * Since this hackage comment / wiki edit stuff needs to happen outside the code (or at least, it would be cleaner that way), I think haddock needs out-of-code documentation. This is interesting for other reasons too: * I like this style anyway. Literate style is cool, but I feel like it can negatively effect the ordering / hack-ability of code. We can either twist up the code to make the documentation read right, or we can twist up the documentation, and not properly explain / illustrate the interconnections between top level definitions. External docs allow for both to read right. * We can have a separate git repository for documentation, and have the community documentation elements of hackage push to it. * If we use a hybrid approach, of both in-code and out-of-code documentation, then it would be very handy to be able to inline the in-code comments into the out-of-code comments. They could then be modified, say to incorporate some user generated docs, and re-incorporated into the code.
Very interesting analysis. The only point I don't agree on is the one about the monad transformer laws. Gabriel and I discussed about this already, and my position is that laws that hold modulo a given equivalence relation are acceptable. In practice, that means that it is possible to write ill-defined functions by pattern-matching on the constructors (like `count'` in the example), but as long as the equivalence relation is clearly stated, it's really not a problem. Although sacrificing performance for code clarity can be a good trade-off in some cases, I believe formal verification considerations should have no impact on the implementation. With a sufficiently powerful type system, invariants can be encoded by dependent types, and equivalence relations by working with setoids. In Haskell, I think it's enough to state them in the documentation in semi-formal language. Anyway, in this case there could be a compromise: keep the `Pipe`/`Conduit` implementation as a free monad (so that the `MonadTrans` laws hold only modulo relations), but hide the constructors and offer a "view" functions instead, returning the less-efficient representation. This way, only functions defined using the low-level API would incur a performance cost, and they would be guaranteed to respect all the invariants. 
Point 3 seems suspicious to me. Do you have an example of where a rewrite rule would fail (in observable ways, not just details of representation) because of the `Free` / `FreeT` thing?
I gave one in my post. I use that rule to factor out the lift and then a second rule for the lift to cancel with runPipe if it is a pipeline.
Firstly, thank you for the analysis. I'm very happy to have others vetting `conduit`. But I'm going to have to disagree on most of your points. As there's already a good thread here on the `MonadTrans` issue, I'll ignore that. I think the only bug for the Monad laws is a documentation bug. For the case of `printer`/`residue`, your example violates the requirement that a `Conduit` can't produce leftovers that were not passed into it. This was an invariant I was able to express in the type system itself in previous versions of conduit, but unifying the datatypes prevents this possibility now. The specific case you're talking about is something I considered adding an assertion to the codebase about, but decided against it. As for the finalization issue, this is intended behavior. A finalizer is only guaranteed to run if the containing pipe has been started. This is an optimization: there's no need to run `sourceFile`'s finalizer if the file was never opened in the first place. It seems that for these two bugs, your recommendation is "just remove leftovers." This is a fundamental difference in philosophy between pipes and conduit: conduit intends to provide features necessary to get certain tasks done, even if this exposes a rough edge to users. I believe that the lack of leftovers handling makes pipes unsuitable for real world applications. (Again, my same claim from months ago stands: prove me wrong by actually reimplementing the conduit ecosystem, and then we can discuss which approach works out better.) I'm really baffled by your claim of Category law violation. Firstly, I never claimed to follow the Category laws. Secondly, your argument is based on a complete strawman: I make it clear in the types that upstream pipes can't return values, and you claim that this needs to be fixed. Thirdly, your `inject` function is completely broken: you're feeding data to the finalizer, contrary to what the documentation says you're allowed to do. I can't even figure out why anyone would think that `inject` *should* work. Finally, data loss is a topic I've covered at length in the [appendix on conduit](http://www.yesodweb.com/book/conduits) (which needs to be updated to version 0.4). There's nothing magical going on here. All that said, I *am* very interested in the pipes solution via parameterized monads. I haven't had a chance to look into it yet, but am very curious to see how it relates. Under the "other bugs" section, I'm not actually familiar with the double-finalize bug. I'm not denying its presence, but would appreciate an actual example. It could be that ResourceT hides a bug, but I've never actually run into anything like this in the wild. Again, I appreciate the thorough analysis you've given here.
I'm still not getting it. Pretend (or maybe don't pretend) that I'm really dense. Can you spell out exactly the situation you're describing with the "nest[ing] a composition within a monad"? I do not believe my statement was at all in error about finalizers. In the case you give, the `PipeM` constructor most definitely *is* evaluated. The fact that you've provided a finalizer without any initialization code is just incidental. I'm still not certain how any of this results in a duplicated release call, or how this means a release call is made in the absence of initial allocation. Again, I'm really dense, can you clarify? You might want to have a look at `sourceFileRange` to get a better understanding of how finalization works. The initial `PipeM` provides a no-op for the finalization, because in this case, no `Handle` has yet been allocated. After the `Handle` is allocated, all future constructors include a call to `release` to ensure early termination works. And when EOF is reached, `release` is called once and then never used again. Can you demonstrate a case where one of the claimed bugs in `conduit` would cause this function to either not call `release` or call it twice? Obviously an exception would cause `release` not to be called, which is the whole point of `ResourceT`. I'm referring specifically to a bug in conduit itself.
I don't know if you read my edit to my first reply, but both of my examples provide pretty convincing examples of how upstream is finalized despite never being demanded from downstream. I don't know how you can interpret that any other way. For the nesting example, what I had in mind was: -- EDIT: My mistake again, but I'm preserving the mistake -- so as not to confuse thread readers in case Michael replies -- to it. do (return x =$= return ()) (return x =$= return ()) ... Both of those lines generate a conduit with residual input that was generated "correctly" and they trigger the code bug I mention in my post. I think that's as simple as it gets. -- EDIT: Retract the last block. However, I still consider it a bug that conduits -- do not correctly handle the user generating residual input by hand, -- which requires hiding the constructors. Also, I went back and checked my notes on `release`. You are correct on both counts here, and I admit I was wrong on this point. The release in the absence of allocation was my error in transcribing `sourceIO` when I was inlining `sourceFile` into `sourceIO`, and the double release bug was because I thought you sequenced `pull0` after `src` rather than writing the constructor by hand. I had forgotten that in conduits you avoid using the monad instance in order to get finalizers correct. So I will downgrade my criticism on those two points from a bug to a simple complaint that you can't write correct conduit finalization code using the monad instance as you can with `pipes`. More generally, though, it should be your job to prove your library is correct, not your user's job to prove it is incorrect.
Actually, the nesting example is still valid. All you need are two sinks where the pull action supplied to each one triggers an `IODone (Just x) y` response immediately. This then causes them to "properly" generate unhandled input which would then trigger the bug.
This is actually a baseless claim. You can't produce an `IODone` result without first consuming some input. If you tried to write such a program, you would see this immediately. Given the number of incorrect statements you've made here, I really hope you'll start reviewing your comments more carefully. Such hyperbole is negligent and does not belong in our discussions.
I think a lot of people in the Yesod community would be interested in discussing this (myself included). The [mailing list](https://groups.google.com/forum/?fromgroups#!forum/yesodweb) would be a great place for the discussion. Just to echo some comments from below: if you could provide a function that returns the HTML, CSS, and Javascript separately, I think Yesod could consume it easily. Essentially, it would look like: let (html, css, js) = compileToTuple ... toWidget html toWidget css toWidget js Then you don't need to get involved in any static file stuff. We could also have a nice high-level yesod-elm package that provides `elmToWidget` that would do all of that automatically (we already have a number of such adapter packages).
No. Firstly because data families are not promoted. Secondly because the kind of `B` (which you want to promote) is `A -&gt; *`, and we don't promote things with kinds other than `*`, `* -&gt; *`, etc. (That's basically because we only have a single sort.)
I kind of agree with you there - CSS and Html seem a little bit messy, on the other hand they let you make "real" websites immediately with all the bells and whistles. I'm wondering how writing applications in Elm with FRP compare to those written in MVC binding frameworks like AngularJS or Silverlight. That kind of application is endlessly better than imperative DOM manipulation, perhaps Elm can do even better. (After all you're still writing Javascript otherwise...) 
I'm interested, but that was a year ago. Is a new edition still under consideration?
&gt;If we were programming in PHP, I wouldn't be writing this response That's exactly what this reminded me of: PHP devs claiming "its not a bug" in response to all the "misfeatures" in PHP.
To me it seems that at this point that you're talking somewhat past each other. I'll try to sum up what I'm missing in your response here as a 3rd party: * Haskell is awesome at enforcing invariants, but claiming that not enforcing them is a bug is still false. It is suboptimal, but not a bug. Snoyman claims this is not only for simplicity but also for performance. * You've not addressed his claim about left-over input support. Not having it and thus not having the difficulties involved is not an acceptable solution * You've not addressed his request to compare a conduit-ecosystem to a pipes-ecosystem. * He claims the monad laws were only broken because you've violated the invariants in the first place. Thus the problem is again just lack of type-level forcing of invariants -- and not a new/different problem. 
This brings up another point which I never really emphasized. Type-classes that have laws reflect a semantic contract with your end-user. If I create such a type-class that has certain laws associated with them and I break them, my users can point to the violation and say "Look, that's definitely a bug because it violates this law". However, if you don't use type-classes (or don't enforce their laws), there is no contract with the user at all and you can simply evade any bug report by saying "That's not a bug" and then it becomes completely a matter of interpretation. The point is that type-classes with laws offer a mathematically rigorous way to define correct behavior, and while you are free to not implement type-classes (like conduits chose not to implement Category), it just means that you guarantee your users less. In this scenario, the correct behavior is whatever the developer says it is and users of the library have no standard of correctness to appeal to when reporting bugs, forcing the debate to reduce to a bunch of opinions.
ResourceT ensures the safe release of allocated resources even in the face of IOExceptions. Pipes' approach to finalization is neat but it doesn't seem to handle those kinds of situations in its current version.
As has been pointed out before, the "contracts" are not that rigorous since they use an implied notion of "equality" (which is just some suitable congruence relation).
&gt; Snoyman claims this is not only for simplicity but also for performance. I disagree, but the reason I have been deferring this point is that I'm busy doing precisely what Michael always asks me to do: show him the code. I'm developing several rewrite rules in `pipes` that take advantage of the stronger laws to optimize frames faster than `conduits`. It already works for my projects, but I still have to demonstrate the rewrite rules will fire under other scenarios. &gt; You've not addressed his claim about left-over input support. Not having it and thus not having the difficulties involved is not an acceptable solution Point conceded. I do plan on working on parsing, but I have limited resources (`pipes` is not related to my job) and until I do I cannot contest this point. &gt; You've not addressed his request to compare a conduit-ecosystem to a pipes-ecosystem. He never actually requested that, but I'll bite. `pipes` has no ecosystem because it is developed solely by me, and all of my time right now goes towards establishing a sound theoretical core so that other people can easily build upon it without ever having to redo the underlying theoretical work that I did. I can only do so much by myself. &gt; He claims the monad laws were only broken because you've violated the invariants in the first place. Thus the problem is again just lack of type-level forcing of invariants -- and not a new/different problem. I addressed this in the comment you replied to. Type-level enforcement of invariants is the biggest strength of Haskell, so much so that I consider the reliance on invariants an actual bug, for all the reasons that I gave in my reply that you ignored (especially equational reasoning, which is the entire reason type-class laws even exist in the first place). I am perfectly okay with Michael saying that this is the only way he knows how to implement it, but just because he doesn't know how to fix it doesn't mean that it's not a bug. That just makes it an open bug. And it's not enough to say "Oh, just don't break this invariant", when reasoning equationally. What sounds like a simple sentence or two in English corresponds to a series of really complicated case expressions three levels deep **every time you associate a monad**, which occurs 2*4^3 times when working through the proof of the associativity (4 cases, all of which require the association because they trigger a nested bind through `pipe` that requires associativity, 3 conduits per each side of the equality, 2 sides of the equality). And a lot of early proof reductions that reduce the search space of the proof to avoid combinatorial explosion are simply not possible because of those gratuitous case statements. So, no, it's not enough to say "just don't break the invariants".
Honestly, the left-over input issue is most of what makes iteratee-like libraries so inherently ugly (after seeing several attempts at the same idea, it's starting to feel more and more inherent, at least), and I'm glad that you've omitted it. It feels like a lack of separation of concerns, in many ways.
Well, I'll be happy to be proved wrong, but I'd much prefer to find a "layered" approach to leftovers where they're encoded a layer up (possibly by increasing the flexibility of the underlying machinery) rather than as part of the machinery itself.
Actually, it does permit ResourceT. It's quite trivial to implement ResourceT for frames, in the exact same way Michael implemented it for conduits. I just haven't done it, because as all things I want to first check to see if there's a more elegant way first (for example, `pipes` showed that you can decouple the concept of finalization from the base monad completely, and it would be nice if you could do that for exceptions, too), but there's a good likelihood I will include it in the next patch anyway since it's a highly requested feature. It might surprise you that other than the bugs I noted in my post Michael and I inadvertently converged on a very similar mechanism to finalization (if you ignore the leftover input handling and some other problems I haven't mentioned yet). Michael does the exact same trick of using stored monads to handle upstream finalization and passing Nothings for downstream finalization, albeit his version has subtle bugs in each that prevent a category instance, which is what I was trying to help him correct. If Michael fixed the problems I mentioned in the way I specified in my blog post, his solution would be very remarkably close to frames.
&gt;Replaces tabs by four spaces Could you add an option to turn that off? I indent with tabs for a reason, some of us old blind people have a hard time with small indentation sizes like 4 characters. Tabs allow presentation issues to be handled on a per-user basis, rather than forcing presentation into the code itself as with spaces.
Right — I was responding to your general statements about typeclasses and laws. I didn't mean to say anything about conduits and pipes.
Yeah, and thanks, actually. Just answering your question helped deepen my understanding of denotational semantics a little because in answering it I realized that our intuitive notions of "minor violation" and "egregious violation" of laws (or, equivalently, the difficulty of their congreuence relations) correspond quite closely to the difficulty of the corresponding proofs.
Can I make a feature request for leading instead of trailing commas (i.e. "Utrecht style")?
For what specific feature? Export lists? Language PRAGMAs? Or list literals?
Hmm, is there a real readability benefit to aligning imports? What I'd really appreciate with imports is something I can pass a file and get all unused imports removed. Doing this manually when -Wall complains is annoying.
&gt; Hmm, is there a real readability benefit to aligning imports? Not always, but for cases like import Data.Map (Map) import qualified Data.Map as M I think it definitely is. &gt; What I'd really appreciate with imports is something I can pass a file and get all unused imports removed. Doing this manually when -Wall complains is annoying. I'd like that as well, but I think it's pretty tricky to implement (need to call out to Cabal etc.). So it's not on the short-term roadmap.
I would have thought that since it's not your library that you don't really get to decide whether it is isn't enough to say "just don't break the invariants". I'm pretty sure Michael has addressed that a few times. Hell, even you acknowledged it in the Reddit discussion: &gt;Regarding documentation/invariants, I suppose this reflects a philosophical difference between our two libraries Which, as an aside, was in response to a comment of Michael's that included this: &gt; (Again, my same claim from months ago stands: prove me wrong by actually reimplementing the conduit ecosystem, and then we can discuss which approach works out better.) Anyhow, you've made it clear that documenting invariants is a deal breaker for you. And, by the fact that this discussion is happening, it's clearly a matter of opinion. The tail end of the comment of yours that I'm replying to really comes across like you've decided you have the right to decide for everyone, in a final manner, what opinion they should have on the importance of type level enforcement of invariants in work done by other people. I've just been watching some technical-thread-drama in another language where someone wanted to apply their opinions to everyone but wasn't taking the time to read and understand the other posts in the thread. I really wasn't expecting to see parallels to that in a Haskell thread quite as soon. I'm not trying to get personal here. Just suggesting it might be an idea for everyone to calm down, wait a few days, then read through all of the comment threads again while trying to understand where each other are coming from.
A tab stop has been 8 spaces most everywhere most always. Indent has varied across communities, as has the friendliness of the relationship between tabs and indentation/layout 
If Michael is allowed to be opinionated, then so am I. This is clearly a double standard.
The only thing, as far as I know, that an iteratee-inspired approach (and yes, I know conduit is not exactly that) has ever had going for it over a lazy IO approach is that it statically disallowed certain types of bad behavior and therefore was supposed to be guaranteed robust with regards to certain sorts of finalization issues. If we say that developers have to make sure they manually respect invariants, I fail to see why we don't just use lazy IO, which is mainly very easy and straightforward. I would also note that the way in which "buggy" is used in this discussion, while not exactly how I would use it, is not all that different from Oleg's argument that lazy IO is "unsafe" or "incorrect". So there's clearly some precedent here.
I think this comes down to whether the invariants are just about using the exposed constructors or other "internals" pieces, or if they are something you must keep in mind when using a high-level API as well. And I'm nowhere near familiar enough with conduits, nor able to understand the documentation at a glance, to say for sure whether that's true. I think all the options on the table (pipes, pipes-core, and conduits) currently expose far too much by way of internals, and one of the results is that it's very difficult to tell how serious some of these things are. Maybe someone with some practical conduits experience can speak to that.
There is no wiki and the code in the readme is monospaced.
So the reason that I'm pretty opinionated on this subject is precisely because Michael's approach is antithetical to (what I consider to be) the spirit of Haskell, and it bothers me that nobody else seems to be interested in correct and sound solutions. I mean, if he wanted a working solution, those already exist in many other languages, but Haskell is about striving for elegance and correctness, not doing the hackish thing.
I will say that for all the problems with lazy IO, a broken monad instance was never among them. (saying this, i now expect someone to pop along and show how lazy IO does mean a broken monad instance any second now, probably in conjunction with `seq` :-))
It's not about whether people are opinionated or not. I would have thought that would have been clear from what I wrote. I haven't come across Michael acting as if his opinion should be imposed on other people's work. If he's been doing that I'm unaware - in which case it's more a case of lack of information than a double standard. If that's the case, then I apologize. It seems like you're reading / trying to understand less and less of what people are writing in response to you, so there's not much point in trying to go down the "discussion" path. Maybe "snark" will work were "discussion" failed... I consider the lack of pony pictures in pipes a bug, and if you lack the ability to add them, then it's just an open bug. All libraries that don't provide pony pictures should change to adapt to my view, regardless of if they think they have good reasons not to. This is the final word on the matter. 
I totally agree. I am very interested in a correct and sound solution -- because I think it will ultimately be more powerful and easier to use. Now, that seems obvious -- why would anyone prefer an incorrect and unsound solution -- but apparently a lot of people do. As you know, I have experimented with developing an HTTP backend around pipes 1.0 and pipes-core, and I am very keen to explore a pipes 2.0 solution. I am very much attracted to your commitment to leveraging a well defined mathematical basis and proving that your solution satisfies the laws. 
Yeah. Personally, I want the LANGUAGE pragma to be as invisible as possible, and taking up the first 20 lines of my source code is not the right direction. I pretty much never read the PRAGMAs. I just add them until the compiler is happy. It seems to me that the primary reason for one-pragma per line is that it is easier to sort them alphabetically by hand. (Which, serves no real purpose anyway). Since a tool is doing the sorting now, that becomes even less relevant. 
I still think that the conduits camp is right about the need for parsing for HTTP, so I will move this to my next priority and if I make progress I'll let you know.
You're still claiming that he requires them in his library while he disagrees. He's free to do what he wants with his library, you're free to use it or not. My point was never about answering people's arguments - it's about the fact that there have been quite a few instances in these discussions where it is clear that you haven't been reading all of what people are writing. That's fine - it's your choice what you do with your time. It just makes it borderline futile to respond at length. As I said in my first comment: &gt;The tail end of the comment of yours that I'm replying to really comes across like you've decided you have the right to decide for everyone, in a final manner, what opinion they should have on the importance of type level enforcement of invariants in work done by other people. I was just trying to point out that that's what I've taken away from what you've written and that others might too. 
The other side of "add them until the compiler is happy" is "delete them until the compiler is unhappy," which becomes super easy if they each have their own line.
Patches are obviously welcome, but it's also more or less on my TODO list -- the first step is to make it configurable in a nice way (command line arguments, a per-project dotfile...).
&gt; it bothers me that nobody else seems to be interested in correct and sound solutions For what it's worth, I am very interested, and I deeply appreciate the blog posts and code that you have published. :) However, I'm also interested in seeing Yesod being put to use in The Real World, and Conduits really were a step up from Enumeratees and friends in terms of usability. Conduits is still only months old, but is has proven itself powerful enough to be put to real use, and I applaud Michael and the Yesod team for their efforts thus far. Sometimes it might seem like Michael is being overly defensive of Yesod, Conduits, etc. (I am reminded of the little discussion that went on when Greg got defensive on a Hacker News thread.) But just keep in mind, the Yesod team is trying really hard to provide a great Haskell solution to web development that can be used *right now*, and the last thing they want is for people to start doubting that Yesod is a good choice. I think that, despite its imperfections, Yesod really is a great choice *right now*, and the future is even brighter, especially thanks to theory-minded folks such as yourself.
&gt; small indentation sizes like 4 characters Now I feel bad for using 2-char indentation in my Haskell code. Or 1-char indentation in Coq proofs.
What I'd like is a magic button that changes `import Foo` into an expicit import list of only the things I actually use from the module: `import Foo (bar, baz)`. Or, a magic button to add the correct import line so I don't have to even think about it in the first place.
If you remove tabs you must replace them with their semantic equivalence, and not 4 spaces. A TAB moves the column position to the next multiple of 8, so a TAB should be replaced by 1 to 7 spaces depending on usage.
Oh, I didn't realize. [Don Stewart](http://twitter.com/#!/donsbot) retweeted it recently, which is how I saw it. I assume that means... *something*.
This would really be great. Maybe the function could take turns asking about each redundant import?
As I said above, due to use of tabs by editors even for the alignments (which aren't particular to Haskell), it doesn't actually allow people to configure their preferred indentation width, it only makes for a messed up code-base.
Yes, I was thinking of our discussion when I wrote that. :) I'm undecided myself; in a way, LANGUAGE pragmas are like imports and we usually don't want those to be as invisible as possible. On the other hand, there's a fixed set of allowed extensions whereas each module can export a number of names and each package a number of modules... So it tends to be more obvious from source code which extensions are used anyway (also evident in how GHC suggests extensions that might make code compile).
[Here's a simple feature](https://github.com/haskell/haskell-mode/commit/ec0caabd37e827a40543aa6ac38e97e779239ba2) which will prompt to remove each import line when you load the module. You need to enable: (setq haskell-process-suggest-remove-import-lines t) (Or `M-x customize-group haskell` and go from there.) It's not on by default as this might not be an ideal use-case, sometimes you're aware of redundant imports but they need not be removed right now. So I need to make a command that can remove imports at any time. But I'll do that when I get home. At the office now.
Awesome! I have other things I need to do first anyway. Good things are worth waiting for. In practice, the lazy IO HTTP backend in Happstack is pretty darn good. I am not sure I have ever experienced the issues that enumerators/conduits/pipes are supposed to be fixing. Perhaps things don't get finalized as early as they could -- but I have no problems running servers for months or years without restarting them. So I can afford to wait a little longer. 
[TIL what a Knol was](http://en.wikipedia.org/wiki/Knol)
Explicit import lists are standard style even in Eclipse/Java, because even though the IDE can search your entire dependency tree to find a symbol, it is still fragile and bit magical in a scary way. And sometimes you are reading code on a web page, not a IDE. Automatic management of those lists is of course a great feature. 
No that's how most editors display tabs, that isn't how ghc treats tabs for calculating indentation level internally.
I haven't used emacs, but I've used a lot of other editors, and a grand total of 0 of them work the way you describe. Typically, editors have a setting for "insert spaces instead of tabs", and that's it. Most editors don't support arbitrary alignment at all, as it is language specific. Those that do, generally support it through plugins or extensions, which almost always use spaces. Haskell is unique in that it is the only language I'm aware of that encourages alignment *instead of* indentation, which certainly confuses the issue. But I don't do that, I treat haskell like I would treat any normal language, and indent it as if it weren't crazy.
Can you give an example of an editor that uses tabs for alignment? I've never seen that before.
I hope so, a new edition would be really nice as things change pretty quick in haskell land.
Right. I do want explicit lists -- I just want the managed automatically. 
right. For conflicts it has to either be really smart (which is hard, since you'll be in the middle of the code, so it won't even compile) -- or it has to be dumb and just ask you. But, it should ask you in a non-instrusive way so that you can just keep typing and come back later to fix it.
The way you frame it, you make it sound like yesod couldn't be used if not for conduits. Conduits, wai and warp did not need to be written to make yesod, happstack had been put to use in the real world for years already at that point. The yesod team is trying to provide a PHP solution for web development, written in haskell. We already have great haskell solutions that can be used *right now*. Yesod isn't one of them.
&gt;He's free to do what he wants with his library, you're free to use it or not. He's also free to warn the rest of us about it, as he is doing.
I agree with this. I once noted somewhere else that I'm actually indebted to conduits to providing a practical alternative until I get pipes completely correct.
&gt; The way you frame it, you make it sound like yesod couldn't be used if not for conduits. No, he didn't. At least I did not interpret it that way at all. He only says that Conduits were a move forward in usability over Enumeratee et al for doing things like streaming I/O. Nowhere does he 'frame it' such that 'Conduits were required for Yesod to ever be usable.' Indeed, Yesod went through well over a year or two of development without them (and used Enumeratee,) and people shipped Real Work using that as the basis, and I think everyone in this discussion is fairly aware of that, as problems with Enumeratee were the basis for creating Conduit in the first place. I think you're reading far too much into it. Judging from what I've seen you've say elsewhere on the matter, you clearly just seem to think Yesod is simply terrible however and have no plans to change your mind on this note. That's fine and you're entitled to believe that. But Yesod isn't *really* the point of this discussion, `drb` only used it as a point for discussing the possible rational behind Conduit's current design to `Tekmo`. I think you're reading too far into it for the sake of making a cheap shot out of nowhere. The Haskell community tends to pride itself on being civilized - even if opinionated on the matter as Tekmo is, which is good. But frankly I don't like the tone or attitude in this post of yours at all, because I don't think it's civilized, and even more than that, it's not even helpful or constructive. It's just an outright cheap shot for the sake of it.
&gt; The way you frame it, you make it sound like yesod couldn't be used if not for conduits. Conduits, wai and warp did not need to be written to make yesod, happstack had been put to use in the real world for years already at that point. Perhaps I was unclear. I simply meant to say that Yesod switching from iteratee to conduit was a Good Thing, and that it should be unsurprising that a development team gets a little defensive of their brainchild. &gt; The yesod team is trying to provide a PHP solution for web development, written in haskell You do realize that comparing something to PHP is pretty much the highest insult you can deliver 'round these parts? That was a little over the top. &gt; We already have great haskell solutions that can be used right now. I agree, Happstack and Snap are quite usable. &gt; Yesod isn't one of them. I disagree.
&gt;I simply meant to say that Yesod switching from iteratee to conduit was a Good Thing How was it good? I mean even for someone who likes yesod and wants to use yesod, how was delaying yesod itself to work on a library and web server a good thing? &gt;You do realize that comparing something to PHP is pretty much the highest insult you can deliver 'round these parts? That was a little over the top. I don't see how it is over the top. That is precisely the attitude conveyed by the yesod developers, and the software they write reflects that. It is all about "this is good enough for me and I don't care if it is good, or correct, or to learn about the basic underlying fundamentals of computer science so that I can make correct software". That is the very essence of the PHP attitude, and it comes through in hackish, incorrect, buggy software like conduits, warp and persistent.
The problem isn't that GHC can't generate code. If it was that simple cross compilation would have been done long ago. The real problem is that it requires a lot of build system work to correctly - and generically - interoperate with a cross compiler 'as you expect'. GHC and Cabal for example both invoke GCC for compilation (at one point or another,) so you need some means of saying 'this is a directory holding a cross compilation toolchain for the target' (similar to gcc's `--sysroot`.) That's just one thing, I'm sure there are tons of others. Another factor is that because GHC has a two stage build process, at one point or another if you want features like Template Haskell, GHCi or DPH, you have to inevitably run code (the stage2 build) on the target itself. So as it stands, cross compilation does eventually break down. But you can have a registered stage1 compiler - you just miss some useful features. At one point, GHC also baked in a lot of knowledge about the target platform into the compiler build, but this obviously breaks in the face of cross compiling at some level. However, a lot of this has been lifted out to be runtime configurable (no more `#ifdef`.) GHC has all the technical work done, from what I can tell. The remainder of the work is mostly boring - hacking build systems and cross compiler GCC toolchains and basically a lot of work that is just *boring*. Getting copies of the RTS working, doing any porting details you need, figuring out how you're going to distribute the cross compiler, stuff like that. Stephen Blackheath has contributed a bit of nice build system work in this area, but still, most of the remaining work is just boring and time consuming, not intrinsically difficult. And people could be working on a million other non-boring things.
I'll add to the list: you should definitely be looking at Haskell start-ups. They don't need convincing of Haskell, they're excited if you know it (it's still a good filter; that Haskellers will be a certain kind of smart). You just need to have the right domain experience and a solid grasp of Haskell, with a demonstrated experience of the intersection of these two, and they will be eager to have you on board (why wouldn't they be?).
&gt; I am reading what he wrote: "the Yesod team is trying really hard to provide a great Haskell solution to web development that can be used right now". That implies that conduits are necessary for yesod to be used "right now". You've taken his phrase completely out of context and you know it. The original author of this post even *himself* says you misinterpreted it. Go back and read that paragraph, and the post above slowly. Yesod has always been usable 'right now'. He is not saying Conduits are imperative to this - Yesod was usable 'right now' long *before* conduits appeared. He's commenting on the methodology that they have taken in their development process, and saying Conduits are a step forward (which you're free to disagree with.) &gt; You seem to be confusing "civilized" with "never saying anything negative". I may not have been clear. No, I'm not, and I never said you couldn't say something negative. There's nothing wrong with disliking design decisions and thinking there are better routes, or disagreeing with their choices. That's fine. This is exactly what Tekmo is doing and most people seem to be happy with the work he's done. It is fine to be opinionated as he is, and it's fine to think there's a better option or route. When I say 'civilized', I am not referring to the literal words you wrote. You do not have to completely agree with someone's decisions to be civilized. It's more about *how you're saying it*. Let's be clear: there is an implicit, underlying tone in the above post, and it's not very nice, especially the last bit. In fact your post is mostly fine I guess, except for the last bit, where you basically imply Yesod is utter garbage not to be taken seriously, and follow this train of thought in other replies. You don't have to praise Yesod, Michael, Conduits or any of the above. You don't have to like any of them, in fact I can tell you don't like any of them at all - I've seen your words elsewhere. But I don't think it's very much to ask to do so in a polite manner. You know what you could have said that would have been nicer, ignoring the bit you misinterpreted? How about this: &gt; There are already other web frameworks in Haskell that work 'right now' and have been shown to work, and I personally think they follow a better route in terms of both design and methodology than Yesod overall. I dislike the approach Yesod takes, for example, in relation to &lt;technical issue&gt; and &lt;social issue&gt; and &lt;design issue&gt;. I think people should really consider these things more closely when choosing frameworks as time goes on, because based on my experience, they're important, and shouldn't be overlooked so casually. 'Right now' I would not choose Yesod based on this. That wasn't very difficult at all, it was opinionated, it directly addresses a few points, and it doesn't have the outright confrontational tone and nastiness you've displayed here so far that basically sums up to "this is garbage, just don't use it."
I've read a few stories like yours, and I think it's great! Would be nice to have them gathered in a single place (HaskellWiki?) with some more details.
Default-configured emacs (unless you enable [Smart Tabs](http://emacswiki.org/emacs/SmartTabs)), default configured Visual Studio, default configured Eclipse, and every other editor I tried that used tabs. I would venture vim does so too by default, but I haven't checked. It's really frustrating to find all the code in a mess where every line is at some random column, because everyone is editing the same file with varying tab width settings.
Can you name an editor that inserts spaces for the post-indent alignments in its default configuration? All editors I've tried/used do the wrong thing with tabs.
&gt; I use haskell in my everyday job. And the way to do it is just bring it to your company. Start using it yourself. Be open about it. Explain the benefits. You'll be surprised how reasonable the management is about it. I wish that were true in my case...
No need for `seq`, it's enough to just `case` :)
Manually align how? I don't understand what you mean. You can't align stuff by pressing tab, that indents. What key are you pressing in what editor?
Yesod does a lot of work to get a lot of static type safety. That's not in the "PHP spirit". Conduits improved the simplicity and performance of a significant chunk of Yesod. 
See "contribute to an open source Haskell project that you're interested in".
I just double checked eclipse, intellij, nvi, vim and notepad++, which is what I have laying around installed somewhere. None of them do what you describe. Intellij, notepad++, nvi and eclipse don't even have any options to do arbitrary alignment of new lines. Of those, only eclipse does any alignment at all, and it only does things like: int foo = 5 string foobar = 6 It does those correctly, using spaces. The only one that actually does what you describe is vim, but only if you are relying on the totally language unaware auto-indent to do your alignment. Which is what I was getting at when I asked how are you doing your alignment. If you use indentation to do alignment, it obviously won't work. You need to use an alignment plugin to do alignment. I'm not sure if there are any generic ones, as so few languages encourage aligning lines to arbitrary columns, but I believe there is a haskell one at least.
Argh! I was just about to write you about this! One question of Michael's gave me the idea of copying your dynamic trick to recover the monad. I'm very interested in recovering the Monad instance for Frames because the parametrized monad bugs me so much. What I was thinking was exactly what you suggested about not statically disallowing awaits after a close, but instead treating it as if the pipe had received a closed signal from upstream (i.e a Nothing) so that if it awaits again it first rethrows the Nothing downstream (i.e. it triggers a "schedule", if you read the code). I really badly want to unify the monad and category instance into a single type and I THINK this might work, but I need to try it some more. I've also been working on stream-fusion-like tricks for pipes. I'll e-mail you about it because I had a lot of questions about how to do this in a way that makes the rewrite rules always fire and I know you have experience working with GHC. I'm going to check out your pipes-attoparsec because I want to see how you layered parsing on top of pipes-core and see if I can apply the same trick to frames.
Yesod uses tons of unnecessary template haskell and qq to get their type safety, which is no more type safety than you can have in any other haskell framework. So, again it is the "its good enough for me and I don't want to know about doing it right" approach. Conduits made for warp, which makes for good benchmarks to bring in more PHP and rails developers who care more about a web server that is fast than a web server that parsers HTTP headers correctly.
&gt; Yesod uses tons of unnecessary template haskell and qq to get their type safety, which is no more type safety than you can have in any other haskell framework How would you generate a human-specified URL parser/generator type-safely without unsafe repetition and without any QQ/template haskell? The other Haskell frameworks use the combinator approach that AFAIK is *also* available in Yesod but is actually less safe. &gt; Conduits made for warp, which makes for good benchmarks to bring in more PHP and rails developers who care more about a web server that is fast than a web server that parsers HTTP headers correctly Do you have any bugs you can point to in HTTP header parsing, or are you just spreading baseless libel?
Space key, many times. The "space" vs "tab" key does not actually insert space/tab in most modern editors. Press space N (N=TabWidth) times, and the editor will insert a tab.
&gt; You'll be surprised how ~~reasonable~~ gullible the management is about it. FTFY. **Source:** From my previous job. I explained all of the technical merits but left out all of the economics of hiring Haskell programmers.
Is it solely eagerly evaluated? It doesn't look much like Haskell to me at all.
&gt; I'm very interested in recovering the Monad instance for Frames because the parametrized monad bugs me so much. Why does it bug you? Only because it is not mainstream (no do notation without NoImplicitPrelude) or is it something more fundamental?
Something more fundamental, sort of. Everything about the monoid/comonoid stuff is completely independent of the notion of finalization and the parametrized monad. The only reason the parametrized monad exists at all is because I specialized the more general solution to work with finalizers, otherwise it would just be a monad. This tells me that there should be a way to decompose the problem into two separate problems, the former being the monoid/comonoid framework and the second being finalization. Then the user would be able to choose whether the finalizer safety is enforced statically, as in my approach, or dynamically, as in Paolo's approach. The underlying issue is that statically safe finalization is inherently not a monad and requires a parametrized monad, but I want to be able to decompose the problem so that users can decide for themselves whether they want a monad or static guarantees. However, the decomposition still eludes me.
I can't remember the last time I used a code editor in a configuration that inserted hard tab characters in any situation, except when I force it when I hack on a spreadsheet file..... I guess some of them allow tabs in factory configuration that I never see in my distros/environments, but I shudder at the thought. 
It seems to have quite a lot of feature for being mainly a research project. Definitely a good effort in my opinion.
An interesting project, but it makes too many Java compromises to seriously interest me. There are no examples of providing type annotations for a function declaration, which disturbs me. It definitely leans a lot more towards Ruby than it does Haskell. Nevertheless, there are some cute features (mostly pattern matching) that at least set it apart as "better than Java".
Words of wisdom. But sadly enough, recruiters (esp. human resources people) are not always as wise as they could be.
I think the `Pipe` type you're providing is much closer to what `conduit` has than what `pipes ` has. I'm seeing the following differences: * `conduit` includes leftover support. In `conduit` 0.5, this will likely be a separate `Leftover (Pipe i o m r) i` constructor. I explained the proposed change in [my most recent blog post](http://www.yesodweb.com/blog/2012/05/next-conduit-changes). * `pipes-core` includes a `Finalizer` on `Pure`. I'm not sure I understand why this would be necessary; can't you always wrap with a `M` constructor to get the same effect? * Your `M` constructor uses a `Pipe` in the second field, whereas my `PipeM` constructor uses a `Finalize`. The reason for this in `conduit` is that the second field of `PipeM` is only used when a downstream `Pipe` has terminated, in which case no further output will be requested. Is there an inherent difference that I'm not aware of? * `pipes-core` has built-in exception handling support, while `conduit` does not. What does this buy you? Are you able to implement a fully exception-safe set of file operations this way? Or is there a different goal in mind?
I doubt there will be type annotations. This looks like a dynamic language, and with implicit casts, too. &gt; Some of these operators work on more than one data type. For instance, the + operator can be used to concatenate strings together [..] In fact, as long as the string appears on the left, any value may be concatenated with it
Thanks, Micheal, that's a fair summary of the differences. Point by point comments follow: ### Leftovers ### The `Leftover` (aka `Unawait`) constructor has been proposed for pipes in a few discussions, and it would be great to be able to add it, but unfortunately it has an unclear semantics in certain corner cases, and as a result, it breaks associativity of composition. ### Finalizer for Pure ### This is tricky. The meaning of `Pure r w` is "a terminated pipe with a pending finalizer". If you have something like: Pure r w &gt;+&gt; Await k h then `h` is going to be invoked *before* `w`, whereas if you were to separate `w` from `Pure`, it would be the other way around. This is exactly the unfortunate finalization delay I mentioned in my post. The reason why it exists is to keep things like: p1 &gt;+&gt; p2 &gt;+&gt; p3 where p1 = finally m1 (yield x) p2 = await p3 = tryAwait &gt;&gt; lift m2 associative. Without the `Finalizer` field in `Pure`, associating to the right would execute `m2` before `m1`, while associating to the left would call `m1` first. The current solution always executes `m2` first, which is suboptimal, but I *think* I know how to reverse it. ### Handler in `M` ### This is different from its conduit counterpart. I asked a question in haskell-cafe some time ago because I was confused about it, too. The handler is used when an exception occurs during execution of the base monad action. This is part of what allows exception handling primitives to be lifted to the `Pipe` monad. ### Exception handling ### The goal is to be able to catch exceptions within the `Pipe` monad in user code. This turned up a few times already in our previous discussions. As I understand, conduit's approach is to execute a "pipeline" in stages, handling exceptions around each stage, and it uses mutable state to implement that. With pipes-core, you can keep everything in the `Pipe` monad, and compose each stage normally. I believe this is the cleaner approach and gives better reusability and composability. There is also a way to implement something similar to conduit's "connect-and-resume" operator, using the coroutine-like interface in pipes-extra. That interface, however, exposes some of the finalization internals, so you can break exception-safety if you're not careful. 
&gt; While overall your pipes package is really clean, this awaiting twice business looks like an ugly hack. I don't see it as a ugly hack, but a natural way to extend the semantics of `await` to exceptional cases. Once you give up on the idea that every `await` should succeed, throwing an exception is the most natural result for a failed `await`. If the failure is caused by upstream termination, pipes-core raises a `BrokenPipe` exception, which, if unhandled, results in the upstream value to be returned. Note that this is *not* a special case, but the normal termination behavior, consistent with pipes 1.0. I don't understand your comment about hurting composability. I'd actually argue that this approach is the most composable one. One nice example is the `defer` function you mentioned. In pipes-core, you can use `discard = forever await` as `defer`, and it will have the desired semantics in every situation, **precisely** because of the way failed awaits behave.
Ah, sorry, I forgot to mention that there is a difference between `tryAwait` as described in my [guarded pipes] blog post, and the actual implementation in pipes-core. While the original presented the inconsistency you describe, the current version will return `Nothing` every time, if used after upstream termination. [guarded pipes]: http://paolocapriotti.com/blog/2012/02/02/guarded-pipes/
I like a lot of things about this package - the exception handling seems to behave in an intuitive way, and I like the concept of the BrokenPipe exception a lot. What still seems unintuitive to me is that the upstream pipe in a composition can provide the return value. I realise this is quite a deep property and comes from the category laws (p &gt;+&gt; idP == p), but it seems strange once you have the BrokenPipe exception. e.g. intuitively I would expect runPipe await to throw BrokenPipe. It doesn't: it just returns (). What would go wrong if you changed this to throw BrokenPipe, and discard the return values of upstream pipes?
&gt; If I'm understanding correctly: in conduit, when the second field of PipeM is called, it's indicating that the downstream pipe has closed, and therefore its only purpose is finalization. In pipes-core, it could also be indicating that the upstream pipe threw an exception. If that exception happens to be BrokenPipe, it indicates that you should finalize. But other values could allow you to continue processing. Is that accurate? No, you can continue processing in either case. The only special feature of `BrokenPipe` is that, if it goes unhandled, it results in the upstream return value to be used, instead of being rethrown as a normal Haskell exception.
I'll think about that some more and get back to you. In the meantime, I'm struggling to understand this behaviour: &gt; let catch a b = catchP (do await; return a) (\e -&gt; do lift $ print e; return b) &gt; runPipe $ catch 'a' 'b' &gt;+&gt; catch 'c' 'd' BrokenPipe 'd' haven't we lost the exception handler on the left somewhere?
I think the confusion here is about the behavior of an `await` in a Producer. Since there is no clean way to disallow awaits for producers, `runPipe` simply feeds an infinite stream of `()` to the pipeline. So the `catch 'a' 'b'` pipe in your example doesn't hit the exception handler at all, since its `await` simply returns `()`.
I didn't mention that because I guess I just think of it as a side effect of the normal scoping rules of each language. Also, that doesn't always apply to GHC-extended Haskell, because of Overlapping-/IncoherantInstances.
Pressing the space key many times inserts spaces in all the editors I have tried. Like I said before, only the line *after* that will be messed up in vim, *if* you are using auto-indent to try to keep things aligned (which you shouldn't do since it is for indentation). I just hit space 24 times and typed some stuff. I saved, exited and opened it again and it is still 24 spaces.
I tried exactly that in all the editors I previously mentioned. None of them converted my spaces into tabs.
Actually, this concept that the upstream pipe can return results (and *must* do so) is one of the things I've had the hardest time grasping in `pipes`. Coming from an `enumerator` background, it was natural to set up `conduit` so that only the `Sink` returns results. I don't really see the motivation (besides getting to say it's a `Category`) for the `pipes` approach. For example, I was really troubled with the following from the `pipes` tutorial: runFrame $ (Just &lt;$&gt; toList) &lt;-&lt; (Nothing &lt;$ fromList [1..10]) Or the fact that in `pipes-core`, you would write: runPipe $ fromList [1..10] $$ consume But it would produce a `Maybe []` return. For contrast, the equivalent in `conduit` would be: sourceList [1..10] $$ consume And the return value from there is `[1..10]`, without a `Maybe` wrapper. This is at least the behavior I would expect, and likely most people coming from the `enumerator`/`iteratee` world would as well. Is there a deep, compelling reason for this setup that I'm missing?
&gt;How would you generate a human-specified URL parser/generator type-safely without unsafe repetition and without any QQ/template haskell? First, why do you assume routes is the issue? Yesod uses template haskell pervasively. Second, see web-routes for how to do it correctly. Third, yesod is the only framework that forces static routes on you, happstack and snap let you choose dynamic vs static routes based on your requirements. I can't even write the application I am currently working on in yesod because of this. &gt;Do you have any bugs you can point to in HTTP header parsing, or are you just spreading baseless libel? It isn't really specific bugs so much as it is an overall lack of regard for the http spec. When I tried last (~7 months ago maybe) I tossed my tiny header test suite at warp and it failed 19/23 tests. All of the 23 requests in question are perfectly valid http requests, just oddly formatted. Trivial things like whitespace can make warp reject valid requests even, it is making no effort at all to be an http compliant web server. Here's a brief "http isn't as trivial as you think" page that shows what valid headers can look like: http://www.and.org/texts/server-http Yesod assumes a whole lot that isn't safe to assume, and so rejects valid headers. It also didn't handle accept headers correctly last I checked.
I was under the impression general programming consensus was converging on it being actively harmful to readability. The way in which it makes things "look nice" is to essentially remove information that your eyes use to learn what is where and to keep context. The unaligned things look "worse" precisely _because_ they are carrying additional information. Consistent style is good, but over-regularizing source code can actually be bad. (Think the information theory sense of information rather than the human sense of the term. I'm not saying that the irregular indentation is carrying any sort of expressible-in-words information, it simply serves to visually distinguish one code file or function body from another with its distinctive pattern, so information in its raw "capability to distinguish" capacity.)
They all insert tabs *for indentation*. They don't insert tabs when you press space several times.
Thanks for the report, I've created a ticket for this (#76 for wai), support for multiline headers seems to be missing from warp. I don't think the and.org reference you give is entirely correct, since for example the HTTP-version grammar in the RFC (2616 section 3.1) doesn't allow spaces in the HTTP version string, but the first example does use them. Edit: some clarification: I think the RFC says that linear whitespace (LWS) is implicitly allowed where the token or quoted-string production rules are used. HTTP-version does not use them, so i think the extra spaces in the and.org example aren't allowed at all. If the version "1.1" was parsed by the token parser, it would become a single token since "." is not listed as a separator and is not a control character. Feel free to correct me if this is wrong (better yet, make a ticket in the WAI repository with a test case)
Actually, I think that example demonstrates a few suboptimal aspects of this approach: 1. As you mention, there's an extraneous `Maybe`. This is a symptom of the larger problem: there's nothing in the type system enforcing the return value we're hoping for. 2. A corollary of that point is that there's nothing in type system enforcing that a *single* value is returned. Imagine that you have another `Pipe` that counts bytes instead of lines. You could place that downstream of `tee count` and the type system wouldn't complain. 3. And this means that the system isn't really composable: it's making assumptions about having a single return value, which may not be true (I don't think returning both a byte and line count is unreasonable in this example). The approach I would take would be to layer a `WriterT (Int, Int)` on top of `IO` (or better yet, have newtype wrappers for line count and byte count). Then the type system can enforce everything mentioned above, and we can keep our code modular. (You could also have two levels of `WriterT`s and then the two count pipes needn't know about each other.) The code would look something like this in `conduit` (apologies for not type checking it): sourceFile input $$ gunzip =$ CL.mapM (\x -&gt; tell (ChunkCount 1, mempty) &gt;&gt; return x) =$ CL.mapM (\x -&gt; tell (mempty, ByteString $ S.length x) &gt;&gt; return x) =$ pipe f =$ gzip =$ sinkFile output There's no need for extraneous `discard`s or `return`s, we return both values, and there's no concern about adding stuff into the pipeline.
They do when you press space several times.
By a 'decent programmer' you probably mean someone with a few years experience in a Go like language. Are you sure you couldn't 'on-ramp a decent MLer in probably a week'?
I'm liking the latter idea more and more. As much as a *fifth* type parameter bugs me, I see a number of advantages: 1. Downstream finalization isn't a problem, because downstream pipes handle their own terminations. 2. There's a sort of "data" loss that can happen in `pipes` and `pipes-core` when a downstream pipe keeps state between awaits, where the data is actually the state of the downstream pipe. This makes that explicit, where it was hidden before. 3. There's something cool going on with the type of `(&gt;+&gt;)`, where it actually at least initially appears to form two simultaneous categories, and the result can be used in more flexible ways. This is exactly what's needed to avoid the extraneous `Maybe` in Michael's example with consume. I put a sketch of a non-finalizing (pipes 1.0 style) implementation at http://hpaste.org/69274
Such a tired argument. How many haskell shops do we have to demonstrate before this "no one knows FP" argument goes away?
That's a nice idea! I'll think about it some more. API-wise, maybe the fifth parameter could be partially hidden by clever use of type synonyms or newtypes?
That's great, but doesn't it get us back in the conduit situation where a downstream filter has to handle termination of upstream explicitly?
Yes, it does. But note that for all the interesting use cases so far, this isn't actually a problem, as you're always awaiting at the top level. In fact, if you're deep in four levels of looping when you await (the situation where this would be a pain), then sudden forceful termination feels like the wrong thing to do anyway. You're essentially leaving work half finished. Or if it really is the right thing to do, it can be expressed using explicit exception handling that makes it clear you're discarding the state of the pipe (my second point), with finalization and cleanup handled using ordinary exception handling mechanisms (my first point). *Edit:* I wasn't clear. It does get us back into the conduit situation in the sense that *automatic* pipe-style termination is lost. But it leaves us still able to do things that are not composable with conduits, like write downstream filters and maps. That's because we let the upstream produce a return value, and the downstream pipe *can* choose to terminate with the same result. That behavior just isn't automatic any more. It also forms a category, which is important to some of us because it implies that there aren't arbitrary limits or land mines on the process of composing things in stages.
In large companies with procedures and stuff, it's pretty much impossible to push a programming language unless you're really high ranking in the company.
With respect to the first point: that is a very principled approach but I don't think its practical for our business. We only have 2 devs and can barely keep up right now. Until we get more staffing in engineering, we can't afford to train a new dev with the more advanced Ruby stuff if they have no experience in it. That being said, we have high standards for candidate experience so we are in fact looking for smart engineers who know what they're doing (with Ruby). On Erlang, we all knew a little bit of Erlang, but the problem is that none of us had ever deployed or managed an Erlang app. Erlang seems to lean *very* heavily on deep knowledge of OTP and its conventions to have a stable and easy to update application. It was less about correct code and more about getting the thing compliant so we could reliably deploy it. The review was helpful for that but the Erlang VM took a nosedive one day and trashed the mnesia database we were relying on. That was a wakeup call that our lack of Erlang troubleshooting experience was just too risky.
so where did php enter into this, oh learned one? i see your strawman curried favor with your three other reddit ids
I raise you [Dice](http://seeker.dice.com/jobsearch/servlet/JobSearch?op=300&amp;N=0&amp;Hf=0&amp;NUM_PER_PAGE=30&amp;Ntk=JobSearchRanking&amp;Ntx=mode+matchall&amp;AREA_CODES=&amp;AC_COUNTRY=1525&amp;QUICK=1&amp;ZIPCODE=&amp;RADIUS=64.37376&amp;ZC_COUNTRY=0&amp;COUNTRY=1525&amp;STAT_PROV=0&amp;METRO_AREA=33.78715899%2C-84.39164034&amp;TRAVEL=0&amp;TAXTERM=0&amp;SORTSPEC=0&amp;FRMT=0&amp;DAYSBACK=30&amp;LOCATION_OPTION=2&amp;FREE_TEXT=haskell&amp;WHERE=)
&gt; Am I the only one confused/frustraited about this? Nope. Problems with cabal install plague pretty much everyone, especially beginner/intermediate haskellers. &gt; Why is Haskell Platform so far behind? Well, in fairness, GHC 6.12 is only 2 years old ([12 June 2010 GHC 6.12.3 released](http://www.haskell.org/ghc/)), although most people will tell you that pre-GHC7 is ancient history. The Haskell ecosystem is moving ridiculously fast right now. The HP release with GHC 7.4.* was scheduled for a May release (if I'm not mistaken); I'm not sure where we're at with that but I'd expect an official HP with the latest GHC pretty soon. The best advice I can give you is to frequent #haskell irc.
So far the Mac release candidate is working really well for me. Thank you so much for putting all of this together!
Which versions of OSX is the new platform working on? And what about compiling on one version of OSX and running the executable on a different other version? (small rant: I think this is a very serious and old problem, plaguing Haskell on OSX; for example 10.5 is not supported since GHC 7.x; before that, 10.4 was supported if you compiled on 10.4, but not if you compiled on 10.5; but now 10.8 will come out, so this continues to be a problem...)
I figure it's not. I think the author was inspired just by parts of the syntax.
Sure, if you're going to pass through the return value, then you're going to have to wait and see what it is. Though... there's nothing to stop you from implementing a pipe that only re-yields the first 10 values, but then keeps awaiting until a return value is produced, and produces it as a result. I can't think of a specific example right now.
 primes = sieve [2..] sieve (p:xs) = p : sieve [x | x &lt;- xs, rem x p /= 0] = p : sieve (filter (\x-&gt; (rem x p) /= 0) xs) = map head ( iterate (\(p:xs)-&gt; filter (\x-&gt; (rem x p) /= 0) xs) (p:xs) ) = unfoldr (\(p:xs)-&gt; Just (p, filter (\x-&gt; (rem x p) /= 0) xs)) (p:xs) = p : concat ( unfoldr (\(p:ps,xs)-&gt; let (h,t)=span (&lt; p*p) xs in Just (h, (ps, filter (\x-&gt; (rem x p) /= 0) t))) (primes,xs) ) = p : concat ( unfoldr (\(p:ps,xs)-&gt; let (h,t)=span (&lt; p*p) xs in Just (h, (ps, minus t [p*p, p*p+p..]))) (primes,xs) ) primesP = 2 : sieveP primesP [3..] -- postponed sieve sieveP (p:ps) xs = let (h,t)=span(&lt; p*p) xs in h ++ sieveP ps -- (filter (\x-&gt; (rem x p) /= 0) t) (minus t [p*p, p*p+p..]) primesE = 2 : g (fix g) where g xs = 3 : minus [5,7..] (unionAll [[p*p, p*p+2*p..] | p &lt;- xs]) "minus", "unionAll" are on [haskellwiki](http://www.haskell.org/haskellwiki/Prime_numbers#Initial_definition) or in [Data.List.Ordered](http://hackage.haskell.org/packages/archive/data-ordlist/latest/doc/html/Data-List-Ordered.html) package. "fix g = xs where xs = g xs" is from [Data.Function](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Function.html#fix). The "sieve" function is meant to get a list of increasing numbers as its input. It will regard the *first* element in the list as *prime*, and set up a filter which will *keep*, in the *rest* of the input list, all numbers such that leave a non-zero remainder when divided by *that prime*. IOW, it will find out that prime's multiples by *trial division* and remove them from the rest of its input list - and on that culled list it will call itself again, not before it has produced that prime as the first element in the list that it constructs as its *result*. IOW, it sets up a repetitive process where the first element of the input list is detached and put aside as the *next* value produced, its multiples are found in the rest of input by trial division and are removed from it, and the process is repeated with the culled sequence as the new input. That means that the (non-trivial) multiples of primes are progressively removed from the input increasing list of numbers above 1, prime after prime, starting with 2. But instead of testing each candidate number for divisibility by its preceding primes, we can just **generate the multiples** from each prime - and that is **the essence of the Sieve of Eratosthenes**. 
From a community point of view, it seems that it would be easier to explain "why is my program so slow!?" to the few people bitten by the lack of monomorphism restriction in a computing-heavy fashion, than it is to explain beginners why the restriction is there and why (not) care. However, if I were to encounter the same problem as Hughes, I would probably have a hard time figuring this out, so in some sense the compiler makes it easy on you by being so fascistic... I'm not sure that your explanations are more helpful than the few other ones. At least, for a beginner audience, you explained the vocabulary, which is useful. You should maybe add the way to fix genericLen without the pragma, by type-annotating len.
I think GHC recently changed the behaviour? I think the monomorphism restriction is now only applied to non-toplevel functions. So all of the examples at the end of the article will now work without error, but the (len,len) example will still be forbidden.
An interesting trick you can do with this style is use an EitherT monad transformer, to get back to the automatic-termination `await` function, await :: EitherT r (Pipe a b r m) a If you don't like the need for an extra type parameter, then you could restrict it to (), which leads to composition having the type (&gt;+&gt;) :: Monad m =&gt; Pipe a b m () -&gt; Pipe b c m t -&gt; Pipe a c m t I personally can't think of a good use case where an upstream pipe returns a result, so this composition function should suffice for most cases. Then again, having an extra type parameter never hurt anyone, and it certainly looks cleaner.
You are not the only one, but we do seem to be either in the minority or less vocal.
oh yeah? well, I'm going to write my own software to get ghc&amp;cabal-install running. with blackjack, and hookers! but seriously, I'm currently adding feature to hsenv to bootstrap cabal for you. this means, hsenv will be the only binary you need to get started with haskell.
Well I might qualify that by: perhaps not all of HP is really/essential for everyone, but some parts surely are.
HP 2012.2.0.0 should install and run in OS X 10.6 and 10.7. Programs compiled on either should run on either. The command line tools from Xcode are a prerequisite. Xcode 3 for 10.6 and Xcode 4 for 10.7. For 10.7 you can now download and install these tools without Xcode and HP will work on such a system. GHC can be recompiled to to run on 10.5 and produce executables that run on 10.5 and later, but such a compiler doesn't easily install or run on later systems. This is a consequence of Apple's choices in later Xcodes.
I'd say there were a lot of cases, e.g. lists, Maybes, Eithers, tuples, where join is *at least* as easy to define as bind. (As an aside, after reading http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html I always thought that the fish operator (Kleisli composition) in combination with return was the most natural way to understand monads...)
Wow, I can hardly say how great I think this is. I finished my education too early, there are too many great things I haven't learned - and every time I get the chance to learn something like this with a reasonable level of effort, I jump with joy. If there exists something similar for the commutative diagrams of category theory, I will be running down their webserver.
[It's been done for kids](http://dragonboxapp.com/), but I don't see why it shouldn't be possible for adults as well!
&gt; So I ask you this, as a beginner/intermediate, which version of GHC should I be using? Use the Haskell Platform. &gt; Haskell platform (version 7.0.4, almost a year old) or the latest (7.4.1)? Why is the Haskell Platform so far behind? The Haskell Platform is not far behind. One year is nothing in computers. &gt; And why doesn't cabal detect these things and get the correct packages, perhaps with warnings that it isn't the latest? Cabal relies on library developers to set their dependency versions correctly. If they don't, or if they depend on features that are only in the very newest GHC, then it's difficult/impossible to use their libraries if you've installed the standard platform. This is a problem of caused by certain library maintainers. They should hold off on using new compiler features until that compiler is commonly installed. All of my libraries work with GHC 6.12 through 7.4. Most of them also work on GHC 6.10. To do otherwise would be lazy and irresponsible.
Your site appears to be firefox-only. In chrome and opera I just get dumped straight to the rss feed of the comments for that post.
Me too.
As a polite suggestion, if you don't really understand something, I'd suggest phrasing it in the form of questions rather than as an explanation. Haskell has sort of been hurt by the number of such "explanations" floating around, with varying degrees of wrongness in them. (Though we at least seem to be past the phase where every Haskell learner must write their own "tutorial" about the monad typeclass. I credit the ability to just link LYAH.) Explanations to beginners shouldn't be full of "but I might be wrong" and "I'm not quite sure", etc, though kudos for saying so clearly.
Strange; I run Chrome on OSX and had no such problem.
What would this do to the Haskell Platform? What is the difference. I pretty much though HP was just that with some extra goodies (Alex Parser I think). 
`Paczesiowa,` I think the advantages are more obvious on Windows and os x, where something like `cabal install network` or `cabal install glut` are certain to lead to disaster. Before the Platform came out I think half the haskelling I did on OS X was trying to get things to link with the right libiconv. (Grr, since it came into my head, I just verified that `haskeline` is still going to be a torment; into the Platform with it!) The idea of 'blessing' some libraries is I think increasingly important: maybe it's just an accident of my experience, but lately that the meme 'Haskell has lame, ill-maintained libraries' seems to have gone viral. This is a judgment that must come from misconstruing Hackage, something you are not likely to do. I don't know if the Haskell Platform is the best way to put a stop to this nonsense, but it is probably part of the best answer. I really do think that the matter is becoming urgent. There needs to be some way of bringing forward things like `vector` and `aeson` and `attoparsec` and so on in some standard listing.
I agree, GHC(i) + Cabal + Haddoc + GHC Profiler + Code coverage seem to be core tools. Where as Happy / Alex / hsc2hs seem to be extra stuff. If you jsut get GHC do you not ge the libraries included in HP? It could be painful to install all of those by hand. Which ones are the core ones? (Is there a distinction? Should there be?). 
Well, if I made any mistakes don't hesitate to point them out! I think it's fine to admit that I'm not 100% certain about something. The fact that I don't yet fully understand monomorphism restriction inside out is not the reason to refrain from writing about what I already understand.
So what is the danger that HP protects me from? 
ok, that's true. I always forget, that not everybody uses linux.
I must strongly but respectfully disagree. One of the best ways to understand something is to try and explain it to others, so posting an explanation for something on one's blog is a perfectly reasonable way to increase one's own understanding and possibly help others as well, even if the explanation is not perfect. Frankly if the Haskell community has so few well-known places to learn important things from the experts that people instead end up learning from the random blogs of beginners then it is not the beginners posting explanations on their own personal blogs that is the problem.
On the other hand, how often do beginners actually run into the monomorphism restriction in practice?
The download page links to the developer wiki which links to loads of resources including the mailing list. http://trac.haskell.org/haskell-platform/wiki
That's fine, if you know what you're doing you're ok. Also keep in mind that the HP is not just for you to install GHC and libs, it's there to tell package authors what to target and test against. So if you're using the versions from the HP then you should have a better time installing various packages. Yes it's not pefect and we do not force package authors to do anything, but a bit of synchronisation of versions we think helps everyone.
&gt; And as a dummy, I don't know what the advanced features even are, all I know is that I wanted to use Data.Repa (instead of Data.Array.Unboxed) to see if I can get a performance boost. Seems like it is a reputable library (with some important contributors) so I just assumed safety. Repa is actually bleeding edge, that's where the problem lies. 
Ok, this was a huge **wat** moment for me. import Data.List f :: (Num b, Num c) =&gt; [a] -&gt; (b, c) f xs = (len, len) where len = genericLength xs Load into ghci: causes "could not deduce b ~ c". Add `{-# LANGUAGE NoMonomorphismRestriction #-}` to the top of that file, loads into ghci without problem. wat. I thought the monomorphism restriction applied only to expressions at the top level that did not supply their own type signature. Now I realize that the monomorphism restriction was being applied to the *subexpression*, which is what caused the "could not deduce" error. Instead of turning off the restriction, you can just give the subexpression an explicit type signature. where len :: Num z =&gt; z len = genericLength xs Thanks for expanding my understanding of the monomorphism restriction.
I don't believe this is live yet, but I am fairly certain SPJ has been pushing for it. Correct me if I'm wrong. n.b. I also believe this is the way to go. Under most circumstances I advocate supplying type signatures for all top-level functions, but there are some circumstances where it is appropriate not to, and the monomorphism restriction at the top level serves only to annoy.
This thread has numerous examples of downstream pipes and filters where the upstream pipe returns a value, so restricting it to () would be defeating the point.
&gt; non-toplevel functions Did you mean toplevel non-functions here? Currently the monomorphism restriction isn't applied to non-toplevel functions (or non-toplevel values in general) at all. So that would be a rather drastic change.
There's no contradiction. There are 3.5 billion women on Earth and 3.5 billion men and still it's sometimes hard to find a date ;-) What we need is a Haskell dating service. 
Or you can get around it more easily by making len a function, and let the compiler infer the type signature: module TestMono where import Data.List(genericLength) f xs = (len (), len ()) where len _ = genericLength xs then *TestMono&gt; :t f f :: (Num t, Num t1) =&gt; [b] -&gt; (t, t1) This also makes it more clear that you're duplicating a computation.
Nitpick: the MR applies to *bindings*, not expressions. Non-binding subexpressions involving inferred polymorphism and class constraints are fine. poly :: (forall a. Show a =&gt; a -&gt; String) -&gt; (String, String) poly f = (f True, f 'a') showPlus s x = show x ++ s okay = poly (showPlus "!") fine = let f x = showPlus "!" x in poly f bad = let f = showPlus "!" in poly f So `okay` works because the polymorphic subexpression isn't a binding, `fine` works because the MR doesn't apply to function bindings, but `bad` fails because of the MR. 
1. job security is a myth. Nobody knows what the future holds. 2. you don't need a million jobs -- you only need one. So, you just need to ask yourself, "Am I willing to become one of the valuable people in this area?" 3. Most Haskell jobs are created by developers deciding to use Haskell in their current job. So the job market is not "companies already using Haskell", but "companies that trust their engineers to make inform decisions". And, honestly, you only want want work for the latter anyway.
It requires rank-2-types. Prelude&gt; :set -XRank2Types -XScopedTypeVariables Prelude&gt; (\(f :: forall a. [a] -&gt; a) -&gt; f . f) head [[0]] 0
One can be a rank beginner at Haskell, and yet fairly expert at general computer hacking. Figuring out how to install Haskell and packages for oneself is very satisfying, if one is up for the challenge.
Thank you!!! If one is migrating to Lion now, Xcode has changed how it installs, and the official Platform refuses to install. The release candidate works, and is the only viable option: http://www.ozonehouse.com/mark/platform/
&gt; One of the best ways to understand something is to try and explain it to others, Yes, two years ago I would have said this too. However, I'd submit the evidence suggests that when doing it _on the Internet_ the damage to the general community can outweigh the benefits bestowed upon the (putative) explainer. It is a frequent criticism of the Haskell community that it's hard for novices to find good explanations amidst all the wrong ones... and it's _true_. It's Google who decides what a beginners sees as "well-known", not Haskell experts. (The key word in the previous paragraph, by the way, is "evidence". I'm not talking about theoretical optimum learning patterns, I'm talking about the real problem reports I see from people "in the field" about learning Haskell.)
&gt; A function in Haskell is called polymorphic if it can return values of different types Rather, a term is polymorphic if it can be instantiated at multiple types. I.e., the return position isn't special in any way. This is also a polymorphic function: toRational :: Real a =&gt; a -&gt; Rational So is this one (i.e., the type-class isn't relevant): foo :: a -&gt; () foo _ = () So, if type variables appear anywhere in a type, then that type is polymorphic. (Perhaps modulo the cases where the variable has an equality constraint to a monomorphic type, a la `bar :: (a ~ Bool) =&gt; a -&gt; a`. Because the only possible way to instantiate `bar` is as `bar :: Bool -&gt; Bool`, and thus it only has one(mono) form(morphic). It's not clear that there's a consensus on what to call these cases.)
Good nitpick; yes, "binding" is what I should have said instead of "expression", and "where clause binding" instead of "subexpression".
Hearsay isn't evidence. :) But I get your point. The issue with letting experts teach beginners is that experts often forget what it was like to be a beginner.
Hackage never modifies the file that the original developer uploaded, and rightly so -- this would cause all sorts of trouble. The automated build on Hackage is fairly rough; while it can detect some issues, it has a very high false-negative rate (build failures with buildable packages), only runs with one version of GHC, and wouldn't have caught my example anyway. It would be possible to set up a proper automated build infrastructure for Hackage, but the sheer number of libraries and their possible combinations means that it would be too expensive for the Haskell foundation to run.
Yes, a program compiled against an older SDK, even on an older system, will run on newer ones. However, it isn't quite so simple with a development system (such as GHC) itself. Fundamental ABI changes were made with 10.6, and the compilation system has to know what SDK it will be combined with. Since the 10.5 SDK is no longer bundled by default, it left us with the option of shipping a GHC that works with 10.6 and later, but not 10.5. Or requiring that every developer hunt around and obtain and install a 10.5 SDK. It is conceivable that GHC could make such choices at run-time, but no one has attempted those more major changes to GHC.
If anyone would like to volunteer to compile and package GHC and HP for 10.5, it would be much appreciated. The process at this point is fairly automated. I'm lucky enough to have two macs available to me at home so I can test on both 10.6 and 10.7. Alas, I don't have the HW, or the time, to do 10.5 as well.
I think GHC / the platform should allow the possibility for me to select an older SDK, with the new SDK (the present situation) being the default. I imagine that could be as simple as providing two/three alternative builds (against different SDKs) instead of one. This probably needs some minor changes in the build system, but I don't think that would as hard as some people seem to believe. They rewrote the whole build system not so long ago anyway...
Repa is indeed bleeding edge and the fact that there are important contributors only compounds the potential problems, since some of them have links with GHC development itself which means that Repa needs drive some changes in the latest GHC and vice-versa... So the latest and greatest Repa often needs the latest GHC (and the Repa code is even developed against GHC HEAD from time to time so...).
&gt; Instead of turning off the restriction, you can just give the subexpression an explicit type signature. That's what I was going to suggest. I think it's a better approach.
Thanks for clarification. I'll correct my post.
Trolls, don't feed them.
I don't think that is accurate at all. I use 7.0.4 and haven't had any problems installing anything. The fact that a tiny number of bleeding edge packages won't work on 7.0.4 now doesn't make the platform a problem. If you are doing bleeding edge stuff, use a bleeding edge ghc. For most of us, platform is great and provides a target for package maintainers to aim for comparability with, rather than having to test on dozens of ghc versions with who knows how many combinations of different platform package versions.
The goal of the Haskell Platform is to provide a stable, easy-to-install distribution of Haskell. It is not on the cutting edge, and hopefully never will be. Depending on super-new compiler features is a social problem in the Haskell community. I have never seen (for example) a C++ program which won't compile except on GCC 4.8, but I'm sure there are already packages on Hackage which depend on GHC 7.5.
I find OverloadedStrings to be pretty useful.
Are the previous year's tasks and winning entries archived somewhere? Google led me to bunch of different pages and icfpcontest.org just redirects to this linked page. 
&gt; Yes, two years ago I would have said this too. However, I'd submit the evidence suggests that when doing it on the Internet the damage to the general community can outweigh the benefits bestowed upon the (putative) explainer. So, isn't the best of both worlds to: 1. Write the possibly inaccurate post. 2. Solicit feedback from other Haskellers, somewhere like reddit. 3. Fix the post.
yes, a sanity saver, but its also a sly knock on all of the various string types anyway, as it cheats through them
well: "Haskell is a functional language. A functional program is a single expression, which is executed by evaluating the expression where Imperative Program is block of statements." You might rethink this - even in Haskell a programm that will always evaluate to the same value might be a bit boring. I will not downvote for this but you really should rework the article - it's very poor as it is.
Thanks. Will update the article. I myself is quite new to Haskell but few of my friends were asking questions like why haskell? what is haskell? So I thought to write an introductory post. In Haskell Boringness can be removed using Impure Function. Correct me if I am wrong.
TemplateHaskell
SO mods screwing up everything again. 24 votes since 6 hours that it was asked so probably shouldn't be closed. This was a good question. I still don't know know why there needs to be a stackoverflow as well as programmers.stackexchange. SO was working just fine before it existed and didn't ever need to be 'fixed'
This reminds me of the similarly obnoxious tendency on reddit: "This belongs in /r/templatehaskellquestions"
It's on its way there with the new DataKinds...
Strictly speaking, it is true. A Haskell program is an `IO ()` or an `IO Int` value.
It's like christmas! Days of fun googling up and reading preprints. The MetaHaskell paper looks interesting: http://www.eecs.harvard.edu/~mainland/projects/metahaskell/mainland-metahaskell-draft.pdf Also the variational type inference paper seems promising, and the one on union and intersection types: http://www.cs.cmu.edu/~joshuad/papers/intcomp/Dunfield12_elaboration.pdf
Also excited to read the Shake paper.
Well, he did direct the reader to LYAH and RWH.
Overlapping/Incoherent instances become even more dangerous in the presence of the ConstraintKinds extension, because you can capture a dictionary for an instance at the wider type, and then use polymorphism to refine it before using it.
Unfortunately (for me) it's directly in .tex. 
And for anyone else looking, [learnyouahaskell's intro chapter on monads](http://learnyouahaskell.com/a-fistful-of-monads) follows this sort of recipe as well.
The 10.4 and 10.5 things are really not so much Haskell as OS X issues, they also plague many other programs due to the move from PowerPC to x86 (10.4 compatibility requires PowerPC compatibility or rather that is the largest group of people still using that version) and from Carbon to Cocoa (or was it the other way around?; 10.5 incompatible)
Personally I think the Haskell Platform is prolonging this issue because the current release at least doesn't include the new cabal install with --solver=modular which does a much better job at installing stuff than the older versions ever did.
&gt; I have never seen (for example) a C++ program which won't compile except on GCC 4.8, That is mostly due to the fact that C++, the language, developed at a glacial pace. There were C++0x programs that only compiled with the latest compiler versions due to using new language features, it is just very rare that there are any new language features in C++.
Not to be confused with /r/commentsaboutannoyingredditorbehavior 
Interestingly, I began learning Haskell in December 2010. I don't think I've ever intentionally used 6.12 in my life. :)
I never thought of it this way before and this gives me a lot of ideas. However, I just want to warn you that your `compose` function does not form a category. I can tell you from the experience of having written a LOT of failed composition functions that when the inferred type of the composition function does not match the type specified by the `Category` class then it violates a law. You can also tell by the difference which law it will violate. Yours will violate the downstream identity law, at a minimum. It's also a really good trick for building an intuition for how to build the `Category`, because fixing the function to infer the exactly correct type usually gets you very close to the right implementation.
There are downsides to dependent types, too :) I'm glad that Haskell is putting in effort to try to squeeze the good out without losing its "soul"!
and the singletons package http://hackage.haskell.org/package/singletons-0.8
No, this has nothing to do with either PPC or Carbon. What I mean is that if you have a 10.5 machine, and you build a usual application, you have the option to link against the 10.4 SDK, so the resulting executable will run on both 10.4 (x86, of course) and 10.5. With GHC, you cannot do this (unless you recompile the RTS and the base library, and do other hacks to pass the necessary options to gcc and the linker; I managed to do this once, but other people reported that this method is fragile enough so that it does not work for compiling GHC itself). The same issue manifests each time Apple does some API change.
I use the haskell platform professionally, and i'm quite satisfied with it. I don't have time for cabal dependency problems. So as soon as a new version of the platform comes out, I install it, and then install a single cabal package: `cabal-dev`. After that, no more cabal, only `cabal-dev`. It's simple and works great!
I like how they included the solar eclipse in the conference schedule.
Yeah, sometimes the law violations are easy to prove from the type. In fact, this should have tipped me off that my original `Strict` category had a problem with it, because the type inferred by composition was: (&lt;-&lt;) :: Pipe b c m r' -&gt; Pipe a b m r -&gt; Pipe a c m r It's easy to show that this can't have an upstream identity because: (&lt;-&lt; id) :: Pipe b c m r' -&gt; Pipe b c m r ... if some identity did exist, it would be able to coerce return values of any type to any other type. Your new `idP` now violates upstream identity because there is no way for it to faithfully transmit a `Left` value followed by a `Right`. This is one of the reasons I remarked in my blog post that getting "downstream finalization" (i.e. exception handling) correct is more difficult than the "upstream finalization" (i.e. finalizer passing). A formal identity won't work because then it won't type-check for composition, but Paolo tried something similar with a `Translate` constructor: Translate :: (a -&gt; b) -&gt; Pipe a b m r ... and then using `Translate id` as the identity. However, I've never encountered a case where there wasn't already a "natural identity" (i.e. `forever $ await &gt;&gt;= yield`), and if a natural identity exists then you have a law violation somewhere because the identity should be unique.
Yes, TH is important, and it is used. But watch out, it also has its [dark side](http://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell).
Notable version changes with this release: ghc 7.4.1 cabal 1.14 &amp; cabal-install 0.14 mtl 2.1.1 &amp; transformers 0.3.0.0 alex 3.0.1 Thanks to: &gt; Joachim Breitner, George Colpitts, Duncan Coutts, Jason Dagit, &gt; Chris Dornan, Yitzchak Gale, Mikhail Glushenkov, Andres Löh, &gt; Gábor Páli, Jens Petersen, Don Stewart, David Terei, &gt; Johan Tibell, &amp; Mark Wright for their efforts and help in putting together this release. -- The Platform Infrastructure Team, Mark Lentczner, release manager 
In particular a big thank you to Mark Lentczner who kindly agreed to be the release manager.
when will be the windows version updated? i still see 2011.4.0.0 on the download page
Download page pointed me to http://lambda.haskell.org/platform/download/2012.2.0.0/HaskellPlatform-2012.2.0.0-setup.exe - stale cache at your end perhaps?
refresh the page in your browser
indeed, thx cycles.
Hmm... Ubuntu 12.04 states it has HP of version 2012.1, but there's no mention of a version like that in HaskellWiki. Will it be "updated" to "correct" 2012.2 or i should install a PPA for this?
In some time trials I performed on Mac, the 32-bit version is a bit faster than the 64-bit version for most programs. The additional memory consumption and cache pressure of using 64-bit pointers and integers slows things down more than the advantage of the instruction set. Unless you are using very big in-memory data sets, or explicitly working with many integers over 32 bits, sticking to the 32-bit build seems sensical. Of course, if your programs do need large amounts of data in RAM, or work with 64-bit integers extensively, then by all means use the 64-bit version. On Mac OS X, you can choose which version, since the system supports both architectures if your processor does. On most Linux based systems, only one architecture is supported by default (you often additional distro. packages to get 32-bit support on a 64-bit system), and so you should install the version that matches the architecture of your system.
The package should constrain which version of TH it needs. It doesn't seem to work with 2.7.0.0.
Thank you all very much!
Great work!
Often, there are problems with external libraries on MacOS X, though. Like homebrew favoring 64bit, QuickTime being 32bit, and so on.
&gt; Not if I deconstruct it by hand and manually feed it those inputs. Except that in the state-based formulation the user doesn't get to deconstruct the pipe, all he can do is feed in some upstream state. Although of course, on the flip side, he can arbitrarily mess with the state from the downstream side, and break the right identity law. If you care about users messing with the The correct type of an await handler in continuation passing style is not Await :: (Either t i -&gt; Pipe i t o m a) -&gt; Pipe i t o m a but it is one of Await :: (i -&gt; Pipe i t o m a) -&gt; (t -&gt; Producer o m a) -&gt; Pipe i t o m a Await :: (x:Either t i -&gt; (either (Producer o m a) (Pipe i t o m a) x) -&gt; Pipe i t o m a The latter is of course not valid Haskell, though you could encode it with type families and/or GADTs. On the other hand using either of these makes it less convenient to use the await function. &gt; One of the reasons I enforce the stronger scenario is so that I can safely expose the constructors and allow other people to extend pipes, rather than building a single monolithic library that tries to anticipate every use case. Many other libraries you find on hackage have opaque constructors. I don't think exposing constructors is the best way to allow extensions, instead a small public interface should safely expose all functionality. &gt; Any solution that permits some form of communication against the stream direction always violates an identity law. I'm pretty sure that this is not a fundamental limitation, so I'll give it a try... I almost have a bidirectional pipe working, i.e. with something like bAwait :: a -&gt; BP' a b c d s b bYield :: c -&gt; BP' a b c d s d Does that count? &gt; Yes, but why settle for such a weak notion of observational equivalence when a stronger solution already exists (and readily generalizes to an even more powerful solution)? Because you don't need the stronger notion; and by settling for observational equivalence you can make the library more efficient and more convenient to use. Do you require strict equivalence for your Data.Map functions, for example? &gt; I can guarantee you that the formal identity is not an identity precisely because it allows upstream manipulation. Also, the lack of a natural identity should be a big red warning flag that you are doing something wrong! What I mean by upstream manipulation is the existence of a function replaceUpstream :: (forall x y. Pipe x y i m t -&gt; Pipe x y i m t) -&gt; Pipe i t o m a If this function exists, then `idP` defined in terms of yield and await will not be able to satisfy idP_yieldawait &gt;+&gt; replaceUpstream f = replaceUpstream f but an extra constructor of the pipe type could easily do it, by defining `IdP &gt;+&gt; x = x`, so also IdP &gt;+&gt; replaceUpstream f = replaceUpstream f This means that `idP_yieldawait /= IdP`.
Arch Linux no longer follows the Haskell Platform because it in the long run was too slow (release wise); many packages would refuse to build with 7.0.4 (old Haskell Platform GHC) and only with 7.4.1. That in addition to the high maintanence of keeping a large number of libraries from Hackage up-to-date (as binary libraries/executables). So for us Arch users, nothing really changes. Arch already has GHC 7.4.1, and cabal-install 0.14. Just use cabal to install packages you want.
This is awesome in so many ways. If you could script a HERMIT optimization pass, you'd have significantly greater control over the optimization process than what GHC exposes.
You can indeed script the transformations. In addition to an interactive shell, the plugin can accept a script file. We also have a RESTful web service in the works.
At the moment, yes, the shell lets you do pretty much whatever you want. Some transformations have preconditions, which are up to the user to verify. That said, we enable the core lint pass, so you should at least know if something too crazy happens. Eventually, we plan to address this, but so far the focus has been on the actual capability to do the transformations.
Although it's not perfect, wouldn't your ordinary QuickCheck tests be run just the same? So if you had good tests you'd be able to catch most of the faulty optimizations until you get around to forcing safety in the optimizations.
That's awesome. Any ETA on something we can claw our ~~dirty~~ squeaky clean hands on?
The [X32 ABI](http://en.wikipedia.org/wiki/X32_ABI), being added to linux distros, is what you want here. Use of the extra registers and new instructions in AMD64 but with 32bit ints and pointers. Should be a good fit for pointer heavy haskell data structures.
O(n)? It is exponential!
At high optimisation levels, does it not memoize correctly to give O(n)? To be honest it's been a very long time since I tried to rely on GHC to be that clever...
I think they need fusion at another level. They need the functions to be fused for use in the parallel evaluation threads. This sort of fusion is different to the stream fusion used in Vector I believe.
Oh, ok then. Thanks for the clarification.
There's some amazingly cool stuff here. The results are also quite impressive (with some of the computations running faster than the reference C implementation... When single threaded!). Maybe should take another look at a position at UNSW...
That's where this is all being developed, the University of New South Wales in Sydney.
&gt; with some of the computations running faster than the reference C implementation... When single threaded! With single threaded **and** immutable arrays (don't forget that part \^\^, it's as important).
Yes, what is the error? What viewer are you using?
But don't worry about it .. It loaded fine on another OS.
Andres and I have been playing with both Repa 2 and 3 and we think the new representation tagging is actually very nice and makes the performance much more predictable. With my educator's hat on, the types are a bit scary (I gave a talk to newbies on this recently) but if you look at the code and not the types then it's all still quite traditional in style. New users do get the idea that we really care about representations because we care about the performance, and the types just keep track of all that for us.
Please excuse my ignorance of the details of HERMIT, I haven't had time to read the paper yet. Wouldn't scripting a HERMIT optimization pass be similar to using a GHC Core plugin?
I didn't even know there was such a thing as GHC Core plugins.
If you copy /boot/arm224_start.elf to /boot/start.elf the main memory on the Raspberry Pi will be increased to 224MB of ram with 32MB for the GPU. Check out the process and files mentioned here: http://elinux.org/RPi_Advanced_Setup#Additional_files_supplied_by_the_foundation By default, the Raspberry Pi has 64MB of ram given to the GPU, and 192MB of ram for main memory. Perhaps that extra 32MB of memory will help?
Right. That sounds like a problem which needs a different approach. Chan as capability is either you set up a chan with e.g. 50 units, and then each worker thread takes a unit and replaces it when done. Alternately, you have a thread that places a unit in every second or something, and each worker thread takes a unit without replacing. The first approach means that you have a maximum number of jobs in some section at a given time, and the second approach means that you are executing no more than X jobs for second. You *can* also write a nonblocking `tryRead` for chans, which I find quite useful.
IVar is a slightly obscure name, I wouldn't blame you for choosing something a bit more descriptive.
I have a related question: I'm a pretty good functional programmer (get paid for Common Lisp, have a very pure style, since CL is not that functional, really), and I understand Monads in a dynamic language setting, but I find Haskell's type system to be pretty hard to get into. That is, for simple things, it is fine - but it seems like for really useful applications of the type system, I rapidly get lost. I'd love to "really get" types, whatever that means - any advice from seasoned Haskell folks?
I am using the supposedly safer SafeSemaphore
Yes you could, but to do the fib example, you would need a reliable inline mechanism, and a way of walking the AST. Which is what HERMIT gives you. It is all about raising the level of abstraction.
I need to spend more time studying this, but the monad instance reminds me a lot of the list monad. Edit: By this I mean that `return` generates a singleton, and (&gt;&gt;=) = concatMap, where it maps `f` to each event and then concatenates the results.
I get the same "corrupt document" error using Adobe Acrobat Reader X on Windows 7 64 bit. It works fine with Chrome PDF viewer, although that is not optimal.
I-structures were indeed invented in Id.
I am a co-founder of a startup that uses haskell, well primarily haskell. It was chosen for technical and economic reasons. We wanted a language like F# (so a functional language) that ran on linux (Azure was going nowhere) and after an extensive survey of functional languages chose haskell. We care a lot less about domain expertise for our business than the haskell and engineering skills a developer can bring. We do believe that the general pool of haskell developers is generally much better than the pool of engineers at large. We have received a lot of interest despite the fact that we are not actively hiring, but will hire if we happen to find someone who will make an awesome addition to our small team. When looking at a prospective employee we do look at any code they have written and pay particular attention to seeing if they have good habits in code, such as applying strictness. We do believe that there is a gap between people that would make good engineers and people who (in our opinion) would be more successful in research. We don't have the resources to help people learn how to write shippable code right now. For a C++ developer we would pay particular attention to the ability to build high performance code. Think things like SSE intrinsics, data structures optimized for current generation intel architecture, etc... While we do spend much of time writing haskell there are pieces that are written in c/c++ for performance and other reasons. I would encourage you to reach out to organizations that are using haskell. If it is a large one you will probably need to find someone in the group that uses haskell rather then just submitting it to a website and hoping that one group sees it. While many organizations such as ours does not actively solicit employees most organizations would not at least talk to someone who could not add a lot of value to their team.
husk was originally based on Tang's book, so the type system in particular should look very familiar. But as you wrote, husk takes the implementation quite a bit farther than the book :)
Most non-trivial programs are going to accrue cruft regardless of what language you use. It is certainly possible to write clean, well-organized programs in Haskell but just like anything else, you need to get a feel for it first. 
I do work for Qualcomm and was even in San Diego the two days of the course but wasn't able to attend because I had something else on the same day. 
Well, there's [Dyna](http://www.cs.jhu.edu/~jason/research.html#fg06), but it just memoises everything. And there are tabled Prologs, though again there's not much thought put into deciding when to memoise and when not to.
It is broken in Windows Vista, using Firefox. Native Adobe acrobat reader does not work too. 
I'm no concurrency expert, but isn't `Lock` a "mutex" and `withLock` a "critical section"?
JavaScript has worth as it's usable, and even the best, in its context. It doesn't make it good, but it most certainly give it worth.
Immutable
Nice writeup. I think this point may be confusing though: &gt; Evacuate any other unbound threads on the same OS thread to the newly allocated OS thread. (This may evacuate zero unbound threads.) In fact there's no evacuation happening, the new OS thread just takes charge of the run queue (we call it the "capability") and carries on running Haskell threads while the FFI call is in progress. When the FFI call returns, the new OS thread gives the capability back to the original OS thread. A similar handoff happens when a bound Haskell thread gets to the front of the run queue: the current OS thread hands off to the bound OS thread. The point about +RTS -N is maybe a bit misleading too. +RTS -N controls the number of capabilities, which is the number of Haskell threads that may run in parallel. It doesn't correspond to the number of OS threads, except that each capability has an OS thread pool and each pool starts off with a single OS thread in it, but that number may grow over time.
I ran into a problem with the platform on the Mac. I have a setup where the utilities in /usr/bin are symlinked to /Library/Frameworks/GHC.Framework/Versions/Current/usr/bin. This way, I can switch the Current symlink to switch ghc versions. With my Current pointing to 7.0, I installed the platform. This removed my 7.0 install! Then, with the Current link pointing to 7.4, I installed the old platform (to get 7.0 back), and now it removed 7.4! I have everything working now, but, although my setup may me non-standard, I don't think this is desired behavior.
I just had a very quick glance at it and noticed two small things: centers !! 0 can be changed to head centers and your program does not compile with GHC 7.4.1 as is because the Num type class does not automatically come with Eq anymore. (The places where you specify Num a should be changed to (Num a, Eq a). As for style suggestions you could cabal install hlint and run it with $ hlint mapgen.hs to see if it comes up with anything else. Apart from that it looks quite good to me :-) (Though somebody else will surely come up with other great suggestions)
It's up now.