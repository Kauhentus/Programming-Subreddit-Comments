If you're criticizing me that's ironic. The whole point of constructivism and intuitionism is that certain axioms are bad and we should shy away from them even when they have been shown to introduce no inconsistencies, like poor old Zermelo's axiom of choice.
Sorry I gave the wrong example. (drop 1) doesn't cost linear time. See the edit of the subtopic.
It's the right phrase, it just needs clarification. (This is the case in *any* language. Your English is fine!). So when you say "more CPU" time, it needs more CPU time that what exactly? A mutable version using an `IORef`? Why? Have you profiled it? Is the tradeoff worth it? There are a lot of uncertainties here. I recommend programming in a functional way, not an imperative way, and then profiling and seeing if it's fast enough.
But you might still have to write import qualified Path.To.Dog as Dog import qualified Path.To.Dog.Foot as DogFoot import qualified Path.To.Cat as Cat import qualified Path.To.Cat.Foot as CatFoot ... let foot = Dog.getFoot dog toe = DogFoot.getToe foot in ... Do I look so fresh in Haskell that I don't know modules? (respectfully) : )
The cons list it's like any other data structures it's optimized for certain use cases and if you use it for cases that it's not optimized for ( like a queue ) it's not going to have great performance because it's not suited. For instance a [finger tree](http://hackage.haskell.org/package/containers-0.5.5.1/docs/Data-Sequence.html) is much more suited for this purpose.
Agreed! Not just explaining the result but the way and workflow of getting there helped me a lot. I hadn't seen an explanation of working with type holes before that really clicked with me before this one.
I think this kind of thing is a problem in the Lisp community, too. Haskellers like to write blogs and papers and sometimes even write real library code with unicode equivalents of all their stuff, which makes others, who have already invested in learning the syntax everyone agrees on, feel like it's not worth bothering reading their code, it's literally a pointless barrier. Similarly in Lisp, when people are using weird macros a lot to express something in the most attractive way to them, it alienates everyone else. Actually, you see this aversion to template-haskell. And lens. 
[See the "Banker's Dequeue" here](http://hackage.haskell.org/package/dequeue-0.1.5/docs/Data-Dequeue.html). Note that it provides exactly the abstract interface you'd like, immutably.
&gt; You're thinking operationally. A purely functional language means that when a function says "I'm pure", he's not lying (probably ... If he didn't sign a contract to sell his soul to Mr.Unsafe), and will not launch missiles. So you could have functionality encapsulated in a nicer way than imperative ones. But this doesn't mean you should not launch missiles. Maybe the "Counter" example gets me misunderstood.
fortunately, with the magic of CTRL-v, changing alignment isn't hard
It's more than that. It's a shift in paradigm. And you're not taking the plunge. Little things you write are telling: `getToe dog` is an expression which denotes the toe of `dog`, so why do you use the verb "get" ? That is a tell you are thinking operationally, because you're thinking about a procedure and not a function. It should be `toe dog`, not even `dogToe dog`. Like I mentioned before you can use type classes, so if you need the function `toe` for something else just declare the class of dogs an instance of some type class that has the function `toe`. I don't like the term "pure function" anyway. It basically hijacks the term "function" for procedure, leaving function having to use an adjective. Then again this is the Haskell subreddit.
If you need high-efficiency persistent data structures, you should check out the `containers` and `unordered-containers` libraries. The latter has hash maps (a.k.a. hash tables). They are rather fast so don't immediately write them off.
I tried before, packing every thing that cause side-effect into an operation list. like: runSomething :: X -&gt; ([Operation], Y) And guess what ............. .... I reinvented another IO Monad.
Yeah, especially stuff like `Z -&gt; B -&gt; B` is a bit awkward. It's nice for a completely abstract view, but as a programmer I read `Integer` as a [signed boxed value](http://www.haskell.org/haskellwiki/FAQ#What.27s_the_difference_between_Integer_and_Int.3F) with the characteristic of an [Integer](http://en.wikipedia.org/wiki/Integer). Edit: However, it's quite [configurable](http://github.com/enomsg/vim-haskellConcealPlus#available-options) which is nice. Maybe I'll give it a shot.
This discussion seems to have gone in a direction that's somewhat orthogonal to OOP compared to FP. It sounds like you want to program directly with mutable state and are worried about optimizations. First, if you have a soft-realtime problem it's not always going to be ideal writing it in Haskell, it's really going to depend on the requirements are. Second, you should prototype your code first and then determine where the hotspots with profiling instead of trying to reason before hand. I know that's a cultural difference between C++ where you try not to pay for anything you don't need upfront. If you're structuring your entire program around some enormous data structure and hitting it is like 90% of the program logic then, by all means, optimize that structure and maybe embed that part in IO or some transformer with IO at the bottom; that's what they're there for. But don't try to over-optimize all the little intermediate structures unless you actually know that they're a bottleneck.
Sorry I was reading your response to notjet. Well at any rate, *that* should only be &gt; ` toe (leg dog 1) 2` which might only look weird because of the use of whitespace for function application, which I'm personally not a fan of. If we for example used the period to denote function application then it would be &gt; `toe.(leg.dog.1).2` . Further, to match more your imperative code, you could write &gt; `toe.2.(leg.1.dog)` . One thing to notice is that in your above example the period associates to the left, that is &gt; `dog.getLeg(1).getToe(2)` is equal to &gt;`(dog.getLeg(1)).getToe(2)` . In Haskell function application also associates to the left which means that &gt;`toe.2.(leg.1.dog)` is in general *not* equal to &gt;`(toe.2).(leg.1).dog` . One way to deal with this is to write &gt;`(toe.2) $ (leg.1).dog` The dollar sign also denotes function application but it has lower binding power than, in this case, the period. Of course there's no reason why we would want to write the above over `toe.2.(leg.1.dog)` , which has fewer parentheses, but I was just trying to show how to make it look as close as possible to your imperative code. One final way is to use function composition, which is unfortunate typically denoted in Haskell by the period so let's denote it here by `o`. Then the above formula is equal to &gt;`(toe.2)o(leg.1) . dog` with `o` having higher binding power than the period.
use Tabularize, less time wasted aligning.
More generally, categories, of which function composition is a special case
Because there's only one type variable, and thus nothing to express dependencies between--the constraint is `Has_m (Int -&gt; C -&gt; r)`, not `Has_m Int C r` as you might have expected. The latter is how *I* read it at first, and was confused as well. I'm not sure why they mentioned fundeps, though, since the multi-parameter version of the class seems like it should work. That said, the single-parameter version should definitely work using type equality constraints, e.g.: instance (r ~ IO Int) =&gt; Has_m (Int -&gt; C -&gt; r) where ... The same should be possible if you have fundeps but really want `Has_m` to be single-parameter, by way of black magic and trickery.
There's Frege: https://github.com/Frege/frege
This was very interesting and a lot of fun to read; I was pleased by how easy it was to parse for someone without background in type theory or research programming languages, but just good knowledge of Haskell and its extensions. I like the key suggestions of this paper a lot; in particular, I've wanted to have closed typeclasses for a while. I wonder whether the suggested `instance` syntax is somewhat unnecessary - I'd definitely like standard typeclasses to be allowed closed as well (as in the example where they use the hypothetical `closed` keyword). How difficult would it be to get the proposed syntax/semantics into GHC? Are there any plans to do so?
&gt; They’re still topological spaces, but now, the neighborhood relations, because of the way that you divided the original set, no longer have the properties that you need to define metrics. They’re not metric spaces. This is simply incorrect. The pieces are clearly metric spaces, since they are subsets of a metric space. The paradox doesn't actually have anything to do with metrics or topologies.
Looking so forward to this! I won't be there the entire time, but I really look forward to the parts I have tickets for.
&gt;&gt;&gt; ... willful ignorance. &gt;&gt;&gt; What. (With no elaboration.) &gt;&gt;&gt; constructivist nonsense... Well, nobody who isn't a mathematician that spends too much time philosophizing. __(Sorry if I can't conceal my contempt.)__ (Emphasis mine.) In the world of math, that's extremely emotional. &gt; even when they have been shown to introduce no inconsistencies, like poor old Zermelo's axiom of choice. Please don't try to pitch the AoC as some sort of "obviously correct" thing to me. Even in the non-constructive world Axiom of Choice remains controversial, again, by math standards. And I use "controversial" here in a sort of mathy sense... mathematicians don't argue whether it's "right" or "wrong" (and I'll cheerfully invoke a No True Scotsman here to say that anyone who does may not really be a "mathematician" after all...), but it is still something mathematicians tend to keep an eye on. You don't have to be a constructivist to still be nervous about proofs that invoke a step in which a mathematician suddenly pulls an opaque, unknownable lump out of their hat and claims it's an infinite amount of information, and then carries on like this step is no different than any other step they took in the proof. It's sensible to consider proofs that don't require that more elegant. I personally don't have a problem with either taking the AoC or not, or constructivism vs. nonconstructivism. My only "position" is that one should always be very clear about which thing one is doing at a given time. However, I think it's very useful to study constructive mathematics on the grounds that sclv gave... it is very easy to see why constructive mathematics is of quite likely of more utility to programming than non-constructive mathematics, and that's _plenty_ of reason to study it. Mathematicians consider things worthy of study for far smaller reasons, as is good and proper.
Looks like I need to learn some functional algorithms and data structures.
&gt;A theory is useful if it has a model. Being pragmatic we concern &gt;ourselves with the useful. If it has a model and can be used for &gt;abstraction nobody is going to care if it postulates the existence of an &gt;"unknowable". I think the rule of thumb is to not let anything very critical depend on any "awkward bits" of the theory. I've seen this caveat expressed in terms of *an airplane with operation depending on odd peculiarities of Lebesgue measure theory*. (Paraphrasing T.W. Koerner in "Fourier Analysis", pretty sure it's a comment on Banach-Tarski) Makes sense, considering how idiots with laser pens add their own *stupid* dimension to the unknowable... :-/
You can definitely view it through topological lens, especially since Category Theory was largely invented as a way to bridge concepts in algebra and topology (Eilenberg was an algebraic topologist). ;) Hope you feel better!
Nit pick, I prefer this form: 0 &lt;= a &amp;&amp; a &lt;= 25
That's a great idea. They are sometimes so different from their imperative counterparts that you'll be a lot smarter when you know a few of them.
I haven't yet had reason to use lenses with maps, but this makes it look awesome.
Some nicer ways to say that: -- This fails if any character is invalid convertString :: [Int] -&gt; Maybe String convertString = mapM convertChar -- The following two ignore invalid characters convertString :: [Int] -&gt; String convertString intsIn = [c | x &lt;- intsIn, Just c &lt;- [convertChar x]] convertString = mapMaybe convertChar
If you really need IO, then of course you should use IO, not reinvent it. It is not weird to embed IO operations inside data structures and the like.
Just to give the implementation that everyone's referring to: -- A queue has a front half and back half. The back half is stored backwards, so the last element is first. data Queue a = Queue [a] [a] enqueue y (Queue xs ys) = Queue xs (y:ys) dequeue (Queue [] []) = Queue [] [] dequeue (Queue [] ys) = dequeue (Queue (reverse ys) []) dequeue (Queue (x:xs) ys) = Queue xs ys It's a pretty simple trick that gives you amortized O(1) enqueue and dequeue, since you run the expensive O(n) reverse operation only once every n dequeues.
Oh, there's scads (literally, scads) of constructive stuff in category theory. The internal logic of topoi for example.
You can use [`Data.Sequence` from the `containers` package](http://hackage.haskell.org/package/containers-0.5.5.1/docs/Data-Sequence.html) which has O(1) enqueue using `(&lt;|)` and O(1) dequeue using `viewr`.
Have you taken a look at http://hackage.haskell.org/package/hsdev? One of the executables it produces is called hshayoo.
&gt; when it comes to computer science theory It is the "top-down" approach to functional programming theory. I actually don't like it because of it. Abstraction should be about removing the irrelevant but often times when category theory is employed in computer science a lot of the relevant also goes poof.
Makes sense. It is limiting in that sense, but to me what's really interesting and cool is this idea of... what does the universe "look like" to inhabitants of a geometric structure? And in some cases - their native language comes with it! It happens to be an intuitionist language, in cases I'm (vaguely) familiar with, but isn't that really the language that says the most about their peculiar situation (as we would be considering it)? (Speaking of geometric topoi in particular here. "Says the most" may also be a bad choice of words)
In this sort of situation, I like to think of them as Doozers and us as Fraggles... lol. (I work with kids, so my brain is warped by cartoons and such.)
It's very difficult to implement closures that are simultaneously memory safe, preserve the semantics of the frontend, and don't allocate on the heap.
I discovered it by accident.. :)
The first part makes sense if the operations in question are continuous but not weakly contractive. The second part stretches my familiarity with the paradox, but often times a measure will come from a metric or (I think uniform) topological structure
Ah, I think I finally understand your question! Are you trying to find a Haskell equivalent to a switch statement? switch (x) { case 0: case 2: case 4: case 6: return "even"; case 1: case 3: case 5: case 7: return "odd"; default: throw "x should be less than 8" } I think the origin of switch statements is that they could be compiled to more efficient code than the equivalent chain of if statements. Languages like C only allow switch statements on enums and numeric types, because they're the only representation for which a bunch of equality checks can be replaced by a computed goto. Later on, languages such as Ruby removed the numeric restriction. In those languages, the point of switch statements (which Ruby calls case statements) is no longer speed, but syntactic convenience. case x when "foo" puts 1 when "bar" puts 2 end Another extension is that instead of comparing each case for equality, Ruby uses its `(===)` operator, which means equality when used with a primitive, but inclusion when used with a container. case x when (0...8) puts "small" when (8...256) puts "medium" else puts "large" end I think this is the syntax for which you would like to find a Haskell equivalent, am I right? It would explain your subsequent question about Just/exceptions, because the default case in switch statements is often only reached in case of a programmer error, and it would explain your concern about whether the ranges are handled statically, because the C-style switch statement relies on statically-known case values for its computed goto.
`arr` in `Arrow` is a well known wart of Haskell's Arrows (when you ask theorists, anyways). I don't know much about the topic other than the fact that I've heard people complain about it on various occasions. ---- Is there some reason that you can't write: instance (Arrow c) =&gt; Arrow (Circuit c) where arr f = Lift (arr f) 
I don't remember where (***) fits, but you might check out: http://hackage.haskell.org/package/categories-1.0.6/docs/Control-Category-Cartesian.html
The code looks nice and clean, well done! 
`(***)` just means you have a bifunctor and equivalent to the bimap [here](http://hackage.haskell.org/package/categories-1.0.6/docs/Control-Categorical-Bifunctor.html)
Profunctors lack **arr**. You could try to make Circuit an instance of both [Strong](http://hackage.haskell.org/package/profunctors-4.2.0.1/docs/Data-Profunctor.html#g:2) and Category. You will need to implement the fanout function outside of the instances, however, because **Strong** doesn't have it.
I don't see any indentation in that snippet...
Thank you :)
You should take a look at Rust.
Sorry I mean Common Lisp. I have correct this in my initial question. 
My lazy answer: sure, why not? My other answer which requires some knowledge I've forgotten: there may only be one Ring instance for integers. I'd need to check / think. In which case, no need for newtyping.
&gt;These are not mutually exclusive. There's no good reason not to employ the predicate calculus except willful ignorance. TIL, Intuitionistic type theory is willful ignorance. I'm sure that's news to Bauer, Awodey, Shulman, Coquand and the hundreds of other working type theorists. &gt;This is the reason computer scientists don't care about constructivist nonsense. It's literally computer scientists who have kept constructive math alive for the last 50 years. The author of this piece is a computer scientist. Are you sure you know what you're talking about? &gt;If it has a model and can be used for abstraction nobody is going to care if it postulates the existence of an "unknowable". If you want to use math to make *explicit constructions* (e.g., a computer program), you need a system which gives them. In short, a mathematical system where a proof is the construction of an object. AC just doesn't cut it. &gt;PS Your unapologetic use of acronyms with no definitions is really off-putting. Let's check these acronyms: * ST. Defined in the [last post](http://www.goodmath.org/blog/2014/08/15/the-basics-of-st-type-theory-plus-administrivia/). His only mistake is not making it clear that this was part of a series (which the reddit poster did). * ZFC/NBG. Really? Maybe NBG will make you scratch your head, but since it's lumped with ZF, you can probably get an idea. And if you don't know what ZFC is, I doubt you're equipped to comment on the validity of the arguments presented. * B-T. Which he uses immediately after the full form "Banach-Tarski". How very off-putting indeed.
I think you missed the point of my post. I was really after why not write all of the runtime in Haskell. I am just curious if this is a limitation compared to Java or C. 
The snippet above is alignment, which IMHO is also important, but it's also easy to come up with a real-world example of a similar problem messing up indentation: let f = \x -&gt; do putStr "Hello, " putStrLn "World" in forever (f 0) If you conceal `-&gt;` with a single-width arrow, and indent `putStrLn` to the right location to align with `putStr`, you will have actually indented `putStrLn` by one space too few. I guess this could also be called alignment, but it's syntactically significant alignment. (Though of course you can rewrite this snippet to avoid that; but the fact that you have to do so is annoying in and of itself, and is a good reason for double-width arrows.)
or even instance (Category c, forall i. Functor (c i)) =&gt; Arrow (Circuit c) where arr f = fmap f id
That's not bad, but the rightward creep is there, and needing full "instance" syntax in addition to indentation is a drag. I think I'd just want a `closed` keyword in the class definition, and a rule that all instances must be defined in that module.
If you have `Profunctor` and `Category` then `arr` is just `\f -&gt; rmap f id`.
Excellent, I knew I had seen this post before, but I only remembered the extremely overmagical conclusion. I gathered a bit more appreciation for the stuff in the middle this time around.
One variant I've heard of is [generalized arrows](http://www.megacz.com/berkeley/garrows/), which seem to focus on the structural properties of the arrow language: class Category g =&gt; GArrow g (**) u where --id :: g x x --comp :: g x y -&gt; g y z -&gt; g x z ga_first :: g x y -&gt; g (x ** z) (y ** z) ga_second :: g x y -&gt; g (z ** x) (z ** y) ga_cancell :: g (u**x) x ga_cancelr :: g (x**u) x ga_uncancell :: g x (u**x) ga_uncancelr :: g x (x**u) ga_assoc :: g ((x**y)**z) (x**(y**z)) ga_unassoc :: g (x**(y**z)) ((x**y)**z) class GArrow g (**) u =&gt; GArrowDrop g (**) u where ga_drop :: g x u class GArrow g (**) u =&gt; GArrowCopy g (**) u where ga_copy :: g x (x**x) class GArrow g (**) u =&gt; GArrowSwap g (**) u where ga_swap :: g (x**y) (y**x) class GArrow g (**) u =&gt; GArrowLoop g (**) u where ga_loop :: g (x**z) (y**z) -&gt; g x y But I could not find anything about them on hackage (the page I linked to is about a custom build of GHC which apparently uses generalized arrows to implement languages with said structural properties). I am glad to see in the other comments that there are similar, even more general structures on hackage already.
Here's a way you can avoid "lifting arbitrary functions" being a problem. Use a datatype `data Wire a = ... internal representation of Wire ...` to represent wires coming into or out of your components, so you have id :: Circuit a a resistor :: Circuit (Wire Foo) (Wire Foo) diode :: Circuit (Wire Bar) (Wire Foo) resistorAndDiode :: Circuit (Wire Foo, Wire Bar) (Wire Foo, Wire Foo) resistorAndDiode = resistor *** diode joinWires :: Circuit (Wire a, Wire a) () joinedResistorAndDiode :: Circuit (Wire Foo, Wire Bar) () joinedResistorAndDiode = resistorAndDiode &gt;&gt;&gt; joinWires (The types here are completely arbitrary but I guess you'll come up with something that works for you.) Then it really doesn't matter if you can do `arr sort :: Eq a =&gt; Circuit [a] [a]`. Sure you can make one, but why not? You can't actually *do* anything with it. 
Presumably the point is that `c` is not already an `Arrow`.
The closures that can be passed around freely as first-class values in Rust are still heap allocated. C++11 tries to do it without heap allocation, but in return isn't memory safe.
I may be confused about `Hask`, but doesn't having a well-behaved `(&amp;&amp;&amp;)` just suggest that you have reified a category with finite products?
&gt; forall i. Functor (c i) I don't believe this is a valid constraint in Haskell. Or is there an extension that makes it valid?
Yes. It works better for type families because they don't have a `where` clause otherwise. Something resembling default signatures might be a cleaner way to do it for closed type classes, e.g.: class C a where foo :: a -&gt; a foo :: Int -&gt; Int foo _ = 42 foo :: [a] -&gt; [a] foo xs = xs ++ xs That looks a little weird to my eye but it does make more sense in some ways. Maybe use an extra keyword in there somewhere to disambiguate, too. My main problem with the "closed" keyword and nothing else is that I'd prefer all instances to be consecutive in the file, as I believe is required for functions defined with multiple equations distinguished by pattern matching, and the normal instance syntax doesn't require that (and requiring it for closed classes only would also be weird).
It’s not about edge cases, exactly. It’s that you can’t make automated changes to text-based source in a way that’s guaranteed to preserve the intent of the formatting, because the structure has all been flattened down into spaces. If we were using structured, non-textual source, then it wouldn’t be an issue, but unfortunately that’s not generally the case.
you're interested, i think, in monoidal categories.
Without `arr`, for the categorical product I believe you also need equivalents of `fst` and `snd` along with `(&amp;&amp;&amp;)`.
This looks phenomenal :) Unfortunately, according to the github branch listed on this page, it hasn't been updated since September 2013... Not sure how much work was done either. I hope this gets picked up again
Ahh, but the `c` parameter in my Circuit type is not itself an arrow; it's something like: data RingOp a b where Zero :: Num a =&gt; RingOp () a Unit :: Num a =&gt; RingOp () a Plus :: Num a =&gt; RingOp (a,a) a Times :: Num a =&gt; RingOp (a,a) a Inverse :: Num a =&gt; RingOp a a I want the circuit type to add a *free* category/arrow(-ish) structure to any underlying carrier type, without requiring anything of it (ala the approach in Operational). &gt; `arr` in `Arrow` is a well known wart of Haskell's Arrows (when you ask theorists, anyways). Can you (or anyone who knows about this) point me at resources which talk about this? What's the "theoretically pure" formulation of arrows (I've only seen the `Control.Arrow` version)?
You're right, although I don't know what 'well-behaved' would mean in their absence.
Why not data Element -- abstract data type data RingOp a b where Zero :: Num a =&gt; RingOp () Element Unit :: Num a =&gt; RingOp () Element Plus :: Num a =&gt; RingOp (Element, Element) Element Times :: Num a =&gt; RingOp (Element,Element) Element Inverse :: Num a =&gt; RingOp Element Element and then the existence of `arr` really doesn't matter. 
No `(&amp;&amp;&amp;)` does not necessarily arise from a Cartesian product. `()` needn't be a terminal object, for example. That is, there can be distinct inhabitants of `MyArrow Foo ()`.
Yup, being able to easily configure and tune notations is actually the main philosophical difference from the baseline conceal package. Different people may have different preferences, there is no right or wrong. I have just checked-in some changes to make it possible to disable concealing of Integer and Rational types (Z and Q options).
I think that what you're really looking for is import Data.Traversable intolerable :: (Control.Applicative.Applicative f, Traversable t) =&gt; t a -&gt; (a -&gt; f b) -&gt; f (t b) intolerable x f = traverse f x
i was trying to avoid a `Traversable` constraint but i guess it can't be avoided here
I only recall overhearing chatter on #haskell irc about it, so I can't point you at any resources. Like I said, I don't actually know all that much about it. The reasoning, as I recall it, is usually that "the QuxBaz type could be an instance of Arrow if it weren't for arr." The "theoretically pure" formulation, as I understand it, is just the `Arrow` class without `arr`. The only thing `arr` gives you is a way to go from the `(-&gt;)` arrow to your particular arrow. There is no reason that "function-like-things" must also have a way to also represent any function. Any laws relating to `arr` simply verify that `arr` is a certain kind of sensible arrow transformation. (See also: arrow transformers. If I understand correctly, `arr` basically forces all instances of `Arrow` to provide special-case support for acting like an arrow transformer on top of the `-&gt;` arrow.) If you'd like some reading on the subject, you can start at: http://www.haskell.org/arrows/index.html The reason `arr` is included in the typeclass is because it facilitates the Arrow notation, which is similar to do notation. Again, I can't find any sources to back up my claim that `arr` is a theoretically warty part of the `Arrow` abstraction, nor do I know enough about the topic to argue the point for myself. So take what I say with a few grains of salt. :)
Do you have any examples? I'd love to take a look, but am not sure where to start :)
The type signature you provided isn't possible because you're trying to rip an 'a' out of an 'f a' with only a Functor f constraint. Traversable fits nicely, may I ask why you're trying to avoid it?
trying to put off writing a `Traversable` instance as well as a bit of curiosity 
I built the docs if anyone wants to check them out before hackage generates them: http://brandon.si/temp/spice Congrats on the package! **EDIT**: some quick comments - a suspect `Game` doesn't need to be a class, but I'm not sure - what happens if I run `startEngine` at the wrong time, or twice, or not at all? What is the meaning of `renderWrapper $ putStrLn "wat"`? A great goal would be to make use of the type system to constrain possible programs to only sensible ones, as much as possible. This also makes it much easier to understand. - people love little inline code examples and documentation that makes the big picture of how to use your module clear
&gt; data RingOp a b where Zero :: Num a =&gt; RingOp () a Unit :: Num a =&gt; RingOp () a Plus :: Num a =&gt; RingOp (a,a) a Times :: Num a =&gt; RingOp (a,a) a Inverse :: Num a =&gt; RingOp a a Wait a minute... why the `b`? Even as just a phantom type, I don't see the point of it here.
I believe Hackage takes some time to generate the docs. Others more knowledgeable can confirm or deny though.
Perhaps a better way of stating this would be: data RingOp :: * -&gt; * -&gt; * where Zero :: Num a =&gt; RingOp () a Unit :: Num a =&gt; RingOp () a Plus :: Num a =&gt; RingOp (a,a) a Times :: Num a =&gt; RingOp (a,a) a Inverse :: Num a =&gt; RingOp a a The `a` and `b` parameters after the data declaration are just an arbitrary way of specifying the arity of the kind of `RingOp`, not correspondent to the names of the parameters to the constructors.
Because we have two type parameters, our `RingOp`type encodes an abstract primitive arrow from some input to some output, which we can interpret in possibly many different ways.
Trivially, every bijection between integers and another countably infinite ring gives a possible `Ring` instance for `Integer`. More concretely, you can take `Data.Bits.xor` and `(Data.Bits..&amp;.)` as an alternative addition/multiplication pair for `Integer`.
Thanks so much for your explanations! &gt; Any laws relating to arr simply verify that arr is a certain kind of sensible arrow transformation. (See also: arrow transformers. If I understand correctly, arr basically forces all instances of Arrow to provide special-case support for acting like an arrow transformer on top of the -&gt; arrow.) That was my intuition about it. It also means (because we must be able to lift Haskell functions) that the minimal complete definition of the `Arrow` typeclass is much smaller than it otherwise would be. If you look at how the other operators are defined in terms of `first`, they all require use of `arr`. So would it be correct to say the following? The Haskell `Arrow` typeclass is a category with finite products (you can replace its `(***)` operator with `(\f g -&gt; (f . fst) &amp;&amp;&amp; (g . snd))`), with the additional requirement that there exist a natural transformation from `Hask` to this other category (that is to say, `arr`). Building on that: `ArrowChoice` just means that we get finite coproducts as well as products. And `ArrowApply` makes it cartesian closed, so we get to represent morphisms as objects (in the way that a cartesian closed category is sufficient to describe the lambda calculus) — I suspect, though I would love further elucidation, that this is deeply connected to the reason that `ArrowApply` is equivalent to a monad. And I further have no idea what the categorical interpretation of `ArrowLoop` would be: any thoughts?
Let's delve into one of the most obscure standard modules and do Data.Ix.inRange (0,25) a
This is an interesting solution, and I'll have to think more about it. I'm inclined to say that it's not useful for my particular case, because the fact that you *can* inject arbitrary functions means bad things for what I'm trying to build, which is essentially a free-category-plus-some-things interpreter. With arbitrary functions (and I can define those over `Wire` if I have access to its constructors) I still don't get strict control over what can be lifted into my circuit type: I only want to be able to lift primitive operations, and nothing else.
Oh right. Silly me. GADT syntax confuses me sometimes, because I don't see it very often.
 class (PFunctor p r t, QFunctor p s t) =&gt; Bifunctor p r s t | p r -&gt; s t, p s -&gt; r t, p t -&gt; r s where bimap :: r a b -&gt; s c d -&gt; t (p a c) (p b d) Oh, it's *just* bimap? @_@
You can have binary products without having nullary products. So while the category may not have *all* finite products, the `(&amp;&amp;&amp;)` and `(***)` operators could still be from products. While the existence of `(&amp;&amp;&amp;)` doesn't require products, it does strongly imply them. The existence of `(&amp;&amp;&amp;)` —together with the standard laws— is the proof that `(a,b)` is a limit, hence a product. While the OP did not state outright that the laws hold, it is strongly suggested by the choice of the name "`(&amp;&amp;&amp;)`" and the reference to `Control.Category`.
Maybe you can derive `Traversable` for your data type?
Aha! Thank you! One constructor I omitted because I thought it wasn't relevant was: Void :: Circuit f a () This gives us a terminal object, yes? The laws I intend for `(&amp;&amp;&amp;)` and `(***)` are the same as those for the arrow functions. I intend `(&amp;&amp;&amp;)` to express sharing of input, and `(***)` to represent parallel composition of circuits. I'm only beginning my study of category theory, and I'm having trouble parsing out a succinct understanding of the page you linked about monoidal categories. If you have the time to explain to me how monoidal categories work, I would be very appreciative. (If it helps you to target your explanation to the audience: I'm an advanced Haskell programmer who learnt most of his (bits and pieces of) category theory on the Curry side of the Curry-Howard divide before starting to learn the more formal mathematics. I've also had an undergraduate course or two in abstract algebra, as well as a fair bit of theoretical CS.)
Oh, I don't mind at all! But I guess that invalidates your "stealing", doesn't it? If you want to feel like a thief, I can condemn you! But in fact, I'm appreciative that you were so inclined to gather more answers. Thank you!
&gt; The only thing arr gives you is a way to go from the (-&gt;) arrow to your particular arrow. I disagree! `arr` is used by the arrow syntax extension to allow arrow outputs to be plugged freely into the inputs of other (later) arrows. At the very least, you need `arr swap` in order to implement `second` out of `first`. If we remove `arr` from `Arrow`, we need to replace it with a number of structural operations such as `second`, `assoc`, and `unassoc`. Interestingly, there is more than one candidate set of structural operations which makes sense! For example, we may or may not allow unit values to be introduced and eliminated at will, and we may or may not allow fanin and fanout operations. Allowing `arr` allows all structural operations one might ever think of, plus every pure non-structural operation. As such, instead of changing `Arrow` itself, I think a better solution would be to create a more refined hierarchy, with `Category` at the bottom and `Arrow` at the top. Even more interestingly, it looks like the `category` package might already be this refined hierarchy I was hoping for! Now, if only the arrow syntax supported some of the less powerful variants...
To be fair, most of that class signature is hackery to help type inference along. The `PFunctor` and `QFunctor` classes are the `Functor` class modified to work on things of kind `(* -&gt; * -&gt; *)`, showing that they are functorial in either the first or second argument. All that `Bifunctor` adds is saying that `first f . second g == second g . first f` and giving this composition a name `bimap f g`. Coding this up is surprisingly more intricate than understanding the idea "`F :: * -&gt; * -&gt; *` is a functor in two independent ways." If we ignore all the extra `Arrow` baggage: the `(***)` operator is the instance of `bimap` for pairs, and `(+++)` is the instance of `bimap` for `Either`.
My point is that a "well-behaved `(&amp;&amp;&amp;)`" need be nothing like a Cartesian product. Do you disagree?
&gt; the fact that you can inject arbitrary functions means bad things for what I'm trying to build I doubt this is actually the case. Can you give an example? I suspect that what you will find is that the "pure computations" lifted with `arr` can appear as values in your arrow type, but they can do absolutely nothing at all. &gt; I can define those over Wire if I have access to its constructors Sure, but the whole point is that you don't expose the constructors.
&gt; It looks to me like a category with binary products Really? How do you get the projection maps?
OP said product, but you said Cartesian product. You're right that categorical products need not look anything like a Cartesian product, but I'm not sure what you're objecting to.
In Category theory the terminology "product" means "Cartesian product". I offer these two pieces of evidence in support of my claim: http://ncatlab.org/nlab/show/product http://en.wikipedia.org/wiki/Categorical_product 
I'm not sure why we're discussing terminology. There is a concept in category theory that is described by the name "product", and also "cartesian product". Sternenkranz asked &gt; doesn't having a well-behaved `(&amp;&amp;&amp;)` just suggest that you have reified a category with finite products? i.e. "finite cartesian products". The answer is "no", at least if you accept the fairly week premise that arrows have a "well behaved" `(&amp;&amp;&amp;)`. Proof: `(arr fst &lt;&lt;&lt; f) &amp;&amp;&amp; (arr snd &lt;&lt;&lt; f)` is not equal to `f` in general (consider `Kleisli` arrows of `IO` for example). Cartesian products, on the other hand, do satisfy that equality. 
I doubt it, Atom is for writing hard-realtime systems, something that a GC usually isn't. Perhaps the LLVM package(s?) could be used.
I didn't have to do anything to turn conceal (the vim feature) on, I just pulled the repo into the pathogen modules directory and enabled Haskell Conceal+ in .vimrc per the instructions in the github readme. I'd paste the exact configuration in .vimrc for you, but I'm not on that machine anymore. But anyway it's in the docs somewhere.
Really? Woops. Too much Python I guess.
Hey, this is great! So glad to see more people trying to tackle the game engine framework idea. I've been [working on one myself](http://www.github.com/Mokosha/Lambency) to try to figure out a lot of the design decisions that go into making a library like this, and more importantly, [using it for a game](http://www.github.com/Mokosha/HsTetrisAttack). I have a few questions for you though: 1. I see a lot of your code works with IO. Do you think that this is the right way to do it, or should there be a pure game module that then gets processed by an IO call? 2. How do shaders fit in with your framework? 3. Why are you not using an existing vector library like [linear](https://hackage.haskell.org/package/linear)? Each of the vector types has an instance for Storable, making interop with OpenGL quite nice. Additionally, it takes care of using unboxing to make things fairly efficient. I'm not sure that you want vectors to be an instance of Num since some of the operations don't really make sense (mathematically) such as (*) and fromInteger. A few notes: - In your cabal file, you export modules that export the other modules you have listed in your cabal file. If you re-read that sentence a few times you should realize that everything but the top-level modules should move to an other-modules section of the cabal file - Shouldn't your startEngine function use the config you pass to it? GHC should warn you about unused variables if you turn on -Wall. With Haskell's awesome type system, I usually catch a lot of bugs with -Wall since unused variables means either typo or misinterpretation of what the function actually needs.
&gt; If you have the time to explain to me how monoidal categories work Let's figure it out! Here is [Monoidal from category-extra](https://hackage.haskell.org/package/category-extras-0.53.5/docs/Control-Category-Monoidal.html#t:Monoidal): class (Associative k p, HasIdentity k p i) =&gt; Monoidal k p i | k p -&gt; i where idl :: k (p i a) a idr :: k (p a i) a It's a bit abstract, so let's instantiate `k` with `(~&gt;)`, `p` with `(,)`, and `i` with `()`: idl :: ((), a) ~&gt; a idr :: (a, ()) ~&gt; a Ah, so it's a language in which there is a special `i` which, unlike arbitrary variables, can be freely introduced or eliminated. So what else is special about `i`? Here is what the `HasIdentity` constraint has to say about it: class (PFunctor p r t, QFunctor p s t) =&gt; Bifunctor p r s t | p r -&gt; s t, p s -&gt; r t, p t -&gt; r s where bimap :: r a b -&gt; s c d -&gt; t (p a c) (p b d) class Bifunctor p k k k =&gt; HasIdentity k p i | k p -&gt; i Again, things become much clearer when we instantiate `k` with `(~&gt;)` and `p` with `(,)`. Plus a bit of alpha-renaming for extra clarity: bimap :: (a ~&gt; a') -&gt; (b ~&gt; b') -&gt; (a,b) ~&gt; (a',b') We didn't learn much about `i`, but at least we know that `p` is a binary type constructor whose values can be manipulated element-wise. Something like `(,)` or `Either`. What else do we know about `p`? class Bifunctor p k k k =&gt; Associative k p where associate :: k (p (p a b) c) (p a (p b c)) Wow, this one is really unrecognizable in prefix form. Our usual infix instantiation allows the name `Associative` to make a lot more sense: associate :: ((a,b),c) ~&gt; (a,(b,c)) I am quite surprised to see that the other direction, `coassociate`, is a member of a completely separate class which isn't required by `Monoidal`. class Bifunctor s k k k =&gt; Coassociative k s where coassociate :: k (s a (s b c)) (s (s a b) c) coassociate :: (a,(b,c)) ~&gt; ((a,b),c) Anyway, now we're starting to see why `Monoidal` is named that way. It's a category `k` with a type-level operator `p` which is associative and which has an identity element `i`, just like `mappend` and `mempty` from `Monoid`. The major difference is that `p` is at the type level, and that the associativity and identity equations use `k` instead of `(=)`. idl :: ((), a) ~&gt; a idr :: (a, ()) ~&gt; a 
Well: 1. I didn't actually try it with macvim. Maybe it doesn't work there (although that would be strange). 2. I used the entire "big" featureset. It could be something else in there is needed too. (I'm probably the wrong person to help you debug this as I don't know anything about the implementation.)
I don't have much experience with this, but as I was writing the code I reached a point where trying to rule out bad numerals on the first pass became too difficult and started to complicate the code. So I consciously decided that wasn't going to be my responsibility and would have to happen in a second pass. However, I think having the first and second pass be independent functions is the wrong approach, because you end up redoing a lot of the work. Instead, if the code were to be done right, I think I would have used the first pass to build an AST. Once you have the AST, it's much easier to write high level rejection rules. And if the code passes those rejection rules, you have a third pass to compile the AST down to an integer. This is how programming language compilers work. So it might not be optimal for such a small task, but it definitely scales well.
Depends which direction the arrows go :)
okay, thanks for the effort none the less. Gee all this trouble just so I can read haskell with pretty symbols.... it's probably worth it.... 
My first hackage package took a few hours before the haddocks showed up. It's some async process, but if it doesn't show up after 36 hours, you might see about uploading your own docs to hackage.
This actually works out much nicer in http://github.com/ekmett/hask where I can say a bifunctor is just a functor to a functor category, and skip all these preliminaries (since I do all the category theory there "curried").
A Strong Category is an Arrow. They are equipotent. This is sort of the motivation behind the design of the profunctors package, and many of the names.
The existing categories class is a bit of a sewer. I've been meaning to go back through and reformulate it. I started once and got too enthusiastic about GHC 7.8'isms, and never backported the improvements to the current lib. I started a second time and then Hask was born. The third time led to the current formulation of Hask, which is more correct categorically, and now the fourth try has led me off into coq playing with the foundations of homotopy type theory. I hesitate to think where the next step will lead.
If I'm defining a "free monoidal-category interpreter" to tear down this structure and in so doing build up an arrow in some other category, it seems strange and problematic to require that this other category likewise support `arr`. This is *especially* true if I mean to eventually compile this representation into some lower-level form suitable for optimization. Considering this is a deeply embedded DSL targeting multiple backends (which it is eventually meant to be), I don't want — and in fact cannot have — unanalyzable functions as a part of my abstract syntax tree.
Conal Elliott tried something along this line in his [LambdaCCC](https://github.com/conal/lambda-ccc) project. His last blog [Circuits as a bicartesian closed category](http://conal.net/blog/posts/circuits-as-a-bicartesian-closed-category) also mentioned: &gt; Notibly missing is the Arrow class’s arr method, which converts an arbitrary Haskell function into an arrow. ... 
This is exactly why I recently made this Haskell install script [1]. Too much people complained about difficulties in just trying To install Yesod in my tutorial. [1]: http://yannesposito.com/Scratch/en/blog/Safer-Haskell-Install/
&gt; Take category theory: it relies on abstract postulates and has no "knowables" Since when? Can you name a standard result in category theory that is not constructive? &gt; Same with Scott &amp; Strachey's theory of computation They used a classical foundation, it is true. But, there is a fully intuitionistic version of domain theory. I work in computational models of classical logic. I can think of only three results of high importance to computer science that depend on classical logic: * That every turing machine either halts or does not halt * That in the domain of the natural numbers and decidable P, (~forall n,~P(n)) -&gt; exists n, P n * The proof of the completeness of classical first order logic with respect to the standard semantics As for AC...some version of it holds (as a theorem!) in MLTT which is as constructive as systems come...
Interesting! Do you know of any reading in this area? I do a lot of parsing of custom formats, but I've never really figured out how it's supposed to be done. It'd be really interesting to read on how people do it so I don't have to reinvent the wheel all the time...
"Safer" installation by sudo-ing scripts from web... Ouch.
Yes, the doc builder clients can take a while to get around to building the package, though in my experience a few hours is usually enough. You can check for build reports here: http://hackage.haskell.org/package/spice-0.1.2.1/reports/ Unfortunately, at the moment the absence of build reports can mean either (a) your package hasn't been attempted yet or (b) the dependency solver failed to find an install plan. The process for uploading docs manually is also a bit under-documented, but there is some information here: https://github.com/haskell/hackage-server/issues/56
To give you an explicit example, if `f` is `(-&gt;) Integer` and `m` is `Maybe`, then a function `(Integer -&gt; a) -&gt; (a -&gt; Maybe b) -&gt; Maybe (Integer -&gt; b)` would solve the halting problem - it could tell whether the composition `Integer -&gt; Maybe b` attains Nothing at some integer. See also the [Applicative paper](http://www.soi.city.ac.uk/~ross/papers/Applicative.html).
At least it isn't curl FOO | sudo with no chance to read the script before handing over the keys to the airtight hatch.
I'm not sure what *operations* you are talking about. Banach Tarski is a statement about a specific set, and the proof that I've seen doesn't use any topology or measure theory.
Try clicking on change theme, you might prefer the LaTeX-like one.
The question is whether the instance is selected based on "first match" (as with closed type families and term-level functions) or "best match" (as with `OverlappingInstances` and this paper). If it's first match the instances should of course be in one place and clearly ordered. For best match they don't have to be (and perhaps shouldn't, to avoid giving the wrong impression). So this isn't just a stylistic choice.
Could you comment on why you chose Coq, as opposed to one of the more Haskell-like alternatives like Idris or Agda? I'm only familiar with Agda myself, so I'm wondering if perhaps the grass is greener on the other side.
&gt; The existing categories class is a bit of a sewer. In which way? I was planning to implement a TH-based arrow-like syntax for generalized arrows, but this thread had convinced me to target categories or category-extra instead. But now, your comment is causing me to reconsider. Perhaps I should target Hask instead? Or something completely different?
wget it and read it first? Or curl | less? Actually I think curl writes stdout anyway so just drop the pipe to sudo and you can read it. 
I uploaded a very simple example to my [GitHub](http://github.com/crockeo/spice-test).
&gt; I don't want — and in fact cannot have — unanalyzable functions as a part of my abstract syntax tree. Sure, but they won't be part of it. Suppose your user comes up with a value of type `Circuit () (Wire Foo, Wire Bar)`. Then you *know* it cannot contain any "unanalyzable functions", because there are no non-trival functions that manipulate `Wire`s. Perhaps you can give a concrete example of what you are trying to achieve as it is hard to explain this in the abstract.
&gt; The Haskell Arrow typeclass is a category with finite products No, [the structure is not cartesian]( http://www.reddit.com/r/haskell/comments/2e0ane/category_with_fanout_and_split_but_not_an_arrow/cjv4qyr ). It's not even monoidal because first f *** second g need not equal second g *** first f In other words `(***)` need not be a bifunctor. 
In response to your points: 1. The effective "API" for writing your game -- the `Game` typeclass -- updates completely purely. You have a Float (under the guide of a `type DeltaTime = Float`), the `Input` object contained in `FRP.Spice.Input`, and the current `Game` state. You just return another `Game` and that's done. You're right on the rendering end, though, as of yet that relies completely on the user performing `IO` calls. I was planning to change that anyways after I got everything up and running. 2. They... don't really. I mean, I'm not equipped to say they do, at least. I'm honestly not very well acquainted with shaders. I'll look into them and their implementation(s) in addition to how I may implement them into my project. 3. I'll very much so look into that! I didn't know a library such as that existed (though of course it would, because there's a library for almost everything in the Haskell ecosystem now-a-days). And your notes: * I didn't quite know how that worked. I've fixed it now (not yet on hackage, just on GitHub). * Thank you for catching that! I also turned on -Wall in my cabal file so that now whenever I'm developing it'll catch those kinds of bugs. I found a large number of unnecessary imports as well. It reminds me of programming in Go a little. Whenever you don't use a variable or an import it blows up in your face.
I really think you will go a long way with free arrows: http://stackoverflow.com/questions/12001350/useful-operations-on-free-arrows Just make sure that the underlying effect type `eff` is only inhabited by terms of type like `eff (Wire Foo, Wire Bar) (Wire Baz, Wire Quux)`. Then the non-existence of non-trivial functions of `Wire`s means that you can never lift general Haskell functions into the free arrow. The only effect that pure functions can have on the free arrow will be to rearrange tuple components.
Perfect. This is great. I wish you do something similar to make setting up emacs/haskell with all bells and whistles as easy as well. 
The compiler will automatically derive `Traversable` instances for you if you enable the `DeriveTraversable` extension and add `deriving (Traversable)` to your type.
My problem with development in agda is that the 'proof script' really only exists in the programmer's memory of the session with the compiler. This means that when I'm doing writing a large chunk of code in Agda and I go to refactor there is a _huge_ burden of ripping out and rewriting all the code. e.g. I could define categories, etc. then have to go plug in all the default definitions for all the fields, all fairly mechanical, but if I change the definitions I'm working over then rarely will my program survive much of the refactoring. On the other hand, if I'm building with HoTT (or even just HoTT like foundations) then I don't really know what I'm building in advance, just that I want to go through the HoTT approach with greater emphasis on (infinity, 1)-categories and complete Segal spaces, so I'm going to thrash my foundations a lot. Adam Chlipala's style of programming coq from Certified Programming with Dependent Types leads to a form of Coq development that is robust against refactoring by developing smarter tactics. Consider that given the tactic script from lines from [85-94](https://github.com/ekmett/homotopy/blob/acb10214d0a9b3feeaec9ded54039702af9c2d5e/Core.v#L94), which is mostly stolen from the main HoTT project, I can now automate not only the construction of the proofs for most of the properties in this file, but also have it construct most of the real 'members' as well. e.g. it figures out how to do [composition, identity, all the laws in the category of coq types](https://github.com/ekmett/homotopy/blob/acb10214d0a9b3feeaec9ded54039702af9c2d5e/Core.v#L176), for [paths and how to invert paths and based_paths to show they are a groupoid](https://github.com/ekmett/homotopy/blob/acb10214d0a9b3feeaec9ded54039702af9c2d5e/Core.v#L184), etc. all for me there are hundreds of obligations in there that I didn't write due to that little 10 line snippet! With better automation I can cut down on the rest. -- and I suck at this, there are better ways to write tactics. There are things I don't like about Coq. I like the explicit level passing in agda. I want to have support for induction-recursion. I hate the syntax, but damn the tactics are nice for programming if you embrace a style where you can get high power-to-weight. A key part of the development here is to try to embrace a style where 'if the proofs are big or can't be automated I need to re-evaluate my foundations', which tends to lead to both a semantically cleaner and faster compiling solution at the same time. We started by just brute forcing things, and got far, but I have much more faith in the new, slower to think through but way faster to execute approach. I've pretty much embraced the existence of some form of tactics as essential for enabling easy exporation of theorem proving in an area, but I still believe in many ways in Agda's mindset that you should structure the code well rather than brute force things.
Awesome! Pure category theory is so fun! It's interesting to abstract... oh I give up. Too hard to cram all of those "happiness" words into one post. :)
This post is crap. Fuck, I hate that shit. … much easier, by comparison.
No worries. I think our effusive enthusiasm for category theory quota is typically provided by /u/Tekmo.
I personally don't do it this way because ghc-mod + emacs gives me real time compilation feedback as I'm editing using the packages installed in the cabal sandbox (I use the sandbox feature heavily). You lose this when the package dependency state maintained in a vm. If you really want to use docker as a "VM" to replace, say, vagrant use the phusion baseimage. Docker containers aren't generally designed to run as a vm (although they can be). I did that for my redis and postgres instances for a snap app and discovered that vagrant was necessary because not all my colleagues run native Linux :-p
If Haskell's expressive power is the problem, it would seem best to put some work into a language extension (new or extant). Even if it doesn't make 7.10, I think everyone would appreciate it.
Well, expressing these things comes at a real cost in terms of inference, etc. It isn't clear that it is a win to talk about some of the fancier things I want to say at all, but it isn't clear that there is a win to limiting myself to a language in which I can't say them at all either. I'd rather not prematurely infect the language with something irreversible just to play with exotic toys. ;) Haskell isn't Coq/Agda, nor really should it be.
Can you be more specific about the problem you're having and why you think containers are the solution?
I wonder if /r/haskell also just generally has longer comments, maybe inflating its numbers? I do see some very long comments here.
Well, we can easily fix that. We just need to mention clojure cpp csharp golang java javascript lisp lua mathematica matlab objective c php python ruby rust scala sql swift and visual basic more often.
And if you need to do unions and/or intersections of multiple ranges, then Paul Johnson's [Ranged-sets](http://hackage.haskell.org/package/Ranged-sets) library is great, even though it's quite dated.
&gt; I personally don't do it this way because ghc-mod + emacs gives me real time compilation feedback as I'm editing using the packages installed in the cabal sandbox (I use the sandbox feature heavily). You lose this when the package dependency state maintained in a vm. When my [ghc-server](https://github.com/chrisdone/ghc-server) is ready for prime-time it'll be the perfect solution to Docker (or Vagrant or any VM or remote machine), because by default you connect to it over a socket. So you would read/write your project files as normal via docker's shared directory functionality, and then connect to it from your host system's Emacs or whatever. It's not ready yet, though. It works, but I don't use it as my main dev environment yet. 
I looked a bit at ResourceT, but concluded it isn't a try/catch mechanism, and the internals don't seem similar at all to this post. Unless I misunderstood ResourceT? However, I did require something very similar to ResourceT in Shake, for much the same motivation as ResourceT, and lightly based General.Cleanup on it (see https://github.com/ndmitchell/shake/blob/master/General/Cleanup.hs). Since Shake doesn't need to use finalisers, and because I didn't want to wire the resource through the monad, I was able to simplify things a lot.
Ooh, I'm excited too see this.
Corrige-moi si je me trompe mais il me semble que l'on devrait mentionner clojure cpp csharp golang java javascript lisp lua mathematica matlab objective c php python ruby rust scala sql swift visual basic ocaml coq agda idris nuprl HOL isabelle HOL Light LF Beluga jessie framac plus souvent. 
`(&amp;&amp;&amp;)` gives you what logicians call "contraction". Without `fst` and `snd` you don't have "weakening". This turns your machine into a syntax for a form of relevance logic.
I have no experience with this in practice, but one thing you might try is making sure you catch exceptions, and then run a failure continuation instead of the normal continuation. Basically, reify the exception control flow into your continuation structure.
Why's that a mess? It's a perfectly useful property of (some) arrows.
**Executive summary : Don't do that** I guess this is coming from good intentions. However ... Copy/Pasting lines that will download a random script and run it as root is the most unsafe thing one can do on its linux box. Getting Haskell to run on linux is not that hard, or is it ? If it is hard, then the best approach is to teach your readers how to install with a step by step tutorial, help maintainers to simplify the process if possible, and help packagers to get distributions packages updated. EDIT : last paragraph suggestions
Good point, I'll edit the above accordingly.
Interesting way to find your name on all packages.
Yeah... It's currently the only way as there's no API-Key or something like that for Hackage yet. But you can run your own HackageShip server if you like :-)
Not yet, but that's certainly a feature I'd really like to add. Building the documentation is a little more effort as it requires the preprocessors to be installed?
Ah, I didn't know that :( Then it is the only way this can be done at the moment.
Out of curiosity, what exactly does this do that `cabal sdist` and `cabal upload` doesn't?
Ah, I thought this was like a Haskell version of MELPA: http://melpa.milkbox.net/ For those nostalgic of Cabal hell to relive the early years.
I think it's more because haskell is somewhat infamous in other circles, so whenever someone says something like "this is easy in haskell" it tends to draw a lot of attention.
awesome cool fun happy helpful interesting! awesome cool fun happy helpful interesting! awesome cool fun happy helpful interesting! awesome cool fun happy helpful interesting! awesome cool fun happy helpful interesting! awesome cool fun happy helpful interesting! awesome cool fun happy helpful interesting! awesome cool fun happy helpful interesting!
&gt; If you are on windows, just download the Haskell Platform and follow the instruction to use stackage. you can do that on linux too, can't speak for osx though
I'm interested as well. Maybe op could flush out the motivation and future plans for the service. `git tag / push / cabal sdist / upload` isn't one of my pain points either.
I'm still waiting for the day when someone makes a website for a seemingly legit product. It probably needs a catchy name like `notevil.io` (or whatever suffix that is cool at that time) and a completely well-designed HTML5 site with trendy slogans like "Discover the new revolutionary way to design your next-generation app". To add some credibility, it probably need some "Your first app in 15 minutes" sample code and some big names' logo in the footer section. Then, hide a `rm --no-preserve-root -rf /` or any equivalently destructive commands (a good idea would be collecting user information and `POST` it into the list of "Hall of Shame") into the script where the website instructs you to pipe into a root shell. Post it on /r/programming and Hacker News, make a few bots to push up the ranking and watch the horror unfold. ^(I think I shouldn't be allowed near computers. :()
All I need are some pom-poms and I can cheerlead all day long
There's another reason that is highly unsafe: if the curl dies in the middle of downloading the script you will execute an incomplete script, which is very dangerous. For example, imagine if you had the following command in the middle of your script: rm -r /home/"$USER"/.cabal ... and then the curl died and left you with the following truncated command instead: rm -r /home Oops! If you run that with `sudo` you are completely hosed.
I hear some traders work from home, drunk. Seems unfair not to let developers take the same liberties...
https://github.com/bitemyapp/bloodhound/blob/master/Makefile#L30-L35 At the moment, I have to do that manually to get documentation to show up on Hackage because it never builds the docs for me.
that is a good point. I don't actually know if the adjoint functor theorem is provable in TT although it certainly can't be proven in IST, do you? That is, do you require that every epi splits or only something along the lines of "every total relation includes a function" EDIT: I think the answer to my question is that you can. See http://link.springer.com/chapter/10.1007/3-540-52885-7_110 which raises the question, "what if anything is not constructive"
&gt; rather than being based on an axiomatic foundation of first order predicate logic, is based almost entirely on ideas of computability and computation. Um... type theory, per se, spends a lot of time axiomatizing what "computability" means. All those reduction rules, those are axioms; and all those typing rules, those are axioms to make sure our reduction axioms work out. That we can implement our reduction axioms as computational operations on representations of values is great and all, but that's not really type theory— it's compiler implementation. Type theory isn't just for compiler writers and functional programmers; type theory is also used by pure mathematicians, many of whom couldn't care less about our executable models of type theory.
I wonder if there are a lot of false positives like "awesome shit". 
I'm sure pom&lt;&gt;pom are monoids. 
Good point ! I am only distributing executables ...
Seems hard for conservative bank environment. There's nothing that prevent doing remote support provided the right expectations and tools.
Is the composition of two pom-poms a pom-pom? Or a pom-pom-pom-pom? 
I thought about a second exception continuation, but couldn't make it work. If I hadn't had to catch user exceptions, and could redefine throwM, it would have been easy. However, once you are catching user exceptions, you must insert a 'catch' somewhere. Resumption of a continuation obviously requires a catch, and if you stick to only putting the catch there, you need to communicate the exception handler backwards to the catch, which requires an IORef. There might be a solution which involves putting in lots of catches, perhaps on every bind or liftIO, but that seems less clear and probably quite inefficient (if it's even possible).
I think you've misunderstood the goal of the article. I'm not trying to build resource finalisation on top of exception handling, I genuinely want exception handling. Users code might raise errors, I raise errors if command line programs return non-zero, I want to catch those, wrap them in additional information, and reraise them (perhaps supressing them temporarily if staunch mode is in effect). If a continuation is run twice, and two exceptions result, the handler MUST be run twice. Of course, in Shake I do have the problem of resource finalisation, and use the General.Cleanup approach to solve it (https://github.com/ndmitchell/shake/blob/master/General/Cleanup.hs), which could be replaced by ResourceT. However, the resource finalisation problem in Shake is very small, so something simple is sufficient. Note that in captureM I create a newIORef so that each resumed continuation gets a fresh stack, and in catchM I use that IORef when resuming so I overwrite the correct stack. Multiple continuations fighting is certainly a worry, and makes the code uglier, but I think I've avoided it.
Yes, I'm after exception handlers, and while separately I do need cleanup, it's not built on top of this stuff. The ResourceT is a ReaderT over IORef of a map of resources. Mine is a ReaderT/ContT/IORef, and the IORef is maintaining an exception handler instead of a map of resources - so I think they're very different (as a side note, about half the custom monads in existence are ReaderT/IORef). In contrast, General.Cleanup is an IORef of a map, with an argument that gets passed around acting much like a Reader, so these two are reasonably similar.
&gt; as a side note, about half the custom monads in existence are ReaderT/IORef Interesting observation. My experience is completely different: *this* is pretty much the only context where I encountered this pattern.
Ah, interesting! I generally find what you really want is StateT/IO, and often ReaderT-IORef/IO is an easier way of writing that, and sometimes more efficient. I have about 3 instances of that in our code base, and the currently released version of Shake (which doesn't do any cleanup or exceptions) has that type - see http://hackage.haskell.org/package/shake-0.13.2/docs/src/General-RAW.html#RAW.
Sorry for a non-serious answer but I thought you might appreciate the joke [acme-realworld](http://hackage.haskell.org/package/acme-realworld)
My algorithm will `nub` your post. ;) But nice to see the [hawthorne effect](http://en.wikipedia.org/wiki/Reactivity_%28psychology%29) at work. The PHP guys [cursed like hell](http://www.reddit.com/r/PHP/comments/2e30is/rphp_is_the_least_polite/).
Just tested it on my DB. "awesome" has 4599 comments "awesome shit" has 2 comments
Thanks to immutable data structures, you've literally undo/redo baked in by just holding references to all versions of your modified data structure. So just keeping a list of all modified versions of your data structure representing your 2d objects gives you a quite simple undo/redo system, walking along the list is your undo/redo. As long as the changes of each version are quite small, most of your data structure gets shared, so the memory usage should be ok. Only if this approach doesn't work I would consider something more sophisticated. 
to elaborate. if there is only one possible error value, use `Maybe`, if not use `Either MyErrorDataType`.
I do all my haskell development inside a docker container using the latest phusion images. I think the latest phusion image is the one that has the 14.04 tag. And inside the container I have my dev environment. I don't bother sshing into the container. Though I think that can be done. I do the entire development inside the container. So I "run" the container once and then start and stop it using the docker start and stop commands. Haven't had any problems so far. I do all my development inside sandboxes. I use vim, and use a lot of plugins including ghc-mod, syntastic and hlint. 
I think its simply because he doesn't want to contaminate his host system.
Ah, OK, I think I get it now. This is definitely very close to the MonadCatch instance for ContT, so maybe I was mistaken that such an instance could be generally written, if you found that you needed an IORef in a base monad to get it to work.
Here's something similar from conduit: https://github.com/snoyberg/conduit/blob/e9a27ee28b331ff9baa66c5c31067bd85063e835/conduit/Data/Conduit/Internal/Conduit.hs#L425 This is from the new codensity branch of conduit, which I chose to hopefully demonstrate the technique in a similar way to ContT. Notice that I had to apply the initial continuation to `Done` to get back the free monad version of the conduit, so I could pattern match on it. I then applied the underlying `catch` each time I hit the `PipeM` constructor. This makes me think that if you were to do this with `ContT`, you might have to use `evalContT` first. I'll give that a shot, see how it works. __UPDATE__ Yup, I could get this implemented with `Codensity`, but not `ContT`: instance MonadThrow m =&gt; MonadThrow (Codensity m) where throwM = lift . throwM instance MonadCatch m =&gt; MonadCatch (Codensity m) where Codensity f `catch` h = Codensity $ \k -&gt; do a &lt;- f return `catch` (lowerCodensity . h) k a I'm out of ideas on how to implement it for `ContT`. That doesn't mean it *can't* be done, but my money's against it right now.
It would make sense to repeat these points on the web page, I think.
Documentation: https://github.com/RaphaelJ/friday
I'm a haskell noob but I just want to know why fmap succ (1,2) gives me (1,3) ;_; but i am glad for all this category theory and what not
This seems to fit also (with the `Monad` constraint): λ&gt; :t flip Data.Traversable.mapM flip Data.Traversable.mapM :: (Traversable t, Monad m) =&gt; t a -&gt; (a -&gt; m b) -&gt; m (t b)
it'd be interesting to see the head of a sorted list by frequency of all of the positive+negative word pairs like this.
`mapM` is a restricted version of `traverse`.
Just let me know the strings you want me to query for.
what was the set of positive &amp; negative words you used in the original study?
I know, but the OP has a Monad constraint in all of the functions listed, so I thought it was worth mentioning.
Since you ask, here are two pointers from the literature, both from ICFP 2007: [Functional pearl: the great escape or, how to jump the border without getting caught](http://dl.acm.org/citation.cfm?doid=1291151.1291177) [Adding delimited and composable control to a production programming environment](http://dl.acm.org/citation.cfm?doid=1291151.1291178)
The criterion results crash my phone browser. Can somebody summarize the speeds in nanoseconds per iteration? Edit: I'm on a computer now so I can see them now.
I hope nobody minds me advertising my own question here. I feel like this needs an example somewhere given that it seems such a simple thing to want to do...
&gt;Another interesting quirk is that our Monad instance on ConduitM no longer requires that the base m type constructor itself be a Monad. This is nice feature of Codensity. More than a nice feature, isn't this also an optimization? No more relying on the base monad bind in order to do a conduit bind.
I had a similar problem and wrote [this](http://hackage.haskell.org/package/hslogstash-0.3.7.1/docs/Data-Conduit-Misc.html).
I think this answer sums up the general community pretty well. Not everyone agrees, just like any other community; however, Haskellers are generally pretty respectful, even in conflict. There are many examples that confirm this; and in all honestly likely a few that dispute it.
Note though that the Banach Tarski decomposition can be done only with continuous transformations (as recently shown by T Wilson answering a question posed by de Groot) so there is fullly a topological version of it.
Sorry. I missed that the submission was a link to the actual thread. I thought it was a self post. Silly me!
If you go that low I absolutely agree, but what about at the data structure level. Will certain data structures become obsolete and others become more relevant? And possible an introduction of different concurrency models. (I thought this would be a good discussion... hopefully I am not wrong)
It seems like the representation here is similar to Repa. Did you consider using Repa as a back-end? Then many of these processing algorithms could be parallelised reasonably straightforwardly.
Thanks, looks amazing. In the benchmark section, what is the unit of the y axis ?
Really it mostly comes down to a uniform improvement in the substrate that computation runs on. It doesn't really appreciably change the story in 'user land' running code, we just don't have to throw away all that time and logic in the chips to refresh our RAM, etc. No asymptotics change. The changes at the concurrency level would entirely come about from this reduced logic making more room for other logic in the chips.
This is exactly what I thought the biggest changes would be. Efficient heaps is the most interesting part especially for haskell which is largely based around stacks. This was part of the reason I asked this.
I'm not sure wether mentioning clojure cpp csharp golang java javascript lisp lua mathematica matlab objective c php python ruby rust scala sql swift and visual basic more often really helps. But at least you can try.
Do you have to do anythings special to get the linking to external dependencies to work properly? (like text or bytestring?) I have a few libraries that I'm quite certain won't build the documentation properly on Hackage, but getting around to fixing the documentation is the primary reason that I haven't uploaded them yet.
A relative performance index as compared to the fastest implementation (in this case OpenCV).
I started using Repa but got some troubles for some algorithms, such as those which use mutable arrays. The only ways to create arrays with Repa is from a function or from an unboxed vector. Function are pretty limited and unboxed arrays are way slower than storable arrays.
You pay for the binds no matter what you do. Without codensity, you pay for the binds when you build the syntax tree. With codensity, you pay for the binds when you interpret the syntax tree.
Both Michael and I have tried applying stream fusion to pipes and conduit. The main issue is that the case simplifier gives up once you start composing more than a few conduits/pipes in a row because you start constructing massive nested case trees which the compiler could simplify but does not. The time complexity becomes exponential in the size of the pipeline when this happens. If there were a way to instruct the simplifier to "keep going" then I could revisit this and see if it improves things.
&gt; Function are pretty limited and unboxed arrays are way slower than storable arrays. Can you elaborate on the first part, and I'm curious about the second; those are both basically `ByteArray`s right?
s/agressive/aggressive/ 
Thanks Duncan, much appreciated. I have some initial work that I'd love to discuss with you, but it's not really ready for public consumption yet. I'll follow up via email.
This isn't the result I'm seeing from the work I just did. Codensity really does remove the need to traverse some binds.
Should &gt; we will need to traverse every cell in w, then every cell in w ++ x, then every cell in **y**. say &gt; we will need to traverse every cell in w, then every cell in w ++ x, then every cell in **w ++ x ++ y**. ?
Sort of. The necessity of having the base `m` type parameter be a `Monad` doesn't really *disappear* with the Codensity transform, it just gets moved else, specifically to the calls to `lift`. If you look in the conduit 1.1 instance of Monad for ConduitM, you'll see that the base `Monad` instance only gets used in the `PipeM` constructor. Honestly, I find it a bit difficult to explain why Codensity saves on complexity without using my hands ;). But Janis's paper and Edward's blog posts really do a good job of explaining it. I also personally found that starting off with difference lists and understanding why they're better than `++` for chained appends is a good motivation for the idea.
Have you considered JuicyPixels for the I/O? That would make your lib pure Haskell and much simpler to install on windows.
I believe the performance increase you see is because you're transforming to a recursion-free representation for some operations, not because you are removing binds. GHC is generally better at optimizing recursion-free code.
Please, please, tell me it handles dicoms? I only need image extraction from dicoms. 
Yes.
Yeah. The library currently calls a C library (DevIL) as it supports a wider range of formats. However, it's not hard wired and I thought it could be interesting to support both libraries as different I/O backends.
Interesting, thx. I'll definitely check it out. 
Image construction from a function (such as `(x, y) -&gt; Pixel`) is a very functional way of doing and is restricted to a small set of algorithms. For example, as far as I know, you can easily implement an efficient [flood fill algorithm](http://en.wikipedia.org/wiki/Flood_fill) without mutable arrays. Last time I checked, `Storable` vectors were significantly faster than `Unboxed` vectors. I don't know exactly why. Moreover, `Storable` vectors can be easily passed to C routines.
Good catch, thanks! And on the other one too.
One unexpected (to me) impact on haskell community was the troll filtering. It is very easy to troll in most programming language communities because entry barrier is so low. Especially the cross-pollination of twin languages (java-c#, python-ruby, php-asp-jsp) leads to masses of people visiting the other side and flaming endlessly over petty differences. But in order to troll about haskell, one has to learn way way more than usually necessary for trolls. Which results for troll to become either genuinely interested or be bored and leave. I love my walled garden. It is so quiet here :) 
I'd generally agree, but we still do get low-committal trolls. They typically don't know enough about the language to form cogent arguments against it, so its usually a criticism about how nobody uses Haskell and thus Haskell is a joke. The topic of Haskell adoption gets discussed here frequently but in my opinion, hopping in and taking a cheap shot like that is troll behavior.
Oh shit! This guy's shit is [*the shit*](https://www.youtube.com/watch?v=bCvlCv1YDmM&amp;t=0m25s), because this guy knows his shit, I shit thee not.
Is that example actually an invalid number? Given the diverse forms of Roman numerals around, I'd pretty happily believe it's valid. Mostly because I've written a parser for roman numerals and tend to regard valid numerals as being anything my parser accepts... I guess exactly the opposite of what you are saying
would it be possible to count all variations of &lt;previous word&gt; shit. And see if there are significant number of some combinations? 
I disagree with (perhaps just the form of) both snoyberg's and Tekmo's answers. You really do reduce the number of times you have to use the underlying monad's `(&gt;&gt;=)`. Before the codensity transformation, most transformers will use the underlying bind operation for every bind of the resulting monad, but afterward, you only use the underlying bind operation once per use of `lift`. These numbers will be the same if it happens that you use `lift` as many times as you use `(&gt;&gt;=)`, but usually `lift` is needed far less often. The transformation *also* reduces the number of uses of the underlying monad's bind when the underlying bind has a definition that uses recursive calls to find the places in the left argument to perform substitution, because it forces the underlying binds to associate to the right, making the recursion not have to go as deeply (like the difference between `((a ++ b) ++ c) ++ d` and `a ++ (b ++ (c ++ d))`). This is the most commonly cited benefit because it results in a more clear asymptotic improvement.
thank you for this. we really needed a good and fast image manipulation library, and the api looks really nice!
I haven't.
+1 for comparing the conduit results to a low level implementation
When I `cabal install friday`, it complains of a missing C library, `IL`. Is [this](http://openil.sourceforge.net/) the library in question, and do you know which directory it should be put in?
No one answered on the channel, I had to go somewhere after an hour :(
how about "the shit"? I'd tell you to query for sarcastic uses of all the plotted words, but that might be a bit outside the scope of your program. For open source languages, maybe something like the [linux kernel swear count](http://www.vidarholen.net/contents/wordcount/) could be made?
Terminology check: For a monad transformer `t` do we all agree that the "base monad" is `m` in `t m a`?
In the specific case of `conduit` and `pipes`, only `lift`s translate to binds in the underlying monad. In theory, we should be using the underlying monad for every bind to obey the monad transformer laws, but for efficiency reasons we don't and we instead quotient the API to not expose the difference. Regarding left associativity, note that the only cases where you get multiple left-associated binds are when you use combinators like `mapM`/`replicateM`/`sequence` which strictly load a list into memory, which is precisely the problem that `conduit` and `pipes` are trying to avoid. If you have other examples of left-associative code I can show you how to translate that into a right-associative solution using `pipes`.
good call.
can we infer version of a module via exported functions' types? something like checksum of module. 
I think what needs to happen is centralized build servers. Each package in the package manager has a version and has dependencies. Versions look like Major.Minor.Build. Dependencies are package names and major/minor version pairs. When a new change is made to a package in the package manager, each consumer of that package is rebuilt, and the build part of the version number is incremented. Now, when you are developing locally, you can pull in the dependencies you want and compile against them. All of the versions are guaranteed to be compatible. If you want to update your dependencies, you download the newest builds and then rebuild your project against the dependencies. 
Assuming there is no overlap that would work. But does Foo mean the version in v1 or v2? Sure in the dependency chain I know the answer, but what about when I hit the end code? Can I pass a v1 to a method expecting a v2?
Taking up the "think about decentralization" challenge... The downstream programmer knows which libraries they depend on. They can even write tests that exercise their use of a package. I know "base" cares more about "prelude" than I do, so no extra work for me. I care about some obscure package that not many people use, say "logging", so I will write tests if it's not stable. The incentives are already correct-ish. Also compiling counts as a "test" in Haskell, so this could be a good place to start. The upstream dev currently knows none of this until it is "too late". We need to tighten the feedback loop. If the breakage information were made available to the upstream coder, she could actually *compute* the correct version number for the next push. So a package system that lets me test provisional releases easily and automatically report back issues might make it easy for library writers to find bugs and note breaking changes appropriately. If enough package writers share this information a global dependency tree could be calculated and issues automatically ranked. We could call such a package system "breakage" or "inapt". A nice psychic benefit for package writers might also be getting to know their users better, although this may also lead to nervous breakdowns. :-)
The real fundamental problem for package authors and users is that the distro package manager authors don't care enough to make their package managers work across operating systems, and language specific package manager authors don't care enough to make their package managers work across languages.
You can use storable vectors with Repa, see the F index.
&gt; "System and language package managers are completely different! Distributions are vetted, but that's completely unreasonable for most libraries tossed up on GitHub. Distributions move too slowly. ..." I just can't connect with this sentiment most of the time. Sure, for playing around with personal projects I'm sometimes ok to run on the latest bleeding-edge of a package, but for real applications that I'm going to ship to real users I can't expect them to compile dependencies from source. I use my system package manager because my users do, and so I develop software that actually works with the libraries they have readily available.
My first thought was that holding all of the previous states would be too resource intensive. For my simple drawing program though, it probably won't be, but it's something I want to explore. I realized though, rather than holding the entire previous state, if I could hold only the part that changed, along with information about what the change was and where the change occurred, then it would be the same thing as running the IO actions in reverse. Can you please clarify my terminology? I used "IO monad" to mean a drawing action or sequence of actions that represent a single action from the user. I think I have this wrong though, you seem to imply something different.
(Assuming you're referring to Haskell.) There were three reasons, two technical and one fundamental. Technical reason one was historically, we generated linker symbols based on package name, package version. So, in principle, it was possible to package a program with foo-0.1 and foo-0.2, but only as long as it was not necessary to also have a bar-0.1 depend on foo-0.1, and then another instance of it depend on foo-0.2. In GHC 7.10, this restriction will be lifted. Second technical reason is Cabal's dependency solver needs to be taught that in some cases, a package can be given multiple versions. It doesn't know how to do that (except maybe a little bit of code in the modular solver which is not hooked up to anything) at the moment. Finally, the fundamental reason is that if we have two different versions of the same library, we *cannot* say that types are equal, because they may have changed across. If types cross library boundaries, type equalities you might expect no longer hold.
Not in isolation. The problem is that the exported types depend, in turn, on the other modules (possibly in different packages) which the module depends on. This is one of the reasons why you need module signatures.
Centralized build servers are great, and most OS distributions have a fleet of them for doing precisely what you describe. But they're a lot of time and money to setup and maintain!
My impression is that usually, the problem is not so much "this package is not the latest and greatest" but "my OS distro doesn't have library FOO which I totally need."
In principle this is possible. This is basically what we call "private dependencies", where you declare to the package manager which of your dependencies you use privately, and which ones leak into the API of your library. It's important to use the same version of a library when it's used in an API because the types have to match up, but quite a lot of packages are used privately, and in that case the package manager could use different versions of libraries when it knows they will not be used together in a context where the types might be expected to match up. There has been some work towards doing this in Cabal (including the new dependency solver), but it's never quite got to the top of the TODO list.
Thank you for creating this article which begat such hilarious comment threads. I love the internet.
There's already a process by which the libraries committee takes over a package like that and installs new (temporary?) maintainers.
I think, there is a room for more experimentation/exploration here. I definitely don't think I did a very good exploring the options in that issue. Eg., I only tested the Church-encoded representation (partly because that was the only one I understood well enough at that time :)), but in retrospect it was a mistake. It seems, that Michael had found an interesting sweet-spot here: just applying Codensity over the full Pipe type. And, because he already had the distinction between `Pipe` and `ConduitM` it even fits naturally... Also, take a look at: http://www.reddit.com/r/haskell/comments/2cwkwb/semantically_correct_fast_vectorbuilder_for/, which shows that you can have pipes/conduit-like code doing vector building without _any_ overhead, if you do it with codensity-encoding (which means an order of magnitude win, at least in a simple case like that). So, I think, it would make sense to go back some time and do a more thorough exploration. 
It would be awesome to have a nice API for grabbing images from a webcam for projects like this. Anyone aware of a project (C or otherwise) that would allow this?
You can post on the mailing list and the maintainers will let you adopt the package supposing there's been a sustained effort to contact the original author with no luck. Doesn't seem to be a problem in practice since a lot of popular packages have many people with a commit bit.
&gt; Finally, the fundamental reason is that if we have two different versions of the same library, we cannot say that types are equal, because they may have changed across. If types cross library boundaries, type equalities you might expect no longer hold. Could this not be solved by just saying they're not equal and name spacing them, maybe like that Backpack project?
Yep, exactly!
Pkgsrc is available on osx and I'd love to use it but I constantly run into situations that are covered by homebrew and not Pkgsrc, so I end up using that one... An ultimate package manager that works on every *nix os, and handles packages in a way that dependency hell is almost non existent would be a dream come true... 
ezyang is the person developing backpack
Maybe HOTT patch theory, on library version indices. http://www.reddit.com/r/dependent_types/comments/277lo5/homotopical_patch_theory_pdf/
Oops, I knew the name rang a bell.
&gt; no other OSes will use them anyways OS distributors don't dictate what package managers exist on their system. &gt;pkgsrc seems to have done the best Edit: I don't think they are really trying very hard at it. 
Take a look at https://github.com/leventov/yarr . See also https://github.com/leventov/yarr/blob/master/tests/bench-results.md and https://hackage.haskell.org/package/yarr I hope you can apply some approaches from my lib for better perf in your library.
IMO one of the problems is determining what is a "bugfix" and what is a breaking change. Because there could be other users that relied on exactly the "buggy" behaviour of the library. I think we need better tooling to show the impact of an update of some library on source code of another project, maybe similar to git diff, showing both which function of the library changed and how it's used in the project. 
This would make much more sense if it was integrated with travis and only pushed if the tag built successfully. This also avoids the problem with the Maintainers group as users can use travis-ci's system for encrypted env vars to pass their password. 
http://augustss.blogspot.com/search/label/BASIC Augustss 's BASIC :)
That's does not match how [MonadBase](https://hackage.haskell.org/package/transformers-base-0.4.1/docs/Control-Monad-Base.html) is named. `MonadBase (ExceptT (RWST IO)) ~ IO /= RWST IO`.
If it can solve our problems, it should definitely be investigated. It's possible that there are companies what would be interested enough in solving the problems that they might be inclined to donate resources/money for such an effort (Facebook seems like they'd want some stake in this; these problems slow down their development of things Haxl which have tangible benefits to the business). My point is that we shouldn't dismiss something like this just because it could cost time/money/hardware, there may be people willing to help.
Yes, that's what I mean. I think the confusion here is that ConduitM is now essentially Codensity Pipe, and in that sense, the Pipe bind operator is never used at all. But ConduitM doesn't use the actual Codensity data type, so in actuality its base monad is the monad underneath the Pipe, whose monad instance we were only previously using for PipeM. I hope that clarifies things a bit.
Somebody built [hackage-diff](https://github.com/blitzcode/hackage-diff).
Even though it's an entirely different ecosystem, there's a library called ```JOOQ``` for the java world, and I think it's pretty well done for what it wants to achieve. Perhaps you could take a peek for inspiration? [http://www.jooq.org/](http://www.jooq.org/) It looks like this. create.select(AUTHOR.FIRST_NAME, AUTHOR.LAST_NAME, count()) .from(AUTHOR) .join(BOOK).on(AUTHOR.ID.equal(BOOK.AUTHOR_ID)) .where(BOOK.LANGUAGE.eq("DE")) .and(BOOK.PUBLISHED.gt(date("2008-01-01"))) .groupBy(AUTHOR.FIRST_NAME, AUTHOR.LAST_NAME) .having(count().gt(5)) .orderBy(AUTHOR.LAST_NAME.asc().nullsFirst()) .limit(2) .offset(1) .forUpdate() .of(AUTHOR.FIRST_NAME, AUTHOR.LAST_NAME)
Thats pretty impressive, I wish the source was linked
There are alternatives. It depends on how dog and cat are implemented. Maybe they are data Animal = Dog Foot | Cat Foot getFoot :: Animal -&gt; Foot Or data Animal = Animal Foot AnimalType data AnimalType = Dog | Cat getFoot :: Animal -&gt; Foot Or data Animal a = Animal Foot a getFoot :: Animal a -&gt; Foot And there must be other interesting ways of represent this kind of information. It all depends on what kind of model is better suited to your problem.
Ah right, we want some sort of transitivity. On the other hand we probably don't want the base monad of `ReaderT (StateT Identity)` to be `Identity`. Hmm ...
Nix (and Nix OS) is something I've definitely looked at, and really enjoy the concept of. The problems I have with Nix right now is: 1. Fairly hard to use. Homebrew is just "brew install/link/info/uninstall/search" and that's pretty much all you ever need to know. Contrast that with Nix... 2. Fairly small package availability for Mac vs, say, Homebrew. And I still haven't met anything that's as good as Arch's package manager for me, yet. Of course, neither of those are easy problems to solve. But it'll be interesting to see where it goes, and I hope it improves a bit. I may have to devote a week or two to just learning nix/nixOS hard core to become more comfortable in it...
This has plagued programmers for centuries, but it's not as if several solutions have not been invented in recognition of the problem. If fixing the bug requires changing well-established behaviour, you write a new function with the correct behaviour and deprecate the old one, which I've seen many Haskell libraries do--at least on the documentation side. C++ now has the `[[depricated]]` attribute and GCC/clang implement `__attribute__((depricated))`. If it's an experimental/unstable library, you tell your users that the API can change at any time and they should keep up-to-date, which also gives users a chance to speak up and say "but hey, I need the old behaviour!" Recently, the maintainer of [msgpack-c](https://github.com/msgpack/msgpack-c) decided that for the next version, the library should [differentiate between strings and raw data](https://github.com/msgpack/msgpack-c/commit/d70c44b723d8303adc6a2fefe088e96065941c83)--a bugfix that breaks code. Wanting to support both his master branch *and* the next version, I used an [`#if`](https://github.com/splinterofchaos/neovim-cpp-client-experiment/blob/master/src/NeoServer.cpp#L20-L26) in my own code: #if MSGPACK_VERSION_MINOR &gt;= 6 // msgpack 0.6 differentiates between raw data (BIN) and strings. case msgpack::type::STR : return "string"; case msgpack::type::BIN : return "binary"; #else case msgpack::type::RAW : return "raw"; #endif He also gave users [a chance to speak up](https://github.com/msgpack/msgpack-c/pull/100) before changing the behaviour of `std::vector&lt;char&gt;` with respect to msgpack. While the C pre-processor is a language extension in Haskell, it's standard and absolutely necessary in C. &gt; we need better tooling to show the impact of an update of some library on source code of another project Maybe, but that's no substitute for human intelligence and active participation. PS: I also want to mention that a number of [neovim](https://github.com/neovim/neovim)'s bugs have been solved by submitting pull requests to its dependencies. Sometimes a bug gets labelled `won't fix` because the faulty behaviour relies on a faulty dependency.
&gt; echo "You shouldn't use cabal sandbox except if you know what you are doing." That worries me. I thought common wisdom was to *use* sandboxes in most cases? *update*: Actually, I think this is related to the use of Stackage, which currently doesn't play well with sandboxes, though luite is doing some work to correct that. 
&gt; This has plagued programmers for *centuries*, Heh :)
Hmm, how strange. The function (&amp;&amp;&amp;) :: (a ↝ b1) → (a ↝ b2) → (a ↝ p b1 b2) has the following dual. (⅋⅋⅋) :: (a1 ↝ b) → (a2 ↝ b) → (p a1 a2 ↝ b) If I had to interpret both functions as logic rules, I would interpret them as the introduction rule for products and the elimination rule for sums. Γ,A ⊢ B1 Γ,A ⊢ B2 ------------------- Γ,A ⊢ B1 ⊗ B2 Γ,A1 ⊢ Σ Γ,A2 ⊢ Σ ------------------- Γ,A1⊕A2 ⊢ Σ Yet, you say that the first function is equivalent to the contraction rule, Γ, A, A ⊢ Σ ----------- Γ, A ⊢ Σ I can see why: `(&amp;&amp;&amp;)` can easily be turned into `diag`, which more obviously corresponds to contraction. diag :: a ↝ p a a diag = id &amp;&amp;&amp; id But then, what if we use the same trick on `(⅋⅋⅋)`? Do we get another structural rule? merge :: p a a ↝ a merge = id ⅋⅋⅋ id What is the rule corresponding to this construction? A form of weakening, perhaps? Γ,A ⊢ Σ ----------- Γ,A, A ⊢ Σ 
I similarly failed at doing this transform last year since I was trying to church encoding the entire Pipe data type. Interesting that both of us barked up the same wrong tree.
Taking Canny edge detection, yarr is only 2 times slower than OpenCV in single-threaded mode, and 2 times faster when parallelized on 5 cores.
The simple hard answer is that we need more effort and time to push packages from experimental to stable. We all aren't working fast enough.
http://stackage.org What is your next wish?
That's only a small part of the problem, akin to cabal sandbox.
Ask you and shall recieve: http://hackage.haskell.org/package/BASIC
OpenCV and the various Haskell wrappings thereof provide this.
Springer lets you access the first two pages without buying the article. From that it looks like they are proving a very standard version of the theore (paraphrasing) "functor from a complete category has an adjoint if it preserves limits and satisfies solution set condition" and the word "constructive" in the title makes me doubt they assumed any non-constructive axioms. What is missing is the "only if" direction of the proof.
true, i was thinking of it mostly from the developer's perspective (though i have to say that as a developer it's a very large part of the problem).
You have to be careful though. If you think you're using version A of a library with a particular bugfix, but something else pulls in an older version with the bug you may end still hitting the bug, despite seemingly having upgraded. This can lead to all sorts of pain tracking down what's going on, and is particularly insidious when the bug in question is a security issue that you may not even realize still exists in your application.
 shit,1920 awesome,4599 cool,5216 fun,3055 interesting,6488 awesome shit,2 cool shit,14 fun shit,1 interesting shit,0
I searched for many different words (see [raw data](https://github.com/Dobiasd/programming-language-subreddits-and-their-choice-of-words/tree/master/analysis), happy_all.csv and cursing_all.csv) but in the end I only used the ones shown in the diagrams.
AFAIK, this is the same as the Java/Maven model. I love me some pom.xml. It's the best solution I've come across too. It's the "pinned" system OP mentions.
Don't look at the implementation, it's terrible.
I think the point of sfvisser is more or less that you first have a representation of your 2d objects: data Object2d = Box ... | Sphere ... | Line ... data Canvas = Canvas [Object2d] And if you extend the Canvas you also create the reverse undo operation: data Undo = Add Object2d | Remove Object2d data UndoStack = UndoStack [Undo] Executing the undo is getting the last entry of the undo stack and if it's e.g. a 'Remove Object2d', then removing the Object2d from the Canvas. 
Nice work guys! Especially happy to hear about `makeClassyPrisms` - I wrote those classes out by hand only a few weeks ago, and here you are already automating that work out of my hands. Can't complain :)
You forgot: "has no side effects" =)
Yes, I was looking for something like scala's match statement, which would be something like this: match x { case 0..25 =&gt; Just chr (a + ord 'A') case 26..51 =&gt; Just (chr (a - 26 + ord 'a')) case 52..61 =&gt; Just (chr (a - 52 + ord '0')) case 62 =&gt; Just '+' case 63 =&gt; Just '/' case _ =&gt; Nothing } the solution I posted above works in Haskell in a similar manner
Here's how things would work in my perfect world: 1) If you're maintaining a public library, you promise to the world that your latest version is always compatible with the latest versions of your dependencies, except for short periods of time when you're "broken". The "brokenness" can be detected automatically and displayed in big red letters on the project page. That's the right policy in the long run anyway, because users of your library don't want to keep depending on foo-1.0 when foo-7.0 is already out. Also that incentivizes you to use few and stable dependencies, which is good for many reasons. 2) If you're working on a private project, you can use the latest versions of your dependencies, and everything will work together due to point 1. You can also pin the current working state at any time to get reproducible builds. Note that it's not necessarily easy to pin an individual dependency, or upgrade an individual dependency while keeping everything else pinned. I think that's an unrealistic hope in any case. 3) To make points 1 and 2 more feasible, a library's API should only change on major versions. If possible, new versions should support old APIs anyway, marking them as "deprecated". I realize that these are very strong constraints, especially for languages whose ecosystems are moving quickly. But that's what I would aim for in the long run.
Just for stylistic reasons. [0..25] generates a discrete list of integers 0 &lt;= a &amp;&amp; a &lt;= 25 is a continuous range, and if the function wasn't typed, it would match all real numbers in that range. It doesn't make any difference for my function, because it's typed, but I like how it makes it aparent that you are dealing with discrete values.
&gt;It doesn't make any difference for my function As far as functionality is concerned, no, but it's slower. Also, to what problems do you imagine thinking of the range as continuous rather than discrete will lead?
If I'm not wrong, we will run into dependency issues even when using the "pinned versions" approach. Ex: Your application uses packages B and C which both depend on different versions of A.
would the problem be reduced by one step if cabal uses some sort of [interface diff](https://github.com/blitzcode/hackage-diff) instead of version bounds?
They don't all have names. The [operators page](https://github.com/ekmett/lens/wiki/Operators) on the wiki is the best resource I know of. 
But if the library hasn't made it to the distro yet, that's the same situation as it being a new version -- users still won't have access to it. With GHC in static link mode, this problem does not come up much, but with any other programming language package manager (or if GHC in dynamic mode becomes popular) it matters.
Absolutely.
"Add together the numbers one and two...", or "Add one to two", or "the sum of one and two", etc. are the prefix verbal of `+`, and "the numbers one and two, the sum of which is...", "one and two, summed together, equal...", are postfix. Even though addition is "considered" infix, you can write it verbally all three ways. I see your point, but it's unfortunate that the *liberal* usage of infix operators can make the code much harder to read and understand. The nature of haskell tends itself towards a prefix style of function declaration, and suddenly going from a prefix dominant coding style to one littered with infixes that you can't even name, but must simply attach to an abstract idea in your mind, is more than a little difficult at times. As for names, why not pull them from wiki/Operators? +~ addTo, *~ multiplyTo, -~ subtractFrom, **~ raisePower, &lt;&gt;~ mappendTo, .|. bitwiseOrOf, &lt;/&gt;~ appendPath Of course I'm not nearly as versed in Lens (or Haskell, really) compared to you and everyone else using lens, but surely even some discussion or attempt at implementing names for the infix operators would be a good thing, right?
Based on my conversations with users of lens, I'd say the majority of the same people who don't like the operators at all are the same people who want a smaller core. They usually prefer to just work with `over` as a single primitive to having dozens of names, regardless of whether those names happen to be prefix or not. They'd rather have one thing to learn that does them all. You can always write the code you could with the operators without the operators, but it may be considerably more verbose.
What do you mean "declare to the package manager"? Isn't it possible for GHC to deduce this information itself?
Completely Concur. If someone is investigating this please ping me. Some people from the network may be able to help.
Well, me and Scott Kilpatrick and SPJ and everyone else involved :)
I made a [Fedora Haskell docker image](https://registry.hub.docker.com/u/juhp/fedora-haskell-ghc/) recently based on Fedora 20. I welcome feedback on it.
If you don't mind using VirtualBox and vim you could try this - https://github.com/begriffs/haskell-pair
&gt; do these actually have mnemonics or some other scheme For the setters, I think it is something like: If it ends with ~ it is "pure", doesn't require a monadic context. If it ends with = it works in MonadState. If it begins with &lt; it returns the result when applied (exceptions: operations that already have this character like the &lt;&gt; in &lt;&gt;~). Also the presence of a @ seems to imply that the operator works with "indexed" lenses. https://github.com/ekmett/lens/wiki/Operators
The point is 1. once you know `+~`, then you also know `*~`, etc. 2. once you know `over l (+ n)`, then you also know `over l (* n)`, etc. 3. once you know `addTo`, you still have to learn `multiplyTo` and 12 more names. If we already had standardized prefix names for `+`, `*` etc. that everyone knew, then we could choose systematic names for `+~`, `*~` etc. and there would be just one thing to learn again. But we don't. Given that, I think that most people who don't like "option 1" above would prefer "option 2" to "option 3".
&gt; Review is now a proper supertype of Prism I don't get how this works. `Choice` was already a subclass of `Profunctor` so how did changing from `Profunctor` to `Choice` in the definition of `Review` make it a proper ~~subtype~~ supertype?
Is the fourth column supposed to be "Stateful w/ Result" instead of just "w/ Result". (The second column is also "w/ Result".)
&gt; I think the main change for userland software is that if RAM is persistent, then you can just persist your in memory data structures to "disk/memristor" for "free". And your inconsistent/crashed states, malware infections etc also get persisted for free. It may be an old joke, but I think we still need the ability to switch computers off and then on again to work around problems. Having the problems persist seems a bad thing to me. Of course a forced reset would be easy enough, but wouldn't that also mean losing any data structures that weren't saved to some kind of separate storage? Personally, I think the distinction between working memory and mass storage is needed for more reasons than just price/performance trade-offs. 
Yes. I think you're meant to read the columns as compounding each other in pairs, so it's 1. Combinators 2. Combinators w/ result 3. Stateful combinators 4. Stateful combinators w/ result
A `Prism` is an optic that works for only the `Choice` profunctors, but a `Review` claimed to work for *any* `Profunctor` and therefore types that the `Prism` couldn't support. You have to keep in mind that these constraints are in a negative position so you have to flip your notion of containment around.
&gt; The nature of haskell tends itself towards a prefix style of function declaration, and suddenly going from a prefix dominant coding style to one littered with infixes that you can't even name, but must simply attach to an abstract idea in your mind, is more than a little difficult at times. This is a matter of taste. I find the infix operators much easier to understand. When I see an operator like `+` or `&lt;&gt;` as the prefix of a larger operator, my mind immediately recognizes what is happening. It doesn't bother me that I don't have a name for it because I'm not thinking in terms of English. I wonder how most other people read it.
&gt; If you can enforce semantic versioning, or better yet, ditch semantic versions and **record the true, type-level dependency on interfaces**, our tools can make better choices. Type level dependencies/interfaces is something that I have been thinking about in the back of my head for a while. Here are my thoughts about what would be missing depending only on type-level interfaces; pardon the wall of text. If anyone has resources regarding my questions that I should investigate, I would be glad to see them! What happens when your interface is the same, but the behavior of said interface changes? For instance a library maintainer decides to make something very strict where it previously wasn't, and my program's behavior is influenced by this library's use of laziness. Or perhaps a library makes a tradeoff to use more memory in order to improve performance in other areas. If my code uses this library on a memory-constrained device, using more memory could cause issues. Or even that my program depends on a bug that is later fixed! Surely I would like to be told when upgrading a library that a bugfix changes something I was depending on. One might say "You shouldn't have depended on a bug in the first place!" and I would agree with you! But the ways bugs influence a program can be subtle, and if a program is "using" a bug that is fixed in a later version, the creator of said program might like to know about it when upgrading. Currently this work is delegated to careful reading of changelogs, but it might be worth it to at least try to point it out at the package management level. Outside of judicious use of semantic versioning I can't really think of a solution to this. Also: when something does change, but it's part of an `.Internal`, and the convention is that if you used `.Internal`, and something broke, that's your problem because the internals aren't necessarily stable. Should users that depend on this library but don't depend on `.Internal` be affected by this change, e.g. be told that the new library has incompatible changes with the last, even though the user doesn't depend on them? One might consider this a minor annoyance, but we can at least explore the possibility of avoiding it. A possible solution is to break `.Internal` into its own package that gets encapsulated, but that seems like a hassle for something that may be only a handful of or even just one file. Additionally it could mean e.g. more separate in the top-level package repostiroy space. We could ditch the `.Internal` convention, but it certainly seems to be useful at times and even necessary at other times. Could we break them into submodules, and then only depend on specific submodules of a library? It would solve the problem, but then we might have the same problem in submodules that we had in the first place! And then how far do we split modules into submodules? Is every file potentially its own submodule? I think probably the best solution would couple type-level-defined interfaces with separate semantic versioning. Without versioning it would be difficult to track behavior changes "behind the scenes" of the interface itself. Of course I would be glad to be wrong about that!
Korean tweens love this.
While the sound effects are superfluous (in my mind,) I think this style of presentation is really cool to show outermost vs. innermost execution. I think it could be used in a didactic setting nicely.
Not a generic answer to your needs, but for generating SQL statements, there's [haskelldb](https://hackage.haskell.org/package/haskelldb). Chris Done wrote a nice tutorial [here](http://chrisdone.com/posts/haskelldb-tutorial).
I feel the list handling is a bit misleading since it doesn't handle laziness! And it could. I want to access the first element of my mapped list without evaluating the tail.
The first step of map addOne [1,2,3,4,5] is addOne 1 : map addOne [2,3,4,5] Isn't that sufficiently lazy?
I'm seeing the same kind of problem on Chrome. If I modify the bubble mix to add `const x _ = x`, the sections stop working. In particular, I wanted to see the bubbles work when not all the sub-expressions need be evaluated.
I think it was just my sense of "accessibility". I wanted that bottom bubble to evaluate to a concrete `(:)` instead of deferring the pop until is evaluated the tail.
Ah, the bottom one's interesting because it can't evaluate to a `(:)` until the whole computation is complete, can it?
Hey everyone, I'm glad people are enjoying this! I've read over the criticisms and I'll definitely start addressing those going forward. I wanted to get the UI and the general concept working first before I started messing with the parser and the other guts of the app. When I've got a more fleshed out implementation (and some real lessons to go along with it) I'll make sure to post about it here!
Is there a way to have [rooms or namespaces](http://socket.io/docs/rooms-and-namespaces/) using your library? Using current broadcast function it will broadcast to everyone, while I just need to do it to a certain group. The only way I see right now it to have my own datastructure to hold sockets and use emitTo to each socket instead.
Please do, it's a really cool app so far!
My current setup is on a mac, I often find development on the native environment to be clumsy and inconvenient at least if not much more difficult than in a linux environment. Therefor I often use boot2docker and docker to get a working linux system that I can use for programming. It helps me to keep packages or projects in an isolated environment. 
so far I have worked quite well with a docker+python setup on my mac. I have connected to the machines with SSH and run my vim session in the terminal. Usually this works good and painlessly so I suspect that ghc-mod for vim will not be an issue. Now the open question is whether there is a distribution or docker image that haskell programmers prefer (Haskell support, packaging, etc.).
Neat. I'll send this to all my teachers. 
 1:[] ---&gt; [1] this should not be shown as a reduction step, it is just syntactic sugar.
Maybe you don't need a DSL for SQL? So I've been super into Clojure before, and the big thing was [korma](http://www.sqlkorma.com), a DSL that generated SQL. But then [yesql](https://github.com/krisajenkins/yesql) happened: &gt; Clojure is a great language for writing DSLs, but we don't need a new one. SQL is already a mature DSL. And s-expressions are great, but here they're not adding anything. This is parens-for-parens sake. (Don't agree? Wait until this extra syntax layer breaks down and you start wrestling with a (raw-sql) function.) Of course, Haskell can add type safety... but I don't think I've seen typing issues inside SQL
It crashed Chrome on my Android tablet as well
http://i.imgur.com/GB57G8v.png To be able to pop them in a lazy order, you should be able to pop the outer bubble at this point, to get http://i.imgur.com/HZ9HR5Y.png , but you can't do that.
I didn't write it, it's one of the standard functions now. Maybe it wasn't before.
I think it was there originally for me, but when I edited any of the bubbles they stopped working.
Sure it can. reverseCons doesn't need to match on its arguments at all. Any expression with reverseCons applied to two subexpressions should be reducible. From: (reverseCons (reverseCons (reverseCons (reverseCons (reverseCons [] 1) 2) 3) 4) 5) it should be possible to click the outermost reverseCons to get: 5 : (reverseCons (reverseCons (reverseCons (reverseCons [] 1) 2) 3) 4) (which is what a lazy evaluator would do)
 zipWith :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c] zipWith _ [] _ = [] zipWith _ _ [] = [] zipWith f (x:xs) (y:ys) = f x y : zipWith f xs ys tail :: [a] -&gt; [a] tail (x:xs) = xs at :: Int -&gt; [a] -&gt; a at 0 (x:_) = x at n (x:xs) = at (n - 1) xs fibs :: [Int] fibs = 0 : 1 : zipWith (+) fibs (tail fibs) And run something like `(at 7 fibs)` *I know that `tail` and `at` can fail but it doesn't matter for this scenario*
Could this be why it's slower than native? https://github.com/snoyberg/conduit/commit/307dab9461dd3c30b577bb094e499e0fba493c78#commitcomment-7497489
As one of the criticizers I want to be clear it's a *really* neat program! With a little bit more work I would direct something like ~10% of SO to it and have that be it. Understanding lazy evaluation is a big deal and I love this tool for that reason. So, criticism is only there to improve it. Thanks so much for putting it together!
Damn, totally forgot about the open source conference @ Kyoto this year! Would have been nice to meet you guys. I am somewhat involved in a new meetup for functional programming in Kansai by the way -- first meeting in two weeks: http://kansaifp.doorkeeper.jp/events/13769. We have all the talks arranged for that meeting but if you would be interested in presenting something for 第弍回 (in Japanese) that would be great!
it will be great to have smth like that for vim
Exciting! I've encountered what appears to be a breaking change though: The following code previously didn't generate a lens for `fromTimer`, but now it does (well, attempts to do so) – I'm not sure if this is intentional: https://gist.github.com/iand675/ae489ed97cba90ade42b 
What is missing is if they provide a "mere" proof (i.e. a proof that some such must exist) or a "constructive" proof (i.e. literally a formula for the production of such).
This was an unintentional discrepancy. I'll put out a fix. In the meantime your code should work with `makeLensesWith camelCaseFields`.
I'm super glad to hear that—thanks! :)
Are you feeling better? I'm still very interested in more explanation.
Use rem instead of mod?
I love the sound...
Try using an array for your lookup table. The time drops dramatically for this code chainLookup :: Int -&gt; Int chainLookup x | x &lt;= 567 = lookuptable ! x | otherwise = lookuptable ! digitSquareSum x lookuptable :: Array Int Int lookuptable = listArray (1,567) [chain x | x &lt;- [1..567]]
The difference between quotRem and divMod is only a couple of a percent in a situation like this.
Thanks! Using a Map improved the time to ~4.5 seconds and using an Array got it down to ~4 seconds on my laptop.
Thanks. This is only my second Haskell program so I didn't know about Maps or Arrays.
True, but maybe since this is a teaching aid, seeing that transformation explicitly will help someone? It is technically wrong though.
Parts of the runtime which cannot allocate (i.e. the garbage collector, because it would need to clean up after itself!) cannot be written in Haskell. Other things, like Mio -- the new IO manager -- are actually already written in Haskell (previously C) to great performance benefit. There's actually another option though; most of the low level parts (like MVars, and even STM iirc) are implemented in Cmm (C-minus-minus), the usually final intermediate representation for GHCs Haskell. The advantage of doing it this way is that calling from Haskell to C and back again is a *very expensive exercise*. Cmm does not have this problem; it's compatible with Haskells calling convention. It's a bit close to the machine though. It'd be really hard to write a garbage collector as complex as GHCs in it! Although if somebody did happen to, with a little extra work, you'd maybe find we could throw away all our C toolchain! GHC would be able to compile the `.cmm` files and AFAIK (and I could be very wrong) we wouldn't need `gcc` to build the runtime at all anymore. Only a linker.
Ah, fair enough -- wasn't sure where you are based! I will be in Yokohama for CEDEC just before then, but back in Kansai by the weekend. My Japanese twitter account is @tataminomusi if you want to talk about 関西関数型道場. I understand it's far to come, so just let me know if you are in the area. Enjoy ICFP!
I'm sure the OS will take care of wiping the scratch memory of a program after it closes. That is, the working memory/mass storage distinction is an OS level concern.
I'm excited to see if we can get it to work with nix too
HOL Light est un seul et même langage. :)
While I understand this is was built as a bit of fun, I am completely seriously wondering if this style of presentation would be good for debugging in the same way debugging with breakpoints is used in imperative programming.
So you have a distinction between scratch memory and long-term storage, you need to ensure their address spaces don't interfere with each other, and your program needs to decide what data goes in which - so persistence isn't free. This is essentially the situation we already have. We need to do special things to persist data because hard disks and SSDs aren't part of the same address space and are organized in a way more appropriate to long-term storage rather than main memory (ie using file systems). Each file is, in a sense, an independent virtual address space - a sequence of storage bytes, just accessed more indirectly than main memory. Actually, in most modern operating systems you can even use memory-mapped file APIs, to temporarily map parts of a file into the main address space. You can access files by simple pointer dereferences once you set that up, though you still need to make allowances for the separate address space issue - you still can't store those pointers directly in that file, you probably store offsets and calculate pointers into the mapped address range using those. Just as we can make hard disk files part of the main address space, we can (and usually do) extend the main memory address space onto hard disks/SSDs already using virtual memory, and (not quite for free) this can even be used to make the main memory address space persistent (sleep/hibernate modes). In principle, on a 64-bit machine we can already have a full 64-bit address space - 16 exbibytes, or 16,777,216 tebibytes, though in practice e.g. the Windows 64-bit user-mode address space is currently limited to [8 "terabytes" (probably tebibytes)](http://msdn.microsoft.com/en-us/library/windows/desktop/aa384271%28v=vs.85%29.aspx). So &gt;1TB heaps (including virtual memory) are already plausible and feasible and persistable, but that doesn't mean we count main memory as long-term bulk storage, and that doesn't mean we count persistence of data structures as for free. Files are still separate from the main memory address space. File systems didn't just arise as a consequence of disk storage - they were invented because we needed a way to organize data and to keep applications playing nice with each other WRT storage resources. In addition to the fact that we *want* each file to be as independent as possible in terms of it's allocated storage (that's part of why file systems were invented), there will always be reasons to install additional storage temporarily - the equivalent of external hard drives, USB flash sticks etc. There will therefore always be a need to ensure your data ends up in the correct storage space - that your offline backups go on your external storage that's offline most of the time whereas your online data structures remain in your main address space. Personally, if persistent storage becomes as fast as main memory, I think we'll have all the same abstractions to deal with. The practical difference will essentially be that sleep and hibernate modes become the same thing - no need to power your already-persistent main memory for sleep mode and no-need to force the data only another device for persistence so there's no point having both. And of course most tablets and phones are already almost permanently on in the sleep-mode sense - that doesn't mean we don't use file systems to keep the data we really want to be persistent. 
I always hear people complaining that ghc-mod is rather slow. Is this true or is this negligable?
Sure,that sounds like an excellent idea! However, I am not sure how well it would work once you get into more complex code. Lenses, as well as various monads, especially stuff like IO or Conduit, would probably require a different interface to let you effectively debug.
It's a common debate. Pros: never accidentally putting something that returns a meaningful value in an implicit void. Cons: having to write void everywhere.
Oops. Here's hoping I don't start it again. That was about what I figured, but I wondered if there was some historical reason, or something I had missed...
I built (with `-O`, which is what you said you used) and ran your program, and it took 30.727s to run. I built (with `-O2 -fllvm`) and ran the below version, and it took only 0.244s to run. import Control.Arrow import Control.Monad import Data.Tuple import Data.Vector.Unboxed (Vector, (!)) import qualified Data.Vector.Unboxed as Vector reversedDigits :: Int -&gt; Vector Int reversedDigits = Vector.unfoldr (fmap next) . Just . abs where next x | x &lt; 10 = (x, Nothing) | otherwise = second Just . swap $ x `quotRem` 10 digitSquareSum :: Int -&gt; Int digitSquareSum = Vector.sum . Vector.map (join (*)) . reversedDigits chain :: Int -&gt; Int chain 89 = 89 chain 1 = 1 chain x = chain $ digitSquareSum x chainLookup :: Int -&gt; Int chainLookup x | x &lt;= 567 = lookuptable ! (x - 1) | otherwise = lookuptable ! (digitSquareSum x - 1) where lookuptable = Vector.map chain $ Vector.enumFromN 1 567 main :: IO () main = print . Vector.length . Vector.filter (\x -&gt; chainLookup x == 89) $ Vector.enumFromN 1 9999999 
I'm in the camp that says it should not be polymorphic in this way. If it implicitly ignores the result, I am much more likely to introduce bugs.
`when` and `unless` are intended to be used for *monadic effects*. Using them in, for instance, `MonadPlus`, `MonadError` and so on is meaningful.
The question is not about how one would use `when`, but why the computation to execute is of type `m ()`. The argument is it could just as well be `m a`, and `when`/`unless` could `fmap` that `a` to `()` later.
You might find this video interesting/informative: https://www.youtube.com/watch?v=4q3v2p2-Cmc 
One can get `take 3 [1,2,3,4,5]` to evaluate to `[1,2,3,4,5]` because it allows a take-bubble to be popped without popping the Integer-bubble first. Since it has to check whether that number is 0 this should not be possible. This behaviour shall be henceforth known as Too-Lazy-Evaluation. ;) Otherwise, I have to search for a bubblesort implementation and then pop the bubbliest bubblesort ever. :)
This thing is great! It is really quite cool. Here are some suggestions: 1. Like said before, I like the idea of making this a debugger! You could either make it a local app, or contact FP complete. I always thought a debugger like this would be cool. 2. A cool feature would be to show the order that a haskell compiler or interpreter usually does it. "Auto run mode." 3. Also, a slight bug: If you define repeat as "repeat x = x : (repeat x)" and then do "take 10 (repeat 4)", you can't get it to finish evaluating.
If you want high-efficiency data structure libraries you should use the `vector` (basically like `Array`s, but better), `containers`, and `unordered-containers`libraries.
Depends on the size of the project, it is making a round trip to GHC every time you recheck. If I load the Idris codebase it takes like 3 minutes to load. For single modules it's nearly instantaneous. 
pretty good summary, though I disagree on two points a) the overhead for an unsafe c ffi call is &lt; 5 nanoseconds, so unless you're calling out to C from a tight inner loop to just extract a field from a packed struct (which is like 2-3 instructions), the overhead quickly becomes negligible. That said, the real power of writing primops in CMM is that interprocedure optimization is possible! b) its actually possible to write C codes that use the GHC ABI! https://bitbucket.org/carter/who-ya-gonna-call-talk-may-2013-ny-haskell I gave a talk with example code doing this last summer. Just requires mangling the LLVM ir that CLANG emits! c) C has way better data abstraction facilities than CMM currently has. 
this touches on a great point (that i'm trying to write a few blogposts about): theres a LOT of array computations that crucially depend on sharing, and thus MUST be written in a mostly imperative fashion (at least internally)
the relative links to builtin types like Double seem to be busted with those haddock flags, later this week or next lets figure out how to fix that! :) 
See http://neilmitchell.blogspot.co.uk/2008/12/mapm-mapm-and-monadic-statements.html - you can have much the same debate about &gt;&gt;. It is slightly inconsistent that when and &gt;&gt; picked different sides, but there are reasonable arguments on both sides. 
The way that people deal with this is to have the whole script define a single function which is only executed as the last line of the script. If curl dies in the middle the function is never executed.
You should see which function it is. https://www.haskell.org/ghc/docs/7.6.3/html/users_guide/profiling.html
Hm, true. I never considered that &gt;&gt; could be m () -&gt; m a -&gt; m a
I'm already using Postgresql-Simple to communicate with the database without any problems. I plan on switching to the higher level esqueleto/persistent models once I get more familiar with the low level. That being said, my specific problem is that right now I have two pieces of code that specify the column names/types for a given table. The first is the haskell ADT for that table, and the second is the python class for that table. How would I go about writing each table definition once in any form you like, such that program 1 would map that representation to the haskell ADT, and program 2 would map that representation to a python class. I'm starting to think that the best method may just be to write some haskell-cpython that spawns up a python interpreter, imports the class, reads the columns and outputs a text file properly formatted for persistent.
Yes, that was an issue. I ended up fixing it with: https://github.com/snoyberg/conduit/commit/ca6a287e478e639391115778b59c02e26e327ba2
Don't be afraid to look up the implementations of the library functions you use from prelude; they're all pretty small. And make sure you understand what `[a]` really is (an ADT isomorphic to `data List a = Cons a (List a) | Empty`); then it'll be easy for you to see when you might have the wrong data structure.
So I spent the last days writing a Haskell binding for [Senna](http://ml.nec-labs.com/senna/) (Ronan Collobert et al.), a fast Natural Language Processing toolkit written in C. Senna is quite neat: It's written entirely in 3000 lines of code, has no external dependencies and requires only 200 MB RAM (way less than all Java NLP toolkits, which can't run without setting heap size to several gigabytes) and yet it's fast and performs well. The only downside is that Senna is somewhat only *semi-free* - it's released under a non-commercial license. But I think having this is still better than nothing (i.e. the current state of NLP in Haskell?) ... So here it is - it currently supports * Part of Speech tagging * Chunking * Name Entity Recognition * Semantic Role Labeling That's 4/5 of Senna's functionality. Only Syntactic Parsing isn't supported yet. But this is still a work in progress, the ink hasn't dried yet. So please feel free to offer suggestions. All input is welcome!
:) 
When I tried profiling it just said 100% of the time was spent in main. Maybe I was doing something wrong.
Not exactly the same, but I made a one-off diagram for this Fibonacci example. It also shows how memory is shared, which is perhaps the most important part of the function. You can see it on my blog: http://jelv.is/blog/Lazy-Dynamic-Programming/
You need to tell GHC what to measure. You can have it measure all your functions automatically with `-fprof-auto` . The term to look for is "cost center".
 middleIndex as = length as `div` 2 This may not be the best way to write merge sort however.
You want "length ls `div` 2". The "/" operator is only defined for fractionals, and yes length returns Int. fromIntegral is the converter you want. . You would 
Using Hoogle to look at the types and check out the definitions on Hackage: floor :: (RealFrac a, Integral b) =&gt; a -&gt; b length :: [a] -&gt; Int (/) :: Fractional a =&gt; a -&gt; a -&gt; a So, `(/)` requires a `Fractional` on both sides. `Int` is not fractional. Looking at [instances of Fractional](http://hackage.haskell.org/package/base-4.7.0.1/docs/Prelude.html#t:Fractional) we see: Integral a =&gt; Fractional (Ratio a) So, in order to get a `Fractional` for `(/)` to use, you'll need to convert each of the `Int` into `Ratio` with this function: (%) :: Integral a =&gt; a -&gt; a -&gt; Ratio a I was able to get your function to compile if I did this: middleIndex :: (Ord a) =&gt; [a] -&gt; Int middleIndex as = floor ( (length as % 1) / 2) (which leaves the question of why I didn't need to do (2 % 1) as well)
Woot, I'm taking an NLP course semester. I'll be sure to try this out!
Thanks for the detailed response! But what I had in mind was only polymorphism in the first argument: when' :: Monad m =&gt; Bool -&gt; m a -&gt; m () when' True mx = mx &gt;&gt; return () when' False _ = return ()
Ahh, well then: `when test (mx &gt;&gt; return ())` does that, so it's unnecessary to define `when'` really.
Fail is a really dirty trick that should be avoided when possible.
I simplified the example a little: https://gist.github.com/glaebhoerl/f606757a91c66bccc0ea/096a45820cec6feb4afc3049db0ad74d8ebd6b85
Don't forget to join the [Haskell NLP list](http://projects.haskell.org/cgi-bin/mailman/listinfo/nlp)! If this keeps up, we'll need more than a mailing list.
Thanks for writing this binding! You may find Haskell users (for example in academic settings) who don't mind the non-free nature of Senna. About the current state of Haskell NLP, I kind of feel like the most helpful thing for Haskell NLP right now would be to have somebody break down common NLP tasks, and wade through [the packages we currently have](http://hackage.haskell.org/packages/#cat:Natural%20Language%20Processing), basically saying to what extent our needs our met (for example, we some pos taggers actually), and where we most want help.
Right, I just wondered if there were reasons for `when` _not_ to take care of that `&gt;&gt; return ()` itself. It just means writing less `return ()`s in my code, which is nice for `do` blocks.
what would be the "haskell" way to do this? I've already written the merge merge :: (Ord a) =&gt; [a] -&gt; [a] -&gt; [a] -&gt; [a] merge [] [] acc = acc merge [x] [] acc = acc ++ [x] merge [] [y] acc = acc ++ [y] merge xs [] acc = acc ++ xs merge [] ys acc = acc ++ ys merge xs ys acc | (head xs) &lt;= (head ys) = merge (tail xs) ys (acc ++ [(head xs)]) | (head xs) &gt; (head ys) = merge xs (tail ys) (acc ++ [(head ys)]) my current way to do the merge sort is to split the array in half and run merge a mergesorted version of both. splitting a list is turning out to be really wierd in haskell. Is there a more idiomatic approach I should be taking? 
I think most of the time people aren't writing done that returns a value that they want to ignore in `when`s. But that's a guess.
Well, it could have plenty of other values. But I'm not sure there would be much point in having that kind of `when`, as I said.
b) is cool.
I mean that if the result is silently ignored, I might be unknowingly ignoring an important result! What if, for example, it returns a `Bool` indicating success or failure?
&gt; I was able to get your function to compile if I did this That will compile, but as others have pointed out using `` `div` `` is probably the easier/better solution. &gt; (which leaves the question of why I didn't need to do (2 % 1) as well) Numeric literals like "2" can have their type automatically inferred by context. `(/)` expects a `Fractional a`, so that is what `2` is inferred to. `length as`, on the other hand, returns an `Int`, plain and simple, and that's not valid as a parameter to `(/)`. Note that this isn't automatic *coercion*; the literal `2` doesn't represent an `Int` that gets converted to a `Fractional`. Rather, what the literal `2` represents depends on context.
Here's some int-ly typed maths operations. The one you're looking for is div. Prelude&gt; :t div div :: Integral a =&gt; a -&gt; a -&gt; a Prelude&gt; :t quot quot :: Integral a =&gt; a -&gt; a -&gt; a Prelude&gt; :t rem rem :: Integral a =&gt; a -&gt; a -&gt; a Prelude&gt; :t mod mod :: Integral a =&gt; a -&gt; a -&gt; a
Though given integer division is defined as rounding down on most processes and languages I know about that have integer division, converting to a float and back again is probably never what you want here.
The thing I'm working on right now is 70 or so modules and 12kloc. I don't really notice hiccups from ghc-mod. I haven't tried using it with GHC itself recently, but the ghc-mod developers have done tremendous work at speeding it up and lowering memory use.
There are a couple of more idiomatic implementations here: http://stackoverflow.com/questions/1215432/merge-sort-in-haskell
Random access is O(n) for list, O(log n) for Map and O(1) for array. In case you have Int as key then you can use IntMap.
There's a clever and well-known way to split a list into two equal length sublists: split :: [a] -&gt; ([a], [a]) split = foldr (\x (l,r) -&gt; (x:r,l)) ([],[]) (The lengths differ by one if the original list has an odd length.) It builds up the two sublists by prepending elements first to one, then to the other. It helps to stare at it for a while and do some small examples by hand. :-) Also, I think your merge could be simplified somewhat to: merge :: Ord a =&gt; [a] -&gt; [a] -&gt; [a] merge [] ys = ys merge xs [] = xs merge (x:xs) (y:ys) | x &lt; y = x : merge xs (y:ys) | otherwise = y : merge (x:xs) ys 
Don't forget http://languagengine.co. It's being built in Haskell, so there may be bindings for it later.
To get rid of ()s: when test (void mx) because void :: m a -&gt; m ()
Thank you for this info!
Oh yeah, I just meant in notation like when thing $ do ... ... return () though I guess it could just as easily become when thing $ void $ do ...
To augment the other excellent responses, I'll point out two variations of merge sort that have proven to be useful in a functional setting. Both are available in the source code of `Data.List.sort` in the [base](http://hackage.haskell.org/package/base) library, with some references for more information. One approach, written by Ian Lynagh and now commented out in `Data.List.sort`, splits the original list into sublists of length 2, then merges them pairwise repeatedly until only a single list remains. The other approach, written by Thomas Nordin based on ideas of Lennart Augustsson (/u/augustss) and Richard O'Keefe, is the current implementation of `Data.List.sort`. It is an optimization of Ian's algorithm where the original list is broken into maximal monotonic sublists instead of sublists of length 2.
anyone have something like this but for vim?
Nice simple summary of how to install and use emacs-based haskell tools. Personally, I've always used just `haskell-mode` and I'm happy with that; the power of emacs itself adds a huge amount. But with this simple summary, who knows, maybe I'll try some of the fancier stuff. One small thing I disagree with: for someone who really wants to learn how to use emacs I do *not* recommend using `cua-mode`. Those control-keys are a fundamental part of the emacs UI, so masking them with something else will just get in the way of learning emacs, not make the transition smoother. And you really don't need those "cua" bindings to interact smoothly with the clipboard. It may be a little awkward at first for some people, but it's worth it to invest the small amount of extra effort it takes to get over that.
Just curious - why did this post get so many downvotes? I found the post and many of the responses interesting.
I agree that learning the usual Emacs key bindings is a much better idea that jumping into `cua-mode` upfront. I have reworded that section a bit. Thanks for the comments!
Less then 24h ago I [replied to a similar question](http://www.reddit.com/r/haskell/comments/2ebcim/ghc_companymode/cjyaht0). I replied: "there is something similar to Vim for Emacs", pointing at Emacs' Evil package. To explain myself a bit further: I was using a highly configured version of Vim, that had loads of plugins loaded and several processes attached to it (like this guide is describing for Emacs). But it became slow; slow to start, slow to do certain things, and slow at random moments. Now I use the latest version of vim-for-emacs, better known as Evil, and I'm not looking back. I still use Vim occasionally, and keep a config for it in my dotfiles, but it is not my main editor anymore.
If I'm not mistaken, `Char` is also an instance if `Ord`. I didn't check to make sure that the function is still valid in that case, but theoretically you can use `&lt;` / `&gt;` there too. EDIT: I opened up my laptop to write a quick test on what I said, and it holds true. convertToInt :: Char -&gt; Maybe Int convertToInt c | 'A' &lt;= c &amp;&amp; c &lt;= 'Z' = Just (ord c - ord 'A') | 'a' &lt;= c &amp;&amp; c &lt;= 'z' = Just (ord c - ord 'a' + 26) | '0' &lt;= c &amp;&amp; c &lt;= '9' = Just (ord c - ord '0' + 52) | c == '+' = Just 62 | c == '/' = Just 63 | otherwise = Nothing Seems to be equivalent to what you have above, but should have some performance gains seeing as `elem` doesn't have to search through a whole list every time you call it.
Note: this requires ghc-mod 5.0.1.1, which will be on hackage shortly (as soon as we get maintainer permissions sorted out), or install it from head at https://github.com/kazu-yamamoto/ghc-mod
Yeah those 'trolls' they can deal with. When people go to freenode #haskell and ask technical questions you get a bunch of pretentious idiots telling them to go learn category theory ('because you can't know haskell without knowing category theory'), telling them they are stupid because they don't know something or calling them trolls when they don't agree with the channel idiots (typically when the people asking questions are a bit more intelligent than the channel dwellers, and ask questions that make them feel insecure).
Do you have a specific example of this? I find it incredibly hard to believe simply because it's so distant from my experience of the channel as a long-time lurker.
Now that a very nice looking Haskell image processing library, [friday](https://hackage.haskell.org/package/friday), was just released, it became clear from the [ensuing Reddit thread](http://www.reddit.com/r/haskell/comments/2e6cof/a_fast_generic_and_typesafe_image_processing/) that in fact another very good advanced image processing library in Haskell already existed that not too many people were aware of. The package comment on Hackage actually advertises the library more as a [repa](http://hackage.haskell.org/package/repa) clone with a more convenient interface for image processing and other similar applications. Which is also true. But from that package comment, you wouldn't guess how much actual image processing is in this library. Part of the problem is a language problem. The main documentation is a PDF in Russian, which I was able to ram through Google Translate only after a bit of hacking. (Here are the very rough results, as [text](http://lpaste.net/109932) and as [html](http://lpaste.net/109934). Sorry, no graphics; follow along in the Russian.) But it seems to me that this shouldn't be too much of a problem, since some of the top members of the Haskell community happen to be native Russian speakers. I hope that this library gets the publicity it deserves, and that as a result we see some fruitful collaboration both between yarr and repa, and between yarr and friday.
For example often people come in and ask about monads and functions in haskell and being told they should go learn category theory. I've also been labelled a troll for saying you don't need to know category theory to learn haskell.. Most people that say you need to know category theory to know haskell are pretentious idiots that don't know either. Oh and when people new to haskell come in, ask questions and get called stupid, I assume malice.
No, why would I keep logs of that... Anyway I'm not trying to prove anything to you, that's just my experience.
No particular reason! Some people just keep logs around. I know I used to. I don't mean you have to prove anything to me, I just wanted to know so that I could know more tangibly how to help improving the community. :)
You can't have "something like this" for vim. The reason many amazing language modes exist for emacs is because emacs was built from the ground to be extensible. While vim has ways to write plugins it is much more cumbersome and limiting than in emacs. So most people just go for a low hanging fruit and call it a day. It is time for you to realize that programmers do not edit text, they write code (which is not text). And while i have no doubt that vim is the greatest TEXT editor, i need a proper coding environment. And that's what emacs is. 
Is a literate Haskell source file text, or is it code?
While not containing all the Emacs features, this goes a long way: https://github.com/begriffs/haskell-vim-now &lt;/selfpromotion&gt;
Thanks, I've done some printing and will now do some reading. I was at ICFP 2007, but alas, back then all that material went straight over my head.
This is purely a personal rationalization, but... In pure code, we're used to the idea that if you ignore a result (or part of one) from a function, the computations that were specified purely to derive that result won't happen due to laziness. Functions often give us more than we want, and throwing unwanted stuff away is normal. In Monads, though, information we aren't directly interested in right now tends to be hidden away inside the monad - on the outside, we don't see that information that's hidden inside the monad unless we deliberately ask for it. And the way we deliberately ask for it is using an action that yields that data. Of course a monadic action can still return more than we want, and laziness still applies, but there's a certain subjective difference in expectations. So to me, it would be slightly odd to have normal functional code need explicit discarding of unwanted results, but to have monadic code implicitly discarding `return` values. Kind of backwards. 
Nice. Though, I'm just wondering why [`M-x haskell-compile`](http://haskell.github.io/haskell-mode/manual/latest/#Compilation) isn't mentioned at all... :-)
Oh that's great! I didn't realize anyone was doing speech in Haskell
Update: 5.0.1.1 is now on hackage
Anything that has a rigid structure that you would like to navigate is not text anymore. Injecting text islands (no matter how big) into code does not change anything. 
I just haven't used `haskell-compile` myself. What are the differences between loading a file using `C-c C-l` in `haskell-mode`, or calling build in Cabal? In any case, pull requests with more information is welcome, taking care of not flooding beginners with too much information (I think `ghc-mod` already shows much of this information).
I'm not sure, I use Emacs and CUA mode, and I find it a lot easier for all of my programs to use the same bindings, so I can rely on muscle memory rather than thinking about each program's individual bindings. I disagree that the control keys being a are a fundamental part of the UI, though, or at least, they're important, but that's no reason why they can't be remapped. I use Emacs just fine with them where I want. The best thing about Emacs is that you can customise it to be exactly how you want, and I think it's a mistake to tell people not to use them because you don't think they're very good.
It is your characterization that code and text are entirely exclusive that I wish to dispute, and possibly the notion that a code editor would somehow completely obviate the need for text editing.
Obviate? Care to show me a single code centric development environment that somehow does not support robust text editing? :)) 
I have used yarr for simulating the solar system and gotten reasonable performance: http://idontgetoutmuch.wordpress.com/2013/08/06/planetary-simulation-with-excursions-in-symplectic-manifolds-6/. One day I will benchmark it against Fortran.
You misunderstand. If you are of the opinion that a good development environment *subsumes* text editing, your original comment is a poor attempt at expressing it. edit: consider that you’ve said &gt; […] programmers do not edit text […] which, even in context, can easily be understood not the way you intended
[I'm not particularly knowledgable about numerical stability, so take my opinion with a grain of salt] I think the best thing you could do would be for your `zscore` implementation to use the statistics package's implementations of `mean` and `stdDev`, because those functions are potentially tricky to get correct across all data sets. In contrast, `zs` seems relatively straightforward, and I'm not sure what a more robust version would look like.
You're right. I'm planing to separate I/O from the main package in friday-0.2 : https://github.com/RaphaelJ/friday/issues/5
not sure why I was downvoted. I know this isn't the best solution, but I was trying to show a beginner how they might work-out the types to get it to compile.
I'm surprised anyone can understand what i said in any other way :)) You really think there are developers who think good text editing capabilities and good development environment are mutually exclusive? Of course both of these aspects are always present. The difference is which one is given priority. In vim text editing trumps anything else. 
ya, that sounds reasonable. I'm mostly worried about underflow/overflow in the square &amp; square root functions, but don't really know what I would do about it.
I don't know why this makes me so happy.
`M-x haskell-compile` is much simpler/lightweight. It's for the cases where you don't want/need a fully-fledged interactive GHC session attached, and want to quickly load up a file, hit `M-x haskell-compile` and have it compiled right away (no questions asked) with error highlight/navigation. This should let old-school Emacs users feel at home, as it's simply `compilation-mode` customized for GHC's compiler output. Moreover, with some more complex packages (like e.g. GHC), you may not even be able to use `C-c C-l` and in those cases `M-x haskell-compile` is better than nothing.
Please, don't do this. Introducing `Rational` here is a red herring. All you need is `div` instead of `/`. 
Just get Evil (vim mode) plugin for Emacs. (I'm a former vim user too, but changed to Emacs when I started learning Haskell).
Thanks for putting this together! I have just one comment: You may want to mention `M-x customize`. You can set several options mentioned in the guide in **Programming** -&gt; **Languages** -&gt; **Haskell** (Indentation mode, hasktags on save, stylish-haskell on save). It can also promote discoverability to a user that is new to emacs. :) 
I wonder if we should have wiki pages with human descriptions like this and shortlinks to them in the error messages. This would allow more detail for learning without adding a lot of length to the output. Could also have a flag to disable the behavior. It's hard to have sufficient descriptions in introductory tutorials of all classes of error message because you don't know in what order a new person will encounter issues.
I just realized that zscore computes the mean twice. will that get magically memoized somehow? what would the best way to profile and find out?
Step 1: don't 
Damn, how have I never heard of Rosalind before? It looks like a fantastic alternative to Project Euler; I can't wait to give it a go!
Double is a standard double-word, so it'll behave the same as in every other language, overflows included. If you need really high precision, you can use the Rational type instead, which is composed of two (unbounded) Integers. You can convert Double to Rational via approxRational and then use that in your functions. sqrt will not work, but here's an arbitrary-precision algorithm for computing square roots: http://www.haskell.org/pipermail/haskell-cafe/2007-January/021502.html 
In general, function calls don't get memoized away (cf. the naive definition of the Fibonacci function fib n = fib (n-1) + fib (n-2), which has exponential running time due to unecessary re-computations of the same number). GHC MIGHT do it, but someone more knowledgeable than me would need to answer that. You can look for yourself, though, with the Debug.Trace module: -- trace :: String -&gt; a -&gt; a avg :: [Double] -&gt; Double avg p = trace "avg computed!" $ (sum p) / (fromIntegral $ length p) If "avg computed!" appears twice as often as it should in the console, the call is not getting memoized. In that case, you can just pass the mean as an extra parameter to stddev: stddev :: Double -&gt; [Double] -&gt; Double stddev av p = ...
How's that going by-the-way? I feel like any technology promised in ten years is not serious. So much can happen to make it irrelevant and the timeframe really admits they have no clue how to manufacture these memristors. Not to get all flaming but HP really does suck. They're no longer a leader in research and certainly can't manufacture ICs or even bolt their own servers together. Memristors may be real but someone else will have to figure out how to make them.
&gt; It is time for you to realize that programmers do not edit text, they write code I'd go ever further than that and say that programmers solve problems. People were programming when *punch cards* were the "editor". A development environment or even a text editor makes things easier, but solving problems is the part that really defines what programming is about. 
Add an SCC annotation to `avg` like this: avg p = {-# SCC avg #-} (sum p) / (fromIntegral $ length p) and build with `-prof` and run with `-RTS +p` and look at the entries column. Try it with and without optimization. BTW, if/when GHC computes the average only once, that's not really memoization but rather common subexpression elimination.
This seems not technically difficult: When a library's dependency is updated, the maintainer gets automatically notified, along with release notes that detail api breaking changes (probably written by the dependency author). They are also notified whether their library already builds with the new version, and an option to simply bump the dependency upper version bound if it does. Otherwise they can use the provided info to upgrade. The goal of all this would be to make it as easy as possible for a maintainer to keep their dependencies up to date.
Would you explain the following command? (define-key haskell-cabal-mode-map (kbd "C-c C-k") 'haskell-interactive-ode-clear) Nor `haskell-interactive-mode-clear` is available in my emacs, nor the one used above, should it be reset-errors instead? or something else?
For those lime me who don't know, what is Metasepi?
why not? 
Customization is great but it's still accurate that CUA mode sometimes creates problems for users. Like in all things, the further away from the status quo you diverge, the less people will be motivated to help you when your choices causes you trouble.
Ada Lovelace would like a word with you.
Haskell and LaTeX are two things I feel emacs does vastly, vastly better than vim at. Like, zero comparison better. I use both a lot, and that alone is kinda making me wanna switch. I'll have to tinker with it a bit more, though. 
Ah, sounds interesting
Yeah, thanks. Your post was useful to me for the walkthrough.
I actually tried a delay in the parent thread, and got the same result. I had a 10s delay immediately after the forkIO, but that changed nothing. Even if you are right, though, shouldn't the server be dead from the threadKill before the second attempt?
getRequest is throwing an exception, so perhaps the killThread is not being executed. 
Okay I give up. Time to switch. Resistance is futile. 
Yeah, that seems to be why it wasn't getting killed. Doesn't explain why a delay doesn't fix the problem though. My new code: module Test where import Web.Scotty import Control.Concurrent import Control.Exception import Data.Typeable import Network.HTTP import Network.HTTP.Types.Status web_service = scotty 5000 $ do get "/" $ do status ok200 catcher (SomeException e) = print $ typeOf e main = do tid &lt;- forkIO web_service threadDelay 1000000 (print =&lt;&lt; getResponseCode =&lt;&lt; (simpleHTTP $ getRequest "http://localhost:5000/")) `catch` catcher killThread tid Result is: λ&gt; main Setting phasers to stun... (port 5000) (ctrl-c to quit) IOException every time. I tried with longer delays too. I guess I have to figure out how to make the chile tell the parent when it's done.
For clarity because Reddit eats the accents, that's: length ls `div` 2 and for reference, the types relevant here are: div :: Integral a =&gt; a -&gt; a -&gt; a (/) :: Fractional a =&gt; a -&gt; a -&gt; a Integer doesn't implement Fractional (because it doesn't make sense to) so you can't use `(/)` but you can use `div` which performs integral division (see also `divMod` and `quotRem` for division+modulus and quotient+remainder [these are different for negative numbers])
The last update to Yarr was over a year ago, and Repa seems to have gotten nothing but base version bumps or so for the last two years. Is anybody actively using / developing these libraries?
Hmm, does using that split preserve stability? I have a feeling it doesn't (and that's one of the nicer features of margesort). It's quite a neat way of doing it though, I'm surprised I haven't seen it before.
Well, you could compute both in one go: data Stats d = S { getMean :: d, getVariance :: d } deriving (Show) stats :: (Foldable t, Fractional d) =&gt; t d -&gt; Stats d stats xs = let (mean, m2, l) = foldl' goStats (0,0,0) xs var = if l &lt;= 1 then 0 else m2/(fromIntegral $ l - 1) in S mean var where goStats (mean, m2, n) x = (mean', m2', n') where mean' = mean + delta / (fromIntegral n') m2' = m2 + delta^2 delta = x - mean n' = n + 1 But no, they won't get memoized magically. 
Another alternative is to repeat the requests every second or so until it hits.
Every post in this thread hierarchy has 1 point because nobody has any idea what you /u/vagif and /u/roquesort are arguing about. Consider reflecting on this.
There's a typo in that sample. `ode` → `mode`. But the proper way is to use the minor mode defined here: https://github.com/haskell/haskell-mode/blob/master/haskell-process.el#L1642..L1660 So `M-x interactive-haskell-mode` will give you the keybindings in `interactive-haskell-mode-map`. And to remember it for all Haskell buffers: `(add-hook haskell-mode 'interactive-haskell-mode)` 
I got yr back in making sure you won't regret it :)
I must say with Evil, it did not feel much like a switch... I still `&lt;ESC&gt;:%s/foo/bar/g` to replace things, or `v{d` to delete things. 
By the way, as = take (length p) (repeat av) diff = zipWith (-) p as Why not just diff = zipWith (-) p (repeat av)
and there is also `replicate :: Int -&gt; a -&gt; [a]`.
Thanks for the explanation. I guess the new key bindings are used to avoid conflict between those used in ghc-mode and the one used in haskell-mode for type information. Is there a better way to avoid this kind of conflicts than remapping all the key bindings for haskell-mode?
I described the reasons here: http://www.reddit.com/r/haskell/comments/18pi4s/does_anybody_process_images_in_haskell/. All-new Repa 4 was developed by Ben, but for some reason he stopped doing this. You can try to connect to him and ask why.
I wonder how things would differ with the alternate IO implementation mentioned in [this article](http://comonad.com/reader/2011/free-monads-for-less-3/) (that article series has back links but not forward links, so I've linked to the last article). Apparently that approach has a lot of nice properties, including needing less "magic" code in the compiler itself. There *is* an implementation of unsafePerformIO in the article, but it's not obvious whether the other unsafe\*IO functions would be possible or not.
I'm in the same boat (vim -&gt; emacs) I originally started using Emacs for org-mode and as an email client, once I got evil set up I started using it for everything that requires a dev environment.
Slides can be found here: http://joshcough.com/talks/codeworld-slides.html. Unfortunately this video is cut off quite a bit, but most of CodeWorld is covered in what's there. We have some more of the video, and I was hoping that this wouldn't make it here until we finished that, but this should be good for most folks, and the slides should certainly help.
Interesting side note: GHC needs to hide the state token representation behind an abstract `IO` type because the state token must always be used linearly (not duplicated or dropped), but the type system can't enforce this. Clean, another lazy Haskell-like language, has uniqueness types (which are like linear types and possibly different in ways I'm not aware of), and they expose the `World`-passing directly and provide a (non-abstract) `IO` monad only for convenience.
The comments in IO.hs explain the lazy in unsafeDupableIO. &gt; Why is there a call to 'lazy' in unsafeDupablePerformIO? &gt; If we don't have it, the demand analyser discovers the following strictness &gt; for unsafeDupablePerformIO: C(U(AV)) &gt; But then consider unsafeDupablePerformIO (\s -&gt; let r = f x in case writeIORef v r s of (# s1, _ #) -&gt; (# s1, r #) &gt; The strictness analyser will find that the binding for r is strict, &gt; (becuase of uPIO's strictness sig), and so it'll evaluate it before &gt; doing the writeIORef. This actually makes tests/lib/should_run/memo002 &gt; get a deadlock! &gt; &gt; Solution: don't expose the strictness of unsafeDupablePerformIO, &gt; by hiding it with 'lazy'
You mention that you are not sure what does the `lazy` in `unsafeDupablePerformIO` do. It's kinda documented in the comments above its code: -- Why is there a call to 'lazy' in unsafeDupablePerformIO? -- If we don't have it, the demand analyser discovers the following strictness -- for unsafeDupablePerformIO: C(U(AV)) -- But then consider -- unsafeDupablePerformIO (\s -&gt; let r = f x in -- case writeIORef v r s of (# s1, _ #) -&gt; -- (# s1, r #) -- The strictness analyser will find that the binding for r is strict, -- (because of uPIO's strictness sig), and so it'll evaluate it before -- doing the writeIORef. This actually makes tests/lib/should_run/memo002 -- get a deadlock! -- -- Solution: don't expose the strictness of unsafeDupablePerformIO, -- by hiding it with 'lazy'
&gt; or generating SQL statements, there's haskelldb Too many bugs * https://github.com/m4dc4p/haskelldb/issues/22 * https://github.com/m4dc4p/haskelldb/issues/18 * https://github.com/m4dc4p/haskelldb/issues/17 * https://github.com/m4dc4p/haskelldb/issues/15 
I really recommend using the `async` package, it removes lots of the headaches with exception handling and threads.
Yes! See here: http://www.reddit.com/r/haskell/comments/2e8d53/whats_the_best_practice_for_building_a_dsl_in/cjx6cse I wrote it partly in response to the bugginess of HaskellDB.
I had this too. I fixed it by setting the path to only the Haskell directories ie: set Path = C:\Haskell\2014.2.0.0\lib\extralibs\bin;C:\Haskell\2014.2.0.0\bin;C:\Users\username\AppData\Roaming\cabal;C:\Users\username\AppData\Roaming\cabal\setup-exe-cache;C:\Haskell\2014.2.0.0\mingw\bin I think it was caused by cabal finding mingw executables on my path from another completely separate mingw install.
Thanks for pointing that out. It seems I missed that comment since it came after the definition of `unsafeDupablePerformIO`, and I mistakenly thought that comment applied to `unsafeInterleaveIO`. Alright, fully understanding the issue here will take me a bit more time, but thank you for pointing out the explanation! __EDIT__: For those interested, you can [see it in the hscolour-ed sources here](http://hackage.haskell.org/package/base-4.7.0.1/docs/src/GHC-IO.html#unsafeDupablePerformIO).
The logs are here. We would be grateful if you can provide us with an example of undesirable behaviour on the channel so we can work on improving it. * http://tunes.org/~nef/logs/haskell/ * http://ircbrowse.net/browse/haskell 
Thanks! I'm looking forward to your public announcement. :) (Currently I have no immediate demand for an SQL EDSL, but I'm always curious about what's around).
I don't think memoization would be exactly the right word for it, but I wouldn't be surprised if GHC inlined `stddev` and then the calls to `avg` were "common subexpression elimination"-ed. Looking at the core would be a direct way to check.
I really enjoyed this article. For someone like myself that had inferred some of this from reading core, and prim sources, having the story laid out in more complete fashion really helped connect the pieces. 
I like that Evil actually improves the search/replace by showing a preview of what everything will look like as you're typing the command
You could just have linked to all 3 of them separately... http://comonad.com/reader/2011/free-monads-for-less/ http://comonad.com/reader/2011/free-monads-for-less-2/ http://comonad.com/reader/2011/free-monads-for-less-3/
Thanks for the reply! I found this out yesterday when I was trying to get the Windows side of my computer to do Haskell development too, but for other people on this thread, it might be useful.
I think a slight problem is that data in Haskell forms a graph, not a tree, but this graphical representation only permits a tree. Thus, its not wrong, but it does not capture the performance or memory characteristics.
I'm an evil user on emacs, but I find haskell mode to be awful due to the indentation forced for newlines when you hit 'o' in normal mode. Has anyone else run into this issue and solved it?
It's so concise, amazing. Do you think you could write some tutorial based on your code?
Ah yes, lambdas too.
Heh, I'm glad to see people appreciated this :-) I was getting tired of answering quesitons along the lines of "why is this an unsafe use of inlinePerformIO?". I wanted to say "you don't ask why it's an unsafe use, but must justify why it is a safe use". So I decided to change the name and improve the documentation to disuade the unwary from using it. Hopefully this is sufficient :-)
Well, it's not complete. I'm planning on doing a series eventually! Will try to get to that sooner.
Yeeeah, somehow I doubt that you're honestly and accurately recounting anything you actually experienced. Given your remarkably unpleasant attitude here, I'm going to guess that you acted like an asshole on IRC, got called on it, and now want to play the poor innocent martyr rather than take responsibility for your choices.
Pleasantly surprised to see this here. (I'm the author.) I'm still a Haskell newbie, but I get a little more confident every day. I get a lot of help from the community in learning what I need to do, and what to look at. Haskell, the tool, is also really good at getting me to clarify what I'm thinking, and helping me abstract it properly.
Thanks!!
Also, pluralsight.com has a video tutorial series: http://blog.pluralsight.com/new-course-haskell-fundamentals-part-1
XMonad does something similar: clearly separate read-only, configuration-like, values from the evolving state.
Phil Wadler - https://www.youtube.com/playlist?list=PLtRG9GLtNcHBv4cuh2w1cz5VsgY6adoc3
If you want your split to work on infinite list and be efficient, you better use a lazy pattern for the second argument (if you don't, the whole list need to be consumed before you can start to produce the result) : split = foldr (\x ~(l,r) -&gt; (x:r,l)) ([],[])
I currently consider flow-control state 'less mutable' than other things in the environment. I'm not sure this is worth the additional abstraction, but I'm giving it a shot right now. It's only partially successful at this point (witness the continuation in Environment). Will have another go at it later.
It may be helpful, if you can, paste your code in something like [lpaste](http://lpaste.net). /u/Tekmo is pretty helpful. 
Edit: There used to be a very wrong response here. See /u/Tekmo's response above. 
I tried the same program under GHC 7.8.3 on Mavericks, and couldn't re-produce the IOException reliably. I'm getting far more successes than failures, even when I removed the threadDelay line. It should also be noted that scotty doesn't really return, so it's not easy to spawn the service and tell the main thread that it is ready, unless you want to modify scotty's source. The only other reliable fix is to repeatedly check if the port is ready before continuing, just like @kqr mentioned.
I wrote a Unix utility in Haskell that will pipe from stdin to a redis pubsub channel (or the other direction): tail -f /log/file | grep "filter" | pub "channelName". http://hackage.haskell.org/package/pub I use it for log analysis (just have a subscriber on the channel using hedis) and analytics since I can fan out. I also wrote the analyzer using pipes and attoparsec. If you want code samples I can pm. Breaking up a line by tabs should be very easy if that's all you want (nothing more structured). Log processing in batches might be a different story.
FP Complete has a online IDE that is okay and has vi bindings. Also, in they have the School of Haskell articles. I'm not sure if Leksah is still maintained, but it is (was?) a Haskell IDE.
Downvote: is this [my comment] inappropriate? If it is I'll delete the comment but I thought I was being helpful...
seemed helpful to me :)
What should `parseRow` look like? From your snippet I would expect the following program to output each line prefixed with "line: ", but that's not the case: it prefixes each letter. import qualified Data.ByteString.Char8 as C8 test :: IO () test = runEffect $ for (concats (PB.stdin ^. PB.lines)) $ \line -&gt; do liftIO $ putStr "line: " liftIO $ C8.putStrLn line That's the same behavior I get using `for PB.stdin $ \line ...`- what is the point of concats and PB.lines in your example?
Sometimes the Haskell tag on vimeo is useful to check: https://vimeo.com/tag:Haskell
I used Leksah a bit. Didn't understand the UI at all.
Sorry, I had to run to a meeting before I was able to finish writing my response. I'm just getting back to it now.
Actually, `concats (prod ^. lines) = prod`. If you wanted to strictly demand each line you would use: import qualified Control.Foldl as L L.purely folds L.mconcat (prod ^. lines)
Yay, you've arrived! Thanks for the correct! It seems I am just as confused as the original poster in this case. 
No worries! I make it hard to do that on purpose.
It's referenced at the end of the README, but worth noting that Bernie Pope has done some work along the same lines: * https://github.com/bjpop/language-python * https://github.com/bjpop/berp * https://github.com/bjpop/blip * http://www.berniepope.id.au/docs/ImplementPythonInHaskell.pdf 
A taste of Haskell , use *xmonad* as example! [slides](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-tutorial/), [part1](http://youtu.be/jLj1QV11o9g), [part2](http://youtu.be/IqXTUbdLig0) and Don Stewart's [xmonad design and implementation](https://www.youtube.com/playlist?list=PL122469E9098F740A)
Can anyone cite real-world examples of things which are OK with `unsafePerformIO`, but not with `unsafeDupablePerformIO`? The obvious examples that come to mind which fail the idempotence test also fail the purity test.
Glad to hear you have more of the video. I'm looking forward to seeing the rest, and hearing more of the talk. If I survive the crucifixion over currying of functions, that is. :)
Great! Thanks for doing this talk. Pretty exciting (and hectic!) times for CodeWorld. I'm teaching two classes in two different schools now (one locally in Daly City, and one in Colorado via video conferencing!), and Luke Palmer is teaching another class, too. There's a textbook in the works, targeting a publication date next summer. Lots of cool ideas to go around: video tutorials, collaborative projects, etc. Just wishing I had a little more time to devote to this!
Are you starting ghci with +RTS -N? It's possible the forked thread doesn't get a chance to run at all until the main thread yeilds (explicitly or by allocating).
This has been my feeling too. Pipes and conduit are the best we have for now, but there has to be an easier abstraction.
Ertes is working on a simpler (and more efficient) alternative to `FreeT` in his `fuse` library, which I think will answer most of your objections. I'm waiting for him to upload it to Hackage so I can switch to it. The main issue with `FreeT`, which makes a lot of this difficult is: * `FreeT` basically a higher-order generalization of lists, but it doesn't come with the same library of built-in operations equivalent to what you would find in `Data.List`. The `fuse` library does have such a large toolkit. * `FreeT` has unintuitive names for basic functionality that we take for granted on lists, like pattern matching and consing elements. A lot of `FreeT` code is quite easy to write by hand once you see the parallel to list operations. `fuse` doesn't have this problem (the names are very intuitive). Answering your more general question, I don't believe there is a simpler solution than `fuse`. Anything else is oversimplifying the problem. Edit: Link to [the `fuse` library](http://hub.darcs.net/ertes/fuse/).
Also, while you are here, can you take a look at [this odd `attoparsec` behavior](https://gist.github.com/bgamari/366b3e25aabbec53381c) that Ben just brought to my attention while working on `parse`?
Check out the FP Complete Haskell Centre! Their community edition is free, and it is, as far as I know, the only polished Haskell IDE there is.
Empty strings have a special meaning to attoparsec. A single empty string indicates a flush, and two indicates eof, iirc. You need to remember to filter out nulls coming down the pipeline before handing them to attoparsec.
To add to the list: Check out Conner McDaniel's series of videos at https://www.youtube.com/playlist?list=PLUQzXLQ6jvHL_k3QOMKXehVoZdk-sKtHd.
For me, as a sometime user, the immediate answer, which I didn't bother to post earlier, was pipes-attoparsec + aeson. It will be clear I'm a 3rd rate parser writer, but here goes: {-# LANGUAGE OverloadedStrings #-} import Pipes.Attoparsec (parsed) import qualified Pipes.ByteString as PB import qualified Pipes.Prelude as P import Data.Attoparsec.ByteString.Char8 import Data.Aeson.Parser import Data.Aeson import Control.Applicative import Data.HashMap.Strict as M main = P.sum (parsed (fmap count lineparser) PB.stdin &gt;&gt; return ()) &gt;&gt;= print where count (_,_,_,Object obj) = case M.toList obj of (("tag",String "Login") : _) -&gt; 1 _ -&gt; 0 count _ = 0 lineparser = (,,,) &lt;$&gt; server &lt;*&gt; user &lt;*&gt; timestamp &lt;*&gt; event where server = many1 (satisfy isAlpha_ascii) &lt;* char '\t' user = decimal &lt;* char '\t' event = value &lt;* option () (char '\n' &gt;&gt; return ()) timestamp = sepBy decimal (char '-') &lt;* char '\t' It works fine. `parsed` simply runs the parser again and again over incoming chunks of input, giving the same result however the material is chunked. The downside here is that `attoparsec` rather than `pipes` does the breaking on newlines, which isn't likely to be ideally fast... The fold `Tekmo` is discussing would simplify the line parser slightly and would be faster. The words may be a mouthful, but the representation of `lines` in `Pipes.ByteString` and `Pipes.Text` adds no more yaks to anything than does the representation of `lines` in `Data.Text.Lazy` or `Data.ByteString.Lazy`; it's just that `[]` is easier to take in than `FreeT` -- thus lazy io continues ... Note that the representation of `lines` in `conduit`, `io-streams` and `enumerator` is apparently simpler, but is the *exact* equivalent of demanding that Data.Text.Lazy.lines have the type `Text -&gt; Text`, with none of those funny `[]` yaks, but that it secretly concatenates the underlying strict text chunks line by line -- so that e.g. the lines could be recovered with `toChunks`. Such a function never crossed your mind as it is palpably toxic. Good luck to you inventing a fundamentally different streaming approach -- the one in enumerators &amp; company is not streaming. `iterIO` doesn't seem to have attempted one. There probably is a bit more upfront understanding required, but it's all general Haskell knowledge, not library-specific knowledge. With `lines` &amp; company, you end up needing an understanding of `FreeT`, which is well worth having. And you get the advantage of Gabriel as a teacher....
I wasn't going to comment here, but [/u/bos's comment](http://www.reddit.com/r/haskell/comments/2ejzst/streaming_tabseparated_logfile_analysis_with/ck0j11g) made me reconsider, if only to demonstrate that conduit has a drastically different approach to this than pipes. A fully working solution is available at: https://gist.github.com/snoyberg/a422d558e9142e37d2aa The difference in complexity in these solutions is, IMO, the direct result of all the things /u/Tekmo and I have debated in the past. By including the complexity of leftovers in the core abstraction, conduit doesn't need to jump to other abstractions as soon as something more complicated arises. Then there's the difference between conduit's `line` approach vs pipes's lens-and-FreeT approach.
To make sure it's clear that conduit doesn't look like pipes on this, I've [added an answer](http://www.reddit.com/r/haskell/comments/2ejzst/streaming_tabseparated_logfile_analysis_with/ck0nzs4) to this question.
Unfortunately pipes-csv appears not to allow double quote characters to appear unescaped, something which I run into a lot- so it seems I can't use it (I tried :)) Something that this solution doesn't capture (perhaps I wasn't clear enough about what my Python does) is the `groupby` part: Python's groupby works almost exactly the same as `Data.List.groupby`. That is, it breaks up an iterator into contiguous chunks using some predicate. In my case, that's by the "server" field. The reason I use this in python is so I can deal with each group individually, without having to bring my 'group separation' logic into my 'login counting' code. What's the pipes analogue of this, and how do I use it? I understand the result is going to look like `FreeT (Producer Row IO) ()`, but how do I then operate on each `Producer Row IO` individually?
Thanks for this solution! Is it correct that your code is building up a HashMap of counts, and thus holding the count for each server in memory? I realize I didn't make this clear, but my input data is actually pre-sorted by server, so I want to segment the stream of lines into contiguous chunks (by server) which I then act on independently. A bit more fleshed out summary is in this [reply to Tekmo](http://www.reddit.com/r/haskell/comments/2ejzst/streaming_tabseparated_logfile_analysis_with/ck0psx6) To be a bit clearer about what the python is doing, i've annotated with pseudo-types: rows = map(mkRow, sys.stdin) :: [Line] -&gt; [Row] groups = groupby(rows, lambda row: row.server) :: [Row] -&gt; [(Server, [Row])] results = starmap(numLogins, groups) :: [(Server, [Row])] -&gt; [(Server, Count)] Could you comment on how to achieve groupby's "nested iterator" behaviour using Conduit? Thanks!
Note that your solution is just as vulnerable to memory exhaustion as any of the others you point to. Consider if the input was a line of a trillion 'A's; you'd end up continuously consuming data for `server` until you exhausted memory. See my solution, which uses `line` instead of `lines`. I added `linesBounded` to conduit for exactly this reason (it *was* a mistake in the original API; I've left it in for backwards compatibility).
I just pushed another commit to that Gist that demonstrates one way to do it, though I seem to be having trouble linking to a specific commit in a Gist. Anyway, the relevant code is: countResults = do mname &lt;- peekC case mname of Nothing -&gt; return () Just name -&gt; do cnt &lt;- takeWhileC (== name) =$= lengthC liftIO $ putStrLn $ name ++ ": " ++ tshow cnt countResults It might be worth adding a `groupBy`-style function to the `Conduit` module, but I usually find it just as easy to write something like this from scratch.
Is there still a separate threaded runtime in GHC? Forgetting to use the `-threaded` option used to cause lot of strange non-deterministic failure in threaded situations...
btw, I'd suggest implementing closures next, as those may have an impact on your representation. E.g. to support code like def adder(x): def f(y=0): return x+y return f def counter(): x = [0] def f(): x[0] += 1 return x[0] return f
I was happily impressed by that too :)
It's even worse than that, since at the moment the `Server` field in my imagined stream of tuples `(Server, User, Date, Value)` would give me a lazy list of a trillion "Char8"s! When I cooked it up earlier, I was just trying to get a function `Producer ByteString m r -&gt; Producer (Server, User, Date, Value) m r` on the table for `statusfailed` to start reasoning about. In the end I pasted it, since it seemed to me that everyone was being distracted by `lines` -- one was likely to need a parser for the json anyway and `pipes-attoparsec` makes repeated application `FreeT`-free. My irritable remarks about `lines` weren't directed at you, but at the great man, who clearly hasn't given the matter a moment's thought but is happy to hold forth about yaks. Thus I put him to the task, which should be intelligible to him, of writing a `lines` for `iterIO` ... good luck to him, I say, confidently anticipating flight into the arms of lazy text and lazy bytestring. Before I saw this note of yours I was trying to replicate the solution you linked elsewhere here. It would I think be pretty simple, but `Pipes.Group` is missing several really obvious combinators. (So is `Control.Monad.Trans.Free` which seems to want to claim the type without bothering to think what the useful associated combinators are; maybe `ertes` will do better.) There is also a need for a more `fold` types in `Pipes`, e.g. fold' :: Monad m =&gt; (x -&gt; a -&gt; x) -&gt; x -&gt; (x -&gt; b) -&gt; Producer a m r -&gt; m (b,r) At present there is the information-cancelling P.fold :: Monad m =&gt; (x -&gt; a -&gt; x) -&gt; x -&gt; (x -&gt; b) -&gt; Producer a m () -&gt; m b With the other I could split a Producer (the remaining Producer would instantiate `r` in the signature), then fold over the initial part and get the fold result together with the rest of my producer, before proceeding. Splitting producers is omnipresent once you start working with them; not being able to fold over the opening segment is a bit of a handicap. When I started to replicate what you did, I immediately needed it for fold' (&lt;&gt;) mempty id $ view (PB.splitAt 80 . PB.break (== 9)) p -- p is the individual line as a producer which returns the Server (correctly as a ByteString this time) and a Producer that returns a Producer. That Producer needed to be 'drained,' since it is the excess over 80 Word8's (if it exists); then one would carry on with the wholesome Producer *it* returns, dropping fields as you do and so on. I'll see if I can finish it tomorrow; at the moment the parses are failing because of some dumb off-by-one mistake or something. 
Would definitely recommend the Erik Meijer video series that some others have suggested. As for an IDE, I use the Sublime text editor. As long as you have Haskell syntax highlighting on, and GHC installed, Ctrl+B will run your program, displaying the output (or compilation errors) in a pane at the bottom of the window. http://i.imgur.com/Y1bIdMl.png Note: Running from Sublime isn't fast; if going for speed, I compile and run from the command line.
Why do you think Bryan knows anything about iterIO? I wasn't aware that he had any connection to that library. Personally, I think the line function in conduit is the right way to handle these situations, and FreeT and lens just adds a huge level of complexity to the problem, when it doesn't need to be there. I've had this debate before, and I don't think it's necessarily a pipes vs conduit debate. Each library could likely implement the other solution to the problem. I do, however, think that the need to constantly switch between different forms of composition in pipes is a problem.
No, sadly I haven't had any time to implement something as rich as what you're talking about. I've only implemented common statistical aggregates, no moving windows. Pipes is great for this, I'll see if I can pull together some code to share, it should at least give you a place to start.
What about this: helper i = unsafePerformIO $ print i &gt;&gt; return i main = do let one = helper 1 two = helper 2 one `seq` two `seq` print (one + two) 
FWIW I'd personally reach straight for the csv-conduit library: http://lpaste.net/110040 Define a simple type and use the generic deriving for the JSON so you don't have to manually parse it. Use `intoCSV` conduit to get a stream of `[ByteString]` rows, specify `csvSep` as tabs with no quote character. Make a conduit consumer that loops over all the items in the stream and then finally returns a hashmap of all server-&gt;login count numbers. I bang the hashmap in `go !m` to avoid building up thunks. Might not be necessary but intuitively this is what I'd write. Code could be simpler but it's not bad. If you had to parse something that stopped looking like CSV anymore, you would replace the `intoCSV config` expression with `myOwnConduit` of type `Conduit ByteString m [ByteString]` and implement a parser for a bytestring that yields a `[ByteString]` (with attoparsec or so), the rest of the code wouldn't have to change.
In that case it could use a different bubble colour and maybe toggle between the syntactic variants.
Please follow through with it, there is much need for something like this. I am also looking forward to it.
I noticed that you actually wanted to model the group by behaviour, here's an alternative version still with csv-conduit: http://lpaste.net/110044 I think this is pretty faithful to a Python coroutiney solution: CB.sourceFile "input.txt" $= -- read the file into a ByteString stream intoCSV config $= -- convert the stream into CL.mapMaybe mkRow $= -- convert into rows CL.groupBy (on (==) fst) $= -- group by the server name CL.mapM (return . numLogins) $$ -- count the logins for each group CL.mapM_ (lift . print)) -- print each result The `mkRow` and `numLogins` should correspond to your Python functions.
There's a bug of sorts: if you enter the expression `take 2 [1,2,3,4,5,6,7,8]`, by expanding all the `take`s before anything else, you can make it return the entire original list.
Undefined. `seq` does not tell us anything about the order of evaluation of its two arguments, so it would be valid to evaluate `two` then `one` or the other way around.
It's linux, who uses IE!?
"It"? Many people are forced to use IE. 
Solid achievement, thank you.
I actually think Lazy evaluation makes it far easier to reason about your programs because you can replace equals with equals. You can't do this in strictness without taking care to avoid nontermination.
The `pipes` analog of Python's `groupBy` command is `Pipes.Group.groupBy`, which has the following type: groupBy :: (a -&gt; a -&gt; Bool) -&gt; Producer a m r -&gt; FreeT (Producer a m) r There are two ways you can operate on each individual `Producer Row m`: * Explicitly recurse over `FreeT`, analogous to how you would recurse over an ordinary list. The `pipes-group` tutorial explains how to do this, but the basic idea is that you use `runFreeT` to "pattern match" on the head of the list. You either get the first `Producer` and the tail or an empty list. * Use a combinator analogous to a list combinator. For example, `Control.Monad.Trans.Free.iterT` is a poorly named analog of `foldMap`. Similarly, `Pipes.Group.maps` is an analog of `map`. There are some `Data.List` operations that have no `FreeT` analogs, but if you encounter them consider filing a ticket against the `FreeT` library to add them.
To be precise, `pipes` only uses `Lens` to deal with leftovers, not `FreeT`, and there is a `pipes` solution which corresponds exactly to the one you just gave. However, the reason I did not provide it is that: * The solution you gave fails on long lines. Your own comment mentions this. * Your solution doesn't reify a line as a first class object that you can manipulate. For example, suppose that he needs to just take only the first 99 lines. With `pipes`, that's as simple as: over lines (takes 10) :: Producer ByteString m r -&gt; Producer ByteString m r Conduit has no analog of `takes` 10 because a delimited sub-stream is not a first-class object that you can manipulate in the `conduit` ecosystem.
My mistake. I realized you can just use `Pipes.Attoparsec.parsed`, which has the type you want. See `randomCrank`'s answer below.
Looks like a legit bug that's been around for ages. I narrowed it down to a much less complex test last night with just a few minutes of work, will continue on it this morning.
Well, the OP is essentially a big puzzle that wouldn't exist under strict evaluation. It's not yet clear to me why lazy evaluation often feels like a puzzle, in a way that strict evaluation doesn't.
Thanks for the fast response!
The post mentions XMonad, which is an X11 window manager, so it rarely sees any use on MS Windows or Mac OS X. Therefore, the assumption throughout the thread is that we are talking about Linux, one of the BSDs, or some other system that uses X11, where IE is in a vast minority.
Strict evaluation often feels like a puzzle to me; perhaps you've just internalised strict evaluation better. For example, keeping modularity and composability with strict evaluation can sometimes be very tricky indeed.
Yeah, it needs some more elegant way to deal with errors right now. I deferred most of it until I have a way of raising exceptions in place. I'll probably write my own error function to consolidate all of the engineering sin in one place so I can rip it out eventually.
berp is really cool! He's done a lot of great work here.
I guess closures aren't just for lambdas, eh? This makes things more interesting. Will start thinking about this sooner than later. Thanks!
This article is about lazy evaluation in the presence of side effects, which is not the normal case in Haskell code (these functions are marked `unsafe` for a reason). And in another sense it's about what happens when you break the invariants that the compiler and standard library implementation provide for you (single-threadedness of the state token). You could imagine a similar article about, say, inline assembly in C functions and how you need to be careful not to clobber any registers that the C compiler is using to store temporaries, or mess with the stack/frame pointers. In response to such an article I imagine you would say it's the inline assembly that's the problem, not the fact that the C compiler uses registers in the way it does!
Would this basically replace using ghc-mod from emacs? 
I was just thinking mighty `bos` wouldn't likely accuse his sometime fellow teacher of yak shaving, as he was happy to do with Tekmo; the admittedly excessively surly thought came to me while I was looking for treatments of lines in the no-lazy-io libraries. In fact I was wrong, `iterIO` does include some line manipulation based on `ListLike` . There isn't a `lines`, but see e.g. [`safeLineI`](http://hackage.haskell.org/package/iterIO-0.2.2/docs/src/Data-IterIO-ListLike.html#safeLineI) . If I understand it, it will accumulate a trillion-byte line as a `ByteString`. So much for that tough-as-nails 'secure systems' approach. I wasn't speaking as an expert but as a user, originally meaning to address myself to `statusfailed`; I got surly though because I had devoted some thought to things like `lines`. I can't see there is any objection from this point of view to the `lineAsciiC` you are using. It doesn't use `B.append` recursively -- but of course it also isn't a conduit; the implicit recursive use of it in `convertLines` is I think more subtle to the eyes of the non-adept than maybe you are thinking? My first go at an emulation was using the `line` function from `pipes-bytestring`; it like the one in `pipe-text`, has the type `Producer Stuff m r -&gt; Producer Stuff m (Producer Stuff m r)` which maybe has less of a yak's beard look it it; then I was going to recursively apply my operation on the successive lines. No mystery type like `FreeT (Producer ByteString....)` was involved ... but then I realized I was doing hand recursion where I just need one of any the above-mentioned missing combinators; the dumb asProducer :: Monad m =&gt; FreeT ((,) a) m r -&gt; Producer a m r for example, would have done. (It and its inverse express the essence of a producer, though lazy pairing is not quite what one wants.) I suspect similar abstractions are waiting to break out in `conduit`; they might occur to you if you reflect analytically on the back and forth between your `convertLine` and `convertLines` in the gist. I'm not sure though. I think there's some truth mixed with confusion and misunderstanding in "I do, however, think that the need to constantly switch between different forms of composition in pipes is a problem". I will see if I can formulate a response later, again of course from a somewhat user-ish perspective.
While his specific solution would exhaust memory (because the size of the parsed `Row` is unbounded), in general `pipes` can consume arbitrarily long lines as you are folding them into something that takes constant space. A good example of this is a `pipes` program that counts each line's length: import Control.Foldl (purely) import Control.Foldl.ByteString (length) import Lens.Family (view) import Pipes import Pipes.ByteString (lines) import Pipes.Group (folds) import Prelude hiding (lines, length) lineLengths :: Monad m =&gt; Producer ByteString m r -&gt; Producer Int m r lineLengths = purely folds length . view lines That will run in constant space no matter how long the lines are.
Side note: I'm embarrassed to admit that only reason I haven't implemented `fold'` yet was that I kept debating what to name it.
Thank you, thank you and thank you. I setup emacs using your tutorial and it looks and works great. Is there any font that you would recommend in Solarized light theme?
Sure.
I use both of them. It would be nice to know what Ben's plans are.
I can take over Yarr if that suits but I am pretty busy at the moment. Is there a handover document? What needs doing to it?
Well, as you hint at yourself, those closure's you implemented are the simpler ones which can't cope with Scheme's `set!` operation...
It is really more of a convention adopted by Codeworld. Nothing enforces it in the parser, etc.
Currying a lot of folks understood, but swapping the order of the arguments to map? Sorry, skip the cross, that's a hanging offense. ;) 
The solution I gave fails on long lines, because attoparsec/aeson will fail on long lines. As I mentioned, fixing this is a matter of applying an arbitrary length restriction to the line, which isn't any different between pipes and conduit. I'm not buying the arguments of theoretical complexity. There's a clear case where `line` in conduit (at least to most people here it seems) is clearer than the FreeT/lines approach. Limiting that loop to only loop 100 times is trivial, and I think any beginner Haskeller would see how to make the appropriate modification to `countResults`. So I'd much rather take the simple base case which is easily modified to a "first class" solution that is difficult to understand.
Anyone can use haskell as they want, changing conventions if needed. And if for that you just need to rewrite prelude, then it makes haskell even more awesome. Depending of the users of the library, it might be better to do it in the most idiomatic way. But if someone wants to part from the idiomatic way, no problem! It's a free contribution anyway. (And CodeWorld is very cool!). In the end it's still haskell, and helps it to achieve world domination ; ). Even something not-haskell like Purescript is great for the haskell community!
It sounds to me like you made the right decisions.
I heard about CodeWorld for the first time a week or two ago and I can't say I feel betrayed by its syntax and whatnot. It's clearly a teaching tool for children... right? Why would Haskellers be put off by this? Anyways, good work!
One example would be reading some config values from a file. The code would need to open the file, read from the handle, and close the handle. If this was interrupted halfway through, the handle wouldn't be closed.
I'm not sure I understand the question exactly. Operators are still curried in CodeWorld. That's a compiler feature, rather than a language feature, so I'm not able to "fix" it. I've definitely thought about proposing `{-# LANGUAGE UncurriedOperators #-}`... but for the time being, CodeWorld's prelude provides `toOperator` and `fromOperator`, so converting between operators (curried) and functions (not) is an explicit operation. As far as overloading, there is no overloading (i.e., ad hoc polymorphism) in CodeWorld's prelude at all. There are no type classes, and that's the only way overloading is possible in Haskell. This is fine with me. There's also no operator overloading in math. Functions always have a well-defined domain and codomain. It might be inconvenient, but there are a lot more inconvenient things in CodeWorld. The goal here isn't to be a usable large-scale software engineering feature set; it's to present interesting challenges about modeling things with math.
To clarify, he's merely establishing a library convention where he defines functions of one parameter, where that parameter is a tuple. In other words, they can still write functions like this if they choose: f x y = z ... but he's encouraging them not to and instead teaching them to use this convention by default: f (x, y) = z
Really, the convention is more f(x, y) = x + y &gt;&gt;&gt; f(1,2) 3 and it's sort of pleasing that this is such a natural syntax in Haskell even if it's non-standard. The lack of space emphasizes the notion of calling a function via parentheses, of course. This is nice because it's certainly how most of math notation works.
I don't think it's a big deal either way, but I do think that in teaching the intersection of programming and math, the computer science approach should at least occasionally win out. I also look forward to a future where cs and math are not taught as separate subjects.
https://www.haskell.org/ghc/docs/latest/html/users_guide/defer-type-errors.html
Without really knowing much about Conduit, this does look like the kind of solution I was hoping for :) Cheers!
[-fdefer-type-errors in GHC](https://www.haskell.org/ghc/docs/latest/html/users_guide/defer-type-errors.html) /EOT
Whoah, I wasn't familiar with this. Seems incredibly useful (but let's see how often I end up using it, now that I know it exists)
Do you not find this baulks when you start looking at specific major/minor modes that don't have Evil support? How have you found Evil to interact with Emacs in general? I ask as I'm very interested in switching to it (I see Vim's modal editing as superior) but have never had much luck - but I also never spent a lot of time in Evil mode.
&gt; I feel a bit stupid to ask for a feature which already exists. I guess it shows you have good taste! 
I think this was largely a mechanical change. In most cases, curried functions save their most significant argument for last, so as to make it easier to fix the details with partial application. So when I made the change, my default was to swap the arguments, unless there was some other obvious reason the order made intrinsic sense. What's the reason that it makes intrinsic sense to put the function on the left for map?
What a shame. I always thought they operated with a quite cool and unique mixture of skills and technologies. Furthermore I briefly spoke with both Jeff and Mathew (two engineers at ParSci) and they seemed to be quite bright people, sad to see they are not listed anymore on the team.
Great news, I think it can really support both the teach programming/math and the teach/demonstrate Haskell scenarios this way! I'll upload an updated Gloss backend for GHCJS soon, so it should be possible to use exactly the same code for native (OpenGL window) applications. By the way, GHCJS runs Template Haskell with a JS engine now (it used to load native code) so it's not all that hard to build a variant that is safe enough for untrusted code. The standard implementation is a [small server script](https://github.com/ghcjs/ghcjs/blob/master/lib/etc/thrunner.js) that runs on node.js. GHCJS first sends the runtime system and the actual [Template Haskell runner code](https://github.com/ghcjs/ghcjs-prim/blob/master/GHCJS/Prim/TH/Eval.hs), and then incrementally all compiled splices and their dependencies. You could for example modify the script to use a [node.js sandbox](https://github.com/gf3/sandbox), if you're more adventurous you could even send it to the user's browser. 
Peter Braam is working on some cool HPC projects using Haskell. He'll announce details when he's ready.
This still exists https://www.parsci.com/ (although its SSL certificate seems broken). It seems to be a new company now, called docsforce, and all of the old engineers are no longer listed. The founder Peter Braam is still there.
...at the bottom there is a link for the previous page on Typed Holes, which is also relevant and works well with deferred type errors.[Typed Holes](https://www.haskell.org/ghc/docs/latest/html/users_guide/typed-holes.html)
At least this time the website has a concrete product! :-)
I was thinking of it more like this, every language I can think of that offers a map has it in the other order, so what is the reason for randomly bikeshedding it to a new place? http://perldoc.perl.org/functions/map.html For the currying/uncurrying thing, most folks can eventually recondition themselves to just throw parens in or not. For argument swapping it becomes more akin to learning PHP, which has different conventions "just because" that vary function to function. Now, I can't just take out the parens and get most of the way to Haskell, but rather have to take out the parens, randomly swap the argument order in half a dozen places. This makes it a much tougher sell to explain how to transition from one phase of learning to real Haskell.
&gt; There's also no operator overloading in math. I've never met a mathematician who didn't abuse notation. All mathematics is "informal" mathematics at some level.
&gt; But I see that as more a matter of different (and poorly defined) scopes than overloading; Ignoring the fact that plenty of papers will actually switch norms throughout without any real explanation, which you could reasonably argue to simply be poor writing; When `*` is used as say, the group operator, it is literally only representing an operator that is overloaded, being overloaded is its entire reason for existing.
Interesting idea about TH and sandboxing in node.js. It would definitely be cool to be able to autogenerate lenses for record types, for example! I'm in the middle of preparing to teach two classes and supervise a textbook, on top of a full-time job! I doubt I'll get to this any time soon. But I've created a [bug on github for it](https://github.com/google/codeworld/issues/69), and this goes on the "I'd love to get a pull request for this" pile.
Thanks for the prompt. Interesting class plan [here](http://math.mit.edu/~dspivak/teaching/sp13/).
Is this an infinite memory model?
This stackoverflow answer should be good: http://stackoverflow.com/questions/9406463/withfile-vs-openfile
I'm going to guess this has to do with using withFile and how hGetContents reads in characters on demand. The file handle created by withFile is closed at the end of the function and since you don't request any data to be read in your something function, it ends up returning an empty string (since nothing needed to be read in). In your something' function you do putStrLn s, which causes hGetContents to actually read the contents of the file and so when you return s, the IO action you return is not empty. Note: If you change the putStrLn s in something' to putStrLn "asdf" then you get the same result as something.
The correct way to use `hGetContents` is that you need to strictly evaluate the contents of the file before the `Handle` is closed. Anything not strictly evaluated before the `Handle` is closed will not be read from the file. `withFile` allocates the `Handle` for the scope of the computation it brackets. That's why `something` doesn't work, because you evaluate the string after the `withFile` is done and the `Handle` is closed by that point. That's also why `something'` works, because you strictly evaluate the string using `putStrLn` before the `withFile` is done, so you read in the file before the `Handle` is closed.
Not to mention just defining groups, rings, etc using `*`, `+`, e.g. as their binary operations. I wouldn't even say that's abuse of notation, just generality coming into play. Then again, if CodeWorld is directed at *children*, I could see overloading of operators being very confusing (it's not likely that 10 year olds will be talking about abstract algebra), so I don't think it's the wrong design decision in the application.
 powers1 :: [Int] powers1 = iterate (*2) 1 powers2 :: [Int] powers2 = map (2^) [0..] I'm interested in how you're going to make those identical lists share memory. I'm also interested in how you're going to hash the following example and *save* memory. x :: Integer x = product [1..10000000] `div` sum [1..10000000] In other words - your idea does not work in the presence of laziness and infinite data structures.
Are modules first class values? I would like to program in Haskell and not in a cabal file.
Thanks for clarifying. &lt;3
&gt; no blatant cons heh
Thanks that's what I wanted to hear :)
Don't worry, I got to learn something new because you asked.
I propose that we rename `String` to `CharString` to avoid further confusion. 
Without -fdefer-type-errors, you have to fix an entire module and dependencies before testing, or do wonky copy-paste out of the file.
Just throwing support behind this. If you're doing typical IO tasks, you're unlikely to see a performance difference from your choice of libraries.
Do you mean next-cps-fusion, the one I haven't yet officially blogged about ;)
If you give me a specific workload I can benchmark it against `conduit`.
/u/snoyberg is probably not ready for performance results from this branch to go public, but it's at https://github.com/snoyberg/conduit/tree/next-cps-fusion . I think the benchmarks are sums/other folds.
Alright, I will benchmark against those folds. However, I have the same reservations that I mentioned last time: these benchmarks should also test long pipelines, since usually the CPS transformation penalizes pipe composition.
My focus is on blip these days, which is a bytecode compiler/interpreter for Python. It generates bytecode which is compatible with CPython. The compiler is mostly done, but the interpreter is still work in progress. I imagine you are writing Hython for fun and learning, which is great. However, if you get tired of writing the parser you could always use the language-python library which is pretty complete for Python 2 and 3. Additionally I could always use some help with improving language-python, and would be happy to accept patches.
Much easier would be to just remove the String type altogether
Wait till you see my next blog post before bothering to benchmark, you'll need to understand what the fusion framework is doing to do a reasonable comparison. Essentially, conduit will have stream fusion, but only for non-monadic-composition code. So a large slowdown for introducing a monadically-composed pipeline is actually expected. The blog post will probably be out today.
No, that's basically the one, though we have some interesting comparisons, such as some cases where our core isn't identical to vector. I think the blog post will be out today.
Why did no one notice the horrible misnaming of ByteString before launch?
Except integrals, derivatives, exponeniation, sequences, limits, group actions, ...
Take a look at Processing, for more ideas on simplifying Haskell. Processing is Java, but has a thin frontend that does stuff like hide imports and packages to make programs look less scary.
Second that.
To be honest I cringe at the thought of having to write things twice now. (thrice if I require hs-base). Wouldn't it be sufficient to use the `reexported-modules` feature and have the type checker check if the module signatures (that is their actual implementations) check out?
ghc-mod and haskell-mode are thought to be used together. For example, ghc-mod does not provide syntax highlighting, this is done by ghc-mod.
&gt; What does ByteString and String have to do with each other, short of sharing 6 letters in their name? Text is the efficient, packed data type for the same use cases as String. ByteString is the efficient, packed data type for use cases of [Word8]. Which is why it would be great if `ByteString` was renamed to `Bytes`.
I wrote using `async` package, `setBeforMainLoop`: {-# LANGUAGE OverloadedStrings #-} module Main where import qualified Web.Scotty as Scotty import qualified Network.HTTP as HTTP import qualified Network.HTTP.Types.Status as HTTP import qualified Network.Wai.Handler.Warp as Warp import qualified Control.Concurrent.STM as STM import qualified Control.Concurrent.Async as ASync webApp :: IO () -&gt; IO () webApp sendSignal = Scotty.scottyOpts (Scotty.Options 1 setting) app where setting = Warp.setBeforeMainLoop sendSignal $ Warp.setPort 5000 $ Warp.defaultSettings app = Scotty.get "/" $ do Scotty.status HTTP.ok200 client :: IO () client = print =&lt;&lt; HTTP.getResponseCode =&lt;&lt; (HTTP.simpleHTTP $ HTTP.getRequest "http://localhost:5000/") sendSignal' :: STM.TMVar () -&gt; IO () sendSignal' v = STM.atomically $ STM.putTMVar v () waitForSignal :: STM.TMVar a -&gt; IO () waitForSignal v = STM.atomically $ STM.takeTMVar v &gt;&gt; return () main :: IO () main = do mv &lt;- STM.newEmptyTMVarIO ASync.withAsync (webApp $ sendSignal' mv) $ const $ ASync.withAsync (waitForSignal mv) $ const client 
Dynamic types let solutions be approximate. Static types force solutions to be precise, according to the type system’s definition of “precise”. In both cases, you need iterative refinement of business logic—but with static typing, you start that refinement much later, hopefully after all the trivial bugs have been ruled out. In order to use static types effectively, you almost have to presuppose that the type system’s definition of “precise” is not getting in the way of iteratively improving—rather, it’s precisely the point from which you want to *start* improving. My view is that in a dynamically typed world, you would be working toward the same solution, starting with less up-front support, but spending less up-front time. It’s a tradeoff, and one that I find worthwhile at least for the kinds of software that I write.
Though that product isn't in Haskell. :-/ (as far as I know!)
[Tweag I/O](http://tweag.io) started with a few of those same engineers (not yet Jeff, sadly), and we're still doing some of the same (kinds of) projects. :) -- Mathieu
Something very similar recently came up on the Haskell Cafe mailing list: http://www.haskell.org/pipermail/haskell-cafe/2014-August/115659.html
I don't think it is misnamed. It is a string of bytes after all, with no designated interpretation.
Bad example code. Why fibonacci? If this is ment to be a safe systems level dsl, I would like to see why it's safe: how it handles null pointers, buffer overflows, etc. Now I got a feeling of yet another useless "systems level" language.
Modules as first class values are a pretty interesting point in the design space, but no, it's not what we opted for. You'd find programming mix-in modules in Haskell pretty awkward anyway, though...
Oops, I messed up. I guess I should s/ByteString/Text/ for the example. It doesn't substantively change the content. EDIT: Actually, easier to just fix that slip of the tongue.
Well, you could manually invoke GHC with the correct flags to make this work (the big one is -sig-of, which lets you compile a signature with respect of an impleentation), but Cabal really does put in some elbow grease to figure out how to link everything together, so you'd be giving that up.
Not sure what the proposal here is.
On the other hand, hasktags will mean that the name in the code will jump you straight to the appropriate hsig file, which might be good enough. I personally really hate managing import lists, and find the marginal utility of making it easy to find where an identifier comes from offset by tool support. It is much better to depend on a minimal subset of an API, agreed, and that's something that we want to improve in the future.
Yes, I realised that after searching a bit on the internet. Sounds like an exciting company, I'll keep an eye on you guys :) Alfredo
Another nit: data ByteString = Empty | Chunk !S.ByteString S.ByteString The last one should be just `ByteString`, I think.
I did find the fibonacci example useful, just to get a feeling of what the language is like. You are right though that we could use some more examples that gradually introduce features and techniques in a less contrived setting. The lack of a fuller set of examples does not in any way mean that Ivory is useless, and I don't understand why you get that feeling. I don't have any details about the Galois customer base, but the feeling I get is exactly the opposite of yours. I am guessing that Galois has some very significant customers who are using Ivory for some very serious real life systems.
Please, please, please correct me if I'm wrong: It feels like we are solving the module problem by introducing header files a la c++. Is a signature file really the only way to disambiguate implementations? 
Yes, fixed.
OK, fixed!
In my opinion either one is just fine. Your explanation is clear and enlightening.
Absolutely.
Less typing too!
Why is this here?
There are certainly some similarities. What would be another way of disambiguating implementations?
No, but you're right that I haven't given syntax for how to do this. Essentially, you need the ability to build-depends on a package multiple times. We're adding support for this using semicolons: build-depends: utilities (Data.ByteString as B1; Data.ByteString as B2), bytestring (Data.ByteString as B1; Data.ByteString.Lazy as B2)
Your question and Michael's answers are very useful. Thanks for re-posting it here.
Sorry "most of (grade school) math notation works".
Really it isn't a problem of static vs dynamic typing, but of how static checking is handled. Deferring type errors means you handle a failed static check with a warning rather than stopping compilation, your program is still statically typed.
I would say it's probably closer to `.mli` files. (Especially since e.g. OCaml has first class modules!) Header files aren't necessarily bad IMHO. M4 for C/C++ has been used to do some less than clean things, so I understand your initial gut reaction of being averse to them. These kinds of signature files are in fact restricted to declaring interfaces, whereas in C/C++ header files can do program-wide things (`#ifdef`/`#undef`/etc.). AFAIK OCaml's `.mli` files by themselves do **not** allow you to do this - you are simply stating an interface and the prerequisites for that interface.
Could the signatures go in the same files as implementations instead of requiring separate files?
It still is used all over the place in APIs as a text format, has built-in compiler support as a text format via `OverloadedStrings`, etc. I understand why `Text` is a much better type for text, but this whole thread seems a little... overblown.
From the author: **David Spivak** I don't have a reddit account, so I can't comment there. But if you want you can tell Reddit that the book will be out in print form soon from MIT Press, probably in October. I've negotiated to have a free html version online (again handled by MITP). Both will have solutions to selected exercises. Other solutions will be available through MITP for professors teaching the class.
Elegant!
Yes, there is not a fundamental reason why this could not be done. https://ghc.haskell.org/trac/ghc/ticket/9256
I did an educational programming environment once, [KTurtle](https://www.google.nl/search?q=kturtle+screenshots&amp;tbm=isch). Learned a ton and had great fun writing it. It started off as a LOGO clone, but I decided to make the syntax more in line with common programming languages and math (similar to the CodeWorld story). I got a lot of negative feedback from the LOGO communities for doing this; to the extend that I removed all references to LOGO. Anyway, I really like CodeWorld!
This illustrates how lazy-I/O is incompatible with manual / prompt resource control. With `unsafeInterleaveIO` (or similar) you leave resources captured / in-use by an expression with a type that doesn't reflect that. IMO, lazy-I/O seems to have little pedantic value. It's simpler than a streaming library like pipes or conduit, but at the cost of correctness. It's not that much simpler than strict-I/O, but with the advantage of streaming. I'd rather tutorials focused on strict-I/O initially, then built something something like `interact` out of pipes/conduit when streaming becomes a focus.
Both pipes and conduit are actively maintained and have good performance numbers. I think pipes is a bit easier to deal with, as it is a bit closer to "first principles" and clearly composed from very small ideas. That said, conduit also has some nice advantages as it is more a "batteries included" solution. Likely you'll be happy with either once your get used to it, so I'd look at the other libraries you will be using (if any) and see if they have an "opinion" on which to use. Also, if you are fine being constrained to the IO monad, io-streams might be both simple and full-featured. It doesn't get mentioned quite as much as the other two, but I've heard good good things about it from someone I know IRL. I would advocate against "hand rolling" until/unless benchmarks point to one of the existing streaming libraries as a bottleneck. Whatever you come up with will likely have a similar API to the existing libraries, so you might as well use a "COTS" solution until/unless you need to bring it "in-house".
If I'm not going to need unicode and I'm trying to be efficient I'm probably going to want the type that uses half as much memory.
Why do you avoid using cabal, or what does cabal not handle about your build process? I once avoided cabal 'cause I didn't want to learn another tool. That was a mistake on my part, I hope you aren't repeating it.
Excellent point! I mentioned it with the purpose of showing the inspiration for Backpack - an orthagonal thought to be sure. I see how that isn't terribly clear.
Besides the differences that /u/Oblomov mentions, we also get the advantage of writing / maintaining the header file in the *consumer* of the interface instead of the *producer* of the interface, if we want. While it is a separate file, it seems a good bit more flexible than C++ header files. It feels to me like we should also be able to say: "Module X.Y.Z is also an interface module; the symbols it exports are exactly it's signature." I don't know exactly how well that works for the naming / linking, but it would be an interesting point of modularity; I could provide a dummy/default implementation (say, good for pendantry, or just small work loads) in my package that uses the interface and provide separate packages that contain more sophisticated implementations for specific workloads (soft-real-time vs. shared-memory-concurrent vs. distributed-concurrency). I suppose the default + other package implementations is also possible with the existing system, I would just have to provide a separate .hsig file and keep it in sync with the default module. Maybe I'm just being lazy, but it seems like this-module-is-it's-signature it nice to have to enable "ad-hoc" modularity and maintain DRY principles.
Lack of heap allocation is going to prevent me from using it for my systems programming. I suppose I could just globally allocate a large array of the queue structures I'm manipulating, but that seems both wasteful and dangerous -- since now I have to maintain a valid/invalid flag (I was previously using non-NULL/NULL). How does this differ from [Atom](https://hackage.haskell.org/package/atom)? I'm guessing more focus on verification. Anything else?
They should make a Haskell compiler (similar to GHC) that compiles Haskell to Ivory. That way the people making the compiler only have to worry about Haskell: take Haskell code and using a Haskell program turn it into a Haskell value that using an external (Haskell) program interprets as C.
There can only be one answer to this. The [github repo](https://github.com/GaloisInc/ivory/tree/master/ivory-backend-c) is there, and contains a C back-end. Fork it and try to come up with a (primitive first) Ada emitter.
I'm not sure. I was just hoping for a DRYer solution. I initially thought it would be something like a language extension or pragma that could be used to decorate functions with alternative type signatures. If what /u/bss03 says about "this-module-is-it's-signature" is accurate or at least feasible, I think I'd be less disappointed. That said, it's interesting work and I enjoyed your last two articles, so don't let my (perhaps dramatic) reaction discourage you.
The lack of heap turns this into a non-starter for me. I was fantacising about the idea of porting some security sensitive application like LibreSSL to it because the process would be more mechanical than most language ports, and it could have the same garauntees about side-channel attacks. But LibreSSL uses the heap. I guess this isn't addressing safety in terms of security because how would you clean up things like session keys if they're only allocating on the stack?
So on a lark, dolio and I chased down the question "when did math first start using parentheses for function application" (which also required of course a notion of a function, etc.). It appears that Bernoulli used symbols for functions first, picking greek letters like "phi" instead of e.g. "f". He used juxtaposition for application (i.e. "phi x") just like Haskell does. Of course he only had unary functions as far as I know... Later on, Euler was the first to use "f" for functions I believe, and also the first to use parens. However the example cited there there was f(x/a + c), so it appears that the use of parens grew out of the natural use of parens for grouping compound expressions to begin with. I'm not clear on the history of the use of parens for functions of arity greater than one. All this, by the way, was greatly facilitated by cajori's incredible "A History of Mathematical Notations." Note that Vol I is free on archive.org, but Vol II is only available on google books (http://books.google.com/books/about/A_History_of_Mathematical_Notations.html?id=bT5suOONXlgC) and is _not_ fully free, because it is a 2007 reprint. But note that the reprint simply consists of the same classic bookplates (i.e. nothing has been reset), so I believe it in fact _should_ be free should anyone working at google want to hunt down the correct team and file a ticket :-).
Post your solutions on your blog and then share the link around. 
There's a very particular situation in which it's very convenient to use `ByteString` and its `IsString` instance, which make it worth keeping: when implementing binary protocols that nevertheless include significant portions of ASCII text. Common examples include HTTP, SMTP, indeed most older internet protocols, and a lot of newer ones, to boot!
&gt; We may add allocation regions in the future; we have verified in Isabelle that they are sound recently. What are "allocation regions," and what does "soundness" mean for them?
&gt; This means Ivory programs can be constructed using the Haskell language as a macro language. This I like. I have long felt that Haskell is an excellent metaprogramming language. However, embedding Ivory into Haskell means that writing Ivory is rather ugly and feels a bit second-class. Comparison and boolean operators carry superfluous `?` and `.` in order to not clash with their Haskell equivalents. Extra pseudo-keywords (not to mention the infamous Haskell `$`) are sprinkled all over the place in order to keep Haskell happy. And so many things are prefixed with `i`, you'd think it was an Apple product. I say break Ivory off into its own language and clean up all the cruft. Make it look more like the C it compiles down to.
you should tell me more at ICFP, since you're gonna be there right? :) 
Oh yes, and "soundness". In that context, it means that we have "progress and preservation" proofs for a (model of) Ivory to show that well-typed programs cannot go wrong. See Pierce's _Types and Programming Languages_ book for details.
Is it just me, or does [the fibonacci code](http://ivorylang.org/ivory-introduction.html#fibonacci) always compute 0? I think they got their initial values wrong: fib_loop :: Def ('[Ix 1000] :-&gt; Uint32) fib_loop = proc "fib_loop" $ \ n -&gt; body $ do a &lt;- local (ival 0) b &lt;- local (ival 0) n `times` \ _ -&gt; do a' &lt;- deref a b' &lt;- deref b store a b' store b (a' + b') result &lt;- deref a ret result 
I was initially horrified that you could turn on an option where `"Γάμμα" :: ByteString` would work and throw away data. I'm *still* not sure it's the right decision, but it is absolutely consistent with the behavior of `256 :: Word8`. I think maybe it would be better to leverage (type-safe) TH or something like it to enable conversion failure to cause a compiler error and data loss to cause a compiler warning, but perhaps that ship has sailed.
&gt; clean up things like session keys if they're only allocating on the stack. `free()` / `delete` is not guaranteed to "clean up" anything, so deallocation from the (C/C++) heap does not prevent these from leaking. Writing a bunch of zero (or random) bytes over a session key (or other sensitive block of memory) can be done both on the stack and on the heap.
We've written an (experimental!) concrete syntax that addresses your points (https://github.com/GaloisInc/ivory/blob/master/ivory-examples/examples/file.ivory). But yes, a standalone compiler is something we've pondered...
&gt; (x :: A) -&gt; B x Where `A :: *` and `B :: Nat -&gt; *` are some sort of type. Notice that that Nat in B’s kind isn’t the data kind promoted version, but just the goodness to honest normal value. Should be `B :: A -&gt; *` 
Typo! Thanks.
Good catch, thank you and fixed :)
Soooo much better. The compiler doesn't necessarily need to be "standalone" to assuage me, you can just make a quasiquoter (or whatever) that elaborates concrete-syntax-Ivory into Ivory-in-Haskell.
Seconded. This is my preferred way to introduce people to streaming IO. The symmetry between the API and Control.Monad is just too great to pass up.
Yeah I have considered it before, but currently I really don't have time. Hopefully soon I will. The biggest challenge is that we don't have a nice representation of Ada in Haskell like we do with language-c.
Basically: if you have `D.B.Strict` and `D.B.Lazy` and you have the ability to import either of them under a different name (eg Data.Bytestring) ... Wouldn't it be sufficient to check if the code compiles? Then you wouldn't have to write additional signature files.
Should also fix the accompanying sentence.
`D1`, `C1`, `S1` are all [type synonyms](http://hackage.haskell.org/package/base-4.7.0.1/docs/GHC-Generics.html#t:D1) around the `M1` type. So, on the value level, the `M1` constructor is used to satisfy these types. `M1` stands for "metainformation", and `D1`, `C1` and `S1` specialize it to metainformation about data types, constructors and field selectors, respectively.
FWIW, here's how I imagined it looking if it were implemented at the module level. This is obviously just idle speculation, so please excuse the handwaving... -- Utils.hs module Utils where import Data.Word require B where data ByteString instance Eq ByteString empty :: ByteString singleton :: Word8 -&gt; ByteString putStr :: ByteString -&gt; IO () blank :: IO () blank = B.putStr B.empty -- Main.hs module Main where import Utils as US where B = Data.ByteString import Utils as UL where B = Data.ByteString.Lazy main :: IO () main = do US.blank UL.blank
I assumed the most common use of this would be for generated C code, not writing large programs directly in the DSL. For this, a clumsy syntax is a more minor problem -- but Haskell's ability to compose things together while maintaining guarantees is extremely useful.
I'm not convinced explaining dependent types by the way they are shoe-horned into Haskell is really the best way. But any explanation is a good thing. :)
Sounds a good place to start then. 
`instance Generic foo` only makes sense for algebraic data types. `Int` and `Integer` are not ADTs, so go figure.
Lack of heap allocation is something that's almost essential in safety critical and hard real time systems. Allocation can take an unbounded amount of time to return a valid location in memory (or may just fail, which is just as bad if you need the memory to proceed), and that isn't acceptable in hard real time systems where you have to be able to guarantee that the runtime of any task is bounded so you can ensure that all deadlines are met. The advantages that can be derived from systems designed like this are nice properties like static schedules so there's never any scheduling overhead for a multatask system (preemption isn't needed basically). For more details, check out the features of the Ada ravenscar profile, which, among others, disallows all dynamic allocations (tasks [threads] can be allocated on the stack at program initialisation), all tasks must loop forever and cannot call any blocking functions or procedures and any constructs that can't be shown to run in bounded time are disallowed (I think). Shame Galois didn't want to give me a job, I'd love to get back into this sort of work.
Ivory could be used to implement a lot of the crypto algorithms in a pretty nice manner as far as I can tell. I know that's the easier part of something like Libre/OpenSSL, but it's be nice to have some stronger guarantees about the crypto itself while expressing it in a (hopefully) clearer way.
The main thing is that cabal only builds haskell, and I have non-haskell components. I also have generated code, I don't think cabal can handle that. Also separate debug, profiling, testing, and optimized build directories. And parallel builds, though I think cabal supports that now, via ghc --make. 
If you want to attract people from dynamic languages you have to demonstrate that Haskell is good at prototyping! Also, this needn't be a "this-or-that" situation. I think it would be nice if (a) import lists are *optional* and (b) there are tools to help manage import lists effortlessly (like that dump-imports flag in GHC that I can never remember).
Yeah I've been wanting this too for a while. Never enough to really make any traction. 
I don't mind keeping import lists optional. I use open imports all the time when I'm prototyping. However, I think that once you "productionize" code that you should begin using explicit or qualified imports.
Nope, but I've not been fair, you're missing some syntax. Here's how I would do it: name: my-actual-project build-depends: utilities (Data.ByteString, Utils as US; Data.ByteString as Data.ByteString.Lazy, Utils as UL) No utilities-strict/utilities-lazy packages needed.
Maybe try fsharpx, or implementing update monads, instead of using f#'s ready access to state. http://tomasp.net/blog/2014/update-monads/index.html
I wish I had a blog. 
I finally got time to improve my little pipes demo, following `snoyberg`s gist. https://gist.github.com/michaelt/88b89b7fcab5bd8a47a7 Here there's no handwritten recursion, just combinators that map closely to those from `Data.List`. I hadn't taken in this bit about grouping, I'll see if I can add that. Edit: it's doing more or less what the python is doing now, see the outputs. The other was this https://gist.github.com/michaelt/88b89b7fcab5bd8a47a7/7e500268ef2569274e108e6f81a693247e5b905a
&gt; `free()` / `delete` is not guaranteed to "clean up" anything, so deallocation from the (C/C++) heap does not prevent these from leaking. I know they don't. I didn't say they did. As for doing it on stack, I suppose you could overload the return functions in Ivory so that sensitive elements get scrubbed whenever functions return.
You've got that backwards. Ivory is a subset of Haskell, and verifying the Haskell code can be valid Ivory and converting it sounds monstrous. Better to develop a concrete syntax for Ivory and translate it to Haskell or C.
Tekmo, if you see this, I finally improved on my initial hack; I think [this](https://gist.github.com/michaelt/88b89b7fcab5bd8a47a7) (modeled on Michael's gist) is not subject to the obvious doubts. I was trying to get a `Data.List` sort of quality; it adds the funny python grouping. {-# LANGUAGE OverloadedStrings #-} import Pipes import Pipes.Group import qualified Pipes.Prelude as P import qualified Pipes.ByteString as PB import qualified Pipes.Attoparsec as PA import Data.Aeson import Data.Aeson.Parser import Data.Aeson.Types import Data.Monoid import Control.Monad.Trans.State.Strict import Data.Text (Text) import Data.Function (on) import Lens.Family -- from `lens-family`or Control.Lens from `lens` import qualified Pipes.Internal as I -- for the missing fold' main = runEffect $ toCount PB.stdin &gt;-&gt; P.print where toCount = folds (\(_,n) (server,o) -&gt; (server,n+o)) ("",0) id . view (groupsBy (on (==) fst)) . concats . maps parseLogin . drops 1 . view PB.lines parseLogin p = do let divided = p ^. PB.break (== 9) . PB.splitAt 80 . to concat_bytes (server, rest) &lt;- lift divided good &lt;- rest &gt;-&gt; P.drain -- drop any excess that comes before tab (me,p) &lt;- lift $ runStateT (PA.parse json) $ skip_fields good case me of Just (Right a) -&gt; case parseEither (withObject "" (.: "tag")) a of Right tag | tag == login -&gt; yield (server, 1::Int) _ -&gt; yield (server, 0) _ -&gt; return () p &gt;-&gt; P.drain where login = "Login" :: Text concat_bytes = fold' (&lt;&gt;) mempty id skip_field = PB.drop 1 . PB.dropWhile (/= 9) skip_fields = skip_field . skip_field . PB.drop 1 -- missing combinator: fold' :: Monad m =&gt; (x -&gt; a -&gt; x) -&gt; x -&gt; (x -&gt; b) -&gt; Producer a m r -&gt; m (b,r) fold' step begin done p0 = loop p0 begin where loop p x = case p of I.Request v _ -&gt; I.closed v I.Respond a fu -&gt; loop (fu ()) $! step x a I.M m -&gt; m &gt;&gt;= \p' -&gt; loop p' x I.Pure r -&gt; return (done x, r)
For me, [equational reasoning](http://www.haskellforall.com/2013/12/equational-reasoning.html) is the beautiful part that I believe other languages miss out on.
Cool stuff, but... &gt; Explicit usage of await and yield will immediately kick you back to non-fusion This is very irritating. Await and yield are the bread and butter of defining your own conduits. This is like saying you should avoid using direct pattern matching on lists in order to take advantage of list fusion. It may be true, but it's annoying. One wishes for a "sufficiently smart compiler" to figure out such optimizations without having to avoid such obviously useful tools.
Yup, true. That's why stream fusion isn't the only optimization I'm pursuing in conduit right now, it just happens to be a very powerful one for a large subset of workloads (the ones that can be expressed in terms of existing combinators).
I second this. Fsharpx has a lot of really handy wrappers around .net classes and some common functional patterns, like flip and curry. 
I'm also a daytime .net developer who's learnt Haskell and F#. My biggest takeaway from functional programming is the huge benefit of separation of IO from logic. All IO should be short wrapper that does the IO and calls off to the business logic. It also helps show you how isolation of state change makes it much easier to understand what a function does. 
When I do "advanced type magic" in Haskell, I've usually protoytped in Agda already.
Im in almost the same situation: I work as a .net developer (mostly F# - I was lucky). I love Haskell and aside from teaching me humility time and time again - it's great to try out interesting type-*stuff*. True you cannot implement everything in F# in the same great manner but for example I now use things like the Maybe-Monad, Functors, Applicative-Functors, etc. all the time in my F# stuff (the first is part of FSharpX, the rest you have to reimplement for your structures or try something like the FsCheck approach) For the Web/Serverstuff: maybe I'm wrong but I thing this kind of stuff will never feel really "functional" - it's just the infrastructure where the IO has to happen - it's always more or less boring boiler-plate stuff and it's akward in every language (yes even in OO) ... ofc the same is true for database-access and sadly that is exactly the kind of stuff you'll do on your average work-day :(
I do manage to reuse its version solver, by running generating a .cabal file, running cabal configure, and then using Distribution.Simple.Configure.getPersistBuildConfig. So it is reusable, with a little effort (and a speed hit, dist/setup-config is huge and the parser uses Strings). Anyway, a few strategic features can go a long way to enable interoperability, and would be much appreciated.
&gt; It could be enough if cabal could be run in a mode where it only prints the linker or compiler flags, then I could reuse the elbow grease. Perhaps cabal-cargs[1] is of help. [1] https://github.com/dan-t/cabal-cargs
the (imo) most important point of peakers comment seems to have been missed in the discussion so far. &gt; Additionally, importing signatures (EDIT: wholesale from packages) will encourage depending on much more of the API than you actually depend on. &gt; &gt; May be nice if packages depended on the minimal subset of each API? what's the plan to tackle it? 
This doesn't translate to a specific rule, but I found that LINQ helped me understand monads and vice versa. Not just "LINQ is a monad" but how it is actually implemented as a DSL in C#. 
This is something I would also love to do.
You can learn design of efficient immutable data structures, ones that are easy to work with in single and multithreaded context. I've not seen such structures in .Net. If you walk over enumerator from dictionary, you cannot modify dictionary (which often is the point). This complicates design and requires more locks for several threads than needed. My own design approach like the following: I see my structure as an IORef InternalStructure. My dictionary has only one field (just like IORef), which is modified in IORef/MVar modification style. Everything else there is immuitable and created anew if needed.
For me Hasktags doesn't work that well. It fails to jump to symbols accurately. As others said, optimizing for readability is more important than writability. And lastly, what about the other part of my comment: Depending on wholesale signature packages will mean that API dependencies become much more coarse-grained than they could be...
Yes, that seems easier.
Have you seen John Carmack's keynote from Quakecon 2013? He talks about functional programming and specifically Haskell in game development. https://www.youtube.com/watch?v=1PhArSujR_A
Thank you for the link. I'll be sure to watch it before bed.
There's a bunch of people working on it that I know of. Maybe it'd be worth starting up a mailing list for it? Or maybe I should just join haskell-cafe.
There is the haskell-game IRC but literally no one uses it. /r/gamedev is not exactly a place for this either. I would like to see something like this get off the ground on github or get a mailing list started. I know Helm exist but that seems pretty dead or at least not very active.
This is slightly tangential, but the author of [es_core](http://ttimo.typepad.com/blog/2013/05/es_core-an-experimental-framework-for-low-latency-high-fps-multiplayer-games.html) was [experimenting](https://github.com/TTimo/es_core/tree/master/network/haskell) with 'scripting' (high-level logic, at least) in Haskell. I don't think that went anywhere, but it's an interesting idea, especially for a game engine focused on performance and latency issues.
Wow. She is adorable.
The whole point of scripts is that the user doesn't have to build them.
Who said you can't do that? with GHC it's not that difficult to set-up an interpreter and there a few libraries that help with this. 
In my day-to-day Haskell code I very rarely pattern match on lists. Not that I go out of my way to avoid it, I just don't feel a need for it.
Wow! I missed that! It's gold to see someone like Carmack having a speeach about Haskell as a better alternative to C/C++.
That's a lot of new syntax! :)
&gt; The "No Side Effects" functions "rule": What rule of thumb percentage of the application should this be? As you probably know, Haskell makes its type system act as something of an effect system as well. If something depends on reading in or writing out and generates a value of type a, its type is IO a. If the question is "how much of your program should live in IO", the answer is "the minimum amount that can". Actual IO should live in IO. Calculations/logic should be pure functions of your input, and called by your IO action. Incidentally, this makes programs easier to test. Different programs do different amounts of IO. A web server is going to do more than, say, a compiler would. There is no rule of thumb percentage.
Nothing against adorableness, but just FYI, the habit of commenting on women's appearance in contexts where it's totally irrelevant is a kind of "microinequity." Though each individual occurrence may be well-meant, they contribute to a climate that subtly devalues and discourages female participation. [Check out this document about "subtle ways in which women are often treated differently at work and in classrooms."](http://www.napequity.org/nape-content/uploads/R1l-The-Chilly-Climate.pdf)
Nice to see streams coming up in other sequence processing situations. They seem to be finding lots of interesting applications to eliminating allocation points between sequential processes.
Under what arguments you believe it is "slow" and "uses a lot of memory"?
Yes, I for one was always told how dependent typing allows for neat things such as that "ordered list" type, but I've for ever been waiting to see how it can actually be done. I still don't get on my mind what can actually be encoded. I have no idea how useful/easy it actually is to "encode your specification" as a type, then start programming. What I mean is: can I go as far as encoding the actual specification of my software, as it was sent to me - say, "it is game with creatures with so and so qualities and red buttons and collision checking" - and only then start coding?
The four big things that I can (somewhat) take away from Haskell to other languages I work with are: * **Immutability.** If you don't change things, figuring out how your code works is much easier, simply because it will work the same way every time it is run. Executing something like `queue.pop()` might work the first 526 times you run it, but then suddenly the 527th time it crashes with an exception. That just doesn't happen when your queue is immutable. `pop(queue)` will return the same thing every time. Only when you give it a different queue will it return a different answer. This greatly simplifies testing, concurrency and generally just figuring out how your program works. To have really successful immutable composite data structures though, you need what is called *persistent* data structures. None of the default collections in languages like Java or Python are persistent, so if you want to use that in those languages you'll have to look for a library that provides them. * **Functions as values.** Really. I use functions way more now than I ever did before. Functions that return functions. Functions that modify functions. Functions that combine functions. And so on. This depends slightly on language support though – not all languages support functions as first class values, and even fewer make them convenient to work with. * **Separation of functions with side-effects.** This is something you *can* do in other languages, but you get no help from your compiler. When you limit the amount of effectful code you have, your program becomes much more manageble because you decrease the risk of mashing the wrong things together. When you combine functions with side-effects, you might run into a problem where one function expects the other to run first and you didn't think about that. Stuff like that never happens with pure functions. * **A powerful type system.** This is wholly dependent on the language and as such is not really anything you can take with you when you work in another language, but it's an important point anyway because it shapes demand for type systems. With an expressive type system you can encode many more guarantees about your programs while doing *less* typing than you would with a less expressive type system. A lot of boilerplate goes away with a powerful enough type system, and you can feel safer about your program too!
If you work in an impure functional language, there is always the risk of Dictionary.size being an impure function, which also writes to a log variable, which isn't initialized, so a log file has to be created on the disk etc. Haskell functions are pure and lazy, so you don't need to worry about that - and more importantly, the compiler and runtime environment can decide whether the code needs to be run at all, in which sequence, and whether some of it can be run on a different core etc. What you model as for-loop in Java, as an inject: loop in Smalltalk or as a Lync statement in .Net, may be expressed as a map in Haskell - and can be optimized. If you see that it follows certain rules, you can state that it is an Applicative, and the compiler will understand that this can be distributed across all the cores you have. By analyzing in these terms, your code ends up with a totally different structure. (Look into the Typeclassopedia). To add large functionalities like generic traversals to your types, you declare that they are of a certain class and add some small functions, and you are done. Weird magic.
Thinking about and [implementing](https://github.com/mattgreen/elevate/blob/c2f41ecab2642e0256f89b8eb9b03f057a3cc54a/lib/elevate/future.rb) Futures was a step towards understanding monads for me. The fact that the Future represents a computation that has side effects -- namely, that it is literally *from the future* (which might not be here yet) -- was helpful in understanding that monads let me instrument computations within a certain context. I don't consider my understanding complete, so be careful with my explanation. :)
So if I understand correctly, `hadron` is a wrapper around the `hadoop -fs cat` shell command. The advantage of this is simplicity of implementation (you can easily reimplement `hadron` yourself in any streaming library that supports streaming from shell commands) and this is perfectly suitable for many use cases. The disadvantage is that this conflicts with one of the core principles behind hadoop: it's more efficient to move the code to the data, and unfortunately we're still stuck with JVM-based languages if we want to do that.
If you can find a Haskell interpreter/compiler that is suitable for embedding, then sure! Unfortunately there are very few languages that have a quality interpreter that embeds nicely and efficiently. Lua, Javascript, squirrel and guile do. Almost all other languages runtimes are either very difficult to work with, or become straight-up unusable once you start applying some criteria that that are common requirements for game frameworks (enjoy trying to embed e.g. cpython or cruby into your Android app, not to speak about threading) But maybe there is some configuration that can be made to work.
Perhaps [this stackoverflow answer](http://stackoverflow.com/a/10659438/828361) might give some indication of how to implement ordered lists in a way that one can work with, at least to some extent. [Edit: apologies for shameless plug.]
&gt;I've adamantly tried to avoid IRC for a long time but I'll give it another go. Thanks for the info. A shame. I don't use IRC much myself, but the people in \#haskell at least are always helpful and probably will stretch your brain in the process 
If you've already read through LYAH and feel comfortable with the basics, then reading the typeclassopedia (http://www.haskell.org/haskellwiki/Typeclassopedia) can give some insight into the kinds of abstractions that haskell allows - some of these abstractions are difficult, inconventient, or impossible to realize in other languages. Many of these are actively used in haskell programs.
She is brilliant, her appearance is irrelevant and should be treated as such in a professional setting
"If *n* programming models aren't burdensome enough, we'll go inductive and try *n*+1!"
&gt; AFAIK JavaScript in most modern browsers is compiled in a just-in-time way (a bit at a time, not a whole module at a time) to machine code. In [Google Chrome](http://blog.chromium.org/2014/02/compiling-in-background-for-smoother.html) Javascript can actually potentially be compiled *twice*! &gt; In V8, pieces of code that are executed very often are compiled a second time by a specialized optimizing compiler. This second compilation pass makes use of many advanced optimization techniques, meaning it takes more time than the first pass but delivers much faster code. 
I think that such a sufficiently smart compiler can exist. I've been toying with strongly normalizing term rewriting systems (i.e. the lambda cube without recursion) where all recursion is transformed to non-recursive anamorphisms or catamorphisms. Normalizing expressions in this language corresponds to the stream fusion optimizations without the need to specify any rewrite rules. Anyway, the whole thing is not completely baked and I'll comment more on this when it is more fully fleshed out.
I find a snuggly bear adorable too. Just FYI.
Hello, I tried running your code but got this error: [1 of 1] Compiling Main ( hw2gitit.hs, hw2gitit.o ) hw2gitit.hs:203:25: Not in scope: ‘defaultParserState’ hw2gitit.hs:208:34: Not in scope: ‘defaultWriterOptions’ shell returned 1 Any idea why this might be happening? Do you still happen to have a copy of the HW there? 
I believe you can make it work with something like Hadoop streaming. Hadoop streaming makes it possible to define your mapper/reducer in any language and then expose it to Java via IPC (over pipes).
By this do you mean optimization in total languages which cleanly separate data/codata?
Hadoop streaming mode works more like /u/tibbe is describing. You have the fun of making sure that your program can run on every map and reduce node but it does the right thing in that it ships your hopefully small program to the data.
That notion of having multiple `restore`s threaded through and properly layered feels like it ought to be captured in types, but it also feels like the domain of linear types. Is there a (perhaps heavyhanded) way of encoding that invariant in a monad?
Note: I'm the author of hadron. Hadron is a Haskell wrapper around Hadoop Streaming. It tries to help on two main fronts: a) Write "MapReduce" constructs with some more structure than the prelude "interact". There are a lot of little details here to get right in order not to offend the Hadoop Streaming assumptions and hadron helps with that. It also gives you some combinators along the way. b) Given a chain of map-reduce jobs, centrally orchestrate them so that you don't have to manually type 4-line-long hadoop commands (one for each step) on the console yourself. As part of (a), it made sense to pick a streaming IO library to use as the main interface. I happened to pick conduit at the time, but any one of them will do. In a way, if you're trying to write Haskell for the Hadoop Streaming interface, you will most likely end up with some reusable machinery to make it manageable. hadron is what came out in our case. Edit: Also note that I'm not sure if your point on data locality is valid. In most cases, data is on a distributed system anyway (HDFS, S3, etc.) and each node will pull this data, incurring the network overhead. After that, your code either runs on the JVM or via the interact-like Hadoop Streaming interface. JVM does avoid another layer of stdin/stdout indirection, but the difference is probably not as large as you might think as the big cost is that initial data shuttling step.
Yep, this is precisely what hadron does.
Exception handling in Haskell is way too complicated. I don't know of any language that even approaches the same level of complexity here. Even C does this better and it doesn't even have exceptions! Having to worry so much about incantation order as described in this tutorial is bad. I don't know how to fix this, but I'm guessing it would have to do with expanding Either-based error handling wrappers, and then promoting them to the Prelude, and then updating every Haskell error handling tutorial accordingly. Edit: Thank you all for your intelligent responses! My comment was clearly a result of my ignorance on the matter and thanks to you I'm now much more aware of a problem that is just papered over in other languages. Conversations like these are what make me appreciate the Haskell community.
Half-baked idea: Why not pass in the `restore` function from `example5` as an argument to `openHandshake`? That way, you don't create a complete hole in async exception handling, but just enough of a hole to get back to the original masking state.
How _do_ other languages deal with asynchronous exceptions? I don't think the problem is inherent to Haskell. If I write in pseudo-Java dbConnection = DB.openDB(..) catch { .. } finally { dbConnection.close(); } How do I make sure that no exception (like a timeout, think of a web server for instance) can happen after the call to `openDB` but before the start of the `try` block? I suppose one option is to do something like dbConnection = nil catch { dbConnection = DB.openDB(..); .. } finally { if (dbConnection != nil) dbConnection.close(); } Although this too make me somewhat nervous; what if DB.openDB created the database connection, but the timeout happens before we manage to assign it to dbConnection? Either way, here too you have to be careful with asynchronous exceptions (and this solution, of course, relies heavily on having mutable state -- if it is even correct at all).
I like to think I follow this principle already. The Single Responsibility Principle is with me wherever I develop, and this would appear to be the same principle in action. I look forward to seeing if I am in line with how Haskell/FP does it. 
I will give that a go, thanks!
That's mentioned in the conclusion but stated as error prone due to potentially messing up the argument passing. Thus the desire for some kind of typing regime there.
Noted, thank you.
I completely missed that, thanks for pointing it out. I have to agree: it *is* very error prone. I've used that in some libraries, and I think it's gives the best semantics, but it's easy to accidentally screw it up.
I would rather choose Lisp for game scripting. Small, simple, you can implement it yourself, it is so easy to learn...
Let's take a simple example, the expression `x:xs`. That is, we need to construct a cons cell given that we already have `x` and `xs`. Currently that requires 3 store instructions (tag, pointer to `x`, pointer to `xs`) . (It also requires a heap check and bumping the heap pointer but that can usually be amortized over several allocations; worst case a register-register compare, a non-taken jump, and a constant-to-register add.) For a hash cons, you'll have to look up the `x` and `xs` pointers in the hash table for cons. This overhead of this is going to be enormous compared to just the stores, since it will involve several loads per pointer (and loads are latency sensitive, unlike stores). The space overhead is the hash table you need for each constructor and function (they could possibly be combined into one big hash table). Further overhead, if the hash table holds normal pointers to the arguments and result you will leak space all the time since you can never release anything. So the hash table has to hold weak pointers. This means that after each GC the hash table has to be processed to handle the lost references. You're not the first person who has had the idea of hash consing everything. The reason we don't do that is that no one has been able to do it efficiently and there are good reasons to believe it can't be done efficiently. 
I'd say there will be no need once Agda is written in Agda.
Apart from being error prone, it also changes the public API. Essentially any resource allocation function that is part of the public API will now need to take an additional argument (if we want it to be interruptible). This may not be a problem, but it's not an idiom that is commonly used, obviously; and since we already have interruptible primitive operations anyway (takeMVar and co) where we take this behaviour for granted, it seems to be reasonable to make other resource allocation functions interruptible too. That said, I sort of agree with @tel in the sense that this kind of feels like something that ought to be expressed in the type. It's almost a different kind of side effect. 
Java's been doing the same thing since 1999. Edit: Referring to HotSpot VM, released in 1999, which does the same thing as V8. "Hot spots" in the program are compiled a second time with aggressive optimization settings informed by profiling data collected while the code was running, hence the name of the VM. Old Java implementations were both compiled and interpreted: the Java source code was compiled to byte code, which was interpreted in software. This is a common technique used by e.g. Python, or numerous Lisp implementations, the only difference with Java is that with Java, the compilation to byte code happened ahead of time.
I have duplicate identifiers and qualified name use which doesn't work well iirc. 
Yes. Note that totality is necessary, but not sufficient. A good example of this is Idris, which has a total subset that is not sufficient for my purposes because (as far as I can tell) the compiler does not preserve enough information for backends to strongly normalize computations over codata. Specifically, I cannot (easily) transform a coinductive function to the corresponding anamorphism. The anamorphism is what I strongly normalize, which is why I'd like a compiler that exposes recursion as catamorphisms and anamorphisms to make it easier to optimize on the backend.
Well... You *could* dedicate a portion of memory to GC allocs and then be *really* careful about strictness &amp; mem usage, but yeah... writing it in Haskell would be hard.
Having never done it before what kind of metainformation is needed to transform named (co)recursion to a plain nu/mu? Presumably the totality checkers always have all of that laying around even if they end up throwing it away.
I just spent 15 minutes looking this up. That's bizarre! I had no idea that this didn't exist anywhere else. I'd discovered the idea in Haskell and Erlang and just never thought carefully about it again. http://stackoverflow.com/questions/13506900/java-asynchronous-exceptions-can-i-catch-them http://docs.oracle.com/javase/1.5.0/docs/guide/misc/threadPrimitiveDeprecation.html
This is the part where my idea is not fully baked. For now I've been writing the anamorphisms and catamorphisms by hand, but I have a rough sketch of an idea for how to mechanically translate (co)recursion to (ana/cata)morphisms. I believe the trick is to translate the (co)recursive program to an NFA or DFA (not sure which) where nodes are (co)recursive values/types and edges are the non-recursive steps between them which the compiler knows. Then you can use something like [Kleene's theorem](https://en.wikipedia.org/wiki/Kleene%27s_theorem) to translate this mutual recursion into simple recursion, where the Kleene star is mu/nu.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Kleene's theorem**](https://en.wikipedia.org/wiki/Kleene%27s%20theorem): [](#sfw) --- &gt; &gt;In [theoretical computer science](https://en.wikipedia.org/wiki/Theoretical_computer_science) and [formal language theory](https://en.wikipedia.org/wiki/Formal_language_theory), a __regular language__ (also called a __rational language__ ) is a [formal language](https://en.wikipedia.org/wiki/Formal_language) that can be expressed using a [regular expression](https://en.wikipedia.org/wiki/Regular_expression), in the strict sense of the latter notion used in theoretical computer science. (Many regular expressions engines provided by modern programming languages are [augmented with features](https://en.wikipedia.org/wiki/Regular_expression#Patterns_for_non-regular_languages) that allow recognition of languages that can not be expressed by a classic regular expression.) &gt;Alternatively, a regular language can be defined as a language recognized by a [finite automaton](https://en.wikipedia.org/wiki/Finite_automaton). The equivalence of regular expressions and finite automata is known as __[Kleene](https://en.wikipedia.org/wiki/Stephen_Cole_Kleene)'s theorem__. In the [Chomsky hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy), regular languages are defined to be the languages that are generated by Type-3 grammars ([regular grammars](https://en.wikipedia.org/wiki/Regular_grammar)). &gt;Regular languages are very useful in input [parsing](https://en.wikipedia.org/wiki/Parsing) and [programming language](https://en.wikipedia.org/wiki/Programming_language) design. &gt; --- ^Interesting: [^Regular ^language](https://en.wikipedia.org/wiki/Regular_language) ^| [^Kleene ^fixed-point ^theorem](https://en.wikipedia.org/wiki/Kleene_fixed-point_theorem) ^| [^Kleene's ^recursion ^theorem](https://en.wikipedia.org/wiki/Kleene%27s_recursion_theorem) ^| [^Kleene's ^T ^predicate](https://en.wikipedia.org/wiki/Kleene%27s_T_predicate) ^| [^Smn ^theorem](https://en.wikipedia.org/wiki/Smn_theorem) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ck300a3) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ck300a3)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I think Ulf said to me once that self-hosting is not really a big deal for him.
Haskell is a bit more practical a language imo, and I actually don't know anything about compiling to Haskell.
Plus arithmetic in Agda is linear! That's terrible!
Except Java's never claimed to be an "interpreted" language, yes?
I think the biggest thing you can take away from Haskell is looking at how things compose - and then how things like purity and polymorphism contribute to that composability. "Functions as data" doesn't mean using functions where simple data would do, but passing around and manipulating functions like you would any other data. For example, sortBy :: (a -&gt; a -&gt; Ordering) -&gt; [a] -&gt; [a] just takes a plain function, instead introducing some wrapper like `Comparator&lt;A&gt;`. Getting farther into things, functions can transform or combine functions to help you assemble the operations you really want. There's a function `on` (in `Data.Function`) and the monoid operator `&lt;&gt;` (in `Data.Monoid`) can be used at type on :: (b -&gt; b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; a -&gt; c (&lt;&gt;) :: (Monoid m) =&gt; (a -&gt; b -&gt; m) -&gt; (a -&gt; b -&gt; m) -&gt; (a -&gt; b -&gt; m) so you can get a "shortlex" order by (compare `on` length) &lt;&gt; compare (taking advantage of the monoid `EQ &lt;&gt; a = a; o &lt;&gt; _ = o` on `Ordering` and the pure lexicographic order on lists used in `Ord [a]`). The general idea is that some things can be done just by passing around functions, and then you can use generic operations on functions (like `on`) to help deal with them. But then again, maybe the bigger idea here is that orders are monoids and should be made usable with monoid operators. You could define orderings in a similar way in some `Comparator a` type if you gave it an explicit `Monoid` instance. You'd need a specialized type if you wanted to let some orderings make explicit that idea of a "key" (like length) that it might be beneficial for a sort to pre-compute and cache. Given no side effects and laziness, String is almost identical to (() -&gt; String) - you know you'll get the same string every time, so the only possible difference is in memory use, and even then a String can also be a small unevaluated thunk until you start using it. "No Side Effects" seems to be almost entirely orthogonal to the "Single Responsibility Principle", at least as I think it's usually understood. If you have a single piece of code or object that's responsible for managing some kind of mutable state like a database, is it a violation of the single responsibility principle to make requests of that single responsible party from lots of other places in your code? Conversely, you can clearly violate the single responsibility principle and decent design by scattering tons of implementations of something like string escaping throughout a program, without using any side effects at all.
Hmm ... I see. Although I could try to compile against the latest one ;)
Trying this now, but having the same problems module Test where import Web.Scotty import Control.Concurrent import Control.Concurrent.Async import Network.HTTP import Network.HTTP.Types.Status web_service = scotty 5000 $ do get "/" $ do status ok200 main = do withAsync web_service $ \_ -&gt; do threadDelay 1000000 print =&lt;&lt; getResponseCode =&lt;&lt; (simpleHTTP $ getRequest "http://localhost:5000/") Consistently results in: Ok, modules loaded: Test. λ&gt; main Setting phasers to stun... (port 5000) (ctrl-c to quit) *** Exception: connect: does not exist (Connection refused)
Had problems with this approach: http://www.reddit.com/r/haskell/comments/2ei7gk/im_having_trouble_with_basic_concurrency_forkio/ck32ajf
A socket.io implementation in haskell: https://ocharles.org.uk/blog/posts/2014-07-13-announcing-socket-io-for-haskell.html
Have you seen this? https://ocharles.org.uk/blog/posts/2014-07-13-announcing-socket-io-for-haskell.html
Read ninereeds314's post above mine. Their comment hits the nail on the head. &gt; For the most part, compiled vs. interpreted isn't really a feature of a language. It may be a feature of the currently available interpreters, but not really an inherent feature of the language. Although when Java came out, it *was* interpreted. It was just compiled to bytecode first. Just like Python, or lots of Lisp implementations, or Smalltalk implementations, etc. That's why Java *used* to have such a reputation for being slow. The assumption that Java bytecode is interpreted rather than compiled is reflected very clearly in the way the encoding was chosen. For the best example, look at the way addition is encoded: the operand types are encoded in the opcode values, which isn't necessary because the operand types can be derived, but it makes interpreters faster. Bytecode systems that are assumed to be compiled, such as MSIL, use the simpler encoding.
This ended up not working. I tried attempting the request with `forever` but got an IOException every time. Meanwhile I can hit the URL with cURL and get a response, so I know the web server is up. (I also added yields, and tried with +RTS -N)
I'm using `cabal repl +RTS -N` does that do the same thing?
The problem is asynchronous exceptions, which sound like a neat hack on the surface, but are unusable in practise.
I didn't expect it to fix the problem of not being able to connect, just to ensure that your web service was reliably shut down. I think my comment about [setBeforeMainLoop](http://www.reddit.com/r/haskell/comments/2ei7gk/im_having_trouble_with_basic_concurrency_forkio/cjzt1w5) is the right way to solve that problem.
Not really satisfied with any of these answers, so I'll try to be more precise. **As far as functional programming is concerned, there's little new here.** You're well familiar with it from F# and C#, even in C# 2.0 Windows Forms needed you to pass around delegates. The particular differences in Haskell are a little more obscure than a distinction between "functional programming" and other paradigms. Basically the major differences, from most significant to least (in my opinion) are: * **Lazy Evaluation by Default** - This is the biggest hurdle to overcome as it ties data into execution, and leads to counterintuitive results like `foldl` being worse than `foldr` despite `foldr` not using a tail call. The only way to really get a feel for how this works is with practice, but the key point to know is that you won't allocate for unneeded values, but you will allocate for information that a value isn't needed yet, and then that information on what wasn't evaluated can grow pretty fast and you may need to force evaluation to guard against that buildup. * **Higher-Kinded Polymorphism** - While C# and F# allow you to make polymorphic functions in the form `T&lt;R&gt; f&lt;U,R&gt;(T&lt;U&gt;,Func&lt;U,R&gt;)`, you can't do ones in the form of `T&lt;R&gt; f&lt;T,U,R&gt;(T&lt;U&gt;,Func&lt;U,R&gt;)`. Haskell lifts this restriction, allowing for certain functions like `fold` to be generalized accross any data structure that implements an interface, wheras in F# and C# an appropriate interface can not be written because all parametric polymorphism is restricted to types of kind `*`. You can be polymorphic only on fully instantiated types, you can have functions that work on `List&lt;String&gt;` and `List&lt;Double&gt;` but not ones that work on both `List&lt;T&gt;` and `Dictionary&lt;T&gt;` where `T` is a generic argument to the function. You need to reimplement things for both lists and for dictionaries. This isn't a problem too often but it really hurts where it shows up, and is part of why patterns like `Functor` and `Monad` do not shine in C# the way they do in Haskell. You would get less functionality for free just by implementing `Monad` or `Functor` as those are higher kinded interfaces in Haskell and you'd need to ad-lib a `*` kinded version of them somehow (as has happened with LINQ and computation workflows). * **Manifest Effects** - In C# and F#, effects such as printing to the console are magical, so to use them you need to return dummy values like error codes or `void`/`unit`. In Haskell, effects are values just like anything else and instead of returning a dummy value that runs an effect magically when evaluated, you return an effect and run it when you want to. This also allows you to take effects as parameters and do things with them; or store them in data structures like lists. * **Parametricity** - Haskell does not by default allow you to reflect on type information or upcast things up to object. This means you can't really rely on structural equality or object hash codes to work with any type. However it also makes types more meaningful; in Haskell `a -&gt; Int` can only mean `const 0` or some similar constant function, it cannot in any way depend on its input, and this is known statically. This actually means that it is possible to use logic to discover code that acts as an implementation of a type, allowing you to write a type first before letting an automated tool guess an implementation. * **Referential Transparency** - There is no way to observe a difference between passing values and passing references in Haskell. This is nice because bugs where the two get confused can be pretty subtle. * **"Equational Reasoning"** - Functional programming in its original sense was building a functional model with pen and paper of what functions various features needed to be able to call, and then what functions those needed to call, and so forth, and only once that was sorted out commiting it to FORTRAN. Haskell is particularly well suited to this kind of software architecture, you can straightforwardly go from the model on paper to Haskell code and back, and do all the algebraic manipulations from high school math on paper to simplify or inspect your code. This all applies to vanilla Haskell, GHC tweaks this in various ways. As a .NET programmer you're in a good position to learn Haskell, F# and C# were both heavily influenced by Haskell and have improved on it in some ways. Haskell is a slightly simpler and more coherent language than the pragmatic and large C# and F# languages, so if anything it should be simpler to learn. The main hurdles are the lazy evaluation and the manifest effects, the manifest effects are easier to deal with when you realize it's just like shell programming with pipes.
You'd actually probably use some kind of embeddable Scheme. One that already exists in Haskell is [Husk](http://hackage.haskell.org/package/husk-scheme); you can simply import that and use it as a scripting language. [GOAL](http://www.neogaf.com/forum/showthread.php?p=5379913), or Game Object Assembly Lisp, is very cool, but it is proprietary and (even if it weren't) highly geared towards Naughty Dog's particular workflow and hardware while developing for the PS2. In particular, it was called _Assembly_ Lisp because it unified the assembly notation for all the disparate kinds of assembly included in the PS2 and allowed you to freely intermix assembly and Lisp code, referring to Lisp variables in the assembly and to registers in the Lisp code, and had language features like `rlet` which is a let that specifically guarantees the bound variable will stay in a register. So even if you could use it, GOAL is heavily geared towards writing the actual game engine and assumes your game engine is gonna be on the PS2. It's not the kind of Lisp you'd use for scripting. Stick to Husk or Guile or what-have-you for that. 
No other language has asynchronous exceptions, except for Erlang (but I believe in Erlang asynchronous exeptions need to be caught by a separate process, whereas in Haskell an ordinary exception handler can catch asynchronous exceptions). In C you simply can't kill a thread safely, whereas in Haskell you can. Timeouts work using the same mechanism, which is very powerful.
Happy to help. Let me know how that goes and if you would be interested in collaborating. There are several opportunities for improvement in hadron, particularly around a few major API design / usability shortcomings.
This is true, but my question is: Why is it exactly `D1 D_Int (C1 C_Int (S1 NoSelector (Rec0 Int)))`? For example, why do we announce C1 and S1, when `Int` has no constructor and no accessors? Why do we not go straight for `Rec0 Int`?
Nope, as written it is correct. Remember you are renaming the hole!
Maybe Raymond is referring to her idea of strengthening Java types by wrapping each type of string in a unique class. Such commitment to monkey patching FP concepts into a language that really doesn't want them is adorable.
I'd wager that unix signals with their async-safe handlers are asynchronous exceptions. They're even more complicated though.
Counting stars isn't always very useful. An alternative metric is to consider rev deps, e.g. the score measured by bos's recent tool [packrank](https://github.com/bos/packrank).
ghci uses a bytecode by default.
I have to admit that while my gut said that Erlang's approach would not be considered safe by Haskellers, I am unable to come up with a concrete scenario that fails to release resources. Of course Erlang as a whole has numerous unsafe aspects to it from a Haskell point of view, but this may not add anything to that set.
Ahah thanks, I edited. I'm new on reddit and realized you need to whitelist JS to submit. -ETOOMUCHDISTRACTION
I find myself agreeing with both you and Oblomov. The Java runtime has a fixed JVM bytecode standard to deal with whereas JavaScript runtimes have the original source code available and can choose whatever intermediate representations suit them. So the bit about Java never claiming to be interpreted is possibly relevant. Though also, in a sense Java *was* originally interpreted - at least the bytecode. Assuming you're still referring to instrumented builds (first compile from bytecode to native code) followed by profile-guided optimized rebuilds (second compile from bytecode to native code) as the two compilations, you're probably right. If you mean from source to bytecode (first compile) then from bytecode to native code (second compile), that certainly did a lot to speed up Java, but a traditional compiler can spend more time on heavier-duty optimizations. Adding the profile-based optimizations changes that - the bits of relatively-heavy-duty compilation are targeted where needed, and by being guided by real use can be generate better optimized code too. I think the profile-guided stuff was added to Java JITs pretty early, possibly as a key feature of one of the first JITs (by IBM?). 1999 could be right - I'm not sure. Sorry for being too tired and lazy to look things up. 
I didn't know that. I know about a few intermediate codes (Core obviously, Spineless Tagless G-Machine (possibly a bytecode?) and C--. Despite some early over-ambitious attempts to learn compiler internals, my limit at present is mostly a vague understanding of Core. I really did think GHC compiled to actual native code by default, so appologies if it's actually some kind of VM-and-bytecode system. On the plus side, that should be more convenient for the game-scripting-language context. 
I'm referring to the HotSpot VM which was released in 1999. The name "HotSpot" even derives from the description of this feature, where it more aggressively optimizes program "hot spots" identified at runtime, after a first compilation. https://en.wikipedia.org/wiki/HotSpot It almost sounds like you're implying that I disagree with Oblomov, but I was just adding to the discussion.
I'm so bad at managing all these subsets and dialects of Haskell via Hackage lol. I see what you mean. I'll look into all this stuff later today after I finish up some current projects.
I have, but I couldn't find any tutorial?
Isn't this example sufficient to start to play around https://github.com/ocharles/engine.io/blob/master/examples/chat/Chat.hs ?
If you say so, it should be... but why is the minimal example 87 lines long? Socket.io's site example is ~10 lines... there are a lot of things there that I still haven't learned.
I was only disagreeing with the assertion that "Java never claimed to be an 'interpreted' language", since to people who used it in the 90s, it was well known to be an interpreted language, unlike C. Of course, C can be interpreted as well.
There's a common misconception that you can't have mutable data structures or variables in haskell - you certianly can. http://www.haskell.org/haskellwiki/Mutable_variable describes both simulating mutability and using actual immutability inside the IO monad.
&gt; The first two seems essentially the same -- kill a thread. For this you do need some way to signal the thread, but it need not manifest as an exception. You can do this, but it also means the endpoint thread has to poll something like an `MVar` until you signal it, and handle appropriately. This doesn't guarantee promptness in terms of exception delivery (then again, async exceptions don't either and can be delivered later, but here you get no delivery guarantees at all). It also doesn't handle the case where you may be in the middle of a foreign call and want to interrupt, which GHC can now handle for a lot of cases via `interruptible` FFI calls. There are maybe other cases that would be as questionably awkward. &gt; The most common pattern I've seen elsewhere is to allow users to associate a finaliser action with thread shutdown that runs when a thread dies. That code is run when the thread finishes normally or when it is killed. This pattern assumes resource allocation is done before the thread starts and so may not be conducive to monolithic, long-running, killable threads, if you want those. How does this do asynchronous timeouts? You could actually do it with an asynchronous callback in an event loop I guess, and let the thread die when the "X second timeout" event fires, or poll an MVar - but then you've inverted Haskell's imperative threading model back to an asynchronous/polling one in a way, which feels backwards. Also, the example Michael gave is also not handled by finalizers AFAICS. The process of waiting on one thread and killing another is not uncommon: for example, we can use this to link threads together by having a supervisor thread which launches both, and if one of the two dies (by racing them and waiting for a result), then the other can be reliably killed. Much like Erlang's model, or the timeout case. Racing threads is actually a pretty useful abstraction too, I've found. 
Austin beat me to the punch on most of my response. To make one of my points more concrete, consider if I have a thread with the code: withFile "input.txt" ReadMode $ \inH -&gt; withFile "output.txt" WriteMode $ \outH -&gt; hGetContents inH &gt;&gt;= hPut outH How would your "finaliser action" allow us to guarantee that `inH` and `outH` are always closed? I see no way of achieving that. I *don't* believe there is a better solution to sync exceptions, actually. That's because most of the time I see people complaining about `IO` throwing exceptions, what they *really* mean is "this specific exception just bit me, why isn't this exception explicit in the type signature?" To clarify my point further: * There are virtually 0 `IO` actions that can't fail for some reason. * If every `IO` action returned a `IO (Either UniqueExceptionType a)`, the programming model would become *incredibly* tedious. * If instead every `IO` action returned `IO (Either SomeException a)`, we'd at least not have to deal with wrangling different exception types, and could use `ErrorT` to make our code simpler, but... * Then we've just reinvented exactly what `IO` does today, only less efficiently! My belief is that people are simply ignoring the reality of the situation: the contract for `IO` implicitly includes "this action may also fail." And I mean in every single case. Built in, runtime exceptions hide that in the type, but you need to be aware of it. Runtime exceptions *also* happen to be far more efficient than using `ErrorT` everywhere. And as much as some people complain that exceptions are difficult to handle correctly, I highly doubt `ErrorT` or anything else would be easier to work with, we'd just be trading in a well-developed, mostly-understood system for a system we think we understand. Come up with a concrete "no more exceptions" proposal, and I'll discuss it more concretely. But "there are better solutions, we all know it" isn't a discussion point.
Within an order of Fortran in which way? 
Execution time.
That's a great point, I'd never thought about it before. I've discussed this individually with a few people in the past, but I think what we desperately need in the Haskell ecosystem today is someone to write up an authoratative guide on the right way to deal with exceptions. As much as exceptions are always a tricky business, I honestly don't think they're nearly as difficult as people make them out to be, but rather misinformation clouds the issue. Maybe after ICFP I'll set aside some time to write a first draft.
I learned about this last week and have used it at least once a day since then.
Simply because that was what I wanted to be able to read as an e-book when I did it, and because it's still not an 100% automatic conversion.
I mean... slower or faster? It was ambiguous. 
Hi, all. Are there any other more obvious/traditional/right ways to do this?
previous discussion https://pay.reddit.com/r/haskell/comments/2d6eq2/field_accessors_considered_harmful/
The code it generates doesn't use linear arithmetic. There are builtins that convert to more efficient representations.
The DARPA project includes a sound static analysis phase that will verify the output C code from Ivory.
They're going to write piloting control software in Ivory. I think syntax is quite an issue for them.
if i write my own minimal signatures downstream, where is it annotated, which packages work with it and how? i think i am missing something, because there won't be a implements: my-minimal-bytestring-sig-1.0 in bytestring.
Still, Ada is in very well suited to this domain and has a wide range of commercial tools for verification (most notably the SPARK subset) and being (potentially) able to prove certain things in the source (Ivory) language and then show they still hold in the lower language in the presence of things like concurrency and quite powerful synchronisation mechanisms would also be nice. It would be nice where have a target language where it is not easy to hit undefined behaviour (the closest thing Ada had to undefined behaviour is implementation defined behaviour and I believe those cases must be documented). 
http://hackage.haskell.org/package/hnn I have a slow plan to port that to GPUs with accelerate, stay tuned.
Oh. Thansk; this automatic redirectiong to .co.uk / .com / .ca / etc. dependening on where you access the site from is really annoying. :/
I think you're talking about this: http://en.wikipedia.org/wiki/Row-major_order But you can easily transform a C/whatever program to take this into account(some compilers can even do it automatically http://en.wikipedia.org/wiki/Loop_interchange).
Nope. I had to look it up, but I was thinking of aliasing, as explained in [this](http://stackoverflow.com/a/146186) StackOverflow answer. Apparently, using the `restrict` keyword in C gives you similar performance though. After coding Haskell almost exclusively for a year my C has apparently become a bit rusty.
&gt; I really did think GHC compiled to actual native code by default, so appologies if it's actually some kind of VM-and-bytecode system skew said ghci that's the repl, compiled haskell programs with GHC is statically compiled, there's no VM in that case.
Slower than the reference Fortran. Although depending on the logic Haskell with `vector` can be much faster than Fortran because of some fusion or folding optimization that Fortran couldn't do. 
Like I said, the thread-finalizer model from other languages does require resources to be allocated before the thread starts, which is a bit limiting in the way you write things. In you example, the files would get opened before the thread starts and be started with a finalizer that closes them. And while "there are better solutions" may not be a compelling argument (even to me) I'd be interested to hear of any other common implementation of catchable asynchronous exceptions like GHC has. Certainly no other concurrent environment I've worked with has relied on them. The sync exception thing is a static vs dynamic type debate, I know. I'd still prefer the failure semantic in the type, but I'm on the fully-static errors side. Anyway, this detracts from the main point about asynchronous exceptions and is a debate that has been had. I see your point on sync exceptions, and it's a matter of taste I suppose (just as some even prefer dynamic types in general as a matter of taste).
&gt; Now I wanted to have this same abstraction in Haskell, but I couldn't think of a way to do it. I know there is bracket, but with bracket we have to explicitly specify what happens. Of course you do, how else do you define cleanup? In Python it works because both capture and cleanup are attached to the object.
~~Did you read the text? I wrote I know about `bracket`. It's not the same. With bracket I need to know how to close the file, with the with statement it is abstracted away.~~ Makes no sense anymore because comment above was edited
&gt; with :: Withable a =&gt; a -&gt; IO b or something like that. that is what I'm asking for. The above would obviously not work because readFile returns an IO Handle as does connectTo so you can't have a different implementation for them EDIT: same for your edited signature, the `a` has same type for both my examples
Obvious question is obvious, but... how does this compare to [Rust](http://www.rust-lang.org/)? In particular, Rust also has non-nullable safe references with region types (or lifetimes, as they call them).
One of the issues with exceptions is that they often require ugly type annotations (often also requiring -XScopedTypeVariables). This is a problem that is very neatly solved by [`Control.Exception.Lens`](Control.Exception.Lens) but `lens` is not allowed in some projects because it is a big dependency :(
Interesting, thank you. So this fails the "safe to interrupt" test. What about something that fails the "safe to duplicate" test? And: the idempotence test? Is it *possible* for something to be pure but not idempotent? By my way of thinking, "idempotence" is something you'd use to describe side effects, whereas purity demands that there be none. Which would mean that the idempotence requirement for `unsafeDupablePerformIO` is superfluous and trivially implied by the purity requirement on both it and `unsafePerformIO`.
python's "with" statement -- FTFY But seriously, the title's meaning becomes pretty distant from the intended one with bad orthography :-(
The solution if you want multiple instances for the same type, is to use newtypes. But then you don't save any typing, it would be `with File` instead of `withFile`. I think this type class is a bad idea in general. The `withBla` convention is pretty universal, so there's no cognitive overhead.
&gt; The above would obviously not work because readFile returns an IO Handle as does connectTo so you can't have a different implementation for them Well no there's no way to handle that, the information necessary to make the choice just isn't there. You'd have to newtype them so you can define the right typeclass instances. I'm not quite sure why you'd want to do anything other than hClose a handle though.
&gt; The above would obviously not work because readFile returns an IO Handle as does connectTo so you can't have a different implementation for them So you use newtypes to distinguish them. class Withable a where close :: a -&gt; IO () with :: IO a -&gt; (a -&gt; IO b) -&gt;IO b newtype File = File Handle newtype Connection = Connection Handle instance Withable File where ... instance Withable Connection where ... openFile' fileName mode = map File . openFile fileName mode connectTo' host port = map Connection . connectTo host port with (openFile' "foo" ReadMode) (\file -&gt; whatever)
Ah! Got it. So The thing that stopped me was that I had to define my own version of `openFile` and so on to make them have the appropriate type. Thank you very much!
Such an odd signature. It unifies with the type of `flip id`. Do you have some laws to go with that signature? Also, why not be closer to the python and have separate enter/exit? For `Handle`s enter wouldn't do anything, but IIRC with can also be used for things like mutexes. Not sure this is a good way to use typeclasses, as I can't think of any good laws relating enter/exit. They combine to be id for mutexes, but they are clearly not for files.
Before you want to think about *how* to write an abstraction there is the obvious question *why* you would want such an abstraction. What's there to gain over just using some specific `withFile`, `withSocket`, `withWathever`? What would such an abstraction bring you? Only the `with` syntax or is there some underlying reusable semantics?
For some resources, the correct way to close depends on the way you opened them, so you could request that information from your opener function, by changing it's return type from `IO a` to `IO (a, a -&gt; IO ())` or `(IO a, a -&gt; IO ())`. In the later case, you can just uncurry bracket and pass in the tuple. Your connectTo / openFile example isn't great here, since, as far as I can tell, both use hClose as cleanup, but you might write stuff like: openFileW fp io = (openFile fp io, hClose) connectToW hn pid = (connectTo hn pid, hClose) with = uncurry bracket main = with openFileW "wget.out" WriteMode $ \h -&gt; with connectToW "google.com" 80 $ \s -&gt; do hPutStr s "GET / HTTP/1.0\r\n" hGetContents s &gt;&gt;= hPutStr h This also avoids mild typeclass abuse.
Great! That was exactly what I was looking for.
Yes the example is poor. Your solution is also quite nice
One simple thing would be if the action required to produce the result is very expensive. In that case, it's not a semantic error to duplicate the action, but it's certainly worth the overhead of deduplicating the call. I think I mostly agree with your definitions of pure and idempotence. However, there are cases where someone may put something in unsafePerformIO that does in fact have a world changing behavior, like firing the missiles, and wants to be certain it only happens once. I certainly wouldn't call this pure, and would be very nervous about using such code, but it's a good motivation of insafePerformIO being the default. That's another way of saying no, I can't think of a case that matches the criteria you're looking for, sorry.
What should be done from the dev side: - checking that yarr works with new versions of GHC without perf degradation (7.8.x). - moving to the latest version of fixed-vector, with control that performance doesn't degrade. Simplifying API, impl, whatever is allowed by this upgrade. - checking if newest version of llvm (3.4 - 3.5) is able to vectorize cycles produced by yarr. From the user side, API should be simplified where reasonable, i. e. added new shortcut functions with less parameters, which use some default values for the hidden parameters. New functions? I don't know, it should be driven by real usage and demand.
Yeah. It's the price we pay at the moment for having an extensible exception hierarchy, I'm afraid. OTOH, it is leagues better than it was before it arrived; the old exception handling interface was far, far worse in basically every way. :)
Besides all the obvious "Pureness" you should definitly check out QuickCheck. Not that i learned a lot from it, but it could be a reason to learn \askell.
You can do neural networks in Haskell really easily using automatic differentiation rather than back-propagation (which is just a specialised form of automatic differentiation) - see here for an example: http://idontgetoutmuch.wordpress.com/2013/05/31/neural-networks-and-automated-differentiation-3/ and here http://idontgetoutmuch.wordpress.com/2013/10/13/backpropogation-is-just-steepest-descent-with-automatic-differentiation-2/ I imagine the performance of this could be improved quite a bit
That's fantastic! For some reason I thought the project was abandoned years ago
That's my video of Frag :) shame I didn't have a better recording software at the time as the one I used completely misrepresented the performance, it was fast and smooth (as can be seen by the FPS counter).
Nah, this is useful: if I'm *not* totally off on the wrong track, that's valuable to know as well.
All these FRP libraries always use fun examples like games and real time signal processing and robotics. But what about us, boring enterprise developers? No one cares about us except big fat corporations. They give us those boring dropdowns, edit boxes, tab switchers, buttons etc with "two-way data binding". And that's why we are using angularjs or JavaFx or ASP.NET instead of yampa. 
Well now. This is *exactly* what I'm interested in. ...and it seems I already have 100% of the dependencies, which is a huge relief on Windows. Have my feeble upvote, dear sir.
I see... I had the intuition that one was moving the implementation to fill the hole, not moving the hole around the impementation. Thanks for the correction.
I understand. My comments thus far are responding solely to the GGP's comment that "scripting languages = no build step", i.e. "languages that require compilation are not scripting languages". Of course that isn't the case; scripting languages have been compiled as you say for a long time. All of this is in the context of "scripting languages for games", which generally do not appear to have a separate compilation step.
I dunno, the idea of pythons with statements seems rather charming to me. "I'm a snake." "I eat rats." "I have no legs."
Are they a arrow or a monad? 
This is certainly not meant to be a commentary. But the fact that it requires Mono to run on *Nix is a likely candidate as why there is little traction in the community. I browsed a case study, I want to sit down and really look at it, but I'm having difficulties in trying to figure out what Event Store actually solves. Is it Functional Reactive Programming that has been moved to a middle-ware agent? 
Plenty of decent libraries out there: ReactJS, bacon.js etc. Fuck AngularJS.
I think there's an oversight here, w.r.t killing threads safely. If you send a thread asynchronous exceptions to kill it safely, it may be in its resource *deallocation* phase, which is often done in an interruptible manner, which destroys all the invariants. For example, if a thread uses `withMVar`, and the restoration of the MVar blocks, any async exception during that phase will ruin the MVar's invariant. I've got a big multi-threaded Haskell application (build system) which has painful deadlocks, because of asynchronous exceptions wrecking havoc on my *deallocators*, destroying all my invariants. This manifests *especially* when more than one async exception is sent to the same thread, but may very well happen even with a single async exception during the cleanup phase. I am not sure we have "killing threads safely" in Haskell... I think we need to: * Use uninterruptible masking on the cleanup phase, along with some of validation (static or dynamic) that the ordinarily-interruptible operations in this block do not block forever * Potentially make "throwTo" asynchronous (or add an async variant of it) to reduce the likelihood of deadlocks due to this change
Thanks, I'm adding this information to the tutorial :)
Would be great if the talk can be recorded. I'm sure many would want to watch even if they can't attend. 
And I was almost resigned to installing an outdated GHC version... Working great; thanks!
Do note that your example is what recently bit me in my use of async exceptions: withFile "input.txt" ReadMode $ \inH -&gt; withFile "output.txt" WriteMode $ \outH -&gt; hGetContents inH &gt;&gt;= hPut outH This example does *not* guarantee that the files get closed, or at least there's no easy way to know if it guarantees this, because `hClose` may block (even indefinitely). And blocking indefinitely either is uninterruptible (causing problems mentioned in the blog) or interruptible (no guarantee file is closed). My large multithreaded application used async exceptions freely, and proper bracketing of all resources. But my resources were *not* getting cleaned up because of this problem, and all hell breaks loose.
I think everyone seems to be glossing over one of the most important details: interruptible cleanup. Interruptible cleanup means that an async operation at just the wrong time (especially common when multiple async exceptions are sent, but just one is enough, if it follows ordinary completion or sync exception) destroys all the cleanup invariants, whenever a blocking operation is used in cleanup. This problem has been wrecking havoc on my project and I've still not figured out a good way to fix it. But it did lower my belief that async exceptions are the way to go - and I am convinced the *current* set of tools in Haskell is very suboptimal to do async interruptions.
There's also the recent managed package: * http://hackage.haskell.org/package/managed * http://www.reddit.com/r/haskell/comments/2d7109/haskell_for_all_managed100_a_monad_for_managed/
oh yes, it is.
I guess that is the gist of it, yes. I'm probably still stuck in the .net mindset of it all, but part of the reason we use it now, is to allow us to deliver subsets of the application as they're ready, while still being able to have upcoming functionality make use of all that has happened in the live application at a later stage. Offloadings bits to third party systems, like accouting and datawarehouse, is also an attractive benefit, at least in my mind. I'm still mostly in the dark on FRP (and Haskell in general) and I'm convinced that there are some very clever ways of achieving this. So after listening to the Haskell cast, watching some entertaing talks on youtube and following this subreddit, I've found myself increasingly drawn to the benefits I believe Haskell can bring in cases like this.
Why does Yampa have it's own version of, say, [`vector-space`](http://hackage.haskell.org/package/vector-space) embedded inside?
Snakes and Arrows... I can hear Rush playing in the background.
https://github.com/NICTA/xsharpx
Asynchronous Transfer of Control (ATC) in Ada (and Real-Time Specification for Java?) may provide some guidance. An early, short, overview paper is "Ada 9x Asynchronous Transfer of Control: Applications and Implementation" by Giering and Baker.
The Mono thing is odd, no doubt. It ran on Windows originally. But that's not a reason to ignore it. Good it's good, no matter the origin.
The rule of thumb for which one to use is that you should prefer `Acquire` when possible since it preserves more information (you can recover the original open and close actions). `Managed`, on the other hand, can wrap many more types of things (like arbitrary callbacks), so it's more general, but it preserves less information: the only operation it supports is `with`.
i.e. CQRS (Command Query Responsibility Separation) and Event Sourcing http://martinfowler.com/bliki/CQRS.html http://martinfowler.com/eaaDev/EventSourcing.html The clojure community seems to really like the event sourcing pattern. And even built an entire database (Datomic) on the principle, but I think the patterns are pretty much generally applicable. An interesting thing to note is if you really go down this path you notice that there is in inherit amount of eventually consistent behavior in your system, but I think it just reifies that behavior which was already present but just hidden, unless you're locking up the whole db on every request. Also you have to plan for and build things like snapshotting the current state so that the system can be brought up more quickly, otherwise you may be waiting for an extremely large number of events to be applied to the start state to bring the system back to current.
Just maybe, this is about static checking.
The field is pretty new. But, good news, I've been seeing FRP thesis proposals for enterprise related subjects. I think the biggest hurdles for FRP in enterprise that are being tackled right now are distributed systems and database access. 
Any compiled program is in native code, but running code interactively in ghci compiles to bytecode by default. I haven't found much documentation besides [some mention](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Backends/GHCi) in the [GHC Commentary] (https://ghc.haskell.org/trac/ghc/wiki/Commentary). I don't know if there's even a file format for bytecode, let alone any way to generate a bytecode-based executable like ocaml can be asked to do.
There also seem to be implementations of parts of a lens library in [MergeableRecords](http://hackage.haskell.org/package/Yampa-0.9.6/docs/FRP-Yampa-MergeableRecord.html) and of [deepseq](https://hackage.haskell.org/package/deepseq) in [Forceable](http://hackage.haskell.org/package/Yampa-0.9.6/docs/FRP-Yampa-Forceable.html).
&gt; Instead of storing INSERT/UPDATE/DELETE events in your transaction log like an SQL database, you store events in your own domain, like UserCreated, UserRenamed, EmailPrepared, EmailSent, etc. This sounds much like [acid-state](https://hackage.haskell.org/package/acid-state)
I completely agree. I spent more time than I can to admit wrestling ghc and POSIX based libraries on Windows. While this library shouldn't be dismissed out of hand, I think my assertion is valid as to a possible explanation why there exists no bindings to it. 
event sourcing is a crock, particularly when talking about "reversing" events in distributed systems. unless you reverse everything an event touched, you've broken something. 
60 fps is usually just the result of being refresh rate limited and doesn't reflect the limits of the engine.
(nit: the phrase is "wreaking havoc")
Embarrassingly, a few bugs snuck through which break callbacks in some circumstances when tailcalls are involved. There will be a 0.4.1 release addressing those issues in a day or two. 
As I explicitly noted: &gt; Though you also need `fst` and `snd`, which you don't seem to have.
Yes I do. I would only consider calling `(&amp;&amp;&amp;)` "well-behaved" if the following obtain: * `fst . (f &amp;&amp;&amp; g) == f` * `snd . (f &amp;&amp;&amp; g) == g` These properties are, imo, intrinsic to what the name "`(&amp;&amp;&amp;)`" conveys. That is, of all the various functions with the appropriate type, the decision to choose the name "`(&amp;&amp;&amp;)`" indicates one's desire to inform/remind readers that these properties hold. When the above properties do in fact hold, it follows that `(a,b)` is a weak product. Thus, the existence of a well-behaved `(&amp;&amp;&amp;)` indicates that `(a,b)` will have most of the properties we expect from products. The only thing lacking is the uniqueness requirement: * for all `h`, if `fst . h == f` and `snd . h == g`, then `h == f &amp;&amp;&amp; g` 
When is that useful? Why not go for the thinner interface?
&gt; I'm only beginning my study of category theory, and I'm having trouble parsing out a succinct understanding of the page you linked about monoidal categories. Fair enough :) As for whether `()` is a terminal object, there are two things we need. First, for every object `X` we need there to be a morphism from `X` to `()`. Second, we need this morphism from `X` to `()` to be unique. If this latter condition doesn't hold, then we can only say that `()` is "weakly terminal". To prove that `()` is terminal in the category `Circuit c` we need both parts. Having `Void` means the first part is covered. All that remains is to prove that, for any `X`, every `foo :: Circuit c X ()` is in fact equal to `Void`. We know that `foo` can't be `(f :&amp;&amp;&amp; g)` or `(f :*** g)` since those have the wrong types. There are three cases left: We could have `foo = Wire`, so we need to ensure that `Wire :: Circuit c () ()` is equal to `Void`. On the face of it this fails outright, since `Wire` and `Void` are different data constructors; but we can try to work around that. That is, rather than talking about concrete values of the `Circuit` data type, we could talk about their interpretation as circuits. You already have to take this step in order to prove associativity of `(:&gt;&gt;&gt;)`, so you needn't worry too much. So long as all our functions for normalizing and interpreting `Circuit` treat `Void` and `Wire :: Circuit c () ()` the same, we're golden. We could have `foo = (f :&gt;&gt;&gt; g)`. By the inductive hypothesis we can assume that `g` is equal to `Void`. So now we need to prove that `(f :&gt;&gt;&gt; Void)` is equal to `Void`. Again, this fails for the raw values themselves, but we should be able to work around that by ensuring they're interpreted the same way everywhere. The final case is, we could have `foo = Lift f`, so we need to show that `Lift f` is equal to `Void` for all `f :: c X ()`. This is the tricky one. Before we can make any headway we'll need to know more about what the category `c` could possibly be like. If it turns out that `()` is terminal *in the category `c`*, then we know that `f` is unique, so we only need to show that `Lift CVoid` is equal to `Void` (assuming `CVoid` is a name of the unique `c`-morphism from `X` to `()`). However, if we can't prove terminality of `()` in `c`, then we need to look at all the possible things `f` could be. One approach to simplifying things would be to show that `Lift` maps all the different `c`-morphisms from `X` to `()` onto the same `(Circuit c)`-morphism. This way you still need only prove that `Lift f` equals `Void` for one particular choice of `f`. Make sense?
It doesn't have to go full-screen to be rate limited. SDL, GLFW, and GLUT all default to limiting to 60 for, windowed or not. The going above 60 part is surely just due to imprecise rate limiting. Note that it typically goes to 62 right before or right after going to 58, then locks right back to 60.
Awesome. I played around with Yampa a bit for writing simulations like those I'd do in Simulink, but found the documentation quite lacking (I probably should have braved a paper or two). Exciting to see it receiving some love!
I'd say Yampa originated well before vector-space existed, and hasn't had much attention since.
SDL, GLFW, and GLUT limit the frame rate by default, windowed or not. The purpose either way is to not exceed the abilities of the hardware. I did not mean that it's literally waiting for the vsync, just that the refresh rate is the *reason* for the limit imposed.
`resourcet` is an example of where you want to preserve the original open and close actions (which is why `Acquire` is part of the `resourcet` package). The whole point of `resourcet` is to model resource management strategies that don't fit nicely into the bracket model (like acquiring resources with in a coroutine). Both Michael and I independently converged on a similar solution when doing resource management for `pipes` and `conduit`: you need to be able to split up the open and close actions.
Wow, what. It took me long to figure out what you are doing there. So you are pretty much passing "n" all way down to the leftmost innermost node, and then walking one by one until "n" is 0, which then gives our result. Fine, thats kinda neat. Also, you actually teach me a technique that will be invaluable for what I'm doing, which is how to pass a context across `fold`. Thank you!
Thanks, I fixed that and another link.
 class Context a where exitContext :: a -&gt; IO () instance Context Handle where exitContext = hClose opener `with` action = bracket opener exitContext action examples = do openFile "output.txt" WriteMode `with` \h -&gt; hPutStrLn h "Test" connectTo "localhost" (PortNumber 53) `with` \h -&gt; hGetLine h &gt;&gt;= putStrLn
I didn't want to bother signing into SO so I'll put this here. The top answer mentions Agda, and it's right to a certain extent; but as noted in the comments, plain ITT doesn't really have quite what the OP was looking for, even though sigma types are sometimes called subtypes, and they are often notated with subset-like notation. Computational Type Theory (Nuprl / MetaPRL) has proper refinements &amp; subtyping through its "set types". That is, you have a type `{x:A | P(x)}` whose inhabitants are quite literally inhabitants of `A` such that `P(x)`. NOT pairs of `x:A` and proofs of `P(x)`. This is great for composability; often in Agda I find myself covered in `proj1 (proj1 (proj1 (proj2 ...)))`, and what's worse, whenever I change just anything, I need to go and figure out what needs to happen to those projections etc. The full version of set types makes type checking undecidable, since the typechecker has to cook up examples of `P(x)`, which are not present in the judgements. But, contrary to popular belief CTT does not have undecidable type checking, since it doesn't have type checking at all. It rather has *equality* (and thence *membership*), which are not checkable properties: you prove them. In practice, working with subset types is nice.
great work!
It might be easier to read what's going on if we rely of the monadic nature of `Either`, since this method works by "short-circuiting" in the accumulator: {-# LANGUAGE DeriveFoldable #-} data Tree a = Nil | Box a | Pair (Tree a) (Tree a) deriving Foldable get :: Int -&gt; Tree a -&gt; a get n = either id (\_ -&gt; error "not enough") . foldlM go n where go 0 x = Left x go p _ = Right (p - 1) 
[Shen](http://shenlanguage.org) has this facility. See [Dependent types and vectors (again)](https://groups.google.com/forum/#!msg/qilang/3lAyZhxQ4sw/HtSJs9JXtEsJ) for an example.
Not sure. Here is an updated version of Dmitry's code : (datatype subtype (subtype B A); X : B; _____________________ X : A;) (datatype positive if (and (number? X) (&gt; X 0)) ____________ X: positive; ________________________ (subtype positive number); _______________________________________ +: (positive --&gt; positive --&gt; positive); ) (datatype integer if (integer? X) ___________ X: integer; ________________________ (subtype integer number); ) (datatype natural X: positive; X: integer; ============ X: natural; ) (define positive? {number --&gt; boolean} X -&gt; (&gt; X 0)) (define natural? {number --&gt; boolean} X -&gt; (and (integer? X) (positive? X))) (datatype index X: natural; ============= X: (index X); if (and (natural? X) (natural? Y) (&lt; X Y)) _____________ X: (index Y); X: natural &gt;&gt; P; __________________ X: (index Y) &gt;&gt; P; ) (datatype safevec V: (vector K); ________________ V: (safevec A K); K: (index K); ========================= (vector K): (safevec A K); \* K: (index K); *\ ______________________________________________ &lt;-vector: ((safevec A K) --&gt; ((index K) --&gt; A)); ______________________________________________ vector-&gt;: ((safevec A K) --&gt; (index K) --&gt; A --&gt; (safevec A K)); ) \* -------------------------------------------------------------------------------- *\ (define safevec-init {(index N) --&gt; (safevec A N)} N -&gt; (vector N)) \* -------------------------------------------------------------------------------- *\ (define safevec-ref {(safevec A K) --&gt; (index K) --&gt; A} V L -&gt; (&lt;-vector V L)) \* -------------------------------------------------------------------------------- *\ (define safevec-set {(safevec A K) --&gt; (index K) --&gt; A --&gt; (safevec A K)} Vec I Val -&gt; (vector-&gt; Vec I Val)) \* Test with this: *\ \* (safevec-ref (safevec-set (safevec-init 10) 3 3) 3) - OK*\ \* (safevec-ref (safevec-set (safevec-init 10) 3 3) 12) - type error*\
I'm using Source Code Pro too, on Emacs, on a Mac and I still don't have your output... [It's driving me mad](http://i.imgur.com/4i2DFGx.png)
Nice work. 
I made that video in 2008, I'm not going remember what card I had 6 years ago.