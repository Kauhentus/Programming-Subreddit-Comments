It seems you are combining different approaches in the different language examples. For example, you used `multiprocessing.Pool().map` in Python – why didn't this get translated to [`parMap`](http://hackage.haskell.org/packages/archive/parallel/latest/doc/html/Control-Parallel-Strategies.html#v%3aparMap) in Haskell?
GMP's mpz type is just a couple of ints and an array of 'limbs', which are 32 or 64 bit values depending on architecture. typedef struct { int _mp_alloc; /* Number of *limbs* allocated and pointed to by the _mp_d field. */ int _mp_size; /* abs(_mp_size) is the number of limbs the last field points to. If _mp_size is negative this is a negative number. */ mp_limb_t *_mp_d; /* Pointer to the limbs. */ } __mpz_struct; 
The colors paper looks really fun. Failing to find the "typed syntactic meta-programming" paper yet (which sounds interesting) I instead ran into this PEPM paper on adding an ApplicativeFix class, which is also a very interesting line of work: https://lirias.kuleuven.be/bitstream/123456789/376843/1/p97-devriese.pdf edit: i've seen the hinze paper before but oh man is it great.
I did read about tagsoup (in particular "Drinking TagSoup by Example"), but it seemed like HXT and HandsomeSoup were easier to use for this application. I can certainly look into switching, I just hoped there was something in the libraries I've been using.
Welp. Better start reading. I might be done by the time ICFP'14 starts.
when does registration start?
The main question we have is how to go about a haskell hack that breaks the usual rules of large-scale system design. In other words, we'd like to take a problem domain dominated by large-scale system solutions and create a small-scale solution using haskell. We have: trade :: [MarketData a] -&gt; Book b -&gt; IO [Order b] main = forever . do . trade What do we do next? 
First off, the default in HXT when parsing is to strip comments, so you need to pass a special option when parsing to preserve them. This means that you can't use Handsomesoup's `fromUrl` method. That's okay though, because that method is fairly short in any case, and you can still use `openUrl`. Example of processing based on comments. This extracts the contents between the comments that say "BEGIN" and "END" on http://snowplow.org/martin/min.html and dumps the tree out. (Note that this assumes that the two comments are children of the same node. If they aren't, it gets trickier): import Text.XML.HXT.Core import Text.HandsomeSoup import qualified Text.XML.HXT.DOM.XmlNode as XN import Control.Monad.Maybe import Data.List import Data.Maybe trimToCommentInnards :: IOSArrow XmlTree XmlTree trimToCommentInnards = getChildren &gt;&gt;. drop 1 . dropWhile (maybe True (not . isInfixOf "BEGIN") . XN.getCmt) &gt;&gt;. takeWhile (maybe True (not . isInfixOf "END") . XN.getCmt) main = do contents &lt;- runMaybeT (openUrl "http://snowplow.org/martin/min.html") runX ( readString [withParseHTML yes, withPreserveComment yes, withWarnings no] (fromMaybe "" contents) &gt;&gt;&gt; deep (ifA trimToCommentInnards (replaceChildren trimToCommentInnards) none) &gt;&gt;&gt; putXmlTree "-" ) return () 
Very cool! Thanks for the tip. I was digging through the HXT docs trying to find exactly what `getCmt` seems to provide. Luckily, it seems that the comments I'm matching on are in the same node. I'll experiment a bit. This definitely puts me closer to my ultimate goal. Thanks again.
I like this general idea, but I think you need to really nail down how you represent `MarketData` internally. I would recommend studying both streaming libraries (i.e. `pipes` and `conduit`) and FRP libraries (i.e. `reactive-banana` and others) for ideas for how to model long-lived incremental sources of data. For mixing lots of concurrent behaviors I will shamelessly plug my own `pipes-concurrency` library, which makes it very easy to gather and connect multiple concurrent data sources simultaneously. You may even be able to use the types from that directly. For example, the `Output` type can be a suitable replacement for your `MarketData` type and it has lots of nice theoretical properties. For example, it's a `Monad`, so you can easily connect multiple outputs together using `do` notation. Also, it's an `Alternative`, so you can simultaneously try multiple sources at once and use the first source that does not fail.
main = forever . do . trade Am I missing something that makes that valid syntax?
No, you're not; it should just be `forever . trade`, although `forever . (do (. trade))` is valid syntax (but probably won't do what you expect).
Nope! Just having some fun. :-)
Okay, good. Didn't mean to be critical - it could well have been a legitimate report of what was written on the napkin, where one is certainly not obligated to get everything right, and it communicates an idea. I just wanted to be sure there wasn't something I was missing that might've made the idea bigger (or even just different).
Yes,, you're missing the fact that I'm a committed but newbie haskell hacker! 
Thanks. I hadn't seen pipes-concurrency in my travels but it sounds worthwhile. Nailing down MarketData is the tough task we're stuck on. Incoming, it's a stream of csv lines from data vendors, but could also include tweets and reuters feeds. Where the line between what is a MarketData and what is an Algo :: [MarketData] -&gt; SomethingElse isn't clear. In the idea bank, we can imagine hundreds (thousands) of MarketData's, many of which are summaries of raw incoming data that have been internally generated by the system, all needing to track dependencies with each other and otherwise robustly work out where they belong. That's the essence of where haskell shines I suspect. 
Err...DRAM is solid-state. I truly hope you're not stuck using a computer whose RAM is vacuum tubes.
Maybe his/her first language is Spanish (or Latin, for that matter)?
The nice thing about `pipes` is that it blurs the lines between sources of data and sinks of data. You don't have to bias yourself to thinking in terms of one end or another.
Thanks, I finally understand the difference between a conduit and a pipe!
Actually, that is true of `conduit`, too. The two libraries are very similar in spirit.
I read a draft of the colors paper a while ago, I want it in Agda already!
`forever` is not a keyword, so it *could* type correctly!
from skimming the paper it looks like that optimization should be possible for non-repa code as well. does anyone know whether there is a chance that it gets integrated into ghc?
So you can now write avg xs = sum xs / genericLength xs and not be called an idiot. A cornerstone of our culture disappears.
What sort of code would you want optimised with this method, other than Repa or DPH code? This is array fusion, so you'll need an array library to do your optimisation on, and here Repa is a clear candidate. The Data Parallel Haskell team and the Repa team are basically the same, so optimisations from one will likely carry over to the other, but there isn't much else I can imagine where these optimisations will carry over unchanged. The GHC plugin converts GHC core to DDC core and back again, and as far as I know there's no real reason not to work exclusively on GHC core except that it's rather painful to do, and DDC allows one to extend the core language with arbitrary primops.
oh, i didn't know they are the same team. but nevertheless, we are using arrays whenever we use vector, don't we? it would be great to have fusion there as well.
Repa uses vector internally, so I'm pretty sure the optimisations carry over. I think you can easily and costlessly convert between Repa arrays and vectors as needed, anyway.
Or even without the `where`. instance Binary MyData
forever = undefined done!
Wow, that's some serious Arrow usage. I'll definitely keep this in mind in case I run into that situation. Very cool stuff.
I didn't know you could do that (leave off the `where`)! Is that only if there are no members?
Is there a video of the IDE session? 
Can this approach be generalized to use with lists? IIRC, stream fusion for lists was not integrated into GHC because of troubles with `concatMap`. Can data flow fusion be of any help?
section 3.2: "fusble"
So wait, has there been any indication that the IDE is going to be FOSS?
My question wasn't whether would they charge money for their product (or more precisely, licensing and offering support for it.) 
Ah, good point, sorry.
I don't think so. What did you see in the announcement that made you think that?
Could anyone, in noob language, explain to me what 'fusion' does and what it means for me as a haskell programmer? (I've been doing haskell for only a handful of months, and consider myself an expert noob)
My feeling is that these tutorials teach Haskell by writing procedural code. Why exercises like "Print squares of numbers from 1 to 10" instead of "Make the list of pair numbers lower than x"? The book "learn you a haskell" is just the right thing to teach I think. It only lacks of more examples of applications, use of common libraries, etc, to learn how to make software.
it is always true that `map f . map g == map (f . g)` Assuming f and g take negligible time, then `map f . map g` takes o(2n) time, while `map (f . g)` takes o(n) time, in that you have one fewer traversal of the list. Fusion generalizes this and related transforms to reduce redundant traversals automatically, while preserving the meaning of your program. Classic shortcut fusion foldr/build can fuse away functions that can be written with `foldr`, but not with `foldl`. This is implemented over most of the standard list functions in base. Stream fusion is more powerful, although there are some quirks, and versions of it are used with `ByteString`, `Vector`, and `Text`. This article introduces a new form of fusion that can't be done "locally, with rewrite rules" unlike foldr/build and Stream, but can capture types of sharing that could not be fully fused using previous transforms. As a downstream Haskell programmer, research like this lets you use well designed libraries and get certain performance improvements, automatically!
I think you are missing some of the subtleties of your problem. The market (or any exchange) can be modeled as a discrete set of events. For instance you won't want to run forever you will want to receive a MarketOpen message from your data provider. Same for market closed. Additionally individual securities can start or stop trading during a day, e.g. an IPO, a circuit breaker getting hit or an intraday news release. As I have commented before our trading engine is a conduit. It takes an arbitrary set of sources for market data, news events, models running outside of the engine, messages from the exchange and broker, etc... and runs them through our conduit. The sinks are typically either a real exchange/broker or a testing exchange. We have found this model to work fairly well. Its fairly easy with this model to have external components that can provide feedback into the runtime e.g. using an external risk system that plugs directly into the FIX engine. We can run multiple "Traders" that are listing to the event stream in parallel. A common use case is we have a trader that simply records market data and broker/exchange messages so they can be replayed in the future. In the middle strategies and tools are built on top of a monad transform called TraderT. It provides basic functionality such as order creation, tracking margin use, monitoring positions, etc... There are then more specialized Traders that offer more functionality such as OrderBook tracking and a variety of test patterns for validating the whole system. This design allows the strategies to not have to worry about implementing basic functionality. So ours might (it doesn't in real life as its much more complex) look something like: runTraders :: [TraderT a] -&gt; [Source MarketEvent] -&gt; [Sink Order] -&gt; IO () runTraders strategies sources sinks = runResourceT $ mergeSourcesByTimeAsync $$ runStrategies strategies =$= routeSinks sinks If you have more questions feel free to ask. In general we have found the use of conduits to be an excellent design pattern. 
Thanks, that was a good explanation! And this conversion, is this done at compile time, or can it be reasoned about at runtime?
As afarmer says, compile time. Here's a nice paper on stream fusion that may be more approachable: http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.104.7401 One reason we want to do this at compile time is to encourage further inlining. For example, if `f` and `g` are small, then we can take `f . g`, inline `.`, then inline `f` and `g` both, substitute the body of `g` directly into `f` and get something extremely efficient. If we compose a `foldr` pattern onto some list builder we may not generate an intermediate list at all! So we can get to "nothing at all," but in order to accomplish this we definitely need the compiler to do some additional work. edit: another nice fusion paper, btw, with a nice way of thinking about foldr/build fusion: http://www.cs.ox.ac.uk/files/4458/p47-harper.pdf
Conor McBride's "[Functional Pearl: Kleisli arrows of outrageous fortune](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf)", solely because of the puns. :)
For me, cloud is the least useful part of an IDE tailored specifically to Haskell.
One of the many evils of Cons-lists is that you have to construct them back-to-front. This doesn't work with flow fusion (or any fusion) if you also want to consume the source data front-to-back. Of course, you could use lazy evaluation, but if your numerical code is performing lots of lazy evaluation then lack of fusion isn't your problem.
"Hardware limit" isn't really correct. Install a 32 bit OS rather than a 64 bit OS and you get different limits on the same machine, but the OS isn't hardware. 
I've gone through the collection of papers I have on my machine, and these are probably my favorite that relate to Haskell. I have a bunch more that relate to higher type theories, like in Coq or nuprl, if you're interested. [Functional Programming with Bananas, Lenses, Envelopes, and Barbed Wire](http://research.microsoft.com/en-us/um/people/emeijer/Papers/fpca91.pdf) --- Erik Meijer is a great writer, and it came out here. Gives a good view of catamorphsims, etc. [Data Types a la Carte](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf) --- One of the most useful papers I've found, it describes how to write extensible data types in Haskell. Sadly, it doesn't work for parsers, as I learned to my regret. Edit: [Compositional Data Types](http://www.pa-ba.net/pubs/entries/bahr11wgp.html) is a great addition to Data Types a la Carte. [Scrap Your Boilerplate](https://docs.google.com/viewer?url=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fsimonpj%2Fpapers%2Fhmap%2Fhmap.ps) --- Again, a very practical paper for anyone working on large haskell projects. Edit: I was asked for some higher type theory stuff. Here it is: [Nuprl's Class Theory and Its Applications](http://www.cs.cornell.edu/info/projects/nuprl/documents/constable/nuprls_class_theory.pdf) --- A good discussion of record-based objects and classes in a dependent type system. Has its issues, but so do all of the translations I've seen. [A Tutorial on Coalgebras and Coinduction](http://www.cs.ru.nl/~bart/PAPERS/JR.pdf) --- Coinduction and corecursion are something I'm just getting started on. This is a good place to start building some intuition, but I find that it lacks any teeth. There's just not enough formal there. [Towards Dependently Typed Haskell](http://www.cis.upenn.edu/~eir/papers/2013/fckinds/fckinds.pdf) --- I haven't read it yet, but I know Stephanie Weirich, and I expect that it's a great paper. I'm really looking forward to it. [MTac: A Monad for Typed Tactic Programming in Coq](http://www.mpi-sws.org/~viktor/papers/mtac.pdf) --- A great paper for anyone who's been frustrated by tactic programming, especially anyone who is comfortable in haskell.
[Ornamental Algebras and Algebraic Ornaments by Conor McBridge](https://personal.cis.strath.ac.uk/~conor/pub/OAAO/LitOrn.pdf)
Just going to throw questions here. Questions made men king,eh. Does the difference between foldr and foldl have to do with lazy evaluation? It's a subject I'm only now am starting to grok and I recall reading something like that
no. try to visualize how foldl and foldr work. remember that cons, `(:)`, is `O(1)` and list append `(++)` is `O(n)`. then you will certainly see if yourself.
thanks for the clarification re specific front-end library.
but the browser can send pretty much every http verb via javascript! and most modern sites do use javascript, so it makes sense to support other verbs.
&gt; Data Types a la Carte --- One of the most useful papers I've found, it describes how to write extensible data types in Haskell. Sadly, it doesn't work for parsers, as I learned to my regret. I only just got round to reading this, and I found it fantastic.
http://stackoverflow.com/questions/4838138/web-scraping-with-haskell Personally I've been happy with dom-selector. It's not really maintained but it works great for me. I wrote this answer: http://stackoverflow.com/a/12817067/516188 I STRONGLY advise you to use CSS selectors to read contents from the page.
Noticed that many are Haskell-related. And this is the main conf, not the symposium. Interesting. 
Having gone back through your previous tutorials after seeing this, let me thank you - these are excellent!
Those those of us who are just starting out into the magic work of Haskell's type system you might find [Origami Programming](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/origami.pdf) more approachable than barbed wire.
&gt; I have a bunch more that relate to higher type theories, like in Coq or nuprl, if you're interested. Yes, please :)
YMMV, to me it is very interesting.
This is great. I'm just finishing up "Learn You A Haskell For Great Good" and always looking for an excuse to really start using the language.
This looks pretty cool! Some possible improvements: -- Takes a lowercase string, True represents vowel wordDesc :: String -&gt; [Bool] wordDesc str@(first:rest) = (first `elem` vs) : zipWith cOrV str rest where vs = "aeiou" cOrV prev cur = cur `elem` vs || (cur == 'y' &amp;&amp; prev `notElem` vs) measure :: [Bool] -&gt; Int measure desc = length . filter id . dropWhile id . map head . group $ desc hasVowel :: [Bool] -&gt; Bool hasVowel desc = or desc endsDblC :: [Bool] -&gt; Bool endsDblC [False, False] = True endsDblC (_:rest) = endsDblC rest endsDblC _ = False endsCVC :: String -&gt; [Bool] -&gt; Bool endsCVC str desc = checkTrailing desc &amp;&amp; last str `notElem` "xwy" where checkTrailing [False, True, False] = True checkTrailing (_:rest) = endsCVC rest checkTrailing _ = False The biggest thing is: use the type system! No need to encode consonant vs. vowel in a `Char` (which has thousands of other possible values), when there are data types with just two values (e.g. `Bool` or `data LetterType = Vowel | Consonant` which you define yourself), and no need to return/pass values that are not needed. Note the `@` pattern in `wordDesc`, this binds the `word` variable to the whole match, and also binds the variables `first` and `rest`, like in a normal pattern match. Next, `measure` can be simplified: `id :: a -&gt; a` just returns it's argument unmodified, which is perfect when we want to filter a list of `Bool`s. Also, note that `measure` looks shorter and simpler without the noisy list comprehensions (as nice as they are, they aren't used that much when operating over a single list, since `map` is often clearer). Then, the pattern matching of the two `ends` functions is slightly obtuse, but it basically says: a list of length 2 ends with two consonants if both elements are consonants (durr), a list that doesn't fit this pattern ends with two consonants if (and only if) its tail ends with two consonants, the last catch-all pattern (which actually just matches `[]`) says that if it hasn't matched anything above, then it definitely doesn't end in two consonants. (And similarly for `checkTrailing`.) Returning to `wordDesc`, I noticed that the pattern `snd . wordDesc` occurred a lot, so it seems strange to thread the `str` *and* the `desc` back out of that function rather than just the `desc`. This simplifies a lot of the code that calls `wordDesc` (by removing the need for the `snd`), but does require adjusting some code: noto = not $ endscvc $ wordDesc $ stm "e" -- becomes noto = not $ endsCVC str $ wordDesc $ stm "e" (Sorry if this is a bit demoralising... I can assure you my first Haskell programs were *much* worse than yours though. :) ) (Lastly, in the vein of "use the types": you should know about [Hoogle](http://www.haskell.org/hoogle/?hoogle=%28a%2C+b%29+-%3E+b), which allows you to search for functions by *type*... it sounds strange, and that example there isn't great, but it's really quite useful: think about the types of the function you need, and half the time Hoogle will spit out the perfect one.)
This is a very nice, gentle, introduction to Haskell: http://learnyouahaskell.com/. The online version of the book is free, but worth buying the paper version.
Many of the criticisms here are legitimate, but the top comment at the time of writing seems to make a few criticisms that I disagree with. I don't have a HN account, so hopefully the author also reads this: &gt; Way to many user defined operators. Haskell lets you define almost anything as an infix operator which library authors love to (ab)use. So you get operators like ".&amp;&amp;&amp;." (without the quotes) because they are functions reminiscent of the boolean and operation. I don't know if I've seen operators that bad. Certainly I've dealt with more annoying crazy operators in Scala than Haskell. &gt; But weirdly enough, many operators aren't generic. String concatenation is performed with "++" but addition with "+". This particular example is working as designed. We have a generic monoid append operator, `&lt;&gt;`, but which monoid for numbers would you expect the default append to use? Product? Sum? some other interesting monoid instance? &gt; Incomplete and inconsistent prelude. It has unwords and words for splitting and joining a string on whitespace. But you dont get to specify what string to use as the delimiter like the join and split functions in other languages lets you do. True, I also found this odd, but we have these functions in the split package, and I don't think anyone uses anything other than that package when they want a split function these days. I think the next point (about there being multiple completing split implementations) isn't a problem, because of this eventual convergence. &gt; There are four different "stringish" types in Haskell: List, LazyList, ByteString, LazyByteString. I think that you mean Text, lazy Text, ByteString, and lazy ByteString. There are also lists of characters (String), but that's rarely useful for anything but simple teaching applications. Having a semantic difference between _text_ and a _sequence of `Word8`_ is highly valuable, I think. They are different types that are unfortunately conflated in other languages, often leading to encoding bugs. I think the family-of-modules approach for the strict/lazy divide is less desirable, and I think repa-style type indices would be a better solution here, but that sort of solution is much more experimental. On a slightly unrelated note, I think we should as a community stop using `ByteString` (or at least strict `ByteString`) altogether, and start using `vector` for arrays, be they of word8 or other types. Someone should implement the remaining ByteString functions not already available for vector, and then we can perhaps even turn strict bytestrings into a type synonym. Using Data.ByteString.Char8 is almost always a mistake, as far as I can tell. &gt; Most Haskellers seem to content with just having type declarations as the api documentation. That's not a fault of Haskell per se, but imho a weakness in the Haskell community. For example, here is the documentation for the Data.Foldable module The API documentation could use a little work, but Foldable is a highly generic type class, so the types really do provide most of the needed documentation. You could read the essence of the iterator pattern paper to understand where the authors are coming from, if you needed more information. I guess that should be linked from the API docs. &gt; This is very subjective and anecdotal but I've found the Haskell people to be less helpful to newbies than other programming groups. Ehh? This is the exact opposite of my experience. 
This paper had a pretty profound effect on me, convincing me of the real value of types. However, I'm yet to actually understand the entire paper :)
While I feel I have a good handle on monad transformers, I don't think I've read this. Thanks!
Yea, I really haven't made much headway with Meijer's paper. The first few pages took hours of studying.
Higher type theory stuff - certainly!
Real World Haskell (http://book.realworldhaskell.org/) is a whole book on Haskell (well, it is a LARGE paper :-)). The book is available online for free. It contains lots of nice examples of real-world problems (hence, the name), solved using Haskell.
&gt;I've found the Haskell people to be less helpful to newbies than other programming groups. Well most people neither have to time nor inclination to read the academic papers that motivate implementations. And stuff like &gt;You could read the essence of the iterator pattern paper to understand where the authors are coming from ... Could be interperted as a bit of a "fuck you" since that paper is 25 pages and starts with bifunctors. I have talked to people who use the ST monad but don't understand how it works and I can probably say that the number of people who are just using things without understanding their formulation is likely to higher in the Haskell community.
Ok, then let me rephrase: HTML is the de facto markup language used to serve RESTful apps...and it has poor REST support.
You're welcome!
Video is on the way.
I have to agree with papers being a bad idea for beginners. Most papers are not written to be beginner friendly. Also, even if they are beginner friendly, beginners won't know that and the very mention of a paper will turn them off and/or intimidate them. This also heavily reinforces the perception that the Haskell community is overly academic. Everybody is writing papers and nobody is writing blog posts or tutorials.
The only thing I would criticize is using generic names like "step5". Other than it is pretty good.
It's not just an IDE, it's also a deployment/application server tailored to Haskell. 
I idle in the #haskell channel on Freenode, and I can't say they're not very helpful. There are a lot of users who consistently help newbies, even about the most trivial things.
Let me just hijack here a bit. The issue I've intended to highlight is academic papers not being accessable. Not that the Haskell community needs to change.
re: Unifying Recursion Schemes by Hinze et al. For the gallery: this SO question should help fill out the context: http://stackoverflow.com/questions/6941904/recursion-schemes-for-dummies The *Recursion Schemes from Comonads* is another must-review, if not must-read: http://cs.ioc.ee/~tarmo/papers/nwpt00-njc.pdf 
I guess not. You just coalesce them, right? :p Kidding aside, that's why you're not supposed to use them.
I'm not trying to relate the two points about newbie friendliness and reading an academic paper. Foldable is a fairly advanced library, and I think the API documentation for it is perfectly fine. But, if you do need more information, I think the paper is quite good, for someone experienced in Haskell programming.
What part specifically do you need clarified? I'm not sure what you want me to say that I haven't already said.
True, I didn't think of that. Come to think of it, aren't there non-portable things in `base` as well? Like the ST monad, which requires rank 2 types?
Data.Foldable is in simpler terms a generalisation of mapM/foldM and friends. Which even intermediate level haskell programmers will find quite useful. There probably should have been at least be a link to the paper in the documentation for consistency reasons.
About the ++ vs + thing, I think the OP's point is that he would ideally like to see more overloading in Haskell. I think the OP understands why it's not but still laments the fact.
Fay doesn't use a separate type checker from GHC, so of course it supports rank 2 types, so long as the syntax changes can be parsed by haskell-src-exts (which they can). 
I don't like the idea of getting rid of ByteStrings. I use them fairly often - when you want to represent a block (or stream) of bytes, they really are perfect. I can see some pros in replacing strict ByteStrings with Vectors, but I think it's a silly idea. Lazy ByteStrings are wonderful when used well, there really isn't an alternative. Having the strict version just makes sense. I love the ability to write an algorithm/library in one ByteString form and strict-ify or laz-ify it by just changing the `import`.
&gt; Using Data.ByteString.Char8 is almost always a mistake, as far as I can tell. Ageed.
Indeed. I have to do a small hack to even compile `vector` with a cross-compiler.
&gt; Depending too much on one compiler implementation leaves our community vulnerable to a single point of failure Agreed. This is why I stick to Haskell98 in all of my published code.
&gt; when you want to represent a block (or stream) of bytes, they really are perfect. I can see some pros in replacing strict ByteStrings with Vectors, but I think it's a silly idea Why? Vectors of words8 are the same representationally as bytestrings, and both would benefit from a variety of optimisations. &gt; I love the ability to write an algorithm/library in one ByteString form and strict-ify or laz-ify it by just changing the import. Making a strict ByteString a type synonym for Vector Word8 would still allow that.
&gt;True, I also found this odd, but we have these functions in the split package, and I don't think anyone uses anything other than that package when they want a split function these days. Can they not just update the standard prelude? This sort of thing has been common in Python at least. 
Yeah, but can they not just update the standard? It seems silly to not strengthen the Prelude where possible.
All the critisims about the language itself are frankly too small to be bothered with. No language is perfect. The biggest problem with haskell is though not language related. Haskell is very hard to push into production environment. Most people never heard of it and are scared. Don't even mention to them that you have to use emacs/vim to code in it. Windows support is shaky. And most windows developers are not comfortable developing on linux even in VM. Hard to train new devs. Hard to find existing haskell devs. Most of them are far away and are not willing to relocate, while the companies are not willing to let them work remotely. You need to be very careful what you are writing with haskell. Do not code UI, even the web apps. Concentrate on server side stuff, services, core business code. Let the UI be developed by easy to find developers (java, c#, php, ruby). Again this is ONLY because you will have a hard time finding replacement for yourself. If this is not an issue for you, or you actually wanna be in a position of a golden child, then by all means, code everything in haskell. 
I just got up to the first bit of Python code. Why not?: if self.b[i] in "aeiou": .... [Vid](https://www.youtube.com/watch?v=EnSu9hHGq5o) Also, all your loops in Python probably don't need to to be collecting indices. The video above may be useful. (30mins) Sorry if I come across as a judgemental dick by the way. Just trying to be helpful.
That makes a lot of sense. For a while I wasn't sure how Fay did the type checking.
&gt; &gt; But weirdly enough, many operators aren't generic. String concatenation is performed with "++" but addition with "+". &gt; This particular example is working as designed. We have a generic monoid append operator, &lt;&gt;, but which monoid for numbers would you expect the default append to use? Product? Sum? some other interesting monoid instance? Right, monoid is simply more generic. Also: `mempty` is generic and I haven't (yet) seen a language that has abstracted the concept of an identity for both numbers and strings/lists/vectors/trees/w/e in the way that `Monoid` does. A valid part of this criticism is that `++` is too specific (which is I expect why the guy (probably) hasn't heard of `&lt;&gt;`). I always wanted `(++) = mappend`, but one day `&lt;&gt;` appeared. Now we have two. Newbies are introduced to lists and `++` and then they realise they should stop using that and start using `&lt;&gt;` when they learn about `Monoid`. That's a pity. Newbies only have to learn about `(+)` once. Oddly, [there was pretty much consensus](http://hackage.haskell.org/trac/ghc/ticket/3339) that `++` made perfect sense as an operator, but seemingly those decent points were ignored and `&lt;&gt;` ended up in there.
I'd like to take this opportunity to get a few things about Haskell, my main language for the past few years, off my chest: 1. The Prelude sucks because it's always there. I shouldn't have to explicitly un-import stuff I don't want. If I wanted the included batteries, I'd `import Batteries`. 2. The Prelude is wrong. `Monad` should be a subclass of `Applicative`. `fmap` should be called `map` instead, which in turn should have been called `listMap` (if anything). 3. Since hierarchical modules were introduced, people started using `Data.*` and `Control.*`. Without a real pattern. 4. Cabal/Hackage is case-insensitive, except when it isn't ([lattices](http://hackage.haskell.org/package/lattices) vs [Lattices](http://hackage.haskell.org/package/Lattices)). 5. Not enough explicit information in type classes. I see lots of documentation that describes a minimal instance definition. Shouldn't we have this explicit in code? At least then we can get some more automated warnings ("your instance W has no binding for X or Y, but at least one is required"). Also, I'd like to see a more formal encoding of type class laws, but the absence of this is understandable.
weak support for embedded platforms (arduino/atmel, iOS, android, ARM), and it makes you sad to program in other languages after you have experience real ultimate power.
I'm getting started on it, how is it further in?
You gonna tell someone who've been accustomed to Visual Studio that "any text editor should do" ? :)) Good luck with that. 
Technically you do not need Visual Studio to code C# too. You can use notepad. But that's not a valid argument is it? Emacs and vim have the most feature rich and convenient haskell modes as of today. Other editors do not come even close. There's an Eclipse plugin that supposed to have some of that power. But my numerous attempts to make it work have always failed. I would be uncomfortable to suggest a newly converted haskell programmer to try out Eclipse plugin just to see him fail too and unable to help him and make very negative first impression on the entire haskell ecosystem. 
If Haskell was more orthogonal then + could be used in different distinct circumstances. For example string concatenation and integer addition.
...and by orthogonal I actually mean operator overloading
Just use `&lt;&gt;` and make your code even general. Suggesting that `+` should work on lists shows a misunderstanding of how typeclasses work.
What would you recommend for a vim user?
I never use `&lt;&gt;`, but I do sometimes use `++` from `BasicPrelude`
Switch to emacs &lt;ducks&gt; :) On a serious note i do not use vim. I use emacs. So i cannot suggest you anything. 
Didn't they form the committee for Haskell 2014 recently? Would this be something for them?
I was trying to build an extensible parser for an extensible data structure. So, I had functors ```haskell data Add e = Add e e ``` and ```haskell data Mult e = Mult e e ```. If I have parsec `Parser`s for each of these, (so a `Parser (Add e)`), I can't use them to derive a `Parser (Term (Add :+: Mult))`, which is what I need. If you know of a way of doing this, I'd love to hear it. I spent about a year working on this, among other things, for my BSc thesis, so I'm pretty sure that there's not a way in the shared folklore or the academic papers. But, I could've missed something, or something could have come up since then. Edit: In [this blog post](http://akhirsch.github.io/posts/2012-11-08-ext-parser.html) I explain exactly what happened.
Well! :q
I think you're missing the point if you think the author misunderstands type classes. I'm fairly sure the author fully understands type classes and sees it as the technical reason `+` doesn't concatenate strings. What the author disputes is the decision to not have a "PythonicPlussable" type class which allows you to concatenate strings with `+`.
outcome of coding in java is a releasable program. outcome of coding in haskell is a journal paper. that can be an upside or a downside.
&gt; The Prelude sucks because it's always there. I shouldn't have to explicitly un-import stuff I don't want. If I wanted the included batteries, I'd import Batteries. alias ghc='ghc -fno-implicit-prelude'
&gt; I don't see the problem with Haskell's operators. They're just ordinary functions unlike in most languages, and once you're used to them, they can boost readability way beyond what any other language is capable of. Except that with prefix functions you don't have to struggle with associativity and precedence.
A reasonable amount of people manage to get EclipseFP working and use it to be productive, so if you want to try again please feel free to contact me or drop a note on the support forum with any issue. If you already have and I've missed them, I'm sorry, I do try to answer people's problems, maybe drop me a mail with the issues I haven't addressed.
You're missing the point entirely. The dude probably understands the technical limitations and in his *opinion* he wishes that you didn't have to do it the way you describe. I didn't know that the Haskell community was so closed minded that they're down-voting opinions now...
I thought [Haskell beats C using generalized stream fusion](http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/haskell-beats-C.pdf) was really interesting and fairly easy to understand.
&gt; I think you're missing the point if you think the author misunderstands type classes. I only think that because of the language he choose to describe Haskell's typclasses. "operator overloading" is not a term most Haskellers would use.
Thanks for the reference. I'll look into it.
I think he sees completely past type classes, I think he doesn't care for them as much as he cares for what he has to type to make the program do something. As such, his problems are not about understanding why you can't do it in Haskell in particular, but rather about why you would not want to do it at all; he questions the design decision that lead to you not being able to do it in Haskell.
The point is that it's got nothing to do with orthogonality (...?) or operator overloading, it was a design decision, and one most of the community is comfortable with. You're being down-voted because 1) that's how redditors disagree -- go reddit; and 2) because your posts don't really make sense -- I'm still not sure what you meant by "If Haskell was more orthogonal...".
&gt;The dude probably understands the technical limitations and in his opinion he wishes that you didn't have to do it the way you describe. No, he doesn't. He had no idea monoid existed.
I'll try it again on a weekend. But frankly i've settled up with emacs by now. I code remotely via ssh in tmux session. I doubt Eclipse can work in such environment :) 
I currently use the plugins described [here](http://haskelllive.com/environment.html) and it works quite nicely. If you go to the home page of the website you can see a video of it in action
I think the main downside I have seen so far is laziness. I can see the benefit of it but after spending most of my time programming in non-lazy languages it's difficult to adapt to (when things go wrong, ie space leaks). I've never been able to find a tutorial or article that explains how and where/when to enforce strictness that is easy to understand - although this may be due to the fact I'm quite dense :)
&gt; I don't see the problem with Haskell's operators. They're just ordinary functions unlike in most languages, There are a few problems with operators. First, they are hard-to-google identifiers. While Hoogle and a good IDE (... we're waiting FPComplete...) should ideally be the solution, the reality is that much of a programer's time is spent searching for documentation and use sites with Google and grep. Operators don't play nice with these tools. Another problem is that operators tend towards being non-modular. When you declare an operator, you give a fix, global precedence value. That, in itself, is weird, as the whole point of modularity is avoiding global anything. Probably worse than that, as a developer reading unfamiliar code making heavy use of operators, it's not at all clear how the operators should be parsed. The extra terseness and clarity of using operators comes with a steep-ish learning curve. At the very least, I would bet my functional programming badge if, out of 10 randomly chosen people on #haskell, even one of them knew the precedence levels of (+), (:), and (&lt;&gt;). 
Sort of. Unboxed (vector) is likely the most efficient representation. But in the last major version bump of Repa the underlying representation is a type variable. There are several including cursored, bytestring, delayed, foreignptr, partitioned, unboxed, vector, etc...
I think we got off on the wrong foot. Let's start off by by defining some terms for you to understand. **Opinion**: "a view, judgment, or appraisal formed in the mind about a particular matter" Source: http://www.merriam-webster.com/dictionary/opinion Now that we know what an opinion is what I've been trying to say is that the author is trying to say that he wishes there was a (in his opinion) better way to used + as a symbol. In the authors mind he wishes it was more "Pythonic" in a sense where + does addition and string concatenation. The author understands why + and ++ are the way that they are but don't like the design choice even after understanding. &gt; The point is that it's got nothing to do with ~~orthogonality (...?) or~~ operator overloading, it was a design decision What? How is the role that operator overloading does or doesn't play in a language not a design decision? If you're telling me that the designers of Haskell didn't at least consider the role that overloading should have, I'm going to have to disagree. The community seems intelligent enough to at least think about it. If everything I've said is still over your head here's another take on what I'm saying by /u/kqr &gt;I think he sees completely past type classes, I think he doesn't care for them as much as he cares for what he has to type to make the program do something. As such, his problems are not about understanding why you can't do it in Haskell in particular, but rather about why you would not want to do it at all; the design decision that lead to you not being able to do it in Haskell. 
As I've progressed in my Haskell skills, I feel like there is kind of a dead zone in the IRC channel. Beginner topics and advanced type-theoretic discussions invariably get attention, but if you get to some "intermediate" level, the questions you have are neither easy nor interesting enough to get attention. Maybe I've just gone to the IRC channel at the wrong times lately and my perception is skewed. :-/
Overloading would be nice in general, but + really should be reserved for commutative operations.
X11 fordwarding. ;)
&gt; You can't fight the language. Fighting with Haskell will cause great pain. When you go with the flow, using lots of strong types, higher order functions and can apply things like a monoid instance to a problem the language is a joy to work with. Very well put. This illustrates to frustrated new users that Haskell is not a cruel langauge designed to be difficult to use, it's just that you don't know how it's supposed to be used yet and you're going about it in all the wrong ways. When you become more familiar with the language it will become a lot easier.
Oh, nobody mentioned this? ;-D http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf I love haskell history~~
https://github.com/dag/vim2hs ?
That sounds brilliant. Is there any chance you could post it somewhere here so I can have a look when it's done? :-)
If anything we'd take more splitting functions _out_ of prelude. This sort of stuff really belongs elsewhere, and remains in prelude for historical reasons. On the other hand, the `split` package _did_ get moved into the haskell platform, and so in that sense is indeed "blessed" as a standard solution.
&gt; Someone should implement the remaining ByteString functions not already available for vector, and then we can perhaps even turn strict bytestrings into a type synonym. Using Data.ByteString.Char8 is almost always a mistake, as far as I can tell. Why is it almost always a mistake? Because it's called a String and people confuse it for a string (as in text)?
They actually are not the same, depending on which `Vector` you're talking about exactly. For instance, `Unboxed` vectors are backed by `ByteArray#`, which is managed by GHC's garbage collector. `ByteString` is backed by a `ForeignPtr`, which makes certain foreign interfacing easier; you don't have to copy to hand a `ByteString` off to C, but you will have to copy an unboxed vector (because otherwise it may move), and you can memory map a `ByteString`, but not an unboxed `Vector`. The vectors based on `Storable` might be `ForeignPtr` based, but I'm not entirely sure. `Text` on the other hand is internally a `ByteArray#` and not a `ForeignPtr`.
Sure, it will most likely end up on /r/haskell when it's out. Don't expect too much though, I got motivated by some discussions with Johan Tibell and Duncan Coutts, where we wished there was some *introductory* material about writing efficient Haskell code -- I've been asking questions around, gathering StackOverflow answers, links to the GHC commentary, papers and what not in order to write an introduction to these stuffs. But it really is an introduction -- hopefully someone (or me) will write some nice material about understanding GHC Core, simple and advanced profiling, etc. The original goal we had was a "haskell performance guide", but that requires a lot of time.
OK, thanks!
Heh, well I didn't think it was quite ready for the limelight yet, but okay. :) I've been assembling this code into a package. I don't know if this implementation will be suitable for The Real World, but if nothing else, it should serve educational purposes.
Not Haskell 2010?
The problem is that you need to make the parsers mutually recursive. There's no way to do this exclusively on the type level, since you need the parsers to be able to alternate back and forth between the different branches of the functor coproduct. *Edit:* That is, the functions you want are: coprodP :: (Parser e -&gt; Parser (f e)) -&gt; (Parser e -&gt; Parser (g e)) -&gt; Parser e -&gt; Parser ((f :+: g) e) coprodP p q e = (injL &lt;$&gt; p e) &lt;|&gt; (injR &lt;$&gt; q e) fixP :: (forall e. Parser e -&gt; Parser (f e)) -&gt; Parser (Fix f) fixP f = Fix &lt;$&gt; f (fixP f) -- thus, given: addP :: Parser e -&gt; Parser (Add e) mulP :: Parser e -&gt; Parser (Mul e) -- we have: addParser = fixP addP -- :: Parser (Fix Add) mulParser = fixP mulP -- :: Parser (Fix Mul) addMulParser = fixP (coprodP addP mulP) -- :: Parser (Fix (Add :+: Mul))
The differences are quite small. I avoid n+k patterns because I am mindful of Haskell2010, but I also avoid DoAndIfThenElse for now. I do use `base`, usually, just because it can be a pain to mix code depending on `base` (all of hackage) and code depending on `haskell98` or `haskell2010`. A significant portion of `base` is fairly portable, however, so this is somewhat less worrying than out-and-out syntax extensions.
The Char8 functions basically assume that the `Char` type is encodable as a Word8. i.e: Ignore the existence of Unicode. 
Oh yes, what a fantastic read this one is!
Thanks ss, That's really helpful. Yes, I'm sure I'm missing many nuances of the problem set but your explanation cheers me up because I understand it and I don't understand many of the iperative-style systems out there and what they do exactly. The forever is more a real-life ambition - we expect the runtime to be always on doing "something" and event happen 24/7 in aggregate, especially when you feed external models back in. If I can just summarise what you sketch out. In your simplified schema there are: Multiple event sources including market and news sources, but you also feed in modelling external to the core runtime but internal to your overall process. Our existing design has the external stuff shoved into the middle of the conduit like so: DataSources =&gt;&gt; Logger =&gt;&gt; Database =&gt;&gt; External Algo =&gt;&gt; Mixin with Internal Algo =&gt;&gt; OrderCreation The runtime is a single conduit - an alternative would be multiple conduits which I think is an implicit feature of many other trading system designs out there. Is having a single conduit a key piece of design genius? Within the conduit there exist multiple Traders, and it sounds like they have a very broad set of tasks to do: from being simple history loggers to quite specialised sub-domain knowledge rich tasks. (Just like the humans do!) Then there are multiple Strategies. I'm wondering where the dividing line is between being a Trader and being a Strategy. Do Strategies, for example, map over the entire conduit (so they're a rule set, say) whereas Traders are components of the conduit - sub-sink/sources? Or even YourConduit == [TraderT a]. Our concept of Algo doesn't sit well within your sketch. For us the working definition of an Algo is closeish to your Strategy but includes things like compression and summary of MarketData (others would call this the CEP engine). It also includes modelling like the external sources you mention. Finally, I think it includes a rule-set that governs some of the choices that Traders make - like which markets to monitor when. Maybe we have some refactoring to do there. Thanks again for your thoughts. 
Maybe I'm missing something, but your definition of PauseT doesn't quite work as I expected it to. If I try: example2 :: PauseT IO () example2 = do lift $ putStrLn "Step 1" lift $ putStrLn "Step 1" pause lift $ putStrLn "Step 2" main = do _ &lt;- runNT 1 example2 return () I'd expect to see two "Step 1"s printed, but I only see just one.
&gt; Most Haskellers seem to content with just having type declarations as the api documentation. Now that I've been using Haskell for a couple of years, this doesn't bother me so much. In fact I *like* how much you can learn just by reading the documentation, v.s. languages that Ruby where docs usually tell you very little. But when I was starting out this was a *huge* hurdle for me. For example the [HXT](http://hackage.haskell.org/package/hxt) module is very useful but very hard to break into if you're a newbie. It took me an entire weekend to understand and write documentation for it (see [here](http://adit.io/posts/2012-04-14-working_with_HTML_in_haskell.html)). So I would have really loved better documentation.
I think it's really helpful and pretty interesting. I'm actually reading the last few chapters on functors, monoids, and monads more than once each, since the concepts are so new to me.
Also, `for = flip map`. And better regex support in the Prelude. And most things in [MissingH](http://hackage.haskell.org/package/MissingH-1.2.0.0). The Prelude needs a lot of improvement.
This guys should check elm
Yes.
That wasn't my Python. It was the "canonical" version written by the same person who wrote the C version. I (hopefully) would have written something a bit more elegant in Python.
That's a really good point. It actually *is* noticeably slower than the Python version. I might do some more work on it after I do the testing I've planned. Trying to get it to run faster would be an interesting/worthwhile exercise.
You're right; PauseT's Monad instance does not obey the monad transformer laws, because each `lift` can introduce an additional "pause" accidentally. From part 2 onwards, however, our abstraction *does* obey the laws: import Control.Coroutine.Interface -- In the works import Control.Monad.Trans.Class (lift) import Control.Monad type PauseT = Producing () () pause = yield () example2 :: PauseT IO () example2 = do lift $ putStrLn "Step 1" lift $ putStrLn "Step 1" pause lift $ putStrLn "Step 2" runN :: Monad m =&gt; Int -&gt; PauseT m r -&gt; m (PauseT m r) runN 0 p = return p runN i p | i &lt; 0 = error "Invalid argument to runN" | otherwise = resume p &gt;&gt;= \s -&gt; case s of Done r -&gt; return (return r) Produced () k -&gt; runN (i-1) (provide k ()) main = do _ &lt;- runN 1 example2 return () 
Thanks a lot. The steps come from the paper and the "canonical" Haskell and Python (and C) versions use them as well. I thought about giving them better names but they're all really doing the same thing, so naming would have been painful regardless.
You're right, there is an error in how I implemented PauseT. This error is not perpetuated beyond Part 1, though, so do read on. As for the ordering, yeah, I can't figure out how to reorder them. Growing pains for School of Haskell. /shrug
Galois gave a talk saying that they couldn't start a business "just" writing Haskell, instead using it for high assurance computing. I'm sure Redhat received similar pessimism about "just" packaging Linux. The cheerleader business model can work for open-source if there's a enterprise need.
Thank you. It's not demoralizing at all. I appreciate the feedback. I did use Hoogle a little bit starting out. I think I used it to figure out if there was a `toLower` available for strings or not. I like the idea of declaring a `LetterType`. That's a lot cleaner. I got carried away making the algorithm match the paper. I like your `wordDesc`. It's pretty much doing the same thing (matching a string against the same string offset by a letter) but mine looped the last letter to the start of the word. It doesn't matter since it's ignored anyway but it did bother me. I really like the fact that you've eliminated the index zipping. That never felt right but that was literally the first function I had written in Haskell so there was a lot of flailing and gasping for air. I think your `measure` should be: `measure desc = length . filter not . id . dropWhile not . id . map head . group $ desc`. We want to measure the consonant sequences in the word, ignoring the first one. Otherwise that is a lot more elegant. Your `ends*` I like as well (try saying that to a person you don't know). It's pretty cool what you can do in Haskell with recursion and pattern matching. I didn't realize you could declare function and patterns like you are in where clauses. That's really useful to know. Thanks again.
Thank you. Like I stated in the blog I just had to pick something to get started. It seems hard to find a clean segue into the language. I think it's a good way to do it since you can ignore the business side of it (is this a good language for this project? etc). I read *Learn You a Haskell* as well. It's a great book.
Ah, right you are about `measure`, although writing `x . id . y` is always exactly the same as `x . y` (it is like writing `x(id(y ... ))` in python, where `def id(x): return x`. So it could even be: measure desc = length . filter not . dropWhile not . map head . group $ desc Also, I forgot one case of `wordDesc`: wordDesc [] = [] No need to define a partial function when it can easily be a total one.
Nice, that's much easier to read. I haven't used `id` before.
You might find [this tutorial](http://www.haskell.org/haskellwiki/Roll_your_own_IRC_bot) by /u/dons useful. 
This is great. Thanks for the link.
Sure, but you would have to write a proposal to the committee. They don't decide things unilaterally
I've read through the tutorial. It's very useful for creating a bot, and I think it also is a good example of what I mean by a request-respond setup. The bot is always `listen`ing to requests and providing responses using `write`. For a proxy, we should listen on both ends, and be able to send to either handle. For instance, maybe we'd have some function that would listen on both Handles, data PairChatter = LeftStatement String | RightStatement String deriving Show getlinePair :: (Handle, Handle) -&gt; IO PairChatter (not sure I got that signature right). Anyway, I'd like to be able to listen on a pair of socket handles for incoming messages, and have the output indicate which direction that message is going. I apologize if I'm not describing this well enough.
&gt; Without [the operators], Haskell would pretty much look like LISP. Some of us actually think that would be a huge improvement. As a long-time Scheme programmer, I very much appreciate the fact that everything has *names*, most often spelled out in *full words*. The other problem with all the operators in Haskell is that you get different library authors pick the same operator for two completely different things. Qualified imports of spelled out names are not a big problem, but the same thing with operators is just ugly.
&gt; Since hierarchical modules were introduced, people started using Data.* and Control.*. Without a real pattern. This. And what's worse, you end up with multiple packages that provide the same module. Frankly, I think the Java package naming style would have been a better idea.
Use `pipes-network` to listen for incoming connections and fork a handler for each new request. You can then pipe data from that connection to your IRC server. I will write up a more detailed skeleton for you tomorrow. Edit: Use `k0001`'s skeleton. He's the authority on this.
Thanks. Yes I was thinking maybe composing pipes together might be the way to go. I'm just a bit fuzzy on what to do after forking, when we need to hold onto that incoming connection and also initiate a connection to the remote server, then set up listeners on both connections simultaneously.
Have you tried GHCi mode in Leksah? If so, I would be interested to know what bugged you most about it?
Will "Who ya gonna call" also be uploaded later? 
I like the video, but I think that the statement that Haskell is often faster than C++ is unfortunate. Haskell's performance is quite impressive and I think realistically performance will only seldom be a reason why one can't use Haskell. But I don't know of any applications where Haskell outperformed C++ while I know of a few where the opposite is true (the infamous shootout as one example). I am afraid claming that Haskell is often faster than C++ can be seen as disingenuous.
I've got it all processed and ready to go. It should be up tomorrow. 
Perhaps this skeleton using `pipes-network` will help you to get started. It listens locally at TCP port 9000, and forwards both incoming and outgoing data to irc.freenode.org at port 6667. {-# LANGUAGE OverloadedStrings #-} module Main where import qualified Control.Concurrent.Async as A import Control.Proxy ((&gt;-&gt;)) import qualified Control.Proxy as P import qualified Control.Proxy.TCP as T main :: IO () main = T.withSocketsDo $ do T.serve "127.0.0.1" "9000" $ \(sock1, addr1) -&gt; do putStrLn $ "Incoming connection from " ++ show addr1 T.connect "irc.freenode.org" "6667" $ \(sock2, _) -&gt; do a1 &lt;- A.async $ do P.runProxy $ T.socketReadS 4096 sock1 &gt;-&gt; fooD &gt;-&gt; T.socketWriteD sock2 P.runProxy $ T.socketReadS 4096 sock2 &gt;-&gt; barD &gt;-&gt; T.socketWriteD sock1 A.wait a1 You will need to replace `fooD` and `barD` with your custom PRIVMSG interception code, possibly using the `pipes-concurrency` facilities to coordinate between them. Consider using `pipes-attoparsec` together with the `pipes-parse` lower-level facilities for parsing the bytes flowing through these pipes. If you remove `fooD` and `barD`, then you have built a simple TCP tunnel :) The [pipes-network documentation](http://hackage.haskell.org/packages/archive/pipes-network/0.5.1.0/doc/html/Control-Proxy-TCP.html) is quite extensive and friendly, be sure to check it, and the [pipes-concurrency tutorial](http://hackage.haskell.org/packages/archive/pipes-concurrency/1.2.0/doc/html/Control-Proxy-Concurrent-Tutorial.html) will certainly guide you on how to coordinate between different proxies running concurrently.
Your effort is greatly appreciated!
Get homebrew. Everything is easy from there (`brew install haskell-platform` or just `brew install ghc`).
I always just install the Haskell Platform, and I've never had problems with that. For more bleeding edge things, the GHC installer also works without problems. You can have multiple GHC's and associated platforms installed without problems. One thing to note is that the default .cabal/config that Cabal or the platform installs is sub-optimal. I'd just comment out all paths in the `install-dirs` section, as well as the `symlink-bindir` option. The defaults are better.
The even bigger problem is that for Haskell to be competitive with C++, the code you write will in many cases not be as straightforward and easy to maintain as your ordinary Haskell code. You're trading clarity for performance, so it's hard to argue for both things at the same time.
ouch
I use Modules [1], a clean ghc install with cabal-dev. [1] [http://modules.sourceforge.net/](http://modules.sourceforge.net/)
Have you looked at [typed tagless final interpreters](http://okmij.org/ftp/tagless-final/course/index.html)? I'm wondering if they would help, or (if not) why they wouldn't. Either way, those lecture notes have a vote from me as one of my favourite papers.
Is the slide deck available?
Seconded. Not just for Haskell; you really want [homebrew](http://mxcl.github.io/homebrew/) in your Mac life.
How long does it take heist to reload when you have a few hundred templates? And does the complexity of the templates make much difference in loading speed? Would it be worth giving heist the ability to reload individual templates?
The haskell school stuff doesn't work in opera. Makes for a pretty bad first impression.
With regards to operators, I think a fair number of people know that + and - are 6, and * and / are 7. And while they may not know : is 5, they likely know it is "lower than math stuff". And I really don't see how this is a problem, people tend to know as much as they need to know. They don't know the exact precedence of every operator because there is no need to know that. It is easy to lookup on the occasions you do need to know.
While I agree with homebrew across the board, I think the pkg installer is easier and sets up better defaults. It switches on profiling, documentation, modifies your path, and all that nice stuff.
Interesting. Does the HP package not have good defaults?
In my experience, homebrew doesn't build the Haddock documentation and the cabal config generated by cabal-install from homebrew is suboptimal. The one from the .pkg installer (the "default" way to get the Platform on OS X, I guess) is a little more sensible, to my mind.
I really should finish updating my slides for "who yah gonna call" then
Johann and Ghani's "Initial Algebra Semantics is Enough!" https://personal.cis.strath.ac.uk/neil.ghani/papers/ghani-tlca07.pdf Still getting new insights out of this. (edit: and on reskimming, i notice this delightful footnote: "Although the price of lengthier code and constructor pollution is unfortunate, we believe it is outweighed by the benefits of having an implementation.") McBride and Paterson's Applicative Programming with Effects: http://www.soi.city.ac.uk/~ross/papers/Applicative.html Hard to believe all this stuff only got introduced in the late 2000s, and only started to come into common usage a little while ago. Wadler's stuff on parametricity: http://homepages.inf.ed.ac.uk/wadler/topics/parametricity.html On the more theoretical side, the foundational stuff that its hard to imagine 'life before'. (Consider this a vote for Reynold's "types, abstraction and parametric polymorphism" as well: http://www.mpi-sws.org/~dreyer/tor/papers/reynolds.pdf as the predecessor to the above stuff / denser version of which the above recognizes certain implications of) spj's stg paper: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.3729 key for getting at least one good mental model of how to think of haskell, operationally. oh, also can't forget okasaki's thesis/book on functional data structures: http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf i haven't needed nearly anything he's actually introduced, but the debit method he explains for reasoning is v. important and hinze and patterson's finger tree paper is also very impressive for data-type tricks, and the monoidal accumulators they introduce combined with their bounds actually makes it pretty handy all around: http://www.soi.city.ac.uk/~ross/papers/FingerTree.html
Well as you can see in the video, the application that I demonstrated has 171 templates. So that's already getting close to your "few hundred". I think the time complexity of loading should be close to O(n) where n is the total number of bytes in all your templates, so I don't think loading speed is an issue for now. I don't think individual template reloading would be worth our effort until it's actually a problem for someone. And actually, Heist already has the capacity for reloading individual templates. In 0.12, we switched to defining template repositories with an IO action [1] [2] instead of a list of templates. So you should be able to implement individual template reloading on your own right now by writing a more sophisticated IO action. [1] http://hackage.haskell.org/packages/archive/heist/0.12.0/doc/html/Heist.html#t:TemplateLocation [2] http://snapframework.com/blog/2013/05/15/snap-0.12-released
Lovely selection, thanks for sharing!
Strangely, the bsod thing hasn't happened since. No idea what that was about. Don't have much choice about Windows though, our entire entity resolution and information management platform is on top of Microsoft tech, and it's what most of our customers run. Multi-million dollar software deals trump personal OS preference every time. 
Next meetup on Wednesday, June 26: http://www.meetup.com/NY-Haskell/events/124061272/
Thanks so much.
Well, there's also an immutable law of the universe that when you give a demo something will go wrong.
You don't actually use id for anything. You only want it as a filler function when a function is asked for but you don't want to do anything. For example, `Data.Maybe` provides a function called `maybe :: b -&gt; (a -&gt;b) -&gt; Maybe a -&gt; b`, to read a value from a `Maybe a` (a value that can be either a `Just a` or a `Nothing`, to represent values that may not exist) but sometimes you don't actually want change it while reading it, but just provide a default value in case of `Nothing`: so you do `maybe 0 id` whatever. That return `0` in case of `Nothing` and the number inside `Just` otherwise.
I'm apparently unique in that I just use the generic binary build. 1. Download whatever version you want from http://www.haskell.org/ghc/ and ignore the warnings that you really want the platform. I really don't want the platform. 2. Unzip and install it locally, versioned 1. ./configure --prefix=$HOME/ghc-version 2. make install 3. create a symlink in my home directory: ln -s ghc-version ghc 4. add $HOME/ghc/bin to my path 5. download the tarball from http://hackage.haskell.org/package/cabal-install 6. unzip the tarball and run setup.sh 7. add $HOME/.cabal/bin to my path To install multiple versions of GHC, just do steps 1, 2, and 3 again. Switching the symlink lets you change versions of GHC rapidly. To fully remove GHC and everything associated with it, remove $HOME/.ghc $HOME/.cabal $HOME/ghc $HOME/ghc-version and undo whatever path changes that were made. I really like this setup because it's simple, and under my full control. But apparently, I'm weird in liking this.
ah no, I don't check irc that often. but i will log into that room now. i'm isomorphismes on freenode.
Hmm, I don't actually remember the actual reason I switched. It could have been that homebrew had a package I wanted that macports didn't have. But I also remember other people telling me they thought brew was better. Since I switched I've been perfectly happy with it.
1. Install XCode. 2. Install the console development tools from XCode. 3. Download and run the Haskell Platform installer. 4. ??? 5. Profit You may eventually want to use `brew` to install a few C libraries that some Haskell packages can use, but that can be a long way down the road.
What am I missing here? I tried the ":{ ... :}" business in every way I can think of, and ghci just gives me "unknown command" or "parse error" no matter what I try.
Oh, I missed that bit about it being new to the latest haskell platform, oops.
This is getting added to haskell-mode today (on GitHub), with some additional support such as letting you select from all possible GHC language pragmas when expanding "lang", for example.
Haskell Platform and you're good to go :)
I'm all for it, yasnippet is very handy, been using it for a while (I started when I was using OCaml and wrote the first yasnippets for ocaml - so dead simple to configure and use!). Are the haskell-mode guys (that's mostly Christopher right?) ok to fold this into haskell-mode?
Yeah, but the problem is that his slides aren't enough for an introduction to that topic. I did read his slides back in the time and took a look again when writing this however, as well as several other articles, SO posts, blog posts and whatnot.
It would be cool if there was a compiler frontend that could compile a small subset of Haskell. Like Fay but for embedded development. I would love that for my hobby AVR projects :)
btw, you should consider adding some conditions so that some expansions which make sense only at leftmost position don't get offered otherwise, e.g. I use the following in my snippets: # name: main :: IO # key: main # condition: (= 4 (current-column)) # expand-env: ((yas/indent-line 'fixed) (yas/wrap-around-region 'nil)) # -- main :: IO () main = do $0 return ()
Arguably you are doing the same by writing it in C++. Except you still get advantages from the type system in imperative-like Haskell code.
Would be great if you could try out the latest dev version of Leksah and let me know how you find it. Press the debug button on the tool bar or select something to send to GHCi and press Ctrl+Enter. * http://leksah.org/packages/leksah-0.13.2.4-ghc-7.0.3.exe * http://leksah.org/packages/leksah-0.13.2.4-ghc-7.0.4.exe * http://leksah.org/packages/leksah-0.13.2.4-ghc-7.4.1.exe * http://leksah.org/packages/leksah-0.13.2.4-ghc-7.4.2.exe * http://leksah.org/packages/leksah-0.13.2.4-ghc-7.6.3.exe There is a WebKit based output pain that displays the results. In GHCi mode build becomes ":reload" and the last command sent to GHCi is repeated after the reload.
I've found that I don't know the precedence levels because I don't *need* to know them. When I'm writing code, the type system will almost always tell me which one is wrong--the most annoying cases are actually things like + and *, because they have very uniform types! In ambiguous situations, I almost always add extra parentheses. When reading code, I just assume that the code works. This, combined with the type system, lets me get the essence of the code without having to worry about which operators take precedence--if the precedence was different, the code simply wouldn't work! Again, there are some exceptions to this, but most operators are nice enough when you consider the types.
Oh! Thanks for that.
:{ and :} have been around since at least GHC 6.10.1 (so any version of the Haskell Platform should have them). The oldest GHC I still have installed is 7.0.3 and it works ok... ~$ /Library/Frameworks/GHC.framework/Versions/7.0.3-i386/usr/bin/ghci GHCi, version 7.0.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. Prelude&gt; :{ Prelude| putStrLn Prelude| "Hello" Prelude| :} Hello
Sublie text 2 with sublime haskell plugin Add the dependencies on the sublime haskell preferences path
What's all this about type sigs at the end? That's not needed afaik: Prelude&gt; let x :: Int; x = 5 Prelude&gt; x 5 
The issue is with transforming function signatures to let-style in the repl. With this syntax the standard map :: (a-&gt;b) -&gt; [a] -&gt; [b] map f [] = [] map f (x:xs) = f x : map f xs isn't valid. Try this in ghci: :{ let map :: (a-&gt;b) -&gt; [a] -&gt; [b] map f [] = [] map f (x:xs) = f x : map f xs :} 
I tried it in GHCi, it worked just fine. ghci --version gives: &gt; The Glorious Glasgow Haskell Compilation System, version 7.6.1 
Hrm, I was using the 2012.4.0.0 platform at the time. Maybe an update in the new compiler?
Is the mystery combinator you're looking for simply `seq`? Wasn't sure what you were asking for here.
macports installs things in places it probably shouldn't (places that need root access), which can cause issues with other programs (and build scripts). also see https://github.com/mxcl/homebrew/wiki/FAQ#why-does-homebrew-say-sudo-is-bad
Glad I could help with something. :)
What winterkoninkje says should work. You basically need to pass the expression parser used in recursive branches as an argument to the parsers for each constructor. It might be worth defining a Parsible class for each constructor, and define the coprodP and fixP as instances once and for all, but that's a minor point.
&gt; measure desc = length . filter not . dropWhile not . map head . group $ desc Oh the pain! Please measure = length . filter not . dropWhile not . map head . group
Would you guys advice learning emacs just for the sake of haskell-mode? Vim/sublime user here
sublime has a very nice snippet mode and yasnippet are very similar to text mate ones. .. I'm sure you could port w/o too much hassle
Wouldn't this be a good use of typeclasses instead of having functions as fields of a concrete data type?
I have added this to my reading list since I don't have time to read it right now. However, can anyone comment on how this relates to [Tekmo's lens blog](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html)? At a first glance the approaches seem similar (and possibly complementary?).
As far as I can tell, the article is trying to solve a different problem - how to abstract different data structures behind the same interface.
Then you'd have made the classic [typeclass antipattern](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) mistake. 
Good lord of all that is well typed: &gt; formWidget= wform $ do &gt; (n,s) &lt;- (,) &lt;$&gt; p &lt;&lt; "Who are you?" &gt; ++&gt; getString Nothing &lt;! hint "name" &lt;++ br &gt; &lt;*&gt; getString Nothing &lt;! hint "surname" &lt;++ br &gt; &lt;** submitButton "ok" &lt;++ br What hath science wrought?
So many operators!
You should make the rotation base case be `rotateR [] = []`. It makes your function total.
Some standard operators: * Functor: `&lt;$&gt;` `&lt;$` * Applicative: `&lt;*&gt;` `&lt;*` `*&gt;` `&lt;**&gt;` * Alternative: `&lt;|&gt;` * Monad: `&gt;&gt;=` `&gt;&gt;` `=&lt;&lt;` `&lt;**` looks like it is an Applicative operator, but it is not. `&lt;&lt;` looks like it is a Monadic operator (and ambiguous as to whether it is `flip (&gt;&gt;)` or a retyped variant of `&lt;*`), but it is not. The other operators don't intrinsically mean anything, but do not stomp over my expectations in the same way. (They do look vaguely Functor/Applicative/Alternative to me, but not to the same extent.) I think it's a good idea to avoid naming operators so that they suggest different Haskell concepts. (Yes, this is subjective to some degree.)
A good rule of thumb is that if your type class doesn't have any laws then it might be better for it to be a record. Without these laws, users of the type class cannot equationally reason about its behavior without consulting the source code of each instance.
I see... I am going to change to another computer soon, so I guess I'll switch to homebrew there.
Looks like it. I'm going to submit a pull request after I stabilize the code (see hvr_'s suggestion as an example).
I'd say these two approaches are complementary. This post is all about how to translate object-oriented style to Haskell without abusing type classes and my lens post is more about how to translate imperative style to Haskell.
Can you send me a link to Sublime's snippets? I'll take a look and see if I can pull some of their good ideas.
it might be easier to learn evil then. i've gone evil for a few weeks when learning haskell but found it not worth it for me.
Now all we need is a quantum computer!
I don't use sublime, I just have looked at their snippet function. But: https://github.com/SublimeHaskell/SublimeHaskell and http://sublimetext.info/docs/en/reference/snippets.html 
Very cool!
With 30 trillion gates...
Wow, this is *exactly* what I was about to start working on! Hurrah, more things I don't have to do :)
So yeah. Basically what's demonstrated here, is that applicative functors are context-free and monads are just applicatives with context. I got this eureka moment when I wrote a scheme parser. I wrote the entire grammar with applicative functors (context free). But, for example, when I wrote a golang parser, I suddenly needed context. Because top-level variable definitions have other constraints than block-level variable definitons. So I had to inspect the value of the parse to decide how to parse further. the `&gt;&gt;= :: m a -&gt; ( a -&gt; m b) -&gt; m b` makes total sense there. You basically "Inspect" the value `a` inside the monad and then decide whatever you want to return based on that inspection (b). 
It all makes sense if you ignore the operators. Of course, I would have no idea what operators to use. We need one overloaded super-operator to rule them all. :)
Informally, a "law" is an equation that methods of the type class must obey for all instances. The monad laws are defined and you can find them here: http://www.haskell.org/haskellwiki/Monad_laws The compiler does not use or verify those laws. They are purely an honor system. More advanced languages like Agda can enforce those laws, but Haskell does not. However, intentionally violating laws is highly frowned upon because it breaks code that assumes those laws are correct. The way the monad laws specifically translate to Haskell is that they formalize our expectations for how `do` notation should behave. For example, we expect that re-returning a value is redundant: do x &lt;- m = do m return x We also expect that `return` has no effects and so therefore can be replaced with `let`: do x &lt;- return y = do let x = y ... ... We also expect that we can nest `do` blocks and they will do the right thing (I won't type this out because formatting it is hard, but you can find it on the link I gave you). All three of those expectations are the monad laws.
&gt; The compiler does not use or verify those laws. They are purely an honor system. Though verifying them with quickcheck, where possible, is good practice.
This may help my code to split and merge modules.
My problem with this presentation is the same as my problem with the original formlets - it tightly couples the view and the form itself. Not only does this lead to some pretty funky code in places, it has a significant drawback of field naming, which makes it almost undrivable from anything but a browser.
I'm hoping it will help me write sloppy Haskell and then later magically rewrite it have fully qualified imports - that's my main want at the moment.
&gt; More advanced languages like Agda can enforce those laws If you are interested how Agda can enforce those laws, read [Chapman et al.](http://cs.ioc.ee/~james/papers/AssistedMonads2.pdf). Ironically they use records to store the proofs.
* &lt;** is &lt;* but it executes the right side even if the left side does not validate * &lt;&lt; add HTML content to a HTML tag * &lt;++ and ++&gt; append and prepend HTML to a formlet * &lt;! add HTML attributes to the formlet * &lt;&lt;&lt; encloses a formlet within a tag . It has the lowest priority This line div &lt;&lt;&lt; p &lt;&lt; "enter a string ++&gt; getString Nothing &lt;! [("attrib","value")] &lt;++ b &lt;&lt; "Thanks" Has type: View Text.Blaze.Html.Html m String and produces: &lt;div&gt; &lt;p&gt; enter a string" &lt;/p&gt; &lt;input attrib= "value" name= ... type="text" /&gt; &lt;b&gt; Thanks &lt;/b&gt; &lt;/div&gt; br, p etc are Blaze-html tags MFlow accept any kind of formatting library as long as it has a FormInput Instance. Currently it supports blaze-html, xhtml and HSP
I have written this, it is in http://src.seereason.com/module-management (most of which is still a work in progress.)
It is not so difficult after a while using them. As you said, if you ignore the operators, it all makes sense. Once arranged the elements, the connector to use depend on the types of the elements. Do I need to connect HTML to a formlet? I use ++&gt;. It is in the opposite order? then use &lt;++ . It is a HTML content for an HTML tag? I use &lt;&lt;. Attributes? I use &lt;!. The rest are ordinary applicative operators (that have their own learning curve, I admit) The operators mix seamlessly. There is only one operator that need explicit parenthesess in MFLow: the &lt;&lt;&lt; operator , that embeed a formlet within a HTML tag. The &lt;&lt; operator is present in many HTML combinator libraries, such are Text.XHtml. But Blaze-Html uses $ and a monadic syntax that is not appropriate for formlets. That is why I added &lt;&lt;. ++&gt; and &lt;++ are present in the reform package with almost the same purpose, except that it is used for adding HTML for formlet labels, rather than for HTML in general
That is nothing. In the link of the article, there is a main menu with 22 options connected by &lt;|&gt; operators with HTML embedded. https://github.com/agocorona/MFlow/blob/head/Demos/demos.blaze.hs This shows how composable are the Haskell DSLs. However, one problem of Haskell is that it is not possible to produce true DSLs until the issue of the error messages is not solved. A DSL must produce error messages in terms of his own domain, not in the domain of Haskell. That is a problem for the acceptance of Haskell in industry. Anyway most of the time the operators used here mix seamlessly and there are a limited set of errors that can be learned.
I just want a good import alphabetizer and organizer that doesn't mess with any of the rest of my code!
https://github.com/LukeHoersten/shnippet/commit/485bd86d615cbf20de6f96b77d735b6dad36b689 Changes made. thanks for the suggestion.
Thanks. They didn't have too much there but it gave me the idea for a few tweaks.
That is exactly right. And this is the reason why to achieve the dynamic behavior in formlets to react depending on the user input, we need a monad instance for formlets. Since the links in MFlow are also formlets that return fixed values, it is possible to create quite dynamic applications not restricted to form inputs, but any kind of interaction.
silly question, but when you say laws, do you mean common, idiomatic laws that we all know like functor, monad, applicative, comonad, etc? Or could we have our own typeclass with it's own laws? 
&gt; I've read things like "ListT m a doesn't obey monad laws". Are those laws defined? Where? Is it Haskell syntax? Does the compiler use those laws to do type inference or even optimizing code? "Laws" are just properties that cannot be expressed in the type system. It's the rough equivalent of, in Java, saying that the input cannot ever be null (or else bad things will happen). Laws typically take the form of equations. Because Haskell has roots in category theory, many of these equations are dictated by the algebraic laws these structures have in math. Just like in algebra, where we know `x + y = y + x` for any real numbers x and y, we also know that `x &gt;&gt;= return = x` for all x of type `Monad m =&gt; m a`
The above is Haskell code... Does it not do what you want?
What an amazing idea. I contort myself into some ridiculous shapes at my desk at work sometimes. 
This is quoted as a comment at the bottom: &gt; I have several problems with your article. First of all, the article do not have in my opinion a well defined target. The reader should be already familiar with haskell to understand it, but it isn't particularly useful for an experienced haskeller. Moreover, the article basically explains how to write an object oriented program in haskell. This is not functional programming.. This is not how a program should be designed in haskell. You shouldn't start by assuming you have some class GameObject which some methods and some subclasses like Apple and Banana. This is OOD! I disagree on the first point. I'm familiar with haskell and it's syntax, but I'm not experienced. I've done lots of toy programming to get my head round the concepts but I've not really done anything big with it. So I guess my question is, who is right here? Is the above good design? If not, what is the best way to solve the typical game design problem of modelling lots of "things" that share some functionality and state but differ in other ways? 
`&lt;**` deliberately alludes to `&lt;*`, despite working only with formlets. For the other operators, `&lt;` and `&gt;` are to be understood as arrows, and the similarities to Functor/Applicative/Alternative operators are cosmetic and coincidental (they're more cons-ish and snoc-ish). (It's my understanding that `&lt;*&gt;` was originally an asciification of [`⊛`](http://www.fileformat.info/info/unicode/char/229b/index.htm) or some similar character.) `&lt;++` and `++&gt;` deliberately allude to `++`. `&lt;!` makes me think of HTML doctypes, though it is alluding to blaze-markup's `!`; it's unfortunate that can't be reused (there is a typeclass). `&lt;&lt;` makes me think of `&gt;&gt;`, but they mean completely different things. Am I odd for having this association, or do other people have it too? I suppose perhaps some people avoid `&gt;&gt;=`/`&gt;&gt;`/`=&lt;&lt;`, stick exclusively to do notation, and so don't really recognise these operators. Thinking up a good operator symbol is harder than thinking up a good function name.
Theoretical laws are preferable, but even having just a couple of common-sense equations is better than nothing. It can be something as simple as: read . show = id The point of these equations is just so that users know how the type class methods interact with each other so that there are no unexpected surprises. Think of these equations like a simple contract with the user of the type class, and these equations make for excellent tests.
Did you also update your blog post with the new formalization? I still find an OList mentioning u.
I'm really not sure who's right. I've seen this kind of thing advocated in a few places. Things I don't like about it mostly revolve around difficulties with closures in general - they're not inspectable and don't play well in certain scenarios, "first class citizenship" notwithstanding. In particular, this seems to yield GameObjects I'm not going to be able to put in AcidState, or derive Show/Read (or Arbitrary) for... This article and the other places I've seen make a good case that this approach is workable, but I've yet to see a strong argument that it's genuinely preferable. In particular, while working with a closed type does mean that I have to make changes "all over" when I add to my type, if I've been factoring things well the only places I am actually pattern matching are genuine corner cases where I should be paying special attention anyway. Of course, this could very well be an artifact of the size or scope or nature of the Haskell work I've done - it's sufficiently small that a lot of seeming patterns are likely to be artifacts. Of course, big closed data type doesn't work so well in providing a library interface, since the user shouldn't be expected to go through and touch the internals... so there's a better argument there. On the whole, I'd say that *for me* it's still an open question, and I'd love to hear more thoughts from others, to either figure it out better collectively or pin it down better in my head.
It is very weird that convention used in the papers is to write code like this: a &lt;- hadamard a when the value on the left-hand-side is *not* a new qubit but just the qubit `a` returned back, especially given that the documentation says that one could equivalently just have written hadamard a which makes it more clear that `a` is being modified in-place, rather than visually suggesting (as the conventional notation does) that a new qubit has been generated that holds the result of the operation. The authors justify this convention as anticipating a future version that uses linear types, which in itself is a good idea: in the quantum world you can only mutate qubits (due to the no-copying theorem) so a linear type system within a functional language is a natural fit. Having said that, Quipper is an EDSL in Haskell, and Haskell shows no sign of implementing linear types in the foreseeable (or any other) future; thus, adopting a convention that is designed for a type system with linear types *when there is no sign of linear types on the horizon* makes no sense and just ends up being misleading about what is going on under the hood.
Would anyone mind explaining this? I only recently got my head around catagory theory.
The term "gate" is somewhat misleading; you should think of a "gate" in the context of quantum computing as being much more akin to an instruction for a classical CPU than a physical piece of hardware. Thus, it is more accurate to think of Qupper as claiming to be able to compile programs that involve up to 30 trillion instructions. The real limit that we are having trouble with is not the number of "gates" but the number of simultaneous physical qubits and the noise levels.
It's a 600 page book. I suggest asking a more specific question.
Homotopy Type Theory is a recent advancement in the area of dependent types. Think Agda, Coq, Idris-style languages if you're familiar with them... otherwise think GADTs on supersteroids gone berserk. Dependent types allow you to be extremely precise with your data types. You can talk about not just lists or lists of strings.... but also lists of strings of length n (for some natural number n). In the far future, it may be the key to getting fast-as-C performance (think removing bounds checking on arrays completely safely) and software verified correctness of a program simultaneously. This isn't software, though. This is a math book. There was a realization a few years ago that equality types (the ability to express x = y in the type system) gave rise to a mathematical structure called a weak ω-groupoid which was giving homotopy and category theorists a hard time. Homotopy Type Theory (HoTT) is a typed lambda calculus that makes studying these things easier. In fact, every data type corresponds to (a very boring) weak ω-groupoid. What this allows mathematicians to do, though, is to create new interesting data types corresponding to more interesting examples of these things. You get a data type for a Circle, or a Sphere, or a Torus. You can define functions between them via recursion the same way you'd define a function on lists or trees. These new fancy data types are called higher inductive types, and while they don't (currently) have any use for programmers, they pay the meager salaries of long beards in the ivory tower. The other novelty of the theory might be more interesting for programmers some day (at least if you believe dependent types will save the world). A guy named Voevodsky proposed a new axiom called the Univalence Axiom that makes HoTT a substatial alternative to the former foundations of mathematics. The Univalence Axiom formalizes a practice mathematicans had been using for a long time, (despite its technical incompatibility with ZFC). tl;dr, the Univalence Axiom says that if two data types are isomorphic then they are equal. Eventually, this axiom may allow a programmer to do some neat things. For instance, a programmer could write two versions of a program -- a naive version and a "fast" version. (Currently, all programmers only write the "fast" version). If you want to formally prove your "fast" program doesn't suck, it's nasty. However, it might even be humanly possible to prove some correctness about the naive version. The Univalence Axiom (once given "computational semantics") may be able to let us prove things about the dumb, slow, reference implementation of a program or library, then transfer that proof of correctness to the fast one. To give a small example for anyone familiar with a dependently-typed language, you may notice that in Coq and Agda and whatever, the first data type you learn (and one you stick with for a long time) are the *unary* natural numbers. That is, you have 0, and 0+1, and 0+1+1, and 0+1+1+1, etc.. We use unary numbers because they are reaaaally easy to prove stuff about. But as any programmer might guess, actually *doing* anything with them is suicide. The Univalence Axiom would allow us to keep on working with unary numbers for all of our proofs, but then swap them out for actual, honest-to-God 2's complement representations when it comes time to run the program. So there's that. Not everyone cares about software correctness, though. But if you're sold on category theory, here's a neat trick. You probably know that equality becomes a hairy, nasty thing in category theory. Two objects can be equal or isomorphic. If you move onto 2-categories, two categories can be equal, isomorphic, or equivalent! And for higher category theory, you end up with even more notions of equaltiy, isomorphism, equivalence, etc, etc. In a univalent foundation of category theory (which appears in the later chapters of this book), we see that all of these notions of equality collapse down into just one. If two things are isomorphic, then they are, by definition, equal to each other. You no longer have to worry about that stupid squiggle over your equals signs, because univalence means that every construction must respect the structure of your data. There are no leaky abstractions in your data types!
Isn't that basically how category theorists treat things anyway, isomorphism = equals?
Yes, it is! Homotopy type theory makes this a formal part of the language, rather than an informal convention. 
Fascinating—I'm gonna get off reddit and keep digging in then.
Not quite. It's easy to get that impression reading nLab, though. On-the-nose equality is still very important when your foundation is Set theory (as it is for virtually all mathematicians). This is why category theorists distinguish between FinSet (the cat of finite sets) and FinOrd (the cat of ordinals of finite sets). These two categories are equivalent, but FinSet is a large category and FinOrd is a small category. 
Just so you know, your comment is getting some love [over at HN](https://news.ycombinator.com/item?id=5916522).
Well, my biggest question is how does this relate to Haskell. Is it possible that Haskell could someday use this or would there need to be a new Haskell-like langauge that has Homotopy Types? 
&gt; The Univalence Axiom (once given "computational semantics") may be able to let us prove things about the dumb, slow, reference implementation of a program or library, then transfer that proof of correctness to the fast one. My naive guess is to arrange it so that an isomorphism of data structures implies the existence of a bisimulation of any programs that act on those data structures 'in the same way'.
GHC is a research language after all. Maybe linear types will be implemented as part of this project! /optimist
If I was able to mostly follow that, will I be able to mostly follow this book?
Go ahead and see. The first two chapters are easy going if you've done Agda before and are comfortable with mathematical writing. 
I've been transitioning to evil, lately. I'm actually working on a project like Emacs Prelude for evil users. With any luck, I'll have a good working system to put up on github by next week, if you're having trouble acclimating. Lots of Haskell-specific goodness, too. I really like the power of vim text objects, but having a real programming language underneath makes Emacs something of a joy to work with.
I cross posted here (and r/math) because I felt the audience interested would also visit this subreddit. It's not directly Haskell related, but could very easily inspire new Haskell and functional programming research.
Remember [this year's April fool's post](http://www.reddit.com/r/haskell/comments/1bfojn/functor_is_now_a_superclass_of_monad_in_ghc_head/) about it? Take that, devious author!
Agda's record system and "type classes" are the same thing, so it's not really that ironic. They would use type classes if such a concept really existed in Agda.
Oh, I hadn't pushed that through to higher categories: Lawvere tends to minimize the distinction between FinSet and FinOrd, but not in the same way as he refers to "the limit" or anything like that.
If fmap was called map, map could still be called map, since it's just a specialized version of the same function.
Except that some people want to use (the explicit definition of) `map :: (a -&gt; b) -&gt; [a] -&gt; [b]` for didactic purposes. I think there should also be a `SimplePrelude` for teaching Haskell, which would make little use of type classes. I believe the current `Prelude` is so inelegant because it tried to find the (non-existent?) middle road between didactic use and productive use.
Well, you could always restrict with an explicit type signature... but fair point. The singnature you give here is actually more restricted than the one provided in Prelude - not sure if that's intentional or a typo, since it seems it could also be useful instructionally.
:) I'm very happy to see this change.
essential functions you need to implement are : -new(graph_name) -add_vertex(vertex_name, graph) -add_edge(from, to, graph) -remove_vertex(vertex_name, graph) -remove_edge(from, to, graph) 
Emacs haskell-mode certainly seems to have the critical mass of the community behind it. By the nature of Emacs, it's just so damn extensible it's hard to beat. If something cool pops up in another editor, someone usually codes up an Emacs version pretty quickly. These are my anecdotal observations.
I've been iterating pretty fast based on user feedback etc. Shnippet now has some pretty unique features like qualified module name definitions (ex: module Snap.Snaplet.MySnaplet where...), qualified input auto-naming (ex: import Data.Text as T -- the T is auto), and John Wiegley has been adding haskell-mode integration with auto-completing language pragmas.
Indeed! And about time!
Note that this change would break *tons* of code. It is well worth it. And not without precedent. The `Eq` super constraint on `Num` was removed not that long ago.
Any timeframe? 
I found it weird that they broke Num just like that, but took so much longer to fix Applicative=&gt;Monad, which is easily a more important issue and possibly less painful.
The real question is whether or not it will break the packages that everybody uses, like the Haskell platform or other popular packages. Most of these packages already make `Applicative` a superclass of `Monad`. If it breaks some unmaintained package that somebody abandoned on Hackage a long time ago then it is not a big deal.
We can only guess. The warnings will most likely be in 7.8, at which point the early birds can start fixing Hackage, however only widespread adoption of 7.8 will make sure people update their packages. Until everything is done, I would estimate one year with a large error bar.
The issue was about the constraint in the `cons` case. Before I had l ≤̂ ⟦ x ⟧ u But now, as pigworker suggested, I have l ≤̂ ⟦ x ⟧ This doesn't change the fact that we need an upper bound.
And there was much rejoicing!
&gt; join is promoted into the Monad typeclass my category theory heart is pounding once more :)
To get unboxed no-GC performance in Haskell is harder and sometimes impossible. Haskell doesn't beat c++ on that front, not even close. If I was a non Haskeller my bs alarms would be going off. They shouldn't try to sell Haskell as something it's not.
`Monoid m =&gt; (m -&gt;)` is called the `Traced` comonad, fyi :)
I totally agree. The focus should be on the true strengths of Haskell. Performance is a ridiculous thing to sit and argue about anyways. It really comes down to an application by application basis. The amount of software that requires the fine grained low level control over performance that C++ provides is likely very low (furthering your argument they shouldn't of been talking about it in the first place). I think emphasizing Haskell's speed compared to Python/Ruby is important, and even comparing it to say C#/F#/OCaml. But there is no point in comparing it to such low level languages as C/C++/Assembly.
This is great, but not nearly as useful as removing `pure`/`return` for deferment to a type-class lower down. If we are going to implement these kinds of changes, why not go all the way? [semigroupoids](http://hackage.haskell.org/package/semigroupoids) To clarify, I mean a hierarchy similar to this: {-# LANGUAGE NoImplicitPrelude #-} class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b class Functor f =&gt; Apply f where (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b class Apply f =&gt; Applicative f where pure :: a -&gt; f a class Functor f =&gt; Bind f where (=&lt;&lt;) :: (a -&gt; f b) -&gt; f a -&gt; f b class (Applicative f, Bind f) =&gt; Monad f where class Functor f =&gt; Extend f where (&lt;&lt;=) :: (f a -&gt; b) -&gt; f a -&gt; f b class Extend f =&gt; Comonad f where extract :: f a -&gt; a 
`Pointed` by itself doesn't have any laws. How do I know that I wrote a correct `Pointed` instance?
Can somebody please explain to me the laws for the coalgebra of a comonad? I read [Russell O'Connor's post](http://r6research.livejournal.com/23705.html) on the subject but did not understand the theoretical origin of those laws. I'd be more than willing to read a paper on the subject if somebody can point me to one.
`Pointed` might not (well, it has to be natural, but that is assumed), but `Bind` certainly does (associativity). `Apply` also gives an associativity condition, but I don't think it is as impressive as what you get with `join` in `Bind`. There are many functors which are `Applicative` in *multiple ways* and some, shocker, which are `Monad`s in multiple ways. Which is the right instance?
&gt; There are many functors which are Applicative in multiple ways and some, shocker, which are Monads in multiple ways. Which is the right instance? Who says there has to be one "right" instance? There's potentially a default instance, but there's always newtype...
I've really been wanting a mainstream Copointed, though I've not thought about it enough to come to any conclusion about whether it's genuinely a good idea...
Very cool, but given these changes does MonadPlus continue to have a point to exist? That is, is the only reason it is being kept around historical accident, or does it offer functionality on top of Alternative that I am missing?
There are laws governing how mzero/empty and &gt;&gt;= interact.
So in other words MonadPlus could (in principle) equivalently be replaced with an empty typeclass that makes the promise that the additional laws needed for the correct interaction of empty and &gt;&gt;= all hold.
This is exciting. This is a feature Haskellers have been hacking around, pretty much since faking dependent types became a thing (or at least, it feels that way to me.)
This is awesome! I can't wait to try it out.
Note that no such laws are stated in `Control.Monad`, just the monoidal ones for `mzero` and `mplus`, just as no rules of 'interaction' between `&lt;|&gt;` and `&lt;*&gt;` are stated in `Control.Applicative` except for the mysterious one about `some` and `many`. 
Not true; go to http://www.haskell.org/ghc/docs/latest/html/libraries/base-4.6.0.1/Control-Monad.html yourself and look up `mzero`, and you will see mzero &gt;&gt;= f = mzero v &gt;&gt; mzero = mzero So indeed there are laws stated for the interaction of `mzero` and `&gt;&gt;=`, although these are the only laws present; the monoidal laws of which you spoke don't seem to be there. You are correct though that no such laws are stated in `Control.Applicative`.
Flipped around from what?
`Pointed`/`Copointed` are mistakes. This is not what I meant.
`Pointed`/`Copointed` are mistakes. This is not what I meant.
There are many semi-comonads that are not comonads. `[]` and `Maybe` come to mind.
http://en.wikipedia.org/wiki/Monad_(category_theory)#Algebras_for_a_monad which look like to me you ought to be able to read them off as the functor laws from a Kleisli category.
http://typesandkinds.wordpress.com/2013/04/01/defunctionalization-for-the-win/
that I think is my point. Just because multiple instances obey the laws (such as they are) for something like `Pointed` does not imply that pointed is useless. `Pointed` might still be a useless mistake, but Tekmo's argument is insufficient to show this.
Thanks! This is what I was looking for.
i don't understand how that gives us term level type applications...
Could someone explain what exactly this gives you?
bos blog post with 2-second solution in 3.. 2.. 1..
Functor/Applicative/Monad are widely used. [The AMP forked off a discussion about Pointed etc](http://thread.gmane.org/gmane.comp.lang.haskell.libraries), which basically resulted in - Pointed isn't governed by any laws - It's not clear where Pointed should go. Pointed standalone? Functor =&gt; Pointed as an intermediate step to Applicative? - Too much granularity if we add a separate class for each of pure/&lt;*&gt;/&gt;&gt;= and their combinations, including "empty" typeclasses that only combine others, such as (Pointed, Bind) =&gt; Monad - Pointed hasn't seen the widespread use of Applicative, which has been known to be practically useful for a decade Another thing to consider is that we can't just break everything. My first AMP text was much more ambitious (remove return, liftM*, ap etc.), but that would have meant such a large compatibility break that the proposal would very likely have failed. I chose this somewhat conservative approach so that the main focus can be putting the "=&gt;" in there, the rest can potentially follow (such as deprecating liftM2+).
Well, I can't really answer your question in general, but here's a simplified version of a case where I found myself wanting this recently. I'm not 100% sure I understand this, but I figure if I mess up somebody will tell me... Imagine we wanted to tag functions according to whether they are [projective, surjective, injective or bijective functions](http://en.wikipedia.org/wiki/Bijection,_injection_and_surjection). {-# LANGUAGE DataKinds, KindSignatures, TypeFamilies #-} data Class = Projective | Surjective | Injective | Bijective newtype Function (t :: Class) a b = Function { runFunction :: a -&gt; b } Now the `Function` type is tagged according to the (declared, not enforced) `Class` of the function. (I assume that users of the `Function` constructor follow the honor system and, for example, only make a `Function Bijective a b` when the wrapped function is truly a one-to-one correspondence.) So now suppose we wanted to use this to define a `compose` operation on `Function s` and `Function t` that figured out, at compilation time, the `Class` of the resulting `Function. The solution goes something like this: compose :: Function s b c -&gt; Function t a b -&gt; Function (Compose s t) a c compose (Function f) (Function g) = Function (f . g) type family Compose (a :: Class) (b :: Class) :: Class type instance Compose Bijective Bijective = Bijective type instance Compose Bijective Injective = Injective -- ... -- Oh, crap, we have to spell out all 16 cases by hand... Well, closed type families, as I understand it, allow us to not to have to list out all 16 combinations by hand. Since any class `t` composed with `Bijective` is `t`, the 16 cases can be combined into no more than 7 (and boy I sure hope I didn't mess this up): type family Compose (a :: Class) (b :: Class) :: Class where -- Note that these cases are tried in order: Compose a Bijective = a Compose Bijective b = b Compose Projective b = Projective Compose a Projective = Projective Compose Injective b = Injective Compose Surjective Injective = Bijective Compose Surjective Surjective = Surjective This toy example tags functions according to `Class`, but this generalizes to the `Category` class: {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Morphism (t :: Class) cat a b = Morphism { runMorphism :: cat a b } deriving (Category, Arrow)
There's no point in mzero/mplus to exist anymore, but I'm not sure the MonadPlus laws can be derived from just requiring Alternative+Monad. But you're right, the class is at least rendered empty by the proposal.
You're right, the monoidal laws are stated in the docs for empty and &lt;|&gt;, but not their interaction with &lt;*&gt;.
If we benchmark the equivalent solutions and use idiomatic Haskell code (like, dont use Strings...) the Haskell solution is actually much faster.
Unidiomatic and unoptimized Haskell comes very close in performance to idiomatic D? Is this really something a D proponent would want to advertise?
If you deeply embed the language, you can overlay linear types. I've done it once before.
Could you show me what you mean by that?
This is a comment I tried to publish on his site, but I got message that it was marked as spam... ------------------------- I must say that what you are comparing here is rather meaningless. Proper unicode handling is a big deal and hard to do, it really slows down things a lot. People put serious effort to make sure it works fast. So comparing the solution that doesn't work with unicode to one that really does is at least unfair. The other thing is using String and expecting it to be fast - it's not. If you care about correctness and speed you should either use Text from "text" library or ByteString from "bytestring". The first one handles unicode properly. The second simply works on bytes (represented as Word8 datatype). Here are my timings: -- ldc2 -O -release -disable-boundscheck w -- ./wfreqd pg100.txt 100 0,75s user 0,01s system 99% cpu 0,764 total -- ./wordfreq-text pg100.txt 100 1,33s user 0,03s system 99% cpu 1,367 total -- ./wordfreq-bs pg100.txt 100 0,44s user 0,01s system 99% cpu 0,453 total -- ./wordfreq-bs pg100.txt 100 +RTS -A2m -H2m 0,39s user 0,02s system 99% cpu 0,409 total Proper unicode solution is 1.78x slower, but an equivalent is 1.86x faster (with proper RTS flags). Code here: https://gist.github.com/Tener/5840164 
I noticed that it breaks Cabal, the ParseResult in Distribution.ParseUtils type only has a Monad instance (no Functor or Applicative :( )
Oleg already has: http://okmij.org/ftp/tagless-final/course/LinearLC.hs You can do the same thing with names but it requires a little more type hackery.
Just bite the bullet and use Agda. :P
I think the issue is that "lower down" from your original comment is ambiguous, and depends on the way people visualize the type class hierarchy in their heads. You meant "defer `pure`/`return` to subclasses", but others read it as "defer `pure`/`return` to superclasses". That aside, `Apply` and `Bind` seem much less pointless than `Pointed`, but are they *useful*? You can write meaningful instances, but do people use them? Are they just unaware of the possibilities? The reason `Applicative =&gt; Monad` gained such traction is that everyone uses `Applicative` and `Monad`.
Pssssh. Who uses cabal?
For a `Pointed` `Functor` you have the law `map f (return x) = return (f x)`. In my experience this comes up more often than a type with just Apply.
Use text unless forced to use string for some odd reason. Byte string for bytes, not text. Relevant for text if you have some utf8 or other encoded text, until you decode it into text or send it away.
Yes, but that holds purely by virtue of parametricity.
`Text` is pretty much always preferable to `String`.
I don't understand the relationship to zippers, but I will say, zippers are cool. :)
As little as possible. It is regrettable that `String` is used so much throughout standard (and other) libraries.
All zippers are comonads, where `extract`retrieves the pointed object and `duplicate` traverses the zipper putting a shifted version of the old zipper at each element. That is, with the list zipper `data ZList a = ZList [a] a [a]`, you would do: instance Comonad Zlist where extract (ZList _ p _) = p duplicate z = ZList (iterate left z) z (iterate right z) To see the kind of things you can do with this, there was a blog post where someone made a 1D comonadic game of life.
Most of the problem seems to lie with Base (since modern libraries are largely using BS/Text) ... using BasicPrelude gets around some of the pain, but the lack of a ShowText typeclass seems to guarantee you won't be able to eschew String for rapid development. I'd be curious what people do about this.
I don't mind `Show` using `String` so much because it's really meant for debugging and GHCi sessions anyway. It's using `Strings` for everything else that bugs me.
No, not all zippers are comonads. The original zipper construction works on arbitrary recursive sum-of-products (at least) data types. And the zipper is for pointing to substructures of the type. The construction for μF is to take the derivative F', and use ([F' μF], μF) as a path paired with the current substructure pointed to. ZList above doesn't actually even match this structure. It is a structure that allows you to point at the elements of a non-empty list; and in general, the 'zippers are comonads' is only true for zippers that allow pointing only to elements (not entire substructures) of non-empty parameterized types.
Really? I thought that, for a functor (F a), the construction was just (F' a, a), and all the structures I have seen that were called zippers followed the same pattern. I'm not even sure if I understand yours. Could you write your version of μF for `[]`? 
John Hughes "Why Functional Programming Matters" http://www.cse.chalmers.se/~rjmh/Papers/whyfp.pdf I'm surprised no one mentioned this so far; it's a classic!
I do kind of wish, for convention and semantics, that there was a popular `class Present a where present :: a -&gt; Text` .
Could you elaborate on why?
Well, I think this part of the problem you mention is the new 'overlapping' aspect of type families that was solved a little while ago. But what this new change also means is that nobody can come along and add some weird `Compose` family instance. That sounds benign, but it's important when you want to define things like: data Even; data Odd type family Flip p :: * type family Flip a where Filp Even = Odd Flip Odd = Even where people can no longer come along and add some weird instance like `Flip Odd ~ Int` or something.* If you have an open type family then certain kinds of type invariants can't hold in general anymore. It's also important when you define things like Peanos and do type-family arithmetic, because you don't want someone adding a random `type instance PAdd Z Int = Bool` Unfortunately the above `Flip` example still doesn't *really* work how we want it to, because we can't declare a closed family as injective or anything. So even given the above definition, while there can be no more instances to complicate constraint solving, the compiler isn't going to see equalities like `p ~ Flip (Flip p)` yet. Which sucks. One kind of interesting approach from a few years ago was 'Type Invariants', where you could say something like: type invariant flip = forall p. Flip (Flip p) ~ p and any instances of `Flip` must collectively satisfy this invariant during type checking, so the compiler then has the evidence to establish such an equality. But anyway, I think this is the first step to things like real injective type families and that'll be great. * OK, I mean in this case you could sort of solve this today by promoting `data Parity = Even | Odd` to a kind and restricting the kind of `Flip`'s argument/return to `Parity` I guess, but due to various restrictions with the way kinds in GHC work I'm pretty sure you can still break this approach with open family instances (by matching on `Any` in a type family.)
`Text` is basically a drop in replacement for `String` and is almost always better. The problem is that `Text` is a library not part of the base, and as such many core things (like say, error handling) are implemented using `String`. `ByteString` should only be used with binary data, but if you care about strings of bytes instead of strings of chars, `ByteString` is the best library around. It takes effort to say write C code that matches `ByteString` performance. Sadly, many things have `ByteString` based APIs that should at least provide `Text` as an option (the worst offender in my book is `Alex`). In a better world `Text` would be in base and would replace `String` through out the prelude. The overloaded strings extension allows you to use string literals with `ByteString` and `Text` as types. I think this is an example of a general problem with haskell performance. We have fantastic libraries for fast code (`ByteString`, `Text`, `Vector`, `Repa`, all the `ST` goodness, etc) but none of them "just work" out of the box. `ST` requires a language extension. `String` is the default representation. Getting any of these libraries requires at least an `import` and sometimes a `cabal install`. The standard prelude functions (`map`,`mapM`,`foldr`,etc) all only work with lists when more generall versions exist.
I don't think `Pointed` is a useless mistake at all, and I've already argued a bit with /u/edwardkmett about it. I wont get into why, though, unless someone's interested.
What types could implement Copointed that couldn't implement Comonad?
Troll of the week award right here!
Good point. It would be good to have a lint that gave all the alternatives to sub par prelude functions
I'm currently working on a [small GUI library][1] that uses the web browser as a display. We would love more examples and feedback on the API. [1]: https://github.com/HeinrichApfelmus/threepenny-gui
This is actually really interesting, I'll try to find some time to play around with this.
Great! Since building HTML pages is rather noisy, we've included a couple of neat combinators in the API, but we're not sure whether we've hit a local optimum in the design space. We'd love to have feedback on those.
Beginner to intermediate is probably incompatible with `:: (Monad m, Proxy p) =&gt; p a' a b' b m r -&gt; p a' a b' (Maybe b) m s` I like pipes (though I use conduit's in favour of them because there is currently more real-world conduit stuff) but they're probably somewhat advanced. Also hinges on what OP's definition of "intermediate" is.
You say that presumably because you think pipes is hard, but we can only make the documentation better if we get new people working with us. @Dooey could do a lot of fantastic work simply by *not understanding* pipes and asking the right questions :)
I'm a beginner/intermediate Haskeller, and I couldn't even understand pipes well enough to ask a coherent question. I literally couldn't get beyond "So... what's all this then?"
Then if you have time could you review the [new tutorial](https://github.com/Gabriel439/Haskell-Pipes-Library/blob/master/Pipes/Tutorial.hs), which I'm writing to be more beginner friendly? You'd have to generate the haddocks, though (unless you enjoy reading markdown in a faint grey font).
Tekmo explained it well. `Pointed` is guilty of the same problems as [Default](http://hackage.haskell.org/package/data-default). Because there are no established laws, the typeclass does not aid equational reasoning. You can only reason about `point` if you read its implementation.
I think one thing that might help the tutorial is adding some motivation as to what problems they were created to solve and when you should use them. Perhaps by showing some conventional "non-pipes" code in the start and converting it to add pipes by the end?
You can write useful code for personal projects with out writing a single pipes singnature. I would not suggest a whole library be written that way but I do think it make it easy to get started.
I'm disappointed by the `ed` mode myself...
Is that the say for conduit and Iteratees then or do some of the other libraries make more sense?
a better solution I just came up with: lets typeclassify `String` and then make replace `Show` with something that uses that typeclass. We could have. class (Data.String.IsString s, Monoid s) =&gt; StringLike s where mapChar :: (Char -&gt; Char) -&gt; s -&gt; s with some extra laws: (fromString s1) &lt;&gt; (fromString s2) = fromString (s1 &lt;&gt; s2) mapChar f (s1 &lt;&gt; s2) = (mapChar f s1) &lt;&gt; (mapChar f s2) mapChar f . fromString = fromString . mapChar f we could then have type ShowSLike s = s -&gt; s --ShowSLike is always symantically equivalent to `&lt;&gt; s` for some `s`. class ShowLike a where showPrecLike :: StringLike s =&gt; Int -&gt; a -&gt;ShowSLike s showLike :: StringLike s =&gt; a -&gt; s showListLike :: StringLike s =&gt; [a] -&gt;ShowSLike s actually, the use of `ShowS` is pretty silly once we typeclassify it, and we could just use `showPrecLike :: StringLike s =&gt; Int -&gt; a -&gt; s` The `like` naming convention is terrible, but it gets the idea across. Probably, `StringLike` should also provide most of the `Text` api (`length`, `unpack`, `intercalate`, `transpose`, `reverse`, etc) with what ever possible given default implementations EDIT: I'm now planning on implementing this.
update: This gets part of the way there; getter now typechecks, but setter still does not. I got rid of the Effect type synonym and expanded it. {-# LANGUAGE ExistentialQuantification #-} {-# LANGUAGE PolymorphicComponents #-} {-# Language ImpredicativeTypes #-} {-# LANGUAGE Rank2Types #-} {-# LANGUAGE NamedFieldPuns #-} module Toy where import Control.Lens class C a where op :: a -&gt; Int data Foo = Foo { _effect :: forall a. (C a) =&gt; a -&gt; Int } apply :: forall a f b. (C a, Functor f) =&gt; ((a -&gt; Int) -&gt; f (b -&gt; Int)) -&gt; Foo -&gt; f Foo apply = lens getter setter where getter f = case f of Foo { _effect = e } -&gt; e -- getter = undefined setter f e = case f of Foo { } -&gt; Foo { _effect = e }
I think they are in some ways worse. Pipes is very disciplined and consistent, it is just also very big and powerfull and alien. Iteratees forces you to think in terms of an odd programming model, and conduit lacks the coherence of pipes (imo) although it may be less alien. 
the new tutorial is fantastic. The "here is how you do some unix examples, now lets explain what is going on" approach is almost always a good one.
It bothers me a bit. One thing I find appealing about typeclasses is that you separate your data and the functions that operates on it (in particular, this lets you put your set of 'render' functions in a 'rendering' module and swap implementations at compile time. You can also group functions together, which increases readability. You can also reuse the same 'interface' (or part of it, should your data have instances for multiple typeclasses) much more easily for different data types. And since you do not operate on closured values, you still get Read/Show instances for free. You lose all these useful properties by tying together data and implementations. I'll grant you that in trivial cases, it may be useful, though, but I'm not convinced it's the way forward in complex situations.
Thanks! :) I chose the Unix examples because I wanted the tutorial to make as much sense as possible to non-Haskell programmers, too.
I think I have used or understand how you are using all the adjectives but "alien". Can you think of any example code, implication, assumption about pipes that made you think that? 
A number of stylistic points. First, you don't need to use case statements to deconstruct types. For instance, your getter can be written directly as getter (Foo { _effect = e }) = e Second, Existential Types are rarely the right tool. In your case, you've designed an interface `C` which say we can convert a type into an `Int`. You've then created an existential type where the *only* thing we know about the type is that it can be converted to an `Int`. Far simpler is to just store that resultant `Int` itself. data Foo = Foo { _effect :: Int } fooOfC :: C a =&gt; a -&gt; Foo fooOfC a = Foo { _effect = op a } This lets you write one of the lenses quite easily effect :: Lens' Foo Int effect = lens (\(Foo e) -&gt; e) (\f e -&gt; f { _effect = e } ) which finally highlights the difficulty you were having. You can't write a lens into an existential type because it's not possible to know which type gave rise to that existential type without using `coerce`. You *can* write a `Setter'` though. fooC :: C a =&gt; Setter Foo Foo Int a fooC f (Foo e) = fmap Foo (fmap op . f $ e) Maybe someone else knows how to get the typesafe coerce to play nicely there.
Yeah, type classes are still amazingly useful. They hit a very nice sweet spot. It's when all the compiler extensions come out that I start to get more suspicious.
I simply don't understand why I would need pipes. I don't get confused by prelude-only Haskell.
The construction ends up being two lists. One list is the elements you've walked past, and the other is the remaining list structure. In more detail: [a] = μF F x = 1 + a * x F' x = a [F' μF] * μF = [a] * [a] But, the original zipper paper doesn't even work with a parameterized type, just a type of binary trees (without even anything at nodes or leaves): data Tree = Nil | Branch Tree Tree data TreePath = Top | Left TreePath Tree | Right Tree TreePath So... F x = 1 + x*x F' x = x + x [F' μF] = [μF + μF] = 1 + (μF + μF) * [μF + μF] = 1 + [...] * μF + μF * [...] = TreePath Paths store a list of the subtrees we haven't walked into on the way down, tagged with the direction we walked instead, and the zipper pairs that with the subtree we're focused on.
PS, if you have any must-have features I should consider, do let me know and I'll see what I can do.
Thanks for your answer. I see what you mean about the existential being pointless here. However, this makes me think that I simplified out some important details of my original problem - I'm not sure I can make the equivalent switch to just storing an `Int` directly. This came up while I was trying to rewrite some code that ran in a `StateT MyState IO ()` monad to instead run in `(Proxy p, Monad m) =&gt; Client (StateP MyState p) Int Bool m ()` I had some containers, the `Foo`, which contained arbitrary monadic actions that could be executed at runtime. So, data Foo = Foo { _effect :: StateT MyState IO () } for this type, `makeLenses` works just fine. However, now my arbitrary monadic actions are polymorphic over p and m, so I have data NewFoo = NewFoo { _effect :: (Proxy p, Monad m) =&gt; Client (StateP MyState p) Int Bool m () } Now, I'm not on familiar ground here, so what I want might not make sense (but I think it does). What I want is to still have lenses over `NewFoo` - I may want to pull out `_effect` in order to run it, or I may want to store a different effect inside. And I would prefer to leave `p` and `m` as polymorphic, since the actions that I store inside each `NewFoo` will not depend on either of them. Can you suggest anything here? It could be the case that I can do something analogous to just pulling the `Int` out of my original `Foo` - but if I can, my own types are beyond my ability to recognize how.
It was tongue in cheek comment. Pipes has a reputation for being ferociously hard to understand. However ocharles has shown me some stuff with it and convinced me that it's worth learning. :-)
I see.
&gt;Beginner to intermediate is probably incompatible with `:: (Monad m, Proxy p) =&gt; p a' a b' b m r -&gt; p a' a b' (Maybe b) m s` o_0 
IF you need, a monadic formlet can be driven by creating a new call that return both the formlt identifier and the value: (id,value) &lt;- giveMeBoth widget So that you can manage it in contexts different from the web. for example in the case of windows widgets.
You should not put the (Proxy p, Monad m) constraint on the data type. Instead, you're NewFoo should have two arguments: data NewFoo p m = NewFoo { _effect :: Client (StateP MyState p) Int Bool m () } $makeLenses ''NewFoo Then makeLenses just doesn't need that constraint, it works perfectly fine without it. But, when you want to write a function that uses the Client, you need the constraint: f :: (Proxy p, Monad m) =&gt; NewFoo p m -&gt; Int -&gt; Client p (StateP MyState p) Int Bool m () f n i = do respond (i + 3) n ^. effect 
i like the tutorial. is there any reason `stdout` and `stdin` are operating on `String` and not on `ByteString`?
&gt; Personally, I feel it's due for a rewrite. Yep, that's true. Maybe start by writing an ex :)
yes. please do. i had to duplicate functions recently because there is no `StringLike` class.
**WHY ARE YOU USING `ImpredicativeTypes`????????? NEVER USE `ImpredicativeTypes`!!!!!!!**
Why's that?
`ImpredicativeTypes` may be hit-and-miss in terms of whether the typechecker can handle it or not, but it's not unsafe or anything. Just a generalization of `RankNTypes`.
While I'm actually in favor of impredicativity in type systems, I was myself just recently told off for using GHC's implementation of it, because it's an incredibly broken extension. Also, you can *always* use `RankNTypes` wherever you think you might need `ImpredicativeTypes`, because you can wrap things in a `newtype` for free.
Even if that were true, equational reasoning isn't everything that type-classes are about. Consider the lens library. If we had Pointed, we could have a separate constraint for Affine Traversals and then a nice guarantee about (^?) when dealing with affine traversals (never throws data away/truncates). 
&gt; excellent quality. Heresy! It uses tabs, how can it be excellent?
And?
One useful use of Pointed would be in lens, to have affine traversals. I think Pointed can safely extend Functor because the type `a -&gt; f a` implies `a` is in a covariant position within `f`. Though maybe there are some cases of invariance, they are probably less interesting.
I disagree, I think introducing Pointed would allow code to have more useful type constraints, whether or not it adds useful laws. I do think Pointed should probably subclass Functor, but it's not really a big deal if it doesn't.
Parametricity is a very low bar for deserving a type class. By that logic we could justify type classing `take`: class Takeable t where take :: Int -&gt; t a -&gt; t a ... since it would have the following laws: take n . map f = map f . take n
I like this a lot! This is also similar to the [`IsText` class](http://hackage.haskell.org/packages/archive/lens/3.9.0.2/doc/html/Data-Text-Lens.html) in `lens`.
Sorry for the spam filter of LiveJournal, that sometimes is a bit too much selective. I have let messages (like the one by 'tener') pass as soon as I can. Currently idiomatic D code is made mostly of foreach loops, with some library function calls. The D code shown in that that blog is very unoptimized (or if you want it's optimized only for compactness and for not breaking its linear chain shape), and it uses a D coding style that was introduced recently. It's the Haskell code that is more idiomatic and probably more optimized than the D code, but this doesn't matter, here what matters is that D bear is dancing. That blog post has several purposes, one of the points is to show how modern strongly statically typed languages as Haskell and D solve a problem that used to require lot of complex Pascal code to Knuth, or a little amount of text-based shell code, and to show the important idea of Unix pipes, that is now common in modern languages (see also C# LINQ, Scala, recent things added to Java, F#, etc). Another point of that little benchmark is to underline what's missing in the D standard library Phobos (like a hash-based counting function, a hash-based grouping function, and lazy range-based string processing functions), and few of the many places where D/Phobos performance is lacking. Regarding the correctness, probably even Windows8 sorts some Unicode file names in a wrong order, and in Unicode even the meaning of whitespace is fuzzy. So in a tiny program like that the correctness is a matter of grade. The D program loads text in an UTF-8 string, and then lowers its case and keeps only a small range of chars. If I load text in binary mode and I use the ASCII lowercase function the D program gets faster, but then some chars like hyphens and curly double quotes in the text I have used become noise. So that program loads Unicode text. This is far from handling Unicode text correctly, but for my demo text it's enough. The Haskell code I have shown in the blog post is copied from another blog. I have tried to improve it a little, but I don't have enough experience with Haskell and handing of Unicode in Haskell, so in the post I have noted that the hyphens and curly double quotes were not handled correctly by the Haskell code I have used. So in a sense the D code was handling Unicode a bit better than the Haskell code. And I have paid this with a little of performance in the D code.
I'll do this. I had been trying to avoid giving NewFoo type arguments, because then I have to give those same arguments to any datatype that contains NewFoo, and this will end up spreading to most of the datatypes in my program. My hope was that, rather than each instance of a NewFoo being bound to a particular p and m, there was a way to have every NewFoo have an embedded action that could be applied in multiple contexts - so, to be able to use the embedded _effect action of a particular NewFoo with p1 and m1 in one place, and p2 and m2 in another. However, your solution should work for me. Thank you!
That kind of class may actually be useful. Though there's little need for a higher kind there. I don't think laws are a requirement for a type class to be useful. They are nice if available but not a prerequisite.
Yeah, that would be great! Are you volunteering to edit our videos for us? Because we'd definitely appreciate the help.
`godofpumpkins` below is on the money. the meaning varies widely and there's no settled story for what it should do, nor even, as i recall, if it ever _will_ become a stable extension. so its a total no-go for production code. meanwhile, you can use the newtypes with a bit more noise, and have something that works properly across many versions of GHC, seamlessly.
Alright, but just don't go overboard with the type classes. :)
[bytestring-trie](http://hackage.haskell.org/package/bytestring-trie) is in need of love. The code, while performance oriented, should be accessible to any intermediate Haskeller. If you're interested, just shoot me (the maintainer) an email and I can talk about what needs doing. I'm at MFPS/LICS this week and may be somewhat incommunicado, but I'll be back home mid next week.
`String` was the old skool way of doing things before we started making real programs that care about the performance and correctness of string munging. Basically, you should never use `String` unless you're forced to by some library (or know you're in one of the few corner cases where it's correct and faster than `Text`). `ByteString` is much faster, but it is intended for "strings" of **bytes**. Not for character strings. So, if you have a bunch of binary blobs, then this is the type of choice. The only time you should use this type for textual data is (a) if the text is guaranteed to have some known byte-wide encoding (e.g., ASCII) ---this is far rarer than you think, and mostly applies to correctly interpreting certain file specifications---, or (b) you're only treating the text as binary blobs (i.e., you never make use of the fact that those blobs happen to represent textual data). `Text` is for correctly handling strings of **unicode characters**. You should always use this if you're thinking about your data as being textual in nature. `Text` is pretty fast, not always as fast as `ByteString` but that's because it's interpreting things correctly rather than treating them as binary blobs.
The problem with `StringLike` is that `mapChar` doesn't always make sense, or isn't powerful enough to do what we want it to (e.g., capitalization). Why not just have `ShowLike` use the `IsString` and `Monoid` constraints directly? Where is `mapChar` used?
The more I think about it, the more I think it is insufficent. I do think you want to be able to define `toUppers` and `toLowers` over any abstract representation of text, although for performance you would want those to be members of the type class also. These are perhaps not needed for `show`, but if the goal is to abstract over text representations more generally then they are.
*double take* The new synonyms are GREAT.
A pointed class could be useful as evident by the current inability to express affine traversals...
What is an affine traversal?
This was [posted 4 days ago.](http://www.reddit.com/r/haskell/comments/1gmyp1/ghci_more_awesome_than_you_thought_at_nyhaskell/) Your URL is different (https:// rather than http://), so reddit probably didn't inform you about it. 
`Lens`: exactly 1 `a` inside an `s`, represented by a Functor constraint: Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t `Traversal`: Some unknown amount of `a` inside an `s`, represented by an Applicative constraint: Applicative s t a b = forall f. Applicative f =&gt; (a -&gt; f b) -&gt; s -&gt; f t `Affine`: 0 or 1 `a` inside an `s`, represented by a Pointed constraint (assuming that carries a Functor superclass constraint as well): Affine s t a b = forall f. Pointed f =&gt; (a -&gt; f b) -&gt; s -&gt; f t Currently, in the `lens` library, the `(^?)` operator might truncate/throw away data, because it instantiates `f` to be `Const (First a)`. And since `f` has to be Applicative, `Const (First a)` has to be Applicative which in turn demands a `Monoid` constraint on the `(First a)`. So we might truncate/lose extra data in a `(^?)` application :-( If instead, we use our separate `Pointed` class and `Affine` traversal, and also split `Monoid` into `Zero` and `Plus` (with laws about their relationship, but no laws about Zero on its own), then we could make `Const a` an instance of `Pointed` if `a` is an instance of `Zero`. `Maybe` could have a `Zero` instance trivially, without any constraints on the `a`. `Const (Maybe a)` could be an instance of `Pointed` trivially without further constraints. And thus we could use `(^?)` safely with a guarantee that no truncation is going on and with no constraint on the `a`, because `Maybe a` cannot truncate. Also, having `Affine` as part of the lens hierarchy is informational. Having more tight type constraints helps convey what a lens can or cannot do. Also consider that when a `Prism` is used as a traversal, it is always an `Affine` traversal, but we cannot express this in the type system :-(
It's a pretty broad question because there are a lot of forms of parallelism in Haskell from an API/performance perspective, really. So, um, all of these? Chronologically, the most recent articles are first: http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/ http://research.microsoft.com/en-us/um/people/simonpj/papers/parallel/ And there's probably more papers on [SPJ's homepage](http://research.microsoft.com/en-us/people/simonpj/) (ctrl+f 'parallel') And you should probably look at [Simon Marlow](http://community.haskell.org/~simonmar/bib/bib.html)'s publications. And there's a lot more but I think that's pretty much all you need for a good while.
What do you mean by truncate/throw away data? Also, I never understood the difference between a `Prism` and a `Traversal`. Could you explain that?
Thanks! I know my question was pretty broad -- thanks for playing along anyway.
For example, imagine we have this: type Val = Maybe (Maybe Id, Int) idTraversal = traversed . _1 . traversed ... getId :: Val -&gt; Maybe Id getId = (^? idTraversal) setUniqueId :: Id -&gt; Val -&gt; Val setUniqueId uniqueId = idTraversal .~ uniqueId And later on, we decide to replace the outer `Maybe` with a `[]`: type Val = [(Maybe Id, Int)] Our code will still compile, but it will break because `setUniqueId` will set non-unique ids to all list elements, and `getId` will get the first id, and truncate/ignore the rest of the id's which may be the wrong behavior. Instead of the compiler helping us find the code to fix, we just got successful compiles and wrong behavior :( If we had Pointed/Zero classes, we could have `(^?)` be restricted to affine traversals only, so `getId` would cease to build. We could also annotate our use of `(.~)` with a combinator to make sure we're only setting one id, and get an error there too. As for Prisms: A `Lens` can be viewed as a first-class product component (i.e: a record field or equivalent) (usable as a total getter and setter). A `Prism` is basically the duel of a `Lens`, and can be viewed as a first-class sum component (i.e: a data constructor or equivalent). The difference between `Prism` and `Traversal` is that: * Prisms support the `review` operation (operator `(#)`). * Prisms are always 0-1 (affine) traversals. Example use of prisms: _Left . _Right # 3 =&gt; Left (Right 3) Left (Right 3) ^? _Left . _Right =&gt; Just 3 From Numeric.Lens: _Right . binary # 59 =&gt; Right "111011" Right "111011" ^? _Right . binary =&gt; Just 59 Right "111011" ^?! _Right . binary =&gt; 59 The awesome thing about all of this, of course, is all the subtyping relationships described in the [lens diagrams](http://hackage.haskell.org/package/lens). For example, if you compose a Prism and a Lens, you get their intersection: a Traversal. If you compose a Prism and a Getter, you get a Fold...
And the new module hierarchy— triple take?
Thanks! That clears up a lot. I still think that example doesn't justify `Pointed`, though. After all, you are using `Pointed` as a "must not have length greater than one" constraint, but why stop there? What if I have a problem where I want a "must not have length greater than two" constraint? I could specify a `BiPointed` type class and get even more precision there, but I'm not sure that it's worth the extra type class. In the specific case of your setUniqueID problem, I think the correct solution is to replace the first `traversed` with `_Just` to specify that you only want a `Maybe` there. When you specify `traversed` it intuitively means that types of more than one length are acceptable at that step.
Currently we have 1 (Functor), and N (Applicative). Once you support composition of 2, you support composition of all N&gt;0 so there's really no useful way to have Applicative-style type-class constraints for N=&lt;specific number&gt;. N=0 is a separate case, though. The whole Functor/Pointed/Applicative hierarchy can be described with these type-classes: class Map0 f where map0 :: a -&gt; f a class Map1 f where map1 :: (a -&gt; b) -&gt; f a -&gt; f b class MapN f where map2 :: (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c type Pointed f = (Map0 f, Map1 f) type Functor f = (Map1 f) type Applicative f = (Map0 f, Map1 f, MapN f) 
Also look at [chak's papers](http://www.cse.unsw.edu.au/~chak/papers/) in the areas of nested parallelism and high-performance computing.
Yeah, you're right. I still think that requiring up to 1 is still a pretty exotic requirement, though.
I love the work Neil has been doing with Shake. I have used Shake to implement a custom build system and it's been great. I do have some issues with Shake's type safety[1], but it's much better than dealing with Make, ant, or related tools. [1] Type-unsafe parts of shake include file paths and globs, "wrong" file path operators, and oracles (both the new and old APIs). If the Shake library exposed a stable lower-level API, these unsafe components could be ignored and replaced.
It's nowhere near as weird as it looks, though. "Usually" when you see that many type variables something "weird" is going on, but the `p a' a b' b` bit is just verbose, not complicated. I dropped myself a little note in the code I have using pipes just to remind myself which order those things all are, but they are "what I am (a Proxy)", "what I send to my predecessor", "what I receive from my predecessor", "what I receive from my successor", and "what I send to my successor". The `m` is "what monad this all runs in", often IO, in which case it's not that complicated. The only somewhat funky thing is the `r`, return value, since at least in my experience it so often just gets ignored/discarded. YMMV.
Thanks. I have clarified the original comment.
There is a law for it. It just happens to be trivial due to parametricity in Haskell.
 p :: () -&gt; Producer Int IO () p () = runRespondT effectfulList Why do all of these functions (I see it in the pipes library too) take a unit argument? What on earth could be the motivation behind that? Keeping in mind that Haskell is a non-strict language, there is no reason for it. Edit: Nevermind, I read the tutorial.
Yeah, I didn't suddenly convert to OCaml if that was what you were thinking. :)
Yeah, that does make more sense than I expected. Thanks. 
The ninja support sounds great.
I can always deprecate the String version to give people a chance to switch. Regarding the dependency, the choice ultimately comes down to what I feel will increase adoption. One of the reasons pipes remains a strong contender despite the huge API instability is because pipes is lightweight and also because pipes has a much broader application than streaming parsing. Adding a bytestring/text dependency compromises on both of those strengths by giving the perception that pipes is just another heavyweight parsing library. So my impression is that adding a bytestring and text dependency just for two prelude functions is a bit overkill and will do more harm than good. People who think bytestring and text are safe and lightweight dependencies should put in the effort to incorporate those two libraries into base.
WFM in GHC-7.0 (released November 2010).
I'm having a very hard time with the first-run file dialog for selecting a package folder. It seems to work sometimes on click and not others. Not really sure what's going on.
Also seeing some UI weirdness http://imgur.com/OIWyCGI
As the speaker for this talk, feel free to ask me any questions :) 
I have one to get things rolling. Where did you get that magnificent hat?
Well, with my prior hat I'd get jokes like "It looks like you just came back from a Koala hunting expedition", so I had to kick it up a notch. I think its a Tiley™ Brand hat. 
Interesting! Just yesterday I came across one of the authors' answer about dependent types in Haskell at StackOverflow: http://stackoverflow.com/questions/12961651/why-not-be-dependently-typed/13241158#13241158
I don't think the general Haskell user is aware how perilously close to something like full spectrum dependent types GHC is. This paper is a very good checkpoint on the way there. It is a solid review of what is currently possible and points out what is painfully so or not at all. The paper also sheds a lot of light on the dependently typed design space, revealing nuances that I am sure were clear to the experts, but certainly escaped me until now.
The [diagrams](http://projects.haskell.org/diagrams) project would love some help coding up some fun examples -- and once you've gotten comfortable using the library there are plenty of little features and additions which you could work on as well.
Hrm, do you mean that this might be because of my older settings mucking things up? I didn't change anything, this is an install over 0.12.x defaults. 
Well, first of all, it seems like create directory doesn't do anything. Also, it seems like you can go up the directory tree by clicking on the breadcrumbs at the top, but that doesn't work. There were other problems too, but those were the worst. 
yup, they're linked to from the vimeo page :) 
Very neat.
Hi, I'm not very experienced in haskell and dependant types to understand half of that paper. Anyway, as you say, does it mean in haskell it's possible to use something more complex than Nat in types for dependantly typed programming?
Amazing. I've never seen such concise use of Free before.
Yes, it is. In a recently submitted paper (sorry, no link yet) we have type level lists of Nats, with (ordering, disjointness) constraints and merging. GHC 7.6 already supports symbols (which are type-level strings), so you can have lists of symbols at the type level too, which is also pretty useful. The only thing I find awkward is the three-fold duplication between * GADT definition (e.g. LessEqual proof object) * corresponding constraint definition (e.g. LE class) * corresponding proof (e.g. producing LessEqual from two Nat singletons)
Very elegant! &gt; "Free monad transformer" is a fancy mathematical name for an abstract syntax tree. I would rather say *a fancy mathematical name for certain types of syntax trees where sequencing plays an important role*.
Interesting approach: source = unlines $ [ "{-# LANGUAGE TemplateHaskell #-}" , "module " ++ mod_name ++ " where" , "import ApiCompat.ShowInfo ( showInfos )" , "import Prelude ( putStr, unlines, concat, ($), Char )" ] ++ map ("import " ++) mods ++ [ "", "main = putStr output" ] ++ [ "", "output = $(showInfos", " [" ] ++ zipWith (++) (" " : repeat " , ") splices ++ [ " ] )" ] Feels hacky, but also pretty clever.
Nice! I have a half-baked package similar to this, but using haskell-src-exts and possibly haskell-names as well. How was this implemented?
Thanks for sharing, Tekmo. I always enjoy reading your blog posts. So pure threading, albeit with round robin scheduling. I suppose you could combine these with par and pseq to get pure parallel concurrency, which is something I could have done with a while back for a uni assignment. I wanted to do a parallel branch-and-bound tree search with alpha-beta pruning. Unfortunately the alpha-beta values required shared state and the `par`-monad's `Ref`s only allow write once, coupled with runtime errors if you write twice. The solution was to use `stm` and push it into the `IO` monad, which was a shame. Does this mean that, for any deterministic scheduler, I could write a pure parallel branch-and-bound search algorithm? Because I think I'd like that. Should this concept, of pure concurrency with swappable schedulers, be made into a library? And aside from the exceptional work already done on the current Control.Concurrency code, why shouldn't we move towards a pure library implementation? It sounds more flexible.
Oh, does this remind me of things. :) This is of course coming dangerously close to Koen Claessen's original, classic '99 Functional Pearl titled "[A Poor Man's Concurrency Monad](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.39.8039)." In fact it should be rather obvious from the definition of `Action` in the paper data Action m = Atom (m (Action m)) | Fork (Action m) (Action m) | Stop that these are closely related! So you see, we've had library-like concurrency for quite a while at this rate! And in almost this exact form no less. Unrelated, I was actually playing with `FreeT (CoYoneda f)` yesterday on IRC and elsewhere, and reimplemented `operational` in terms of it (almost 100% compatible.) So I ported your example to my [CoYonedafied operational](https://gist.github.com/thoughtpolice/5846508#file-coyooperational-hs-L272) for giggles. And it rolls right off the tongue like yours! I've come to really enjoy using free monads to structure stuff like this. :)
Yes, that's correct. I will update it. Really, free objects are the general purpose syntax ... things. Edit: There, updated.
In this part of the scheduler: -- Thread done: Remove the thread from the queue Free Done -&gt; go ts' Pure _ -&gt; go ts' ...what's the meaning of `Pure` as a case here? I get the sense that `Free:Pure` is roughly analogous to `Right:Left` in an `Either` type, but I don't understand the semantics. ----- [edit]: Thank you all for the explanations!
I must step in here to say that this "free monad = AST" idea actually held me back when I was trying to understand free monads. The problem is that, as far as I can see, the analogy breaks down when you have an "instruction set" functor that involves function types: data TerminalF next = PutStr String next | GetStr (String -&gt; next) Now every `GetStr` means that you can't see "inside" that node unless you execute it with some input string. Basically, the choice of continuation depends on the choice of string, and the logic for choosing between continuations is inside an opaque function. This sort of thing is one of the reasons why I ended up writing my own [`free-operational` package](http://hackage.haskell.org/package/free-operational)—I needed to perform static analysis of what is effectively a `Reader`, and yet [I couldn't find an "instruction set" functor for `Reader` other than the opaque `r -&gt; a`](http://stackoverflow.com/questions/15426320/how-do-i-implement-reader-using-free-monads). Basically, the only free type I've found so far that truly lives up to the name "abstract syntax tree" is operational `Applicative`s— free applicatives with a Coyoneda instruction set...
&gt; Unrelated, I was actually playing with FreeT (CoYoneda f) yesterday on IRC and elsewhere, and reimplemented operational in terms of it (almost 100% compatible.) So I ported your example to my CoYonedafied operational for giggles. Oh, hey, [I did this reimplementation a few months ago](http://hackage.haskell.org/package/free-operational), and threw in a `Control.Applicative.Operational` because it's the one I really wanted. [GitHub here.](https://github.com/sacundim/free-operational)
hiding right under my nose!
That's right. Requiring void is more elegant but I wanted to avoid explaining Void.
If you hit the `Pure` constructor, that means that the computation has ended without explicitly invoking `done`. In other words, your `Thread SomeMonad a` "terminated" with an `a` "result". His round robin scheduler ignores the thread's result value, but we could instead record them. roundRobin :: (Monad m) =&gt; Thread m a -&gt; WriterT [a] m () roundRobin t = go (singleton t) -- Begin with a single thread where go ts = case (viewl ts) of -- The queue is empty: we're done! EmptyL -&gt; return () -- The queue is non-empty: Process the first thread t :&lt; ts' -&gt; do x &lt;- lift $ runFreeT t -- Run this thread's effects case x of -- New threads go to the back of the queue Free (Fork t1 t2) -&gt; go (t1 &lt;| (ts' |&gt; t2)) -- Yielding threads go to the back of the queue Free (Yield t') -&gt; go (ts' |&gt; t') -- Thread done: Remove the thread from the queue Free Done -&gt; go ts' Pure a -&gt; tell [a] &gt;&gt; go ts' 
Consider this code: test :: Thread IO () test = do child &lt;- cFork if child then lift (putStrLn "child") else done The last line of this turns into if child then Free (putStrLn "child" &gt;&gt;= return . Pure) else Free (return Done) Applying the `if` inside the fork gets you the following data structure: Free $ return $ Fork -- parent branch (Free $ return Done) -- child branch (putStrLn "child" &gt;&gt;= return . Pure) When interpreting the parent branch, we do x &lt;- return Done and get `Done`, so we are obviously done. But when interpreting the child branch, we do x &lt;- (putStrLn "child" &gt;&gt;= return . Pure) which prints `child` to the console and returns `Pure ()`. There's no more commands to execute, so the thread is done. If you need further help understanding the semantics of Pure, try implementing a scheduler with the type: roundRobinWithResults :: Monad m =&gt; Thread m a -&gt; m [a] which returns [1,2] when presented with this code: test = do fork (return 2) fork done return 1 
It's already been implemented multiple times. There's the ListLike library, and my own monoid-subclasses with the TextualMonoid class. 
Is Prompt just another Free/Operational/Unimo monad? How does it compare to the others?
Nice! I figured I wasn't the only person doing this (I made some tweaks based on yours, even!)
It can't be written. Ultimately no matter what you do to access an existentially or universally quantified field will effectively put a naked existential in positive position. *existential fields* This isn't the problem you have but I'm including it for completeness: If you have an existentially quantified field then think of what the type the use of the getter for your lens would have to have: foo^.someLens would have to be able to give back a naked existentially quantified type. GHC doesn't have these, and SPJ has indicated it probably never will. *universal fields* If you have a universally quantified field the story almost looks tractable for a second, until the brokenness of impredicative types gets in your way. Here (which is the case you have) the type of the lens would expand to Functor f =&gt; ((forall a. C a -&gt; Int) -&gt; f (forall a. C a =&gt; a -&gt; Int)) -&gt; Foo -&gt; f Foo There the problem is the need for `f (forall a. C a =&gt; a -&gt; Int)`. This is an `ImpredicativeType`, and those have basically been given up on in GHC and has never worked consistently across the GHC versions. There was some hope in the pre System Fc era to get them to work under FPH, with whatever the boxy and wobbly type theory of the day was, etc. but development on that front has since stopped more or less cold. *fibrations/natural transformations* One potential resolution would be to talk about some kind of "indexed lens" or fibration of a lens. Unfortunately a different concept than the notion of indexed lenses provided by lens. apply :: Functor f =&gt; (forall a. C a =&gt; (a -&gt; Int) -&gt; f (a -&gt; Int)) -&gt; Foo -&gt; f Foo This function can actually be written, but then can't work with any of the lens combinators! A form of lens could be built if the index was placed in an appropriate place in the type that supported this: type Flens s t a b = forall f j. Functor f =&gt; (forall i. a i -&gt; f (b i)) -&gt; s j -&gt; f j then you could work with custom combinators for this sort of fibration, but then the trick is attaching the class constraint to the index, dealing with the type inference woes you'd get from rank-2 types everywhere, not being able to work with the rest of the lens types, etc. *change the problem* You can also refactor the type to get rid of the existential type by inlining the contents of the dictionary most of the time. This may be prohibitive or impossible, but should work for this simple example. **tl;dr** You're up a creek
I don't really like this claim that `Free` or `FreeT` is somehow inevitably tied up with syntax trees, or essentially related to the idea of separating an algebra from its interpreter, something that we do all the time in FP. `Free` just provides a variable which can be substituted into the fixed point of a functor. That's it. Is the fixed point of a functor a 'syntax tree', whatever that means? What about `Mu f` or `Cofree f`? Do those count as 'syntax trees'? I don't know, but I find Tekmo's explanations to be misleading and I worry people who don't know better are walking away with some half-formed idea that anytime they want to separate the interpreter of an algebra from its syntax a free monad needs to be jammed in there somewhere. The [recent thread on the financial contracts paper](http://www.reddit.com/r/haskell/comments/1bzu9h/composing_contracts/) indicates to me that there's some confusion being spread here. 
Indeed! And in his other functional pearl "Parallel Parsing Processes", Claessen explains why it is good to have the continuation monad on top of the Action datatype: If you implement &gt;&gt;= directly on the type Action, it becomes linear in the size of its left argument. This becomes fatal when you use left-recursion in a monadic computation, because the computation will then take quadratic time. This is much like the use of ++ on lists, which is also linear in its left argument, and generates quadratic behavior when used left-recursively. Hughes-lists (of type [a] -&gt; [a]) are a solution to this. Continuation monads are therefore the Hughes-lists of free monads!
I think this is an old paper. I implemented Prompt between the publication of the Unimo paper and Operational's implementation (which was inspired by Prompt); at the time I was unaware of Unimo. Prompt is the Free Monads for Less version of Operational. I wanted to put a link here, but comonad.com seems to be dead. [There's some relevant code here though](http://hackage.haskell.org/packages/archive/free/3.4.1/doc/html/Control-Monad-Free-Church.html). `Prompt p` is equivalent to `Control.Monad.Free.Church.F (Ask p)`, with this functor: data Ask p a = forall r. Ask (p r) (r -&gt; a) instance Functor (Ask p) where fmap f (Ask p k) = Ask p (f . k) It uses continuation passing style to avoid some of the interpretive overhead of Operational and Unimo; it is at least somewhat possible that `runPromptC ret prm someProgram` compiles directly down to applications of `ret` and `prm` given the right inlining. I prefer it over Operational for two reasons: 1. I wrote it. :) 2. There was a great quote in the Free Monads for Less article that said 'We cannot get away ScottFree'; Operational is basically a Scott-encoding of the same free monad that Prompt encodes, and by doing so it gives up some performance that cannot be recovered. EDIT: I think it's appropriate to say that Ask is the Mother Of All Functors.
Isn't the NATTY-in-Natty question resolved by providing the constrained version, and a smart constructor: sy :: Natty n -&gt; Natty (S n) sy n = natter n (Sy n)
`Mu f` is syntax trees of closed terms. `Free f` is syntax trees of open terms with variables ranging over a type. `Cofree f` is syntax trees of closed terms where each node is decorated with a value ranging over a type. `Mu f` ~ `Free f Void` ~ `Cofree f ()` There are other variations to be sure.
Awesome, thanks for the dive. I'm tempted to use Prompt just because of that pun... Also, `Ask ~ CoYoneda` for those (like me) familiar with the other terminology.
Oh cool, thanks for that name, I'll use it in the future. I'm just a regular working engineer, so I'm not up to date on the category theory links to the stuff I make. I'm slowly becoming more fluent!
Everything old is new again! `Control.Monad.Free.Church.F (Yoneda p) a ~ Prompt p a`. [As implemented here.](http://hackage.haskell.org/package/MonadPrompt)
This whole thread is making my mind race more. Dammit Edward Kmett, get your website back up! (I need to read Free Monads for Less again :( )
Thank you, I am familiar with the definitions of `Mu`, `Free`, and `Cofree` and what they mean. :) IMO 'syntax tree' is kind of an informal term that can have different connotations, I was just using it there to refer to some sort of defunctionalized representation or 'pure syntax'. It sounds like you would consider even a `Cofree (a -&gt;) b` to be a 'syntax tree', which is fine, but that's not the way I was using that term in my comment. Anyway, I do not want to get into a discussion about the precise definition of 'syntax tree' on this thread, but hopefully the general point I was making was clear. :)
`Action` is not a free monad, though. It's not a monad at all. The monad they use is `Cont (Action m)`.
I do think fixpoints of functors as such (i.e. `Mu`) are tied to notions of syntax in a very deep way (and somewhat at the heart of "separating an algebra from its interpreter"). The free construction is a variation on that, with plusses and minuses, but I'm increasingly less enamored of it. Catamorphisms on `Fix` have a whole bunch of nice equational guarantees that come with them. Equivalent constructs on `Free` give you less reasoning ability, I think (although I'm sure dolio or someone can step in and provide a bunch of nice examples counterweighing this sentiment :-P). The problem is the monadic structure itself tends to work out very well for certain types of interpreters, and rather poorly for others. I also suspect that for some reason `Cofree` tends to be more generally useful than `Free`. Largely, I suppose, because element-generic streams and their operations are fairly fundamental.
This inspired me to make: https://github.com/Peaker/cthreads/blob/master/main.c
I had to see it to believe it! :) I updated the post to mention this. The resemblance is uncanny.
I have removed my nonsensical-ness.
&gt; Free just provides a variable which can be substituted into the fixed point of a functor. This kind of explanation is opaque to people who don't already know the answer. I have to give them something concrete to latch onto so that they can make the jump to the correct intuition. You have to start from a specific example before you can understanding something in its full generality. This is the same reason why good monad tutorials begin from specific examples before discussing the abstract notion of a monad.
Is this not merely the Codensity transformation? Well, at least in this case. As `dolio` pointed out, `Action` is *not* a monad (they use `Cont (Action m)` in Koen's paper, sorry for my dumbness!) although in this case, `Free ThreadF` certainly would be. And we can CPS it to improve performance, although Codensity is more restricted with the right amount of power. I am going to have to muse about this more. Actually I should probably just read the paper and actually start trying to prove some of this stuff.
Presumably, GHC will eventually be usable as a proof assistant on a par with [Falso](http://www.inutile.ens.fr/estatis/falso/).
I think his site is now Turing complete and accidentally evaluated to ⊥. Edit: Actually it's up, just with poor formatting and really long page loads: http://kmett.com/reader/2011/free-monads-for-less/ http://kmett.com/reader/2011/free-monads-for-less-2/ http://kmett.com/reader/2011/free-monads-for-less-3/
You're right of course. I should not have said that Pointed has no laws, but rather that any legal instance will trivially satisfy those laws.
That's a good point, and a good example.
I work with the CodeEval team and wanted to let you know that CodeEval now supports the Haskell community and we'd love to invite everyone to join. CodeEval is a great place to compete against fellow developers as well as have fun solving challenges. Please let your friends know or tweet about it! 
It's not just you. I just checked and it looks like the theme is the default one so the .css files are not editable. We might be able to fix it by configuring a different Gtk3 theme. On OS X we bundle Adwatia with the Leksah binaries. It is a nice theme, but actually had the same problem with nesting of splitters and notebooks (we were able to partly fix it by patching the .css).
Thanks! So this will work if each branch of the fork can be computed independently without any data dependencies between them. Then you could just aggregate the results. This would definitely be possible if you changed from using a free monad transformer to a free monad where the base monad was optional like this: data ThreadF m x = Fork x x | Yield x | Done | Do (m x) type Thread m r = Free (ThreadF m) r Then you could guarantee that when you are not using the base monad (which I'm assuming is state in your case) that the computation can be parallelized. I'd have to know more about the specific problem to give a more specific answer, though. I think this should be written up into a library in some form. There is still some more thinking I need to do about it. I don't like to release half-baked libraries if I can help it and I feel that there is still more to this topic than meets the eye, even if I can't quite articulate precisely what yet.
I implemented the algorithm using shared state but it was still deterministic, which is why being push into IO felt like a shame. I had non-parallel versions which didn't need to share state and they were properly pure computations. Since the scheduler is pure and the calculations are pure it might be possible to represent the alpha-beta cut-off values as a lazy list of values which are calculated by running the scheduler. It's an interesting problem so if I get the time I might play around with it but I suspect if I do I'll likely find the lazy list of values is difficult. It needs to be immutable to be shared in a pure environment but it needs to be calculated as you go. Even if it can be done making it work in a healthy to write manner seems like it'll be even harder.
that's because comonad.com where all stylesheets are hosted is down. block requests to comonad.com and you will instantly see poor formatting :D.
That might help, at the cost of third bunch of constructor symbols.
You bet. Working on a screencast video right now.
Can you explain why? I feel like Prompt is better in every way except for the naming of the structures; `operational` has `instr a` instead of `p a`, for example, which is clearer. `operational` is slightly more powerful in the same way that it's hard to implement `tail` using `foldr` but easy using `caseList :: (a -&gt; [a] -&gt; b) -&gt; b -&gt; [a] -&gt; b`, but that comes at the cost of interpretive overhead, and in almost all cases your interpreter is a fold anyways. In the cases where you might want a non-fold interpreter I've found it simple to make the relevant places just another effect in the language.
This made it click for me. Thanks a lot!
The biggest, most obvious selling point to me is that it allows you to write blatantly non-terminating things, stick them pretty much wherever you want, and still have your program "just work". It lets you write algorithms the same way you'd explain them to a human, safe in the knowledge that it won't run itself into a blackhole unless that's the only way to do what you asked (usually). &gt; Counting up by twos starting from one, throw out all numbers that aren't prime. In a strict program, that is a useless function. It will run forever and not produce anything but heat. In a lazy program, it's a building block. You can just add more sentences and it's all fine. &gt; Of the remaining numbers, throw out all of the numbers with sharp edges and more than two syllables or more than three digits. Still ok in a lazy language, in a strict language we wouldn't have gotten this far. &gt;Now give me the sum. Tada! The lazy language gives you a result without having to go back and change any previous steps. Composability! Ok it's not a great example, but it still demonstrates the point. 
It is possible, though, and I actually once did something similar in a search engine of mine where I have a lazy list of results that I would compute in parallel purely. However, I eventually abandoned that because the parallelism wasn't improving significantly and it was easier to just spin up more search instances to improve parallelism. You can find one of my earlier attempts here on this Stack Overflow question: http://stackoverflow.com/questions/12959930/throttling-parallel-computations/12960910#12960910
Do keep in mind that Mezzo is an early research prototype so far, not a full-fledged language. It would make more sense to compare it to DDC, Eff, VeriML, etc., than Haskell, OCaml, F# or Scala. Mezzo is looking at a point of the language design space that is related to effect systems and separation logic. It could give ideas on how to do some things better than with monads.
I signed up for beta. But all i got was an email confirming it. When do i get an access to the actual product? Also our company operates under strict HIPAA regulations. We will definitely not host any of our source code on someone else's cloud. 
With the new kind-level extensions, you might be able to put a single type parameter on all your data structures and then instantiate it in NewFoo as '(p,m) This would leave you relatively safe for further changes that might change the type variables, and works even in the base case where you use '().
somehow that did not show up here before. it's a few weeks old by now.
What does HIPAA have to do with *source code*? Client data, sure, but source code?
Maybe she's referring to the deployment system. For the application to run in the cloud, the data has to be in the cloud as well. It's also possible that actual client data is needed for testing during development, but this is probably a sign of something seriously wrong with your development process.
Ask our lawyers. But they strongly do not recommend giving up the source code to anyone. That's why we do not use github etc, even their private repos. Perhaps it is not because of HIPAA but for other business reasons. The end result is the same. 
No there was a post about it a few weeks ago. Let me see if I can find it. edit: Here it is: http://www.reddit.com/r/haskell/comments/1fvv5q/ghcjs_introduction_concurrent_haskell_in_the/ The url is different: http://weblog.luite.com/wordpress/?p=14 rather then: http://weblog.luite.com/wordpress/?p=14&amp;date=2013-06-03
This doesn't mean anything until you reference us the worst.
i knew there was an announcement. but somehow i remembered, that it said then, that it's not yet installable via cabal. (well it says so now as well, but it also says it is. so i am just confused, i guess)
I like the irony of you linking to the darcs source code ... on Github.
I don't want to call anyone out, plus it's a highly subjective topic.
I knew someone would say this, but I like being able to flip through colored source code before downloading it. There is a darcs web interface extenstion, but the darcs website doesn't use it.
I think ghcjs is going to be installable through cabal in ghc 7.8 if everything goes as luite plans. Which I hope it is because I have been playing around with in vagrant making reactive web interfaces and it is fun. If you install ghcjs right now you can use cabal to install hackage packages for ghcjs with `cabal --ghcjs install &lt;package&gt;` however, it is a little memory intensive though 2Gb is not enough to avoid using swap for some packages.
Thanks! I was mostly just using the tools I knew at the time. Using GHC's API directly would be much less hacky. But essentially that's what this is achieving, albeit in a somewhat roundabout way. I did try avoiding the usage of a temp file and "runhaskell", but I think there ended up being an issue with running TH from interpreted code. That might have also been an issue for running hint in TH, but also not so sure there (I was working on this project fairly briefly, about a year ago). Another option would be to invoke ghci or some other program from TH - I know that would work. The main value here is the top level declaration pretty printer - https://github.com/mgsloan/api-compat/blob/master/src/ApiCompat/ShowInfo.hs . It attempts to pretty print in such a way that textual diffs sometimes turn out to be reasonably semantic. See, for example: https://github.com/mgsloan/api-compat/blob/master/examples/template-haskell.api.diff
I wasn't suggesting that as an explanation for a newcomer. And I don't take issue with the general strategy of moving from concrete examples to the abstract. I am saying, very simply, that your post is confusing and misleading, for the reasons I gave in my original comment, and your response of 'but I'm trying to give an intuition for newcomers, so it's okay' is not satisfactory to me. :)
A small public service announcement: If, for whatever reason, your VirtualBox can't figure out how to do NAT, e.g. if your default adapter is wlan0 (and the setup fails to run apt-get, look up FQDNs, etc..), you can switch to Bridged networking by editing ghcjs-build/Vagrantfile: just add: config.vm.network :bridged anywhere in the config section. If you already tried to build the box, and failed because of the adapter, you can do `vagrant destroy` -&gt; `Y` -&gt; `vagrant up`. The base box image won't be re-downloaded, just copied.
Stumbled across the same topic from the lisp world: http://www.jwz.org/blog/2008/03/most-positive-bignum/
I disagree with the premise that my explanation or choice of terms is wrong. I believe the notion of free object is more appropriate for syntax than the notion of an F-algebra. Just because I'm not teaching something exactly the way you learned it doesn't mean that my pedagogical approach is wrong, and I think you overstate the harm (if any) of my choice of words.
Did you know about this https://enterprise.github.com/ &gt; GitHub Enterprise is distributed in the OVA format, compatible with VMware and VirtualBox.
Thank you for signing up. The first users will get their beta accounts at the end of the month; we will then let people on in a controlled rate. Yes, several companies don't let their work on any cloud. That's why we expect to release a downloadable version too, as mentioned in the video.
hey what were those new llvm bindings you mentioned?
cminusminus.org no longer exists. Where can one find a spec for the language? The haskell wiki points only there.
&gt;Code is like poetry; most of it shouldn't have been written. best programming quote ever.
I don't know anything about Happstack but if you add a `FilterMonad` instance for `ErrorT`, there's no reason you can't make it automatically derive `FilterMonad Response` for `MyApp`. Looking at the instance of `FilterMonad` for `ReaderT`/`StateT` [here](http://hackage.haskell.org/packages/archive/happstack-server/latest/doc/html/src/Happstack-Server-Internal-Monads.html#line-573), it should be fairly simple to implement one for `ErrorT`.
If for some reason I haven't appeared in #haskell for a while to be answered "What does that mean?" to some simpleton question, I can always get my weekly dose of shachaf here.
For the moment, I am happy to throw unicode support away with the default filepath. Note that files in Shake are implemented on top of the Core API, as just an instance of Rule, only using things that are exported, so it would be entirely feasible to have an alternative unicode file API. You could avoid the &lt;/&gt; // confusion, but it would come at the cost of making things like file patterns more verbose (I'm guessing you'd want to turn // into a real operator), so I don't think it's worth it. The problem with the different types of &lt;/&gt; is real enough, but easily avoided by importing the right module, so that doesn't seem too bad. So there is a lower level API you can use to build the File operations, but I generally try to keep the API stable - you should be able to build on any piece without a risk of stuff breaking. There are occasional non-compatible changes, but the most recent one I am aware of was to the Rule instance - where the low-level pieces changed, but the high-level pieces stayed exactly the same. The OpenShake project, https://github.com/batterseapower/openshake, tried using associated types to give a type safe oracle. I found the trade-off was a lot of type signatures and plumbing for additional type safety, but the additional plumbing was just too expensive. For your own build system, what are you hoping to do differently to Shake? All the points above could be solved by a few new Rule instances, or a thin wrapper over what is there already, without reimplementing all the dependency/parallelism pieces.
Our mirror on [hub.darcs.net](http://hub.darcs.net/darcs/darcs-reviewed) may be of interest. I think Ganesh and Guillaume have been doing some heavy cleaning lately, but that there's still a good amount of work separating Darcs into nice logical layers.
Great post, I've never dared to look at Core before but now it does't feel that alien anymore.
Hah, don't worry, it'll get significantly hairier pretty quickly! :-)
Cool! That's a nice place to send people who are looking for problems to solve with Haskell to gain some experience.
I've always found Oleg's code to be amazingly idiosyncratic and hard to grasp even aside from the mad tricks he pulls.
Yes, why not the up-to-date mirror on darcs hub, for the love of roundy! [http://hub.darcs.net/darcs/darcs-screened/changes](http://hub.darcs.net/darcs/darcs-screened/changes) is the bleeding edge, or you can watch [http://hub.darcs.net/darcs/darcs-screened/changes/atom](http://hub.darcs.net/darcs/darcs-screened/changes/atom) in a feed reader. 
From the first example I clicked on but I've seen it a few times: what's the reasoning for using `case` here rather than `where` or `let`? unsafeWithInternals :: B.ByteString -&gt; (Ptr Word8 -&gt; Int -&gt; IO a) -&gt; IO a unsafeWithInternals ps f = case BI.toForeignPtr ps of (fp,s,l) -&gt; withForeignPtr fp $ \p -&gt; f (p `plusPtr` s) l
I wish I didn't believe you..
Performance, most likely. The `case` statement makes sure that the expression is evaluated to WHNF. In contrast, `let` and `where` just bind the tuple elements to the corresponding names without evaluating anything.
This is nice! You should put this on the front page of your website.
I wonder why people like to pick one fixed monad-transformer stack and stick to it for their entire application. It's not just you, and in fact you haven't explicitly said that you planned to do this, but I keep seeing this pattern everywhere and it bugs me, as I feel like this should be an anti-pattern! Everywhere else in Haskell, we like to give functions their most general type, so that they can be reused easily. Monad transformers shouldn't change that; if your function only needs one or two monads to do its job, then it doesn't make sense, to my naïve eyes, to specialize that function so that it only works with one particular stack of monad transformers. Is it just because of all the extra calls to lift and hoist? If each function lives in a different monad-transformer stack, I imagine that each call site will need a bit of lifting in order for the calls to type-check.
&gt; If each function lives in a different monad-transformer stack, I imagine that each call site will need a bit of lifting in order for the calls to type-check. Not if you use the `mtl` classes! Then you can seamlessly mix code using different monad transformers without any (explicit) lifting, and you can then simply specialize them to your chosen stack when, and only when, you need to run them.
This is precisely what I'm doing. There's a lot of derivations in the full code for the various transformers in the stack. I probably wouldn't use mtl if I couldn't change my stack around without having to change my functions. That's just about the handiest feature of the library. 
Have a look at [Lambda the Ultimate](http://lambda-the-ultimate.org/).
Benjamin Pierce made a [list of the most influential PLT papers](http://www.cis.upenn.edu/~bcpierce/courses/670Fall04/GreatWorksInPL.shtml), as nominated by the Types mailing list. For more cutting edge stuff, [/u/gasche](http://www.reddit.com/user/gasche) collected a list of [this year's ICFP preprints](http://www.reddit.com/r/haskell/comments/1gg2xf/preprint_list_for_icfp13_papers_additional_links/).
ICFP '13 is coming up. Stay tuned.
What does `:@` mean? As in: (x :@ y) &gt;&gt;= f = (x &gt;&gt;= f) :@ (y &gt;&gt;= f) Where does it come from? Google and Hoogle don't tell me anything. 
The issue is that it'll just randomly stop working between GHC versions. If your application only ever needs to run in the short term on whatever current version of GHC you happen to have installed right now, you may be okay, but if you try again on the next major platform, you can expect it to break again and keep breaking in the future.
That's a really neat idea. So, no javascripting etc. will be necessary right?
Awesome works guys. I like vim, but this is more integrated then my current setup. Pull in Hare for refactoring support. 
I noticed that the IDE seemed to know what the type of "increase" should be even when the definition of the same went missing. Was that because it hadn't forgotten, or was it deriving it from the usage? If it was the latter case, I think that's pretty nifty.
Sorry I missed your comment earlier. I'll look into doing it on AWS directly, it should probably be able to unpack the prebuilt image if the right operating system is installed.
They don't seem to be compiling with -O2, which causes slow runtimes and stack space issues without a lot of manual tweaking. 
How soon can we expect beta invites?
It highlights errors with a pinkish red background. Did you miss that or do you want more red?
I got the implementation of the `FilterMonad` for `ErrorT`, but it looks like that isn't going to solve all my problems. My initial idea was that I could write code like home :: MyApp () home = do conf &lt;- ask let l = confLogName conf liftIO $ logM l INFO "Entering `home`" dir "home" $ ok "Welcome home!" liftIO $ logM l INFO "Leaving `home`" So that I could perform all my processing in such a way that I could log out lots of information and have good control over what is happening in my app. It doesn't seem like this works with Happstack, though. I think I'm going to have to go back over the basics of Happstack, maybe writing a few simple apps that aren't using a transformer stack.
Strachey is always so straightforward and, in his own way, pragmatic. A total joy to read!
This is extremely impressive, but I am not sure who the target users are. If unrestricted access to cabal is not available, I won't be able to break my project up into multiple packages. If I'm involved in long-term development, can I git branch and rebase as easily as I'm used to? For on-the-go hacking, or learning web app development, this seems like a great tool.
I like the design direction that you guys took! Do you intend to add integration for tools like stylish-haskell and, especially, hlint (or working on your own version of these tools?)
Looks pretty cool and I'm sure this required a massive engineering effort from FP Complete but I can't help but feel that this is relatively gimmicky. I just can't imagine day to day Haskell programming to be done in the browser.
Thanks. I didn't mention in this video, but the IDE does at least support vim key bindings. (You turn them on from your user settings page.) And as I recently mentioned in [this much shorter video blog](https://www.fpcomplete.com/blog/2013/06/fp-haskell-center-video-blog) we will indeed be supporting local deployment -- and later, local hosting of the IDE -- in subsequent releases.
Starting within a week, but it'll take us many days to get through them all -- initially in small groups so we can keep a close eye on quality.
Looking forward to your feedback on performance etc. when you give it a try. I find that I quickly forget that I'm in a browser, other than enjoying the cloud-style features (like switching to a second laptop with a different OS and opening the same session -- maybe I should demo that next time).
Have you tried running it in something like [Fluid on OS X](http://fluidapp.com)? Did it work well? The thing that I miss from web apps is integration with certain features of the native environment (be it something simple as menu bar options or the ability to have the IDE managed by my window manager).
I like it. Code navigation and built in help are awesome. A few questions. - github only or any local git repo? - autocompletion? - auto import? - cabal files support? 
Yep, no JavaScript. I'm the type of person who does Haskell and only Haskell. ;-) The focus is on "GUI library", the fact that it communicates with the web browser is more a technological choice. It's a bit like an XWindows server / client thing.
Impressive. That's a lot slicker than I would have expected after such a relatively short time.
They mentioned higher up that the IDE has built in Vim key bindings that can be turned on. Not sure how much of Vim it implements, though.
I've only just started by PL research career, and so I don't have any publications to show you, but I can tell you some areas I've worked in and what they're about: 1) Type inference is an interesting field that is still getting some academic attention. Full inference for system F is undecidable, let alone any more complicated than that. The trick is finding an appropriate subset of the language that can support inference, while not making the subset so small as to be useless. 2) Program verification is an interesting topic that I predominantly work in, and it has moved a lot closer to programming languages over the last few decades. The integration of verification and programming languages is a huge area, encompassing model checking and contract checking, static analysis, type systems, and language semantics. The thing that really brought the two together was type systems, though, particularly dependently-typed languages - The formulae-as-types correspondence is an observation that a constructive proof of a property is equivalent to constructing a program that has a corresponding type. Dependent type systems allow for types to contain values - in other words, you can write proofs about programs themselves, which make them expressive enough to form an elegant foundation for both advanced programming languages like Idris or Agda and theorem provers like Coq. The further up the chain you go in this area, the closer you get to being a mathematician. On the other hand, a lot of work still needs to be done to make dependent types easier to work with and more practicable, as well as implementing such features in "mainstream" languages like Haskell (we're slowly getting there). 3) High-performance computing and parallelism in high-level languages is another reasonably popular and quite interesting topic. Research in this area ranges from handling of nested data parallelism to domain-specific languages for GPGPU computation.
Exciting, particularly the app deployment. That's likely the simplest Haskell web deployment ever. Are there any plans to make the web-ide and the deployment feature more loosely coupled in the future? Allowing people to code in the ide of their choice then use your hosting platform, or the other way around, might make more people comfortable about using your product.
You say the web deployment feature is for Yesod apps, but I assume it's actually any Wai app? Is there an interface for non-wai based web applications to use? Also, I noticed that conduit was on your approved library list but not pipes :( This sort of worries me, in that now if I write a package, I will not just have to put it on hackage, but I also have to lobby FP complete to include it in their IDE, or risk making a group of Haskell users never even see my library. Couldn't you allow fetching arbitrary hackage packages in a local cabal sandbox?
This looks great. Feedback about merge conflict resolution: Would it not make sense to rename the "Resolve" button to "Mark as resolved"? Maybe it's just me but I feel like it could confuse developers, especially those who are not well experienced with git. For example, someone might interpret it as the button that will let them start resolving the issue -- since it would be the first actionable thing you see on the page if the conflict is further down in the file. Or maybe the editor should scroll down to the first conflict in the file?
Video: http://vimeo.com/69280214 Sorry for the lower quality this time. We had to fall back to an iphone camera at the last minute. Thanks to Sparky for jumping in and helping on short notice, however! It was a very informative talk, and I'm glad we managed to get it recorded.
Maybe look in to CS research? I have an entry level Haskell job at a CS research institution.
How is the current state of FRP in lazy evaluation? Is the generated JavaScript also lazy? The stability of the GHC compiler is a huge advantage, but I do like Elm right now because it's strict.
I'm not a real researcher, but I do some interesting stuff with [program synthesis](http://www.cs.berkeley.edu/~bodik/synthesis.html). This is basically a set of approaches for automatic programming: we write programs that write programs :P. It's a fairly new field, but there have already been some interesting results. For example, Excel 2013 has a feature called ["flash fill"](http://research.microsoft.com/en-us/um/people/sumitg/flashfill.html) (look it up on YouTube) that comes directly from some [MSR work](http://research.microsoft.com/en-us/um/people/sumitg/pubs/synthesis.html) on program synthesis. You could also check out some cool stuff done by others at Berkeley, particularly the [SKETCH synthesizer](http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-177.html) which lets you write programs with holes in them and the fills the holes in automatically. It's pretty cool. You can install it from source on [BitBucket](https://bitbucket.org/gatoatigrado/sketch-frontend/wiki/Installation). The guy who initially wrote SKETCH is now a professor at MIT, so they have some synthesis stuff going on now too. I also know of some people at Stanford, and there are probably more at yet other universities.
Yes Smart Git is great we use it all the time. I think github enterprise would add integrated issue tracking and code review processes. We use a number of different tools for this now and they are not well integrated.
There will be.
Are you trying to learn the way the mainstream is doing GUIs, explained for a functional programmer audience, or are you looking for a functional approach to GUI programming?
Shameless plug: http://hackage.haskell.org/package/htrace - hierarchical tracing for debugging lazy evaluation. Trivial in both usage and implementation, and helped me solve a memory leak I've been fighting for a couple of days, in just an hour or so.
Let me just throw in that, even as the product stands right now, you can decouple these somewhat. The deployment system allows you to provide any arbitrary URL containing the application you want to deploy, so you could develop and build locally and deploy to the app server. Of course, you'll need to make sure you're using an almost identical environment to the one we have set up, which is why we recommend building from our IDE.
Right. I'm wondering what people's experience has been with time/space guarantees with ghcjs frp
If you want to get a feeling for the innards of a domain-specific language implementation without too much effort, my team is working on a small language for modeling and simulation of hybrid systems (think robots) called Acumen. It is open source and you can see it, as it is being developed, at our development repository on Bitbucket. A good starting point to understand some basic aspects of the language is the imperative interpreter: https://bitbucket.org/effective/acumen-dev/src/6da749c497a23e7bcb9095dd308af52923da485f/src/main/scala/acumen/interpreters/imperative?at=master Pull requests are welcome! ;)
look at the data declaration in the slides, its an infix data constructor. See the simple at the top of the docs for bound http://hackage.haskell.org/packages/archive/bound/0.8/doc/html/Bound.html , the working example i go through is based upon that
Huh, thats no good, someone should fix that. or recover the c-- spec somewhere... seems i never saved a copy myself
Fancy features .. is the incremental/partial-program type checker open source? It should be, because, well, that's what a Knight of the Lambda Calculus would do .. or, maybe Lambda Hood, skulking about the syntax trees, finding goodies and passing them out to the emacs and vim users. please!
is there no graphical three-way or even two-way diff tool?
IDE as a web app... Seriously people, stop shoehorning everything into a browser!
Events are processed starting from each 'top-level' event / behaviour pushes, one at a time in the IO monad. So every time a real world event happens (like someone entering text or clicking a button) you get a lazy functional computation to work out what real world output state (e.g. what to display on the screen) changes, but before that can actually be displayed, it needs to be computed. You don't generally get lazy computations building up between top-level events (although you could potentially if you used collectE to compute something over time but hardly ever let that event go anywhere else - then you have a space leak, but you could fix it with a strictness annotation).
The successful commit/push messages were in red and it looked strange.
In that case vim may need to be worried! 
You should probably think about whether a web-browser based user interface would be adequate for your purposes. Depending on the concrete task, that may be easiest solution (or may be not). Another approach which can be *relatively* simple is to use OpenGL/GLUT directly. That would be rather limited in scope though. It would probably help if you could say more about your project.
Not yet, but I'm sure it's on the cards. It's an obvious next-on-the-list feature.
TypeHoles integration?
sounds interesting, but why here? is there a haskell port in progress?
Ah, you're right. I didn't think of those.
Would installing the latest Haskell Platform, plus the latest master branch of Stackage be sufficient to replicate your environment? I presume that FPComplete updates the version of Stackage they use from time to time. If one were to try to replicate FPComplete's environment locally, how would one know that it has become out-of-date?
Technically you could already code however you want and then just git pull into the IDE and deploy.
1. Does it work well on iOS Safari? 2. Is there access to ghci / some other REPL ?
What is the current status?
Release imminent, though there is a major API inconvenience that takes some work to fix.
Nice. Thank you. I will give it a try as soon as is released. :)
Sorry, I should have clarified. By environment, I really mean the system environment. If you use static linking locally, your binary shouldn't have any dependencies on Haskell libraries.
&gt; Another approach which can be relatively simple is to use OpenGL/GLUT directly OpenGL is just pencils and brushes drawing onto a canvas provided by the OS. Although OpenGL offers very powerful tools, it's also nontrivial to use and inherently imperative in it's execution model. GLUT isn't even a poor substitute for a GUI toolkit.
https://github.com/kentonv/capnproto/tree/master/compiler/src
I’d be very interested in either!
Wow, that's really an old versión of darcs! The current (= 1 year old) module organization is much better, check it out.
The integration with Github is fantastic. I´m glad to be the one that recommended to implement the IDE as a Web application in the original post: http://www.yesodweb.com/blog/2012/09/building-haskell-ide#comment-656975471 Finally we have a full development environment for Android and Apple pads. That is really good news. 
Thanks! A browser-based interface might well be adequate. My one concern would be memory issues, for a reason I’ll come to below. The project in question: roughly, it’s building and exploring certain large, intricate mathematical objects. Several of the main ways of exploring them are visual; so currently, the typical interaction is a loop of giving some parameters, outputting an image to file, and then (after looking at the image) changing the parameters. One thing in particular that is clunky in a text-based interface is specifying a location within the large object; it’s easy to imagine a point-and-click approach simplifying that. The one concern I would have about a browser-based interface (though again, this is coming from a position of great ignorance, so it may be a non-issue) is memory. The objects can get reasonably large — outputting the diagrams to postscript, I’m often dealing with files of 100+ MB or so. Mightn’t high memory usage be more of an issue in a browser than with other approaches? (Hopefully, of course, it should be possible to chunk the diagrams up a bit better for a graphical interface anyway, but due to the nature of the objects, that’s a non-trivial problem.)
Thanks for creating this! Here are some things I would want to see: 1. Better code navigation: see all places where a function is called, search for file names in current project (like emacs anything), show list of functions / types defined in current module so I can fuzzy search through them and jump to definition, quick switching between opened files (like jump to previous opened module by keyboard shortcut) 2. Text searching capabilities and emacs "occurs" command (I assume you have it, but this was not shown in demo) 3. Well, it would be great to have a debugger with variables inspection.
I usually use OpenGL for visualization in this type of scenario. How are you generating your figures? If you've already sorted out how to produce an image you want to see, you can even set something up to let you update parameters in GHCi while a GLFW window shows the result. A GUI with buttons and sliders definitely has some advantages, but I've had visualization projects in the past where a GUI layer I wrote ends up unused due to the development hassle of it all. It helps to be realistic: if you will have non-programmers using the tool, you need a GUI. Otherwise, a more programmatic interface has its own advantages.
Actually it makes a lot of sense. Most of us work with repository systems, the code that is on our local machine is just whatever we are presently working on. A web browser IDE is not functionally very different from this but it comes with some added advantages, saved sessions that can be accessed from any computer, distant end control of library support and environment generation, or collaborative code development where you don't have to spoon with the person you are working with. The only thing you lose from this deal is potentially not having access to development if you lose your internet connection. So overall IDE as a web app adds a number of improvements without any particularly significant drawback.
I always feel like these binary protocols (thrift, msgpack,protobuf, etc) seem to miss the point. Rarely is the throughput of your distributed system limited by the efficiency of the transport protocol. Usually the bottleneck is around the application logic, which is typically orders of magnitude slower. Not only that but binary protocols are a nightmare to debug from a trace, where as text-based protocols can be trivial. My point being that if you think you need a binary protocol to speed things up, you probably don't.
[list of accepted papers](https://github.com/gasche/icfp2013-papers#readme)
Does it have interactive sessions (like GHCi)?
Please, somebody, edit out the audience interruptions.
You should crosspost this to /r/cpp and possibly /r/programming.
Is there a paper on this? I'd rather read a paper than go through a video.
Sorry, but no there isn't a paper for this talk.
Thanks for the suggestion. I was able to crosspost to /r/cpp, but reddit didn't like it when I tried /r/programming as well. Feel free to post to /r/programming yourself if you like.
Good point. We'll adjust that. Thanks. 
1. Yes; the level of ongoing support will depend on the level of demand. Currently we test on iOS Safari periodically but not daily. 2. A simple REPL UI is not in there yet. Working on it.
Not in first beta. Working on it.
Nevermind. This link is the world! http://www.haskell.org/haskellwiki/Learning_Haskell I leave this question undeleted for other beginners.
I will definitely be following up on this, then. I think you're doing great work and I'm glad someone does what I don't currently have the resources to do!
I am really excited to get this on my Ipad.
This book might be good for a "real world" example: https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours
I have issues with this presentation. The presenter improves his sample program in four stages: 1. Go from lazy IO to strict IO to ensure file handles are closed. 2. Apply return strictly to ensure the files are processed as they are read. 3. Move from String to strict Text 4. Use Text.copy to avoid holding onto the entire Text values. The problem is that this is actually a good example of a place where lazy IO is useful. The fix in stage 2 solves the problem in stage 1 without the need for strict IO, because the file must be consumed before the result is "return"ed. Lazy IO then allows the file to be processed without requiring the entire file to be in memory at once. For this reason, in stage 3 it's actually significantly faster and more memory efficient to use lazy Text instead of strict Text. Stage 4 no longer becomes strictly necessary since the keys only keep chunks of the lazy Text in memory, so it becomes a trade off of time vs space (but it's still a useful point).
Excellent points. You forgot the possible collaboration features of using the web, or making use of caching to support disconnected operation. There are endless possibilities.
I did mention collaboration :D. 
I highly recommend http://www.seas.upenn.edu/~cis194/lectures.html - kind of like LYAH and a bit beyond, but with exercises. If you've got functional programming experience you should be able to get through it pretty quickly.
There's also: * http://acm.wustl.edu/functional/haskell.php * http://www.scs.stanford.edu/11au-cs240h * http://shuklan.com/haskell/index.html
GHCJS has almost the same space behaviour as GHC (up to a constant factor): We use the stack in the same way, have blackholing (so all references in a thunk get nulled immediately when the thunk is entered, this makes exception handling a bit more tricky, but prevents memory leaks in loops) and tail call optimization. There might be minor differences: Most of the memory management/cleanup work in GHCJS is done by the JavaScript runtime, but some of it has to be done by our own cleanup handler, which runs less often. Therefore it might take a little more time before weak ref finalizers are run. There might be a few edge cases that we have missed though, but those can probably be fixed.
Any time you pass a recursive datatype down a long pipeline of channels/signals, and it doesn't actually evaluate until the end.
Hi, I'm developer of this RSS reader and want to share with you why Haskell is a big win for my project. First. Haskell is a fast compiled language. Most RSS readers are written in Python, Ruby or PHP. And they're all quite slow or requre a ton of servers to handle the load. With Haskell I'm able to quickly implement complex features and don't think much about performance. Second. Haskell is very high level and handles concurrency well. It's not a problem to write very generic code or spawn thousands of threads. Third. There a many good libraries now (much more than 10 years ago ;). I want to thank Bryan O'Sullivan, Michael Snoyman and all other package authors for their efforts on making Haskell true platform. I'm especially like text, riak, aeson, http-conduit and warp packages. They're used 24x7 under load and I haven't found any major problems with them. And few quirks. Unfortunately not all packages are equal in quality. Many libraries are toys or don't work well under load. But at least there are libraries to take some code from. Bindings to C libraries (as well as C libs itself) are evil. I've had problems with almost every C package I've tried (curl, regex-pcre, hsdns). Long-lived ByteStrings can cause memory fragmentation and ten-fold performance drops. So I'm using Text for everything except I/O. Haskell was missing fast malformed html/xml parser. But it took only a couple of days to write fast-tagsoup package that parses tens of MB/sec. In general with Haskell I can quickly write high level code that runs fast. And (at least for my project) I'm seeing that most tasks now are CPU bound (due to fast SSDs), not I/O. So compiled yet high-level languages shold gain more popularity now. If you have some questions about reader implementation details feel free to ask me. PS: reader is profitable, so I'm thinking it could be counted as a Haskell success story ;) 
Thank you. That helps a lot.
Actually it's not official demo. More like test account. Trial is free for 30 days and it's a real demo.
It would be better to post this when it actually launches.
&gt; Long-lived ByteStrings can cause memory fragmentation Lazy bytestrings or strict bytestrings?
Start with one of the haskell web development frameworks. They usually have tutorials covering simple practical tasks, like interfacing with user, reading / writing to database etc. There's even a book for one of them: http://www.yesodweb.com/book
I tried bazqux based on the technology - I hoped it would be relatively quick and problem free, and it has been. It's great to hear these details, and that it is profitable. If you have the time, then I have almost unlimited questions. Eg: Where have you used Ur(/web) and where Haskell ? When working in one, what do you miss about the other ? Are you the only developer ? Would it have been profitable without the unexpected end of Google Reader ? What experience had you with Haskell/Ur before this ? Any other languages you seriously considered for this project ? Do you run it on amazon servers ? How many ? What were and are your biggest challenges ? When did you start work on it ? Thanks and congrats! --a happy customer
sometimes you do tho
How does that trigger a space leak?
&gt; Unfortunately not all packages are equal in quality. Many libraries are toys or don't work well under load. But at least there are libraries to take some code from. I have a list of maintainers that I know are beyond extremely reliable, who create industrial-strength packages, push the limits of quality, and are extremely friendly to work with when I have problems. That list grows daily as I meet more of the people who maintain stuff on hackage. Unfortunately, I just cannot trust any package that I cannot find an endorsement for from someone I know.
&gt; Where have you used Ur(/web) and where Haskell ? Ur/Web is used mostly as a way to not write in JavaScript, generate html, simplify serialization, ajax and as a part of web server. On server-side it just calls Haskell code via FFI (final executable is linked using ghc called from shell script named 'gcc'). &gt; When working in one, what do you miss about the other ? Ur/Web has very weak library support, misses most Haskell conveniences (especially in pattern matching) and have quite long compilation times. But I can't easily convert Haskell code to JavaScript and web part is easier in Ur/Web. &gt; Are you the only developer ? Yes. &gt; Would it have been profitable without the unexpected end of Google Reader ? Probably it would took much more time and features to do to make it profitable. &gt; What experience had you with Haskell/Ur before this ? About 3 years of Haskell and 4 years of OCaml (mostly full-time job). No experience with Ur. &gt; Any other languages you seriously considered for this project ? Did not considered. I don't like dynamic typing and didn't want to code in JavaScript of Python. And I didn't knew anything about webdev. When I sought Ur/Web I decided to try it for web part. And never considered anything except Haskell for feeds fetcher. &gt; Do you run it on amazon servers ? How many ? No. It's too costly and many sites blocks requests from AWS (and other cloud services). I'm running it on 8 dedicated servers at Hetzner and thinking to add more. Although it's not as easy as adding Amazon instances but I'm getting much more performance for the same price (you can read more on Amazon prices/performance at http://openmymind.net/Why-I-Dislike-ec2/) &gt; What were and are your biggest challenges ? Making it work. Feeds can have any imaginable errors (look http://inessential.com/2013/03/18/brians_stupid_feed_tricks for incomplete list of examples). And now scaling it while answering ton of mails and implementing new features. I didn't expected such traffic and didn't even distributed feed fetcher. Only database (Riak) and search engine (ElasticSearch) is distributed. Fetcher is running on a single machine but still have 3x capacity (Haskell FTW!) and there are some parts which are easy to distribute (and since my reader is not free I can remove feeds of users with expired free trials -- next thing to do ;) &gt; When did you start work on it ? First attempts in 2009 then few days a year and full-time since 11/11/11.
Same thing. If I see BOS as a package author I can trust it. Other packages usually need testing. But I'm seeing more and more industrial-strength packages. And that's a good thing.
It's fast! I like the idea of using Ur as a front-end language to avoid writing bad HTML and JavaScript and then using Haskell as a backend server for more general purpose tasks. I'm happy that it's profitable for you, thanks for the experience report.
If you push anything to hackage, please let me know ;)
So what is the stricter type for 'make_type_and_value'? make_type_and_value : (a : Set) → a → (b : Set) × b make_type_and_value t v = (t , v) At 47:30, the presenter struggles to find the answer. And so am I! Of course, changing the last 'b' to 'a' will also typecheck, but it's not stricter, it's again too loose but in a different way. It looks like it should be possible to express the fact that the returned type 'b' will be the same as the given type 'a', but so far all the types I found which express this require the body of the function to be changed, e.g.: make_type_and_value : (a : Set) → a → (b : Set) × (a ≡ b) × b make_type_and_value t v = (t , refl , v) I'm starting to think that the original function could not have had a stricter type. Agda needs intersection types!
Good counter point. I hadn't considered that. /s
Very impressive project! I hate to be that guy (actually, I'm not sure I do), but have you considered accepting bitcoin payments? Many bitcoiners will naturally gravitate toward services that allow payment in bitcoin so it could be a good way to get a boost early on. There are many services that automatically convert (and price) payments to the currency of your choice so you don't need to actually hold coins.
To elaborate on efrey's point: If you're implementing **both** the application and the transport protocol it uses then you're in a position to say when enough optimization (if any) is enough. But, if you're writing **only** the library -- and intend it to be for general use -- then your users will appreciate you not making assumptions about where their bottlenecks might be.
I'm trying it out now, and I REALLY LIKE IT. Copying / starting with a google reader style UI is a great way to roll. Is there any limit in the paid version to how far back you'll keep the feed history items? Once GHC JS gets released, might you transition to it for your frontend, or does ur/web offer any upside over that approach? Either way, great work, i'm very excited, and means I don't have to write my own haskell feed reader in my time :) 
Hi Tekmo, you seem to have disabled anonymous comments on your blog, so I'll ask this here. I don't understand why this is not a Monad instance: instance Monad Resource where return a = Resource (pure (a, pure ())) m &gt;&gt;= f = Resource $ do (m', release1) &lt;- acquire m (x, release2) &lt;- acquire (f m') return (x, release2 &gt;&gt; release1) [EDIT: ~~I think your Resource (presumably Monad) is basically a wrapper around `ContT IO`.~~ Actually I'm not so sure about that, but it's got a `Cont`ish flavour to it.]
Just to let you know your reader is also on the front page of Hacker News: https://news.ycombinator.com/item?id=5961570 Some people there are also interested in an experience report, if you repost your description this might drive some additional interest. The reader is awesome and it is great to read about this Haskell success story!
This isn't a problem unique to haskell by any means though. These kinds of problems are endemic in open source packages. 
Understood, but with something like this I think it is important to get knowledge about the service out there before it launches so that the community have a chance to have a say in what they want included and can help shape the service to their own requirements rather than what one company *thinks* they want.
This was really interesting and has definitely whet my appetite for knowing more about denotational semantics! Anyone have any pointers to good books or papers on the topic?
[Semantics with Applications](http://www.daimi.au.dk/~bra8130/Wiley_book/wiley.html) is pretty good - that link has a pdf of the old version. There's a newer edition you can get from Amazon. I originally came across it [on this page](http://matt.might.net/articles/books-papers-materials-for-graduate-students/#pl), which also has some other interesting books and papers listed.
You monad instance is not exception safe. If the second acquire fails, then the first resource is not released. But I see that Tekmo's Applicative instance has the same problem If all you do with resources is `runResource`, you could indeed use continuations: newtype Resource a = Resource { withResource :: forall b. (a -&gt; IO b) -&gt; IO b) } -- aka. ContT IO runResource = withResource id fromAcquireRelease a r = Resource (\k -&gt; bracket a r k)
The original type signature make_type_and_value : (a : Set) → a → (b : Set) × b is alpha equivalent to yours: make_type_and_value : (a : Set) → a → (a : Set) × a which is in other words: make_type_and_value : ∏ (λa. a → ∑ (λa. a)) Which is trivially inhabitable by `λva vb. (va , vb)`. This explanation: &gt; It says, given some type "a" and a value of this type, the function will return a pair of any other type "b" and even produce a value that type b out of nothing. implies that both a and b are introduced by ∏s.
I see what you mean, the Haskell-style signature indeed looks like it's promising too much. However, the type variables which appear in Agda's type signatures work slightly differently than in Haskell. In particular, there is no implicit forall binding all of the free type variables; instead, each type variable needs to be bound explicitly. The (a : Set) → ... part is such an explicit forall, because it appears to the left of an arrow. The other part, (b : Set) × ..., appears to the left of a cross, so it doesn't mean forall b, but instead means exists b. Thus, the Agda interpretation of the original signature is not that the function promises to able to produce a pair of any type requested by the caller, but instead the function promises that there is a type for which it is able to produce a pair.
Thanks, you're right, I mistakenly thought about implicit forall.
that would motivate the bytestring replacement library written with vector http://www.haskell.org/pipermail/haskell/2011-October/023021.html
I've been trying to work out how best to ask such a question myself. Thank you for asking it :)
I think Yesod Web Framework is a canonical example for that. I'm a "traditional" imperative business programmer and Yesod has impressed me.
It's been sort of in the back of my head ever since I started working with this kind of software, coming from a background of academic/hobby programming using a lot of FP. Actually at first I was still somehow convinced that FP wasn't really "practical," and that the combination of OO and relational databases was pretty well worked out and good, and that I should just stop being such a contrarian, or something like that. But now I more and more see how immensely difficult it is to leave a maintainable legacy of OO code, and how little thought usually goes into design and abstraction...
I'm not the author of this website ... but in my experience, most C libs are badly designed, heavily relaying on side effects, so it's difficult to build a friendly and pure Haskell binding.
Same here, I've just not been learning it as long as you yet. Part of me is still saying FP is pointless because I've not been able to make anything in it yes, while in 1 weekend I wrote a game of Ultimate O's and X's in Python.
Maybe a slower pace of work is part of what's needed! Rich Hickey has a talk where he starts by asking "When's the last time you spent a *whole hour* thinking about something?" Businesses want things done really quickly... But when you've been working really quickly for five years, the product owner starts questioning your estimates: "How could this possibly take so long?" Because the code is a mess! Why? Because we do everything quickly.
I hadn't thought about it like that. That's a really good point.
I think we can fix the monad instance (and, consequently, the applicative instance, too), by writing the following: m &gt;&gt;= f = Resource $ do (m', release1) &lt;- acquire m (x, release2) &lt;- acquire (f m') `onException` release1 return (x, release2 &gt;&gt; release1)
Thanks. That makes a lot of sense.
What's so hard about writing such a game in Haskell?
Already posted in the comments of the other GHCJS post. You can install the prebuilt environment on Amazon EC2 with the Ubuntu 12.04 LTS 32 bit image (and other similar systems) by running the following script: #!/bin/bash sudo /usr/sbin/useradd -d /home/vagrant -s /bin/bash -m vagrant sudo locale-gen UTF-8 sudo apt-get update sudo apt-get -y install axel ghc cd /home sudo axel -n 8 http://hdiff.luite.com/ghcjs/ghcjs-prebuilt.tar.gz sudo -u vagrant tar -xzvf ghcjs-prebuilt.tar.gz Log in as the newly created `vagrant` user: sudo -u vagrant -i And everything should work, with GHCJS in the Path. (it uses axel instead of wget because the connection to hdiff.luite.com, which is hosted in Germany, is sometimes a bit slow. Axel downloads with multiple connections) 
&gt; Agda needs intersection types! Spellcode has them, but you don't need them for this signature! http://www.reddit.com/r/programming/comments/1h9t4s/using_agda_to_design_and_refactor_c_programs/casltwy (Not that my language is particularly useful without any implementation ... :/ )
I only pushed fast-tagsoup, hsdns-cache (which I plan to replace with pure Haskell version, due to infrequent assertion failures in ADNS) and http-conduit-downloader (bit specific for my needs and not yet updated to latest http-conduit version)
Segmentation faults, assert violation, sporadic errors caused by memory corruption. curl needs special setup to work with SSL in multi-threaded environments (and many other quirks). Now we have http-conduit so there is no need for curl anymore. adns (hsdns package) sometimes show assertion failures and can hang resolving for a few seconds. A bit changed pure Haskell dns package works much more stable but doesn't resolve domains sometimes. regex-pcre can return exception of error code -4 = PCRE_ERROR_BADMAGIC = junk pointer. I'm using pure Haskell regex-tdfa instead.
Don't want to bother with non-standard methods of payment at the moment. And not many people have requested this.
Yes please, put it on hackage. I use brackets to open/close resources all the time. Would love to play with new monad and see if it buys me anything. 
I think that's pretty accurate too. This thread has inspired me to follow the Yesod quick start guide and get a web-app running (which is what I mostly use).
I keep last 500 items (no difference paid or trial). Don't think I will have time to rewrite about 6k lines of Ur code to GHC JS. And beside running Haskell in JS there is FRP, easy html generation, easy serialization, absense of dead links.
Now that you have it up and running on EC2, reset the root password to "test" or something. And now in the dashboard, you create an AMI similar to taking a snapshot. http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami.html
Huh? If you bind the tuple elements to their corresponding names, doesn't that mean the tuple has been evaluated to WHNF?
So your language has both dependent types and subtyping? i.e., the value 4 has type Nat as well as the type (singleton 4)? As opposed to a construct such as data singleton {i} {a : Set i} : a → Set i where the : (x : a) → singleton x which would require an extra "the" constructor in the implementation of make_type_and_value? make_type_and_value t v = (the t , v) EDIT: oh, right, intersection types. So of course it has subtyping. EDIT 2: do you plan to work on your implementation on github? your language sounds like a project I would like to follow.
I work at a company where we built a an enterprise application suite in a mix of [Kawa Scheme](http://www.gnu.org/software/kawa/) and Java that has all of those characteristics: * The application is delivered as a customer-internal web portal with personalized reports and dashboards for thousands or tens of thousands of end-users. The per-user personalization was based on the customer's organizational hierarchy. * The application is organized around our own home-built [OLAP](http://en.wikipedia.org/wiki/Online_analytical_processing) engine, with our own DSLs for stating the contents of reports and dashboards. * The application is custom configured for each customer—the database schema was late bound and configured. * There have been more than 10 major releases, often with substantial upgrade requirements between them. Though the bulk of the more complex parts were built in Scheme, it isn't "purely functional" by any stretch of the imagination. As it happens, I am now in the process of designing and prototyping the next generation of the OLAP engine, from scratch. We are building it in Java, however—the company's old owners were never happy with the Scheme thing, and the new owners simply won't have it. This is about describing the content of reports that need to be executed for tens of thousands of end users every day—things like whether they are meeting their sales targets, spending too much time on the phone, etc. The heart of the new design I'm working on is a abstract syntax tree representation that's heavily based on the Reader applicative functor. So the project right now is about building an AST type that's based on Reader + a very few extra things, and building a variety of "compilers" that transform the AST into "workers" that execute the calculations using a variety of planned back ends: immediate in-memory execution, translation to SQL queries, translation to MDX queries, MapReduce jobs, and reactive-style realtime networks are the main ideas now. The more theoretical FP stuff has been of enormous value: for my design I've been able to make use of typed lambda calculus, standard type classes (Applicative, Category, Profunctor), etc. [Denotational design](http://conal.net/papers/type-class-morphisms/) has been particularly helpful (see also [Luke Palmer's simpler introduction to the concept](http://lukepalmer.wordpress.com/2008/07/18/semantic-design/)). So I've been able to get a lot of stuff "for free" out of these concepts—for example, the fact that applicative functors have a left-associative normal form translates into optimization rules that make for dramatically more efficient MapReduce jobs. More generally, having a simple denotational semantics for the AST means that I get equational laws that give me correctness conditions and candidate optimization rules for back ends. 
Welp, i'm gonna take the leap then. Awesome. Edit: ok this is the nicest reader I've tried by far. http://i2.kym-cdn.com/photos/images/newsfeed/000/264/200/acb.jpg
Best implementation out there that I have seen so far. And I have tried very possible reader implementation out there including Feedly, AOL and Digg as well. It works and it works fast. That's the only two things that should be there. And comment tracking is an awesome feature to have. Gonna get the paid subscription for sure. Great work, dev :)
Thanks, I'm now following you on github, but I couldn't convince my RSS reader to follow your blog. I think the fact that there are no entries makes it think that the RSS file is invalid. EDIT: what is Elementscript? EDIT 2: If you want to practice implementing a dependently-typed language using Haskell as the implementation language, I recommend following Conor McBride's paper on the subject, [Simply Easy! An Implementation of a Dependently Typed Lambda Calculus](http://strictlypositive.org/Easy.pdf‎). EDIT 3: fixed broken link.
Elementscript-μ (elementscript-micro) is a tiny, minimal (pretty much esoteric) language I'm also designing (separately from Spellcode) that is meant to be an experiment with what I call "infix homoiconicity," by which I mean, literally *everything* is an infix operator application. Even parentheses. I don't plan on using Elementscript syntax for Spellcode, because I don't think a language that I plan for people to actually use should limit itself in syntactic nature (I'm looking at you, Lisp), but ES-μ should be simple enough for me to get some experience in language implementation, after which I can start work on actually implementing Spellcode. The name Elementscript comes about because I originally thought about a set of related domain-specific languages based on the infix-homoiconic syntax, starting with Elementsong (for world description and transformation, e.g. for game worlds or simulations) and WebElements (for static web page description and transformation). ES-μ won't be useful, but it will share that same syntax (in fact, it will be nothing *but* that syntax, and some very simple lambda-calculus-like semantics). EDIT: Thanks for the paper! I'll definitely take a look. EDIT 2: Your link to the paper is screwed up, although I managed to find the paper anyway. :)
It's not async-exception safe; if an async exception is thrown between lines 2 and 3 in your sample, release1 will never be called. Adding a `mask_` before the `do` is probably sufficient to solve that.
Here you go: m &gt;&gt;= f = Resource $ mask_ $ do (m', release1) &lt;- acquire m (x, release2) &lt;- acquire (f m') `onException` release1 return (x, release2 &gt;&gt; release1) Would you be interested in this? The reason I ask is that `conduit` has the same issue as `pipes-safe` where you can't reuse `withXXX` idioms and you have to build your own bracket abstractions on top of the raw open and closing motifs.
&gt;combination of OO and relational databases was pretty well worked out and good If you leave out the OO part, it does work fantastically well. Relational databases solve all the problems you mentioned very nicely, use them instead of trying to awkwardly re-implement their features in your code.
Time is needed for this. Need to change sources (move hardcoded private keys outside, disable some features), need to change way I'm working with source (move TODO.org out). And then need to support people to help them understand what code does. Plus all comments are in Russian. While making it open source is probably good marketing strategy I currently have no time for this.
I think you don't have shops of 25 haskell devs grinding away on one project, so an orthogonal question is what the type system and compile time checks buys you, for 1-10 devs. This blogs pretty good: http://failex.blogspot.com/ --------------- People frame these questions in different ways: "how do you handle incomplete and stale data in dynamic problem domains with malicious users, unreliable hardware and networks?" You can google phrases like: (open ended answer to O-E question) - expression problem - CAP theorem - formal methods and code generation - covariance/contra-variance in java /c# ------ "Enterprise systems" case studies: http://corp.galois.com/systems-software http://www.haskell.org/haskellwiki/Haskell_in_industry http://cufp.org/conference/schedule/2012 https://www.fpcomplete.com/business/resources/case-studies
Yes, that implementation looks correct. I'm not sure if I agree with this assessment. As I've mentioned in the past, `conduit` (and I believe `pipes`) is fully capable of using the `bracket` idiom. As an example, see: https://www.fpcomplete.com/user/snoyberg/library-documentation/resourcet#resourcet-is-not-conduit The problem is the ability to acquire a resource within an pipeline, such as the interleaving demo from the same tutorial: https://www.fpcomplete.com/user/snoyberg/library-documentation/resourcet#interleaving-with-conduit. As I understand it, `Resource` isn't really adding anything new here relative to `bracket`, and therefore the same resource allocation limitations of the standard `bracket` pattern would apply here. In other words, I continue to believe that the most powerful and flexible approach to resource allocation is to hoist it to some external handler, such as `resourcet` or `regions`.
Nope. The expression let (a,_) = foo in bar a is equivalent^* to the expression bar (fst foo) If `bar` is sufficiently lazy, the pair `foo` will never be evaluated to WHNF. In contrast, the purpose of the case expression is to evaluate `foo` to WHNF before continuing with the evaluation of `bar`. ^* up to a very subtle detail in space usage
&gt; Multi-user with authentication and authorization with some complexity: user groups, capabilities, hierarchies, etc. Prolog!
http://en.wikipedia.org/wiki/SAP_AG
Is there a way to export the OPML data in the trial version? Also, while I see why you might not want to do this, will you add ads as an alternative to paying?
What I meant by that is that you can't take an existing bracketed computation of type: (a -&gt; IO b) -&gt; IO b ... and lift it to work within `ResourceT` or some other finalization framework. In order to define a bracketed computation using `pipes-safe` or `conduit` you need access to the original open and close functions. `Resource` lets you compose handles while preserving the open and close routines without locking you into a continuation in the `IO` monad.
&gt; Edit: I also fixed the comments to allow anonymous comments again. Thanks
Options (top-right corner) =&gt; Subscriptions =&gt; Export OPML. No plans for ads.
We sure like to add edits to our comments :) I think starting with a variant of lambda-calculus is very wise. I'm quite curious at how such an infix homoiconic language might work. The homoiconicity restriction doesn't sound that bad, I guess you just need all of your primitives to have exactly two arguments, using tuples when more than two is needed. But how can you represent parentheses as infix operations? The example type you gave for make_type_and_value had a few example infix operators, such as &lt;[ pi ]&gt; and &lt;[ pair ]&gt;, and I can easily see how you could have written (x &lt;[ arrow ]&gt; ...) instead of (\x -&gt; ...). But parentheses?? I really wonder what you have in mind for those!
I'll add some more information clarifying that. As for setting up an EC2 auto scaling solution yourself? Of course you could but there is the convenience part that Storm Cloud Systems will provide and pricing will be better than an equivalent AWS deployment. I don't want to get into specifics about some of the features that are in the works at present but I can say that those looking for a reasonably priced, flexible solution that removes the headache of having to administer medium / large web applications / services should keep an eye on progress.
Why were the new and old owners unhappy with the use of Scheme? Does that make it more difficult to hire new employees?
In advance, sorry for the poor sound quality. Our usual equipment wasn't available. 
I see. Well I'll definitely move over from Newsblur once this happens! 
The syntax I used above was my tentative (and desugared) syntax for Spellcode, not ES-μ. Their respective syntaxes are completely unrelated. :) Re. parentheses: The idea (for ES-μ) is to use `( = x -&gt; y -&gt; y app1 x` and `) = app1`, where `app1` is the identity function (with an infix-appropriate name). `app1`, or something like it, will probably be primitive (as will `-&gt;`, for lambdas, and `=`, for definitions). Then, when `(` and `)` are given appropriate fixities (which will be extremely low, possibly the lowest of all except for `=` and `;`), you can write `a ( b ) c` where `a`, `b` and `c` are arbitrary ES-μ expressions, and have it mean `a b c` (that is, `b` applied to `a` and then `c`), except that you effectively override the fixities of any applications in each of the three expressions with respect to how they are parsed relative to each other, producing exactly the results that you would think parentheses should produce. Similar results are possible with `[` being just like `(`, `]` being the nil constructor for a list, and `,` being the cons constructor. Again, given appropriate fixities, you then have a perfectly good list literal syntax, without any of it being built-in. I should mention that any non-empty string of non-whitespace characters is a valid identifier in ES-μ, and operators (that is, everything) are partially applied on the left or right implicitly whenever an operator with a lower fixity is to either side. So, for example, because `map` has a lower fixity than `*`, `5 * map [ 1 , 2 , 3 ]` means, in Agda-like syntax and converted entirely to prefix, `map (_*_ 5) (_[_ (_,_ 1 (_,_ 2 (_,_ 3 _]_))))`. EDIT: fixed some spelling mistakes EDIT 2: The "Agda-like" syntax I used above will actually be valid Spellcode syntax, with the expected meaning (although the `_`s won't necessarily do anything in particular, unlike in Agda). EDIT 3: I just realized that some my ES-μ syntax examples were wrong; I've corrected them.
I don't think I can help, as seem have different ideas about what constitutes useful but I found LYAHFGG incredibly useful as a starting resource when I first had a look at Haskell. Being able to sit down with ghci and work through all those small examples really helped me get a feel for the language, the syntax, and its philosophy. I don't think I would have done nearly as well with "Real World Haskell". I guess my point is it's funny how different people learn in different ways. 
I agree that yesod is a great example of this... Multi-backend auth. is awesome! Web auth has so many conditionals, monadic structure really makes the code cleaner and easier to understand.
I don't think OP is asking about frameworks that provide features x, y and z. They're asking about custom enterprisey information systems that make use of x, y and z (perhaps by using a framework).
Didn't see that, but this is the actual video and not just the slides.
On it's face, I agree with your statement, at least in principle with what you are saying. Practically, a binary protocol or even an entire middleware layer is about more than just throughput. Which I am sure you are all too aware. So here is the part where we probably diverge, I see your statement about how nightmarish a binary protocol is to debug. But having done a lot of network engineering and troubleshooting, when someone has invented their own way to pass packets around a network, often at that point, for better or worse it is up to network engineering to prove that their network is not to blame. Without getting too much of a soapbox about it, however much you feel that debugging wastes time ... packet analysis without a formal specification is probably just as bad. Not to mention that it is probably more costly, especially in the long run. On the flip side, a personal project, mock or a multitude of other reasons may not benefit from AMPQ or whatever else mentioned. But, if you are rolling a moderately distributed application that is enterprise scale or larger. I hope that the rationale for not using a transport library is better than they suck to debug from a trace. A lot more companies do this than they should. Anyway I still agree with you, just up to a point. EDIT: readability 
Wow. This is either brilliant or insane, but I'm not sure which! The parentheses would only work around the infix part, right? For example, you couldn't do 5 * ( a ( b ) c ) * 4 , could you?
Sounds great. I definitely want to join beta test version (if it doesn't charge me any.)
I still don't completely "get" denotational design, although I think I'm a bit closer there after reading Luke's post. The part that makes denotational design difficult for me to get is that I haven't yet seen an example of good denotational design that doesn't end up using `Reader`, so I don't have more than one example from which to build the general intuition for how it works.
This is pretty great. Amazingly fast, and the "get full post" functionality's exactly what I've always wanted. Once you can hide the feeds list with a hotkey and move from one feed to the next with spacebar or j, I'd say this is actually a much better product than Google Reader. Props.
As a Haskell newbie that's been in for about a year and a half now and a heavy user of functional programming before (Erlang and Scheme in production); I will tell you: simply because you are not immediately productive in a language that is foreign does not mean the paradigm is pointless. I get that you get that (hence why you're sticking with it) but I think when you finally "grok" *why, expert and newb are singing the praise of Haskell* you'll understand **why** FP matters. Particularly the degree of functional programming Haskell embodies. I also came from Python for web applications. While maturity of libraries is missing in only some areas, as a language, I wish I could program in *nothing but Haskell*. Keep at it. Try to see the bigger picture being presented to you by Haskell.
Actually, I think they work in any case! Consider `( a ) b c` for some arbitrary expressions `a`, `b`, and `c` (such that the resulting AST applies `b` to `( a )` and then to `c`). Note that `)` has a marginally lower fixity than `(`, and both of them are lower in fixity than any other operator (other than `=`, `;`, and possibly `-&gt;`, none of which we consider here). We are thus certain that, regardless of what `a` might be, `( a ) b c` parses as (using { ... } to group things in English) the application of `)` to { the right partial application of `b` to `c` } and then to { the right partial application of `(` to `a` }. If you evaluate away the paretheses, you end up with the application of `b` to `a` and then to `c`, which is exactly what we intended. Likewise, `a b ( c )` is parsed as the left partial application of `)` to { the application of `(` to { the left partial application of b to a } and then to `c` }. Evaluating away the parentheses, as before, will again give you just the application of `b` to `a` and then to `c`. Just to drive the point home, let's look at `( ( a ) ) ( b ) ( c )`. Since `(` is right-associative and `)` is left-associative, this is parsed as the left partial application of `)` to { the application of `)` to { the application of `)` to { the left partial application of `)` to { the right partial application of `(` to { the right partial application of `(` to `a` } } } and then to { the right partial application of `(` to `b` } } and then to { the right partial application of `(` to `c` } }. This once again simply reduces by evaluation of the parentheses to the application of `b` to `a` and then to `c`. If I've done any of the mental evaluation wrong, please tell me. :) I think the only caveat is that, in some cases involving right partial application of `(`, the thing it's partially applied to might need to be a function, but that's not a problem in ES-μ because *everything* is a function in the lamdba calculus! :P EDIT: Added a few commas.
I am currently helping to rewrite such a system that is currently written in Java (probably slightly simpler than you're imagining but about 250k lines of code). Java is a language with horribly poor abstraction capabilities, and I'm finding it far easier in Haskell. I prefer Haskell as a general-purpose programming language to any other language I've used. One of the major benefits of Haskell is that it has allowed me to write a relational query EDSL (basically an improved API to HaskellDB). The relational world is a great fit for so many problems, but people's appreciation for it has been ruined by SQL and object relational mappers, which is are each abominations in their own particular way. 
Semantic design is simply that you 1. Start with a very simple mathematical model for the types and combinators you want to implement. ("An image is a map from points in the plane to colors", "A dictionary is a collection of `(key,value)` pairs") This model may be horribly inefficient if implemented in Haskell directly, or it may not even be implementable at all. 2. Write an implementation in Haskell that is correct with respect to the model. In particular, the combinators commute with the mapping from the Haskell representation to the semantic domain. In pseudocode: semantics (x `combinator` y) == semantics x `combinator'` semantics y
Ahh, thanks for the clarification. To me, this is a chicken-and-egg scenario: `Resource` itself isn't terribly useful for `pipes` or `conduit` until other package authors start using it. And until we bake in support, package authors probably won't see a need to use it. I personally haven't felt the need for this arise yet, as most packages that have scarce resources provide the open/close semantics. I wouldn't be opposed to including this datatype and some helper functions in `resourcet`, but I'm uncertain of how useful it will end up being.
Oh I quite agree. When I first started using Python over PHP I found it a nightmare any time I used Dictionaries as I was used to Ordered Arrays from PHP. On the flip side of that, Haskell has a bigger gap between it and the rest of the languages I know than any of the languages I know have between each other. I will stick at it, I've read through much of Learn you and recently picked up Real world.
Although you already seem to have found what you were looking for, try the Haskell School of Expression, by Paul Hudak. Very practically minded book.
Thanks. Unfortunately it looks like storing the AMI is not free, and it's an extra step to keep it updated along with the ghcjs-prebuilt archive. I hope it's not a problem to just have this script. I'll add the new instructions to the next blog post.
Quote from the site: &gt;Will the beta cost money? &gt;Yes. The beta will cost money but the charges will be a lot lower than they will be when the site launches properly. The exact discount offered to beta users still hasn't been decided but this page will be updated when it has. 
Why?
&gt; you don't have shops of 25 haskell devs grinding away on one project Oh yes you do...
It is a primitive operation that can be combined with mergeModules to construct operations such as "move this symbol from module A to module B", or "split this module into two modules, one containing symbols x y z and the other containing all the rest".
Does this obsolete [ghc-mod](http://www.mew.org/~kazu/proj/ghc-mod/en/)? For instance, can it use flymake? Also is there some easy to read documentation for it? 
Hey Vladimir, I just wanted to say congratulations. It's great to see that BazQux is profitable for you! And, also, it's great to see people using Ur/Web more in the real world. It's a fantastic platform for a lot of web applications (in a lot of ways Haskell just can't quite match.) As a question, you suggested earlier that you linked the Haskell code into your Ur/Web binary and just use that. Did you ever think of using something like a message queue system to pump requests and responses around between multiple services in BazQux (like a Haskell service to do DNS resolution, one that will pull things from the targeted feeds, etc?) To me, Ur/Web - due its design - has always felt very unix-y in a way, like, "do one thing right and very well," so when it comes to non-web app interfaces and dealing with external services, Haskell is obviously a better choice. I mainly ask if you ever considered this approach, because compiling and linking things correctly sounds like it is a huge pain to get right with many tricky bits. And you could instead have tiny amounts of Ur FFI to push things down a queue, that other things would read/respond to. Or something like that. Just curious. Anyway, congrats again!
So if I understand this correctly, you have two categories, one of which is your "meaning" and the other of which is your implementation, and your semantics are a functor from implementation to meaning. The `operator` would be a composition operator in both categories and the equations you just wrote would be one of the functor laws. So my understanding of semantic design is that you define a functor from something complicated to something simple so that your user can focus on understanding the simple thing and trust that the complicated version preserves their intuition, thanks to the functor laws.
Prolog is based on facts and rules, so you coudl create a database. employee(chris). employee(mike). employee(phil). sales(chris). trader(mike). admin(phil). access(website, E) :- employee(E). access(customers, E) :- sales(E). access(exchange, E) :- trader(E). access(mainframe, E) :- admin(E). 
Can I donate? I would be interested in helping to set something up that tracked a github branch, built this, and managed the latest AMIs
No. It works with `ghc-mod`, it does not replace it. haskell-mode will control subprocesses like your REPL and Cabal, do syntax highlighting, sorting, indents etc. `ghc-mod` will be used with flymake to compile your buffer, if you set the appropriate hook for `haskell-mode`. That's all. You could technically set the same flymake hook for like, any mode. It just wouldn't be useful. As for the documentation, as the release notes say, they're working on an Info manual. In the mean time, installing it and going through `M-x haskell-customize` will give you a big list of possible configuration options, and you can rummage through the modules in the source code to get an idea of what's there.
Thanks for answering. I'll keep ghc-mod then. Still there seems to be some overlapping... e.g., Haskell mode customisation mentions to hlint checks and hoogle searches, both also provided by ghc-mod. 
From my understanding of Conal's principles, the intention is usually as much to present a simple interface to the user as to derive behavior that's "harmonious" with the rest of your design. I personally came to this understanding by reading his FRP Push-Pull Semantics paper. I highly recommend it even if you don't care about FRP—once you begin to understand his notational distinction between Haskell and meta-Haskell (where his denotation functor exists) it's a great narrative. In that article (and a few of its predecessors) he walks through iterations on a design of an FRP signal. The "denotation" he chooses is just a continuous function with some FRP-related relations (the target category) and he is able to use the functor laws to work backwards to derive how more sophisticated implementations and laws must behave. My current mental model is thus that denotational design is a process that starts with a nice denotational category and then iterate many elaboration categories (potential implementations) deriving their properties by trying to write functors from them to the denotation. Eventually you get all the way to a category that can be efficiently implemented.
That's first post on failex is fascinating. I always wanted a good example of why non-typeclass equality is such a code smell to me these days.
Yes -- the expectation is that the function passed to `simpleHTTP` is going to return a `Response` or something that can be turned into a `Response` using `ToMessage`. But, in `home` you have the return type `()`. You create the response `"Welcome home"`, but then just throw it away.
It looks like someone created a `ServerMonad` instance for `ErrorT` but none of the other instances. I have patched your code here to include the `ErrorT` instances for now: https://gist.github.com/stepcut/5896403 But, I will fix the upstream later today or tomorrow.
So in a way, this would be a little bit similar to what I do with `pipes` where I specify something like: P.map :: (Monad m) =&gt; (a -&gt; b) -&gt; () -&gt; Pipe a b m r map id = pull map (g . f) = map f &gt;-&gt; map g The idea is that `map` reflects my intuition for how pipes should behave when they are being used as pure transformations and then I can work back from those functor laws to see if my actual implementation obeys that behavior correctly. Given enough of these behaviors I might be able to fully specify the underlying implementation.
Hmm, I really want to learn emacs but I don't know how.
Would be *SUPER* nice to automate the ghci :browse feature in haskell-mode. Like have a "C-b C-r" to browse a module when hovering over it. Any ideas on how hard it would be to do this?
It seems like you may have caught some errors in my logic. Every unexpected argument flip or lack thereof that you pointed out with your comments seems to me to be a valid complaint. I may have to revise my ideas on how to handle parentheses in ES-μ! Perhaps I need two (or maybe even three) different sets of parentheses, for use in the three different possible positions in a given infix application expression. I hope it doesn't come to that, but it may. :( Oh well, at least it works for the infix part, and it's not like ES-μ is going to be used for production code (God, I *hope* nobody uses it for production code!). :) EDIT: Thinking about it, I may just need `{` = `)` = `flip id` and `}` = `(` = `id`, because I don't see any other reasonable implementations for parentheses-like things. You would then use `{` and `}` for the infix part and `(` and `)` for the side parts. I'll have to see, though. It could fall through just like the last plan did. :/ EDIT 2: Corrected Markdown formatting EDIT 3: I only just now noticed your final (for now) words thanking me for spending time explaining it to you, to which I say, you're very welcome! It's a pleasure to talk to someone about things I'm enthusiastic about, and especially to someone who can double-check my work like you just did. :)
I'm gonna trust you on that tutorial (not that 100SEK is a lot of money). Thanks!
Let me know how it goes and if you have any questions. djohnson dot m at gmail. I don't know how I would have learned haskell without emacs. I have snippets for all the type class declarations I haven't memorized yet like (Arrow, Category, etc.) and little snippets for each import. So "cco" == "import Control.Concurrent." Little things that make you way more productive. Plus you can connect to #haskell irc in emacs with erc so you can ask stuff on the fly.
I'm not sure what you mean. It's very flexible, and captures role-based entitlements very well.
Actually feed fetcher is a separate program written completely in Haskell. Front-end does not do fetching. Ur/Web linked with Haskell is used only for web front-end and it pushes new subscription requests to "queue". The "queue" is just a few keys in Riak which fetcher polls regularly. After subscriptions written front-end polls each few seconds to see are they ready or not. This simplified design allows to restart fetcher at any time and don't bother with additional message-passing component in system. Probably in future I will use some kind of message queue, but not yet sure. Being a paid reader means that I only need to fetch feeds for customers and for free trial users. So I have less problems with scaling things (although there are many free trials right now) and can focus on features. And linking is not that complex. I wrote simple Haskell program where types and foreign functions are described. Then it generates type definitions for both Haskell &amp; Ur/Web, serialization, calls to Riak and FFI calls with type annotations, so I get compilation errors when FFI description mismatches Haskell function type. But my dream is ability to write Haskell code that runs in browser with HTML markup, JS events integration and easy JS FFI (just running JS is not enough, it should be easy to integrate it with DOM). Probably it won't be as correct as Ur/Web (won't check dead links, html wellformedness and some security stuff). But it can be a good start. And if generated JavaScript can somehow use asm.js and run really fast it would be fantastic.
You can hide feeds list by pressing 'f'. Moving from feed to feed by spacebar is also planned.
What editor/environment do you use now?
Mostly Sublime with a couple of plugins (like HaskellBuddy which is pretty nice). I also have a pretty good grip on vim, but it's not my preferred editor. Emacs just seems to have an incredibly steep (and long!) learning curve.
Yeah! That's how I interpret it. Again, I'll go ahead and highly recommend Conal's papers on FRP for examples of his philosophy in use. Section 4.6 and 9 of "Push/Pull" stuck out to me where he finally works up how "time" must operate in the domain of FRP's `Signal` values. He ends up using a stack of clever ADTs which slowly rebuild Time "as denoted", but such that infinite processes can be resolved piecemeal using "improving" lower bounds and eventually motivates Conal's `unamb` combinator.
If I haven't overlooked something, the way GHCi is used by `haskell-mode` GHCi should be able to support all features that `ghc-mod` does. It just needs to be implemented. So I'd guess, `ghc-mod` makes more sense being used w/o any REPL or in combination with the `inf-haskell` REPL.
That should be pretty easy actually...
...you could check out the side-bar on /r/emacs
Are the following of any use? * [boomerang](http://hackage.haskell.org/packages/archive/boomerang/1.3.3/doc/html/Text-Boomerang.html) * [zwaluw](http://hackage.haskell.org/packages/archive/Zwaluw/0.1/doc/html/Web-Zwaluw.html) * [invertible-syntax](http://hackage.haskell.org/package/invertible-syntax) * [nvertible Syntax Descriptions: Unifying Parsing and Pretty Printing](http://www.informatik.uni-marburg.de/~kos/papers/unparse.pdf) If I've missed what you were asking for, don't hesitate to call me stoopid.
You use GMail right? The benefits are the same: 1. Google sorts everything out for you. Your mail just works. Spam filtering just works. Searching through a gig of mail just works. 2. It's the same anywhere you go, any system you're on. 3. It's more stable than your own computer and whatever backups you're (probably not) doing, it will outlast your machine. The question is whether you're willing to accept the trade off for those benefits. The majority of people are.
One idea would be to make a dependently typed version of [bidirectional arrows](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.153.9383). However, this won't give you the "atomic" isomorphisms for free (though usually for quite cheap), and it does suffer from both structural issues (which are easy to fix), a rather dense notation (which may be fixable by a DSL), and only limited (tagged) pattern matching using the (invertible fragment of the) ArrowChoice combinators. A bonus in this regard, though, is that a lot of the original paper details the authors' ideas regarding generic programming with bidirectional arrows, and I'm quite sure that most (if not all) of these will carry over cleanly to a dependently typed setting. While this won't give one true isomorphism-for-free -- for that one would probably need to define a programming language fragment with inverse semantics à la [Janus](http://dl.acm.org/citation.cfm?id=1244404) -- it may allow the library designer to produce something akin to an invertible combinator calculus with (certified) invertible atomic isomorphisms (with proofs supplied by the library designer) and (certified) invertible combination via the arrow combinators. In this case, it may be possible to use this as a back-end for a more elegant DSL for certified invertible programming (which could possibly be done as an embedded DSL in Idris using the syntax features). Do shoot me a PM if you want a partner in this endeavor, by the way :)
I'm using gmail with mail client. Also I'm storing my code in a 'cloud' - github/bitbucket/hub.darks.net. Can I dump my emacs config/seesion/whatnot to some 'cloud' like ftp, google drive or even specialized service for haskell programmers? Yes, I can! (Though I don't do it, because I don't need this feature) Is it possible to create a closed source IDE tied to even more closed server with a normal gui, without all the browser nonsense? Yes it is!
I have tried what you typed and didn't get any errors
 a = 4 This defines `a` as 4. sum1 b = a + b This defines `sum1` as a function that takes one numeric argument, which is called `b` within that function; the function returns `a + b`, and since you've defined `a` to be `4`, it should be the same thing as sum1 b = 4 + b thanks to referential transparency. sum2 a b = a + b This defines `sum2` as a function that takes two numeric arguments, which are called `a` and `b` within that function; the function returns `a + b`. Within the function, `a` no longer refers to the `a` you defined as `4`, but really to anything you pass as the first argument to `sum2` (terminology: this redefinition `a` "shadows" the toplevel `a`). Through alpha-renaming, you could also have defined `sum2` as sum2 x y = x + y and it should behave exactly the same way. --- I do not understand why `sum1` gives an error in your case, but without the actual error message itself, it's only speculation. Can you give us the full error message? It might not look very useful to you, but to more advanced Haskell people, it should make the issue a lot clearer.
As @redditparislife said, it's perfectly working. Are you sure you haven't typed something as *sum 1 7* or *sum17*? Because, for me, *sum1 7* returns 11 as expected. Writing *a = 4* means that you're defining an element of type *Integer* with a constant value of four. Or, more properly, a constant function of type *Integer* of value for.
I don't get any errors. For the last line, I get, as expected: ghci&gt; sum1 7 11 But to answer your question, about what `a` is: it actually leads a double life here. Letting `a=4` means that any *unbound* occurrence of `a` expands to `4`. For example: ghci&gt; a 4 ghci&gt; a + 1 5 ghci&gt; a * 2 8 In the function definition `sum1 b = a + b`, `a` is unbound, hence expands to 4, so that `sum1 b = 4 + b` is an identical definition (in this context). However, in `sum2 a b = a + b`, `a` is bound. It's easier to see in lambda notation: sum2 = \a -&gt; \b -&gt; a + b Here, `\a` *binds* (the second occurrence of) `a`. Nothing gets expanded to `4` here. Rather, what happens is that when `sum2` is applied to a value like `2`, the `a` in `a + b` takes on the value `2` (by beta conversion): (\a -&gt; \b -&gt; a + b) 2 = \b -&gt; 2 + b Even in ghci, where `a=4`, you can still do things like ghci&gt; (\a &gt; a * 2) 6 12 So `a` leads a double life in the sense that, when unbound, it expands to `4`, and when bound, it's just like any other bound variable. By alpha conversion, we could rewrite `sum2` equivalently as: sum2' = \c -&gt; \b -&gt; c + b or sum2' c b = c + b In technical terms, `sum2` and `sum2'` are alphabetic equivalents. 
GLFW works really well with Haskell. And it is multi-platform (Windows/Linux/Mac). www.glfw.org cabal install GLFW 
That 'unbound' condition is important to understand. Well put.
I discovered an error in my sum1. I apologize (Even more embarrassing because I made sure to quadruple check before posting, and its only one line long D:) . Thanks for explaining the shadowing, that makes a lot of sense. Quick follow up question :). bmiTell :: (RealFloat a) =&gt; a -&gt; a -&gt; String bmiTell weight height | bmi &lt;= skinny = "You're underweight, you emo, you!" | bmi &lt;= normal = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= fat = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" where bmi = weight / height ^ 2 skinny = 18.5 normal = 25.0 fat = 30.0 Here we use the keyword "where" because we only want it to define it locally, right? If we defined them all globally previously there would be no problem, except a super waste of name space. Is that correct? My last question ( I promise!) Is that when I delete the where but leave everything else the same, I get *parse error on input `='* . I don't quite understand the problem, is it that I am trying to define something globally from within a local area? Edit: I lied, that wasn't very quick. Sorry. 
Embarrassingly enough, I made a mistake one sum1. I know! I feel like deleting my account out of shame. All of those functions could fit on one line :( At least the other questions were not caused by errors as well haha
&gt; Here we use the keyword "where" because we only want it to define it locally, right? Yes. It's comparable with `let ... in ...`. &gt; If we defined them all globally previously there would be no problem, except a super waste of name space. Is that correct? Mostly. You could move `skinny`, `normal`, and `fat` to any surrounding scope you'd like, and it'll work. It'll clutter that scope, and in some cases, it's highly undesirable that a (non-)function called `normal` is already defined. However, `bmi` depends on the values of `weight` and `height`, which are parameters to `bmiTell`. You can't move that to global scope without making big changes. &gt; My last questions ( I promise!) There will never be a last question ;) &gt; is that when I delete the where but leave everything else the same, I get parse error on input `=' . Whitespace matters in Haskell. Did you write it like this (which won't work, because Haskell doesn't understand what you're writing): bmiTell :: (RealFloat a) =&gt; a -&gt; a -&gt; String bmiTell weight height | bmi &lt;= skinny = "You're underweight, you emo, you!" | bmi &lt;= normal = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= fat = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" bmi = weight / height ^ 2 skinny = 18.5 normal = 25.0 fat = 30.0 or like this (which won't work, because `weight` and `height` aren't defined outside `bmiTell`): bmiTell :: (RealFloat a) =&gt; a -&gt; a -&gt; String bmiTell weight height | bmi &lt;= skinny = "You're underweight, you emo, you!" | bmi &lt;= normal = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= fat = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" bmi = weight / height ^ 2 skinny = 18.5 normal = 25.0 fat = 30.0 
You are correct about the `where` scoping. It's basically just a way to keep local definitions out of the global namespace and to make your high-level definitions cleaner. The parse error is because GHC doesn't know how to read ... | otherwise ... bmi = weight / height ^ 2 ... in roughly the same way that a C compiler wouldn't know how to read { puts "hello"; }
Thank you, I really appreciate your help. I feel much better about the logic of haskell now. 
Thanks!
Yeah I made a stupid mistake. Luckily my other questions where valid haha 
Relax, that's pretty common! Good that you've understood your mistake! :)
This is very similar to the [`CoupleT`](https://github.com/duairc/couple/blob/master/src/Control/Monad/Trans/Couple/Internal.hs) monad from my [`couple`](https://github.com/duairc/couple) package. My naming is a bit different; I had originally called this `Resource` just like you, but when I generalised it to be able to wrap around any monad (using the `MonadTry` class from my `layers` package to make it exception/"zero" safe) I figured that meant I should rename it to `ResourceT`, but this is something different to `ResourceT`. I chose the name `Couple` because it "couples" an open action with a close action. A `CoupleT` action can be run using the `with` operation, which is the same as your `runResource` action (basically `bracket`). The `couple` package does have an `acquire` operation, but it's considered unsafe and is instead called `unsafeDecouple`. The "safe" way to decouple a `CoupleT` action is to lift it into the `DecoupleT` monad, which is basically a version of `ResouceT` whose only operation is `decouple` (which "lifts" a `CoupleT` action into `DecoupleT` and returns the resource/release tuple).
Where can we see it?
Hey, everyone! I am reading Haskell School of Expression and would recomend that to anyone in the same boat as me; ie can program functionally but wants to use Haskell. The good thing is that this book focuses on cool, interesting programs. Appearantly Haskell Road to Logic and Mathematics is cool too. And for beginners there is Introduction to Functional Programming with Haskell. 
Is this aimed at web hosting or data processing (/both?). I'm very interested on the data processing side, where I want to be able to spin up lots of machines but potentially for very short periods of time (i.e charging by the second rather than hour), or have long running services that are charged by their actual usage levels. Does that sound like something you might offer? I'd be interested in the beta if so, but if it's web-oriented then I'm not really the beta tester you want.
I don't really have anything to add to your main question; however, I can show you a small self-contained example of modeling a linear type system in Agda: https://github.com/bobatkey/sorting-types/blob/master/agda/Linear.agda . It shows a sort function typed in a linear type system, and then using the properties of the type system to prove just from the type of the sort function that its result is indeed a permutation of its argument.
The general term for this stuff is "bidirectionalization". Janis Voigtländer (http://www.iai.uni-bonn.de/~jv/) does a lot of work in this area. As I recall, the "combining syntactic and semantic bidirectionalization" paper should cover a fair amound of ground: http://www.iai.uni-bonn.de/~jv/icfp10.pdf Also related is Benjamin Pierce's Boomerang project. The term "lens" itself comes from Pierce, although there was related, parallel, and prior work that used a variety of other terms: http://www.seas.upenn.edu/~harmony/ Pierce's website has some more recent papers on the topic than the boomerang stuff, although it seems like his research is now moving in other directions: http://www.cis.upenn.edu/~bcpierce/papers/index.shtml 
So they aren't objects. Yes, certainly not. That was a strained analogy, useful for some specific examples, but not for understanding the whole idea. But now they are neighborhoods in cellular automata? Until they aren't... This whole exercise sounds a lot like "a comonad is a co-burrito". A comonad, like a monad, is a mathematical abstraction that can apply to any concept that satisfies a certain set of laws. One can find examples of specific comonads, and understand them. One can gain some familiarity with reasoning about the structures and laws, and practice applying them to a lot of different examples. But like monads, trying to find the one true metaphor that explains them is doomed to fail, and which metaphors make the most sense to any given person says as much about that person as about the abstractions involved.
I added the missing `ErrorT` instances to `happstack-server` in darcs -- but I am not yet going to upload it to hackage. I have another big change coming up that will require modifying a bunch of the same .cabal files that will need to be modified for the new `happstack-server`, so I am going to do that all at once. I update the example so that the response is actually shown now: https://gist.github.com/stepcut/5896403 I just did three things: 1. added a `toResponse` in the line: dir "home" $ ok $ toResponse "Welcome home!" 2. commented out the line after so that the log message is not the last line 3. update the type sigs of `home` and `simpleServer` Instead of commenting out the last line of `home` I could have done: home :: MyApp Response home = do conf &lt;- ask let l = confLogName conf liftIO $ logM l INFO "Entering `home`" res &lt;- dir "home" $ ok $ toResponse "Welcome home!" liftIO $ logM l INFO "Leaving `home`" return res 
Objects are {co, bi,}algebras.
You misread the section of my post concerning `up'`. I said that the definition of `up'` was in fact not correct and then later show how to derive the correct version (i.e. `extend up`, which does not operate in reverse chronological order). However, this was easy to miss because it was in the exercise section.
Yeah, further discussions with people who commented on this metaphor suggested that perhaps fluent programming was a more appropriate term for the specific examples that I introduced.
Comonads are just monoids in the opposite of the endofunctor category, duh!
Or just algebras. It depends on what you mean by 'object'.
I for one appreciate your pedantry. However, our definitions are equivalent; a comonoid is a monoid in the opposite category.
Dan, could you explain a little bit? Like is a lens (coalg of the store comonad) an example of an "object"? What's an example of a bialgebra in Haskell?
Well, as a counter, PiCloud charge by the millisecond (http://www.picloud.com/pricing/) and have an overhead measured in seconds. They're currently my go-to solution for a lot of things, although some of their more advanced features are focused on Python. I was hoping some other services would appear in the same market, particularly with functional languages that promise impressive scaling. They won't necessarily run all your work in parallel, of course, because running 1000 5 second jobs on 1000 machines would be insane. However it'll find spare capacity to run a lot of your work, and I may have 10-50 processes running at the same time but end up paying for less than 1 hour of compute time. &gt; and aws can spin up more instances and more types of instances (which you really care about if you are doing data processing) and more types of domain-specific integrated services (emr/map-reduce, redshift, dynamodb, etc) which probably is even more valuable than anything else Perhaps. I'm more interested in exploring data, so being able to run a 30 minute job in 1 minute means a huge gain in productivity for me. Given amazons per-hour pricing, most of my work would be prohibitively expensive. GCE is interesting, but as it's focused on web work it doesn't have the same scaling abilities (at least, it involves a lot more work on my side to spread the work out). I might have a look at it again though and see how it's changed. 
[My coalgebra interpretation makes a lot of sense IMO.](http://www.reddit.com/r/haskell/comments/1gtb60/comonads_and_objects_revisited/)
Amazon have some great products. We use them for a few services at the moment and have had no problems at all but I think that they can be beaten when it comes to EC2. Unfortunately I can't get into details at the moment but I think that some might be pleasantly surprised with what can be achieved.
Comonads are inside out spacesuits filled with apples duh
What kind of {co,bi,}algebras? Comonad?
It depends on what you want! You can talk about algebras and coalgebras of plain old functors. Those are basically the same thing as the (co)algebras of the (co)free (co)monads generated by those functors.
I have some personal experiments in reactive programming that rely on such coalgebras that I've been meaning to put out in public but I'm about &lt;--------this--------&gt; busy.
Would you agree that it makes more sense (at least when looking at your other examples) to use `Store Celsius` than `Store Kelvin` for thermometers?
Coalgebras for which comonad? A `Behavior`-like one?
Something like Traced t for timed behavior, then you throw in any old functor for reactivity and mix it together. One of my rough experiments is on my github page labeled `fluorine`. I really don't want to try to explain that mess though :)
I'm still digesting your last post, but I think you were on the right track and you were probably right. Give me some more time to think about it.
Here's another thing: some people describe objects as elements of greatest fixed points of functors. That is the same as a greatest fixed points of cofree comonads. Thus, I'm thinking that information hiding is using greatest fixed points instead of coalgebras. In other words: implementations : interfaces : classes :: coalgebras : comonads : greatest fixed points Note (you probably already have) that the difference between comonad coalgebras and functor coalgebras is whether or not you have laws for how the 'object' must work.
Yes. If you think of comonads as interfaces, you can think of coalgebras as implementations of interfaces. `Store` is then the interface of a field. A `Lens` is a coalgebra of `Store`, or, in this metaphor, an *implementation* of fields.
What about an example of an "object" that is an algebra for a monad?
I'm with you so far except on the part about greatest fixed points. Could you explain that line of thinking a bit more?
Well, let's pretend that Hask has all greatest fixed points of comonads (all terminal objects in all comonad Eilenberg-Moore categories). In that case, a standalone type that contains exactly the coalgebras for a given comonad W is the greatest fixed point of W (the terminal object in the Eilenberg-Moore category of W). If we see W as an interface, nu W (the greatest fixed point of W) could be considered a sort of OO-class (if classes are defined to be the unit of information hiding). Whenever we have a comonad coalgebra `a -&gt; W a`, we can inject `a` into nu W in a way that preserves the structure. This prevents us from accessing the original value in any other way than the comonad, which, in a sense, is information hiding.
^ this. I would love to see people adapt vector anywhere where we are not using FFI.
Thanks! I think I've figured out how to use it now, and now I just just the easy part of implementing my server. I've still got a lot of work to do before I get good, but this definitely helped to point me on the right path.
So if I understand your analogy correctly, the `w` would be the uninstantiated class, and when you wrap a value of type `a`, such as in `w a`, then that is like an instance of that class. What you're saying is that you can reify `w` as a value without "instantiating" it if you just use the greatest fixed point so that it keeps wrapping itself without bottoming out on a specific concrete `a`. This ensures that the only operations you can use are those on `w`, forcing you to program abstractly over the class and not use any properties of a specific instance, (i.e. a specific `a` that `w` wraps).
See for example the free monad stuff that gets posted regularly. An algebra of a free monad is a way to 'run' the monadic value to get a result.
Can you show any example of OO that has an object that corresponds to an F-algebra?
So if I were to use Stream comonad as the interface, then something like `repeat :: a -&gt; Stream a` would be the coalgebra. Can you give an example of your information hiding in the context of `w = Stream` (or another comonad if you prefer; I just picked one I thought might be simple).
I've heard them described as "data in context".
Any type where you have some method that takes multiple values of that type and creates a new object.
Wait, when you said greatest fixed point I thought you meant of the original comonad. So what happens if there is no cofree comonad equivalent to the given comonad?
Well, then you use the gfp of the comonad. However (unless I missed something) the gfp of a cofree comonad is the same as the gfp of its functor.
That's impossible in Java-style OO.
Sounds like Java-style OO sucks.
I thinkg that is just a name for things present in Haskell, like map, filter, fold, etc. Examples: http://jesseliberty.com/2011/01/27/linq-and-fluent-programming/ https://www.sellsbrothers.com/posts/Details/12692
A very very good book. Haskell books seem to be written by very smart people. No accident.
Stop objectifying monads!
The team of four devs I work with are building such a system, a Yesod webapp and a data collection daemon which inserts data into the database shared with the webapp. The purity and abstraction are great! Using Persistent for the database layer and defining the database schema in Haskell using the Persistent DSL. We're also using the Esqueleto DSL for writing SQL queries. The daemon runs on multiple cores and uses an STM TQueue for a work queue. Haskell and Yesod have been a pleasure to work with. The only pain points have been the API changes with new versions of Yesod and Persistent. Thats a small downside compared to all the positives. 
Regardless though, analogies are extremely useful for understanding what is going on at a more fundamental level, so I think that finding analogies with more familiar programming concepts is a useful exercise. Even if they are not perfect, if the differences are well explained, they can be an extremely good pedagogical tool.
Imagine the entire universe was burrito filling. Comonads are the pockets of non-burrito space lined with a tortilla wrap that you might find yourself living in.
Possibly the most hipster hackerish thing I've seen all week. I like it!
Are folks using the indentation? I gave up on the indentation and just use M-i and I have my Tab key hooked up to indent-relative. Too often with the built-in indentations I would hit tab repeatedly and it still wouldn't get the right indentation, especially of comments with Haddock markup. I have wondered if it could do a better job if it allowed/required you to specify a style for the code, just as C mode requires you to pick a style (like K&amp;R, for instance.) Short of that, even a file showing the recommended style to use would be helpful.
Cool! If you would be able to post some small example of a bit of domain logic that's not too secret, that would be fun to see. I'd post some of my Java stuff but I don't want to ugly up the subreddit. :)
So, obvious question, anybody here who has received their invite yet? Thoughts?
Since Mesa only supports up to OpenGL 3.1, setting `displayOptions_openGLVersion` to `(3, 2)` causes GLFW to fail on my machine. (So much for it being a 'hint'.) You might want to add a quick test to ensure `True &lt;- openWindow opts` holds, otherwise it will explode quite horribly. Anyway, the examples work fine with OpenGL 3.1 and GLSL 1.3 (after tweaking the shader versions, of course). Very neat! 
I am missing something. In what sense is this postmodern? Does “postmodern” have a specialised meaning w.r.t. Haskell of which I am unaware?
I'm honored to see such cool things being done with Vinyl, thanks Anthony!
Haskell code is generally not indented, it is aligned to arbitrary columns.
I was building on two articles: "Modern OpenGL in Haskell" and "Vinyl: Modern Records for Haskell". I quite simply ran out of modern.
Thanks for working that out!
The difference is?
I don't have a clue about relational algebra nor HaskellDB, but I was thinking about investing some time learning about it. Can you point out what you thought was worth improving in the HaskellDB API so that I can keep that in mind when learning?
&gt; if the differences are well explained This is the main problem. The whole purpose of teaching by analogy is to avoid such allegedly confusing pedantry. As soon as you have explained the differences sufficiently to prevent the learner from drawing incorrect conclusions out of the analogy, you have lost all the supposed benefits of the analogy in the first place.
HaskellDB is essentially an excellent relational database library. The major flaw is that the `Query` datatype it uses to represent queries is given a `Monad` instance. This leads to all sorts of undesirable behaviour, particularly in relation to aggregation. See 2.6.3 of http://etheses.bham.ac.uk/1186/1/Yao10PhD.pdf for an example. In my API I replaced the `Query` `Monad` with a `QueryArr` `Arrow` which makes much more sense. The drawback from a usage point of view is that Haskell's Arrow notation is non-standard and somewhat more cumbersome than the nice `do` notation. In addition, personally I find the record syntax very limiting and I think it gets in the way of abstraction and composability, so my EDSL eschews that. When Haskell itself has a record system it can easily be layered on my API to provide a record interface to HaskellDB that is hopefully more in-tune with the rest of the language. The HaskellDB codebase is nicely written and it was fairly straightforward (though non-trival) to layer my EDSL on top of it, and that saved me a lot of time. I know the codebase fairly well now, so if you have HaskellDB questions feel free to PM me.
It isn't. class ABC { public int a, b, c; //easy constructor public ABC(that ABC, andthat ABC){ return new ABC(this.a, that.b, andthat.c); } } I think that's what you asked for. It can be made polymorphic in the number of arguments, too, but that requires more boilerplate.
Great stuff! But what about the terms of service? I would like to import a project from my github account, but what kind of rights does that imply for FPComplete? I can't find anything in the TOS except "By posting content viewable by others, you grant us a non-exclusive, worldwide, transferable, sub-licensable, royalty-free license to use it." which doesn't look too comfortable to me. In comparison, [Github's terms concerning copyright][1] are &gt; We claim no intellectual property rights over the material you provide to the Service. Your profile and materials uploaded remain yours. However, by setting your pages to be viewed publicly, you agree to allow others to view your Content. By setting your repositories to be viewed publicly, you agree to allow others to view and fork your repositories. which I find clear and comfortable. [1]: https://help.github.com/articles/github-terms-of-service#f-copyright-and-content-ownership [Bitbucket][2] is a bit less clear, but writes &gt; Unless otherwise specified, End User retains ownership of any data or other content or information that End User provides through the Hosted Services, including any code uploaded to Bitbucket (as described below) ("End User Data") &gt; [...] &gt; End User hereby grants Atlassian a non-exclusive license to copy, distribute, perform, display, store, modify, and otherwise use End User Data in connection with operating the Hosted Services. [2]: http://www.atlassian.com/end-user-agreement/
I think the underlying functor would be considered the 'interface', and having that be an interface was what I meant was impossible.
No Applicative parsing?
Just for the sake of discussion, IIRC, parsing an IP address in the formats supported by inet_aton is fairly involved.
 data IP = IP Byte Byte Byte Byte ip :: Parser IP ip = IP &lt;$&gt; parseByte &lt;* symbol "." &lt;*&gt; parseByte &lt;* symbol "." &lt;*&gt; parseByte &lt;* symbol "." &lt;*&gt; parseByte then it's just toInt :: IP -&gt; Int toInt (IP a b c d) = a .|. b `shiftL` 8 .|. c `shiftL` 16 .|. d `shiftL` 24 not sure if fully correct. typing this on phone. but you get the idea. 
 octet :: Parser Word8 octet = do x &lt;- decimal :: Parser Integer guard (0 &lt;= x &amp;&amp; x &lt; 256) return (fromIntegral x) parseIP :: Parser IP parseIP = (\d1 _ d2 _ d3 _ d4 -&gt; IP d1 d2 d3 d4) &lt;$&gt; octet &lt;*&gt; char '.' &lt;*&gt; octet &lt;*&gt; char '.' &lt;*&gt; octet &lt;*&gt; char '.' &lt;*&gt; octet Doesn't look too unclear to me.
Thank you! I'll keep your comments in mind.
Very true, although that might be less clear for beginners.
&gt; [..]when a parsing error arises, parsec gives you a lot more information than attoparsec. &gt; The lack of these features in attoparsec is precisely what makes it faster. Why? Is there some fundamental restriction in performance when dealing with things like errors (which should very uncommon)? Does this relate to low-level concepts such as branch prediction? is ghc unable to optimize?
Some people might also enjoy an article I've submitted a while ago, but on Parsec this time: https://www.fpcomplete.com/user/adinapoli/the-pragmatic-haskeller/episode-5-a-simple-dsl
Well, I mean stuff like 97.0x1e.0376 is acceptable to inet_aton.
You put location information in your tokens. 
I'd avoid the first solution as it's not lazy enough (`reverse` + tail-recursion in list algorithms is an anti-pattern in Haskell) `groupsOf''` is probably easiest for fusion algorithms to optimize. Although I think `groupsOf'` is the most elegant.
Me neither. But then, I understand guards, inline type annotations, and applicative syntax pretty well since I've used them for a while. Two years ago, you'd have lost me entirely... and I would have been the target audience for that article.
AFAIK, The build function is there to provide the compiler with the ability to perform eta-conversion (i.e. turning `\x -&gt; abs x` into just `abs`) If you you have a utility function that builds a bunch of lists using cons (:) and then passes them back out, and then the next function you pass it to just pulls the list back apart, the compiler can't do anything to optimise that. It is *forced* to pack those values into cons cells, and then unpack those values in the next function. However, if all you're passing back is a function that when called will perform a cons, then GHC is smart enough to get rid of the intermediate step entirely, and "fuse" the two operations together.
I've always used groupsOf n = takeWhile (not . null) . map (take n) . iterate (drop n) 
So if I'm reading the source comments correctly, Data.List.Split writes chunksOf in a way that only makes sense to make list fusion work, and then disables it by writing their own version of build, in order to keep the module portable Haskell 2010.
Why do you need the existentially quantified type here?
Are you talking about forall x. ? It has to be there to bring x into scope (I think). How would you do it?
attoparsec-conduit provides [conduitParser](http://hackage.haskell.org/packages/archive/attoparsec-conduit/1.0.1/doc/html/Data-Conduit-Attoparsec.html#v:conduitParser), which tokenizes together with the position range of the token. I use that in xml-conduit to provide positioning for error messages where the tokens are properly formed but create an invalid XML document (e.g., `&lt;foo&gt;&lt;/bar&gt;`).
a &lt;- return b isn't generally a great idea in do-notation, it's better to use let a = b.
Like Tekmo's groupsOf: chunks _ [] = [] chunks k xs = x : chunks k xx where (x,xx) = splitAt k xs Same as groupsOf': chunks' n = unfoldr chunk where chunk [] = Nothing chunk xs = Just (splitAt n xs) 
As I understand it, It doesn't matter that the definitions of build are different, GHC's simplifier will remove the build step alltogether. It would be nice if GHC was smart enough to optimise away constructors in the same way that it optimises away functions, but until that time, the small inconvenience of defining your function in terms of build once, in a library, is more than paid for in speed of execution in all the programs that use it.
&gt; The whole purpose of teaching by analogy is to avoid such allegedly confusing pedantry. I disagree. The point of teaching by analogy is to cover a large portion of the material by making reference to concepts already understood by the audience and leveraging that knowledge to further their understanding of a new concept. It allows to not start from scratch for every new concept being taught. It is precisely the differences that need to be explained, but that is a big part of what "teaching" is.
I daresay you do not actually disagree with me on what an analogy is. Where we disagree is on the difficulty of explaining the differences between analogies and the things you are actually trying to teach. Especially in the world of computer science, analogies are typically [very far from the mark](http://www.reddit.com/r/programming/comments/1cobpl/functors_applicatives_and_monads_in_pictures/c9imzut), and it would actually be easier to just explain the damn thing than to hint about it and then explain why it was a lie.
I find it hard to garner useful meaning from that title.
Already posted: http://www.reddit.com/r/haskell/comments/1h24rv/hpcwire_lustre_founder_spots_haskell_on_hpc/
I also want to point out that you can even make these top level declarations since they don't depend on any variables freshly bound within the `do` block: main = do -- There exists 2 numbers in the domain of {0..4} that are greater than 2. print $ askQ (EX 2 [1..4] id $ P (&gt;2)) [0..4] -- No letter exists that is a blank space print $ askQ (UQ abc id $ P (/= ' ')) abc -- For each class there exists 2 students that have enrolled in that class. print $ askQ (UQ classes fst $ EX 2 students snd $ P (\e -&gt; e `elem` enrollment)) [(c,s) | c &lt;- classes, s &lt;- students] abc :: [Char] abc = ['a'..'z'] classes :: [String] classes = ["Math","English","Science","Art"] students :: [String] students = ["Tom","Bill","Sarah","Rose"] enrollment :: [(String, String)] enrollment = [ ("Math","Sarah") , ("Math","Bill") , ("English","Bill") , ("English","Rose") , ("Science","Rose") , ("Science","Tom") , ("Art","Tom") , ("Art","Sarah") ] By doing so you make it clear that these values are known at compile-time and you distil out the run-time behavior into `main`.
Cheers, good to know. That's quite sweet.
&gt; Profiling, in GHC, adds annotations to a program, so that information can be collected at runtime to determine its space and time behaviour. This invariably makes programs slower, so profiling is only used as a debugging tool. Except Doaitse found a program which ran faster (nearly twice as fast) with profiling. http://hackage.haskell.org/trac/ghc/ticket/5505
If you're on the phone, the redundant brackets seem even more curious :)
I use the `a &lt;- return b` form for two very specific reasons * The binding is monomorphic (although recent GHCs make let monomorphic if you enable extensions) * You can pattern match existentials (and GADTs) using do bindings but not let. Thus let Refl = proofThatAEqualsB is prohibited while Refl &lt;- proofThatAEqualsB works
It would have to be cross-platform... Sometimes we embed C libraries in the packages themselves, often in a directory called "cbits". But llvm being C++ makes things pretty hard.
Not about Haskell per se, but about the use of math in programming, and mentions the Lisp culture.
Some potential leads: * [Shake](http://community.haskell.org/~ndm/shake/) - "make as a DSL in haskell". * [run cabal from cmake](https://bitbucket.org/arrowdodger/cmake-findcabal) (I think) - ~~llvm can be built with~~ cmake [can generate make files for llvm](http://llvm.org/docs/CMake.html)... Still no appealing solution in sight...
The greatest problem with people's perception of math, is that they think arithmetic is math.
I strongly suspect your time would be better spent porting hydra to use the new llvm general bindings on hackage. Llvm/llvm-base packages have a finicky build system by comparison and as of the past few weeks, substantially less dev love than llvm-general. Likewise llvm the c++ lib is not something you should yak shave on doing a custom install. Use the great work of others. 
Also c bits is usually a very small amount of code, llvm is a massive lib that takes a while to build 
This isn't really on topic, but my problem with closed solutions to integer recursions is that they almost always pass through the real domain, and that almost always means using floats, and then you have the problem that proving correctness rests on delicate error bounds and properties of the implementations of things like `exp`, instead of the comparatively familiar methods for things that involve only integers.
Programmers who don't take the time to properly model their problems will end up making a mess. The best way to properly model problems is to understand the math. That said, programmers spend relatively little time tackling problems that are hard to model. More time is spent fighting around inadequate interfaces and integrating all sorts of junk that has conflicting philosophies. So, it's easy to understand how math can get left behind when so much time is taken up by cleaning those messes. However, math is the major force that helps prevent those messes in the first place.
Hey now, arithmetic is quite deep. The biggest problem is that people think that calculation is math. I'm a math major and I don't even know how to use a calculator anymore.
I don't think the author's description of mathematics is accurate. The Lisp mentality that you don't need mathematics is certainly misguided, but mathematics is more than just applied analysis. Abstract and pure mathematics have the greatest potential to lead to solid abstractions and powerful programming idioms, and that's something that a lot of Lisp people and certainly this author seems to have completely missed.
'Doaitse Swierstra' sounds exceedingly Frisian to me, to be pedantic.
Besides, factorials grow *fast*. If you have a real application, odds are there are very few factorial values you need. If you have 32 bit integers, for example, there are 13 factorials before you get overflow. For 64 bit integers there are 21. Fibonacci numbers aren't that much worse - 47 before 32-bit numbers overflow, 93 for 64-bit. This rapid growth is of course very common for recursive functions on integers. Fibonacci numbers are pretty slow as these things go - the [closed form](https://en.wikipedia.org/wiki/Fibonacci_number#Closed-form_expression) has powers of quite small numbers. So - lookup tables are an option. And O(n) with n this small may be faster than waiting for that memory access - maybe even if it's already in cache. Especially as your loop will only need a few registers, and should be branch-prediction friendly. I can't guarantee that this will be faster than working with floats, but I wouldn't bet against it - and even if it isn't, as you say, it's probably not worth the float hassle factor. **EDIT** - I should also have said that if you need more than 64 bit integers, you're in big-integer territory. Even extended precision floats only have 64 bit significands - you can probably get some larger exact results because the least significant bits are zero anyway, but the limit won't be that high. As for prompting us to ask questions - that's what math is for, not programming. We sometimes forget it when we talk about source code being for people to read, but that's mostly maintainers. The purpose of software is to do the job that users need it to do. 
To be exact, it's about as Frysian as Benedict Timothy Carlton Cumberbatch is British.
My understanding is that build fusion is *not* built into ghc's simplifier, but is achieved by explicit rewriting rules in the [GHC.Base](http://lambda.haskell.org/platform/doc/current/ghc-doc/libraries/base-4.6.0.1/src/GHC-Base.html) module, such as the following one: "fold/build" forall k z (g::forall b. (a-&gt;b-&gt;b) -&gt; b -&gt; b) . foldr k z (build g) = g k z This rule will only trigger for the build which is defined and in scope in that module.
I wouldn't use a data type at all, see DR6's answer.
EDITED Gist with DR6 Solution. Your way of doing it is very clever and more Haskelly, but I don't think it's easier to use and you've moved very far away from the math's syntax. With some helper functions, it's very easy to write in a clean syntax that's very close to math's. ask2 UQ classes (EX 2) students (`elem` enrollment) ask3 (EX 3) schools UQ classes (EX 2) students (`elem` schoolEnrollment)
True, but I'm considering using this as a personal library, so the main function is very temporary and for testing only.
Fwiw -- I once compared the O(n)-exact and O(1)-double implementations, and (for the rather limited example of) Fibonacci, the O(1) solution rounded to a `long` happens to give exactly the correct answer until the `long` overflows. (I compared implementations because it'd be much more time consuming to *prove* when the closed-form solution starts diverging from the true solution.)
&gt; I find the preceding [closed-form] solutions to be more beautiful than their recursive counterparts. While I agree with the advantages the author mentions, the one big gap is "how on earth do you know that closed-form solution matches the original problem?" You can ignore the math only if you have a mathematician who's solved it for you. If you come across a *variant* of such a problem in your work, then it really helps to understand going from: - recursive program [code mirrors the recursive definition] to - memoized-program [store all previously-solved inputs/outputs -- automatic] to - dynamic programming [only store 1/n or less of all previously-solved inputs/outputs -- requires cleverness specific to the problem] to - a closed-form solution [usually done only for specific classes of recursive formulations]. Of course, if I ever reach that last step personally, I'm back to just trusting work done by mathematicians, if I can find such results at all. So the author's point that "different programmers want/need to live at different points along the math/brute-force tradeoff" still holds, I guess. While Fibonacci gives a very nice story and progression of solutions, many real-world problems fall to brute-force, and if not then memoization should be in every programmer's toolkit, and after that you give up or invest in putting some very smart (and math-trained) people on the problem. 
No, but I totally want them to prompt these questions during design and prototype stages. I do that frequently, personally—"this feels like an interface that ought to be isolated", "what laws should this uphold?", "is this effect applicative?". But we're talking past one another. If you just want to deliver software and you already know exactly what's needed and how best to do it then you're doing less mathematics and more carpentry.
You can ask and answer questions in the English language. Engineers do it all the time. Mathematics is a useful tool, but you don't need to derive nontrivial new proofs to do it. [**EDIT** meaning ask and answer those questions]. For example "what laws should this uphold?" is a question about the application and practicalities. You don't choose the laws to fit the abstraction - you choose the abstraction to fit the laws implied by the application. Math may be important to determine the laws, but you'll be following standard known procedures that you can look up as and when you need them. In any case, you don't prompt questions by writing different code. The which-design answers are expressed in code, not the questions. You might mention alternatives in comments, but even that's probably clutter. I've never claimed to be a mathematician. I use mathematics when I need it, sometimes without understanding it, just like someone who looks up the closed form of the Fibonacci sequence on Wikipedia and translates it directly to code. OP claims every programmer should be a mathematician. IMO OP is wrong. If a new part of mathematics suddenly becomes relevant to programming, I'll learn what I need about that. I've been looking at category theory a little for that reason, only what I've found is that I *don't* need it. I need to understand functors, monads and the like which came from category theory, but to a programmer they're just weird names for idioms and design patterns. Of course category theory seems to be mostly definitions anyway, a lot like set theory as at least one tutorial points out, but being familiar with the meanings of "union" and "intersection" doesn't make me a set theorist. Don't get me wrong - mathematics often defines its own abstractions rather than looking to metaphors, and those abstractions are often useful. Programmers do that too sometimes. Even so, that's just using existing math as and when it's needed. Some familiarity is useful, but OPs quotes from Paul Graham and Steve Yegg - two of the quotes he's arguing against - tell you that anyway. Your "carpentry" point is significant, but not the way you suggest. Both carpenters and sculptors work with wood, but doing very different jobs. There is some art in carpentry, but end-users get to appreciate that art. There is some art in programming too, but the end-users don't see the code - the art with the wide audience is the user interface, and that's psychology not mathematics. 
No, you can derive an exact integer solution using `O(log n)` matrix multiplications (of integer matrices, so all arithmetic is exact). This is the same time complexity as real exponentiation by doubling once you exceed the limit of machine precision for the standard square root function.
DR6's solution is, IMO, much cleaner for matching the syntax of maths as well. In math we write things like: forall a:A. exists b:B. R a b which translates to: forall' A$\a-&gt; exists' 1 B$\b-&gt; R a b The main difference is using "A$\a-&gt;" instead of "a:A." or "a∈A." One big usability problem with the datatypes approach is that it doesn't give a good clean way for the quantifiers to bind variables. Also, it creates this artificial problem of needing `ask2`, `ask3`,... in order to deal with different arities.
You know what, you (and others) are absolutely right. I haven't been approaching this problem correctly. (No sarcasm btw)
&gt; Of course category theory seems to be mostly definitions anyway, Not really. That's only in the beginning when laying the groundwork for doing CT. The same could be said for abstract algebra, or most other areas of mathematics. For example, in order to have algebraic objects to talk about (e.g., rings, monoids,...) we need to define them; but once we've done that, the "real mathematics" is when we try to figure out, e.g., what the ideals/ultrafilters are for some particular object. The same idea holds in CT. Just defining functors, natural transformations, monads, etc, that's the groundwork. The "real mathematics" is figuring out when particular functors form an adjunction, or what the Kan extensions of a particular functor are, or when we have a fibration over some category, etc. Of course, a great deal of this work requires looking at particular categories (e.g., Set, Vect, CPO,...) and the CT is used mainly as a unifying terminology so that mathematicians working in different areas can communicate easily. Of course, CT also embodies a particular view of how mathematics should be done, a view distinct from the one offered by set theory or other foundational systems.
Apparently the mathematical mind Evan possesses is no guard against the shallow stereotyping mind. ESR, Paul Graham, Steve Yegge--but does the implied connotation change if we continue to draw other names from the Lisp world? Peter Norvig, Guy Steele, G J Sussman? More hackers ignorant of applied mathematics? Why do I even have to bring up three counter-examples to remind you all that such broad stereotypes are *never* correct? And then, a completely fictitious history pulled out of the air of some isolated but ignored Fortran school of programming tucked away in more "technically inclined" parts of government like "NASA, Defense, etc"--never mind the actual programming language choices NASA and DARPA made in the 80's and 90's. This isn't serious writing. It's nothing more than the same childish antics of Us vs Them along with the myths needed to create distinctions where they don't exist. In attempting to counter "myopic" views, he's done nothing more than to show his own complete blindness.
the llvm-general lib has a different major version release for each major version of LLVM itself, that should solve your problems. 
And from the other side, the biggest problem with mathematicians is that they forgot how important constructive calculations are. Seriously, who the hell came up with the good idea of basing everything on a non-initial-object (viz. sets) which require quotients and a very subtle definition of equality to work? And then witness the appalling lack of concern about non-computability in the field of real? Thankfully the need for applied maths is finally pushing back enough that "real" mathematicians are thinking about these problems now.
&gt; That's only in the beginning when laying the groundwork for doing CT. OK. But doesn't that support my point anyway? Knowing what functors, natural transformations and monads are doesn't make me a category theorist. It wouldn't even if I really knew them, which isn't even quite true - having read some tutorials doesn't mean I remember everything. And still wouldn't even if I understood every word in your comment. I can program effectively, even in Haskell, without being a category theorist. I will be using mathematics, but I won't need to be a mathematician in the way OP suggests. 
I don't get it. What's deep about arithmetic?
I'm not sure what you mean by manually, but yes, there's a little extra work to generate and propagate the locations.
LLVM takes a long, long time to compile. That might influence your decision.
Hey this would be my FP teacher next year! cool. 
And then extra work to get those locations into error messages. What I'm hoping for is something that carries over location information automatically, generates useful error messages automatically, just as I get when I use parser combinators directly on a file, but on the tokenization pass result or such.
By arithmetic GP may mean number theory, things like FLT, GC, etc. 
To my mind, the most important reason that a programmer should learn more mathematics is that there are many interesting programming roles that are completely closed off to you without the requisite level of mathematics! Just off the top of my head I can think of roles in data analytics, scientific programming, academia, finance, engineering, medicine, graphics and gaming.
amicorum, not amoricum. Book of friends.
I attended his course on Advanced FP a couple of years ago, and I really learnt a lot from him :)
Thanks. I fixed it.
Consider using [ThreadScope](http://www.haskell.org/haskellwiki/ThreadScope) to find the problem. It's certainly possible that someone here can just glance at that and tell you the problem, but as program size increases past the utterly trivial, it rapidly becomes difficult for even experts in a language to be absolutely correct about a performance profiling issue without using profiling tools. Note I didn't say "Haskell" there; it has nothing to do with Haskell, this is a general problem. I've been programming in Perl for 15 years now and when I have a performance problem, I still reach straight for the profiler, because it's just so darned easy to overlook that _one thing_ that turns out to be the problem. (And spend hours optimizing the code that turns out to run in 10ms in your several-second process.... excuse me, make that "optimizing", with scare quotes.)
You can grab the [threadscope](http://hackage.haskell.org/package/threadscope) tool off hackage and use it to determine what parallelism is already occurring. The thing about Haskell is that the compiler can often optimize computation across multiple threads for you without you even needing the Control.Parallel library. You should remove the parallel strategies, compile without the `-threaded` flag, run it with `+RTS -N4 -RTS`, and view the results with threadscope. Then you will have more information needed to make a decision about where you might be able to improve the parallel computation, rather than just guessing at where you think it should be.
I don't think your program does what you think it does. In `main` you build a list of four IO actions, evaluate them in parallel (which does nothing), and then only run the last one, which is a long-ish serial computation. You might want to replace `map` with some kind of concurrent `mapM_`, or get rid of the `bench` stuff.
I wound up having to 'cbits' all of MPFR, because it had to link dynamically with the copy of GMP that was already used by GHC's runtime system. You can actually embed sub-packages fairly readily if you don't want to worry about linking with the copy already on the machine by using an autoconf build and then setting up the copy of the other package as a sub-package. There are some tricky bits with getting the resulting library to link though and I haven't actually worked through all the details on all platforms.
I hope people don't forget the fact that computer science is started as a branch of mathematics, and predates the invention of the first computer. It is math by itself, and it has adopted a wide range of concepts from both applied math and pure math.
GHC will only ever use one core unless you have explicit parallelism. There's no optimising computation across multiple threads done implicitly in GHC.
This is all true, but the program above is _evaluating_ IO actions in parallel, when the author presumably wants to _execute_ IO actions in parallel. This is clearly resulting from a misunderstanding of haskell semantics, and is not a performance tuning issue.
If you compile with the threaded flag and give it more than 1 core, threadscope will show multiple cores used. I've had problems where the implicit parallelism was better than the explicit, and I got better performance with less code.
You don't have to use nested maybes. Why not just define your own type: data Status = Continue | Draw | Victory Piece
If you have `type GameResult = Maybe Piece` then `gameOver :: Board -&gt; Maybe GameResult` is not so unsightly. Or you could define `data GameResult = Draw | WinFor Piece` --- isomorphic to the above, but more explicit about what it means.
Another paper describing similar ideas, but without the Free monad: http://people.cs.missouri.edu/~harrisonwl/drafts/CheapThreads.pdf (Harrison, JFP 2004)
I would say that, if you don't like `Maybe (Maybe Piece)`, you could use `Either Bool Piece`(in the article's style, it's equivalent because the first is 1+1+2=4, while the second is 2+2=4): it would mean that either one of the two wins, in which case the winner is represented, or no one does, in which case it represents whether the game has ended or not. I don't see a problem with the first one, though: it actually models it more correctly IMO, because it it's `Nothing` it means the game continues, while if it's `Just something`, the game ended and the `something` tells you how. With your first one, you get always your trash parameter, while mine doesn't play so beautifully with pattern matching. Let's say we want to print which one won: printer :: Either Bool Piece -&gt; IO () printer s = putStrLn (interpretation s) where interpretation (Left False) = "Game isn't over" interpretation (Left True) = "Draw" interpretation (Right p) = show p Let's do this with yours ... interpretation Nothing = "Game isn't over" interpretation (Just p) = maybe "No one" id . fmap show $ p
Yeah, that sort of explicitness is what I felt like I was losing by nesting `Maybe`. I like your `data GameResult` a bit more than your `type GameResult`, for a similar reason.
The first idea is a cuter way to use `Either` than the example I gave above -- thanks for that! I agree that in some sense `Maybe (Maybe Piece)` is a more natural representation; I just kind of dislike the look of it, and I don't think I've much seen it in others' code either.
You're welcome!
Rather than explicitly recursing as you do in: askQ q xs = case q of -- ... UQ dom get q -&gt; case dom of [] -&gt; True (d:ds) -&gt; if askQ q $ filter ((==) d . get) xs then askQ (UQ ds get q) xs else False How about: askQ q xs = case q of -- ... UQ dom get q -&gt; all (\d -&gt; askQ q $ filter ((==) d . get) xs) dom Also, pet peeve, if you're going to have them be `a` and `x` in the type, why not refer to them as the same in the definition? askQ q as = case q of -- ... UQ dom get q -&gt; all (\x -&gt; askQ q $ filter ((==) x . get) as) dom 
Similarly, you can get of the recursion in EX n dom get q -&gt; if n &lt;= 0 then True else case dom of [ ] -&gt; False (d:ds) -&gt; if askQ q $ filter ((==) d . get) xs then askQ (EX (n-1) ds get q) xs else askQ (EX n ds get q) xs Replacing it with EX n dom get q -&gt; (==) n . length . take n $ filter (\d -&gt; askQ q $ filter ((==) d . get) xs) dom `filter` is lazy, so the `take n` step makes sure you only try as many as you need to get to `n` true cases. If you don't care about that, you could do: EX n dom get q -&gt; (&gt;=) n . length $ filter (\d -&gt; askQ q $ filter ((==) d . get) xs) dom If you define atLeast :: Int -&gt; (a -&gt; Bool) -&gt; [a] -&gt; Bool atLeast n p as = (==) n . length . take n $ filter p as Then you get a nice parallel with your universal quantifier askQ q as = case q of -- ... UQ dom get q -&gt; all (\d -&gt; askQ q $ filter ((==) d . get) xs) dom EX n dom get q -&gt; atLeast n (\d -&gt; askQ q $ filter ((==) d . get) xs) dom
lazier: exists' :: Int -&gt; [a] -&gt; (a -&gt; Bool) -&gt; Bool exists' n l f = (n==) . length . take n $ filter f l
Not only that, but he misspelled "parallelism" :-)
The implementation of the Java Standard Library for the Lego Mindstorms robot says that 4^2 = 15 because of stupid stuff like this. There was supposed to be an overloaded case for integers. They have it, but they just cast back and forth.
I am inclined to agree, but given how awkward it is to talk to third party installed libraries from hackage, I'm not ready to rule out the horrible manual binding.
I have many functions similar to your recommendOtherProducts function in my code. I honestly couldn't step through how/why they work anymore, other than the type checker says so. (Maybe that's one of the downsides of relying on the type checker too much?) One thing I do that helps is adding type annotations to the variables in the where clause.
You probably want to try: * Building a Monad. Perhaps just State start with. * Look at some other Haskell code to see how dudes are formatting their code. * Try not to go over 80 chars per line. * Pull some nested function definitions out to the top level (the `where` definitions for everything don't buy you much in terms of clarity here. 
Hi, you seem to be doing pretty well. Some suggestions: * Use [Johan's Haskell style guide](https://github.com/tibbe/haskell-style-guide). * Use hlint. Using hlint regularly will make you a better Haskell coder. It's amazing. * If lines get too long, find good spots to break down what you're doing into pieces. Use more 'let's to make your process clear. * Speaking of 'let's, you don't need to say 'let' every time in a block. Just continue on the next line with another binding at the same alignment. No 'let' needed. * You seem to be building your code upward. But it's much better to code top-down. So code your main first, at the top. Things get more and more detailed and nitty gritty as you head downward, as needed. Other than that, just always be on the lookout for people who are better than you are. Clone their repos, study their stuff, become more like they are. I often study [Johan Tibell](https://github.com/tibbe) and [Bas van Dijk](https://github.com/basvandijk).
I didn't mean to. :( *duck*
Thank you. This is why I posted this here. I haven't had any good critique of any of my code (I don't know any Haskellers IRL).
How is that even an argument? Why wouldn't you put the essence of the program first and then all the nitty gritty details at the bottom?
Indeed. I'd advocate using llvm-general, and it is worth noting that llvm general has support for downloading and building it's own copy of llvm. So per se, llvm-general already supports both soltutions! :-)
Once again, there is no implicit parallelism. GHC will run your computations on one core, although it may change which core that is, and may do GC in parallel (not sure about that). The parallel annotations do incur some overhead, and if your granularity is too small then it will be slower than the sequential equivalent.
That's only one way to think about it. EDSLs are another. http://www.paulgraham.com/progbot.html. Except in Haskell I often find myself "extending the language" with custom types first and foremost. So the line isn't as black and white as it might seem. 
This article has opened my mind. Too often we have been focused on the **Beauty** and **Elegance** of our abstraction, instead of realizing that our goal is to solve the problem **efficiently** and **correctly**. (Shown clearly in the case of the recursion)
That's a different (and more interesting) discussion, though. &gt; It's worth emphasizing that bottom-up design doesn't mean just writing the same program in a different order. When you work bottom-up, you usually end up with a different program. Instead of a single, monolithic program, you will get a larger language with more abstract operators, and a smaller program written in it. Instead of a lintel, you'll get an arch. 
I think `sortBySales` is clearer if you wrap it appropriately and give it an explicit type signature. Knowing what the tuples `Map.toList` gives back mean is something you just have to know, I think. I might write `(comparing $ negate . snd)` instead of ``(compare `on` negate . snd)``. sortBySales :: Set Product -&gt; [Product] sortBySales products = map fst . sortBy (compare `on` negate . snd) . Map.toList . Map.filterWithKey (\ k _ -&gt; Set.member k products) $ sales (If I have got the type wrong, I apologise... although it would support my point.)
Good suggestion thanks. I think you got it wrong, so you are right ;) You forgot the sales value, which come from the outer function. Something I thought was a good idea at the beginning, but now, not so sure. 
What makes you think I programmed bottom up? With Python, I usually design my interfaces from the top, asking myself what I would like to be able to write, without considering implementation. Then I use TDD to build the implementation. Porting that to Haskell, I end up writing data types and *undefined* functions, with type signatures. Then I write a test from nominal case, then the implementation. After that I iterate writing test refining the nominal case, and for error cases. Sometimes I feel like error cases have to be included in the nominal case, doing it later would break too much things
The quicksort being a terrible example has been brought up many times before and the answer always seems to be quicksort is a mutable algorithm and not that well suited for functional programming. The merge sort algorithm has comparable performance and is much easier to program in a functional language. Haskellers certainly do care about performance (it is one of its main selling points) and space usage but it is difficult for a beginner and even an intermediate haskeller to begin to tackle. EDIT: to move my selling point qualification
So whyyy does the wiki still have quicksort
I just thought of jjeeb's code of a sample of a larger code body. Clearly his product database is state that can be confined to a state monad instead of explicitly passing around the product database everywhere. If we just consider the snippet of code I then agree with you dave4420 very much so.
Eh, what's wrong with the simple quick sort example? It has *O(n·log n)* complexity even with the ordinary append `(++)`. (Assuming that pivots are ok.) As an additional benefit, the definition minimum = head . quicksort will give an algorithm that finds the minimum of the list in `O(n)` (!) time. The important point is that you can capture the essence of the algorithm in just *three* lines of working Haskell code. Sure, it's not in-place, but the focus is on programmer productivity. If you can do quick sort in three lines, then you can do all other kinds of complicated algorithms in three lines. Don't get hung up on premature optimization.
Time to edit the wiki, maybe replace it with mergesort? 
At the bottom of that section is a link to http://www.haskell.org/haskellwiki/Introduction/Direct_Translation where it is stated that 'The quicksort quoted in Introduction isn't the "real" quicksort and doesn't scale for longer lists like the c code does.' I don't understand that. Why doesn't it scale for longer lists? I guess the `filter`s do two passes of `xs`. Any other issues?
OK, I am sorry, it is O(n log(n)) , but it still spams the heap with list allocations, which severely threaten locality and (I think) require boxing of every element (unlike an array, say) The minimum trick is nice, but can you do the same for maximum? I think not, because, List. (Yes, you could make the comparator method a function argument, which would be rather elegant. That would be a great feature to show off in the wiki) The 3-line version isn't quick sort in a lines, that's part of the point. It's a kind of pivot sort, but not quick sort. Quicksort is quick, on real hardware, not just asymptotically in theory, because it is in-place. 
&gt; I don't understand that. Why doesn't it scale for longer lists? I guess the filters do two passes of xs. Any other issues? Yes: `++`is inefficient and too strict for linked lists, because it must traverse through the whole first list. In general, you shouldn't use linked lists when you want to concatenate and shuffle your structure.
&gt; can you do the same for maximum? I think not, because, List. Challenge accepted. import Data.Ord maximum = unDown . head . quicksort . map Down where unDown (Down a) = a Sure it's a little longer, and yes, it's less practical than just writing a `quicksortBy`, but it works.
I've used Maybe Maybe in https://github.com/laanwj/Purecoin/blob/master/Purecoin/Crypto/EcDsaSecp256k1.hs#L67
Although you only have O(log n) multiplications, those multimplications are of big integers. You can't get better than O(n) time since the number of digits in the answer in \Theta(\log(\fib(n))) which is \Theta(n). 