I'm not sure how RankNTypes would address the aim here, as the point is to support -- and type check -- operations that work for the _specific_ types needed to represent a particular data set. 
Not really. Maybe Silk tried it, but I don't know of anyone else using it.
The integration with Charts as it is now should be OK (just putting in vectors), but I don't think Charts is high-level enough yet for people used to ggplot etc. That's definitively something that could be worked on (and something like this would make it much more salient).
Nice. Perhaps it's time to pick a new hscolour default, or at least a new default for hackage.
I use "full application" as a logical extension of the term "partial application". If you can partially apply a function, then you can fully apply it. I can see where it's not totally satisfactory, in higher-order use cases. But then I wonder if we have to abandon the term "partial application" too.
Thanks for reading! Are you sure "main" is a closed expression? When you unpack most monadic type signatures you're left with a function that requires an argument to inflate the monad, e.g. "State ()", or "[a]". Of course the magical IO is harder to reason about ... 
I'm pretty familiar with both of those pieces of work, but still not seeing how they give you names and types derived from actual data or unboxed storage. I'd be happy to be proved wrong!
I think functions like this are usually called *eliminators*. Dual to constructors and useful for pattern matching. Contrary to folds, eliminators are not recursive even when the datatype is recursive.
Yeah, I'm not a fan of TH for many reasons, but this is a case where one specifically wants to avoid writing code that is already captured in some non-Haskell text. 
&gt; Are you sure "main" is a closed expression? To the limit of the information in the Haskell 2010 report, yes. While none of the constructors (or their arguments, if any) are documented, we definitely "know" that an `IO ()` is either bottom/undefined or one of the constructors of `IO`, fully applied / saturated. It is possible that one of the arguments to the constructor is a function, but that is unimportant when talking about the standard interpretation of a Haskell program. Any necessary arguments to that function will be provided by the implementation.
Is that an intended feature of `Typeable` or just a property of the current implementation?
There is a paper about type providers in Idris (or was it Agda?).
https://ocharles.org.uk/blog/pages/2014-12-01-24-days-of-hackage.html Is this the final link for the series (says hackage instead of GHC)? 
[Other way 'round.](http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/) "An immediate consequence of laziness is that evaluation order is demand-driven. As a result, it becomes more or less impossible to reliably perform input/output or other side effects as the result of a function call. Haskell is, therefore, a *pure* language." Haskell was really put together as an open research language for laziness, though it is strongly influenced by Miranda so purity quickly became *the* solution to handling effects in the new, lazy language.
Maybe this is a nitpick or just completely tangential to the article but this: &gt; "Nothing" (Haskell's spelling of "null") is a problem for me. null and Nothing/None are very different because of how they are treated in the type system. You have to opt-in to accepting a Nothing/None. null is a member of every (reference/pointer) type, implicitly. It's a big difference in meaning, not just spelling. Accepting Nothing/None even means that you have to decorate normal values with a call to Just/Some; it's extremely explicit compared with null. `undefined` (and bottoms in general) is closer to null in my mind. It infects every type, every expression, and generally causes the current thread to crash if it is actually used.
I've not worked through the code in detail, but the overall shape of the design is one i'd agree with. 
reference: http://www.monokai.nl/blog/2006/07/15/textmate-color-theme/
I posted this here hoping that people would poke holes in it, so thank you for poking the most holes in it so far :) &gt; It might work with the current ghc, but it's not based on Haskell semantics. After bringing in `unsafeCoerce` and `unsafePerformIO` I was less concerned with what the documented semantics were and more with what actually occurred, but fair enough. &gt; First, the subexpression unsafePerformIO newUnique is a CAF so the compiler might share one one copy of it for all uses I didn't know about that. After running a simple test with ~~-O3~~ -O2, indeed it does! ~~Do you know if there's a way around this?~~ Nevermind, I seem to have figured it out. &gt; Second, how do you stop the compiler from duplicating the subexpression unsafePerformIO newUnique? I would have thought a `NOINLINE` annotation would have helped. &gt; Third [...], what do you do about dynamically loaded code? I hadn't thought of this at all! 
Thanks
Do they also work across different programs? For example, what if I wanted to pass a `TypeRep` between a server and client?
If you like monokai, you should take a look at [badwolf.](https://github.com/sjl/badwolf/) Heavily inspired by, but IMHO slightly better than vanilla monokai. I switch the pink for a more monokai-like purple though.
But the *Welcome* post has already that name. I'm referring to the *Special page* that is like a calendar for the complete series.
&gt; insert into posts (user_id) values ((select user_id from user where email=$1 limit 1)); I think you are missing a pair of parens in your original query. In any case, rewrite this as `insert into posts (user_id) (select user_id from user where email=$1)` and it'll work fine with 0 or 2 or more results...
I run OS X on the last model 17in macbook pro. I will run this box until it dies or they bring out a bigger screen. I run ubuntu under vmware fusion and do all my haskell stuff there. cabal sandboxes and version pinning in .cabal keeps my code from breaking--except for my bugs of course. I personally don't run haskell stuff under osx simply because I deploy code on linux and I want to avoid the last 1% of difference between linux and osx as a dev box. That said, plenty of other people are happy doing haskell dev work on osx. For the last 7 weeks I have been trying nixos on a vm. It has been a pretty deep but interesting rabbit hole (kind of like Haskell :-), but I am starting to see some major benefits in terms of quickly changing compiler versions and profiling settings without fear of breaking my install. Over on ubuntu I am stuck with ghc 7.4 and afraid to mess too much. 
Thanks for the hard work! $2 /u/changetip
/u/tomejaguar, kraml wants to send you a Bitcoin tip for 5,206 bits ($2.00). Follow me to **[collect it](https://www.changetip.com/collect/201469).** [ChangeTip info](https://www.changetip.com/tip-online/reddit) | [ChangeTip video](https://www.youtube.com/watch?v=_AnfKpypMNw) | /r/Bitcoin
&gt; I've compared some larger queries that we have and confirmed that postgres is able to optimize what opaleye generates to the exact same query plan as manually written queries give. [...] I'd prefer if opaleye generated explicit joins rather than nested queries. Hmm, I admit I haven't delved too deeply into the postgresql optimizer, but my experience has been that hand-written subselects are usually not optimized into joins, and that hand-written joins are often quite a bit faster. So it might be nice to understand the difference here... 
Any chance this can be ported to the 64 bit version of the Haskell Platform?
Have I read 17in MBP ? That's what I'm looking for ... Is it an new one or an old one ? 
Thanks!
If someone can explain to me the subtleties of different forms of queries I'm happy to have a look at how to get the best form out of the code generator.
PhDs. First thing I noticed.
Hi, I have a question about "restrict". Did you consider at some point making it a **QueryArr a a** instead of a **QueryArr a ()**? The former makes more sense from a "ArrowPlus" perspective I think. Maybe it is a bad idea to think of QueryArr as ArrowPlus-like at all?
In my case it's Greg Weber and great Shelly tool https://github.com/yesodweb/Shelly.hs
As far as I can tell, lateral join doesn't give you any additional expressive power. Therefore I find myself confused why the query optimizer can't do the necessary transformation itself.
Do you mean restrict' :: QueryArr a a restrict' = proc a -&gt; do restrict -&lt; a returnA -&lt; a This could be a helpful combinator and if it seems to come up a lot I can add it. I don't see what it has to do with `ArrowPlus` though.
Hmm, I take it back. It seems that lateral join actually allows you to write aggregate :: Aggregator b b' -&gt; QueryArr a b -&gt; QueryArr a b' which is something you can't do without it! That's why Opaleye currently restricts that `aggregate` signature to `a = ()`.
I feel like "I told you about this flawed solution so I could show you this better one" doesn't work as well when you're breaking the post up over multiple days.
You are right that there's significant overlap between them. View patterns seem more direct to me. Pattern guards disconnect matching from the pattern. Also, view patterns are "point-free". I would use view patterns if the function is short and simple to be written inline (ideally, a single name), and pattern guards if there's a more substantial computation to be performed before pattern matching. But if I had to give up one of them, I'd stay with view patterns.
Nice. Wish there was something like this in elm. Just today I had to convert a Json datatype into my own type. As I traverse the Json data structure, I have to do a lookup (resulting in a Maybe), but I can't do something like convert fields = case Dict.get "field1" fields of Just x -&gt; Just { x = x } _ -&gt; Nothing instead I have to do convert fields = Dict.get "field1" fields |&gt; (\maybex -&gt; case maybex of Just x -&gt; Just { x = x } _ -&gt; Nothing which seems overly verbose. Although there may be a better way, and it's a result of my noobishness.
Pattern synonyms and view patterns work flawlessly together. pattern ViewL h t &lt;- ((,) &lt;$&gt; V.head &lt;*&gt; V.tail -&gt; (h, t)) 
I won't be discussing view patterns anymore, so maybe I didn't close this article properly. Rather, I'll be looking at another extensive that is similar. However, view patterns is certainly not flawed - there are many things it does that don't have a replacement.
&gt; postgresql-simple, Persistent, Esqueleto, HaskellDB and Opaleye. +hasql +groundhog maybe time to make a wiki page along the lines of "the javascript problem" -- "the SQL problem". then we can also mention the NoSQL-db interfaces there, and (the) native solution(s): acid-state.
Oh yes sorry! I can't type words.
/u/cartazio was/is working on this, but I haven't heard any updates so it might not make it into 7.10.
The IRC channel on Freenode has gone from 1000 to 1500 in the last 6-12 months and has more people than the Java, PHP, and Ruby channels. I think for Haskell stuff in Australia, people go to stuff like YOW/LambdaJam. There's are active FpSyd and Brisbane FP communities as well. There's also a #haskell.au channel if you want to come hang out with Australian Haskellers. Goodly number in there. Edward Kmett seems to make it out to Australia regularly, he's in Melbourne right now. Haskell is definitely not dying. [My guide for learning Haskell](https://github.com/bitemyapp/learnhaskell) is about to crack 1400 stars, for example.
I dont really know one way or the other. I would think that if we can build wxWidgets using MinGW-w64 then we should be able to do so. Pretty sure it would have to be built with [this version](http://git.haskell.org/ghc-tarballs.git/tree/18e0c37f8023abf469af991e2fc2d3b024319c27:/mingw64) of MinGW-w64 in particular, or at least a 4.6.x version. The reason the current version is 32-bit only is that I saw this line in the wxHaskell documentation: "We support building only with the gnu mingw32 C compiler (windows-mingw)." And so I started by building the 32-bit version, because I didn't really know what I was doing and I didn't want any additional roadblocks.
I have zero hard evidence, but I've been watching the Haskell community and there's a real feeling energy and enthusiasm. Also, while not really hard evidence, the subscriber count used to be about 7000 and now I see it's at 18,000+.
That does look more comfortable.
Nice. This serves well in Idris as the 'with' rule, so Im glad to see it in GHC as well.
For the last year or so: http://www.reddit.com/r/haskell/comments/1modra/13000_readers/
Almost forgot about Shelly! Thanks for the link.
+1 Roman Cheplyaka convincingly argues for fewer equations here: http://ro-che.info/articles/2014-05-09-clauses putting it all together with LambdaCase to avoid naming everything works nicely.
What city are you in? The Brisbane Functional Programming Group has a lot of Haskell interest, and we've got 612 members at the moment. If you're in Brisbane you should come along :)
Yes.
Very very much an intended feature of `Typeable`. It is important for serialization tools that want to check what they deserialize, to anyone using plugin infrastructures where dynamic code gets linked in, etc. Removing it is a non-starter, because it is already in wide use and very much part of the intended use of `Typeable`.
I have to respectfully disagree with Roman. One of the things I like about Haskell syntax is it pushes more of the logic up and to the left. In imperative code we are told to avoid too many right indents. It usually means the code should be refactored and simplified. I find that in my Haskell code this is even more true. I can pattern match and guard away most of the if/else logic branching that led to the move to the right. This tends to lead to much more clear and obvious common versus edge case code.
Your instance (Typeable f, Typeable a) =&gt; Typeable (f a) won't actually work. The `typeRep` will be constructed in a new situation each time, getting new, very different, IDs. This will only work for simple monomorphic cases.
http://redditmetrics.com/r/haskell
Regarding AusHac, one of the main reasons it stopped happening was because I got a job where I wasn't using Haskell and had no time to organise it. I've just started a new job at NICTA which will hopefully mean I have more opportunities to run AusHac. Haskell is by no means a dying language, there are more and more people using it every day, with more commercial use all the time (my work at NICTA will be using as much as Haskell as possible). It's also very influential in some of the more popular languages like C++ and Java, and is being adopted by large corporations to get real work done (see Facebook's Haxl library for one nice example).
Some numbers: Boston Haskell has swelled by a factor of 3x in the last year or so. The #haskell channel is over 10x the size it was when I joined the community. I routinely run into new faces and collaborators. I'm interacting with ~350-400 collaborators nowadays across all of my projects. You have folks in the game industry sniffing around about using it since Carmack's QuakeCon keynote. Even a little niche channel like #haskell-lens has 130 users in it. Haskell seems very very far from dying to me.
One bit of equivocation that would be nice to correct: in English, "order" refers either to how things are placed in sequence (e.g. ascending or descending order) or to a command ("emperor" is from Latin imperator, literally "the guy who gives the orders"). An imperative program is a sequence of commands that change the computer's state, and while yes, their order of execution usually is significant, that dependency isn't why imperative programs/languages are called imperative.
The point about adding parameters to the function made easier by using one equation seems correct to me. Is there any syntax for this "use case" when writing several equations? 
Some feedback from a Haskell newbie: Why not simply include the flag that ghci must be run with in order to use the extension? $ ghci -XViewPatterns It would make the post more self-contained IMHO. About this code listing: lensDownloadsOld :: Map HaskellPackage Int -&gt; Int lensDownloadsOld packages = case M.lookup "lens" packages of Just n -&gt; n Nothing -&gt; 0 1. Where is Map and HaskellPackage being defined or imported from? 2. Is the M in M.lookup a qualified name of Map? I would imagine the rest of the examples in the first section would work if I had answers to those two questions. In the second section, why not include just one more line for import Data.Sequence ? Including this one line would make the post more self-contained. Only the last code listing in the second section will compile! I would prefer if you made it clearer in either the writing or by using a different listing style to differentiate between code that can be copy/pasted and run and that which is just an example or illustration. Only the very last listing, last :: Seq a -&gt; Maybe a last (viewr -&gt; xs :&gt; x) = Just x last (viewr -&gt; EmptyR) = Nothing even compiles (and only after including import Data.Sequence at the top)! 
As an aside, for those of you who are also new to Haskell and running ghci in a comint buffer, eval (make-comint-in-buffer "ghci" "*ghci*" "ghci" nil "-XViewPatterns") somewhere in emacs to enable a ghci with the ViewPatterns extention.
I was thinking the same thing, but when I opened up GHCi and gave it `typeRep (Proxy :: Proxy [Bool]) == typeRep (Proxy :: Proxy [Bool])` it returned `True`. I just tried it again and it gives the expected behaviour (False) if I assign `typeRep (Proxy :: Proxy [Bool])` to two different variables before comparing them. Weird...
Except maybe `Category`, `Arrow`, etc. 
I think you wrote that a bit hastily. This isn't equivalent.
Right. `Maybe a -&gt; b` is not the same at all.
Oops, you are right. I'll retract my answer.
The [HCAR](https://www.haskell.org/communities/11-2014/html/report.html) is getting bigger year by year. So I hardly think that Haskell is dying or not growing.
Group by byte and have 256 cases for each byte. This avoids any manipulation of raw bits and should have better speed, although the code will be uglier. (Possibly use TH to generate this)
&gt; Where is Map and HaskellPackage being defined or imported from? Map is (presumably) being imported from the `containers` library, from the `Data.Map` module. I'm not sure `HaskellPackage` is a real type, though if it is it's probably in the `Cabal` library. It's not important for the purposes of the post though, as it's just some type representing a package and you don't do anything with it. &gt; Is the M in M.lookup a qualified name of Map? Usually. M is commonly used as a qualified name for `Data.Map`, much like V for `Data.Vector`. For extremely commonly used libraries like these it makes sense to use a "standard" qualifier that people will recognize.
Isn't it better to use the .ghci file in a project or in home dir?
I think it's mostly an aesthetic choice, and depends a lot on what that particular function looks like. Thanks for mentioning LambdaCase though, that's been bugging me forever.
Yes, it's isomorphic, but there are a bunch of class instances we don't have for yours, such as Functor, Applicative, Category, Arrow etc. That's why I want a new type.
An alternative answer! I hacked that together while I was playing with this.
That's essentially 1/3rd of what Yampa's arrow types are.
You can guarantee it's balanced like this: {-# LANGUAGE GADTs, TypeOperators, DataKinds #-} import GHC.TypeLits data Tree :: Nat -&gt; * -&gt; * where Leaf :: a -&gt; Tree 0 a Node :: Tree n a -&gt; Tree n a -&gt; Tree (n+1) a I don't see what's wrong with using the fact that it's balanced, though I think you need to know its exact depth in order to exploit that (the GADT encoding does give you that information, though it might be tricky to get at if you're still learning). EDIT: You do need a new-ish version of GHC, 7.8 at least I think.
The former case is using the same dictionary and handing it in two places, the latter case is reconstructing the "same" dictionary in two different contexts from scratch. The problem is your `unsafePerformIO` here is really actually fundamentally unsafe for non-first-order use.
I've never worked with `Category` or `Arrow`, but generic `Functor` and `Applicative` instances seem pretty easy to make here. Here's a quick example I made: import Control.Applicative import Control.Monad data ConstFunction a b = ConstantCF b | FunctionCF (a -&gt; b) instance (Show b) =&gt; Show (ConstFunction a b) where show (ConstantCF p) = "ConstantCF " ++ show p show (FunctionCF f) = "FunctionCF" instance Functor (ConstFunction a) where fmap f (ConstantCF x) = ConstantCF (f x) fmap f (FunctionCF g) = FunctionCF (f.g) instance Applicative (ConstFunction a) where pure = ConstantCF (&lt;*&gt;) (FunctionCF f) (FunctionCF g) = FunctionCF (ap f g) (&lt;*&gt;) (FunctionCF f) (ConstantCF x) = FunctionCF (flip f x) (&lt;*&gt;) (ConstantCF x) a = fmap x a This can be loaded with ghci. Although, tbh, these instances seem really... unhelpful. Maybe I did it wrong?
&gt; It is important for serialization tools that want to check what they deserialize I'm not sure a TypeRep is the best way to do this, but I'll accept that it is the way it is done today. &gt; Removing it is a non-starter, because it is already in wide use I don't really consider that a good excuse for keeping something around. `fail` gets plenty of use and I'd still love to see it exorcised from `Monad`. &gt; and very much part of the intended use of Typeable. But, this is. I do think that if TypeRep is supposed to be something you can write to disk/network/shmem, and then read in from another process, we probably should document it's contents / serialized form more clearly so that we can ensure this level of compatibility (e.g. between GHC and UHC or between GHC 6.8 and GHC 7.10).
Genuine question out of curiosity: Based on this tutorial (skimmed rather than thoroughly read, I must admit), it seems like all proof objects have to be constructed explicitly in Agda, which I assume would become cumbersome rather quickly. Why would I prefer this to a tactics-based approach like Coq's? Or have I just missed Agda's equivalent?
&gt; I'm not sure a TypeRep is the best way to do this, but I'll accept that it is the way it is done today. If it isn't the best way to do this then we'd need to go invent an identical system that _also_ provided those guarantees. At which point what is the point of having a second system that just offers fewer guarantees and doesn't work with dynamically loaded code? It doesn't help your case that the proposal made here also has the disadvantage of not working. =) Way back in the day, the approach mentioned here is very close to what `Typeable` used to do. It'd allocate a fresh ID for each type when forced, and build out a tree of them to describe composite types. This fixes the non-working `(Typeable f, Typeable a)` proposal here. However, it completely crapped out when folks went to use `Dynamic` with `hs-plugins`, `ghci` could cause all sorts of havok with it, no dynamic loading for anything like `hint`, etc. works with it. It is just an evolutionary dead-end. Nothing works together here. As someone who tried using the old system, I do not pine for those days to return.
&gt; I do think that if TypeRep is supposed to be something you can write to disk/network/shmem, and then read in from another process, we probably should document it's contents / serialized form more clearly so that we can ensure this level of compatibility (e.g. between GHC and UHC or between GHC 6.8 and GHC 7.10). GHC has changed around its internal representation for a `TypeRep` at least 3 times since the early 6.x era, so while such a thing would be a nice goal, I don't think we've demonstrated sufficient stability to say that we are confident we won't change it again. In fact, if package keys are becoming part of names in 7.10, it seems that likely to be about to change it all _again_ on you.
One may argue that tactics are an imperative hack and the resultings proofs are write-only code.
Conal likes these and has written a bunch of articles that use them: They appear in the `reactive` source as: https://hackage.haskell.org/package/reactive-0.11.5/docs/src/FRP-Reactive-Internal-Fun.html#Fun You need to be careful with consumption though to respect that any function consuming (ConstCF b) and (FunctionCF $ const b) should evaluate to the same result, even if they take different paths to get there.
Original author here. I wrote this when I was still learning. Now I'm a lot more experienced in type theory. So I've updated some sloppy terminology i noticed when this version was posted in the original github project. I'll host it on my own website in a few minutes. Edit: New version is here http://learnyouanagda.liamoc.net
&gt; lenses are super ugly *grabs popcorn*
I stand by my statement.
I definitely agree on javascript as a compilation target. I am using Fay, but looking at ghcjs.
That's good to know, thanks!
Using a fix point is a correct solution. Another pragmatic solution is to store your annotations in a Map and look them up when you need them. This way you won't have to touch the AST. 
In Sydney, the FP-Syd meetup which is (un-intentionally) somewhat biased towards Haskell has been getting 30+ people to every meeting this year. Two years ago, the meetings were getting 20-30 people.
It seems there are far more advertisements for as Haskell jobs now than there were 12 months ago.
Ugly in what way? Line noise? That could be said about any custom operator. I don't use lens much and have very little experience with it but a couple things I noticed: There are places with a lot of duplication. Like in `calcWidth`, you have a lot of repetition in accessing `root._1`, return $ rt &amp;~ root._1.content.width.= w'' &amp;~ root._1.padding.left.= plf &amp;~ root._1.padding.right.= prt &amp;~ root._1.border.left.= blw &amp;~ root._1.border.right.= brw &amp;~ root._1.margin.left.= ml &amp;~ root._1.margin.right.= mr You could reduce some duplication by moving `root._1.padding` and `root._1.margin` into two local variables. Or just `root._1` into a local variable. let root_padding = root._1.padding root_margin = root._1.margin return $ rt &amp;~ root._1.content.width .= w'' &amp;~ root_padding.left .= plf &amp;~ root_padding.right .= prt [..] (I'm not 100% sure that'll work. I don't have an interpreter handy. Also, there's probably a nicer way to do this -- I'll let someone more familiar with lens explain that.) Also, the documentation for `&amp;~` actually mentions that this is what you should be doing, return $ rt &amp;~ do root._1.content.width .= w'' root._1.padding.left .= plf [..] Couple that with local variables, and you've got return $ rt &amp;~ do root_content.width .= w'' root_padding.left .= plf root_padding.right .= prt [..] That's fairly nice compared to the alternative of 10-level deep record updates.
This is an instantiation of the [Lift](http://hackage.haskell.org/package/transformers-0.4.2.0/docs/Control-Applicative-Lift.html) applicative, although it doesn't get you that many useful instances (except Applicative itself). import Control.Applicative.Lift type ConstFunction a b = Lift ((-&gt;) a) b constant = Pure function = Other app :: ConstFunction a b -&gt; a -&gt; b app = unLift 
I've submitted a pull request which I hope makes some improvements that sell 'lens' a little better. Just a side-note, I really think switching to a less "fluid" indentation style helps a great deal in readability, by reducing how claustrophobic the code is (... on this admittedly small screen).
That's probably exactly what I'd write, too.
Nesting: foo ys | (xs:_) &lt;- reverse ys = case xs of xs' | (x:_) &lt;- reverse xs' -&gt; x vs foo (reverse -&gt; (reverse -&gt; x:_):_) = x &gt; foo [[1..3],[4..7],[8..10]] 10 
Hi, thanks for your feedback! Originally, I was going to use literate Haskell, but that forces me to write `import` statements at the top, which I found a little distracting as I didn't need the imports until later. However, it sounds like it's worth doing - so I'll be sure to try and make the future posts literate. Note that today's post does have a [code listing](https://github.com/ocharles/blog/blob/master/code/2014-12-02-view-patterns.hs) - but it would probably help if I linked to it!
When/where is the next one?
We had one last Tuesday at the Red Hat offices, we have a hack night tonight (that's where I am now), and we've got the FP with the Stars panel event on Saturday in the lead up to YOW. We normally have talks on the 3rd Tuesday of every month, with hack nights the Wednesday 8 days after that, but we have December off, so the next regular night will be on January 27th. Details are available [here](http://www.meetup.com/Brisbane-Functional-Programming-Group/).
Or the absolute weirdest set of coincidences imaginable.
I'm not totally motivated by last :: Seq a -&gt; Maybe a last (viewr -&gt; xs :&gt; x) = Just x last (viewr -&gt; EmptyR) = Nothing vs. the equivalent last :: Seq a -&gt; Maybe a last s = case (viewr s) of xs :&gt; x -&gt; Just x EmptyR -&gt; Nothing but the examples in the [user guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/syntax-extns.html#view-patterns) or on the [Trac wiki](https://ghc.haskell.org/trac/ghc/wiki/ViewPatterns#Examples) are either a touch too abstract/elided (I think appropriately, though, so it's probably just me) or not really "aha"-y for me. Does anyone have an example of a really gross construction that was made more fluent with ViewPatterns?
It's super exciting. In London there are a load more haskell opportunities than 2/3 years ago. Not an insane amount but the growth is immense. 
It's definitely instructive, though not as fleshed out as I might hope. I'll read his paper, at very least. It looks like it's really nice for constructing interpreters, at any rate, so that's something to go for. Thanks for the link, and the post! 24 Days of... is my favorite Christmas present!
For one, `network` has been around since 2006, and it may have accumulated quite a bit of technical debt by now. I wonder if it's time for clean `network2` rewrite, or if the current `network` can be improved w/o breaking backward compat for every user of the current `network` package...
&gt; However it looks like the library is not actively maintained. To my defense, I've been on vacation and I have another ten or so libraries I maintain. But yes, the library could use a lot of love (without breaking existing users). It needs better docs, more tests, and a general deep look at how we handle failures. &gt; If it's the library's problem, how come nobody complains about this? It's not possible to write stable software with this library and it's downloaded 18113 times in last 30 days! Clearly it is possible for some sub-set of use cases since people do use it in production code. I agree the library could be much better though.
&gt; One nice thing about Agda is that it forces you to choose your representations very carefully, since you cannot sweep a bad design under the rug with tactics. IMO, it forces you to be a bit too careful with representations! The strict positivity condition on datatype declarations makes it way more painful to do datatype-generic constructions than it ought to be. As a result, we often end up proving things for particular data types when we should be proving them for classes of datatypes. I think the modularity problem you identify would be much less of an issue if Agda had a better story for generic programming. (If I understand him, this is why Conor McBride is so hot on having a closed universe of type formers.) Coq has the same problem, for the same reason, though you can hack around it with tactics.
About maintaining the library: https://github.com/haskell/network/issues/62 this one is from 2012 and touches some similar points with my issue report. There are other issues waiting to be solved too. To be clear: I'm not blaming anyone, this is a community-driven project and this is no particular person's responsibility, I'm just saying that _as Haskell community_ we should have maintained the library better. (here what I'm trying to do is exactly that) &gt; Clearly it is possible for some sub-set of use cases since people do use it in production code. Why? Can't people just specify upper bounds of `network` dependency?
"Types are Proofs" nooooooo no no. types are propositions. programs are proofs. this is a bad sign. in text you use it correctly (more so, anyway, havent checked thoroughly), but you misuse "judgment" as if it were another way to say "type"/"prop", which is categorically false.
Are they really anti-modular tho? Agda, unlike Haskell, has a proper module system that permits useful abstraction barriers and implementation-independence.
Based on just the number of inquiries I get in my inbox asking if I'm open to relocating somewhere or consulting on some Haskell thing, the ecosystem is definitely alive and kicking. ;)
I like lenses but I don't like the operators. In my, admittedly very limited, use of lenses I just stick to the longer function names (`over`, `set` and `view` etc.).
I fixed the "types are proofs" error in the new version. Did you look at the new version? I don't have control over third party blogs. I'm using the word "judgement" in the sense of an inductively definable assertion in the metatheory, but you're right that I am a bit loose with words in the last chapter that's there. Seeing as we're essentially embedding propositional logic within Agda's type theory, I used the word "judgement" to refer to types because TT is the metatheory we're using. I plan to scrap that final chapter anyway, I'm not very happy with it. In any event, my usage is hardly "categorically false".
&gt; http://learnyouanagda.liamoc.net [Héhéhé](http://natsintt.tumblr.com/)
Sorry that was not the correct example. I meant something like this # \d t Column | Type | Modifiers --------+---------+----------- x | integer | y | integer | # select i from bar; i --- 1 2 3 (3 rows) # insert into t (x,y) values (1, (select i from bar)); ERROR: more than one row returned by a subquery used as an expression This also fails on 0 results if y is not null.
If you want tactics in Agda, you can write tactics in elisp. /troll
IMO `network` package should use extensible exceptions. But `network` is older then extensible exceptions in Haskell (they were introduced ~5 year IIRC), and it is part of core set of libraries. It is hard to do the switch, because a lot of code will be broken. I don't like the idea to return `Maybe ErrNo` from `IO` action, but I agree that the current situation is problematic. Breaking code is bad idea too :)
One correcting comment logical equivalence isn't bijection! ℕ is "logically equivalent" to the unit type (defined by, say `unit : Unit`), but they are certainly not in bijection. 
tnx. too bad it only got the last 2 yrs of history.
Ah, good point. I'm too used to classical logics, where they are of course the same. I'm going to scrap the last chapter anyway, but I'll make sure not to make the same mistake in the next version. Edit: I have corrected the page, anyway, to avoid misleading people.
Regarding the awkward interface: originally, `Network.Socket` was intended to be a primitive low-level direct wrapping of the C API, not intended for daily use. Then `Network` provided wrappers with a more Haskell-friendly interface. But it turned out that people found that `Network` had problems. I'm not sure what all the problems were; the comment at the top only mentions "TCP only", but there must have been other issues. So `Network` was deprecated, and people were told to use `Network.Socket` directly. I understood that to be a temporary measure until a better wrapper interface was built. But as they say, there's nothing more permanent than "temporary". It is actually still rare to use `Network.Socket` directly - the vast majority of network code goes via a handful of protocol-specific or application-specific libraries. Perhaps that's why there hasn't been enough of an impetus to provide a better low-level API. Regarding buggy behavior: it could be, and if so, thanks for studying it and reporting it! But I must say that many people - myself included - have been doing a huge amount of network programming over the years, and it really does seem to just work, at least for TCP. It's true that UDP probably hasn't been exercised very much, so there I would be less surprised to find significant bugs. There are people who either are now or have been in the past working intensively with the internals of this code all day long. It's definitely worthwhile to get them involved in the discussion. One of them is /u/tibbe, of course. Others are Simon Marlow, and the authors of high-performance http servers like warp, snap-server, and happstack. EDIT: Oh, sorry, you are not reporting buggy behavior, you are reporting the inability to get the return code and errno via the API. Yes that is a very good point.
After edit 'haskellPackages....' to 'pkg.haskellPackages...' in shell.nix, I've got success. Although I don't know why. $ nix-shell --pure shell-fail.nix error: attribute `haddocset' missing, at /home/dontdieych/cis194/shell-fail.nix:14:6 (use `--show-trace' to show detailed location information) $ nix-shell --pure shell.nix [nix-shell:~/cis194]$ haddocset Usage: haddocset [--hc-pkg CMD] [-t|--target DOCSET] [-q|--quiet] COMMAND [nix-shell:~/cis194]$ exit $ diff -u shell-fail.nix shell.nix --- shell-fail.nix 2014-12-03 21:00:07.950125794 +0900 +++ shell.nix 2014-12-03 20:51:27.541826747 +0900 @@ -11,5 +11,5 @@ haskellPackages.cabalInstall haskellPackages.ghcMod haskellPackages.hdevtools - haskellPackages.haddocset + pkgs.haskellPackages.haddocset ];}) $ cat ../haddocset/default.nix # This file was auto-generated by cabal2nix. Please do NOT edit manually! { cabal, Cabal, conduit, conduitExtra, exceptions, haddockApi, mtl , optparseApplicative, resourcet, sqliteSimple, systemFileio , systemFilepath, tagsoup, text, transformers }: cabal.mkDerivation (self: { pname = "haddocset"; version = "0.3.0"; src = ./.; isLibrary = false; isExecutable = true; buildDepends = [ Cabal conduit conduitExtra exceptions haddockApi mtl optparseApplicative resourcet sqliteSimple systemFileio systemFilepath tagsoup text transformers ]; meta = { homepage = "https://github.com/philopon/haddocset"; description = "Generate docset of Dash by Haddock haskell documentation tool"; license = self.stdenv.lib.licenses.bsd3; platforms = self.ghc.meta.platforms; }; }) 
&gt; I don't like the idea to return Maybe ErrNo from IO action The point is to keep the library low-level, safe and transparent -- sockets API reports errors using `errno` and I think we should provide it to users to allow learning about `errno`. Then we can wrap things in higher-level libraries/modules to throw exceptions instead of exposing `errno`. So both functionalities can exist together and I believe this would provide users a good API. If you want exceptions, use higher level wrappers. If want to read `errno`, go with lower-level module. &gt; Breaking code is bad idea too :) Again, I don't understand why people can't just specify upper bounds of `network`. New `base` will break lots of code too(because of `Traversable/Foldable-Burning-Bridges Proposal`), that's part of working with Haskell. (That being said, I'd also be very happy with `network2` rewrite)
&gt; Why? Can't people just specify upper bounds of network dependency? Because the Haskell/Hackage community is divided on the usefullness of the PVP... one half of us thinks upper bounds are the right thing to do (it's the purpose the PVP contract in the first place to be able to specify upper bounds ahead of time), the other half doesn't believe in PVP/upper-bounds... (just google for pvp/cabal-hell/upper-bounds blogposts etc to learn about the two opposing viewpoints)
&gt; The definition of the natural numbers tends to appear awfully often when discussing Type Theory. Here is an attempt at documenting this strange phenomenon. I don't get it. Vectors are also a recurring example when discussing dependent types, proving commutativity of addition is (or so I hear) common when discussing proofs, and geometric shapes are a common example when discussing inheritance. What's so strange or funny about using proven, standard examples when introducing or extending a subject? Is the goal simply to gather lots of type-theory papers and blogs at the same place? 
I said that I didn't _like_ the `errno` -- just an opinion :) Yes, `base` will break code. And that is bad. But at least the code will break at compile time. Both `errno` and extensible exception will break code silently. (With `errno` you'll get warning in some cases, but will not in other cases) ADD: upper bounds will make packages not compatible. You can't use two packages that depend on different versions on `network`. Rewriting it as `network2` will work only if the packages don't export anything `network`-related, otherwise they are not compatible again.
How are Agda developments anti-modular? 
Looking fwd to "Real World Agda", ;)
More generally, I think the exception handling in Haskell is an awkward area of the language. It seems to me, coming from Java and its checked exceptions, that the exceptions that can be thrown by a function are part of the type of this function, and should easily be visible, without reading the code or the documentation. The main argument against is usually "but most errors you can't recover you should just abort the program" which is very limiting, as your case proves.
I agree.
I'm pressed for time, and I will admit that this is the sort of thing I usually end up working out in the repl, but I think you can even elide most of the local variables: return $ rt &amp; root._1 #%~ (&amp;~ do content.width .= w'' padding.left .= plf ...) In my head, I read this as more-or-less "working on rt, focus on root._1 and apply the following changes". I will freely concede, though, that the examples in the documentation tend to focus on such simple structures that it sometimes take a little flaliling in the REPL on my part to figure out how to use them to come up with something sensible.
Please don't. It's great work, and I appreciate it a ton.
You could also do it without GADTs like this, but I think the lookup function might require a logarithmic number of allocations or something annoying like that. This particular version also only permits non-empty trees, but you could use a Maybe-like wrapper to fix that. data Tree a = Bottom a | Deeper (Tree (a, a))
Is there any advantage to this over just using `a -&gt; b` and `const` for constant values? `(-&gt;)` has tons of instances too.
This is not the main argument against breaking changes. The main argument is that many libraries need to support older and newer versions of their dependencies. That in turn means that if you make a breaking change to a core library, all the downstream libraries need to live with hard to maintain CPP #ifdefs for the next couple of years. This way well-intended clean-ups often make code worse, not better. The better way to improve things is to add new types and functions.
Sorry about that. I don't really know what the errors are, I just shared it cause I thought it was interesting. Do you want me to delete the submissions?
 #%~ (&amp;~ do Oh my.
There are a couple of issues with the current `Network` convenience module, some fundamental and some due to lack of attention: * It only works for streams, hence not for UDP. This is due to assumptions in `Handle` that reading 0 bytes means EOF. This is not true for connection-less protocol like UDP. * It uses `Strings` for binary data, which is semantically incorrect. * It only exposes a small fraction of the sockets API. I also have some, perhaps more subjective, problems with `Handle`s in general and thus the `Network` module in particular: abstracting over stream-like things by converting them to a concrete data type (i.e. `Handle) is bad. Our I/O libraries are lacking a basic abstraction for streams. What we have today is one concrete low-level type (`Handle`), which is almost as low-level as the OS file abstraction, and a zoo of different designs for high-level abstractions (iteratees, pipes, conduits, etc). We're are missing a mid-level abstraction that's present in most languages (e.g. Java, Scala, Python, etc). Something like this: class ByteSource s where read :: s -&gt; IO ByteString readN :: s -&gt; Int -&gt; IO ByteString close :: s -&gt; IO () (In fact, the implementation of `Handle` has one of these internally, but it was never exposed in the public API.) Having something like this would make it easier to provide a mid-level API for stream-based sockets.
It's made even more awkward by us almost never documenting which exceptions our functions raise.
&gt; Again, I don't understand why people can't just specify upper bounds of network. As a community, we have learned the hard way that the library dependencies problem is very hard. When you "just" specify upper bounds, that solves the problem for certain use cases, and makes things even worse in others. &gt; (That being said, I'd also be very happy with network2 rewrite) /u/tibbe is suggesting adding new functions to the existing library, not a completely separate one. I think that is a much more sensible way to proceed. It will cause much, much less integration pain for a lot of people, while still allowing us to move forward.
How is a `ByteSource` class better than data ByteSource = ByteSource { read :: IO ByteString, readN :: Int -&gt; IO ByteString, close :: IO () }
On both Warp and http-client, when I rewrote the internals to not use any streaming data abstraction, I ended up with something almost identical to that, e.g.: * https://github.com/snoyberg/http-client/blob/ae82e09c4658babf0aca7edd98669f0b941beeae/http-client/Network/HTTP/Client/Types.hs#L65 * https://github.com/yesodweb/wai/blob/41839e865e6161c2159acd13b4de4839d61e0686/warp/Network/Wai/Handler/Warp/Types.hs#L96 I baked in the leftovers/putback stuff, but it could easily be separated out into a different abstraction layer.
Nah, it's OK. I've resolved the errors now, and most people have been very supportive.
That was it!
Thanks for the excellent work. Read the first chapter and it's very interesting. How much Classical logic knowledge does one need to finish this book ? (I'm working through the third Chapter of Velleman's How to Prove book and thought maybe I can finish this book and do the exercises of Velleman's book using Agda ? )
&gt;It's not. The purpose of the PVP contract is semantic versioning. Preemptive upper bounds are one way of attempting to leverage semantic versioning; they are not the whole purpose. if that's not the whole purpose...what else (other than providing the contract to legitimate version bounds) requires semantic versioning? 
The image URL is a snapshot of the mail received from Erik Meijer out of his current FP course in EdX. Edit: The following text is in the image: &gt; Learning never stops, and many students asked about what to do next. If you &gt;want to see more practical applications of functional programming you may &gt;consider to follow the next installment of Principles of Reactive Programming. If &gt;you want to pursue more fundamental concepts, watch this space form my &gt;upcoming Category Theory for Programmers MOOC. And of course there are &gt;(way too) many interesting course available on EdX and an infinite supply of &gt;functional programming papers, tweets, and blogs on the interwebs. &gt; &gt; Thanks again for taking part in this journey, and a big round of applause for all &gt;the students who asked and answered questions on the forum. 
&gt; GHC has changed around its internal representation for a TypeRep at least 3 times since the early 6.x era Well, we should at least document what *level* of `TypeRep` compatibility is available, even if it is not much. Or, perhaps `TypeRep` wasn't *really* meant to be serialized and that should be handled by a different mechanism?
They top out at 15 inches now. The resolution on the 15 inches is good but size still matters. 
can io-streams be considered such a desired midlevel abstraction?
I know agda but not coq, so take the following with a grain of salt. There is a standard agda technique for automating obvious proofs: compute a proposition which is easier to prove. For instance, suppose you wanted to prove that `_&amp;&amp;_ : Bool -&gt; Bool -&gt; Bool` is associative. One way to write down an explicit proof would be to pattern-match on all 2 * 2 * 2 combinations and use `refl` as a proof in each case: explicitProof : ∀ x y z → (x &amp;&amp; y) &amp;&amp; z ≡ x &amp;&amp; (y &amp;&amp; z) explicitProof false false false = refl explicitProof false false true = refl explicitProof false true false = refl explicitProof false true true = refl explicitProof true false false = refl explicitProof true false true = refl explicitProof true true false = refl explicitProof true true true = refl This is tedious. But notice that if we expand out the forall into all the cases at the type level, we get a larger type but a smaller proof: prf : (false &amp;&amp; false) &amp;&amp; false ≡ false &amp;&amp; (false &amp;&amp; false) × (false &amp;&amp; false) &amp;&amp; true ≡ false &amp;&amp; (false &amp;&amp; true) × (false &amp;&amp; true) &amp;&amp; false ≡ false &amp;&amp; (true &amp;&amp; false) × (false &amp;&amp; true) &amp;&amp; true ≡ false &amp;&amp; (true &amp;&amp; true) × (true &amp;&amp; false) &amp;&amp; false ≡ true &amp;&amp; (false &amp;&amp; false) × (true &amp;&amp; false) &amp;&amp; true ≡ true &amp;&amp; (false &amp;&amp; true) × (true &amp;&amp; true) &amp;&amp; false ≡ true &amp;&amp; (true &amp;&amp; false) × (true &amp;&amp; true) &amp;&amp; true ≡ true &amp;&amp; (true &amp;&amp; true) prf = refl Of course, this only works because `Bool` has a finite number of inhabitants. I wrote an agda library called [agda-finite-prover](https://github.com/agda/agda-finite-prover) which takes a proof that `Bool` is finite and a description of the proposition you want to prove, and does the forall expansion for you: finiteBool : Finite Bool finiteBool = finite xs w refl where xs = false ∷ true ∷ [] w : FiniteWitness xs w false = # 0 , refl w true = # 1 , refl automatedProof : ∀ x y z → (x &amp;&amp; y) &amp;&amp; z ≡ x &amp;&amp; (y &amp;&amp; z) automatedProof = auto finiteBool 3 (λ x y z → (x ⋅ y) ⋅ z ≈ x ⋅ (y ⋅ z)) refl It's not that much shorter because you first have to prove that `Bool` is finite, but if you had to write many proofs about the booleans, it might be worth it. Also, in the years since I wrote this library, agda has gained some reflexion abilities which might allow the duplication of the proposition (once with `_≡_` and once with `_≈_`) to be removed. Anyway, the main point is that you only have to type one `refl` instead of eight. Again, I haven't used coq at all, but my impression is that coq provides you with a useful but finite set of tactics, while agda allows you to implement your own proof techniques.
Knowing a lot of classical logic is perhaps not advisable. Hilbert-style systems very much go against the grain of Agda's constructive theory. Proofs relying on contradiction or the axiom of choice, or even extensional equality (in the sense of functional extensionality), may not be straightforward to express in Agda.
Not to ruin the joke, but isn't that exactly the goal of Idris?
I don't want to sound pedantic, but couldn't you have just copy pasted the text into a Reddit text post instead? Then it would display at the top of this page as well as being searchable.
Coq also allows you to write tactics, but in an untyped, somewhat unprincipled tactic language called ltac.
I think it's okay to use synchronous `IO` exceptions because at least the `IO` shows up in the types, but asynchronous exceptions are problematic because they are not signaled in the types.
Only if they are tracked at the type level. So, not so much with the `IO`-based exception mechanism.
Do you mean async exceptions, or impure exceptions? In other words, there are generally three ways to "throw an exception": 1. Throw an exception synchronously: `throwIO` 2. Throw an exception to a different thread (async exceptions): `throwTo` 3. Create a value which, when evaluated, will cause an exception to be thrown: `throw`
Yeah, I certainly avoid throwIO in my code. Most of the time `Either ex res` makes a better exception handling monad. I'm sure async exceptions will bite me in the future, but not yet! A lot of people hate Java's checked exceptions anyway. If you could abstract over them, I'd like them, but you can't, so I usually end up subverting the system.
Yeah, that would have been more sensible. I have updated the comment to include the text as I cannot change the URL now. 
I'm surprised that these aren't considered gauche here. Only a matter of time, I suspect?
Haskell is most definitely not dying. I'm one of the co-organizers of the New York Haskell meetup. We just passed our two-year anniversary and we now have over 800 members. I believe our October event had the largest attendance we've ever had with 146 RSVPs with 80+ people actually attending. We also recently announced a new conference on typed functional programming called [C◦mp◦se](http://www.composeconference.org/). Initially we were unsure what kind of a response we would get. But rather than struggling to fill our speaker slots, we have far more than we can accept! I personally have been writing Haskell professionally full-time for almost five years now, and I keep hearing about more people getting full-time Haskell jobs (including another one just this week). I predict that as more companies using Haskell start generating solid software that starts to see the light of day, Haskell's benefits will become more and more evident.
Haskell is definitely growing. In Australia in particular. FP-Syd has grown consistently for the last several years, and it has been around before most of the other sydney programming communities (it began in 2008). AusHac was last in 2011, but I believe it also started in 2011 or 2010. There weren't that many AusHacs. The organisers aren't students anymore and it's harder to organise things when you have a work schedule. 
Hail Fellow Nicta-er. Here's to keeping our jobs in the next few years! After the recent corporate meeting it really feels like a sinking ship :/
You might want to look into finger trees. http://staff.city.ac.uk/~ross/papers/FingerTree.html
Not wanting to tread into this broiling holy war too deeply... I find that lenses work best when used lightly. There is a remarkable power to weight curve available if you just stick with very basic traversals, lenses, and prisms and probably no combinators besides `view`, `set`, `over`, and `toListOf`. Beyond that, try making a nice StateT stack using `zoom` to focus in on smaller pieces of state. That can also have a huge power-to-weight ratio.
I take it that [mtac](http://plv.mpi-sws.org/mtac/) hasn't caught on, then?
Ok, then use this: insert into t (x,y) (select 1, i from bar); If you need, you can move the 1 into a `values` table literal, and join: insert into t (x,y) (select x, i from (values (10),(20),(30)) val(x) join bar on true); Alternatively, you could use a temporary table in place of the `values` literal. Postgresql-simple does have explicit support for generating the `values` syntax, see the documentation for the [Values](http://hackage.haskell.org/package/postgresql-simple-0.4.8.0/docs/Database-PostgreSQL-Simple-Types.html#t:Values) type.
It's not so different. The main point was that `Handle` is bad, because it bakes everything (Unicode support, universal newlines, buffering) into a single package. A package that's opaque and you cannot easily make new instances for. As for classes vs records we need some more detailed designs to judge the merits. One upside with classes is that I can write a function: worksOnAnyStream :: Stream s =&gt; s -&gt; ... and just pass it a `Socket`. With the record approach I have to manually wrap the `Socket` in a record (using some helper function). If there's one canonical mapping from a type (e.g. `Socket`, `ByteString`, etc) into a bunch of functions (e.g. `ByteSource`) then a type class adds quite a bit of convenience. EDIT: It just struck me that you could also do something in-between e.g. class ByteSource s where toByteSource :: s -&gt; ByteSourceRecord I haven't thought about whether it makes sense. It would give you a different option that newtypes if you want to pass a different set of functions.
It's definitely quite a bit closer. There are questions whether e.g. leftovers/putbacks/buffering should be part of the core concept or be broken out into different classes (or records, as per @tomejaguar's comment above).
It's not in widespread use, as far as I know. I look forward to its widespread adoption.
It's safe to say that the jury is still out on the best methods for constructing proofs in a dependently-typed language. Tactic-based approaches exemplified by Coq offer the promise of natural-style proofs that are concise and resilient to change, but carry the pain of imperative-style programming with them. Syntax-based approaches exemplified by Agda are more verbose, but are cleaner (in that tough-to-quantify but *you can feel it* way) because they're functional at their core. Is the middle ground (for some definition of middle) the right answer? Certainly Idris falls in this boat. But we still have a ways to go before we can answer this fundamental design question about proofs in programming.
It takes zero skill to post a snarky comment on HN, it takes a lot more skill to write about Agda. Don't put much stock in what the HN herd thinks. Thanks for writing this, it's appreciated.
Please leave it up. After the holidays I might have some time and I really would like to try some Agda. Your material looks really good, so I will definitely read it (it is still available). So thanks in advance! Your Agda tutorial will make some hacker happy :-)
Great! Could you please include a link to that in each of the posts?
Could be a little less rude with your comment. If rephrased as "did you mean to say {...} here" it would carry the same information content and be more friendly way to communicate your concern with the terminology.
Oops, I should have clarified that I meant using something like `error` in a pure computation (or any other failure that doesn't show up in the types, like partial functions).
&gt; It's not that much shorter because you first have to prove that Bool is finite, but if you had to write many proofs about the booleans, it might be worth it. Also, in the years since I wrote this library, agda has gained some reflexion abilities which might allow the duplication of the proposition (once with _≡_ and once with _≈_) to be removed. Anyway, the main point is that you only have to type one refl instead of eight. &gt; You can also write tactics directly in OCaml. Although this amounts to a DSL for an untyped language in OCaml so it doesn't really save you anything.
I was thinking to myself "this would be nice as a literate Haskell program"! Thanks for doing the series, it allows a newbie such as myself to get acquainted with some of the features of Haskell. It is also nice to read the comments section here as well. 
Thanks for the clarification. In that case, I totally agree with your previous statement.
It's not quite a classical vs constructive logic thing; even adding LEM (in a way that makes sense) doesn't invalidate this fact. It's an issue when we begin to think of propositions as things that contain (multiple) elements that problems arise. E.g., if we take a HoTT-like definition of proposition (all elements of the type are equal), then between *propositions*, we do have logically equivalent is the same as bijective (but not between general types). edit: Also, it's probably not a big deal for something so basic, but there's also a distinction between *logical equivalence* (two things imply each other) and *type equivalence* (which allows us to transport structure). In fact, equivalence of types is equivalent to bijection.
Any plans to publish this physically? I would buy. I like books :)
&gt; I don't want to sound pedantic In computer science, pedantry is a virtue not a vice, IMHO. The devil is in the details!
&gt; It's not quite a classical vs constructive logic thing; But in most classical logic, all propositions are either _equal_ to true or false. In HOL, for example, the type of booleans and the type of propositions is the same. There isn't a number of elements for a given proposition. I guess just adding LEM to type theory doesn't give you that stricter notion of classicality, though.
the nix package manager under OS X. I have yet to actually use nixos itself. Just the nix package manager on top of os x/centos/ubuntu. I love the fact that it just lives in its own little world and does not interfere with the global package system at all. 
Is it safe to assume this course will also benefit those studying category theory without the direct intent of applying it to programming? Perhaps as a supplement?
I agree with lenses often being ugly. Luckily some of this ugliness can be limited by avoiding the operators. If you do decided to use them (weirdly I always use ``.=`` and ``^.`` but avoid all the others, so it's understandable), then they can be easier to read if you include whitespace. For example: l^.root._2 I would write as: l ^. route . _2 Although many tutorials seem to prefer: l ^. route._2 Anywho, in many cases lenses are a necessary evil. This all IMHO of course!
I know Idris wants to be a language useful from general purpose programming. But I'm not aware Agda and Coq have no such ambitions. For what I understood, Haskell also was mainly useful in academics for the first many years of it's existence.
Sign me up as well. I have important Haskell networking code using sockets at work.
I understand the reason to be for async exceptions, as they represent signals between different processes, but why should somebody synchronously throws an exception when they can just simply return that value by normal means?
The set of exceptions that can be thrown by a function consists of **1** The exceptions thrown by the function itself. **2** The exceptions thrown by any functions called within this function. **3** The union of exceptions thrown by **every** function that can be passed as an argument to this function. The third point means things get out of hand very quickly if you don't constrain the exceptions that can be thrown by arguments.
This looks very exciting. What is the general difference between Coq and Agda? I have been doing some Coq and Agda looks to lie in the same category.
Is the strict positivity really necessary? Intuitively it seems like we ought to be able to define a fixpoint of any type, if we flip around least &lt;-&gt; greatest fix points for contravariant positions. Lets use mu F for the least fix point of F and nu F for the greatest fix point of F. Intuitively mu F are values of type F(F(F(F(...)))) where the values only have a finite nesting, and nu F are the values such that the context only has a finite nesting. For example when F t = nil + cons Int t, we get mu F are the finite lists and nu F are the potentially infinite lists. A consumer of mu F can be infinite, and it's guaranteed to terminate anyway because any value of type mu F is finite. Dually a nu F can be an infinite list, but a consumer of nu F must be finite (i.e. can only inspect a finite prefix of the list). This guarantees termination. Things don't terminate when we use an infinite value in an infinite context. Now if we look at a type such as F t = Nil | Nocs (t -&gt; int), we get into trouble if we do mu F = Nil | Nocs (mu F -&gt; int), or nu F = Nil | Nocs (nu F -&gt; int). This allows us to do: bad :: (mu F -&gt; int) -&gt; int bad f = f (Nocs f) bad (Nocs bad) &lt;--- oops To fix that we should define: mu F = Nil | Nocs (nu F -&gt; int) nu F = Nil | Nocs (mu F -&gt; int) So in contravariant positions the least/greatest fix points flip around. This immediately rules out `bad` because we apply `f (Nocs f)`. This is a problem because we pass an infinite value into an infinite context, which leads to nontermination. With the new types that's ruled out, since if `f :: mu F -&gt; int` then (Nocs f) must have type nu F, so it can't be passed into f. The requirements for fixpoints still apply: every recursive reference over a mu F must be guarded by a case analysis and every recursive reference over a nu F must be guarded by a constructor. This data type still allows us to define some funky values: weird :: nu F weird = Nocs (\x -&gt; case x of Nil -&gt; 0 | Nocs f -&gt; 1 + f weird) Fixpoints can maybe be generalized even more. If you have the type of binary trees T = Nil | Node (T,T), then you can imagine several different fix points. One is the least fix point where every path from root to leaf is finite. The other is the greatest fix point, where every path from root to leaf may be infinite. But there are two other natural options: the one where a path down the left is finite, but down the right is infinite, and the reverse. The idea is that if you have such a tree where left paths are finite but right paths are infinite, then if you descend down the tree and you make sure to go left once in a while, you'll always terminate. You can only nonterminate by going right an infinite number of times. So to construct such a tree you could start with a credit n, and to construct a tree with credit n you construct a tree with credit n-1 for the left subtree and a tree with credit n for the right subtree. Dually, to consume such a tree you start with credit k, and every time you compute recursively down the right branch your credit for that branch will be k-1 and every time you go compute recursively down the left branch you can keep credit k. The credits n and k together ensure termination when you compose a producer with a consumer, because the depth with which we actually go down the tree is bounded by n+k. When n goes negative we blame the producer of the tree, when k goes negative we blame the consumer of the tree. So in terms of types you could denote that informally as: T_(n,k) = Nil | Node (T_(n-1,k), T_(n,k-1)) An inductive binary tree would be: Tind_(n,k) = Nil | Node (Tind_(n-1,k), Tind_(n-1,k)) So we credit the producer for both branches. A coinductive binary tree would be: Tcoind_(n,k) = Nil | Node (Tcoind_(n,k-1), Tcoind_(n,k-1)) `T_(-1,k)` can be interpreted as the uninhabited type, and `T_(n,-1)` can be interpreted as the unit type. Let's look at inductive lists for simplicity: Lind_(n,k) = Nil | Cons (int, Lint_(n-1,k)) We can have a function like: fold f z xs = case xs of Nil -&gt; z | Cons (x,xs') -&gt; f (fold f z xs') This is a list consumer. In terms of credits we see that the list consumer is decrementing the list producer's credits. Every time we case analyze a Cons(x,xs') the credits associated with the type of xs' are decremented. So those credits will eventually go negative, and when they go negative we can blame the list producer for nontermination. For Lcoind we can have: ones = Cons(1, ones) We see that the credits of the type associated with the recursive reference to ones are being decremented. So when they go negative we can blame the consumer for nontermination. Now it's easier to understand why the weird type works: W = Nil | Nocs (W -&gt; int) We add these credits to get an inductive weird type: W_(n,k) = Nil | Nocs (W_(k-1,n) -&gt; int) And these credits to get a coinductive weird type: W_(n,k) = Nil | Nocs (W_(k,n-1) -&gt; int) Note how the order of n and k gets changed in both cases. Suppose we have `W_(2,2)` to mediate the interaction between producer (let's call her player A) and consumer (let's call him player B). First the A is asked to provide a value of type `Nil | Nocs (W_(2,1) -&gt; int)`. So suppose A produces a `Nocs f`. In order for B to use that f, he needs to come up with a value of type `W_(2,1)`. If he makes a `Nocs g`, then A needs to come up with a value of type `W_(1,1)` to use that g. Then in turn B needs to come up with a `W_(1,0)`, then A with `W_(0,0)`. Then if A produces a `Nocs h` for that, in order to use that h, B will need to produce a value of type `W_(0,-1)`. But that's the unit type, which is easy to create. So he puts a unit value into h, and h has to produce an int. Hopefully that is not too vague (or incorrect...).
Yep. I've actually heard that people do this!
This sounds like a great initiative, but what are the advantages to creating a `network2` vs a major new version of `network`? Is the only benefit that it would help people who refuse to follow the PVP? It seems a shame to have package versions expressed both with version number metadata and package name text. In terms of development, a new package written from the ground up sounds fine, but I hope that it could eventually become the next major version of `network` so we don't have to take on the burden of explaining unfortunate names in perpetuity.
How do I support multiple versions of `network` if you introduce new types and functions? If there is benefit to using the new things, then I would need something like #ifdef to support both old and new. 
FP101x MOOC which uses Haskell to teach Functional Programming has 27000+ subscribers if I recall correctly. I also maintain a Haskell magazine on Flipboard and the past weeks I get nearly daily a new follower (my guess is that this is related to the FP101x MOOC).
Even in the intuitionistic version of most logics, logically equivalent propositions are equivalent. It even holds in the calculus of constructions (and so, is true is Coq). It doesn't break down until we take the very liberal interpretation of types as propositions (sometimes called "propositions as all types").
In fact, many of the other lens combinators can be implemented directly in terms of `view`, `over`, `toListOf`, and `zoom` (and `hoist` if you don't use the `Zoomed` type class). This is a good exercise for learning which parts of the lens library are essential and which are not.
Well you can just avoid cutting over to the new stuff for a few release cycles!
They exacerbate the general Haskell issue of having to read lines both backwards and forwards at the same time. Aesthetically, they look out of place in any code that isn't 100% lenses.
sibip: I wish to take this course. Do you know of any way to be sure to be notified when signups start?
This would be awesome. I've tried reading though Benjamin C Pierce's category theory book and I always get stuck. Hopfully relating this back to Haskell would. Also Vazirani quantum computing course on edx was amazing so I have high hopes for this course.
Yeah, so the benefit of preserving the old API in new versions is to let old code build with the new library. Normally, you'd use an upper bound to indicate incompatibility with a new version, but let's say things are more complicated. The problematic scenario is if you (or one of your dependencies) want to use new network alongside a dependency that needs an older version itself. So this is a classic case of wanting to support private dependencies, and isn't the work to support that pretty far along? Maybe the timing is working out such that a major new version of a core library like `network` could be released around the time we get this new feature in cabal/GHC. That would be an awesome step forward!
That's good, u/zvxr points out above that there's a lens named `zoom` which lets us drop straight to `root._1` without defining extra lenses, so we'd end up with: return $ rt &amp;~ zoom (root . _1) do content.width .= w'' padding.left .= plf; padding.right .= prt --I like to group these into logical units [...] which is a lot nicer. Edit: reddit's doing something weird with the character alignment inside the code box.
&gt; I will freely concede, though, that the examples in the documentation tend to focus on such simple structures that it sometimes take a little flaliling in the REPL on my part to figure out how to use them to come up with something sensible. This really annoyed me because most of the examples essentially work out to "look how we can use lenses to say something in the same amount of space but less legibly". The documentation is not great at showcasing real uses of lenses.
&gt; Ugly in what way? Line noise? That could be said about any custom operator. That is in fact a common complaint about custom operators, and why many people avoid them as much as possible.
Its model is better than the typeclass solution Johan is advocating above, but only because it punts on most of the issues raised by that approach. For instance: what do you include in a typeclass like `ByteSource`? OK, you can read and close the stream. What about seeking? Buffering? How about pushback? Out-of-band data? Datagrams? What's nice about io-streams here is that it doesn't try to conflate streaming with object identity or lifecycle management: you attach a streaming machine to a resource, rather than treating the resource as if it were intrinsically a stream with "is-a" semantics.
Very nice, /u/tomejaguar! It seems nobody asked this question, yet: could you use applicative functors instead of arrows? I thought I've read somewhere that these are two sides of the same coin but that the proof was too large to fit on the margin. For better or for worse, arrow syntax is somewhat frowned upon (yes, I know applicative functors don't even have syntax sugar!).
Here goes [my franchise](http://bartoszmilewski.com/2014/11/24/types-and-functions/) :-(
I've been looking at these courses for a bit with the hope that I can convince my boss to pay for one or two members of my team to attend if/when they came to NY ($1700 + tax + two days "off" is not the easiest sell to a non-technical CEO at a startup), but it's difficult to gauge. The fast track definitely seems too basic for them, but the advanced seems... not too advanced, just a little scattershot. I feel like there's a more pragmatic middle ground that assumes one understands e.g. type classes, usage of common instances, frequently-seen language extensions, etc. and shows how to bring it all together to greater effect in industry. Friends and colleagues I'm helping (in all modesty, as I can hardly tell where my skill level lies on the spectrum) with Haskell often remark that they chug through exercises, blog posts, examples I give them, etc., feel great in their new understanding, and then when browsing source on Hackage can't recognize what they've learned or when working on something themselves can't work out how to piece things together nicely. I want to recommend a course to them, but the [course description](https://skillsmatter.com/courses/465-well-typed-advanced-haskell#programme) doesn't necessarily sound like it's what they're looking for (maybe the bottom section, a bit). Can anyone who's taken this course comment?
&gt; Just a side-note, I really think switching to a less "fluid" indentation style helps a great deal in readability, by reducing how claustrophobic the code is (... on this admittedly small screen). Your formatting changes made my eyes bleed :P I think in general a pull request is a bad system for something like this, where I'm asking for community input, because it's more of a conversation between you and me. That said, I'm implementing the changes you suggested (mostly) but rejecting the pull request because I don't want to bother fixing the formatting changes.
I think there's enough room for more than one resource on this topic. I've tremendously enjoyed your articles so far. MOOCs aren't for everyone. :)
Yeah, eventually it’ll get old. For now, it usually comes off as supportive of the language &amp; community.
The first example looks very similar to what an `InputStream` and `OutputStream` pair would provide. Have you explored that as a basic streaming abstraction?
You mean io-streams? It's not an option for a simple reason: it pulls in way too much extra stuff unrelated to a core abstraction. The whole purpose of moving away from conduit (which I was quite happy with) was to be streaming-framework-agnostic. Moving over to a different large abstraction with a huge dependency list would have been a step backwards, not forwards. There are also some practical issues about performance optimizations that I can get away with here. For example, in Warp, I know that we never put back more than one piece of data, so it can be a simple `IORef ByteString` that never does any appending. Also, instead of needing a `Maybe` wrapper, since it's specialized to `ByteString`, we can use the empty `ByteString` to indicate end of stream.
\ThankYouBasedMeijer
A decent system would take as a (type-level) argument the sets of exceptions thrown by arguments (including function arguments). A similar type system is described by Wadler, in "Well-Typed programs can't be blamed" if I recall correctly.
It's literally the first thing on Google... https://www.coursera.org/course/reactive
Ok. 
There have always been two camps, all the way back to the beginnings of the language (see the section on "expression- vs declaration-style in "Being lazy with class"); I tend to fall into the declaration style camp myself. I have a running list of ideas for alternate universe haskells, and in one of them you don't even get any lambdas. 
Doesn't the RoseNode pattern match all possible RoseLeaf values? (With `rest = []`.) Or is there some magic that prevents this swallowing? Does GHC warn if you try to match the RoseLeaf pattern after trying to match the RoseNode pattern?
Ah, the cases of `doSomething` either need to be flip-flopped, or you can make the change I just made in an edit :)
If something like that could be added to Haskell, I'd be keen for the chaos of a Haskel Report 20xy and a GHC 8.0 :) 
On a phone here, can you see when it starts?
Wow, people are dicks. The loud minority always need to turn a good thing ugly. 
Not just you, Elm specifically requires you to write all functions like this. Feels too much like the top level is a special case. 
I think the first one should work... What error are you getting? You might just need an extra set of brackets around the argument to case. EDIT: The first one definitely compiles for me... could you elaborate on the error you're getting? I got it to compile at http://share-elm.com/sprout/547f8a04e4b00800031ffd32
&gt; feel great in their new understanding, and then when browsing source on Hackage can't recognize what they've learned or when working on something themselves can't work out how to piece things together nicely Hi! This is in part what my [guide](https://github.com/bitemyapp/learnhaskell) is designed to address by laying out the recommended materials for more advanced topics that you'd want to understand in order to be able to read other peoples' code on Hackage. The guide takes people through installing Haskell, learning the basics via cis194 and NICTA Course, picking up some odds and ends via O'Sullivan's Stanford course (including some important intermediate patterns and GHC extensions), into things like monad transformers, GHC Core, graph algorithms &amp; data structures, Yoneda/Coyoneda, etc. I'm also working on [a book](http://haskellbook.com/) which will take people through the basics into intermediate territory, falling short of full coverage for (Kmett | Eisenverse | Oleg) code comprehension.
This course is aimed at people with some knowledge of Haskell: http://www.cs.nott.ac.uk/~gmh/cat.html
No way, please keep them coming!
I see. Thanks for the explanation! 
Yeah, most people I talk to have found your guide but still feel as they do - a list of disparate, somewhat incomplete resources, though helpful, doesn't really add up to one complete resource. Good work, though. I usually recommend [this](http://dev.stephendiehl.com/hask/); it seems well-considered and having one author makes it read coherently, but it's really like Haskell spark notes rather than a guide. I'm still waiting for a canonical resource to recommend; fingers crossed it'll be your book.
You can also write tactics in Agda itself, which is much more sane.
&gt; the general Haskell issue of having to read lines both backwards and forwards at the same time This is one of my biggest gripes with Haskell. Thanks for putting it so well! 
Btw, here's another example use-case for `PatternSynonyms`: [optimizing the internal representation of `Data.Version.Version`](https://groups.google.com/forum/#!msg/haskell-core-libraries/q9H-QlL_gnE/WUSqogVY89QJ)
So far `TypeRep` has demonstrated the ability to be serialized within a single GHC major version. Across versions we've failed to provide stability there in the long term. I'd be happy to say we should document at least the former. I'd love to aspire to having longer term stability in this area, its just that we seem to keep changing it.
Would, e.g. `network-3.0` be a superset of the API exposed by `network-2.*` or have an incompatible API? If it's incompatible, then you'd run into the problem of not being able to link `network-2.*` and `network-3.*` into the same program at the same time. With a separately named package `network2` you could depend on both package generations at the same time in one program.
I've come to actually like writing in Mattieu Sozeau's Program construct in coq. You write code in a fairly Haskell-like high level language, and then it incurs obligations wherever it needs to elaborate that out into Coq. When combined with the ability to write a custom obligation tactic, you can pretty much write whatever embedded domain specific language you want, with domain-specific rules for discharging those obligations.
I prefer a well written book over any MOOC.
I'd have it be incompatible. To the extent that linking maintained and old code together is needed, this is a great motivating case for the ability to link different versions of a library into one executable. I know there's been a lot of work done on package IDs by Edward Yang recently, and pushing this capability through cabal would be a huge boon for just this scenario. EDIT: I realize I'm advocating a hard line stance, and accept that a compromise solution might be preferable, especially if support for multiple library versions isn't on a realistic time horizon. In such a case, providing the nice new API under some particular hierarchical name makes sense. The old interface can be deprecated, and eventually we could even re-export the good interface from a nice name like `Network`.
Very cool. Reminds me of the dlist trick in a way.
OK that's true. io-streams is indeed batteries included but the underlying basic abstractions (InputStream/OutputStream) are not more than what you described + putback.
At the end, when talking about purity of a function that just returns some compile-time information. It has me thinking about an `IO`-like functor (probably a comonad?) that could be used for general meta programming. I could imagine functions like `splice :: CT (Expr a) -&gt; a` and `rtConstant :: CT a -&gt; a`. Has there been much exploration of that? Admittedly, this question is quite a bit off-topic, but it seems like it might give better semantics to things like C's `__FILE__`, `__LINE__`, and `__func__` predefined macros / identifiers. This CT functor would be just as pure/impure as IO but it would serve as a explicit border between compile-time evaluation and run-time purity. Just curious; I think what has been implemented in Idris looks like it will work pretty well in practice.
Notice that if we actually make FFI to read errno this will certainly be broken - errno is a value stored in thread local storage and can change non-deterministicly (from the view of any one Haskell thread). The solution should (does?) somehow obtain errno at the time of the error and squirrel it away as a Haskell value.
No, arrows have inputs whereas applicative functors don't. They are certainly not the same thing, though they are closely related.
In the BlendMode example, ghc can do exhaustiveness checking on functions that use the ADT. What about functions that use the equivilant pattern synonym, can ghc somehow tell that setUpBlendMode AlphaBlending is defined but that it's not defined for the other values?
Mine too. =)
It's a big help when you're writing functional libraries. It's not important for application development, though it is interesting to understand why Haskell can do its magic and why people are talking about some functions breaking laws and such. 
My experience is that it is very useful when you want to know if a thing can or cannot ever exist. I've wasted many years chasing after projects I now can prove are impossible. Category theory has proven invaluable to me to find ways to "stop looking" for a solution where none will ever exist. Without it, I'd still be chasing down various ratholes for impossible solutions and not producing useful code right now. YMMV.
Lots of very smart, crazy people have discovered and named patterns which crop up in mathematics, and these also turn up in computer programs. Being aware of these patterns or abstractions might help you organize or understand in a more fundamental way the code you're writing, figure out how to simplify an API, or realize "there's probably already a type class for that", write abstractions that compose well with others' code, etc. **EDIT**: Oh and it provides a language that lets you communicate about complex, abstract things in a precise way (while people around you wonder if you're just trolling), in very few words. Dan Piponi's blog has years of (sometime more, sometimes less) practical applications of category theory, and he's a great writer: http://blog.sigfpe.com/
&gt;having to read lines both backwards and forwards at the same time. Not sure what you're referring to. The general use of operators, perhaps? &gt;Aesthetically, they look out of place in any code that isn't 100% lenses. Agreed.
&gt;a list of disparate, somewhat incomplete resources, though helpful, doesn't really add up to one complete resource. I know :( - I'm working on it! &gt;I usually recommend this; Yes, I link to Diehl's guide in my, uh, guide. Guides all the way down. &gt;it's really like Haskell spark notes rather than a guide That's why it's not more prominent in my guide. People are better off doing exercises and working through stuff, but it's still an excellent reference. I use it all the time. &gt;waiting for a canonical resource to recommend; fingers crossed it'll be your book. You and me both. Thank you! I'll be working on the book after I get back from the gym tonight :)
In my mind, one of the most important topics we cover in the advanced course is undersanding evaluation. This is an important topic for writing real code and it's not one that's generally taught (let alone taught well). Programmers experienced with other languages can get up to speed quite quickly with Haskell but they carry with them their pre-existing ideas of how programs execute. It's a matter of learning a new set of rules for how lazy functional programs execute, and then the vast majority of the silly space leaks and performance bugs that beginners make go away. 
yeah, you're right, I was a bit obnoxious sorry /u/kamatsu :(
I've done some preliminary market research and it seems like your teaching-category-theory franchise will do just fine :)
If you're using Agda as a meta-theory, then it's a perfectly fine statement, but then the Curry-Howard stuff necessarily recedes into the background. In CH, its props-as-types, whereas judgments are something else entirely (which are typically completely left out of FP, which often has only one judgment: `A true`)
If you're struggling with Awodey I think Aluffi might be a bit tough - although I recommend them both :) What's your maths background like? If it has been a while, and you want to get through those two books (which I recommend quite a bit), I usually recommend the following - How To Prove It by Velleman (to get back into maths, comfortable with proofs) - A Book of Abstract Algebra by Pinter (since a chunk of category theory examples comes from that field, plus it's a good warm up) - Conceptual Mathematics by Lawvere and Schanuel (since it covers the start of category theory slowly) One thing I will say about How To Prove It and Conceptual Mathematics - don't get discouraged early. One of the first chapters of How To Prove It can look pretty intimidating, but it's just trying to make some larger points - after that it starts with truth tables for boolean operations and ramps up at a gentle pace. Conceptual Mathematics follows a pattern of a rigorous / more dense chapter, followed by several informal chapters covering the material more slowly. I know people who've been stung by one or both of these :)
As someone who is currently learning How to Prove it (in Chapter 3), how easy do you think it is to learn TAPL after finishing that ? I heard type theory uses Intuitionistic Logic and classical logic won't be of much use there.
Any chance you guys can throw this up on hackage?
Unfortunately no. Watch this subreddit or the edX page regularly and you are sure to get notified about this.
&gt; However, view patterns have the advantage of being able to perform actual computations -- something that (to the best of my knowledge) pattern synonyms cannot do. Note that you can combine pattern synonyms with view patterns (as last mentioned [here](http://www.reddit.com/r/haskell/comments/2o341e/24_days_of_ghc_extensions_view_patterns/cmjbdoa)), so it's not either-or. Wrap up your view patterns in a pattern synonym for the best effect.
The [GHC entry on HCAR](https://www.haskell.org/communities/11-2014/html/report.html#sect3.1) has some teasers about that but not optimizations and extensions planned for the next 5 years.
Lol. That's like I gave you the Trolly Problem in ethics and you answer "I apply the breaks" :) Yeah, in real life I'd probably look for something like that to get rid of the case analysis too. 
That's the sort of thing I figured he was talking about, but composing lenses with (.) reads left to right, so I don't understand why hrothen says lens makes it worse.
On the contrary, one of the best ways to learn a new subject is to find and try as many different approaches as possible. Please don't stop writing these!
Monthly plug for www.dohaskell.com
Sounds compelling -- can you provide a couple of examples if possible?
Why aren't there more instances of `Adjunction` in Haskell? Why isn't there a `Cocont`? Why are there so few comonads? Why do monadic lenses not make sense?
This is pretty tangential, but shouldn't it be: (buf0 &lt;&gt;) I've always had the idea of treating leftovers as adding things onto a stack, so that the last thing you put on is the first thing you read next.
If we really are talking about a low-level abstraction, then I have two other problems with InputStream/OutputStream: * A low level abstraction should be intended to be non-abstract. Yes, people can cut themselves on it, but that's why it's low-level. * Having OutputStream always have a Maybe wrapper is a bad idea. In WAI, we addressed the flushing issue by having a separate flush function. I also have problems with io-streams approach to resource allocation, which is unclear at best. Does sending Nothing close the OutputStream? I believe the philosophy is that it doesn't (resource allocation is handled outside of the streams themselves), but then how is Nothing an "end of stream"? I think /u/tibbe's interface is far closer to a universal streaming abstraction. If we're going to generalize, get rid of the chunk size, and get rid of close (which seems to be what io-streams would want), I'd probably go for: data InputStream a = InputStream { isRead :: IO a } data OutputStream a = OutputStream { isWrite :: a -&gt; IO (), isFlush :: IO () } To detect EOF on reading, each input type would need to define its "empty," which for datatypes without a sensible empty, we'd need to use a Maybe wrapper. Adding putback/leftovers on top of InputStream would be trivial, as would adding connection closing. However, at that point, a typeclass structure like /u/tibbe mentioned would allow us to have a hierarchy of operations: class InputStream s where type InputStreamElem s isRead :: s -&gt; IO (InputStreamElem s) class InputStream s =&gt; InputStreamPutback s where isPutBack :: s -&gt; (InputStreamElem s) -&gt; IO () class InputStream s =&gt; InputStreamClose s where isClose :: s -&gt; IO () and so on. __EDIT__ Out of curiosity, I tried implementing something along these lines: https://github.com/snoyberg/snoy-extra/blob/master/Data/Streaming/Universal.hs I tried to keep the ability to have arbitrary values in the stream and optimized operations for nullable values like ByteString. PutBack is implemented as a separate concept, and closing the stream is an optional feature. I also added a test of wrapping up this interface with conduit to prove it's doable. That said, I'm not really loving it. There are probably ways of improving on the design.
If it's really important to you, you could check out the [Google Chromebook Pixel](https://www.google.com/chrome/devices/google-chromebook-pixel/), which has a 3:2 screen ratio. From what I hear it's possible to throw full-blown Linux on there besides Chrome OS. Expensive though...
To be clear, I'm sure it's a fine course. I wouldn't mind taking it myself and was hoping you'd be bringing it back to NYC, though early February is crunch time for us so I don't know that I'll have the time. I was merely observing that, based on the descriptions, it seems like there's a space between the fast-track and advanced courses (which are suggested as complementary, sequential courses on your site) where a lot of early-intermediate Haskellers struggle and isn't served in a nice holistic way anywhere that I've found. In most fields, actually, I see this same intermediate gap. There are innumerable books that'll teach you basic grammar and vocab in various languages, and advanced speakers have a wealth of native resources they can delve into, but material can be scarce or scattered for the nuanced in-between. I'd like to see someone (more ambitious/qualified than I) put together something in that space for Haskell; but again, perhaps your courses do cover that better than I'm imagining. See you in February, maybe.
&gt; We're are missing a mid-level abstraction that's present in most languages (e.g. Java, Scala, Python, etc). The more I think about this statement, the more I disagree. The core network library should supply low-level functions that work on raw buffers, and functions that take and receive ByteStrings -- that's it. You can trivially implement whichever higher-level abstraction you want to use on top of that. There's a zoo of streaming interfaces, as you put it, but that means there's no justification for creating the N+1'th solution to this problem and shoehorning it into the middle of one (or more) of the platform libraries.
Do pattern synonym declarations appear in export/import lists and haddock?
I've thought about that question in general as well, and now that I've found an answer, I will not let this opportunity slide to pimp my blogpost! http://unsafePerform.io/blog/2014-07-12-arrow's_place_in_the_applicative_monad_hierarchy/
It's not actually my complaint, it's the complaint I receive from users. I tried doing this with conduit initially, and got pushback. Every extra dependency I add to warp or http-client is met with resistance. I *do* wish we lived in a world where people would stop considering a package like vector or blaze-builder as a "heavy" dependency.
&gt; Has there been much exploration of that? Not that I know of. There's the Q monad for TH, but that's not really what you're getting at. A lot of the really nice metaprogramming work is going on in Racket, and there's not a lot of motivation to express things in terms of comonads there! Idris's type providers also make a compromise between effectful operations at compile time and purity. The semantics of an Idris type provider are essentially "run this IO computation and extract the result, splicing it in as the RHS of a definition". But that's got no access to the source code of the file that calls it.
I am not sure what GHC team has planned, but these are the things I wish it had planned: ---- Unboxing: I'd really really want GHC to unbox everything (that it can) in C++ compilation style. By that I mean: * Unboxing even lazy values (to avoid indirection). Stick thunk information next to the value itself. * Unboxing polymorphic values (specialize code as necessary to do this) Boxing causes indirections. Indirections thrash the CPU cache. Thrashing the cache kills performance. Boxing also causes more allocations. Allocations cause more GC, which we pay for in pauses and runtime. ------- GC I believe that the nursery and Haskell's GC in general are considered so good that we don't need more optimal allocators like LIFO (stack-based) allocators for local code. I disagree with this. Note how Haskell benchmarks are only really impressive until allocations get in your loops. A LIFO allocator (after escape analysis) is O(1) alloc and O(1) free without amortization, copying or GC costs. It also reuses the same cache lines so it is much more cache-friendly. I think GHC should probably do escape analysis and use LIFO allocation rather than heap allocation whenever necessary. ------ Lifted products and exponents I think the Haskell committee made a big mistake: In order to make `seq` without a type-class palatable, they declared type products and exponents to be lifted. Besides the bad consequences for parametericity, this has some really bad consequences for performance too. It means that GHC cannot optimize `f . id` into `f`. Many other useful "obvious" optimizations are foregone because they might accidentally refine the termination properties of products and exponents, which could avoid having termination properties at all in the first place! Especially when you throw in much specialization: When specialization realizes that many of the types in actual use are products and exponents, it *should* allow a bunch of extra optimizations to shoot, but due to lifted products, it cannot! Changing this now is hard, due to the prevalence of `seq` without a class. Either: * Allow `seq` on unlifted products and exponents, but have it be a no-op (this will break a lot of assumptions!) * Add a Seq constraint that is propagated implicitly for all types and type-variables, without having it for products and exponents. Code that attempts to use `seq` on a product or exponent directly will not compile. You can always wrap your code in a Seq-able newtype. I believe the second option will break code but will be worth it, just like AMP, Eq,Show=/&gt;Num, etc.
Thought this was in /r/Marvel for a minute :)
Do you have an idea about how your unboxing proposal could be achieved? I believe one reason lazy values can't be unboxed is that the resulting value can be bigger than the thunk itself. Perhaps the extra space could be allocated if the type of the value is known so your idea about specialization can be used here.
&gt; if that's not the whole purpose...what else (other than providing the contract to legitimate version bounds) requires semantic versioning? This is like saying, "what else could be the purpose of a type system in a programming language other than restricting you from writing the code that you want to write?" Many people made that argument, and Haskell and other modern FP languages finally demonstrated to the world that it is not true. On the contrary, higher-level semantically rich descriptions of algorithms add a huge amount of power. But it took many years of research before we learned how to do it in a practical way. The same is certainly true for semantic versioning in build systems. It should be easy for us as Haskell programmers to realize the power that semantic versioning will add to build systems. But we also should have realized how hard it would be. Cabal has only recently reached the point where it is just as easy to manage dependencies with semantic versioning than without, and part of the reason is that at the beginning we didn't work on it hard enough. Instead, some people were advocating writing intentionally wrong semantics in their cabal files in order not to be "restrictive". It's understandable, because the limitations of earlier versions of cabal made it extremely painful to manage dependencies in various contexts. They just wanted to be able to get work done. But now I think we are past that stage.
&gt; Is the goal simply to gather lots of type-theory papers and blogs at the same place? It's a side effect. The goal is a bit tongue in cheek: a mate of mine was giving us a hard time every time he would spot the definition of the natural numbers on one of the boards in the office because it's such a basic thing and it pops all over the place in TT. Because of that, I started noticing the definition quite a lot in pretty much everyone's work and figured I might as well log that. I've been pretty inefficient at remembering to add the links though.
We had quite extensive discussions about unboxing polymorphic fields at ICFP 2 years ago. There are some tricky corner cases, but I believe it to be doable.
What are some examples of these corner cases? I've done this by hand a couple of times of fairly elaborate types and never ran into any problems.
From the GHC documentation: In export/import specifications, you have to prefix pattern names with the pattern keyword, e.g.: module Example (pattern Single) where pattern Single x = [x] As for haddock - I don't know. If at all, it would have to be a very recent version.
RoseNode does not look like a bidirectional synonym.
They seem to be shown in haddock output yes: https://hackage.haskell.org/package/sdl2-1.3.0/docs/Graphics-UI-SDL-Enum.html Although I didn't find it in the gl2 documentation (didn't look at the source code), so it might be a new thing.
Thanks for the feedback. Yes, you're right, there's certainly some gap. The "Advanced Course" is generally better for people who have already written quite a bit of Haskell code themselves, even though it doesn't strictly speaking assume any knowledge that wouldn't be communicated in "Fast Track". More experience definitely helps, though. There are also a few intermediate topics that would deserve being covered in more detail and getting a dedicated course. Not that it helps in this case: (1) I'm also (still) working on a book. This is a long time in the making already, but I'm still optimistic it's going to happen eventually. (2) We're currently developing a few new recurring Haskell courses that we're eventually planning to offer via Skills Matter. I'll definitely take your suggestions into account.
That does look interesting, thanks!
It's there, see for example [`Graphics.GL.Core32.GL_TRIANGLES`](http://hackage.haskell.org/package/gl-0.6.1/docs/Graphics-GL-Core32.html#v:GL_TRIANGLES).
Open [bugs](https://ghc.haskell.org/trac/ghc/query?status=!closed&amp;failure=Runtime+performance+bug&amp;type=bug&amp;order=id), [feature requests](https://ghc.haskell.org/trac/ghc/query?status=!closed&amp;failure=Runtime+performance+bug&amp;type=feature+request&amp;order=id) and [tasks](https://ghc.haskell.org/trac/ghc/query?status=!closed&amp;failure=Runtime+performance+bug&amp;type=task&amp;order=id) for performance optimizations in the GHC issue tracker.
Nice, I didn't know about this! Very cool.
They show up in Haddock with a pretty strange syntax of pattern synonym type signatures that is **not** what will be used for actual type signatures in GHC 7.10. If you would like the upcoming GHC 7.8.4 to use the eventual real syntax, please complain here, loudly and ASAP: https://www.haskell.org/pipermail/ghc-devs/2014-December/007512.html
&gt; Incidentally, Coq actually constructs formal proof terms when you prove something using tactics. I've heard of these terms getting so big that Coq choked---which is a nice case-in-point demonstration/evidence of the fact that a tractable proof with tactics may correspond to an infeasibly large formal proof. This used to happen regularly when we were working with Matita. Some of our proof terms generated by tactics were gigantic (hundreds of megabytes or more) and used to regularly crash the system.
It's not some magic property of either pattern synonyms, or view patterns; it's just that the right-hand side of a pattern synonym definition is a pattern, and a view pattern is also a pattern...
Now we need the converse. Any ideas how to improve theorem proving in Agda/Idris?
Sure, but sometimes it's easy to forget how these things interact.
Imagine that. I would never have guessed. ;)
&gt; I believe one reason lazy values can't be unboxed is that the resulting value can be bigger than the thunk itself. Surely the other way around would be the problem? If the thunk is bigger than the value then the thunk needs extra room just to store itself.
Wouldn't the `map` read right to left, especially if it was in a longer chain of transformations: map (f . g) . filter (h . i) &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
Interestingly checked exceptions are considered by many people in PL research one of the bad design choices of Java! Even James Gosling himself considers it a failed experiment. Particularly bad is the case where library creators make an Interface without exceptions and/or the particular exception you need to call when implementing that interface.
Hmmm... what do those people recommend then? Can you provide some sources/reading material? I'm interested as I am often coding in Java and try to make my stuff as well-typed as possible. What do you recommend in case of Haskell?
Hello, I don't know much about Agda so I wanted to let you know that your book is very readable and I enjoyed a lot the beginning of the read, at least. Now I'm stuck with: DEPRECATED: Propositions and Predicates So I guess I have to wait now. Keep the good work! 
Just found out this can be enabled directly in ghci Prelude&gt; :set -XViewPatterns
In Java, putting the exceptions list is a good documentation practice. The problem is when developing interfaces and/or abstract classes you end up needing to put a 'throws Exception' almost everywhere to permit the implementors to send checked Exceptions. Or else they will be unable to, they will have to resort to use unchecked exceptions (or silently hide the error), which might end up causing problems.
In Java, there is also the case of methods with lists of 20 different exceptions. At this point, checked exceptions become useless even as documenation.
My answers would be "No; No; No;...". Not very useful, so I'll comment instead. I don't have my packages in Stackage, because they don't have big user base. I don't use it for personal projects, because after cabal sandbox was introduced, I almost forgot about dependency hell. (Yes, I understand that Stackage has indirect value even for me.) I should use Stackage at work, but I don't. (Right now I use cabal freeze, but I'm not completely satisfied with it.) But I'm the only haskell dev in the team (well, we got other one a month ago), and I know what to do when something it wrong. But I really think Stackage would be useful in production. 
Noooo! I've been refreshing that page every day waiting for more! I know I left my email, and it *claims* I'll be notified, but what if it doesn't!? Your articles are written perfectly for my learning style, so I hope you keep them coming. Pleeeassseeee.
I think these intermediate/advanced courses focus too much on choice of libraries and don't focus enough on idioms/architecture. I can teach a person how to use pipes/conduit/lens/stm/async/mtl/etc., but that doesn't get them that much closer to understanding how to architect a large Haskell application. People need more a of a "big picture" idea of how all the pieces fit together.
Very interesting, thank you for the data point! :)
If Java had sum types I suspect this would not be such a big problem.
Thank you for providing more detailed answers here, every bit of information is very much appreciated. (And thank you to everyone who's been answering the survey!)
This is awesome, it's like a type-provider but less crazy since you can see and edit the types that were generated! Can't wait to see what it does with a non-trivial database schema.
http://www.mindview.net/Etc/Discussions/CheckedExceptions
Agreed. There's a lot of resources which are nicely curated for "How to become a beginner in Haskell", but what's severely lacking are curated resources for: 1. "Real World Haskell" (outside of the book, of course) 1. "How to get from beginner to intermediate in Haskell" I'm currently struggling with those two things right now. 1. In other language ecosystems there's plenty of scratch to finish tutorials on full projects. It's significantly harder to find that in Haskell. 1. It seems like a lot of the "beginner -&gt; intermediate Haskell" transformation comes from a deeper understanding of categories, typeclasses, and certain idiomatic libraries. And there's plenty of that out there, but its teaching style is catered to a certain academic audience, and I haven't found a great expert opinion on valuable, seminal resources.
Wouldn't having unlifted products by default also mean that you have to add a way to specify lazy constructors? Or am I misunderstanding?
A long time ago I was working on "modular Prelude" which encouraged the use of record wildcards as a form of faking a more powerful module system, in the spirit of the final example from this post.
Interesting - do you have any notes of that?
Is it because the pattern matches on them become irrefutable and require extra destructuring thunks? Perhaps evaluation order on products can be undefined? In other words, they can be treated as lifted when it helps the optimizer, or have their termination properties [un]refined when it helps. I am not sure I really want this to be well defined if the cost of this is the compiler failing to optimize many useful, finite things. 
You can refer to an unboxed value, or copy it. You could have a thunk that represents a wait for a different thunk to be evaluated and then copies it.
Programming is ultimately all about abstraction. Category theory is a toolbox full of abstractions which are *consistent*, *well-defined* and have incredible *power-to-weight* ratios. By "consistent", I mean they are similar in nature and always *compatible*. They are richly interrelated and fit together like puzzle pieces, which makes it easy to work with multiple ones all at the same time. This is particularly important because if you base your abstractions on category theory, they will immediately translate to and work with other people's categorical abstractions. (I mean, there might be some overhead in code for going between them, but the ideas wil be consistent.) By "well-defined", I mean they are specified at a level of rigor and precision common to mathematics but all too rare in programming. There are no ambiguities or forgotten details hidden away: the definition specifies *everything* about the abstraction. Since they are specified formally, there is far less place for mistakes or ambgiuities to lurk compared to the natural text in libraries and standards that you're used to. The most important part, though, is the "power-to-weight ratio". On the one hand, many categorical abstactions are powerful: they can express non-trivial things. If you have a monad, you can immediately use an extensive library of monad helper functions which are guaranteed to work for it no matter what. There is enough structure to be useful. At the same time, the abstractions are *simple*: they tend to be defined as a handful of elements and a handful of invariants (algebraic laws). A lot of the time, it's like an interface with only two or three functions which have to behave in a certain way. They can still be difficult to learn just because of how *abstract* they are, but at least there aren't too many details to keep in mind. More importantly, this also lets the abstraction apply in a lot of different places. This means that they're widely useful and, perhaps more interestingly, they can illuminate connections between things that seem completely different at first. Very valuable. So ultimately, category theory is useful as a suite of abstractions for organizing and thinking about your code, rather than necessarily helping with very specific domain tasks. If anything, I find this makes it more valuable because it's so pervasive and widely applicable.
It might, but I suspect the focus will be very different from a non-CS category theory course. The things that programmers find important tends to be pretty different from the things that, say, algebraic topologists do. But it should be good as a supplement.
Will you post the results of the survey?
I was looking for the constructor form of this last week and wondering why it didn't already exist. Which it does. Specifically because I was getting uncomfortable with applicative syntax for big constructors.
Yes Gabriel, I totally get what you mean. This is why we do not plan to focus on any choice of libraries in the first two course and on the third one we plan to concentrate on the big picture too with a lot of examples and case studies of bigger projects. You are right, we have to keep this in mind!
Yes, that's exactly it (and why seq on unlifted tuples requires concurrency). But personally, I like them lifted. It agrees with my (operational) intuition what triples are. 
Interesting point, I will definitely keep this in mind and will include bigger projects that the participants can finish at home even after the course. I agree that this is an important part in making from "beginner to intermediate" and then beyond.
&gt;I welcome any kind of title change suggestions "Haskell for the Industry" or "Applied Haskell" or "Industrial Haskell" all sound good to me. &gt;I don't think it's a good idea to be as modest to say that anything that is not publication worthy new science is not advanced at all. That's not what I mean. If you were teaching me the research, that would qualify as advanced. No need for any novel work. I would also consider non-trivial applications of things like like Impredicative types, Functional dependencies, GADTs, and Type Families to be "advanced haskell". RankNTypes would be borderline and closer to intermediate IMO. Even Fun deps can be kinda borderline, closer to intermediate-advanced (mtl). IT, FDs, GADTs, and TFs are often used in "advanced" Haskell to design type-safe APIs. Balancing type-safety &amp; explicitness against how inferrable your types are is an important property of API &amp; DSL design in Haskell.
But it means that `(a,b,c)` differs from `(a,(b,c))` :-( And thus we cannot generalize our code on tuples of any length. It also means that `f () = ...` differs from `f = const ...`. I think `seq` on unlifted tuples can simply be banned - if you really need it, you can always wrap the tuple in a newtype that is an instance of `Seq`?
&gt; IT, FDs, GADTs, and TFs are often used in "advanced" Haskell to design type-safe APIs. To be fair we have GADTs and Type families in the outline under "Advanced types" on http://www.nobleprog.co.uk/training/advanced-haskell But I agree with you, our target with these courses are people who want to create stuff with Haskell and we do not intend to target researchers. We are not a doctorate school after all. :) And yes, if we focus on the third course on existing APIs and ideas and building on top of them, then we will choose some title like yours. I like "Applied Haskell"...
The RTS stuff you cover looks good. I noticed mention of the "unsafes" - do you explain the semantics of IO WRT thunks and what unsafePerformIO does? In terms of difficulty / or how far along you'd expect the Haskeller to be when taking your course, it seems like http://www.nobleprog.co.uk/training/advanced-haskell varies a lot. You open up with basics like the typeclassopedia, but you touch on more advanced stuff than that by a fair bit. Do you cover polymorphism / RankNTypes? &gt;To be fair we have GADTs and Type families in the outline under "Advanced types" Are you teaching the bare minimum to understand how to read and use code that uses them or are you teaching how to wield them? Sadly, most blog posts about them are the former. If y'all are working up training material that touches on the latter I am going to be very excited.
The crash on exit bug was fixed in [version 0.3.2.1 of HsQML](http://blog.gekkou.co.uk/2014/12/hsqml-0321-released.html).
Yes, I'm fine with all of these things. :)
It would be easier to just format a nice PDF and toss it onto a print-on-demand site like [Lulu](http://www.lulu.com/home) or similar. (There are a handful of others; Lulu is just the one I've used.) The book might end up being a few bucks more than it would be with a full printing run, but Lulu would require minimal effort and still make a print edition widely available, with the added bonus of being easy to update if changes need to be made.
Good news... Now that everyone is on the "isomorphic" bandwagon (some code should run both on the server and in the browser); it would be great to have my Hamlet-templates run in JS-land as well.
I am pretty excited about smallArray# and the implications for unordered-containers :)
&gt; I really think Stackage would be useful in production. Why production? (and not development, and staging environments, for instance) &gt; I'm the only haskell dev in the team (well, we got other one a month ago), and I know what to do when something it wrong. Maybe it's a by-product of using Yesod, as then I find the dep-tree is so big that it regularly causes annoyance -- to myself, but even more so within the team. That's where Stackage really came to rescue for me. 
Downvoting because self-submitted content is against reddit's rules. /s I was wondering what you were going to do this year. Good choice!
Upvoting, because I like it and don't care about reddit's rules. 
(In case it wasn't clear, I also upvoted it.)
If the Devil is in the details, then God is in the initial conditions.
My problem with `RecordWildCards` is that given an identifier, a simple text search can't tell you where it came from. It could be from any of the three records you're wildcarding on in the current context. Because of that, I prefer `NamedFieldPuns`. It makes it explicit which fields you're reading, and from where.
I'm not
The old readme and bitrotted code is about all I've got. https://github.com/DanBurton/modular-prelude#readme
Can anyone explain why the template haskell code has to be executed on the target, instead of being run on the original machine?
I'm the one responsible for the Hacker News post... sorry! It's the first introduction to a dependently-typed language that pulled me in -- instead of me having to force my way through -- and I wanted to share the joy. Sad to hear it's had the opposite effect... and thanks again for your work.
I forgot about lulu; that makes the print/shipping trivial. 
&gt; fWorker = Worker &lt;$&gt; fWorkerName &lt;*&gt; fWorkerPosition &lt;*&gt; fWorkerFirstYear I always like formatting that as: fWorker = Worker &lt;$&gt; fWorkerName &lt;*&gt; fWorkerPosition &lt;*&gt; fWorkerFirstYear Which is fairly similar to the way I would do it for non-applicative syntax: cWorker = Worker { workerName = cWorkerName , workerPosition = cWorkerPosition , worketFirstYear = cWorkerFirstYear } The former is positional and the later is by-name, but it generally works well.
Would [ApplicativeDo](https://ghc.haskell.org/trac/ghc/wiki/Applicative do-notation) or [idiom brackets](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/idiom.html) come close enough? To clarify with the example from /u/bss03's answer: fWorker = [| Worker fWorkerName fWorkerPosition fWorkerFirstYear |] using idiom brackets, but that would force the ordering which is probably **not** (edit: forgot "not") what you want to do.
Yes, that's the plan.
I was just wishing for this the other day and it's not even Christmas yet :-)
Importing from modules without an import list has exactly the same problem. 
But isn't refering to an unboxed value the exact same thing as boxing it ? I believe my rough understanding of the subject might be completely wrong ... For example, if I have a big "unboxed" list, it should be represented as a big continuous chunk of memory for efficiency. Now if I want to alter the head of the list, all that chunk needs to be copied, right ?
I see no reason why the Applicative do-notation proposal wouldn't manage to turn: do workerName &lt;- fWorkerName workerPosition &lt;- fWorkerPosition workerFirstYear &lt;- fWorkerFirstYear return Worker{..} into Applicative form since it doesn't increase the power of Applicative, you can rewrite it to: (\workerName workerPosition workerFirstYear -&gt; Worker{..}) &lt;$&gt; fWorkerName &lt;*&gt; fWorkerPosition &lt;*&gt; fWorkerFirstYear As a sidenote, `alet` was suggested for a different use in the paper called [Fixing Idioms! — A recursion primitive for applicative DSLs](https://lirias.kuleuven.be/bitstream/123456789/376843/1/p97-devriese.pdf).
Agreed, except `hasktags` can work with that.^1 For record wildcards, tags will take you to the record definition which admittedly still gives you some information about the source of the identifier but is not as convenient. ^1 If you haven't integrated [`codex`](https://github.com/aloiscochard/codex) + [`hasktags`](https://github.com/MarcWeber/hasktags) into your workflow, you're seriously missing out. Try it.
Of course that's what it would turn it into. I just don't like the syntax because it suggests a kind of strict ordering of effects that isn't necessarily there with `Applicative`.
That's fair, new syntax is expensive though so being able to piggyback on both existing syntax and an existing extension is promising
Stop copying shitty software design from 1992 and consider a replacement. Why does a browser exist? Essentially because the OS level ~(presentation layer) is awful so they built an OS in an OS replete with task manager FFS and tab bollocks.
The idea is that if you need to refer to an already unboxed value for sharing, it can be indirected in the new position, potentially, and unboxed in the original position. Though actually boxing is more expensive than a simple indirection (the latter implies specialization for the indirected type). Also, you can copy the value if it isn't very large.
I encourage everyone to answer, even if you don't use stackage. It is a great initiative and helping them by answering will only improve it.
Ah I missed the specialization bit, thanks !
e.g., http://www.reddit.com/wiki/selfpromotion Also I was not being serious, but there has been a bit of a kerfuffle on reddit lately regarding self-submitted content.
Being more precise, [here](http://cerco.cs.unibo.it/browser/src/ASM/AssemblyProofSplit.ma) is one of the proofs that used to cause a problem, from line 100 to line 2601. The entire thing used to (I see from looking at it that some more work has been done on it since I last looked at it and it is now not a complete proof) take the best part of an hour to process and typecheck and pushed Matita to its limit. This was one of the main lemmas in the eventual proof of correctness of a concrete execution time preserving optimising assembler for an 8-bit micro-controller.
I understood you weren't serious, I just didn't realise submitting OC could be so controversial. One would think people could just apply common sense and it wouldn't be a problem...
I think it gave me 15% insert performance or so. I have some other ideas for more radical improvements (i.e. a completely new internal representation for the hash-array mapped trie), but that requires quite a bit of work.
Template Haskell and Hamlet have actually been working on GHCJS for a long time already. I implemented the out of process approach last July/August, but before that, GHCJS used the multitarget approach, loading native code for TH, see hamishmack's explanation of the drawbacks. What we're doing now is just integrating this back into GHC. A [patch has been accepted](https://phabricator.haskell.org/D501) to make the implementation more straightforward for GHC API clients, in GHC 7.10. The next step is figuring out how to best expose the functionality in a normal GHC. The main downside of Hamlet on GHCJS is that it still uses the same blaze / ByteString based backend as in native code. While this works, `ByteString` support is implemented with typed arrays, it's not ideal for performance and code size. I started working on a different backend around ICFP, translating the templates to DOM nodes (or virtual dom nodes) directly, but due to other work I haven't had time to finish/publish it yet. I made some plans to do a similar thing for blaze style combinators, but Hamlet was easier to start with, since you control the translation directly in the TH code. Eventually it should be possible to choose an optimized backend for JS, but still have the normal Hamlet implementation available, for example to find out whether a particular bug/problem is caused by a specific backend.
[Ariadne](https://github.com/feuerbach/ariadne) solves that problem, though I do agree with your point.
It's a bit silly. I suppose some people are hoping that if less content were submitted by its own creator, then at least there would be something of a filter because you'd only see content that impressed an independent reader enough for them to want to share it. But ultimately, that's a dumb way to do filtering. It's too arbitrary, and too easily gamed, and so has a poor correlation with actual quality.
I think the Haskell community throwing tons of code from different authors into one single namespace willy nilly is one of the most terrible aspects of Haskell development. I've been bitten by this (in others' packages) many times :-( That said, I think one record wild-card has this problem but it is much safer due to affecting a (usually) small scope.
I don't believe at all in that "exceptions in documentation". I use a library that stores data in a database. I need to check the documentation for the exceptions that library code may throw. OK. But the database layer code may also throw exceptions, so I need to know what database library is being used and go and read that documentation. Oh, the database is accessed over the network. What network exception can then be thrown? This is hell.
I don't get it. This says "exceptions are bad because you can catch them and swallow them, let's do it the Python way, you can catch exceptions there if you want". So you can swallow exceptions in Python too, what have we gained? The argument that developers prefer to write catch(Exception e){} instead of throws Exception seem a bit weak. Empty exception block can trigger warning in IDEs and compilers, no? It"s like the weak vs string typing argument "let's do tests and see at runtime what happens" except it's even worse. Not only are developers bad at imagining what kind of bad inputs may come in, now they also need to explicitly test for error conditions like "the network went down" to see what exception may be thrown....
iirc [company-ghc](https://github.com/iquiw/company-ghc) is able to do this.
Awesome, this looks great. For some reason I was unable to find it through google, I'll check it out.
Maybe, but Agda and Idris take different design decisions. Idris feels like a dependent Haskell in that it doesn't do much new besides dependent types, while Agda has things like modules which are awesome and simplify agda's syntax somewhat and instance arguments which are new and not as tested as typeclasses.
Yes, we discuss the different levels of unsafePerformIO and the problems that arise. About polymorphism (and RankNTypes): probably we cover some of it. BTW, the whole idea behind this corporate courses in small groups is that they will be tailored to the client's needs and knowledge level. About GADTs and Type Families: this also depends on the client. It would be good if we were to have actual API design discussion, but there is only so much one can fit into the two days.
On the other hand, that's also one of the great things about Haskell. It's not a monolithic blob of features – it's more of a simple base with a loose association of popular libraries from which you can pick and choose the parts you need. It does have its drawbacks though, I agree.
For some reason my Emacs autocompletion does not capitalize correctly. For instance I get `Overloadedstrings` (downcase `s`). Anyone knowing a fix perhaps?
You missed the `worker` argument in the last example.
Also encourages posting OC under sock puppets I guess.
Can any explain to me why template haskell is so hard to implement in a platform agnostic way? Isn't it just haskell code that makes new definitions, so why can't GHC just interpret this code when it needs to?
/u/ocharles has some interesting work on using virtual-dom nodes directly in his [Francium repo](https://github.com/ocharles/Francium/blob/master/Francium/HTML.hs#L158).
Well in principle one could achieve this by monomorphizing all functions in the program. But there are two challenges there: * monomorphization is a global program analysis (doesn't play nice with separate compilation); * polymorphic recursion.
With [GHCi-ng in Emacs](https://github.com/chrisdone/ghci-ng#using-with-haskell-mode) you can [just press `M-.` on the identifier and it jumps to the `Foo{..}` pattern](https://github.com/chrisdone/ghci-ng/blob/master/README.md#go-to-definition). 
I'm not sure I disagree but the conclusion doesn't really seem to follow from the premise. Types are imperfect obviously, and information contained in the type signature is not always sufficient to determine if the function is appropriate for a certain use. I'm not sure that any type system would ever have that property though, so we're left with attaching other bits of human consumable information ( documentation ) and using search tools like Hoogle or proof search techniques. Yes, the whole system is imperfect but what's the alternative here? In untyped languages you have far less information to work with and basically have to rely on documentation alone.
How does company compare to auto-complete? I use auto-complete which falls short for me in situations like the OP describes. As a side note, I also recommend looking at tools like codex or haskdogs which will download and create tags based on every dependency in the cabal file.
I've seen that when using company-mode without company-ghc.
thanks!
Suppose you want to generate different code depending on the endianness of the processor. The call that tells you endianness is evaluated during compilation, but it has to give you the same answer it would at runtime.
It'd be nice if TH had a "pure" fragment which could do no such thing. I feel like a majority of TH usage "in the wild" does not encounter such problems.
You still have the same problem, though. How do you find the right class for what you are trying to do? Documentation is the only way.
What about "people who have used these functions have also used ..."? One could mine existing code for functions commonly used together and make suggestions depending on your context.
That's pretty close to doable today in emacs: ghc-mod can type a fragment of code for you and let you know the modules in scope, hoogle can search for options, then use company-mode to display them.
Does this idea work with type inference though? 
I'm actually planing to do something like for my sublime text plugin when I get the time. The challenge to get this feature using ghci. My current plan is to simply put a `_` after the cursor, load the file into ghci, get the suggested type `t`, try and do `:t func :: t` for every function `func` in scope. EDIT: Looks like that doesn't work. Say `t` is `[a] -&gt; b` - then `:t length :: [a] -&gt; b` does not work. Instead we want to do something like `:t (Proxy :: Proxy ([t0] -&gt; t)) :: Proxy ([a] -&gt; Int)` (`[a] -&gt; Int` being the type of `length`). EDIT2: Turns out `:t (undefined :: [t0] -&gt; t) :: [a] -&gt; Int` is even easier. And replacing `undefined` with `(let a = a in a)` means you don't even need the Prelude imported.
 __ / \ | | ------------------------------------ @ @ | Hey it looks like you're trying to | || || &lt;--| ZYGOMORPHISM; would you like me to | || || | help you with that? | |\_/| ------------------------------------ \___/ 
I assumed assignCareerChip would use the previous value of worker. 
Absolutely! I've long been advocating a lightweight Haskell dialect for metaprogramming, more Hugs than GHC.
I feel that the Haskell culture has gone a bit overboard with all these different combinators. There seems to be a sweet spot of vocabulary size for both writing and reading code. For some reason, gluing together a million different opaque functions never gives me the same joy of hacking that I got from QBASIC with its handful of primitives. That's why I have a weakness for languages that encourage doing things "in idiom" instead of giving them dedicated names.
Except for when the method you want to use is located in a totally different class. Also primitive types don't do that well (at least in Java).
Wouldn't a `Q` monad lacking `qRunIO` be more or less sufficient?
 __ / \ | | ------------------------------------------ @ @ | :: (Comonad w, Functor f, Monad m) =&gt; | || || &lt;--| (forall c. f (w c) -&gt; w (f c)) -&gt; | || || | (forall d. m (f d) -&gt; f (m d)) -&gt; | |\_/| | (f (w b) -&gt; b) -&gt; | \___/ | (a -&gt; f (m a)) -&gt; | | (a -&gt; b) | | | | what's the problem? | ------------------------------------------ 
Ha, that's a good point. We ought to get around to renaming it UnsafeInt anyway.
The comment thing is weird the first time you run into it. From [section 2.3 of the Haskell Report](https://www.haskell.org/onlinereport/haskell2010/haskellch2.html#x7-170002.3) &gt; An ordinary comment begins with a sequence of two or more consecutive dashes (e.g. --) and extends to the following newline. The sequence of dashes must not form part of a legal lexeme. For example, “--&gt;” or “|--” do not begin a comment, because both of these are legal lexemes; however “--foo” does start a comment.
Thanks Graham, I am going to study it. Also thank you for writing Programming in Haskell. Together with Erik's MOOC it made a significant contribution to my understanding of Haskell
Certainly, and I always try to close my imports when I'm "finished" with the module, but while developing it's way too cumbersome to keep micromanaging my import lists.
Some advocate the view that a Haskell function takes only a single parameter. E.g. the type a -&gt; b -&gt; c should always be looked at as: a -&gt; (b -&gt; c) 
`undefined` has type `a` and so has `fix id`. That is they are of all types. There is no way to fix that without solving the halting problem or losing Turing completeness.
I was referring more to the general discussion of how laziness works in Simon's book. The relevant text is the start of chapter 2: http://chimera.labs.oreilly.com/books/1230000000929/ch02.html#sec_par-eval-whnf
No, to disallow things like `fix id` being of every type you'd have to be able to prove totality of all expressions which amounts to either solving the halting problem (which is obviously impossible) or giving up turing completeness for your language. That's what I meant. I'm not entirely certain that it makes sense, it's been a long day.
Can you explain the part about nested pattern matches? Do you just mean that with nested pattern matches you will evaluate past the whole expression's WHNF?
If only the `!` were postfix, you could have written `hello loud! = ...` ;)
And now for the hat trick: embed this beautiful work in Postgres itself as PL/Haskell. If only I had a magic wand.... 
If you do case myList of x:y:_ -&gt; ... Then you are forcing the first two cons cells. WHNF is just when the first cons cell is evaluated (or nil is evaluated, which is the other alternative).
'tis a pity ghc-mod doesn't work for me anymore.
No that does make sense, but I just thought that being fully total was at ends with being Turing complete.
&gt; There is no way to fix that without solving the halting problem or losing Turing completeness. This is not true. Assume we are working with a version of Haskell that doesn't let you define non-terminating pure computations. Then we can still use `fixIO :: (a -&gt; IO a) -&gt; IO a` to define non-terminating computations (giving us Turing completeness), but all such computations will only exist in `IO`, i.e. you can prove `forall a. IO a` but not `forall a. a`.
Totality conflicts with Turing-completeness because a Turing-complete language by definition allows you to write a program which simulates a Turing machine performing an arbitrary computation, terminating when the Turing machine does so. Such a program will not terminate on all inputs, and thus would be inexpressible in a total language. (You can, however, define programs in a total language which run a Turing machine for a bounded number of steps, or define a potentially-infinite stream of Turing-machine states of which only finitely many will be accessed during program execution.)
It's OK. You just have to look at this snippet for few days or weeks, and that feeling of weird will disappear. Even Haskell designers spent more than 3 years to get used to it before they decided to release the language ;)
I've used Propellor in a limited capacity. It's a very useful, well-designed tool that can do some pretty amazing things (I in particular love the way it handles DNS zone file generation) so long as you're fine with the assumptions it makes about your systems. As a simple but indicative example, the `Distribution` type (corresponding to which Linux distribution your hosts are running) [is defined as](http://hackage.haskell.org/package/propellor-1.0.0/docs/src/Propellor-Types-OS.html#Distribution): data Distribution = Debian DebianSuite | Ubuntu Release deriving (Show, Eq) I want to be clear that I'm not saying this is a _bad_ choice—I actually think that making such assumptions is one of the things that helps Propellor be as simple, clear, and easy-to-use as it is—but it is an assumption nonetheless, and one to be aware of when you start using it.
Forgive my ignorance, but isn't the desugaring incorrect? step (!s, !l) a = (s + a, l + 1) is equivalent to step (s, l) a = s `seq` l `seq` (s + a, l + 1) Isn't it? The bang pattern means that the patterns (usually just variables) marked with '!' are strict, not the expressions they're used in. Or am I misunderstanding?
Chokes on an error, something about can't find package-id. Usually some fundamental package like array. Reinstalling hasn't helped, can't find any github issues about my problem. Googling gives me nothing.
Is that like how monads are used in Agda or Idris for example?
Good to hear! Thanks again; it's a tremendous public service.
propellor --spin $HOST is supposed to update /usr/local/propellor on the remote host for you. This is generally done by pulling changes from a central git repository. My guess is you might not have such a central repo. A new feature recently added (in 1.0.0) is that propellor --spin $HOST can now ship the git changes to that host directly over a ssh connection, without using a central git repo at all.
Running propellor as non-root is an interesting idea. I have not considered it before, but it seems doable, with some tweaks.
Yes, you are right.
Ok, great. That answers my question. I think that specific host was having issues. Working great on a new host!
If this is just me then down vote to oblivion, but the calendar page of the blog doesn't seem to be being updated with the new articles. Also I'm really enjoying the articles, better than chocolate.
So they all show up in incognito mode. Probably some weird caching my end.
I'm pretty sure I've seen that. If reinstalling doesn't work, first try running `ghc-mod check` at a command line in your project directory to make sure it's finding the sandbox. Then try turning on debug mode (I think it's just a variable called `ghc-debug` you need to set to `t`) to see if there's any useful output there. Edit: I just remembered a way to confuse things. If you upgrade your GHC, it's easy to be left with a sandbox that has libraries for multiple versions. You want to verify that the `cabal.sandbox.config` points to the right one. If you want to be sure and have some spare time, you can always delete and rebuild the sandbox.
Link for the interested: /r/haskellgamedev: Discussion and resources for game development with the Haskell language. --- ^This ^is ^a ^bot ^and ^won't ^answer ^to ^mails. ^Mail ^the ^[[Botowner](http://www.reddit.com/message/compose/?to=DarkMio&amp;amp;subject=BotReport)] ^instead. ^v0.4 ^| ^[Changelog](http://redd.it/29f2ah)
Cross-posted to create a gathering point for anyone using Haskell for Ludum Dare 31.
Nope, that's it. I have a recent Dockerfile which has the lines (and not much more): ADD ./config config ADD ./static static ADD ./dist/build/app/app app 
I would have used the C preprocessor to expand to: *Main&gt; case 'A' of 'B' -&gt; undefined *** Exception: &lt;interactive&gt;:3:1-28: Non-exhaustive patterns in case then catch the error message in the `IO` monad (`unsafePerformIO`). Low tech, but possibly sufficient.
You want to program on android to what end? Just trying out functions? or full-on cabal projects?
That would be the Partiality monad; see e.g. http://www.cse.chalmers.se/~nad/publications/danielsson-semantics-partiality-monad.html
I am just learning haskell in the moment, but yes cabal would eventually be useful.
Couldn't you just create a hierarchy of types and abstract the problem to a higher level in the hierarchy? That's the work around for Russell's Paradox and Goedel's Incompleteness which are very similar to the Halting problem.
You can just put fix in a monad.
Currently (slowly) reading this http://math.mit.edu/~dspivak/teaching/sp13/CT4S--static.pdf and enjoying it so far. Anyone has any comments about this book?
You might like (&gt;&gt;&gt;) arrow operator which can replace (.) dot operator: map ( (2 * ) &gt;&gt;&gt; ( + 1) ) &gt;&gt;&gt; init &gt;&gt;&gt; tail [1..10] I think, that this way it is less confusing to people from imperative world. Dot operator is good if it sounds good: return . goodNight goodNight &gt;&gt;&gt; return 
Interesting that the bangs cause strict evaluation only after application of all arguments in the LHS, meaning that: step (!s, !l) a = ... Has different strictness than: step (!s, !l) = \a -&gt; ...
Another problem is that the interpreter works with bytecode, and bytecode is only available for the current module, or at best the current package. GHCi loads static or dynamic libraries for dependencies. A possible route would be to keep some sort of bytecode around for everything, for a generic TH platform (or perhaps a generic 32 and 64 bit platform). Some constructs, like unboxed tuples are unsupported with bytecode, so this would require lifting those restrictions. Also, very common packages like `bytestring` and `text` use foreign calls to speed up some operations, operand sizes of our generic Template Haskell platform might not work for those. What we'd end up with is a separate compilation path for the bytecode platform, which would end up with different definitions from the native target (conditional compilation in the `text` package to disable or adjust the foreign calls for example). Eventually I'd like to see this type of multiple targets supported in GHC, but right now it'd require much bigger changes than running TH on the existing target architecture, for which generated code is readily available. Since running TH on the target still has its own merits, for those situations where you actually would like to run a computation that depends on specifics of the platform, it seemed like the best starting point.
Have you seen https://ghc.haskell.org/trac/haskell-prime/wiki/BangPatterns ? There might be something good there, but it's somewhat annoying this article is full of "maybes" and "we could"s
Do they have different strictness?
`Just` starts with an uppercase letter, so GHC can't distinguish it from a module name, so it parses `Just.read` as `read` in the `Just` module (which does not exist).
I understand that, but I though it could resolve the "ambiguity"
How? What if there is a function with the qualified name `Just.read`, as well as a constructor `Just` and an unqualified name `read`?
No, `Just.read` is not the same as `Just . read` and not supposed to be.
How are you testing this application? Can you provide instructions so others can reproduce your result? Are you using optimization (`-O2`)?
bitemyapp has this link among a few others to learn about these things in [his guide](https://github.com/bitemyapp/learnhaskell/blob/master/README.md#laziness-strictness-guarded-recursion).
Ok. Thanks
 * I would likely turn `Model` and `PredLoss` into proper records. Tuples are suboptimal for a few reasons: * When used in a type-synonym like this you can easily substitute in any tuple of three `Double`s in place of a `PredLoss` and the compiler won't warn you * They aren't self-documenting: Records allow you to name and attach documentation to the fields * Tuple fields are lazy: When you have a package of values like `PredLoss`, you often want all of the fields to be evaluated when the `PredLoss` is evaluated. For this reason, strict fields is usually what you want. Records allow you to attach bang annotations on the field types. This reduces the number of bang patterns necessary in your implementation and will save you from numerous thunk leaks. * When I implement iterative algorithms (as your `train` does) I often just return an infinite list of iterates. This allows you to eliminate your `epochs` argument and enables the user to compute statistics on intermediate iterates. * The comments after the parameters can be haddock comments (using `-- ^ $COMMENT`) * As mentioned by /u/tom-md, you refer to the size of the input sample in several places, destroying your ability to consume in constant space. 
Author here. This is a bit too [soon](http://imgur.com/PeAWEsX), as not all documentation is ready yet. I am happy to answer any questions either here, or in #haskell-deployment on freenode.
If you're learning Haskell, I would recommend the most official desktop channel (haskell Platform), because you'll find by far the most support for it in tutorials, mailing lists, etc. etc. support for other platforms such as mobile are expirimental.
Yeah, with the latter applying just the first tuple argument forces evaluation of the bangs, with the former need to apply both.
I feel like it would be helpful to have a more concise how-it-works page (that jumps directly to the mechanism behind snowdrift). The [current intro](https://snowdrift.coop/p/snowdrift/w/en/intro) is a bit long. I wonder if it loses people. **EDIT**: Oh, after a little clicking, I see there's this [mechanism page](https://snowdrift.coop/p/snowdrift/w/en/mechanism)… which still feels a bit long :-). There must be some way to boil it down so it can be grasped at a glance. (Also: I notice I find myself tempted to use expressions like “snowball effect” and then hesitating for fear of clashing with the snowdrift dilemma based naming)
Features list reads like make. Is this just a makefile? What does it do?
Very cool! It also should be possible to annotate each usage of `Var` with haskell binding name (if any) to make the assertion even better: "assertion failed: Eq (Var "local1" (Just "r")) (Lit (Integer 54))". Then it can be pretty printed to "assertion failed: r == 54)"
If anything, Halcyon is what _cabal-install_ wants to be when it grows up. A rant titled [“Give me back my sanity”](https://gist.github.com/grownseed/4fd2e91eca829cc039de) was making the rounds recently. While the complaint is about installing Pandoc, it applies equally well to installing most Haskell applications. With Halcyon, you can go from zero to Pandoc in one command: $ halcyon install pandoc If an install archive is available for your platform, Halcyon will download and extract one tarball, and you will be done in 10 seconds — or your money back. Otherwise, Halcyon will build Pandoc from source, first installing all of its dependencies. And I do mean _all_ dependencies. No GHC? No problem, Halcyon will install it for you. No _cabal-install_ archive available? Halcyon will bootstrap it for you. Pandoc is actually quite simple to install, as it does not require any `build-tools` or `extra-libraries`, which _cabal-install_ cannot deal with. Halcyon can do more than that. For details, please see the [example applications](https://halcyon.sh/examples/), or the [“Hello, world” shootout](https://halcyon.sh/shootout/).
The current version of Halcyon is written in a GNU _bash_ DSL called [bashmenot](https://bashmenot.mietek.io), which attempts to prevent some classes of issues common when writing vanilla _bash_. Also, every Halcyon commit is checked by the excellent [ShellCheck](https://github.com/koalaman/shellcheck) static analysis tool. There is an unreleased prototype version written in Haskell, which indeed _can_ deploy itself. Perhaps next year.
That's a neat trick too! I avoided using an external pre-processor because it could change the locations GHC reports in its own error messages. 
Thanks for the suggestion, I may have to incorporate that into the actual plugin for Ivory :)
What about autostart? Don't you need systemd or upstart config files to autostart your app? 
Thanks for all the hard work on this Miëtek, Rehno tells me this is making our lives much easier! - Andrew, co-founder @ CircuitHub
How is this different from Nix? http://nixos.org/nix/ 
The tuple combined with bangs is muddying the waters a bit. Here are the separate desugarings. f (x, y) z = ... means f = \xy -&gt; \z -&gt; case xy of (x, y) -&gt; ... and f (x, y) = \z -&gt; ... means f = \xy -&gt; case xy of (x, y) -&gt; \z -&gt; ... Likewise g !a b = ... means g = \a -&gt; \b -&gt; a `seq` ... and g !a = \b -&gt; ... means g = \a -&gt; a `seq` \b -&gt; ... From here we can see why the first of each pair does not touch its first argument until the second is applied whilst the second of each pair has some strictness in its first argument before the second is applied. 
While it shares many of the same ideas, including pervasive hashing, Halcyon is much less ambitious than Nix. However, Halcyon does not introduce a new package format. Halcyon uses regular Cabal packages, and is compatible with existing Cabal repositories, including Stackage. Halcyon is also designed to support incremental development on platforms with limited resources, such as [Heroku](https://haskellonheroku.com). My explicit design goal was to be able to see the results of a `git push heroku master` deployed within 30 seconds.
I just used this a few days ago with haskell-on-heroku and while I didn't get everything to work perfectly, I am going to keep an eye out for this in the future, the whole layered approach to builds seems like a really good thing and I generally dig the approach. I am having some trouble understanding the terminology though, a "deploy" to me typically means sending something to servers, so `halcyon deploy` seems like it would be making some connections and sending data / instructions to a server, but I don't think that's what's going on? As the author said, this isn't really "ready" and the docs aren't there yet, so some confusion is understandable :P
Thanks for the kind words. While instructions for [Haskell on Heroku](https://haskellonheroku.com/) are placeholder at the moment, the Halcyon [user’s guide](https://halcyon.sh/guide/) and [user’s reference](https://halcyon.sh/reference/) are reasonably complete. Naming is difficult. There is an incredible overloading of terms used in the contexts of building, installing, packaging, deploying… `halcyon install` can be used on any machine, from servers to workstations. Currently, Halcyon only deploys to the machine on which it runs. This may be extended in the future. However, Halcyon does use remote storage for build artefacts — so indeed, connections are being made, and archives are being transferred. I would love to hear more about the issues you ran into. Please join us in #haskell-deployment on freenode, and I will get your stuff going. 
Thanks for letting me be a part of the 24 DOGE! I hope everyone likes the next couple of posts!
Thanks for the thoughts. First, I don't think there's any good solution to the problem of the mixed-metaphor. It's not the end of the world, and we just need to emphasize the main metaphor, and we can joke about the silliness of the mixed metaphor when it comes up ("let's clear the awful snowdrift so that we can get lots of wonderful snow…! wait…") As for the explanation, the *best* approach is *definitely* to let the system speak for itself, by improving the process at a pledge page such as https://snowdrift.coop/p/snowdrift and provide more clear stats / graphs / feedback about the impact of each pledge. We're working on those things. Secondarily, we have a goal to produce another video that better explains the mechanism directly. So, that explains our plans currently. Always room for improvement though, and we *love* constructive feedback, so thanks!
It is possible to use halcyon to deploy to your own server instead of deploying to heroku?
Absolutely. [Halcyon](https://halcyon.sh/) is completely independent from Heroku, and can run on any UN*X system. [Haskell on Heroku](https://haskellonheroku.com/) is the Heroku-specific bit.
Great! Thanks for your quick reply.
A small addition: RebindableSyntax allows you to overload more than do notation. For example, if_then_else, literals, monad comprehensions and arrow notation are also rebindable. Useful for e.g. DSLs. 
Edit: stm-containers is substantially slower than what I'm using right now. It makes the whole thing about 15% slower. Oh, I didn't know about that library. I'll take a look at it; thanks for the tip. Right now, I just have a Vector (TMVar (Map k v)). The vector is immutable, and the (atomic) map to use is decided by the key. This way, it's unlikely that multiple threads will try to use the same map at the same time, so there's going to be minimal TMVar contention. Looks like with stm-containers, I might be able to replace DataStore.hs.
The talk was given by Simon Meier at the HaskellerZ Meetup in Zürich on 28 Aug 2014. You can see the code [here](https://github.com/meiersi/blaze-react) and try out the demos [here](https://meiersi.github.io/blaze-react/).
Reminds me a bit of [Memcached](http://en.wikipedia.org/wiki/Memcached)
Congrats /u/myetech! This has taken an inordinate amount of pain out of the deploy process for us, deploying circuithub.com to Heroku. When kicking off a project on Heroku it's only natural that the first thing you're going to reach for is a buildpack... and this definitely set you off on a path of anguish with a "larger than toy" Haskell app. So, I am extremely happy to see this hit reddit (even if it is a bit earlier than intended)! The nicely modular approach with Halcyon being a separate library from Haskell-on-Heroku also seems really promising - I have a feeling that it's likely be quite helpful for anyone needing some kind of automated haskell builds/packaging or a generic installer, whatever it may be.
The fact that you've got it working trumps the largest objection, namely that it's just too complicated :) I fear the dream of a lightweight metalanguage is really a pipe dream. At some point you'd run into a limitation and wonder why you're not using the language you want to be using. I also think a lot of my complaints about TH stem from poor GHC performance, so that's an entirely different way to make progress. 
This is super exciting work, so please resubmit when you're happier with the state of the documentation!
Couple of comments; will update once I've finished reading the article: I like the choice of extension. I don't like that `NoMonomorphismRestriction ` isn't explained anywhere in the article. I also find the comparison at the beginning confusing, as you have to not only change the direction that you're reading the lines, but also realize that there are multiple columns! *edit 1*: The first example, even if deemed silly, is really compelling. So much so, that I had to stop reading to update this post *before* I finished reading the rest! *edit 2*: &gt; What if we want to do take a list and output a string? I'm guessing this should be &gt; "What if **what** we want to do **is** take a list and output a string?" Final verdict: Despite the minor griefs I have, excellent article. I'm glad Ollie invited you to post this extension. In fact, this is probably my favorite post so far. I also appreciate the plug. While I already knew about Bang, its mention served as the perfect example of how this extension is making your life easier as a developer in the *real* world.
devio.us has ghc 7.6.3, they are giving shell accounts for free.
I actually wrote this sample code a long time ago and don't know off the top of my head where `NoMonomorphismRestriction` is necessary; I can take a look at this and update that butchered sentence later. Thanks for the support, and I'm glad you enjoyed the article!
Write a `Functor` instance for `T3` and then `rewrap32 xy = (fmap fst xy, fmap snd xy)` Write an `Applicative` instance for `T3` and then `rewrap23 (x, y) = (,) &lt;$&gt; x &lt;*&gt; y`.
Is this extension not just a surefire way to write unreadable code? Are there any "out in the wild" examples of idiomatic usage?
I was thinking the same thing, however even in run-of-the-mill haskell one needs to know what monad is in play when using do notation to understand its behavior to begin with.
If you want, for instance, to use `do` syntax with an indexed monad then `RebindableSyntax` is your friend. That being said, there's a reason this isn't commonplace (moreover, things don't always [work as you'd expect](http://stackoverflow.com/questions/11042685/rebinding-do-notation-for-indexed-monads).
I think the set-monad example is idiomatic. At least, I wouldn't mind seeing it in code.
Incredible work! Kicks the 'lack of deployment tools' argument against Haskell on the web right in the head!
I'll even write out the instances: instance Functor T3 where fmap f (T3 x y z) = T3 (f x) (f y) (f z) instance Applicative T3 where pure a = T3 a a a T3 f g h &lt;*&gt; T3 x y z = T3 (f x) (g y) (h z)
Thanks. I am reminded of [“Haskell in Production: The good, the bad, and the ugly”](http://www.shimweasel.com/hs_gbu/). It’s time to get rid of the ugly!
Thanks, Andrew. It feels good to be able to help out a startup doing important work, in Haskell to boot.
OK, this is almost correct. Halcyon is smarter than Docker, but less smart than Nix. Halcyon re-uses previously built Cabal packages, but only at the sandbox granularity level. However, Halcyon tracks more types of dependencies than just Cabal packages.
Fantastic, I've definitely taken my fair share of snippets from your emacs configurations so thanks! I'm also glad to see the DevelMain functionality built-in as it's incredibly useful. One other small enhancement I recommend is to bind a key to `haskell-compile`. Then, you can jump straight to any error that arises in the corresponding compilation buffer. e.g. (global-set-key [f11] 'haskell-compile) (global-set-key (kbd "C-c e") 'first-error) I've tried deriving compilation-mode in interactive-haskell-mode as well to get the same jump to error functionality (without much luck yet). Not sure if there is a better way to do this. 
Its probably worth looking at how intra-application links are created in each of the frameworks, since I know that's one thing that Yesod makes typesafe (not familiar enough with the other frameworks to say). I missed this on first read, but this post is from about a year ago, so things maybe have changed since then.
Incredible collection, your tools are a great boon to productivity. Thanks for the awesome work!
I would really like to know how you have entities react to each other in this system. For instance - you want entity A to change color and start reacting to mouse clicks after entity B enters within 10 pixels of a certain position. My gut reaction was to create a component that allows you to store some effect on the behalf of an entity, but as it turns out it's rather difficult to nest effects of (Eff r ()). I ran into a "Context reduction stack overflow", which is a first for me.
I already had these instances, but it did not click how to use them :(
But is it web-scale?
inb4 pvp controversy
While rebinding for a specific case, as here, makes for an easy-to-follow example, in practice you'd probably be better off rebinding in one place only, for clarity's sake. For example, I've seen the `Set` monad done before as: class Return m a where return :: a -&gt; m a class Bind m a b where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b instance Ord a =&gt; Return Set a where return = S.singleton instance (Ord a, Ord b) =&gt; Bind Set a b where (&gt;&gt;=) = setBind -- as per the article ... The advantage of that formulation being that you can use the same definition of &gt;&gt;= for any standard monad too (its instance would just have no restrictions on `a` and `b`) - so you can use do-notation for more things, without having to rebind it different ways in different places.
Yes. [This section](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/bang-patterns.html#bang-patterns-sem) of the documentation describes exactly how the [Haskell 2010 Report](https://www.haskell.org/onlinereport/haskell2010/) would need to be modified to account for this extension.
Thanks, that looks like a much smaller amendment than I expected
If performance wasn't a concern, I'd rather see the set converted to a list, use some comprehension/`do`-notation and then convert it back to a set... working directly on sets is more to the point though.
Why does the article generalize the first function before using Hoogle to search for it? Plugging in the first, ungeneralized function gives `maybe` as the first result. That kind of fuzzy matching is one of the key selling points of Hoogle.
Using emacs as my Haskell IDE, I still find the indentation mode quite not satisfying. I have tried `simple-indent`, `indent`, `indentation` and now `hi2`. I will probably end up using `structured-haskell-mode`. Do you know if it plays nicely with evil ? Thanks for the post.
For some reason I can't see a lot of the text. For instance, in this line: &gt; This works well as a shortcut, so instead of writing lift, or lift . lift, or lift . lift . lift, we can write magicLift, and let it figure out how far to lift. ... I can't see "lift" except when I copy and paste it somewhere else.
Sorry to hear that. What browser are you using?
Google Chrome. I just tried opening it in an incognito session which I think disables all extensions, and the problem persists. [Here's how it looks](http://i.imgur.com/i2D62au.png)
Wouldn't that mean existing LTS installations could become API obsolete, something one might want to avoid? Edit: apparently that's not a primary concern, given that they plan on allowing new features. 
I can't reproduce it on anything I have, including Chrome/Linux and Chrome/Windows. Google finds many different bugs about webfont rendering under chrome, but all I've found should be fixed by now. Have you tried clearing the cache?
This sounds really great! I sure hope we can get to cycles of much, much more than three months though.
Is there a way to model joins where the type of the values depends on some heterogeneous schema and isn't some uniform value type across the whole table?
The README file says "pretty fast", but what about the benchmarks? I ran your test using GHC 7.8.3 on a 3Ghz i7, and I can't get Client.hs to exceed around 30k operations per second: time ./dist/build/KVClient/KVClient real 0m2.963s user 0m1.124s sys 0m1.638s I am assuming you're testing 50k stores and 50k gets, but I don't know Haskell very well, so I'd like to know if I'm wrong. EDIT: Found [this](/r/programming/comments/2ohjdf/kvstore_highly_concurrent_inmemory_keyvalue/cmn7ivv). I'm exactly right.
Sum types.
It looks like any type at all could be used as the values, so you could use some elaborate sum type or Aeson's JSON representation or whatever.
FWIW interactive-mode supports the usual compilation-mode commands. C-x \` to jump to the next error, or `RET` on the error line to jump to it. Though, flycheck has a superior UI over both this and compilation-mode, I think, so I'll be adding support for that fairly soon. 
Yes, I don't use any of the indentation modes. Just SHM and hindent. I'm not sure about evil support, I'm unfamiliar with its issues.
I think you could always just create a map indexed by a different key for joining onto a different column. Basically, if I have some data: myData :: Set a Then we can always: createUniqueIndex :: Set a -&gt; (a -&gt; k) -&gt; Map k a However we like and Gonzalez's idea should still work.
Could we get a summary compare/contrast of halcyon with the existing yesod and keter deployment? Briefly, it works like this: You type the command `yesod devel` in your source code folder and point your browser at `localhost:3000`. This creates a database and all required tables if needed, and you see your running app in the browser. Every time you save a source code change to disk, the app blinks in the browser and you see the new version immediately, with the existing database tables migrated if needed. You type the command `yesod keter`, and the current version of your code is hot-swapped to the production server. New connections see the new version immediately. Existing connections to the previous version continue to operate normally; the previous version is shut down only after all of those are completed. Database deployment/migration happens automatically. In either of the above scenarios, automatic database migrations do not happen if it's not clear how the migration should be implemented. In that case, manual intervention is required, and that's what you likely want. Compilation happens very quickly, because only modules that have changed are recompiled. While the `yesod` command is only supported for yesod apps, a keter server can deploy and serve any web app, even non-Haskell ones. You zip up your app as a tarball and upload it to an incoming folder on the server. The tarball also contains a simple config file - minimally with just the path to your app executable within the tarball, but much more can be configured. Your app executable needs to support a few environment variables so that keter can tell it what port to serve on and where to find the database.
Very cool. I always enjoy reading Gabriel Gonzalez's blog posts. Not only is the idea very interesting, but I feel like I improved my intuition about how hash joins work as a side effect.
Oh, I meant exactly what you said in your other post. By 'inter-application' I just meant URLs linking to your own site, rather than a link to google.com or something.
Ah, indeed it does. I hadn't noticed that command before. Glad I didn't go through too much trouble. Thanks!
Why would you rather see that? Other that the to and from conversion could, wouldn't it look identical? 
3 months is a very short amount of time to call it "LTS"
Understood. I think "intra" would fit slightly better the "inter" in this case.
I don't know. Forth is nice to play with and it encourages giving intermediate stack operations names. I think combinator based design is quite useful and in fact generalizes quite well to other settings where types aren't the focus. At the end of the day a lot of programming comes down to inductive constructions that end up being folded by some computation and you don't need types to program in that style.
Excuse my ignorance, but what does API obsolete mean?
Yeah, I think you're right -- it's pretty straightforward when the whole table's in memory like this. (Although I think you want something like `Set a -&gt; (a -&gt; k) -&gt; Map k (Set a)`, since there's no guarantee the function maps different `a` to different `k`.)
Yep good call, edited.
&gt; Snap didn't support overloading a route with multiple handlers. If a handler failed the request was rejected, it was not passed off for another handler to try. The first sentence is false (Snap can route just like happstack, using the Alternative instance), and in the second I think the author means to imply that this is a bad thing? The `route` combinator builds an efficient trie out of your routing tree so matching is `O(log n)` in the number of routes, whereas if you glue routes together using `msum` like in the happstack example, routing is `O(n)`.
Thanks for making it that much easier to spend more time in emacs. I'll probably never give up vim, but the case for spending more Haskell-time in emacs is just getting stronger and stronger. :)
Another fun read, thanks /u/Tekmo
&gt; Why such short support windows? The strawmen of three months between releases and a one month grace period are ridiculously short support windows. The reason I propose them is because- like I mentioned in the design goals- we want the smallest delta from how people work today. Right now, there is no concept of stable versions, and we're trying to introduce it. Starting off with a more standard support window of something like two years would be a radical shift in how library users and authors operate today. Three months is very short, but it's long enough for us to test the process. As time goes on, we should have serious community discussions on how long a support window we should have. (I, for one, am fully in favor of extending it well beyond three months.)
Yes, this is highly related to Scalding. I'm on Twitter's Processing Tools team, the principal contributors to Scalding, although I primarily work on other internal tools. Right now Avi Bryant and Oscar Boykin are working on a general intermediate language for relational operations and one of the target front-ends is Scalding. Oscar asked me for some input on some suggested relational operations and this post is an extended version of my original feedback. To answer your question: yes this is principally for joining by a primary key, although the primary key can itself be a tuple (i.e. correspond to multiple fields).
You're welcome!
Are Tables contravariant on the keys? Do they admit a Profunctor instance?
Ran into same issue. Seems to be a CSS/Font issue with fira mono. Its not installed on my system, but Firefox doesn't seem to fall back to monospace. Quick workaround was just deleting fira mono for the css rule locally
Without the `keys` field it is just `Kleisli Maybe`, which is both a `Category` and `Profunctor`. With the `keys` field I don't think any of those instances are possible, because they all seem to require a function of type: (a -&gt; b) -&gt; Keys b -&gt; Keys a ... which I believe is impossible in general.
How fast is "pretty fast" for you? Low tens of thousands seems like pretty standard fare to me. EDIT: Added request lumping. My laptop now gets 1,162,790 requests per second (over 34 times faster)!
Neat! I was once a heavy Scalding user, so some of the names and concepts looked familiar. Looking forward to see what comes out of this. (Drifting further off topic: there's very similar work happening in the Samza project right now at https://issues.apache.org/jira/browse/SAMZA-390 -- I'm sure they'd be delighted to have input from Twitter people.) And thanks for clarifying; that makes a lot of sense, especially given the context.
The problem with flycheck on reasonably large projects is that it can easily take up 5-6 seconds to finish a build cycle. Then, often, it starts a compile cycle while you're editing, and then you have to wait for two cycles for the squiggly red lines to move to their correct positions (or disappear) which can be quite confusing. If `ghc -fno-code` had generated the intermediate files necessary to allow it to be run by flycheck across the whole code-base, that'd probably solve the problem.
It's probably worth checking out [summingbird](https://github.com/twitter/summingbird), which is the closest thing Twitter has to streaming SQL. I also highly recommend checking out Algebird's [aggregators](https://github.com/twitter/scalding/wiki/Aggregation-using-Algebird-Aggregators) (basically identical to Haskell's "beautiful folds", which I summarize in [this post](http://www.haskellforall.com/2013/08/composable-streaming-folds.html)). They map very well onto SQL abstractions.
When reading the code, that probability based contention avoidance really stuck out to me. Its very cool. This is a great little project. So few dependencies makes it easy to understand what's going on.
Very insightful talk, thanks for posting.
That the API has changed, rendering newer applications unrunnable on an older installation of LTS Haskell. 
anything happening with this? *Also interested*
Nice exposition. A few thoughts: 1) The `Num` instance for `Keys` seems a bit gratuitous. 2) It seems like we should be able to implement a monad instance for this `Table`? 3) What is the advantage over `data Table k v = Table {def :: Maybe k, rest :: Map k v}` or something similar?
Sounds like a great idea. Also, I just wanna say how much I appreciate all the things you guys at fpcomplete are doing for the community :)
When I said "new packages" I meant packages that we previously not available, and therefor unable to break any API contract. I think you understood it as new versions on packages; which Snoyman outlines a procedure for already in his post.
That's right. You provide a way to render your application state to HTML annotated with event handlers. When triggered, the event handlers cause state transitions. For purely browser-event-driven applications, it's as simple as that. I should note that `blaze-react` is currently about exploring the design space - which is a nice way of saying that it's not ready for serious use. Do play around with it though. I think this represents a really nice way of writing web applications.
No. Docker containers do not run a PID 1. Docker is a process abstraction: the container runs what you tell it to run via the `ENTRYPOINT` and `CMD` directives. In this case, you might want: ``` ENTRYPOINT ["app"] ``` Then `docker run my-image` will run the app executable.
I'm not the author and the post is quite old so it's understandable why those are omitted.
As mentioned in the comments to Conal's post, there is an algebraic basis for talking about "nullary functions" (see, e.g., Mac Lane and Birkhoff's *Algebra*), but this is a different use of the word "function" from its customary use in Haskell to describe values of a type constructed by `(-&gt;)`. It's really best not to equivocate the two.
IMO such hyper linking is already out of scope for many of the frameworks being compared. Something like scotty is mostly useful in generating HTTP based API's so linking is mostly useless. 
It should be downloaded from google web fonts — I don't understand why it doesn't :/
It's not immediately clear from the text that `regularListComp` and `parallelListComp` are supposed to produce different results, I had to reread the intro several times to figure out that the author didn't just have the wrong idea of how list comprehensions work.
Are you sure your fiblike example works? You are accessing (tail (tail fibs)) in the computation of the 3rd list element ... looks like a blackhole to me.
No, you misunderstood me. If my calendar application is written against LTS after the `time-management` package was added, it will not build on the server where LTS was installed before the `time-management` package was added. In other words, the API on the server is obsolete, because the newer API (which includes `time-management`) has been made current, and some applications will be written for it.
In the algebraic view, a *nullary operation* on a set X is a function *f: 1 -&gt; X*, which amounts to selecting an element of *X*. More generally, an *n-ary* operation on *X* is a function *X^n -&gt; X*, where *X^n* is the n-fold cartesian product of *X* on itself (and *X^0* is defined as *1 = {1}*). (*Algebra*, Mac Lane and Birkhoff, p. 13) This is the closest thing we have to a formalization of "nullary function" and its application to Haskell is somewhat dubious. A value `x :: a` is isomorphic to a function `f :: () -&gt; a` that selects `x` only up to bottoms, etc. On the other hand, I think describing a value of type `Monad m =&gt; m a` as a "nullary function" is a dangerous false equivalence. Haskell functions are not C functions and we should not be describing thigs like `main` in a way that makes it seem as if they are.
What if app crashes? Will the docker restart it? Or do you have to manage it yourself? 
Perhaps I misunderstood your question. You do need something to start and manage the docker container itself. That something might be upstart or some other PID 1 init-like on the host, or it might be something like CoreOS. The docker image itself does not need to run an init-like. See also https://docs.docker.com/articles/host_integration/ 
 oldest :: Int -&gt; [Character] -&gt; [String] oldest k tbl = [ first ++ " " ++ lst | Character{..} &lt;- tbl , then sortWith by birthYear , then take k ] Where are `first` and `lst` coming from? Should those be `firstName` and `lastName`, which is what I would expect `RecordWildcards` to generate?
Just tried it in ghci and it works as expected. I think the thing to notice is that `fiblikes` is defined in terms of `fibs`, not itself.
Thanks! I think I got the probabilistic contention avoidance idea from http://haskell.cs.yale.edu/wp-content/uploads/2013/08/hask035-voellmy.pdf . See section 3.2. I believe that this is what the current version of GHC uses.
Take a look at the [redis benchmarks](http://redis.io/topics/benchmarks). If I do: redis-benchmark -r 1000000 -n 2000000 -t get,set -q -P 16 SET: 232666.36 requests per second GET: 288517.03 requests per second Where do you get the idea that 30k operations per second is standard? Which standard?
You can tell flycheck to only check when you ask it to, which is the setting I would use. I don't like that "check all the time" for the reasons you stated and also because I don't want to be constantly told that my code is broken when I know it is. It also depends what command flycheck is running. It sounds like whatever flycheck checker you're using is doing too much work. My experience with haskell-interactive-mode, which uses :load and GHCi + -fobject-code, is that checking a module, on average, across various codebases I've worked with, needs to take no longer than a second. Waiting longer than that for a single module would start to be hard work, and if you're used to "easily 5-6 seconds" then I think it's time to change your development setup (or maybe buy a new machine...). That would be awful. E.g. for a big module like hindent's Pretty.hs (big because it defines a lot of instances, which seems to be slow for GHC to compile): -&gt; :load "/home/chris/Emacs/packages/hindent/src/HIndent/Pretty.hs" &lt;- [2 of 2] Compiling HIndent.Pretty ( /home/chris/Emacs/packages/hindent/src/HIndent/Pretty.hs, /home/chris/Emacs/packages/hindent/src/HIndent/Pretty.o ) &lt;- Ok, modules loaded: HIndent.Pretty, HIndent.Types. (0.78 secs, 930447384 bytes) Even that takes under a second. If you're using template-haskell then there's an initial overhead for loading the libraries into the interpreter, but that happens just once. Subsequent compiles already have them loaded. Doing a "cold" compile with GHC would have the overhead every single time, though. -fno-code isn't an option in almost all work codebases I have because it doesn't support template-haskell, sadly.
Ohh, ok, overlooked this. In that case, it's ok, of course.
Well, to be fair, redis is written in C, uses request lumping, and is written by industry experts :) I also can't speak to what that test is doing. I would think that a redis benchmark written by the redis people probably generates quite favorable numbers. Redis is "very fast". This project is "pretty fast". 
Ah, I suppose I should do that, now that school's ending. ty
Hard to say. From my very casual understanding, low tens of thousands is about what I expect. I looked through my web history for "benchmark" and found something I read a while back: http://www.slideshare.net/tazija/evaluating-nosql-performance-time-for-benchmarking
The graph on page 28? That looks like it is for 1k messages, over ethernet, not for tiny 20 byte messages, to localhost. Have you seen this? http://yoshinorimatsunobu.blogspot.co.uk/2010/10/using-mysql-as-nosql-story-for.html
The server performs better over ethernet than localhost on my machine. The OS only has to do half the firewall rule lookups and read/write calls. I'm strictly limited by the kernel on my machine. And again, that guy is an expert using expertly-written software. I think you might be misunderstanding my intentions in writing this program. I'm not trying to revolutionize the database industry. This is just a fun demo project that I'm sharing.
I also suspect the general case is impossible, but you could provide a Profunctor instance when the values are Maybe, right?
Don't sell yourself short: Mediocracy is how we got mysql in the first place. Benchmarks are a great way to make software better, because it tells us (and others) what we think is important, and it gives us a way to measure our improvements.
Within three months I could get "good enough" by just not updating, most of the time. A year would be a minimum term for me to find something like this useful.
1) Yeah, I really wanted just a semiring instance for `Keys` (\*shakes fist at Prelude\*). The reason I wanted to go through the semiring intermediate (instead of using `intersection`/`union` directly) was to hint that the `Alternative` and `Applicative` instance for `Table` are interacting in the same way as `(+)`/`(*)`. In fact, you get these additional laws for `Table`s analogous to the semiring laws: -- x * (y + z) = (x * y) + (x * z) t1 &lt;*&gt; (t2 &lt;|&gt; t3) = (t1 &lt;*&gt; t2) &lt;|&gt; (t1 &lt;*&gt; t3) -- x * 0 = 0 t1 &lt;*&gt; empty = empty 2) I'm not sure if you can implement `Monad` for `Table` (I haven't tried). If I had to guess, the hard part is figuring out what to do with `keys` for `(&gt;&gt;=)`. 3) I think that would work, too. The only reason I picked the one in the post was that it was easier to implement the `Applicative` / `Alternative` instances. Keep in mind that the only reason I introduced the `Table` type was just as a stepping stone towards introducing the `Alternative` trick.
on 2) we can think of `Table` as `Reader` with some extra structure, and that should spell out how to implement such a thing, not that it seems deeply useful.
Yes. That is a typo. I'll fix it along with the formatting issues in a PR later :)
&gt;Mediocracy is how we got mysql in the first place. Haha, cheers to that. &gt;Benchmarks are a great way to make software better, because it tells us (and others) what we think is important, True. In this case, the most important thing to me is that this software is very easy to read and understand, which is why I'm benchmarking it in terms of LoC :)
That means a lot to me, thanks :)
/u/emarshall85 makes a good point but if I'm being completely honest, I just made a typo. :) The code in the post works but what I really intended to write was something like: fiblikes :: [Int] fiblikes = 0 : 1 : 2 : [ x + y + z | x &lt;- fiblikes | y &lt;- tail fiblikes | z &lt;- tail (tail fiblikes) ] ...which is probably what you thought I meant. This would cause an infinite loop without the third element of the list. I'm going to leave the example in the post alone, but this is really what I meant to write. Thanks for catching it!
I am not so sure about the utility of the parallel list comprehensions and the monad comprehensions. A powerful thing about functions such as map, foldr, zip, etc. is that you look at the code and you immediately see the intention of the author when you see these names. Zip is very telling. However, the parallel list comprehensions remind me a lot the imperative for loop style, where you don't see the intention at all unless you analyze the whole code. So, it's looping through a collection... is it mapping? is it folding? is it zipping? You cannot easily see (esp. because the difference between using "," and "|" in comprehensions is small and if somebody does not know that it's easily missed). The examples for monad comprehensions are not convincing either. Why not use what everybody understands (do notation)? Maybe I just don't appreciate comprehensions in general as a programing style. I don't use them at all.
For context, this is what he's referring to: http://lpaste.net/106833
For people not familiar with the term, "sum types" refers to something like `Either` (i.e. a type that has multiple constructors). [This post](http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/) goes into detail about why we call them "sum types".
Because Christmas is on the 25th?
 instance Category Table where id = Table All (\k -&gt; Just k) (Table j g) . (Table k f) = Table (???) (g &lt;=&lt; f) So close, but not possible determine the "defined" keys unless we can either enumerate "All" keys or run the lookup function in reverse.
&gt; if we were to join our "infinite" table against a finite table, we get back a finite table Inner join, yes. "Outer join", no (we will get an infinite table). That sentence was a little confusing coming right after the outer join example.
I'm still a beginner so I don't really have any input on this, but David Miani made a pretty good answer about this on Stack Overflow. You might want to check it out [here](http://stackoverflow.com/a/1016986/3949424).
Loose, incomplete, unordered list of things I might expect—(edit: to be clear, not all of these, but any of them might be a reasonable interesting conversation between intermediate Haskellers). I'm not sure if I think an intermediate Haskeller ought to know all of these things, but I would think that they should be able to quickly grasp and handle any or all of them. * Free/operational monads and knowledge about the Codensity trick (but perhaps not "reflection without remorse") * Transformer stacks in both `transformers` and `mtl` style * Total comfort with any kind of `fold` trick like "`foldl` from `foldr`" and using pairs * Total comfort with everything in `containers` and `unordered-containers` when and why to use them * Creation of complex transformer stacks involving state, effects, etc * Familiarity with libraries: * pipes/conduit * aeson * attoparsec * lens or lens-family-core * Could build a parser combinator library from scratch * Build your own monads and applicatives * Foldable and Traversable are obvious to instantiate and use * Embedded DSLs in initial style * At least one kind of capture-avoiding binding method * Basic heterogenous lists * GADTs enough to make a simple typed DSL or to make a balanced tree * Doesn't get confused by laziness, knows when to apply strictness annotations * Knows about and comfortable with both using and building their own RankNTypes * Basic concurrency, probably via something like `async` over straight-up `forkIO` and proper handling of async exceptions (although reading Simon Marlow's book should be easy and should fix this!) * Basic STM * Knows what a category is and why `Functor` is like a Functor * Knows what existential types are and how to work with them * Knows the phantom variable trick
Isn't it true that list comprehensions don't actually desugar to the do syntax in the article?
Somebody who has built something in Haskell qualifies as intermediate in my book, no matter how inelegant it is
I would agree that 'type-safe routing' is generally intended to imply a system where an algebraic data type is used to enforce a consistency between code that is generating URLs and the code that is routing the URLs. This precedent was set by the `web-routes` system that first appeared in Happstack and later in Yesod. I am not sure what term to use to describe the `dir`/`path` system shown is this blog post. While it does provide some automatic type conversion, it doesn't really seem that 'type-safe' as many errors are only caught at runtime, not compile time..
Free use and contribution to some of the most advanced libraries and techniques in Haskell. Massive applied category theory expertise. Low-level work/GHC contributions. Avid familiarity with new research in FP and probably personal contribution. I'd imagine that it's probably a 5 year commitment to study at least and that less than 10% of Haskellers are "expert". But this is just my personal definition.
I believe "SQL outer join" would be: data InclusiveOr a b = L a | R b | Both a b outerJoin :: Table k a -&gt; Table k b -&gt; Table k (InclusiveOr a b) outerJoin (Table k1 f) (Table k2 g) = Table (k1 + k2) (\k -&gt; inclusive (f k) (g k)) where inclusive Nothing Nothing = Nothing inclusive (Just a) Nothing = Just (L a) inclusive Nothing (Just b) = Just (R b) inclusive (Just a) (Just b) = Just (Both a b) I've added a data type `InclusiveOr` which expresses the fact that at least one value (if not both) will be present.
Contribution to projects is probably a bad metric, a lack of interest in contributing to open source projects doesn't really say anything about the person's abilities either way.
Given the number of GHC contributors is at ~45, I'd wager that 10% figure is too high. Not meaning to troll. Just reminded of the number of posts I saw for jobs seeking "expert" Haskellers and matching that against your definition of expert. Note that I don't argue with your definition. I'm not even sure I could consider myself a beginner in Haskell, let alone properly scope the definition of the terms "expert" or "intermediate" in conjunction with Haskell. More than anything, I suppose it just makes the learning curve feel that much steeper. In any case, forgive my ramblings.
I agree. I don't think I could reasonably create a complete definition of what "expert level" might be... but I do think that many of the people making contributions like I named are so skilled. Others may easily be too even if they don't contribute.
See [my other comment](http://www.reddit.com/r/haskell/comments/2olrxn/what_is_an_intermediate_haskell_programmer/cmofr1b). I think plenty could be a GHC contributor but don't for whatever reason.
Hello again, guilty self-awareness! I trust you got a good night's rest?
I've actually contributed a little code to Algebird; it's one of my favourite Scala projects, and I encourage anyone here who also writes Scala to check it out. I've never noticed the parallel between `Aggregator` and Haskell's 'beautiful folds', though -- thanks for pointing this out. Summingbird's also pretty cool, but the batch / realtime hybrid style seems to only work well for commutative operations, and that's always turned out to be a bit restrictive for me. (I have [my own horse](https://github.com/bkirwi/coast) in this race now, though, so I suppose I'm biased.)
I use Tasty and the reason for its existence has been explained to me as an actively maintained version of test-framework. As I understand it now, test-framework is actively managed again. I could be wrong about all of that. I'm interested in hearing the value proposition of HSpec as well.
I'm not sure whether my notion of "expert" corresponds *at all* to that which people are asking for when they want to hire expert Haskellers. I like Haskell for having such an incredible learning curve. It's a language which allows you entry into a fruitful field of learning. You can't likely study Python for 5 years and learn new things (not that you couldn't learn things in other fields or on other topics—SciPy is there, of course!) but there seems to be a smooth path from accomplished Haskell developer to top-notch FP researcher... if that's the path you want to take. To be clear, I think anyone who's even just "intermediate" according to my above definition would be more than capable leading the architecture of a new Haskell project.
As far as I can tell, they all offer roughly the same functionality. I like using HSpec because of the test discovery and the lightweight syntax, but I don't think I'd suffer much from switching to anything else.
This list is utterly bonkers. I agree with about 10% of it, and think the rest is shamefully bad advice. It mixes points that are almost trivial with things that are simultaneously hard to master and hardly ever useful. This is honestly the kind of crappy advice that has me paying little to no attention to the Haskell community any longer. A noisy subsdt of silly people has collectively gone nuts for abstractions that provide little value other than making them feel special. It is most disappointing.
Be the change you want to see. Provide a better list. 
I wrote a whole book about the subject, smart guy.
Do you plan on releasing a second edition?
This list is extreme. I'd say a strong intermediate Haskeller might know half of this stuff.
It's hard to get enthused about the amount of work involved, to be honest. I'd rather ride my bike or play with the kids.
What if I updated it for you, just to make the examples compile?
The current proposal *does* allow for new features to be added in point releases, so that's a given already. In other words, 2.1 will be backwards compatible with 2.0, but not forwards compatible necessarily.
The question of "in flux" is an interesting one. Are you asking for purposes of API stability? In that case, it doesn't matter whether the package has 17 new major version bumps in a month, because LTS Haskell will be shipping the same major version throughout the rest of that cycle. If, on the other hand, you're asking whether the package itself is well tested, I'm not sure if the total time in LTS Haskell is a good indication. There could be packages that were added to LTS Haskell 5 minutes after they were written, or packages that have been battle tested for years that simply aren't in the package set.
This is a good question, and exactly the kind of stuff I hope gets brought up in these discussions. I don't see any reason why this shouldn't be possible, though /u/kqr and /u/yitz are bringing up interesting points. All I'll say is: I hadn't considered this question previously, and I can't see any *technical* obstacles to adding new packages. The social aspects are- as usual- far more interesting.
There's a bit of a chicken-and-egg problem here, which I'm trying to break. We need to get people to contribute to this system, which they won't do if it has too high a barrier to entry. And we won't find out where the problems are until people start using it. And users will prefer longer tails of support. I'm hoping we can get some people to test it out initially and then quickly ramp up to a realistic support period. I agree that the utility of this at 3 months is less than ideal (though I *do* think it still has value). I hope that people don't avoid participating because it's not yet the best it can be.
Here's maybe an intermediate notion of an intermediate Haskell programmer: 1) You are given the names and signatures from Data.List, and the Prelude (IO excepted) and you can reimplement it from scratch, modulo performance. 2) You are given a working but inefficient implementation of a data structure such as Data.Map or Data.Text and you know how to use the tools at your disposal to generate at least one or two significant improvements. Everything else is just... more knowledge.
This is a pretty great offer. I'd be stoked to see an updated version.
Yeah, it's a good idea. 
Without the rider "should be able to quickly grasp and handle any or all of them" I'm certainly not an intermediate haskeller, by these criteria. 
Thanks - that gives me something to think about. I also figured out the "context reduction stack overflow" problem. It has nothing to do with embedding effects inside effects. Instead it seems to be a simple problem of the type checker only wanting to traverse N steps through type contexts. I guess by default my ghc traverses 21 steps into the stack and my Eff type had &gt; 21 components. My conceptual hurdle with the solution you provided is that for every combination of effects a possible reaction may have you must add another component. Let's say you want your MouseReactC to update an id's position - your MouseReactC would then be something like a MouseEvent -&gt; Position Which would then mean that your MouseReactC would be of type: type MouseReactC = Component (MouseEvent -&gt; Position) How would you encode an update of possibly many different components in this fasion? type MouseReactC = Component (MouseEvent -&gt; *) Maybe there's a way to do this with GADTs? I have to think a bit more...(after thinking) yes - I think there's no way to get away from adding a new handler for each component (as opposed to storing an effect itself). I think at least one way to make it work would be something like: data MouseReaction = MRPosition (MouseEvent -&gt; Position) | MRRotation (MouseEvent -&gt; Rotation) | MR... And then your system will have to handle matching and running the update. Thanks again, you've given me some ideas.
I consider myself an intermediate, but the link within the link you posted here was *excellent.* There are links within the links that are really great also: https://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems https://projecteuler.net/index.php?section=problems 
I wholeheartedly agree. Working productively with Haskell for several years without relying on GADTs or RankNTypes for anything, I'm not even sure all this stuff relying on GHC extensions should be called Haskell at all (for now).
Let's see this become more popular than Mercurial so it can co-opt the shorter name :)
Seriously? The guy wrote a book on Haskell and you're going after him for it not being a book about *intermediate Haskell*? Jesus tapdancing Christ. How many books about Haskell have *you* written? Actually, how many books *period* have you written?
&gt; Edit: apparently that's not a primary concern, given that they plan on allowing new features. I realised that already. But thanks for the clarification. :)
The fact that you can see the potential for improvement means that you have progressed beyond it. If you attained that knowledge as a result of building it, then the definition is still a good one.
I might get to intermediate level in a few years then. Good to know. 
Clearly, I will never become an expert. 
I stay away from GADTs and rank N types on purpose in production code. 
You're an intermediate Haskeller when you've have used Haskell to solve a problem of your own or someone elses. This is especially true if you were paid to do it.
Ok. So why Tasty over HSpec? I'm a little confused...
&gt; Familiarity with libraries: &gt; &gt; pipes/conduit &gt; aeson &gt; attoparsec &gt; lens or lens-family-core I think that's a particularly wrong definition. A good programmer should know about concepts and not individual libraries. In this case, this would probably be: * Iteratees * (De-) Serialization * Parsing * Lenses (at least the simple kind, with `data Lens = Lens { get .., set..}`)
Tasty wraps over HSpec. So I use HSpec "in Tasty". Besides HSpec, Tasty also wraps over other test-suite-libs; so you can use several test-suite-libs in one big Tasty-wrapped test suite, that has uniform command line options. I hope this explains.
I can give you something to do.
This post was catharsis given textual form.
2) probably would require understanding Array, right? Given that, I'd go for something a little more broadly applicable like "can implement typeclassopedia themselves".
beginner-ish here, would like to here your proposal :)
No worries. Good luck.
I'd consider anyone who can read any given Haskell package on Hackage, snoop the source to understand its guts, and actually manage to compose it properly an intermediate Haskeller. I'd consider any definition testing nonstandard knowledge of type classes as 'expert' rather than intermediate, as that stuff comes with time and obsessive reading/regurgitation rather than actually being able to just sit down and build anything with documentation/examples at hand. Composing a large non trivial transformer stack is hard for anyone, but if you can lay the rock solid structure of an app down in a couple days you're still more productive and went through more brain-cycles than any intermediate programmer of another language. 
This post comes out as bitter and arrogant, and I think it misses the mark (the whole discussion about private methods enforced through social conventions deserves a subtler treatment that "Weaksauce."). I understand we sometimes write to vent steam or express "pet peeves". But we don't have to *publish* the result. And we certainly don't need to link it to reddit. Mob_Of_One, is this really useful on r/haskell? Downvoted.
Thanks for the write-up..!
I'm not sure whether you're the post's author and whether "more like this" means more reddit posting of similar content, or more posts of this nature. In the second case, I think good posts can indeed come from the same idea. Translating Python code into Haskell block-by-block is an interesting exercise and I'm sure people would be interested. The trick to make the result pleasant to read is to be exceedingly fair, generous, to the program you're translating. Do mention the strengths of the Haskell version (that's kind of the point), do sometimes point out mistakes in the input source (if you're confident they're really are; otherwise just say you're not sure and changed for this reason), but otherwise just assume that the input source is great and write as you regarded it and its author highly. It's not about hypocrisy, it's about being able to do a comparison/critique without the negativity that unfortunately comes along.
My point of view is that you describe the techniques and benefits of typed FP well but there is an overtone of arrogance that detracts from the helpfulness of the post rather than augments. I'm sure you could change the tone and end up with a much more helpful article. 
He's the post's author.
tl;dr steelmanning I'll edit it when I'm in a better mood.
I use Haskell at work, why?
I struggle daily with impossible to maintain Python code. I am not happy about this. I refuse to pretend much of what they do is sensible, but I will edit the post some other time.
IIRC, I read somewhere on your blog that you are working on a book ?
Pfft, it's an easy way to farm karma - I just let others write the articles ;)
I started this a few years ago because it was common in the Perl community, which is where I started my programming career. I really enjoyed it over there, so I decided to bring it over to Haskell. It seems to have caught on in the FP community in general since (woohoo!) - with PureScript and Rust now having a "24 Days" series this year.
I use doctest. I think its method of listing an expression with its value is exceptionally simple, and that this method is best suited for purely functional languages because our expressions really do have a single value. So there is no need for a setup method to prepare any state, to create mockup objects, nor to assert anything about how the state has changed afterwards. The doctest library also supports quickcheck expressions, but that's not where the library shines. Testing IO code is another story. I'm also using doctest for those because I like its simplicity, but now I do need setup and cleanup code, so it's not as elegant.
I think that anybody that would understand `a . b $ c` wouldn't have trouble understanding a curried definition, provided you gave the type signature. But avoiding point-free is nice for documentation, just because you need to name a certain value. I don't feel that the style of Haskell used in this article is basically condensed pseudocode, because it's unfamiliar to many programmers. The stuff in the IO monad is kind of self-evident, but just after it there's this breathtaking line concatMap (++ "") . nub . map (pktLine . (++ "\n")) . foldl' (\acc e -&gt; if null acc Arguably it's best to conceptualize what's going on if one think about composing operations on whole lists, but for many programmers it works better it if name the intermediate values (in this case in the inverse order..)
Clearly my listing is non-comprehensive....
I was mentally referring to Oleg's "finally" tagless style. More directly, I just mean simple ones which just use normal ADTs and interpreters.
Interesting how "intermediate" in context of Haskell (by your definition, but also by some others in this thread) is a whole different realm of a programmer then the same in a popular languages like Java/Ruby/PHP.
I'm not very knowledgeable about EDSLs, but I think the "final" approach is the one described in this paper: http://okmij.org/ftp/tagless-final/JFP.pdf. The initial approach is just using normal algebraic datatypes to represent the syntax.
Adding to the list will not help me become an expert. :)
I try to stay with Haskell 2010 as far as I can. The one exception is MPTC and FD, which are sometimes too useful to avoid. Furthermore, I use extensions to Haskell 2010 that I regard as just syntactic sugar, e.g., TupleSections. What I try to avoid is extensions that are a semantic headache, e.g., GADTs. Don't get me wrong, I like cool features as much as anyone else (I suggested GADTs 10 years before GHC got them), but for production code I think it's best to stay very conservative and avoid too much cleverness.
Sorry for necroing an old thread, but consider using git with just the local repo by yourself even if the rest of the team doesn't. Keep your versions easily in check and commit changes from the "the world" to at least separate them from your own.
I understand that this is just a first pass -- and I hope it gains enough success to go for round two :)
Why? Edit: Ok, fine, fuck you all. Sorry for trying to learn.
Yes, oops!
&gt; that they personally are interested in and already understand
&gt; MPTC and FD And the award for the laziest person on the thread goes to…Lennart! \*Applauds* 
Me neither! :) I think parallel list comprehensions have a place, and my guess is that monad comprehensions are more powerful when you're able to make use of the `mzip`, `mgroupWith`, and `guard` type expression translations. I wrote this specifically to inform people that these things exist while showing a few examples (which probably aren't indicative of the strength of the extension). I would love to see some real world usage of `MonadComprehensions` if anyone has some sample code out there.
If it's not in a Haskell standard, it's not Haskell. Just like GNUC is not C.
I'd argue there's a difference between knowing all the Haskell tricks and techniques, and applied category theory, and being able to lead the architecture of a new project--Haskell or not. There's a trade-off between using the power of the advanced techniques (e.g. GADTs) and the simplicity and accessibility of the code.
This, so much. Hope to get to this point soon 😊
I had a Python job and the only way I could use Haskell was to quit. I feel your pain! :| 
&gt; I think the rider was the most import part of the post Yes it was, and unfortunately most people seem to have missed it! I guess big bulleted lists always stand out more prominently than clarifying remarks above them. 
I think I was never quite clear, but I certainly meant for these lists to be more `Either` than `(,)`.
What do you mean by “nonstandard knowledge of type classes”?
Does anyone know how the diagrams were made?
I am going to be more obvious about what I was getting at because I must not have been clear. bos was complaining about the list he commented on. He provided a number of (what I found to be) valid criticisms. However, his comment was not specific when it came to the parts he disagreed with. For example, he said 10% of the list was fine, but didn't specify which 10%. He said some parts are trivial and some are hardly ever useful, but again, didn't specify which. My comment was prodding him to get specific. His responded to my comment not with a better answer to OP's question, but with a comment to let me know that he wrote a book about "the subject" which in context would mean "what it means to be an intermediate Haskeller" what which he meant to be Haskell in general (I'm assuming he is talking about RWH here). I then point out that his book (or at least the parts I read) talks about Haskell in general, but does not answer OP's question, which is all I wanted him to provide. So in from reading your comment I gather that you have missed the point of my posts (though that is probably my fault, which is why I'm rewriting this). I don't want him to write a book. I just want to know what HE considers to be intermediate Haskell; I believe it would be more helpful than simply a criticism of another person's opinion. A list of bullet points would be perfectly fine.
It does, thanks!
Great idea. Why not make this a community project? Set up a github repo or some such and I'll definitely be making pull requests.
Don't you ever worry running out of it! Martin-Löf is there for example.
It is Creative Commons licensed so this would be doable, but we'd need to get the source code from somewhere.
No requirement of understanding `Array`. `Text` would take some knowledge of the FFI, but that's a good thing. The point of 2) is to demonstrate an ability to analyze and optimize code, not just to implement it.
That's an interesting offer, thanks. Let me think about it and get back to you.
I agree with you. I think the tone was quite insulting and not helpful. However I think the sentiment behind it was correct (although that does not excuse the tone). We shouldn't stop people criticising Haskell just because they are "from a distance". The people who are put off Haskell should be listened to. I expect you were just criticise the harsh tone more than anything else. If so, ignore this!
&gt; It's really great that I can have IO encoded in the type signature, but that was never been a selling point for me. Pure functions are easier to test, verify, and reason about. Purity supported by the type system *is* Haskell's biggest selling point, imo. So I can see why someone who doesn't care much about it would view Haskell as not very special.
Tests.hs: main = do test1 test2 etc 
&gt; What I try to avoid is extensions that are a semantic headache, e.g., GADTs. Are you describing Haskell as you like to use it, or Haskell as you like to implement it :-P
I love doctest but it's super easy to write tests which are in fact never run.
I like to think of IO as the backbone of an app. I've been doing Snap and handlers exist in a MonadIO. So IO is the backbone. A human's backbone is pretty darn important, and the "core" of the app - threading its way through most of it. But then you have other stuff to make the thing work. Flesh, organs, etc each do their job. Pure functions fit onto the IO backbone and do their work. Sometimes they're whole subsystems with their own monad stacks, sometimes they're small self contained functions. Anyway, not sure why I typed that, but that's how I've come to see programs. IO pushes through a ton of the program, but isn't the program itself. The program is all the stuff you hang off the side.
I've not had chance to watch the talk yet, so I apologize if this was already covered. What support is there for reusing vanilla Reactjs components from a blaze-react application? What about the other way around: using blaze-reactjs applications from a vanilla reactjs component?
I don't see a link to the underlying lhs source. Is it available somewhere?
I can bang in a nail well enough with the back of a screwdriver; nonetheless, I generally reach for a hammer.
Great, then let's all write all of our stuff in assembly!
I've posted my intro on this subreddit before (at https://www.reddit.com/r/haskell/comments/2mjnvq/a_small_introduction_to_haskell_looking_for/). I've revised it such that it explains Haskell more naturally instead of rushing past the details. Hopefully this will help programmers understand more about Haskell. :) As always, I'm looking for feedback and am willing to learn. :) EDIT: Updated to revision 3.
I'd lambda encode it: var left = function(x) { return function(f, g) { return f(x); }; }; var right = function(x) { return function(f, g) { return g(x); }; }; Then case analysis is just: result = x(function(leftCase) { // ... }, function(rightCase) { // ... }); Of course, you might get some funny looks from your colleagues.
I love: type b &lt;~ a = a -&gt; b for hilarity, and type a &gt;&gt;- b = forall m . Monad m =&gt; a -&gt; m b to show off how `&lt;=&lt;` is like `.` for Monads. Can do something similar with Comonads.
It could be useful to indicate where some of the short var names come from (`InR`, `unK` ...). Unless it is a strategy to force the reader to go and find out by reading the linked functional pearl ;-)
I agree, it's easy to forget the "|"! That's one of the reasons why it's important not to skip the "red" part of "red-green-refactor".
How so? Do you mean forgetting adding the file to doctest? I rolled my own doctest discovery function for that. My Doctest.hs just walks through all sources and runs doctests, so I don't add files manually.
As gelisam said forgot the "|", add a blank line between 2 tests etc ... Things we doesn't generate any warning and are hard to spot.
I edited my comment, hopefully that helps.
I'll agree that putting effects into the type system is really helpful but I think it is also a bit of a blunt instrument. You can have all these great reasoning tools if your code is pure, but as soon as it needs, say the system clock and nothing else, there is nothing to distinguish that call at the type level from one that launches the missiles. This is what keeps me interested in things like extensible effects, or how Purescript encodes its effects with row types.
private message sent.
It looks like callbacks, seems pretty natural for JavaScript :)
From personal experience, choose something that is active when buying into open source. It usually happens that the problems don't uncover themselves until 8 months down the line when its far too late
I often stumble on these "lang doesnt matter" arguments. When translated to the physical world the fallacy of them becomes sooo clear... It is very hard to: Make good food with bad ingredients. Make a A-quality machine by only using worst quality ingredients. Create a great football team from novice players. Etc. Etc.
I don't know, given the choice between some nice clean Haskell2010 or deeply GHC specific code that's using/abusing a ton of corner cases of the implementation. I'd choose the former every time for code that I'd have to maintain and keep stable in perpetuity. 
Could use pwstore-fast or http://hackage.haskell.org/package/bcrypt
I use Python and Clojure at work. It's total nonsense. Double-digit percent of my coding hours are lost to tracking down type errors in an inefficient manner. I can't use functional patterns in Python and Clojure because they're misbegotten, broken languages. You can't even really do FP in a practical way in Scala ([don't](http://www.reddit.com/r/scala/comments/1tijwv/a_review_by_edward_kmett_of_functional/ceiq90i) believe [me?](http://www.quora.com/Reviews-of-Functional-Programming-in-Scala-2013-book/review/Edward-Kmett)), let alone untyped languages.
I like how he referred to Homotopy Type Theory as Homeopathy Type Theory
File an issue about releasing the latest onto Hackage, see how long it takes for them to 1. Respond 2. Push the damn release
I also use SHM and evil but I haven't touched the config for either in a while so I'm not sure how readable it is at the moment. https://github.com/phenguin/dotfiles
Yeah, I know Per, but that doesn't make me a Haskell expert. :)
I've implemented a lot more than I would ever want people to use. :)
Best reply yet!
Well this turned out longer than I intended. I think this is a nice introduction so don't take the number of comments the wrong way :) I tried viewing the introduction through the eyes of a newcomer that tries to understand and run code without reading ahead: * A word on functions and why they are important. * Consider showing a *complete* function definition before covering function signatures. * Redirecting `echo` is a bit clunky. * “When we use the right arrow notation, we denote a function that takes in all parameters except for the last, then returns the last.” could use a rewrite. * There is a massive leap from `add` to `pows`! The only explanation of `pows` is the section title and the type signature. Keep in mind that lists haven't been covered. Before showing what `pow` does you mention lists and `(:)`, the definition of a recursive higher-order function `iterate`, parametric polymorphism and type variables, operator sections, partial application, anonymous functions and laziness. That's a mouthful for an introduction that doesn't assume knowledge of FP, this may blow the reader's mental stack! One alternative is to cover some basic operations (and their types) first `[1,2,3] ++ [10,20,30]`, `map addByThree [1,2,3,4]`, then infinite lists: `take 10 [0..]`, `take 10 (map addByThree [0..])`, then operator sections: `map (+ 3) [0..]` and then you can introduce an equivalent definition in terms of the more fundamental and simpler `map` function: `pows base = map (base^) [0..]` which may suit the target audience better. * A similar issue with `fibs`. This may be a matter of preference. * Pattern matching could appear sooner in the introduction with simple functions like `length` or `map` to illustrate them. I would personally like to see more interactive use (how to define functions in GHCi, basic commands like `:type`, `:load`, `:reload`) and examples of usage, but that's up to you :) Prelude&gt; let plusThree :: Integer -&gt; Integer; plusThree x = x + 3 Prelude&gt; plusThree 10 13 Prelude&gt; :type plusThree plusThree :: Integer -&gt; Integer Prelude&gt; (+ 5) 10 15 Prelude&gt; (\x -&gt; x + 5) 10 15 You can also emphasise modularity and laziness (“Lazy evaluation is perhaps the most powerful tool for modularization in the functional programmer’s repertoire.” from Hughes' paper and the “Reuse” section from [Lennart Augustsson's blog](http://augustss.blogspot.co.uk/2011/05/more-points-for-lazy-evaluation-in.html)). Keep up the good work! (`\x -&gt; pos * x` should be `\x -&gt; pow * x`)
I might not believe you... But I sure would not dare you question Edward Kmett's opinion! :-P
Yep, saw it - sorry. Work has been nuts lately. I'll reply tonight.
You joke but that style is actually used in [The Algebra of Programming](http://www.amazon.co.uk/Algebra-Programming-Prentice-Hall-International-Computer/dp/013507245X)
Used any libraries from Hackage lately? I'm not going to name names, because I don't believe public shaming works or is the right thing to do, but abandonware is really common on Hackage. Often very little package maintenance hygiene, little/no documentation, etc. Hell, I've written packages guilty of this in varying degrees, but I have the excuse that I am writing a book and have to do all this stuff after hours. Including the book.
Yes. Are you suggesting that this problem is more common among hackage library writers?
Just in terms of presentation, take a look at this: http://peter-murach.github.io/github/
&gt; I'm not going to name names, because I don't believe public shaming works or is the right thing to do, but abandonware is really common on Hackage. Often very little package maintenance hygiene, little/no documentation, etc. Looks like you've edited this part in so let me address this in a second comment. I don't think the problem itself is anyway unique to haskell but is somewhat exacerbated by the upper bounds zealots (some of them at least) letting things go stale and not managing their beloved upper bounds more proactively. I think the solution to both problems is to simply use a curated source - stackage. If a package is not on stackage then a double take is needed to see if its being maintained as well as it should be.
In terms of presentation its a giant picture of a unicorn and it doesn't explain why i'd want to use it or how and i don't see how documentation with a unicorn is better than one without (aside from the obvious).
I agree with you that it isn't a particularly hackage problem. Its just the case when there are a _ton_ of libraries released over a _ton_ of years that some will fizzle out and others won't. I think we'd be much better served by improving hackage with more features and metrics to help wade through all the packages out there, rather than place the blame on everyone that does us the _enormous favor_ of releasing code for us to use and enjoy. In any case, I'm of the school that you should only use any library you're comfortable with reading the source of and potentially maintaining yourself :-)
Why all the distracting colors? What does a unicorn have to do with a gem to interact with the Github API? The presentation is detailed but it is very unprofessionally designed and does not seem to encourage contributions in any way.
&gt;I think the solution to both problems is to simply use a curated source - stackage. Stackage encourages/enables not following PVP, which is problematic. I like the other aspects of it. &gt;by the upper bounds zealots Oh I see how it is.
&gt;you should only use any library you're comfortable with reading the source of and potentially maintaining yourself :-) Well, there goes 99% of programmers. I'm comfortable with that, you're comfortable with that - most are not.
A little panache goes a long way in imbuing confidence. 
&gt; I'm not sure Haskell has that critical mass, and if we do it's not evenly distributed across all of the hackage libraries. I'd tend to agree. We need more users and more hands on deck all around. I think you've diagnosed this well.
Just being able to sort search results by date of last upload would really help
Tone arguments don't really add to conservation either. If you disagree with bos, say why you do and explain your reasoning. All he did was critisize a philosophy that some people in the community espouse. Abstract ideas should always be a free topic of debate.
Pandoc is better maintained and loved than most. Issue tracker zero would be nice, but isn't the most important thing in the world. Vibrant OSS projects usually have a voluminous issues list and pandoc is probably the most popular Haskell Github project. Pandoc has a lot of documentation and examples compared to most Haskell libraries - that helps a lot.
Emphasis, on the "no example of usage". I'm willing to put up with a lot to use library code but having to reverse engineer an entire library and read the entire source code just to get something up and running is mentally exhausting when I just want to focus on the problem. A single example can go a long way and save many hours of frustration. I get really worried about the future of Haskell in industry with people assert that reading the source should be the default assumption.
That's a great suggestion! By the way, the source of hackage is on github (https://github.com/haskell/hackage-server), and there's an active freenode irc channel for #hackage. Patches for features such as this are very welcome!
&gt; I'm willing to put up with a lot to use library code but having to reverse engineer an entire library and read the entire source code just to get something up and running is mentally exhausting when I just want to focus on the problem. A single example can go a long way and save many hours of frustration. Very strongly agreed. &gt; I get really worried about the future of Haskell in industry with people assert that reading the source should be the default assumption. Sadly, this attitude is something i regularly encounter from other programmers, who seem to feel that it's unreasonable for them, as experts in the code they've written, to provide basic usage examples, but eminently reasonable for code *users* to have to spend hours trying to comprehend that possibly-complex code and making rough guesses as to how to actually make use of it.
Lets at least *try* for a higher standard rather than simply giving up. We have a nice language which most of us believe makes us more productive. Letting at least a little bit of those productivity benefits translate into making things nicer for users (docs, examples, etc.) would be a good thing to strive toward.
I think we're talking at cross purposes. You're describing a solution to make actively maintained packages more accessible. I'm describing the problem that many packages just won't be actively maintained. Meanwhile the linked article tackles neither problem, but just maintainers being insufficiently responsive by some standard to pull requests.
&gt; This is honestly the kind of crappy advice that has me paying little to no attention to the Haskell community any longer. A noisy subsdt of silly people has collectively gone nuts for abstractions that provide little value other than making them feel special. It is most disappointing. What would you like to see more of, hold you interest, etc?
Yeah, I meant inner join
I am not sure if I can add anything of value, that Haskellers who are better than I have already stated. I'm not sure that as others have suggested that library lists are the measure to go by. Especially since, library documentation has traditionally been a weakness in the community. I think overall this is improving. What about gaining more than a fundamental understanding of types and how they operate in the language. Seems to be the biggest stumbling block that deters people from the language. Maybe the next step is scaffolding your program with only type definitions once that compiles, then filling in the functions with actual code. I think that library familiarity in this case is a by product of reaching or obtaining this level of comfort with the language. 
Is it broken? I had no idea! That's the first report I've had on it since forever. You're right that I needed to migrate my repos when patchtag went kaput. I got halfway there and then was distracted by other projects. Thanks for the report, and I'll get on it :-) (note: I didn't add it to stackage and I don't know who did)
We don't have a style that necessitates reading the source, and many fine packages have great documentation. Some don't. Yawn.
Thank you :)
Please use scrypt, it is more secure than bcrypt. https://hackage.haskell.org/package/scrypt `encryptPass :: ScryptParams -&gt; Salt -&gt; Pass -&gt; EncryptedPass`
I'd maybe modify this to say "You should only use any library you're willing to learn from". Case in point, I contributed a small set of patches to [Aura](https://github.com/aurapm/aura) and learned a lot about Haskell through the PR process. Though I clearly had less experience than others contributing to the project, rather than get "Wat?" or "You gotta be kidding me!" responses, the spirit of my contributions was commended and I was gently nudged toward more idiomatic solutions. In short, the project improved, and I improved as a Haskeller (despite having regressed in that regard over the last couple of months). 
Some technical notes/corrections: * The logics type systems encode don't correspond to "provability" any more than any other logic with a sound and complete proof system (relative to the intended semantics) does (indeed, dependent type theory has exactly the same ability to internalize its own provability predicate--that is, to allow meta reasoning within the system--as does classical arithmetic). * We have known how to interpret classical logic as a typing scheme for a programming language since Griffin 25 years ago, and indeed, full first order logic + ZF set theory has a propositions-as-types interpretation due to Krivine. Pure (as in total) functional languages correspond to intuitionistic (or minimal) logic but the propositions as type correspondence is a much more general result. Sanity for a logic from a proof theory standpoint is a cut elimination procedure, and things with cut elimination procedures are equivalent to programming languages.
I've never had to reverse engineer a lib from source except to fix a bug or add a feature. I don't know what libs you're talking about.
I would love to have 168 pull requests, but I imagine it comes with its own meta-issue (too many fixes). Best bet to get a pull request looked at might be to help the maintainer fix this meta-issue and clear the backlog. Blog post author could * Help improve the CI system * Improve tests * Code review of some of the pull requests * Test some of the pull requests * Ask the maintainers what else might be useful
Actually, I just cabal installed it fine with the latest platform. Is there an actual break, or are you just pointing out that I need to update the repo and improve the documentation? Sometimes, when you build a package with minimal deps, it just lasts!
Are you kidding? You don't depend on any of Edward's libraries which is this giant transitive graph of undocumented code. Good documentation on Hackage is overwhelmingly the exception and not the norm. 
Which of ed's libraries? You mean `lens`, which has more tutorials than perhaps any element of the Haskell ecosystem except monads? Or `ad` which is very well documented? Or... which? (On the other hand, if you want to understand e.g. `profunctors` I would suggest no amount of "examples of use" will help you -- reading the code to something like that or `hask` sort of _is_ the point)
I used this style in MHask to emphasize how duals are really just the same thing with the "arrows flipped". http://hackage.haskell.org/package/MHask-0.3.0.0/docs/MHask-Monad.html http://hackage.haskell.org/package/MHask-0.3.0.0/docs/MHask-Comonad.html
&gt; If you can't infer how to use charset off of the types of the main module alone, then you really shouldn't be using it. ... You want better documentation? Go read MacLane. This is the kind of arrogance that drives people away from the community frankly. It's the "if you can't read the types and immediately know what this library does then get the fuck out you don't belong". Maybe that's not the intent you wrote your comment with, but that's sure what it sounds like on my end. I have no problem with this code existing, and it's the prerogative of the author what they do with their time. All I'm saying is that proportionally to other language ecosystems, Haskell is disproportionately less documented and it's a problem for those who unlike you don't immediately grok that a library is meant to be used for.
That's a perfectly valid question. But typing on my iPhone is too painful to answer it. :)
Very rarely. The short-curcuiting behavior of the either monad when encountering a Left value is tuned for dealing with errors. In other cases, it generally seems clearer to use a custom type.
Anyone that has used Haskell in anger.
There is literally no purpose to that library other than to correspond to categorical concepts. It has no other use. It exists on hackage as a teaching aid. There is no set of "simple examples of use" that would make sense. There is nobody who will ever say "I want to use the adjunctions library but the examples aren't good enough" because the adjunctions library is not designed for that sort of use to begin with. Hackage is a big bag of a lot of things and going through them all with the "are there examples of use" checklist will miss the point in many cases. It is an arrogant remark I made, but it is not targeted at newcomers, it is targeted as you, as you obviously feel like you are a semi-knowledgable person, but are not acting like one at the moment. This obstinacy has not, shall we say, brought out my most civil attitudes.
You could have known it the same way I found out: by typing `cabal install boolsimplifier`. Again, yes, I know the repository site I was using died, and I need to update things to point to a new repo.
I care about the documentation and version control, not a one-off install. This thread is about documentation and maintenance, not cabal installing an ancient relic.
Calling a package you apparently don't actually seem to care about and only pulled up for illustrative purposes claiming it needs to be upgraded to 7.8 when it doesn't an "ancient relic" hardly wins you any friends. If you want to have a positive impact you seriously need to work on your attitude. In fact, you probably should have titled this thread "how to discourage open source authors" because nothing makes me question my decision to release open source code into the world more than this sort of entitled passive-aggressive whining.
I'm a bit a more knowledgeable than a beginner, yes, and the only reason I'm commenting is that the constant "just read the f-ing source" philosophy was one of the biggest hurdles to learning Haskell in the first place and the ability to divine which libraries to use on Hackage was not something that came without a lot needless struggle to do very basic things that need not be that way. You very clearly seem to care about how newcomer's [view the community](http://comonad.com/reader/2014/letter-to-a-young-haskell-enthusiast/) and at the same time seem to be an apologist for a philosophy that was ( for me ) the biggest barrier to entry. I don't meant provocate or criticize, I'm just telling you my experience. I view the lack of examples and that culture as sort of intellectual macho chest-thumping and it's very off-putting when everyone on IRC assumes that you implicitly can just look at a library and "know" how to use it or figure out it's entry points and this is very uniquely Haskell phenomenon in my experience. 
&gt; Calling a package you apparently don't actually seem to care about and only pulled up for illustrative purposes I need it for [Bloodhound](https://github.com/bitemyapp/bloodhound/) so I can normalize [filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-bool-filter.html)/[boolean](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html) queries into a more efficient form. I will not depend on a library for which I cannot get documentation or source code for, as Bloodhound has production users. This idea originates with Carter who suggested that I do query optimization after I mentioned I wasn't happy with my filtering Seminearring instance.
&gt; But the haddocks do have documentation that explains things! They don't explain how to use the library from the topdown, they're just a bunch of independent comments and signatures. It's the equivalent of handing me a box of eggs, some flour, yeast, and frosting and assuming I know a) this can be used to make a cake b) I know how to make a cake. Hackage and particularly Edward's libraries exemplify this. Yes, I'm implying "documentation" to mean a set of instructions I can use to make the library do a domain-specific task. Haddock documentation is typically not that, it's the list of ingredients without the recipe. For example, I've been trying to figure out how to use trifecta for like six months. It diverges from Parsec in subtle ways and even just putting together the pieces has still eluded me. I don't have "the recipe" for Trifecta and it's not obvious how the provided Haddock docs would get me there. I'm clearly not the [only person who feels like this as well](http://www.reddit.com/r/haskell/comments/23dxli/why_are_examples_completely_absent_from_hackage/).
You can get the sourcecode from the tarball which is on hackage. You can also get the source code by clicking the source link, where you will discover it is a single very small and readable module: http://hackage.haskell.org/package/boolsimplifier-0.1.7/docs/src/Data-BoolSimplifier.html That said, I do appreciate the reminder to relocate the repo since patchtag shut down, and a little top-level blurb on how to use it is definitely a good idea.
Many libraries are precisely the equivalent of a box of yeast. Useful in many situations, but not very interesting on their own. Not all libraries have a "this is the use pattern, now go do it" model that can be followed. So your complaint seems not to be about a lack of documentation, but about a lack of worked examples. And your claim that you've had to read the source is probably not the case -- rather, you meant you had to read a bunch of types. And here I do think we agree -- there are _some_ libraries, _sometimes_, where complementing the nice haddocks with some high level explanation could be a great help, and often package authors don't take this extra step. But again, culturally, I don't think this is about "intellectual show-offs" or "chest thumping"or "arrogance". It is just that writing haddocks comes naturally as you work on a module and writing top level documentation is a royal pain and often package authors don't take the extra time and effort to do so. Berating them won't help. Asking nicely, and occasionally submitting your own patches will.
A random fun fact: I wrote this blog post almost entirely using Plover, an open-source stenography program. (Did some formatting and the code segments using qwerty.)
You shouldn't need a monoid to turn a lens or prism into a traversal. The definition of `setT` for `Traversal` looks suspect to me. I don't see how you can sequence applicative actions through it. I also don't see how it can work when given an empty list. That gives you a function `s -&gt; t` which I doubt exists in general. Perhaps I'm missing something. 
&gt; The logics type systems encode don't correspond to "provability" any more than any other logic with a sound and complete proof system (relative to the intended semantics) does (indeed, dependent type theory has exactly the same ability to internalize its own provability predicate--that is, to allow meta reasoning within the system--as does classical arithmetic). The article didn't claim otherwise. It just said that you should think in terms of provability rather than Truth to understand things more clearly, which I think is good advice. On the other point, Curry Howard was precisely about constructive logics, not all logics. PML's invention of type theory was with regards to intuitionist logic and math, and explicitly constructive. Sure, you can introduce excluded middle and still get a type system of a sort, and this has yielded an interesting line of work. But this hardly seems in the scope of an introductory article. In the offending sentence, I suppose "Type systems" could become "Type systems like those we see in common functional programming languages" just to remove even a hint of ambiguity. In any case, I would argue the line of work you refer to is really about the relationship of classic to constructive logic more generally (the former embeds in the latter, as is well known), and the fact that this translates to type systems is really an afterthought.
I've used a tool called Diagrammix. Link and a bit more discussion can be found here: http://www.reddit.com/r/programming/comments/1cgi2x/reimplementing_git_clone_in_haskell_from_the/
&gt; As of last week, GHC officially has no more .lhs files in its source repository; instead, all files have been converted to .hs and are now much more consistent with each other. Curious on why Literate haskell has been abadoned ? 
And giving out about libraries people provide for free on their own time is helping how, exactly? Congratulations, you've just convinced a few people never to publish anything open source so that they don't have to deal with the sense of entitlement some users have towards anything given to them for free. 
Thanks for the rather comprehensive review. :) I'll look through all of your comments and see if I can revise the tutorial. Edit: I've decided to remain with `iterate` so as to skip pattern matching till `fibs`. Edit 2: Updated to revision 3.
&gt; But again, culturally, I don't think this is about "intellectual show-offs" or "chest thumping"or "arrogance". I can't convince you of an opinion, but comments like this where Edward [insists on intentionally not writing documentation to prevent cargo culting](http://www.reddit.com/r/haskell/comments/1yvfmc/programming_with_types_not_tutorials/cfoajnr) ( just to cherry-pick an example, and not pick on Edward personally) come off as paternalistic and this what I view as intellectual chest thumping. Most communities embrace libraries as a way to abstract over individual experience so that we can all work at a higher level of abstraction, not as a form of hazing others. Hazing might be a slightly harsh word but I lack an equivalent concept to express "forcing your worldview on others who just want to use your library". &gt; So your complaint seems not to be about a lack of documentation, but about a lack of worked examples. Yes, the lack of examples is what I've stressed the entire thread. A library isn't documented until it has examples. Ergo, most of Hackage is undocumented and types are not sufficient to bootstrap an understanding or start using a library. We can quibble about semantics but that's the essence of the point I'm making. I think/know it's one shared by many beginners.
I wrote [Bloodhound](https://github.com/bitemyapp/bloodhound/) because a viable Elasticsearch for Haskell did not exist. I didn't harangue anybody about it, I just did it. However, I didn't launch without examples of how to use the library in the README either. The examples in the README took a fraction of the time the actual writing of the library took and have saved my users a lot of time. It's common courtesy to include at least a couple simple examples of how to use your library, if not end-to-end documentation.
&gt; On the other point, Curry Howard was precisely about constructive logics, not all logics. Curry-Howard for intuitionistic logic is interesting. Curry-Howard for classical logic is thought-provoking and changed the way we think about what "constructive" means. I'm not sure what the historical intention was (Per Martin-Löf (PML) certainly intended to build a basis for constructive mathematics, but that doesn't mean the whole community shared this point of view), but in retrospect I find it important to mention the computational content of classical logics, as philipjf did. &gt; But this hardly seems in the scope of an introductory article. On the contrary, I think we should dispell the myth that only intuitionistic logic is really "computational". This excellent introducory article mentions substructural logics and fractional types, it could have a paragraph over the computational content of classical logic.
&gt; It just said that you should think in terms of provability rather than Truth to understand things more clearly, which I think is good advice. I really don't. "Provability" makes it sound like you are talking about formal proof, and the whole point is that you are not. All logics have proof systems but those are different from the internal notion of truth. What I think this idea of "provability" should be getting at is "computational realizers" but those are not proofs, and the theorems are not proofs "about" computational realizers in the sense of meta-mathematics (although they have realizers). If you think in dependent type theory about implication P -&gt; Q a witness of this fact is *not* a function from "proofs" of P to proofs of Q, it is rather a (computable) function from P realizers to Q realizers. These are really different things because they have to do with different amounts of awareness to context. You can, as a meta result in ITT, prove there is no proof of the fact that FunExt = forall (f,g : A -&gt; B), (forall x : A, f x = g x) -&gt; f = g yet FunExt -&gt; False is not a theorem either. This leads to so much confusion with people that I so want to avoid. &gt; In any case, I would argue the line of work you refer to is really about the relationship of classic to constructive logic more generally (the former embeds in the latter, as is well known), and the fact that this translates to type systems is really an afterthought. To some extent. But that really isn't all that is going on. My group is interested in classical logic because we are interested in evaluation order. The thing about pure (total) functional programming is that it makes operational distinctions go away completely: for a PL theorist this is a bad thing! In the real world we care about the call-by-value/call-by-name distinction and also non-local control. Type systems related to classical logic are important because they let us access those things (related: linear logic + mix rule provides a type system for dealing with important things like concurrency and parallelism). In my mind, functional programming/intuitionism is a special case of full linear logic and its computational interpretation--classical logic interpretations occupy other special cases of the same underlying logical/computational structure. Yes you can encode classical logic into intuitonistic logic (in that functional programming is universal) but that doesn't speak to the primacy of intuitionistic logic IMO (or at the very least, full linear logic is even more primary) But that is not my main complaint: it is that the ideological promotion of intuitionism on the basis of its having an evidence semantics is *dishonest*. There are very good reasons to not want to work with classical reasoning, but the desire to have a computational interpretation is not one of them. Rather, we want to avoid classical reasoning because of the desire to have a *particular* computational interpretation that is very nice (functional programming) and for axiomatic freedom. Purity is a nice property: it is equivalent to saying "every term is equal to a value" which is a very strong reasoning principle. From the perspective of logic, that is exactly: you have a single conclusion that must be used linearly (which is to say, intuitionistic logic). That is pretty nice, but so are lots of other things.
But why is main bound?
Yes, :) . Except that, whatever you return from each case is assigned to `result` and then execution continues afterwards.
All the time. For two reasons. One is that it is the basic sum type - meaning that any time you need a one-off value that can be one of two possible types, you use `Either`. The prelude function `either` comes in very handy in those situations. Second is that the `Either` monad's short-circuiting is the basic tool for dealing with complex logic when simple pattern matching and/or guards aren't good enough. It allows you to avoid ugliness like deeply nested cascading if-then-else. The `Either` monad is especially useful in a monadic context, using the [either](http://hackage.haskell.org/package/either) package of /u/edwardkmett. In the past the `Either` monad was restricted to use only for error handling, because the `Left` type was constrained to have an instance of the `Error` class from the `mtl` package. Thankfully, that rather annoying constraint was lifted several years ago.
The link is to a nice discussion on Hacker News about [this blog post](http://coffeenco.de/articles/language_is_not_important.html) by Alex P. Most people in the linked discussion disagree with the premise of the blog post, which is the title of this reddit thread.
Following the pattern from tuple to triple, a 3-option sum should be Triether. (Pronounced either treether or tryther).
I'm really confused by this type. First it looks like, as you said, a basic sum type, that you use when you need either a type or another one: the "OR" version of (,). Then you have the Monad instance, which totally breaks the symmetry, and make "error" oriented. It feels that those two types which deserves different purposes and have different semantic should be two separate types, which seems usually the common practice . So I avoid to use Either at all, because I'm not sure what is its accepted semantic is, and "type OR" or "super Maybe". 
Was this "tuned for dealing with errors" the original purpose of Either, or has it been abused later on ?
Awesome! Never in my life had I expected to see a demo written in Haskell! Lovely! :)
I think [this paper](http://arxiv.org/pdf/1007.0825.pdf) is what I was thinking of.
As far as we know now, this could be caused by any of the following: 1. Some difference in the way ICU builds their DLLs for 64 bit Windows. 2. Some difference in the FFI bindings that is needed specifically for 64 bit Windows. 3. A problem with GHC's linking on 64 bit Windows. 4. A problem with Cabal, e.g., some option or flag that needs to be passed by default on 64 bit Windows. This is an important (and extremely high quality, thanks to Bryan O'Sullivan) library for the community, and very important for us, so I'm posting about the issue here to raise visibility. Any ideas about possible approaches to investigate or eliminate any of the above possibilities would be highly appreciated.
Thanks. I managed to copy/paste it and it works fine. These extensions are giving examples of constructs I need for my ghc-exactprint, so the series is a great source of test data. See https://github.com/alanz/ghc-exactprint/blob/wip/tests/examples/ListComprehensions.hs and https://github.com/alanz/ghc-exactprint/blob/wip/tests/examples/MonadComprehensions.hs
Of course you can! The idea that there is a nice, fixed, structured tree of "things to build in sequence" has little to do with the real world, where there is a mass of small things to do and too few people to do them all. Simple example of something that ought to be relatively simple but would be helpful to a lot of people: Google Analytics and BigQuery APIs in (idiomatic) Haskell. Both are used in a lot of companies, and neither is likely to be something the "better than" people are going to be interested in working on. Yeah, there's a perfectly working one in Python, and in practice everybody will just import and tweak it - not worth spending a few days rewriting existing work, right? But in our case at least, that meant introducing Python in a - so far - pure Haskell codebase, just to be able to get data out of Google servers. Very annoying. 
What did I just watch?
Could you provide a precise quotation or link to exactly what he says?
Indeed. It would probably be best if we essentially got the source-code for the entire book(assuming it is written in something like LaTeX or some such, I know little about this stuff) so we could possibly have a community-run version of the book, that could be updated as the examples are fixed, as opposed to just getting the source-code for the examples, fixing them and then having to wait until the book gets updated.
https://en.wikipedia.org/wiki/Demoscene
Not a very clear statement, but here is the subtitle file https://github.com/fptudelft/FP101x-Content/blob/master/transcripts/FP101x%20-%20Programming%20in%20Haskell%20Chapter%2010%20-%20The%20countdown%20problem%20Part%201.srt#L693
Can't see the code in question, but it sounds like it's mainly for convenience / pretty syntax. I would favour the more specific typeclass, but it is nice to have the cleaner syntax sometimes. If you don't mind using GHC extensions, though, you can have the best of both worlds using MonadComprehensions!
Oh, man, you've been missing out! [Stargazer by Orb &amp; Andromeda](https://www.youtube.com/watch?v=w9y8Lv_sSFw) is one of my favourites just for the neat effects and the menger sponge. And Farbrausch usually creates [stuff that's *very* technically impressive](https://www.youtube.com/watch?v=1dcrV_7JpXQ). All 10 minutes of that is just 64 kilobytes. 64 kilobytes.
His justification seems to be that he can use list comprehensions with lists but can't with Maybe. I wonder whether using LANGUAGE MonadComprehensions would satisfy him.
If the authors would support this then I think it's a great idea.
Right, it's hard to discuss the specific case without seeing the code. But in general, I find that the syntax for Maybe is not at all less pretty than for lists. Even without MonadComprehensions, there is a whole toolchest available for maybe. It allows you to write code that is natural for zero-or-one, which is really quite different than zero-or-more. Some examples: `maybe, fromMaybe, listToMaybe, maybeToList, mapMaybe, catMaybes`, and the `Maybe` instances for `Functor, Applicative, Alternative, Monad, MonadPlus`, and `Monoid`.
Yeah, you're right, I brought up MonadComprehensions because he mentioned list comprehensions specifically, but tbh I can generally get anything I want done with Applicative
Why allowing to return things that will be discarded? what if the person who return them expect them to be processed in some way (which should intuitively according to the type)? That should be a type error, imho this suggestion from Meijer is a bad suggestion.
Oh cool, nice project!
&gt; GHC would interpret operators as type variables, not particularly useful! This was actually pretty useful, but doesn't work anymore. You could do the following: f :: Arrow (~&gt;) =&gt; a ~&gt; b Nowadays, this is no longer allowed to be able to get all the cool operators at the type level, type families, constraints etc. The best you can do is this: f :: Arrow arr =&gt; a `arr` b
I stand corrected :)
Another demo, also written in Haskell, got 1st place at the same compo: [A moiré misszió bemutatja: Verschlimmbesserung](https://www.youtube.com/watch?v=J0Ae6ZRkWaI)!
I mostly agree - using a list instead of `Maybe` means the compiler doesn't know the intent that at most one member should be present, so getting that wrong is detected at run-time and potentially far from the cause. Where `Maybe` makes sense, I can't easily think of a case where I'd use a list. OTOH, playing devils advocate, it can be a kind of trade-off. Using `Maybe` makes the intent explicit to the compiler and (with a type signature) to the programmer, but on the other hand, using a list may be clearer because of the simple familiar syntax. Also, just because in principle this gives weaker enforcement doesn't always mean errors are likely to occur. Ada allows and encourages the use of range-restricted subtypes. These aren't subtypes in the OOP sense - the machine representation is identical to the parent type, but tighter bounds are enforced on legal values. This allows more errors to be detected earlier - though not always at compile time. But there's a price to pay. More explicit rules means less conciseness. Also, not all calculations just magically work out to yield the right type so you can get a little extra type-casting clutter. Choosing between `Maybe` and a list is essentially the same thing but for list-length instead of numeric value. We don't have a complete set of all possible bounds-restrictions of list length available in a simple-to-use form (that I know of) so clearly sometimes we're expected to ensure that inappropriate values can't occur by means other than the types. There may be a library I'm unaware of - IIRC it should be possible with current extensions. 
It sounds like what he wants is something that unifies to both `Maybe` and lists. That sounds like `MonadPlus` (using standard libraries), or better yet [`MonadThrow`](http://haddocks.fpcomplete.com/fp/7.8/20140916-162/exceptions/Control-Monad-Catch.html#t:MonadThrow) from `exceptions`. The advantage of the latter is that it allows you to specify a meaningful exception value that will be used by instances like `Either` and `IO`.
I recently realized you could do this: type f $ a = f a foo :: IO $ Maybe Integer foo = return $ Just 1 I might even start using this, since on the value level `($)` cleans up expressions, by removing the parentheses; no reason it couldn't do the same in the types.
&gt; We don't use the Maybe type, we're using lists because it's much more convenient. If you are a Haskell programmer and you're using the Maybe type, please reconsider to use lists. Because, as you see here, it's a little bit nicer because you can use list comprehension like this. I dislike that he's teaching people to use the **wrong type** because there's some rarely used syntactic sugar for a different type. &gt; What do others think of this? It's unconvincing reasoning and poor advice. It makes me wonder what other questionable habits he teaches. 
Weird, I was reading about this two days ago! I found the discussion in "[A Notion of Classical Pure Type System](http://oai.cwi.nl/oai/asset/1939/1939A.pdf)" by Barthe et al. interesting, and the introduction gives links to a few dozen other relevant papers. Whilst it's certainly true that Curry-Howard extends to classical as well as intuitionistic logic, it seems to me from my brief reading on the subject that the extension to classical logic is not as "natural" as the intuitionistic case. In particular, reduction has to be controlled, and in the paper linked above the conversion rule of the CPTS makes a distinction between "principal" and "minor" rules, with only principal rules being used to test whether types are convertible. Further, some important meta-theoretical properties that one would expect a type system to possess (e.g. subject reduction) are fairly fragile in the classical case, and may fail to hold depending on how exactly you set the system up. This is discussed in Section 8.2 of the CPTS paper above.
Haskell seems so fitting for a demo. Grats! In more personal news, I just thought of how to make a cool fractal out of Nicholas Cages. 
There are quite a few, for example these written by yours truly: * all of these: http://www.pouet.net/groups.php?which=9910 (including the first place at the same event) * all of these except "john squidman": http://www.pouet.net/groups.php?which=7355 * and also these: http://www.pouet.net/groups.php?which=8081 
Yeah, the title is misleading. I was more interested in sharing the impressions of Haskell in the wild. Even the blog post itself is more illustrative than just "Languages don't matter".
Nah, the source code is not exactly nice. Also usually these are quick hacks, which makes it even worse... The plan is to evolve at least some parts into reusable libraries, and release those. For example the OP uses the [LambdaCube](http://hackage.haskell.org/package/lambdacube-engine) engine, and mine uses the libraries [vect](http://hackage.haskell.org/package/vect), [vect-opengl](http://hackage.haskell.org/package/vect-opengl), [bitmap](http://hackage.haskell.org/package/bitmap), [bitmap-opengl](http://hackage.haskell.org/package/bitmap-opengl), [stb-image](http://hackage.haskell.org/package/stb-image) and [executable-path](http://hackage.haskell.org/package/executable-path) which are all on Hackage.
...a thread in which I called out: &gt; I'm perfectly happy to add documentation that reflects common usage scenarios. I'm perfectly willing to write documentation when there is a call for it. I'm also happy to go write more code. I tend to do whichever one is calling to me at the moment, and when I get someone who is willing to collaborate with me on a project, that one usually wins. If it is a thing that I think strongly deserves end-user attention I spend a lot more time documenting it than if it is a thing that is a quick hack to address a passing issue. e.g. `bytes`? it is a shim to deal with the fact that I absolutely had to ship a thing that worked with both `binary` and `cereal`. It has no grand plan other than extract the common interface. I'm not wedded to it other than I actually needed it in production and others asked if I'd ship it. `approximate` was part of analytics. I split it out because I had a user ask if I would. The user never asked for any more work on it, it sped up their AI application by a factor of 4, and I stopped working on the project. I keep it maintained, but generally don't have anything that needs it. It could benefit from some additional documentation, I spent 2-3 days building up the monstrous pile of magic constants that make it work, but then I stopped needing it, _and nobody cared_. `adjunctions` is a package that exists because it 'should exist' from the types, but if we look at adjunctions from Hask -&gt; Hask, then there are only instances isomorphic to the adjunction that give rise to state. All other adjunctions from Hask -&gt; Hask are isomorphic to that. Why isn't is super-awesomely documented? Partly because this fact is slightly disappointing. More often than not the projects of mine that get documented are the squeaky wheels that need more attention. Some are smaller shims that get factored out because I'm using them in more than one place, and want a common place for the abstractions to live. Most of my least documented packages are on hackage because I had code that worked that was happily working for me behind closed doors and I had someone ask me to take time away from what I was doing and package it up for them. If I'm looking at the code I'll clean it up and document it. If someone calls my attention to it, I may clean it up and document it. I'm spending an appreciable fraction of my time on backfilling documentation these days. I apologize for not having enough hours in a day to make you perfectly happy, but my attention is rather split across a _very_ large amount of code. I have a job, and it occasionally sucks up my time and attention for a month at a time. I can keep the plates I have in the air spinning. I can keep everything up to date. I can juggle the pile of collaborators I have that use my code, and I can merge all their pull requests in a very very timely manner. I can't do that, hold a job, sleep, manage the amount of travel I'm doing at the moment, and simultaneously write all the documentation to bring every single person on the planet simultaneously up to speed on everything from scratch. If you'd opened up and talked _to_ me, rather than about me, to the effect that you'd like to see more documentation for these projects, my response would have been "yes, I agree absolutely", and I would be more than happy to talk with you about the state of affairs, where I'm at on this process, and how to proceed. *How to discourage open source contributions?* Call the guy writing them names. I'm trying as fast as I humanly can, but frankly its posts like this that cause me to give up and go do something else, leaving another plate spinning, because I just don't want to deal with this sort of judgmental bullshit. If that is arrogance, I guess I'm arrogant, because otherwise I have no other way to deal with noise like this than to just walk away and try to come back to it some time when the rage subsides.
A different and occasionally useful generalization is data Both a b = One a | Two b | Both a b (better names are welcome :)
What other semantics would you have in mind for this monad? As far as I can tell, this is the only reasonable behavior. Just because one might think of it it as error handling doesn't mean that is its only purpose, either. 
Here are some reasons to stay away from esoteric Haskell extensions in production code: * The code will be read and changed by other people. Being too clever will make changes more difficult. * The particular extension you're using might disappear or change if it's very new. * You want the code to work with as many Haskell implementations as possible so you have a choice if your favorite compiler bit rots. Simple extensions, i.e., syntactic sugar, can be changed/removed easily (mechanically even), so they are much less difficult to deal with. 
I'm not familiar with this paper, I'll have a look. It looks like it's working with single-conclusion judgments with a specifc role given to double-negation as a primitive. I'm not that surprised it feels "not natural" -- I think multi-conclusion judgments are more natural for classical logic. It is correct, however, that the meta-theory of classical systems is less well-understood, and in particular it's easy to make *wrong* dependent classical systems (or PTS in the general case). The same could be said of linear logic, however.
Oh man, that's amazing! Thanks a lot!
Yeah, it's single-conclusioned, but they have a reason for that mentioned somewhere in the paper that I can't immediately recall.
(Copy of my email to Haskell-cafe list below) Didn't look into details, so a couple of general advises: 1. On 64-bit platforms gcc and GHC silently link against Visual C-created import libraries, but resulting exe segfaults. Import libraries should be recreated with native 64-bit dlltool. 2. Perhaps the most important difference between 64-bit Windows and Linux ABIs is that "long" is 4-byte on Windows.
In my opinion OP is exaggerating. I've finished the course and the impression that I got was that he advised to use just a list instead of `Maybe [a]` __in the examples he gave__. I believe it was in parser combinators section, where he says that he asks anyone who prefers using `Maybe` to reconsider and just use lists because the resulting code is slightly more elegant. I doubt that what he meant was that a list is always better than using `Maybe`. &gt; What do others think of this? Personally I use `Maybe` occasionally and I agree that if used "everywhere" it gets in the way. Somewhere it fits spectacularly well, and somewhere it feels clumsy. In many cases `Data.Either` seems much more useful to me, because I can return additional information instead of `Nothing`. That being said, I'm still learning.
&gt; insits on intentionally not writing documentation to prevent cargo culting I _never said these words_. I agreed that it is useful to be able to reason about types in a thread where the author was saying "hey you can reason about types in the absence of examples", and that having these skills is beneficial, because sometimes the tutorial doesn't yet exist. I don't think that is a particularly controversial statement. I _also_ said in that thread that I'd be perfectly happy to add documentation that reflects common usage scenarios. I hardly see how either one of those comments rises anywhere near the level of "hazing" you seem wont to accuse me of.
Urist McBrogrammer enters a strange mood.
I don't even play df, but I still recognize this :) 
Sorry if my point has been unclear. Rather, I'm saying that, if you depend on a library, and if it has few users, then you should be prepared for the possibility that the maintainer will disappear or cease maintaining it, and then it will become unavailable, and you will be left holding the bag for maintaining it. This has happened to me more than once, so its just an unfortunate thing born from experience. If you're not building code "for production" or "to last" or as an author of a serious library, but just trying out new things, then sure, just trying lots of libraries is fine. But for real work, when you depend on a library, you have to think about what that word "depend" really means, and understand that this also means you are in a sense _committing_ to a library. Dependencies aren't free, by nature, in any language, and I think it hurts new users more than anything to encourage them to think of Hackage as something that they should just be able to install and use anything from -- it never was that way (not just for new users, but for anyone), and it never will be -- there's too much variety, bitrot, complexity, etc. People who upload packages with lots of fragile dependencies and churn have packages that themselves become fragile and churn. There are a world of other packages that carefully minimize dependencies and can last for years and years without any need of change, continuing to just work. Encouraging people to try libraries is fine. Encouraging people to release libraries with more dependencies than necessary just leads to pain down the line. I think I overstated or misstated the point above, so I can understand the confusion.
Shameless plug: I'm working on a small DSL with a friend for 2d-animations that runs in WebGL (through fragment/pixel shaders). It's inpired by Pan, eg. `type Image = Position -&gt; Color` and `type Animation = Time -&gt; Image`. Here's a small demo: - http://youtu.be/_VKXYy0L1Vg And if you have WebGL: - https://dl.dropboxusercontent.com/u/17085113/index.html It's not on Hackage yet, but here's the GitHub repository: - https://github.com/Ahnfelt/uld-hd
Like others mentioned, you can use `MonadComprehensions` to get list comprehension syntax for `Maybe`s. I just wanted to add that `Data.Foldable.toList` is a handy function when mixing `Maybe`s and lists if you specialize it to `Maybe`: toList :: Maybe a -&gt; [a] It has two nice theoretical properties, too. First, it is a monad morphism, meaning that: toList . (f &gt;=&gt; g) = (toList . f) &gt;=&gt; (toList . g) toList . return = return Practically, this means that `toList` distributes over list/monad comprehensions: toList [ e | x &lt;- a | y &lt;- b ] = [ e | x &lt;- toList a | y &lt;- toList b ] This is the reason why the MonadComprehension trick mentioned in this thread is equivalent. Edit: This next part is wrong. I was confusing `toList` with the reverse function `listToMaybe` ~~In fact, `toList` is not only a monad morphism, but also a `MonadPlus` morphism, which is a fancy way of saying that it also distributes over concatenation:~~ -- EDIT: This law is wrong toList (a `mplus` b) = toList a `mplus` toList b toList mzero = mzero So `Maybe` actually is a perfectly suitable replacement for lists for the cases where you want to prove in the types that you have at most one element. As long as you have `toList` you can freely mix `Maybe` code and list code and it will do the right thing, according to the above [functor laws](http://www.haskellforall.com/2012/09/the-functor-design-pattern.html).
Very cool. I'd like to try it on something real, but I just really got going with Snap and while it did require some plumbing as you say, now that I've made that plumbing and am sharing it between all my services I've mostly forgotten about it. [How] do you envision servant handling, for example, cookies? Can I encode that into the route? How flexible will that be?
I also use `Either` to [exit from loops](http://www.haskellforall.com/2012/07/breaking-from-loop.html).
You could say the same thing about `Maybe`, frankly: there's a just-data interpretation (a nullable / optional value) and a error-oriented interpretation (a result, or a failure to create one). I'm not sure trying to figure out which is the 'right' interpretation is useful, vs. just deciding what semantics you want and picking a type that has the right behaviour / laws. It might help to point out that the current monad instance is the only possible one to write for `Either`, because of the way the type parameters are ordered.
Your point about provability is interesting. But I suppose we have "internal" and "external" notions of provability. The same goes, though, for "truth". I would be concerned about moving to anything involving "computable" or "computational" as well, because there are cases where our realizers don't compute :-P. The point to me is that as an article attempting to popularize a subject, its the sort of hand-wave that makes sense in my mind to better provide a first-order intuition. On the latter point, my observation had nothing to do with "ideological promotion of intuitionism" nor did, I think, the article. The connection between intuitionist logic and programs is historically primary. It begins with the BHK interpretation, runs through the invention of realizability by Kleene, and then through Howard. The extension of this result to much wider contexts, including classical, is certainly not insignificant. But again, in an introductory article, it seems completely fine to only stick to the most accessible main line of development.
Servant in essence consists of these "API combinators" (like `:&gt;`, `:&lt;|&gt;`, `Post`, `Get`, etc.), and then classes that use types made from the combinators to do something useful. The most notorious example of such a class is `HasServer`, which provides you with the `serve` function, but there are others for generating docs, and client code. You can add more combinators, and more classes that handle the combinators, very easily (so I've been thinking about the API type design mostly as a neat plugin system). In particular, you could come up with a new combinator, `WithCookie`, write a `HasServer` instance for it, add it to your routes, and call it a day (though I haven't thought about it for long enough to be sure that's necessarily the right approach). Of course, if anyone does that, they should do it as a library so that other users can make use of it too! 
Yes we could have a combinator to look up a particular cookie value or something like that. Basically: if the information is somehow accessible in the HTTP request, then we can make a combinator for it. It'll be as flexible as we want, all it does is encoded in a few class instances.
It's pretty easy to embed classical logic inside TT using the double negation monad. return :: a → ¬¬ a bind :: ¬¬ a → (a → ¬¬ b) → ¬¬ b join :: ¬¬ (¬¬ a) → ¬¬ a are all theorems, and `join` gives you double negation elimination, so LEM can be proven inside this monad. So you can do classical proofs, just put them in this monad.
Congrats on the release! I'm considering opaleye for use in production in the near future!
That doesn't sound like a compelling argument. You can work productively in ruby for several years. It doesn't mean that you can't get huge productivity gains by changing your ways. Also things like GADTs look to me (personal opinion, I am not a professional developer) as something that is *good* to have in production code, or in code that will be used by someone else, as it let you encode more invariants, preventing more errors.
pwstore-fast [provides pbkdf2 support](http://hackage.haskell.org/package/pwstore-fast-2.4.4/docs/Crypto-PasswordStore.html#v:pbkdf2). Would that be acceptable? Or only the implementation provided by the pbkdf2 package?
&gt; `Maybe [a]` Most of the time, seeing that explicit type is probably a mistake. It definitely requires some documentation on the specific differences between `Nothing` and `Just []`. If there's no intended difference between those two, `[a]` is a better type (convert with `fromMaybe []`, if required). If there's is an intended difference, writing the documentation will tell you if a more complex sum type (e.g. something `Either`-based) is a better type. Now, if that type happens to be a specialization of a more generic type, no problems.
Nice work! The type-safe routing here is pretty incredible and the only thing I have seen that matches (and this actually surpasses by bringing in query parameters) the type-safe routing of Yesod without compromising on boilerplate. I didn't see a mention of this in the docs: does this framework always assume JSON? How does one send back HTML? Can both be sent back for the same route?
http://lpaste.net/116089
I'm pretty sure in Haskell that you'll need a `:+` instead of just a `+` there, but maybe that is relaxed in GHC with the right extensions.
Wow this seems really nice! Can't wait to trt it :) I love how it uses type-fu but in a simple and clear way (at least I can understand what it does).
The Monad instance for Either does indeed break the symmetry of the type. `Either a b` is monadic in both `a` and `b`. If you think of one as a failure monad then the other is a success monad that short-circuits on the first success. That `Either` is monadic on the left is the essence of the throw-catch laws which are the same as the return-bind laws. Take a look at /u/Tekmo 's errors package and this post (http://www.haskellforall.com/2012/07/errors-10-simplified-error-handling.html). 
Thanks! Feel free to tell me about your project and ask me any questions by email (linked in the Opaleye README).
It doesn't compose well. :/ If library A has an IoA monad and library B has an IoB monad and both are restrictions on IO via newtype, you may have to do an IoC to get them to work together instead of doing something more direct. Free/operational monads or custom MonadA type classes seem to compose a little better. Both can come be performance penalties though. :/
The `Monad` instance for `(,)` (well actually `(,) a`) breaks symmetry too!
There is likely nothing I can say that won't likely make you angrier, though I edited the comment to hopefully qualify my words so that it comes off as a comment about overarching ideas and not individuals. This is only a statement of my feelings having had to wade through a lot Hackage code and my interpretation of the IRC community response to inquiries about undocumented code. I understand your point better but my comment still stands that it feels like forcing undocumented code on others is a form of intellectual chest-thumping, compared to other languages where a smaller set of curated libraries is often much documented and accessible. Let me be clear. How you spend your time is your business, and you seemed to misinterpret my comment as being a moral judgement on how you spend your free time on open source. There's nothing to apologize for. You are not obligated to do anything, and it's not my or anyone else's place to tell you what you should or shouldn't do, nor do I think I made any gesture toward that end. My English is not perfect, so I apologize if it is misinterpreted since there is often no distinction between accusative and "statement of fact" forms. 
Thanks! But we have query params [too](http://haskell-servant.github.io/servant/Servant-API-QueryParam.html) (and even a [Yesod-inspired QQ for writing your API type/routes](http://haskell-servant.github.io/servant/Servant-QQ.html) ). Right now we do assume JSON, but we'll hopefully get around to implementing a more flexible content-type negotation soon - and yes, the idea would be that the response content-type would depend on the accept header.
I *do* have an `HTML` combinator in a work project. To compose HTML on the fly like with `lucid` or `blaze-html`. If it's for serving a static file or more, you can use `serveDirectory` that acts as a `Raw` endpoint. We don't (yet) mix different formats for a given route/endpoint, but it's been on the back of my mind for a while and I may have a solution to this problem. Basically adding a (type-level) list of formats to `Get`, `Put` etc that witness the supported formats for a given endpoint.
Thanks! Very interesting
Even without monad comprehensions, the do-notation works for all monads including Maybe.
Looks similar to [wai-routing](https://hackage.haskell.org/package/wai-routing).
[Ask and thou shalt receive.](http://hackage.haskell.org/package/these-0.4.2/docs/Data-These.html)
Thanks! I'll answer on the café. #1 looks very complicated. If that's the problem, why do 32 bit Visual Studio DLLs work? The C long type is not mentioned anywhere in the FFI bindings, so it doesn't look like #2 is the problem.
I think the question is perhaps a bit strange. It's kind of like asking *"How often do you find yourself using a hammer?"* The only sensible answers here is *"Whenever I need to hammer a nail."* or maybe *"Whenever it is the right tool for what I need to accomplish."*
This is beyond awesome, thanks for sharing! 
 type a &gt;~ (c :: * -&gt; * -&gt; *) = c a; infix 2 &gt;~ type c ~&gt; b = c b; infix 1 ~&gt; f :: Arrow arr =&gt; a &gt;~ arr ~&gt; b
I like that this package exists. I'm skeptical that its names are better.
I don't know if there is a really good name. I just figure it's better if we all use the same terrible name, instead of having several different terrible names.
Thank you. :)
Thanks for clarification! To make it even more fiblike, write 0:1:1:... or 1:1:2:...
I feel the same way. "Read the types" people say, but that's often coming from people who already grok them. Coming from the other side is much harder. Hackage has a clunky documentation interface, IMHO. I wish it used something a little more common (like Markdown) and looked prettier. Do we have much of a presence on https://readthedocs.org/? Examples are key.
I think he means to say that `[a]` subsumes `Maybe a`. `Maybe a` is redundant and strictly less useful in the sense that there are strictly less ways to construct values from it. Basically, `Maybe a` is just a list with a size constraint. This is like saying that Integer subsumes Boolean, in that we can represent True as 1 and False as 0 and throw away Boolean as a totally distinct datatype (and the C programming language did precisely this). In part, this is a reaction to the current "fashion" in programming technique around asynchronous programming. Some people say "promises are all you need; you don't need asynchronous streams". He's pointing out that they got it backwards. It's like the current async fashion is saying "Boolean is all you need; you don't need Integers". 
Agreed.
What are the rules for this competition? Typically Haskell would have too large executables if that is a constraint.
I hereby request that anyone who succeeds in their struggle to understand a package or part thereof sends a documentation patch to the maintainer like I did with my very minuscule contribution to `lens`: * https://github.com/tomjaguarpaw/lens/commit/3765fe5dc688d638142dc7ef639a08fed8d0d143 * https://github.com/tomjaguarpaw/lens/commit/9e999a7e33d07e84fc5d61aba100b0a0d9715ffd I also request that package maintainers be as willing and speedy to merge documentation patches as /u/edwardkmett. Slowly but surely we will improve the state of the documentation.
Community documentation done wrong is horrible, as PHP demonstrates. Those comments in the docs pages are a maze of bad and worse snippets.
Here we can see /u/Mob_Of_One practising what he preaches with a long section of examples in his package `bloodhound`: https://github.com/bitemyapp/bloodhound
My approach with Opaleye is to make the tutorial a Literate Haskell file that is compiled as part of a test suite. That way I can be sure the tutorial will not get out of sync with the API. https://github.com/tomjaguarpaw/haskell-opaleye/blob/master/Doc/Tutorial/TutorialBasic.lhs
There is a lot of information to be gathered in the mailing list of the current PHC competition (https://password-hashing.net/). Basically, not all the things you might want from a good password hashing function is not delivered by the current alternatives (scrypt, bcrypt, pbkdf2). There should be a compelling solution that can be recommended without hesitation soon !
For now yeah, but I can help someone write a `snap-server` backend if there's need for it :)
Good point. How does servant being at the type-level compare to wai-routing being at the function level? What's the benefit of being encoded at the type level?
That's a really good idea, one problem I had with the README examples was keeping the snippets up to date. I was actually writing [our book](http://haskellbook.com) in LaTeX and Literate Haskell...this gives me an idea for using pandoc to generate my README and validate the code as part of my tests. Thanks for sharing this!
Types are valuable, but examples are critical for anything non-trivial. 90% of the times I've been unable to do something in a jiffy in Haskell, it has been for want of examples. The other 10% was just slow-down due to needing to perform type-tetris (hi `lens`! &lt;3 )
How often do you use `(a,b)` rather than defining your own product type? Using `Either a b` as a domain-neutral sum type is useful for the same sorts of reasons.
I have no idea, I only know about the different algorithms, not the Haskell implementations. 
“Read the types” is a lazy response.
Totally agree! A few minimally runnable examples are incredibly valuable - you don't necessarily need to document every single little function and parameter. 
&gt; Do you know of any good reader for lhs file ? Github is a bit raw. I do not. I've been wondering about that. A web service that takes a Literate Haskell/Markdown hybrid and renders it to HTML would be pretty cool. &gt; The fact that I cannot see "Doc/Tutorial/TutorialBasic.lhs" for Opaleye in hackage is quite a disappointment. I'm not sure what I can do about that. It couldn't be a Haddock so you'd have to just view the source. At that point it doesn't seem any better than just clicking through to the GitHub and viewing it there.
I'm not sure there's additional value to be gained from complaining about the state of documentation in aggregate at this point. (and personally with say ruby/python/javascript, I find the documentation situation much worse) So I think the main bits of documentation that help with learning a library are: examples, and cross-references (e.g. "see also 'foo'" or "can be composed with 'bar' for ..."). It might be nice to get a bit more support from haddock for the former (e.g. maybe allowing them to exist at the bottom of the source file (or make an examples file a separate entity?) , or have them collapsed by default in the HTML for readability) but otherwise the tooling is there. I think you need to build consensus around what "good documentation" means specifically; is it an example with every function, or maybe every function *used* in an example, or what? If a strong consensus can be reached then ideally our tooling can be made to encourage it as well. Obviously, as op acknowledges, no one sets out to create a library that is inscrutable so standards might be able to help in cases of "this is obvious to me because I wrote it". I could see some sort of comments system being useful, with a lot of careful thought. Better would be some standards, which would encourage documentation patches.
Defenitely gonna try get servant-client working on GHCJS. Type-safe communication between client and server FTW. EDIT: I forked it. lets get this party started: https://github.com/arianvp/ghcjs-servant-client EDIT2: Would it be possible to split the `servant` package in a `servant-common` and `servant-server` package? This way we can split off the server related code so that `servant-common` can also compile on `ghcjs`. Which will make it easier to run `servant-client` on `ghcjs`. Anyhow I'll fork it such that it is like that. But maybe it's something you guys want to consider as well? Running this stuff in the browser has a lot of awesome potential!
I've made a similar attempt at type-directed web apis (https://github.com/plcplc/typed-rest), but it seems servant has recieved a little more love than mine. I restricted the type of path signatures with DataKinds, but now I see that it appears to be more appropriate to just use kind *. Good luck with it :-) 
Well no, the question is more like, "how often do you actually use the claw part of the hammer?"
No, it's not a strange question. It's about the usage of Either outside of errors. &gt; Whenever I need to hammer a nail &gt; Whenever it is the right tool for what I need to accomplish Statements such as the above are meaningless.
This means we can reinterpret it (the API's type) in any way we want, instead of enforcing one particular monad in which these things should live, or other designs like that. Using the same type, we get handlers, client functions in haskell and javascript, and docs. All of that inferred from the type.
I generally aim for 100% doc coverage, but I find it's key to have examples, e.g. [ini](http://hackage.haskell.org/package/ini-0.2.2/docs/Data-Ini.html), [scrobble](http://hackage.haskell.org/package/scrobble-0.2.1.1/docs/Scrobble-Client.html), [shell-conduit](http://hackage.haskell.org/package/shell-conduit-4.5/docs/Data-Conduit-Shell.html), [freenect](http://hackage.haskell.org/package/freenect-1.2/docs/Freenect.html), [formatting](http://hackage.haskell.org/package/formatting-5.4/docs/Formatting.html), although the better docs [are here](https://github.com/chrisdone/formatting), and [lucid](http://hackage.haskell.org/package/lucid-2.5/docs/Lucid.html). That's in my own libraries, but I've only contributed docs to [aeson](http://hackage.haskell.org/package/aeson-0.8.0.2/docs/Data-Aeson.html#g:1) (the "How to use" section). It wasn't particularly immediate for me to be able to use the aeson package when I first tried it, and the pitfalls confused me, so I felt compelled to write about it. I'd like to have the time and patience to contribute more docs to other people's libraries, but the opportunity hasn't arisen yet. I've some time set aside to put an obnoxious amount of docs in [the SDL package](http://hackage.haskell.org/package/SDL-0.6.5/docs/Graphics-UI-SDL.html), and I have been given maintainer access to do so. If I can give some encouragement to people who want to help the state of Haskell understanding, it would be that if you write a blog post about how to use a library, make a clone of that post and convert it to Haddock and open a PR on that library. People shouldn't have to find your blog to grok a library, it's wonderful when a library contains all the explanation you need to use it. If you have plenty of time, you could convert Ollie Charles's 24 Days of Hackage posts to documentation for those packages.
Our documentation can be vastly improved, but I don't think this is a healthy way to approach the problem. I think this is a case where modeling what you want to see and asking others to follow your example will be far more effective. Incidentally, I often see observations and comments on twitter turn into arguments or misunderstandings. For this reason, I would caution you (or anyone) against trying to have a discussion via twitter. In my opinion, it causes more harm than good far too often. In conclusion, while I agree with the cause "please improve documentation", I'm downvoting this *particular* thread.
&gt; PHP had the ability to add comments to every single documentation page from the get-go, probably 10-15 years ago. This requires only a few lines of code, not a GSoC project. And we're still dealing with the fallout of this. Do you even realize how much damage a handful of bad examples on php.net have done? People still use the old mysql_ API, years after it has been deprecated, and for every person who cries in terror that interpolating values into your SQL queries directly is horribly wrong, three more pop up who point at the documentation pages that still recommend this sickening practice in 2014. Please please no.
Am I seriously the only one sick and tired of examples in documentation? I want to know what operations mean, not see some pre-baked example of one way to use them. Examples make very poor documentation, and I've *always* felt that way.
Here are the democompo rules. It wasn't a standard 64k / 256k format, but rather a 128 MB one: * Only executables allowed, no animations or movies * **Max. size 128 MB compressed, max. length 8 minutes.** * Should run on the compo machine (i7, GTX Titan, Win7-64), or bring your own hardware that can be connected to the beamer! (Video: VGA, RCA-composite, audio: 3.5 jack, RCA) * Entries sent via email are also accepted. Remote deadline: november 28. Contact: pasy (at) freshmindworkz (point) hu * Entries delivered locally should be delivered on either USB or optical storage! 
&gt; I'm not sure what I can do about that. It couldn't be a Haddock so you'd have to just &gt; view the source. At that point it doesn't seem any better than just clicking through &gt; to the GitHub and viewing it there. Somehow I was just hoping that hackage would be the web service that renders lts to HTML ;-)
&gt; Big file sizes are still looked down upon, though, because usually that means it's not that procedural. Ssshhh, I hear Carmack screaming in pain...
What's wrong with Python documentation? The quality of the stdlib documentation is excellent - there is a synopsis, examples, time complexity for data structures. The common external libraries have good docs as well, eg. Flask, django, requests etc
I'm a little confused .. how can toList . return = return ? The types don't match. I don't know what the type of return is but I know that it has to produce a Maybe a for toList to ingest. And because toList produces a [a] then that doesn't match what return returns.
I believe most cases of failure should have some attached information describing why there was a failure. So, most of the time `Either ex a` is better than `Maybe a`. Maybe there are some good examples of a function returning `Maybe [a]` and not some more or less general type, but I don't see any on hoogle. `Maybe String` and `Monad m =&gt; m [a]` are more reasonable, but not `Maybe [a]`, IMO.
Both points are good! I don't know wai-routing very well, so I'll speak rather more generally. Generally, I think the principle is: if at compile-time you have information that could be useful to prevent your program from going wrong, it belongs in types! The alternative, from a safety perspective, is using QQ and TH to throw an error at compile-time if, say, there's a broken intra-service link[0], or if the nice HATEOAS descriptions you've worked so hard on are out of date, or if your javascript client library needs a new release. I'm not saying you couldn't wrap wai-routing in TH and have compile-time safety in these cases, but it's just not pleasant to have to use TH that does opaque things all the time. And more importantly, TH/QQ-enforced safety just doesn't feel as natural, and isn't as familiar, as having GHC yell at us for having forgotten to fix something. You could do a lot of this with GADTs, but you'd still have the type, and now additionally an extra thing. Which is another advantage, sort of pedestrian in nature: if you just use types, everything that cares about it has one fewer parameter! Without them, the user doesn't have to pass the library functions anything. I'm not entirely sure if there aren't advantages to using GADTs (e.g., their being more easily manipulated) that in the end justify using them in this case, but I'd first like to see the use cases. And the signature is there to document which endpoint you're working on, or which subsection of the site, or which header you expect and will use, or (soon, it seems) which content-types you're going to accept. In short, I think it's just a little more pleasant! But I think users will decided that for themselves. [0] We have some experimental [type-safe intra-app links](http://haskell-servant.github.io/servant/Servant-Utils-Links.html), but we still haven't put much lipstick on it! 
Any reaosn you haven't posted the full thread? Feels like a bit of twisted irony, it's hard for me to comment on a discussion about the lack of documentation which isn't fully documented.
I'm just learning Haskell just now. I'm glad I read this because I've already committed to building something useful (probably a web service) no matter how hard I find it. I was looking for some way I could contribute in return so I promise to contribute clarifying documentation any time I work something out after a struggle.
I didn't want to pull in the original participants without their consent.
Have you seen my guide https://github.com/bitemyapp/learnhaskell and my site http://bitemyapp.com/ ? There's a silly little Scotty example here: http://bitemyapp.com/ There's also the Freenode IRC channels #haskell and #haskell-beginners if you need help, library advice, etc.
While this is certainly more general programming interest related than Haskell related I wanted to post it to have commentary in the Haskell community about what the best kind of documentation is and what should be strived for in Haskell packages.
Left "except I was a beginner for years"
&gt; &gt; `toList . return = return` &gt; The types don't match. Sure they do. You are just thinking monomophically. I'll annotate everything and it'll be more clear: (toList :: Maybe a -&gt; [a]) . (return :: a -&gt; Maybe a) = (return :: a -&gt; [a])`. Sure, `return` is assigned two different types here, but both can be unified with it's principal type `Monad m =&gt; a -&gt; m a`. You just have to think polymorphically. Similarly, the type assigned to `toList` is not it's principal type, which is `Foldable f =&gt; f a -&gt; [a]`. Actually, because of the interactions of the `Foldable` and `Monad` laws, we get that `Maybe` can be replaced with any functor that follows both the `Foldable` and `Monad` laws.
Perhaps someone could go through all packages that have a "24 Days of Hackage" blog post and add a link to it? The "24 Days of Hackage" posts often provide a nice, short introduction. Just so people know where to find examples.
That's fair enough. It's just hard (for me) to weigh in on a conversation for which there is a large gap in the context. I must say though, I wasn't aware anything was private about twitter. Speaking of documentation in Haskell in general, though, I personally have felt like things have been getting better. That might simply be because the new libraries I've been hearing about have been through blog posts or similar (OpalEye, Lucid, servant). However, were I to want to solve a specific problem, I feel like documentation lacks in examples. Just yesterday I was looking ot see how feasible it would be to add a manual tiling algorithm to xmonad and was overwhelmed. While there were plenty of "getting started" guides for those interested in moving their panels or changing their key bindings, I failed to location any resources on how to write a new tiling algorithm. I should also note that there is plenty of [api documentation](http://xmonad.org/xmonad-docs/xmonad-contrib/XMonad-Doc-Extending.html), but it didn't seem very accessible, especially when clicking on a few of the links doesn't explain the relationship between the different types or how to actually use them.
Thanks I'll check them out. You've also reinforced something I've noticed about the Haskell community so far — always quick to help those in need!
Yup, my mistake!
Thanks. I'll remember this and try to think polymorphically next time !
Usually there both are size-limited competitions (typically: 64k, 4k, 1k) and also an "unlimited" demo competitions (where unlimited is something like 128M). The latter has the advantage of being able to use proper music and graphics without spending ages of development time to generate them algorithmically. In this case there was only a single unified compo, as this was just a small event showcasing last year's top demos, with the competition being a "bonus feature".
I quite like this approach.
&gt; I think you are looking at it through a lens of general libraries only. Probably. I value reusability in code, even if I'm not good at designing it up front. :/ &gt; People write specific application code as well. Sure, but then the `a` in `Maybe [a]` wouldn't be an unrestricted type variable, it would either be a specific type or at least have a type class constraint. &gt; No, I don't care what database error happened, I am just going to print a generic "internal server error" either way. So Maybe is precisely what I want. That may be what I show to the user, but my log files need more detailed information so I can debug a production problem without violating SOX. &gt; Taking your statement to its logical conclusion, Maybe shouldn't exist and everyone should be forced to use Either (). No. Absence is not failure in many cases. Or, perhaps absence is the *only* "failure". In both those cases something like Maybe would be fine. But when you are dealing with the generic list type, "There are none" is already semantically represented by `[]`. So, either having the `Nothing` case is redundant or it actually has some *other* information attached to it, and that information can be the `ex` in `Either ex [a]`.
As an instance of Monad (and so asymmetric) there is no other semantic. There is still though a difference between "I'm expecting a result of A, or a result of B" against "I'm expecting a B. If it fails A is the explaination". In the first case I'm prepapre to do something interesting with the A. In the second, I need to handle an error, which is slightly different : Not in a mathematical point of view, but in a real world point of view. 
Is there a reason that this couldn't be used as routing for more general HTTP applications? I kind of chafe at the specialisation to JSON alone. Might be nice to be able to return HTML or XML or plaintext as well.
Right, this is about adding either a few more combinators or tweaking the current ones by adding some list-of-supported-format at the type-level. If there's an interest in this, I guess we'll just have to make it happen :)
I am doing this course also and Erik always words his personal opinion carefully, e.g. "I think". A note I made during the course (near the end of Declaring Types and Classes I): "Erik is not a big fan of Maybe -&gt; still stuck with Maybe -&gt; have to get rid of this eventually". At least it made me think, and will make me think in the future if I should use Maybe or not, or if there's a better option. IMNSHO that's bad teaching, more the opposite; making me think, and the tutor making it clear it's his/her *opinion*. Moreover, the course finally made me get into Haskell. Something that self-study with LYAHFGG and Real World Haskell couldn't do. Some edits: Another note I found (Functional Parsers): "Erik is not a big fan of the Maybe type -&gt; thinks the code with lists (*in this case*) is more elegant.". Again note how I wrote down "thinks" and also "in this case". By the way, I am not really a fan of talking about someone when he/she is not around; I wonder why you didn't contact Erik and ask him to clarify this? The course environment makes it very easy to "drop him a line".
Very similar indeed! I like your `:/:` combinator better...
I second this. Python has always had excellent documentation even for unpopular libraries you find on github.
There's pretty good support for having examples at the bottom of the file by using "named chunks" of documentation https://www.haskell.org/haddock/doc/html/ch03s05.html Yesod.Form.Bootstrap3 and Aeson use this approach https://github.com/yesodweb/yesod/blob/bfcb2876f0f70876a6630c26ca167838bbf78ba9/yesod-form/Yesod/Form/Bootstrap3.hs#L267 https://github.com/bos/aeson/blob/67ea6491600cae9a6bf8a87a558badadffcac791/Data/Aeson.hs#L170
I tend to write documentation that I think would help me the most if I was looking at it for the first time. For me, that usually just means I put denotations next to type definitions; usually the types of the functions are more than sufficient, so long as I know the denotations of the types involved. In fact, tons of documentation to read dramatically slows me down, *especially* when there are examples. My assumption when I see examples and long explanations is that the library is sufficiently complicated and mysterious that you have to think about how the implementation *works* just to use it, and in fact that turns out to be true in the majority of such cases, so I don't feel like it would be wise to skip all that documentation. When it turns out that it's actually just verbosity, I get annoyed. To be clear, I do write long explanations with examples when I think they are necessary to use the library correctly. It's just that I don't think that is justified so often.
You are not the only one.
Documentation needs to be part of the initial release of a package and should be written by the author. Asking your users to document your package for you is a huge waste of a lot of people's time because: * For every user that volunteers to document something there will be 100 users (not exaggerating) that wasted (a lot of) time trying to figure it out and gave up * For that one user that volunteers, the amount of time they need to invest to have the same context as the package author is significantly larger than the time it would have taken the author to document their own package So if I have the choice to spend a few hours documenting my package or wasting hundreds of hours of my user's time, I'll always choose writing the documentation every time. Unless of course my time is 100x more valuable than their time (hint: it's not, no matter who you are). That's not to say that users shouldn't contribute documentation, but you need to at least put up some scaffold for them to work with instead of asking them to scaffold it for you.
i certainly agree that examples shouldn't be used as a (very poor) *replacement* for reference documentation; the dual of the lack-of-examples problem is no solution. :-) But i feel documentation should make an effort to include examples *in addition* to the reference documentation.
I ordered How To Prove it (thanks!); I am sure I need something like that. I am not (yet) struggling with Awodey; haven't started yet. I read the first few pages of Aluffi and no (real) problems yet. Also thank you for the other two recommendations. Aside do you (or anyone else) know if there is a kind of "create your own study group" site? I think it's a good idea if such a group could be started to go through Velleman. 
IMO, a good doc should contain both.
With the Tardis monad that is mentioned in the end, you can do some pretty slick stuff. Many algorithms that take two passes can be done in one pass. It makes it much more readable when you can just request the value needed in a certain position in your code from the future, instead of messing with structuring the output for a second pass. The downside is bad performance because of the thunks/laziness. I wish there was a way to optimize away that overhead.
This is so awesome. I was always turned off by the boilerplate in the other type safe routing libraries so I've been using Scotty for everything. What should I google to learn about that magic type programming you're doing? I've never seen that before. I had no idea you could define type operators like that. We are just getting started with micro services at work, and I have been wanting a way to describe an API, and make sure that it matches what a client expects. I'm guessing you just import the generated client lib into the consuming service and it will not compile if it doesn't match. Is that correct? I'm really excited about this. 
I completely agree with you. There seems to be in increase in the amount of finger-pointing and pettiness in the Haskell community. Instead of approaching problems with an optimistic outlook "Hey, wouldn't it be great if we did ___, it will help attract users", we get a pessimistic "Why can't you people get it right?!". I tend to think a big part of the problem is twitter (as a platform for community engagement) than anything else or anyone in particular. It's just not a viable way to have an important conversation without upsetting people. The character limit forces you to throw out any nuance, caveats, or exceptions. I am fairly certain that if I were starting to look into Haskell today I would be turned away by the negativity (mostly on twitter). I have higher admiration for communities that see problems and try to solve them by cooperating (whilst respecting) than I do with communities that try to lay blame for the fault. Because of this I've actually made the conscious decision to get _more_ involved in the community. For years I didn't see it as important, but now I feel have to get involved just so that I can make it a more positive place. I have gained so much from learning about Haskell I only want to spread the joy :) 
Rather than add more type parameters, if you can document how I can write a JSON route and an HTML route and re-use handler code, I would be happy with that. I mostly use JSON, but need some HTML or a file download in a few spots. I don't think a lot of use cases actually need multiple representations for a route. The case where it is most needed is an API that wants to return either XML or JSON for client convenience. In this case using ToXML or ToJSON based on the request header would work well.
Not yet. I've done some work with sorting the routes, but there's a little more to do, and I haven't really thought about the binary search part. It's all the type level, so fun, but a little verbose! I'm also not sure it's a priority. Getting some benchmarks set up, for instance, also still needs to be done ;-)
I think there are two concerns here: 1. Library authors should write *great* documentation. 2. Expert library users should feel compelled to contribute their learnings to continue to grow the documentation I'm not trying to indicate that either author wasn't aware of this, but I wanted to make the distinction clear. The right decision is that *both* of these occur. So the real dangers are those which prevent or slow either (1) or (2). Tekmo highlights, I think, that (2) should not create an excuse for the library author to feel complacent about (1). I think what's especially important is that the library author (or someone else on a core contributing team) is uniquely positioned to write "tutorial" or "overview" style documentation which is tremendously useful for onboarding new library users. It is highly unlikely that user contributors under (2) can ever construct these documents. But that said, a contribution in the style of (2) is an important contribution. Library maintainers should encourage documentation patches as much as they encourage code patches. Users should feel encouraged to contribute them.
At pretty much the very moment you sat down to start complaining that I do this as a way of 'intellectual chest thumping' I was down in Australia giving a talk at a conference. In that talk I was being very very explicit that if you do use a piece of technical or mathematical jargon you have to always be willing to break it down. I do that. A lot. I spend an awful lot of time on different fora explaining the concepts behind almost everything it is I write. That isn't always in the form of documentation within the package, it often takes the form of tutorials, articles and the like. I write a bunch of code, put out some articles or give talks about where and how it is appropriate for use, and sometimes, that draws a spark, a bunch of users gather around the project and we continue to evolve it quickly. e.g. with regards to say of `kan-extensions`, I've written several blog posts diving deep on the notion of Kan extensions in particular, but then I've also spent dozens of blog posts trying to give intuitions for how they fit into the grander scheme of things. I didn't write it to 'thump my chest' I wrote it because _I_ wanted to understand it. Along the way I managed to write some articles that helped a bunch of other people understand it, and now we have a fairly large community of folks here who have some idea what it is all about who have at least passing familiarity with the concept. It has led to at least a dozen folks coming up to me at conferences and telling me that those articles were how category theory finally clicked for them. The concepts behind it are probably as abstract as anything else I can write about though. Kan extensions literally underly all of 1-category theory. I've been playing with them pretty much since I first found Haskell trying to find new uses for the abstraction, because it is so fundamental. I can write them up in as much of their full formal generality as can be expressed in Haskell where we have to replace certain categorical constructs with nearby functional programming constructs, but it is very hard to break them down into "common sense" terms. I just don't know how to do it. I'm not showing off when I write about them. I'm explaining them in the only vocabulary I know how, and trying very very hard never to _lie_ about something I say about them. Along the way... * we've found that we can use them to make code faster. "Asymptotic improvement of computation over free monads" * that they underlie a lot of work on effect systems if you attempt to split them into a CPS'd "request-response" model. * that I can use them to build a monad from every comonad, so there are fewer comonads than monads, because there are some monads you can't turn back. You seem to take my work as coming from a place of showing off, a negative place where I'm trying to put down other people. It very very much isn't. My _entire_ interest is in building a large body of correct code that others can build upon. I'm exploring these abstractions because I'm absolutely terrified that in a few years I'd be stuck exactly where I am now, using exactly the same tools. That is why I flail around trying out all these projects, doing all this exploration. I could do it far far more carefully and be more careful about bringing every one along with me as I go. If I optimized for that rate of change, I'd be able to explore a lot less of the space. I'd understand it a lot less, and I'd have a lot less code for those who have been able to follow along behind me or beside me to build upon. The farther out afield I go, the more interesting stuff I find that others can build atop, and help me document and test. But that said, I _do_ strongly believe in documentation and outreach. There are two things you need if you are going to climb the ladder of abstraction. You have to be willing to offer those behind you a hand to help them climb up behind you, but you _also_ have to be able to give them a reason to follow you up the ladder. Part of that latter segment for me has been trying to demonstrate through "broad strokes" the level of productivity it affords. I use math terms, because if I use the right vocabulary for an abstraction them it becomes more accessible, not less. It becomes googleable; it becomes something someone can look up on the internet. With that there becomes a larger body of existing literature that continues to grow up as others talk about the thing, and that I can use to better understand what it is I'm doing, and which I can try to translate as I can into more common sense terms and try to teach others as well as I can. Could I do a better job curating such resources? Absolutely. Could I do a better job writing documentation and breaking things down? Absolutely. Is it done because I hate you and don't want you to learn and because I want you to give up on Haskell because I'm so much smarter than you and have figured out these concepts? Absolutely not.
How hard would it be to make the generated JavaScript client library use a different request lib? We have node services that will need to call our haskell services, and they can't use jquery. 
&gt; (and personally with say ruby/python/javascript, I find the documentation situation much worse) I find this opinion incomprehensible. Ruby/Python/JavaScript libraries tend to have fantastic docs, especially in comparison to Haskell libraries.
Yes, we need to improve our documentation. But I think it's more a manpower problem than anything else. The languages/projects you're comparing us to have orders of magnitude more manpower contributing to them. Good documentation is *hard* and getting it right takes a lot of effort. Furthermore, tutorials/examples are even harder because you typically need an outsider to help generate quality tutorials. The insiders know it all inside and out, so they don't realize what things are not obvious. Right now I would say that Haskell is in the awkward teenage years. And awkward teenage software projects rarely have the kind of documentation you're talking about. So can we do better? Yes. But I don't think it's because the community has some inherent bias against documentation.
I do type-tetris in my REPL mostly, sometimes ghc-mod supported in my Emacs. Mostly the REPL though.
Just a shower thought but would it be possible to implement a trie at the type level. 
Can I have a summary / conclusion?
We should do it whenever possible, though. Let me give you an example that pertains to T-SQL. There are plenty of examples out there that show how to aggregate text from multiple rows and concatenate it into comma-separated values grouped by some other column. E.g., to turn id name 1 a 2 b 1 c ... into id names 1 a,c 2 b A popular trick is to fool SQL Server into thinking it's dealing with XML and take advantage of its XML aggregation and concatenation abilities. But the problem is these examples almost always deal with multiple columns from multiple joined tables. Most of it is superfluous. All you need to show the concept is a single table and two columns, like the above. People can and will figure out the interactions for themselves.
Either is used in a symmetrical way in ArrowChoice. I wouldn't ever use it, but there you go. 
&gt; That information was already logged, I do not need to continue passing it further on when it is no longer relevant or useful. I question your design. You seem to have a `logError :: ex -&gt; IO ()`, `doThingsWithDBStuff :: Maybe a -&gt; IO ()`, and `getDBStuff :: IO (Maybe a)`, where the `logError` call is stuffed inside `getDBStuff`. Instead, I would have `getDBStuff' :: IO (Either ex a)`, `showInternalServerError :: IO ()` -- your Nothing case, `doThingsWithDBStuff' :: a -&gt; IO ()` -- your Just case, and then combine them with `getDBStuff &gt;&gt;= either (\ex -&gt; logError ex &gt;&gt; showInternalServerError) doThingsWithDBStuff'` Depending on how often I used that lambda I might even have a `logExAnd :: ex -&gt; IO a -&gt; IO a; logExAnd e next = logError e &gt;&gt; next` Once you've entered the failure / slow path, you shouldn't merge back into the success path until after you no longer need values from the acquire path. &gt; Yes, but the other information can be simply "a failure occurred". But, even in your example it *isn't*. You've already admitted that the operation failure does have other information that needs to be logged.
Do you have any pointers towards information on how to make interesting glitchy looking stuff, interesting distortion, maybe even making that sort of old screwed up VCR look?
Does the different representation make a difference in what error messages are possible when routing falls? For instance wai-routing can specify that a cookie is mandatory and that it must contain a specific key. Then slightly different error messages are returned (as the response body) for the cookie not being present, or not containing the key, or it not bring possible to cast the key's value to the desired type. Are these features that servant would be able to support? Please don't take my comparisons to wai-routing negatively. I ask because they both seem like compelling routing choices. Edit: Forgot the link to the examples of the error messages. https://github.com/twittner/wai-routing/blob/develop/test/Tests/Wai/Route.hs testEndpointH gives a good example.
&gt; A web service that takes a Literate Haskell/Markdown hybrid and renders it to HTML would be pretty cool. The best solution I've found is to use [markdown-unlit](http://hackage.haskell.org/package/markdown-unlit) and symlink to `*.lhs` files to `*.hs.md`. Then GitHub will render the Markdown file nicely and you can still view the source. That's what I did for [my library's readme](https://github.com/tfausak/strive/tree/v0.6.1#readme). Edit: Doing that also means your `*.lhs` files can be part of your [test suite](https://github.com/tfausak/strive/blob/v0.6.1/strive.cabal#L108-L121). 
I agree, a properly documented library should not need examples.
I'd like to add, as an additional reason for why library authors might consider shifting their priorities towards better documentation, that bad docs are a rather effective way to diminish the utility of a library significantly. The most beautiful API design, optimisation and type safety net, which countless hours were likely spent on, can be almost useless if prospective users are unable to understand it, negating all that effort to a certain extent.
Typo or something I don't understand? bestBirthYears tbl = [ (the birthYear, firstName) should it be: "(~~the~~ birthYear, firstName)"?
As a supplement/alternative to examples, I wonder how far we would get with some sort of interactive graph that showed how different argument and result types in the library fit together? For instance you hover over a library function's argument and see which other library functions produce such a type, and hover over the result type and you see other library functions that consume such an argument. Has anyone experimented with that? How much would that help with the issue?
I honestly think this is a big part of Haskell's documentation shortage. Haskell doesn't often (granted, as a vast over-generalization) attract the personality type that think crappy work is still better than nothing. A lot of library authors have plans to write glorious introductions to their work, but are not willing to settle for releasing a half-baked first draft. This includes me; I've even paid people to write a book about CodeWorld, and I have over a hundred pages of detailed introduction and examples and exercises. But I have yet to announce it widely because I find it embarrassing at the moment... Sadly, documentation is often half-baked. Or when it's fully baked, it rots quickly due to the lack of compile-time checks.
* `doctests` inside literate haskell files need an extra pair of code/comment [wrapping](https://github.com/sol/doctest/issues/92#issuecomment-66235740) * `doctest` only checks examples inside `Haddock` comments (start with a `|`). Instead of: &gt;&gt;&gt; thing 5 5 Try: &gt; -- | &gt; -- &gt;&gt;&gt; thing 5 &gt; -- 5 
As someone said, the only interesting numbers are 0, 1 and _n_. It helps to be as specific as possible. For example, regular expressions have '?', '*', '+' to express the possible cardinalities (0/1, 0 .. n, 1 .. n). The difference applies to `Maybe a` vs. `[a]` 
Maybe there is a hint in the related papers... For the type of [parsers](http://www.cs.nott.ac.uk/~gmh/monparsing.pdf): &gt; Returning a list of results opens up the possibility of returning more than one result if the input string can be parsed in more than one way, which may be the case if the underlying grammar is ambiguous. For the [countdown problem](http://www.cs.nott.ac.uk/~gmh/countdown.pdf): &gt; Such failure could also be handled using the *Maybe* monad and the **do** notation, but limiting the use of monads in our programs to the list monad and the comprehension notation leads to simpler proofs.
In the past year our most serious Haskell bugs came out of (my own) zipWith code, usually where there was IO involved. Just to mention for beginners, consider something like zipWithExact where alignment matters...
I've had a thought. We can make a Monad instance for `Control.Applicative.Backwards`, I believe, given that it's underlying monad is also MonadFix. {-# LANGUAGE RecursiveDo #-} import Control.Applicative.Backwards import Control.Monad.Fix instance (MonadFix m) =&gt; Monad (Backwards m) where return = Backwards . return m &gt;&gt;= f = Backwards $ do rec y &lt;- forwards (f x) x &lt;- forwards m return y instance (MonadFix m) =&gt; MonadFix (Backwards m) where mfix f = Backwards $ do rec x &lt;- forwards (f x) return x Is there a reason these instances aren't already in transformers? ---- n.b. `rec` is just syntactic sugar for `mfix`, so the pragma isn't necessary to write the instance: m &gt;&gt;= f = Backwards $ do (_, y) &lt;- mfix $ \ ~(x, _) -&gt; do y' &lt;- forwards (f x) x' &lt;- forwards m return (x', y') return y
This is a really good post, if it were a little less discursive, I'd make it my default link for people wondering how to write documentation.
Interesting, I've had a hard time implementing (rewriting) a multi-pass image processing algorithm in Haskell. Maybe I can clear things up a bit with this.
Sorry for the mouthful of a title. The primary motivation for this was Cloud Haskell, because right now you have to use Template Haskell to do things like calls or Remote Table creation. With this, we won't need to use TH hacks to support stable references to external pieces of code (even things like remote lambdas). Furthermore, this work does not require any component of GHCi or the linker: it uses a regular kind of `StablePtr` underneath, and should be portable to all platforms GHC currently supports. This means even things like iOS or an ARM machine should work just fine as part of a Cloud Haskell application with a custom network-transport layer, for example, with nice integrated compiler support. There will probably be more revisions of this feature in the future, but this is a great start, and it was a nice and very comprehensible piece of work IMO. Lots of thanks to the Tweag.io individuals who contributed this!
Erik follows closely Graham Hutton's book. Page 118 of Programming Haskell: &gt; Failure within *eval* could also be handled by using the *Maybe* type, but we &gt; prefer to use the list type *in this case* because the comprehension notation then &gt; provides a convenient way to define the *eval* function. Emphasize on "in this case" by me.
Just don't challenge anyone who thinks themselves powerful and knowledgeable. They don't take kindly to being called out on their power trips.
I was really happy to stumble upon `RecursiveDo` in designing an actor model concurrency library. We have `Behavior`s (a function that takes an input, does something (including sending messages), and returns the behavior for the next message), newtype Behavior a = Behavior (a -&gt; IO (Behavior a)) ...the ability to fork or `spawn` them, and `Mailbox`es (the sink end of a channel that can be used to `send` to a spawned actor). What we'd like is to be able to do spawn :: Behavior a -&gt; IO (Mailbox a) But this has a problem: we can't create a `Behavior` that sends to itself: do mb &lt;- spawn behaviorClosedOver mb -- `mb` out of scope! Nor have mutually-recursive actors: do mb1 &lt;- spawn (beh1CLosedOver mb2) -- out of scope! mb2 &lt;- spawn (beh2ClosedOver mb1) -- okay To get around this we might separate the creation of a channel from the spawning of a behavior *on* that channel, like: newActorChannel :: IO (Mailbox a, Source a) spawn :: Source a -&gt; Behavior a -&gt; IO () But this brings a whole new set of problems: how can we make sure we only call `spawn` once on a `Source`? What should happen when we `send` to a `Mailbox` with no one spawned? etc. So anyway the `MonadFix` instance for IO was a perfect discovery: do rec mb1 &lt;- spawn (beh1CLosedOver mb2) -- okay! mb2 &lt;- spawn (beh2ClosedOver mb1) ...although I wish I hadn't looked at the implementation.
Everybody knows demo code is not nice. It’s not supposed to be nice. Still, people can and will learn from it — if you decide to release it.
Thank you for releasing your code.
that is actually really interesting. Earlier this year I was toying with the ultafilter axiom and how it seemed to be connected to state. I hadn't realized that Krivine had already come up with a realizer.
&gt; I don't even play df Another life wasted. :(
I said that `Maybe [a]` is almost never the principal type you want, because the semantics of `Just []` vs. `Nothing` are anything but clear. You are going to have to give me a more concrete counter-example, since you haven't been clear enough to convince me that `Either ex a` isn't better than `Maybe a` in your example, either. It is remarkably rare, IMO, that `Maybe a` is a better error monad than `Either ex a`.
&gt; The large amount of code that does what you think is bad shows that your opinion is far from universal. Not really. There's a large amount of code in the wild with security bugs, but (almost) no one likes them or would recommend writing that code the same way in the future.
But GADTs? Surely GADTs are viable.
I'm pretty sure pandoc does this.