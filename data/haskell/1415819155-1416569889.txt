I guess it depends on how you are using it. Haskell is a high level language, with tons of non-obvious performance optimizations, and lazy evaluation enabled by default. This makes it harder to reason about performance. There are certain attacks which measure performance of a cryptographic API, and use the obtained data to guess a private key. Since it is harder to reason about performance in Haskell, these attacks may be easier to perform - unless you use Haskell in some non-obvious way, for instance, to generate C code.
I've always taken the "right way" to be a decision tree like 1. Do I have a prior range for package (version X)? 2. If so: Have I added anything which relies on newer functionality? 3. If so: delete the prior range and start over 4. If not: 5. adjust an upper bound as if new as needed, 6. but retain the lower bound 7. If not: Am I only using core functionality? 8. If so: set the range to be &gt;= X &amp;&amp; &lt; majorSucc(X) 9. If not: set the range to be &gt;= X &amp;&amp; &lt; minorSucc(X) In this workflow, the hardest steps are (2) and (7) since you have to judge what someone else might consider to be core or new functionality based on your own impressions of it. Some packages go out of their way to make this distinction clear, especially those which are in "long term maintenance mode". Others, not so much at all and you just guess.
For starters, buffer overflows in Haskell are impossible unless you intentionally shoot yourself in the foot. Various other exploits are also less likely because of the high level nature of the language and because of the type system. For example the parameter parsing exploits in Rails would likely be impossible in any sane Haskell web framework because they don't blindly use HTTP parameter names to access database columns.
Didn't somebody suggest at some stage some kind of "call home" function of cabal , that would upload to hackage the build plan and the result (succeeded or failed), so that for each package we could gather information on which plans work (which cabal could always safely rely on) and which don't (that cabal would always avoid). The package owner could then always at least upload the plan that worked for her when she uploaded the package. You could use cabal in safe mode (always use a plan that has worked for somebody else) or "bleedingedge" mode (use the most recent version of everything unless it's known to not work). I though I had read about that somewhere, there are probably some issues to be considered....
I have a specific and principled epistemic reasoning for not putting upper-bounds on my packages, namely, I want proof of breakage. If my library works contiguously from the lower bound to the most recent version, I'm *not* setting an upper bound until I know something breaks. (I don't trust people's adherence to semver/PVP, among other things, even though I strive to adhere myself.) I realize this means I might miss something, but the build-breakage friction caused by overly strict upper bounds costs more programmer time than the hypothetical we're discussing. I'd rather attack the problem that actually occurs 98% of the time than increase friction out of fear. Not setting upper bounds is considerably less objectionable than what Maven/Ivy/et al. do and they still get away with it much of the time in spite of themselves. (I have had Maven's loose version enforcement break things for me before - in an untyped language) That said, I have had trouble getting dependencies to behave themselves on some of your stuff, but I don't know if those projects are representative of how you handle dependency versioning or not.
After having read the TOC it looks better than I expected. Nothing I would buy personally but something I'd see someone new to gamedev use as an introduction.
If I were you, I'd focus on formal methods for program verification. Make a note that even gradual/incomplete verification tools can eliminate common classes of error, and cite Haskell as an example of this. I'd like to think that Haskell is the vanguard of a movement towards formally verified software, but it's far from complete in that regard.
I think it's "shouldn't change behaviour people are relying on". Fixing bugs is a behaviour change. Trading speed for time is a behaviour change. There are very few changes that are genuinely zero impact, but plenty that are likely to be negligible impact.
It needs to be a service so the time lag between a new version coming out and a new package being released is minimal - running locally won't cut it. But certainly all the proto-cabal stuff is reasonable, and I'd leave it to the discretion of whoever does implement it.
That would be awesome! I'd certainly be grateful of it in the few places I do have upper bounds.
The parsing itself would not, and would be useful, and is almost certainly the place to start. The reasons to go from there to compile time code generation are to skip the parsing at runtime (ideally it can happen just the once while building your package), and to possibly generate types based on the content of the parsed file. The reason for Template Haskell in particular is that - in my opinion, held with low confidence - it's the best way of generating Haskell at compile time. You could certainly generate that code with other approaches instead, if that's your preference.
Uploading to hackage without first guaranteeing that things don't break is not acceptable for any system that wants to keep invariants. So some stashing area needs to exist for a repo system. Since Hackage doesn't do anything to keep invariants it should be treated as simply an ftp server and an ftp server can't be broken just because it contains software that does not build. Hackage is broken by design if you expect anything more. Stackage keeps a simple invariant so Hackage is the staging place for stackage. If stackage was used as the staging area for Hackage consistently and Hackage supported atomic uploads across multiple packages then it could get somewhere, but I would much rather see Hackage remove features as it's monolithic nature is hurting Haskell. Stackage is proof that Hackage is broken in so many ways. At a bare minimum stackage should already have been a plugin for Hackage. Killing Hackage and replacing it with a git repo and a few extensible shell scripts would be *technically superior* in so many ways.
I would agree with /u/IsTom on this, not because of a perceived lack of credibility, but because polishing a game requires you to learn a lot of lessons. I would love to see a book that explains these lessons ("Ah, so that's how you write a better game loop, because of such and such timing issue."), but this would require actual game code to learn them from.
I don't think this is the full TOC. This is just the TOC for the finished part of the book. The book is still work in progress and some chapters mentioned in the sample are clearly not listed in this TOC.
In the old days it was possible to attach an electric drill to an odometer to run the mileage of a car all the way to 99999999 and back to zero before adding some smaller more attractive number of miles. If you're the doorman to a club and your clicker is counting people in would you expect it to wrap when it reaches its limit? If you're taking photos with a camera and you run out of space on the memory card would you expect it to start at the beginning and overwrite your previous photos? Would a film camera spool back to the start and start double-exposing previous shots? That the machinery made this possible to do is not the same thing as saying it was "supposed" to work that way. If you add two large numbers together and exceed your range there is a very limited set of circumstances in which it's sensible to wrap back around. Most of the time in languages where this happens we have to go to the effort of checking carefully for "wrongness". Is our new value less than the previous?
Also [this](https://news.ycombinator.com/item?id=7559981).
Place, the virtual desktop metaphor with specific dimensions or the file system itself. There is no need to have a "distance" between two files. 70s engineers (or managers), forced users to abuse a purely logical space as a place to shuffle around pseudo-paper—or almost bridging the figurative to the literal, stupid icons: "whoops I just dropped that (pseudo) file in one of thirty (pseudo (pseudo ie shortcuts)) directories when I meant to drop it in the (pseudo) waste paper bin".
That's unsigned integers, a different matter. 
This does not sound referentially transparent.
There is no point in calculating this exactly. You have to assume that a user prefers the newest release over an older one, and then you don't have to solve the same problem, only sample the space and assume it is smooth.
Your problem would be trivial to solve if cabal could stimulate Hackage time. No need for upper bounds.
Not putting bounds also wastes untold hours of your users' time. And it is very likely worse because instead of getting a version bounds error from cabal-install, the user gets a cryptic compile error. What will your SLA be for the person who has been using a version of your package that is 3 major releases behind when they ask you why a change in a completely different package suddenly broke them? Are you going to be willing to support that old code even though your development activity is focused on the latest version?
&gt; If you stick to stable stuff, you're usually fine. Well the cool thing about Stackage is that it extends the definition of "stable stuff" far beyond the boundaries of the Haskell Platform. If you just use cabal/hackage, the default is to "grab the latest" rather than "grab the latest stable." People are free to make unstable releases onto hackage, and that is the problem. Stackage finds a set of constraints on hackage packages that can all work together without breaking each other, that is the solution.
In that case the upper bounds would be there implicitly. All of these solutions use upper bounds. Stackage even has upper bounds. In it's case, the upper bound is &lt;= the versions it has curated.
It should be a hook in Hackage. Hackage needs to stop being centralized, inextensible, taking down the universe when it has a hiccup, impossible to upgrade, etc. Hackage needs one and only one thing: a server to server extension API.
No developer should use Hackage directly. We have NixOS and stackage so why is what happens to uses of obsolete systems relevant? What is relevant is what uses of the best tools experience. That is what drives productivity. Nobody can help the Luddites' productivity.
Though amusing, the PVP/religion paragraph was less of a "comically going over the top" and more of a straw man to knock down. However, the underlying argument is one that I strongly agree with. Instead of relying on a protocol and expecting people to adhere to it, we should instead rely on an *automatically enforced* protocol. This is, in my opinion, the Haskell way: humans make mistakes; let's automatically prevent some of those mistakes.
FRP is relevant to game programming in Haskell.
Thanks for that clarification. I updated my comment. This is the TOC from the PDF I downloaded today.
Maybe they don't require the user to specify an upper bound, but they fix packages to a single version. That is, in effect, an upper bound. Cabal+Hackage works fine for me the vast majority of the time. In his blog post, Michael says he spends 2 hours per week curating. I spend probably 2 hours per month and I'm using just Cabal+Hackage to manage projects with well over 200 dependencies.
The trouble is that while the Num instance of Int8 is consistent with the "modular arithmetic" meaning of Int8, other instances like Ord and Integral are consistent with the "subset of Integer" meaning. If you mix the two meanings in the same computation, your program will probably behave unexpectedly unless you paid special attention to overflow or overflow happens to be impossible in your program for external reasons. In general, I prefer a program that crashes over a program that produces a wrong answer, because a crash is something I know I need to fix, while I might never suspect that the wrong answer is wrong. That is the whole principle behind assertions. So while the behavior of Int8 isn't type-unsafe, it's in a sense as bad (or worse) because it makes it easy to write wrong programs. A better design might be to have a "modular" Int8 type that is only an instance of Num and Eq, and a separate "range-restricted" Int8 type that has a Num instance that fails on overflow, together with Eq, Ord, and Integral.
What's wrong with Int8 Ord instance? In what sense it follows "subset of Integer" behavior? What you will get in C is not a crash. It's compiler optimizing-out parts of code. It's very difficult to catch.
&gt; but with haskell you can't then proceed to access invalid memory by going to myarray[-116]. Well, you can of course, inasmuch as libraries like array, vector and text are written in Haskell, rather than being part of the language. I recently spent a pleasant afternoon finding several integer overflow bugs in those libraries.
I'd like to see more discussion of how they relate to [thirst](https://hackage.haskell.org/package/thrist)s and [Reflection without Remorse](http://okmij.org/ftp/Haskell/AlgorithmsH1.html#reflection-without-remorse); it's alluded to but not expanded upon. I'd also be interested in seeing a hackage package. Very cool writeup!
Perhaps I just couldn't find it, but having an API reference on the site to the standard modules would be helpful.
That's a very adversarial way of asking. Could you tell me a bit about yourself first and what your own answers to those questions are?
http://www.reddit.com/r/haskell/comments/2m2gxa/is_haskell_more_secure_than_other_languages/cm0iv6f
Perhaps. In some respects this does feel like a rehash of the debate we had at icfp. :-)
I think what people have been trying to say is that these operations _are_ safe, because they're well-defined. You're supposed to take them into account and work around them:
You could also do that, but would you really want to treat different overflows as actually different values?
Chucklefish (the makers of [starbound](http://store.steampowered.com/app/211820/)) are working on a new title [wayward tide](http://blog.chucklefish.org/?p=154) written in haskell. They thought about using helm but didn't like the 2d focused design decisions so they're making their own game library build around SDL2, which-if their [previous](https://twitter.com/Tiyuri/status/511420173308669952) comments indicate anything-could be opensourced.
That's not an argument against trying to find a better design though. After all JavaScript's `+` and `==` operators are also well-defined, but nobody considers them paragons of language design.
&gt; There is no file, just the function. Any group of distinct bit sequences (or archaically "files") can be referenced by location which means you lose determinism because files can be moved around the file tree. Instead some reproducible hash value, or composite by appending bit length may be used. Any request for the data is always going to be the same at any point in the machine's history, any request for the group as expression via the list of hashes should be the same when evaluated. Current file references are it's own race conditions due to the mutability of the pseudo geometry. As long as collisions can be dealt with, for applications like specifying versions of a ~~file~~ bit sequence hash referencing seems appropriate. Metadata might change but a hash is reproducible. Functionality might be identical in each scenario, but I think conceptually their are gains. Folders and files make way for functions and datatypes. Beyond that it makes far more sense to think of a peripheral especially something like a printer as a function not a file.
&gt; can be referenced by location which means you lose determinism because files can be moved around the file tree They can't. The list of hashes from your example is not referentially transparent. The system I described is 100% referentially transparent.
Unfortunately, no, because there's no typesafe way of constructing a list of values of different types without wrapping them _somehow_. You could, however, cut down on boilerplate with helper functions or naming conventions. For example: type Basket = [Fruit] empty :: Basket empty = [] add :: (ToFruit a) =&gt; a -&gt; Basket -&gt; Basket add x xs = toFruit x : xs basket :: Basket basket = add (Apple Red 5) $ add (Apple Green 6) $ add (Banana 2 30) $ empty 
Thrist's are generalized for more then just functions, but you can't unpack one and switch out a piece of one and zip it back up, like you can with a cascades. I still haven't made it through Reflection without Remorse paper, but, from skimming it looks closer to thrists than cascades.
If, in the unlikely scenario where I still needed the overflown value, yeah sure. Otherwise I'd just use `Maybe Int8` which would be isomorphic, as you said. It would be more idiomatic than using a custom sum type.
Well put. It makes your point very clear: let's neither attack both the in-favour and the against upper-bounds camps for the wrong reasons, but focus in tooling instead. Switching to Stackage has made my life (a beginning Haskeller) noticeably easier. Thanks for you work on all this. You managed to put yr finger on the pain point, and worked on an (or even several) actual solution(s). Well done! 
No, it knows [B] is providing the same data as before. This is guaranteed by the system. For one, I don't believe we ever *forget* the data, but this is in the kernel, so its possible that we do. But the actual request would go over ames again, and give the same data before, as long as they aren't running a malicious version of urbit with a different clay, because the actual *ames* requests are also referentially transparent. Urbit also does create hashes of every file, so in the case that we did "forget" something, we could sanity check if we wanted to. I'm not sure we do though, because its a global namespace. (Actually, I can confirm we don't, because updating the data is actually a union of old data and new data, prioritizing the old data). You can't get something by hash, but it would be trivial to implement, I remember thinking about doing that and then being too lazy. Functional programming on top of unix doesn't make any sense to a degree, for example the file sync layer is absolute garbage and needs to be redone with FUSE instead of inotify or whatever the current system uses. But unix is used as a crutch and not part of the system.
The problem is that once you upload a package without upper bounds it permanently poisons the dependency solver, turning informative dependency resolution failures into uninformative build failures. This isn't a hypothetical concern: I've been bitten on this for my libraries. This is why upper bounds are a better default, because omitting them is a decision you cannot completely undo later on.
So the core data type is `CascadeC` which reifies composition of a category: -- from Cascade.hs data CascadeC (c :: * -&gt; * -&gt; *) (ts :: [*]) where (:&gt;&gt;&gt;) :: c x y -&gt; CascadeC c (y ': zs) -&gt; CascadeC c (x ': y ': zs) Done :: CascadeC c '[t] -- specialize to functions type Cascade = CascadeC (-&gt;) -- specialize to monads type CascadeM m = CascadeC (Kleisli m) -- specialize to comonads type CascadeW w = CascadeC (Cokleisli w) So, like thrists, you can do generic composition.
I've got a release of Bloodhound impending, I *could* add upper-bounds now. The question is whether it's worth it or not.
My point is that it's too late. Once you upload a release without upper-bounds you've poisoned the constraint solver.
My understanding is that there is a real difference, it is not just culture. Compile time guarantees are easier to produce in Haskell compared to say python. So in Haskell when you deploy your program you can have a near guarantee certain types of problems will not happen. My impression from my years of hobby writing python is that this would be hard to reproduce in python. That most of the time the framework tries to provide guarantees through best practices, but if one of those best practices is actually is not followed to a T you will not find out until when it is catches the problem at run time, possibly at test time, if you have enough tests. So from what I can tell it is not just culture. In haskell I have more tools to make compile time guarantees.
Let me rephrase what /u/dstcruz is saying a different way. Stackage is a "viral" solution in the sense that it undermines other competing solutions. The more Stackage packages that release without upper bounds, the more dependent we become on Stackage to build anything at all. In a sense, Stackage is solving a problem that it partially created. The wide number of Hackage-related build failures began precisely because people who bought into Stackage began removing upper bounds. Just because Stackage is a more "viral" solution does necessarily imply that it is a technically superior solution. I judge the quality of a solution by how much it automates the process, and Stackage still does not automate curation of dependencies.
Not if I deprecate the old releases :P
Thank you :)
If you think Stackage does no automation, you're just clearly speaking out of ignorance. I'm sorry, but you're showing the antithesis of everything I'm asking for in this blog post: presumption that the current approach is good, lack of understanding of alternative solutions, and refusal to test out new tooling.
Not true with the current tooling unfortunately. Imagine if in between two "samples" there's a bad version that has an incorrectly lax lower bound on a dependency (e.g., it allows QuickCheck 2.6 when it really only compiles with QuickCheck 2.7). If you now pull in another dependency that demands QuickCheck 2.6, you can get build failure.
I wish that, instead of making *assumptions* about how Stackage worked, you would either try it yourself or ask how Stackage works. A new package can make it into a snapshot without 24 hours quite easily. Is that not cutting edge enough for people?
Yup, that's one option among many. Most companies I've spoken to (and FP Complete as well) have created custom tooling to ensure reproducible builds, which solves this problem. Stackage is another tool in that problem space.
Fair enough. I'd make two counters to what you're saying: 1. Stackage really didn't start this debate. People have been avoiding upper bounds for a long time. I won't go into details on that, it's well documented (including in this thread!). 2. I think of people relying on PVP upper bounds as creating a negative externality. They are demanded that other people do work (namely, manage upper bounds) so that they don't have to deal with the problem in a different way. Running a community by demanding that others do work they don't want to is a terrible idea. To go even further: the reason this PVP debate is so heated is because some people have created incredibly fragile development environments, and refuse to admit how fragile they actually are. Then, instead of admitting that fragility, they blame other people for not conforming to their ideals for breaking their tools. That's destined to simply created a lot of hostility. I went the tooling approach because it *doesn't* have that negative ramification.
The publicly visible side of Cove development has thus far been a one-man task, and that man is also the lead developer of Wayward Tide. I've been consoling myself with the assumption that the "making the game" part of his job has been too demanding to publish any tutorials at the moment.
See, you're actually ignoring the point of this blog post. Yes, you've made all of these arguments before. No, I'm still unconvinced by them. And those dozen or so people I met last time I went to a conference who said they were uncertain of using Haskell beyond hobby purposes don't care who's fault it is that `cabal install` failed, they just no it did. You've been advocating PVP upper bounds as "the solution" for a while now. Whether you want to admit it or not, it's failed. If you want to blame the rest of the world for the failure, go ahead. But the *fact* is that many people run into dependency issues with the toolchain and policy you're talking about. And many of those problems disappear when using better tooling (Stackage being the best contender in this space IMO). So we can continue this debate where you insist that you're right, and I insist I'm right, and get no where. I want us to move on to actually solving people's problems. If Stackage doesn't solve those problems, tell me where it's falling short. If it does solve those problems, why aren't we moving it to the default toolchain?
That's true. Blacklisting old releases actually would fix that.
Well if the default database access library in Haskell did not allow use of strings to submit SQL statements, then it doesn't matter what the developer's concerns are. I actually am not sure if this is the current state of Haskell database access libraries. But there is nothing technically preventing it.
You might be interested in [session types](http://www.eecs.harvard.edu/~tov/pubs/haskell-session-types/) if you haven't come across them before. It's been a while since I've seen much done with them in Haskell - although I could have missed some stuff. I've been playing around with them - trying to improve the usability of the types via some of the more recent Haskell extensions - but don't have anything ready to share at this point. The [ABCD group](http://groups.inf.ed.ac.uk/abcd/) looks to be doing some awesome things - Scribble looks pretty cool / similar to Protocols after a few seconds clicking and glancing. Looking further into Scribble and the associated work is on my queue once I make some more progress with the 2 party form of session types...
Sorry to be a wet blanket, but... https://github.com/haskell/cabal/issues/1792
Most of my curation time is spent sending pull requests [like this one](https://github.com/rampion/ReadArgs/pull/5), which simply relax upper bounds. Anyway, saying "we're all using upper bounds" is a bit of a lie. It seems clear that /u/hastor is complaining specifically about developer specified upper bounds, not upper bounds that are automatically generated by a system like Nix, Stackage, or something else.
You can disagree without being disrespectful. I maintain several packages which are included in Stackage. The only value that Stackage provides to me, as a package maintainer, is being automatically notified that my upper bounds are stale. Stackage does **not**: * Automatically submit pull requests to my repositories to update stale upper bounds * Verify that new upper bounds would compile correctly Right now, I still have to perform tests and update my upper bounds appropriately, which is no better than the state of affairs before Stackage.
I'm not being disrespectful, I'm saying that your statements are wrong. Don't be overly sensitive. You said: &gt; Stackage still does not automate curation of dependencies That's simply not true. Your next comment gave an example of automation that *does* occur, and listed two things that you claim don't occur. While they are somewhat true (Stackage itself doesn't send pull requests, it just creates an automated process under which I get notified to create a pull request, which in turn verifies that the upper bounds will compile), that has little to do with your original statement. Stackage *does* automate the curation process. I really have no idea why you're trying to say otherwise. And I'm sorry you don't see the value in Stackage as a package author. Many others do. You're probably indirectly benefiting from Stackage in a huge manner, because many upstream packages have fixed their dependency ranges because of Stackage reports. Nonetheless, this blog post (if you read it) clearly was *not* talking at all about Stackage benefits for package authors, but for package users. So while your personal feelings on this as an author are certainly interesting and useful information, they're not relevant to the point at hand. __EDIT__ And just to be clear on automation: if everyone follows the maintainer's guidelines correctly, then Stackage as an automated process will regularly spit out new snapshots without any intervention from a human being at all. It requires intervention when there are problems in the upstream. And even the process of fixing that is semi-automated, in that lots of useful debugging information is spit out to notify package authors. That fits automation by any definition of the term I've heard.
If you have time, I think a blog post detailing how FP Complete handles reproducible builds would also be very interesting/inspirational. This post was great; sparking discussions like these is why /r/haskell is the only language-specific subreddit I frequent.
Thank you. I can describe it briefly here: we have a list of every single version of each package we allow, and a tool that wraps around `cabal` that adds appropriate `--constraint` arguments. However, this is really finicky to keep updated, and essentially duplicates my work on Stackage. So we're planning on moving over to Stackage itself soon. The only reason that we haven't done that yet is that we're trying to do that in step with our move to a fully Dockerized build environment. Now a Docker-based build environment... that's worthy of a nice long blog post :)
We're looking at doing some React-based stuff now. I haven't seen any explanation yet of the relative merits of virtual-dom and React, do you have any insight?
I thought seL4 was proven using Isabelle/HOL. I could imagine maybe using Coq, but surely not something as immature as Agda.
Some reasons I'm not using stackage: - It doesn't have lots of packages (e.g. trying to build darcs fails on FindBin and terminfo, I don't know what else is missing) - It doesn't have snapshots going back more than 6 months so doesn't help with using GHC 7.4 or GHC 7.0 - It would be a pain to keep updating my cabal config to point to the latest snapshot - I think the model is too restrictive, I want to be able to vary individual dependencies independently
I see I'm being downvoted, so clearly my opinion on that subject isn't shared. However I still think this is at best an anecdotal problem : * Maybe a managed language makes timing flaws more common. I am not really seeing the kind of scenario where this is true, but I might lack imagination. * A lazy language might expose easy to exploit timing attacks. This one clearly is a straw man. Bailing out early happen all the time in all kind of languages (think about `memcmp`). There might be more occurrences of that behavior in Haskell, but writing crypto always was a terrible idea in any language. And it seems that at least the author of `tls` has a clue about what he is doing. Now look at how many CVEs are timing attacks. I found 65 [here](http://web.nvd.nist.gov/view/vuln/search-results?query=timing&amp;search_type=all&amp;cves=on). I don't know why there are only so few as I'm certain most web frameworks are vulnerable to the memcmp-like hash comparison during authentication. Perhaps because it's so hard to exploit. Now contrast with all the classes of vulnerabilities you can avoid using a managed language and type enforced invariants. Clearly, the fact that it *might* be easier to introduce timing attack with a lazy and/or managed language might be important to mention (and it's unclear to me it's all that different than with C), but if I was to point a potential security problem with Haskell I'd rather look at the big runtime.
Thrists are existential in the intermediate types of the path, while cascades make those explicit.
Like thrists do, you could allow -XPolyKinds.
More implementations and benchmarks (under ghci): http://stackoverflow.com/questions/26727673/haskell-comparison-of-techniques-for-generating-combinations
I don't get how it is disingenuous. She clearly states in the free sample that using Haskell is the "hard choice". She's making an experiment in making a game with an "unlikely" tool, not touting Haskell as a panacea for game development.
That's actually not good enough because the maintainer might have been building against something other than the most recent set of packages on hackage at the time of upload.
I wish I had those skills.
The position seems like tons of fun, lucky the guy which will land the job ;)
I wish there were junior positions for jobs like this.
* If you're missing packages, you can use the inclusive snapshot, which has all packages. But better yet would be to get those packages added to Stackage. * It's true that if you're using older GHCs, you can't use Stackage today. Fortunately, that problem will phase out of existence, as all old snapshots will be kept in perpetuity. * I'd argue that having to manually make a decision to move to a new snapshot is a feature, not a bug. (I've written up longer answers on that before, but I can't find it.) But if you really think the bleeding edge version is better, go ahead and set your remote-repo to: http://www.stackage.org/alias/fpcomplete/unstable-ghc78-exclusive (or equivalent, see links at: https://github.com/fpco/stackage/wiki/Preparing-your-system-to-use-Stackage#snapshot-types). * You can use the provided cabal.config and tweak it if you want. However, I'd argue that this is *not* what we want to be promoting as the default way users interact with the package set. You may want to play around with individual dependencies (though to be honest you haven't given any good motivation for *why* you want to do this), but for most users it's simply a recipe for broken builds. __EDIT__ I forgot to say: thank you for this input, this is the kind of discussion I was hoping we could start having around Stackage.
One trait PVP upper bounds share with religion is proselytizing. PVP upper bound doctrine requires its believers to convert all other's to become followers, and to denounce heretics and excommunicate them\*. It even requires historic conversions, rewriting history to make old packages born again as PVP upper bounds followers. Do I really think this is a religion? Of course not. Am I amused that I can easily find so many parallels? Certainly. And I readily admit that plenty of my feelings on other tech subjects (including Haskell) could be compared to religious beliefs. All hail the mighty monad! \* Case in point being an exchange regarding banning some packages from Haskell Platform because they didn't have upper bounds.
This might also be of interest : [here](https://news.ycombinator.com/item?id=8592757). This is a comment by somebody who is well known for exploiting crypto flaws. I didn't expect string-comparison timing attacks to be actually exploitable in Java !
&gt; If you're missing packages, you can use the inclusive snapshot, which has all packages. Doesn't the inclusive snapshot invalidate your arguments about how Stackage solves upper bound problems? &gt; But better yet would be to get those packages added to Stackage. I'd rather spend my own time maintaining upper bounds :-) &gt; I'd argue that having to manually make a decision to move to a new snapshot is a feature, not a bug. I time my 'cabal update' calls to control this already. I don't want to have to go to a webpage and then copy and paste a link into a config file, and *then* run 'cabal update' to do this. &gt; It's true that if you're using older GHCs, you can't use Stackage today. Fortunately, that problem will phase out of existence, as all old snapshots will be kept in perpetuity. What guarantees are there on the longevity of Stackage - e.g. what if FP Complete disappears? &gt; But if you really think the bleeding edge version is better, go ahead and set your remote-repo to: http://www.stackage.org/alias/fpcomplete/unstable-ghc78-exclusive Is there a way to do this for multiple GHCs simultaneously? I switch versions all the time. &gt; You can use the provided cabal.config and tweak it if you want. Are there instructions on doing that? I find the idea of statefully editing a config file offputting. From my point of view, the --constraint parameter to the cabal command-line is easy to use and nice and local. &gt; You may want to play around with individual dependencies (though to be honest you haven't given any good motivation for why you want to do this) - I like the "keep the installed version if it will still work" optimisation (I may be misunderstanding, but with stackage won't I have to upgrade everything that's changed when I update to a new snapshot?) - If my code doesn't work with the latest version of something for some reason, I can still update most other dependencies and just hold that one back, giving more leeway to fix the incompatibility at my leisure (for example darcs was constrained to haskeline 0.7 for some time, the HTTP tests still don't work with warp 3.0). - I can check the effect of a single package version bump in isolation, e.g. to track down a regression 
btw, as for your downvoting, I didn't downvote you, but, did you really mean to say that "getting authentication right is certainly *not* hard"? 
&gt; Doesn't the inclusive snapshot invalidate your arguments about how Stackage solves upper bound problems? It's a compromise position. It weakens the guarantees, but still solves the majority of build problems. For more information, see: https://github.com/fpco/stackage/wiki/Stackage-Server-FAQ#whats-the-difference-between-inclusive-and-exclusive-snapshots &gt; I'd rather spend my own time maintaining upper bounds I think that's short-sited. An email to a package maintainer requesting they add their package to Stackage takes you a few minutes, and that maintainer will get notifications when their package stops building with newest dependencies, which will continue to help you even if you don't use Stackage. &gt; What guarantees are there on the longevity of Stackage - e.g. what if FP Complete disappears? It's an open source project, and I have no intention of stopping my work on it. &gt; Is there a way to do this for multiple GHCs simultaneously? I switch versions all the time. This is a limitation of cabal, along with the fact that it won't allow overriding remote-repos in a sandbox. I personally use either hsenv or Docker to work around both limitations simultaneously. &gt; Are there instructions on doing that? I find the idea of statefully editing a config file offputting. From my point of view, the --constraint parameter to the cabal command-line is easy to use and nice and local. Yes: download the cabal.config to your directory. That's it. It's [documented here](https://github.com/fpco/stackage/wiki/Preparing-your-system-to-use-Stackage#using-a-cabalconfig-file-simplest-usage). If you're comparing this to manually giving `--constraint` options, I don't think you've tried this out, because it's quite a different experience. &gt; I like the "keep the installed version if it will still work" optimisation (I may be misunderstanding, but with stackage won't I have to upgrade everything that's changed when I update to a new snapshot?) Yup, that's a fair tradeoff. I'd rather let my CPU spend the time rebuilding than spend the time myself tweaking version bounds to make sure I have a good package set. (And please don't try to claim that PVP upper bounds make this easier, I've spent plenty of hours fiddling with this on commercial projects with dependencies that have upper bounds.) The other two points can be trivially addressed by using the cabal.config approach, though I'd argue what you're describing should not be something we recommend most users ever do. Please don't take offense at this, but: is it a fair guess that you don't normally work with a team of developers on applications that are deployed into production?
Well this was written in the heat of the moment ;) Given that most authentication schemes will involve some kind of external general purpose database, I'll have to say it's probably really hard to avoid timing attacks of some sort (account enumeration). Hash comparison should be constant-timeable easily though. There are other more crippling attacks that you will need to defend against.
Most SQL libraries allow raw strings for performance reasons. The DSL can't cover all the features of your specific SQL backend, so when you use its advanced features you might need to type in the query manually.
Signed\* integer overflow. Unsigned integer overflow is well defined with modular behaviour. C is weird. (Yes, I understand why it's like that. I still don't agree it's a good idea to use for code that's supposed to run on the most popular like 5 platforms.)
Well, the "cutting-edge", I wanted to say that these particular packages have additional installation hurdles that do not stem from version numbers: * The `wx` package has an external dependency on a C library, `WxWidgets`, where most of the problems lie. * GHCJS is not on hackage; at the moment, [installation][1] requires you to modify the `cabal` executable. * The person that wanted to try `repa` wanted to try a version that required an unreleased version of GHC (if I remember correctly). Does Stackage help me to install these packages? That would, of course, be totally awesome. :-) That said, I don't think that it is sensible to expect that any of these examples is easy to install, but I wanted to point out that some people who wanted to try Haskell did expect it. [1]: https://github.com/ghcjs/ghcjs#installation-steps
&gt; An email to a package maintainer requesting they add their package to Stackage takes you a few minutes I'm not stopping you doing that :-) If this was the only reason I wasn't adopting Stackage then I might do it, but I'm a long way from that. &gt; Yes: download the cabal.config to your directory. That's it. It's documented here. If you're comparing this to manually giving --constraint options, I don't think you've tried this out, because it's quite a different experience. Wouldn't I have to keep switching to a different version of this file to work with different GHCs? The attraction of --constraint is that it allows me to use the defaults found by the solver (which work well most of the time) and focus on the specific overrides I need at the time. &gt; (And please don't try to claim that PVP upper bounds make this easier, I've spent plenty of hours fiddling with this on commercial projects with dependencies that have upper bounds.) Well, my own personal experience is that they do help: typically I only have problems with projects that don't have upper bounds. I've also found that the solver (combined with sandboxes) has got a lot better in the last couple of cabal releases - whereas before it used to be a real pain to understand the problem when it got confused, now it either finds a solution or clearly pinpoints the reason it can't. &gt; Please don't take offense at this, but: is it a fair guess that you don't normally work with a team of developers on applications that are deployed into production? I don't at present, but I have done in the past. We didn't have any problems with using hackage then. None of this is to say I don't see consistent snapshots as useful, but (a) they're not the only possible model and (b) I think they belong as a layer *above* things like hackage, not as a replacement.
...sounds like unsolvable in general
Sadly I won't aquire these skills anytime soon, neither in Haskell nor in a different language. I'm busy writing dull business code and wrestling with a legacy system. I got into Haskell because the promising parallel capabilities but didn't see much use of it in my toy projects. Currently trying to rewrite a part of our system in Haskell where I hope to utilize more parallelism. It's just not very motivating to code it (in your free time) and knowing it will never be used anyway. To build up real expertise you would need to do stuff like this as part of your job I guess. 
That would be a drastic change.
Not answering your precise question, but I vaguely recall that Scala's type inference isn't as good as Haskell's (in part) because it has to deal with subtyping.
Why _two_ combinators specifically? - [Iota and Jot](http://en.wikipedia.org/wiki/Iota_and_Jot)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Iota and Jot**](https://en.wikipedia.org/wiki/Iota%20and%20Jot): [](#sfw) --- &gt;__Iota__ and its successor __Jot__ (from [Greek](https://en.wikipedia.org/wiki/Greek_language) [iota](https://en.wikipedia.org/wiki/Iota), [Hebrew](https://en.wikipedia.org/wiki/Hebrew) [yodh](https://en.wikipedia.org/wiki/Yodh), the smallest letters in those two alphabets) are extremely minimalist [formal systems](https://en.wikipedia.org/wiki/Formal_system), designed to be even simpler than other more popular alternatives, such as the [lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus) and [SKI combinator calculus](https://en.wikipedia.org/wiki/SKI_calculus). They can therefore also be considered minimalist computer programming languages, or [Turing tarpits](https://en.wikipedia.org/wiki/Turing_tarpit), [esoteric programming languages](https://en.wikipedia.org/wiki/Esoteric_programming_language) that are designed to be as small as possible but still [Turing-complete](https://en.wikipedia.org/wiki/Turing-complete). Both systems use only two symbols and involve only two operations. __Zot__ is a continuized version of Iota that includes input and output. &gt; --- ^Interesting: [^Iota](https://en.wikipedia.org/wiki/Iota) ^| [^Unlambda](https://en.wikipedia.org/wiki/Unlambda) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm19v9u) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm19v9u)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I agree on the library front (e.g., wx), and we don't have a solution in the Stackage world. FP Complete *is* working on some Docker-based build stuff, which would address these issues to some extent\*, but it's not ready for prime time. GHCJS is something I'm working on with Luite. We had a mini hacking session on this at ICFP. For something like this, Stackage can be useful since you can create pre-release snapshots. For example: http://www.stackage.org/stackage/36679b567e01e162593b95f2d17d499d3197a09b (note: probably doesn't work). I suppose if someone was motivated they could use this custom bundle mechanism to create package sets intended for not-yet-released GHC versions. If anyone's interested and needs more information on how to do it, contact me. But I have no plans on putting together such snapshots myself :) \* It wouldn't make it trivial, for example, to build multiplatform executables, but would allow you to easily build a Linux executable on any OS.
I'm not recommending replacing Hackage, so I'm not sure why that even comes up. I mentioned in my blog post that I often times need to drop down to Hackage to perform my curation work. And I fully expect that there are cases for which Stackage isn't sufficient, maintaining a core library with a long support cycle being one of them. That's how I do dev on Yesod and other libraries I maintain, for example. And this is exactly what I've been saying for [at least two years](http://www.yesodweb.com/blog/2012/11/solving-cabal-hell). Living in the wild west has its place. But even someone who's maintaining a core package and therefore *needs* to battle that instability should be able to recognize the value to a normal end user of simply having a set of packages working and ready to be used.
Didn't the ghc 7.8 release have lots of improvements on parralel IO? https://www.haskell.org/ghc/docs/7.8.1/html/users_guide/release-7-8-1.html &gt; GHC now has a brand-new I/O manager that scales significantly better for larger workloads compared to the previous one. It should scale linearly up to approximately 32 cores. I have literally zero experience with what effects it has in real life, but may be of interest. 
Although the article doesn't mention Agda, Idris and Coq by name, the screenshot at the top is Agda code, and those are indeed languages in which you specify mathematical specifications (in the form of ultra-precise dependent types) of what you want the program to do, and then the implementation is checked against that specification. Haskell's type system isn't as precise as those languages, but it's much closer than, say, Java. That being said, I think the article sweeps many important issues under the rug. First of all, they never answer the question in their title: Why don't we use formal methods to write provably-correct programs? Because doing so takes a lot more time per line of code, that's why, and software development projects are typically in a hurry to ship. So it's a tradeoff: some languages allow you to be more correct, while other languages allow you to get stuff done faster. I like Haskell because it strikes a very good balance between those two extremes. For example, the article mentions Hoare logic and displays its most important inference rule, and that rule [can be encoded in Haskell](https://gist.github.com/gelisam/9845116) in the form of indexed monads, which aren't much harder to use than regular monads. Of course, regular monads aren't immediately obvious to most people, which is why I say it's a good balance, not a slam dunk :) Another part of the article which irked me is the quote &gt; the committee put forward in recommendation 32 “that software for key infrastructure be provably secure, by using mathematical approaches to writing code.” But formal methods typically guarantee correctness, which is only part of security. I'm not super familiar with security research, but I know that there are many side-channel attacks, such as timing attacks, which you wouldn't think of checking for if you were focussing on using formal methods to ensure correctness, as we typically do with Agda and friends.
Can't upvote this enough:)
I must have misunderstood your argument in the previous thread - I thought that the premise was that the problems caused by missing upper bounds aren't relevant because we could completely replace Hackage with Stackage. &gt; But even someone who's maintaining a core package and therefore needs to battle that instability should be able to recognize the value to a normal end user of simply having a set of packages working and ready to be used. Yes, and I've already said that consistent snapshots are useful. I'm explaining why I personally wouldn't use Stackage. 
so `CascadeC (c :: t -&gt; t -&gt; *) (ts :: [t])`, yes?
Reflection without Remorse is a rediscovery of thrists with an application in mind. I would summarize it as: * Certain monadic access patterns are slow. * Methods by which this is typically addressed (codensity transform) also suffer from poor performance, but in other access patterns. * My, doesn't this sound a lot like `dlist`? * If we need to both build a list up and examine it as we do so, `dlist` is slow, but `Seq` is not. * Some fancy type and typeclass machinery to get `Seq`s of functions which compose end-to-end to fit in Haskell. * Mentioning that they've basically just defined free categories in Haskell. * Benchmarks showing that they're right about performance.
&gt; running a malicious version of urbit Or not even running urbit. One of my thoughts was building something like freenet, which would leverage a global hash system, into the OS. Regards forgetting data, perhaps you never got it. Someone passing a href is passing a where not a what, and they are definitely not RT. My starting point was actually a functional re-imagining of HTML—there is no paragraph but there is a function that can build a paragraph given the correct input. There is no HTML tag but there is a function that builds a HTML document, pass the same input to a text function, you get a sane text version, etc.
Let me rephrase my argument then based on this confusion: * The upper bound issue is completely irrelevant if you're using Stackage. * There are many issues that are preventing people from using Haskell, or making it more difficult to use Haskell, that revolve around dependency solving. * Stackage solves a very large percentage of those problems. PVP upper bounds have been tried for a long time, and have *not* solved those problems. * Therefore, instead of beating the drum of "use PVP upper bounds" every time this comes up, let's discuss better tooling. And my proposal for better tooling is Stackage. * Hackage should remain what it is right now, but in its current form it should *not* be the default thing that end users face. * If Hackage one day gets the functionality Stackage is providing now, I'm happy to revisit that statement. * Some people will be stuck using Hackage and battling dependencies no matter what. My condolences to them (myself being among that number). But even those people *should* be using Stackage or other improved tooling in the right use cases (e.g., building applications). Is that clear?
Yes, thanks. Given that we're all agreed that Stackage isn't universally applicable, I think it's still important to do as much as we can to make Hackage work.
What would be the point? You can model specific subtype relationships in Haskell98 and full subtyping in Haskell+MPTC. I guess, unless you consider downcasting essential, in which case you would need typeable as well.
How does that make type inference on a JVM-based language worse than it would be in Haskell+subtyping?
See [this paper](http://gallium.inria.fr/~remy/coercions/Cretin-Remy!fcc@lics2014.pdf). I don't think many people know the answer to this question, it would be an open research problem. I agree with Lennart though, massive changes to the Core language are probably not a good thing for anyone and adding subtyping would be very non-trivial to get the implementation right. 
Definitely undecidable by Rice's theorem. Any combibators that are equivalent would have the same result of such an algorithm, so you're looking to accept what is essentially a family of RE functions, which can't be done. 
Well my first thought was 'Why is this in /r/haskell if its a python project'
And anyway, is there any field which game programming draws on that's a 'closed' research area?
Great to hear that FP Complete is succeeding in in getting businesses to switch to Haskell. An application in medical data processing seems like a good fit for the combination of high performance and reliability; if this works out it should be a good showcase for Haskell and FP in general.
Reading these comments, I am starting to form a controversial opinion: Haskellers with the level of expertise being asked for here, and I think it's safe to say /u/snoyberg is one of those, shouldn't work at established Haskell projects like this. They should form their own businesses, attract clientele and give them solutions in Haskell, given their seniority and thus their power to influence their clients. I'm thinking this is what FP Complete is doing. In short, experienced Haskellers should treat FP Complete, and maybe FP Bridge and Flying Frog Consultancy, as a blueprint for what they should do. Why? Because there are a lot of junior-to-mid-level Haskellers out there, plugging away at their own personal Haskell projects, who will never get the chance to ship a Haskell product in industry. Companies like the above would give them the nucleus around which to gain that real expertise and transition from Padawan to Jedi, as it were.
I really don't want to drag this out any further, but... that's a red herring. No one has proposed any way of making Hackage better. Between this thread and the last, there were a few false claims about omitting upper bounds, and some clarifications on my part of what Stackage can and can't do. I haven't seen anyone proposing a way to make Hackage better. And if your answer is upper bounds, it's clear that we've already tried that and it hasn't worked. I'm not going to rehash every one of the discussions I've had on this for the past few years. But that solution has been tried and, if you're still having trouble, it hasn't worked. That includes social reasons, like I mention in the blog post.
What types of workloads? 
For what it's worth, it's not me downvoting you. Your opinion seems perfectly reasonable to me, even if I don't agree. That said: The [article I linked earlier](http://openmirage.org/blog/introducing-nocrypto) is by a group that actually built a crypto library in OCaml. They specifically mention GC timing attacks as an issue and that " The basic approach is to leave the core routines which we know are potentially timing-sensitive (like AES) and for which we don't have explicit timing mitigations (like RSA) to C, and invoke them atomically from the perspective of the GC." [According to RedHat](https://securityblog.redhat.com/2014/07/02/its-all-a-question-of-time-aes-timing-attacks-on-openssl/), at least part of the purpose behind AES-NI in Intel CPUs is that, even in unmanaged languages, AES implementations tend to be very, _very_ cache-sensitive, and therefore prone to timing attacks. AES-NI implements AES in silicon, and purposefully makes the implementation run in fixed time. You'll notice that no point did I say "it's a complete loss, managed languages suck for crypto, let's pack up and go home". I think they're definitely a good idea. All I'm saying is that there are caveats, and it's in everyone's best interest to show the bad with the good, and let people understand the tradeoffs being made.
&gt; there were a few false claims about omitting upper bounds Those claims are only false *under the condition that you're using Stackage*. That's my key point. If you're using Hackage then the lack of upper bounds can and does cause breakage. &gt; No one has proposed any way of making Hackage better [...] And if your answer is upper bounds, it's clear that we've already tried that and it hasn't worked. I've read a good proportion of what you've said about this in the past and I still disagree with you. Upper bounds have a cost and a benefit. The more we can do to get the cost down (e.g. the automatic pull requests idea) the more the balance will swing in favour of using them.
I don't think we have the same hypothesis. I am talking about writing a random application, where a timing attack is clearly a non-issue compared to basing all your trust on a large runtime system that is not audited (AFAIK). Writing a crypto library is something else entirely. That said : * IMO, timing attacks are the least of our problems with C libraries. Memory corruption / leak bugs are far more crippling than any timing attack. So writing a crypto library in C is a worse idea than writing it a memory-managed programming language, or something like Rust (even though you will have to be careful about buffer reuse). * The places where you need to be careful about timing attatcks are probably the same in any languages. I expect to be completely wrong on that point, as I am no crypto dev. But as you say, you can write these parts in your favorite low level language and sleep easy. * There are reasons to think types can help. In Haskell you can newtype your secrets and guarantee you *never* perform a time dependent comparison on them, or any operations except those you specifically checked. This limits the scope of the audit for that kind of flaws. 
I agree that the default for subtraction should be saturated and not partial. Defaults are really powerful, and so we should pick principled defaults ;) Having a newtype-wrapped version with partial versions of the functions is a good idea of course, I just feel that the fewer partial functions we have by default, the better. Also, good to see that "What About the Natural Numbers" was brought up, looks like Prof. Runciman is getting what he advocated for. Better 25 years later than never :)
I'm pretty sure it means you no longer have unique most general types. This makes type inference much more complicated.
I wouldn't mind doing dull business code if it was in haskell.
What's the point of asking any hypothetical?
Well, I consider myself to be a pretty good Haskeller, and I used to have a small (non-Haskell) business, but it turns out the skills required to be a good co-founder are not the same as those required to be a good Haskell programmer.
Oooh, interesting paper. I'll add it to my queue! And yeah, it certainly wouldn't be a trivial effort, that's for sure...
Ive answered that non retort about four times already. I'm not going to bother saying the same thing again. If you can't be bothered to read and respond to what I've already said, there's no point trying to have a meaningful discussion.
wait... wouldn't that cause polymorphic generic code relying on Num laws to produce wrong results without warning?
getting a hypothetical answer? :-)
&gt; I agree that the default for subtraction should be saturated and not partial. Somewhat related: Is there a nice, well-agreed-upon package that tries to get rid of horrid typeclasses like Num and builds a principled hierarchy in which Natural would be a monoid?
Well, you tried starting your own business, which I think is something everyone can respect. It's accepted that a lot of startups don't make it.
Its amazing how you are being critical of a work before even reading the table of contents. I saw the strange loop talk in person, and I have to say that it was pretty good. The talk was never intended to be for those who are already familiar with game programming or masters of Haskell programming. Her audience at Strangeloop, for example, was made up of many who had never encountered a line of Haskell before and I remember most people being of positive opinion as they exited the room.
Or girl 😊
Exactly :D
...but not silently! 
How are you not choking on the irony of what you just wrote? You've invested significantly in pushing your own solution and trying to get people to participate in your blessed solution and zealously spread the good word in this thread.
I think I was quite clear that I'm religious about my views too. __EDIT__ I was distracted when I wrote this response. While true, it ignores the heart of what my complaint is in the blog post, namely: PVP upper bounds have been pushed for years. We still have lots of problems installing packages from Hackage directly. Despite this fact, PVP upper bounds advocates insist that PVP upper bounds will improve the situation, despite evidence to the contrary. That's why I imply this is faith. By contrast, I'm making very direct claims about what Stackage will and won't do, and I'm encouraging everyone reading to try it out and test those claims. I claim that rarely, if ever, will you run into a situation where you get an invalid install plan. On the other hand, I don't claim that all build problems will be solved. Missing system libraries, for instance, are still a problem. So do I believe Stackage solves problems better than continuing to talk about the PVP? Absolutely. But I'm not asking for a leap of faith, I'm asking people to perform a simple experiment: try it. If it's better for the majority of use cases most of the time, we should start moving over to it as the default toolchain. I see no objective standard of measure coming from PVP upper bounds advocates. *That's* why I feel my comparison- while not intended to be taken seriously- has a grain of truth to it.
Absolutely!
I don't know which ones, if any, provide a monoidal Natural type. Two packages that provide more fine-grained hierarchies are `numeric-prelude` and `algebra`.
There is `numeric-prelude`, but that package makes the controversial design choice of relying solely on module qualification to disambiguate names.
It might interfere with type inference
Right, that's the idea :-)
I wonder if `MarkupM` itself can be made a monad transformer, sidestepping the need for a `Writer` here.
In Haskell, when the type inference algorithm sees `f x` it creates an equality constraint: the type of `x` is equal to the type of the first parameter ot `f`. In a language with subtyping, this constraint is an inequality constraint instead: the type of `x` must be a subtype of the type of the first parameter of `f`. This extra degree of freedom means that you can't infer as many things without using heuristics.
IIRC the subtyping is the biggest problem. Scala actually has a really clever inference algorithm but you can't infer everything when you allow subtyping.
Would you mind talking about the product? I can email as well if you'd prefer.
What are some concrete advantages of this? o_O
This is awesome, thanks for taking the time to do this! I'll have to poke around in your code to see if I can speed up my own library. From the looks of your code, you seem to have a more advanced handle of Haskell.
I'm not 100% sure that you're implementation is correct w.r.t. the laws, but is there any reason you allow multiple ways to represent the same thing? You could define things like: data Many2 a b = Many2 (Maybe a) (Maybe b) deriving (Eq, Show, Ord) The other cases can be covered with the right combination of Maybes, so they're sort of redundant. If you need to represent the other cases you could still define a second type (or even just wrap Many2 in a new type), and then your toManyX and castX functions would be a little more self-explanatory (since their implementation would be implied by their type). You also don't need to tuple up things before pattern matching. mappend x y = case (x, y) of (x', NoneOf1) -&gt; x' (NoneOf1, y') -&gt; y' (x', _) -&gt; x' can become mappend x NoneOf1 = x mappend NoneOf1 x = x mappend x _ = x I suspect what you're trying to get at is (First a, First b, ...). First is just a new type wrapper for Maybe that makes its monoid instance left-biasing, and tuples are monoids iff their elements are monoids, so you get Monoid (First a, First b, ...) for free without any extra work on your part. EDIT: also, whenever you're looking for things on Hackage try to click `contents` at the top-right of the page to make sure you're looking at the most up-to-date version. Base is 4.7, so the monoid page you should be looking at is https://hackage.haskell.org/package/base-4.7.0.1/docs/Data-Monoid.html. Google is unfortunately not your friend when looking for stuff on hackage, though I don't think anything relevant has changed relevant to your post.
The `LiftMonoid` newtype would be useful in its own right (e.g. for `Maybe`), but shouldn't be used as a reason not to give a `Monoid` instance for `IO`.
These monoids are isomorphics to tuples of maybes. (Just 3, Nothing) &lt;&gt; (Nothing, Just "hello") =&gt; (Just 3, Just "hello") (Just 3, Nothing) &lt;&gt; (Just 5, Nothing) =&gt; (Just 8, Nothing) These are indeed monoids, as are all compositions of tuples of monoids. In the Data.Monoid package you can find the First/Last types, which would give you e.g. (First 3, Nothing) &lt;&gt; (First 10, Nothing) =&gt; (First 3, Nothing) (Last 3, Nothing) &lt;&gt; (Last 10, Nothing) =&gt; (Last 10, Nothing)
`First 3` doesn't work, alas, nor would it combine with `Nothing`. You need `First (Just 3)` and `First Nothing` (or just `mempty`) 
But.... what would be the difference between that and just executing all of the monadic actions in advance and passing things in as parameters? The reason why StateT etc. are useful is because you can branch on the results of stateful events to decide what IO actions to execute. but none of blaze's writer actions have any results, so... 
My bad. Also there is no default monoid instance for integers, so these examples won't really compile. The point hopefully still stands, i.e. that this type of functionality is easiest to implement as the composition of monoid instances over tuples.
Nothing special, just managing Google adwords, making landing pages, paying a call center, and choosing clients whose products were expensive enough for profit sharing to pay back for all that. With the new innovative products announced by startups on HN every day, I'm sure you expected something more glamorous... But it's good to remember that not all businesses are startups!
The elephant in the room is IO actions are not necessarily associative. I don't see this mentioned (did a search for 'assoc') but maybe it was. -- UPDATE I'm wrong
How aren't they? They certainly aren't commutative, but using the applicative instance to derive &lt;&gt;, you basically have `(x &lt;&gt; y) &lt;&gt; z` equals "do x then y, then do z, and append the results, whereas `x &lt;&gt; (y &lt;&gt; z)` equals "do x, then do y then z, then append the results. The sequencing of actions should be unaffected.
&gt; (First 3, Nothing) &lt;&gt; (First 10, Nothing) &gt; First didn't work quite since on my machine it has the type `First :: Maybe a -&gt; First a`. The other problem I have with it is that Maybe's instance of monoid requires the contained to be a monoid too, so you can't use it to get sum of 2 lists. Though, useful stuff I didn't know about.
I proved the monoid laws for this general pattern in Appendix B of [this post](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html).
If you compare the implementations of `WriterT` and `StateT` you'll see that both monad transformers "only" pass around results as parameters. `WriterT` and `StateT` are useful for the same reasons, they let you write cleaner code by doing tedious work (passing around parameters) for you. Also, you don't branch on the writer content but on the monadic actions, which in turn can be stateful (e.g. empty IO input, or combinations with other monad transformers like `StateT` etc.).
Yeah I didn't think this through. I sort of assumed this instance didn't exist for that reason :p. like /u/htebalaka said the effects will only detect the order which is unchanged.
How is subtyping different to a type constraint on x?
If you are trying to build templates by composing a bunch of fragments together, then doesn't executing all the monadic actions in advance become a larger and larger burden on the caller? And, let's say a fragment decides it needs to do another IO action -- now anyone calling that fragment must update their code?
First has to be put on top of the Monoid. There are 4(?) different monoid instances that Maybe can have: left-biasing, right-biasing, short-circuiting, and Monoid-wrapping. The default is the Monoid-wrapping, over-riding the default mempty to be equal to Nothing, but if you do `(First (Just 3), First Nothing) &lt;&gt; (First (Just 10), First Nothing)` you'll get the instance you want, and the Monoid constraint on the elements will disappear.
I'm talking about type inference. This might have written what I was talking about in a clearer way: http://stackoverflow.com/a/22533286/90511
Could you elaborate on why you feel that totality is essential even when it (necessarily) leads to semantics which don't match the underlying mathematical intuitions in many cases?
For starters, you could make a newtype wrapper around some existing monad (or a stack of them) in order to clean up some API. I've done so, for example, with [HDBC](http://hackage.haskell.org/package/HDBC)'s `Connection`, wrapping a `ReaderT Connection` and implementing a monadic equivalent of the `IConnection` interface. This is mostly boilerplate and thus not the most exciting of projects, but it has somewhat helped my understanding of the `transformers` and `mtl` packages.
I thought there were some issues with exceptions, aren't there? Or was is trouble for some other typeclass?
Thanks, that's interesting! So 70% of those packages have no (cabalized?) tests at all. 
I once implemented a monad transformer for memoising monadic computations in a particular way. And [haxl](http://hackage.haskell.org/package/haxl) uses a specialised `Haxl` monad for performance reasons, even though it can be implemented IIRC as `ReaderT` over `ContT` over `IO`. Implementing custom monads, like writing recursive functions without `map`, `fold`, etc., is generally considered a “low-level” thing you don’t need to do all that often. On the other hand, it can be very useful to notice that some data type you have admits instances of `Functor`, `Applicative`, or `Monad`, since you can benefit from lots of existing machinery that works on such instances. 
You can play games with bottoms but I just had a brain fart
ill look at this, although the concept of doing this seems rather intimidating to me given my inexperience
&gt; Implementing custom monads, like writing recursive functions without map, fold, etc., Could you tell me more about this? How would such monads be written? Sorry to focus on just this point, the rest of your post is well noted
IMHO virtual-dom as well as React tries to solve a problem specific of javascript: the lack of a intuitive, composable way to update the HTML-DOM tree. So it creates a composable intermediary, a lineal virtual-dom. Basically, it is string concatenation. When changed, the intermediary lineal description is rendered. But the rewrites and re-rendering are expensive, so they need to update only what is changed. That problem disappears when there is a way to write directly the HTML-DOM in a composable way, using a monoid or a monad. Then, to rewrite any branch, just go to the element and update it also in a composable way. That is what I want to detail below: Basically, both virtual-dom and react use intermediary lineal images of the dom just to be able to do this: div [atribute "width" 100] $ ("hello") &lt;&gt; div [attribute "height" 200] "whathever" Since strings are composable in javascript as well as in any language. But trees are not. That composable syntax is impossible in javascript with the HTML DOM directly. But Haskell has a lot more tricks. It is possible to compose functions as monoids. these functions may be builders of DOM elements. these builder functions create dom elements, attributes etc using DOM primitives inside. That is what ahem.... what my perch library does. https://github.com/agocorona/haste-perch It updates directly the HTML-DOM in a composable way as if they were blaze-html elements IN the browser: div $ do div ! atr "width" "100" $ "hello" div ! atr "height" 200 $ "whathever" and so on. `div` here is a builder function. it can be composed this way, monadically or with a monoid syntax. Both are equivalent, like in the case of the blaze-html library. but while blaze-html concatenate strings, perch concatenate builder functions that create trees. In the above example, in creates a div node with two more divs inside. Do you want to update an element? modify it directly. Since it does not update an intermediary description, there is no need to detect differences in the intermediate description to avoid to render again the whole description. It simply does it in the DOM. forElems ".classtochange" $ this ! width "300" ! atr "class" "changed" The above expression is similar to a jquery modification. it is applied for all the elements that match. in lineal descriptions like the ones of virtual-dom or react each element is not directly addressable. To address the elements is as expensive as to sequentially rewrite the description with a new one. The need to detect differences is then a consequence of a lineal description. And the lineal description a consequence of the need of composability. But a set of monoidal combinators of functions that update the DOM are composable . And it is pure Haskell, no need to use Javascript libraries.
Not sure if this is exactly what jlimperg was talking about, but Monad is a great way to make a safe API around some functions that might otherwise work live in `IO`. For instance, say you have a library with the following functions: setup, teardown, fireMissiles, duck :: IO () And the documents say that you can only `fireMissiles`, and `duck` after calling `setup`, and before calling `teardown`, and the latter two functions must be called each only once, and if you forget to call `teardown` it will eat your harddrive, etc. etc. This is a real shit API. Instead we can wrap it and expose the functionality behind a nice API that doesn't permit wrong usage: -- we won't export the data constructor to user, only the type: module M(MissilesGame(), runMissilesGame, fireMissiles, duck) where newtype MissilesGame a = MissilesGame (IO a) deriving Monad runMissilesGame :: MissilesGame a -&gt; IO a runMissilesGame (MissilesGame io) = setup &gt;&gt; io &gt;&gt;= \a -&gt; teardown &gt;&gt; return a Something like that at least.
The whole notion of `forElems` feels very imperative though, and it's exactly that which React et al free us from. I no longer have to consider changes to a HTML document in the context of what the previous state might be - I simply emit the entire tree that I want, and only the minimal amount of work is done to realise that. I can't say I can really see how that plays out in `perch`.
That does sound like a very useful thing indeed. My project was primarily for the purpose of eliminating the explicit passing of `Connection` to every function (with a side dish of generalisation from `IO` to `MonadIO`), so not too much was gained. But a nice exercise nonetheless.
Rwh was after real world experience. 
&gt; short-circuiting What's this one look like? All of them have to have: (in order to be a Monoid with mempty = Nothing) Nothing &lt;&gt; Nothing = Nothing Nothing &lt;&gt; r = r l &lt;&gt; Nothing = l left-bias is First, with the `Just l &lt;&gt; Just r = Just l` rule. right-bias is Last, with the `Just l &lt;&gt; Just r = Just r` rule. Monoid-wrapping is the default with the `Just l &lt;&gt; Just r = Just (l &lt;&gt; r)` rule. The only other combination I can think of is `Just l &lt;&gt; Just r = Nothing`. That would be quite odd, and I'm pretty sure I wouldn't describe it as "short-circuiting".
The inner workings of transformers/mtl can be a bit tricky at times (particularly for `ContT`), but luckily this is an area where the type system will prevent the vast majority of errors. (At least of those errors I was inclined to make, which were plentiful.) If you try a project like this, be sure to take a look at [GeneralizedNewtypeDeriving](https://www.haskell.org/ghc/docs/7.8.1/html/users_guide/deriving.html#newtype-deriving) if you haven't already. This will save you a truckload of tedious boilerplate.
&gt; "Having cabal-install automatically determine upper bounds based on upload dates is entirely possible." That's not just entirely possible, it's genius! The absence of an upper date implies that on the day I uploaded the package, I was testing with the most recent version of its dependencies (my Travis instance ensures that anyway). Therefore, if you want PVP compatible dependencies on any of my packages, just take the date, determine what the packages I must have been testing with were, and round up to the next 0.1. I don't know anyone who is arguing for upper bounds that doesn't do exactly that, just by hand. And it also provides a great way for Hackage to "bump" the bounds without changing the .cabal file, you just need to "kick" the upload date into the future.
It's not only an idea, it's implemented in Stackage. Just set remote repo to: http://www.stackage.org/hackage-view/pvp/ Caveats at: http://www.reddit.com/r/haskell/comments/2m14a4/neil_mitchells_haskell_blog_upper_bounds_or_not/cm13tl4 This is the kind of improved tooling that seems to always be ignored in favor of "you should have put in upper bounds." I officially gave up trying to work towards any compromise solutions with upper bounds fans after my emails on this approach never amounted to any real followup.
Then make it a monoid not a monad, and don't use do notation
Well, say you have a data type: data OneOrBoth a b = One a | Both a b Then you notice “hmm, I think I could write a `Monad` instance for this”, so you implement `&gt;&gt;=` and `return` in some sane way. In this case, in order to conjure an `a` value out of thin air, and to combine `a` values in a sane way, I require `a` to have a `Monoid` instance. instance (Monoid a) =&gt; Monad (OneOrBoth a) where return = Both mempty One a &gt;&gt;= _ = One a Both a1 b1 &gt;&gt;= f = case f b1 of One a2 -&gt; One (mappend a1 a2) Both a2 b2 -&gt; Both (mappend a1 a2) b2 Sometimes, the instance turns out to be useful. In this case, it’s not particularly—this is the type of computations that must log an `a` at every `Both` step, or log a final value and terminate with a `One` step. [I implemented this](https://github.com/evincarofautumn/Extra) ages ago, but never bothered to put it on Hackage. Now /u/edwardkmett will probably come along and tell me how incredible this type is, and how it’s already been implemented in one of his packages. :P 
Yes, that's why I wrote "(cabalized?)" but that probably wasn't clear. I also don't always cabalize my tests, so the percentage of packages with tests is probably higher.
I'm not /u/edwardkmett, but this is simply `MaybeT (Writer a) b`. :)
&gt; And it also provides a great way for Hackage to "bump" the bounds without changing the .cabal file, you just need to "kick" the upload date into the future. Wouldn't it still need explicit intervention to decide whether this should happen? I thought it's the need for that intervention that puts people off upper bounds.
Fair enough, but the developers have a different opinion.
yeah the point is that their opinion is wrong since they made a monad instance that violates the monad laws -.-
I think the right solution would be to extend the do-notation to support more things than monads.
A number of the ideas seem to depend upon synchronization between different package maintainers. The first suggestion (which sounds a lot like the second) seems nice at first blush, but I wonder if people really will notice/bother to switch to hard upper bounds when a breaking release comes along after the user of that package has uploaded their own. Regarding your post on implicit blacklisting, what happens if I notice that x-0.4.3.3 will break my package, so I bump my x &lt; 0.5 down to x &lt; 0.4.3.3, but one of the many other packages people use isn't broken by that change and continues to specify &lt; 0.5? I think the first problem with all these conversations is the lack of agreement on which is the bigger problem - excessively restrictive bounds increasing conflicts, or excessively open bounds increasing the potential for errors. It seems like you've gotta take from one to give to the other.
it could be called `matchElems` . I have to say that the virtual-DOM-React approach is ideal for functional reactive frameworks like Elm and others, since them manage the page as a whole. Every signal may change the whole page. I´m not an expert and my knowledge of this is far from accurate, but it seems to me that there are no event scopes, since the events feed a single entry point that re-render the whole page, as far as I know. Or alternatively they modify placeholders in a static template. Without React , Elm and all the functional reactive frameworks were limited to the second option. With it, they can perform more dynamic DOM effects. without React, it was hard to create a responsive TODO application in a functional reactive framework. [Monadic functional reactive] (https://www.google.es/search?q=monadic+reactive&amp;oq=monadic+reactive&amp;aqs=chrome..69i57j0.3260j0j7&amp;sourceid=chrome&amp;es_sm=93&amp;ie=UTF-8#q=monadic+functional+reactive+programming) may be more promising, since the events can be catched and managed locally (and the DOM updates are local too) they don't affect the whole computation and the re-rendering of the entire page . As the paper is the Google search says: "In other (non monadic) FRP approaches, either the entire FRP expression is re-evaluated on each external stimulus, or impure techniques are used to prevent redundant re-computations" . That is the problem that Monadic Reactive solves. I don´t know how to to slip here [hplayground](http://github.com/agocorona/hplayground) which is a monadic functional reactive framework for Browser applications. For this framework, Perch is perfect. But also is fine as such, with event handlers managed by perch itself. 
I keep wondering if there's a way to build an algebra of lenses over the HTML tree such that all updated operations are expressible both as changes and as diffs, then you play in the land of composing functions `(HTML -&gt; HTML)` and perhaps minimal diffs are created automatically.
I would love to get Amsden's TimeFlies AFRP framework into this space. Now events update only the relevant parts of the page and those updates can be effected via a simplified diff.
Note that that property isn't true, since `drop` returns the empty list if you want to drop more elements than are in the list.
 mconcat [ foo , bar , baz , quux ] tends to be `do`-notation-y enough for me.
Well, it is true *if* the naturals saturate at 0. length :: [a] -&gt; Nat xs = [(), (), (), ()] length (drop 5 xs) = (length xs) - 5 length ([]) = 4 - 5 0 = 0
That was non-intuitive for me at first, but it makes a lot of sense. Starting to really appreciate the value of equational reasoning. Thank you.
Yeah, unless it gets too nested, then those commas become unbearable.
You're welcome! Also, the proof is much more intuitive if you rephase the `Applicative` class and laws using the `Monoidal` formulation that Edward Yang [describes here](http://blog.ezyang.com/2012/08/applicative-functors/). Then the proof becomes almost trivial. This formulation is also described in [the original `Applicative` paper](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf).
Edward Yang wrote a really nice post on the [fundamental problem of package management](http://blog.ezyang.com/2014/08/the-fundamental-problem-of-programming-language-package-management/) where he argues that package management is by nature a distributed problem and that it therefore comes with all the problems associated with distributed programming (like synchronization).
How can a fragment decide it wants to do another IO action unexpectedly? None of the blaze things have any results to even branch on. Anything that *does* have results.... well, would be in IO and not depend on anything blaze anyway, right? 
&gt; The wide number of Hackage-related build failures began precisely because people who bought into Stackage began removing upper bounds. I wasn't aware of that. Who did that?
Doesn’t this also mean you can’t prove whether an arbitrary system is Turing-complete?
Great, then I don't know whether the do is a monad, an applicative, or a monoid... -.-
You, Brian O'Sullivan, Edward Kmett, Michael Snoyman are the most common authors when I have a build failure related to missing bounds.
Ah, but I've never bothered with preemptive upper bounds. I wasn't aware that Edward didn't put preemptive upper bounds on his packages, I'd pegged him as a preemptive upper bounder. I'll ask him. Granted, Michael probably changed his behavior after giving up on this particular policy. Is Bryan using Stackage? I suppose working at Facebook they might have the need for a stable package set. I've no idea.
No principal types. You can see what an attempt at that might look like in Scala. Having worked with it, I don't think subtyping is generally worth the pain, but perhaps nobody's figured out how to make it pay for itself.
Also, I don't want to make it sound like you guys are malicious or to blame. You all just happen to be prolific so if there is a build error it's more likely to fall under one of those authors.
I think the best way look at Graham Hutton book, chapters on Parsers and I/O.Try to implement the code in those chapters in Ghci. It will give you good starting point building intuition. After that look up "you could have invented monads" by Dan Piponi. I don't think you need worry to much about implementing monads in order to understand them. What helped to get a better sense about Monad is to look at it a data that you can transform with monadic operations (&gt;&gt;=) and return. Note, return is building blog for a more complex monadic functions. ( I assume that you know the type of &gt;&gt;= and return have.) If i am wrong in the above explanation please correct me. 
What about writing a simple DSL that does something you're interested in? What _are_ you interested in?
Which is ironic :(
I'm under the assumption that Backpack is going to clear up a lot of problems given packages move to it.
&gt; There seems to be wide consensus that B is a much better default, but one vocal opponent disagrees. I don't disagree, I just want to make sure that changing this default isn't going to result in a whole lot of broken programs.
Let's see, can we write another of those halting-problem reduction proof? We'd need to encode "system" as a function of some kind. Ah, I know: each such system could be represented as a simulator program S which accepts as input a string P representing a program in the system, which runs program P in the system, and which returns whatever P outputs (or never returns if P doesn't). Now, we want to determine whether program A terminates. Take a system which is known to be turing complete, for example turing machines themselves, and modify its simulator S into an alternate simulator SA as follows. First, SA behaves as S, that is, it simulates its input program P. Next, if P halts, SA executes A. Finally, if A also halts, SA returns the output of P. Now, if A halts then SA behaves as S, which is turing complete. Whereas if A runs forever, then SA runs forever on all inputs, and so is unable to simulate turing machines which halt, so SA is not turing complete. Therefore, if we had an oracle which could tell us whether any such SA was turing complete, we could solve the halting problem. That was fun!
&gt; instance Monoid (Maybe a) &gt; mempty = Just mempty &gt; Nothing `mappend` _ = Nothing &gt; _ `mappend` Nothing = Nothing &gt; (Just a) `mappend` (Just b) = Just (a `mappend` b) Huh. Okay. TIL. &gt; Note the default behaviour doesn't actually even need a Monoid constraint; [...] Yeah. I'm not sure that matters too much (that we use Monoid instead of Semigroup as a constraint) since we don't really verify our Monoid (or any typeclass) instances, so you can generally "fake it" if you need to. For example, only expose the Maybe wrapper and just leave mempty for underlying type as `undefined`.
I had another thought regarding this. When programmers try to be founders and the startup ends up not being successful, that doesn't necessarily mean that _programmers_ aren't good at making startups; after all, lots of people try to start businesses and don't make it. It just means there's no guaranteed formula for a successful startup, for anyone from any background.
The monad transformers are an excellent set of building blocks for making custom monads, they take care of a lot of the work necessary to implement one. So try to figure out for yourself how to wrap some combined monad transformer, for example: data MyState = MyState{ history :: [String], printDebug :: String -&gt; IO () } newtype MyListMonad = MyListMonad (StateT MyState IO [a]) Try to figure for yourself out how to instantiate MyMonad into Functor, Applicative, Monad, MonadPlus, MonadIO, and MonadState. Most of these instances can be done with just one line of code. Another fun example might be to create your own wrapper monad around one that instantiates MonadError (in the Control.Monad.Except module): data MyError = FOO | BAR | ERR String data MyState = MyState{ history :: [String], printDebug :: String -&gt; IO () } newtype MyErrMonad = MyErrMonad (StateT MyState (ExceptT MyError IO) a) The above can be instantiated into Functor, Applicative, Monad, MonadState, MonadError, MonadIO. You can also instantiate the "fail" function of the Monad class like so: instance Monad MyErrMonad where return = ... a &gt;&gt;= mb = ... fail = throwError . ERR Try a few combinations of monad transformers, see if you can get your custom monad to compile. Try making monads that lift IO and ones that are pure, without IO. It is challenging, but once you get the hang of it, you will be an unstoppable Haskeller. 
&gt; you seem to have a more advanced handle of Haskell. Probably not ... I just happened to have read about morphisms when I wrote *kdtree* ... Actually that gave a pretty nice boost in performance. You achieve the same timings without "cheating" ... So who's more advanced now? :) Apart from that I really like my approach of lazily returning all neighbors sorted by distance and than just implementing all other functions on that. 
That's the price for any kind of overloading. Anyway, you can always use an explicit type signature.
Awesome, thank you. Of course, my conjecture is that while you can’t prove termination for arbitrary programs, interesting programs are not arbitrary. That’s the basic bet of type systems, really—failing to type some correct programs is fine, as long as they’re not interesting. 
¿Por qué no los dos? I'd rather have first 10 and last 10.
I believe this is fixed and the full log is now shown. Not sure if this is in a released version or just in HEAD though. 
While working on my own projects, I tried to use the same set of dependencies (not necessarily all of them) for each project. The goal is to make sure that I can move around part of those projects (e.g. extract some code, make a package out of it and use it in two projects). The approach I took was similar in spirit to Stackage: I maintain a Docker image with GHC + all my dependencies. When I compile a project, I use that image and disallow access to Hackage. That way I can see when I have introduced a new dependency that I don't exlicitely installed in the Docker image. With the time moving forward, I started to pin more and more dependencies (and live more and more in the past). By this, I don't mean choose a specific version on my projects cabal file. I mean choosing a specific version for packages installed in the Docker image, often indirect dependencies of my project cabal files. This means that while I took an approach close to the idea of Stackage, the reason that make me change my dependency versions is too loosy upper bounds. I like the idea of a set of packages that can be used as foundation for related projects. I like the idea of Stackage. But I believe that this idea can be done only on top of a PVP compliant Hackage. In the case of Stackage, people have decided to fix their package very quickly, in order to offer the next snapshot. But if you want to create your own set from Hackage, without a known starting point (w.r.t. to versions), and without people necessarily keeping track of your set, the PVP helps a lot.
I understand and relate to that point of view completely. I think it's addressable with some amount of work (running many test suites, reviewing a lot of code that uses the primitives, etc). I'm somewhat frustrated by the point of view that we should require each and every cleanup to manually decide what to do with async exceptions because of the extremely rare situation that something useful can be done, while making virtually all uses of bracket subtly incorrect in the process.
This is very common for package managers and build systems. It outputs the most relevant bits (why it failed), and then points you to the log if you want more information. I don't see the problem. That's the way I prefer it.
What about a separate typeclass for [saturated arithmetic](http://en.wikipedia.org/wiki/Saturation_arithmetic) operations such as `(∸)` ? All the `Bounded` integer types would have instances as well. 
If you're going for conciseness: kCombinations n = filter ((== n) . length) . filterM (const [True, False]) (not tested)
I strongly disagree with your notion of "fixing sync part". Your exception handlers for the sync case do not fix either sync exceptions' handling nor async exception handling. They merely minimize the damage done by the sync exception - but your program is already going to be at least partially incorrect. I think there's a pervasive idea (not only in the Haskell community) that exceptions can be "handled" and the error/situation can be fixed. This is rarely the case except in some very local situations (e.g: catch file-not-found and immediately handle that). Usually, if a sync exception that occurred the operation that was attempted cannot be salvaged any longer and a different strategy should be attempted, and often there is no possible different strategy. Thus, the best way to "handle" sync exceptions in cleanups is to work very hard to make them an impossibility in the first place. If you failed to do that, then the next thing you might do is non-locally salvage the situation (e.g: rebuild part of the database). If you failed that, you might give up on that operation hoping it isn't too important, but you've almost certainly already lost. To say that due to this we should "do the same" with async exceptions simply does not work.
This is similar to the following cases: - What is the square root of a negative real number? - What is the division of integers n/k where n is not evenly divisible by k? In these cases too, the number that we want isn't in the original type. For n/k Haskell chooses not to provide division on integers. int/int is a type error. If you want integer division you have to use n `div` k. For square root of a negative number Haskell uses the floating point convention of returning NaN. I personally don't like saturated subtraction, because you lose the primary law of subtraction: (x - y) + y = x I think a better choice is either: 1. Disallow subtraction on naturals, require manual or automatic conversion to integer. 2. Give subtraction the type nat -&gt; nat -&gt; int. Along with a `saturatedSubtract` function in analogy with `div`.
Can I see some of your HTML code written like that? It might not be as horrific as I expect.
You missing the point. Yes, "fixing sync part" mostly means "minimize the damage", but that doesn't matter. When I compose two cleanups, I want to "minimize the damage" too, so I do "cleanup1 `finally` cleaup2". Here I rely on each cleanup to do correct thing, and I don't need uninterruptibleMask. I just perform whatever default behaviour of each cleanup. If e.g. cleanup1 can't do correct thing in case of async exception, then it should use uninterruptibleMask internaly. There are cases when I can't compose cleanups easely, e.g. when cleanup2 depends on a result of cleanup1. But that cases probably requier special handling to minimize damazy in case of sync exceptions anyway.
The question really is if you tailor the output to the developer or an end-user just trying to install some software. In the latter case, it's preferable to limit the output to not overwhelm the user.
Have you seen https://ghc.haskell.org/trac/ghc/wiki/ApplicativeDo
This level of intervention is both slight and easily automatable - it's far less than uploading a new package version.
&gt; All IO actions, including ones usable in cleanup context, try to ensure the same exception semantics (effect does not happen in case of exception, as much as possible). I think this is essentially the "strong exception-safety guarantee" usually discussed in the context of languages like C++, for what it's worth.
Yes, I mostly agree with your summary, thank you. I found that "philosophy" the only acceptable though, because otherwise you'll fail to "minimaze damage", regardless of async exceptions. It is "brittle" mostly because of bad documentation, e.g. missing contracts, lack of tutorials, etc
But how often do the last 10 lines really contain why it fails? If there is a stack trace for example then chances are very low that you'll find the relevant info in the last 10 lines.
The intervention comes in two parts: the decision and the actual change. The decision either has to be made by a human, or by automated testing. That's no different to the current world. Actually making the change seems like the less difficult part anyway. It's not hard to imagine writing a script that does the bump+upload. That's just a trade-off against the complexity of having this upper bound logic on the server. 
Well, C++ did completely different design decision -- it bans exceptions in cleanup ections (destructors). If destructor throws excaption *and* we are already handling other exception, then the program is terminated. I can argue that cleanups in C++ follow *my* approach, because they *do* try to minimize damage. The difference is that they don't rethrow exceptions if they don't want the program to terminates.
I think your philosophy has two major disadvantages: * Requires far more work to review the code that goes into cleanup contexts * Fails in much worse ways (broken program invariants as opposed to deadlocking) when the review is not done properly And one minor advantage: * When async exceptions occur during a cleanup handler, it can be used as a useful hint for altering behavior. I don't think I know of any actual example of this in practice, though. So why prefer that philosophy over the opposing philosophy which simply blocks async exceptions completely during cleanup? Is it because you think the "damage-containing" you do for sync exceptions somehow helps you here? Because if so, that is our source of disagreement on the philsophies. Also, you've not addressed the other disagreement, which leaves me completely puzzled: It sounds like you prefer stopping an unbounded-blocking-operation even if it comes at the expense of the guarantees the bracket is supposed to give. Another way to view it - is that you don't believe brackets should ever give any useful guarantees!
Yes, I actually misquoted a bit, sorry - I just wanted to point out that "All IO actions ... try to ensure the same exception semantics (effect does not happen in case of exception, as much as possible)" is essentially the strong exception guarantee (and thus it might make sense to pull in discussion of existing literature on this topic). I didn't mean to make any statement about how this relates to cleanup actions.
Let me repeat: I'm not against uninterruptibleMask in bracket per se. So the advantages and disadvantages you described are false. I just think that you don't need it in many cases (if you follow my philosophy of course). I prefer my philosophy because it is safer and easer to reason. (But I still find haskell broken w.r.t. exceptions, including async. I claimed that a number of times on reddit and #haskell) re unbounded-blocking-operation: I prefer to think about them carefully each time. Blocking is not an universal solution here. I think usually it is enough to have eventual guarantees. E.g. that child process will eventually be terminated.
What is “program slicing”?
I've noticed that when I ask hackage to change the cabal file, it adds a field called "[x-version](https://hackage.haskell.org/package/haskell-awk-1.1/haskell-awk.cabal)". Maybe cabal-freeze should store both the version and the x-version? Is it possible to ask hackage for a previous x-version of the cabal file?
One potential issue is that there isn't a unique Monoid instance for IO here. In particular, there's an analogous potential Monoid instance to Maybe using throwIO and catch. In case of ambiguous instances the usual Haskell approach is to use newtypes.
That context is useful "often enough". Besides, opening the file isn't exactly hard. But don't get me wrong, I don't feel strongly about this change.
&gt; Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. (from [[Wei84]](http://www.cse.buffalo.edu/LRG/CSE605/Papers/slicing-weiser.pdf).) You can find an example of [program slicing](https://en.wikipedia.org/wiki/Program_slicing) on Wikipedia.
&gt; All IO actions, including ones usable in cleanup context, try to ensure the same exception semantics (effect does not happen in case of exception, as much as possible). Should hClose close file descriptor if it fails to flush buffers? Currently it does, but it shouldn't according your philosophy. But you can't have even basic exception safety then. You *need* special kind of actions (cleanups) to implement exception safety
&gt; I judge the quality of a solution by how much it automates the process Based on this metric, are there any solutions you can currently recommend?
scares me a bit that they thought that security measure would work... the whole proxy business is not new
No
The `Monoid` instance you describe would be better suited to being an `Alternative` instance, IMO. You allude to other possible `Monoid` instances for `IO`: what are they?
Native speakers would find it "demanding" too, and it's daft to think anyone would infer the archaic meaning. 
Nothing personal offhand, but here's some from `Apiary`: http://hackage.haskell.org/package/apiary-1.1.0/docs/src/Data-Apiary-Document-Html.html
What kind of failure? Is it possible to eventually retry and succeed? In that case I think it should block indefinitely until it succeeds. If the buffers are surely lost, then the program has already entered a bad/inconsistent state, at which point it may as well close the handle and throw an exception. That's because whether it closes or not, a harmful side effect had already happened so the guarantee of no effect due to failure is lost anyway. I disagree that it is because it's a cleanup handler. It's actually because it's one of the occasions where you cannot implement exception safety so you need an ad hoc API/contract instead. This is true for actions that are unrelated to cleanup too.
These date-based solutions are definitely better than nothing, but I think they're still suboptimal. What matters is not what date it was, but what packages the author was building against at the time he uploaded. In many cases this will probably be the most recent available, but there will probably be times when it won't be. If we're writing a tool to add upper bounds, why not just choose them based on the packages that were just used to build? It seems to me that if we're adding missing version bounds, we need to do it client side rather than have hackage do it. If hackage does it, then the cabal file ends up being different from the one the user uploaded. That might not be a huge problem, but it seems like one that should be avoided if possible. I like the historical hackage idea for fixing missing bounds in the past. It seems much less invasive to me than actually going back and editing cabal files. It's certainly conceivable that there might be some people out there depending on the absence of bounds and it would be nice to not mess them up.
The idea with implicit blacklisting was that if the user specifies &lt; 0.4.3.3, then that will override implicit blacklisting and do what you want. In most cases, if 0.4.3.3 breaks your package, that means that the maintainer made a mistake with his release. In that case, I think you should ask the maintainer to fix the problem and release the fix as 0.4.3.4, in which case you won't need the &lt; 0.4.3.3 constraint because implicit blacklisting will always use 0.4.3.4. If a different package specifies &lt; 0.5 and you're using &lt; 0.4.3.3, then Cabal would choose the version that satisfies both, which would be 0.4.3.2. However, some people have suggested addressing this problem with the ability to declare "private dependencies"--dependencies that don't impact the external API of your package. If I use a state monad under the hood in my library, but I only expose a single function `foo :: String -&gt; Foo`, then we could make mtl a private dependency of my library which would mean that it wouldn't have to be the same version as the mtl used by other packages in the build plan. I'm not sure if anyone is working on this, but if it was done properly and was easy enough to use, it should make more things buildable that aren't right now.
You know, I once started making a list of all the things that people "ought to know" about web security, and it's _huge_, and as in this particular case, far more than something one can learn by just reading appropriate standards or anything. No, it's not at all surprising to hope you could get away with this and discover it doesn't work in the real world.
but exponential work to tell when you're done... The original code can do `kCombinations 1 [1..n]` in O(n). That code does it in O( 2^n ).
Last I knew this was only the case with parallel builds (e.g. when cabal is invoked with `-jN`).
&gt; For your everyday development workflow, you really want to use cabal build. The only trouble with this advice is that sandboxes, which are cited by many as the best way to combat Cabal hell, pretty much require the use of `cabal install`. 
Here is what I could find after an extensive search of the internets: https://github.com/jthornber/cslice EDIT: just noticed it's for slicing of C code, and not binaries. Looks like you're SOL.
This is exactly what has been causing me trouble. At the very least, the feature should be disabled when using cabal sandboxes.
I think you are right, because exception safety in C++ is studied better. Thank you for useful tip.
Not only that, but I distinctly recommend some papers back in the day recommending this practice circa 2005.
I have, and I'm waiting for it.
The danger there is that you've given the user no way to do any other `IO` actions in the `MissileGame`, and if you have two such 'nice' abstractions they don't compose, unless you turn them into transformers, etc.
While on the subject of cabal and versioning, when distributing applications it would be very help if authors made available the output of `cabal freeze`. I realize the advice is "... you should only use cabal freeze on applications you develop in-house, not on packages you intend to upload to Hackage. " [link](http://blog.johantibell.com/2014/04/announcing-cabal-120.html), but when building an application often I don't care what versions of packages it uses as long as it works. 
This sounds vaguely like a feature of QuickCheck noted [here](https://www.fpcomplete.com/user/pbv/an-introduction-to-quickcheck-testing) &gt; "when a counter-example is found, QuickCheck attempts to shorten it before presenting using a shrinking heuristic. This is great because randomly-generated data contains a lot of uninformative "noise"; a short counter-example is much more useful for debugging our programs" So the idea would be to characterize the behavior you seek a test failure. 
https://github.com/haskell/cabal/pull/1856
the list is huge, and regular people probably don't know all the "ought to know"'s but when you make something like yesod I would expect you're the type of person who can make that list. That being said I don't want to talk smack i think these people do a great deed and fantastic work :) So I apologize if it came out like I was doing that.
[yall](http://hackage.haskell.org/package/yall) has monadic lenses, I believe. I don't think they're all that well behaved.
I don't think they are either. An important difference to rays is that yall's monadic lenses can be composed with each other and have a notion of input other than given by the monad. Rays only compose with (pure) lenses and the monad provides all inputs. Maybe that solves some of the problems.
Yes, now I see! I think this is actually bound to be much better behaved
You could consider using an entity component system. http://www.reddit.com/r/haskell/comments/256yqv/an_entity_component_system_in_haskell_xpost_from/ http://en.wikipedia.org/wiki/Entity_component_system
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Entity component system**](https://en.wikipedia.org/wiki/Entity%20component%20system): [](#sfw) --- &gt; &gt;__Entity-component-system__ (__ECS__) is a [software architecture](https://en.wikipedia.org/wiki/Software_architecture) pattern that implements concepts from [Composition over inheritance](https://en.wikipedia.org/wiki/Composition_over_inheritance) using a database-like structure. Common ECS approaches are highly compatible with [Data-driven programming](https://en.wikipedia.org/wiki/Data-driven_programming) techniques, and the two approaches are often combined. &gt; --- ^Interesting: [^Electronic ^component](https://en.wikipedia.org/wiki/Electronic_component) ^| [^Electrical ^efficiency](https://en.wikipedia.org/wiki/Electrical_efficiency) ^| [^Systemic ^risk](https://en.wikipedia.org/wiki/Systemic_risk) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm2l2xb) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm2l2xb)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Bracing for /u/edwardkmett
What programming style use this for making the GUI (FRP, Lenses)?
&gt; unless you consider downcasting essential Even among OO-purists, downcasting is considered code smell. There's even patterns to avoid it (Visitor, e.g.). Now, I've seen my share of it, but I'm never heard anyone really call it *essential*.
I think **MonadicFold m a b** is roughly analogous to **Kleisli (ListT m) a b**, with the advantage that you can easily compose them with regular folds. 
The library doesn't build in any particular high-level abstraction. It's binding to [Qt Quick](http://qt-project.org/doc/qt-5/qtquick-index.html), so the key feature is that you design your interface using a (non-Haskell) DSL called QML. Your QML view then data-binds against a model implemented using the HsQML library. It's a callback mechanism essentially, combined with the ability to signal events asynchronously from the Haskell side as well. It should be feasible to hook up one of the the FRP libraries to drive a QML view, and I know some people have looked in that direction but I've not done much FRP myself.
Hm, judging from the Effective constraint which forces the functor to be co- and contravariant (i.e. ignore its arguments), Identity is ruled out as the functor used by an action. The only (sensible) co- and contravariant functor I know is Const a, so it seems that actions and monadic folds are in fact restricted to getting and folding.
Oops, URL a bit too long, go [here](http://phaazon.blogspot.co.uk/2014/11/foreword-abstracting-what-shaders-are.html) instead ;)
This is really awesome news! Congrats, I look forward to experimenting with it.
&gt; If you're the doorman to a club and your clicker is counting people in would you expect it to wrap when it reaches its limit? Yes. I don't want it to saturate, I want it to be incredibly accurate and being able to continue incrementing is important for that. Although, I would hope that it would signal, so that I would be aware I had exceeded a mechanical limit and could adjust. &gt; If you're taking photos with a camera and you run out of space on the memory card would you expect it to start at the beginning and overwrite your previous photos? No, put I also wouldn't expect a fixed number of picture slots. Also writing over the fixed value 1111...1111 with the fixed value 0000...0000 is very different from writing over an arbitrary image with a different arbitrary image. &gt; Would a film camera spool back to the start and start double-exposing previous shots? Depends. My father had both kinds.
Nothing special about Any and Nothing though. I get just as annoyed when I accidentally return unrelated case class constructors from different branches of a case and it infers the return value to be Serializable or some other common superclass.
Hats off!
In the D programming language, [DustMite](https://github.com/CyberShadow/DustMite) minimizes D source code, commonly used to make a minimal test case that reproduces an error.
How do you handle schema migrations?
I'm not the author, but I've been using [Sqitch](http://sqitch.org/) for my latest project and am mostly enjoying it. It'd be nice if it came with some utilities to aid verification, but otherwise happy.
Synchronization between processes/machines is a _hard_ problem, but synchronization between people - volunteers, spread over the world - is an impossible problem.
&gt; In this case, foo should specify a bound of bar &lt; 0.4.3.3. In this case, the solver should respect that bound and only consider 0.4.3.2. But 0.4.3.1 would still be ignored as before. I guess this is what confused me then. It sounds like 0.4.3.3 continued with implicit blacklisting, and thus x &lt; 0.5 and x &lt; 0.4.3.3 could not possibly be fulfilled.
While this sounds interesting, how would slicing a binary be safe? If the binary is arbitrary it could be an embedded python exe or who knows what. How would the program know which pieces to cut out?
The [original paxos paper](http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf) used synchronization between human volunteers as the running example, so I think the problem is fundamentally no more difficult. All you're doing by depending on human volunteers is increasing the failure rate.
That's pretty much all they are.
This is really cool.
Minor nitpick: in the ActiveRecord way of things you're also supposed to put a unique index on fields that are supposed to be unique (in addition to a rails validation, the rails validation tends to make it easier to give good errors). Likewise you can easily mark fields as not nullable in your migrations (and it's recommended that you do that in most guides that I'm aware of) The other things are database specific things indeed impossible in plain activerecord 
What do you find is missing with sqitch?
Is this using techniques like Nikola for those `lam`s?
 Move them into a separate package you say? That's all I needed to hear!
"Binary File" Is pretty darn vague. Do you have a file format such as ELF or DWARF? Do you have an instruction set you are hoping to work with? These sorts of facts should certainly accompany the question.
Ed is the creator of the prevailing lens package (called `lens`, unsurprisingly) and often champions the idea that monadic lenses (which rays are similar to) aren't a good idea to implement since they do not follow any well-defined set of laws.
&gt; it infers the return value to be Serializable I think you'll find in those cases that `Serializaable` **is** the principal type. 
Do you have any plans for representing swizzling? That is one thing that I think shader languages handle well.
yes, but don't understimate what a great hashmap postgresql is.
having read roman's post to the sqitch mailing lists, i know that one of the pain points is views that have to be rebuild whenever the underlying table changes. that _is_ a pain point, but i am not sure what a tool _atop_ of postgresql can do about it. arguably that is something that postgresql might provide.
Not missing per se, just our experience with it hasn't been great so far. One annoying thing is that postgres requires dropping and recreating views when a column type is changed. It's not sqitch's fault, but it doesn't do anything to help there, either.
I briefly touch on this in my post, but an FRP library has to fully evaluate entire functions mapped over behaviors/events, even if only part of the result is changing. For example, lets imagine our designer gives us a HTML template with a lot of structure. A lot of this content is static, but a small part changes. If you write a single function that takes the dynamic content as input, then you will recompute the *entire* document whenever that changes. To get around this, you have to be careful and make `pure` behaviors for the static content, and then combine multiple behaviors into the final document. It's certainly doable, but now the programmer has to do more work to achieve performance.
Yes I have, via combinators like `sx`, `sxy` and so on. This is more limited to what we can achieve in GLSL, like `.xxyw` though. However, I could write the whole possibilties ;)
I don’t know Nikola. However, the lambda might disappear as I introduce statements :)
Which is why it is interesting if rays - while pretty similar - are different enough to not have the problems that he encountered with monadic lenses. This seems quite likely, since rays do not compose with other rays like lenses would. Additionally, since all processing of the of ray's data is done in pure code, one profits from the laws of pure lenses in reasoning about code with rays.
Yeah, a "what are the key differences between PureScript and Haskell?" page could be very handy. I spent a while looking for something like that on the PS site, but couldn't see it anywhere.
This is a very nice project indeed! I have one problem though: Both with this version and the previous, I'm seeing segfaults *when I quit a program* (including the sample ones), but only every now and then. And I'm never able to reproduce the segfaults under gdb or valgrind, so it's kinda hard to debug. Is this known? Let me know if some more information will help you somehow.
Glad to hear you like it, barring issues :-). I have been able to reproduce this on Linux by running a very short lived program repeatedly until it crashes. Is that the platform you're using as well? I've performed some preliminary analysis and created a [bug to track fixing this](http://trac.gekkou.co.uk/hsqml/ticket/26). I do encourage people to e-mail me or file a bug on the [tracker](http://trac.gekkou.co.uk/hsqml) if they have issues so that I can get them fixed.
You'll need to post more code, for example the definition of class `IsTree` and the `BTree b` instance for it. Furthermore I suspect your `IsTree` class is not actually what you want. The `mark` method for the `BTree b` instances is going to have type `BTree (BTree b) -&gt; BTree b` which is unlikely to be useful. I'd recommend just ditching the typeclasses altogether. Why do you need them? Just write functions.
In the sense I think you're asking: it's not only not suggest and not "legal," it's not *possible*. Stackage uses Hackage as its upstream, and therefore if you want to get a package into Stackage, you *have* to release it to Hackage first. There's a slight tweak I can say to that, in that if you create a custom bundle and upload that to Stackage server, you can include arbitrary new packages. But that's not the official Stackage snapshots, just user-contributed, which can be useful for beta testing. If that doesn't make sense, don't worry about it. I haven't really elaborated on that functionality of Stackage server yet (and unfortunately don't have time to do so right now).
Thanks for your answer.. The def. of the class isTree: class IsTree b where sectionbeam :: b -&gt; [b] isleaf :: b -&gt; Bool First I tried mark :: b -&gt; Int but then there was the error could not deduce b ~ Int... I have to do this for uni and it has to be in this form :S 
We're going to need more details. Still rather vague ...
Yep, that's what I was wanting to understand. Thanks!
the homework gives the data BTree and NTree and asks for a) a (constructor ) class isTree with the methods sectionbeam and isleaf b) a subclass isMarkedTree of isTree with the methods mark, nodes and edges... (with defaults for nodes and edges) c) instances of isTree and isMarkedTree for BTree and NTree...
What the compiler is telling you is that it expects `Integer` to be an instance of `IsMarkedTree`, meaning you'd have to give the following declaration somewhere: instance IsMarkedTree Integer where ... Most likely this is not what you want, if the name of your `IsMarkedTree` class makes any sense. ;) So, the next step is identifying why the compiler requires this declaration. Helpfully, the error message tells you that this is due to an application of `mark` to an `Integer` value. So, one potential cause of the error is you calling `mark var` for a variable `var` which you think is a `BTree` but which actually is an `Integer`. The error message will likely also give you a line number, so look for uses of `mark` there and check their arguments.
I applied `mark` to the `BTree ex_b = Bnode 1 (Bnode 2 (Bleaf 3) (Bleaf 4)) (Bnode 5 (Bleaf 6) (Bleaf 7))` so that's where the `Integer` value comes from... no, it doesn't give a line number, it says no instance for (IsMarkedTree Integer) arising from a use of mark in the expression: mark ex_b in an equation for it: it = mark ex_b
Oh, I think I see what is needed. I suspect you want class IsTree t where sectionbeam :: t a -&gt; [t a] isleaf :: t a -&gt; Bool class IsTree t =&gt; IsMarkedTree t where mark :: t a -&gt; a Does that sound right? [EDIT: corrected variable names in `IsTree`]
I have followed this topic for quite a while in the Ruby community. The (anecdotal, but valid) personal experience of many women has been that being the only woman in a group is intimidating. That they've been hit on, stereotyped ("ohh, is your boyfriend a programmer"), and generally harassed. The success of groups like RailsBridge is good demonstration that there are women who want to program. But they don't feel comfortable being new in environments where they'd stand out. Not everything needs to be predicated on a scientific study. We should listen to real experiences of women who have said something, and take it into account. Especially when some things are generally just a good idea. A code of conduct at a conference or meetup is an excellent way to keep out other racism, sexism, gendered harassment, etc. Similar to a code of conduct is being careful about the role of alcohol at meetups and conferences. Being conscious of the fact that not everybody wants to drink will make people feel more comfortable. Serving nice sodas (price on part w/ beers, not limited to coke), mocktails, etc. Just keeping things on par w/ the alcohol offerings and non drinkers will feel better about hanging out and integrating into the group. We should still research and gather real feedback, but this is about community, community norms and being decent to each other. Some is specific to women, but most is just remembering that many people are not like you in some way, and that they should be accommodated.
yes!! that works :) thank you very much!! 
Great :) Yes, it should be the same way. If you have any problems just come back and ask.
Wrt the missing line number, in this case the error message gives you the precise code that produces the type checking error, so that's even better. ;) Now we can deduce what the compiler is trying to do: * `mark` takes a `BTree b` and returns a `b`. The value you're passing to `mark` has the form `Bnode 1 (...) (...)`, so its type is `BTree Integer` (since `1 :: Integer`). Thus, `b` is instantiated with `Integer`. * However, `b` must be an instance of `IsMarkedTree` as per the class definition. (`mark` effectively has type `IsMarkedTree b =&gt; BTree b -&gt; b`.) Since `Integer` is not an instance, you get an error. See /u/tomejaguar's remarks for how to fix this problem.
Thanks, it works for nodes and edges too now :) if I have problems with the instance `NTree` I'm going to ask you... but it seems like this sub didn't like my question :S 
It seems as if you've ignored the point I was making. I'm not sure if I should restate it... Should I? To reply to something you said: &gt; We should listen to real experiences of women who have said something, and take it into account. Not necessarily, because experiences might not be relevant. For example, suppose that you put an advertisement on google, expecting to get 10,000 clicks. Unfortunately, the advertisement did not have the click-through rate that you expected -- you got only 10 clicks. The experience of your site of the 10 people who clicked is entirely irrelevant if your goal is to understand why your expectations were incorrect. It does not make any sense to ask them questions about that experience. (If anything, you would ask them about why they clicked, but even then, you'd still be asking the wrong people.) So the point I was making before (which I guess I have restated, in a way) is that before you decide whether to focus on the experience of women attending your group, you have to know whether the experience of women attending your group is the reason for the disparity in attendance. It very well might not be.
You are making the argument that we should not make any changes at all until we have done rigerous research. I think that's a silly proposition. This isn't a published journal article we're working on. We have plenty of anecdotal and repeated evidence that women want to program (generally, drawing from booked-solid railsbridge and similar), and actual statements saying that specific women feel uncomfortable and unwelcome in some tech communities. We should act on that evidence, and make simple reforms to make our communities more welcoming to newcomers and outsiders. &gt; before you decide whether to focus on the experience of women attending your group How about instead of focusing on women in particular, we generalize what I'm saying to making our groups more inviting for anybody different. The things I suggested are generally good for communities - lets just do them. 
&gt; You are making the argument that we should not make any changes at all until we have done rigerous research. Not at all. I haven't said anything about what we should or should not do. I just think that your presumption of the source of the disparity is incorrect. I've only tried to explain the way in which I think it is incorrect. (I haven't even argued that you *are* incorrect.) I proposed a simple experiment using meetup.com RSVPs as a real possibility, but also (probably primarily) as a thought experiment to pinpoint exactly where I thought we disagreed. We don't necessarily need to do rigorous research, but some kind of evidence would be necessary to determine who is right. However: &gt; We have plenty of anecdotal and repeated evidence that women want to program (generally, drawing from booked-solid railsbridge and similar) That's not the right evidence to determine it.
Just putting this out there, but... Weirdly, I actually understood monads better once I tried to implement a game using several layers of monad *transformers*. The reason a game worked well, I think, is that games are usually composed of several layers of functionality (which maps naturally to monad transformers): You'll want some way of generating entitiy IDs, you'll want a source of random numbers, a way to draw stuff, a way to track time for each of the individual actors/entities in your game world, etc. The great thing is that you can keep adding layers (at the cost of having to add "lift"s ad nauseam) sort of like Inception! :)
&gt;I do believe that there is real value in pursuing functional programming, but it would be irresponsible to exhort everyone to abandon their C++ compilers and start coding in Lisp, Haskell, or, to be blunt, any other fringe language. It wouldn't be that irresponsible. His advice is likely to be misinterpreted. I'm currently noticing a trend of unreadable functional idioms applied to imperative languages (or multi-paradigm as they like to be called now). Functional programming in Haskell works because the syntax helps you instead of working against you like in C++ or Java. Sure every language can let you write a function that takes an int and returns an int. It's when you get into the fancy stuff that you lose your sanity. 
I don't see how it becomes easer is we have only 2 async exception, but otherwise I totaly agree. I think we should find a solution or remove async exceptions completely. The proposal is not a solution obviously. If we decide to remove async exceptions, then uninteruptibleMask could be a good way to minimize an impact of them in deprication period. But if we finally find a solution, then the proposal can make implementing the solution harder or even imposible. Because it is (more or less) easy to introduce uninterruptibleMask, but almost imposible to remove it. We need more solid design instead of random walk in design space. 
I am currently in the process of developing a new version of the `happstack-authenticate` library. I am at the stage where I need to test it in the context of an actual end user application. In my case that is a `clckwrks` application. At a minimum I have the dependency chain: example-dot-org -&gt; clckwrks-plugin-page -&gt; clckwrks -&gt; happstack-authenticate So, whenever I tweak `happstack-authenticate` I need to rebuild all those things to get an executable I can actually test. `cabal sandbox` works very well for this. I use `add-source` to add all the dependencies into my local project directory, and then if I modify anything in the chain I get the proper amount of rebuilding done. Except if there is a build error I now have to dig around and try to find the right `.log` file so that I can actually see the error. 
What are you going to replace `Point2d` with? Can I suggest something from `linear`? :)
&gt; Functional programming in Haskell works because the syntax helps you The semantics too (especially?).
The article lacks smart words like covariance/contravariance. It would be nice to mention those terms, as if a reader encounters them later, he would know exactly what these mean.
So if I’m understanding right, Tomato &lt;: Vegetable Vegetable -&gt; Soup &lt;: Tomato -&gt; Soup (Tomato -&gt; Soup) -&gt; Soup &lt;: (Vegetable -&gt; Soup) -&gt; Soup So every time we add an arrow to take our type to some other type (invert control), the subtyping relationship flips around?
That's essentially it, yup!
The problem is that not everyone can write everything from scratch. That is why progress comes in small steps, even if we would like otherwise.
It should be noted that this article is from 2012. Lenses didn't exist and most of the other niceties (FFI, SYB, MTL...) were still being researched heavily. Haskell has changed a lot the past few years, and John Carmack knows that, too. He's been talking about it earlier this year. 
Damn. I'd never even thought about subtypes of functions before. Thanks.
You didn't write the homework question, so it's not your fault at all. :) Was this homework written specifically for Haskell? Or was it for a language assumed to be an object oriented language?
I'm not sure about binary files. However, it would be neat to see mutation testing (even something simple like [jester](http://jester.sourceforge.net/)) applied to Haskell source. Just swap out expressions with calls to nullary constructors of the type or calls to `arbitrary` at the some type, and see if your tests pass. It's actually a lot more accurate than more simply generated code coverage meterics across your test suite. There was some library around mutation testing that was talked baout in a thread here about 6-9 months ago. I can't remember the name; maybe it was MuCheck as that's all Google is coming up with for "Haskell mutation testsing" for me.
It's also interesting that even in a language without subtyping (such as Haskell), you still have co/contravariance: types that support 1. functor (covariant, `(a -&gt; b) -&gt; f a -&gt; f b`), e.g. List 2. contravariant functor (contravariant, `(b -&gt; a) -&gt; f a -&gt; f b`), e.g. Predicate T = T -&gt; Bool 3. bifunctor (invariant, `(a -&gt; b) -&gt; (b -&gt; a) -&gt; f a -&gt; f b`), e.g. Aut T = T -&gt; T You also have "bivariant", types that support `forall a,b. f a -&gt; f b`, i.e. type constructors that do not even use their argument. Co- &amp; contravariance in languages with subtyping is just a special case of this where the function is a coercion from a subtype to a supertype.
I prefer Haskell questions go to /r/haskellquestions. I'm also subcribed there and try to check it often.
&gt; The article lacks smart words like covariance/contravariance. I thought at first that this was meant ironically, as a compliment. Sure, mention those. In a footnote.
Because of this, I would have preferred some sort of apple/orange/fruit salad metaphor over the presented tomato/eggplant/soup metaphor.
In depth? There's not a single line of code.
It actually boils down to arrow being a contravariant functor in its first argument (its also covariant in its second argument, making it a profunctor). Think about it as follows. If `A` is a subtype of `B`, then there should be a function `subtype :: A -&gt; B`. But we can lift this relation to any functor by applying `fmap subtype :: Functor f =&gt; f A -&gt; f B`. As an example, `Maybe` is a functor, so `Maybe A` is a subtype of `Maybe B`. Similarly, for any contravariant functor we can lift the subtyping relation, but it gets reversed: `contramap subtype :: Contravariant f =&gt; f B -&gt; f A`. You can find the `Contravariant` typeclass in the [contravariant](http://hackage.haskell.org/package/contravariant) package. There are other contravariant functors, as well as profunctors.
yes. 
To which question?
I thought about them, but I didn't really know where to work it into the text. And of course, that's what comments are for :)
Haskell is a great starter language for your situation. Once you get into it, you might look at [Ermine](https://github.com/ermine-language), as it's specifically developed for financial stuff. The main guy who develops it, Ed Kmett, is also a big Haskell developer. If I recall correctly, he's the head of the committee maintaining the Haskell standard library. Chris Allen has a nice [tutorial](https://github.com/bitemyapp/learnhaskell) on GitHub. Learn You a Haskell is aimed at people who already know imperative programming (Python, Java, C, etc). There's another book aimed at complete newbies, called [Programming in Haskell](http://www.amazon.com/Programming-Haskell-Graham-Hutton/dp/0521692695/ref=sr_1_1?ie=UTF8&amp;qid=1416099128&amp;sr=8-1&amp;keywords=graham+hutton+programming+in+haskell). I haven't looked at that book, but I've heard good things about it. If you have any questions, you can always ask here, or in the `#haskell` channel on FreeNode. I'm `pharpend` in the channel, if you ever want to talk to me.
I'm actually removing Point2d; it was originally there purely to implement the test cases, but I've reorganized the modules such that Point2d doesn't show up in the public library API.
Generally speaking, is it easy in Haskell to get information from websites like Yahoo finance? What about visualizing data?
If you can explain how you would do it in another language, we can easily point you to the equivalent libraries in Haskell
I think [R](http://www.r-project.org/) might be a better fit for what you want to do. As a language it's weird (based partially on Scheme, but with objects and a lot of annoying special cases thrown in over the years), but it has MUCH better support for exploring data, visualizing it, and doing statistics than other languages. You might also find the [RStudio](http://www.rstudio.com/) IDE helpful.
I feel like I'm very close to getting function subtyping, but will need to sleep on it. What I 'understand' so far: When A &lt;: B, if you need a B you can use any A. But if you need an A you can't use any B. When B -&gt; C &lt;: A -&gt; C, if you need an A -&gt; C you can use any B -&gt; C. But if you need a B -&gt; C you can't use any A -&gt; C. I don't quite 'get' why the last bit is true; will think about it using /u/ezyang's Tomato &lt;: Vegetable example.
Why not just start with a cabal freeze or stackage snapshot, and then "whitelist" any necessary upgrades as you do the long term support by tweaking your snapshot or freeze file? Perhaps one thing that's missing here is an easy way to replace (in a given environment) the packages that get bug fixes?
Tomatoes are clearly fruit. It's a fleshy red ball with seeds at the center. Just because it doesnt taste very good on its own doesn't make it a vegetable (and Ive seen people bite into a tomato like an apple in old 50s tv shows). Why wouldnt the author choose broccoli or something? The whole article became too bothersome to read. Don't even get me started on people who seem incredulous that corn is a type of grass just like wheat or barley.
I'm going to assume that by *efficiently* you mean efficiently in terms of **your time spent doing the task**, aka *programmer efficiency*. Instead of, say, the run-time or memory usage of the program you wrote. * Pros: In terms of programmer efficiency, Haskell is a very good language. Laziness, purity, and algebraic datatypes make it easier to separate concerns than in most other languages. That gives you better composability and in turn is a huge boon to prototyping and algorithm design/implementation. Not having to worry about low level details saves you a lot of mental energy and debugging time. Additionally, due to purity and a powerful type system it supports expert Haskell programmers while they refactor their code. With smart use of types you can get a reasonable level of confidence that you haven't broken things. Add property based testing (QuickCheck or SmartCheck) and you can go from pretty sure to almost certain. With cabal and hackage you get access to a decent set of libraries. That can save you a lot of time. Whenever you have to write something from scratch you need to be prepared to invest a solid chunk of time. Letting other people do that for you can give you more productivity gains than the language features I already mentioned. * Cons: Haskell is a conceptual departure from other mainstream languages. It requires a different way of thinking and this can be hard for some people or require retraining. It sounds like this won't be an issue for you. Getting acceptable performance from Haskell can be a challenge for beginners. Laziness isn't the easiest thing to learn to reason about correctly. Hackage doesn't help you pick good libraries. You see everything everyone has submitted equally. Learning how to mentally filter the package list takes time and experience. You may get burned picking a library that doesn't work well for you and feel that you should have just written it yourself. Haskell is lacking in the visualization department, but it is getting better. I've heard that these days you can find libraries for plotting that produce nice charts, but I haven't tried them so I can't speak to how easily a beginner can get started using them. Matlab, octave, sage, R, or mathematica are all going to be far better for visualization out of the box. I would say that you should expect to need to: * invest a lot of time leveling up from beginner to intermediate Haskell programmer; * learn about topics that are orthogonal to your high level goal, such as language features (types, laziness, monads, etc); * evaluate lots of libraries on hackage; and * write your own libraries and FFI bindings as appropriate, as it's unlikely hackage will have everything you need. I hope that helps!
Yes, but in the relevant business domain i.e. cuisine, tomato is a vegetable.
This is a great answer, exactly what I was looking for. And yes, when I said "efficiently" I was referring to mental overhead. How long did it take you to get decent at Haskell? A friend of mine said the rule of thumb is 1000 hours to become an expert at most languages.
Why not play around with it? That's the easiest way to understand what's going on. (Use a class and an implementing datatype as it's subtype, for example.)
It's to mess with nitpickers.
Thanks! Good question. It took me a while but I'm not really sure how many hours went into it. For several years, I spent a lot of time in #haskell on freenode. Asking questions, helping others, writing code, and so on. I started from a stronger programming background than you. I was already proficient in C/C++, Java, common lisp, and probably a few other languages. Peter Norvig has this to say on the topic: http://norvig.com/21-days.html
**tl;dr Yes to the first question, no to the second.** # Yahoo Finance That is pretty easy. Yahoo Finance has a [very extensive API](https://developer.yahoo.com/yql/). You can think of API as meaning "programmer-friendly website." It actually means "Application Programming Interface." Your program would generate a request and send it to Yahoo Finance's server. Yahoo Finance's server would respond with the data you want, in either JSON or XML. Both of those are pretty standard, and most every language has parsers for them. Haskell, in particular, has a fantastic JSON parsing &amp; writing library called [Aeson](http://hackage.haskell.org/package/aeson). A library is basically a program written to be used in other programs. Generating the request and getting the data is pretty easy, and uniform across most languages, aside from crazy languages like Brainfuck. The next step is to take the raw JSON data, and parse it into something useful. This is where Haskell excels. I won't go into specifics, but this is really Haskell's strong point. # Graphics Haskell is just awful at graphics. I'll leave it at that. If graphics are really important to you, learn something like Sage. Haskell is just horrible at this. **Edit**: To be clear, there's no reason there couldn't be a good graphics library in Haskell, it's just that nobody has made one so far (at least none that I know of). 
But it's relevant to good nutrition. :(
R is designed for your use case. It's not a very good language, but everything made in it is designed for your use case. Haskell is a general purpose language. It's superior in overall design and could be better than R at what R does. I don't think it's that far behind, but it might take a lot more work for you to start being productive.
&gt; Botanically, a tomato is a fruit: the ovary, together with its seeds, of a flowering plant. However, the tomato has a much lower sugar content than other edible fruits, and is therefore not as sweet. Typically served as part of a salad or main course of a meal, rather than at dessert, it is considered a vegetable for most culinary uses. One exception is that tomatoes are treated as a fruit in home canning practices: they are acidic enough to process in a water bath rather than a pressure cooker as vegetables require. Tomatoes are not the only food source with this ambiguity: green beans, eggplants, cucumbers, and squashes of all kinds (such as zucchini and pumpkins) are all botanically fruits, yet cooked as vegetables. http://en.wikipedia.org/wiki/Tomato#Fruit_or_vegetable.3F
"fruit or vegetable" is a false dichotomy. They are overlapping terms from different domains. 
What about: http://projects.haskell.org/diagrams/
Fruit and vegetable are not mutually exclusive terms.
think of it this way: say you're defining a function like so: foo f = f x where `x :: A`. Now obviously, as long as the argument type of `f` is more-general than `A` -- i.e. as long as `A &lt;: B` where `B` is the arg type of `f` -- then you can apply `foo` to any such `f`, because you're giving the right sort of arg. So slightly more concretely, imagine foo f = f (0 :: Integer) If you know that `Integer &lt;: Real` lets say, then any function on `Real`s will happily take `0 :: Integer` as an arg, right? So: you can use any `Real -&gt; C` where you need an `Integer -&gt; C` because all of the inputs you need to handle are indeed handled (among others). So, just like `A &lt;: B` is true because an `A` can be used when you need a `B`, so too `(B -&gt; C) &lt;: (A -&gt; C)` for the same reason -- `B -&gt; C` can apply to everything `A -&gt; C` can (and more), so of course you can use it wherever you need an `A -&gt; C`. It's very important to pay attention to the use of "need" and "can", btw. Also consider: if `f :: B -&gt; C` and `A &lt;: B` then: \x -&gt; f x :: A -&gt; C is fine, because `A &lt;: B` so you can turn `x :: A` into `x :: B` and apply `f` to it to get the `C` you need. So you can obviously turn a `B -&gt;C` into an `A -&gt; C` like this. We ought to expect, tho, that `\x -&gt; f x` is the same as just `f`, so we have another way to look at the subtyping here.
i dont know if i would explain a library as a *program* as i think thats rather confusing. perhaps code pre-written for use? although that doesnt have very positive implications also what kind of graphics library are we talking about? opengl or gui or what? I ask because i am very interesgted in writing a graphics library, and wliling to commit the time
&gt; A friend of mine said the rule of thumb is 1000 hours to become an expert at most languages. He (or she) is probably just parroting a guideline originally found the the book [Outliers](http://www.supercoach.com/tvdetail.php?recordID=528). Those numbers "feel right" to me as well, but the [data is a little mixed](https://www.google.com/search?q=expert+at+anything+in+10000+hours), I think. Even if the time to become proficient with any single programming language (or skill) were exactly the same, we'd still have to ask which tools are better, because we don't have time to learn all of them. For your specific problem, I first thought or the R programming language. I don't know the language well, but it's supposed to make it relatively easy to import n-dimensional data and perform well-known analyses and visualizations on it. I also believe it is relatively easy to call to/from C/Haskell/R so if R doesn't seem to fit your needs in the future, you can mix in other technologies as needed. See also /u/jefdaj's [comment](http://www.reddit.com/r/haskell/comments/2mf8rw/is_haskell_right_for_me/cm3pie8).
If you are a biologist, sure, but it is a vegetable in the culinary sense, which is what is being talked about here. Context is important.
I guess you missed the part of the article that was a subtle nod to this fact. "Sure—I’m not a botanist: to me, tomatoes are vegetables too."
That's very true. I could probably write it, which says a lot. ;)
same lel
The `Monoid` instance is the only one that can implement the recursive lifting trick (i.e. `instance Monoid a =&gt; Monoid (F a)`), whereas `Alternative` instances cannot implement that trick. So if you require all `Monoid` instances to coincide with `Alternative` instances you are essentially forbidding the use of recursive `Monoid` instances.
this was much more understandable than the article
Revert the change to your config, do `cabal install --only-dep --reorder-goals --dry-run -v3` and you should get more information about where the failure occurred or a successful build plan. 
The original file is in the tarball, and the latest one from the package description link. I don't know if you can access intermediary revisions.
sorry--don't think it really adds up. R is excellent at what R does, so it's weird to say that it isn't a good language. Haskell would probably be terrible at system programming (no one writes kernel code in Haskell) but is still an awesome language. Criticizing R for not being good at things it doesn't try to be good at is like bashing Haskell for not being assembly.
Doesn’t taste very good on its own? You must never have had a fresh tomato. The bright red, watery variety you buy in a supermarket is optimised for shipping and visual appeal, not flavour.
Another con I think should be added to the list: because Haskell is such a big departure from other languages, your experience in it will not be nearly as transferable as, say, going from Java to C++. If you plan on using C++ or Python (the two other languages OP suggested) in the future, it'll take you longer to switch than if you started with one of those to begin with. On the other hand, Haskell not being either of those is a big part of what makes it awesome. EDIT: words
It really depends. Personally, it took me way less than 1000 hours to become decent at Haskell, but that's because I have a background in mathematics and had some previous exposure to programming (though the latter is usually a hindrance when it comes to Haskell). Of course, if you can find a mentor, learning will be even faster. For the stuff that you want to do (statistics), I think that Haskell is a reasonable choice. (The visualization part may be trickier, you may need to spend some time setting up the `diagrams` library.) The language tries to get out of your way and helps you focus on the problem you actually want to solve (summing numbers, finding percentiles, etc). Time invested in Haskell is definitely not lost, you can take knowledge gained here to other languages. (The same is not necessarily true in the other direction.)
This post is hopelessly optimistic. If you look at the actual code people write in C++, very little of it is actually exception-safe. Oh, and there's a vast difference between having to handle an exception *at every conceivable point* and having to handle exceptions *at certain specific known points*. That's why exception-safety in C++ is so hard when writing *generic* code.
Yeah, it's *obviously* a berry. Ruined the whole article for me... oh no, wait, it didn't.
&gt; You can't know statically. I don't need to. I need to know dynamically. The question is whether retrying is known to futile or not (dynamically). If it is known to be futile, I will raise an exception, close the handle, and break exception safety contract (because there's no way not to!) If it not known to be futile, I will block indefinitely, trying to flush the buffers, hoping not to lose the precious data after all. &gt; It is consistent, but probably unknown. It is important, because you may not care of buffers (and usually you don't care) I completely disagree. Usually you care a lot if your data is lost. But maybe that's just my background in high-reliability storage systems :-) &gt; relies on hClose to release file descriptor in case of exception Then it is all broken because underlying POSIX close does not actually let you guarantee that. &gt; You can't ignore exception safety just because you can't provide strong guaratrees Exception safety *is* the strong guarantees. If you can't provide them you've already broken exception safety. &gt; You actually don't need srtong guarantees, basic exception safety is enough in a lot of cases What is "basic exception safety"? &gt; If you claim that hClose should close file descriptor in that case, then I don't see difference between our "philosophies". The difference is that hClose, in my philosophy, isn't special. If it wants to throw an exception and can do so without having a visible side-effect, then it should. Another difference is that in my philosophy, any action can be a valid cleanup action (e.g: kill&amp;wait-for-process), and there's no subset of actions that are fit to be cleanups and others that are unfit.
&gt; So the advantages and disadvantages you described are false What do you mean? Do you understand the advantage of giving actual guarantees in bracket, for example? i.e: my withProcess bracket actually guaranteeing the process does not exist when it leaves. Do you understand why the proposal make this happen by default -- and how it is an advantage? &gt; I prefer my philosophy because it is safer and easer to reason Your philosophy is orders of magnitude harder to reason. Instead of reasoning just about sync exceptions in cleanups, now you need to make sure you remember to reason about what happens in async exceptions, which will virtually always need a different code path (and you will see that as soon as you look at examples other than hClose which you have a very weird spec for). &gt; I prefer to think about them carefully each time. Blocking is not an universal solution here. I think usually it is enough to have eventual guarantees. E.g. that child process will eventually be terminated. Program-wide, it may be enough. But the whole point of *bracket* is to guarantee the invariant about the inside vs. outside of the bracket. Otherwise, it's pointless to use a bracket at all. The unbounded blocking in a bracket simply represents the unbounded time it is taking to release the resource. If you don't mind leaking a resource - why do you mind leaking the thread that is bracket'ing on the resource?
Between "Basic safety" and "no safety" there's an extra level: "Requires non-local intervention to restore safety". e.g: Rebuild of part of the database, re-establishing a connection, etc. &gt; There is nothing complex in async exceptions per se, but exception handling is complex itself, and async exceptions just force us to fix bugs. And that is excellent, isn't it? Strongly disagreed: Sync exceptions related to the code you're running and its preconditions. Async exceptions are unrelated to the code at hand. You can work hard to establish the guarantee that various sync exceptions cannot happen in various places, and have strategies to handle any remaining sync exceptions. You can't have the same strategy for async exceptions. Another difference is that many sync exceptions indicate the situation is really bad -- and may require us to do nonlocal restoration of program state or even abort completely. With async exceptions, due to the availability of masking -- this should never be the case. &gt; Haskell has embedded tool to test exception handling, we call it "asynchronous exceptions". That potentially makes haskell the best language ever. Lets learn how to use the tool instead of ignoring it I think there's a basic misunderstanding here. He's missing that masking lets you establish async exception safety that isn't possible with sync exceptions. Conversely, sync exceptions allow you to reason about when they might happen and how to handle them in ways that are unavailable for async exceptions. Therefore, we should address each kind of exception with the tools available, rather than taking the LCD of both and writing unreliable programs that don't handle either safely.
Well, you got downvoted a lot for that, but it is also what I guess is true (don't *know*, of course).
Tomatoes are clearly a biological fruit, clearly a culinary vegetable and clearly not a culinary fruit.
&gt; This post is hopelessly optimistic Well, haskell is broken w.r.t. exceptions (regardless sync/async difference), but I still hope there is a solution. So the optimism :) &gt; If you look at the actual code people write in C++, very little of it is actually exception-safe I tend to agree, but in haskell the proportion is even worse. The key difference: in C++ it is possible to write exception-safe code. &gt; vast difference between having to handle an exception at every conceivable point and having to handle exceptions at certain specific known points It don't understand exact meaning of "conceivable point" and "specific known points". Async exception can be raised at any "safe point", but in exception handler they are masked, so it is not the case. Otherwise I don't see difference for exception handling (see "General approach" section.) &gt; That's why exception-safety in C++ is so hard when writing generic code. I agree with /u/gnzlbg. I'll just add authoritative reference: http://www.boost.org/community/exception_safety.html (see section 2 Myths and Superstitions) Thank you for the reply, I really appreciate it. I'm sure the biggest problem we have with exceptions is silent ignorance. 
No, it's much dumber than that. It doesn't analyze migrations to determine which views even exist, let alone which of them need to be recreated.
Looks like you know the topic pretty well. Could you please comment the article itself? Where you agree and where disagree, where it is not clear enough. I *really* need more feedback.
&gt; The uninterruptibleMask solution solves the cleanup issue for any number of async exceptions, not just 2 No, it doesn't because the issue is not async-specific. Different languages make different design decisions here. E.g. C++ bans exceptions from cleanup actions. Other languages (java, python) lets you access both exceptions (the original one and the one thrown in process of handling the original exception). See http://haskell.1045720.n5.nabble.com/Control-Exception-bracket-is-broken-td5752251.html It is unfortunate that haskell made a decision to ignore double-exception issue. &gt; I wonder what you think is the problem with async exceptions, after the new proposal is implemented? Then I'll probably try to convince everyone that we should ban exceptions in cleanup actions. So the proposal could be step into correct direction -- unbreaking exception handling in haskell. That solution is pretty valid, but it is unfortunate, because it makes async exceptions less useful, at least for me. 
It seems to me that statements like "A is a subtype of B" can be completely replaced by statements like "a conversion function from A to B is available", and that kinda demystifies the whole covariance/contravariance thing. The question in the post then becomes this: f :: (a -&gt; b) -&gt; ((a -&gt; r) -&gt; r) -&gt; ((b -&gt; r) -&gt; r) f = ??? And the answer is: f x y z = y $ z . x Right?
&gt; Then it is all broken because underlying POSIX close does not actually let you guarantee that. It is simply not possible to guarantee, regardless async/sync difference. &gt; Exception safety is the strong guarantees. OK, but it is widely used in a bit difference sense. Different terms make discussing pointless. &gt; What is "basic exception safety"? &gt; Basic exception safety, also known as a no-leak guarantee: Partial execution of failed operations can cause side effects, but all invariants are preserved and no resources are leaked. Any stored data will contain valid values, even if they differ from what they were before the exception. See http://en.wikipedia.org/wiki/Exception_safety 
Realistically, taste doesn't care about botany, and, consequently, neither does culinary taxonomy.
This seems relevant to the discussion: [Catching all exceptions](https://www.fpcomplete.com/user/snoyberg/general-haskell/exceptions/catching-all-exceptions) (by Michael Snoyman) 
You can start with [the ICFP talk](https://www.youtube.com/watch?v=_XoI65Rxmss) to get an idea of what it's about.
I think R is a better fix for this particular purpose. R has a big community and libraries. For example there is a package specialized in finance, http://www.quantmod.com http://r-forge.r-project.org/projects/quantmod A post about using R for finance: http://blog.revolutionanalytics.com/2013/12/quantitative-finance-applications-in-r.html 
You can never reach "no-throw" in Haskell unless you just uninterruptibleMask in main and all new threads and never unmask. Which makes SIGINT and killThread not work, among other things.
Yeah, I also swizzle a lot. I plan to introduce that anyway :)
I wouldn't use GHC for kernel stuff, but I could imagine doing kernel programming in Haskell given the right toolchain.
Interesting point of view. lazy evaluation makes it at least hard to achieve no-throw. (E.g. there is an issue with `unsafePefromIO`, but I'm not sure I'll buy it. It is unsafe anyway.) I tried to think about that, but still didn't come to any conclusions. So could you please elaborate? Because if you are right, then we should stop using async exception right now, deprecate `throwTo` and friends, and fianlly remove async exceptions ASAP. A lot of code relies on `takeMVar` to be no-throw (modulo deepseqing, which itself is not trivial to do right) Removing async exceptions is valid design decision, though unfortunate :)
I would recommend R for this application as well and will reply separately on the subject but I have to take issue with your assertion about graphics in Haskell. I use Haskell for charts in preference to R. Not only can you do fairly standard charting like this: https://idontgetoutmuch.files.wordpress.com/2014/06/044408c77f048f73.png?w=1560 but you can also do diagrams like this: https://idontgetoutmuch.files.wordpress.com/2014/06/c0da41e12161e76b.png?w=780. It's not quite ggplot yet though.
You will have more fun in Haskell but if you want to get up and running quickly, R is your best bet. In addition to the many financial and statistical analysis packages, there is also a very active mailing list on financial analysis. E.g. if you want to build a GARCH model you will get lots of help.
It's probably reasonable to say that 1000 hours entails proficiency
Haskell takes much longer to learn than other languages, with corresponding returns. I don't think it's a bad thing, just something to be aware of. Switching away from Haskell into another language is very easy; switching to Haskell is very hard.
R doesn't scale, either conceptually or in terms of performance. This is what "not a good language" refers to.
&gt;Haskell is just awful at graphics. I'll leave it at that. If graphics are really important to you, learn something like Sage. Haskell is just horrible at this. Have you checked out [Helm](https://github.com/switchface/helm) which is inspired by [Elm](http://elm-lang.org)?
Remember that one of the best ways to learn is through knowledge transfer. Seek collaboration with others, you'll find amazing returns on that investment.
I've found that most mainstream languages are loose subsets of Haskell. :)
Tell us how you do it then. :)
&gt; "A is a subtype of B" can be completely replaced by statements like "a conversion function from A to B is available" The former implies the later, that's for sure. We generally have some additions on top of the later before we call it the former though, but what those additional requirements are is not always clear. For nominal subtyping (which is what we have in C++, Java, C#, Python, Ruby, and Scala) we require an explicit programmer statement of the subtype relationship (which we take the transitive closure of) instead of basing it on properties of the relevant datatypes.
["Knowledge is knowing that a tomato is a fruit. Wisdom is knowing that a tomato doesn't belong in a fruit salad."](http://en.wikipedia.org/wiki/Miles_Kington)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Miles Kington**](https://en.wikipedia.org/wiki/Miles%20Kington): [](#sfw) --- &gt; &gt;__Miles Beresford Kington__ (13 May 1941 – 30 January 2008) was a British journalist, musician (a double bass player for [Instant Sunshine](https://en.wikipedia.org/wiki/Instant_Sunshine) and other groups) and broadcaster. He is also credited with the invention of Franglais, a fictional language, made up of French and English. &gt; --- ^Interesting: [^Steam ^Days](https://en.wikipedia.org/wiki/Steam_Days) ^| [^Michael ^Palin](https://en.wikipedia.org/wiki/Michael_Palin) ^| [^Alphonse ^Allais](https://en.wikipedia.org/wiki/Alphonse_Allais) ^| [^Instant ^Sunshine](https://en.wikipedia.org/wiki/Instant_Sunshine) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm44393) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm44393)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Is there any language where a user-written conversion function from A to B allows the compiler to treat A as a subtype of B? Also, if A is a subtype of B, doesn't it imply that the conversion function must be the identity, or at least that any conversion is performed under the covers by the compiler, so that no conversion function ever needs to be explicitly applied by the compiler?
Haskell is in no way ready to become a mainstream language. My experience of learning haskell is that there aren't very many good tutorials compared to other languages and pretty much nothing interactive. Once you have gone through learn you a haskell you are stuck in a horrible place. You write haskell and three things can happen: 1) Parse error. Means something, somewhere went wrong, enjoy finding the bug yourself. 2) Intendation error: Somewhere in your code there is a wrong indent, missing parenthesis or something else missing. Enjoy finding it yourself. 3) Wall of text error message. GHCi is world leading in providing worthless error messages. So many words and so few that tell me how to fix what is wrong. When I code in C, Java, Python, Mathematica, C# or any other language and I google my error message I get an answer. When I google my haskell error messages I get nothing. Haskell has a small fanbase for a reason. It is seriously frustrating to code in Haskell. The terrible lack of documentation and online resources for haskell makes haskell unusable. 
Wasn't Outlier's number 10,000 hours? I always internalized it as &gt;5 years of hard work.
&gt; When I code in C, Java, Python, Mathematica, C# or any other language and I google my error message I get an answer. I've been frustrated by Haskell's error messages, too, but this particular advantage has more to do with the relative popularity of these languages.
Back when I was a Java programmer I wanted it sometimes but now that I use Haskell any time I can get away with it I don't even use single inheritance. *shrug*
&gt; It don't understand exact meaning of "conceivable point" and "specific known points". Async exception can be raised at any "safe point", but in exception handler they are masked, so it is not the case. Otherwise I don't see difference for exception handling (see "General approach" section.) I'm having trouble understanding the article and discussion (maybe some code examples would help me?) but I wonder if Oremorj is referring to the following: because all code in C is effectful you must be careful to handle exceptions and restore state at every point in your program, whereas with haskell we don't have to care at all about catching exceptions in pure code.
If it quacks like a fruit?
&gt; My experience of learning haskell is that there aren't very many good tutorials compared to other languages Agreed. But you should point out that there are *some* very good ones, like LYAH. &gt; Once you have gone through learn you a haskell you are stuck in a horrible place. That's harsh, but I agree with you if the sentiment is that between "beginner" and "advanced" the learner is perhaps left more to his/her own devices than with other languages. &gt; Parse error. Means something, somewhere went wrong, enjoy finding the bug yourself. With a parse error, GHC will tell you the line number. Parse errors are no harder to figure out the cause of in Haskell than in any other language. &gt; Intendation error: Somewhere in your code there is a wrong indent Nonsense. Sure, indentation in "do blocks" might throw off a beginner at first, but it's no more insurmountable than what you experience when you incorrectly indent multiple blocks in Python. &gt; Wall of text error message. GHCi is world leading in providing worthless error messages. If you haven't given your terms explicit types, I can agree that GHC(i) can give pretty scary and hard to digest error message. But when you have explicit types? No! Quite to the contrary! &gt; missing parenthesis You're exaggerating. GHC doesn't throw more of a fit than any other compiler for any language when there's a missing parenthesis. &gt; When I code in C, Java, Python, Mathematica, C# or any other language and I google my error message I get an answer. When I google my haskell error messages I get nothing. It seems to me that you have a very unhealthy relationship with compiler errors. Instead of understanding what's wrong, you ask Google for a "fix". That's going to get you in trouble in any language, regardless of the number of Google hits you get (I do agree that there's probably a lot fewer for Haskell errors). 
&gt; It seems to me that statements like "A is a subtype of B" can be completely replaced by statements like "a conversion function from A to B is available" I've always been a little uncomfortable with that aspect of C++. My intuition is that convertability isn't always what you want. There might be a conversion which is basically ok but only there for convenience, and then when you want to check if some pointer points to a Foo it might say yes when you don't really want to consider it to be a "true" Foo, just something that could be converted to one.
Until Rails 4.2 there wasn't support for adding database level foreign keys using the Rails migration methods, and I've seen several people prefer creating "foreign keys" at the Rails level with something like: has_many :comments, :dependent =&gt; :destroy Persistent automatically creates foreign keys for a relationship like this: User username Text Email email Text user UserId Maybe but doesn't natively support something like `ON DELETE CASCADE` or `ON DELETE NULLIFY` as far as I'm aware.
&gt; takeMVar mvar can throw async exception unless mvar is full. You can try to guarantee it is full or you need a strategy to handle the exception if you can't guarantee that. That's actually an interesting example; a `BlockedIndefinitelyOnMVar` exception will be raised in your thread, but there's nothing you can do to recover your thread (i.e. it's still killed, although you can try to perform some final action in a handler).
I actually haven't had those sort of problems with Haskell. The compiler error messages are more helpful than most languages and more succinct than errors in languages like C++. Sometimes I get wall of text errors in Haskell when my types don't line up correctly but if you get in the habit of declaring types manually it helps the compiler out a lot. I haven't gotten many confusing Haskell error messages. Learn You a Haskell and Real World Haskell are excellent tutorials.
Currently async exceptions are less useful because of cleanup being brittle by default and inability to trust code to behave with async exceptions. With the proposal, async exceptions become more useful because they won't break code.
Do you have a concrete example of what you want to be able to do more easily?
Your best bet is to give concrete examples of what you're trying to accomplish.
side effects are explicit in haskell. that is a good thing regarding correctness, but sometimes gets more cumbersome to write. that's a tradeoff you will have to deal with. of course there are things that would make it more tolerable. a full-blown effects system makes it easier to work with an io monad. and idris' `!` with "local type" `Monad m =&gt; m a -&gt; a`, that you can use within an `m` do block, would make many idioms more concise to write.
Please ask a specific question, perhaps on stack overflow 
i guess that post is triggered by the recent discussion regarding masking or not within `bracket`. see [the libraries mailing list](https://www.haskell.org/pipermail/libraries/2014-November/024231.html) for a lengthy discussion about it.
Maybe I should expand the statement, it was clear enough in my head but something was lost in translation. Let's say we have a program that relies on subtyping. Now if we forget about subtyping relationships and assume instead that the types are unrelated, the program doesn't typecheck anymore. My claim is that we can make it typecheck again (and have the same semantics as before) by assuming that each subtyping relationship corresponds to a conversion function, and inserting appropriate calls to these conversion functions throughout the program. That's not a completely obvious claim, and its truth might depend on the language. Maybe in some languages you'll need to do more than just insert conversion functions. But it seems to me that many "tricky" questions about covariance and contravariance, like the one in the OP, can be solved by rephrasing them in terms of conversion functions instead, and no interesting features are lost after such rephrasing. Does that make sense?
&gt; IO is almost aways the easiest part of the work in other languages, as well as the place where you care less about potential bugs Are you serious? You've never had a bug due to unexpected interactions through shared global state?
Ah, ok. While it is correct, and probably still an issue for deprecated `block` and `unblock` primitives, it is not an issue for `mask` because claver trick is used to prevent that. See here for more details: http://www.well-typed.com/blog/97/ (The article is written much better then mine, and covers related background, so I'd recommend it)
That is *my* fault that you don't understand the article. I'm already working on example, but I'm not sure it will help. Is there anything you understand from the article? I need something to start from. Probably I skipped too many "trivial" things, that are *not* trivial. I'll appreciate if you point me to such things.
&gt;Agreed. But you should point out that there are some very good ones, like LYAH. Yes for very baisic stuff. Then it kind of ends. For Java there are online courses for how to make anything. Haskell has the absolute baisics. &gt; Nonsense. Sure, indentation in "do blocks" might throw off a beginner at first, but it's no more insurmountable than what you experience when you incorrectly indent multiple blocks in Python. It might only be me but the line of the error is often very wrong. My GHCi very often puts the error on the last line. With python I get a lot more precise errors. It nearly always gets the line right. If I miss a ; in C or Java it doesn't take many seconds to fix. I wrote a small mistake in a if statement on the third line of a haskell program and it put the error on the last line. Took ages to go back line by line deleting stuff until I had 3 lines left. A tiny error in 7 lines of code, giant error message: testahaskell.hs:2:23: Couldn't match type ‘Integer -&gt; Bool’ with ‘Bool’ Expected type: Integer -&gt; Bool Actual type: Integer -&gt; Integer -&gt; Bool Probable cause: ‘beta’ is applied to too few arguments In the first argument of ‘filter’, namely ‘beta’ In the expression: filter beta [a .. b] testahaskell.hs:3:25: Couldn't match expected type ‘a -&gt; Bool’ with actual type ‘Bool’ Relevant bindings include zeta :: a -&gt; a -&gt; Bool (bound at testahaskell.hs:4:23) x :: a (bound at testahaskell.hs:3:20) beta :: a -&gt; a -&gt; Bool (bound at testahaskell.hs:3:15) Possible cause: ‘zeta’ is applied to too many arguments In the expression: zeta x 2 In an equation for ‘beta’: beta x = zeta x 2 where zeta x y = if x == y then True else if x `mod` y == 0 then False else beta x (y + 1) Failed, modules loaded: none. Prelude&gt; for the code primlist :: Integer -&gt; Integer -&gt; [Integer] primlist a b = filter beta [a..b] where beta x = zeta x 2 where zeta x y = if x == y then True else if x `mod` y == 0 then False else beta x (y+1) python gave the error Traceback (most recent call last): File "main.py", line 17, in print(primlista(2,30)) File "main.py", line 15, in primlista return list(filter(zeta,integers(a,b))) TypeError: zeta() missing 1 required positional argument: 'b' I wonder why haskell has a dificulty attracting crowds.
&gt; Is there anything special with BlockedIndefinitelyOnMVar? Yep! See here (which I think I meant to post with my comment): http://blog.ezyang.com/2011/07/blockedindefinitelyonmvar/
Yes, that was what I was talking about, but it seems I misunderstood how dynamic casts work. I am relieved to hear that it works that way instead of the way I thought.
For beginners its not uncommon to only write single threaded programs. If they also managed to avoid using global variables, I could easily see them not encountering those issues. Some types of side effects (such as reading and writing files) are easy to get 95% correct even in languages that don't help you manage side effects.
tomato &lt;: fruit &lt;: plant
Well, the answer depends on exact meaning of "2 async exceptions". If you mean "2 exceptions of type `AsyncException`", then the issue is not solved because you can throw `AsyncException` either synchronous or asynchronous. But probably you mean "2 exceptions that are raised asynchronously (via throwTo)". Then the issue even doesn't exist in the first place. Because exception is async or sync only when we are throwing it. When thrown, it is just an exception, there is no more way to say how they were thrown. So there is an issue when 1st exception is catched inside `bracket`, and 2d exception is asynchronously thrown. So lets say about 1 async exception while handling other exception. But the 2d exception was catched and rethrown multiple times before comes to your hands, *synchronously* rethrown. So there is very little sense in such distinguishing. But that is a theory. In practice if the proposal will be implemented, such multi-exception case will occur really rare. But number of bugs will remain almost the same. They just will be hidden, and will occur in really rare situations. But they *will* occur. As I said, I wanted `uninterruptibleMask` in `bracket` multiple times. One of proposal drafts is about adding RTS option to enable `uninterruptibleMask` in production and disable in development. I don't think it will work, but probably something similar will. But your proposal is unrevertable, we will not be able to recover is we find something better.
You could say they're types. It just so happens that the subtyping relationship holds for both `tomato &lt;: fruit` and `tomato &lt;: vegetable` even though `fruit &lt;/: vegetable` and `vegetable &lt;/: fruit`.
The filesystem is a sort of global state. It helps a lot to know when something cannot be modifying it.
I can give you a tomato, therefore I can give you a vegetable (because I have a tomato, and tomatoes are vegetables). If you give me a vegetable [any vegetable], then I can give you soup, therefore If you give me a tomato, then I can give you soup (because I need a vegetable, and tomatoes are vegetables). If you give me a process for turning tomatoes into soup, then I can give you soup, therefore If you give me a process for turning a vegetable [any vegetable] into soup, then I can give you soup (because I have a tomato, and tomatoes are vegetables). Notice how in the first and third examples, the subtext is "I have a tomato." In the second, the subtext is "I need a tomato". Inserting another arrow is just flipping the "have/need" switch. Notice how important the clarification [any vegetable] is in the third statement. If you give me a process for turning *some particular* vegetable into soup, then I probably can't give you soup, unless that "particular vegetable" happens to be tomato, because I've only got a tomato.
&gt; no one writes kernel code in Haskell [au contraire](http://stackoverflow.com/questions/6638080/is-there-os-written-in-haskell)
Sure, it helps if you are writing bomb proof programs, but even when you are accessing the file system and haven't used any mechanism to guarantee that nothing is modifying it, 99.99999% of the time, it turns out that nothing else is modifying it, so it would be pretty easy, especially for a beginner, to never have problems accessing the file system in impure languages.
I agree that the Haskell error message is a bit unwieldy and (worse) doesn't identify the problem clearly. However, it could have been made significantly more useful by giving type annotations for `beta` and `zeta`, like so: primlist :: Integer -&gt; Integer -&gt; [Integer] primlist a b = filter beta [a..b] where beta :: Integer -&gt; Bool beta x = zeta x 2 where zeta :: Integer -&gt; Integer -&gt; Bool zeta x y = if x == y then True else if x `mod` y == 0 then False else beta x (y + 1) This gives the much more concise and helpful error message tmp.hs:10:28: Couldn't match expected type ‘Integer -&gt; Bool’ with actual type ‘Bool’ The function ‘beta’ is applied to two arguments, but its type ‘Integer -&gt; Bool’ has only one In the expression: beta x (y + 1) In the expression: if x `mod` y == 0 then False else beta x (y + 1) This actually illustrates quite well how Haskell's flexibility necessarily leads to less clear error messages in some cases. In languages where you have essentially no type inference (`MyObject obj = new MyObject();`) or which don't have automatic currying, the compiler has much more information to work with. But that comes at the expense of not having those features, which is also quite annoying. In Haskell, you can find your personal middle ground by giving type annotations only where you feel they make sense, rather than everywhere.
Yeah. There's still something I don't understand though. Surely not every function from A to B gives rise to a subtyping relationship, otherwise Int would be a subtype of String because you can print it. But then what does ezyang mean by "(a -&gt; r) -&gt; r is a subtype of (b -&gt; r) -&gt; r"? Sure, we've found a very nice conversion function. But what exactly are the requirements for a conversion function to be interpreted as subtyping?
&gt; If you mean "2 exceptions of type AsyncException", then the issue is not solved because you can throw AsyncException either synchronous or asynchronous. You know what async exceptions are, so why play a silly semantics game? If someone throws an async exception at me when I'm masked-uninterruptible, it cannot be caught and rethrown as a sync exception. &gt; But number of bugs will remain almost the same. They just will be hidden, and will occur in really rare situations You've repeated this plenty of times but you've not actually established this at all. &gt; One of proposal drafts is about adding RTS option to enable uninterruptibleMask in production and disable in development Why would I want my development version to be buggy and partially abort cleanups randomly? 
My understanding: * Exception-throwing should mean that side-effect was not done when possible (including in hClose). * When it is not possible to guarantee side-effect was not done, the semantics should be decided on a case-by-case basis (e.g: hClose losing its buffers should close the handle and use a special exception that is documented to have this unfortunate behavior). * Sync exceptions should contain the damage caused if they happen * Cleanup handlers (like C++ dtors) must happen, and sync exceptions mean they weren't. Thus, one must know exactly what sync exceptions are possible and take care to prevent and/or handle each possible sync exception on a case-by-case basis, in addition to "containing the damage" as above * In cleanup handlers, however, async exceptions would normally partially abort the cleanup, which is almost always a **bad idea**. Therefore they can and should be fully masked. You seem to believe that there is something that can be done when some *arbitrary, unknown* sync exception happens in your cleanup. Thus, handling that kind of exception safely means you handle async exception safely, and better use the latter to see you're safe for the former. But you are wrong: There's nothing correct that you can do to handle *arbitrary, unknown* exceptions in your cleanup. Thus, you want to get rid of all the async ones and then think hard about the remaining possible sync ones.
&gt; It is simply not possible to guarantee, regardless async/sync difference. It is possible to guarantee that async exceptions do not prevent a cleanup such as hClose from happening. Just mask them! It is not possible to guarantee that sync exceptions in hClose (e.g: underlying POSIX close) will not prevent hClose from happening. Very different beasts. 
&gt; Again, regardless sync/async difference The difference, again, is that async exceptions become "No Throw" trivially with mask. &gt; I'm arguing the opposite from the very beginning. The article itself is about that. I probably missed your arguments (so sorry), could you please replicate them? Sync reasoning: I have a spec for a function. For example: takeMVar. It can only throw a sync exception if my code has bugs. If my code has bugs, I'd rather it panic'd and froze because I have no idea what the consequences of the bugs would be. Doing wrong things is much worse than doing nothing, and perhaps a watchdog can restart the crashed process. So I decide my handler for takeMVar sync exceptions freezes / aborts the process. Async reasoning: Now, during takeMVar an async exception may occur, just like it may occur anywhere, and there's absolutely nothing that it means about the invocation of takeMVar, and is basically an abort-request for that operation. So, I want to let it abort and re-throw when abort is acceptable, and I want it to be uninterruptible-masked when abort is not acceptable (cleanups). See how the two strategies for reasoning are unrelated? &gt; I'll show an example. takeMVar mvar can throw async exception unless mvar is full. You can try to guarantee it is full or you need a strategy to handle the exception if you can't guarantee that. You're not reasoning about whether or not it can throw an async exception, but about whether it can throw async under "mask". Without "mask", it can throw anyway, and with "uninterruptibleMask" it cannot at all. So essentially you're just reasoning about whether it is interruptible. Your claim that most code runs under "mask" is disputable and in any case not quite relevant: If the context is abort-able, then you don't need "mask" or "uninterruptibleMask". If the context is not abort-able, then you need "uninterruptibleMask". That's all the reasoning you need to do. You don't need to worry about MVar being full or not and the kind of reasoning you need for sync exceptions. 
I wouldn't say Haskell is lacking in "visualization" but in a wider scope of signal processing, matrices, vector math, etc. The ecosystem is just a mishmash of incompatible types and algorithms.
This is the github repo: https://github.com/Frege/frege.
Define "is a Haskell" :) It's not a sub-set nor a super-set of the Haskell syntax; as the repo says its a language "in the spirit of Haskell". 
It is very interesting that you showcased ghcjs and mentioned that FP Complete is working on some tooling for it. One of the reasons i've been holding off on fay is because it always felt like a stopgap until a proper solution emerges. Can you comment on why not haste? Also for such ambitious project like ghcjs having only one developer actively working on it is kinda scary. Raises questions of long term support and survivability of the project. 
It looks great! This is IMO exactly the sort of thing we need to attract more developers and encourage people to make new projects in PureScript. Thank you!
Choose Haskell. - If you want a deep understanding of what programming fundamentally is. Learning to program is often the same as learning to think properly. - If you expect to be still coding in twenty years time, would like the code you write today to last, and want your efficiency to be maximised over your career. - If you're the type of person who likes to generalise. 5 years of 12 month data is one pass - what about analysing all potential ratios at all time periods at all time grains at once? Haskell encourages you to generalise and encapsulate the entire problem domain. - If you're prepared to look outside the haskell box for visualisation. Once you can get your data to the browser, there is a world of solutions out there, such as D3.js. Choose R - If you see programming as a means to an end. - If you take a quick look at ggplot and get it (I miss it so much!). - If you want to get stuff done quickly: you want to maximise your efficiency over a year. - If you're prepared for any code you write to fairly quickly degrade in the sense of being unable to productively generalise and extend it over time. - If you want an environment where a lot of the work has already been done for economic/finance/quant tasks and libraries. I learnt R first, using it in finance but reached a point where my efficiency declined and I found myself spending too much time debugging. I switched to Haskell, spent a year bemoaning my lack of real world productivity, but you emerge from the other side way ahead. Choose both! 
Yes, you're correct.
Not sure, I upload manually with this script: https://raw.githubusercontent.com/ekmett/lens/master/scripts/hackage-docs.sh
The reason why `myid1 = id` doesn't compile is that `id` has type `forall a. a -&gt; a`, but you need a value of type `Any -&gt; ()`. Since `Any` and `()` are not the same type, there is no concrete type at which `a` could be instantiated so that `a -&gt; a` would become `Any -&gt; ()`. The fact that `Any` is a subtype of `()` does not change this. In the following implementation, on the other hand, `x` has type `forall a. a`, and we need a value of type `()`. We need a concrete type at which `a` could be instantiated so that `a` would become `()`, and the obvious choice is `a = ()`. myid1 :: Any -&gt; () myid1 x = x Your implementation of `myid2` is rejected for the same reason, and so is the following version: myid2 :: ((Any -&gt; r) -&gt; r) -&gt; ((() -&gt; r) -&gt; r) myid2 f = f The reason it doesn't work this time is that the `forall` is still nested inside of the type `(Any -&gt; r) -&gt; r`, so there is no opportunity to instantiate `a` to anything. In order to instantiate the `a` to `()`, we need to further eta-expand the functions until we get to `x :: Any` and instantiate it to `()`. myid2 :: ((Any -&gt; r) -&gt; r) -&gt; ((() -&gt; r) -&gt; r) myid2 f g = f (\x -&gt; g x) By the way, I'm not sure if it's proper to say that `Any` is a subtype of `()`. There is certainly an implicit conversion between `Any` and `()`, but as far as I know, Haskell doesn't have a rule A1 :&lt; A2 B1 :&lt; B2 -------------------- A2 -&gt; B1 :&lt; A1 -&gt; B2 as is typical for languages which support subtyping. So I'm not too sure what kind of subtyping /u/ezyang is referring to in the post.
If only GHC were more of a cross-compiler all these little haskell-for-JavaScript-type things could be folded into the main line.
This really is the thing to do. Also see: http://hackage.haskell.org/package/neil https://github.com/ndmitchell/neil http://neilmitchell.blogspot.com/2014/10/fixing-haddock-docs-on-hackage.html
There's a problem with using scripts on my end: a lot of symbols don't get links generated.
Registration is not yet open, but the Call for Presentations is live: http://www.composeconference.org/call/index.html I know it's short notice, but I'm hoping for some great submissions, and we already have a few exciting keynotes lined up.
&gt; I read a few days ago that someone discovered a memory leak in Gloss Are you talking about me? I [fixed a leak in Gloss](http://gelisam.blogspot.ca/2014/08/homemade-frp-mystery-leak.html) a few months ago, but I didn't consider the blog post interesting enough to submit it here. The fix was merged to GitHub in August, like you said, but the latest gloss-1.8.2.2 does not contain the fix: I did `cabal get gloss-1.8.2.2` and the resulting code still contains the `worldStart` references I had removed. In fact, there doesn't seem to be any difference between the code for 1.8.2.1 and 1.8.2.2! That's probably a mistake...
what percentage of coverage do you have? cabal haddock --hyperlink-source --haddock-options="--print-missing-docs --built-in-themes" 
Please check out http://ocsigen.org. ;) Also, as mentioned already, I'm quite curious why you didn't pursue Haste instead, since it already has client/server capabilities.
I've pinged the infra people, it should be catching up now and they've put in measures to deal with what was causing the problem automatically.
I'll add that GHCJS was surprisingly easy to install. The reputation made it sound hopeless. It took a while to build everything, but I encountered no difficulties. I just made an hsenv and then followed the instructions in the README. It maintains its own package database as ghc does, so you don't have issues with the environment/package database. I'm using it with Yesod (a la yesod-fay) at the moment on a project and it's fine. If you were waiting for it to get easier to try, stop waiting. Go try it.
In the debouncing example, he sleeps for a second before trying to takeMVar again. Doesn't this mean that the second click after the first click would try to putMVar in an MVar that has a value, and thus block? Would this cause the putting code to block for a second?
I would say that's true! However, I was actually intending to put a "like" in the title. As in "Frege is like a Haskell for the JVM." My bad.
I'm not sure what you're trying to find out here. Each module has its own percentage. Why would the coverage have anything to do with this anyway?
You need to have installed the documentation for a package for haddock to generate links to it. I recommend adding "documentation: true" to your .cabal/config so that you never forget to generate it.
TheCatsters videos are a treasure.
The putting code could be using `tryPutMVar` which is non-blocking. My question is what happens in this scenario: 1. debouncing thread blocks on takeMVar 2. putting code puts a value into the MVar 3. debouncer takes the value - performs the action 4. 1/10th of a second later the putting code puts another value 5. debouncer wakes up from threadDelay after sleeping a second 6. debouncer takes the new value from the MVar and performs the action So two clicks 1/10 of a second apart results in two actions performed? Or does something else happen? 
The short answer: No, that will never happen. 
Frege is a famous person. http://en.wikipedia.org/wiki/Gottlob_Frege
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Gottlob Frege**](https://en.wikipedia.org/wiki/Gottlob%20Frege): [](#sfw) --- &gt;__Friedrich Ludwig Gottlob Frege__ (/ˈfreɪɡə/; German: [ˈɡɔtloːp ˈfreːɡə]; 8 November 1848 – 26 July 1925) was a German [mathematician](https://en.wikipedia.org/wiki/Mathematics), [logician](https://en.wikipedia.org/wiki/Mathematical_logic) and [philosopher](https://en.wikipedia.org/wiki/Philosopher). He is considered to be one of the founders of modern logic and made major contributions to the [foundations of mathematics](https://en.wikipedia.org/wiki/Foundations_of_mathematics). He is generally considered to be the father of [analytic philosophy](https://en.wikipedia.org/wiki/Analytic_philosophy), for his writings on the philosophy of language and mathematics. While he was mainly ignored by the intellectual world when he published his writings, [Giuseppe Peano](https://en.wikipedia.org/wiki/Giuseppe_Peano) (1858–1932) and [Bertrand Russell](https://en.wikipedia.org/wiki/Bertrand_Russell) (1872–1970) introduced his work to later generations of logicians and philosophers. &gt;==== &gt;[**Image**](https://i.imgur.com/0YWBPZn.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:Young_frege.jpg) --- ^Interesting: [^Begriffsschrift](https://en.wikipedia.org/wiki/Begriffsschrift) ^| [^Bertrand ^Russell](https://en.wikipedia.org/wiki/Bertrand_Russell) ^| [^Analytic ^philosophy](https://en.wikipedia.org/wiki/Analytic_philosophy) ^| [^Kurt ^Gödel](https://en.wikipedia.org/wiki/Kurt_G%C3%B6del) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm4myib) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm4myib)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Yes, your blog post was what I read. I don't remember how I came across it. The bug seems like a fairly serious one! Hopefully the fix gets propagated to the official release soon.
Yes! I love it. Fay, Haste, Elm, Frege, PureScript, Idris. I'm sure I'm missing some. 
Agda, of course. And if Haste counts, then you should count ghcjs, too. Coffeescript, Roy, and Scala have traces of Haskell, although that's kinda stretching it.
&gt; Can you comment on why not haste? Firstly, the mundane reasons: I only had enough time to talk about two options, so I chose the two at the ends of the spectrum (Fay being easy to install/less features, GHC being more involved to install/more features). I also promised Luite at ICFP that I'd talk about GHCJS. Now the technical. Firstly, I really wish someone more informed than me had a good comparison of Haste and GHCJS (if such a comparison exists, please link it in this thread so no one's listening to uninformed me). From everything I've seen, Haste seems like it provides compile-time Haskell semantics, but runtime Javascript semantics. The cool business of using green threads and STM seems to not work in Haste. For the purposes of this talk, I definitely wanted to show off full power of Haskell (consider the talk a "why Haskell is so much more awesome than your language" in disguise ;) ). GHCJS was great for that purpose. Now that we're trying it in a real project, having access to STM and green threads is a huge boon to productivity. I'd argue that you don't even realize how much you're missing it until you start programming with it again. &gt; Also for such ambitious project like ghcjs having only one developer actively working on it is kinda scary I'm not convinced that there *is* only one developer. I have a feeling that there's a distributed team of Luites roaming the planet hacking on GHCJS in every corner of the globe. Think that's crazy? Well, can **you** prove there's only one Luite? That's what I thought. Seriously, I don't think Luite *is* the only person who's working on the codebase, and historically that's definitely been true. He's taking up the lion's share of the work right now, but I don't think that means no one else is capable of working on the codebase in his absence. Also, with a project like this, we eventually have to reach a tipping point where people start using it for real projects before there's enough critical mass for more people to join the project. I've experienced that with Yesod, and I'm sure other maintainers of large projects can speak to that as well. In other words, I believe that if we start using it in earnest, the problem will solve itself.
You're right about `tryPutMVar`. You can [see the code yourself](https://github.com/snoyberg/polyconf-2014-haskell-webapp/blob/ghcjs/Client/Debounce.hs). You're also correct about the analysis of the scenario. To clarify: I originally wrote this debounce code for a completely different use case, namely flushing log files. Part of the coolness of this demo is that a server side technique for logs files works on the client side as well. That said, I can imagine that in real life, you'd want to add a cooldown period to the debouncer as well. There are two easy ways I can think of to do it, depending on how precise you want to be: 1. In the worker thread, delay by `cooldownPeriod` and then `tryTakeMVar` before sleeping for `oneSecond - cooldownPeriod`. The problem with this is that `threadDelay` may take more than `cooldownPeriod`, which could be problematic (though that's probably unlikely). 2. Instead of just `tryPutMVar signal ()`, use STM and put the current time. Then have the worker thread compare the time in the `TVar` against the last time it ran and, if the difference is less than `cooldownPeriod`, ignore it.
haste and ghcjs are haskell as far as i'm concerned. Fay strips out enough that it may not be haskell on the nose. in any case, what are the defining features of "a haskell"? purity + syntax + typeclasses? I'm pretty sure non-strictness can't be a defining feature if we want to have a list like that, as Idris wouldn't qualify then. (And nor would standard charter's Mu, which is certainly "a haskell").
It's a shame [Leksah](http://leksah.org/) and [HaskForce](http://carymrobbins.github.io/intellij-haskforce/) don't appear in it, as people need to be aware of the IDEs that are out there.
Is it possible (or, I suppose just as importantly, practical) to use the type-safe API calls in Yesod between the client and server with Elm? Also, in his thesis, evancz gives multiple reasons for not using Haskell in the browser (and hence, the motivation for producing Elm in the first place), the most convincing to me being that FRP is better implemented in a strict language. Your thoughts?
That is so mechanical... But I guess things work in that way.
https://www.haskell.org/communities/11-2014/html/report.html -- HTML version.
Anyone can contribute to HCAR. Please suggest it to the authors of Leksah and HaskForce!
https://www.youtube.com/watch?v=RDsuPdMptCU is a short video I put together a year or so ago showing Frege building via my frege-maven-plugin and OSGi. It was fairly off-the-cuff but ask away. I also publish an OSGi aware frege bundle to Maven Central for anyone wanting to quickly get up and running. I'll be updating my https://github.com/talios/frege-testing repository shortly with the latest plugin changes.
I haven't read Evan's thesis (do you have a link handy? I'd like to try to get to it), so I'm incredibly cautious to actually make any comments on it. However, regarding your specific statement here about FRP and strict languages, I'd say: 1. I'm not an FRP expert. 2. There are lots of people who *have* implemented FRP in Haskell, so it's certainly not impossible. 3. We're currently experimenting with client side solutions based on React, and I'm cautiously optimistic about it. Whether that counts as FRP or not is probably up to debate regarding the definition of FRP, but regardless I think my answer would be: I think there's a really good story for how to do client side dev in Haskell. It's probably not trivial to use the exact same trick I demonstrated to make Yesod/Elm calls, but having some intermediate representation of the request/response pairs and then generating appropriate Haskell and Elm code is likely not too difficult a task.
I wrote a small introduction to Haskell for a couple of IRC users and decided to share it here. As I'm still a beginner to Haskell, I'd appreciate it if this document could be checked for any factual errors. Also, I'd like to know if complete newbies to Haskell have trouble reading this document. I hope that this document will at least be of use in showing the elegance of Haskell to new Haskellers. :) EDIT 2: I've replaced the sieve with Fibonacci. I am planning to throw in a real-life example too (any suggestions?).
There was a Nix expression floating around just a couple of days ago. 
it was specifically for haskell
Yeah, it was on some person's github. I really want it to be in nixpkgs - I tried to merge it in but it breaks if you do that. Once it's in nixpkgs we can start packaging up libraries and really building up the nix infrastructure for ghcjs. 
From the Elm website, I think this is the thesis: http://elm-lang.org/papers/concurrent-frp.pdf 
I believe frege is only similar to haskell - there are some differences that make interop with the jvm world easier and the language more performant on the jvm. I'm not sure how much of ghc it could use. 
https://github.com/talios/frege-testing was just updated to use a new release of the tiles-maven-plugin and a new frege-maven-tile along with a new frege-maven-plugin ( annoyingly I was having (*#&amp;$(# isses with oss.sonatype.org so had to split out the tile to a separate artifact ). These should all be in Maven Central shortly.
What's the threshold for accepting a submission? I notice that several entries seem to be mostly about describing/advertising a Haskell library/package. Should more package authors submit HCAR entries for their packages?
Hi, thank you for your questions. I've edited my post to answer them.
Hi, thank you for your response. However DustMite seems not to be using program slicing but an ad hoc method.
Hi, thank you for your questions. I've edited my post to answer them.
You should have linked to a gist that renders nicely the markdown text.
Thanks for your comments! Here are my responses: &gt; 1) It took me quite a while to visualize how the 'pows' function builds up the final list. Similarly to the 'sieve' function, I think an explanation where you show the first few recursions could be helpful here. That appeared to be the case to me too; I've tried to explain the sieve function, but perhaps more examples would be required. &gt; 2) I'm not sure if calling 'primes' a variable is correct. Calling it a variable implies that it's mutable. I think technically it's a top-level expression. True; I'll check this carefully. I was thinking of "constant" (so as to allow readers to relate to the language). &gt; 3) Where you show the structure of a list comprehension you could possibly mention that one could draw values from multiple lists, based on multiple predicates, although for your example doing this isn't necessary. This introduction isn't meant to teach Haskell (rather to get newbies interested), which is why I omitted such details and tried to simplify most of the information. &gt; 4) If you're planning on expanding this introduction in the future I'd suggest adding the famous 'quicksort' function to it. While it doesn't do in-place sorting and as such it's not efficient, I think it's a beautiful showcase of Haskell elegance. I'm probably partial to it somewhat as it was the function that got into the language. :) The 'quicksort' function is covered in the first reading I linked to. I didn't include it because even though it was elegant, it wasn't particularly efficient and can't really be considered to be "true" quicksort. --- Thanks for your comments once again!
paradoja just linked his thesis -- it's the same link I would have given. It's worth a read, at least the parts giving an overview and motivation. He also addresses the existing Haskell implementations of FRP. I found it quite accessible and interesting, and I do not have a formal computer science background. Anyway, I found his arguments convincing, and will probably be trying a real-world project soon in Elm. It would be really awesome if I could get type-safe api calls from the client to the server, but as you said, it's probably not trivial and would take some work to make it play nicely with Haskell.
Unfortunately, markdown headers aren't supported in literate Haskell (GHCi throws an error when I attempt to use those) and the use of `&gt;` to denote Haskell code to be executed clashes with Markdown which parses that as a blockquote. I've tried to type it out nicely (ensuring that each line is less than 80 characters long, with the exception of the URLs) for readability.
You described your understanding of exception safety and exception handling in other thread (link for other readers: http://www.reddit.com/r/haskell/comments/2m6swm/making_bracket_uninterruptible_during_cleanup/cm4dr4i?context=3 ) It appears that your understanding differs from mine. The whole article is built on top of widely adopted approach, developed by David Abrahams (see references in the article). I briefly described it, and then showed how it applies to haskell with it's async exceptions. The point is that the approach works well, and handling async exceptions is not special in that approach and doesn't require uninterruptibleMask everywhere. Now I see that you disagree with the approach. That explains all misunderstanding between us, and that is excellent result on itself, I really appreciate that. It is perfectly fine to disagree, but you can't falsify the article and my arguments based on that. If you see any flow in my arguments, then I'll be glad to discuss them. But I didn't expected that anyone will disagree with Abrahams' approach. Actually I don't know any other solid approach (that doesn't mean it doesn't exist). So I'm not ready to discuss that. Probably /u/simonmar will describe his authoritative opinion. But you should agree, that the approach I described is solid and works well without `uninterruptibleMask` in `bracket`. I probably should make one step back and try to convince community that David Abrahams' approach to exception handling is the way to go. Actually it is already implicitly applied in base library. 
In fact, I was not talking about the js_of_ocaml compiler, but more about the client/server integration (which is the "eliom" library). I'm confident the "compilation to JS" problem will be solved in haskell (even if I don't agree with the way it is solved by GHCJS). A clean client/server integration is an entirely different problem, which has been already investigated in some other languages. :)
tnx for clarifying.. &gt; even if I don't agree with the way it is solved by GHCJS could you please (point to a place/comment where you/someone) explain(s) why?
Previous exposure to programming isn't a hindrance. Such a myth.
I'd rather not have support for garbage in the ghc main line though
GHCJS is being shipped with 7.10? Do you have any more info about this? It would incredible if it did! 
You can use [Setext-style headers](http://daringfireball.net/projects/markdown/syntax#header) to get around the GHCi error. Of course, GitHub won't render Literate Haskell files as Markdown (see [github/markup#196](https://github.com/github/markup/issues/196) for their rationale). But for some strange reason, Gists will. I copy-pasted your file into a Gist: &lt;https://gist.github.com/tfausak/ad197e458b5664251b86&gt;. 
As far as I understand, GHCJS embed the whole GHC runtime in JS, including the GC. Is that right ? I just don't think it's a good idea, it's too bloated and I'm very dubious about the performances.
"earliest opportunity": http://www.reddit.com/r/haskell/comments/260z7z/hom_reactjs_for_haskell/chmvxmp certainly not a certainty :) changed my wording, not to confuse more people...
maybe it can only be embedded (or have parts of it embedded) if it is actually needed.
If I remember it, I'll try to contact the authors before the next edition. I'd love to integrate those entries too.
btw you can make those curved arrows symmetric by `with &amp; arrowTail .~ lineTail &amp; lengths .~ ...)`
Thanks, I'll convert it to Markdown. :) EDIT: Converted and shifted it to a Gist.
&gt; The 'quicksort' function is covered in the first reading I linked to. I didn't include it because even though it was elegant, it wasn't particularly efficient and can't really be considered to be "true" quicksort. Worse – not only is it not a "true" quicksort – it is not a quicksort at all! What makes quicksort quicksort is the in-place swapping of elements to split the array into bigger than and less than the pivot. Saying the slowsort is not a "true" quicksort is like saying a 747 is not a "true" fighter jet. I support your decision to not include it.
Quite true; that is what I think quicksort should be too. The fact that it uses constant extra space is what makes it preferred against other sorts like mergesort.
Nice article. The pattern tuple? -&gt; state! is wonderful.
Sure. I tried to give a short explanation [here](http://www.reddit.com/r/haskell/comments/20u8qf/ann_typesafe_clientserver_communication_in_web/cg79i1t) when discussing with the Haste guy. i will be happy to answer more questions. 
&gt; `ON DELETE NULLIFY` The standard spells this as `ON DELETE SET NULL`, IIRC.
I think I understand it. If I'm right, my next question would be: doesn't this mean that the Javascript needs to be regenerated each time a parameter changes? I have some experience with this in the Yesod world. The solution we've come to generally is that large blocks of Javascript that don't change should go into static files instead of Widgets, to avoid unnecessarily retransmitting the same data. If you're talking about just embedding some data for the client side to read, I'd normally just do that either via a `data-` attribute or, possibly, as a Javascript identifier. It seems like the Eliom approach gives type safety (a good thing). Is there something else it provides that I'm missing?
No, the whole javascript is known statically, except a set of "holes" that are given by the server during page generation. These "holes" are values, not code. Edit: The issue is that it's always hard to describe a ten years old project in just a few sentences. :D Eliom gives you a bit more than just typesafety by allowing you to write code that will be shared between client and server. For example, a widget will be written once, and you can use it to generate the HTML statically on the server ... or insert DOM elements on the client. You can go even further and use type trickery to write the same code which will make different things on the client and the server (for example, interrogate the database on the server, or interrogate the server on the client). It can makes your code very concise. Another example is an frp signal that will become the handle of a websocket by crossing the client/server frontier.
Fair enough, but I'm talking about issues of bandwidth and caching. For multiple reasons, it's nice to have a pre-computed Javascript file and serve that to all users, together with a small delta (either in its own Javascript file or as part of the HTML) that fills in the holes. I'm a bit concerned that that's not possible with the approach you're describing.
So, from the bandwith and caching point of view. When an "application" is declared, we optimize away all the call to server by using AJAX and not reloading the JS at all. But even if you don't use that, the javascript is still entirely precomputed. Only a (id, value) table is dynamically generated.
If I remember correctly, he comes down quite hard on the ability to write efficient FRP implementations in Haskell due to laziness. I'm no expert, but my understanding has always been that efficiency in FRP has little to do with laziness and more to do with figuring out how to purely represent that some subset of the effect network needs to be able to be updated independently of other parts. This is the push/pull FRP that Elliott writes about and implemented in `Reactive`, though his mechanism strikes me as playing a strange game with laziness. The writing is clear as to what push/pull needs to do, though. I've been fiddling with Amsden's TimeFlies implementation for a little while as well. It's an AFRP implementation which achieves push/pull semantics even in Haskell.
You're welcome :)
What specific thing that I said makes you think I disagree with David's approach? Do you think David threw exceptions from destructors/cleanups? Of course he did not, because that would be illegal! So his approach is actually compatible with mine and my understanding. * In cleanups, do not allow exceptions, just like in C++ dtors which David endorses. * In all operations, strive to give as strong guarantees as possible: i.e: exception during hClose should *not* have an effect. Here you think it should close the file anyway, so it is *you* who disagree with David. I would add to David's approach that sync exceptions allow and require more specific local reasoning about what could go wrong, and that should be addressed, especially in cleanup handlers.
It's also good to use a separate directory for each library, executable, and test suite. You have 'tests' but the executable and the library are both in 'src'. Cabal will recompile the library together with the executable second time. The separate dirs also allow not repeating dependencies of the library in the executable.
Your sieve, if I'm reading it right, is really just trial division. The [Genuine Sieve](http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf) is a bit more complex.
Only reason I didn't do that, is that most people will stash Main.hs at the top-level and I didn't want to give the reader more work to do. It was already getting fiddly towards the end. Good suggestion, thank you!
read learnyouahaskell at http://www.learnyouahaskell.com . That'll be a pretty good start. There is also a great set of online tutorials from beginner to advanced here -&gt; https://www.fpcomplete.com/school This is where I started.
For me, personally, purity is *the* thing. In principle type classes aren't *that* important (since they can be emulated by dictionary passing), but they *do* indirectly enable a compact notation for monadic computations... which are again important because of purity. (I'm sure there's other ways to do it, but...) So I think it might actually be the combination of purity and type classes.
Is there a reason to use `Double` instead of `Rational` in this case?
I hadn't really thought about it. The values will be stored as floats in the SVG file and parsed as such. I don't know how fast these sorts of calculations are with `Rational`. Would it be noticeable if you are converting a large number of elliptical arcs?
an automatic project starter with sandbox, tests and all the fixins. [holy haskell project starter](http://yannesposito.com/Scratch/en/blog/Holy-Haskell-Starter/)
Yes, yes, lots of `sin` and `cos` and `pi` so it wouldn't work. I would be interested to try out a higher precision number types but it's probably overkill considering the application.
All they need is a frege web framework to take off. 
The whole approach is based on the idea that you should rely on exception safety guarantees instead of a known list of exceptional cases. So unless the subcomponent provides no-throw, then you should be ready to handle exception from it. Note: you should be ready to handle *any* exception from the subcomponent, otherwise the approach is not modular (e.g. it relies on internal implementation and breaks on virtually any change in the subcomponents; also it doesn't work for generic components). That vanishes difference between sync and async exceptions under `mask` (modulo the additional complexity mentioned in the article.) Virtually all `IO` actions can throw both sync and async exceptions. So if you have any issue with async exception, then most likely you have the same issue with sync exception. But sync failures are rare, they occur only in very special situations. So if you apply `uninterruptibleMask`, then your CI server will stop reporting issue, but your application will fail in production, sooner or latter. &gt; In cleanups, do not allow exceptions, just like in C++ dtors which David endorses So you want to ban exceptions in cleanups. The last time I asked, your answer was "No". I states multiple times that I *want* to bad exceptions in cleanups, but unfortunately haskell *allows* them, both sync and async. And you can't eliminate exceptions from hClose. You even can't list them, because Handle is a generic component. hClose is exception-neutral with respect to underlying IODevice -- hClose rethrows all exceptions from it. &gt; In all operations, strive to give as strong guarantees as possible: i.e: exception during hClose should not have an effect. Here you think it should close the file anyway, so it is you who disagree with David. According to David's approach hClose should *not* throw (sync) exceptions, so it *should* close file descriptor. In haskell it throws, but still closes file descriptor. That makes it different from other actions in haskell. That is not what I *want*, that is what I *have to deal with*. &gt; I would add to David's approach that sync exceptions allow and require more specific local reasoning about what could go wrong, and that should be addressed, especially in cleanup handlers. I have nothing against that. Sometimes it is event unavoidable, because subcomponent, you are using, provides stronger guarantees if you satisfy some contract, and you are OK to trade additional complexity for additional guarantees. And async exceptions are not different here: components can provide stronger requirements when wrapped into `uninterruptibleMask`. ******* Speculation below ************** If you want to hear about *ideal* world IMO, then: (1) cleanup actions should not throw while we are handling other exception. (2) hClose should not throw sync exceptions (David doesn't state anything about async exceptions, so we are free to throw them if we find solid way to provide exception safety in that case; obviously, we should satisfy (1) too). That follows from (1). (3) uninterruptibleMask is not necessary in cleanups except some special cases (4) exception handler should not take arbitrary long time. I'm almost sure that *ideal* semantics is possible (I'm not easy with (4) though), and I have few ideas. I'm still trying to formalize them, so I'm not ready to argue here yet. But anyway, that will require a lot of changes in haskell, or even new language. So lets discuss current situation. 
I don't think GHC is actually IEEE compliant regarding floating point numbers yet. /u/cartazio knows more
If you want to step up - http://mmhelloworld.blogspot.co.nz/2012/07/frege-servlet.html could be a good starting point to work from.
I think the title refers to starting a project, not starting to learn Haskell.
I would be interested to find out more. The [ieee754 package](http://hackage.haskell.org/package/ieee754-0.7.4/docs/Data-AEq.html) defines `~==` which has been really useful for using QuickCheck on anything involving `Double`. It says: &gt;For real IEEE types, two values are approximately equal in the following cases: &gt;- at least half of their significand bits agree; &gt;- both values are less than epsilon; &gt;- both values are NaN. But I don't know if that is part of the standard or just picked by the implementer. Does the standard stipulate the amount of error allowed? Should something like `atan (tan x) ~== x` always hold true?
&gt; So if you apply uninterruptibleMask, then your CI server will stop reporting issue, but your application will fail in production, sooner or latter. The thing I keep repeating and you're not seeming to get is: For many contexts, there is *nothing you can do* to "safely" handle the exception. For example, an exception during cleanup. Thus, removing all async exceptions is not "hiding a problem". It is reducing a big problem (async+sync) to a small problem (sync only) which *hopefully* you can eliminate, but more likely, you can just try to contain. &gt; So you want to ban exceptions in cleanups I don't want to "ban" them - because I don't see how you'd implement that. I'd be OK with banning them if we had a way to do it statically. If they're banned dynamically, I don't see how it (e.g: immediate program termination) is any better than just letting them through. But of course they are a big problem! Which is why eliminating the async exceptions is a big reduction of the problem. &gt; And you can't eliminate exceptions from hClose. You even can't list them, because Handle is a generic component. hClose is exception-neutral with respect to underlying IODevice -- hClose rethrows all exceptions from it. I can list them *for specific cases of hClose which I can reason about*. And likely, there's no good way to handle these exceptions, and also likely: the handle *cannot* be closed in many of these cases (again, if you look at the underlying POSIX specification of close, you can see this). &gt; According to David's approach hClose should not throw (sync) exceptions hClose is not necessarily a destructor. There can be a lower-level function that closes which is not a destructor. But if it is a destructor, then it indeed cannot throw at all -- and that's a big problem too, because there's no way to signify a problem even happened. This is a strong argument in favor of having a non-destructor way to close and get some guarantees, or in Haskell, not treat "hClose" as if it were a destructor (IO actions aren't destructors, bracket finalizers are). &gt; cleanup actions should not throw while we are handling other exception &gt; ... hClose should not throw sync exceptions How do they signify problems that break their contract and mean the cleanup did not actually happen? &gt; uninterruptibleMask is not necessary in cleanups except some special cases *Every* interruptible cleanup action in bracket (that follows "strong exception safety", at least) requires an uninterruptibleMask around it, except some special cases -- to avoid a huge source of potential exceptions during the Haskell equivalent of a destructor, thereby *reducing* the problem. I don't understand how you get this reversed, except if you do not understand that bracket is supposed to give cleanup-execution guarantees. &gt; exception handler should not take arbitrary long time Nobody wants it to -- but what do you prefer? Taking a long time (when an underlying resource is taking a long time to actually be released) or breaking invariants willy-nilly? If you want to carry on despite the resource not being cleaned up - you can always use a different IO thread to do extra work *despite* the cleanup not running. Why do you want to destroy my bracket guarantees? The code after bracket isn't the only context you can carry on in.
&gt;(though the latter is usually a hindrance when it comes to Haskell). As somebody that teaches a lot of Haskell, including to non-programmers, I'm going to agree with `applicativefunctor` and say that this is definitely a myth. Programmers are just big blubbering babies and need to get over themselves. Non-programmers have a much harder time of it because of how much more they must learn (Emacs, git, etc). The difference is that non-programmers aren't used to being able to shift trivially between Python &lt;-&gt; Ruby &lt;-&gt; JS &lt;-&gt; Golang and so haven't been conditioned to expect learning a new language to be trivial. Whereas, programmers are used to learning a new syntax, maybe a feature or two and then getting on with it. That is, it's conditioning &amp; behavior - not intellectual. Learning the same language 4 times in a row with different syntax and thinking you've learnt something substantial makes you soft and sets your expectations up the wrong way. Learning Haskell itself isn't even that hard compared to what it takes to be productive and proficient in other languages without littering bugs all over the place (Ahem, C++) - it's just alien. We're rapidly getting better at teaching Haskell. It'll be good soon.
Link for "Scrape your typeclasses" : http://www.haskellforall.com/2012/05/scrap-your-type-classes.html 
Didn't really RTFG, but I think Monoid is the biggest argument in favor of the so-called "newtype hack" as an elegant solution, because we happen to have names for the concepts Sum and Product. If the "hack" is the need to wrap/unwrap explicitly, then maybe the new `coerce` has that solved.
&gt; You're passing the Monoid dictionary explicitly at call site The type class instance to data type ("dictionary") transformation is trivial. Implicit parameters (with errors on ambiguity) are less easy to do, but both GHC Haskell and Scala do them. With these two things, you get the nice syntax of type classes without the highlander assumption around type class instances. BUT, you also lose the advantages. You get most of these back if you have type-level equality tests for your dictionaries, but we don't usually see that. (I know OCaml modules are supposed to help here, but I'm not quite sure how.)
You both misspelled "scrap" :)
Why does a tutorial explaining the basic structure and syntax need to be Linux specific?
Sorry if I was vague. Just tutorials on haskell
Sidebar. 
Saying “`Int` is a `Monoid`” is not accurate per the definition of monoids. There are multiple law-abiding definitions of `Monoid` for `Int` (`Sum` vs. `Product`), just as there are multiple law-abiding definitions of `Applicative` for `[]` (list vs. `ZipList`). I’m arguing that the `newtype` hack is a hack because it’s working around the inability to parameterise typeclasses by functions. We want to be able to say “`Int` and `(+)` form a monoid”, but we can’t, so we introduce a nominal type `Sum` which represents our function, and parameterise the typeclass by *that*.
I’m saying that the ability to pass functions as type parameters is what gives C++ the ability to express the definition of a monoid (or another algebraic structure) which Haskell lacks. So we should consider the implications of adding this ability to Haskell. This is also related to Haskell’s lack of a module system in the ML sense. [My reply](http://www.reddit.com/r/haskell/comments/2mlovg/monoids_without_newtype_hacks/cm5iudo) to /u/ocharles has more detail.
Google is your friend. 
Nice, Chris! Thanks.
&gt; Haskell is just awful at graphics. I'll leave it at that. If graphics are really important to you, learn something like Sage. Haskell is just horrible at this. I'm working on fixing that part now. =) * http://hackage.haskell.org/package/gl * http://github.com/ekmett/quine Polarina's sdl2 bindings are also pretty great to use.
This reminds me of a tangentially related question I had earlier: when the compiler implements optimizations for certain patterns like folds, is it analyzing the code to spot those patterns or actually looking for the function names?
Not sure but my understanding was each operation has epsilon error so that would be 2 epsilon worst case?
As much as I hate Java and the JVM, I would be a huge fan of having JVM as a compilation target just because it would help people get off of that ecosystem and it would help kill scala.
I just ran into a technique in some code I was working on, and it made me think about this post from a month ago... Existential types cover lots of use cases; consider data System = forall s. MkSystem { init :: IO s, update :: s -&gt; IO s, shutdown :: s -&gt; IO () } But how do you store one of these between init/shutdown? You have to package it up in *another* existential: data RunningSystem = forall s. MkRunningSystem { state :: s, init :: IO s, update :: s -&gt; IO s, shutdown :: s -&gt; IO () } It's awkward, and requires a strange dance between init and shutdown, or some reference with a Maybe value, or whatever. Back in C-land, we'd write it like this: struct System { void* (*init)(); void (*update)(void *); // returned state implicit via mutation void (*shutdown)(void *); }; // example vector&lt;System&gt; systems; vector&lt;void*&gt; states; void startup() { for(auto system : systems) { states.push_back(system.init()); } } (Yes, I know I'm mixing C and C++ idioms, I just want to get the point across) We'd rely on the systems to cast void* to their proper type. Everyone knew this pattern, and it's totally safe, but the type system never could express it.
Let me try to rephrase int_index's response (which is pretty much what I would have said). The point of type classes isn't simply that you dispatch on a type. It's that the type that you are dispatching on is inferred from the type the method is used at, so it doesn't need to be (indeed, can't be) selected explicitly. Your C++ program selects the type explicitly on lines 51 and 57. So, at that point, what's the advantage of wrapping up the function into a type parameter rather than simply passing it on the value level? Then, you can translate the program back into Haskell as data Monoid a = Monoid { mempty :: a, mappend :: a -&gt; a -&gt; a } addition = Monoid { mempty = 0, mappend = (+) } sum xs = mconcat addition xs What expressiveness do you lose this way compared to your C++ program?
See also [MemoTrie](https://hackage.haskell.org/package/MemoTrie)
Unconstructive invective doesn't belong in this community. Language like this does not further meaningful discourse, and only serves to alienate people.
For 'graphics', have you seen [Chart](http://hackage.haskell.org/package/Chart)? IIRC it's pretty damn excellent.
The answer to this question largely depends on what recursion patterns you're willing to seek. Behold: https://www.haskell.org/haskellwiki/Zygohistomorphic_prepromorphisms Additionally, if you don't consider the use of `fix` to be explicit recursion, that alone gets you everything. I'd consider that cheating with formalities, however.
I suspect the votes would be very much different if you didn't label an elegant, carefully thought thru and battle-tested solution a hack. This made you look careless and quite arrogant. Otherwise __it's an interesting read__ and it's very nice to see the spread of the functional programming roots into other languages, so I still upvoted. However my instincts suggested otherwise for the reasons I already mentioned.
I thought “`newtype` hack” was a well known term. I certainly didn’t invent it. But thanks for being considerate. I really was just trying to point out that the ability to parameterise typeclasses by functions would let you avoid `newtype` wrapping, with an example in C++.
Thanks; must've picked up nullify from Rails and thought it was standard SQL.
None. The C++ program is merely an example of how passing functions as typeclass parameters would let you avoid `newtype` wrapping and directly express the mathematical definitions of algebraic structures. I presume that in Haskell the instance selection would remain implicit!
&gt; &gt; Agreed. But you should point out that there are some very good ones, like LYAH. &gt; Yes for very baisic stuff. Then it kind of ends. For Java there are online courses for how to make anything. Haskell has the absolute baisics. OK, I'll agree with you there. Between basics and advanced (i.e. academic papers), Haskell tutorials are a bit lacking. As for your error message example, I think GHC is doing a wonderful job! Sure, the message is long, but it *starts off* by saying the exact same thing as your Python error message! Python says on the 6th line of its error message: TypeError: zeta() missing 1 required positional argument: 'b' GHC says on its line 5: Probable cause: ‘beta’ is applied to too few arguments The first part of the error make for even clearer reading than Python's errors, if you ask me: testahaskell.hs:2:23: Couldn't match type ‘Integer -&gt; Bool’ with ‘Bool’ Expected type: Integer -&gt; Bool Actual type: Integer -&gt; Integer -&gt; Bool In the first argument of ‘filter’, namely ‘beta’ In the expression: filter beta [a .. b] GHC in other words is telling you: *"Something's wrong on line 2. I expected a function of type Integer -&gt; Bool, but got one of type Integer -&gt; Integer -&gt; Bool. This function was the first argument of filter, namely beta, and I'm talking about the expression filter beta [a .. b]."* How can it possibly make itself clearer? It then goes on to reason likewise about zeta.
Could you please write your solution in pseudo-Haskell? If you pass `mappend` and `mempty` as typeclass parameters, what methods do you have?
Absolutely. I envision a typeclass where just `mappend` is a parameter. class Monoid (a :: *) (mappend :: a -&gt; a -&gt; a) where mempty :: a Then you have some instances: instance Monoid Int (+) where mempty = 0 Or more general instances if you have more overloading: instance (Num a) =&gt; Monoid a (+) where mempty = 0 And then, as in the C++ example: mconcat :: (Monoid m mappend) =&gt; [m] -&gt; m mconcat = foldr mappend mempty 
List functions are defined in a special way using streams in a non recursive way, which allows GHC to optimize them. It doesn't look for function names like foldr, but it does look for other function names afaik, specfically to/from stream conversion functions. For more info the keywords you're looking for are "haskell stream fusion".
I know that a monoid is defined by a set and on operation, but I just don't quite get the motivation for your work. Either way you have to choose the operation to work under, so it's not like your approach is saving in that respect. While with `newtype` you have to `newtype` all the elements, combinators like `ala` mostly get rid of that noise.
See also [Memo functions, polytypically!](http://www.cs.ox.ac.uk/ralf.hinze/publications/WGP00b.ps.gz)
Hmm, how are Agda and Idris "Haskell-like" languages in the same vein that Standard ML and OCaml are "ML-like"? The latter two are both strict, impure, higher-order languages with largely the same module system. On the other hand, Haskell is non-strict, Idris is strict with laziness annotations, Agda is neither and both, Idris and Haskell have typeclasses whereas Agda has instance arguments, Idris has tactics, Agda does not, and tactics don't even make sense for Haskell, and so on.
So how would you write sum and product with an implicit selection? sum, product :: [Integer] -&gt; Integer sum xs = mconcat xs product xs = mconcat xs How is that supposed to work?
So, what when I want to distribute my haskell package to somebody not in the Haskell community? Say I have created an executable which user have to run. Should I package every single dependency up as a package for my distro? This seems to be what some people who have haskell packages in AUR do, but due to the huge amount of dependencies haskell projects often have, something always seem to be broken.
&gt; The answer to this question largely depends on what recursion patterns you're willing to seek. Zygohistomorphic prepromrphisms don't require any more recursion primitives than refix, really. See [recursion-schems](https://hackage.haskell.org/package/recursion-schemes-4.1/docs/Data-Functor-Foldable.html#v:zygoHistoPrepro) for proof by construction. &gt; use of `fix` [...] cheating with formalities In Haskell, you have to do some work to get back polymorphic recursion if you write stuff using `fix`. But, besides that I would generallry consider a fix-point operator more fundamental than unrestricted recursive references, so I wouldn't consider it cheating at all.
My biggest fp peeve is peeved: &gt; In Haskell, it is idiomatic to write as much of your code in higher-order functions like *folds*, maps, and *unfolds* as possible. Using folds and unfolds is *nearly always* worse than explicit recursion with pattern matching. An (un)fold is a primitive (co)recursor, and so they have no useful equations: to reason about one, you pretty much have to set up a full (co)inductive argument. As a result, an (un)fold is no easier to reason about than explicit recursion -- in fact, it's the same thing, only with a pointless detour through higher-order functions thrown in. The biggest of all the wins of functional programming is that it lets us write first-order functions, and it's just perverse to give that up for no good reason. Conversely, maps and reductions are nearly always superior to explicit recursion, precisely because they have equations that can let you shortcut inductive arguments. 
Facebook Galois
Pretty much irrational? They aren't even algebraic!
How could I write a signature for just this part? do micros &lt;- state $ ByteString.splitAt 8 days &lt;- state $ ByteString.splitAt 4 months &lt;- get return (micros, days, months) The many parameters to MonadState get confusing
If you compile your Haskell executable statically (which I believe is still the default), you can distribute it without distributing any purely Haskell dependencies. FFI-imported libraries can be another matter, of course.
&gt; Facebook That's a bold faced lie ;-), Standard ~~Charted~~ Chartered maintains the largest Haskell code base in the world, whereas Facebook does what exactly in Haskell? If they are using Haskell it's absolutely miniscule compared to their C++/Hack code base. Heck, OCaml usage at Facebook probably dwarfs that of Haskell. Saying that, with Marlow and O'Sullivan working at Facebook, things *may* change over time, but to say that Facebook uses Haskell in the same capacity that, say, Twitter uses Scala, is stretching the truth more than a little bit. Prove me wrong if you can, would be shocked if Facebook had become a Haskell shop overnight...
C++ has no type inference to leverage, so you don't have type classes. Explicit dictionary passing is exactly analogous to this, and you don't need "newtype hacks" for that either.
How is Haskell associated with Facebook? I know Simon Marlow works there, but has Facebook ever said they have major processes written in Haskell? 
I'm not saying it's not "miniscule" (because I expect it is!) but I went to a talk on [Haxl](https://github.com/facebook/Haxl) recently and apparently they're rolling that out slowly, replacing their original DSL.
https://code.facebook.com/posts/302060973291128/open-sourcing-haxl-a-library-for-haskell/
https://code.facebook.com/posts/302060973291128/open-sourcing-haxl-a-library-for-haskell/
&gt; If they are using Haskell it's absolutely miniscule compared to their C++/Hack code base. Are you at Standard Charted? I'm pretty sure last time I spoke to Lennart he said that the Haskell code was barely a drop in the ocean, so the same argument should apply to them - if I got my facts right there.
It's true. They also don't use haskell for absolutely everything like Jane Street uses OCaml, but it is a sizable chunk of their trading infrastructure. I wish SC would contribute to open source like JS does, I know a lot of people who would kill for a strict-by-default Haskell compiler with good C++ interop and the rest of the bells and whistles.
/u/bos also works at Facebook
There are several optimizations involved, but the RULES system does match on patterns by name. Various frameworks rely on functions being expressed in terms of some primitive operations. Sometimes it's efficient enough to define functions that way and rely on inlining, but otherwise RULES are used to replace a function before the rest of the optimization framework does its thing, and then turn it back to the direct implementation if it wasn't optimized away. I think foldr/build is almost entirely done with RULES. Stream fusion uses rules a bit to expose stream implementations of list functions, but also relies heavily on other optimizations. Streams are defined with non-recursive functions, and operations on streams transform those functions. Being non-recursive, lots of general purpose optimizations (inlining, case of case/constructor, etc) are effective at combining and simplifying stages of stream operation.
But with fold, you have equations like: foldr o z . map f =.= foldr (o . f) z How do you have that with explicit recursion? Likewise, unfold: map f . unfoldr g z =.= unfoldr (fmap (first f) . g) z These are nice shortcuts to have, and they're faster too. 
Problem is that he defined mempty in function of mappend, it's not guaranteed that you have different values for mempty for different monoid instances (e.g. the First and Last monoids from Data.Monoid both have Nothing as the mempty value). Edit: not enough sleep, I shouldn't have posted this
You might be the exceptional case :) We've interviewed people from all over the world, it's just that the bar is higher when we have to do more work to get you to Amsterdam.
Met a bunch of Silk folk at ZuriHac this year - fantastic team to be working with! Good luck with your search :)
Thanks Oliver!
not really, theres TOOO many businesses using haskell for that sort of centralized dynamic to make sense. 
wat? there's TOOO many businesses using Haskell to, you know, name any of them?
I don't think that's an entirely accurate representation of what's happening. The runtime does contain much of the functionality of what GHC offers, but it's generally a lot simpler, since the JavaScript environment is single threaded. For example we were able to fix some bugs in the [STM implementation](https://github.com/ghcjs/shims/blob/master/src/stm.js) last year before GHC. The GC you mention is not really a garbage collector, since it only scans the heap, marking everything that's reachable. The implementation does sit in a somewhat inappropriately named file (`src/gc.js` in shims). The heap scanner is used to collect the data for heap profiles (profiling support and stack traces were added in a GSoC project last summer), start finalizers and notify the user about deadlocks (thread blocked indefinitely in an ... operation). Additionally it does some cleanup tasks that can in some situations reduce the amount of used memory significantly. But it's not necessary to run the heap scanner all the time. If you have debugged a program and are confident that you don't have deadlocks, profiling tells you that you can do without the additional cleanup and you don't need finalizers, you can disable it altogether for production and the program will continue to work. There is also quite a bit of heavy weight code in the packages, for example the `base` package is included more or less as is, with only a few modifications where POSIX calls are replaced by `node.js` calls. This makes it possible to compile many Haskell programs directly to JS, for example a `tasty` or `test-framework` testsuite can be compiled without changes, and run with node.js, even if the test suite needs to read or write files, start processes etc. Cabal `Setup.hs` scripts can also be compiled to JS and run with node.js, which in fact is what happens when you install a `setup-type: custom` package with GHCJS. For me as a compiler dev this is all great, since it gives me immediate access to a vast amount of code to battle-test the compiler: I can just pull packages from hackage, feed them to GHCJS and run`cabal test` on them. For deployment it not yet optimal, since it does add a bit of dead weight to the code. You can get rid of the node.js-specific JS parts of the code by linking with the `-DGHCJS_BROWSER` option, but you still get the IO buffering/encoding layers from `base`. I'm reluctant to fork packages as central as this, so the plan (and the main reason i joined the haskell core libraries committee) is to modularize the base package ([Split base proposal](https://ghc.haskell.org/trac/ghc/wiki/SplitBase) ) instead. I think this can probably ship with GHC 7.12, but GHCJS might include it earlier, since it uses a mostly independent set of libraries. All in all, the code size for the RTS isn't all that big compared to many popular JS libraries, and it's constant. That's why I've been focussing more on improving the optimizer recently, making it easier to add more powerful rewrite rules without breaking things.
https://www.haskell.org/haskellwiki/Haskell_in_industry
interesting, thx
the wiki is down, but https://www.haskell.org/haskellwiki/Haskell_in_industry are some. Not all those companies are public about their secret sauces :) 
What about fpcomplete? Also seen: Alpha Heavy -- don't know much about these guys. Chucklefish -- the guys who wrote Starbound and Wanderlust Rebirth are writing their next game in Haskell using FRP.
thx :)
&gt; Haskell is just awful at graphics. I'll leave it at that. Leaving it at that doesn't help anybody. Can you be more specific? I believe haskell has great graphics support, including: Low level rendering libraries: [opengl](http://hackage.haskell.org/package/OpenGL) [sdl](https://hackage.haskell.org/package/SDL) [cairo](http://hackage.haskell.org/package/cairo) Higher level drawing APIs: [diagrams](https://hackage.haskell.org/package/diagrams) [gloss](http://hackage.haskell.org/package/gloss) and various application specific drawing libraries like [Chart](http://hackage.haskell.org/package/Chart) (disclaimer - I maintain this). 
I am not at SC. Facebook is arguably more of an OCaml shop than a Haskell one at this point; case in point, they just launched [Flow](https://github.com/facebook/flow) written in OCaml. I wonder if Haskell were not lazy by default, would its adoption be far greater than it currently is in the enterprise? Will never know the answer to that question, but it is interesting to see OCaml, a lesser language in terms of power/functionality, taken up in place of Haskell. 
&gt; You might be the exceptional case :) We're talking about programming though, an exception is usually bad :P
I am interested, but I am merely a university student and I don't believe I'm qualified.
You should tweet about the outage, and change your cloudflare 521 page to something informative.
They are looking for the exception case with a view to catching it to be fair :-)
Do you have or will have any opportunity for new grads? I would love to work in Haskell instead of Python/Java/Javascript.
So Silk has been around for a while and the biggest question I have is do you currently make a profit, and if so how do you make your money?
Silk is a growing company and we're looking for good people mostly all the time. If you're a bright and motivated developer with a broad interest into functional programming we definitely encourage you to apply for a job at Silk. We can't promise a fulltime Haskell job for less experienced programmers initially though. We occasionally hire new grads as frontend developers with an FP interest and slowly introduce them to our Haskell backend. This seems to work well. But our current focus, hence this post, is an experienced Haskell developer to support us working on larger more challenging projects on the backend. People with real world engineering and software design experience from who we can learn a lot as well. But there's no reason to assume a new grad can't be that person of course.
Had a phone call/conversation with Salar back in August. Can confirm, Silk sounds like a team of genuinely nice guys and a good opportunity. Now if I only were willing to move to Amsterdam.. :)
&gt; OCaml, a lesser language in terms of power/functionality Debatable, modules are extremely powerful. Of course power is a difficult thing to quantify. The module system allows most of Haskell's power (and some power Haskell doesn't have), but it is very verbose so that power is much less used than in Haskell. For functionality multicore is an obvious place where Haskell outshines OCaml, but then OCaml's JS compilation is much better than Haskell's, which is important to Facebook.
Being a recent grad from a masters course, as well as having one year (non-Haskell) web dev experience, this is something that definitely interested me. Who should I get in touch with?
Great, thanks.
How experienced are we talking? I have read many a tutorial and done some light programming in Haskell but not full-featured application development. I have done plenty of functional programming in Python and Javascript though... Would love the opportunity to live in the EU but it sounds like that is a point of contention. I have everything else in spades I feel though.
Nope, we currently don't charge for usage and therefor are not making any profit. :) We have some very supportive investors that allow us to spend effort on product improvement and market growth instead of short term revenue right now. For now our focus is solely on building an awesome platform for structured data.
It's pretty similar language wise, somewhat similar library wise and totally different infrastructure wise. 
Great article. Agree so much with the Lack of Purpose problem. See http://www.reddit.com/r/haskell/comments/2lgjo9/women_in_haskell/clvhyd2 I think we need some Haskell tutorials from the point of view of doing stats, like they have with R. Or some other domain.
haxl
I'm not looking for a job. Just wanted to say that I just watched your 80sec video and *love* the idea. I wish you folks all the best!
There is nothing like Jane Street for Haskell, but Microsoft should be recognized and lauded for having sponsored a lot of ghc development. 
FP Complete!
Probably true, but the have million lines of Haskell, so I've heard, so that a lot as far as Haskell goes.
I did explain the convenience of not having to be specific in your intent WRT code but she still thought it was crazy.
By infrastructure do you mean the compiler toolchain? Is there anything you can tell us about the runtime system of Mu?
&gt; OCaml's JS compilation is much better than Haskell's Interesting. How did this come about? Is there something about OCaml that makes it easier to compile to JS? More work been put into it? Something else?
If Alpha Heavy counts as active, I suppose Wellposed counts too :-)
Compiler, IDE, profiling, documentation, FFI, ...
And any interesting info about the runtime system to let on ... ? :)
I think that there has probaby been more work on it. I also think that js_of_ocaml benefits from using OCaml's bytecode as its input. OCaml bytecode is basically a simple lisp which map's quite naturally to efficient JavaScript. Working from the bytecode also means OCaml libraries can be used in JavaScript without having to recompile them, so no messing around with build systems.
It doesn’t exactly need a lot of changes, and this is only the tip of the iceberg. When I left Facebook shortly after we open-sourced this, the Haxl core library constituted only about 10% of the Haskell code binding Haxl to Sigma (the write-time spam filtering service) and all of the data sources (TAO and a dozen others). That doesn’t include the ~60,000 lines of Haskell code autotranslated from the FXL language they’re migrating away from.
&gt; What are these ‘foo’ and ‘bar’ functions? this naming scheme is probably one of my biggest pet peeves with programmers, computer scientists, and software engineers. they are nothing but a common trope that serve no purpose other than to alienate beginners. i refuse to use them. as a person with a master's in math and some programming background but with only one formal class in programming, i was confused as hell in my first assembler/microcontroller class. the professor just used foo and bar all over the place and it took me a while to realize that these are some special instructions or registers but bullshit variable and function names that aren't helpful whatsoever. imagine using these names in a calculus course with no explanation at all. it doesn't and shouldn't happen. even most calculus teachers are quite explicit about the conventions and use of function and variable names.
x and y?
Agree that learning by example/practice is best. Difficult abstract concepts, for me, don't really sink in until I've used them over and over in a practical context. Monads I'm looking at you here. And then when new concepts are based on others that I only grasp in an abstract sense, things get confusing fast. Haskell is a language where you must practice and master each concept before moving on to the next one.
I think tmnt9001 is pointing out how maths has its own conventions about naming arbitrary variables that beginners just have to pick up on (n,m are probably integers. x, y, z are probably reals). Sometimes conventions even down to the font. For example, in some contexts \mathfrak{} is means "this is a Lie Group." 
Registration fees will be minimal, we just haven't worked out how minimal. We can't post a schedule until we receive proposals for presentations and then accept some! In the meantime we can list, as we have now, our invited talks.
*[Functional Programming with Structured Graphs](https://www.cs.utexas.edu/~wcook/Drafts/2012/graphs.pdf)* may be relevant. I only had time to skim the article, apologies if it's not.
I'm not a big Haskeller, I've only done some tinkering. Here's what I see, some of which are just questions that came in my mind and may not be an issue at all. apply :: Stack -&gt; Atom apply (op:xs) = case op of ... This code is very repetitive. In every case you're either applying one or two arguments to a function; and the arguments always come from the same place. Seems like there should be a way to reduce the duplication. push :: IORef Stack -&gt; Atom -&gt; IO () push s a = do modifyIORef s (a:) This one is a question on my part. Why are you using IORefs instead of just passing a list around directly? I'm guessing it's because you want to be able to use do notation, but then why not the state monad? I've done something like this once but I used State because I knew it would work (and wanted to play with a monad), not necessarily because it was the best choice. numberify :: [a] -&gt; [(Int, a)] numberify = go 1 where go _ [] = [] go n (x:xs) = (n,x) : go (n+1) xs Now this one I know how to help on! This is perfect for the [zip](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-List.html#g:17) function. It looks like this: zip :: [a] -&gt; [b] -&gt; [(a, b)] So your first list is just incrementing numbers and the second is your original list. That means you should be able to do something like: numberify = zip [1..] and it will do exactly what you want. push :: IORef Stack -&gt; Atom -&gt; IO () push s a = do modifyIORef s (a:) when (isOperator a) $ modifyIORef s evalStack I see what you're doing here (always push, sometimes evaluate) but it took me a couple of reads to be sure because of the when and it's formatting (it looks quite a bit like a where). Is there a clearer way to express/format this? Honestly I'm not sure I've even used 'when' so this may be one of those 'I just had to search my memory and it's not 2nd nature yet' kind of things for me.
"Real" Real numbers are computationally weird structures. In particular, much of mathematics tends to assume you can uniquely identify some number `x`. But to do so we'd be able to determine whether or not it is true that `x` and `y` are equal. Assume we have a function `eq` which computes that eq :: RealReal -&gt; RealReal -&gt; Bool We might imagine the operation works by comparing their binary expansion, but with real numbers there's no reason to ever be certain that the binary expansion is "done". In particular, if you imagine they're represented by lazy streams of these digits then you can see that `eq x y` will, should `x` and `y` differ, return `False` in finite time but will, should `x` and `y` be the same, never terminate—after all, they always *might* differ in the next digit, or the next, or the next... So, this means that `RealReal` equality is semi-decidable which brings all kinds of issues forward. You end up losing non-continuous functions like `signum` because you literally cannot determine whether or not the `x` in `signum x` is greater than or less than 0 in finite time—it might *be* zero. Interestingly, this notion of continuity and smoothness should work fine for many sorts of analysis since delta/epsilon proofs are exactly the kind of reasoning which `RealReal` supports. The other way to go about it is in a dependently typed language like Coq you can have a theory of reals which is non-computable but embodies the kind of representation and non-computational nature of the reals as we know them. This is obviously significantly different from what's available in Haskell, though.
This is exactly what the newtype hack does behind the scenes w.r.t explicitly passing a different dictionary.
True, you did. In fairness, you've told me quite a bit about Python and Clojure and Java and its magical bean factories. Between the formal logic and the generative syntax, though, I was probably never going to find it not crazy.
whoa. is there a haskell / scala feud I didn't know about? (am new to haskell) 
&gt; Strangely this type checks &gt; &gt; exp = ... :: Exp String &gt; &gt; rhs = "foo" :: RHS () &gt; st = Assign rhs exp The types `RHS String` and `RHS ()` both expand to `String`, so `exp` is a perfectly good value of type `RHS ()`. `type` is meant to be used to give shorter names to long types, not to annotate a concrete representation like `String` with a phantom type. &gt; However, by changing RHS into a data def instead of a type def it fails to compile. As it should. Unlike `type`, using `data` is an appropriate way of annotating a concrete representation with a phantom type. `newtype` would be even better.
&gt; The idea is that assignment should be type checked by matching the typ parameter, but once it is, it should be forgotten so that I can combine statements w/ different types. Yes, phantom type parameters in type synonyms don't work, because the compiler will expand the type synonym first, and only then try to do type checking (when it will try to unify the type variable `typ` with something). So the fact that you intended `vec` to be `RHS (Pair Int)` gets "lost"; it is really just a `String`, and any `RHS a` will also match `String`. If you spin up GHCI you can see what has happened: Thing&gt; :t (vec :=) (vec :=) :: Exp typ -&gt; Statement typ You may find the comments starting with "Note [Type synonyms and canonicalization]" to be of interest: https://github.com/ghc/ghc/blob/master/compiler/typecheck/TcCanonical.lhs 
Wow, what an amazing response! &gt; This code is very repetitive as far as apply is concerned, there is most likely a far more efficient (repetition-wise) and I went through many iterations to figure out when and where the condensing of the stack should be done. I'll go back and see what I can do because I think you are right Again I struggled with how to store and mutate the stack, and `IORef`s just happened to be the easiest way I came about implementing that. I think I could redesign it just using pure functions from `Stack -&gt; Stack`. To put it shortly, there is no good reason I used `IORef` other than laziness. As for your last point, `push` was much, much longer, pattern matching on arguments and such before I realized that much of it could be refined and optimized. I do have a tendency to reduce to the point of illegibility and so having feedback about the readability of my code helps to write better code. but for me, `when` suits my needs most clearly. I hardly think to use when when developing. Instead I come to write some code of the form: if foo then bar else return () which is precisely `when`'s function. when foo bar recognizing the basic structure of `when` and re-factoring.
Our Haskell and OCaml code bases at Facebook are of roughly comparable sizes: each language has a fairly small but very active and capable team using it. That said, I am fairly sure we have fewer people and less code in FP languages than StanChar.
Are you talking about say the problems of representing irrational number. Like I could understand why it would be hard to say Pi == Pi, if you're representing them as a binary number, but say Pi represented a function of some kind, then it would be easier to assert Pi === Pi (if Pi is an object). But just generally, why is it so hard for computers to get reals with decimals precisely? (Though it would still be hard to assert two function equal the same thing again on second thought :), I only mean if those functions are exactly the same, then they of course are equivalent -- though this wouldn't necessarily only apply to irrationals). Any link to this on Coq. Someone brought up Coq while I was asking this earlier, I'd be very interested to read how Coq represents real numbers.
It is certainty doable, Haskell even has libraries for it. Howerver it is quite slow and has many other issues. http://hackage.haskell.org/package/numbers
Really nice article. I agree we tend to overlook these "scaffolding" type issues in lots of our learning materials -- this is an important reminder. I think we actually get similar, if less striking, problems with developers coming from a windows/.NET ecosystem or the like, who are used to IDEs, or with developers from a background in interpreted languages where there's no compiler in the mix at all, etc.
It's more like this. x : Real y : Real If I were to just give you those two variables and assert their types to you you would have to play with a lot of fire to do math on them because they "might" be equal and unless you have a pre-set tolerance for how close is "close enough" you could hit infinite loops all over. You might be fine of course, you just don't know. Anything which accidentally produces an infinite representation (in whatever representation you have) will cause danger. To see this playing out in Coq take a look at the standard lib https://coq.inria.fr/distrib/V8.3pl1/stdlib/Coq.Reals.Reals.html In Rdefinitions you see that we... actually have no representation of the reals at all. They're just abstract things which have abstract operations like R0, R1, Rplus/Rmult. In Raxioms we start to see assertions about how these abstract operations behave: we assert that addition is commutative, for instance. But there's no proof—we couldn't prove it if we wanted to. It's axiomatic alone. We can inject naturals into the reals with INR, but really all we're doing is noting that as soon as you have R0, R1 and Rplus you can embed O and S from nat. And so that's all there is to it in Coq—a totally abstract definition useful only for properties, not computation.
&gt; the professor just used foo and bar all over the place and it took me a while to realize that these are some special instructions or registers but bullshit variable and function names that aren't helpful whatsoever Why didn't you just ask what they meant? Or google it? If I [google 'foo',](https://www.google.com/search?q=foo) the [first link](http://en.wikipedia.org/wiki/Foobar) is wikipedia, which explains 'foo' well.
In case people don't know, that's Bryan O'Sullivan, one of the authors of Real World Haskell.
Cute when a toddler says it. It's much different when an adult talks like one.
Nice! Instead of rolling your own Show instantiations, it might conceptually make more sense to create your own PrettyPrint typeclass. Also, look into using Text and the OverloadedStrings language extension. Personally I like to use Text if only out of principle (when, for example, performance is not a big concern). Lastly, you might have a good use case for the "interact" function, which is clever and fun to use. :)
Still no visual studio support for it though.
As someone in the same position, this is an honest question. I really don't get it. I only ever studied Haskell, so…
Added my comment. Short version: there's a book that really is written to work for non-programmers. It's the Haskell Wikibook and people should recognize that it's possibly the best Haskell intro in existence. https://en.wikibooks.org/wiki/Haskell
Lets say you have a large list. In most other languages, you can change one element of the list to something else in O(1) time (which means the time doesn't get longer with the size of the list). In an immutable language, you have to recopy the whole list which takes progressively more time the bigger your list gets. It also takes more memory to do this. You generally are paying a performance penalty for immutability.
Update: as https://status.haskell.org/ now describes: "Many services are now restored. The wiki will remain slow until we have had time to configure a more performant connection to mysql. At this time issues other then slowness with the website should be reported." A _huge_ thank you to the admin team who dropped other plans and jumped in to accelerate this jump off the old server. You all rock!
I'm quite confused.. Maybe I ought to be though. I'm sure there are limits to what a programming language can compute (and I'm not just talking about the halting problem), I imagine the way numbers are represented have a lot to do with it -- and the limits of the hardware. Maybe another good question to ask is this, and this would apply to the other comment in this thread. If haskell, or any programming language for that matter, told me two reals (e.g. two arbitrarily large floats) were equal -- how could I check and know if it was correct or not. 
The last bit is what the original article was trying to make clear. You can't know that Bob, who has arbitrary, nonspecific vegetables, will be able to use your tomato soup recipe, but you know for sure that he can use your generic vegetable soup recipe. You also know that Alice can use either recipe, since she has told you that she has tomatoes, which can be an ingredient in either a generic veggie soup recipe or (obviously) a tomato soup recipe. Another example: if I need a function mapping integers to widgets, I can always use function that maps real numbers to widgets in its place. But the inverse is not true, because numbers that I end up passing in might not be in the set of integers.
Adding and removing elements from lists is always O(n).
In what reality?
Is "recursive" the correct terminology here? I thought "inductive" was.
I think I'm starting to understand what you're saying now. To assert that two reals are equal, it can only make sense if the *way* they were computed were equal -- more so than the representation themselves. So they're equal if they follow all the same rules (axiomatic) or not. 
&gt; Standard Chartered Sartre: Darn Tech, Dad
I'm not sure what "an immutable language" is, but typically in functional languages, immutable data structures are implemented as persistent data structures. This means that rather than recopying the whole list, you create a new data structure which often shares most of it's storage with the previous value. In many cases the tradeoffs result in implementations based on persistent data structures out performing implementations based on mutable data structures. For example: http://swannodette.github.io/2013/12/17/the-future-of-javascript-mvcs/ 
He means "lists" as opposed to arrays, vectors, etc. That is, "linked lists".
Thanks for the inside word. The opportunity to bring Haskell, or at the very least the FP paradigm, into Facebook is pretty huge in the long-term. Meanwhile Twitter is tanking, and Scala's not doing to great either (sad face, day job). Will be interesting to see where the computing landscape is at 3 to 5 years from now vis-a-vis C-derived langs vs. those of the JVM. Haskell's been evolving at an impressive clip and Microsoft is coming to Linux/OSX, advantage C...
interesting. why would they do that when they have F# ?
First that is explicit in my mind, while you said you wanted it to be selected implicitly. A different problem is that now your implementation is leaking into the type, I could alternatively write sum :: (Num m) =&gt; [m] -&gt; m sum = foldr (+) 0 and end up with a function of a different type even though both definitions ought to be exactly the same. Under the record based approach using `mconcat` is not visible in the type. Another difficulty with your way is that sometimes you want multiple monoid instances for the same type in the same function endpoints :: [Maybe a] -&gt; (Maybe a, Maybe b) -- current way endpoints xs = (getFirst (mconcat (map First xs)), getLast (mconcat (map Last xs))) -- alternatively when using a record instead of a type class endpoints xs = (mconcat Monoid.first xs, mconcat Monoid.last xs) -- with instance selection based on the function endpoints xs = ???
Serialisation of functions. Other than that, not much significant and interesting.
Insertion and removal from linked lists is O(1). For removal, the operations are: * Update the pointer of the previous element to point to the next element. * If doubly linked, update the back-pointer of the next element to point to the previous element. * If using manual heap allocation: free the memory of the now-removed element. For insertion, the operations are: * Update the pointer from the previous element to point to the new element. * If doubly linked, update the back-pointer from the next element to point to the new element. * Set the pointer of the new element to point to the next element. * If doubly linked, set the back-pointer of the new element to point to the previous element. Note that push/pop/etc. are special-cases of insertion and removal (insertion/removal at the beginning/end use a subset of these steps).
Haha, yes we do. :) You were heading to ICFP that week right?
&gt; You end up losing non-continuous functions like abs because you literally cannot determine whether or not the x in abs x is greater than or less than 0 in finite time—it might be zero. You mean `sign`. `abs` is continuous, but not differentiable. Some non-differentiable functions exist in most computable representations of reals (though not in all constructive versions of reals; see synthetic differential geometry). For instance, with the Cauchy sequence representation, you can simply apply `abs` pointwise.
Sure, immutable data structures can employ many tricks for better performance, and functional programs that employ immutable data structures do often have good enough performance. They can even beat other libraries that rely on mutable data structures (especially in arenas where performance doesn't matter like website interfaces).
...[and all the best Hackage libraries.](https://hackage.haskell.org/user/BryanOSullivan)
[This video](https://www.youtube.com/watch?v=TS1lpKBMkgg) and [this comment](https://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu?context=3) are all you need to know why you should avoid Scala. Also, who would ever want to write all that syntax. Scala developers hands must hurt like hell.
The experience we're looking for is software engineering experience more than Haskell experience. So if you think you can build complicated backend systems, and know enough Haskell to pick up the advanced stuff on the job, do apply. Regarding candidates from outside the EU: like I said above, this is not a deal breaker, it just raises the bar a bit.
I don't know, but Haskell development comes out of Microsoft Research, while F# is now a main .NET language. So if I had to guess, I'd say that Haskell is funded as a research language, with features moving over to F# and eventually C# when they have proved themselves.
Also, ghc was around long before F#. 
Not just functions, basically serialization of everything. And serialization preserves the graph structure of the serialized value. 
Yes, thank you. I think that math is usually worse than programming in that aspect. Because sometimes x and y are used to mean "nothing in particular", much like foo and bar; and other times they mean something specific like the width of a square, where a programmer would use a more meaningful name. 
&gt; And if said computation is deterministic, it will. Why do you want to mutate? Because it would be a lifeless, static system, and completely uninteresting. No. Like pure functions, mutation (more precisely: mutable variables) are just a model. As an alternative, you can model time-varying systems with functional reactive programming without ever mutating anything; in fact, that's how physicists have done it for centuries.
Static typing and adts are kick ass
To others: GHC was released before Windows 3.1 :)
I'm a big fan of Haskell but I will concede that sometimes you need mutable values for performance reasons. Using a mutable array is faster than creating a new list every time you need to modify entries.
Could be less of an issue now that the whole .NET stack is open source. Not that I'm trying to steer you away from Haskell ... :)
&gt; where a programmer would use a more meaningful name. Sometimes. i, j, and k are all common variable names, and depending on what you're writing, x, y, z, w, v, u, s, and t can be common too. Admittedly a lot of the work I do is as much maths as programming though :)
I'm not a Clojure programmer, but from the talks I've seen and the people I've met, the community is as friendly as the Haskell community. They seem to have a good cabal-like tool with leiningen, so I think in terms of ecosystem both are comparable. I think the biggest differences are on the language level: Haskell has static types, is lazy, and has (IMHO) a very clean syntax. Closure doesn't have types out of the box (though there is core.typed), is strict, and has lots of parentheses ;) Another factor might be libraries. Clojure has java interop, which gives you a huge amount of libraries. On the other hand, some things like STM are really only practical in Haskell. Personally I'd choose Haskell for the static typing alone, but that's the answer you're going to get on the Haskell subreddit, I guess :)
regarding side effects, other than the return code, anything your program achieves is a side effect. images on the screen? side effect. Output to a file? side effect. Data sent over a network? Side effect. Interaction with any output capable peripheral? So yeah... as for poorly controlled, well I make games. We make lots of bugs for sure, but some of us make computers do things nobody thought was possible.
Unfortunately I'm compelled by curiosity to "get the bottom" of Haskell. 
&gt; some things like STM are really only practical in Haskell Could you elaborate on this? Clojure has [support for STM](http://clojure.org/refs). Is there something missing from the Clojure implementation IYO?
Don't choose. Learn both. They're both incredibly good tools for some purposes and both will provide you with different insights. They're also relatively easy to pick up, at least on a basic level (learning basics of clojure is a 3 days of reading of blogs and tutorials, haskell took me a bit longer but not by much). Haskell tutorials and conversations are slightly more theoretic and use slightly more arcane nomenclature, due it's roots in academia, but once you get used to them it's not that bad. Haskell is more pure, but then you can avoid non-pure aspects in Clojure for most of the time. It's harder to shoot you in the foot with a statically typed language, but it's also harder to shoot anything ;) You will have to spend some time to get your program to even compile, but once it does it's harder to break it.
If you have data structures involved other than just a list, then it's not a list any more. It's something else.
I was always under the impression that without the `STM` monad, you could have arbitrary IO (in particular changing a mutable variable) in your transaction, which makes rolling back transactions impossible. IIRC this is why the C# version of STM was eventually discontinued. How does this work in Clojure? (Like I said, I'm not a Clojure programmer, so I could be totally wrong)
There was a good talk by Rich Hickey about Clojure STM. I'm sure you can find it on Google or YouTube and there's probably more than one. If I recall correctly the gist of it was that STM is not a singular thing -- there are many approaches. The talk was a good overview of what's available in Clojure and I think there was also some comparison to Haskell. IMHO it is worthwhile to learn both. If anything else you can borrow some ideas and see some wildly different approaches (like core.async which I found very weird at first but ultimately very useful)
You seem ideally suited for Scala and Clojure on the JVM.
Consider this: struct list { struct list *prev, *next; }; void list_add(struct list *where, struct list *what) ... void list_del(struct list *what) ... #define list_foreach(head, item) for(item=head-&gt;next; item!=head; item=item-&gt;next) Is this not a doubly linked list implementation? Now imagine I use it thus: struct request { struct list chronological_order; struct list by_port; ... request data ...; }; ... list_add(&amp;chronological_request_list, &amp;request-&gt;chronological_order); ... list_add(&amp;port_request_list, &amp;request-&gt;by_port); Is it no longer a linked list? Of course it is 2 linked lists. And now, given a pointer to a request, we can do O(1) delete of all the lists it is in: list_del(&amp;request-&gt;chronological_order); list_del(&amp;request-&gt;by_port); This is how mutable linked lists were originally designed to be used. Then came a bastardization of them as "containers", made even more entrenched by bad list libraries such as STL's std::list that confused people about what lists are even useful for in the first place. If you use lists correctly, as shown above, you get true O(1) add, delete, splitting of 2 lists, joining together 2 lists, and more. 
It just rolls back the STM controlled variables. Obviously it can't roll back I/O, but neither can Haskell. The difference is that in Haskell it's statically disallowed whereas in Clojure this relies on the sanity of the programmer.
why not throw f# in the mix? it satisfies all three of your criteria.
Everybody I know who learned Haskell as a first language has wondered this.
As a beginner I've found that I can often understand how a function works more quickly and clearly when I can see its structure with explicit recursion, though others may not share my experiences regarding this. I'm getting to the point where I'm comfortable reading code using folds for example, but for someone who's perhaps considering writing a tutorial for people new to the language I'd definitely recommend sticking to explicit recursion. I know this isn't a technical approach to the question, but I thought it's worth considering. Code readability is important, but it's a subjective topic.
I'm a beginner fresh to Haskell with a bit of Python experience, and I really liked that Fibonacci numbers solution, although I think I'll have to read it a couple of times to fully understand it.
I agree with that. It's all about the context. In math there are ambiguities all over and you come to expect them whereas in programming things are a bit more strict and accurate. When you mix up meaningful variable names with foo/bar newcomers don't really know what's happening. I witnessed that with a lot of friends that were learning programming for the first time(undergrad CS course).
Maybe something like this: * [Exact Real Arithmetic](https://www.haskell.org/haskellwiki/Exact_real_arithmetic) * http://www.dcs.ed.ac.uk/home/mhe/plume * http://math.andrej.com/2008/08/24/efficient-computation-with-dedekind-reals * http://www.doc.ic.ac.uk/teaching/distinguished-projects/2009/m.herrmann.pdf * https://karczmarczuk.users.greyc.fr/arpap/lazypi.pdf * https://hal.inria.fr/inria-00075792/PDF/RR-0760.pdf * https://hackage.haskell.org/package/numbers/docs/Data-Number-CReal.html * http://r6.ca/FewDigits
I agree. The worst problem with using metasyntactic variables like "foo" is that it exacerbates the "lack of purpose" problem. If I get to see a new concept in terms of "foos and bars" I will have to struggle to figure out how it might be useful in a real-world situation. If I see the same concept in terms of "employees and salaries" I immediately have a real-world model I can grasp at, test the concept on and reason around. Forcing the student to come up with a real-world model *at the same time* as they are trying to understand the concept in the first place is a great way to cause a mental traffic jam. Coming up with a real-world example is, to begin with, the responsibility of the teacher. If the teacher can't do it, then perhaps either the students are not ready for the subject, or the subject just isn't as useful as the teacher wants it to be.
Nowadays, I'm a Haskell programmer and enthusiast. But my first functional love was Clojure. Clojure is in some aspects like Python: it has a vast amount of tools, it's straightforward, easy, flexible and pragmatic. It will take you a long way; from a single file to a distributed cluster on AWS. In Clojure you will produce clean, maintainable and almost-pure code. And if you want to have some mutability lying around it won't be a problem. But this wasn't enough for me. Clojure triggered something inside me/ I loved pure code, it was just natural to reason about, without moving parts. Maybe difficult to write but trivial to test, use and refactor. But, completely pure code was becoming a nightmare to write as projects got bigger. For example I ended up with lots of extra arguments on my functions, explicit state, or often it could become difficult to reason about complex abstractions. And I wanted more. So I learnt Haskell. I learnt that Functor and Applicative give you pragmatic ways to handle a million different complex data structures and abstract data types without caring about their implementation. I learnt that Monad gives you rational ways to structure logic and the order of computations, giving you more power than in an imperative language I know ("programmable semicolons!"). I learnt that you can handle errors in pure and explicit ways. I discovered that almost everything can be composable; I can have a thousand computations that may fail, run them in parallel trivially and still catch all those errors in a single line while using the same operators I use to print text to the screen. I fell in love with currying and how easily things can work together if the language lets them. Also I learnt that concurrency can be a beautifully simple endeavour, that there are actually a lot of ways to do it and that it actually makes things faster without adding unnecessary complexity. I learnt how rich types can give structure, meaning and modularity to a piece of code (almost) for free. And all this in one language, with a great package manager, a lovely collection of well thought libraries, an industrial strength compiler, testing tools, profiling options, and a great community full of the smartest people I've ever met. I haven't touched Clojure since. 
Glad to hear that; if you felt that my explanation was inadequate, do let me know! :)
Haskell is more radical and has a steeper learning curve, while almost anyone with object-oriented imperative background can program OCaml and be productive. Not making full use of the language, but, productive. In multi-language environment that makes a greath difference.
But you have to learn what the convention is, which is the point I was trying to make. EDIT: But - just like in programming - you still have to learn what the convention is which is the point I was trying to make. 
I think I'm starting to get it now. If you need a Vegetable, I can give you a Tomato because a Tomato supports all 'operations' of Vegetables. If you need a Vegetable recipe, I can't give you a Tomato recipe because a Tomato recipe may use some 'operations' only a Tomato supports. If you need a Tomato recipe, I can give you a Vegetable recipe because a Vegetable recipe is guaranteed to use only the 'operations' that all vegetables, including Tomatoes, support. Ergo Vegetable recipe is a subtype of Tomato recipe. Neat!
pun intended
I'm surprised Chris Allen hasn't chimed in yet. He [wrote](http://bitemyapp.com/posts/2014-04-29-meditations-on-learning-haskell.html) about switching from Clojure to Haskell. It doesn't answer the question "Which shold I choose between Haskell and Clojure?", but does answer "Why did I switched from Clojure to Haskell?" which may be just as valuable. 
I have this long-term plan to make an interactive learning system with Haskell as my first teaching topic which will be aimed at someone who has no technical knowledge other than basic high school knowledge and common sense. I think that for reference, books are okay. A properly indexed hyperlinked system is better, but a standard book is better than nothing. For teaching, I feel that traditional books are lacking significantly and proper learning requires interactivity. Rather than a linear step-by-step start to finish book of pros lecturing you and then giving you a heap of exercises like most books, I want to make a book which is a graph of interdependent topics that can be broken down into one short description and a series of questions. That's the principle. Here's [a quick wireframe](http://chrisdone.com/example.png) (don't take it too literally, the example is not well thought through) I made the other day to illustrate. Every topic consists of: * A title * A description * A question and answer-validator generator * A set of dependencies Title and description are self-explanatory. They should give enough information for the learner to answer the questions below. The dependencies serve as "required knowledge". If the question requires knowledge that is not contained within either the topic description or in the dependencies, then you've failed to make a topic properly. There should be no static questions, but rather questions which are generated from a random seed and are coupled with code that can validate answers by means of quickcheck testing. I'm thinking something like formlets meets quickcheck. Nobody should ever (or not often) be asked the exact same question more than once, it should be an infinite list of slightly different questions. It should also not really look at the actual text the person writes, but test how the thing runs. I learned from tryhaskell that if you expect people to solve a problem in a very specific way, syntactically, they'll be annoyed and dislike the tool. With the QuickCheck-like approach, you can give evidence to the learner why their answer doesn't hold up. The learner will answer the questions until they feel that they understand the topic, and then they will hit “I know this”. That'll be recorded and now they can move onto other topics. Also, questions shouldn't be multi-choice. Multi-choice is indicative of a question which relies on knowledge that hasn't been taught properly. I didn't put it on the mockup, but you could also have reverse dependencies or sibling/cousin topics, so "What to learn next" to give a nice starting point to go forward. But you should always be able to go to an overview of a graph which shows the whole subject. You should be able to organically learn what you want when you want and track what you've learned. Once you've inserted a topic for all the parts of vanilla Haskell you can start covering domain topics. How to use Parsec to parse things, how to use hmatrix to work with matrices, the statistics package to work with distributions, diagrams to draw things, whatever. The good part is that you don't have to, as a user, go through the whole of Haskell from basic types to monads to type families and data kinds, just to figure out how to do one of the more domain-specific tasks. With explicit dependency tracking, you can open something up and say "I don't get this, I'll go through the required knowledge". It means you can approach learning Haskell as whatever you want to use it for. I think of this as "perspectives". Everyone has their own reason for learning a subject, in this case Haskell. Just want to learn the language, as a language geek? Feel free to consume all the Haskell language-specific knowledge. Just want to learn about Haskell's concurrency? Jump straight to that and only learn the required parts. Just want to try out laziness? Same thing. What Haskell knowledge is required to demonstrate laziness? Not much, actually. It's very fundamental. Anyway, the interesting problem to solve is the questions and validator part. But I haven't started on it yet. I have datalog ideas for extending this to other domains outside programming, but that's a distant future project.
&gt; after all, they always might differ in the next digit, It does not follow that they are different just because they differ in one digit 1.00... == 0.99...
You don't need a stack but do keep it and add numbers and some operators later. Notice that type Node = (Char, Maybe Node) as used in your code (i.e. you use `Maybe Node`) is isomorphic to the normal list. Nothing --&gt; [] Just (x, Nothing) --&gt; x:[] Just (x, Just (y, Nothing)) --&gt; x:y:[] -- and so on. The `:` notation will make it clearer when you take something off the stack (when pattern matching), and when you push something on the stack (in recursive calls). I would name `process` and `processChar` something like `interpret` and `step`. The two first definitions of `process` can be reduced to the second. I'm not sure why you return lists (i.e. Maybe Node) instead of just booleans (when you add numbers and operators, you will want to return Maybe Number).
I did! Total braino, thanks!
I know that I do not need a stack here, but this was a task from a lesson at my university (formal languages), and I wanted to implement the stack automat as it was discussed in the lesson. Thanks for your comment, nice to know that you can use list-pattern matching for types!
Thanks for your comment! I wanted to use a stack (see [here](https://www.reddit.com/r/haskell/comments/2mrklo/review_request_please_review_my_stack_based/cm6y1hd) ), though.
*Lie algebra
Yes, you can. There is some special support for the list and tuple syntax built into the language, but otherwise you can view `:` (for lists) or `( , )` (for tuples) as regular constructors (i.e. pattern matching works). If you use characters valid for operators, you can even have your own constructor that begins with `:` that can be used in infix form. E.g. data List a = Nil | a :&gt; List a
Perhaps the difference is that Clojure programmers are more sane than C# programmers ;) Seriously, perhaps there's more of a culture of not using many mutable variables (and IO in general) that makes this less of a problem in Clojure. In C# (and OO in general) every object function might mutate its state, which can't be rolled back in general.
If you're interested in testing—though I haven't read GOOS so, maybe I don't know what it means there—then you should check out `QuickCheck` and `simple-check` in Haskell and Clojure respectively. The second is a nice little clone of the first.
And you're paying a reasoning penalty for mutability. The standard wager is that if you do need mutability optimizations they probably can be isolated according to the standard "premature optimization is the root of all evil" rule. Profile where they're useful and then use IORef or STRef.
How would using this look like? Something like 1 :&gt; Nil -- results in [1] ?
F# isn't particularly opinionated - it's a hybrid of imperative OOP and functional programming like Scala. Haskel is extremely opinionated in comparison, as is Clojure. 
I love that static types in Haskell tell you so many things. For example, you can search by type on [Hoogle](https://www.haskell.org/hoogle/) and the results are generally useful. It's nice to first write the types of your top level expressions, then write the expressions themselves (type-directed programming). I say it's "nice" because the Haskell type system is more powerful and helpful than Java or C#. Try Haskell a bit, to see if you would like it. If you are wondering where to begin, alongside the many tutorials you can check [What I Wish I Knew When Learning Haskell](http://dev.stephendiehl.com/hask/).
The "rely on the sanity of the programmer" argument can also be used to argue that we don't need STM, because normal conditional variables and mutexes just "rely on the sanity of the programmer". It's a bad thing.
OK, here's a solution with a stack: match :: String -&gt; Bool match = step "" where step s ('(' : t) = step ('(' : s) t step ('(' : s) (')' : t) = step s t step s t = (s == "") &amp;&amp; (t == "") It also scales easily to multiple kinds of brackets: match :: String -&gt; Bool match = step "" where step s ('(' : t) = step ('(' : s) t step s ('[' : t) = step ('[' : s) t step ('(' : s) (')' : t) = step s t step ('[' : s) (']' : t) = step s t step s t = (s == "") &amp;&amp; (t == "") 
I first noticed the term "persistent data structure" in Clojure. In what sense are they persistent? To me "persistent" would mean stored on disk or database or something else longer lived than the running process. Where does this term come from? 
Sorry. I meant "monadic reactive" not "monadic imperative" . Changed The second part is a fact, since it is hardly mentioned as an alternative. I know only a single paper -referenced in the tutorial- and a few posts. There is no full implementation, nothing on hackage as far as I know. And the very reason for the "reactive" concept is to do things in a different way than blocking imperative solutions. monads seems to evoke something similar to blocking imperative expressions. Therefore .... But a monadic sequence hasn´t to be blocking. It is as declarative as any other functional expression. What the monad does depend on the monad instance. A monadic sequence can run backwards, it can be stopped, restarted, it can start in the middle of an expression that receives an event etc. 
I like the language. I played with f# in my c# days, before FP was the NextBigThing. At the time, it failed on community and tooling (VS - shudder) I did like the language, though. In the application I was writing, it was incidental, so it didn't really stick at the time. Methinks the student wasn't ready.
So you're saying there's away around creating a new list each time you need to modify it if you really need to? Haskell appeals to me on so many levels, but I just feel dirty doing that, even though I realize it's not likely to make a difference for anything I'm likely to ever work on. 
We make use of the monadic bind in reactive programming (in javascript though) quite a lot, but I'm not sure if this is the same bind. We have event streams `R a` which forms a monad. So your streams can switch on other streams and can be flattened: `join :: R (R a) -&gt; R a`. A Haskell version of this concept can be found [here](https://gist.github.com/sebastiaanvisser/9639321). This allows you to write code like: setupWindowTitle = do doc &lt;- currentCanvasDocument title &lt;- docTitle doc dirty &lt;- docDirtyState doc return (title ++ if dirty then " (with changes)" else "") So if the canvas switches to a new document, or the document title or dirty state changes the window title changes with it.
Any code that you'll like to share?
You can have standard mutable state inside of particular regions marked by the `ST`. There's nothing strange about that, it's just that standard mutable state is a subset of immutable FP. There are tricks to optimize it and it works great. http://jspha.com/posts/mutable_algorithms_in_immutable_languges_part_1/ http://jspha.com/posts/mutable_algorithms_in_immutable_languages_part_2/ http://jspha.com/posts/mutable_algorithms_in_immutable_languages_part_3/
I like this very much. I made a model of a small digital system using reactive banana and on one hand it was fun and simple, but on the other hand every time I add another component to the system my simulation time doubles. So, today I will try reimplementing the model with monadic reactive technology and see if it can cure the 2^N scaling issue. Edit: 2^N, not N^2.
Well it does give you a guarantee of no IO inside your transaction. In zero lines of code. That's a lot of expressiveness per line of code! 
So what you're saying is that if a programmer is able to not perform I/O in a transaction, then he must necessarily also be able to use mutexes &amp; condition variables instead of transactions? Not performing I/O in transactions is a *far* easier task than converting a program that uses transactions into an equivalent one that uses mutexes and condition variables. You see the difference between a feature that lets you write a new class of programs, and a feature that restricts you to a certain class of programs. In this respect STM is more like higher order functions: they let you write programs that you could not write before. It would be incorrect to say that if you can rely on the sanity of the programmer to use untyped higher order functions, then by the same argument you can rely on the sanity of the programmer to code without higher order functions at all. Sanity means that the programmer does *not* do certain stupid things (like performing I/O in a transaction), whereas with higher order functions or STM, having to do without them means that the programmer has to do something else instead (namely, manually transform his program to be first order in the case of higher order functions, or manually transform his program to use mutexes &amp; condition variables in the case of STM).
What's your beef with VS? I've only ever seen it considered state-of-the-art, so now I'm curious about what makes you dislike it.
Yes that case is different, since there is no waiting for an event inside the monadic expression. rather that that, the procedure is called by an event (possibly as part of an event handler). See some examples of the hplayground framework, which uses this same monadic reactive in the browser. For example this factorial example: http://tryplayg.herokuapp.com/try/fact.hs/edit here the event is triggered in the middle of the monadic sequence: main = runBody $ do print $ "enter the number" num &lt;- inputInteger Nothing `fire` OnChange print $ fact num 
It's funny how both making things possible and making things impossible are called "being expresive" by different people. :)
Fine. let´s see what happens....
I see, this is indeed on a different abstraction level. Thanks!
swoooosh 
`Maybe Node` is equivalent to `[Char]`
Lots of IO actions in Clojure also test to see if they're executing in an STM context and throw an error. This helps discover lots of STM-violations dynamically.
Huh, why?
learn both but don't use clojure for a big project. haskell is so much better.
We've been busy working on our production system :)
Here's a real world example from the core.async implementation in Clojure: [When I first wrote the core.async go macro I based it on the state monad. It seemed like a good idea; keep everything purely functional. However, over time I've realized that this actually introduces a lot of incidental complexity. And let me explain that thought.](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg) [What are we concerned about when we use the state monad, we are shunning mutability. Where do the problems surface with mutability? Mostly around backtracking (getting old data or getting back to an old state), and concurrency.](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg) [In the go macro transformation, I never need old state, and the transformer isn't concurrent. So what's the point? Recently I did an experiment that ripped out the state monad and replaced it with mutable lists and lots of atoms. The end result was code that was about 1/3rd the size of the original code, and much more readable.](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg) [So more and more, I'm trying to see mutability through those eyes: I should reach for immutable data first, but if that makes the code less readable and harder to reason about, why am I using it?](https://groups.google.com/forum/#!topic/clojure/wccacRJIXvg)
To me, expressiveness means to be able to do more in less code. Whether that is to communicate between threads or guarantee the absence of certain errors is irrelevant. 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Metasyntactic variable**](https://en.wikipedia.org/wiki/Metasyntactic%20variable): [](#sfw) --- &gt; &gt;A __metasyntactic variable__ is a [placeholder name](https://en.wikipedia.org/wiki/Placeholder_name) used in [computer science](https://en.wikipedia.org/wiki/Computer_science), a word without meaning intended to be substituted by some objects pertaining to the context where it is used. The word [foo](https://en.wikipedia.org/wiki/Foo) as used in [IETF](https://en.wikipedia.org/wiki/IETF) [Requests for Comments](https://en.wikipedia.org/wiki/Request_for_Comments) is a good example. &gt;By mathematical [analogy](https://en.wikipedia.org/wiki/Analogy), a metasyntactic variable is a [word](https://en.wikipedia.org/wiki/Word) that is a [variable](https://en.wikipedia.org/wiki/Variable_(mathematics\)) for other words, just as in [algebra](https://en.wikipedia.org/wiki/Algebra) letters are used as variables for numbers. Any symbol or word which does not violate the syntactic rules of the language can be used as a metasyntactic variable. For specifications written in natural language, [nonsense words](https://en.wikipedia.org/wiki/Nonsense_word) are commonly used as metasyntactic variables. &gt;Metasyntactic variables have a secondary, implied meaning to the reader (often students), which makes them different from normal [metavariables](https://en.wikipedia.org/wiki/Metavariable). It is understood by those who have studied [computer science](https://en.wikipedia.org/wiki/Computer_science) that certain words are placeholders or examples only and should or must be replaced in a production-level computer program. &gt; --- ^Interesting: [^Placeholder ^name](https://en.wikipedia.org/wiki/Placeholder_name) ^| [^Metavariable](https://en.wikipedia.org/wiki/Metavariable) ^| [^Fnord](https://en.wikipedia.org/wiki/Fnord) ^| [^Xyzzy ^\(computing)](https://en.wikipedia.org/wiki/Xyzzy_\(computing\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm7314z) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm7314z)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I guess they would be pretty unreliable now. I would consider a smarter algorithm a form of AI
Laziness sometimes causes excessive memory use, which is a bad thing. Have you heard of space leaks?
Interesting. They go after the idea of correctness by construction. Useful.
Sometimes the cost of doing a computation eagerly is significantly less (in space and/or time) than the cost of allocating a thunk to do it later… especially when it's going to be unconditionally evaluated. In other words, sometimes maximal laziness ends up being more work, not less.
Usually inductive would be correct. But look at the Graph type: the traversal does not rely on any induction in the type. Many structured could exhibit similar behavior or be isomorphic to inductive types, yet not be inductive.
Did you try the drClickOn from hackage which is the library from the paper? What is missing from the implementation?
Sorry to hear about the RAID failure. A similar thing happened to us a few months ago. It was a simultaneous complete failure of two RAID disks, destroying both our main VM container and our backup server at the same time. Amazing work recovering from this so efficiently! Thanks!
Started in Clojure, then went to Haskell and built a 6k to 7k LOC app, and now I'm back at Clojure for the most part. Haskell felt very tedious to me when my codebase started to surpass 5k LOC. Haskell development is playing bumper car with the compiler. It gets old really fast. Clojure was much more enjoyable with REPL development and bottom-up design. You really should learn both, though.
Hi, I write a lot of Clojure (1-9) This is my post on why I shifted over to Haskell: http://bitemyapp.com/posts/2014-04-29-meditations-on-learning-haskell.html *Learn Haskell* - here's my guide for doing so: https://github.com/bitemyapp/learnhaskell I agree with clrnd, but my experiences with Clojure were way less pleasant. Hard to solve type errors *all the time*. 1: https://clojars.org/bitemyapp 2: https://github.com/bitemyapp/brambling 3: https://github.com/bitemyapp/revise 4: https://github.com/bitemyapp/clojure-template-benchmarks 5: https://github.com/bitemyapp/blackwater 6: https://github.com/bitemyapp/trajectile 7: https://github.com/bitemyapp/clojure-transit-datomic 8: https://github.com/bitemyapp/bulwark 9: https://github.com/yogthos/Selmer/graphs/contributors (contributed more to the design than the implementation)
I remember Lennart telling me that they built an incredible Visual Studio plugin for Haskell at Standard Chartered, far above anything else in the Haskell ecosystem. They just can't release it :(
In which situations do you use currying?
I think you could probably broaden your scope of what AI encompasses.... especially if you are excluding smart algorithms and fancy math in your definition :P AI isn't much *except* for algorithms and fancy math. And AI definitely does not equal bayesian models and probability alone. 
Being lazy has a cost, you have to allocate a thunk, store that thunk etc. You also how the pathological cases where you end up with a space leak.
STM doesn't prevent race conditions, it relies on the programmer to place atomic blocks correctly, just like mutexes have to be placed correctly. It's a whole lot easier to get your atomic blocks right than getting your mutexes right, of course!
I hadn't thought of that, I was really only thinking of time. Thank you
You can define conversion functions between the two types: to :: [Char] -&gt; Maybe Node to [] = Nothing to (c:cs) = Just (Node c (to cs)) from :: Maybe Node -&gt; [Char] from Nothing = [] from (Just (Node c cs)) = c:from cs ... and these two conversion functions are completely reversible: to . from = id from . to = id That's what I mean when I say that the two types are equivalent. Anything you can do with one type you can do with the other. Side note: the formal term for this is that these two types are "isomorphic" and the functions `to` and `from` are "isomorphisms".
Hi! I'm here: http://www.reddit.com/r/haskell/comments/2mr7ks/im_debating_between_haskell_and_clojure_xpost/cm75b93 Any specific questions that you think should be answered? I didn't say a lot about it because: 1. I have a post on it 2. I don't really know what people would wonder about. I mean, you just look at my angry workday Twitter feed and see why I don't like Clojure (or Datomic) on a near daily basis.
http://www.reddit.com/r/haskell/comments/2mp3fb/learning_haskell_as_a_nonprogrammer/cm6sf4y
Don't think we need the artificial type of intelligence, just more of the human kind working at writing better strictness analyzer passes and disseminating knowledge about such things. 
People are using machine learning to predict likely programs based on large code bases, termed Big Code. For example JSnice can de minimize minimized JavaScript with machine learning. Such techniques could be used to predict when strictness is needed to avoid space leaks. This could guide the developer rather than optimizing the compiler.
TIL. Thanks a lot for your explanation!
This is exactly what I was thinking
Yes-ish. Strictness analysis is done in GHC to avoid the cost of allocating a thunk in some cases. Making this "smarter" will help and machine learning / AI might be a way to make it smarter or inject strictness annotations or both. It might also be possible to do some of this analysis at runtime via a JIT process, and data that is only available at runtime might be fed into a machine learning / AI system as part of that process as well. There's a relatively well-known proof that call-by-need (as implemented in GHC Haskell) performs a provably minimal number of reduction steps. So, from that angle all strictness analysis and `seq` can do is hurt. But, this doesn't directly translate into real world time (or especially space) advantages. A thunk can keep vast amounts of data "live" and prevent it from being garbage collected despite the result of the thunk being small (memory-wise) and the calculation of the thunk being short (time-wise). In addition, the transparent laziness provided by Haskell can add an additional layer of indirection / boxing that hurts cache locality and cache locality is a big deal for time performance.
That would be what I was looking for. Thank you. I only just started learning haskell, and now I'm on to Coq, Agda, and Idris...
&gt; However, you may eventually need packages not on Stackage, at which point you will need to use the inclusive snapshot. Or be a hero and [contribute the package (scroll down to "Steps to Contribute")](http://www.stackage.org/) .
&gt; AI definitely does not equal [B]ayesian models and probability. But, a little bit a non-determinism / dependency on historical data isn't necessarily a bad thing for traditionally deterministic processes. For example, by doing essentially random scheduling of ready tasks, Shake lessens the competition for CPU that happens when may compile tasks are scheduled at the same time and the competition for IOPS that happen when many linking tasks are scheduled at the same time, without knowing a priori that the CPU-bound/IO-bound status of any single task. AND, it turns out that, evaluation of pure code is a process that is resilient to reordering of the evaluation of sub-expressions, so you could have an "unreliable" component scheduling the evaluation of sub-expressions and get the same result. That component could be some ML / AI based on Beyesian correlations plus a bit of random perturbation, for example.
That would also help, definitely.
You're welcome!
For the ones that do not know, the paper is [here](http://homepages.cwi.nl/~ploeg/papers/monfrp.pdf). I possibly looked previously at a different version of the paper which did not mention the hackage library. I don´t remember the box examples. Anyway I looked for literature after creating hplayground, which implement basically the same monadic mechanism of the tutorial and I did not looked at the paper in detail until now. But the problems that solves and the advantages of the approach are the ones that I appreciated the most from the paper. What I see is that the library of the paper seems to use blocking semantics. Since it uses SDL https://www.libsdl.org/ for the example. So there is a single "hearing" point active at each computation that is looking for events. The computation continues when it receives the right event. Please correct me if someone know the example more, but it seems that the computation is in a blocking state waiting, like a console application. My approach is different: the monad simply configure an event handler for the first `waitEvent` of the computation and then finishes. The rest is done by event handlers. when an event arrives, the event scheduler call the event handler just configured. That event handler execution may configure more event handlers if there are more waitEvent sentences downstream. Once configured, there are many "hearing points" (callbacks), one for each waitEvent. Since there is no central IO state, the event actions can be executed fully in parallel (but, wait, this latter is not as easy....) Once configured, there is no valid and invalid sequences of events to consider but any combination of them are valid. That means that it may execute the event handler corresponding to a previous event, upstream from the one that has received previously, while the one of the paper can not. Like in the case of the inputs of a console application, they have a strict order since the computation has a single execution point. That is his weak point. it can not manage the example of my tutorial without enclosing it in a loop and adding conditionals since the sequence has to contemplate explicitly all possible combinations of events. But I may be very wrong. In the other side it mentioned in the paper that there are primitives to emit signals. I understand that as if the computations can send new events sort to speak. I don´t know very well the mechanism. I don not understand very well Yampa and so on. Neither I´m an expert in signal processing....The paper describes a more complete signal processing API, but in essence it uses also Applicative operators for composition of signals (it call it &lt;\^&gt;). And all the primitives are in essence combinations of the basic monadic-applicative combinators that I also have defined. 
&gt; discover lots of STM-violations dynamically In Haskell, we just discover them statically. Unless you use unsafePerformIO, then we also discover them dynamically. :/
I think it depends on what you want to build, if you are building websites, ClojureScript is a very compelling option right now and if you are doing enterprise, being off JVM can be a dealbreaker. In the end you will learn faster if you are working in it fulltime, my impression is it's a lot easier to get hired as a Clojure beginner than a Haskell beginner. If this is wrong, please PM me with beginner haskell job opportunities!
Yup.
&gt; STM doesn't prevent race conditions I think maybe you are using it wrong, or maybe calling something a data race that isn't one. Not all timing bugs are data races. In Haskell, TVars don't have data races.
Well in Haskell all functions are curried by default. Partial application can happen whenever it feels natural to you. plus :: Int -&gt; Int -&gt; Int plus a b = a + b plusThree :: Int -&gt; Int plusThree = plus 3 
&gt; JVM is not my cup of tea. If you have to deploy to the JVM, Frege is pure and functional in the style of Haskell. If static, inferred types are what you want and purity is not required, Scala is actually pretty good.
you're welcome!
Yes indeed.
For example, if you know you definitely will use the result of that computation's value in the near future. The primary benefit of laziness is that you can skip doing work which isn't used. Of course, this is a bit of a simplification and may not *always* hold. For example, laziness might allow fusion or certain types of optimization in some cases. It also may allow more idiomatic or pleasant programming styles.
None that *I* think shoud be answered, since I never took a liking to Clojure, personally. I just happened to have recalled seeing your musings on the subject and thought it would be valuable input. I am curious if you ever got the Closure parser running in order to do a runtime comparison wit the Haskell version. I suppose one thing I would be interested in is what types of idioms people tend to adopt when using a lisp as compared to Haskell. I can honestly say that I've never noticed such a focus on the importance (and elegance) of types until coming to Haskell.
&gt;I am curious if you ever got the Clojure parser running in order to do a runtime comparison wit the Haskell version. Author shifted goalposts, refuses to admit code was broken/ridiculous, and I'm not doing it for him since he'll just say I made the code slow on purpose - so I'm at an impasse there. &gt;I suppose one thing I would be interested in is what types of idioms people tend to adopt when using a lisp as compared to Haskell. Lisps are generally a lot more imperative. Generally they'll abuse implicit side effects a lot (aspect oriented programming, metaobject protocol, etc.). I can't think of anything compelling. We have macros in Haskell as well anyway. I guess Haskellers are more likely to use a templating language for templates, Lispers are more likely to reify content in a macro DSL? Example: http://weitz.de/cl-who/ If we talk about Clojure specifically, transients and STM are both more dangerous than their equivalents in Haskell because nothing in Clojure statically prevents mistakes in the use of either. You don't have green threads, so you can't really just fork or send off futures willy-nilly in production code, so you have to worker-pool-ize stuff a lot more aggressively than would be necessary in Haskell. OTOH, in Haskell you'll want to use a streaming library if memory use should be strictly controlled. Lispers are more likely to solve all their problems through interactivity - debugging a live process, that sort of thing. This is partly because it is often difficult to reason clearly about the code. Old-time Lispers hated Clojure in part because it meant losing a lot of nice debugging facilities and interactivity. The debugging in Clojure still sucks, years later. Haskellers are more likely to want code that narrows down what they have to think about, have machine-assisted static analysis, and get their interactivity via a REPL or Emacs integration. Lot of cultural differences. Lispers spend more time debating matters of taste because don't have any principled foundations to work on top of, whereas Haskellers are more likely to be kicking around algebras with more concrete terms of engagement. Biases: I was a Common Lisp and Clojure user. Yeah, I was one of those AOP people in CL and Clojure, but not all the time.
It's still cool. Thanks for posting.
Data race is a C++ term. If you transliterate the meaning of that term into Haskell, then yes STM prevents data races. But I said race conditions, not data races. Race conditions is a more general concept which means that the result of a computation is non-deterministic depending on the execution order. STM still has that: if you update a variable concurrently in 2 threads in 2 atomic blocks, then the order in which that update happens is non deterministic.
[Abstract Datatypes for Real Numbers in Type Theory](http://www.cs.bham.ac.uk/~mhe/papers/realadt.pdf)
Thanks. Shame about the benchmark. The link to cl-who was interesting. I see what you mean about things being more imperative. 
&gt; I see what you mean about things being more imperative. Hahaha, if you think that's [imperative...](http://www.unixuser.org/~euske/doc/cl/loop.html)
If you don't want non-determinism, you don't want STM -- or concurrency for that matter. In you want determinism, you want pure parallelism which doesn't use STM in Haskell, but rather the [Par](http://hackage.haskell.org/package/parallel) monad.
Persistent here is about preserving previous versions upon modification. Wikipedia has a pretty good article, http://en.m.wikipedia.org/wiki/Persistent_data_structure. Durable is sometimes used instead of persistent in your example.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Persistent data structure**](https://en.wikipedia.org/wiki/Persistent%20data%20structure): [](#sfw) --- &gt; &gt;In [computing](https://en.wikipedia.org/wiki/Computing), a __persistent data structure__ is a [data structure](https://en.wikipedia.org/wiki/Data_structure) that always preserves the previous version of itself when it is modified. Such data structures are effectively [immutable](https://en.wikipedia.org/wiki/Immutable_object), as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. (A persistent data structure is *not* a data structure committed to [persistent storage](https://en.wikipedia.org/wiki/Persistent_storage), such as a disk; this is a different and unrelated sense of the word "persistent.") &gt;A data structure is partially persistent if all versions can be accessed but only the newest version can be modified. The data structure is fully persistent if every version can be both accessed and modified. If there is also a meld or merge operation that can create a new version from two previous versions, the data structure is called confluently persistent. Structures that are not persistent are called [ephemeral](https://en.wikipedia.org/wiki/Ephemeral_(disambiguation\)). &gt;These types of data structures are particularly common in [logical](https://en.wikipedia.org/wiki/Logic_programming) and [functional programming](https://en.wikipedia.org/wiki/Functional_programming), and in a [purely functional](https://en.wikipedia.org/wiki/Purely_functional) program all data is immutable, so all data structures are automatically fully persistent. Persistent data structures can also be created using in-place updating of data and these may, in general, use less time or storage space than their purely functional counterparts. &gt;==== &gt;[**Image**](https://i.imgur.com/JjzhUOk.png) [^(i)](https://commons.wikimedia.org/wiki/File:Purely_functional_tree_before.svg) --- ^Interesting: [^Hash ^tree ^\(persistent ^data ^structure)](https://en.wikipedia.org/wiki/Hash_tree_\(persistent_data_structure\)) ^| [^I/O ^request ^packet](https://en.wikipedia.org/wiki/I/O_request_packet) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm7ci5q) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm7ci5q)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Imperative? All those loops look quite functional to me, aside from the obvious intermingling of IO everywhere.
One experiment you can try yourself is with tail recursion. (note: with -O2 set, the strictness analyzer normally picks up on tail recursion and makes it strict) 
&gt; Lots of IO actions in Clojure also test to see if they're executing in an STM context and throw an error. This gives me a slight headache. Why would anyone want to wait so long to find out their program is broken, when they could be told instantly? :(
&gt; ClojureScript is a very compelling option right now What's your take on Fay or GHCJS?
Yeah that's true. However I'm wondering if it has to be a syntax extension? Like is there some function magic I can do to do like magic f (a `apply` missingVal `apply` c `apply` missingVal `apply` e) That syntax is terrible, but it might be possible. I just have no idea how the types would work for this (they might not work at all). At any rate, I guess this is a good time to start learning TH.
Summary of qualified *opinions*: * "Don't use Clojure for big project." * "Don't use Haskell for big project." * "Don't use Ruby for big project."
can't you just do \x -&gt; \y -&gt; f a x c y e or am I missing something?
GHCJS is the future for Haskell users. It's already quite nice to use.
That doesn't work arbitrarily. I want to be able to do this with any function with any number of arguments. 
ah, misunderstood. 
You can kind of hack it as a family of infix combinators: _1 f x a = f a x _2 f x a b = f a b x _3 f x a b c = f a b c x cat7 a b c d e f g = concat [a, b, c, d, e, f, g] main = print $ (cat7 "a" `_1` "b" `_2` "c" `_3` "d") "X" "Y" "Z" Or an uncurried version using the reader monad, using lists here instead of tuples because I’m lazy. main = print $ (cat7 "a" &lt;$&gt; (!!0) &lt;*&gt; pure "b" &lt;*&gt; (!!1) &lt;*&gt; pure "c" &lt;*&gt; (!!2) &lt;*&gt; pure "d") ["X", "Y", "Z"] I would suggest using TH, though. 
I know I said I was gonna implement full CSS but I've been kind of sick and the CSS spec is a black void in which no joy can exist. Had to do a lot of debugging for this one and dealing with the nested structs in Layout.hs was incredibly annoying, so I'm leaning towards refactoring it with lenses, even though that will add a bunch of dependencies. Might also add some basic openGL code to paint to a window.
Immuatble just means it does not change. Persistent means it "kinda changes" in the sense that it has operations that create new copies of the data structure that have modifications.
&gt;Persistent means it "kinda changes" in the sense that it has operations that create new copies of the data structure that have modifications. Nope, this is also what immutable means, in the context of a data structure.
I thought about this once. What I decided is it's probably more trouble than it's worth. What thylacine222 suggests *is* a general solution, just as a pattern rather than a single function. Any partial application can be represented by wrapping original function in a new one with fewer arguments. This wrapping can always be handled using a wrapping function, but you need different wrapping functions for different patterns of partial application. Currying works because it's a convenient special case - the wrapping function is the application operator `$`. But if you want all patterns of partial application to be supported, you need a more general notation - one that can't exploit being a simple special case. Lamda just isn't that bad a notation for expressing a pattern of partial application... \b d f -&gt; fn a b c d e f The notational overhead isn't much more than repeating the three arguments that you don't want to apply - listing the arguments you want to keep as arguments. Sure lambda is more general than that and not optimized for this, and you can invent slightly more concise notations, but is it really worthwhile? How often do you need this? And how often is it really *only* partial application as opposed to e.g. using the same value for two or more arguments, re-ordering some arguments... 
https://hackage.haskell.org/package/HoleyMonoid might come close to what you intend to do.
It's very common. A lot of people tend to use the term currying (like apparently the author of that blog) when they mean partial application, and while they are related concepts, they are very different things. You can usually tell what a person means by context, though.
I attempted to answer this [in the comments](http://superginbaby.wordpress.com/2014/11/18/learning-haskell-as-a-nonprogrammer/comment-page-1/#comment-36) of the post: --- &gt; \* Honestly, I still don’t really understand imperative languages. Why would you want things to mutate? Why do you want so many, seemingly poorly controlled side effects? Imperative languages could be viewed as historical accident due to the early slowness of computers. All computers are imperative underneath – behind the scenes, Haskell is mutating things and causing side effects for you. The first language was basically assembly code (“add 1 to the number in the working register, save the result in this other register”), which was imperative too, because it was only a slightly higher level of abstraction than the hardware. C was built on assembly, and it was imperative too. A big reason was probably that they wanted to give programmers maximum control over the (imperative) assembly generated from their source code. Computers were far slower in those days, so there was no room for wasted computation. The advantage of imperative code is that it is easier to see how to speed it up – you can see what code is run in what order, and you just need to remove or optimize some of those operations. Whereas if C were functional, programmers might have complained that they didn’t know how to make their program fast enough, so they wouldn’t have used it. Later, more abstract imperative languages probably use this rationale for being imperative, to some extent. And part of the reason might be that the language creators were just used to imperative thinking, because C is imperative, and so they didn’t think of other styles. Finally, some problem domains are all about mutation – mainly video games. Using a functional programming language where mutation is harder is counterproductive – imperative languages are easier for such programs. Though this traditional wisdom may change as more patterns and specialized abstractions for functional game development are found – perhaps eventually functional languages will become the new standard for game development. --- I’m not sure if this view of history is correct, and I’d be interested to hear otherwise. 
One more reason I like keyword, because this is not something Haskell can do in an ideal fashion.
If you have to pick between complexity and completeness, I would rather have less complexity and maybe just have a subset of CSS. :)
I, too, would like to see keyword arguments given nice syntax in a pure functional language with static inferred types. You can sort of fake it with a throw-away POD data type, and things like `RecordPuns` or whatever the extension is called. Anonymous record types (doesn't Ermine have these) could make it even nicer, I think.
Agreed. Wren has done some work developing a lambda calculus with keyword args. I'd love to see it extended further.
This is wicked. The fact you didn't use advanced type systems features is actually really great. It makes Idris seem less daunting for us plebian haskellers out there. Thanks for sharing!
Anyone who goes to wikipedia or haskell.org will see Haskell is "purely functional". This was always a bit vague for me. I would explain that IO is handled by a special data type, executed from the entry point of the application to all connected operations of that type.
How about "definition"? pi = 3 defining `pi` as 3. 
It does work for a function of any number of arguments (Try it!), because the type `F` may itself be a function of any number of arguments.
Hey! I heard you talk about this on the Zulip. It's super cool to see you posting this on here too!
Does [this help](http://stackoverflow.com/questions/12129029/how-to-change-the-order-of-arguments/12131896#12131896)? (Unfortunately that link to the semantic editor combinator blog post seems to be down, but there's a lot of information in the SA answer itself.) *edit*: There's also a package inspired by that approach on Hackage: https://hackage.haskell.org/package/sec-0.0.1
Currying is specifically "A -&gt; (B -&gt; C)" as opposed to "(A /\ B) -&gt; C". In Scala, functions are not usually curried but can be partially applied.
I think Tsuru capital does things with Haskell. They're somewhat similar to Jane Street. http://www.tsurucapital.com/en/
After feedback from users, I'm shifting the stability (in the README) to stable even if it's not going 1.0.
Let's figure it out! Applying a function to some arguments where some arguments are special makes me think of Applicative notation: f &lt;$&gt; pure a &lt;*&gt; hole &lt;*&gt; pure c &lt;*&gt; hole &lt;*&gt; pure e Since the holes not only have an impact on the computation, but also on the type of the result, it can't really be a simple Applicative. Maybe an Indexed Applicative? I've never even heard of Indexed Applicatives before, but I know about indexed monads, whose computations have an impact both at the computational level and at the type level, thereby constraining which computations can be performed when. They look like this: class IMonad m where -- return :: a -&gt; m a ireturn :: a -&gt; m i i a -- (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b ibind :: m i1 i2 a -&gt; (a -&gt; m i2 i3 b) -&gt; m i1 i3 b That is, they look like ordinary monads, except each operation is annotated with a pair of indices, and two consecutive operations must have matching indices. Let's see whether we can adapt the idea to Functor and Applicative: class IFunctor f where ifmap :: (a -&gt; b) -&gt; f i i' a -&gt; f i i' b class IFunctor f =&gt; IApplicative f where -- pure :: a -&gt; f a ipure :: a -&gt; f i i a -- (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b (&lt;**&gt;) :: f i1 i2 (a -&gt; b) -&gt; f i2 i3 a -&gt; f i1 i3 b The code so far is only based on the shape of the problem, so there is no guarantee that the actual problem will also need consecutive operations to have matching indices. It's also not clear what the indices will end up representing. Let's see, what do we need to represent? We need both `ipure a` and `hole` to specify that they are supposed to be used at a position where an `a` argument is expected, and `hole` also needs to specify that an extra `a -&gt;` will need to be prefixed to the final type. So how about this: ipure :: a -&gt; Partial r r a hole :: Partial r (a -&gt; r) a That is, the indices would be used to accumulate the arguments skipped by the holes. Let's check that the types work out: &gt; :t f A -&gt; B -&gt; C -&gt; D -&gt; E -&gt; R &gt; :t f &lt;$$&gt; arg A &lt;**&gt; hole Partial r (B -&gt; r) (C -&gt; D -&gt; E -&gt; R) &gt; :t f &lt;$$&gt; arg A &lt;**&gt; hole &lt;**&gt; arg C &lt;**&gt; hole &lt;**&gt; arg E Partial r (D -&gt; B -&gt; r) R Not quite right: `D -&gt; B -&gt; r` is in the wrong order. It makes sense: starting from `r`, the first hole adds `B -&gt;`, resulting in `B -&gt; r`, and the next hole adds `D -&gt;` to that, resulting in `D -&gt; B -&gt; r`. We'd need the extra arguments to be added from right to left instead of from left to right. Which can be obtained by tweaking the way in which `(&lt;**&gt;)` combines its indices: class IFunctor f =&gt; IApplicative f where -- pure :: a -&gt; f a ipure :: a -&gt; f i i a -- (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b (&lt;**&gt;) :: f i2 i3 (a -&gt; b) -&gt; f i1 i2 a -&gt; f i1 i3 b &gt; :t f &lt;$$&gt; arg A &lt;**&gt; hole Partial r (B -&gt; r) (C -&gt; D -&gt; E -&gt; R) &gt; :t f &lt;$$&gt; arg A &lt;**&gt; hole &lt;**&gt; arg C &lt;**&gt; hole &lt;**&gt; arg E Partial r (B -&gt; D -&gt; r) R Perfect! Now we just need to find an implementation which matches our types. data Partial i i' a = Partial { runPartial :: (a -&gt; i) -&gt; i' } partial :: Partial a b a -&gt; b partial p = runPartial p id arg :: a -&gt; Partial r r a arg = ipure hole :: Partial r (a -&gt; r) a hole = Partial (\cc x -&gt; cc x) instance IFunctor Partial where ifmap f (Partial cc) = Partial (\cc' -&gt; cc (\x -&gt; cc' (f x))) instance IApplicative Partial where ipure x = Partial (\cc -&gt; cc x) Partial cc1 &lt;**&gt; Partial cc2 = Partial (\cc -&gt; cc1 (\f -&gt; cc2 (\x -&gt; cc (f x)))) I really didn't expect to encounter continuations here, but they match the type, and it works! &gt; partial (f &lt;$$&gt; arg A &lt;**&gt; hole &lt;**&gt; arg C &lt;**&gt; hole &lt;**&gt; arg E) B D R Well, that was fun. Thanks for the challenge!
 f :: A -&gt; B -&gt; C -&gt; D -&gt; F foo:: A -&gt; C -&gt; E -&gt; (B -&gt; D -&gt;F) foo a c e = (\b d -&gt; f a b c d e) 
Wow, this is exactly what I was looking for, thanks! 
This would be awesooooome!
Arguably this is merely a compiler problem; the same way people used to use C macros instead of functions in performance critical code
Alternatively post the simpler one as well as a link to the lens solution experiments. This is if you decided to have a go at the css spec without lenses after reading the parent comment.
All the functions which help you draw something on a screen don't have side effects.
Do you know if the code examples are posted?
"bar" is never used without first writing "foo" so I don't see the problem. Also, you're not often in a context where you're saving actual bars, as in pub or ingot or whatever, in variables in the first place, either.
That sounds exponential.
Well, to be fair, it is a trade-off. Static types usually do require *some* extra effort that is not required in an equivalent untyped program. Just now I had to refactor something where I had to propagate a type-variable through a large chain of types-using-types. Without types, this wouldn't be necessary. Also, static types force you to be honest: when you make a "small" change that suddenly makes your function have effects, you might be aware of how that is fine in the several use cases of that function. However, with a static type system like Haskell's, you now have to go and change all the types of everything that uses it to be honest about it. This honesty *costs* you when you change, and helps you when you read/maintain. We probably both believe the honesty benefits outweighs its costs, but it's not convincing to everyone.
&gt;That or maybe what I consider "picked up" is very different from you. Yes, clearly it is. Also I don't thing it's OPs first encounter with FP, him having experience in ruby, c# and knowing that scala and js aren't opinionated. Even if it was, Clojure is a _dead simple_ language with some wrinkles around interoperability, but that's something I don't consider part of "picking up". Even macros aren't part of "picking up" for me. You can write tons of happy functional clojure code without writing a single macro. 
I guess this is the Holey Monoid http://hackage.haskell.org/package/HoleyMonoid-0.1/docs/Data-HoleyMonoid.html
This matches my experience precisely. I learned Java, C, Python, C++. Then I learned Clojure, and it was fun and beautiful and enlightening, but I could not tolerate dynamic typing after a while. Too many useless bugs. Then I learned Haskell, and it took me around two days to realize I was done with Clojure. It made me remember why I used to love programming. It lets you build elegant things the right way.
_Opting_ into a restriction is in and of itself expressive. You're communicating that the behaviour disallowed by that restriction is undesirable. If you're permanently stuck with that restriction, that's when you're losing expressive power.
putting something on the screen is literally a side effect.
Or you could do the gymnastics /u/gelisam did and go with partial (f &lt;$$&gt; arg A &lt;**&gt; hole &lt;**&gt; arg C &lt;**&gt; hole &lt;**&gt; arg E) B D What you choose depends on what your idea of fun is!
You're right.
In my case I think it's a win because there's a multiplexer which selects one of several components to react to incoming data. What's more, a lot of the time, the clock is ticking and only a small state machine is reacting while most of the system is dormant. The concept I was missing was that every dormant component is being recomputed at every single clock tick. But even that does not explain why each additional component doubles the simulation time.
There's also [Hasklig](https://github.com/i-tu/Hasklig/), which is based on Source Code Pro and has ligatures (not that many though) for Haskell.
This confusion seems to stem from real world Haskell, as discussed in [this thread](https://www.haskell.org/pipermail/haskell-cafe/2009-January/053333.html). Edit: [And it goes back further](http://lambda-the-ultimate.org/node/2266)
The math done to figure out what to put to the screen is not. Ergo every part of putting something to the screen is not a side effect.
This issue seems to come up frequently, for example [here](http://lambda-the-ultimate.org/node/2266). I'll just stick with that currying is curry :: ((a, b) -&gt; c) -&gt; a -&gt; b -&gt; c and that function application in Haskell only has one argument and returns a function with the remaining arguments.
I don't believe there are snippets, but you could probably go through the history on [GitHub](https://github.com/clojure/core.async) to compare.
I've encountered this as well. I believe it's a known GHC issue, but it's not easy to fix. I believe what I did was just leave specific comments about the warnings the code will inevitably generate if compiled with -fwarn-incomplete-patterns.
Nice writeup. What I'm missing is how it actually solves "cabal hell", but maybe that wasn't the purpose of this post?
Thanks go to /u/dstcruz, but I'll keep the karma. :)
From [Ollie's post](https://ocharles.org.uk/blog/posts/2014-02-04-how-i-develop-with-nixos.html): &gt; Nix helps me avoid Cabal hell. The Haskell package repository inside Nix (haskellPackages) is a lot like Stackage. Rather than having every version available, we only have the latest version available (or very close to that). This makes things a lot simpler. Furthermore, we have a build farm that informs Nixpkgs maintainers when things don’t work out, and we tend to push these issues upstream and resolve them as fast as we can. It’s extremely rare that Cabal gets in the way for me these days. Also, see [Duncan's post](http://www.well-typed.com/blog/99/) about improving Cabal which mentions Nix. Thousands of versions of the same package can coexist. However, if e.g your program depends on `lens`, picking a different version of `lens` will yield a different result, Nix doesn't pretend you'll have built *the same thing*. That's one of the consequences of the pure model behind Nix. Whenever you'll concretely be building a program or library, the exact resulting binary/object code is pretty much entirely determined by everything the nix expression for this program or library depends on (dependencies, config, compiler, build options and what not). In this setting, cabal hell doesn't make sense. You change any of these things =&gt; you're not building the same thing as before. Once you fix the inputs, you're guaranteed to get the same output. And for the purpose of this explanation, you can consider the haskell packages in nix a set of nix-expressions with "fixed input", so if the build farm can build it, so can we. In reality, they are actually all parametrized over the ghc version, etc, which lets us build packages with ghc 7.6.3 and 7.8.3 for example.
From the FA.: You should not suffer any problems with package dependencies, because the nixpkgs maintainers take care to offer a set of packages which are mutually compatible. As far as I understand, automatic builds of nixpkgs are performed. Build problems are reported to, and eventually fixed by the nixpkgs maintainers. So, using nixpkgs gives pretty good guarantees compared to plain cabal, even though issues might temporarily sneak in.
Thanks! I'll try to improve these glyphs
 cabal install pandoc pandoc --from=pdf --to=epub3 foo.pdf See the pandoc documentation for tweaking EPUB output to your liking.
These fonts made me discover this opportunity to create ligatures so necessary, I will always be grateful to their authors
Very nice! Do the ligatures have the same width as the combined width of the non-ligature versions?
Is there anyone here who is familiar both with Ruby and Haskell? I've been coding in Ruby for a few years and am getting into Haskell in Meijer's Edx course, and I am really puzzled by "cabal hell". How on earth is it that Ruby's "bundler" has solved such a computer sciencey problem so well and Haskell hasn't? It boggles my mind. The "sandboxes" seem to be roughly equivalent to rvm gemsets, but that doesn't handle the dependency solving and tracking in Gemfile.lock. Am I misunderstanding what cabal hell is? Is it actually some larger problem that bundler doesn't address at all? Is there a reason not to implement Ruby's approach?
I haven't played with closure yet, but learning Haskel was an adventure. Half the tools and patterns I'd gotten used to relying on were pulled out from under me. I've been playing with it on and off for a while now I can write trivial programs, but I wouldn't say I've picked it up yet.
Dependency solving is handled by cabal, and the lockfiles are similar to `cabal freeze`. Many people don't use or don't know about freeze though, and I think haskell also has a culture of smaller packages with more versions, and perhaps stricter bounds. Those things combined make things trickier, though the problem is already a lot smaller than it used to be.
You shouldn't ligate `--`. It ends up looking like —.
Looks great, finally got me to try nix. I found a few small issues though: &gt; This modifies your .profile, so logout/login is advised. It doesn't seem to modify the .profile, it just prints a message at the end that you should do so. &gt; nix-env -iA cabal2nix That didn't work for me, I had to drop the `A`.
Not Haskell, but Spire tackles this issue: https://github.com/non/spire The Reals in the library are based on ERA, written in Haskell by David Lester. 
Installing a package in Ruby doesn't involve compiling it against specific versions of other libraries. Here is an explanation of the diamond dependency problem which is cause of much of the problem: [link](http://www.yesodweb.com/blog/2011/11/cabal-src)
Very useful hints!!! Thank you!
Thanks! Yes, width of &gt;&gt;, for example, is the double exact of &gt; The &gt;&gt;&gt; ligature has the triple weight of &gt; I've already tested the font with text editors in different OSs and it seems working with monospaced font anyway
Great! That's pretty important while editing
You might need to use `nix-env -iA nixpkgs.cabal2nix` if I remember correctly how nix works on non-NixOS :) 
I recall Richard Eisenberg mentioning that this will probably be fixed in 7.10. Can't find a source online for this, so maybe I am misremembering? It would be lovely if it were, that's for sure.
Yes. When using hackage/stackage, the state of your system is pretty much impossible to recreate. It is simply the result of a bunch of mutations caused by you running 'cabal install' with no record keeping. With nix, the state of your system is the result of evaluating a declarative expression. You can wipe everything out and re-evaluate the expression and get the same end result. It provides sandbox type functionality on a system-wide level. Not just for cabal packages, but for all system libraries. It provides atomic upgrades and rollbacks -- and the ability to instantly jump between different points in build history. Via the hydra build farm, you can often avoid having to build at all and just download binaries. If you are working on web apps you can use the nixops extension to add support for cloud deployment. If you want to apply patches to packages before building them, it supports that as well. If you are working with a team, you can easily maintain your environment by checking in your nix config files to github (or cvs, etc). You can easily have multiple profiles and do things like jump between your work environment and your home environment. You can make your Linux dev environment and your OS X dev environment match by simply using a common config.nix. If you modify some library that a bunch of other libraries on your system depend on, it will properly rebuild everything. If the build fails, it will still leave your system in a consistent state. If you want to be bleeding edge, you can build packages straight out of their darcs or github repositories. And many other things. For Haskell development, I highly recommend the helper scripts in this repo: https://github.com/jwiegley/nix-config
What font is this? Is it free? This is nice!
Pragmata Pro. http://www.fsd.it/fonts/pragmatapro.htm It isn't free, but it is worth the money. I switched to it from Inconsolata and have never looked back.
It’s PragmataPro http://www.fsd.it/fonts/pragmatapro.htm Unfortunately my attempt to make it become open source has failed and so is still a commercial font
the C# attempt failed because it was too ambitious: they tried to track *all* variables within the transaction automatically and transparently, without requiring the programmer to limit which variables should be tracked. The Clojure implementation was successful because it tried to solve a much easier problem; require the programmer to explicitly mark which variables should be tracked under STM and impose constraints on those variables - much easier. 
I don't really see the point of this post...
Nix can manage packages outside of the scope of cabal and stackage. If your Haskell library needs to link with a specific version of Postgres or LLVM then Nix can include that information in the build and pull the binary libraries for these packages along with your Haskell code.
I find after a few looks and returning to this list that I'm not a fan of the /= /== ligatures. I like the idea of distinguishing /= and /== using off-center /'s, but the fact that the horizontal lines end at the slash is very off-putting to me. It's probably worth including (&lt;$) and ($&gt;). On closer look at (~&gt;) I feel like the squiggle should end up pointing, at least a little bit, toward the peak of the (&gt;) as it enters. The way the glyph is now, the squiggle ends up directed toward the top bar of (&gt;) and it throws me off since I have an expectation of symmetry.
How's business btw? Hope you don't mind me asking
No feedback about these ligatures here. So this is slightly off topic. However I just had to say that this typeface is beautiful, and honestly I'm thinking of buying it even though I don't write much Haskell at my job. It's just so well put together.
Oops. I meant: nix-env -iA nixpkgs.haskellPackages.cabal2nix Thanks for the report.
Actually, stackage provides reproducible environments, via snapshots. Yet, one needs to nuke the package database (and recompile everything) to get back to a known state.
The fact that ligatures aren't supported either by emacs or vim is such a shame... :(
Perhaps I'm missing something fundamental in my understanding of cabal and such, but when you run `cabal2nix` the resulting nix expression does not mention versions of any of those packages. So, after jumping into a shell via `nix-shell --pure` and running `cabal build` how does `nix` help here for "reproducibility" if I update my `project.cabal` file to depend on a new version of `lens`? Again, I'm open to having a misunderstanding of something fundamental in cabal / haskell package management, so be gentle ;)
I'm not a fan of any of them. Yes, they look nice but when I run across them in the wild I wind up spending a few minutes trying to figure out which sequence of characters they are supposed to represent. Most aren't a problem, but sometimes you run across a real head scratcher. It kind of reminds me of "#define BEGIN {" in C.
&gt; Standard Charted Chartered maintains the largest Haskell code base in the world. I'm slowly catching up. I'm a bit over half way there. ;)
I realize I was conflating ligatures with Unicode. I guess I'm just getting old, I prefer good old ASCII. 
1/ If you enter a pure environment and then just cabal install your deps, that won't change anything indeed. (maybe I'm not understanding what you say correctly, in which case please comment back on this message and I'll try to be clearer) 2/ If you're in a specific "profile" or if you're building your code using a nix expression, things are different. For the trivial scenario, all your deps will be looked for in the `haskellPackages` set. The `cabal2nix`-generated nix expressions however usually let you tweak which (for example) `lens` package to use, which lets you use the one from `haskellPackages` or a fresh git checkout of `https://github.com/ekmett/lens.git`. The output will however be different and the result of the build will be identified as such (will have a different hash). Now, if you want to reproduce that very same build (the one with `lens` from git, and specific versions of all your other deps), you can just pin all the versions and package sources in a nix expression and hand that nix expression to your coworkers or friends. The `haskellPackages` package set just happens to provide a set of packages that are known to work well together and build fine, but you can override everything as you see fit.
I think this clarified things. One quick follow-up: In the following two scenarios: 1. `cabal2nix ./. &gt; default.nix &amp;&amp; nix-build` 2. `cabal2nix ./. | (...wrap in callPackage...) &gt; default.nix &amp;&amp; nix-shell --pure --command "cabal build"` - Does anything different happen in either? - In the first case, who is resolving the version of dependencies? Is it somewhere in `cabal.mkDerivation`? - In the second case, where does `cabal` end up installing stuff? In `~/.cabal` or somewhere in `/nix/store`? thanks for your help. PS - Believe it or not, I'm actually running NixOS and have been doing something similar to the OP. However, my inability to understand this subtlety has been bothering me for a bit.
Yes, they moved this ticket to the next milestone (7.10.1). But unfortunately this does not mean it will be fixed in that scope.
custom ligatures are so beautiful... is there a vim plugin for this yet? or for sublime? Edit: Sorry, I didn't know these were meant for sale. I'm going to totally buy this.
This series is fantastic!!! It'd be cool to see an image of the output in the post. Here's a link for the lazy: https://github.com/Hrothen/Hubert/blob/master/tests/rainbow.png
Oooh, open source. I wonder if I could somehow generate a stackage snapshot to correspond to the jessie (Debian) release and have cabal-install from jessie automatically be pointed at it... Seems like it could be advantageous for when I'm trying to write something targeting our production servers instead of the latest.
my language transition history: ruby -&gt; clojure -&gt; haskell -&gt; clojure why I choose clojure: macro; why I choose haskell: pure (monad); why I choose clojure again: repl is fun, cabel was bad, leiningen is awesome; why I am here replying to your comment: repl fun ends whenever you have that big chunk of ugly debugging message blow up in your face: ClassCastException java.lang.Double cannot be cast to clojure.lang.IFn ... blah blah blah Still continue clojuring, but I don't think it is that fun anymore. 
Any chance you could do a rolling average for the attendance graphs? They're rather spiky.
[Haskoin](http://haskoin.com/) is an enterprise multi-sig wallet startup, and they've also developed a [bitcoind implementation](https://github.com/haskoin), as well as client (plus gui soon), all in Haskell. Several interesting challenges to discuss - ECDSA implementation in Haskell, bug-compatibility with bitcoind, etc.
["Tom Schrijvers and colleagues are working on this."](https://www.haskell.org/pipermail/glasgow-haskell-users/2014-September/025253.html)
I hope you didn't. Sublime and vim don't support fonts with ligatures.
Makes sense. Rich Hickey really is the kind of guy who "do things in his head". And he even did a presentation to proof this: [Hammock Driven Development](https://www.youtube.com/watch?v=f84n5oFoZBc) But I am not sure if it's right to call this a bad thing, especially when you are young. But if I want the coding career to age well with myself, I need to reconsider this very well ...
Let's take an example: [algebra](https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/libraries/haskell/algebra/default.nix). { cabal, adjunctions, distributive, mtl, nats, semigroupoids , semigroups, tagged, transformers, void }: at the top says that the whole default.nix is a function of these things. This means the package doesn't rely on a pinned down version of those but takes them as argument. 1. will not work AFAICT since nix-build applied just to `default.nix` will not be able to find all the dependencies, all it knows about is that nix file. You have to pass additional arguments to point at a nixpkgs (or to some private repo where it'll be able to find all the packages it needs). If you do that, it'll put the result in a `result` dir I think, in your current dir. The content of that dir is what would go to the nix-store if you had `nix-env`'d that package, if I'm not mistaken. All the dependencies will be installed in the nix store. 2. This will, I think, put the result in a `dist` directory in your current directory. All the dependencies will be installed in the nix store too. - I think I outlined the main differences. - Nobody unless you point at a specific nixpkgs repo. - I answered above. I may be mistaken, in which case someone more knowledgeable should correct my answers, but that should give you a better intuition of how packages get tied together. You'll probably want to wander around in the nixplgs repo, especially in `all-packages.nix`, around the haskell specific bits.
I got to hunting the internet for the reason for this and lo and behold [hasklig](https://github.com/i-tu/Hasklig) the most recent commit is 7 days ago, readme indicates emacs support is in the works, and something about a gvim patch. So someday maybe.
ahh, why not!
Types let me use my rather limited brain capacity for stuff the computer *cannot* do. It also gives me a denser encoding of intent to work with in my head than Clojure code.
&gt; Like I said, I'm not a Clojure programmer Then it's surprisingly threadsafe - a basic "var" is thread-local and generally dynamically scoped if it's mutable at all, and sending a request to an "agent" only goes through if the transaction commits. That leaves "atoms" (basically a raw compare-and-swap if you really want speed) and Java interop as the mutable state you might change in a way visible from other threads. There's also a macro "io!" which throws an exception if run in a transaction that you can use to annotate stuff if you want.
Came here to say the same thing. Nicely done.
I'm not personally sure it's all that bad; not that I don't mind improvements, but some of the points there seem quite logical to me. - Coloring the last element of a type: In most cases, the last argument is the type of the return value. Of course, this isn't strictly true (for example, you can think of `fmap` as something that lifts a function to a functor, instead of taking a function and mapping it over a data structure), but it's still somewhat logical. This lets you quickly see the return value of a function if it is fully applied. - Coloring the function name in a type signature: This delineates it from names of types, and lets you quickly see all the top-level functions in a block. - Name qualifiers highlighted: A different color than builtins would be nice, but I see nothing wrong with highlighting that particular syntactic element. I'm ambivalent about the other two points. All in all, I'm not terribly dissatisfied with the highlighting, though it *would* be nice to have a different color for the name qualifiers. What was Github's highlighting like before? Does anyone happen to have any pictures or ways to reproduce the old highlighting, to compare?
Maybe this is related to [this](http://www.greghendershott.com/2014/11/github-dropped-pygments.html) ?
Interesting, it's the same implementation underneath, even though it's not quite solving the same problem! We could not use the HoleyMonoid library directly to solve the OP's challenge, because the library is meant for composing values with identical types: mappend :: m -&gt; m -&gt; m Whereas we want to combine function arguments which have different types. Yet all that is missing from HoleyMonoid in order to also support holey function arguments is an instance of IApplicative! So, to recap: 1. To support a holey version of function application, we need an indexed applicative typeclass and an instance for HoleyMonoid. 1. To support a holey version of monoidal composition, we need an indexed monoid typeclass (more widely known as the Category typeclass) and an instance for HoleyMonoid. Does the list go on? I could find at least one other case: 1. To support a holey version of monadic composition, we need an indexed monad typeclass and an instance for HoleyMonoid. instance IMonad Partial where ireturn x = Partial (\cc -&gt; cc x) ibind (Partial cc) f = Partial (\cc' -&gt; cc (\x -&gt; runPartial (f x) cc')) Anything else? What other composition mechanisms do we have?
Ok. See you there.
PragmataPro is the best typeface around. I don't regret buying it. Frabrizio is very responsive to user suggestions and requests. Haskell ligatures look amazing.
Ligatures for greek letters would be strange, because the width of the ligature would be a lot shorter than the text "alpha". PragmataPro already supports the full greek character set, you can set a font-lock substitution in your editor if you want it.
You support &lt;*&gt; but not *&gt;, and &lt;$&gt; but not &lt;$ and $&gt;. Also &lt;+&gt; but not &lt;+ and +&gt;. It'd be nice to see those supported.
Hi, I teach a lot of Haskell. Here's my guide for learning Haskell: https://github.com/bitemyapp/learnhaskell We have an IRC channel on Freenode called #haskell-beginners which is all about helping new people. If you're new to IRC [you can use this link](http://webchat.freenode.net/?channels=%23haskell-beginners&amp;uio=d4) to connect to Freenode's webchat. Here's a section on Cabal: https://github.com/bitemyapp/learnhaskell#what-are-haskell-ghc-and-cabal and some guidelines: https://github.com/bitemyapp/learnhaskell#cabal-guidelines
Thank you so much. Though I have a question, what is IRC?
http://en.wikipedia.org/wiki/Internet_Relay_Chat Way programmers have been chatting on the internet for ~three decades.
The correct solution is to semantically tag everything in HTML, so users install whatever style sheet they want.
If you're completely new to everything, you likely shouldn't be learning Cabal first. It's essentially a package manager for Haskell libraries, but the [Haskell Platform](https://www.haskell.org/platform/) has all the libraries you need to get started in one convenient package anyway. If you just install that, you shouldn't have to bother with Cabal for a while. As for beginners' textbooks, [Learn You a Haskell for Great Good](http://learnyouahaskell.com/) is one of the de-facto standard options.
You almost certainly can do that. Stackage Server actually has a feature I haven't explained yet, but plan on writing a blog post about in the next week or two: anyone can upload a custom snapshot. I've used that for providing prerelease versions of packages for testing, but it can certainly be used for the case you're talking about. The nice thing there is that you won't have to host your own Stackage Server separately (though you're obviously more than welcome to do so if you wish).
Neat! This looks like a serious usability improvement. That said, my brain trips over `with element_ [attributes]` every time, no matter how many times I see it. It might be the argument order -- "with attributes" is a cromulent phrase in English, "with element" not so much -- but flipping them would also be awkward. I can't think of any good alternatives aside from the probably-too-cute `a(n) element_ [attributes]`.
element_ \`with\` [attributes] reads pretty well.
Really impressive. First of all it's good to see how Stackage site becomes more and more user friendly. Hadocks, now all this information. README and changelogs! Also I am waiting for new sandbox features. Thanks for working on Stackage! You guys are making haskell-life much simpler! 
Could you have two suffixes? For example, `'` implies no attributes, but `_` implies attributes (or vice versa): page2 = html' $ do head' $ do title' "Introduction page." link_ [rel' "stylesheet", type' "text/css", href' "screen.css"] body' $ do div_ [id' "header"] "Syntax" p' "This is an example of Lucid syntax." ul' $ mapM_ (li' . toHtml . show) [1,2,3] Perhaps it would be good to have such a shorthand for such a common task. I don't know if this is really an improvement, just adding to the discussion.
I think flip-flopping between ' and _ is confusing. I like the way with reads actually, so maybe a good combination would be something like: page2 = html_ $ do head_ $ do title_ "Introduction page." linkWith_ [rel_ "stylesheet", type_ "text/css", href_ "screen.css"] body_ $ do divWith_ [id_ "header"] "Syntax" p_ "This is an example of Lucid syntax." ul_ $ mapM_ (li_ . toHtml . show) [1,2,3]
I agree, your suggestion is definitely better. That said, I still don't know if the shortcuts are worth it; would take a bit of experience to really figure it out.
Agreed, they might end up being nice, but even when compared to using the with function as an infix it really only saves you two \`s and a $ or some parens if there is a 3rd argument. 
I didn't, where can one use ligatures, so I can use that awesome font?
`with` should be an operator. Why not `!` like in blaze?
Exactly, I think resembling HTML read flow is a good enough excuse to introduce an evil operator. I see myself writing `(!) = with` whenever I import `Lucy`. Otherwise looks like a great readability improvement.
What's the GHC issue mentioned in the post?
This was discovered and fixed as https://github.com/haskell/haddock/issues/333
I was literally trying to build something like this last night! I have a plan to get rid of the extra operators though. Edit: looks like my plan was the same as yours (thought lucid was blaze when I skimread the article) 
I know some of it's outdated now, but I personally found Real World Haskell to be a lot easier to work through than LYAH.
Now I get an error building cabal2nix. Any hints? Linking dist/build/HsColour/HsColour ... Running Haddock for hscolour-1.20.3... Preprocessing library hscolour-1.20.3... haddock: unrecognized option '--package=hscolour-1.20.3' unrecognized option '--verbose' Usage: haddock [OPTION...] file... &lt;snip usage&gt; builder for '/nix/store/lbh2d4j131w2mifr793vrxdmbgh024va-haskell-hscolour-ghc7.8.3-1.20.3-shared.drv' failed with exit code 1 cannot build derivation '/nix/store/rg63anzcmsfxkx5q7sqa0rqacvydj1nz-cabal2nix-1.71.drv': 1 dependencies couldn't be built error: build of '/nix/store/rg63anzcmsfxkx5q7sqa0rqacvydj1nz-cabal2nix-1.71.drv' failed 
&gt; Now the feature has been merged to be able to specify remote-repo in the cabal.config file in your project root. Once this is released you'll be able to [..] Sorry to disappoint you but it has not been released yet. The next release of cabal-install should contain it.
No problem. I can build cabal-install if I want to.
Thank you for these kind words!
I'll add them just today
&gt; Hmm... Now that I've written it, I'm not sure. It's technically easier to type (no shift required, as with _), but I could see myself easily getting distracted with the close proximity to double quotes. I did exactly the same thing! I tried out both primes and underscores. I just needed a single character to differentiate HTML terms from non-terms, but like you concluded, while easier to type (saves a chord key press), primes at a glance are a bit more distracting.
Thanks for letting me know your considerations. I know it takes time in order that my design will become digestible :)
The prior work that I'm aware of is the [xhtml-combinators](http://hackage.haskell.org/package/xhtml-combinators) package. Definitely possible, I'm not sure about the usable part. I asked the same question back in 2010 at CamHac when Blaze was a twinkle in Jasper's eye but no one at the time seemed interested in the notion. I vaguely suspect a closed type family could nicely express parent-child relationships without the need for any actual classes, but I haven't fiddled with the idea yet.