Zippers are the solution to updates without massive tree walking. For the purposes of algorithms or data structures, zippers are analogous to pointers, giving you O(1) updates and O(1) local motions.
And setters calling setters on members is not "massive tree walking" how exactly? I don't see a big difference between the two paradigms here at all.
idk if this is really relevant but when i was doing some toy physics simulations i found taking the derivative of the 'world' data structure and using a comonad to work pretty well.
Note that `updateWorld` can be simplified to updateWorld objs = zipWith (update objs) [0..] objs
If I don't have to be correct, I can be *very* fast. It's the combination of the two that's harder than either alone. But it's easier to add performance than add correctness.
The definition of normalisation precludes non-termination.
Whatever happened to Harrop anyway?
Got it. Thanks!
&gt; special type “()”, pronounced “bottom” since when? &gt; “()” is a type that contains no values ditto 
Very interesting article and the blog overall. Especially liked the "Purely Functional Retrogames" series: http://prog21.dadgum.com/26.html
You can try accessing objects created mid-frame, all you need is good order on actions and some laziness. Just read from the frame you're producing.
Maybe I drank too much, but merely speculating, I think that guy get "()" wrong because of "IO ()", which many people think is equivalent to the return type "void" of other languages, when you "don't want to return anything". But he don't get the meaning of the Unit type, that is, "I don't care but I have to put a type, so I choose this dummy type that contains a single dummy value".
Small world! Yes performing with his roots band Rafiki next week.. 
There has been some work on supporting minimal definitions. That is at least a step in that direction. In theory you could define a _much_ more complicated system. I don't know that anyone will though, as it isn't clear that the benefit is worth the complexity.
Unconvinced that Haskell has a high up front cost. I find it the fastest language to develop in and it has great prototyping features (type signatures and undefined). It has a high cost to learn if you have spent 22 years mastering C, but I'm sure the reverse is true :)
any blog posts on this?
That was awesome!
We just have one, let's not be alarmists.
Thanks, corrected
Opps! My bad. Corrected! Anyway, there is something I didn't undertood in Pipes tutorial. It says: "The concrete type synonyms use () to close unused inputs and Void (the uninhabited type) to close unused outputs" (http://hackage.haskell.org/package/pipes-4.0.0/docs/Pipes-Tutorial.html#g:9) What's the rationnal for that? From the following diagrams, Void and () seems to be used much more liberaly... 
That's right, although I wanted to hide this kind of details, since the audience is not functional programmers (and non programmers at all for some of them)... How could I do that and stay correct?
Oh wicked, in Sheffield? I might go and see you.
what does "taking the derivative of a data structure" mean?
Do you know of a good comonads-for-complete-idiots tutorials? Something that arrives at comonads starting from a real-world problem preferably, rather than one of those "an xxx is defined as these Greek letters where blah blah are-you-asleep-yet" academic papers.
So if I undertand you use: * Void at the very end of the pipe-chain, where no more pipes can be chained, * () at a point of the chain where you want to say "no interesting data here", but you still want to chain other pipes (interresting data may flow in the other direction)?
If you think of polymorphic data structures as functions, then zippers are the derivatives of data structures, analogous to computing the derivative of the equivalent function. [This paper](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.8611) goes into the details. These zippers usually (always?) form comonads. I remember that I read a clearer exposition of this idea elsewhere, but I can't remember exactly where. Perhaps somebody else has an alternative reference they might recommend.
Are you thinking of this? http://chris-taylor.github.io/blog/2013/02/13/the-algebra-of-algebraic-data-types-part-iii/
Thanks, corrected!
I saw an interresting post on your blog about live coding and revision control: http://yaxu.org/cyclic-revision-control/
No, but that link is even better than what I remembered, so I prefer your suggestion. :)
Right! `Void` outputs cap each end of the pipeline and `()` is used to prevent interesting data from flowing in one direction while still allowing interesting data to flow in the other direction.
In a language like C, reasoning (even formally) about the amount of memory your program is consuming is quite straight-forward. Especially if you avoid dynamic allocations altogether.
so "taking derivative" in this context just means "creating a zipper for my data type", is that right? (btw, I read the post, thanks)
That's right. A derivative is just a way of deriving what the zipper for your type should look like.
I feel like a free monad would have been a better choice here.
I would combine the arguments into logically nested records. For example, "place to draw rect" and "size of rect" can be combined into a single `Rectangle` record. Then the `Rectangle` record can be combined with "colour" and "border colour" into a `ColouredRectangle` record. Then use lenses for easy access to the various components of these deeply nested records.
For toy examples that might be true. For anything involving any level of abstraction (e.g. not knowing what exactly a function you call does or a struct you use contains or how many elements a container holds) not so much. Note that I am not arguing here that that particular problem is easier to solve in functional languages but it certainly needs languages with a well defined execution, memory,... model. C is not one of those languages.
Something about hammers and nails.
Hammering nails is a side effect. With a free monad you could "purify" the core of your bookshelf-building procedure, letting you focus on the actual design, rather than on the details of hammers and nails. This more abstract representation would allow you to build a prototype using toothpicks, styrofoam and a fake plastic mallet.
I had records like this at first, but I found it awkward to extract the values. I split the Rectangle up in order to use that uncurry technique more effectively, but now that I think about it I don't need the Cairo.translate in that code at all. I've not used lenses before. I've read about them, but they were a little over my head. I get them a little more after trying now, but I can't figure out how they solve the particular problem of passing each of the many 'fields' over to the Cairo functions easily. Here's what I have so far: data Point' = Point' { _x, _y :: Double } deriving (Show) $(makeLenses ''Point') data Size' = Size' { _w, _h :: Double } deriving (Show) $(makeLenses ''Size') data Rect' = Rect' { _point :: Point', _size :: Size' } deriving (Show) $(makeLenses ''Rect') data RGBA' = RGBA' { _r, _g, _b, _a :: Double } deriving (Show) $(makeLenses ''RGBA') drawColoredRectAtPoint'' :: (Int, Int) -&gt; Rect' -&gt; RGBA' -&gt; RGBA' -&gt; Cairo.Operator -&gt; Cairo.Render () drawColoredRectAtPoint'' canv rect bg fg op = do Cairo.renderWithSimilarSurface Cairo.ContentColorAlpha (canv^._1) (canv^._2) $ (\s -&gt; do Cairo.setSourceSurface s (origin^.x) (origin^.y) Cairo.renderWith s $ do Cairo.setSourceRGBA (bg^.r) (bg^.g) (bg^.b) (bg^.a) Cairo.rectangle (rect^.(point . x)) (rect^.(point . y)) (rect^.(size . w)) (rect^.(size . h)) Cairo.fillPreserve Cairo.setSourceRGBA (fg^.r) (fg^.g) (fg^.b) (fg^.a) Cairo.stroke Cairo.setOperator op Cairo.maskSurface s (origin^.x) (origin^.y)) where origin = Point' 0 0 I'm using `^.` to access the fields here (you can probably guess my OO background), and it's clearly more verbose than what I'm doing with the generalized uncurry, which interestingly enough still works, basically unchanged, with the records I'm using for lenses. instance Uncurry (Double -&gt; Double -&gt; Double -&gt; Double -&gt; r) Rect' r where f -$- (Rect' (Point' x y) (Size' w h)) = f x y w h drawColoredRectAtPoint'' :: (Int, Int) -&gt; Rect' -&gt; RGBA' -&gt; RGBA' -&gt; Cairo.Operator -&gt; Cairo.Render () drawColoredRectAtPoint'' canv rect bg fg op = do Cairo.renderWithSimilarSurface Cairo.ContentColorAlpha -$- canv $ (\s -&gt; do Cairo.setSourceSurface s -$- origin Cairo.renderWith s $ do Cairo.setSourceRGBA -$- bg Cairo.rectangle -$- rect Cairo.fillPreserve Cairo.setSourceRGBA -$- fg Cairo.stroke Cairo.setOperator op Cairo.maskSurface s -$- origin) where origin = Point' 0 0 I assume there's probably something in the Lens library to do what I want, but I'm not sure where to look.
Hmm, that may be due to a cronjob I setup to delete everything in `/tmp/tryhaskell`, to cleanup the [mess created by GHC](https://ghc.haskell.org/trac/ghc/ticket/4114) when evaluating something results in a compile error (it makes a bunch of files that it doesn't cleanup). I've adjusted my cleanup script to only remove files older than one minute. Let me know if this problem occurs still.
Do you know about the `RecordWildCards` extension? It's a good way to "open" all the fields of a record into current scope. So one might do foo = callThingie Thingie { foo = 1, bar = 2, mu = "zot" } callThingie :: Thingie -&gt; … callThingie thingie@Thingie{…} = show mu ++ (foo * bar) ++ more thingie more :: Thingie -&gt; … more Thingie{..} = show (foo + bar) It's kind of like having first-class dynamic scope, you can choose to pass it on or not, and where to open it. I don't think a lens helps you read nested records, normal records are fine with that: `personName.customerPerson.entryCustomer` is plain old Haskell. Lenses help you _update_ nested fields, which it doesn't seem you're doing anyway.
 Type Haskell expressions in here. λ 2 Unspecified error. Have you installed mueval? λ getLine Unspecified error. Have you installed mueval? λ Oops!
Hey, I hadn't heard of that extension, that will probably come in handy later. It doesn't help here though, I'm kinda trying to do the opposite. I specifically don't want to open any of the fields of the records, but still pass their values to all of Cairo's functions. I can't think of a way to do that without uncurrying the records. One option would for me to wrap the entire Cairo API with functions which take records as arguments instead, which would simplify writing all my code, so eg: module WrappedCairo where rectangle :: Rect' -&gt; Cairo.Render () rectangle (Rect' (Point' x y) (Size' w h)) = Cairo.rectangle x y w h setSourceRGBA :: RGBA' -&gt; Cairo.Render () setSourceRGBA (RGBA' r g b a) = Cairo.setSourceRGBA r g b a ... Might seem a bit excessive to wrap the API like that, but I'm making hundreds, going into thousands of calls to Cairo functions, and it'd be much simpler and less repetitive to type `rect` instead of `x y w h` each time 
Looks good, hopefully I shall be there!
&gt; One option would for me to wrap the entire Cairo API with functions which take records as arguments instead, which would simplify writing all my code, so eg: ... This sounds like the way to go. It was my first thought when I read your introductory sentence: "I'm trying using Cairo and every function takes a bunch of arguments - there's no types to represent points, sizes, rectangles, colors and whatnot." RecordWildcards would probably make writing those functions nicer: -- here you have to fill in the '...' data RGBA' = RGBA' { r :: ..., g :: ..., b :: ..., a :: .... } -- here you do not. :) setSourceRGBA :: RGBA' -&gt; Cairo.Render () setSourceRGBA (RGBA' {..}) = Cairo.setSourceRGBA r g b a
Getting this too.
 Type Haskell expressions in here. λ 5 + 7 Unspecified error. Have you installed mueval?
I recently wrote my own FT2 bindings and OpenGL rendering code as well. Regarding the texture padding, you can tell OpenGL to use a different row alignment like this: GL.rowAlignment GL.Unpack GL.$= 1 In the end you'll have to pack the glyphs into a texture atlas, though. Switching textures is a fairly slow operation. I'm using a basic kD tree based bin packing algorithm with a simple leaf merging tweak to prevent the trees from becoming too imbalanced. Works really well!
Yes although my role is fairly background.. Doing this on Saturday at the Rutland though: https://www.facebook.com/events/1396417010610384/
Impossible given current design of ihaskell - not in general :) It should be pretty easy to stick my backend behind any front-end. It speaks an easy protocol via network, and should be accessible from anywhere. Should be ready to do it without the network, too. Shoot me an email - let's talk! (Email listed at end of that notebook.)
Why don't free monoids get as much love? :( or how about free semirings??? The real fun starts with the free category of a relation. That's where shit starts getting real.
Works like a charm. Thanks :)
Unfortunately, this popular metaphor is incorrect. It's important to understand the difference between a sequence and a series.
The advantage of the free monad representation is that you can output values while the computation is running rather than deferring all output to the end.
That is not _uniquely_ an advantage, one should note.
You can have multiple pointers at the same time, all of which sport O(1) reads/writes.
Correct. However, it is an advantage compared to the monadic list transformation which Chris advocates (i.e. anything of the form `[a] -&gt; m [b]`).
Right, but limiting your functionality to programs that immediately return is not really a solution to the problem.
 λ getDirectoryContents "." Directory not found: . hmm. 
I think that your generalised uncurry looks really good. It actually looks like an elegant solution. The only other solution that I can think of that would allow you to use a single name, but pass arbitary values into internal functions would be by passing partially applied functions into your program instead of tuples. -- types to talk about closures type C1 a = (a -&gt;r) -&gt; r type C2 a b = (a -&gt;b-&gt;r) -&gt; r type C3 a b c = (a -&gt;b-&gt;c-&gt;r) -&gt; r type C4 a b c d = (a -&gt;b-&gt;c-&gt;d-&gt;r) -&gt; r mkC1 a = \f -&gt; f a mkC2 a b = \f -&gt; f a b mkC3 a b c = \f -&gt; f a b c mkC4 a b c d = \f -&gt; f a b c d type Canvas = C2 Int Int type Point = C2 Double Double type Size = C2 Double Double type RGBA = C4 Double Double Double Double mkCanvas = mkC2 mkPoint = mkC2 mkSize = mkC2 mkRGBA = mkC4 infixl 1 (&lt;$) o (&lt;$) f = f o drawColoredRectAtPoint' :: Canvas -&gt; Point -&gt; Size -&gt; RGBA -&gt; RGBA -&gt; Cairo.Operator -&gt; Cairo.Render () drawColoredRectAtPoint' canv pt sz bg fg op = do Cairo.translate &lt;$ pt Cairo.renderWithSimilarSurface Cairo.ContentColorAlpha &lt;$ canv $ (\s -&gt; do Cairo.setSourceSurface s &lt;$ origin Cairo.renderWith s $ do Cairo.setSourceRGBA &lt;$ bg Cairo.rectangle &lt;$ origin &lt;$ sz Cairo.fillPreserve Cairo.setSourceRGBA &lt;$ fg Cairo.stroke Cairo.setOperator op Cairo.maskSurface s &lt;$ origin) where origin = mkPoint 0 0 I don't know how useful this is, or even how helpful it truly can be in practice. I wonder if this can be generalized with type-level literals in some way... EDIT: Disregard my types, I should not reason about types in my head while tired.
Which game engines do this?
I really fell in love with that particular extension. It's so handy for building complex records where you might need a lot of monadic code to initialize the fields. Or when you want to directly route function parameters into a result record. Also if you're using a Reader/State you can bring all the fields of it into scope, removing all those gets/asks calls. I can see why some people might think it of it as obscuring data flow etc., but it makes so many different things really terse.
Well, the monad used seems to be "too big", since you can do silly stuff like `unreadLine`.
There's an `Error "str"` defined in `instance Monad m =&gt; Monad (ResultT' m)`. It's worth also noting that `ResultT'` is a perfectly fine monad transformer isomorphic to `either`'s [`EitherT String`](http://hackage.haskell.org/package/either-4.1/docs/src/Control-Monad-Trans-Either.html)
Right now I'm trying to figure out the basics of glyph metrics. Even with the freetype tutorials it's difficult to see what the different metrics are, what they relate to and how to scale them. That's even before kerning. But eventually I see a pretty easy to use font rendering lib coming out of it.
I think chreekat's suggestion is the simplest path forward (though I kind of like tWoolie's too). I would suggest creating these functions as you find you need them. Also, give them the same name as the original function and define them in a separate module hierarchy that reflects the original hierarchy of the Cairo modules. (Perhaps even eventually send a patch to gtk2hs-users@lists.sourceforge.net, the cairo Hackage package maintainer.) If your wrapper definitions are all as small as I think they will be ---and also strict in the arguments that you're unwrapping --- then I expect GHC to eliminate the wrapper overhead. Good luck.
&gt; In a language like C, reasoning (even formally) about the amount of memory your program is consuming is quite straight-forward. Compared to what? I don't think it's nearly as simple as you're indicating. &gt; Especially if you avoid dynamic allocations altogether. Sure, but any complicated memory structure is going to use dynamic allocation, and that's almost always when you want to reason about memory consumption.
Not exactly the zipper, but the one hole contexts - `τ × τ' list` is a zipper, not just `τ'`.
Your advice about type-classes seems unidiomatic to me. What's wrong with just a record of functions?
You can have multizippers based on continuations too, according to Oleg.
... I'm really stupid. No wonder it sucked all the errors. TY.
Compared to reasoning about space use in higher level languages. GC makes things less tightly bound, lots of dynamic allocations everywhere make memory use complex. Laziness makes things much more complex to predict. Not sure what you mean by complicated memory structure, but our complex distributed storage controller has mostly some bounded memory pools and lots of data structures linking them up (intrusively). This allows us to do various kinds of mappings, and support some arbitrary level of concurrency, knowing exactly how much memory each unit of concurrency can take up in the worst case.
That let you do an arbitrary sequence of updates on each of those spots in O(1)? Color me skeptic :-)
If you need to have a precise bound on memory usage, sure, it's a good reason to use the "no dynamic memory in C" thing. Or if you want extremely high performance (in which case you might want to actually use dynamic memory...). Otherwise, I see no reason to use C over a functional language.
We do extremely high performance, which is a reason to *avoid* dynamic memory allocations (except for some O(1) allocations from memory pools).
I don't think it'll be 10% slower. IME, it's a factor of 2-5 slower. For many (most!) programs this is worth the ease and expressiveness. But for high-performance, long running programs, it isn't.
"holding it up as a model for how to do pure IO" is very different then "this is the best way to do it". Clearly Chris shows it as "a" model to do pure IO. And anyone who with knowledge of this being a suboptimal way hopefully enlightens all of us and should be thanked for that -- right? And "encouraging people to use his library" is different from "this is the best way to do it", and indeed he did not state it is the "best"; but announcing a new package on /r/haskell is a form of advertising it. Personally I did not know about IOSpec, and I must say that even after reading the descriptions of both packages I'm still in the dark of how they may ever be usefull for me.
&gt; Clearly Chris shows it as "a" model to do pure IO. And anyone who with knowledge of this being a suboptimal way hopefully enlightens all of us and should be thanked for that -- right? If they have a valid argument and they do it nicely (this is in the general sense, not talking about anyone above) then yes. I was just arguing that I'm not sure it should be held to such a high standard and one which I think the author did not intend, eg: &gt;&gt;The advantage of the free monad representation is that you can output values while the computation is running rather than deferring all output to the end. &gt;What do you mean by “while the computation is running”? The computation has to end and produce a result immediately, it's not a continuous Haskell runtime session. So by the sounds of it this is something that would work in a lot of cases. Sure, it may not work in all of them and at which point a free monad would be better, but I expect most of the time you don't need a library to tick all the boxes - just the ones you care about. &gt;but announcing a new package on /r/haskell is a form of advertising it. I don't think it's announcing it. I read it much more of a "this is how I did this" discussion blog post thing. Anyway, I expect I'm in over my head a bit and I wouldn't want to speak for the OP - he said after writing over 1000 characters about it. I admit it, I'm a terrible person. Next I'll be arguing against the all powerful free monad :P 
Right. I completely forgot about that.
See [Data types a la carte](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.4131) which is the paper behind the `IOSpec` library. I also wrote up a similar explanation [here](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html).
Not sure, it's been ages since I read the paper.
I installed yi--with the pango frontend--on Linux recently without difficulty. I can't speak to Windows, but on Linux installing yi with or without pango is probably a lot easier than it used to be.
I smiled,
Just return 0 every time. Not correct, but look at the performance!
This is a cool idea - thanks Chris! Why doesn't it work the first time? λ putStrLn "hi mom" :: IO () λ do line &lt;- getLine; putStrLn line &gt; hi mom hi mom :: IO () λ putStrLn "hi mom" hi mom :: IO () 
[Check this](http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html).
I've had a similar situation to yours recently, where I needed to represent Coords, Sizes, Colors, etc. I found a solution that I'm very fond of, using the lens and the linear packages. Linear has a very handy set of V(ector) types which make operations like pointwise arithmetic, pointwise application, etc. go very smoothly. They come with the R* classes, which provide lenses into each field. I've also written a couple helper functions to interact with the Cairo-style (or Gloss-style, in my case) parameterization of expecting rows and cols, or widths and heights, individually. All that said, here's my implementation of what you've written: {-# LANGUAGE FlexibleInstances, MultiParamTypeClasses #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} {-# LANGUAGE TemplateHaskell #-} import qualified Graphics.Rendering.Cairo as Cairo import Control.Applicative import Control.Lens import Linear newtype Coord' a = Coord' { _coordV2 :: V2 a } deriving ( Eq, Show, Num , Fractional, Epsilon, R1, R2, Additive , Functor, Applicative, Monad ) makeIso ''Coord' newtype Size' a = Size' { _sizeV2 :: V2 a } deriving ( Eq, Show, Num , Fractional, Epsilon, R1, R2, Additive , Functor, Applicative, Monad ) makeIso ''Size' newtype Canvas' a = Canvas' { _canvasV2 :: V2 a } deriving ( Eq, Show, Num , Fractional , Epsilon, R1, R2, Additive , Functor, Applicative, Monad ) makeIso ''Canvas' data Rect' a = Rect' { _rectCoord :: Coord' a , _rectSize :: Size' a } deriving (Eq,Show) makeLenses ''Rect' instance Field1 (Rect' a) (Rect' a) (Coord' a) (Coord' a) where _1 = indexing rectCoord instance Field2 (Rect' a) (Rect' a) (Size' a) (Size' a) where _2 = indexing rectSize newtype RGBA' a = RGBA' { _rgbaV4 :: V4 a } deriving ( Eq, Show, Num , Fractional , Epsilon, R1, R2, R3, R4, Additive , Functor, Applicative, Monad ) makeIso ''RGBA' type Coord = Coord' Double type Size = Size' Double type Canvas = Canvas' Int type Rect = Rect' Double type RGBA = RGBA' Double r2 :: (R2 f) =&gt; (a -&gt; a -&gt; r) -&gt; f a -&gt; r r2 f r = f (r^._x) (r^._y) r4 :: (R4 f) =&gt; (a -&gt; a -&gt; a -&gt; a -&gt; r) -&gt; f a -&gt; r r4 f r = f (r^._x) (r^._y) (r^._z) (r^._w) drawColoredRectAtPoint :: Canvas -&gt; Rect -&gt; RGBA -&gt; RGBA -&gt; Cairo.Operator -&gt; Cairo.Render () drawColoredRectAtPoint canv rect bg fg rator = do Cairo.renderWithSimilarSurface Cairo.ContentColorAlpha `r2` canv $ \s -&gt; do Cairo.setSourceSurface s `r2` origin Cairo.renderWith s $ do Cairo.setSourceRGBA `r4` bg Cairo.rectangle `r2` (rect^.rectCoord) `r2` (rect^.rectSize) Cairo.fillPreserve Cairo.setSourceRGBA `r4` fg Cairo.stroke Cairo.setOperator rator Cairo.maskSurface s `r2` origin where origin = 0 :: Coord The parts I'd like to point out are the Num, R[1-4], and Applicative instances derived for the types, as well as the 'r2' and 'r4' helpers. There are other nice instances you can derive for the types, like Fractional, Epsilon, Additive, Foldable, Traversable, etc. if you need them. The nice thing about the Num related instances is that it allows for nice arithmetic without the explicit unpacking and repacking into the record types. For instance, where you might write: a, b :: Coord -- defs for a and b c = Coord (xa + xb) (ya + yb) where Coord xa ya = a Coord xb yb = b if Coord were an instance of Num, you could instead write: a, b :: Coord -- defs for a and b c = a + b There are lots of nice things to do with the types from Linear, and they go very well with the tools from Lens. The end. 
Well, some algorithms lend themselves to GHC's optimizations more than others. Often, the effort required to get C-like performance from Haskell is more than the effort required to do the same in C. For larger composite systems, the trickery typically used in a language like Haskell is going to be more and more difficult to get the performance you want, at least in a guaranteed way. New versions of libraries and the compiler can more easily break some of the optimization magic than in a language with simpler operational semantics. &gt; the extra time it takes to call malloc can pay off by staying in the same memory range, making everything fit in the CPU cache I don't get this at all. How does malloc help things be in the same memory range? malloc implies pointer chasing and indirections. Memory pools imply locality.
&gt; 2014 Theme: Data Science with Functional Programming Great, that is exactly what I do! But there is exactly one talk whose title fits this theme. And looking at the [abstract](http://skillsmatter.com/podcast/scala/data-science-using-functional-programming), that talk has absolutely nothing to do with data science either. Maybe I won't go then.
There is a "standard" definition of "container" in the work of the "containers gang". c.f http://www.cs.nott.ac.uk/~txa/publ/cont-tcs.pdf for a start. Furthermore, such containers correspond to so-called "polynomial containers" c.f. http://mat.uab.es/~kock/cat/polynomial.html These discussions of containers are different (I think) than the containers discussed here. But the citation given at the end on "finitary containers" I think refers to the containers discussed above. To be more precise, the functors we can talk about in Haskell with the containers analogy (as given in the linked blog post) are precisely those that are "representable" (i.e. isomorphic to (R -&gt;) for some R). But there are many functors that are not representable! Here's an obvious one -- Maybe. Here's another obvious one -- (Bool,). etc. These two examples, along with plenty of others, _are_ finitary containers, but are _not_ representable. There are other functors that are neither containers nor representable. (edit: one easy example of this, if i recall correctly, is the cont monad for any given r: `(a -&gt; r) -&gt; r`. Since containers are strictly positive, this isn't a container in any of the literature cited above. It sure is a functor though!) The nlab has some discussion on the relationship of yoneda and representability: http://ncatlab.org/nlab/show/representable+functor 
I really don't think it's worthwhile to reimplement good code. Why not use what's already there, instead of bloating your own code and run the risk of getting it wrong? I agree, it's not as immediately clear what the types are doing. It takes a step over to Linear to know that it's doing the right thing. I think the solution is documentation, not code duplication.
I don't think I understand what you're using the tree for - locating the data structure that represents the character (for example, a data structure that holds the char's uv coords in the texture)?
I'm a bit bothered by the pedagogy of this post because of the ambiguity of "container". It ends up being defined as "things which have `fmap`", so it just ends up as some terminological noise. I think it's a great idea to talk about ideas of extraction, motivated perhaps by which Functors admit Comonad instances (or Copointed). This lets you separate Stream and Store from Reader, Maybe, List, Const. I also think it's a great idea to talk about representation and show that it implies `Comonad` and how Maybe and Const now differ since we can write `Const () ~ (Void -&gt;)`. I also love the Typeclassopedia build-up from Functor to Applicative to Monad. I just feel like the whole thing would be better served by trying to talk about when functors are or are not "container-like" instead of arguing that functors *are* containers (as long as you define containers to be things which are functors).
That's true, but there's also no `r` such that there is an isomorphism `[a] ~ r -&gt; a`. In fact, the only isomorphisms of that sort are precisely the ones with to `Vec a n` on the left, for some fixed `n :: Nat`, and in those cases, `R ~ Fin n`, or `Stream a` with `R ~ Nat` for the countably infinite case (I don't know about uncountable cases).
Yeah, and [a] isn't representable unless you treat it as an always infinite list. In that way it doesn't inherit the `Reader` `Applicative` and `Monad` instances—it steals them from Set. This is most clear (to me) in that `return :: a -&gt; [a]` makes a size 1 list. Representable tends to mean "fixed size" container.
This is why I don't know what /u/sclv intended by that statement, because it has very little to do with containers as a whole, it only relates to a small subset of containers at best.
I edited my post above to make clear I was referring to the containers analogy _as given in the post_. My point being that there is a general class of containers, but what the post discusses are all, as far as I can tell, representable functors instead.
It implies monad but not comonad. `Reader r ~ (r -&gt;)` is representable but unless you can create a trivial `r` you can't write copoint.
Aha. Well, thanks to the march towards dependent types, Haskell probably has real containers too. I wouldn't be surprised if some sort of data kind thing make it possible. You just need something like data Con (S :: *) (P :: S -&gt; *) where wrap :: (s :: S) -&gt; P s -&gt; Con S P I don't know if datakinds let you do that yet, or if you need singletons to get it to work or what. At the least, you could do it with SHE.
In the post I'm actually talking about *all* functors, representable or not. Yoneda provides a mapping from the Reader to any functor, but in most cases it's not an isomorphism. You can't "repackage" any container into the Reader.
The datatypes and method instances in Linear are usually one liners, so I feel they aren't worth losing clarity and pattern matching for (or nesting pattern matching another level down if you expose the newtype constructors). Datatypes are so lightweight in modern functional languages like ML and Haskell that I regard "DRY" as less of a virtue than "SWYM" (say-whay-you-mean - maybe not snappy enough as a phrase to catch on...). As you are using projection functions heavily in `drawColoredRectAtPoint` I'd also be concerned that the code is less efficient - *harder to read* and *slower* aren't trade offs I'd want to make for DRY.
That's a valid point of precision, and what we typically have is therefore the store comonad (R, R-&gt; a), but note I was following on from your casual aside above that we should "talk about representation and show that it implies Comonad".
I do mention IO in the post as a container that provides no way to retrieve its contents. But we know there's stuff inside it because we can fmap over it or use monadic bind to muck with it. 
In my experience, "`Functor` ~= containers" is a confusing analogy (as is the often accompanying "law" that `fmap` "preserves shape"). It just falls apart when you deal with types like `IO` or `Parser`. But perhaps a better analogy is that `Functor`s are **producers**. This really just amounts to `Functor` being a *covariant* functor—`fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b` does is connect the *input* side of the function to the `a` in `f a`. Under the analogy: * `IO a` is an IO action that produces a result of type `a`. * `Maybe a` in a producer of zero or one `a`s. * `[a]` is a producer of zero or more `a`s. * `Parser a` is a parser that produces zero or one `a`s. * etc.
I think that's an imprecision. You've described yoneda quite clearly before, so I hesitate to say "misunderstanding" :-) Yoneda says that (forall a. (e -&gt; a) -&gt; f a)) =~ f e. So if we rewrite that to make the reader clear, its saying: (forall a. Reader e a -&gt; f a) =~ f e. So any f embeds into the functor category of natural transformations into it, sort of. But that isn't a mapping from reader to any functor! That says that a _natural_ mapping from reader to any `f a` is isomorphic to that `f a`. Its not a statement _about_ representability, but rather a statement _using_ it.
Oops! I forgot I wrote that exact same thing while thinking the same thing. I think the whole idea became more and more precise in my mind as I was writing that response.
Except that there's not stuff inside it. That would be like claiming `ls` contains a list of files, which would be kinda silly.
ls is a lazy list of files
While I like calling them "producer" when talking about contravariant functors and profunctors in particular, I have to point out that this intuition fails, too. If you try to interpret it naturally as the idea that we can "get out" an `a` from `f a` then we're sunk for empty lists, `Const`, `IO`. I usually extend it to the idea that a `Functor` is a type which can produce a value, at least within a context. I.e. we're not necessarily going to see `extract :: f a -&gt; a` but the functor at least knows how to interpret a continuation of such a function. That's just what `fmap` is fmap :: (a -&gt; b) -- Counterfactually, *if* we had an `a` -- this is what we'd do with it -&gt; (f a -&gt; f b) -- The Functor *interprets* this counterfactual -- and reflects it in the type Thus we think of `Const` as the functor which just ignores your continuation. `IO` the functor which runs a bunch of continuations in the RTS and `fmap` introduces a certain kind of continuation step. Lists are the functor which runs your continuation in each counterfactual, non-deterministic world. So, functor lets you interact with types "as-if" they were containers. When they *are* containers you might have `extract`. When they're not then something more exotic might be happening. This also has a nice tie-in with the representable functors since then we're just chaining continuations as functions.
Overall, a pretty good year for Haskell. This will likely be my final year of recording/predicting/judging SoCs. I don't do very much Haskell programming these days and I'm no longer very in sync with the community, so I am both less interested in SoCs than I used to be and less able to subjectively judge their value. There also does not seem to be any particular interest by many people in the topic, so there's no reason to force myself to continue doing it (as I had to this year, delaying initial recording until June or so, much longer than I should've). I will probably do a last update next year to finalize all judgments, perhaps do a logistic regression to see if Cabal &amp; GHC really are as dangerous as my gut says, and deal with broken links, but otherwise I'm done. (I think the SoC mentors should at least be tracking projects similarly to my page, but if they don't want to, that's their business.)
Hah, that reflected my thoughts exactly (except that I don't currently do data science in Haskell, just would like to again some day).
&gt; While I like calling them "producer" when talking about contravariant functors and profunctors in particular, I have to point out that this intuition fails, too. If you try to interpret it naturally as the idea that we can "get out" an `a` from `f a` then we're sunk for empty lists, `Const`, `IO`. I don't share that intuition. (Of course, this is yet another problem with analogies!) To my mind, "producer" does not imply that the consumer can "get out" items whenever it chooses, nor a guarantee that an item will be produced. For example, in UIs frameworks we often conceive of the keyboard as a producer of keypress events. There is no implication that the consumer has a guaranteed way of getting out an event from such an event stream—the user may never press a key!
How about using the symtab as the state of the AST traversal? See State monad.
I know it's silly, but it's a defensible kind of silliness. From the point of view of language semantics, the contents might as well be sitting there inside IO, since we have no way of verifying its existence programmatically. And we are not trying to provide semantics for the external world where the actual interaction takes place. 
Before the stuff you're asking about, think for a second about what a symbol table does: it maps objects that are equal by content to the same object. I.e., it turns content equality into reference equality. So here's the first problem: Haskell doesn't have reference equality! So a symbol table that maps strings to interned strings is not possible. The closest you can get is to map strings to some *other* type `t` in such a way that equal strings always map to the same `t`, and `t` supports a cheap equality check. The [`Data.Unique`](http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-Unique.html) from `base` looks like a reasonable candidate for `t`, but you could get away with simpler types like `Int` if you like—as long as you can guarantee that no `Int` will ever be used for two non-equal strings. A symbol table in an imperative language is often implemented as a mutable hash table that is updated as you go along. A typical implementation might look like this (pseudocode): class SymbolTable { private HashTable&lt;String, String&gt; table; public String intern(String str) { if (table.contains(str) { return table.get(str); } else { table.put(str, str); return str; } } } In Haskell you can simulate this with a `StateT (Map String Unique)` monad transformer. Pseudocode: {-# LANGUAGE GeneralizedNewtypeDeriving #-} -- Encapsulate the symbol table and state monad transformer -- inside a SymbolT monad transformer newtype SymbolT m a = SymbolT { unSymbolT :: StateT (Map String Unique) m a } deriving (Functor, Applicative, Monad, MonadTrans, etc) runSymbolT ma = runStateT (unSymbolT ma) Map.empty intern str = SymbolT go where go = do table &lt;- get case Map.lookup str table of Just unique -&gt; return unique Nothing -&gt; do unique &lt;- lifIO newUnique Map.insert str unique table put table return unique I haven't tested this or worked out the types—but you're going to need the `MonadIO` class somewhere for the `Unique` type if you choose to use it (if this is too bothersome, you can probably get away with using an `Int` counter in the `StateT` monad).
Sorry for not being precise. What I mean is that there is a natural transformation from reader (which is really Hom(e, a)) to any functor f. If this natural transformation went both ways, then f would be representable. 
&gt; And I'll re-emphasize the `Const` functor since it's guaranteed to never actually "produce" a value of its type parameter. It merely accepts and responds to our counterfactual wishes that it did. In my experience, people don't really have a hard time understanding limit cases like that one when you point them out. There are similar things everywhere in software—`/dev/null`, the [Null Object Pattern](http://en.wikipedia.org/wiki/Null_Object_pattern), etc. What's more difficult is getting many people to see these as useful tools instead of curiosities. For example, I was surprised the first time I saw how the `Const` functor was used to implement lens getters from the `forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t` type...
Some of those project proposals you evaluated must be quite old. CUDA backend for DPH seems like a terrible idea.
Yes, that one was first proposed 6 years ago, according to the bug report timestamp. But two years ago, you thought it was ["unnecessary"](http://www.reddit.com/r/haskell/comments/fid5w/haskell_summers_of_code_retrospective_updated_for/c1gutlo), not "terrible". What's changed?
Well the Yoneda lemma (in this Haskell version) just says _if_ we have such a natural transformation (and we can have many!), _then_ that is isomorphic to f. It doesn't give any particular natural transformation. What is "the" natural transformation given by Yoneda from `Reader e a` to `Maybe a` for example? And for what `e` does it hold? I know you can give me _a_ natural transformation here, but I can give you _lots_ of them. What about a natural transformation to `(Int,a)`? Why should we pick any int in particular (but of course we _must_ pick one!).
Traverse the tree, build a symbol table as we go along, and use something like `execState` to get it back?
The question here is whether your code is a derivative of pHash or an original work. The usual standard is to ask if your code is within an ["arm's length"](http://stackoverflow.com/questions/1394623/can-i-dynamically-call-a-lgpl-gpl-software-in-my-closed-source-application/1394867#1394867) from the other code: if you need to know about specific data structures or internal state of the code, or if your code is so entangled to the other that they together are effectively the same software, then your code is a derivative work. If you merely use public interfaces of a separate software, your own software is not a derivative. Anyway, it's widely believed that if you link a library in your program, then your work is a derivative work of this library (even though there are interesting exceptions - for example, if there are many libraries that implement a common interface, then writing software to comply to this interface is not a derivative work of any such libraries). I don't know how well this was tested in court. Anyway, that's why you need to license your whole program as GPL if you happen to use a single GPL library. This is not the case with LGPL, however, because it explictly permits you to link even if you have a different license. That's why the FSF [recommends library writers to release them under GPL](https://www.gnu.org/licenses/why-not-lgpl.html), so that only GPL software would benefit from them. I suspect that any program that links against pHash must be released under GPL. That's the whole point of having a GPL library (as the link I provided says). If the whole software needs to be under GPL, then your bindings needs to be under GPL, too. That is, even if you were able to release them under MIT, it's not feasible for your users to take advantage of MIT license - they must use GPL for everything, even for your hypothetical MIT-licensed code. The exception would be if there were multiple implementations of pHash. (Being GPL-compatible means that when combined with GPL, you can relicense the code to GPL. In practice this means that anyone using your library - or any other MIT library - must use your code as if it were GPLed, to fulfill the requirements of pHash) I can't comment whether it's *legal* to release your code using MIT, that is, whether your code isn't a derivative work. An interesting question is to know whether your bindings were automatically generated by an algorithm (like many OpenGL bindings, Qt bindings, GTK bindings, etc), or were hand-crafted. In the first case you probably can't even claim copyright over the bindings (since they were just mechanically transformed from the original code) and thus can't license it, but in the second case you have, althrough I don't know if it's derivative. Frankly I don't see a point in releasing the bindings using the MIT license.
Another thank you for your contributions. They are highly appreciated. I for one would be interested in statistical analysis of the dangers of GHC/Cabal contributions. Hopefully someone will take over to work on a SoC 2014 retrospective as well. These things are useful and a nice compilation of ideas whether they are completed or not.
Hooray! Finally an event on a weekend! I'm studying abroad this semester so I'm finally able to go to events like this, but so many of them are mid week and it conflicts with classes.
MIT is my default. Simple as that. My (possibly misguided) view of the MIT license is that its the i-dont-give-a-shit-just-don't-take-credit-for-my-work-as-your-own license. The only thing I remotely care about with most of my open source software is that my effort is not completely erased, so MIT seemed to fit. Since I've seem some compelling evidence that MIT was actually the wrong choice I have re-released the package as GPL.
Travis CI is the best
Under OS X, maybe `brew switch ghc &lt;version&gt;`?
To be clear, I love illustration by analogy. I would absolutely use some of the things from the earlier post, and I love the violin-container intuition laid out here. What I don't like is "are". Functors *are* none of these things. Functors aren't even anything—`Functor` is a class, some types instantiate it, and `forall f . Functor f =&gt; f a` is a very strange thing that's kind of close to what we're all talking about. `Reader r` absolutely *is* a computation. `Identity` *is* a "container" by anyone's intuition. `forall f . Functor f =&gt; f a` is like a computation in that you can use `fmap` to post-compose some post-processing. `forall f . Applicative f =&gt; f a` is like a computation where you can line up a few independent computations sequentially. It's also like a container with velcro on each side that we line up in a row. `forall f . Functor f =&gt; f a` is like a container where you're allowed to transform the innards if there was anything inside at all. Every time I've seen someone derailed about these concepts it begins with treating Functor as a noun, continues to holding on too tightly to a favored analogy, and finishes up in a tremendous headache trying to ask the question "what *is* a Functor?!". This isn't even the right category of question to ask; it's nearly a grammatical error. To me, the only step forward is to say: List, Identity, Const, IO, Map, Set (kind of), Reader, Compose, Product. See how while they're all different they're all kind of the same..?
Wow, it's true what they say, knowledge is a curse. I had no idea people could make that kind of mistake! But if the target audience doesn't know the difference between a typeclass and a concrete datatype, will saying that "concrete instances of Functors are *like* containers" really be less confusing than the shorthand "Functors are containers"? Maybe the issue is that there is too many "Monads are burrito" tutorials and not enough "typeclasses are meta-burrito" tutorials.
Does GSoC awards make Google one of the major sponsors of open source Haskell?
"Functors are a container" creates a difficulty for learners trying to understand `(-&gt;) r`. It is a harm to make such a claim without immediately sharing a concrete exception to keep the learner's mind open.
That's lesson 2 :)
I think there's something a little challenging in the concept of typeclasses and constrained types which takes a little time to work its way into comfortable intuition. There are a bunch of programmers who aren't even familiar with interface/implementation, too, which makes it an even further jump. I think language like "Monads are Burrito Factories" does tend to lead to more confusion. I think it's fine among experts to say that Functors are Containers and speak loosely about the universally quantified type, but it's worth noting that it's an abuse of language and can lead to a category fallacy when you're first learning.
I thought contravariant functors are analogous to "consumers", not "producers". Or you meant than "producers" is helpful when we are discussing covariant and contravariant functors at the same time , and want better imagery than "variance". 
&gt; One last step, harder this time. What is the container interpretation of a contravariant functor? Try to figure it out. It's tricky, but you can do it! &gt; Gave up already? Here is a hint: think of a violin case. Is it still a violin case when you remove the violin? Why? Aha, now you're thinking with containers! Yeah, no idea. Also I didn't understand the violin analogy. :(
The latter.
I'm not sure what it means to be a container, but having kind `(* -&gt; *)` is certainly insufficient. newtype Endo a = Endo (a -&gt; a) `Endo` has no `Functor` instance. We could even think of it as a kind of container of Monoids extractish :: Monoid a =&gt; Endo a -&gt; a extractish (Endo f) = f mempty
&gt; Haskell is probably the wrong choice for productivity and workflow integration in this area as a newcomer as well, this has been a very sad fact to me. it is has caused me to be hesitant on who to commit to: haskell or f#. f# has all the goodies for this type of stuff ready to roll, but haskell has a more alluring syntax. f# has units and objects, but haskell has nicer function type definitions. and so on. all haskell really has going for it in this area, to a newcomer like me, is the chart and diagrams libraries. but good luck getting these installed as one is based on cairo and diagrams doesn't install on windows (last time i tried). but even with that, going through the diagrams tutorial, it seems the writers were more inclined to show off rather than teach the library. i don't find the syntax of the api all that great, at least what i've been presented with so far, and i find myself just wanting to go draw diagrams in processing or something else. as an example to back my claim up, "Almost everything is based around the concept of monoids" is right at the start of the tutorial with nothing more said. so, why should i care if i even know what a monoid is? the thing is, the diagrams library should be something of a welcoming party for people to haskell, as this type of post isn't the first of its kind. i've even asked similar questions, and there have been plenty of others. by the way, this isn't directed towards you, but the comment i quoted got me thinking about this again. :)
What is a functor? A structure preserving map between categories.
A contravariant functor in the computation interpretation is one whose type defines the input rather than the output type. You can only put something of a particular shape into the computation. So by that reckoning the container interpretation defines the type it holds - like a violin case can only hold something violin-shaped.
Cool, thanks!
High performance OCaml relies mostly on the imperative subset of the language. That is, no closures, no functional goodness. You are talking about memory allocation, so this isn't directly relevant, but I just thought to add this.
Hi, Gwern Haddock patches have been applied and now in HEAD: http://www.haskell.org/pipermail/haskell-cafe/2014-January/112207.html
`State s` is definitely not a container, at least.
This is all becoming very abstract, almost as abstract as the actual definition of `Functor`, but without any precision at all. On #haskell merijn gave me the analogy that functors are producers of output. I like this one since it sounds like informal language, but it doesn't actually have an informal meaning and so I can't see any sense in which it could actually be proved "wrong".
An implication is a logical proposition, like `(a -&gt; b)`. Entailment, `a |- b`, is something that you can either prove or dispove, but it's not a proposition in your logic. Basically it says that from the proposition a (your premise), you can prove b. This is a statement about your proof system itself. For example, in classical logic you can prove that `a, a -&gt; x |- x` using the [elimination of implication](http://en.wikipedia.org/wiki/Modus_ponens). Note that in many logics you can move a premise "through" entailment, so it appears in an implication in the other side (I forgot the name for this property...). So if `x |- y` (that is, you can prove y using x as a premise), then `|- x -&gt; y` (with no premises you can prove `x -&gt; y`)
The problem is that Haskell `Functor`s are a very special class of category theoretical functors (given that they're endofunctors on Hask, a very special case of a category). One wonders if there's a simpler definition (or explanation) of just `Functor` (not functor).
&gt; `forall f . Functor f =&gt; f a` I don't understand what you're saying with this. It seems this is equivalent to `forall b. b` (using the `Functor` `Const b`).
I thought arrows were like burrito factories. I'll never learn.
It's only getting easier. And if the course has a fixed budget of time and money, I think it's spent a lot better if you concentrate on the stuff that the students wouldn't (or even couldn't) pick up on their own.
It's only getting easier. And if the course has a fixed budget of time and money, I think it's spent a lot better if you concentrate on the stuff that the students wouldn't (or even couldn't) pick up on their own.
shameless plug that seems related: https://github.com/hvr/multi-ghc-travis :-) (this handles only the Ubuntu/LTS/x86_64 arch though)
&gt; Personally, I find those analogies very useful, as long as you know exactly which parts of the analogy apply. This is why analogies are bad. The same is true of "object inheritance follows an is-a relationship. Provided you understand the LSP and know that more cases of is a are not valid subtypes than are". To pick a common analogy every programmer knows is misleading. There are two problems with this: 1. The analogy usually has more exceptions than relevant cases. Certainly true in our OOP analogy of the is-a relationship. A square is a rectangle but certainly is not a subtype. A timetable is a list but certainly is not a subtype of list (my time table is sorted, a list allows arbitrary insertion). 2. You need to understand what the analogy is trying to hide before you can actually safely use the analogy. If I need to understand the LSP before I can safely think about when is-as are not subtypes then why not just teach me the LSP. I find this is true of Haskell. Most analogies are misleading and can only be properly grasped by people who don't need the analogy. I found this with Monad. I found it with Lens. A Functor is a construction that maps between categories. I'm not sure what is hard about this? This is a very simple concept that is made complex because of our language.
It seems a GSoC project which involves modifying ghc has the odds stacked against it?
Really? I've seen advice isomorphic to "use a free monoid" quite often when somebody writes a needlessly recursive function that could be expressed as a computation over lists. 
Well don't forget that Microsoft research support a large amount of the development of one popular open source Haskell implementation.
I was following parent post's use of "finitiary container" to be more precise about "container", with the caveat that I don't know technically what a "finitiary container" is :-|
With full admission that I am moving the goalposts and still not being technically precise, `Either a a ` seems to be covered by a further "natural" extension of representability, generalizing `r -&gt; a` to `r -&gt; (a)^n`. That of course may introduce other complications/problems/contradictions.... 
&gt; something violin-shaped. Like a Tommy gun.
I'm fine with that so long as you translate it to Haskell as "a functor is a `Functor` implementation, a specific choice of `fmap`". At this point we can separate the conversation from "what is a Functor" into "what are the properties of relating categories such that Functors exist?" I think this is useful pedagogically, but hard to motivate. Nobody wants to learn so much new vocabulary and concept just to figure out why I used `map ~ fmap`. Edit: Further, categorical definitions are very higher order (and then, later, very very higher order) which tends to be a little bit of a weird step when you first see it if you're not mathematically inclined. I'd argue that it's a super valuable weird step to take for anyone who likes working with formal systems, so maybe it's worth biting that bullet.
I mean the universally quantified type. The two best ways to see it are by packaging it in an existential so we can view it positively data AFunctor a = forall f . AFunctor (Functor f =&gt; f a) Or by thinking of writing a function that handles a higher-rank type. In any case, we end up having to manipulate an object for which the only thing we know is that it instantiates `Functor`. Intuitively we can think of it as the limit sum of all types which implement `Functor`. I bring it up because it's the very notion of a single type that "blurs" together all the properties of Functor instances. I feel it's the implicit target of a lot of "Functors are X" style arguments. It's always tempting to analyze just some piece of it forall f . (Functor f, IsAContainer f) =&gt; f a and then talk about how that smaller piece is less abstract and has better defined intuition, but it's worth being precise to say that the intuition will only work over that fragment of the true quantified type. Which is all really complex for a beginner, so I just avoid treating Functor as a noun.
Is your `AFunctor a` inhabited for any `a`?
[I'm pretty happy calling a type that instantiates `Traversable` a finitary container](http://r6.ca/blog/20121209T182914Z.html). If you look at the Coq proof they ended up defining a Finite Container (FinContainerSpec) to be a type which has a shape and a cardinality. class FinContainer f where type Shape f card :: Shape -&gt; Int Then they prove something that might mean `Traversable` and `FinContainer` are interderivable, but I'm not certain that's a valid interpretation.
It's inhabited for specific `a`s; they're not quantified. &gt; :t AFunctor (Just 3) AFunctor (Just 3) :: Num a =&gt; AFunctor a &gt; instance Functor AFunctor where { fmap f (AFunctor fun) = AFunctor (fmap f fun) } &gt; :t fmap (flip replicate ()) (AFunctor (Just 3)) fmap (flip replicate ()) (AFunctor (Just 3)) :: AFunctor [()] It just means we've forgotten everything about our type constructor except that "it's a Functor". Now we have to operate over the universal sum of all instances, or just give up and use their common interface. class Functor c =&gt; IsAContainer c where extract :: c a -&gt; a data Store r a = Store r (r -&gt; a) deriving Functor instance IsAContainer (Store r) where extract (Store r ra) = ra r data AContainer a = forall c . IsAContainer c =&gt; AContainer c instance Functor AContainer where fmap f (AContainer c) = AContainer (fmap f c) instance IsAContainer AContainer where extract (AContainer c) = extract c &gt; extract $ fmap (+ (2 :: Int)) (AContainer $ Store 3 (+1))
I am interested in applying for Summer of Code this year, what sort of pre-requisites are expected for haskell projects? What is the best way to prepare?
[TV Tropes warning](http://tvtropes.org/pmwiki/pmwiki.php/Main/SenselessViolins)
Interesting, it has lots of inhabitants but you can't do anything with them besides `fmap` them.
Yep. This comes up all the time in lens since you'll often be working with functions like `(forall f . Applicative f =&gt; a -&gt; f a)`. So when you define your lens you have to write foo :: Functor f =&gt; (b -&gt; f b) -&gt; a -&gt; f a leaving `f` as the universally quantified type. When you write your `get`, `modify` pair you pick specific Functor instances. get :: ((b -&gt; Const b b) -&gt; a -&gt; Const b a) -&gt; a -&gt; b get le = getConst . le Const modify :: ((b -&gt; Identity b) -&gt; a -&gt; Identity a) -&gt; (b -&gt; b) -&gt; a -&gt; a modify le f = runIdentity . le (Identity . f) And then we depend on `(forall f . Functor f =&gt; (b -&gt; f b) -&gt; a -&gt; f a) &gt; ((b -&gt; Const b b) -&gt; a -&gt; Const b a)`. Subtyping in Haskell!
Right, `a` isn't the contained object. `a -&gt; a` is the contained object, `a` is just the variable part of the type. Now that you remind me, that's the example that originally really drove home for me the reality that Typeclasses aren't always containers (which is even more of a common and more incorrect beginner's intuition than "Functors are containers") If you think of `mempty` values as the contained objects of type `a`, you still need a's Monoid constraint in there, which cannot be stated on Endo itself due to kind mismatch. I bumped into this sort of thing recently in a program, and learned about type families and the mono-traversables Hackage package (including MonoFunctor) With MonoFunctor and `(Monoid a) =&gt; Element (Endo a) = mempty::a` (not then actual syntax, but I think it might work in a type class instance block) ...then you get the Functor back because you aren't focusing on the `a-&gt;a` function anymore, you are back to focusing on the value type (and pointing at just the one value `mempty::a` per "contained" type, with the endomorphic function hiding in the typeclass instance dictionary) I should stop handwaving :-| This is why we should build intuituon from the technically precise definitions and a broad set of examples, not force preexisting metaphors onto new definitions... 
I tend to like the "plug adapter" analogy, which is very close to the violin case. If we have WallOutlet American we can install American-style plugs. If we don't have any, we'll need an adapter like `adapt :: European -&gt; American` and then we can contramap it into place. contramap adapt :: WallOutlet American -&gt; WallOutlet European Of course this sort of intuition begs for plug :: Plug ty -&gt; WallOutlet ty -&gt; Connection But that's not implied by Contravariant at all
In many ways, yes. The mentoring payments provide the vast majority of what we use to run haskell.org. This was the first year we were actually set up to [take donations](http://www.haskell.org/haskellwiki/Donate_to_Haskell.org) from other sources through SPI, though, so we'll see how it balances in the future.
This struck me as a very nice variant on Martin Escardo's style.
Something I never understood. A mapping is supposed to be kinda like a function, that is, a relation between the origin category and the destiny category. Why does it that mean in Haskell that you need `fmap`, but no `return`? And, in the general case, how do natural transformations make sense? A function between functions? (I understand Haskell functors, but not really categoric ones).
I was about to say "Wow. Such topology. Many compactness."
Thanks for the retrospective, I think Haskellers definitely appreciate this. I think you can increase the accuracy of your reporting and reduce time spent on this by contacting the project mentors/students directly rather than googling.
Functors in category theory consist of two parts. One maps objects in the category `C` to objects in the category `D`. The other maps arrows `f : A -&gt; B` in C to arrows `map f : F A -&gt; F B` in D. In category theory, map is usually denoted `F` just like the action on objects. Sometimes these two components are called `F_0` and `F_1`. The other requirement is that category structure is preserved, which is where `map id = id` and `map (g . f) = map g . map f` come from. For `Functor` in Haskell, both categories are the same. Natural transformations from `F` to `G` are families of maps `phi_A : F A -&gt; G A`, and the structure that functors have is mapping, so to preserve that, `phi_B . map_F f = map_G f . phi_A`, where `f : A -&gt; B`. Mapping followed by transforming is the same as transforming followed by mapping.
Sorry for the delay - new motherboard issues keeping me offline. What's wrong with "just a record of functions"? Maybe nothing, I don't really have the Haskell experience to say. At first sight it sounds like you're asking me to manually maintain my own vtables when an existential type would do it for me, making me suspicious that I'd get the same basic problems just hidden behind additional complexity, but I don't have the experience to say whether my first impression makes sense. Maybe I'm just being an idiot. 
The article starts at page 21. I could not find the source code anywhere, so this is just a copy crustulum of the article https://github.com/MasseGuillaume/TypeLevelInstantInsanity
Your intuition is correct: for the functor to be complete, we indeed need to go from `a` to `f a`, that is from objects in the first category to the objects in the second category. However, the objects of the category Hask are types, not values. `return` is a function which maps a value `x :: a` to a value `return x :: f a`. This is not what we want. Instead, we want something to map the type `a :: *` to the type `f a :: *`. This is achieved by the type constructor `f :: * -&gt; *`, not by `return`.
So you are addressing more the issue of "how to make the symbol table work" than "where does the symbol table live", right? The reason why I ask where it lives first is because if I want a structure external to the AST, then I need to figure out a way to distinguish between different `Var` nodes (`Var "a"` at one point and `Var "a"` at another point are equal, but I want to consider their identities); I thought maybe storing the path to the identifier would be a solution. If I store the symbol information directly into the AST, I could simply look up the symbol information at a particular node; this could take more space, but with persistent data structures, maybe it's a workable solution?
&gt; Personally, I find those analogies very useful, as long as you know exactly which parts of the analogy apply. I find analogies very useful as well. But, I treat analogies critically, and "functors = containers" just doesn't pass my test. Here are the problems: 1. It breaks down for many very important functors, like `IO` or parser combinators. 2. As I mentioned in the previous discussion, I suspect "functors = producers" is a strictly superior analogy. It looks to me like it has all of the virtues of the containers analogy, and at most a proper subset of the failings. So I'd argue that even under your premises, containers are just not that great of an analogy for functors. I'll add that I recall being completely mystified some 5 years ago at what the heck Haskell people meant when they referred to monadic values as "computations." So I don't think highly of that one either.
This tripped me up when I first learned the concept as well. The mapping of objects is just saying that it takes the type `a` to the type `f a`, not that there exists a morphism between `a` and `f a`, after all in general `a` and `f a` don't have to even be in the same category Re: naturality its a little bit stronger than a mapping from the image of `f` to the image of `g`. This mapping is required to be "natural" such that if you were to map from `A -&gt; B` under `F` going from `F A -&gt; F B` before applying the transformation `phi_B :: F B -&gt; G B`, or map `A -&gt; B` under `G` going from `G A -&gt; G B` _after_ transforming with `phi_A :: F A -&gt; G A`, you have to get the same answer. This condition is pretty strong.
Oh, conditionals vs. sequents. Got it. The broader lesson here is that the vocabulary and notation for logic is very parochial. For example, I was taught that *entailment* is the semantic relation of logical consequence (`a` entails `b` iff `b` is true in every model that `a` is), not the proof-theoretical concept you've just described.)
&gt; So you are addressing more the issue of "how to make the symbol table work" than "where does the symbol table live", right? If I understand you right, both. The symbol table lives as the implicit state of a `StateT` monad layer, encapsulated with `SymbolT` as a `newtype`. Computations that use the symbol table use monad transformers with `SymbolT` somewhere in the stack. I don't know how much you know, but well, this answer requires some basic understanding of the following two things: 1. The state monad 2. Monad transformers
Oh, that's because in the notation there are two turnstiles, one is the [single turnstile](http://en.wikipedia.org/wiki/Turnstile_%28symbol%29), ⊢ for synctactic consequence, which is what I described: a ⊢ b if you can derive b from a (I learned it using [natural deduction](http://en.wikipedia.org/wiki/Natural_deduction), so: a ⊢ b if there is a derivation tree where b is the root and all open leaves are a) And there is the [double turnstile](http://en.wikipedia.org/wiki/Double_turnstile), ⊨, which is semantic consequence: a ⊨ b if b is true in all models where a is true. Indeed in classical logic those two *different* notions of [entailment](http://en.wikipedia.org/wiki/Logical_consequence) are equivalent, because classical logic is [sound](http://en.wikipedia.org/wiki/Soundness), which means that if a ⊢ b then a ⊨ b, and it is also [semantically complete](http://en.wikipedia.org/wiki/Completeness), which means that if a ⊨ b then a ⊢ b. Notice that we are using "if .. then .." in statements *about* the logic, so the completeness / soundness are metatheorems of the [metalogic](http://en.wikipedia.org/wiki/Metalogic). And of course a powerful logical system might not be complete, in that we won't be able to derive *everything* that is true (but we would like to work with sound systems, because it sucks to be able to prove things that aren't true!) Edit: I found it, that metatheorem I mentioned where you can move premises around the turnstile is called [deduction theorem](http://en.wikipedia.org/wiki/Deduction_theorem). I find this stuff so cool, but I never studied much beyond basic logic :/ Edit 2: I just noticed that you *know* all those logical concepts and only the word *entailment* was confusing/ambiguous. You are probably right about it: I learned logic in Portuguese and didn't actually use the word entailment, so for me it might be either a ⊨ b or a ⊢ b. But anyway, at least I found the deduction theorem again.
That really helped me understand, thanks!
&gt; I learned logic in Portuguese and didn't actually use the word entailment, so for me it might be either a ⊨ b or a ⊢ b. The two turnstiles are extremely common, but there's other notations as well. Again, the thing I'm warning against is not to assume that all logicians use the same terms and symbols to mean exactly the same thing. Even without the Portuguese vs. English thing, it's problematic. The most annoying case I ever faced was that in the same year I studied two constraint-based grammar formalisms that both used the word "subsumption," but each used it for a relation that corresponded to the *inverse* of the other one's. Goddamnit.
i'm currently using diagrams to help me learn haskell beyond reading learn you a haskell, but to be honest, i'm finding the api not that user-friendly. at the very least, it's not beginner friendly. maybe i am spoiled by libraries like processing, but even watching a developer of the library is not the same experience i've gotten from using and watching people use processing. to be clear, it's not the functional or the vector space aspects of the library that bother or confuse me. it's more about the design of the user-facing portion of the library. haskell is supposed to simplify things, but as i've gone through the tutorials, i find myself not being convinced that this functional implementation of a drawing language is better than an object-oriented one. even the type of a basic diagram is obtuse. why is it Diagrams B R2 and not just Diagram? i don't really care about the real reason behind the scenes but this is an example of a usability issue in my opinion. also, the examples are not that great, particularly when compared to other drawing libraries. i'm trying to get to the point where i can contribute, but i'm not there yet. my last complaint is that i think it's too focused on generating a file. to even get the text of a diagram, you have to access backend functions that aren't clear. i understand that the diagrams library has multiple backends, but "rendering" an SVG object to text seems backwards to me.
Thanks for the feedback! To address your first point, diagrams is *not intended* to be beginner friendly! It is intended to be powerful and correct. Making graphics is complicated; hence the API is complicated. With that said, we do try hard to have good documentation, so if you find things that are confusing or not explained well, we would love to hear about them. You're absolutely right about the other two points, though. We certainly do need better examples, and I also agree it ought to be easier to generate a diagram without writing it to a file.
I don't think Diagrams is trying to be the Processing of Haskell. From the Diagrams manual: &gt; Flexibility, power, simplicity: in general, you can have any two of these but not all three. Diagrams chooses flexibility and power, at the expense of simplicity. (In comparison, the excellent gloss library instead chooses flexibility and simplicity.) In particular, the types in the diagrams library can be quite intimidating at first. As a newcomer to Haskell, it's understandable that you find Diagrams a bit daunting, as it takes advantage of advanced Haskell features to achieve its flexibility and power. It is a good example to study if you'd like to learn more about designing powerful libraries in Haskell, but perhaps not a good tool for a beginning Haskeller to perform basic drawing operations while learning other aspects of Haskell. But you may find that if you take the time to figure it out and understand why it is designed as it is, you will understand Haskell better in the process. I'm not sure why you say that Haskell is supposed to simplify things--that's not really the point of Haskell. I would say that Haskell is meant to make it easier to reason about programs mathematically/logically. This is not a property that comes for free, but Haskell has successfully reduced its cost over time. Anyway, I don't mean to diminish the difficulty you are having, as I experienced some of that as well when I started using Diagrams and Haskell libraries in general. I just think you may have different expectations than the Diagrams authors intended, although I am sure they would welcome ideas that would make it more approachable without otherwise compromising it.
thank you for the response. i have a few follow up questions. please don't feel like these are directed towards you to answer, but they're just stuff that i thought of while reading your response out of my ignorance and curiosity. * the R2 was clear, but the B in the type is not. i'm not for sure why having a Diagram type definition implies you must be able to compose different backend objects. it seems reasonable to limit that but allow composition of objects with the same backend but still different dimensions (using projection mapping or other more complicated embeddings). can you not currently embed a diagram in R2 into R3? * why is diagrams even caring about objects outside of 2 or 3 dimensions? that generality doesn't seem immediately useful to me. if it is useful beyond trivial cases, it seems i might want to do something interesting with Diagrams and exterior calculus, but you've stated that you can't compose different dimensional objects. this is confusing in terms of generality and usefulness tradeoffs. * as a general user of Diagrams, i don't want to have to learn about PHOAS to understand Diagrams. learning Diagrams isn't the end goal here. i already have complicated things to learn and reason about outside of Diagrams, and i simply want to use Diagrams to aid in that pursuit. my effort in using a library should be in that direction, and not in the direction of learning the guts and design that is deeper than the API.
&gt; Diagrams is really written for "power users". why? also, gloss seems to be a bit limited and not actively developed on.
thank you for the response. &gt; diagrams is not intended to be beginner friendly! It is intended to be powerful and correct. in all seriousness, i don't see the conflict. it's also confusing in what sense you mean by "correct". i mean sure, processing isn't as general as diagrams from the outset, but i would consider processing to be very powerful. powerful both in the sense of what it's capable of and how it enables its users. something similar could be said for vvvv, but it certainly has its own problems. personally, i believe things can be complicated but elegant and simple in their design. straying from that is a compromise almost always. this is a bit of my problem with the haskell community. it wants users but it doesn't treat them well. if you want to learn our programming language then here's category theory thrown at you from everywhere (even though you don't need it to do haskell) and here's complexity thrown at you from the get go just if you want to draw pictures (even though it's not needed). it may sound like i'm expecting Diagrams to be a bit of a cheerleader for haskell, but i don't see why not. it seems to say that from the website's tone anyway. maybe the tutorial just needs to be re-written to be more staggered/layered. i don't know. i'll continue learning it though. &gt; Making graphics is complicated; hence the API is complicated. that is not an excuse that needs to be heard by users. :) maybe it'd make sense for me to play around with making my own library *on top* of Diagrams, treating it like some low-level language like OpenGL or something. maybe that's what it's intended to be. i don't know though so far from the material.
i know Diagrams isn't trying to be processing, although processing is at its core a java library outside of the development environment. that most people are familiar with. and yes, i guess i do have different expectations (i'm honestly not trying to hate here though :), as there are a lot of things that have flexibility, power, and simplicity. for example, the theory of differential forms and exterior calculus has all three. haskell as a whole has all three. and yes, i definitely need to spend more time with haskell and Diagrams. i'll take your advice on using its design to learn more about designing in haskell. &gt; I'm not sure why you say that Haskell is supposed to simplify things from haskell.org: "Haskell … allows rapid development of robust, concise, correct software." that sounds like it simplifies *some* things! :) more than that, it's a higher-level language: it's supposed to simplify things for the domain or context it's to be used in. just as calculus is simplified by smooth manifold theory in certain contexts (like not caring about calculations or coordinate systems), which is in turn simplified by categories and topoi, again with context. i'm certainly taking in everyone's feedback into my learning haskell, but the community should reciprocate as well. sometimes, it feels that the haskell community and libraries are a wall built ten feet in the air with no ladder.
* The `B` in particular is a bit of a hack, making it possible to switch backends without having to change the types of your diagrams. But in any case, whether you can compose objects between different backends or vector spaces is not the point. The point is, unless the type of a diagram has information about the vector space or backend, we *cannot even say* what is allowed and what is not! We can only have types like `Diagram -&gt; Diagram -&gt; Diagram`, and what happens if you give it a 2D cairo diagram and a 3D SVG diagram? Who knows? Maybe we want to allow mixing certain backends; maybe we want to allow promoting 2D diagrams to 3D diagrams in certain ways (this is being worked on currently, BTW); but in order to even write down these operations we need backend and vector space information in the types. [Note: the reason the backend in particular needs to be in the type is actually a bit subtle, and has to do with the fact that backends need not all support the same set of primitives, and can even add their own special primitives.] * The short answer re: generality over vector spaces is "because we can"; it was not that difficult and seemed much more elegant than having two types `Diagram2D` and `Diagram3D`. Currently we only have 2D and 3D diagrams, but who knows what people will come up with? And in any case, there's a lot more you can do beyond just changing the number of dimensions --- e.g. using 2D or 3D vector spaces over different numeric types, etc. * You don't have to learn about PHOAS to understand diagrams. In fact, PHOAS has nothing whatsoever to do with diagrams. That was just an example heisenbug threw out of the power of type parameterization. (I agree PHOAS is super cool but I think heisenbug was being overzealous; it's certainly not a beginner topic.)
I wouldn't insist that byorgey or any volunteer programmer do the work for free, but a flexible+powerful system is definitely improved by having a few simple derivatives that each fix some of the degrees of freedom and provide a less of cluttered API. Even Edward Kmett allows `Simple` in the Lens API.
How about some wrapper/convenience modules to provide a "Glossy" experience , so a beginner could start with simple diagrams and then grow into the full system, without learning all of it at once. Not saying you must do this, just asking if you agree it would be a valid addition to the package suite.
&gt; in all seriousness, i don't see the conflict. Well, that's fair. Maybe a better way to say it is that we have *chosen to focus* on making diagrams powerful, rather than making it easy to learn for beginners. We only have a few people with limited time working on diagrams. We certainly would welcome more documentation, examples, tutorials, etc. making it easier for beginners to learn. &gt; it's also confusing in what sense you mean by "correct". Yes, that was rather vague. I think what I meant is that we try never to cut corners: we always try to do things with maximum generality, handle every corner case correctly, and don't settle for approximations or things that work "in most situations". This naturally results in a more complex API. &gt; personally, i believe things can be complicated but elegant and simple in their &gt; design. straying from that is a compromise almost always. Yes! I completely and fervently agree with you, and this is exactly what we are trying to do with diagrams. If you have specific examples of things in the diagrams API that you think are not designed elegantly or simply (and I am sure there are many), we would love to have you point them out and help us brainstorm something better. &gt; maybe it'd make sense for me to play around with making my own library on top &gt; of Diagrams, treating it like some low-level language like OpenGL or something. Sure. I use diagrams directly all the time, but depending on what you want to draw it can make a lot of sense to build on top of diagrams. (For example, see the Chart and Chart-diagrams packages.)
Yes, absolutely.
(From The Monad.Reader Issue 8 September 10, 2007)
If you want to show off some lens awesomeness, you can add gap :: Traversal' ArrowOpts Double to the API and now you can set the `headGap` and `tailGap` simultaneously.
Ah, excellent idea! Done: https://github.com/diagrams/diagrams-lib/commit/274201d0ac3366fce77f300cb02194e147a2c792
DPH supports *nested* data parallelism, Accelerate does not, afaik why would it be terrible to have CUDA backed NDP framework?
NDP support is current work for Accelerate. We hope to have it available in the not-too-distant future.
This isn't my first Haskell project, but it is my first hackage project. Comments/criticisms are welcome!
Hey, glad you enjoyed it :) I might have a copy of the darcs repo somewhere, but there's a haskellwiki page with some links at http://www.haskell.org/haskellwiki/User:ConradParker/InstantInsanity
There are a few ways to answer your question with various degrees of exactness. The construction that gives rise to Applicative/Monad relies on the fact that Functor is a functor goes _covariantly_ from a category to itself. The "twist" that is introduced by `Contravariant` as it turns `a -&gt; b` into `f b -&gt; f a` precludes such niceties. This means that the answer is more or less "no". Now, there is a `Contravariant` functor at the heart of one way to describe the adjunction that gives rise to the `Cont` `Monad`, but in general `Contravariant` doesn't expand to provide the kind of structures you can build atop `Functor`. So in that sense contravariant coexists with Monad in a way that the two are 'used together', but that is just wordsmithing. My `lens` package actually does "use Contravariant with Applicative" but not in the sense you are describing to construct folds, but it basically uses the fact that if something is both a `Functor` and a `Contravariant` then to comply with both sets of laws it basically can't care about its argument! Every such Functor+Contravariant is isomorphic to `newtype Const b a = Const { getConst :: b }` for some choice of `b`. Similarly if something is both `Applicative` and `Contravariant`, then it is really a `Monoid` with an extra useless type argument that isn't used. e.g. `newtype Const b a = Const { getConst :: b }`is `Applicative` if `b` is a `Monoid` and is `Contravariant` in that it doesn't use its second argument at all.
Nice read and well written, thank you very much!
man one of these days i will get into the haskell club. it seems like a lot of fun. but for now all this stuff just makes me feel dumb. will try again when smarter, best wishes lotion booger
&gt; the reason the backend in particular needs to be in the type is actually a bit subtle, and has to do with the fact that backends need not all support the same set of primitives, and can even add their own special primitives Would an alternative be to make the backend signal a runtime error for unsupported operations? Usually, moving error checking from runtime to typechecking time is a big win. But for diagrams in particular, it seems the usual pipeline is that you write your program, then compile it and immediately run it once to create the diagram output, so compiletime and runtime errors happen at the same point in time from the perspective of the user. So maybe it would be worth simplifying the types here?
Have a look here: http://ocharles.org.uk/blog/guest-posts/2013-12-21-24-days-of-hackage-contravariant.html
Well, `Functor` represents functors `Hask -&gt; Hask` while `Contravariant` represents functors `Hask^op -&gt; Hask`. In order to make applicative work for contravariant functors, we will need to remove the exponentials in the definition, as there aren't coexponentials (that is, exponentials in the opposite category) in Hask. class (Functor f) =&gt; Monoidal f where zip :: (f a, f b) -&gt; f (a, b) unit :: () -&gt; f () (I wrote the types in a slightly unconventional way. Please bear over with this.) Now, there are a few ways to dualize this: class (Contravariant f) =&gt; AlmostExponential f where destruct :: (f a, f b) -&gt; f (Either a b) triv :: () -&gt; f Void class (Contravariant f) =&gt; Uhm f where project :: Either (f a) (f b) -&gt; f (a, b) useless :: Void -&gt; f () class (Contravariant f) =&gt; AlsoAlmostExponential f where construct :: f (Either a b) -&gt; (f a, f b) alsoUseless :: f Void -&gt; () class (Contravariant f) =&gt; Don'tAskMe f where somethingElse :: f (a, b) -&gt; Either (f a) (f b) iGiveUp :: f () -&gt; Void Pick what's useful.
This is `Covariant f` plus `Monoid (f a)` 
This ended up being a longer reply than I originally intended, but if you're just starting out, this might be a good read. I'm not good at Haskell by any means (I'm at about four days of 8 hour days under my belt in Haskell). I tried, on and off, for about a year to learn it, but it never really clicked so I couldn't stay with it (I would do a tutorial here or there, read through a couple chapters of LYAH and quit). Whenever I had to do something I would just fire up Python - I know it (almost) like the back of my hand. I couldn't justify wrestling with Haskell for an hour over something that would take 20 seconds in Python. Until last week when I saw the iHaskell project, and decided that I would learn Haskell (I think it was me saying "I don't know Haskell" that made me think - "I should know Haskell..."). Here's what I did. Read [Learn You a Haskell](http://learnyouahaskell.com/chapters) quickly (read through about syntax - chapter 4, maybe a little farther). Do all the examples (write them out!). Most of it won't make sense, it will probably all be a jumble of words. Sit for a bit (wait a couple days), let it sink in. Read through what you read again. Do the examples again. Maybe go a little farther. You'll have the basic syntax, and you'll have a starting point for Googling around. Then, set your target higher than what you can do with your Haskell knowledge. Make it something you like, or you think might be achievable (I saw [this](http://www.reddit.com/r/learnprogramming/comments/1uxb4i/if_you_had_to_give_a_beginner_a_unique_program_to/ceml5c8) guy's post and was like "that sounds kinda doable..."). It took probably 8 hours to make it work. I looked everything up (this is where LYAH will help - a jumping point for weird words). When I got done, I reworked it. I rewrote it 4 times, using guards, different functions, doing everything in a `do`, different styles of checking the dice roll. The game isn't complicated by any means, but making the game fit the syntax is a huge hill to climb. Just to make myself not depressed for how much time it took, I wrote it in Python. It took about 5 minutes, and it worked on the first run (a Haskell impossibility - for the time being). When I got stuck, I asked for help on IRC and reddit, people are incredibly helpful, although you will probably feel lost when they're explaining things, just read the words, try out any suggestions, and let it sink in. It's hard to have a conversation when you ask a question and people reply with "You need to use a monad monoid bytestring, curry it, then use point free syntax with lambda functions and a strict foldr, that's all you need to do!". The terminology is scary. When you work something for yourself, rather than parroting what a book says, you have to solve your own problems. You start laying a trail of cookies that you'll pick up later. You'll have an experience from wrestling with the random number generator that every time someone says "IO sucks!" you'll be like "Hey, it kinda does! IO is kind of annoying!". When I learn something, if I don't lay my future self a trail of cookie crumbs, I'll lose interest in learning that thing. I think that's why I never got over the first Haskell hill. I just always followed the books and tutorials until stuff worked, but I wouldn't have any new trails to follow because the books and tutorials just throw you lightning bolt solutions, you don't really have to think. I think this is partially why Learn X the Hard Way is successful - at the end of every chapter there's guided open ended questions that let you think for yourself. After making something, I read through more of LYAH. But this time when I did, I had an experience with some of the things that they were going through, rather than reading it on LYAH for the first time. Things clicked easier because I solved a problem with do blocks not returning an expression, or any of the 100 of other problems you have to solve to make a Haskell program compile for the first time. All you needed was a `return ()`, you didn't have to rewrite your whole program! Right now I'm working through the [99 Haskell Problems](http://www.haskell.org/haskellwiki/H-99:_Ninety-Nine_Haskell_Problems), I'm at about 30, but I skipped a few (they get hard quickly). Some other good things to use are: [iHaskell notebooks](https://github.com/gibiansky/IHaskell). I think easier than compiling or running things from the command line for rapid fire testing. (Think before you test!). But, the multi lines and separate cells are really nice. Having the types of stuff come up right after you type a function is really nice. SublimeText works well too as it has a Haskell build environment. Also, [this](https://github.com/karan/Projects) could be a good resource for figuring out what to do. The biggest thing, I think, for learning something new is to try out something on your own, without having a resource that will tell you the whole answer - you have to piece together the solution for yourself, and figure out why things didn't work when you break it. If you solve all the problems with it, then refactor it, you're well on your way to laying out a nice trail of cookie crumbs to follow when you want to continue through the forest.
Just to add to this, I found http://exercism.io to be a great resource. It's like 99 Haskell Problems, but more interactive. By that, I mean that you not only have a set of unit tests to accompany your tasks to help measure how close to a solution you are, but once you submit your code, you can receive feedback from others about your implementation. 
http://i1246.photobucket.com/albums/gg614/Shuq_Aris/2246901-i-know-that-feel-bro.jpg
Well, throwing runtime errors does not feel very "Haskelly", but I do see your point. Unfortunately, it's not that simple (at least not that I can see). This would work if there were some fixed set of primitives and backends could decide which of them to implement (and throw a runtime error for unimplemented ones). In fact, the set of primitives is open. The way this is accomplished is by storing each primitive in an existential wrapper, along with a proof (i.e. type class dictionary) that the backend mentioned in the type of the diagram knows how to draw it. Without knowing the type of the backend I am not sure how one could do this. Perhaps it would be possible using some sort of hack involving `Typeable` but I'd rather not go there. In any case, I personally do not think runtime errors for slightly simpler types is a good tradeoff anyway.
We've been able to receive them ever since we joined SPI, but didn't try to encourage them as we had no idea what we might spend the excess money on and there's no point in just accumulating a large bank balance. Are there some concrete ideas that would benefit from having more money available?
Cool! It would be neat if this could use SQL schema descriptions (CREATE TABLE and so on) as input. Also, in the perhaps-not-so-useful-but-still-cool category, Persistent has a nice TH schema DSL: Person name String age Int Maybe deriving Show BlogPost title String authorId PersonId -- FK deriving Show Would be totally sweet if I could run that through erd from inside emacs. :)
The trick is to arrange the state of the community until SPJ is compelled to explain WTF this stuff is. Then you understand. He doesn't say anything different but somehow there is a SPJ understanding field* that makes stuff work. His video on Lens remains the best introduction to that concept I've seen. *think like the Steve Jobs reality distortion field but useful for people who are under its influence
if it's easy, it ain't haskell.
Unfortunately, I think that this approach is inherently unsafe. By making *any* `ST s` able to be converted to any other monad, you break the safety given by `ST`. For example, you can now write `unsafeRunST`: -- note the lack of a forall unsafeRunST :: ST s a -&gt; a unsafeRunST = runIdentity . perform Additionally, I don't think that there is any solution to this problem. Essentially, you want to run the `ST` monad as your `Conduit` runs. However, the data type is a highly branching tree, not linear, so a user could, for example, run linearly through part of the `Conduit`, then take two branches at once. To have referential transparency, you would have to somehow detect this then rerun all the side effects that came earlier to create a new "`ST` session." I think that this is impossible.
Firstly, directly using `perform` *is* definitely unsafe, and is marked as such in the documentation: &gt; Note that while the usage of this typeclass in @conduitSwapBase@ is completely safe, misuse of it can be dangerous, much like misusing @unsafePerformIO@. I'm not sure I understand your comment about `Conduit` being a "highly branching tree," can you clarify? __EDIT__ The closest I could come to a case that sounds like what you're talking about is the case of a *base* monad which does backtracking, such as the list monad. But in such a case, each `ST` block ends up getting run separately, e.g.: import Control.Monad.Trans.Class import Data.Conduit import qualified Data.Conduit.List as CL import Data.Conduit.Util import Data.STRef sink :: Monad m =&gt; Sink Int m Int sink = conduitSwapBase $ do ref &lt;- lift $ newSTRef 0 CL.mapM_ $ \i -&gt; modifySTRef ref (+ i) lift $ readSTRef ref src :: Int -&gt; Source [] Int src top = mapM_ yield [1..top] main :: IO () main = print $ do top &lt;- [1..10] src top $$ sink I'm far from convinced that the implementation is correct, but if you have some more concrete description of the problem (or better yet, a counter example), I'd appreciate it. __EDIT2__ I *did* come up with a demonstration showing this failing with a backtracking base monad: src :: Source [] Int src = do x &lt;- lift [1..2] yield x y &lt;- lift [1..2] yield y yield $ x * y I think this can be solved by constraining the base monad allowed, but there's nothing inherent in Conduit itself that introduces such backtracking.
It really is different. You can read haskell experts' code and have no idea what it does, then suddenly have a ton of new topics to research. I like that. I don't really ever want to be too comfortable because I realize I'm not smart and will never know everything -- you have to consistently improve to make something of yourself and Haskell makes it really easy to find ways to do so.
That was pretty much where I was going with it. We joined SPI at the very start of 2012, so I guess it has been 2 years now, not just the 1. There have been several proposals that would increase our burn rate. Examples off the top of my head based on random proposals or suggestions I've seen: * Setting up community build-bot resources. This might be useful for things like helping provide better Windows support. * Paying a real sys admin to be more proactive maintaining the servers. That said, Gershom's work in getting more folks involved in server maintenance has gone a long way to reduce pressure in this area. * Polling the community for things it wants as a whole that aren't being addressed by the more commercial interests behind the Industrial Haskell Group and funding some development on those through, say, Well-Typed. * Switching the server infrastructure around to use a content distribution network to help ensure reliabilty in the face of the occasional denial of service attack, and potentially allow read-only access to things like most of hackage when the backend goes down. * As haskell.org represents the open source Haskell community we could also look into helping to fund local Haskell hackathons to help increase adoption and community cohesion. To branch out that far may require a more formal charter and careful consideration, though. If we had more money available to us there are ways we could put it to work! The trick is nailing down specifics and not over-promising on what we can deliver.
When you start going down this road you want to understand positivity and negativity. Intuitively, positive values are introduced by construction and negative values are introduced by need. For instance if we look at the type of a function a -&gt; b We've just referred to two types, `a`, and `b`, but they are different. When we are *given* this function, it gives us a method of *creating* `b`s and, after application, we'd be able to walk away from it with a concrete `b` right in front of us—something we could pass elsewhere. This means that `b` is positive with respect to the type `(a -&gt; b)`. When we are *given* this function it also introduces `a`, but will never provide us one. Instead, it introduces a *need* for `a`s, a way to consume them. In order to apply this function we need to get an `a` from *somewhere else* and then after application we will have *consumed* it. This means that `a` is negative with respect to the type `(a -&gt; b)`. (Note the "we are *given*" bits above, they'll become important later.) --- Now the moral of the story is, with respect to your question, that positive type variables introduce Functor instances and negative type variables introduce Contravariant instances. We can write these for `(a -&gt; b)` using a newtype to flip the `a` around to being the final type parameter. newtype InvFun b a = InvFun (a -&gt; b) instance Functor ((-&gt;) a) where ... instance Contravariant (InvFun b) where ... There's also `Profunctor` which describes a type with kind `(* -&gt; * -&gt; *)` where the first parameter is negative and the second positive. This fully describes `(-&gt;)` so it's more often used that `InvFun`. instance Profunctor (-&gt;) where ... --- Let's focus back on negativity and positivity, though. The core idea is that there is a duality between production and consumption, between values and continuations, that plays out in any program. The simplest example is the one argument function described above, but with more constructors and higher order types it can get more complex. As a slightly more complex example, consider the multi-argument function a -&gt; b -&gt; c -&gt; d -&gt; e n -&gt; n -&gt; n -&gt; n -&gt; p I've annotated this with **n**egative and **p**ositive positions to highlight what's intuitively obvious—each "input" is consumed and is thus negative while the output is positive. To fully generalize it, take a look at the pairing function (,) :: a -&gt; b -&gt; (a, b) :: n -&gt; n -&gt; (p, p) Here we see an example of multiple positive types, which is new, and also the notion of a single type variable being both positive and negative in a total type. This is a common feature because I've been a little bit blurry about what exactly is "positive" or "negative" in a type. Type constructors introduce these notions at each *position* $1 -&gt; $2 and then type variables *inherit* them when they're put at a position a -&gt; b [a, b / $1, $2] which allows us to alias or name-clash sometimes a -&gt; a [a, a / $1, $2] where now `a` has inherited both a positive and a negative nature. --- As a quick aside, if we have a type newtype Endo a = Endo { appEndo :: a -&gt; a } where the type variable is internally used both positively and negatively then we can instantiate neither `Functor` nor `Contravariant`. `Endo` has masked the important distinction between positivity and negativity. The opposite can occur as well data Ignore a = Ignore where `a` is a completely phantom type variable. Now we can instantiate *both* `Functor` and `Contravariant` instance Functor Ignore where fmap _ Ignore = Ignore instance Contravariant Ignore where contramap _ Ignore = Ignore and indeed if you ever see a type like (Functor f, Contravariant f) =&gt; f a you know that `f` must be ignoring its `a` parameter. In fact, we can write absurdF :: (Functor f, Contravariant f) =&gt; f a -&gt; f Void to prove it. This is what Edward talks about in his post—it's used frequently in `lens` where such abstract types occur all the time. --- Higher order types also provide a lot of weirdness when deciding how to interpret positivity and negativity. Consider the following (a -&gt; b) -&gt; c Are we consuming an `a` or producing it? The trick comes back to the notion of being *given* the function that I referenced at the very beginning of this response. Here are are being *given* a function (`(a -&gt; b) -&gt; c`) which asks us to *produce* another function (`a -&gt; b`). Our own dueling notions of being given or being demanded to produce is exactly the same duality between positivity and negativity again. When we must produce a function of type *a -&gt; b* then we write the following f a = ... In the context of that `(...)` notice that we are now *given* an `a` and must *produce* a `b`. This is the opposite polarity of what happens when we're given *a -&gt; b* as in the beginning of this response. For this reason, we flip polarities: When we are *given* the function (a -&gt; b) -&gt; c we assign the polarities (p -&gt; n) -&gt; p More generally we might have (a -&gt; (b -&gt; c) -&gt; d) -&gt; e -&gt; f (p -&gt; (n -&gt; p) -&gt; n) -&gt; n -&gt; p See if you can follow that through—it involves two "context shifts" as you dive into higher and higher order arguments. --- The upshot of this higher order madness is that we can start to reason about `Cont` nicely. `Cont` looks like newtype Cont r a = Cont { runCont :: (a -&gt; r) -&gt; r } so using our new polarity reasoning techniques we can see that Cont r a Cont x p So, `r` is unreasonable, but `a` is strangely *positive* even though it's not immediately clear how we could think of `Cont` as producing an `a`. This does mean that `Cont` is a `Functor` instance Functor (Cont r) where fmap f (Cont go) = Cont (\c -&gt; go (c . f)) And we actually can write a function to *extract* the `a` extract :: Cont a a -&gt; a extract (Cont go) = go id though in general positivity *doesn't* actually mean that we can fully extract the positive type—just that we can operate on it sometimes as though we could. --- (Edit: see below for commentary on why this isn't the most correct segment.) And that's the final thing to note. While positivity and negativity come up very frequently as the nature of being "available" or "in-demand" it doesn't actually mean that you'll ever be able to get a handle on a positive type directly nor that you'll be able to provide a type to a negative position directly. The entire process may be more abstract than that. For instance, if we examine `Profunctor`s again we might imaging a "chainable pair". Profunctor p1 =&gt; p1 a b Profunctor p2 =&gt; p2 b c Running off our intuition that `(-&gt;)` is a `Profunctor` we might want to combine these two `Profunctors` by taking the positive, output `b` from `p1` and passing to the negative, in-demand `b` in `p2`. This is not in general possible, but we have a `Category` class to determine when that might occur -- Sidenote: previously I wrote... -- -- class Profunctor p =&gt; Category p where -- id = p a a -- (.) = p b c -&gt; p a b -&gt; p a c -- -- but that way totally wrong: there's no Profunctor superclass, it's just class Category p where id = p a a (.) = p b c -&gt; p a b -&gt; p a c which provides a generalized `(.)` allowing us to "feed" `p1` into `p2` (if they both instantiated `Category` too). Now this almost feels like the notion we're looking for—it certainly is that notion exactly then `p` is `(-&gt;)` and `(-&gt;)` provides an instance of `Category` in the obvious way. The difference is that we might have more obscure ways of performing `(.)` that just "feeding" values. For instance, we could imagine matrices of dimension `i j` as implementing `Category` (*kind of*—don't think too hard about what the type variables must be) identityMatrix :: Matrix i i matrixMultiply :: Matrix i j -&gt; Matrix j k -&gt; Matrix i k instance Category Matrix where id = identityMatrix (.) = flip matrixMultiply But this certainly isn't a notion of feeding values from the output of one profunctor into the input of another—it involves a sum over a large number of multiplications. This hopefully will drive home the notion that positivity and negativity being dual and somehow annihilating one another does not necessarily mean that we're ever going to get a "top-level" handle on their values.
Slides are posted here: http://gbaz.github.io/slides/13-11-25-nyhaskell-diagrams.pdf Our next meetup is this coming wednesday the 22nd, and we're back to the two talk format. Both promise to be excellent: http://www.meetup.com/NY-Haskell/events/159623642/ 
I was working on a neural network library for my thesis in c++. It was called [miind](http://www.sciencedirect.com/science/article/pii/S0893608008001457) and the main idea was to separate network architecture, network implementation and node evaluation algorithms. Thus, the excessive use of template meta programming. The networks are treated like functors and the algorithm to evaluate the node activation is a function which is mapped over this functor. I always thought that c++ templating was a pain with lots of boiler plate code. When I discovered Haskell I thought that it is exactly the kind of language for this library. I actually started working on a Haskell implementation loosely based on miind, called [snarf](https://github.com/Snarfnetworks/snarf).
This is a great tutorial! It solves a real-world problem, compares to another implementation, and is entertaining. My one fault would be that it uses `lens`. I am a fan of `lens`, but I feel very strongly that it should not make an appearance in the working code of beginner tutorials. `lens` is a complicated, theory-based library, and that could easily put off a lot of newbies. I think many of us here came to Haskell from the practical side, and slowly our interest in the theory increased, but being thrown into the heavy theory when you're just getting started is a scary experience. &gt; let login = body ^? key "items" . nth 0 . key "login" It looks ugly, but it’s terse. In fact each function (^?), key and nth has some great mathematical properties and everything is type safe. I understand the author's thought process behind this statement, but I think it may not be interpreted as intended. To someone inexperienced with `lens`, this code makes precisely zero sense, and looks very daunting. A beginner's tutorial should be accessible above all else, and this `lens` code ruins that. Being terse for the sake of being terse has no place in code intended to teach beginners, *especially* when it creates ugly code. I feel like simpler, non-lens code would be a much better fit here. A beginner could be excused for seeing the `lens` code, looking up `lens`, thinking "Wow, they need to do *this* just to handle JSON? That seems so excessive. It's so much easier in the language I came from" and giving up on Haskell with a bad experience. NOTE: this post is not trying to bash `lens` in any way at all. It is an incredible library, and I am in awe of /u/edwardkmett every time I run into it/use it. But it is complex and daunting, and not beginner-friendly. This is no fault of the library, simply a property of its generality.
Profunctor can't be a superclass of `Category` as it implies a canonical embedding of Hask. `arr f = lmap f id` or equivalently `arr f = rmap f id`, which you don't get for all instances of `Category`, e.g. data a == b where Refl :: a == a
I agree it would be cool, but UML is a big beast. I'd like to keep `erd` as a simple tool that does one thing reasonably well: make ER diagrams.
&gt; Cool! It would be neat if this could use SQL schema descriptions (CREATE TABLE and so on) as input. The problem is that requires handling N different SQL dialects. It's much easier to the [let the client generate the `er` file itself](https://github.com/BurntSushi/nfldb/blob/master/scripts/nfldb-write-erd). It should only be a few lines of code querying whatever meta tables your RDBMS has. &gt; Also, in the perhaps-not-so-useful-but-still-cool category, Persistent has a nice TH schema DSL ... Would be totally sweet if I could run that through erd from inside emacs. :) Write a quick and dirty program to translate it to `er` format and then pipe it into `erd`. :-)
Yes, I [added it](https://github.com/snoyberg/conduit/commit/5577348b2520aca1c0f44bb30fbe74fd20a671db) after writing this comment. I had thought about using that naming originally, but for some reason I decided against it.
Oomph, I knew I was stretching there. I think the entire last segment is a wash, tbh.
You can actually implement `Matrix` as a category by choosing the kind of the type of objects they have to be of kind `Nat`, so it isn't a total wash, but pretty much. =) 
I tried to wave my hands so quickly as to fly, but no avail :)
Awesome, thanks!
&gt; The problem is that requires handling N different SQL dialects. Ahh, standards... &lt;3
&gt; also, gloss seems to be a bit limited and not actively developed on. Things a beginner won't give a single fuck about.
You'll forgive an ignorant question, I hope, but what would be wrong with a less general solution that used only the safe `runST`? Something like {-# LANGUAGE RankNTypes #-} import Control.Monad import Control.Monad.ST import Data.Conduit import Data.Conduit.Internal -- Dodge the need for ImpredicativeTypes by wrapping the forall newtype STFA a = STFA { unSTFA :: forall s . ST s a } instance Monad STFA where -- ... runConduitSTFA :: Monad m =&gt; ConduitM i o STFA r -&gt; ConduitM i o m r runConduitSTFA = ConduitM . go . unConduitM where go :: (Monad m) =&gt; Pipe l i o u STFA r -&gt; Pipe l i o u m r go (Done r) = Done r go (Leftover p i) = Leftover (go p) i go (HaveOutput p f o) = HaveOutput (go p) (return (runST (unSTFA f))) o go (PipeM mp) = PipeM $ liftM go $ return (runST (unSTFA mp)) go (NeedInput p c) = NeedInput (go . p) (go . c) Though this can't quite be right: the example `freqSink` example can't be written because `STFA` isn't (easily?) a `PrimMonad`. 
I think you'll run into the first issue when trying to create a new `STRef`. Consider the following attempted implementation: newSTFARef :: a -&gt; STFA (STRef s a) newSTFARef = STFA . newSTRef GHC, however, complains with: Couldn't match type `ST s0 (STRef s0 a)' with `forall s1. ST s1 (STRef s a)' Expected type: a -&gt; forall s1. ST s1 (STRef s a) Actual type: a -&gt; ST s0 (STRef s0 a)
thanks friend I don't know why I needed to be told that but I did. Build something real or you'll never get anywhere. Pretty sure I've even said that to people but for some reason Haskell makes my brain melt and I'm beggin for the 'lightning bolt solution'. Haskell craps sounds like as good a start as any, wish me luck.
this looks cool, nothin like some unit tests to calm my nerves
Well, if you only care about the final exported symbol map, might work. Other approach is to make AST type to be 'AST a', then you could store the local symtab in 'a'. See the Annotated modules in haskell-src-exts. But the best is to get started and see where it ends. Should be pretty concise in Haskell.
a beginner doesn't care about investment?
I've noticed that a lot of tutorials sacrifice (a pretty muddy notion of) usefulness in favor of simplicity. But that's what LYAH is for, really. I'd prefer it if tutorials used all the things that are actually used in practice, even if they have to stop to explain a few things to do so. Foldable and Traversable are extremely powerful and popular, but it seems like tutorials are lacking. I guess the issue is that there's a gap between beginner "it's still okay to use String and lazy IO", intermediate "let's try a streaming IO abstraction, always use Text/BS, check out Foldable/Traversable", and expert "here's this craziness". Some of the expert writers make an effort to appeal to intermediate readers, but I understand that it's much easier to write to an audience more like yourself. It just feels like there's a dearth of really solid intermediate level tutorials, and I think that's because the notion of "intermediate Haskell" is pretty difficult to nail down. I think I could comfortably call myself an intermediate Haskeller, and I often feel that the tutorials I read are firmly in the beginner camp or the expert camp. I get bored with the beginner material, and the expert material is often almost opaque. The 24 Days series has been great because it just about perfectly hits that intermediate "here's a really cool library that you should use because X, Y, Z, and here's a mostly complete example".
I was hoping you would implicitly infer that you should apply this to other options: such as a traversal over `headColour`, `tailColour`, and `shaftColour` (assuming that these three colours are independent, and assuming you feel there is a canonical order, otherwise use a setter rather than a traversal) and maybe other elements where you think that it is a common use case to set multiple fields to the same value.
&gt; I get bored with the beginner material, and the expert material is often almost opaque. Agreed, I frequently feel this way as well. I just keep trying to muddle along :)
An updated Type-Level Instant Insanity in D language (original version by user/FeepingCreature): http://dpaste.dzfl.pl/f21100d6 It compiles in about 3 seconds with the latest dmd compiler on a 2.3 GHz CPU. (You can also implement Allowed with a staticMap from the std.typetuple standard module, partially applying Compatible.) This version is template-based. In D you can also write normal run-time code, and also you can run about the same run-time functions at compile-time (Compile-Time Function Evaluation). Some parts of this code are not strongly typed at compile-time, so here U is not required to be a color and C is not required to be some instantiation of Cube: struct Cube(U, F, R, B, L, D) { alias Up = U; alias Front = F; alias Right = R; alias Back = B; alias Left = L; alias Down = D; } //... alias Rotation(C) = Cube!(C.Up, C.Right, C.Back, C.Left, C.Front, C.Down); If you want those type annotations you can add them like this (that also allows the D compiler to produce nicer type-level error messages if you do something wrong): enum IsColor(T) = is(T == Red) || is(T == Blue) || is(T == Green) || is(T == White); struct Cube(U, F, R, B, L, D) if (allSatisfy!(IsColor, U, F, R, B, L, D)) { alias Up = U; alias Front = F; alias Right = R; alias Back = B; alias Left = L; alias Down = D; } //... enum IsCube(T) = is(T : Cube!Types, Types...); template Rotation(C) if (IsCube!C) { alias Rotation = Cube!(C.Up, C.Right, C.Back, C.Left, C.Front, C.Down); }
My hesitation came more from lack of documentation in real projects. It's nothing I have time to do right now, but looking through how BazQux was implemented will probably be vastly informative. I also have a lot of concerns about working with a compiler that has "lots of rough edges". I got the same impression when I was playing with Ur/Web before which unfortunately leaves it as a "thing to play with" instead of a thing to really put weight on. Earlier movers than I could pave the way and reduce hesitance a lot.
I've been very interested in your analyses over the years. Retrospectives like these are an excellent form of feedback on how we're doing and where things need work. But there's always a time to move on Thanks again for all your work! :)
According to [this](https://getsatisfaction.com/codeeval/topics/haskell_support), it has done for 7 months. Unless they've just added Haskell-platform too?
I wonder if it would be possible to implement an `StT` monad transformer that provides the same guarantees as `ST`. The reason I ask is that if you could then you could use the [`distribute` function](http://hackage.haskell.org/package/pipes-4.0.2/docs/Pipes-Lift.html#v:distribute) from `Pipes.Lift` (you can also implement an analogous function for `conduit`) to pull out the `StT` layer and run it like this: runStT . distribute
I can understand that. I meant to include something in my original post, so let me clarify. I have no problem seeing the `lens` code in the tutorial, but I think the non-lens code should be included in the final body of code. In this tutorial, that li e is very opaque. It doesn't really show the benefits of lens at all. It would be better for both the beginner and for attracting people to the library if non-lens cods was shown first. Then the authoer could say "this works, but it's a bit long. Here's how we could've written it using the `aeson-lens` library. This is a bit more advanced though, so don't worry if you don't understand it." It does what you want - exposes people to the library - but it shows what it does and the longer alternative, so you can actually see what the advantage of using `lens` is.
Who is this guy and what is he doing on /r/haskell ?
Me thinks that darcs repo should end up on github.
I think the kids these days call it trolling though traditionally it's called "being a tosser". I've yet to see a single one of their comments that wasn't scathing, dismissive and condescending while at the same time demonstrating no actual knowledge of or interest in Haskell. Is it any wonder they're always a net negative score?
First of all, you can access `unsafePerform` by using `conduitSwapBase`: unsafePerform = runPipe . conduitSwapBase . lift Second of all, in terms of branching, I meant that `Conduit`s are, internally, represented as a tree. Because of this, it is possible for a user to run different computations that started with the same prefix of computations, and so expect to be run immediately afterwards. With `ST`, this means that you can run two computations that both expect to start with the same state, but instead run one after the other. ~~As an example, the `ResumableSource` abstraction exposes this:~~ src :: ConduitM () Integer Identity () src = conduitSwapBase $ do ref &lt;- lift (newSTRef 0) forever $ do val &lt;- lift (readSTRef ref) lift (writeSTRef ref (val + 1)) yield val bad n = (as, bs) where (rs, _) = runIdentity (src $$+ take n) (_, as) = runIdentity (rs $$++ take n) (_, bs) = runIdentity (rs $$++ take n) main = print (bad 5) ~~If you run this without optimization (CSE will break this easily), I believe you will get `([5,6,7,8,9],[10,11,12,13,14])`, when the two elements of the pair should clearly be the same.~~ I don't understand the semantics of `conduit` to get this to work right now - for some reason, I'm failing to understand what is going on even if I use `IO` and linearize everything. EDIT: missed where keyword in `bad` EDIT2: It's swap, not switch, and the example doesn't actually work.
So I can't use Contravariant with Applicative or Monad. Is there any way to force fmap to have the type `fmap :: (a -&gt; a) -&gt; f a -&gt; f a`?
I have a suspicion that u/TheGoodMachine is u/EvilMachine reincarnated. Since that account is deleted it's hard to search for it. But see, e.g: http://www.reddit.com/r/haskell/comments/16kqe0/recommended_haskell_web_framework_for_beginners/c7xb9b1 Similar pattern overall, though he does seem to have gotten rather sour lately. He's definitely not stupid, but the comments certainly haven't been contributing much, recently or historically.
~~It seems like they're using runhaskell or similar, because their reported runtimes seem to match up with the timings I get from running my program interpreted (albeit that CodeEval seems to be using **really** slow/overloaded hardware).~~ ~~This seems like serious bullshit on the part of CodeEval. Most of their problems are pure algorithmic (many are combinatorial problems, see [Ugly Numbers](https://www.codeeval.com/open_challenges/42/)), and yet we can't take advantage of compiler optimisations that make Haskell fast. For this question, I either have to contort my answer chasing performance improvements (and hence losing clarity), or rewrite in one of the languages they _do_ compile just to get in under the 10 second limit.~~ EDIT: Ok, so according to [this thread](https://getsatisfaction.com/codeeval/topics/haskell_support), it is compiled with -O2. Still, the compiled version on my (underpowered) laptop takes a fraction of a second to run, while theirs takes 4.6 seconds. That seems like they're not really giving much processing performance to our code.
Cool, best of luck on your journey! The quote struck me as a bit odd, and that's because it's missing a word: &gt; It is not **only** the violin that shapes the violinist, we are all shaped by the tools we train ourselves to use, and in this respect programming languages have a devious influence: they shape our thinking habits. Chris Done's got a typed up version here: http://chrisdone.com/posts/dijkstra-haskell-java
I think the only part of your reply which may not be true is that conduit does any branching. The only thing I think you may be referring to is how the NeedInput constructor has two fields, but only one path there is ever taken. However, your comments about ResumableSource are likely correct. I spent some time thinking about this last night, and when I woke up this morning, I think I was fully convinced that there needs to be a type class constraints that states clearly that some kind of mutable data is being used, liked MonadUnsafeIO. Thanks for the feedback on the post, it's exactly the kind of challenge to an idea I love hearing!
It's a really nice idea, but as discussed above, I'm fairly certain having a branching base monad will break it. We could restrict it, but then we'd have the problem of a base monad that can be paused and run multiple times, which would include both Pipe and Conduit. If only we could have all the nice things ;)
I've [posted an update blog post](http://www.yesodweb.com/blog/2014/01/update-st-monad-conduit) on this subject based on gereeter's feedback.
Cabal/Hackage would sit in their chart as having: * Semantic Versioning (per the PVP) * Self-Hosted (with effort) * Not JSON Based * Not Licenses Mandatory (it's a field in the project, but it'll default to AllRightsReserved) * Not Signed * Mirrors Available (luite has one, and I think fpcomplete has one as well) * Easy Publish Basically it would have the same items lit up as Bundler. 
isn't cabal more of a package builder that sort of moonlights as a package manager?
Hello - good luck and much FUN!
Kind of. The package management part is actually outsourced to `ghc-pkg` (or similar for other compilers), but that shouldn't be a user's concern. Or if you're referring to the lack of certain package management features, then it is just that — the lack of features.
What have sheaves to do with diagrams? I am intrigued.
I hate doing this, because I love seeing enthusiasm about haskell, but the "quicksort" in this blog post isn't. "quicksort" really should refer to the in-place algorithm on arrays Tony Hoare originally described and that has become so popular, and we should be very careful about the use of the name when we move away from that algorithm. The Java quicksort is a space efficient algorithm for working on transient arrays. The Haskell "quicksort" is a beautiful but not particularly efficient algorithm for working on persistent lists. This is a "partition based sort" but it isn't quicksort. The standard haskell tutorial also should probably not use this example. 
I've found that (http://www.seas.upenn.edu/~cis194/lectures.html)[this] kind of subsumes Learn You A Haskell - it references it where appropriate in the recommended reading, it covers similar material, and it has extremely good exercises to get you writing code from the start.
Yeah, you are right. This wouldn't work with `Either` or `[]` as the base monad.
&gt; Things got improved a bit lately but I'm still putting this function to variable and reading it before call instead of direct call to disable inlining. Woah, that must seem weird. Having something like NOINLINE Pragama would be nice I guess
Caveat with Haskell: Versioning is fundamentally broken, mainly because some guys thought it would be a good idea to add upper dependency bounds to the suggestions of how you should design your dependencies.
An interesting discussion; a couple of neat tricks for type-safe programming are shown
I think it's a nice example, but one has to be careful pointing out that it's not really quicksort.
Oh yeah, there is no problem if it aborts early.
Did donations really generate that much money?
You can't upload to Hackage with AllRightsReserved, though, so that's *kind of* license mandatory.
Fair nuff. I hadn't tried.
Unfortunately it's not that easy. Inlining is also used by compiler to remove polymorphism and higher-order functions during compilation of server-side code. Although for client-side code it probably should be possible.
It's a shame Quicklisp (http://quicklisp.org) isn't widely known. Ever since Zach Beane introduced it a few years ago, it pretty much ignited the Lisp community (i.e., all 5 of us were thrilled ;) ). It's basically a distro of Lisp packages, with Zach acting as steward. Anyone who can write a clean Lisp package and post it on github (or a few other places) can contribute to the growing library, and all are installable with a single command. Since introduction, it has become fundamental to the Lisp ecosystem, as fundamental as Emacs and SLIME.
Personally, I have to agree with /u/nikofeyn . My experience is that simpler type signatures are often better, and I think that the Haskell `Prelude` is a role model in Haskell API design. With more fine-grained type than that, things get unwieldy fairly fast. (I have [ventured][1] into the realms of higher rank myself, but I am no longer convinced that the extra type parameter is a worthy trade-off.) That said, I do think that the actual code in the diagrams [tutorial][] and [manual][] is very simple and clear, it is mostly the type signatures that I find rather complicated at times. [tutorial]: http://projects.haskell.org/diagrams/doc/quickstart.html [manual]: http://projects.haskell.org/diagrams/doc/manual.html [1]: http://hackage.haskell.org/package/reactive-banana-0.7.1.3/docs/Reactive-Banana-Switch.html
That's a relief.
I wholeheartedly agree with this. This is what porting glibc's quicksort to Haskell actually looks like: https://gist.github.com/etrepum/8446054 (this is roughly a line by line translation, could be much shorter &amp; more idiomatic)
I read "widely known" as "known outside the lisp community" - especially after the joke that there are only 5 people in the (common) lisp community.
http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html was a great resource for me understanding the different monads. That article along with some help in #haskell I converted a library from manually passing state around into something that uses the StateT monad.
I ran into it back when fiddling around with an internal Hackage mirror.
thanks pointing out this tutorial, I opened new collection "Monads" and added it.
Good point: I should have clarified that I was referring to the "Common Lisp" community. Very valid, thank you!
Because dynamic linking permits patching and smaller runtime sizes, to name but a few. There are very good reasons why compilers, runtimes and OS abandoned static linking.
Talk about a security nightmare
Not if you do virtual images deployments.
ugh! There's the ugly girl we've been missing.
The mapping of objects to objects can instead (and equivalently) be viewed as a mapping of identity arrows to identity arrows. Categories are often about the arrows more so than the objects!
The biggest problem with this concept is this: What happens when you have a teeny update in the libc? Since almost all packages depend on the libc, you had to update *all* the binaries to stay consistent. If they found a solution for this, that would be great.
Congrats on 1.0!
Congrats on doing the blog series I wish I had done :) I also remember wishing that I had contributed to the updating of the docs and wiki pages that were a little out of date and confusing when I was learning. Many of them have the kinds of errors that are obvious for experienced haskellers to work around, but are confusing for new people. If you're interested in this, let me know when you come across examples of this and maybe we can contribute by patching them up. You listed 6-line-quicksort as the reason for your blog's existence. I hope you aren't discouraged to find out that that example is actually misleading (I'm sure I don't have to tell you that haskell solutions often are shorter and clearer than imperative solutions). Wonder if we can think of one as pithy as 'quicksort', but which isn't so easily shot down. That would make for a great improvement to the haskell wiki's [intro](http://www.haskell.org/haskellwiki/Introduction), which puts 'quicksort' front and center! Looking forward to reading your next post.
Care to expand ?
If there's a security bug in a library that is dynamically linked, all you need to do is update that library. If it were statically linked, you would have to update every binary that uses it.
The point is, that teeny change might be a fix for a security hole in the libc. You don't want to have security holes, do you? In an ideal world were software is free of bugs, your coment would make sense.
The corollary is that you can introduce a security vulnerability in many dynamically linked programs by updating a single library. 
Static linking would only solve part of the issue. You might have dependencies on things like webserver/database/libraries (for interpreted and VM languages). Another cool tool to use with NixOS is NixOps which lets you define your whole network of services and deploy them. Edit: words
I've been running NixOS for months now and LOVE IT. Everybody got excited about cabal sandboxes (and for good reason), but I couldn't get quite as excited because I already had it - I just run `nix-shell 'cabal run'` and I get a sandboxed `cabal run`. The idea of system configuration works extremely nicely - I love having a centralised configuration for my whole system. Furthermore, the Nix language itself is very concise - most Haskell packages are tiny. [Here's a Nix expression for something I'm currently working on](https://github.com/ocharles/tasty-rerun/blob/master/default.nix). I have started parameterising all my expression on `haskellPackages` so I can easily compile against different GHCs or enable profiling (`nix-shell ... --arg haskellPackages 'with import &lt;nixpkgs&gt; {}; haskellPackages_ghc763_profiling'` is enough to do that).
Yes, you do, but how's that different from any other distribution? Due to the way Nix is built all those binaries can be (and currently are) produced by a build farm, so in that sense the time to release the upgrade is a problem that can be solved by throwing more hardware at it. if you want to avoid the problem of network overhead, then most executables that link to `libc` probably only need their `RPATH` updated - so rather than transmit the whole closure, you may just be able to transmit just the delta. Furthermore, as /user/eversinglelastname suggests we are working on a solution for "multiple outputs" for derivations, which means downstream can specify tighter dependency requirements.
Without Nix, sure, but with Nix you can do dynamic linking and just ship entire closure and have things resolve as they should.
All you need to do is to update your system. Yes. In both cases.
I only recently had a run-in with NixOS and I quite liked it. As you mentioned it's sort of like cabal sandbox/virtualenv/etc. at system level. Another cool point is that everything is (or should be configured) in configuration.nix. As somebody who spends most of my time in networking world, I always kinda wished there was an equivalent of "show running-config" for linux. One command that shows all (or most) of your config and you could easily copy to another box and have everything in place.
I'll be honest, I was convinced by this... ...until I realized just how many programs I have always had on my machines that were not handled by my package manager. Updating the managed part of the system is a snap. Remembering all the dozens of unmanaged packages and updating those by hand is Not Happening.
While that might work for Facebook, I think having this kind of system-level (or well, packaging system-level) sandboxes/virtualenvs is more general.
There's plenty of stuff you can get done without Text and iteratees and lenses. They seem to be the new monad: the fear spreads that you can't get anything done without this bit of technology. Go at your pace and step up to advanced tools when you need them. 
Does NixOS force you to update the programs it doesn't manage? I had assumed that it was only hashing the programs it was managing (through its package manager).
Congratulations on the release. Now with the use of lenses it'll be a lot easier for me to wrap my head around the notation. =)
Right, but only the parts of your system that need the security bug fixed will need to be recompiled. The rest of the system can keep running with the old version and wait for a recompile when you have the time. Not quite as quite as just replacing a single libc for sure. You cheat if you really want to change what is in the store and not touch the hash for a quick fix, but it has similar consequences as throwing around unsafePreformIO as a quick fix.
=) This was really a sort of "first pass" at lens integration, converting all the fields for records of optional arguments into lenses. We also have some more ambitious plans for providing nontrivial lenses, traversals, etc. over diagrams and paths, making it much easier to apply "edits" to diagrams.
It would be mistaken though to think that Cabal is at the same usability level as Bundler. There is an important item missing from the chart: consistent builds (move to another computer or wipe out your current install and install the same dependencies). I know that Bundler and NPM support that and Cabal does not. Also, I think the cryptograhpically signed point takes for granted that things are being sent over https, which again is not the case for Cabal. Rather than JSON based, we could make the field "not XML based", and cabal would get credit there.
You would not need to check each package, only each application. If it is a web server, that sounds like it needs update. If is is a computer algebra system, that may not need an update right way and can wait. 
I was simply going by items in the chart, not overall utility.
I don't mean to say that analogies are absolute, just that they apply to concrete things with abstract properties better than abstract things which just capture the scaffold of a problem. Analogies are too small to fill the latter, but a lot of examples and the corresponding analogies might.
Arrows?! Oh, arrows.
Hah, it wouldn't be a byorgey project if it didn't come with a Typeclassopdia :-) http://projects.haskell.org/diagrams/doc/manual.html#type-class-reference
When people say Haskell simplifies things, they mean things like: * easier to test *thoroughly* (quickcheck) * extreme modularity+composability, so no copy-paste boilerplate code patterns (and their attendant copy-paste bugs) * Haskell is high ROI, with high "I" in that you pay a lot of up front cost to get your code to compile, and the reward is very few runtime bugs (except space leaks :-/ ) and as you pike on more and more features, cost per feature is approximately constant instead of increasing as your code base increases.
Does Gloss have an SVG output (or can another library provide that)? Gloss seems to be OpenGL backed, not SVG (text) generating. Gloss is simple, but only overlaps a subset of use cases (live animation, not publishing an image).
Now I want ghci/haddock type signatures to use p and n -based variables instead of a and b and c
This is like saying that you shouldn't use functions in your code because a security vulnerability in a single function will affect all code that uses that function
My rule of thumb is that if the reading material is interesting then it is not spamming.
This is a nice way to remind something I have been thinking about recently: You don't necessarily need to see a value of of type `a` en route to a value of type `f a`, for any type constructor `f`. `a` values may be produced later, or even not at all if `a` is a phantom/proxy used for type-level programming.
...which is why /u/tel mentioned it as the thing we are talking about when we say "a Functor is a thing that...." 
&gt; A square is a rectangle but certainly is not a subtype. This is frequently misunderstood. An immutable square is certainly a subtype of an immutable rectangle. This sounds uselessly pedantic, but it captures the difference between isA in Java, which lacks type-level immutability (though you can get it defacto with `final`) and Haskell. In Java, a mutable Rectangle can be stretched, but a Square cannot. In Haskell, a Square isA valid subclass (well, type-class instance) of Rectangle: class Rectangle r where height :: r -&gt; Int width :: r -&gt; Int instance Rectangle GeneralRectangle ... instance Rectangle Square where ... stretch :: Rectangle r =&gt; r -&gt; GeneralRectangle stretch r = GeneralRectangle (height r) ((width r) * 2) stretchToSquare :: Rectangle r =&gt; r -&gt; Square stretchToSquare r = Square (length r) Completely valid and correct for `r::Square` ! The discrepancy appears because people don't think about mutability when drawing analogies from math to Java. Haskell solves that problem :-)
gloss is easier for someone just getting started in programming. That is all :) 
&gt; A Functor is a construction that maps between categories. What do you mean by "is a"? :-) The problem with analogy isn't the imagery, its the imprecision of which properties are part of the analogy and which are not. Same for language like "is a".
Is there an article somewhere that talks about how to figure out what exactly fusion, specifically foldr/build, will fuse away? I try to look at small examples with ghc-core, but it's so hard to read. And besides, it can't tell me what will definitely *not* fuse.
This is very interesting, unfortunately Ur/Web didn't quite work for us. We evaluated the use of Ur/Web (among with other contenders, e.g., OPA, Ermine, Scala) for building information systems. In the end, we chose Dataphor. Some of the reasons: 1. GUI is not tied to web browsers (sometimes we have to implement specialized GUIs e.g. for mobile or for kiosks) -- Ur/Web could certainly be adapted for that, but that needs some work, currently GUI programming is still low-level 2. Dataphor provides a really good implementation of relational algebra, it's both easier to use than the Ur/Web (faithful) embedding of SQL and is still amenable to low-level query optimization if need be. Not to mention view updates, constraint enforcement and event handling rules. 3. Dataphor's much simplified database programming language is a good compilation target for ORM (Object-Role Modeling) tool NORMA. So we use NORMA to implement CSDP (the conceptual design procedure), which simplifies database design, then have the relational database generated for us, and we are left with tweaking screen forms and writing reports. When we enhance our tooling (e.g., finish mapping all conceptual constraints to Dataphor) this is going to improve as well.
I'm wondering if Diagrams could be used for interactive visualization?
Duncan Coutts' dissertation is a pretty good source: http://community.haskell.org/~duncan/thesis.pdf Specifically, Section 1.3
Haha you're being too hard on the community. It's steadily growing, just lurk on the IRC (150+ members on at any time) or the comp.lang.lisp But yes we don't have an army like Java or C++. Well, as _300_ put it : "They will know that free men stood against tyrants, that _few_ stood against many. That is enough."
What's the motivation for purescript if one can compile Haskell to JS? I see you have types matching Javascript (which is nice) and you compile to straighforward JS code, so your model closely matches JS expectations, which might be good or bad. One point where it seems bad is that you compile curried functions to function-that-returns-function which perhaps make them less or more efficient (I'm not sure, but I found it ugly), but also make them not follow regular JS interface (so if you export a curried function, it will have a weird interface for JS code). Perhaps a better approach could be to make all functions take all arguments, and when partially applying make another closure. (actually I'm unsure if you gain anything from that, but you would avoid currying when you don't need to eg. if you receive 3 arguments and you apply 2, you "save" one currying. I'm unsure if you could transform code to have even less currying, but it seems to go against your goals) Also, what about functions with a variable number of arguments (common in JS native code)? Or functions with horribly complicated interfaces like "if the first parameter is an object with that property I will expect two more integers, if not I will expect one more string", which you might encounter if you rely on other JS code. Another thing you seem to have is row types, does it work like OCaml? Another thing is whether you type error messages are sane (I suppose that to preserve some sanity it would require type annotations) ... one more comment, why do you have blocks *and* do notation? are blocks more "low level"? (I suppose that even if so, all their features could have been replicated inside a do block)
This kind of reference is actually very useful, because the functions in Diagrams are very abstract and it's important to know what the inputs and outputs of the functions are. I wish something like that would exist for Lenses.
Good luck. Since you come from Java, you might also be interested in [Frege](https://github.com/Frege/frege), a Haskell-ish language for the JVM with Java inter-operability.
Man , i was planning on starting one in a week. Good luck , i hope you won't mind if I do one too! 
I've detected a hexadecimal color code in your comment. Please allow me to provide visual representation. [#ccaabb](http://color.re/ccaabb.png) *** [^^Learn ^^more ^^about ^^me](http://color.re) ^^| ^^Don't ^^want ^^me ^^replying ^^on ^^your ^^comments ^^again? ^^Respond ^^to ^^this ^^comment ^^with: ^^'colorcodebot ^^leave ^^me ^^alone' 
Never heard about Dataphor. It seems to be Windows only (and that's automatic no-go for me) and focused on developing Windows Forms CRUD apps. I can't see why Dataphor is better for this task than just to use Delphi or C#. 200 pages of documentation only shows few lines of code and many screenshots. It's not functional, not for the web. Frankly, I don't see how it's relevant to the article. And never heard about Ermine, thanks. "Haskell with row types" looks interesting. But again it's not for the web. And the main point of Ur/Web is the simplicity of interactive client-side web programming.
I've been thinking a bit about playing with it for a while. What are the problems one would stumble upon compared to e.g. debian?
Congrats! Cool! My plan for next few days will be to make embedding Diagrams possible in hoodle. currently latex is directly embeddable using xetex, pdfcrop, pdf2svg. lol
Good question! At the moment, no. In theory, maybe, although I think it would probably need to be faster first. We did some optimization work for the 1.0 release---making diagrams faster, use less memory, and decreasing the size of the output SVG files---but diagrams wasn't designed with performance in mind.
The big difference between `(&amp;)` and `(#)` is their fixity: `(&amp;)` infixl 1 and `(#)` is infixl 8. Neither was chosen lightly. The high precedence of `(#)` allows us to write things like `circle 1 # fc red ||| triangle 2 # lc green` without needing extra parentheses. However, you're right that `example &amp; lw .~ 0.5` would be a more uniform interface. It's certainly something to revisit in the future.
There's an effect system now (cool!) but the language guide still starts off by showing a side-effecting hello world that relies on an FFI import that produces unmarked side-effects.
Recently I implemented a program to transform some input into an SVG-based diagram. I was thinking if Diagrams could be used instead, the API looks very nice. Under "interactive visualization" I meant specifically the ability of users to "edit" diagrams (i.e., select, drag and drop or change primitives incrementally). Since users are inherently limited in the amount of change they can do, I think performance isn't much of a concern here, but I may be wrong. Sorry for the confusion.
&gt; The big difference between (&amp;) and (#) is their fixity: Ah, to allow infix operators like `.~` that bind tighter than `&amp;`. Personally, I have come to prefer ordinary names like `set` to the introduction of more infix operators like `.~`. It may require more parenthesis around the arguments, but I don't find the infix symbols from `lens` to be particularly well-motivated. I also prefer `#` visually. So, I would enjoy writing the diagram as example # set lw 0.5
Absolutely. Haskell needs more documentation, byorgey is a role model.
The main argument against linking everything statically is an argument from bugs and security holes. You basically rely on the builders for every package depending on a library to update their programs quickly after a critical bug or security hole in a library. If you look at packages and distributions shipping with e.g. openssl today you can clearly see that they do not in fact update whenever openssl has a critical security hole. How much worse would it be for libraries with a lower visibility.
That assumes work done by package maintainers in your distro doesn't matter.
Or you could just use Puppet to pin the packages to specific versions on all those hosts (pinning is the apt term but most other distros have a similar concept).
This Haskell meetup group in the Washington, D.C. area just had its first meetup in December. So this meetup (on Monday, January 27) will be the second. We had a great turnout in December and are excited to bring together the Haskell community in D.C.! Haskellers of all backgrounds and levels of experience are encouraged to come!
Glad to see a meetup group start in DC, I remember looking a while back and being surprised there wasn't anything around here. 
I'd considered changing that example to use the `Eff` monad, but while the PureScript code would be tidier, it would require a bunch of imports and unfortunately generate larger Javascript due to the type class machinery used by `do`. Edit: I've updated the example, but omitted the Prelude from the Javascript. I think it's still a good example of how to get a first result out of the compiler.
Oh nice! I may very well take a ride down!
Can you do a blog post on how you use Nix for Haskell development? I've only begun to dip my toes in that water. It's actually pretty important for to use Nix for everything once you start it. If Nix isn't managing your software development, then your executables eventually die when their dependencies are garbage collected by Nix.
It's odd that you are developing row types without checking OCaml, it has an incredibly well designed OO system with row types for methods. Unfortunately it's kind of disjoint of other OCaml features and there's a lot of duplication (like row typed objects vs. Caml records) and as such many people avoid using the OO features. [ perhaps it's a bit like your blocks vs. do, since row types and objects, the "O" in OCaml, came to Caml much after the rest was in place ] Your implementation of curried function is simple, yeah. But this (and also, your lack of support of foreign functions with grotesque types) makes a kind of "impedance mismatch" between purescript and javascript, which probably goes against the goals of the language. I mean, something like coffeescript offers an 1:1 interface, while purescript might not play well with existing code (or play well with code that will reuse it). But I may be wrong. In any case it's an intermediary between coffeescript or js11 (which merely add syntax sugar and some small features) and compiling Haskell to js. I like it, but perhaps the *differences* to Haskell would bite me, since the language is superficially so similar. I was toying with coffeescript and js11 as javascript substitutes, I might investigate purescript further. Right now I remember that the reason I discarded js11 is because it had no emacs mode.. do you plan to make one for purescript?
PS: OCaml does have [polymorphic variants](http://ocaml.org/learn/tutorials/labels.html#Morevariantspolymorphicvariants) (I don't actually use them though). And actually I think they are a form of row type (the variant name is itself be embedded in its type, like [&gt; \`A of int ] is the type of \`A 1), in addition to the OO row types.
Yes, as I say, I'd like to be able to emulate it, and this was discussed in the original project announcement thread, but there are some issues. Specifically, keeping multiple argument data constructors, and data-constructors-as-functions alongside polymorphic variants seems to present problems. See the `variants` branch on GitHub if you're interested in what I had implemented previously.
I think you forgot to post the script.
What's the advantage of doing this over just using glibc's quicksort directly via the FFI?
I just recently used diagrams to produce annotative overlay postscript files that I then composite into a video. It all worked out pretty nicely. If you have issues with one back end, try another!
Interesting. I'm just curious why this is a better way of collecting information than others, e.g. Wiki.
People still do, see various APIs (such as Google maps)
&gt; Not Signed There was a lot of excitement a while back about at least improving this, if not fixing it. What ever happened with that?
&gt; So, who goes through the packages and checks them? The same person who would decide weather or not to update in any other package manager. In the organizations I am familiarly with someone has this responsibility. &gt; Who decides that a CAS is suddently not a security risk? I would assume the same person who make most of the security decisions. In the organizations I am familiar with someone has the responsibly for deciding the what gets up dated when to minimize security vulnerabilities with out unduly causing hardship on other team members due to system down time. Am I wrong in thinking that all other package mangers face the same problem when security updates break functionality or cause large down time? 
I feel like describing Java and C++ as *tyrants* may be dramatizing the issue just a bit.
I think -XDataKinds and -XTypeOperators and perhaps other GHC extensions (which were introduced after this article was written) may make some of the "insanity" less crazy. (For example, type-level lists are directly supported). How much of the rest (like the function application) is also supported directly now, and for the parts that aren't, what are the reasons? I ask becuase some of the fundeps/type family/GADT hackery I have seen feels like programming in "assembly language" with Peano arithmetic etc, and I'd expect these things to become supported with a sane syntax and sugar once they are proven valid. (Which has been done for DataKinds, for example, yielding a much nicer/higher-level API into type-level naturals) http://www.haskell.org/ghc/docs/latest/html/users_guide/promotion.html 
Yup, you're thinking of cloaca, a combined birth canal / urinary tract / asshole: http://en.wikipedia.org/wiki/Cloaca
*Here's a bit from linked Wikipedia article about* [***Cloaca***](http://en.wikipedia.org/wiki/Cloaca) : --- &gt; &gt;In zoological anatomy, a **cloaca** /kloʊˈeɪkə/ is the posterior opening that serves as the only opening for the intestinal, reproductive, and urinary tracts of certain animal species. All amphibians, birds, reptiles, and monotremes possess this orifice, from which they excrete both urine and feces, unlike most placental mammals, which possess two or three separate orifices for evacuation. &gt;The cloacal region is also often associated with a secretory organ, the cloacal gland, which has been implicated in the scent marking behavior of some reptiles, amphibians and monotremes. --- [^(**Picture**)](http://i.imgur.com/IefIZ1N.jpg) ^- **^An ^avian ^cloaca ^or ^vent; ^in ^this ^example, ^a ^red-tailed ^hawk ^\(Buteo ^jamaicensis\)** [^(image source)](http://commons.wikimedia.org/wiki/File:Avian_cloaca.jpg) ^| [^(about)](http://www.reddit.com/r/autowikibot/wiki/index) ^| *^(/u/emchristiansen can reply with 'delete'. Will also delete if comment's score is -1 or less.)* ^| ^(**Summon**: wikibot, what is something?) ^| [^(flag for glitch)](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=bot%20glitch&amp;message=%0Acontext:http://www.reddit.com/r/haskell/comments/1vfgz0/how_can_i_replicate_something_like_cloact/cet83e3)
Take a list of integers, say `[1, 2, 3]` and add 1 to them: you get `[2, 3, 4]`. Take a tree that contains integers, you can do the same thing and you obtain a tree of integers. Even though these two structures look different, they actually have a lot of things in common, and one of these things is that you can iterate through the elements they contain and turn them into different elements. Data structures that have this property are called "Functors". And now that you know that, you will probably realize that you already know many other structures that are functors, even though you never realized they were called that. 
So you think any distribution has enough manpower to go through all 50000+ packages if one security leak occurs? This would surely take more than a day, enough time to exploit the security hole. Other package managers don't face the problem because updating the libc is enough. No need to update all other packages.
I found this reminding of language for interactive fiction such as [Inform 7](http://inform7.com/). There are surprisingly interesting links between fiction/narration and type theory, see for example [Chris Martens'](http://www.cs.cmu.edu/~cmartens/) work and [thesis proposal](http://www.cs.cmu.edu/~cmartens/proposal.html).
Somewhat ambiguous title. At first I read "the author never stopped bragging about it". But I think your intention is "the author never stopped his effort in favor of something else, like bragging about it on reddit." Right?
I've not finished reading yet so please excuse me if this is mentioned, but how are rewrite rules applied? I ask wondering if a rewrite rule which was enabled by another rewrite rule will ever get forgotten and also if the order in which rewrite rules are applied can affect the end result.
The dedicated section of the GHC manual is pretty good: http://www.haskell.org/ghc/docs/7.6.3/html/users_guide/rewrite-rules.html In particular, the compiler will try to apply rules as it optimizes a program, so a rule may not fire in the first place, but when some inlining exposes a rewriting opportunity, it'll most likely fire. It's however sometimes tricky to inline just the right amount in order to make rules fire in *client code*.
I find your title obnoxious. If posting something you've done to Reddit gives you a little boost, more power to you. And on a more practical matter, it can get you valuable feedback that can save you a great deal of time and effort. Maybe you avoid a dead end, or invest more time into making it more accessible so that you can grow your audience and get better feedback later. There's a pretty good article I read somewhere (Joel on Software, Coding Horror?) that is tangentially relevant. Basically, the thesis is that a lot of open source efforts seek collaborators either too early or too late. Too early and the effort tends to fizzle; too late and you've wasted a lot of effort and/or don't have much room for a collaborator to really get invested into the project. Feedback here, is a very limited form of collaboration.
Seconded. This is also the reason why successful researchers never work alone -- it's way too easy to get caught in a dead end or spend a lot of time on an approach that cannot work.
I am just impressed that people are emotionally capable of working that way. I can't. I didn't really want to put down people posting their work on reddit. I just wanted to point out what I personally see as emotional strength. A kind of strength I personally don't have.
This commit is pretty much larger than my entire current project. https://github.com/RaminHAL9001/dao/commit/9893d64c7a54c01ba2df86b30c261a52bf801cef
"There is no hope trying to mix distro installation with cabal installation" That SICP reference is a great read to understand the built-in sandboxing mechanisms available in cabal/ghc. 
Gabriel. There's a very clear ordering to when rules fire. You may want to experiment with adding phase ordering info to the rules. There's like 3 standard phases, but you can use as many as you like. I forget the details but I can dig up some examples if you like. Most ghc optimizations on core have a very clearly defined phase order. 
This is all very nice. Laws are not just nice for composition and correctness but also performance. Terminology quibble: as far as I can see this is not stream fusion, but something else in the more general area of shortcut fusion.
&gt; I didn't really want to put down people posting their work on reddit. So who forced you?
If I remember correctly, Tekmo is currently not sure whether or not the pipes equations hold in presence of non-termination; he has only proved them assuming totality -- which I think is the right way, pragmatically, to reason about *pure* code, even in an impure language such as Haskell.
What about simply "You can verify that a rewrite rule is safe using equational reasoning"? I think your readers should by now be educated enough to decide when this proof techniques applies. (They probably also know that you are in love with algebraic laws and in particular category theory, but you seem to enjoy mentioning that in the middle of your extremely-interesting-in-any-case posts so much that I wouldn't want to sound critical on that part).
in silk you can very quick and easy collect references to some kind of information from various sites, but main advantage is that you can embedded the collection's List, Grid view to other web sites. you can find embedded code under "EXPLORE" link. 
Is there a talk to those slides? Or even a paper? Slides by themselves are a wholly inadequate medium.
Thanks for the correction! I'll edit the post to mention this. Also, I'm a big fan of your work, by the way.
Fwiw, the commit messages could be improved by following the common Git commit formatting conventions...
The downside to defining your rules for higher numbers of phases is that if client code wants to benefit then they also have to build with at least that number of phases. For this reason I sometimes wish the default number of phases was higher.
You need to be willing to encounter software that's not packaged, and to package it yourself. Packaging it yourself is fairly straightforward, but underdocumented imo - which means you will also need to be willing to become part of the NixOS community and ask a lot of questions (of course, we're all really happy to help answer these!). Along with that, it's a pretty hefty paradigm shift, and if you aren't willing to work with the underlying philosophy (embracing Nix wherever possible), you may well find the experience painful.
Sounds like a good idea! I'll try and write something up soon.
Ah, I see. Yes, transforming some input into an SVG-based diagram should work well. As for having users "edit" diagrams, that is certainly something we want to support. One piece of the puzzle is already in place: using the "query" feature, you can easily map from coordinates (e.g. from a mouse click) to any information you want, e.g. what primitives in the diagram are located at that point. The thing we are still missing is a nice way to actually apply "edits" to a diagram (though there are kludgy ways to work around it). But that is one of the next big features we want to add.
Yes, independently of `(&amp;)` vs. `(#)`, the idea of making things like `lw` into lenses is an intriguing one. Though I am not sure what it would mean to have them be anything other than setters. For example, what would the result of `(foo # set lw 0.5 ||| bar # set lw 0.6) # view lw` be?
Cool! Let us know (via IRC or the mailing list) if there's any way we can help.
An interactive fiction engine written in Haskell in an interesting idea... 
As i library developer he does not know where and how his library will be used. So he cannot assume it is going to be used only with small inputs. I found that unsafe option is practically never worth it because gains are dubious at best yet chances of suspending all threads for a noticeable time are very real. 
fair enough. In my own work in progress library i actually wrap all my ffi calls so that on small inputs i call unsafe, and above a small threshold i call the safe ffi variant
That would be nice, but I'm 99% certain this is not how it works (determined experimentally).
My first time encountering someone complaining about the formatting of commit messages. It lead me to the [git reference](http://gitref.org/basic/#commit) and a more brief [tbaggery](http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html) post. Useful, thanks.
He uses that later kind of as prototype-based inheritance since without something like new there's no way to make another complex number of the same existentially quantified type.
This is where I'm not sure I agree. If you need to write a documentary on your commit then maybe you are committing too much? I find myself regularly breaking commits into as small a unit that makes sense and passes tests. Usually once you do that a pithy one liner is enough explanation. Also I generally work against some sort of issue tracker so you can redirect to the bug or story number if you need to provide more context.
[packaging gnome3 seems to be in progress](https://github.com/NixOS/nixpkgs/issues/247)
Thanks for posting, this material is interesting and somewhat easy to understand. I made it five pages before getting lost!... *The signature should be a* strictly positive functor: *it should be functorial in the state type (in order to allow the definition of the unfold for the final coalgebra semantics)*
Just watched all the videos. Looks very cool. Looking forward to trying it out. Very clever to show the buffer with the keystrokes. 
So awesome! you should definitely put a link in the README on github
I was going to ask about this: don't you need to have a consistent theory in order to prove such properties? My understanding is that Haskell isn't actually consistent, unlike languages like Agda or Coq.
Reminds me of https://gist.github.com/NicolasT/2764181/raw/e4e9592b28e619edb76f2db293c291c6c19d04ee/Lookup3.lhs
Good thing we program in a language with laziness because we'll definitely require an infinite structure to model a container to hold the I.O.Us you've been racking up.
So Paolo Capriotti has been helping me transition my proofs to Agda. He already has completed the proofs for unidirectional pipes so it looks like a complete Agda proof might be in reach.
Thanks! I am ignorant about this stuff. Agda can be a framework for writing proofs, but how can you write an Agda proof about an inconsistent language like Haskell? (the issue here being that an "undefined" or similar expression can break many proofs) Is there some way to recover consistency (for example, by restricting Haskell somehow)? Would this be useful for real world, production Haskell code? I read somewhere that some people were using paraconsistent logics in program verification, in order to handle inconsistency while escaping the [principle of explosion](http://en.wikipedia.org/wiki/Principle_of_explosion).
I clicked over to chrisdone's style guide (which SHM enforces), and found a bunch of advice that is the opposite of what I have read elsewhere. Chris's advice: ==== Never use (&gt;&gt;) where do will do: main = do bar mu Not main = bar &gt;&gt; mu Operators Never use operators when a simple English name is provided: len = fmap length getLine demo = over _1 (+2) (5,4) Not: len = length &lt;$&gt; getline demo = _1 -- whatever the lens operator is to do `over' Never use ($) to avoid parentheses: len = foo $ bar mu zot -- bad len = foo (bar mu zot) fork = forkIO $ do go go -- bad fork = forkIO (do go go) Always space out multi-operator expressions: foo = x y * z * y If you can't resist using ($), never mix it with (.) like this: foo = foo . bar . mu $ zot bob Use parens: foo = (foo . bar . mu) zot bob (Tee hee!) ==== I don't know why it ends with "Tee hee!". Is the whole thing a joke I don't get?
Inconsistency prevents the type system of Haskell from being completely honest, but so long as Agda is consistent then theorems proven in it should be trustworthy. Part of any proof about behavior of a Haskell program will have to include non-termination. I have no idea how that's done in Agda, but you'd like to prove statements like Gabriel's for (yield x) f = f x holds even if `x = _|_` or `f = _|_` or both. That's a reasonable thing to ask for even in Haskell.
Well I don't know if Haskell has ever been formalized (specially the newer language extensions) and I might be saying nonsense, but with an inconsistent *theory* (such as a flawed Haskell formalization) you could prove "false" and from this prove anything, and if "everything" is true due to inconsistency then any proof is useless. It's different from the situation where you can handle \_|\_ as some kind of error or non-terminating computation that, albeit inconvenient, does not explode your theory. Perhaps we are talking about different kinds of inconsistency, I would like to be corrected or better informed.
Thanks for that insight. I would love to use a language that's formally verified, but having a precise mathematical definition is only the beginning - the most popular SML implementation, SML/NJ, is known for [deviating](http://mlton.org/SMLNJDeviations) from the standard, while I don't know of any mathematical proof that mlton *actually* implements the standard correctly. While I know SML, but don't know how to take advantage of the standard for producing mathematically-correct code. I'm also not smart enough (yet!) to write Adga code.
For http://www.lighttable.com, via http://www.chris-granger.com/2014/01/07/light-table-is-open-source/
In most Haskell code, ' means strict if it has any convention at all. Better to name the ffi bindings _safe and _unsafe and have the exposed interface switch from one to the other at some input threshold. 
Please don't censor yourself to placate passive aggressive critics who prefer to snipe instead of contribute.
Holy blue smoke! I hadn't seen that LightTable was open-sourced. I'll have to give it a whirl.
Thanks for spotting that. I originally wrote `zip` and then changed it to `sum` but didn't double check.
chris has lisp roots, he has weird taste.
Don't worry. I feel that the link to my equational reasoning post is enough of a positive endorsement for equational reasoning in Haskell. Plus, my blog is pretty much non-stop cheerleading for Haskell (it's right there in the blog title), so I don't think the endorsement of Haskell is lost on any readers. However, I wouldn't back down on something of greater substance.
My recommendation: Nothing. While quirky, the situation is wholly synthetic and would never appear in hand written code, and easily avoided in generated code (though unlikely there too).
I am not sure if the usual caveats apply in the case of simple, pure foreign function calls like those to a hashing function.
Probably easiest to expose them in separate modules, a `.Safe` and a `.Unsafe`, and default to one. Defaults matter, though, and it's important that whatever default chosen is appropriate for the *average end user*. Given that the *point* of using a hashing algorithm like `MurmurHash3` is performance, it seems to me that making unsafe the default is wisest.
No, that is not directly related to consistency. You can reason equationally on C or PHP programs, the question is to know which equational reasoning rules you are allowed to use. The more side-effects you have, the less you can do when arbitrary functions may be called (eg. you're not allowed to reorder them). Already in presence of non-termination, some reasoning techniques that look morally right are not available. But there are sound reasoning systems for effectful program -- and it's even a very exciting research area. Consistency of a proof system allows to use the type system *itself* as a reasoning tool. You cannot trust a proof *written as a Haskell program*, but that doesn't mean that you could not trust proofs *about* Haskell programs.
I was able to follow along, but I agree some other material would be easier.
How does one install this?
&gt; C has a definition, but it looks a bit informal to me and I suspect that nobody actually wrote the whole C semantics in a formal language This has actually been done not one, but several times, for extremely reasonable subsets of what people actually use as the C language. The most pervasive effort in that area is [Compcert](http://compcert.inria.fr/), a formally-correct compiler for (a large subset of) C. It has formal semantics of C, and various assembly languages, and mechanically-checked proofs that compilation preserves the semantics of the input program. Regarding undefined behaviors, there is a difference between compiler and program analysis tools. Compilers usually implement a *refinement* of the standard semantics where more behaviors are defined -- it is correct for them to do so. For example, Compcert will choose a specific evaluation order, and some programs that would have exhibited undefined behaviors with another order will execute correctly with this stricter model (while formally they don't have a semantics, so it would be ok to fail). Making more behavior defined is fine for compilers, but not so for static analyzers where you want to capture any chance of misbehavior, to let the user know that the analyzed program is safe not only with a given refinement of the C semantics (Compcert's or Gcc's or Clang's), but is actually safe with any possible amount of allowed undefined behavior. See [this blog post](http://gallium.inria.fr/blog/bytes-and-pointers-in-compcert/) for example for a discussion of this aspect. Some tools specialize in precisely capturing as many undefined behaviors as possible, such as [this C semantics](http://fsl.cs.illinois.edu/index.php/Defining_the_Undefinedness_of_C) based on the rewriting framework K. On language specifications. One language has a well-specified semantics: SML. For others, we do as we can, and researchers routinely approximate semantics to make progress. Languages that have a standardized specification (C, C++, Haskell98) are sometimes seen as better-place on the road to formalization, but a textual, informal specification is often very far from actual precision, while a language with a simpler design or well-defined proper subsets (such as System FC as a formal core for GHC Haskell), even with a single implementation, can actually make our life easier.
I think it's actually helpful for the Haskell community to have some kind of sanity check between well-deserved enthusiasm and borderline marketing speech, at least in places where the latter is not expected (Tekmo is not trying to attract investors in this blog post). I'm sorry if you find that "passive aggressive"; the sarcasm in my comments probably didn't help with that, but happily Tekmo is good enough to ignore it. In the context of this specific blog post, do you mean something specific by "contributing"? -- besides actually working out whether said equations actually hold in presence of bottom, which is a bit too much work for me right now.
As far as I'm aware that was only a conceptual demo, not an actual implementation.
You can safely ignore that sentence (at first). It just says that the signature should have certain properties to work.
CakeML!
I'm tempted to suggest that unary negation is a mistake. I'm rather a fan of the Zen of Python: "Special cases aren't special enough to break the rules.".
 foo = foo . bar . mu $ zot bob is not: foo = (foo . bar . mu) zot bob but: foo = (foo . bar . mu) (zot bob) or: foo = (foo . bar . mu . zot) bob 
&gt; So you think any distribution has enough manpower to go through all 50000+ packages I was not talking about distribution maintainers. I think you mixing your conversation with aseipp here. &gt; Other package managers don't face the problem because updating the libc is enough. No need to update all other packages. I thought you could just update libc. It would be an impure operation so you would loose some of the normal benefits you get with nix above and beyond other package managers, but you would not loose out either. I have done this operation with libc but the I have preformed other impure operations with dynamically linked libraries to get some applications to work. If you have tried this and failed I would be interested in hearing your insight on why it failed.
Unless I'm really misunderstanding, that sounds a lot like Stackage. Of course, that hasn't really caught on (yet?).
Yes, unary negation was a mistake in Haskell. It should either be removed or generalized to arbitrary prefix operators. I also believe it was a mistake to make unary negation have the same precedence as binary minus. I said so way back when, but I lost that argument.
*Rerun*? Where's my cooking-based puns? I want `tasty-seconds` or `tasty-picky-eater` ;-)
Extract the [ZIP file from GitHub](https://github.com/jetaggart/light-haskell/archive/master.zip) in the `plugins` directory inside the Light Table directory. Or, as in Light Table documentation: Write plugins for Light Table? The plugin API isn't available quite yet, but it's something we're working furiously on. You can, however, load some plugins that already exist in the wild simply by putting them in /path/to/lighttable/plugins/plugin-name.
 Good catch. That makes the advice even more confusing.
actually doesn't work with this method.
Thanks for your input. I am aware that extensional equality has a specific meaning that is only loosely related to this issue. If there's a term for "cut 'n paste expression equality ignoring ambient environments", I'd like to hear about it. The Report says [this](http://www.haskell.org/onlinereport/exps.html#sect3.9) about parenthesized expressions: &gt; The form (e) is simply a parenthesized expression, and is equivalent to e. So parentheses enforce an order of application, no surprises there. The Report describes line [comments](http://www.haskell.org/onlinereport/lexemes.html#sect2.3) like this: &gt; An ordinary comment begins with a sequence of two or more consecutive dashes (e.g. --) and extends to the following newline. The problem with -- is that it is also a sequence of two operators (that can be broken up using parentheses, as seen above) as well as a possible name for a single operator. Nested comments avoid this by using curly braces, which are special characters in Haskell; […][no legal lexeme starts with “{-”](http://www.haskell.org/onlinereport/lexemes.html#sect2.3)[…] Assuming the latter (like Haskell does), the fact that the Haskell implementation does not complain about an undefined operator, but silently produces an arbitrary result, is worrisome. Assuming the former, the input line is not ambiguous, as stated in NegationBindsTightly, because the unary negation operator exists. An arbitrary result is still produced. I'd much prefer an error. Your first example could be solved through a unary minus that binds even more tightly than function application. The warning you propose does sound reasonable. There might be some weirdness with sections, however? Though, it's hard to imagine them to become even worse.
If you believe there is an error in the logic of Tekmo's post, pointing it out is a contribution, if you point it out in a way that clearly indicates the problem and possibly a solution, and doesn't mock the author for his point of view. Mocking is better suited for people pushing actively harmful ideas, not people slightly conflating mathematical theory with programming practice when making claims that about programming practice. I used the word "censorship" because complaints about Tekmo's blog aren't just about a piece of text, but they can affect the quality and quantity of future publications, and I for one do not want to see reduction in haskellforall output rooted in a concern that its informal language isn't precisely correct in metamathematical ways that are tangential to the topic at hand, or because the human author is compelled to suppress his enthusiasm. You obviously have a firm grasp on the mathematic at issue, certainly more than I do, and I welcome edificational pedantry you offer. But save the condescension for Saturday scholars like me who just skim for understanding ;-) (Yes, a Haskell program can have bugs if you don't reason about the fully loaded computation model. You still get far better results with a basic model than you do when coding in a less logic-conformant and logic. And Tekmo wasn't making any dubious claims inside theory itself). 
Excellent stuff. Informative and, also, entertaining.
I'm going to have to save this link for later when I get time again to work on some bindings to C I've been trying to implement. This will definitely help me figure out where all those segfaults are coming from, and more importantly why they're occurring.
This very good news! I hope it will be part of the ghc distribution.
That's nice, thanks!
The Haskell plugin listed in the LightTable plugin manager is https://github.com/heyLu/lt-haskell Seems like it's not related to light-haskell.
Thanks for that in-depth response! &gt; Making more behavior defined is fine for compilers, but not so for static analyzers where you want to capture any chance of misbehavior I was thinking about that, *compilers* already choose the implementation-defined (and undefined, unspecified) aspects of the language. But if they wanted some algorithm to be usable between different compilers, it would be nice to consider the algorithm for the general C case. If it were commonplace to reason about C generally, perhaps verifying that such code works wouldn't be so expensive. &gt; SML I was thinking this, but I pointed out [here](http://www.reddit.com/r/haskell/comments/1vl74c/haskell_for_all_stream_fusion_for_pipes/cetvcp5) that SML/NJ deviates from the standard for arbitrary reasons (both silly and well reasoned). I have no idea of mlton is proved to actually implement SML - for a piece of software as complex as it, I must assume that it doesn't.
Only setters, indeed. Which raises the question of why it's a good idea in the first place... But I think that the "advantage" of `set lw 0.5` over `lw 0.5` is that setters are to required be idempotent. A function `lw` could be any transformation, but any function obtained from `set` must be idempotent. I think that's a nice visual indication of a formal property.
Future Haskellers salute you for your efforts!
Thanks. Good and --for me-- very helpful article. Maybe the hackage docs can link to the article (or otherwise maybe incoorporate some of it).
Yeah, I agree that the Hackage docs are totally unhelpful. At the very least they should link to the paper or something. The problem is that the project has no repository or bug tracker.
I was thinking about just that a few days ago, actually. I have no idea if we have HPC-as-a-library, which I would imagine is a prerequisite for this.
What about more fundamental stumbling blocks? Are there messy real-world things that Nix's abstractions can't handle? The existence of NixOS suggests not, but is there a simple proof that Nix is as powerful as, say, apt? E.g. a guide to converting an apt package to a Nix component? (I'm not really familiar with Nix, or apt for that matter, so I apologize if this question doesn't make sense.)
Go back to r/programming 
FYI there's some work in progress for giving ghc proper stack traces that should land in 7.10
Wow! That was quite an entertaining read!
I know &amp; I'm excited! ;-) Links: http://www.reddit.com/r/haskell/comments/1rexqq/the_haskell_cast_4_simon_marlow_on_parallelism/cdmv7jb https://ghc.haskell.org/trac/ghc/ticket/3693 I think for debugging I have a decent solution (for my purposes) now, I think I'll see how the new stack trace stuff in 7.10 works out before I seriously look into making my profiler work with GHC programs.
So far it is mostly that the cost of implementing and maintaining the control for a problem we haven't yet had has outweighed the benefit. A lot of these ancillary goals were shed in the process of just getting hackage 2 out the door. Now that it is shipped a lot more is possible.
The question makes sense, but I'm biased in my answer - if there was anything it fundamentally couldn't do, I wouldn't be using it. A nix expression for packaging is not much more than a language to drive a shell session - so in that sense it can do anything you could script in a shell (in fact, most sections of building packages are lines of shell script structured in a Nix expression). We don't have any guides about how to convert things yet.
I don't think anyone is choosing Haskell over Go on the basis of "style". They are languages with fundamentally different semantics that are designed for different kinds of problems.
Why all the hate on databases? What your application can do with data is cool, but what your customers can do with that data is amazing. Hopefully at some point we can get to the point where a more specialized storage mechanism can provide the same flexibility as a SQL database, but that trend only started gaining traction recently IIRC.
Nice! There is an ongoing effort currently into making more sense of GHC's stack, so this is quite timely. So far we have two approaches to the actual stack unwinding part: - doing it from the RTS, which might not be what you want for a crash - leaving it to GDB, which despite improvements will probably never work that well Using a specialised external tool like this for debugging is an interesting alternative indeed. I wonder whether it would be possible to share the code with the RTS somehow?
I think it makes sense to share as much code as possible with the RTS as most functionality is already in there. The issue is that the code doing the debugging will most likely not run in the process which crashed. Of course, RTS code assumes it can just deference pointers like usual, not first copying them into the current process or mapping the memory pages etc. Also consider that you might have a 64bit debugger looking at a 32bit executable and vice-versa, and that there are different configurations of the RTS (threaded? debug? profiling?) with different memory layouts. An option is to actually run the debugging code in the crashed process (most platforms can do code injection through APIs like Win32s CreateRemoteThread or Mach's thread_create), or install a signal handler from within the RTS, but that's also not without its issues and challenges.
Do you really mean to say that you know people who have chosen Go over Haskell specifically for its syntax? I will take your word for it, but I find it surprising. Choosing the language which supports the more common syntax ("using normal indentation") seems a somewhat superficial distinction when you consider the deep differences between the two languages. I would expect that most people who choose Go over Haskell do so because they believe its semantics are a better fit for their application domain, or because Go is more familiar to them. The familiarity is not a simple matter of syntax. Haskell's syntax is different because it requires a different way of structuring code. Sure, if Haskell embraced a "style that works with normal tools" it might attract more users, but I don't know how it could do so without compromising more important language concerns. Haskell does not have a history of making sacrifices for the sake of mass appeal.
Can you think of an automatic way of doing this? The problem I encounter is that we can't just copy a single chunk of stack memory into the debugger process. For instance, at minimum we need a way to compute the size of a stack frame, otherwise we can't walk the stack. The RTS has a function called stack_frame_sizeW, but to call it we need more than just the closure on the stack. We need all the data it references and might need to look at. So I needed to modify stack_frame_sizeW to deal with this (See here https://github.com/blitzcode/ghc-stack/blob/master/ghd/rts_wrapper_noprof.c#L35). An idea would be to inject some code into the target process and run the RTS functions directly in there. Alternatively, one could try to map all the memory or do on-demand mapping in a page fault signal handler or something like this. It's also possible to 'undump' a core dump back into a live process, so maybe that's an interesting approach as well. fun problem ;-)
This was a sort of a reflection for myself, not trying to hurt people. Anyway I thank mn-haskell-guy for publishing it here. I worked a lot with J2EE and SQL databases. This is not about how bad these two guys are, but about what opportunity was lost 15 years ago with [ObjectSpace Voyager](http://www.cis.upenn.edu/~bcpierce/courses/629/papers/unfiled/VoyagerTechOview.pdf) for having something like a business plataform that scales to the internet. 
This would probably involve adapting stack_frame_sizeW &amp; co so they can work for memory outside the current process. Doesn't help that that particular routine is performance critical, which reduces the options here somewhat (preprocessor? copy? ugh). The upside is that from there it would be possible to make this just another tool built alongside GHC (as augustss suggested). Theoretically we could even make an Haskell program out of it, as Arash is currently working on an Haskell-land API for stack traces. Hm.
I guess, but I think stuff like this just makes APIs confusing, and the burden of developing a good library binding is up to the developer, which involves some trade offs. I think that functions like MurmurHash3 are fast enough that when used correctly, `unsafe` is precisely what you want, to maximize its speed and throughput benefits. They are tiny, don't block, take excellent advantage of processor resources, etc. They're also designed for relatively small inputs obviously, so that helps put an upper limit (practically) on how it will be used. The primary reason I could think of as to why `unsafe` might be a really bad idea is if you let users blindly stuff huge amounts of input into it (resulting in a DoS through thread blocking in the RTS,) but A) letting users stuff *gigabytes* into your interface non-incrementally is bonkers anyway, B) it's not designed for that (so who knows how its collision properties might change or degrade) and C) you could just do some other attack which is likely easier anyway if you want a DoS. So I think the downsides here are pretty small, but the advantage of a fast function is clear.
That's the route I chose for the ghd program, re-use as much structure and constants as I can, use modified RTS code to deal with the cross-process marshalling of data structures. I don't think it's possible to have a clever pre-processor hack or so, the only option that I can think of is something on the virtual memory level, like running the code in a cloned / undumped process or having a page fault signal handler which maps memory from the target process on-demand. I also think that for a release quality tool we'd need something less fragile. The RTS is already complicated enough, I don't think anybody wants to maintain some second debugger / profiler codebase. I did not look at the GHC Haskell APIs yet as it seemed easier to write this kind of program in C/C++, especially since there is no Mach / ptrace API wrapper for Haskell and I already had C++ atos code.
An easier productivity win might be a flag to run *all* tests in the most-recently-failed order (with never-run tests given priority equal to failures on the previous run).
I agree, the use cases for Haskell and Go have a good amount of overlap. Good tooling and style concerns can make a difference.
With any luck, Haskell will be able to continue avoiding success until the next generation fp language fad comes along and it can then be retired and join the rest of the abandoned half-finished fp languages: sml, clean, ocaml etc.
You can't even put two operators next to each other without whitespace, since they will be interpreted as one operator. Hence +- wouldn't work either.
&gt; the community's chosen style is to never indent anything, and align things instead. I am curious, what do you mean by that?
Mind the italics. Your sentences should be full of emphasis because of their content alone. Also, the internet is saturated with beginners-learning-Haskell blogs. If you're writing it just for yourself, that's cool. But perhaps it would be more interesting to see how you can tie in your Haskell experiences with geometric group theory (or any other interesting subject that might distinguish it).
You will then also love the stuff in /r/dependent_types/. Even more category theory over there. And another decade away from mainstream :-)
This is so cool!
Looks to me like a consequence of the implementation method. The more obvious "just macro expansion" semantics requires the definitions of pattern synonyms to be exported (which is one reason why SHE has to generate .hers files). As far as I can tell, this implementation uses opaque matcher functions which take success and failure continuations. I guess that saves work at the cost of introducing weird corner cases.
The first line of your blog post has a spelling error, I'll let you find it!
Not sure I like this. It would make me nervous every time I used a pattern synonym: "Does this have the semantics I expect?" Perhaps in practice the use cases don't intersect very deeply with this issue. I'll have to experiment with its use. It's not very often that I work over partially defined values, but it's always nice to know that it Just Works.
Cool. This was me a year and a half ago. I like to think that the Curry-Howard correspondence means that writing Haskell code feels a lot like writing a math paper. The comments are exposition and the rest is definition-theorem-proof :-)
Yay!
More than a decade. 
You make a good point. I use too many large adjectives, and sometimes italicize them, but that's not the mark of good writing.
Oops, that's embarrassing. Fixed. I was so focused on trying to figure out how Octopress worked, I wasn't proof-reading carefully.
That's good advice. Thanks.
If you don't mind me asking, what is your background? Were you looking for a job like me? And if so, did you manage to find one?
I don't know what you mean by "unsafe in that sense" - I don't think I was trying to mislead anyone or made any statement about how unsafe is unsafe. Why can't you use unsafe for calls that take longer than a microsecond?
Patrick Wheeler's trick is really clever. He noticed that you could single-handedly replace all the function in `Pipes.Lift` with a single `distribute` function. The implementation is really clever: distribute :: ... =&gt; Proxy a' a b' b (t m) r -&gt; t (Proxy a' a b' b m) r distribute p = runEffect $ request' &gt;\\ hoist (hoist lift) p //&gt; respond' where request' = lift . lift . request respond' = lift . lift . respond You can read that as: * Begin with this type: Proxy A' A B' B (t m) r * Insert a new `Proxy` layer below the `t`, using `hoist (hoist lift)`: Proxy A' A B' B (t (Proxy a' a b' b m)) r * replace every `request` with `lift . lift . request`. This migrates all requests from the outer `Proxy` layer to the inner `Proxy` layer (requests generalize `await`): Proxy a' a B' B (t (Proxy A' A b' b m)) r * replace every `respond` with `lift . lift . respond`. This migrates all responds from the outer `Proxy` layer to the inner `Proxy` layer (responds generalize `yield`): Proxy a' a b' b (t (Proxy A' A B' B m)) r * Now you can delete the outer layer using `runEffect`, since it no longer has any `request`s or `respond`s and it will type-check as an `Effect`: t (Proxy A' A B' B m) r ... and you're done! This is a great example of how category theory abstractions (like `hoist`, `lift`, and the `request`/`respond` categories) let you manipulate things in a very abstract way, yet it just magically does the right thing. Also, Patrick is defending his thesis today, so everybody should wish him luck!
I should note that this post was too long for a text post, so I've finally been forced to use the NFS domain I registered but never found anything to do with!
I don't think it's any worse than strict constructors. Another parallel could be drawn with the strictness of regular functions which also doesn't show up in the type or anywhere else.
The one FP Complete retweeted, just a little biased, don't ya think? ;-) 
thanks for that link and suggestion.
This is great! Generally I think vendoring (putting forks into a /vendor directory) is a better, but it still needs some automation like this. Maybe this can be made to work with a local fork and a local-repo instead?
well, you shouldn't use unsafe ffi calls for things that take more than a few seconds, because the safe ffi overhead is neglibable if your op takes more than 5-10µs. during an unsafe ffi call, the GC can't run, and no other haskell thread can use the cpu capability that was running the thread that made the unsafe ffi call! :) 
One correction: (&lt;$&gt;) is not a synonym for pure, it's a synonym for fmap. It's related to pure because of the applicative law, (pure f) &lt;*&gt; x = f &lt;$&gt; x
Thanks! I knew I was playing fast and loose with definitions there. I'll correct the article.
The solution to a large number of beginner blogs is not to ask beginners to stop writing, but rather to ask experts to write more.
I believe the problem with reasoning about Haskell is not so much purity (which Haskell has, outside of FFI-related stuff) as non-totality. Haskell functions are pure, but they are not the same thing as the mathematical concept of functions between sets. The semantics of Haskell functions (and the pure subset of functions in most languages) can be represented mathematically in Domain Theory, in which the functions are between pointed complete partial orderings. These define a "definedness" or "information content" ordering within the poset. Each is inhabited by the special lower bound value "bottom" which has no information content. This represents non-termination. You can "lift" any set to a flat semantic domain by taking its union with the "bottom" value, so Haskell functions are *close* to standard functions. There are a number of papers on algebraic manipulation of functional programs that get into the details of how to prove equivalence of programs in this new realm of functions between domains. But it turns out that Haskell throws another wrench in things with non-strict evaluation, which does not necessarily preserve "bottom"; I.e. in a strict language, f(bottom) = bottom, but it is not necessarily so in Haskell. This prevents a definition of sum types (although the dual problem occurs for strict languages and product types!) In strict functional languages like SML, non-termination is considered to be an effect. In Haskell, non-termination is treated by the type system as a value instead, making the domain-theory underpinnings explicit. So in that sense, a looping or non-total function is still pure, as the undefined value explicitly inhabits its type. It turns out that all this doesn't make a huge difference in practice; the "Fast and Loose Reasoning is Morally Correct" paper linked elsewhere does a good job of justifying our tendency to manipulate our programs algebraically and assume that they'll still be correct.
I can't help thinking there's a fully general first-class pattern implementation waiting to be discovered, perhaps along the lines of `Prism`.
On a slightly-related note, does anyone know whatever happened to Camp? The website is dormant and the [ML is just spam](http://projects.haskell.org/pipermail/camp/) as far as I can tell.
Ooh, it works in conjunction with view patterns! Now that is awesome! When the view patterns feature was initially proposed, I was not too fond of it, because it had a different syntax than ordinary patterns. But this extension brilliantly decouples view patterns into two parts (1. apply any function, 2. get nice syntax)!
I disagree with your analysis. I could just as well claim that C functions are pure, because I decide to see them as denotations in the proper space of `#World`-passing functions -- which is the same kind of best-of-both-world point of view people use when they claim at the same time that monadic code is pure yet just as readable as imperative code. I personally suspect that the reason why domain semantics reserves such a specific status to non-termination is that its major success is in giving a model for the *untyped* lambda-calculus, which is fundamentally non-total yet has no other forms of effect. I would say that divergence is not fundamentally different from other effects (only simpler), but I reckon that there is no consensus in the programming language community on this point, and I'm not an expert in type-and-effects systems. It may be the case that further research (or existing research I'm not aware of) explains otherwise, but for now I stay with my gut feeling. You call `seq` "another wrench". Is forcing an effect? To me that would be similar to considering that writing in memory is pure, the problem is when (anyone) reads it. Finally, I think one should make a distinction between the fact that call-by-{name,need} calculi have no clear-cut notion of "value", or embed non-evaluated terms in that category, and the fact of allowing computations (in a call-by-push-value sense, if you wish) that diverge (or act on state, raise exceptions, etc.). Sure, people are good at neglecting the latter fact in most situations and reasoning about it when they need to, but I'm not sure that justifies considering non-termination "pure". I think focused/polarized calculi are the most promising way to think about evaluation order, in a way that is orthogonal to the presence or absence of side-effects, and/or the totality of the calculus. They tend to have different, proper notions of products and sums (with a richer zoology than the purely categorical constructors, because as [recently explained](http://lipn.univ-paris13.fr/~munch-maccagnoni/files/duploids.pdf) the natural models have non-associative composition). 
[-XPatternGuards](https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/pattern-and-guard-extensions#patternguards) should also work. Some day we might see class pattern methods.
Exactly; or help beginners become experts, while getting them addicted to writing.
Yea, and for those packages that we don't have expressions for, there is `cabal2nix`, which lets you create a package in seconds.
Just a small typo, I believe. &gt; Think of a Maybe Int as a box that might contain ~~and~~*an* Int and might not. 
I didn't pay much attention to this, always thought it was just a way to give a name to (and reuse) X in 'case ... of X -&gt; ...' etc. But looking at the Wiki page there seems to be a lot more to it! Somebody please write a tutorial ;-)
Nice syntax for view patterns was indeed a big motivation for the pattern synonym proposal.
Databases are probably the single most powerful development in commercial software engineering. Of course people hate them. Interestingly the modern trend to NoSQL is pretty much reinventing COBOL. Something this article accuses the Java DBMS focus of doing. It certainly has its place, lots of companies still use COBOL, but 99% of the time if you have any serious quantity of data you want a SQL database.
Thanks for that! Fixed.
Very good!
Historically the incoming money was from GSoC, not donations - it's not huge sums, but it's more than was needed for hosting costs which was all it was being spent on.
To understand &gt;&gt; and &gt;&gt;= you need to understand monads. Try reading this http://learnyouahaskell.com/input-and-output or this http://book.realworldhaskell.org/read/io.html
This is `prompt` using do notation: prompt s = do putStr s line &lt;- getLine return (read line) 
Your basic understanding is exactly right. Now you just have to generalize. :)
Just spitballing here. I like how GHC implements all of these things as language pragmas instead of right into the language. Do the GHC devs typically do this to test the waters, then add in these features naturally as time goes on? I don't really see this elsewhere. Always seems like other language designers are just throwing new features into languages all the time.
Also 'fynonym' in 3rd paragraph of Applicative section.
(author of the linked OP here) @timthelion wow, thank you so much for your kindness, and for sharing my work! Although I promise you, once I get this thing done I will be shamelessly self-promoting all over the Internet, including on Reddit. I just don't think there is anything to brag about when you have written all this code over such a long period of time that doesn't do anything yet. But the read-eval-print loop is working now, and I have incorporated most of the features I need to do some proof-of-concept demo programs. So I see a light at the end of the tunnel.
Maybe the OP is a [fan of the long S](http://en.wikipedia.org/wiki/Long_s)?
There is a Haskell standard which evolves quite slowly. The idea is that without LANGUAGE pragmas you get the standard language. As people try extensions and the consensus is that they are useful they will get incorporated into the standard. 
I gotta say if I use non-`IO` monadic code I tend to use `&gt;&gt;=` and in `IO` I'm more likely to use do-notation. I don't know why.
Well, I do talk to people about my ideas to avoid dead-ends and to hatch new ideas, but I never talk about my project because I don't think it is presentable yet. One problem I have is I maybe too often incorporate a new idea into my code which sets me back a few more months working out the kinks. But this is a personal project. I have no deadlines and no users, so I can make as many changes as I want without anyone complaining. I would prefer to get as many features as I can coded before I have to worry about dealing other people who might complain about the changes. Yes, I also have heard that open source efforts fail because they start too early -- and I definitely think my project is still too early to become a community project, even though I've been working on it for years. I need a few demo programs first before I begin advertising it, and seeking collaborators. Once I have something that I can make a YouTube video about, then I will start boasting about my work and accepting collaborators, if anyone would be interested.
The do syntax makes the most sense to me in all monads. They are all linear code that looks a bit procedural.
If you put that into a file and ask GHCi what the type is: Tmp&gt; :t prompt prompt :: Read b =&gt; String -&gt; IO b One option is simply to narrow the type in a type signature: numPrompt :: (Read b, Num b) =&gt; String -&gt; IO b numPrompt = prompt You could also narrow it at the point you call it, but the syntax for that can be quirky at times and/or depend on a couple of extensions, depending on where you try to put the type signature. It's probably best for now just to create a specialized second function like this. For didactic purposes, may I also point out how I wrote simply `numPrompt = prompt`, and does not have to be `numPrompt s = prompt s` (though this is also legal); it's an important part of understanding Haskell functions.
Oh, cool. So ... if Cabal translates cleanly into Nix, and Nix has been around for a decade, why are we all still using the Cabal that everyone knows and hates? :-) 
Out of curiosity, has anyone tried recreating Nix as a Haskell DSL?
You don't need to understand monads *in full generality* to understand IO in Haskell. You will necessarily understand some things about monads if you understand IO in Haskell.
That's what I thought. I really like that.
Your problem in the original source is that you're trying to find the successor of a string, not a number. The simple and wrong (but right enough for a toy example) solution is to use the "read" function to turn name into a number. It's wrong because read is partial - if it doesn't know how to read what you passed it, your program dies. Your difficulty in understanding the second example stems from overreliance on do-notation. The translation is that ordinary lines in a do block are combined with (&gt;&gt;), lines which contain a binding are combined with (&gt;&gt;=). Specifically, "a &lt;- b" becomes "b &gt;&gt;= \ a -&gt; ...", where the body of that function is the entire rest of the do block.
Actually, the language *is* the ontology, at least the concept I am trying to prove with this proof-of-concept project is that actual computer code, not just plain XML or JSON-like structured data, is better at modeling real-world ideas. I have made it a homoiconic language -- there are built-in data structures to model the language itself. You can then perform queries on these data structures. You can also execute them like ordinary code. As I am writing this, I am thinking I should write it into the README.md file as well. But I will definitely be writing more documentation and examples, hopefully soon.
You may disagree with my analysis, but in that case I believe you must also disagree with the [Haskell Report](http://www.haskell.org/onlinereport/basic.html). From section 6.2: &gt; As a consequence, \_|\_ is not the same as \x -&gt; \_|\_, since seq can be used to distinguish them. For the same reason, the existence of seq weakens Haskell's parametricity properties. C functions *themselves* are not pure, even if a denotational description of them in term of domain theory is. When written out, the denotational description of a C function has a type that looks very different from the surface syntax of the C language and its native types. To reason denotationally about C, you have to first translate to a denotational semantics and then perform your reasoning. This, for an impure C function, will make plain the side-effects of the function in much the same way that Monads can be used to describe effectful functions in Haskell. In fact, that's precisely how Monads got introduced to CS theory in the first place! Haskell, in contrast, explicitly uses the domain semantics in its native type system. There would no doubt be some additional complications in a full denotational semantics for the entire language, but the mapping is much more transparent and thus easier to use in direct manipulation of programs. In Haskell, "bottom" is a real value in the static semantics, and it really explicitly inhabits all types. In C and SML, it is not a real value; it only exists in a denotational interpretation of the language semantics. There is no such value that inhabits the native types of those language. Now, is the description of the Haskell language in the Haskell Report the best way one could describe the semantics of a lazy, pure functional language? I guess you think the answer is no; but that's the way Haskell itself is defined.
Just a text editor and the haskell platform will be fine. A bit of command-line know-how can't hurt.
&gt; You call seq "another wrench". Is forcing an effect? To me that would be similar to considering that writing in memory is pure, the problem is when (anyone) reads it. It is not an effect in Haskell because the type system is built to deal with it. Alternatively you could consider it to be an effect modeled by the type system, but in either case it's explicit in the types so functions (in their direct interpretation rather than via some alternate semantics) are still pure; the same input will always give the same output. It just makes equational reasoning more difficult if one wants to provide full proofs rather than just "morally correct" reasoning.
It's really orthogonal to view patterns. But view patterns and pattern synonyms have a good synergy, and allow normal looking pattern matching on abstract data types. 
I would use [Sublime Text](http://www.sublimetext.com/) with [SublimeREPL](https://sublime.wbond.net/packages/SublimeREPL) and [SublimeHaskell](https://sublime.wbond.net/packages/SublimeHaskell): it satisfies my requirements. But, for sure, you can try [FP Haskell Center](https://www.fpcomplete.com/business/haskell-center/overview/).
That's fair, I was wondering if there was another reason than the pause of that capability. 10 microseconds of `murmurhash3` is 50 kilobytes of data on modern hardware, so I suspect having `unsafe` be the default would only affect a negligible portion of users. Anyone passing in a Haskell `String` that consists of more than 50 kilobytes (is that 25 or 12.5k `Char`s?) is asking for performance issues in their code :) Edit: Actually, I'm guessing these murmurhash3 functions would spend *more* time converting the input to a CString than they spend in the hash function itself for almost all input sizes.
That's Great! I was trying something like that but my type signature was: prompt :: (Num a) =&gt; String -&gt; IO a I was wondering why it wouldn't work, since a must be Readable too. Thank you
NoSQL is the realization that if you don't care about ACID you shouldn't have to "pay" for it (not in the monetary sense, in the complexity/development one). It is a great trend, just not the world takeover move it is being touted as.
It's not a synonym at all, is the issue. It's a homonym.
I agree. It just find it hilarious that after decades of bitching the grand salvation of "web scale" data was COBOL tables, more or less. Of course none of these people realise they are essentially advocating for a return of COBOL.
I see what you mean on both points, you're right on the first point, on the second point, although I agree with you in general for my case I needed a numeric value, no it'll never be modified but it is needed. But I agree with you in the sense that I've actually got an edge case rather than the norm and the lib should provide the byte string as a default. Useful feedback, I'll make updates to address the issues you pointed out, thanks.
As far as I am concerned just updating the libc wouldn't cut it with nix. nix does not allow "impure" operations. You had to update all packages that depend on the libc as well.
What does it feel like to be so smart?
I wonder what other languages you are using. This is a pretty typical approach for any language that actually has a standard.
Maybe I'm just blind to it -- I honestly use Haskell for almost everything so I don't have a tight grip on any other languages' design standards. I use Java &amp; C++ for school though, and used Groovy a lot for a job a couple of years ago. I've not seen anything like this with those languages. But, like I said, I don't really keep track of the development of them so it's 100% feasible that I'm just commenting on reasonable language design as a whole, haha.
People new to Haskell often struggle with understanding how functions can return values that in a lot of other languages would be considered polymorphic on the return type. The key thing to understand in the case of Haskell is that the compiler will evaluate at compile time which instance of a function is actually needed based on the location at which the function is used. E.G. in this example read is polymorphic, there exist many instances of the read function that all return different types of values. Depending on the type expected at the point in which read is invoked the compiler can deduce exactly which read instance is required in this particular case. Importantly this ability to deduce a specific function necessary from amongst a larger set of possible functions is transitive with respect to functions calling other functions, so for example the return type of prompt is also polymorphic in that it's capable of returning any value for which there exists an appropriate instance of read (that the compiler knows about, I.E. that's been either imported or declared in the module). You will often see errors crop up if you attempt to do something that hinders the compilers ability to determine which instance of a function it needs to us. A silly example of this is the following: silly :: (Read a, Show a) =&gt; a -&gt; a silly = read . show . read . show Given the above, the compiler can't work out which instance of read it should be using in the middle. It knows which instance of the first show it needs because it match the type of the argument passed in. It knows which instance it should use for the last read because it matches the expected return type. But which instance of the first read should it select? Looking at the code it seems obvious it should pick the one defined for the 'a' type given as an argument, but the compiler has no way of knowing that's what was intended so it will complain with an error and refuse to compile until you provide it a bit of help on the type it's expected to use in this instance. As for your question about &gt;&gt; and &gt;&gt;= those are just the monad operators. You've been using them all along you just didn't realize it because they're hidden by the syntactic sugar of do notation. In the case of the IO monad (which is the one you're using here) the context provided by monad is that of sequential operation. I.E. conceptually IO forces things to happen in order. In this context &gt;&gt;= means do the thing on the left first, then take the result of that and pass it to the function on the right and evaluate it. Once again in this context &gt;&gt; means take the thing on the left, evaluate it, throw away whatever value it generates, then do the thing on the right (a keen observer will note the &gt;&gt; is a special case of &gt;&gt;= and is in fact by default implemented using &gt;&gt;=). As implied by the previous wording this is not in fact the only context that monad can provide, merely the one in the case of IO. In other contexts, such as Maybe, it can represent a failure condition. In the context of Maybe &gt;&gt;= means evaluate the thing on the left, and if it's Nothing just skip whatever is on the right, but if it's Just some value then pass that value to the function on the right and evaluate it. The context when dealing with the list monad ([]) is rather mind bending and ends up being something like combinatorial evaluation which I'm not going to go into right now. The point being, Monad is a way of (among other things) adding context to a group of functions, which in this case is being used for sequencing. As a final note on Monad, return is also a function of Monad and just means wrap the argument its given in the appropriate monad. With the exception of the historical wart of fail, &gt;&gt;, &gt;&gt;=, and return represent the entirety of the Monad class.
I agree with Axman6 here, but I'll be a little kinder. The bit/byte-twiddling seems like it might be dangerous in that you could be changing the expected values of `MurmurHash3` for well known inputs. It might not matter for you but it would matter for any database-using software. Have you tested your functions against MurmurHash3's output (byte-for-byte)? Also, I think the discussion of `unsafe` below goes down the wrong path for optimizing for performance. I'm a little ashamed I participated in such a flagrant display of premature optimization, actually. I would guess that your code as it stands spends *almost all of its time* converting the input `String` to a `CStringLen`. As well, I think there's a race condition in your code regarding memory that is about to be freed. I won't assume you know the implementation details or not - so if you're already aware of the following, I am sorry for being repetitive. That said: ## String This is a Haskell `String`, which is a type alias for `[Char]`. That's not an array though, it's a list. Lists are implemented as independently allocated cells in memory, where each cell contains an element (a `Char` in this case) and a pointer to the next element. Example cells for Hello World: 0: `H, -&gt;1` 1: `e, -&gt;2` 2: `l, -&gt;3` 3: `l, -&gt;4` 4: `o, -&gt;5` 5: ` , -&gt;6` 6: `W, -&gt;7` 7: `o, -&gt;8` 8: `r, -&gt;9` 9: `l, -&gt;10` 10:`d, -&gt;11` 11:`!, END` Each cells location is indicated by a number. Those cells can be stored anywhere^1 in memory. The following numbers are short hand for where memory consisting of the `Char` and the pointer to the next cell. Also a simplification, but it will suffice. So, valid sequences in memory of `String` cells include: `| 0| 1| 2| 3| 4| 5| 6| 7| 8| 9|10|11| ` `| 1| 6| 4|11| 5| 9|10| 0| 3| 7| 2| 8| ` `|11|10| 9| 8| 7| 6| 5| 4| 3| 2| 1| 0|` The first seems OK. It's wasteful - each cell points directly to the one to its right, but at least the processor will have an easy time reading in all of the cells in order. The second is... random. The processor will have to jump around a lot to read it. And the last is *terribly* inefficient, as processors do not like to go in reverse order. (They are optimized for sequential, increasing access.) But that's not even the worst possible arrangement. This is the reality of allocating Haskell lists in memory: `| 6|a_really_big_object|wacky_uninitialized_memory| 8| 2|11|other_memory| 9| 0|10|not_your_data| 4|some_big_stretch_of_nothing| 3|something_probably_big| 1| 7| 5|` ## A Race Condition and a Memory Leak I believe your code has a serious race condition and a memory leak in its first few lines of `murmur3Raw`, copied here: murmur3Raw :: String -&gt; Int -&gt; MHV -&gt; IO [CUInt] murmur3Raw val seed ver = do val' &lt;- withCAStringLen val $ \x -&gt; return x -- ^ This function, withCAString, causes this function to be racey. let cstr = strFromCStr val' let strLength = strLFromCStr val' -- ^ these two lines are a code smell with FFI that should indicate you should look for races outPtr &lt;- mallocArray arrSize doHash ver cstr strLength (fromIntegral seed) outPtr peekArray arrSize outPtr where arrSize = 4 strFromCStr :: CStringLen -&gt; CString strFromCStr = fst strLFromCStr :: CStringLen -&gt; CInt strLFromCStr i = fromIntegral $ snd i --version value size seed out doHash :: MHV -&gt; CString -&gt; CInt -&gt; CUInt -&gt; Ptr CUInt -&gt; IO() doHash X86_32 v s se o = c_x86_32 v s se o doHash X86_128 v s se o = c_x86_128 v s se o doHash X64_128 v s se o = c_x64_128 v s se o The function `withCAStringLen` has the following definition in Hackage: &gt; `withCAStringLen :: String -&gt; (CStringLen -&gt; IO a) -&gt; IO a` &gt; &gt; Marshal a Haskell string into a C string (ie, character array) in temporary storage, with explicit length information. &gt; &gt; * the memory is freed when the subcomputation terminates (either normally or via an exception), so the pointer to the temporary storage must not be used after this. The subcomputation is the second argument. Here's `withCAStringLen` and its arguments laid out against your code: val' &lt;- withCAStringLen val $ \x -&gt; return x -- withCAStringLen :: String -&gt; (CStringLen -&gt; IO a) -&gt; IO a That subcomputation is just a `return`. So the memory for the `CStringLen` returned could be freed *immediately* after that line of code. That's risky! You want to use the `CStringLen` immediately after that line of code. The following lines of code peek into the returned value of the `CStringLen`, which is what caught my eye initially. Your function should probably look like this: murmur3Raw :: String -&gt; Int -&gt; MHV -&gt; IO [CUInt] murmur3Raw val seed ver = withCAStringLen val $ \(cstr, strLength) -&gt; do outPtr &lt;- mallocArray arrSize doHash ver cstr strLength (fromIntegral seed) outPtr peekArray arrSize outPtr where arrSize = 4 doHash :: MHV -&gt; CString -&gt; CInt -&gt; CUInt -&gt; Ptr CUInt -&gt; IO() doHash X86_32 v s se o = c_x86_32 v s se o doHash X86_128 v s se o = c_x86_128 v s se o doHash X64_128 v s se o = c_x64_128 v s se o But this still has a memory leak - we need to free the array. Right now you `mallocArray` but don't free it. We can chain your allocations using another function, though: murmur3Raw :: String -&gt; Int -&gt; MHV -&gt; IO [CUInt] murmur3Raw val seed ver = withCAStringLen val $ \(cstr, strLength) -&gt; allocaArray arrSize $ \outPtr -&gt; do doHash ver cstr strLength (fromIntegral seed) outPtr peekArray arrSize outPtr where arrSize = 4 doHash :: MHV -&gt; CString -&gt; CInt -&gt; CUInt -&gt; Ptr CUInt -&gt; IO() doHash X86_32 v s se o = c_x86_32 v s se o doHash X86_128 v s se o = c_x86_128 v s se o doHash X64_128 v s se o = c_x64_128 v s se o `withCAStringLen` and `allocaArray` behave similarly; they temporarily allocate some memory and free it when the subcomputation is complete. With the changes above to `murmur3Raw`, there should not be a race or a memory leak. ## Conclusion My pessimistic guess would be that your program spends 99% of its time reading all of the cells of the `String` from memory and jumping around. Even with the changes above to make the code less racey, I think you might be interested in using the `Bytestring` library to change your code to work with raw sequences of bytes. The `useAsCStringLen` function from `Data.ByteString` is probably relevant here. ^1 - Not really anywhere, but the details would distract from the point. You can assume that they can appear *nearly* anywhere.
I guess I forgot the details, but I remember seeing View Patterns when I used Data.Sequence years ago, and this pattern syntax example looks very similar.
Sandboxes allow you to do this pretty easily already, essentially do a cabal unpack on a directory and then just make a small script to create your sandbox with that directory already added as a source and check all of that into version control (excluding the /dist/ dir via .gitignore or similar in each added source).
I wasn't criticizing your project, just the headline. =)
It's cool man, I realized that. But I still thought I should explain why I've not been talking about my projects.
As an addendum I should note that a data structure in Haskell (so far as I understand) at runtime is more like a struct containing pointers to other structs (boxed values). Because data is immutable (in general) the runtime is free to re-use any in-memory values between structs, so for instance two different data structures that both contain the same string can both use the same pointer to the in-memory instance as it's guaranteed to never change.
`cabal sandbox add-source` is very buggy for me or at least it doesn't work how a normal hackage package does. But maybe a properly working version of that is the best solution.
This is very well documented! I enjoyed this reading and learnt a thing or two about liftM. 
i don't necessarily think i'm in a position to give advice on that front due to the nature of my situation, which is probably outside the norm. however, i can tell you that at my company (a high-tech engineering firm making both software and hardware) that C#, C++, and Python are easily the most popular languages for development in R&amp;D. i have no idea what the IT or web departments use. i am personally trying to learn haskell and C# well, one for its promise and the other for its currently utility and UI development. getting a position in haskell (which i am no where near capable of yet) would require me to find a job outside my current company.
Umm. You are using ByteString right? Converting that to a cstring is a no op. If not, you should really consider using ByteString. It's really meant for for this use case and friends. It's totally reasonable to use a hash function on a large binary piece of data. A key piece of engineering a good library is to make sure things behave well on allowed inputs. Hashing a 100mb binary archive or something is totally plausible. 
Thanks for the very detailed answer. I did already know about String (I initially used it because that's what I get from the lib I'm using whose data is hashed) and have already taken Axman6's comments on board and made it possible to accept both ByteString and String with ability to add other types. I didn't know about the memory issue you pointed out, I read http://www.haskell.org/haskellwiki/FFI_cook_book but overlooked the fact that in their example the CStringLen doesn't escape the lamba and is used straight away as you've done as well. I'll take your suggestions on board. I'll also write some tests against the original to ensure the expected output is the same for the same inputs. Thanks again
I've been keeping an eye on Elm for a while now, and I'm almost sold on using it. The thing that would really push me over the edge and start using it is an example codebase of a real world project using it. Does anyone know of where I might find one?
It worked fine in the three or four packages where I used it on internal projects so far. What are the problems or bugs you observe?
Wow, an answer! o.O I think this might be true. FP'ers don't want to say much about it, it's like "the compiler fixes it". I think another FP-like thought is to not have data that doesn't need to go together bundled in the way I describe.
The whole Elm site is written in Elm.
I have a **very very prototype** but &gt;1K LOC spaceship game available. http://johnpmayer.github.io/celestia/
I agree, though I think `IO` is the most obviously useful monad for anyone learning the language. But I did have trouble with the transition; that was some of the motivation behind this post. `Either` would have been a great example! I'm just more familiar with `Maybe`.
Yes and no. You might want to look into lenses (although I should warn you, the lens library which is the most popular flavor of lens is a bit intimidating structurally). Using lenses you can basically "fake" an OO style and it can ease a lot of the difficulties of doing stateful updates. E.G. it allows you to do something like the following: updatePos :: Dino -&gt; (Int,Int) -&gt; Dino updatePos d p = d &amp; dinoPos .~ p
That does make sense since monads are kind of about sequencing computations.
when I added 200+ packages the do nothing build took 30 secs, but that was at least 6 months ago.
I shall research into these "lenses"! Thanks for the tip! 
I'm not the original poster (that's /u/zcourts). The person who posted this is also not using a `ByteString` right now - all his code takes `String` as input. On top of that, using only the library functions available (not internal knowledge of `ByteString`), converting it to a `CString` is not a no-op, the `ByteString` library only exposes conversion via `useAsCString` and `useAsCStringLen`, the latter of which is implemented in terms of the former. It's a good thing you mentioned the no op-ness of `ByteString` conversion because I will have to amend my reply to /u/zcourts. The `memcpy` alone will have a substantial performance cost relative to ` MurmurHash3`. On modern hardware `memcpy` I think is ~10-20 gigabytes/second, and `MurmurHash3` is 5 gigabytes/second. Not to mention the cash thrashing that memcpy will do. *Edit:* There are unsafe versions of the above functions which will suit his purposes.
Thank you for reading that gargantuan post of mine. I'm glad I was able to provide some useful advice. I have one more thing to mention, as something /u/cartazio caught my eye and I realized that my advice about `ByteString` overlooked something pretty substantial. Right now the `UseAsCStringLen` function is implemented in terms of a `memcpy`. The problem is actually that `MurmurHash3` is, ironically, so fast that it's almost as fast as a memcpy (at least within a factor of 2-4 on most systems). So by paying for a `memcpy` up front, you're going to thrash your cache reading your input twice and pay for all the overhead of doing that. If you're willing to really dig around in the internals of `ByteString`, you can avoid the memcpy, but it also makes your implementation brittle w.r.t. the internals of `ByteString`. You can fix that by using C preprocessor style `#if` blocks around your code though. At least you could protect the library from changes to the internals of `ByteString`. Edit: Use the `Data.ByteString.Unsafe` module and the `unsafeUseAsCStringLen` function within it. It's only unsafe in the sense that it's morally incorrect to use it with code that isn't provably pure. Fortunately you know that `MurmurHash3` is a one way function that does not modify its output. *The most optimized* version of your function that I can think of is: murmur3Raw :: ByteString -&gt; Word32 -&gt; MHV -&gt; IO ByteString murmur3Raw val seed ver = unsafeUseAsCStringLen val $ \(cstr, strLength) -&gt; do outPutr &lt;- mallocBytes 16 doHash ver cstr strLength (CUInt seed) outPtr unsafePackAddressLen 16 outPtr where doHash :: MHV -&gt; CString -&gt; CInt -&gt; CUInt -&gt; Ptr CUInt -&gt; IO() doHash X86_32 v s se o = c_x86_32 v s se o doHash X86_128 v s se o = c_x86_128 v s se o doHash X64_128 v s se o = c_x64_128 v s se o What this saves over the original is: 1. The only allocation is the 16 bytes for the output (and the ByteString overhead) and there is no race because the input is converted to a CStringLen in place. 2. There is no call to `free`, because the return value is converted to a ByteString in-place. 3. `fromIntegral` is replaced with the `newtype` constructor - which is optimized away by the compiler. This code should optimize down to basically just a C call and a small, fixed allocation overhead.
This is insanely relevant to my interests. Very cool.
Good idea. I will do that.
And here I was literally only twenty minutes before I saw this thinking about that Dominion simulator I was going to write.
Those are great examples of how short code can be in Elm over JS + libraries, it's interesting to see. I think having a separate link from examples on the front page would help, under the label "Projects using Elm" or something along those lines, with links to the code. 
I wanted this too! Now I feel like a fool.
I think you're getting confused because gergoerdi's example uses both. It defines pattern synonyms using view patterns. It allows you to say things like (using the defined pattern synonym): foo :: Seq.Seq a -&gt; ... foo (xs :&gt; x) = ... instead of (using a view pattern): foo :: Seq.Seq a -&gt; ... foo (Seq.viewr -&gt; xs Seq.:&gt; x) = ...
Ahh, thanks. Yeah now my memory is jogged that the "viewer" wasn't invisible with ViewPatterns alone. That's a mark of a good intuitive user-experience design -- that the user no longer remembers that the old way was different.
No need to apologize, I think your advice is solid. Thanks!
I wonder how many Haskellers play Dominion. We should start a club.
Hahaha, I got partway through implementing one myself before I gave up because 1. It was way more complicated than I thought to design a simulator that was generic enough to handle all the cards 2. I couldn't imagine myself actually using it much, since I don't play with anyone else who's interested in studying strategy to such an extent. If I do, I'll just trounce all my friends and no one will enjoy playing with me. And here I was thinking I was the only one who thought of a Dominion simulator in Haskell.
Not necessarily. Here's a [monad-free IO tutorial](http://www.haskellforall.com/2013/01/introduction-to-haskell-io.html) that I wrote.
Cool! I once spent a couple days thinking about how to formalize Dominion using small-step operational semantics. This would let you express the cards as "programs" that don't rely on any primitives of the implementation language, express the cards in textual form, prove properties of cards &amp; kingdom card selections... I never got to writing any code though. I'll check this code out!
That sounds interesting. I've wondered about games like "Magic the Gathering Online" and "Hearthstone", which are complicated card games, with many interactions. Magic in particular was also written by humans, so a clever idea that was clear, was always included, with no pushback from the coders about how much time it'd take to implement. So: how is it implemented? A DSL of some variety I'd assume.
Try [Gloss](http://hackage.haskell.org/package/gloss) if you want to do some simple vector graphics and animations. I've used it for a few experiments and it's generally a joy to use, and very simple (iterate a model object with a time delta). &gt;this is a bit of my problem with the haskell community. it wants users but it doesn't treat them well. ...here's category theory thrown at you from everywhere... More of an impedance mismatch between those that reason about programs differently than someone who just wants to play around/get shit done in Haskell. I too don't give a shit about co-semi-endo-whooajawhatsitz and think the community would be more accessible if these kind of things weren't used as justifications for "what makes Haskell good".
Not [linear](http://hackage.haskell.org/package/linear)?
Never use read. Use readMaybe: Text.Read.readMaybe :: Read a =&gt; String -&gt; Maybe a 
A while ago I spent a lot of time looking into game theory and especially Monte Carlo Tree Search. Super fascinating algorithm, started working on my AI for Go (http://www.blitzcode.net/misc.shtml#Gopher). It seems this algorithm also applies to the Dominion game, maybe have a look, MCTS / UCT / UCB etc. might be a sound way to evaluate strategies quickly.
I recommend `linear` instead. It's by ekmett and used in (at least) the `not-gloss` project.
Thanks! I'd like to get back to this and see how to make it more reliable &amp; usable. With the upcoming 7.8 release, and the improvements planned for 7.10, I'm not sure what the right approach is, though. One could write a stand alone debugger, add facilities to the RTS or hack on the compiler to produce better symbol and call frame data for gdb etc. Not sure how GHC HQ feels about these different approaches!
I did notice the hackage docs said the vals were a copy but didn't think there was a way around that. What I'll do is make it possible to use both versions of murmur3Raw I think. And mark this one as 'unsafe' although I get what you're saying about the fact that referential transparency won't be affected because I know Murmur won't modify the data after. Thanks again
First, if every element has attributes, for example div, it is better to just make the "div" function take a list of attributes and pass and empty list if you don't want attributes. "div[] $ do" is easy to type. You could also use type classes to simplify the names of functions like "childText", and just pull everything together under a single class instancing a "child" function. You could do something like this: class Child a where child :: a -&gt; BuilderMonad () instance Child String where { child = childText } -- let the "child" function work on other BuilderMonads instance Child (BuilderMonad ()) where { child = id } type Attribute = (VarName, VarValue) clas :: VarValue -&gt; Attribute clas val = ("class", val) div :: [Attribute] -&gt; BuilderMonad () -&gt; BuilderMonad () div attribs = ... example = div [("id", someVar)] $ do child "Some Text" child $ div [clas "some-class", clas "other-class"] $ do child "This time no attrs, but with an atomic var and class" readAtom atomVar 
You say that functors and monads are mathematically equivalent. Actually, the monads are a subset of the functors.
I would love to see a Hearthstone simulator, I very like this game and Haskell. Someone should write it, maybe even me, hmmm...
Yeah I agree. This is the base for a few other tools I want to make, such as a constraint based solver for dependency versions (something vendoring can't do, since versions depend on the environment). I will make the source available one day so you can run it locally / in a team. You could add a hook for `cabal test` and have a good build bot. But right now it is tightly coupled with some web services. 
Whoops, thanks for that.
TIL this function comes with ghc. Thank you. (Comes with ghc 7.6.3 anyway; doesn't seem to come with ghc 7.4.2 unless it's hidden away in another module somewhere.)
Magic the Gathering is actually Turing Complete: http://www.toothycat.net/~hologram/Turing/HowItWorks.html
Where is yours?
FYI prompt x = do putStrLn x number &lt;- getLine return number is the same as prompt x = do putStrLn x getLine 
Do forkIO to call that function so it doesn't block the original thread? Or experiment with a patched version of the call that's safe?
You can play online on that awful website.
University of Maryland - Bitcamp http://bitca.mp/
I was lucky enough to get a pre-print from the authors and it's really nice to see that this idea works out in the end. That said (as Edward notes) we're still a few steps away from a straightforward implementation in GHC. To do this for lists we probably would not want to be using HERMIT and so would need either a direct implementation of the custom rules in GHC, or some extensions to GHC's rewrite rules language.
Technically yes, but I was looking more for the software architecture answer to: "How do you represent a system that is so complicated in a DSL or similar such that you maximize clarity of rule writing, and maximize correctness".
Check out the [wiki](http://www.haskell.org/haskellwiki/ZuriHac2014) and please [register](https://docs.google.com/forms/d/1mMTlVNnWQvac1oWSXWQLBO9GVvqv8a_fYTFrR8UNuDI/viewform) if you would like to attend. See you in Zurich!
Neither of these will change anything, whether or not you're using `-threaded` or `unsafe`, etc. First, `timeout`, `throwTo`, etc use asynchronous *Haskell* exceptions, which mean that they cannot interrupt foreign calls. This is true whether they are `safe` or `unsafe` foreign calls - it's generally not possible to interrupt a foreign call that a Haskell thread has made, meaning the usefulness of `timeout` and related functions is basically gone. The reason is pretty clear - there's just not a good general way to do it, whether your call is in a thread pool (`safe`) or called inline (like `unsafe` calls.) The best you can do without being *super* tricky, is to `foreign import safe` your own `close` call, and use `network` internal modules to pull out FDs and close them manually. Then, at least, your entire program isn't at the mercy of a blocked `unsafe` call on the UNIX socket... There is some good news, however: [This is doable in GHC 7.8](https://github.com/ghc/ghc/blob/master/docs/users_guide/ffi-chap.xml#L83), which supports the `interruptible` FFI extension, which allows you to use `throwTo` against a thread in the middle of a foreign call. `interruptible` = `safe` call + "you can interrupt this thread asynchronously." It does this on POSIX machines by sending `SIGPIPE` to the responsible thread, causing an interruption. IMO, there are a few things that should be done: * Some of the functions in `Network.Socket` almost certainly *should not* be declared `unsafe`, such as `close`. Things like `htonl`, OK, I can buy. Others are decidedly non-blocking (due to the way they work with the RTS.) But things like `close` most certainly should not be `unsafe` when they have these characteristics, which can lock up the entire RTS as it waits to GC. * For GHC 7.8, `network` should probably be updated to allow `interruputible` foreign calls. Again, `close` is an example of this.[1] * You **may** be able to work around this similarly to the way `interruptible` works in 7.8: send a signal to the thread responsible, for example SIGPIPE, which should be enough to get `close` to respond with `EINTR`. However, I'm not sure of a way to guarantee this works, because a `safe` call might be scheduled off onto a thread pool, and you need to know the correct pthread to send it to... I can, of course, think of ways to work around this, but they're all pretty awful. [1] Note that most people (incorrectly) don't expect `close` to ever fail or anything (or really do anything except "immediately return",) but in the case it's interrupted it'll return `EINTR` due to a signal, and you might legitimately need to retry. Making `close` interruptible slightly changes the guarantee you might get from it, since before I doubt most people saw it or handled it. But frankly lots of people never deal with `EINTR` correctly, for `close` or anything else...
previous posting [here](http://www.reddit.com/r/haskell/comments/1vfpp1/pepm14_the_hermit_in_the_stream_inside_206105/), not that it garnered that much attention...
I have not heard of a builder monad before. I'll look into it :)
Forgive me if I am wrong but would it not be more natural to define effects as functions (T.PlayerId -&gt; T.Dominion (Maybe T.Followup)) rather than the additional level of redirection where you have to use the usesEffect function to apply the effects. This would also mean that you could supply definitions of the functions in the same files as the cards rather than seperately in internal.hs. Is there anything you are looking to add to the project? I am keen to get some more haskell experience and this looks perfect!
I know some people that did, and I know some people that work there. They all say that it's a great place to work, but as far as I'm aware they're not looking at hiring anyone at the moment.
That's a good idea. It would be nice to refactor that code. If you want to do that, feel free :) Other things I want to add: 1. Tests 2. More cards 3. Functions to analyze the results. For example, average # of VPs that a strategy gets you. 4. The Monte Carlo idea mentioned in this thread to compare a lot of strategies quickly. If you want to work on any of these, [just send me an email](http://www.google.com/recaptcha/mailhide/d?k=01qEVKs9TsYYdtF4O7rT7f_g==&amp;c=3FfFzKI7BIqgIUwquBnFSg==).
Thanks, this is really helpful! Yet another reason to be excited for GHC 7.8!
I never looked at ghcjs before, but it seems this talk has a demonstration of the debugging features: http://vimeo.com/80895330 (will watch that later!). The upcoming improvements on the DWARF debug symbols should make unobtrusive sampling profiling work, at least for flat profiles. I don't know if the current implementation supports all situations, like FFI calls and such, though. Some profiling tools (like mine, oprofile, valgrind to some degree) use frame pointers for stack traversal and won't automatically benefit from the improved debug symbols. Hierarchical profiling will be more complicated. Haskell will never have a 'normal' call stack, so there will always be some overhead for maintaining one or less useful data from Haskell's native stack. There are many papers on call graph analyses for more typical languages, but I don't think there's much work on lazy, functional languages with tail calls and such. It seems this is pretty much an open problem! I also replied to the discussion in the 'Show stack traces' ticket: https://ghc.haskell.org/trac/ghc/ticket/3693
great! These are reasonable questions! I'm excited that more people are trying to think about these matters, debugging is hard and improving ghc tooling in that space has lots of room for improvement in doubtlessly endless ways! :) 
Hi Charles, it's always possible to cancel your reservation. No worries. I hope to see you in Zurich!
Cool, hope to be there too!
Heh, no I missed it too. Needed a better article title :-)
That website is a nightmare, but I love Dominion so much.
so you have some ideas for a richer rules framework for GHC, care to elaborate?
I don't know whether or not Haskell hackers actually call it "builder monads," but the "BuilderMonad" example I described above is a kind of stateful free monad that evaluates to a pure, stateless functions. This style it is essentially a framework for building APIs that you can use to think in a controlled, stateful way about solving a problem. Modules like Data.Binary.Put and Data.Text.Lazy.Builder are all examples of "builder" monads. Basically, they are a newtype wrapper around the Control.Monad.State.Lazy.State data type provided in the mtl package. You can derive the interfaces for Functor, Applicative, Monad if you compile with the the -XGeneralizedNewtypeDeriving option to GHC. For example: newtype BuilderMonad a = Builder { runBuilderMonad :: State CloactData a } deriving (Functor, Applicative, Monad) So here, the CollactData would be the data type you define to store the document you are building, it could just be a "String" (or better yet, a Data.ByteString.Lazy.ByteString). Then you define your API functions like "div" which would modify the ColactData, which would look something like this: div :: [Attribute] -&gt; BuilderMonad () -&gt; BuilderMonad () div attribs childBuilder = Builder $ do let (classes, otherAttribs) = partition ((=="class") . fst) attribs modify $ \colactData -&gt; colactData{ ... } -- set the state before it has been updated by the child childBuilder modify $ \colactData -&gt; colactData{ ... } -- set the state after it has been updated by the child Within the BuilderMonad, you have a State monad, where you can use functions like "modify", "get", and "put" to read and write the state of the ColactData. 
There is, of course. But to get there we need to start thinking in terms of eliminators rather than taking case analysis to be primitive. For example, given some ADT `data Foo = Bar X Y Z` we can't distinguish between exporting the constructor function `Bar` and exporting the destructor pattern `Bar`; we can only choose to export both or neither. The reason for this limitation has its roots in thinking of the two as being the same thing, which in turn is due to thinking of case analysis as primitive.
Intuition-wise there shouldn't be any difference. Consider the following definitions: pattern P x y &lt;- ... f (P x y) = ... f ... Here, to see if the first branch of `f` matches we'll first match the pattern `(P _ _)` and then if that succeeds we'll match the subpatterns `x` and `y`. data T = C X Y |... g (C x y) = ... g ... Here, to see if the first branch of `g` matches we'll first match the pattern `(C _ _)` and then if that succeeds we'll match the subpatterns `x` and `y`. The only difference is that matching against `(P _ _)` may involve more work than matching against `(C _ _)` since `P` could be checking more than one thing. The mental model for evaluation is the same though: first match the top level pattern, then recurse and match subpatterns. ----- The problem, IMO, is the use of the term "alias" which suggests that we're doing a simple macro expansion— whereas, in reality, we're constructing pattern automata. The new feature allows us to construct pattern automata which have different failure modes than the ones we could have defined before[1]. These failure-mode differences are the direct cause of the strictness differences, as well as differences in performance (due to how quickly a given pattern will fail on non-matching terms). Pattern automata are all well and good; they have a long tradition in the logic programming community and, IMO, they're the right way to go about thinking of case analysis. However, they are subtly and significantly different from the way functional programmers typically think of case analysis. Because of this difference, it appears as though "pattern aliases" break referential transparency— since we can't simply substitute the definition of a pattern "alias" into its use sites. The real problem here, I think, is that we are fostering the wrong mental model; and that's what we should work on fixing. Unfortunately, it's not entirely clear to me how we could change the syntax to afford the correct model without causing undue bloat in the code text. [1] There's no increase in formal power here. In the old system we could achieve the same semantics by chaining together a series of pattern automata / case expressions. The new system simply allows us to combine these into a single automaton. This ability to fuse automata can, in principle, lead to performance improvements —which is why pattern automata are used in logic programming— though it's not clear to what extent GHC takes advantage of this;
Have you looked at [blaze-html](http://jaspervdj.be/blaze/tutorial.html)? It's an extremely complete, highly performant DSL for writing HTML in haskell, based on a variation of MonadBuilder. snippet someVar atom = do div ! A.id someVar $ do "Some Text" div ! A.class_ "some-class some-other-class" $ do p "This time no attrs, but with an atomic var and class" readAtom atom &gt;&gt;= toHtml If you don't mind getting a bit of Template Haskell in your project, there's also [Hamlet](http://hackage.haskell.org/package/hamlet), the Shakespearian HTML templating system. Hamlet is backed by blaze-html, so it's just as fast and easier to read. snippet someVar atom = [hamlet| &lt;div id=#{someVar}&gt; Some Text &lt;div .some-class .some-other-class&gt; &lt;p&gt; This time no attrs, but with atomic var and class #{ readAtom atom } |]
My advice to you, as someone who got a good chunk of the way through a Haskell Dominion implementation himself, is to be suuuper generic. There are cards, for example, whose cost (ex: Peddler) or effects (ex: City) depend on game state. In my implementation, basically every property of a Card was a function which took in the game state (usually ignoring it). That said, you've got a way cleverer design already than I had, so I'm sure you'll figure it out!
Thanks! Pushed a new version.
Is your version up somewhere? I'm curious to see how someone else implemented things.
Instead of reinventing the wheel, I would much rather see xmonad updated to run on Wayland, the new replacement for the Xserver. http://wayland.freedesktop.org/ If you really want to write your own display server, maybe consider implementing the Wayland protocol? That would get you a compatible, Haskell-based alternative to the C implementation.
Thanks, I was not aware that Wayland existed. I will have to look into it. It is because I would like a small project to fiddle around with.
If you have source code of that FFI you can simply redefine some functions as *foreign import safe*
I never got it fully working, hah. I was getting back into Haskell and found it too difficult a starting project, so I switched over to working on a kingdom card set generator, which I'll probably make a post about either here or in /r/Dominion within the week. If I do get back to it and get it fully operational, I'll let you know though!
Opaqueness was an explicit design goal of mine. I wanted these pattern synonyms to be on the same level as functions in terms of encapsulation -- you should be able to export a pattern synonym without exposing its definition.
I don't think `PatternGuards` would work, since they are not part of the pattern, they are part of the binding's right-hand side (like regular guards). I think what you're thinking of is the [ViewPatterns alternative](https://ghc.haskell.org/trac/ghc/wiki/ViewPatternsAlternative), which is not implemented in GHC (or any other Haskell compiler as far as I know) yet.
I think that's misguided, and contrary even to the name of the feature. One should not change a behaviour just by giving it a name. But it's pragmatically unlikely to do all that much damage. I'll defend your choice in practice, but not in principle.
You want to write something like this as a beginning Haskeller as a "small project"? Don't get me wrong: I would love to see something like this and even consider helping doing it (as I always wanted to help/participate in a (medium to large - IMO) Haskell project. But isn't this a bit to ambitious?
When you say "of course" do you mean there's a theory, or that it's been successfully implemented in practice? I would like to see a language where it works. 
[Here](https://github.com/pyb/zen)'s a similar idea implemented in CL, it's a X server based on OpenGL. In case you need a reference implementation.
This is a huge project. Even figuring out the Wayland terminology will likely take quite some time, and the docs aren't too impressive.
Emacs + haskell mode
I'm not a good Haskell programmer but I think it would be very fun to attend anyway. Is that be ok?
Like kosmikus said, beginners are definitely welcome to.
Wow, use_reddit_sparingly, you pretty much described my exact thoughts. So did you come to some conclusion about Haskell, Processing, or Julia? I'm finding it hard to leave Matlab and venture into some other tool, but I really want to. Let me know what your opinion is now, hopefully you've tried some of this and that and know more now..! Cheers!
I use `add-source` heavily and it works great. The only exception is when I'm using yesod (yes, Greg, I see that it's you). I don't know if it's all the TH or the huge and complex graph of dependencies, but I find that with yesod projects cabal rebuilds the outside sources every time, even when not needed. But even in that case there are some good work-arounds: 1. Use `yesod devel`. It only rebuilds the yesod app. 2. Use `--snapshot` with `add-source` if the added sources don't change very often. 
Wow, useful derivatives of tasty already. Good work Ollie and good work Roman!
I think you are right that it would not be the best starting project. I will start out with something smaller and then maybe when I am more confident in haskell I will give it a try.
For the purpose of learning a functional programming language, I worked my way through most of Learn You A Haskell. It is a powerful language and (I think) relatively straightforward. But, it will take a long time before I am using it as I have used Python and Matlab in the past (i.e. producing plots and examining data on-the-fly). I did see that Diagrams (Haskell graphics package) recently released Ver. 1.0, and people are excited about that, so I hope to continue to dabble in Haskell. But for the time being, as others have pointed out here, it isn't the tool I need for my work. I already know Python well enough to make it my "go-to" for solving problems quickly; it will be my Matlab replacement. I am intrigued by Julia too, but haven't played around with it yet. In the future I may devote some time to Julia as well, (having already gotten a taste of the "essence" of functional programming by working through LYAH), and see if it would make any more sense to adopt Julia over Haskell. If you're looking for a Matlab replacement and already have done some work in Python, adopting it as your Matlab replacement is almost certainly the easiest option. If not, maybe Julia is worth checking out. Supposedly you can call Python modules from Julia, too---so you have an easy fallback if there is a Python library you really want to use and Julia doesn't have an alternative. It sounds like a more versatile language all-around than Haskell; the downside is it lacks the "purity" that Haskell-heads seem to love. :)
Yeah, I would disagree with the report's phrasing as well. It's not wrong, though: adding `seq` changes the observational equivalence of a language with non-termination, just as adding memory read changes the observational equivalence of a language with memory write. I get your point about Haskell being closer to its denotational model; that's an argument of *quantitative* rather than *qualitative* difference, when you're close enough you get to consider it "pure", because you're making less of a jump. I'm still not sure about the part on "bottom is a real value", because it's unclear whether that same argument could apply to a call-by-need version of C -- yes, in a lazy language computations are values, but this should be independent of whether they are effectful or pure. I think we'll have to agree to disagree on the fine details of this. But yes, your position clearly is in fine company.
This is great. Down-to-earth conversation in which Brent shows how one can approach Haskell language, libraries, and community without being scared or confused off.
The answer linked is by Eric Lippert, the lead developer of C#: http://blogs.msdn.com/b/ericlippert/
In a way it has. A lot of functional features exist or are integrated in modern languages. I don't consider this the best way to do it, but due to inertia and market forces, the industry seems to prefer using old languages with new features than new languages. I'd say, we are still in a transition phase.
On the comments on one of the sections, Ken debates one of the ITA folks about the article "spoiling" the programming challenge. It's a great conversation, and my takeaway was "just really grokking the blog post is better evidence of computing talent than any independent solution actually submitted." I got a job offer from ITA during that era. My Python solutions to the challenges (including that one) were easier to write and uglier+buggier than Ken's solution, which took me long time and Haskell study to comprehend.
Is the species paper available as a draft already? I'd really like to take a look at it.
This podcast is really awesome - I haven't listened to no 5 yet, but I'm downloading it for my next nature hike. You've been able to gather a really great group of Haskellers, and discuss both the Haskell ecosystem, as well as some really indepth theory/practice stuff that is still accessible to beginning Haskellers. Huge kudos, and thanks!
lots of great links at: www.haskell.org/haskellwiki/Tutorials in particular: Learn You a Haskell for Great Good! http://learnyouahaskell.com/ Real World Haskell http://book.realworldhaskell.org/
&gt; Real-world programs are all about side effects and mutation. This is just dumb. Why do people keep saying this as though pure functional languages don't allow side effects and mutation? I use Haskell because it allows me to use side effects and mutation far more effectively than any other language I know of.
Usually I use `readLn` from Prelude for this: main = do num &lt;- readLn :: IO Int print $ succ num
So what happened? Did you help him understand monads? What's the thrilling conclusion?
It's a nice function to have, although it can be replaced with a simple use of Prelude function `reads`: readMaybe :: Read a =&gt; String -&gt; Maybe a readMaybe s = case reads s of [(val, "")] -&gt; Just val _ -&gt; Nothing
On Linux, I often see a glitch due to line bufferization: the prompt is printed on completion of the whole string only, so it is not visible during input. To handle this: import System.IO readLnWithPrompt prompt = do putStr prompt hFlush stdout readLn
i really liked the "hard and fast" tutrial, which has an interactive version here: https://www.fpcomplete.com/school/starting-with-haskell/haskell-fast-hard good luck and enjoy! :)
did you two manage launch the missles? :)
Agreed. I abstain from your survey! Insensitive clod.
Was it a flight from SFO to Seattle?
Thank god that plane did not crash. We would've lost half of the population of haskellers :) 
Year after year, I still find all these threads to be pretty much useless. For instance, Haskell lets you drop down to "mutable mode" and to C. It's just hard to get right, especially if you haven't practiced during years. Also, you're not forced to write all the bits of a project in Haskell, just be wise in your choice. I know Eric knows what he's talking about, I'm not criticizing him. But the only way to know what's hidden behind his words is to actually experience what one goes through when learning a functional language. Except maybe for some of Edward (Kmett)'s rants, I haven't really been happy with answers to these questions. I always feel like it's not really "spot on".
As a beginner myself, I've been going through Learn You a Haskell For Great Good, and I have to say it's superb. Give it a shot.
Not Haskell, but I had to go on a business trip when I was part way through reading Haruki Murakami's Kafka on the Shore. Normally I'd take a book with me for the flight, but this time I couldn't squeeze it in my hand luggage. The passenger next to me was obviously better at packing, as he got his copy out a couple of minutes into the flight. 
We talked about how nifty fpcomplete.com is and how much Haskell has improved in recent years , then he went back to reading and I tried to keep my kids calm. My wife said that I can follow in his footsteps and quit my job to become an independent consultant, when our kids are older. Nothing thrilling, but encouraging.
&gt; People keep saying it because people keep saying it. Recursion is hard to understand. :3
Simply stating someone is wrong is not worthwhile. If it is obviously true you are stating a simple fact which is uninteresting, the downvotes will convey that already. If it is not obviously true then you are skipping over the valuable part, *why* it is wrong.
&gt; Why do people keep saying this as though pure functional languages don't allow side effects and mutation? He didn't. He said stateless programs sound amazing until you realize state is what everything has to deal with. He doesn't expand to mention that by carefully manage *where* your state is you can get the majority of the benefits of stateless programming with state, because that "carefully manage" is incredibly non-trivial in many fields, so not really of interest to the question of "Why isn't FP used everywhere right now?" Take the stereotypical battlefield simulation. For it to work it needs mutation or some other crazy method, thus **being stateless is not a benefit**. It is immaterial whether it is FP or not, the stateless aspect is not valuable since it is unusable. If you take his comments from the standpoint of where the OP appears to be (I just learned these couple awesome facts about FP!) and consider them just reality checks and not dismissals, his arguments make a lot of sense.
Should make a movie about this: *Snake Lemmas on a Plane*.
I don't need an introductory lesson in FP. Everything he said was correct if you take it as "your simple list of problems doesn't describe reality" instead of how you and 2 other posters took it as "FP cannot succeed because of these problems with your list". To reiterate, the first line of the question and answer, which you seemed to miss: &gt; Why hasn't functional programming taken over yet? &gt; Because all those advantages are also disadvantages. Or in other words: FP is not the main programming style **right now** because solving the issues that come about from the benefits of it is still a subject of ongoing research into computer science. I love FP, I believe Haskell is ready for production use. I am not at all surprised a language that has PhD theses written to introduce optimizations isn't the #1 method of programming in the world. Remember your 3 biggest fields in CS are systems programming, game programming, and business programming. The first two lag decades behind in abstraction layers for performance reasons, and the latter is built off a large basis of imperative programmers.
But he didn't say that, the question being asked said that, he was specifically pointing out the problem in saying "Because FP is stateless it is superior".
Thanks so much for the kind words everyone. Chris and I are thrilled that you are enjoying these!
&gt; despite interviewing some yahoo the first time. Agreed. This guy seems cool, though.
Well, if the OP meant that the benefit of FP is that it doesn't allow you to have any state in your programs, then the OP's question was wrong. 
I consider the Typeclassopedia as one of the best pieces of technical writing I have read. There are not that many papers that summarize so many abstract concepts so clearly to the uninitiated. 
C# doesn't use them, but I think it makes sense for their use case.
import Data.Dennis.Miller.Ratio
I'm trying to come up with a witty way to relate Haskell to Kafka on the Shore. Doesn't help that I haven't read it yet...
Thanks for the post! I've "dabbled" in Python and thought that although I can see how it's different, it still looked a lot like Matlab. I don't know why, but that discouraged me. I think that I'm stuck with matlab because of all the toolboxes (I actually use a lot of them). Other languages that don't have all those toolboxes (i.e. libraries) are not gonna make it for me, see [Yorick](http://dhmunro.github.io/yorick-doc/) for example. Julia seems promising because of the python libraries access..! I might start looking into that. I agree with you though, it would be so nice to have something like Julia but made out of Haskell... 
As someone who might follow in your footsteps if grad apps don't go well next year... &lt;3
If you keep digging with Python, before long you'll see where it surpasses Matlab in terms of versatility. Some examples that come to mind are list comprehensions; also "in" notation e.g.: for i in [1, 3, 5]: print i I'd suggest checking out an IDE (I use [Spyder](https://code.google.com/p/spyderlib/); get it with Python(x,y) if you're a Windows user). Getting the hang of tab-completion menus is a really powerful way to learn what methods etc. are available in a given context. All this stuff makes Python a lot more efficient than Matlab even when you're starting out imho. And the fundamentals of the language and syntax are just more consistent than in Matlab. Hmmm, Yorick.... I'd better not. lol
Is it not [this](http://www.cis.upenn.edu/~byorgey/pub/species-pearl.pdf) one?
&gt; [This](http://www.seas.upenn.edu/~cis194/lectures.html) other way round
I bet you're right (I'm on Ubuntu though). Well, I'll see what I end up doing..! Thanks for all the input! Please update this if you ever end up moving on to something better..
No, it's not. Let me ask Brent if he minds if it gets put somewhere public.
FYI: that yahoo is the person you're responding to. ;)
At least with Windows 95 solitaire you had a gui and could move the cards with a mouse. A dos prompt card game is even less exciting. A programming language without a gui can only go so far.
That is awesome news. But to be fair the reactive course on coursera was a bit all over the place. Hopefully he will stick to the material he showed on Channel 9 and maybe streamline it a bit (like drop the ski combinators mention from the lectures).
Nice!
&gt; the 2 d structure and how modules are built is all very confusing Do you normally use some super secret Eclipse prototype that comes with red/green goggles?
I know, I made a hilarious and original joke, didn't I?
I've been annoyed that there isn't a Haskell course on Coursera or Udacity. It would be an awesome way to react more new users. I'm glad somebody is finally stepping up.
Let me just go ahead and whoosh myself then.
Sorry, I'm not sure I understand exactly what you mean. I'm not rounding anything. Is there a clear distinction between a survey and a poll?
This is great news. The Channel 9 lectures were excellent, definitely a big help when starting with Haskell.
Ditto. I applied once in 2012 and received the same (prompt and polite) reply, but never heard back after that. I figure my application was just relatively weak.
It's like Inception http://www.reddit.com/r/haskell/comments/1udwos/xkcd_references_haskell_crosspost_rxkcd/
Yorgey says around the 20:55 mark that 'inverses destroy information'. How does that work?
[Say no more](http://replygif.net/i/163.gif)
&gt; despite interviewing some yahoo the first time. You houyhnhnm some, you lose some.
I have had it with these motherfucking... side effects? I'll see myself out.
I think the way forward is a better rewrite system in GHC. I'm playing with a new implementation of the simplification process which avoids some of the strictness issues in the paper. I have some notion of what extra capabilities would be needed for RULES, but haven't put it down on paper yet. The sort of summary is: add view patterns to RULES. In some sense, this will make RULES more like TH and less like they currently are, so maybe this will be an additional feature rather than a modification of the existing RULES system. I'm hoping to put it together in a wiki page and see if someone will host me for an internship to work on it.
I do, but haven't written it down in any detail yet. I'll refer you to the short extremely vague idea here: http://www.reddit.com/r/haskell/comments/1vuyu2/solving_stream_fusions_concatmap_problem_new_paper/cexmf0w
Ah, I see. Thank you.
EclipseFP is easy to install and use. http://eclipsefp.github.io It should definitely use more love 
Odd, I've always thought of groups as *preserving* information, not destroying it. If you know the sequence of operations that have been applied to an element of a group, and also know the final result, you can retrieve the starting element by applying inverses. But you can't do the same with monoids in general.
Why is that? I've been playing hearthstone, and it's a pretty small ruleset. What cards or interactions are tripping you up? 
I'd just say that having inverses is a strong thing to ask for. Groups are less dynamic for it.
Your logic doesn't work when purity is touted as one of the primary benefits of functional programming. You call it controlling state, but unless you understand several complex topics that doesn't make sense to most people learning.
Again, why does everyone treat the argument as black and white, and everything he said as attacking FP as a paradigm. All of the things you said to refute his point *support his point*. You cannot do logic that is stateless, full stop, it isn't generally useful. That is all he is saying. When you try and simplify FP down to "it is default pure" you aren't saying something valuable. If you instead started with the basis you are implying "Is the FP or IP method of managing state (explicit vs implicit) superior?" then you can have a discussion like that. FP tends to hook with immutability, since everyone has dabled in it and likes it. Once you have dabled and realized the power you need to see the flaw, which is what he is pointing out. Only then are you willing to go through the pain of figuring out how state works in FP. And to be fair, while FP has a vastly superior state management mechanism, it is also much more complicated to understand, and unless you see why state is necessary (which is trivial to show) there is no reason to pay that price.
I thought I was the only person who wasn't satisfied with the Reactice course. Seemed like nothing but positive feedback in the forums.
Well there were lots of complains in the forums about assignments. Martin's part was totally unnecessary and confusing. I only took the course to watch the Erik Meijer lectures which were pretty good. But he seemed not as as enthusiastic as in other videos (like the Channel 9 ones for example). I also find the scala syntax and naming conventions obscure a lot of the monadic stuff.
He told him all about astronauts wearing space suits eating burritos filled with nuclear waste...
Its mostly Haskell that is the stumbling-point, as I'm new to it. A little while ago, I posted a [question about the DrawM monad](http://www.reddit.com/r/haskell/comments/1ug5nl/can_someone_explain_the_drawm_monad_to_a_beginner/) in the [game-probability package](https://hackage.haskell.org/package/game-probability) , but didn't get any response, and I haven't revisited it since. Modeling the hearthstone game seems largely straightforward, but I haven't nailed down all the edge cases: i.e. simultaneous deathrattles.
Interesting, how does it resolve events that occur simultaneously? ie, two 1/1 creatures, each with a distinct death rattle. Both die to a mass-inflicted damage spell, which resolves first? Is there any documentation on the nitpicky edge cases like that? 
I was literally thinking about this today. I'm in college and a student in one of my classes presented something she made on Prezi, I'm totally going to apply. Thank you for reaching out! Edit: What's your email address?
jobs@prezi.com
Also check out https://github.com/jekor/jcoreutils
The standard format: laszlo.pandy &lt;AT&gt; &lt;COMPANY&gt; .com Btw you can view email addresses in Google Groups by viewing the raw original message and clicking: show unmasked email addresses.
Googling the phrase yields nothing. Still trying to understand what this means.
I spent a few very pleasant days at Prezi HQ in Budapest a few months back when I went out there to give my talk on [succinct data structures](https://www.youtube.com/watch?v=uA0Z7_4J7u8) to the Budapest Haskell User Group. It was a very neat space (the main office floor was an old ballroom converted to hold a bunch of desks) and I had a lot of fun talking to people there about Haskell. In addition to work on Elm, they also have Csaba Hruska, one of the authors of [Lambda Cube](http://lambdacube3d.wordpress.com/) over there.
Is there any hope of ever using Haskell libraries with Elm?
Since there is a JS FFI in Elm now and GHCJS, I don't see why not.
[Fay](https://github.com/faylang/fay/wiki) seems more relevant to what you are asking for.
[They did make a movie about this](https://www.youtube.com/watch?v=etbcKWEKnvg).
On some video interview Erik Meijer said he likes the first edition of this book more than the second. What do you guys think of this? He also said it is one of the best books on functional programming. 
I don't mind at all. I had been waiting until we incorporate feedback from the reviewers but given the interest we might as well put it up now.
I've only tried with shared libraries, but this sort of works: test_c.c: #include "test_stub.h" int main(){ hs_init(0,0); haskellPrint("abc"); return 0; } test.hs: {-# LANGUAGE ForeignFunctionInterface #-} module Test where import Foreign.C haskellPrint = print foreign export ccall haskellPrint :: CString -&gt; IO () compile like this (I assume there's a better way than hard-coding ghc versions, but no idea what...): ghc -fPIC -shared -dynamic --make test.hs -o libtest.so -lHSrts-ghc7.6.3 gcc -o test test_c.c -I/usr/lib64/ghc-7.6.3/include -L. -L/usr/lib/ghc-7.6.3 -l test -lHSrts-ghc7.6.3 And then run like this: LD_LIBRARY_PATH=/usr/lib/ghc-7.6.3:. ./test And, it gives this: 0x0000000000400814 which I assume is the textual description of the C pointer that's being given to "print". That's a pretty ugly build process (and no idea how it translates to OSX), but maybe somebody else can chime in with better build ideas.
This is the incantation that worked for me, with no need to bring `gcc` into it: ghc -no-hs-main test_c.c test.o -o test I got the same results you did.
It was a pretty imprecise statement. I just mean that if you have some value m and then you combine it with m^-1, poof, you have the identity element --- you can't remember that you used to have an m. For example, with a version control system you typically don't want actual inverses---when you revert a commit you still want to remember the fact that you made the commit and then reverted it, so it is not a true inverse since it is not the same as doing nothing.
Chris, are you going to do more *Haskell From Scratch* videos, or are Haskell Cast and Real Life (tm) taking up too much of your time?
You need to list more options if you truly wish "well-rounded" results. An "Other" option does not offset the increased ease of specifying a choice with merely a bullet.
I'd love to have an easy way to use Haskell as a server backend for Elm applications. An FFI is one thing (so you can communicate to your server over some protocol,) but being streamlined would be great.
&gt;coreutilhs give newbies a taste for what "real" Haskell programs might look like I'm not sure this is true. Real (haskell) programs: 1) are not one-liners in IO monad; 2) work with typed data; 3) are written with performance in mind; 4) are more or less standard-compliant (if there're standards in the field).
I wrote the review: http://vorba.ch/2014/review-haskell-financial-data-modeling.html. The book is really bad, although the review sounds a bit more positive.
You're right, the file was cached when I tested on accident. Fixed!
Yeah, it's like combining matter and anti-matter!
If you click on the tips you can see the discussion, when there is discussion.
&gt; Second, as the others said, you will see a pointer output if you use print. That is expected because type CString = Ptr CChar. So it is actually just a pointer (just like it is in C,) and the Show instance does not dereference it. Here I just used puts to demonstrate a working example. :) Thank you! Is everything on the FFI passed as pointers? Specially I'm actually interested in `Bool` and such. Do they all do that in general, or is `String` special? Also, what's with passing values from haskell to C? Do you have to do the same thing?
I have lately been doing clojureScript. Although it is not as aimed for general direct graphics and interaction as Elm is, it has been rather fun. Elm has been on my watch list.
I used to spend half an hour on a train commuting to work a while ago, and would always write Haskell during that time. I was overjoyed when once (!) someone noticed it was Haskell I was writing. I think he said something along the lines of "wow, don't see that every day" (my reply was something like "no, but you should!")
Thanks for the interest! I've added a installation section. Please try it out and give feedback when it doesn't work.
Hence the quotation marks. I guess "real" tasks might be more apt. I expected this kind of feedback, but I didn't find a better way to put it. If you have a more fitting description of the project, then feel free to send a patch to the README. As for one-liners - not all of the programs are this. And I am planning on implementing e.g. grep and other less trivial programs than "yes" and "true". Furthermore, in time, I might implement options as well.
Good stuff, thank you.
&gt; This is just dumb. Why do people keep saying this as though pure functional languages don't allow side effects and mutation? The OP never said this, you are putting words in his mouth and attacking a strawman while insulting him in the same sentence. Nice. All he said is this: &gt; Real-world programs are all about side effects and mutation. Which is quite accurate. He never said that Haskell can't model side effects nor that purely functional languages don't do side effects. 
I've been writing a library for this. It uses template Haskell. See [this discussion](https://groups.google.com/forum/#!searchin/elm-discuss/haskelm/elm-discuss/dfc-Eyucro4/d3jmm9EMGCkJ). EDIT: provided direct link
That's wonderful! Looking forward to a public announcement with examples :-)
You can tell which types will behave as pointers by examining the definition of the type. Enter `:i &lt;type&gt;` in `ghci` and look for `Ptr`s. But in general, no, just types that correspond to pointed types in C.
Assuming you're talking about the subreddit title (and what you see if you have /r/haskell in .rss form) I think `Haskell :: Reddit` is both descriptive and succinct. It loosely says "Haskell, of type Reddit" to me, which is a fairly accurate description. I'm not sure it matters that it doesn't compile/isn't a complete program.
I was really bummed out to have missed you, I was there just a week later. At least I got to take some of your Haskell stickers to Helsinki. ;)
People still use CVS? And there are only 4 options in the survey form?
This title sounds like something straight out of [SCIgen](http://pdos.csail.mit.edu/scigen/)
None of the images load for me.
You need to reverse the order of `FreeT` and `StateT` so that `FreeT` is on the outside and then `StateT` is on the inside. Then your interpreter for your `FreeT` structure will have access to the game state.
No problems in that regard here (and i don't have JavaScript enabled for that site).
This subreddit has the best post titles.
Same here on both Chrome and Safari. The svg files are getting downloaded, but apparently the rendering is failing. Update: the js console shows this error message: Resource interpreted as Image but transferred with MIME type text/xml: "http://h2.jaguarpaw.co.uk/images/dlist-appends.svg". 
Hmm, strange. I guess I'll have to convert them to .png instead. 
Now with PNG graphics rather than SVG, since some people were having trouble with the latter. (You may need to Shift-click-reload to clear your browser's cache, or my server's).
A slightly different approach, but I tend to use a big free/[operational][1] monad first and then try to figure out the interaction of different effects when writing the interpreter. In other words, I would simply merge the state monad and the `TextlunkyCommand` effects into a single monad. [1]: http://www.haskell.org/haskellwiki/Operational [2]: https://github.com/HeinrichApfelmus/operational/tree/master/doc/examples#readme
Note that the technique goes way back to the beginnings of functional programming, definitely way before Janis' paper where the OP apparently first met it.
Is it akin to "Dynamic Syntax" (Ruth Kempson et. al.)? "Dynamic Syntax is a formal model of utterance description which tries to articulate and substantiate the claim that humans’ knowledge of language is essentially their ability to parse spoken language in context. DS provides an explicit model of how hearers build incrementally (that is, from ‘left to right’) a semantic representation (an interpretation) from the information provided by the words they encounter and from contextual information. From this perspective, knowledge of language is not so much ‘knowing that’ (‘competence’) but ‘knowing how’ (‘performance’), which leads to a number of challenges to current thinking about syntax." (quote from http://www.soas.ac.uk/courseunits/152900093.html)
I said "*codensity transformation* first outlined ... by Janis Voigtlander", not DList.
Yes, that's right. Then if you use `runFreeT` or `iterT` you will have access to `StateT`.
I thought it sounded quite straightforward.
At this point it is still unclear whether Wayland will indeed replace X. Yes, some people talk like it will but adoption rates for support are pretty close to zero and even if they manage to pull it off it will probably be an effort similar to the IPv4-&gt;IPv6 migration in terms of timescales as X just doesn't seem to be as bad in practice as Wayland supporters claim and the features thrown out compared to X aren't quite as useless as claimed either.
| | Real-world programs are all about side effects and mutation. | Which is quite accurate. No, it is not. A tiny portion of real world programs deals with the mutable part, the rest could easily be written in a referentially transparent way. Take your average web app. At first glance it is all about I/O and mutating state, sending data to the server, receiving data from the server, updating the DOM,... However when you look more closely what happens is essentially serialization which is completely referentially transparent followed by some I/O which is exactly identical for all webapps (sending HTTP body, the result of a pure computation, to the server, waiting for a response and calling either success or failure functions with the result), followed by deserialization which is pure again, followed by some decision making based on the data and the current DOM which is most likely pure, followed by some DOM manipulation which is pure, followed by a tiny mutating bit replacing the current DOM with the manipulated one, followed by pure rendering of the DOM,... Yes, there are little bits of mutation and I/O in between the pure bits but the vast majority of the code is in fact referentially transparent. it might use mutable data structures but those are merely a performance optimization, given the same input it returns exactly the same output which is the essence of functional programming.
Just noticed that your background is surprisingly similar to mine, only difference being that I'm in physics rather than math, but I've also found myself gravitate heavily into computer science (and likewise, not planning to stay in academia). Haskell is a really amazing language and once I learned it other more popular languages just seemed to pale in comparison! I hope you have a wonderful time with it! :)
I'll be bringing more Haskell stickers to Europe this summer. I'm heading over to France in July for a wedding and probably a talk, and may try to get in another trip or two before ICFP/CUFP this year in Sweden this September.
Yes, but IMO that paragraph leaves room to /u/apfelmus interpretation.
Using lists in you programs is a bad practice. Functional or not. You should never use them. http://www.youtube.com/watch?v=YQs6IC-vgmo Edit: you may use them if you have guarantees that compiler 'cheats' by converting them to vectors.
Lazy linked lists make for excellent control structures, especially if they are fused away by shortcut fusion. However, you are correct that they make for terrible data structures.
Add this to your file: data Reddit = Haskell | Programming | Funny | Pics | ...
John Hughes was that "clever person" not mentioned in the "Improving Performance" chapter - hence DLists are often called Hughes Lists. I've seen comments (possibly from Lennart Augustsson here on Reddit) that Hughes Lists were common folklore before Hughes's "A Novel Representation of Lists" paper. Incidentally there was a long thread on Haskell Cafe in 2010 regarding whether DLists take their antecedence from Prolog's difference lists or whether Prolog's difference list is a different thing (and Haskell's DList is mis-named). The thread starts here and continues into November: http://www.haskell.org/pipermail/haskell-cafe/2010-October/085757.html
Also apropos the "Why did we need functions?" chapter - there's a paper from Olivier Danvy showing that binary trees are defunctionalized DLists/ Hughes Lists - http://www.brics.dk/RS/01/23/BRICS-RS-01-23.pdf.
Ah, I see, thanks! The `Tree` data type is also known as the *term representation* for lists, at least that's how Hughes called it in his paper ["The Design of a Pretty Printing Library"][1]. It may be more apparent by using different names for the constructors: data List a where One :: a -&gt; List a Append :: List a -&gt; List a -&gt; List a The advantage of this term representation over the `DList` representation is that you can also pattern match on the head and tail in O(1) time, at least in an amortized sense and if used in an ephemeral way. The equivalent for monads would be the term representation data Program instr a where Return :: a -&gt; Program instr a Bind :: Program instr a -&gt; (a -&gt; Program instr b) -&gt; Program instr b I have [implemented][3] this in the [`operational` library][2] which is available on hackage. (EDIT: added remark about amortization.) [1]: http://belle.sourceforge.net/doc/hughes95design.pdf [2]: http://www.haskell.org/haskellwiki/Operational [3]: https://github.com/HeinrichApfelmus/operational/blob/master/src/Control/Monad/Operational.hs#L198
&gt; The advantage of this term representation over the DList representation is that you can also pattern match on the head and tail in O(1) time, if used in an ephemeral way. I don't understand this. It seems that if the tree were very left biased it would take O(n) time to walk to the leaf containing the head of the list.
You prefer the second then?
I think the second edition is fantastic and recommend it widely. I haven't done enough of the exercises in the first edition to have as strong an opinion on it. 
Yes, that's true. I meant O(1) in an amortized sense (edited my comment) and only if you use it in an ephemeral way. Basically, the left-biased tree will be turned into a right-biased tree after you've looked up the head, at which point all remaining operations are O(1) worst case. You can amortize the initial O(n) cost over the length of the list, i.e. the number of times you call the append operation.
Thanks, those are nice references. I included them.
Richard O' Keefe summarises the differences between Prolog difference lists and Hughes lists very well in that thread: http://www.haskell.org/pipermail/haskell-cafe/2010-October/085769.html
Thank you! That makes sense.
So you mean it's basically supposed to be used like a splay tree? That's not very purely functional friendly. :)
You must be living in a fantasy world. Last time i checked most of the 'cool' Haskell code crawled and eat gigs of RAM. So don't preach to me about 'garbage collectors' and 'what if' or manual memory management. Stack is just a plain dumb vector, not a list. Another thing is that you don't know how GC works. GC is not a magic wand form your Haskell wet dream. GC wont defragment your list - it is not its purpose.
Of course. Every data that can be fragmented will be fragmented. Lists were good option for a data structures in the era where you had direct access to memory without delays. Today RAM access is just as bad as network access. Haskell software often crawls because of overuse of this - because of so called 'laziness' everything ends up in the list, one big fscked up list. And if you want performance you have to write code that is so ugly that some monitors break just from displaying it.
**Thanks for linking me to that!** I had not encountered "Dynamic Syntax" when I was doing my masters thesis. From the abstract of this paper it seems this kind of formal modeling is exactly what am trying to achieve. Since I am not familiar with that Dynamic Syntax, I don't really know how it is typically implemented in a computer system. My implementation uses ordinary UNIX-like "glob" expressions to encode semantics and associate it with an anonymous function, and the function is called when the glob expression matches an input string. The function can do anything since the Dao programming lanugage is a full scripting language. So it is powerful, but unfortunately that also means it might require programmers to write more code, and it may run slower, in order achieve the same programmatic behavior that could be achieved with a more specialized Dynamic Syntax system. I hope in time, if a community forms around my project, there may be some good libraries developed to solve this problem.
Pretty much. Still beats difference lists, though, where `tail` is not amortized to O(1).
Yes. The opposite would be "persistent", which means that old copies of the data structure are still available and used. See [Okasaki's book][1] for a more detailed analysis. [1]: http://www.cs.cmu.edu/~rwh/theses/okasaki.pdf
I'm not trying to sell anything. Keep in mind that you are in /r/haskell. It's not like I popped up out of nowhere in some unrelated place and spouted some nonsense about how linked lists will solve all your deepest problems. Also, I'm not the one who believes in voodoo. I *understand* how it works. You're the one calling it voodoo. 
The #haskell-ops IRC channel is what you're looking for. Ops there typically don't ban on a whim, but if you can make a reasonable case for getting unbanned then it's worth a try. 
http://ircbrowse.net/browse/haskell?q=lovewithacaveat If you're talking about the conversation on the 24th, yeah you were being annoying, but I don't think you deserved to be banned for it. Hope it all works out!
I know a bunch of people have been working on it (for multiple interpretations of "it"), though I'm not sure whether they'd count as having "successfully implemented [it] in practice". Research languages being what they are. Also, fwiw, I was referring to the "fully general first-class pattern" part of things, not necessarily to the `Prism` part. It's been a long time since I looked at prisms and didn't quite grok them from that cursory reading.
Hey thanks! I was just wondering that yesterday, for serving SVG from Yesod. But then ImageR did it for me automatically anyway :-)
It's straightforward if you're familiar with other kinds of fusion, at least in a basic level. [This is a good start, I guess](http://www.haskell.org/haskellwiki/Correctness_of_short_cut_fusion).
&gt; because of so called 'laziness' everything ends up in the list, one big fscked up list. What list? I hope you're not talking about the runtime, they didn't just implement the thing from Essentials of Programming Languages you know. &gt; And if you want performance you have to write code that is so ugly that some monitors break just from displaying it. Not unless you're going for hand optimized C-speed, which is quite ugly as well. You can get decent performance improvements just by specializing some stuff and using the right libraries.
DList seems like `shows` from `Show` class. Why is DList less front and center in List processing in Haskell?
ICFP and CUFP?
Thanks, indeed that makes sense. I wonder what's wrong with my server setup ...
It makes fundamental use of the cancellation law, which is a property of groups, but not monoids. In any case, the whole point of the exercise is to show that all elements are equal; the one element monoid is a group.
I think he's referring to the free group on the 26 letters modulo the homophony relations.
Your operational article is quite readable, thank you! Two questions: 'Operational' seems very similar to the 'Free monad' model that has become popular in recent years. What is the relationship? 2. &gt; Second, and this cannot be ameliorated, we lose laziness. The state monad represented as s -&gt; (a,s) can cope with some infinite programs whereas the list of instructions approach has no hope of ever handling that, since only the very last Return instruction can return values. That seems serious! Is it not possible to upgrade the "list of instructions" to "list of instructions, each of which Returns a 'current' value in addition to its regular function"? IOW: interpret :: StackProgram a -&gt; (Stack Int -&gt; [Trace a]) where `[Trace a ]` is a list of stack snapshots, or the top of the stack, or "Writer-style" log data, whatever is appropriate for the intepreter, perhaps chosen as a parameter so the stackprogram-programmer can choose how much space to spend on the trace data.
You assume `a^-1` exists, even if you can't express it as a product of generators. Then you use it to prove we == wee Implies w^-1 we = w^-1 wee implies e == ee etc
The "questionable" homophone pair veldt/felt actually seems to be crucial for this to work for the letter "v", at least going by the provided list! I found proofs the other 25 letters fairly quickly just scanning the document by hand, but had to write a short script to finish it. Veldt/felt seems to be the only homophony class with words containing different numbers of v's.
In cat.hs you swapped interact id for getContents &gt;&gt;= putStr was there a technical reason ?
This problem is cute as hell.
Ups sorry, forgot write that; the one in Sweden.
ICFP covers a reasonably broad range. I attended my first ICFP in 2006, and got a lot out of it, despite being relatively new to type theory and PL in general. CUFP in particular also includes a tutorial component where we do a couple of days of tutorials in addition to the main talks.
Okey, sounds interesting. I doubt I'm knowledgeable enough, still would be fun to meet some people with similar interest.
 interact :: (String -&gt; String) -&gt; IO () interact f = do s &lt;- getContents putStr (f s) So, no. I just think that using id like that makes things look odd.
someone on /r/math suggested `sheaves/sheafs` which i had to look up but is valid. of course using algebraic terms for algebra examples is a nice boon to that word.
Yes, `ShowS` is a `DList`, specialized for `Char`.
`leaves/leafs` would work too (as in: she leafs through a book)
Do Americans really pronounce adieu as ado??
Thanks!
That's really nice looking (just skimmed through so far), thanks for writing these up! I'm really interested in doing HPC type workloads in Haskell, and such a detailed tutorial with lots of benchmarking and analysis is much appreciated. There's also 'write a synthesizer in Haskell' on my endless TODO list, this will come in handy. btw, I was wondering if this code could help speed up the popular JuicyPixel image library? I have no idea what DCT is used for the JPEG decoder there, but given that it's a pure-Haskell library your code might be faster than what's used now.
This kind of comes under the "implement all the real-to-real transform types" since JPEG uses a discrete cosine transform. At the moment, my code only does complex-to-complex transforms. I know I'm going to have to implement all those other transform types at some point though, so this might be something to look at once they're done.
And thanks to this, `operational` can use the term representation, giving O(1) `bind` and O(1) amortized access to head and tail. In contrast, `Free` is stuck at either O(n) for left-associative uses of `bind` or O(n) access to the tail. In other words, operational provides an asymptotic speedup that `Free` cannot achieve for fundamental reasons.
&gt; That seems serious! Is it not possible to upgrade the "list of instructions" to "list of instructions, each of which Returns a 'current' value in addition to its regular function"? That doesn't help. The problem is this: Consider the program import Control.Monad.Trans.State import Control.Monad example :: State Char String example = forever (put 's') &gt;&gt; return "Hello" Thanks to lazy evaluation, we can actually query the result of this infinite program &gt; evalState example 's' "Hello" because the last `return` is used in a left-associative way. Note that we can only query the return value, it doesn't make sense to query the state, because that one would depend on all (infinitely many) instructions in the program. When using lists of instructions, this kind of evaluation is not possible, because you can never "see" that last `return`. In a sense, proving that the special type `s -&gt; (a,s)` fulfills the monad laws gives you more information than proving that the corresponding `Program` type fulfills the monad laws, and `example` makes use of this additional information. 
I didn't even think of the DCT in JPEG being an integer one, apart from the other reasons you list. Thanks for the quick reply guys, JuicyPixels is just the first thing that popped into my head as a potential use case.
The definition of the DCT in the jpeg standard (ITU-81) works on real elements; but as you can imagine much work has been done to implement it using integers, and that's what I used =]
As would prove/proof
Something something about losing your cat(egorie)s?
As a native (British) English speaker, none of these sound like homophones to me.
Looks like APL to me. Poor Haskell... Why are you writing code in such a messy way? Nobody is going to learn Haskell if code looks like this. Do you want to scare people away?
For getting started with type theory, my usually recommendation is to start with [Types and Programming Languages (TAPL)](http://www.cis.upenn.edu/~bcpierce/tapl/) and move on to [ATTaPL](http://www.cis.upenn.edu/~bcpierce/attapl/). If you find you want to keep climbing to higher powered type systems by all means keep going in that direction with something like Zhaohui Luo's [Computation and Reasoning](http://www.amazon.com/Computation-Reasoning-Computer-International-Monographs/dp/0198538359/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1390835631&amp;sr=1-1&amp;keywords=0198538359). By the time you're done with that you probably have a good enough background to seriously explore [the Homotopy Type Theory book](http://www.amazon.com/Homotopy-Type-Theory-Foundations-Mathematics/dp/B00DVZHHWA), but that book really does assume you have a solid background in intensional type theory. For category theory, [see my Quora answer](http://www.quora.com/Category-Theory/What-is-the-best-textbook-for-Category-theory?share=1) for a detailed breakdown by background.
For type theory, [Bob Harper's book](http://www.cs.cmu.edu/~rwh/plbook/book.pdf) is pretty good. [Programming in Martin-Löf's Type Theory](http://www.cse.chalmers.se/research/group/logic/book/book.pdf) might also be useful. For category theory, I've been working through [Category Theory for Computing Science](http://www.case.edu/artsci/math/wells/pub/ctcs.html), which, as the name suggests, draws its examples from CSci rather than from general mathematics. For me, that's easier to follow.
&gt; some pronunciations I would say "all"
there's also [this answer](http://stackoverflow.com/a/13879693/849891) by Daniel Fischer on SO from Dec 2012 which discusses this stuff too and shows the (.) -&gt; ($) tree restructuring with some fine ASCII art.
It took me a while to cotton on, but it's particularly silly and a great "result". I wonder if rhyming lists can be used to deduce other joke homophones like ("fish","ghoti")?
Oh wow that's great. Stackoverflow tells me I'd already upvoted that answer a year ago, so I must have seen it. I've I'd remembered I don't think I would have needed to write my article.
As a Dutchman victim to sensibilities, hearing my compatriots pronounce words like 'proof' and 'prove' as though they were the same makes me want to whip out my pair of Peruvian calfskin gloves and slap them across the face with it.
Can you explain this a bit? I don't know cobol.
You lose some benefits, however, from not being able to mix and match.
Fair enough. Though if he's looking for "type theory and category theory" I'm doubting he's coming from a strong algebraic topology background. ;)
COBOL's one redeeming feature is its excellent file handling facilities. COBOL had the ability to set up records and create a B-Tree indexed table for those records back in day 1. It then also has a rather huge syntax for performing queries into those tables. COBOL has always had lightning fast but inherently non-ACID table access which is the hallmark of NoSQL. It'd be pretty trivial to map COBOL file handling onto a NoSQL backend and use existing COBOL programs. Though in all likelihood the NoSQL systems will struggle to outperform the more mature COBOL solutions in existence.
Well, of course. :) What I meant to say was that some words that are homophones in some area might not be homophones in some other area.
note to self: a primer on topos theory that i might be able to follow is here http://arxiv.org/pdf/1012.5647v3.pdf
Right -- this is more to clarify for other people that might be interested in HoTT.
Mix and match? Could you elaborate?
[Here are the slides.](http://llvm.org/devmtg/2013-04/thielemann-slides.pdf) (PDF)
Just wanted to update; it worked. Thanks again, /u/Tekmo.
I wrote a DCT based image compressor many years ago and remember looking into how state of the art 8x8 (I)DCTs were implemented after I got the naive transform working. Very tricky stuff, converting an existing implementation certainly seems like a really good idea!
The intent behind `read` and `show` is that they render Haskell types using Haskell conventions, so abusing them in this way is a bit icky.
Thanks for the feedback. That's fair enough, though as it's intended mainly for interactive use, it shouldn't be an issue.
I was working on a personal project yesterday and ran into the same question. I know `show` and `read` are well defined in terms of being inverses of each other, and being machine - readable format. Is there a standard typeclass that indicates: "This type has a way to turn it into a human readable form, suitable for most output" - Java has `toString()`, ruby has `to_s`, etc. I know I can make my own, but is this a case that's handled by the standard library?
/u/jaspervdj did an internship there IIRC.
There isn't a distinct one and `Show` is often abused for that purpose, especially for types which don't have a nice Haskell serialization. People have been grumbling about the need for such a class for a while, though.
Is there any common name given to it, even if it's not actually in the library. I made a class called `Pretty` with the single function `pretty` as its interface. `ToString` might be ok. `ToHuman`... 
Are there any decent-sized example projects using HGamer3D 0.3.0 (or older versions)? I'm interested in seeing what it can do.
http://hackage.haskell.org/package/mainland-pretty-0.1.1.0/docs/Text-PrettyPrint-Mainland.html#t:Pretty
But I'm not really worried about pretty printing though, so that brings a fair bit of complexity. I just want to have something where I can print a poker hand as `"Ace of Spades, Two of Diamonds, ...."`, rather than a show instance which may be `"[AS, 2D, ...]"`. Pretty printing may then build on top of that, but it seems like a large dependency there for a typeclass which really should have 1 function. 
[Mathematical maturity](http://en.wikipedia.org/wiki/Mathematical_maturity) is that sense of the shape of mathematics that comes from not being afraid of proofs, from having passing familiarity with lots of areas of mathematics so you can spot distant connections more implicitly, etc. CftWM is hard for me to recommend in isolation to someone at an undergraduate level without a wider view of the world of mathematics, as a lot of the examples will be pretty far afield of what you've seen. It is good to pick up, and you can get a sense of where your weaknesses are though by going through it, though.
#####&amp;#009; ######&amp;#009; ####&amp;#009; *Here's a bit from linked Wikipedia article about* [***Mathematical maturity***](http://en.wikipedia.org/wiki/Mathematical%20maturity) : --- &gt; &gt;**Mathematical maturity** is an informal term used by mathematicians to refer to a mixture of mathematical experience and insight that cannot be directly taught. Instead, it comes from repeated exposure to mathematical concepts. &gt; --- ^Interesting: [^Lattice ^\(order)](http://en.wikipedia.org/wiki/Lattice_\(order\)) ^| [^Joseph ^Ehrenfried ^Hofmann](http://en.wikipedia.org/wiki/Joseph_Ehrenfried_Hofmann) ^| [^Books ^on ^cryptography](http://en.wikipedia.org/wiki/Books_on_cryptography) ^| [^Outline ^of ^mathematics](http://en.wikipedia.org/wiki/Outline_of_mathematics) [^(about)](http://www.reddit.com/r/autowikibot/wiki/index) ^| *^(/u/edwardkmett can reply with 'delete'. Will delete if comment's score is -1 or less.)* ^| ^[Summon](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I don't think CftWM is a good first book for anyone, actually. its an indispensable reference though. Cat theory is a big realm of stuff, and its good to be able to look up a whole bunch of theorems on any basic object.
I can buy that general assessment. I'm somewhat biased in that finally bulling through Mac Lane after a half dozen tries was how I actually learned category theory, so it has been proven to work in at least one case. ;)
You're welcome!
If System F is what you care about I don't think books are the answer. After someone had read TAPL, I would suggest some classic Wadler (since they tend to be very readable and cover core system F material) * Theorems for Free * Recursive types for free * The Girard-Reynolds isomorphism Also, it is probably best to read them in that order. The book *Proofs and Types* by Girard et al is pretty good for getting a feel of the denotational semantics of System-F like languages, but I would suggest some solid PL background first.
The dependency is not for the typeclass, the dependency is for the combinator library that makes writing pretty-printers very easy. It just so happens that the typeclass is provided as a convenience.
Ah. In the first example, I had thought that the problem was that infinite structures don't expose their 'head' in finite time. But your newer example uses a more tree-like infinite structure. 
This certainly seems useful, I've come across this pattern a lot! I wonder, what's the best way to address a mix of local / shared state? Normally I'd have a Reader with (T|M)Vars for the shared state and a State with the thread local one. It seems like neat idea if I could just use State functions and the underlying typeclass would automatically distinguish between values that require a transaction and those who don't. Can I compose the state operations somehow like regular STM? i.e. if I have two modify's, is there a way to run them as a single transaction?
I don't really see the point of this; you appear to have hidden all the useful parts of STM behind an abstract interface that's essentially equivalent to an IORef. What difference would it make if you used an IORef instead of a TVar? STM doesn't guarantee that you are free of race conditions, it only gives you a toolbox for managing them. The same is true of the traditional RDBMSes that inspired it.
Yes, but external links, videos, slides - is pretty cool stuff too.
We just recently needed a quick-and-dirty way to serialize/deserialize a highly nested and complex data structure, with components from many libraries. The only reasonable way to do it was `Show` and `Read`. We did have to do some clean-up work - mainly adding missing instances. But this was still much, much better than rolling our own. So please, be considerate of others, and keep up your `Show` and `Read` instances, following conventions. Thanks!
Never say never. If you are an absolute beginner and are just trying to get the hang of how "read in, print out" works, it's not yet time to throw in the additional complexity of `readMaybe`. No matter how important the benefits, and no matter how minuscule the complexity.
The problem is that GHCi uses Show to print values. When you work with complex data types, say an AST, the output becomes completely unreadable if you format it as Haskell code. Compare suc : A → Var A to TypeSig (ArgInfo NotHidden Relevant []) (Name (Range []) [Id "suc"]) (Fun (Range []) (RawApp (Range []) [Ident (QName (Name (Range []) [Id "A"]))]) (RawApp (Range []) [Ident (QName (Name (Range []) [Id "Var"])), Ident (QName (Name (Range []) [Id "A"]))])) 
Look up Buildable in text-format
More specifically, it needs to be composable with read parsers that use lex.
I'm not following very closely, but if you're going to use a free monad then your base functor shouldn't contain return and bind. I would do data Exp a = ReadAccount (Int -&gt; a) | WriteAccount Int a | SetVictory Bool a | OnTimer a and then use `Free Exp` as the monad you work with. 
That's right... Although I need to use a GADT because sometime my return type is not an "Exp a" but a more specific "Exp Bool" or "Exp Int"... The arguments for some primitives is not a base type (like Bool) but an expression itself (like Exp Bool). In the case of SetVictory, it would be "SetVictory (Exp Bool) a" in your notation I suppose.
&gt; Although I need to use a GADT because sometime my return type is not an "Exp a" but a more specific "Exp Bool" or "Exp Int" I doubt you actually need a GADT. Can you be more precise about why you think you need one? 
He's right, his constructors are too specific to use a normal ADT.
OK, now having read more closely what you wrote, it seems like the problem is that `Exp` tries to model both some condition of your game state, and the dynamics of your game state. This seems very odd. Why not have `Exp` model solely the condition on game state, and a `Command` type which models the dynamics?
Right - more specifically this seems to make it impossible to modify more than one var in an atomically block, which is really the whole point.
No, read and write belong together. The argument to SetVictory should be of a different type though.
That would be a good idea, yes. However, maybe you want to be able to mix them. I use a different approach for a similar problem in a library I'm developing. data Writes -- there are no values of types Writes: and we don't want them data ExpF r a where ReadAccount :: (Int -&gt; a) -&gt; ExpF r a -- I think this is what you actually want, as you want to fill the Int at runtime WriteAccount :: Int -&gt; ExpF Writes () -- Note how as we're going to use a monad, the arguments don't actually have to be exps themselves SetVictory :: Bool -&gt; ExpF Writes () -- You want this, right? OnTimer :: Exp r a -&gt; ExpF r a -- write a Functor instance type Exp r = Free (ExpF r) readAccount = liftF $ ReadAccount id :: Exp r Int writeAccount i = liftF $ writeAccount i --etc expressionWithEffects :: Exp Writes Int expressionWithEffects = do i &lt;- readAccount writeAccount (i+100) return $ i*2 expressionWithoutEffects :: Exp () Int expressionWithoutEffects = do i &lt;- readAccount --writeAccount (i+100) -- this wouldn't compile now! return $ i*2 Basically, the argument `r` tracks the effect. Effectless constructors don't specify it, so then can take any type in it, effectful ones use our `Writes`, and if you want to force an expression to not have effects you just make the type `Exp ()` or something. `ReadAccount` uses a function because it's a placeholder for an `Int`, but doesn't contain one.
That's great. How do I make ExpF a functor, though? Do I have to add continuation parameters on every primitives? Like in http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html Also, I don't undertand your ReadAccount: ReadAccount :: (Int -&gt; a) -&gt; ExpF r a For me it should be: ReadAccount :: ExpF r Int As it takes no argument and returns the account value (stored in game state) I also really want "SetVictory" to take an expression as a parameter: SetVictory :: Exp Bool -&gt; ExpF Writes () So that I can store the victory condition in a field of the game state. Then, each time I want to know/display if victory has been achieved, I can re-evaluate this victory field in a stateless way. But the condition should not contain any "writing" instruction! 
Yes, I think so. Now that I think of it, otherwise it's not a `Functor` at all. You only have to add them on the writing ones.
I will give you C+. Fist thing you do when you deal with someone else's code is reading, to understand it, so it has to have descriptive names (functions, variables, structures, types, etc...). Using single letters in a function which has more than 2 lines or 5 tokens is a big NO, because people on average can focus on 5 things max. Especially when your code may crash and log will spit out tons of these meaningless symbols. Another thing is that your code has no comments, so everybody has to read it whole just to guess its purpose. "Processing" is not a good description, because you could equally replace it with "doing something with ...". 
Dear creator of that blog, please use a larger font size for the code. You use a 12 px font for the text, and a 7.5 px font for the code. Even when playing with the zoom on my browser, I cannot get both to a comfortable size at the same time. I eventually dove into the CSS and changed `font-size` in the `body` tag from 62.5% to 100%, and now it's nice and readable.
Just for printing. I have solved it myself so far, but was really just looking for a standard approach. It seems like a common need: "Present this data as a nice text string for human consumption". I wonder why it's not standardized at all. Is it simply something people solve on a per-project basis?
This post is from way back in the days when Spencer Janssen was still active. And when the "memoization" meme was a fad.
Haha, you have a really good point. I didn't think of that. What if I export a combinator called `statefully` as well, with type `(s -&gt; STM (a, s)) -&gt; StateC s m a`? Then you get a proper interfaces for useful transactions. Or would it be better to have `statefully` as the sole "state" combinator? Edit: `statefully` renders the other combinators obsolete, I guess. I really didn't think this through. ReaderT (TVar a) is probably a better solution...
&gt; Not so elegant if you had to explain cryptic names in your code. You are entitled to your opinion, but many in the Haskell community seem to agree that unnecessarily descriptive names can make code harder to read. Of course there are many languages where that is not the case, but in a Haskell type signature short names make it much easier to see the relationships between the types and the overall "shape" of the signature. Many people find this preferable when the structure is more important than the naming, as is the case with a concept like "pattern functor". &gt; At lest you did it. You are in these 0.1% of haskellers who actually try to write readable code, not APL. When you find it difficult to read code written by 99.9% of the users of a given language, that does not mean that all those users do not write readable code. It could mean that you have not fully assimilated the idioms of that language.
On the contrary I think the overall Haskell style is very nice and perfectly idiomatic.
Ah code-master has a new account.
I know APL and from my experience after few weeks you loose ability to understand your own code, so don't tell me about "assimilation". "Unnecessary descriptive names" sounds like unnecessary hand washing to a physician. Would you like to be examined by someone who sometimes forgets to wash his hands or does not do it at all?
Would you like to give a link to what you consider well-written Haskell? 
This is quite good example: http://www.reddit.com/r/haskell/comments/1wdaho/elegant_memoization_with_higherorder_types/ Not all of it but it is easy to read by a newbie, in contrary for ex. to this: http://learnyouahaskell.com/input-and-output#bytestrings Ex.: &gt; "B.cons 85 $ B.pack [80,81,82,84]" If they teach people to write mess like this...
OK, another question: can you take a small snippet of the OP's StackExchange post and improve it so it is more readable according to your criteria?
Using `S` for `Lazy` is very misleading! `S` usually stands for strict. I suspect that was a refactoring that wasn't seen through properly. But the question continues ... what would you have written instead? If you could give several examples that would be helpful to demonstrate your point.
Don't overuse typeclasses. OOP programmers often try and make a type class rather than a type. Image shouldn't be a typeclass, it should be a generic type: newtype Image p = Image (Array Coord p) Then rename GrayScalePixel and ColorPixel to PGM and PPM respectively, and have an extra method in your Pixel typeclass: class Pixel p where ... imageType :: p {- ignores -} -&gt; ImageType instance Pixel PGM where imageType _ = PGM ... Now I suspect you can implement the `Image` functions merely once, using `ScopedTypeVariables` as needed to get at the required type. 
This looks like very nice code. Your use of classes is very logical, and the code looks nice. In response to you specific points: 1. **Abusing of the type system** I don't think you are really abusing the type system with you classes, as mentioned before. However, in terms of improving the code duplication, you could make a parametric `Image` type, like so: newtype Image p = Image (Array Coord p) deriving Show instance Pixel p =&gt; Image (Image p) EDIT 2: As kamatsu mentioned, using this parametric type makes the `Image` type class unnecessary. 2. **Parsing multiple values in a function** Instead of directly passing `ByteString`s in and out of your functions, you could use a preexisting parsing library like `attoparsec`, which would have a number of advantages. First of all, you don't have to manage all the state yourself. Second of all, you get nice prewritten combinators like `many` which do a lot of work for you. Thirdly, `attoparsec` is nicely optimized, and would also allow you to incrementally parse image files. 3. **Performance** As mentioned above, using `attoparsec` for parsing might help. Much more importantly, however, you should replace `ByteString`s in you encoding functions with `Builder`s. This has a number of huge performance benifits. First of all, the actual data is only made once. With your current approach, you encode all the pixels, then copy all that data into a single string, then copy that again to merge it with the headers. Using a `Builder`, none of this copying will happen. Additionally, it has many high performance encoders already written for you, like `intDec`, which will encode an integer for you much faster than combining `show` and `pack`. Lastly, since it outputs data incrementally using lazy `ByteString`s, you never have to hold the encoding in memory - you can just write it out directly. 4. **Branching in mixed IO/Maybe functions** This issue is solved by using `attoparsec` instead of building your own parser. EDIT: formatting
Thank you! I'll look into builders. As for using attoparsec, since I'm doing that for learning purpose I prefer to start with writing my own parser. Once I'm done refactoring I'll look into attoparsec. If you do not think I abused the type system, could you tell me what has to be done for bin2asc to support PPM and PGM? Currently GHC forces me do add a type signature to "decode", even though "convertImage" can operate on both image types.
Thank you! I'll read about scoped type variables
Not sure I understand what your mean 
What does [APL](https://en.wikipedia.org/wiki/APL) mean?
Just finished going through this. It was incredibly helpful, thank you for the pointer! 
Oh, I thought you meant me. Didn't notice the indentation of your message :-)
In which I also link to a mini-review for emerging languages in OSCON 2013. I hope it helps! :)
Let me know if there are any issues getting it to compile or run correctly. I've only tested this on Linux. (I imagine installing SDL is probably the biggest potential stumbling block.)
This could be argued both ways. My go to solution for when show gives sugared output is to use `gshow` and `:print`. The real solution would be to not use Show for output in GHCi. Rather, there should be a `showsPretty` method in the typeclass which defaults to `showsPrec`, but which can be changed to give more readable output. That should make everybody happy (except that you then can't derive the default show if you want to change showPretty).
Got it. Thanks 
You don't need a GADT to do this if you don't limit yourself to building the ADT using the raw constructors. Also, you don't need to pass `Exp`s as arguments to the constructors. Here's a simple example of what I mean: data ExpF a x = ReadAccount (Int -&gt; x) | WriteAccount Int x type Exp a = Free (ExpF a) readAccount :: Exp Int readAccount = liftF (ReadAccount id) writeAccount :: Int -&gt; Exp () writeAccount n = liftF (WriteAccount n ()) ... and so forth. You can then connect these together using `do` notation. example :: Exp () example = do n &lt;- readAccount writeAccount n See [this post on free monads](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) that I wrote which explains in greater detail common idioms for writing free monads. Also, since you are specifically asking about delimiting effects, you may also want to read [this post](http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html) about how to use free monads to isolate effects.
While extensible syntax would make this slightly prettier, I don't see this as messy. I have never used bytestrings and I could still tell at a glance what this code was doing. Of course, I would have to look up what B is to know its strictness and performance implications. It seems like we just value different things. You like longer names to have more detail at your fingertips whereas I prefer short names to make it easier to look past the details. 
Can you post some screenshots?
Here's the current test scene: http://jsnow.bootlegether.net/testscene.png It's kind of ugly (not too mention, a little too dark). It's meant to show that shadows, textures, reflections, lighting, and boolean operations on solids all work properly, and that rendering times are reasonable even if you have thousands of objects in the scene. Someday, perhaps I'll make a prettier demo. To give you an idea of what defining a scene is like, here's how I made the little red spheres that fill the background: lattice = let n = 10 :: Flt in bih [sphere (vec x y z) 0.2 | x &lt;- [(-n)..n], y &lt;- [(-n)..n], z &lt;- [(-n)..n]] The sphere constructor takes a vector and a radius as arguments, and "bih" is the constructor for an acceleration structure that takes a list of objects and creates a tree of bounding volumes, so that rays can be tested efficiently against the objects. All objects can be move, rotated, scaled, given textures, or used to build bigger composite objects.
The continuation parameter is the `a`. ReadAccount already has it. Maybe OnTimer needs it too. Shouldn't OnTimer go somewhere else, however?
The programming language.
Prisms (from the lens library) *are* first class, abstract, composable, pattern matches and I think a lot could be gained from these discussions if they were couched in these terms. For example, let-pattern Foo(x) = Nand(_, x) would be a "partial-prism", which turns out to be exactly the same thing as a "partial-lens" a.k.a. an affine traversal. There is probably a lot of merit in allowing affine traversals in pattern matches, while requiring them to be used as "setters" (rather than "reviewers") on the expression side. Edit: let-pattern Op(a,b) = (And(a,b) | Or(a,b)) is also a partial-prism a.k.a an affine traversal. This can be easily be seen as essentially the same as the `Foo` case above by noting that the data type `And (a,b) | Or (a,b) | ...` is isomorphic to the data type `Bar (Bool, a, b) | ...` and that under this isomorphism the let-pattern becomes let-pattern Op(a,b) = Bar(_,a,b) 
how do you compose prisms into something equivalent to an exhaustive case statement?
Something like this. The syntax is up for grabs, and `done` really needs to be an `IsomorphicToVoid` typeclass method. To check exhaustiveness you really need all your data fields to be fully polymorphic. But I think it's a good start! Prelude Control.Lens Data.Void&gt; let done = either absurd absurd Prelude Control.Lens Data.Void&gt; :t done done :: Either Void Void -&gt; c Prelude Control.Lens Data.Void&gt; let match p f g = either f g . p Left Prelude Control.Lens Data.Void&gt; let f = match _Left ("Left " ++) (match _Right ("Right " ++) done) Prelude Control.Lens Data.Void&gt; f (Left "foo") "Left foo" Prelude Control.Lens Data.Void&gt; f (Right "bar") "Right bar" 
Are release notes being crafted on this? I've heard a great deal of news on what's to be included in 7.8, and I'd love to see all that accumulated in a single place. Things like: mio-based scheduler, TypeHoles, iOS backend... Very exciting! :)
I noticed an edit in their website in the last month, but no indication that it shut down.
I'm incredibly excited, even though this made me upset :( "On OS X 10.7 and beyond, with default build settings, the runtime system currently suffers from a fairly large (approx. 30%) performance regression in the parallel garbage collector when using -threaded."
Only marginally related, but it seems to fit: Are there any real world open-source projects that make some good use of FRP in general, or reactive-banana in particular? I've been interested in using FRP for a while, but I just couldn't really wrap my head around it and all the tutorials I looked at didn't really help me (there's not exactly a lot of them though). I think actually reading some code and see how it is applied might help me understand how it is used in the real world.
Just to clarify, this is only the 7.8 *branch*, and not yet a *release*. In this branch only bugs will be fixed, other than that it's a feature-complete RC for the release that will follow in a couple of weeks.
In case anyone is interested: typo in the release notes: "these no ceased to really help"
&gt; 30% was the average overall slowdown, over the whole set of tests Ohh. Well then I take everything back and assert the opposite. :-)
Does [threepenny-gui][1] fit your bill? [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
Can someone highlight the highlights in the changelog? I can't really tell why this is a major version bump.
&gt; Can someone highlight the highlights in the changelog? I can't really tell why this is a major version bump. "Replaced more of our home-grown types with standard ones" is the most obvious pervasive change that I can see.
Changes that deserve major version status in my mind: * *Simplified Each, Ixed, and Contains. They are no longer indexed.* This will break some user instances, I believe. * *Simplified Cons and Snoc. Now they must be a Prism.* This will break user instances and user code. * *Simplified Contains. This necessitated losing many instancs of Contains...* These might include user instances and it breaks any code which relies on those instances. * *Replaced more of our home-grown types with standard ones.* Tame `lens` users won't notice this except that the error messages will change, but adventurous users might have to rewrite some code. It also offers an opportunity for people to import fewer packages and maintain lens compatibility. * *Control.Lens.Combinators + Control.Lens.Operators* Not a breaking change, but a rather dramatic change in the API. This is probably the weakest argument on this list. * *We're exiling Control.Lens.Zipper to a separate package.* Thus requires everyone using that code to grab an extra package. * *Switched to exceptions from MonadCatchIO-transformers.* This can be painful if your use of `MonadCatchIO` lead you to a lot of `lifted-base` use. * *Changed unwrapping and wrapping to have the same constructor-oriented order as a Prism and renamed them t _Wrapping and _Unwrapping respectively.* Thus breaking every use of `Wrapped`. In general it seems like a lot of simplification occurred, though not as much as had been discussed in the GitHub issues, I believe.
You may find some projects built with Reactive Extensions / Elm / ReactiveCocoa on Github. Quick [search](https://github.com/search?l=Elm&amp;q=elm&amp;ref=simplesearch&amp;type=Repositories) returns some [frontend usage](https://github.com/AshFurrow/FunctionalReactivePixels), [mobile app](https://github.com/ReactiveCocoa/AnyWall), [FSM implementation](https://github.com/erikprice/StateSignals) or [http server](https://github.com/jfromaniello/Anna).
Bryan O'Sullivan will be testing this with his infrastructure once the binaries are out (he did it before back in ~Nov or so, and the major problems that were legitimate bugs were fixed.)
The first digit changes when it breaks my other libraries. That enables me to stay sane. If I had to actually bump second digits on 80+ libraries every time I shipped any PVP breaking change, I'd block for 2 days on each update, like I used to! That said, lens 4 has been about a year in the making. There are tons of little type signatures that got more generic and little performance tweaks. The main reason for shipping now is that it is able to build on GHC HEAD, unlike lens 3.10.
Very nice paper, thanks for posting! Working through it right now. It's strange that they claim that the extensionally reduced version findBool p = p True is somehow more lazy than the expanded findBool p = if p True then True else False Surely that isn't the case.
Fixed in the repo. Thanks.
&gt; Simplified Each, Ixed, and Contains. They are no longer indexed. This will break some user instances, I believe. That actually breaks all user instances. Same with Cons, Snoc, and Contains.
Does anyone know why? I don't run performance critical code on my Mac, but I'd be very curious to know what's behind it.
Huh. What sort of beast satisfies `(Contravariant f, Functor f)` ? Besides `Constant` / `Const` that is. (And why do both of these exist?, both copyright Ross Paterson?)
Note, that there is `Proxy` from @ekmett's tagged which clearly states that.
Yes, this has to do with thread-local state performance in OSX. Full details: https://ghc.haskell.org/trac/ghc/ticket/7602#comment:33 TL;DR: The current __thread workaround can be optimized by future clang releases on OS X if they choose, and GHC folks can investigate the pthread implementation further to restore the performance on OS X 10.9 (Mavericks) in a future version. 
I remember looking at the examples, but I think only the CRUD example used the reactive-banana bindings? I could be wrong though. As tsahyt I can't wrap my head around it from what I saw.
That's exactly the trick. The only way that `f` can instantiate both of those is if its type parameter is phantom. Lens uses a kind of ad hoc subtyping system where constraints like that are layered atop one another as your "lens-like" type gets more and more constrained and specific. So, to wit, once you get to `(Contravariant f, Functor f) =&gt; f` you're dealing with a `Getter`-alike.
The release notes say that prefetch is only on the llvm backend. That's out of date. Should I write a wee patch to update that to reflect that it's supported on ncg (though a no op on not x86 architectures?)
Note that threepenny-gui doesn't bind the reactive-banana API at all, but has something very similar in the [`Reactive.Threepenny`][1] module. The [CurrencyConverter][4] may be a simpler example. Much like Haskell itself, FRP may take some effort to learn. There are some resources on the web, for instance my [tutorial slides][2] or a [stackoverflow question][3]. [1]: http://hackage.haskell.org/package/threepenny-gui-0.4.0.2/docs/Reactive-Threepenny.html [2]: http://apfelmus.nfshost.com/blog/2012/07/15-frp-tutorial-slides.html [3]: http://stackoverflow.com/q/9210412/403805 [4]: https://github.com/HeinrichApfelmus/threepenny-gui/blob/master/samples/CurrencyConverter.hs
woops, misread them. notes are correct :) 
This is a really big one. It makes it possible to write `Getter`s and `Setter`s with only a `transformers` dependency for `Getter`s and no dependency at all for `Setter`s. Now I just wish that `lens-family`/`lens-family-core` would also switch to using these types as well so that these `Getter`s and `Setter`s could be used with both packages.
Regarding your question about monad transformers, the easy way to optimize them is to replace `WriterT` with `StateT` and try to avoid `EitherT`.
if you build your GHC 7.8 using a real GCC, theres no perf issue on OSX. The issue lies with building the GHC RTS using clang on OS X. (austin, please correct me if i'm wrong). 
What is the expected time from here to release? 
Waiting for GHC 7.8 is making me more sympathetic to the people that are still waiting for Perl 6. Can't. Wait. to play around with it.
If that's the case, have the other issues with clang builds been cleared up? I've been happily using a brewed gcc with ghc since moving up to Mavericks, but if I can take a step out of my build process (even with a GC penalty), that'd be good progress.
I got the proposal in this morning. Here's another link: http://cppcabrera.blogspot.com/2014/01/my-oscon-2014-proposal-case-for-haskell.html . Posting here to avoid spamming and since it's directly related. In this follow up blog post, I share the process, and the video I submitted to OSCON detailing what I aim to talk about. Thoughts welcome. I want to make this an amazing talk about Haskell! :)
Unfortunately, my case seems to be even tougher than that. I have replaced the type type EvalP = RWST R1 W1 () (RWST R2 () S2 IO) by newtype RWT r w m a = RWT { runRWT :: r -&gt; IORef w -&gt; m a } type EvalP = RWT (R1,R2) (W1,W2) IO flattening the transformer stack, getting rid of state and simulating `Writer` by a `Reader` and an `IORef`. This has improved performance by a factor of 6, but it's still not enough. :( Profiling indicates that most of the time is still spent in calculating closures for the `(&gt;&gt;=)` combinator, and GHC Core contains expressions like let mx63 = (case ... of { .. -&gt; (\_ _ -&gt; av36) }) in mx63 xv12 xy37 -- av36 :: State# RealWorld -&gt; (.., State# RealWorld) that could be simplified by even more inlining. Essentially, I want an inner loop go :: A -&gt; (R1,R2) -&gt; IORef (W1,W2) -&gt; State# RealWord -&gt; (B, State# RealWorld) where all binds and `return` and `liftIO` have been fused together, but I don't know how to get there.
Have you tried using normal state instead of an `IORef`? Also, have you played around with `INLINABLE` or `INLINE` pragmas? Alternatively, is all the core logic contained within a single module? If you can link me to the relevant source code location on your repository I can give you more detailed feedback.
Am I right in think that this release allows dynamic linking in ghci on OS X? If so I can use the llvm package from ghci now :) 