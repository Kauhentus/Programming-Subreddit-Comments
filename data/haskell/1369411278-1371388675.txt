Better is of course subjective. I believe that BSD3 is the most used (in terms of package counts and probably package use) on Hackage. Many companies try to avoid using and contributing to (L)GPL licensed software, and LGPL may conflict with GHC's aggressive inlining and static linking.
If you have a large tower of functions that depend on a single function that fail, it sounds like an anti-pattern. I usually decompose my code into a series of stages, some of which may fail and some of which may not. I only make them agree on a common monad at the very last moment when I combine them in the final overarching code block. Here's a trivial example of what I'm talking about: foo :: Either String Double bar :: Either String Int bar = fmap truncate foo baz :: Either String String baz = fmap show bar That would be the anti-pattern I'm describing. I'd refactor it as: foo :: Either String Double bar :: Double -&gt; String bar = show . truncate Then I combine them at the last moment using: fmap bar foo Structuring it this way means that `bar` can be completely oblivious of any changes to `foo` (like adding or removing the `Either` monad or changing it to a more sophisticated monad transformer). Admittedly that's a very simplistic example, but I have more complex code bases that still follow this same principle. P.S. I'm not downvoting you! I don't know who is doing that.
The Alpha Heavy guys are great. Deeply knowledgeable about both markets and software stuff. Steve's blog http://www.steveseverance.com/ Nathan's blog http://breaks.for.alienz.org/ Alpha Heavy's Github https://github.com/alphaHeavy 
&gt; But you do have to write your code in that way because there is nothing useful that you can do with the Nothing value so every function that is dependent on the original function must also return a Maybe. This is true independent of whether or not one prefers to use the term "impure" to describe a Maybe --- the fact remains that you can't escape from the Maybe monad in this case because there is nothing meaningful you can do with the Nothing value so you have to keep propagating it. This is simply not true. That is the whole point of fmap and bind... You will have a top level function that uses fmap &amp; bind to chain functions together and only the ones that return Maybe values need to know about the maybe values... The rest can be just normal functions... In fact Haskell gives you these incredibly powerful tools to deal exactly with this. Also there is the communication aspect. If a developer sees an Either return type and is *forced* to handle all scenarios they will be more conscience to use the library correctly. If however there are hidden exceptions and there is no way to tell what triggers them it is really hard to know that you are using the library directly. I'd rather having things communicated in types than be forced to look at potentially incorrect or outdated documentation.
The problem is exceptions are black box. You can't see what exceptions a function is going to throw, or why. Error types are better because they are encoded in the type system. This leads to more correct code from the start. You do not have to rely on potentially outdated or incorrect documentation to figure out how to use a function correctly. Also the Maybe and Either types do not extend through all of your code if written properly. In fact it should only affect the very top layers. The rest of the code can just deal with simple types. This is the entire point of fmap and bind...
There's an easy way to check: -ddump-simpl and see if the tries have been floated up to the top level.
Oh if you insist. What is Quickfix? Why are more people using OCaml? How are you using it? 
I will have to give that a try. Thanks. I didn't know about that option.
Looking at their employment page I see the positions trader and programmer. Both positions seem to involve writing code. Any chance you could clarify what the difference between the two would be?
You can also use `lens-aeson` and make a lot of code for manipulating aeson objects just go away.
Definitely! `lens-aeson` is great! I use `lens-aeson` when I'm in the mode of "picking through a JSON object" and regular `(ToJSON, FromJSON)` when I'm thinking of JSON as a serialization method.
I am essentially asking about cases like this: f :: Parser String f = anyInTrie t where t :: Data.Trie.Trie () t = Data.Trie.insert "foo" () $ Data.Trie.insert "bar" () $ Data.Trie.empty (slightly simplified as usually the Trie values have more than two entries and might be generated by some list to Trie conversion function). If f is used all over the place will t be evaluated every time f is used or just once (as it is essentially constant)?
If `f` doesn't have any parameters and it has a monomorphic type, then it will be evaluated only once. In effect `f` becomes a global constant that is lazily evaluated.
&gt;I explicitly stated that I think it's okay to use error in a case where you know that the value will never be evaluated (because in a more ideal setting, you'd have some proof of that which would remove the obligation to provide a value there). I have read and re-read your post (Edit: specifically, the parent to my previous reply, let me know if you were talking about something else) and don't see that being made clear anyways. &gt; The case of "if this value is required then the algorithm that this library is based on is actually incorrect", is not what anyone is complaining about. Then given that I have been saying over and over again that this case is what I have been referring to, I have absolutely no idea why people have been generating so many complaints about it. :-) If people were really okay with this, this thread should have had a depth of 2 or 3 rather than ten.
&gt; If you have a large tower of functions that depend on a single function that fail, it sounds like an anti-pattern. I usually decompose my code into a series of stages, some of which may fail and some of which may not. I only make them agree on a common monad at the very last moment when I combine them in the final overarching code block. Perhaps it would be helpful to describe a specific situation to make the kind of situation I am envisioning more concrete. In a project that I am working on, I have a case where I need to merge two trees. The two trees *must* match the precondition that they agree where they overlap, or else the merge is impossible. Although this sounds like a case where we should expect and account for failure, there is one more key piece of information: these trees will always, always, always have come from the same generator, because for any run of the program there will be exactly one generator and all trees will be coming from it; you would have to actively go out of your way to generate incompatible trees and then intentionally mix them together to run into a problem. Given this, while it is true that I could have the function return an `Either` or a `Maybe`, it will in practice *never* be the case that the functions are given the wrong input unless there is a bug in the program --- which, again, requires almost active effort to have happen in practice as only a single source of trees is ever available so all the trees that the program will ever handle during a run will be guaranteed to be compatible. Thus, adding the overhead of an `Either` or a `Maybe` is a cure that is worse than the disease: if there is a bug somewhere or memory corruption has occurred, there is nothing useful that can be done in pure code so we have to jump down to I/O, so it is far more convenient to use exceptions which do this for us without requiring us to manually propagate anything. Using, say, `Maybe` in this case would be a waste because in practice 100% of the time (minus memory corruption) it would follow the `Just` case and hence be pointless. So in short, my ultimate point there are times when, pragmatically, exceptions make more sense than `Either` or `Maybe`, as the presence of a failure means that something is horrible broken about the code in an unknown and hence irrecoverable way, and there is no point in cluttering one's code with `Either` and `Maybe` in this very specific subset of failures because they do not offer anything useful over exceptions except, at most, that provide a warning to the user that he or she needs to read the documentation to learn about the failure cases. To be clear, throwing exceptions in pure code should be... well, the exception, rather than he norm. In pure code I don't think that one should normally be throwing exceptions; `Either` or `Maybe` is more appropriate. I just think that the case where a function is partial but where a working program will never run into the place where it doesn't apply is better handled with exceptions rather than cluttering the code with `Either` and `Maybe` when they offer no benefit in practice. &gt; P.S. I'm not downvoting you! I don't know who is doing that. Yes, one of the pleasures of discussing these things with you is that even if we disagree I know that you are responding to me respectfully, which has not always been the case here.
I know, it's just that that site doesn't explain what it does.
&gt; Given this, while it is true that I could have the function return an Either or a Maybe, it will in practice never be the case that the functions are given the wrong input unless there is a bug in the program So I have mixed feelings about this. I know that real Haskell code imposes situations like these where you know that a partial function is safe even when the types do not prove the safety. I encounter these in my own code. However, even in situations like these I tend to play it cautious and use `Maybe`/`Either`, even if I know the partial equivalent is safe. The reason why is that the invariant(s) in the code base that make that particular scenario safe could be distributed throughout the code base. If somebody other than me were to pick up my code later and modify it, they might inadvertently break these invariants without realizing that other partial functions rely on them. So I prefer to do the total thing as a precaution. At the same time, I'm not a complete stickler for this. On occasion if the invariant that makes a partial function safe is located within the same code block, I'd be willing to make an exception because very little context is necessary to understand the interrelation between the two. It really boils down to whether I think the overhead of keeping track of these invariants will get in the way of refactoring or maintaining code later on.
My guess is more people use OCaml because Jane Street uses OCaml, and they've been around longer than any of the Haskell shops I know of.
It's an implementation of the [FIX protocol](http://en.wikipedia.org/wiki/Financial_Information_eXchange)... a kitchen sink of financial messaging. Many brokers and exchanges support some FIX flavor, so having a decent FIX engine around can ease your integration pain (or at least substitute it with FIX integration pain).
Oh, but f is not a constant, it is a Parsec Parser which is a monadic function parsing a particular bit of input, so they do have parameters, they are just abstracted away in the Parser type.
It is a constant in the sense that it is not a function, and that its body needs to only be evaluated once. It is a constant of type `Parser String`. Here is a simple example in Ghci showing that f is only evaluated once λ&gt; import Text.Parsec λ&gt; import Text.Parsec.String λ&gt; :set +s λ&gt; let f :: Parser String; f = string x where x = show $ length [1..100000000] (0.00 secs, 524820 bytes) λ&gt; parseTest f "a" parse error at (line 1, column 1): unexpected "a" expecting "100000000" (2.70 secs, 4000445468 bytes) λ&gt; parseTest f "a" parse error at (line 1, column 1): unexpected "a" expecting "100000000" (0.00 secs, 524392 bytes) Here `f` and `x` are only evaluated once.
I know about rust already. I like it and follow it's progress but **currently** it is lacking in some features (such as higher-kindred and higher-rank polymorphism) and you can not write a functions which are enforced to be referentially transparent in the type system. if I recall correctly at one point rust did support pure functions but it got complicated so this was dropped, at least in a rust like F# all variables are immutable by default. I follow Rusts progress a lot and for me as of current the language is still in flux both in terms of syntax and features. It's still too early days for me. I also prefer Haskell's syntax but don't get me wrong I like Rust and i think it is quite awesome already, I'd rather use Rust over Go, D, or C++ any day. 
Fair enough. I think that the one thing that we can both agree on is that we should all be programming with dependent types to solve this problem once and for all. :-)
Is there some where I can find more information about this project? like does it apply all the various ways you can manually annotate strictness in GHC Haskell? and information like the current status, gotchas etc. EDIT: I kind of figured out how it works by reading [Monad.Reader 12](http://www.haskell.org/wikiupload/f/f0/TMR-Issue12.pdf)
Idris does look awesome but it seems like there still quite a bit of work to do there and lack of libraries (and community?) 
That's very cool, I didn't know GHC supported plugin extensions, is this paper what [strict-ghc-plugin](https://github.com/thoughtpolice/strict-ghc-plugin) is based on? 
I'm not suggesting to completely get rid of lazy evaluation, just swapping around the defaults. Swap strictness annotations into lazy annotations and every expression being strict by default. 
I do, I've mentioned something about Disciple [here](http://www.reddit.com/r/haskell/comments/1ew6js/ghc_modding_qs/ca5pl8z).
You could just generate LLVM IR from Haskell, rather than use any of Clang's machinery.
I know what the docs say, but I wouldn't rely on file IO through lazy bytestrings to have any particular performance characteristic. If you only need the first few bytes of a file just read them. That being said, have you tried using [splitAt](http://hackage.haskell.org/packages/archive/bytestring/0.9.2.0/doc/html/Data-ByteString.html#v:splitAt)?
As I have written, I did already settle on SplitAt, but I would rather use Data.Binary.Get if at all possible. I do need the rest of the lazy bytestring, not just the header.
&gt; As a rule GHC evaluates every function call; it doesn't do common subexpression elimination. actually, sometimes GHC does just that... I've had a few cases where an unexpected CSE optimization caused space-leaks...
Agreed. :)
`runGetIncremental` takes the input as a strict `ByteString`. Just feed in a reasonably sized chunk at a time.
Like so, this is equivalent to `runGetOrFail`: import Data.Binary import Data.Binary.Get import qualified Data.ByteString as Strict import qualified Data.ByteString.Lazy as Lazy parse :: [Strict.ByteString] -&gt; Decoder a -&gt; (a, Lazy.ByteString) parse _ (Fail _ _ str) = error str parse (x:xs) (Partial f) = parse xs (f (Just x)) parse xs (Partial f) = parse xs (f Nothing) parse xs (Done x _ val) = (val, Lazy.fromChunks (x:xs)) main :: IO () main = do src &lt;- Lazy.readFile "/dev/random" let (val, src') = parse (Lazy.toChunks src) (runGetIncremental getWord32be) print val
FWIW my forecast as an observer of Rust myself: - *Higher-kinded polymorphism*: I'd give good odds that this will be added at some point, unless there's some unexpected technical obstacle. Nobody is really opposed to it, somebody just needs to do the work. And given that Rust's target audience is frustrated C++ programmers, a language which *also* has the feature, it won't be just Haskellers asking for it. - *Higher-rank polymorphism*: 99.9% sure this will never happen. It's one of the defining differences between Rust and Haskell, along with strict evaluation, explicit memory management, and (for now) referential opacity. The problem is that for higher-rank polymorphism you need all instances of a polymorphic type to have the same shape regardless of their type arguments (in other words, to be binary compatible with each other), and if everything is unboxed that's impossible. (The 0.1% chance is in case someone comes up with something clever involving unsized types.) - *Referential transparency*: Not likely and not soon. As you say they used to have it, and it didn't work out. Coming up with a design that carries its weight is difficult, especially given the target audience. *Maybe* in the far future, like Rust 2.0.
A glance at http://hackage.haskell.org/packages/archive/binary/0.7.1.0/doc/html/Data-Binary-Get.html shows 2 ways (besides the strict `Bytestring` way with `runGetIncremental`): One is: [runGetOrFail](http://hackage.haskell.org/packages/archive/binary/0.7.1.0/doc/html/Data-Binary-Get.html#v:runGetOrFail) which gives you the remaining ByteString. Another is to use [getRemainingLazyByteString](http://hackage.haskell.org/packages/archive/binary/0.7.1.0/doc/html/Data-Binary-Get.html#v:getRemainingLazyByteString). However, its documentation mentions that it forces generating the remaining lazy ByteString into memory.
No incentive might take it too far. The release-as-open-source incentive always exists in the form of getting contributions to your code from strangers. Perhaps the competitive advantage lies higher up with what you do with the code.
&gt; Higher-kinded polymorphism: I'd give good odds that this will be added at some point, unless there's some unexpected technical obstacle. Nobody is really opposed to it, somebody just needs to do the work. And given that Rust's target audience is frustrated C++ programmers, a language which also has the feature, it won't be just Haskellers asking for it. I've been using C++ commercially for about 7 years and longer before then. You can emulate higher-kinded polymorphism in C++ to a certain extent via template-template parameters but they are quite limited and are rarely used. I've even met C++ programmers who have been programming much longer than me who are surprised to discover that this feature even exists. &gt; Higher-rank polymorphism: 99.9% sure this will never happen... I believe you could have some form of higher-rank polymorphism in a language where all types are un-boxed and implements polymorphic types via monomorphization since I can emulate higher-ranks in C++ e.g.: struct fn { template &lt; typename Tp &gt; void operator()(const Tp&amp; x) const {} } template &lt; typename PolyFun &gt; void my_rank2_fn(PolyFun poly_fn) { poly_fn(1); poly_fn("foo"); poly_fn(1.0f); } my_rank2_fn(fn()); I use higher-rank polymorphism in a shader-based software renderer I wrote, unfortunately polymorphic lambda expressions are not currently supported in C++ which would have made my code more elegant. Anyways I could probably live without those features using Rust (they have a proper macro system and syntax extensions) probably most of the time but as I was saying earlier to me right now Rust still isn't quite stable enough in terms of language syntax, it seems like the Rust guys are still refining those aspects. 
Minor typo: "how can you possibly get any work done done?"
My original line of thought was that if I went down this road I would have to do a lot more work where as doing AST &lt;-&gt; AST transformation I can get a lot of features for free out the box like all the optimization passes that Clang would apply before generating code for LLVM, I maybe able to map most or all the runtime just in terms of C++11's run-time, maybe even the possibility of easier inter-op between C/C++ and Haskell like in Chicken Scheme. I could be completely wrong though and I'm talking utter non-sense. 
Are those changes based on GHC plugin extensions like ghc-strict-plugin? 
I don't really know. You would have to ask him.
Did you consider using something like the conduit or pipes libraries?
The argument isn't any harder to make than the argument that there is value in using a programming language with a good type system. It's not just a question of how far into runtime you find the error. Once an error only appears at runtime, it is very difficult to guarantee in general that it will be caught by the programmer and not at a later more expensive stage - QA, or even in a deployed application. Sure, you can write specific trivial applications where the programmer will catch the error, and you can catch many more errors by providing good unit tests. You can do all of that in any dynamic language too, for your entire program. But in Haskell we value principled methods that provide strong guarantees. We do need a way to type string literals as Text instead of String, and right now OverloadedStrings is the only game in town. But IMHO that should not be seen as a license to write OverloadedString instances for things that throw errors at runtime.
Thanks for writing this post with sensitivity for those of us who are not fluent in Agda. I really enjoyed it!
Thanks. It seems that `runGetOrFail` (as suggested by Peaker) also does the job. I do wonder why `getRemainingLazyByteString` must force it to memory :( It would be more more convenient to be able to stay in the monad. 
I did, but I have zero experience with streaming libraries. Do you think I should use it for parsing, or what exactly? I thought parsing in `pipes` is not ready yet.
Not necessarily a tall building. I've been in elevators like that before. They need to fix the elevator. But Greg's really nice pitch about FP seemed to make the time fly.
Aha. Thanks!
Absolutely, yes. If your calculation might fail, then the return type should express that. Then the compiler will automatically check, at compile time, that you didn't forget to handle that case anywhere it is relevant. You'd be surprised how much of your code actually doesn't need that though, so you get the additional value that the return types express to the reader where problems might happen.
Well, if you want to continue to do binary-style parsing, you do stay in the monad. Just continue with further parsing after reading the header. But here you want to "apply a function to the remaining payload". Doesn't that mean that you *don't* want to stay in the monad? I actually think that the types forcing you to finish your monadic calculation at that point are doing you a favor. You are taking control of the input stream away from the monad. How can you be certain that what your function does will interact well with the internal stream processing of the monad? If you need to do more binary-style parsing later, you can always restart a monadic calculation. You are in control of what lazy (or strict) bytestring gets fed into that after your manual function is finished.
I don't work for Jane Street, and, therefore, have no authoritative knowledge, but I would suspect that Jane Street and many of the other OCaml shops use it due to the relative ease of working in a strict language and the maturity of the OCaml ecosystem (at least at the time when Jane Street made their selection). I suspect OCaml's strict nature is more appealing in the context of trading systems (as opposed to financial modeling, for example). Additionally, I suspect at the time I estimate Jane Street made their selection, it would have been difficult, given the maturity of the Haskell development ecosystem, to select Haskell over OCaml. Would be interesting to hear from someone who really knows (Jane Street probably says somewhere; I haven't checked).
The probability of taking outside contributions into a trading system is very small. The risk often outweighs any potential benefits. Most of the people that you would want to contribute to a code base cannot for legal reasons. Also most firms view their technology as a key asset, not something to be given away. While I might disagree on what is important with others in the automated trading world there is a high technological barrier to entry. It can take years to build out the basic technology needed to start developing strategies. While we don't mind releasing pieces of non-core technology we would probably never release any of our core trading technology, research tools, or other core technology. Even if we did most people would still have to buy market data to use it. We use only real data feeds e.g. ITCH.While it would be interesting to see open source being more disruptive in the finance world I don't see much actual trading technology being released.
&gt; How can you be certain that what your function does will interact well with the internal stream processing of the monad? Assuming that `getRemainingLazyByteString` would do what I expect it to do (get the rest of the unconsumed input lazily), then all would be well. The reason I would like to stay in the monad is much clearer code and less plumbing. Compare: Staying in the monad: flip BinG.runGet input $ \do a &lt;- BinG.getWord64be b &lt;- BinG.getWord64be rest &lt;- BinG.getRemainingLazyByteString return $ doSomething a b rest vs using `runGetOrFail`: let Right (rest, _, (a, b)) = BinG.runGetOrFail (liftA2 (,) BinG.getWord64be BinG.getWord64be) input in doSomething a b rest I am using the second version only because the first one blows up the memory. I find the first one preferable because it is easier to write down, has a better flow, and I do not have to bother looking up how exactly runGetOrFail returns the result etc.
Can't wait for Tekmo to see this. :D
This is a very old cabal-dev bug, and a lot has changed in cabal-dev since then. But someone reported something similar today on #haskell after setting "`shared: yes`" in `.cabal/config`. Does GHC do something different with the package database when compiling with shared enabled? Is there some other path that needs to be set in cabal-dev's local copy of the cabal file in order for this to work?
The vast majority of optimisations LLVM does are on the IR level.
&gt; this library makes the rather arbitrary assumption that all text content of an element precedes all child elements Does it? You can evaluate code on School of Haskell; you can also change the code before evaluating. Also, this is not a library; it is a short tutorial (and the "slides" for a presentation) about unfolding and applicative combinators. Anyone who desires to create a library out of these ideas is free to do so.
Jane Street uses OCaml since Minsky used it in grad school. And OCaml is much better than the excel it replaced. But I don't believe its a superior approach. JS has had to build their own core library. With a few notable exceptions we have not had to do that.
audio problems? 
You are right that it isn't bad. It is not bad at all. But it is kind of longer winded and does not flow as well, in my opinion. Up to personal taste, I guess. That however is minor. The other issue is that I needed more time to figure out how to do it (study the package in detail rather than just using runGet) and did not expect getRemainingLastByteString to have this problem. I will probably ask the author about it. Anyway, thanks for the discussion and your suggestions.
So, I think you're misunderstanding how parametric polymorphism works. If I have a function: f :: a -&gt; a Then there is only _one_ valid, total and terminating implementation of that function: f :: a -&gt; a f x = x When you use a type variable like `a` in a type signature, you're saying you can define a value of that type _regardless_ of what the type used for `a` is. That means, if you try and do this: f :: a -&gt; a f x = x + 1 It will fail, because I could substitute `a` to, for example, `Bool`, and this would be an obviously type incorrect program because addition is not well-defined on booleans. If we place a _constraint_ on the type variable `a`, we can say that we can substitute `a` for any type _that meets some conditions_. More specifically, the condition that the `Num` type class must have an instance for that type `a`. foo :: (Num a) =&gt; a -&gt; a foo a = a + 1 -- this is valid Note that more specific type signatures are also valid: foo :: Int -&gt; Int foo a = a + 1 While the type you provide does not have to be the most general type, it cannot be more general than the type that is inferred by the type checker (except in the case of GHC, where let bindings are not generalised, but let's ignore that for now).
You need Num to perform mathematical operations, like + and ^. You need Eq to perform ==. And you need Enum to use [1..l]. The first type signature allows for only one possible implementation: pyths l = [[l]].
There's also: pyths l = [] pyths l = [[]] pyths l = [[],[]] pyths l = [[l],[l]] pyths l = [[l,l]] and infinitely more valid permutations of those.
I was thinking the same thing when I wrote my reply, he could probably explain the pipes version of it much better than I explained the conduit version.
Go ask on stack overflow. This is a news sub. Reported. Also you shortcutted this into a XY problem, you questions should be "what are type classes". Edit- Downvote all you want, but with the recent flood of newbie questions that clearly doesn't belong, it's time the mods have decided exactly how much they are going to tolerate.
Well, we're not talking about sets here but types. You could pull in a formal statement of parametricity to reason about your type system, but here's a less rigorous way of talking about it that still holds a little formal weight: To talk about the inhabitants of types, we shall have to talk about programs. For the sake of this discussion, we'll say that the programs in our universe of discourse are all total and terminating, and that we judge two programs to be equal if they are alpha-beta-eta equivalent. 1. The number of total, terminating inhabitants of a function type `a -&gt; b` is |`b`|^|`a`| (proof left as exercise). 2. The inhabitants of a type `forall a. t` are the (infinitely long) intersection of the inhabitants of the type `t[u/a]` for all `u` (this is the direct consequence of the typical typing rule for `forall`). The number of inhabitants of this type is therefore no larger than that of the smallest possible substitution for `u` on `t` (by "smallest possible" I mean the substitution for `u` that minimizes the size of `t`). 3. From (1), we can conclude that the number of inhabitants of `Empty -&gt; a` (where |`Empty`| = 0) is 1. 4. From (2) and (3) we can conclude that the number of inhabitants of `forall a. a -&gt; a` is at most 1. 5. `(\x-&gt;x)` is a valid inhabitant of `forall a. a -&gt; a` 6. From (4) and (5) we can conclude that identity is the only valid endomorphism on all types. If that proof doesn't satisfy you (and it shouldn't), then "Theorems for Free!" by Wadler gives an approachable introduction to parametricity.
Yes. Reynold's parametricity theorem. 
This is a consequence of Yoneda embedding. In the category of Haskell types, polymorphic functions are just natural transformations. I'm omitting some details, but you can use Yoneda embedding to construct a bijection between the set of polymorphic functions of type a-&gt;a to the set of functions from () to (), and there is clearly only one such function. [This post](http://blog.sigfpe.com/2006/11/yoneda-lemma.html) is a great introduction to the Yoneda lemma using Haskell.
Yeah, I have no idea what those are about. They go away a couple of minutes into the video, FWIW.
why should the lgpl conflict with static linking? it does not. don't spread fud, please.
Go search for "lgpl static linking" and "ghc haskell lgpl". If you're a lawyer and believe the internet is wrong or misguided, please write a blog post explaining your position. I'm sure the community would appreciate the clarification.
I've heard some stories from math people who give regular talks about various mathematical topics, where there is always a category theorist who sits at the back of the room offering remarks about how this or that result is trivial from a categorical perspective. Seeing as you can get by entirely with parametricity here, there's no need to pull in the categorical sledgehammer just to do a little type theory. Now I understand how they feel :P
Well I don't think these types of posts should be encouraged.
I have a type `Fmat`, which I am not exporting any constructors for or providing any additional information. Can you tell me what it means to have a list: [1..l] :: [Fmat] Can you tell me what it means to square an `Fmat` type without having been provided with a definition of multiplication, or any functions at all that work on `Fmat`? The first type signature suggests you can answer both of these questions.
This is the best answer. For the benefit of others, it was originally called the "abstraction theorem" and was presented here: http://www.mpi-sws.org/~dreyer/tor/papers/reynolds.pdf see also wadler on the girard-reynolds isomorphism: http://homepages.inf.ed.ac.uk/wadler/topics/parametricity.html
Is there any deeper reason for why `a` and `t` are swapped in `IFree`? If not, I suggest using this: data IFree (f :: (k -&gt; *) -&gt; k -&gt; *) v a t = Return (v a t) | Roll (f (IFree f v a) t) type (:-&gt;) (f :: k -&gt; *) g = forall a :: k. f a -&gt; g a class IFunctor (f :: (k -&gt; *) -&gt; k -&gt; *) where ifmap :: (g :-&gt; h) -&gt; (f g :-&gt; f h) class IMonad m (v :: * -&gt; k -&gt; *) | m -&gt; v where ireturn :: v a :-&gt; m a ibind :: (v a :-&gt; m b) -&gt; (m a :-&gt; m b) instance IFunctor f =&gt; IMonad (IFree f v) v where ireturn :: v a t -&gt; IFree f v a t ireturn = Return ibind :: (v a t -&gt; IFree f v b t) -&gt; IFree f v a t -&gt; IFree f v b t ibind f (Return x) = f x ibind f (Roll k) = Roll (ifmap (ibind f) k) &gt;-- wtf category am I working over here `IFunctor` is an endofunctor in the category of functors from `k` to `Hask` (call it `Hask`^`k` for short), so you're working in that category. `IMonad` looks like a [relative monad](http://www.cs.nott.ac.uk/~txa/publ/Relative_Monads.pdf) on `v`, so a monoid on the category of functors from `Hask` to `Hask`^(`k`). You can't just use composition of functors as the product of the monoid to type `ijoin` (i.e., it's not as simple as `m (m a) -&gt; m a`): `ibind id` won't typecheck, there's `v a` in the way! As the paper notes, you need to take the left Kan extension along v of the first argument: type (:*) (f :: k -&gt; *) g a = (f a, g a) data Lan f g a = forall b. Lan (f b :-&gt; a) :* g b -- I stole Lan from category-extras and adapted it to what I needed, -- the proof that this is the left Kan extension in this category is left as an exercise for the reader type (:=&gt;) (f :: * -&gt; k -&gt; *) g = forall a. f a :-&gt; g a -- morphism in the category of functors between Hask and Hask^k. type (f :. g) a = Lan v f (g a) -- (f :. g) = Compose (Lan v f) g And now you can write `ijoin` as: ijoin :: (m :. m) :=&gt; m -- ijoin :: (m :. m) a :-&gt; m a -- ijoin :: Lan v m (m a) :-&gt; m a -- ijoin :: (Compose (Lan v m) m) :-&gt; m a -- ijoin :: (exists b. (v b :-&gt; m a) :* m b) :-&gt; m a ijoin (Lan f m) = ibind f m Also, `ireturn` has the type you'd expect, `ireturn :: v :=&gt; m -- v a :-&gt; m a`. So `IFree` should be the free relative monad on `v`. That's very cool! It's also the first time I see a pratical application of relative monads outside of category theory. Good job! P.S.: I basically went on autopilot in this post, I haven't checked anything, if anyone notices any mistake in my reasoning, please report them.
The swap is so that the leaf type is the last type in `IFree`. I find that presentation cleaner: A monad should have its varying parameter last. I don't think the swap newtype noise is a big imposition. I like how you flipped the arguments in IFunctor to make it more obvious which category it is an endofunctor over, though. Overall your presentation is better I guess! I've read that paper up to the definition of relative monad but I couldn't find any compelling examples so never finished it. I had a feeling this would be a relative monad but didn't feel like working it out. That's cool. I like the idea of presenting it as a left kan extension. I would really like `exists` to be a keyword in GHC so that I could write types like that without CPS transforming them. I am not as big a fan of `:-&gt;`, `:=&gt;`, and `:*` ;)
/r/haskellquestions exists. Nobody uses it. (I agree with your broader point.)
Don't use this. I'm the maintainer. Frankly, I don't know why people like or even care about that plugin very much. It was never meant to be something serious at any point. It is really just a demonstration of how you can 'easily' write a plugin which does something fiddly with Core. The transformation it does happens to be easy. With syb, it's a very short amount of code, so it's rather well suited for educational purposes, but pretty much nothing else honestly. The notion of a 'strict haskell' which it introduces is not a very useful one. In particular, by rewriting `let` statements with `case`, it is blind and extremely dumb, and gives you very little power. Especially since annotations can't be attached to arbitrary binders, but are instead stored as metadata about the top-level identifier, in the interface file. This code is also quite obviously semantics breaking in tons of ways, because it will change source code like this: let x = error "boom" in ... term with x ... Into: case (error "boom") _x of __DEFAULT -&gt; ... term with _x ... Which is clearly not the same thing as _x goes to WHNF. In practice this will break a shitton of stuff, I can pretty much guarantee it. Large amounts of Haskell code relies on these idioms in subtle ways, so you're going to be rewriting user-facing code. This doesn't take care of unboxing things, strict arguments, or anything like that either. Inlining also means things you wanted lazy might become strict. That could be bad. Which sort of hints at the *real* problem. The source language has no means of accurately reflecting useful information about the context in which expressions or applications are lazy, or strict, other than a few basic things like bang patterns or `seq`. And those aren't typed, so you can even break your program (remember, `seq` can be evil. But also changing the reduction strategy blindly can clearly introduce non-termination.) So the plugin is pretty worthless without richer semantic information. Finally, IMO, source language laziness/strictness annotations - beyond simple cases like WHNF on patterns - are madness unless they are truly integrated with the type system and can be checked. Data types and arguments tend to have fairly simple semantics (although reboxing can occur.) But when you begin to talk about strictness or laziness annotations in function types for example, things will probably get weird. There were recent discussions about this on `ghc-devs`. OTOH, Disciple has already done some of this work. But nobody has done this work in the context of Haskell, and there's a lot of it (also keep in mind, we like type inference.) It seems like you're fundamentally aiming to change Haskell into a completely different language, and that's going to be a lot of work no matter how you cut it, I think, full-time job or not. But again, if you continue, I simply wouldn't suggest you use this. Anything you do use in any further capacity will not look like this after very long, I can guarantee it. It doesn't solve the problem you want to solve (regardless of whether I agree with approaches.) It solves no real problems anyone had, as far as I can tell.
Ah, yes, lists are involved, thanks for the clarification!
You don't need the v parameter. You can drop it everywhere, and then you get the "regular" free indexed monad. The only change to the xml part of the code is type XMLTemplate a t = IFree XMLBase (XMLVar a) t
I am writing a commercial game in Haskell (OpenGL). I have 15+ years of professional C/C++ game development experience. I used to think that lazy evaluation was a problem (as in "surely eager evaluation is faster?") but it turned out I was wrong. A case of "early/bad optimization". Lazy evaluation is highly optimized in Haskell (turned into eager evaluation when it makes sense). Space leaks I catch using the excellent GHC runtime performance tools. Space leaks doesn't happen often though. *Especially* compared with how often memory leaks/uninitialized pointers/... tends to happen in C++.
(You should take what I say with a bucket of salt; I'm no GHC hacker.) I think the runtime is very disconnected from the Haskell code you enter, and doing generalisations is difficult. Fundamentally, a Haskell program is not much different from a C program, only with "more stuff" in it. I do also think that visualising the execution to such a low level is pointless unless you want to become a GHC hacker. If you're doing it for performance reasons there are profiling tools which automate that for you.
The commentary has a very detailed description of elements of the rts: http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts See the section on heap objects in particular for a whole bunch of stuff: http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects Simon Marlow's answer here is also relevant: http://stackoverflow.com/a/3256825/371753 But this is all very technical. A sense of what this all _means_ can come from spj's two early books on the subject: https://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/index.htm https://research.microsoft.com/en-us/um/people/simonpj/Papers/pj-lester-book/ Perhaps less introductory than the two books, but covering the same ground in a more condensed fashion is the STG paper: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.3729 Oh, and by deep evil magic you can inspect (including visually) the state of the heap at runtime: http://hackage.haskell.org/package/vacuum
Honestly the best way is to read the ghc dev wiki, and the source code. Honestly unless you're writing super high performance systems where optimal register usage is a bottleneck, don't though :-). That said, disassembling the Haskell object code for libraries can be a fun read, and sometimes is illuminating wrt code quality. I should more strongly say it a different way: without a solid understanding of how to use high level techniques to write performant code in Haskell , such as fusion or the worker wrapper transform, which can sometimes have nX order improvements in performance (choose any integer n, we can manufacture an example ), an understanding of the runtime system will hinder rather than help you. I'm saying this as someone who's actually spending a lot of time staring at the run time system. Unless you're trying to write super fast numerical codes or something similar where small changes in a tight loop can have huge differences in performance, understanding the ghc rts won't help you. Phrased differently: ghc has a well designed runtime system etc. it's not perfect, but it's good enough that unless your code is unexpectedly slow (and should be a good algorithm / generating good code), you will not get anything out of learning the rts in your day to day engineering. If you do it, do it for fun/ learning. 
To answer your first implicit question (what is a good way to understand performance of Haskell programs) rather than your second explicit one (understanding the runtime), for programs compiled via GHC, the first thing you want to learn is to read Core. It is (approximately) a simple subset of Haskell to which GHC converts programs. The benefit of reading Core vs your source is that the final core has a reasonably straight forward interpretation in terms of the runtime: 1. case operations force computations in order to perform matching. 2. lets allocate memory for the thunk representing the right hand side. which may be what you are looking for. Of course the above, like any summary of a sophisticated system, is an approximation. A couple of tips about reading core (lots better available online): 1. For learning, lower inlining settings improve Core readability and correspondence to original code (at a cost to performance) 2. Core is printed with a lot of information you in particular might not need at the moment, learn to focus on the subset you need.
The STG paper in particular really helped me get what was going on inside Haskell.
UHC's source is probably easier to grok.
Thanks. 
The analogy between types in haskell and sets in category theory is pretty good, to be honest. functions are endomorphisms on types in the sence that functions take us from type to type and as we only have whole functions in haskell, If I read types, I actually read it as sets: f :: Int -&gt; Int -&gt; Int: f :: ℤ -&gt; ℤ -&gt; ℤ so now I know if I call f, i will get ∈ℤ 
What tools are you using? I've been using OpenGL/GLUT to do stuff like this http://www.youtube.com/watch?v=i_yvL-3HONw, but I'm trying to become effective with Ogre instead
Perhaps more abstract than you were seeking, but I recommend the "fast curry" papers by Simon Marlow and Simon Peyton Jones [http://research.microsoft.com/en-us/um/people/simonpj/papers/eval-apply/](http://research.microsoft.com/en-us/um/people/simonpj/papers/eval-apply/). This view of the STG machine can be visualised with MiniSTG [http://www.haskell.org/haskellwiki/Ministg](http://www.haskell.org/haskellwiki/Ministg). Here is an [example program trace](http://www.berniepope.id.au/html/ministg/step0.html).
Beware the full laziness transform! (This is the one I've had problems with, rather than CSE, though they do similar things).
I think of a haskell program as being a directed graph. Each edge represents a use of a value, and each node is a source of a value. A node can be: * An unevalulated thunk, i.e. an application of a function to values where no evaluation has taken place * A thunk that is in the process of being evaluated, so some of the other nodes it depends on have been evaulated to WHNF, some have not. * A WHNF value, meaning a basic primitive type or a specific constructor pointing to other nodes. WHNF is "Weak Head Normal Form". There is also a special `main` node whose evaluation determines the behavior of the program. The runtime has the responsibility of evaluating thunks to WHNF. Which thunks does it evaluate? Whichever ones `main` depends on.
&gt; I have no comparable intuition for reasoning about Haskell runtime behaviour. All I need is an animation or set of slides or something depicting the state of memory throughout the execution of a simple Haskell program. The keyword you need to look for is "graph reduction". Years ago I learned it via this self-paced tutorial: https://research.microsoft.com/en-us/um/people/simonpj/Papers/pj-lester-book/ As a next step, Bernie Pope's Ministg looks highly promising pedagogically as you get closer to what really happens in a GHC-compiled program. Understand though that a compiler-optimized Core (which is just slightly above the abstraction level of STG) typically looks very different from an extrapolation of the human-literate source. But depending on your needs, an awareness of graph reduction may be quite enough. 
You don't need category theory to talk about sets. Furthermore, sets from (naive) set theory and types are quite distinct things - one was designed to fix the other :)
&gt; Is there a formal proof somewhere that the identity is the only endomorphism valid on every sets? Digression: This sentence is actually a really good example why category theory was invented. Because set-theoretically, it's ungrammatical. One can't speak of a *particular* endomorphism *on every set,* much less ascertain its validity. Formalizing the intuition leads directly into the 2-categorical concept of a natural transformation. Working backwards from that gives us functors, and from that categories, and from that objects &amp; arrows. 
To most of us in the Haskell community, it's important to be welcoming and kind to each other. We value that far more than making sure people post the right kinds of questions in the "right" places.
It's worth trying it out now. It's already pretty nice. Also trying it now helps catch any problems that aren't caught otherwise. 
Based on the idea of vacuum there are some newer tools that allow more detailled inspection of the heap: * Textually: [ghc-heap-view](http://hackage.haskell.org/package/ghc-heap-view) * Graphically: [ghc-vis](http://felsin9.de/nnis/ghc-vis)
Lots of changes, notably I abandoned monadic lenses -- I give credit for edwardkmett for this change.
I can't build it anymore on my Windows machine. Previous build worked fine. The error is "Not in scope: `Env.lookupEnv'" in Restricted.hs with the latest haskell platform. It think you're using safe version of a function that hasn't made it into the haskell platorm yet. Normally I don't complain about things not working in Windows, but it was nice to have a good and easy gtk package for once. I thought you should know.
divip posted this a while back and some interesting debate ensued between him and edwardkmett [here](http://www.reddit.com/r/haskell/comments/1cemr2/lgtk_lensbased_gtk_interface/) Primarily I believe it was down to there being limited non-trivial laws for monadic lenses- edk: &gt;That is pretty much why lens still doesn't offer monadic lenses. There is no coherent set of laws for them. There were also issues with Refs. Hopefully someone with a better understanding than me will synthesize.
I couldn't reproduce this with Cabal's native sandboxing.
I am a beginner too, and when I get a type error, I delete the type signature and I check what Haskell inferred with `:t function` in GHCI. It can help! :)
My understanding: LGPL does not forbid static linking; it requires that users be able to replace the LGPL code. That is usually implemented via dynamic linking, but could potentially be implemented by distributing object files (or source) to your users from which they can rebuild the binaries with updated versions of the LGPL libraries.
Why was the Haskeller vulnerable to [tempest attacks](https://en.wikipedia.org/wiki/Tempest_(codename%29)? Because his programs were statically typed.
Force refresh in your browser, after that, or use this link: http://www.haskell.org/platform/?2013.2.0.0 **Haskell Platform 2013.2.0.0** *featuring* GHC 7.6.3 - 52 packages - 650+ public modules - 4 tools *New packages this release:* attoparsec - case-insensitive - hashable - unordered-containers *and a major upgrade to* OpenGL and GLUT 
Due to the nature of GHC there isn't really a way to drop in a single replacement package without recompiling from source.
I just started learning Haskell, so I may be wrong. But from what I have seen so far Haskell won't give you much of a chance to keep bad habits. The language is just too different from what you may expect and simply won't allow a lot of things. It mostly forces you into a functional style. It's not like switching from C to C++ where you could go on doing things like in C. Or maybe switching from PHP to Ruby. (or any other moves like that). Ruby still would have a for loop, though nobody ever uses it (we use things like .each), so some people new to it may want to use it because in the languages they learned so far it was the normal way to do things. Haskell has no such fallbacks, loops are done with recursion and that's more or less the only option you have.
The download link for Mac OS X is wrong. On the web page it is http://lambda.haskell.org/platform/download/2013.2.0.0/Haskell%20Platform%202013.2.0.0%2064bit.pkg , but actually it should be http://lambda.haskell.org/platform/download/2013.2.0.0/Haskell%20Platform%202013.2.0.0%2064bit.signed.pkg . "signed" is missing.
1. A lot of the vocabulary will be deceptively familiar. Don't assume that any word means what it used to. 2. The error messages can be scary. Try not to guess and check until your program compiles; learn to understand the error messages.
Both links on the web page fixed. The "official" link is the one with the `.signed` component. Both will work for now.
1. Static types are your friends. Think in types, then in implementations. 2. Avoid for-loop-like constructs, they're terrible style. They might be good for the first few lessons, but they result in very un-Haskelly code. Think in structures rather than loop variables: get familiar with maps, folds, unfolds, and scans.
Thank you. You should also update the content in md5sum.txt and sha1sum.txt. There're four pkg files now.
Is the general advice still to uninstall everything, then start fresh with this new Platform?
When was that ever the advice? It is my experience that as long as they contain different GHC versions, multiple different platforms can coexist very well. If you switch a few symlinks (assuming you're on either Mac or Linux) you can switch back to the old version.
Some people may not want to have multiple versions coexisting. I know I don't. The Haskell Platform is not small, and I have no reason to keep the old one around.
Play a lot within ghci and use :k and :t to check type notation of expressions this way you will learn a lot and see how to debug things using types. 
&gt; loops are done with recursion Explicit recursion is actually highly unidiomatic Haskell. Look at maps, folds and scans for an idiomatic alternative.
A couple of examples of the first point: * `return` is not a keyword. It is a (badly-named) method of the `Monad` typeclass. * A typeclass is not like an OOP class. It is more like an OOP interface, but even that is misleading. A couple of examples of the second point: * (Not because you are used to imperative languages, but because you are used to whitespace-insensitive languages:) If you get your indentation wrong you will get an error message that doesn't tell you your indentation is wrong. Some rules of thumb: * Ensure your text editor does not save tab characters, but converts them to spaces. * If you wrap a long line of code, subsequent lines should be indented more than the initial line. * Lines in a block following the `where`, `let`, `do` and `of` keywords must be indented consistently. * You will encounter a lot of type errors. (*I* get a lot of type errors.) [Learn to understand type error messages.](http://stackoverflow.com/questions/9177222/decimal-and-binary-number-handling/)
How about when you need imperative looping, like an input/output loop for a text based interface?
Things have changed a bit since the original STG paper - the switch to a single stack when the RTS was rewritten in GHC 4.0, and more recently the switch to eval/apply from push/enter, described in the fast curry paper: http://community.haskell.org/~simonmar/papers/evalapplyjfp06.pdf
If you want an infinite loop, use `forever`. Otherwise, use a combinator that unfortunately isn't present in Control.Monad: loop :: Monad m =&gt; (b -&gt; m (Either a b)) -&gt; b -&gt; m a loop k x = either return (loop k) =&lt;&lt; k x (You could also write a variation using `Maybe` that gives a `m ()`.) Yes, `loop` is implemented using recursion. Unless you have a trivial loop body, it's better to factor out the explicit recursion and the loop body into separate functions.
Excellent work! a picknit "Problems" link in right sidebar could read "Wiki: Package Versions/History; Troubleshoot; for Platform Developers", or some such above issue commented on here: https://news.ycombinator.com/item?id=5778538 ----------- the Version comparison chart s/b updated or deprecated http://sol.github.io/haskell-platform-versions-comparison-chart/ 
Sure, but I wouldn't want to call it 'the general advice'. That makes it sound like if you don't follow it, things might break, which isn't the case, AFAIK.
Haha. Haskell programs have no bugs, but that turned out to be a trivial property of the null set since no-one can get their program to compile.
Oh, damn. I didn't see the new release because Chrome had the page cached... Maybe I ought to subscribe to a newsletter or something. It builds with both versions now. Thanks!
Because once he stored a value in it, he couldn't use it again.
&gt;Avoid useless identifier prefixes. In particular, avoid putting stupid type prefixes in front of every record field. If there is a clash, I'll use a qualified import.
This is a point of contention that won't be resolved by making definitive statements. I use prefixes. Qualified imports for records suck to read, write, and manage. They are ridiculous on non-trivial projects. Not gonna change any time soon.
&gt; Notice that the meanings of (:) and (::) have swapped. (:) is “has type” and (::) is cons. Boo. Talk about bikeshedding for no reason. The biggest attraction of Elm was that it was Haskell syntax; that's no longer the case when you mess with things programmers have learned to parse idomatically.
Yes, I realize it is inconvenient folks coming directly from Haskell, but I think it is the right long-term choice. As I mentioned in the post, Haskell is the odd one out on this decision. Agda is also a direct descendant of Haskell and it uses `(:)` for "has type", I think because types are so common in Agda compared to consing. The primary motivation is as follows: -- Haskell way type Object = { x::Float, y::Float, velocity::Float, angle::Float } -- Elm, Agda, SML, OCaml, Coq way type Object = { x:Float, y:Float, velocity:Float, angle:Float } I find the latter a lot easier to glance through. The record system gets used quite a bit in Elm, so I thought it'd be worthwhile to optimize the syntax for that case. edit: Scala, typed Racket, F#, and Rust appear to use `(:)` as well.
There's always `fix $ \loop -&gt; do print "Round and round"; loop` too.
You can find very short descriptions here: http://www.google-melange.com/gsoc/org/google/gsoc2013/haskell I'd especially like to know more about the plans for Qt bindings! Edit: typo
A rule of thumb I've been using across all languages is that the fully qualified name of any entity should not be redundant. `User.UserRole.Roles.getRoleForUser` should probably be something like `User.Role.get`, if you have a Config module of some sort the word config should not appear in any of the variables or methods, etc. It seems like such a simple rule, yet so frequently violated (even by me). Exceptions for languages with such weak module systems that you have no choice, like C, but those are, slowly but surely, fading.
Just a friendly reminder that we would love your jokes over at /r/programmerhumor.
One if these days I have to try Idris for something real.
As a PHP programmer who started learning Haskell ~3 years ago, there are some things I thing you should try hard to understand. Kinds, type declarations and constraints (all covered in Learn you a Haskell), I made many ridiculous errors at first because I didn't understand them. 
To be fair, that's often the only sane thing to do.
Great list. Big +1 for expose internals. 
Why does the intuitionistic Haskeller hate the continuation monad? It peirces his programs! Seriously though, let's not let /r/haskell devolve into jokes and memes, ok? Pretty please?
I am also generally a fan of Haskell's syntax, but I think you've made the right decisions with the operators. The type specifications need the brevity. I also think the application operators are a neat upgrade. They look a little more like pipe operators, which should be easier to intuit.
[Idris](http://en.wikipedia.org/wiki/Idris_%28programming_language%29) uses this notation as well. I agree with your choice. :)
Yes, so currently one would have to distribute the source. However, note that one would only have to distribute the source to those who they've otherwise distributed the software to, and (unlike the GPL) there is no restriction on the license under which that source is distributed (other than that it must be permissable to build it for the purposes of replacing the LGPL library). Of course, this is all per my understanding and IANAL...
I am helping that project by hoping some use of my fficxx (FFI to C++ generator : http://github.com/wavewave/fficxx ) which has been quite successfully used for HROOT project (haskell binding to ROOT : http://ianwookim.org/HROOT ) We may try to make some automatic C++ header file parsing and build some infrastructure of more general haskell-C++ FFI interface, hopefully leading to a successful binding to Qt and such big C++ library.
Why? How is: import Foo.Widget as Widget blah = Widget.name baz Any more difficult to read, write, or manage than: import Foo.Widget blah = widgetName baz In fact, I argue the first is better because now when the domain of discourse is obviously widgets, I don't need to keep reminding the user that I'm talking about a widget's name and not any other kind of name. Furthermore, if the fully qualified name now usually becomes redundant: Foo.widget.widgetName is just ugly. There's a reason people stopped using Hungarian notation.
Good job being on the right side of history :)
Eh, I write Haskell most of the time, and when I write Agda I switch into (:) for types effortlessly. I don't understand Haskell's use of (:) for lists, it seems like the complete opposite of the right syntactic choice. However, when Haskell was first being developed it was lazily evaluated first and powerfully typed second, and so I guess the Haskell implementors assumed Lists would be so universal and prevalent due to their use as _control structures_ that they gave them higher syntactic priority than type judgements, which are optional anyway. Now, the priorities in Haskell development have reversed - types first, laziness second, so people don't care about lists so much (not to mention, most list-based code uses combinators or the list monad to manipulate the list, not explicit cons)
`brew update &amp;&amp; brew upgrade haskell-platform`
I found I had to re-read the chapters on every section more than once, normally I can read it once and start messing with code. I also found the project euler problems really helpful. I'll probably never need to use their code ever but it was a set of easy(ish) problems for me to actually **do** something in Haskell.
How would this be different from ordinary exports and prefixing? Except that you'd use underscore instead of the dot, and start it with lower case.
qt!
I agree, it just seemed a little quiet recently so I thought "what the hell".
I rarely agree with Chris Done, but this is an example. Though prefixes are not ideal either. But I also agree with the majority of the points of the original article, especially about minimal dependencies and exposing internals.
It doesn't scale. import Foo import Foo.Person as Person import Foo.Company as Company -- and another import for every record type blah person company = woof (Person.name person) (Company.name company) ...is more tedious than... import Foo blah person company = woof (personName person) (companyName company) I dislike all that import boilerplate. Presumably you see it as a price worth paying. (Aside on Hungarian notation: people stopped using Hungarian notation because [they were doing it wrong](http://www.joelonsoftware.com/articles/Wrong.html) and not surprisingly as they were doing it wrong they didn't get any benefit from it. A better reason not to use Hungarian notation in Haskell is that Apps Hungarian is designed to paper over the inadequacies of C's type system. That hasn't prevented the Hungarianesque `x:xs` idiom from taking hold.)
The number of modules required and the amount of boilerplate required to create each module (new file, .cabal file additions, module header) is the problem.
I agree. There are so many libraries where I've ran into this (I think network-conduit was the most recent one) that I'd like to see a compiler flag to allow importing hidden things.
Missing the most important piece of advice: write some goddamned tests, and set up a buildbot ([travis-ci](https://travis-ci.org/) is free and integrates with github, you have no excuse) that runs your test suite on every commit. It's easy to set up the tools to compute a test coverage report on every build. I motivate myself to test things by making a game out of [keeping the bars green](http://buildbot.snapframework.com/job/io-streams/HUB=2012.2.0.0/HPC_Test_Coverage/?).
Use the `uninstall-hs` program, which is included with the platform (at least on Mac, and I think on Linux too).
Feel free to provide feedback and critique. Pointing me in the direction of a good tutiorial for making .cabal files would be appreciated, too.
How is `x:xs` hungarianesque?
The example code he describes is here: https://github.com/JHawk/enterpriseFizzBuzz Next meetup is tomorrow (Wednesday) and should have some great talks as well: http://www.meetup.com/NY-Haskell/events/119371832/
Sorry I haven't posted the slides yet! I'll get them up later tonight. Next meetup is tomorrow, and features a talk on ghci, and another on ghc internals: http://www.meetup.com/NY-Haskell/events/119371832/
Presumably there would also be something along the lines of a: `open Window` feature.
Is there some kind of aggregate change log somewhere?
Thanks! This is exactly what I'm looking for. Unfortunately, I can't seem to install them properly; I keep getting esosteric error codes and dependency issues even when I clearly have all the dependencies installed. Any tips?
Can you show me how to install these? The usual cabal install doesn't work because of dependency issues and for some reason I'm unable to install from source (whenever I do runhaskell Setup.hs configure, I get dependency issues in spite of the fact that I have all the prerequisite packages installed).
Don't worry about events. The runtime does that for you: IO operations like reading from files or sockets are transparently asynchronous. Use the inexpensive threads from Control.Concurrent (i.e. forkIO), and some or more of MVars from Control.Concurrent, TVars/TChans from Control.Concurrent.STM[.TChan], or Chans from Control.Concurrent.Chan, and write blocking code to your heart's content. You'll find that your application is as fast as or faster than a Twisted or JavaScript plate of callback spaghetti forged, through blood, sweat and tears into an unspeakable amalgam of tiny functions. Edit: Typo
Since it should clearly be `value:values` for readability's sake.
Idris is interesting, but without interactive editing, I'm not sure any dependently typed language can become mainstream.
I'm not sure that any dependently-typed language can become mainstream right now at all, given how conservative the mainstream is! Interactive editing a la Agda is a really cool feature, but I use Idris regularly without it, and it works fine.
Too bad OpenGL is required when building from source. I'm currently looking and build log and most of time it's busy compiling OpenGL stuff (on a console-only server). What a waste...
I prefer `firstElement:restOfTheList`.
Indeed, that does seem better.
For the "two-level types" approach, there is [this library](http://hackage.haskell.org/packages/archive/fixplate/0.1.5/doc/html/Data-Generics-Fixplate.html). The disadvantage is that it only works for singly recursive types (in theory one could make it work for mutually recursive types, but it's probably not worth the pain)
You can use "cabal init" to get a bare-bones .cabal file, otherwise I recommend looking at existing .cabal files as examples - it's a really great way of learning. About the code: I recommend splitting the code across few modules and possibly exposing internals that might be useful to third parties. Also try to use proper libraries when applicable: remFileExt should really be replaced by System.Filepath.dropExtension (from filepath library). Personally I also strongly dislike any functions that silently write to filesystem or perform other sneaky behaviour. This is why I think testAllAlgorithms needs better documentation about it's behaviour. Some examples (or a link to github) within the module would also be a good idea.
I can't imagine programming Idris, honestly. The interactivity of Agda is so incredibly useful. I've watched some of these videos on Idris and it's just impossible to follow because of the back and forth. I couldn't deal with that in real life.
There's interactive theorem proving. You introduce variables, saying essentially "see appendix for proof", fire up interactive mode, do the Coq thing, then tell idris to append the proof to your source. Which has the advantage that the actual code isn't littered with proof details and that you don't need to suffer the agony of having to use emacs.
I'm not a fan of tactics. But I don't mean interactive tactics, I mean interactive *programming*, the sort you see in Agda. And on a Mac, at least, there's no real agony in using Aquamacs for that purpose. If I wanted, I could abstract all the proof stuff away into an auxiliary module, too, but that's a different issue.
Idris will (hopefully) get interactive editing eventually, but it wasn't my priority at first. The back and forth you see in videos is largely because there's only so much you can fit on a screen at once in a talk. I tend to have an editor and a REPL open at the same time and use metavariables to find out types of intermediate expressions.
So the second project is clearly a way to make use of multiple local cores; it sounds like the first is heading towards a haskell distcc.
The visual back and forth isn't the problem, it's the interaction back and forth that I can't wrap my head around.
shouldn't part of that work using lenses. e.g. data AST = ... makeClassy ''AST data AAST = AAST {_aAst :: AST, _aAnnotation :: Annotation} makeClassy ''AAST instance HasAST AAST where ast = aAst now you can use the lenses from `AST` with `AAST`: i.e. f :: HasAST -&gt; HasAST
OK, nice start. Now how about adding more details for one or more ways to set up the prefixes. The trick is how to work with more than one version of Haskell Platform, plus one or more GHC installs without the platform. Trust me, even if you think you don't need to do those things, you *will* need to. One way is Brent Yorgey's [suggestion](http://byorgey.wordpress.com/2012/11/01/using-multiple-versions-of-ghc-in-parallel-with-gnu-stow/) to use GNU stow. That sounds like a nice workflow, but I haven't tried it yet. After trying a few possibilities, my favorite right now is this: * Put both HP and the corresponding GHC into the same prefix, `/usr/local/haskell-platform-&lt;version&gt;`. * Put versions of GHC without HP into `/usr/local/ghc-&lt;version&gt;`. * Create a symlink `/usr/local/haskell-platform` to the version I currently use most often. * Add `/usr/local/haskell-platform/bin` to the default path. * When I need to use a different GHC, prepend `/usr/local/haskell-platform-&lt;version&gt;/bin` (or `/usr/local/ghc-&lt;version&gt;/bin`) to the local path. You don't need to delete `.ghc` and `.cabal`, as long as you don't need two different installations of exactly the same version of GHC. Different versions of GHC know how to play nicely with each other in `.ghc`. For `.cabal`, the only problem is if you need different versions of `.cabal/config`, and at least for me that is totally orthogonal to which GHC I happen to be using. When that has happened in the past, I wrote some shell scripts to swap things around and/or run certain commands in the context of a certain `.cabal/config`. Those scripts turned out to be a lot more complicated than what I originally imagined they would be, and the whole thing felt hacky. I'd be interested hear about other solutions people have for that. 
We use an annotated expression type: Expr a Usually we just use: `Expr ()` for unannotated types. If you want to write a simplifier that works while just keeping existing annotations: `Expr a -&gt; Expr a`. If it can't keep them: `Expr () -&gt; Expr ()`. If it can sometimes keep them but not always, or maybe it needs to generate its own subexpressions: `Expr a -&gt; Expr (Maybe a)`. And so forth. Expr is also a `Functor`, `Foldable`, and `Traversable`, which is an immense help.
That would be nice for people who go the apt route. It probably wouldn't be too hard to set up and maintain once Joachim's Debian package makes it out of experimental into stable, which usually doesn't take too long. But I usually install HP from source, after installing the corresponding GHC version from the binary tarball. See [this current reddit thread](http://www.reddit.com/r/haskell/comments/1f8730/basic_guide_on_how_to_install_ghcplatform_manually/).
Alternative semantics http://imgur.com/kYeIiqz
"x" has nothing to do with the type of "x", which is something like a list element, so that's nothing to do with hungarian notation.
Use of `recursion-schemes` may give another type class to generalize towards (http://hackage.haskell.org/packages/archive/recursion-schemes/3.0.0.2/doc/html/Data-Functor-Foldable.html). data WithAnn a f = WithAnn a (f (WithAnn a f)) type instance Base (WithAnn a f) = f instance Functor f =&gt; Foldable (WithAnn a f) where project (WithAnn _ a) = a 
Just wondering: how often can you treat the annotations generically right that and how often do the expression transforming functions also have to transform the annotations at the same time?
Can you explain why you run "cabal install cabal-install" both before and after the update? I don't understand what this accomplishes.
Here's the slides: http://gbaz.github.io/slides/meaning-04-2013.pdf
Shouldn't you derive the "colour values" of the letters by reading a (terminal) font instead of hardcoding them?
A few of us in my department just tested this out and ran into an issue. Just thought I'd post our solution in case it helps anyone else. For context, we are using departmental computers that do not allow the use of sudo, so all installs must be local. Additionally, the Linux distribution we use is a modified version of Slackware 13.37 If, when running `make` in the haskell-platform directory you run into an error like this: Setup: Use of GHC's environment variable GHC_PACKAGE_PATH is incompatible with Cabal. Use the flag --package-db to specify a package database (it can be used multiple times). Error: Configuring the HUnit-1.2.5.2 package failed make: *** [build.stamp] Error 2 It could be because the build script is setting GHC_PACKAGE_PATH when it doesn't need to. Our fix was to remove line 69 from the scripts/build.sh file. This line looks like: `GHC_PACKAGE_PATH="${ORIG_GHC_PACKAGE_PATH}" \`. After removing that line, run `make clean` and then start the process again (`./configure ...`). Cheers.
Forced interactive editing is a catastrophe, and is not actually consistent with the character of a language as a language, rather than as some sort of application.
You should write up a quick tutorial for how to do this on Haskell wiki.
You don't have to imagine it, you already know how to write a gigantic sub-set of it, and in fact the most important part of it. And in all your preliminary practice you never made use of an interactive IDE app ... -- Just write Haskell with `:` for `::` and a few other (frankly uniformly tiresome and groundless) alterations, and you'll hardly go wrong so long as you're with Haskellish types -- which are forever the principal types of practically programming anyway. Then start adding better types..., prove a couple little theorems in the repl, etc. You'll never look at Coq or Agda again, and will curse them as the infantilizing 'theorem' proving helpers they are.
Nobody has yet mentioned the #haskell channel on freenode. You will always find very friendly help there if you have any queston from the most basic to the most advanced. 
NY Haskell Meetup, as someone nowhere near NY, thank you very much for the recording of these sessions you've been doing. You've had some quality presentations in there and I'm very glad to have been able to watch, and that they aren't disappearing into the ether. I feel like I ought to pay you dues too.
Comonads are just the right abstraction for annotating trees. Take the cofree comonad for example: Cofree f a = a :&lt; f (Cofree f a) Every level of the tree is annotated with an `a`. The `cobind :: (Cofree f a -&gt; b) -&gt; Cofree f a -&gt; Cofree f b` is: cobind f w@(_:k) = f w :&lt; fmap (cobind f) k The interpretation is straightforward: `cobind` takes the annotated subtree at a given node and computes a new annotation there. This is done recursively for every subtree. `cobind` exactly captures synthetic attribute grammars! What about inherited attribute grammars? These are tricky but Edward Kmett once showed me how you can get these: Annotate your nodes with *functions* that are lazy in their argument. Now `cobind` can give information to the annotations at subnodes during its computation. Caveat: If your annotation functions are simple, cobind will be more expensive than it needs to be because it does all the work required for a node without any sharing. If your annotations can be folded from the leaves you can compute them incrementally and save a lot of computation by hand-rolling the recursion instead of using cobind. There is a comonad reader article on this but comonad reader seems to be down.
I find that dependently typed programming without things like coverage analysis with automatic case split is very painful. Same with refinement of a goal. It's very nice to be able to C-c C-r in emacs, both with and without proposed (partial) goals, and have emacs transition smoothly to the next bit of code. Maybe it's not tricky for everyone, but it is for me.
Who said anything about forced interacting editing? You can edit your Agda code as blindly as you like. Tho you're a bit daft if you do.
Ah, so the issue is just that the more specific types make those tools more valuable. This makes sense. In practice, I can get by with what Idris has, but if I couldn't occaisionally make a metavariable and inspect it's goal type, I'd feel pretty lost myself :-) I'm looking forward to interactive editing!
Well, Agda doesn't _force_ you to use the Emacs mode. And compilers with editor integration are quite common - take a look at Eclipse and Visual Studio! 
I don't think Coq or Agda are the slightest bit infantilizing. I think they're both very useful. Stunning technical achievements. Have we gotten our first pro-Idris zealot who disparages all other systems? :-)
Yes, take a look at Eclipse and Visual Studio. 
Apologies, I was being facetious.
Try Agda! We've already got interactive editing there!
Congratulations! Just out of curiosity, are you still in Googl, working for Prezi as "consultant", or you changed your job entirely? :P
It's not about needing the assistance so much as it is being able to think more fluidly with it.
Sign me up too. I like Idris more than Agda and Coq. First just because of it being more classical compiler that doesn't require emacs or its own editor. Then because of its power and things you can do with types treating them as values of type Type. And then because it produces small executables that do real work, not just theorems. (I'm Dmitry from the mailing list)
You have a dupe of this without the last line.
Thanks, fixed.
Ehm, WAT. There's a monad structure on this...
&gt; Q: Is Prezi secretly trying to take control and make Elm all about zooming? &gt; No :) Although I am a full-time employee of Prezi, I still have full control of the direction of Elm, and my primary goal is the success of Elm as an independent project. Are you sure?
Yes, this was initially a big concern of mine too, but the incentive to specialize Elm is actually surprisingly low. The basic argument is that a language used widely in industry is going to be faster; have better tooling, libraries, and resources; be easier to hire for; etc. It quickly became clear when talking to Prezi that this is the ideal outcome for both Prezi and Elm. The history of Python is a good case study on the incentives at work here. See the [history of Python](http://python-history.blogspot.com/2009/01/personal-history-part-1-cwi.html), especially [part 2](http://python-history.blogspot.com/2009/01/personal-history-part-2-cnri-and-beyond.html), for how things went for Python. Haskell also is kinda similar in that a bunch of big GHC contributors all worked at Microsoft Research.
Okay, but if future updates suddenly start to contain way too many zooming-related functions...
Yeah I know. I won't be sharing my answers, and I won't look at any answers until I've finished my own. I can send you my answers to the first 2 if you want proof. I'm somewhat surprised that its working well for you. It seems like Haskell really sucks at bit twiddling, I find myself wishing for c, where I can just do bitshifts and xors on chars and have it work.
What a coincidence — I saw your question a minute after sending an e-mail to Matasano! When I’m done with the first set (hope it won’t take that much time) I can show you my solutions.
If Prezi forces Evan to work on features that are only useful for them, they can't expect Elm to develop an ecosystem (network of people) that will help it the most in the long run. I'm sure they realize this and I know Evan realizes this. Also if it gets zooming functions... Then I'll have to make Supreme Commander in Elm.
Well, now I'm wondering if that part of the Q&amp;A wasn't a joke anyway.
I'd like to see this too, but I've heard that one reason it might not be a good idea is because currently, you have guarantees that unqualified imports will never pollute qualified namespaces. For example, import qualified Prelude as Window import Graphics.UI.Gtk Under the qualified exports scheme, suddenly `Window.show` became ambiguous, even though the import list does not make this immediately obvious.
Congratulations. This is great news! :)
I'm doing the problems in Haskell and most of the way through the second set, so far the code is all looking _fairly_ elegant. I've been writing a utility library for it in parallel, which has helped - I've been keeping the test coverage high and refactoring to clean the library up as I go along. Not sharing the problems or the answers is the one condition of the challenge that keeps getting highlighted. Instead of sharing mine or looking at other people's code I'm currently thinking I might keep the codebase around to fiddle with as I learn more about Haskell API design / testing / benchmarking. At the very least I'd wait until you've gotten through all 48 problems before looking around. Amongst other things, looking at the current state of my utility library would imply quite a few things about the second set of problems.
I just google stalked you a bit. You should update your [website](http://evan.czaplicki.us/). &gt;I recently graduated from Harvard with a degree in Computer Science. I am now working at Google as a Software Engineer.
Hi guys! Sean from Matasano here. I think you all already know this, but just to be clear: Please don't post answers online anywhere. If you want to discuss your solutions privately with a friend - by all means go for it. We'd just really like to avoid solutions being posted in public forums. Thanks! And glad you're (I hope!) enjoying the problems.
Yes, it should, but I am unsure how I would get bitmaps of all the characters of a font without having to dive into some graphics library that supports *.ttf formats. If you have suggestions any would be appreciated.
I moved on to pine and then mutt many years ago. ;)
[This library](http://hackage.haskell.org/package/compdata) is another option, but it makes doing so for mutually recursive types almost painless.
I don't know why you're lumping Coq and Agda together. Agda is extremely similar to Idris in design and approach.
A while ago I remember reading some interesting posts from you about how Coq's imperative tactics scripts can't really be considered a "proof" because they don't communicate any valuable information about the proof structure for the user. In what why is Idris's tactic system any better in this regard? I thought that, if that was your opinion on tactics, that you would prefer Agda to Idris. For what it's worth, I agree tactic scripts are horrible to work with when you actually want to understand a proof (not merely accept that a proof exists). Sadly, writing proofs in Agda is usually quite a bit more excruciating, although the editor-integrated automation is getting better and is quite helpful.
&gt; types treating them as values of type Type You can treat types as values of type Set in Agda, too. &gt; And then because it produces small executables that do real work, not just theorems. You can compile Agda to JavaScript, epic (the same backend as Idris), and Haskell code.
I found compdata to be beyond me. I spent about a month reading through the papers and trying to understand the library (as well as having a few correspondences with the authors) well enough to get it to work for my project; I couldn't get beyond implementing semantics for a simple **eager** (it was tricky to prevent reduction under lambdas) functional language (very similar to [this one](http://siek.blogspot.com/2013/05/type-safety-in-three-easy-lemmas.html)). I got the feeling that if I eventually grokked it, I'd feel a sudden **woosh** of power and understanding (kinda like monads), but I didn't have any more time to sink into understanding it.
[Video with slides](http://www.infoq.com/presentations/Running-a-Startup-on-Haskell) [Slides without video](http://bos.github.io/strange-loop-2011/talk/talk.html) 
That sounds misleading and ghetto.
A real world example: I've used haskell-src-exts and attached type information to every node in the AST. It's very easy since haskell-src-exts has "location" information in every node, but you can pick your own type of "location", so you can put type information there.
Also, maths in general uses `:` for types. f:ℝ→ℤ is what you see in maths, so it seems like the right choice. It's unfortunate in my opinion that Haskell chose the opposite way. Especially given that all infix type constructors have to begin with "`:`", the cons constructor seems like a special case to me. In my mind it would be nicer (more consistent?) if it were "`::`" instead.
I suppose my point was, it might be counter-productive to learn about how Haskell programs are compiled if you're new to functional programming (which I don't know that you are or aren't). Haskell programs that are designed to be efficient are rarely elegant and if you know how to make things efficient, then it can be tempting to do so. The best Haskell programs I've seen have been written to be understood in a declarative way rather than in an operational way.
Old but good talk by Bryan O'Sullivan.
Anywhere they actually care enough to have an office mood manager sounds like a cool place to work... And to work on haskell too, colour me jealous! When are they opening a London office? :-)
I have tried Agda. It's very nice! I like it a lot!
They're quite useful for writing Java or C#. What are you getting at?
I think your claim is unsupported by any evidence. How does getting your editor to introduce a case split require less interesting thought that going through the motions of typing it by hand?
Is this possible in Agda? swap : (a,b) -&gt; (b,a) swap (a,b) = (b,a) v : fst $ swap ("hi there", head [Int, String]) v = 42 Works fine in Idris. BTW, Idris doesn't use epic backend anymore. 
Welcome to Budapest! This is probably a very good time for us to try out Elm, so in that sense the move is already looking successful
I believe they're using the collective progress of participants to see how much people learn and how difficult the questions are. If the answers are available online, you suddenly lose a lot of potential information from your dataset.
Were you working on Elm as your 20% time at Google? I thought that they could hold control of the project if you were employed with them.
And my claim is that you're an arrogant ass.
To heck with *arrogant ass*, I begin to suspect that *gibbering madman* better fits the bill.
Bit twiddling rocks in Haskell if you use the right types.
yeah for enumerable rational numbers.
Just tried out Elm last night. It looks great! However, I am having a bit of trouble getting started, the fact that it is easy to end up with a blank browser page and no error messages makes debugging difficult. Any advice on how to debug (especially as you are just figuring out the language?)
Sure. There's a huge difference between reading about crypto constructs and attacks (or even reading code for them) and implementing them yourself. It's just something you need to do yourself to get a feel for how all the parts move together and why things work. Parceling out the problems in small sets motivates people to work through them. True, if we published everything, more people would read them. But I think many people who otherwise would have put in the work would not go to the trouble. We're really only interested in people who want to put in the work. So that's why we ask people not to share problems or post solutions. I know I'm not your real dad and I can't make you do anything, but I would very much appreciate it if people would respect our wishes. Thanks! EDIT: Addendum to this. There probably will be a time in the future when we are no longer supporting this (i.e. checking solutions and answering questions), and at that point we will simply publish the full set of problems. But for now and the foreseeable future, this is how we're choosing to distribute.
Using universe polymorphism, sure: v : proj₁ (swap ("hi there" , head (ℕ ∷ String ∷ []))) v = 42 Where the definition of swap is from the standard library: swap : ∀{ℓ₁ ℓ₂}{a : Set ℓ₁}{b : Set ℓ₂} → a × b → b × a swap (a , b) = (b , a) Admittedly the universe polymorphism, plus Agda's explicit quantifiers for all variables (as opposed to Idris' defaulting quantifiers), make this a little more syntactically cumbersome, but I think the jury's still out on whether cumulativity or universe polymorphism is the right choice.
Congratulation. Elm is a great and thanks for working on it full time now.
My proposal is available online here: http://www.google-melange.com/gsoc/proposal/review/google/gsoc2013/refold/71002
No, I did not work on it as a 20% project for that reason.
No, Idris's tactic machinery is no different from coq's in this respect, of course not -- how else could anyone use it? I think it isn't documented except by listing the commands. However, because its purpose subserves the composition of actual programs, i.e. the definition, delimitation, or selection of individual values from among many, it makes perfect sense in Idris. (What the language designer's purpose was is here as everywhere irrelevant, but I expect this is a not-too-wrong account of it.) One makes a proof for example to convince the compiler of various things about one's actual program, i.e. one's actual definition, or value-selection. No one would be so idiotic as to spend forever in the idris proof machinery, but many really are so idiotic as to spend forever in Coq IDE when they ought to be designing new languages. The prejudice that the standard-issue idris user is an avant garde programmer, and the standard-issue coq and agda user a 48th rate mathematician, is one we would do well to propagate, it contains quite a bit of truth. 
I didn't say anything about Coq and Idris being similar in the post you're replying to. I'm referring to the fact that Agda is far more similar to Idris than it is to Coq.
Can you present this evidence?
The fools! I'll destroy them all!!
Easy, it makes you dependent on a particular IDE, and a whole system of little helps. Agda of course hasn't gone too far in this direction, so it isn't the system of unreason that coq is.
I saw you were coming to Google and made a mental note to welcome you once you'd started. By the time I remembered to do that, you'd already left! Sorry I missed you, but it looks like you found the right place for you at Prezi.
That's weird. Wouldn't it make sense to require that [a..b] is sorted?
To quote the Haddocks on [`UpwardEnum`](http://hackage.haskell.org/packages/archive/prelude-safeenum/0.1.0/doc/html/Prelude-SafeEnum.html#t:UpwardEnum), my emphasis: &gt; A class for upward enumerable types. That is, we can enumerate larger and larger values, **eventually getting all of them**. As I understand the bolded part, the `Integer` instance provided in the code violates it. I should throw in a couple classes that I once wrote for one of my pet projects, that fall into this same space: -- | Types whose values can be exhaustively enumerated. class Enumerable a where -- | All of the values of type @a@. If @x :: a@, then @x@ must -- occur at a finite index of 'everything'. everything :: [a] -- | Types that can be enumerated within provided bounds. class Boundable a where type Bounds a -- | All of the values within the specified bounds. within :: Bounds a -&gt; [a] This allows for things like `instance (Enumerable a, Enumerable b) =&gt; Enumerable (a, b)`, that the `Enum` class can't deal with. The latter class is blatantly non-2010, but it allows each instance to decide what counts as a "bounds" for enumeration.
&gt; As I understand the bolded part, the Integer instance provided in the code violates it. I don't believe so... At least, the intent of the specification is: given any `x`, for all `y` such that `succeeds y x`, it must be the case that `y` occurs within some finite prefix of `enumFrom x`. Thus, the force of "eventually getting all of them" is that any (finite number) of them can be gotten in finite time--- not that all (potentially infinitely many) of them can be collectively gotten in finite time. If you have a better way to phrase the requirement (in English) I'm open to accepting patches. If you still think the `Integer` instance doesn't meet this criterion, I'd be interested in hearing why.
The wording doesn't fit for `Integer`. To "get all of them" would take infinite time, even though for any given `Integer` it should only take a finite amount of time.
Just ignore him. He's trying to justify his bad attitude.
Alternative wording: &gt; That is, we can enumerate larger and larger values, eventually getting any larger value in finite time. or &gt; That is, we can enumerate larger and larger values, eventually getting any larger value in finit applications of `succeeds`.
but that comment is obviously tongue-in-cheek (hope i got that phrase right ;-)). everybody knows that you cannot enumerate an infinite set in finite time.
&gt; it's just that the enumeration ordering is not required to have any particular mathematical significance. I think I understand what the author's going for in having a mathematically principled notion of enumeration. But doesn't this make the Enum instance for rationals pretty much useless?
I don't understand what the use case is for enumerating all the rationals in an interval, or indeed enumerating anything in a way that doesn't agree with Ord.
I'm having some troubles porting my application from lgtk 0.4 to lgtk 0.5, when I use "cell" the resulting window is empty, any idea what I might be doing wrong? Here's the code: Sim.hs: http://sprunge.us/fehd vis.hs(lgtk 0.4 version): http://sprunge.us/GQcX vis.hs(lgtk 0.5 version): http://sprunge.us/RLDa
Yes, and that is the claim I responded to. Of course I agree with it. The suggestion that someone might confuse coq and agda is a little strange. 
The point is that with such documentation, `Integer` clearly has no valid instance. Not having such an instance would be a rational decision. Having such an instance along with documentation that says it shouldn't is not.
My objection to the wording is that as I understood it, it entails this: (-1) `elem` enumFrom 0 Looking at the wording again, I see now what's going on: the sentence is ambiguous between an "all larger values" and an "all values" reading.
&gt; I would like to make a gallery of lgtk applications (it does not matter whether these are outdated or not). May I include your png (just the picture, not the code)? Sure! You can put something like "Simulation of a file system made of blocks", there's no public url yet.
You may be interested in my [countable package](http://hackage.haskell.org/package/countable), which has some overlap.
i get your point now. thanks for explaining.
If you can enumerate Ratios in an unintuitive order, why not enumerate Float and Double in an unintuitive order? Specifically, treat the binary representation as a sign-magnitude integer.
Just to make it clear - of course we're always trying out the newest features, of Cabal as well as all the parts of the GHC toolchain. And they're great. But as a commercial development shop for the enterprise, there is a well-defined process for releasing stable software. It will probably be close to a year before it will be possible to integrate the newest features of Cabal into our production build process. And even then, we will still need to support older versions of our software for a long time after we release them to customers. So it is very important for us to understand how to build shared libraries using the current sandboxing tools.
&gt; But doesn't this make the Enum instance for rationals pretty much useless? It's a special case of a more general idea that is certainly useful: the product of two [recursively enumerable sets](http://en.wikipedia.org/wiki/Recursively_enumerable_set) is also recursively enumerable. Using a different class that I once wrote, let me illustrate the general pattern: -- | Law: every @a@ value occurs at some finite index of -- 'everything'; i.e., -- -- &gt; (`elem` everything) == const True -- class Enumerable a where everything :: [a] instance (Enumerable a, Enumerable b) =&gt; Enumerable (a, b) where everything = cartesian everything everything cartesian :: [a] -&gt; [b] -&gt; [(a, b)] cartesian [] _ = [] cartesian _ [] = [] cartesian (x:xs) (y:ys) = [(x, y)] ++ interleave3 vertical horizontal diagonal where vertical = map (\x -&gt; (x,y)) xs horizontal = map (\y -&gt; (x,y)) ys diagonal = cartesian xs ys interleave3 :: [a] -&gt; [a] -&gt; [a] -&gt; [a] interleave3 xs ys zs = interleave xs (interleave ys zs) interleave :: [a] -&gt; [a] -&gt; [a] interleave xs [] = xs interleave [] ys = ys interleave (x:xs) (y:ys) = x : y : interleave xs ys Note that the list comprehension `[(x, y) | x &lt;-xs, y&lt;-ys]` doesn't work here, because if `xs` is an infinite list and `ys` is non-empty we will never reach any pair whose second element is anything other than `head ys`. This sort of thing is useful for, say, search problems where you're trying to recursively enumerate a set that contains a solution. In the case I was using it, I was trying to compute the inverse image of `(Bounded c, Bounded d) =&gt; (a, b) -&gt; (c, d)` functions, so I needed to generically be able to enumerate all the `(c, d)` pairs.
So you are saying, then, that avant-garde programmers are 48th rate mathematicians?
At least in the past, it was a very bad idea to re-install packages that come bundled with GHC at a different version number. Since `cabal install cabal-install` causes that to happen, I avoid ever doing that. I get new versions of Cabal and cabal-install when I upgrade to a new version of GHC. I experienced those problems some time ago though. Can anyone say confidently that this issue no longer exists for recent versions of GHC? Otherwise, I would not recommend this.
Exactly. A better title would have been, "My ignorance of Haskell is causing confusion." 
No, they aren't mathematicians at all, except per accidens.
Ratio has the same problem. If it's a bad idea for Float, it's a bad idea for Ratio. It *looks* like the solution there was to use a newtype, which you could do with Float and Double, too.
Right, if you have a newtype to make intent explicit that's fine.
If I use [a .. b] for numeric types I expect the list to contain the "same" elements regardless of the type, anything else is counterintuitive IMO. 
Your interleave function is better defined as interleavexs = foldr interleave [] interleave [] xs = xs interleave ys [] = ys interleave (x:xs) ys = x : interleave ys xs
Great work! Why is it called prelude-safeenum?
haha that would be awesome :)
&gt;shachaf: The trouble with peano arithmetic is that it stops at 88. It sounds like there's some vaguely interesting context here?
A peano has 88 keys.
Oh I see. Amusing, but now I can't help feeling disappointed that there wasn't an interesting compiler bug or something.
http://en.wikipedia.org/wiki/Ultrafinitism
I was wondering why the `a` is part of the type class header: class Monad m =&gt; Var var a m Since I imagine you'd like `a` to be polymorphic almost all of the time. Then I saw that some instances want to further constrain this `a`: instance ByteArraySlice a =&gt; Var IOUVar a IO instance ByteArraySlice a =&gt; Var (STUVar s) a (ST s) Can't you make the constraint itself part of the header instead of the `a`? Like this: class Monad m =&gt; Var var ctx m where newVar :: ctx a =&gt; a -&gt; m (var a) readVar :: ctx a =&gt; var a -&gt; m a This way you prevent people from writing very specific instances like instance Var IOUVar Int IO And you can still express the constraint by writing: instance Var IOUVar ByteArraySlice IO 
Ok, but what's it for?
After twenty years we finally have variables in Haskell! Within five years you won't even need a PhD in category theory to use them! ;-) Yeah I've no idea what it's for either. I suspect that means it's not for me. The README is not so illuminating.
Specifically, it provides unboxed variables and mutable (boxed and unboxed) tuples (really, mutable structs without field names). It is much more efficient when multiple variables may need to be used (of varying types - where a boxed array cannot be used - though even then, its a little faster). The boxed and unboxed benchmarks on the associated github page may be of value (and I intend to add more documentation). The idea was to emulate the structure of `array`, but for mutable variables and tuples.
What about an STM instance for Var?
I am very open to adding this - my only concern was the `stm` dependency. If this isn't a big deal (much of `stm` is baked into ghc, anyway, right?), I will add it in the next version (today or tomorrow, most likely).
I am still struggling to understand why laziness actually leads to better modularity and hopefully by extension reuse. Many features of haskell do lead to better reuse including higher order functions and monad transformers. The crux of the argument seems to be 'using lists as control structures leads to better modularity of software'. Its easy to agree for operating over large lists where a subset might be needed, however the other examples seem to have less clear benefits. While infinite precision square roots are interesting I typically don't use numbers larger than 64 bits. Also root finding is often done with an approximation, although I can imagine there are use cases that require it. I find the "AI" example to be interesting. I could see an extension being to consume the tree for 20ms then terminate, selecting the best move found so far. In practice I don't know if its actually vastly superior to currently used techniques. Its essentially the same as searching through a list for the best move. So got it that using lists as a control structure is great for operating over lists or problems that can be composed as such. Are all (or enough) problems in software really list problems or optimally composed in such a way to make them so? 
Hah :) Normal-height elevator moving at normal speed. But when the door opens at the end I just awkwardly keep talking for thirty minutes.
I wanted to demonstrate how you can take a data structure apart and work on the pieces, so I avoided the way that you mention, although your way is of course much more reasonable in real life. And the properties are unthorough because I wanted to save space. Do you think I should put more emphasis on showing normal toList idiom and good QC habits, by making your changes? Thanks for the input!
One case *outside* of Haskell where 'laziness' leads to greater modularity is [LINQ](http://en.wikipedia.org/wiki/Language_Integrated_Query), and in particular [LINQ to Entities](http://msdn.microsoft.com/en-us/library/bb386964.aspx). For example, I can use LINQ to define a *base* query that represents all records matching a given search criteria. And then I can compose the base query with a 'count' operation to calculate how many pages there will be in the search results. And then I can compose the base query with 'skip' and 'take' operations to retrieve the data for each page as it is viewed. And I could also easily compose the base query with different 'select' operations, depending on what data will show up in the search results. Besides the obvious benefits to code factorization and modularity there is also a potentially huge efficiency benefit because when results are finally demanded, the framework can use the whole query definition to fully optimize the actual database query that is run. Of course the fact that LINQ is 'lazy' is not too surprising considering that the guy primarily responsible for the design of LINQ, Erik Meijer, is a former Haskeller.
Could you post some examples as part of documentation?
Can't reach that URL. Any mirrored copies of the paper? Google wasn't of help...
Thanks for the reply. I understand your explanation, though I don't necessarily agree with your philosophy.
Laziness is also *less* modular when it comes to understanding the space usage of programs.
This resolves one of my long-standing dislikes about TH.
My burning question is: how much do you roughly think will need to see this cool new stuff merged into master and released? Will it be available under the form of a LANGUAGE pragma? :)
Not sure exactly what your question is, but I will attempt to answer :) As the blog post mentions, my hope is to have this in 7.8. It will replace the existing Template Haskell implementation, so all you will need is the TemplateHaskell LANGUAGE pragma. The new untyped brackets and splices are strictly more lenient than classic Template Haskell splices and brackets, so no existing code should break.
Part 1: https://docs.google.com/file/d/0B4UsAaOjX7GRODhibGhjV2FtVnM/edit Part 2: https://docs.google.com/file/d/0B4UsAaOjX7GRTG9SVmdZb29wVU0/edit
Sweet! 
"print a Template Haskell expression to a file and compile and load the file" So for compilation, the expression is printed and immediately parsed after it. It is hard to replace this by the identity function in the implementation?
It's not the identity function because the parser does not return Template Haskell abstract syntax---it uses a different representation. The change would be a non-trivial modification to the GHC API.
I'm probably not the only one in /r/haskell who believes Haskell has long surpassed Common Lisp even before and without Template Haskell :)
7.2 isn't really worth bending over backwards to support. It was really only current for a very brief period as a technology preview. It was never used as part of a platform. Also, you _really_ need fundeps in your typeclass if you actually want the code you write with this to be usable. class Monad m =&gt; Var v c m | m v -&gt; c where That said, even with that, inference is going to be pretty abysmal in the absence of explicit type signatures all over the place.
I tend to think that the difficulty in understanding space behavior of lazy evaluation has nothing to do with modularity. It is a part of spec, therefore use wisely. Space behavior of a given module can always get very intricate in the interaction of other modules. I think that the more valid statement about laziness and modularity is that to enhance modularity, having a different evaluation option is helpful, but needs to be used carefully in practice since programmers often assume one evaluation model (usually strict). 
A jit compiler would help
Looks awesome. Will there be a less ugly syntax, though? Maybe `[...] for [||...||], ie "making a TExp" $[...] for $$(...) ie "expanding a TExp" and for single-name things and things you splice into a TExp, `name ? Also, will we see more work on this runtime code gen stuff, such as running optimizations? It's a slippery slow to people running amok and templating everything in the name of optimization (see C++), but still very useful. IIRC using the GHC API will make you leak memory when you load things. I don't know if that can be resolved though.
No more backticks, please :( - there are people that use non-English keyboard layouts.
I think laziness is useful for extending the language with control structures and making infinite data structures (which are useful for modeling some problems.) In other languages, there are often a sort of semi-lazy list implemented (called streams or generators or whatever.) However, they are rarely *really* lazy, since you cannot often map functions over them, since they then have to be evaluated. So you have to reduce them first and then apply operations, instead of the more logical way of applying first and then as a last step reducing to whatever you know you need at the time. It should be said though, that I don't think lazy evaluation by default is necessarily required or even a good idea. Just as long as it's there as a tool, I would do fine with strict evaluation for most things. But! I *do* think lazy evaluation by default has been an important property of Haskell in it's development, since it forced the committee to go for controlled side effects, which is absolutely critical for Haskell to be the Haskell I know and love.
Ok, so the answer was rather simple... And actually already in the blog post, but I just didn't think it was an option. I installed Crouton on my Chromebook to use it as Linux dev machine (I consistently get 8+ hours out of the battery running Crouton in chroot, so i'm actually not using any other laptop anymore, besides when I have to compile for iOS). Crouton, as detailed by the top Google hits 'how to install crouton' installs Precise. I thought that this was for a reason and that the reason was that newer versions don't work. Because it takes almost no effort / time to reinstall the Chromebook and because my software is on github and my other files are encrypted on my server (I basically need a private key installed, internet + one command to restore a new Ubuntu based laptop to my working environment), I decided to change /etc/apt/sources.list from precise to quantal and run update/upgrade. After that the GHC and Haskell-platform both are 7.4.2, ghci is working and we can rejoice and be merry! Hope this will help more people. And definitely try out the $249 chromebook; it's really a great little dev machine, especially now that Haskell is working fully and not partially. 
Why just lists? Basically every structure, e.g. infinite game tree bring constructed... 
I'll be trying that next. Small steps; I had Ubuntu break horribly on ARM devices in the (near) past while updating, but yes, I'll be trying that next! Thanks.
It was a real question, I don't even know if your chromebook will not burst into flame :D I'm waiting for a refresh but these little machines seems nice
I think you guys are misinterpreting augustss. He's obviously implying that we should have posted it earlier and at least once a year!
Someone was talking about replacing GHC's LLVM printer with calls to the `llvm-base` API (FFI)... that would make a JIT much more feasible.
Link to HN discussion: https://news.ycombinator.com/item?id=5801438
OK, so basically it did work for 7.4.2+, but then it broke again and nobody had time to fix it. 7.6.1 is in this boat. HEAD is still broken, although it was close to working at one point. The hope is to fix this in HEAD as much as possible. I hope by the time 7.8.1 is released we can actually have ARM binary distributions for a few platforms that can be distributed (RPi, ARMv7+ boxes, etc.) I fixed a few things on the ARM build for GHC but haven't had time to get back to it. I have 3+ ARM machines that are capable of building GHC in a reasonable amount of time (including a Chromebook running Ubuntu *without* Crouton) and really need to turn them into GHC build bots sooner, not later. The annoying part really is that GHC takes a very long time to build. ARM machines certainly have the raw CPU numbers, but they lack the memory units and bus speeds you see on Intel-class machines, so for most developer-workflows, they're a lot slower to build on. The best machine I have found to build GHC is the ODROID-U2, which is a 1.7gHz quad-core Cortex-A9 machine with 2GB of RAM. Headless, this can build a `quick` flavour GHC in about 4ish hours or less. However, you definitely need eMMC for this to be fast. Chromebooks have SSDs, but the dual-core CPU is a little more limiting in the compile times. Ideally, we could cross compile to ARM from an x86 platform, unpack a binary distribution and then test it on that machine and know stuff works. Cross compilation is much better in HEAD, but it can't build stage2 yet, so you don't get GHCi. It's not clear if this will be done in time for 7.8.1. It'd be great, though. (Finally, one unfortunate thing to note is that while GHC 7.8.1 will have dynamic linking support by default, ARM probably won't have this. This is because the LLVM backend currently doesn't support dynamic linking, and it's the *only* backend for ARM.) Anyway, hopefully this will all get much better in the (near) future. GHC HEAD is really shaping up on a lot of fronts here, so hopefully this won't be problematic for too much longer.
Yeah they definitely are. I haven't been happier with hardware since my openpandora (I like to play PS1 and MSX games and I haven't found anything better than pandora or the original systems; the original systems are not very portable nor do they contain a full linux with all my favorite tools and editors :). But for $249 this really is a brilliant buy... I have a host of ARM machines, but most of them, although having 9+ hours battery, are too tiny to actually do a lot of work on (although I managed to make a nice game during the drive from my vacation place to my home which almost straight compiled for the desktop and mobile behind a normal laptop). This thing is big enough and usually gives me more than 8 hours anyway (I was compiling 7.4.2 during a car drive to the gym, while it was doing it reported, for 1.5 hours, 4 hours left; when it finished compiling, it reported 6.5 hours left... that's the advertised battery life and it actually made around that in the end; normally i don't have the load of compiling that kind of use base, so it does much better). 
Thanks for the great answer! There are no ARM machines with more memory? Those ODROID's do look good; I have a ton of ARM machines, but none of them compiles very rapidly; because I have a lot of them, it's not bad to have a few compiling while doing something with the others. The dynamic linking support for LLVM ; that's a GHC or LLVM issue? Or; any way I can understand what is going on on that level of GHC without reading the entire sourcecode? :) I coded compilers in the past, but for 8-bit machines, so those have nothing to do with the stuff which is produced these days, however I do understand most of it on some levels. 
Yeah my dream machine would have 10+ hours battery and as high res as they can come up with (despite having been behind a computer since 1981 almost all the time, my eye sight is improving every year for some reason; I can read incredibly small font without straining and that's why I love very small laptops); Haswell with nice memory and a bit larger ssd would, I guess, be perfect. I wouldn't really care about the price at that point. You can buy stuff with 10+ hours battery now, but it's expensive or bad specs or both. Pre Haswell, the Thinkpad Helix seems to be the best candidate. Maybe Haswell will make life better for us? :) But for now, this is definitely the best balanced laptop I have had; yes the resolution is low but not too low, the keyboard is not that good, but come on; $249... There are plenty of $500+ laptops on the market which are a much worse deal. Also; most laptops you put Ubuntu on it (even the Ubuntu XPS 13 I believe, excuse me if i'm wrong) have a horrible battery life compared to running Windows / Mac OS on it (driver issue they say); that's not the case here. 
And I'm going to write a program that can distinguish based on calling "succ"? Snark aside, though, I am not sure this is bad policy. Haskell has often found good uses for things included out of mathematical correctness.
Same here. I'm on Fedora 17, although it seems the platform is inconsequential. The script seems to be directly violating the directive of cabal-install. 
Indeed, the interaction is not specified. However, I'm not sure why that distributivity "law" is so expected. We do not expect the following to be true: map (*2) [i..j] == [i*2 .. j*2] therefore we should not expect the general case that: map f [i..j] == [f i .. f j] Thus, the only way to argue about `map fromInteger` is by arguing that `fromInteger` is special in some way... probably because it is "like an identity function" or some such. The problem, then, is that we must be careful about what "like an identity function" actually means. `fromInteger` gives an embedding of `Integer` into numeric types, but nowhere is it specified what exactly this embedding must preserve--- other than, presumably, being a `Num` homomorphism (though I don't believe this is actually specified). This is part of the reason why things like `Enum Float` and `Enum Double` are so problematic. The functions `(+1) :: Int -&gt; Int` and `(+1) :: Double -&gt; Double` are not the same functions, and therefore we do *not* get that: map fromInteger [i .. j] == [fromInteger i .. fromInteger j] :: [Double] And the fact that we do not get this is one of the things that we must warn newbies about. So given that the supposed "law" already fails, what next?
Agda is no sense stricter about totality or productivity -- though in Idris you can turn on `%default partial` if you like. I could indeed have said that psygnisfive could write Idris by writing elementary Agda. But I preferred the other formulation since, 1) it also gave the reason for this, and thus aligned with familiar conversational maxims, and 2) because it is actually false for reasons I will attempt to state below (see "Hello World") As a programming language Agda differs not one jot from Coq (proper), the ambient theory is a bit different. The principal difference is that Agda has post-Turner syntax, i.e. the only possible kind, and actual Coq is ante-deluvian garbage we would rather not see. That the users don't actually look at it tells us something about pre-Turner syntax. It is true that Coq has a high-class proof helper; this doesn't keep the standard paradigmatic use of Agda, the only kind ever discussed on the list, from being exactly same; one just writes out long hand what as Coq users they procure by infantile barking of imperatives in a language that makes PHP look good. The difference between Idris and (Agda and Coq) is that in definining one's terms in Idris, everything is driving toward the composition of `main :: IO ()`, everything is organized around that possibility; one's hard drive is littered with executables in no time. It is like Haskell in that one begins with a Hello World program. With Agda, one does something like this occasionally as some sort of theoretical act, rather as people use YNot with Coq. This is not just the difference between 'theoretical' and 'practical'; it affects the actual character of the symbolism. Or rather, it is what makes actual symbolism possible, though the so-called type-theorists are too unhistorical to have noticed this aspect of the tradition from which they have constructed their bailiwick. To paraphrase Frege, it is only in the context of a possible `IO ()` that a word means anything. What can be connected with something like that only by indirectly is mere mechanism, something like decorative lines *a la greque*. The role of something like IO types (though not necessarily them) is far more decisive than they are accustomed to recognize. One must begin with them, or something like them, they are the basis of intelligibility and the user's capactity of expression, and then move to supply the user with ever more spectacular thoughts. 
The `CalkinWilf` newtype is provided mainly to demonstrate its possibility —and to provide the reference for those interested in such things—, not as something intended for use in production code. As far as incorporation into haskell', it's just the type classes that would be incorporated, not `CalkinWilf` (which could be moved into base if desired). That said, I could imagine using it. For example, given a small enough integral type (e.g., `Int8`) we may in fact want to enumerate all the rationals of that base type. Perhaps we're trying to solve some system of equations and are restricted to rationals for some reason. Or perhaps we're trying to exhaustively prove some property of our programs. For larger integral types we could still use CalkinWilf with tools like smallcheck. Sure, the ordering is obscure, but it is complete afterall so it fits well with smallcheck's idea that as the depth approaches infinity we will eventually check everything. Or perhaps we're doing something like delta debugging and so we need a notion of "simpler" rational values in order to minimize the given program; mayhaps the CalkinWilf notion of "smaller/simpler" would be effective there.
autoconf and Makefiles can do the job fine, if that's your aim. Reading your exchange, is there an issue with calling haddock yourself?
I like it, however I don't understand the use of the `Algorithm` data type / `getAlgorithm` instead of simply passing the algorithm functions around.
I've looked at this, the author said to not use it if Cabal or ghc --make works fine enough. It looks a little undocumented?
&gt; If you provide a patch (using a newtype, a la Ratio), I'm willing to consider it. Yeah, sorry, I don't actually have a use for it, I just thought it was an odd inconsistency. &gt; However, do note that the Haskell Report does not state that Float/Double are IEEE-754, therefore you'll need to come up with something portable (e.g., based on the RealFloat API). Oh. Well, that's a sensible reason to not want to deal with it...
Personally, there's two benefits to Haskell for my work. One is that Haskell itself is a very good tool for quite a lot of application domains. Haskell is great, among other things, for building parsers, compilers, DSLs, and such things; and it is really good at making abstractions, so you can express really complex calculations in a fairly concise way. The fact that the language makes no concessions in terms of purity is also great: it takes a while to get used to, but it also makes for code that is very easy to refactor and reason about (especially in combination with the sophisticated statically-checked type system). The other one is that it changes (or rather, extends and enriches) the way you think, giving you a larger, more flexible programming toolbox. Many of the things that are possible but non-obvious in other languages are common practice in Haskell.
As a practical example, I use Happstack and web-routes-boomerang, and I can directly map urls to types and vice-versa. This means that my application code can distinguish - at compile time - a user id from a post id. This isolates all of the url parsing logic in one spot and eliminates an entire class of bugs. On the other side, given a post id, I can have a url automatically generated so that I can change my url scheme without having to update any place in my application where I output a url.
I don't disagree that there should be a typeclass for "enumerating *things*", it just seems odd that it should be the same one as gives the [a..b] syntax for "enumerating *intervals*".
So it's fine to do something like that? Haskell won't explode in my face if I don't use Cabal?
Of course its fine! I usually start with something simple like: GHC = ghc GHCFLAGS = -Wall HS_FILES = $(shell find . -name \*.hs) program : $(HS_FILES) $(GHC) $(GHCFLAGS) --make main.hs -o $@ and go from there. 
Haskell code is just so much shorter than regular imperative code. Additionally, it's very easy to refactor without breaking anything.
One of the main reasons I moved from Java to Python was the improvement in refactoring ability. The move from Python to Haskell has had the same size of benefit, if not more.
How about a fast x86 emulating ARM with qemu? For Debian packages there's infrastructure in the form of qemubuilder and cowbuilder.
Control.Arrow makes this almost too easy. By the way, the word you removed - "INCREDIBLY" is actually inside the puzzle. Your algorithm failed to detect it. ([It's diagonal](http://i.imgur.com/dRLRfFg.png)) import Control.Arrow import Data.Array.Unboxed import qualified Data.Set as S main :: IO () main = putStr $ unlines [ unwords [ write (x,y) | y &lt;- [0..height] ] | x &lt;- [0..width] ] where (width, height) = snd $ bounds grid highlight = getpos write ix | ix `S.member` highlight = "\x1b[32m" ++ [grid ! ix] ++ "\x1b[0m" write ix = [grid ! ix] getpos :: S.Set (Int,Int) getpos = S.fromList $ concat [ moves ix f w | ix &lt;- indices grid, f &lt;- directions, w &lt;- targets, checkWord (moves ix f w) w ] where directions = [ first pred, second pred, first succ, second succ, succ *** succ, succ *** pred, pred *** succ, pred *** pred] moves ix f w = take (length w) $ iterate f ix targets = words $ last $ lines raw inBounds xs = all (inRange $ bounds grid) xs checkWord xs ws | inBounds xs = ws == (map (grid !) xs) checkWord _ _ = False grid :: UArray (Int,Int) Char grid = listArray ((0,0),(19,19)) $ concat $ words raw raw :: String raw = "N Z C G S M Z C M Q A C B O S W R E D V\n\ \X O K P G H N R E X T C V P E Z G W F V\n\ \W C D T N C P I Y F M E R J S Z W L E S\n\ \T R L K Z A S T M Z J V O N W F C Z X K\n\ \E S N N O H R W Y J I S Z A S M Y I S P\n\ \K T M W J R A D F W F M G W L I G O Q Q\n\ \C L D O L W G A M U V K G N I K C U F S\n\ \V X L K U O G H H I E Y Y Z C I S F B H\n\ \G J W B V U O R F H P E Y M N I I T O Q\n\ \G P V S N Y Y M R N H S A C J F J X H Z\n\ \T G J M F B R L E M H N R C S R I J M M\n\ \K V S A J V O N A W J E A F U U D W L B\n\ \P B H R H H Y A D B D W D Y G S W H A A\n\ \I Q I T M K I A Q I Y M W E A T U M J M\n\ \P P X W A T C H B H Y T Y Z D R S U Y U\n\ \V Q J N T L X L F V S D P G P A E Q E K\n\ \S Z N A E C Y N E J K H T W M T S P Z M\n\ \G Q M S E X T L R G Y G U B Y I J I A B\n\ \J C U D P E Z P M D W F Z G I N Z L L Y\n\ \P Y G X F W H P N T X R S I F G C C L T\n\ \Find the words:\n\ \FRUSTRATING INCREDIBLY FUCKING WATCH SMART"
It took me a long time to clue in that **aeson** was just an **attoparsec** parser; the function doing the heavy lifting being Data.Aeson's [json](http://hackage.haskell.org/packages/archive/aeson/0.6.1.0/doc/html/Data-Aeson-Parser.html#v:json) which has type Parser Value (I daresay that when **aeson** was written Bryan assumed everyone looking at it would know about **parsec** and **attoparsec** and instantly get that his new library was just a parser written in **attoparsec**. Since I encountered **aeson** first, that wasn't obvious to me). Meanwhile **io-streams** has a function able to feed an InputStream into a Parser; see System.IO.Streams.Attoparsec's [parseFromStream](http://hackage.haskell.org/packages/archive/io-streams/1.1.0.0/doc/html/System-IO-Streams-Attoparsec.html#v:parseFromStream) and [parserToInputStream](http://hackage.haskell.org/packages/archive/io-streams/1.1.0.0/doc/html/System-IO-Streams-Attoparsec.html#v:parserToInputStream). So if your JSON input is a series of objects, and your parser is consuming one at a time, you can quite readily hook the two together and use that in the handler function you're passing to [receiveResponse](http://hackage.haskell.org/packages/archive/http-streams/0.6.0.1/doc/html/Network-Http-Client.html#v:receiveResponse). AfC
How is Python easier to refactor than Java? In Python you basically cannot know whether a function will be even called or not without having strict conventions. Refactoring in Python means a lot of grepping and not being completely sure if you have done the right thing, because typos/indentation errors and whatnot only become visible when the specific code path is executed.
If you have good test coverage, Python is *much* easier to refactor than Java. Even just creating a closure is a pain in the neck in Java.
You'll immediately notice that the compiler catches a lot more of your mistakes, especially if you code in an idiomatic Haskell style. This makes you a lot braver about adding features, modifying code, and refactoring, as if you had a large suite of tests set up for you for free.
&gt; Additionally, it's very easy to refactor without breaking anything. Uhh... I disagree. It's very easy to be confident that after you've finished your refactoring nothing has broken, but adding a constructor to a heavily used data type is a pain (and it's a change I've had to make to my code several times). Tool support would be very nice, and IIRC, HaRe is the only attempted tool, but it's not up to date.
I think /u/Verroq already did a good job of critiquing your code, but I would like to ask if you could share your PS1 variable and associated functions to go with it. I like the current track and a few other elements of it that I would like to incorporate into mine.
This is exactly how I've been explaining it to a Ruby programmer friend of mine—it's like having a large set of *very fast* tests built automatically for you. TATFT for free.
Do the monad laws define when the pruning must happen? I.e., at every step or when the monad is run? Edit: I thought I had an example where it mattered, but it didn't. Now I'm not sure if it does matter.
Thanks for the tip on incredibly. I removed it because I couldn't find it when I tried to solve it (granted, I suck at word searches so...), but after your comment I realized there was a bug in how I did diagonals. It now finds it, I'll put a correction on the blog and credit you. Also, motivated me to finally learn about arrows, so that too.
Gotta shrink the interfaces down to a hair's width, in my experience. Correctness in Python is much easier to reason about than in Java because there are fewer characters in front of your face. Of course the same can be said of Haskell, probably to a greater degree.
I am fairly new to Haskell, approaching it from R and C++. My work is mainly writing simulations that need to be both fast and concise. I enjoy R for quick data manipulation, but I found that the code got ugly and sloppy quickly, and there is nothing in the R language to enforce non-sloppiness. When I needed performance, I would previously turn to C++/Eigen. It seems like there is a lot of good stuff going on in terms of high performance numerical Haskell. * [Accelerate](https://github.com/AccelerateHS/accelerate) - A language for expressing matrix and vector computations, with different compute backends. Right now, you can write accelerate code that generates and runs CUDA kernels at runtime. That is really cool! * [Monad-par](https://github.com/simonmar/monad-par) - If you can express your code as a DAG of function compositions, monad-par can be very aggressive about optimizing its scheduling of these processes. The DAG shows that, for a given pieces of data, what functions need to run and in which order. * [meta-par](https://github.com/simonmar/monad-par/tree/master/meta-par) on top of monad-par, schedule accelerate computations on local CPU, local GPU, or even stuff across a network My work is mostly in an academic settings, so being able to make formal proofs about the code is very appealing. Also, there is a big academic component to Haskell, so it is nice because that is a familiar world.
If you are simply interested in creating data structures, marshaling (or "serializing" or "pickling") the data, sending it across a channel, like a pipe, or a socket, and then having your server un-marshal (or "de-serialize" or "unpickle") the data and do stuff with it, then probably all you need is the Haskell equivalent of Python's "pickle" module, which is "Data.Binary" http://hackage.haskell.org/packages/archive/binary/0.4.1/doc/html/Data-Binary.html You simply instantiate your data structure into the Binary class, which has a "get" and "put" function. Once you have done this, you can use functions like "encode" to convert your data to a "ByteString" and dump that ByteString into a pipe or socket. On the server side, you use read a "ByteString" from the pipe or socket and "decode" to convert it back to your data structure. If you want your events to be encoded as JSON, you can uses libraries like "Data.Aeson" which also provides you with an "encode" and "decode" function, but these functions do not convert to binary, they convert to JSON-formated data. Also of importance will be "Control.Concurrent" which will allow you to work with multiple threads. "forkIO" for example allows you to run an "IO" monadic computation in a separate thread. Communication between threads can be accomplished with Mutex's called "MVar's" which are similar to "Data.IORef" objects that store arbitrary stateful values in memory. You must compile your program with the "-threaded" option to GHC in order for your program to actually use multiple CPU threads.
The thing is that you don't lose anything in Haskell, even if you use it "only" as an imperative programming language. If it can be done in Java, then there is an equivalent and still somewhat shorter way to do it in Haskell.
i have this same question, really. i have been messing with haskell quite a bit but not enough yet to have a very firm grasp on how to do things that are trivial with states and objects. i have this feeling it's really not as hard as i am making it. i dunno.
I think it is a matter of learning how to use Haskell's libraries to do stateful things. Believe it or not, if you need stateful stuff, I have found that simply using the "Control.Monad.Reader" monad with a "Data.IORef" or "Control.Concurrent.MVar" (for multi-threaded apps) is good enough. Put every important stateful object in it's own MVar, and create a data structure that contains only references to these MVars: data World = World{ environment :: MVar MyEnv, roster :: MVar [Player], worldMap :: MVar MyWorldMap } Initializer: newWorld = liftM3 World (newMVar mempty) (newMVar []) (newMVar mempty) -- assuming MyEnv and MyWordMap instantiate Monoid Then, your reader monad would look like this: type WorldIO a = ReaderT World IO a Then you can write public API functions like this: updateWorld :: (World -&gt; a) -&gt; (a -&gt; IO a) -&gt; WorldIO () updateWorld whichField updateFunction = fmap whichField ask &gt;&gt;= \field -&gt; modifyMVar_ field updateFunction updateWorldMap = updateWorld worldMap updateRoster = updateWorld roster updateEnvironment = updateWorld environment Since ReaderT instantiates Control.Monad.IO.MonadIO, you can use "liftIO" to run abritrary IO functions in your "WorldIO" monad, like for reading or writing files. loadPlayer path = do gotPlayer &lt;- liftIO ((readFile path &gt;&gt;= readIO) :: IO Player) updateRoster (return . (gotPlayer:)) -- shorthand for (\playerList -&gt; return (gotPlayer : playerList)) **You then would write most of the rest of your program in your "WorldIO" monad.** When it comes time to write your main function, you need to initialize the World and then use "runReaderT" to convert your "WorldIO" monadic function into a function that updates the "World" data type in the IO monad: runWorld :: WorldIO a -&gt; World -&gt; IO a runWorld worldMonad worldData = runReaderT worldMonad worldData You can run all of your WorldIO computations in their own thread very easily. Lets say you have a computer player and a human player and you have programmed each with your "WorldIO" monad. You could also have a "displayWorld" function that reads the World and converts it to a graphical representation in your user interface. So you could write something like this: main = do mainWorld &lt;- newWorld forkIO (runWorld humanPlayer mainWorld) -- reads keyboard/mouse events and updates the World forkIO (runWorld computerPlayer mainWorld) -- updates the World based on a simple AI. forkIO (runWorld (forever (displayWorld &gt;&gt; threadDelay 100000)) mainWorld) -- ^ converts the World to a graphic and re-draws every 100 miliseconds Now both players and the display all run in their own thread and update the same stateful world data structure. Using MVars won't prevent all deadlocks, but it does prevent race conditions and automatically releases the lock if an exception occurs. If you **only** use "modifyMVar" (never "takeMVar" or "putMVar"), and all of the data in your MVar's are only updated with "pure" functions, then you are guaranteed to never have a deadlock. It's really simple once you get used to it. 
You should try Scala. Almost as concise as Python, much more type-safe than Java, similar FP capabilities to Haskell. 
To add to this, there are also the awesome *lens* library to access and "mutate" state. [Examples](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html)
This is an awesome point-by-point post. Thank you for all this info!
Seems like /u/Ramin_HAL9001 and /u/bheklilr are here to save our days.
It might be a bit too complicated to wrap my head around, but I'll give it a look.
There's some extant examples you can look at out there. Look at the [Game section of Hackage](http://hackage.haskell.org/packages/archive/pkg-list.html#cat:game), `cabal unpack` the interesting-looking package, and see what you can find. That said, while what you may find may "work", there's no particular guarantee it will do anything more than that, specifically, no guarantee it'll be idiomatic, efficient, or elegant. But in general, there's no particular reason that normal functional programming breaks down here. Mutation is no more required here than anywhere else. Usually, games have a core loop in them, and where your imperative core loop may be: * receive input * mutate world in response to input * update user's display in response to mutated world the functional loop of * receive input * construct an updated world in response to input * update display with new version of world is not fundamentally impossible or anything. Since I'd suggest that only the first and third step be in IO and the middle step be pure, it is true that the pure approach is a bit more hostile to just tossing mutation in where ever, and it will push you to have a cleaner loop from day one, but I call that a plus anyhow. It is also true that Roguelikes can encounter certain performance problems due to their intrinsic O(n\^2) space requirements as the playing field size increases, and it is true that you may need to deal with that. One possibility is to do what MazesOfMonad does, and use a Data.Map mapping from (Int, Int) to a Cell. When you update a Cell, you get a new Data.Map that is mostly recovered from the original, and many Roguelikes are only updating a small number of cells per game loop cycle. (See src/MoresmauJP/Maze1/Maze.hs and surrounding files.) If you need to update many cells at once, there may or may not be an efficiency gain to be had in constructing a new Map with the newly-overridden cells, then calling `Data.Map.union` on the two maps to get the new one. (The Hackage documentation also suggests you may want `Data.Map.unionWithKey` with a merge function that always takes the right-hand-side values; criterion could answer that concretely, which you can do once you have realistic data to feed it.) A yet-more complicated answer that may be yet faster is to create a Vector of one sort or another (there's a lot of nuances here depending on what you're looking for), and run a loop more or less like the following: * Get user input * In pure code, collect all the changes you wish to make into a list (or something) * In one shot in the ST monad, modify the vector with those changes * Update display with the new state I've been kicking around a Haskell-based idea for a Roguelike that I will almost certainly never write, but that's more-or-less how it would work. Additionally, it would break the level/world up into square tiles that live in a Vector, and what the engine would do is for each tile, query everything inside of it that might want to take an action for what the entity wants to do, collect that all into a list of desired changes, then resolve any conflicts that may emerge (two entities that want to move on to the same square, for instance), and finally commit the results to the mutable vector when all relevant information has been collected. If your world has some sort of "speed of light" or something such that you can strictly characterize how many of your "neighbor" tiles has to be examined before you know for certain that all relevant actions have been collected, you can even do quite a bit of this work concurrently. But that's pretty complicated. Honestly, the Data.Map approach will take you a long way with very reasonable performance if you're writing a relatively conventional Roguelike. It isn't perfect, as the type system won't help you with constraints like "a given map has a cell for every valid coordinate and all cell coordinates are valid" but just using hidden constructors and some carefully tested public interfaces can be adequate.
Aside from what Tekmo said; the depth of abstraction available to the programmer (while remaining type-correct) in Haskell not only makes you faster/more productive (once it is natural to you) but also makes your programs much *clearer* and *elegant* without magic. It's similar in intuitive beauty to the elegance of a mathematical proof.
&gt; But a game like the one I'm working on relies heavily on states or object oriented data. The problem is that you've already decided that your game should be implemented using OO.
This is exactly the problem that `pipes-concurrency` solves. It's 100% deadlock safe, lets you specify bounds on the channel size (and writers will respect it), and it even comes with a [game example](http://hackage.haskell.org/packages/archive/pipes-concurrency/1.1.0/doc/html/Control-Proxy-Concurrent-Tutorial.html) in the tutorial.
Well, stateful machines like games are a bit weirder to understand in Haskell because they are not as native. That is good, not bad: because you can create your own mechanics from scratch. Take a look at [this](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html). Even if you are new to haskell, you should be able to read between the lines and see that he does pretty cool things that would be much messier in a language that "supports" similar things natively(like when he uses `units.traversed.health` to modify the health of all units at once, as if it was a single attribute: try doing that on actual OOP).
Gripe mode: on. `(func &gt;&gt;= \predicate -&gt; if predicate then doSomething else return())` is `(func &gt;&gt;= flip when doSomething)`. `guard` uses `mzero`, which can result in different behaviour.
While related to the expression problem, I don't think it's fundamental. The expression problem requires that the code not be modified, while here, code modification is kinda the point.
 data Direction = North | South | East | West deriving (Eq, Show) &gt; This is essentially just declaring an Enum by any other name. data Direction = North | South | East | West deriving (Eq, Show, Enum) Fixed that for you. Now you are undeniably declaring an Enum, and not by another name :-)
In my experience, CPU emulation via QEMU etc is still far too slow (even on Xeon-class server machines) to be tolerable unless you're absolutely not willing to pay any money whatsoever for hardware - which is completely reasonable, FWIW. The ODROID is simply a lot snappier in my experience over a fast LAN. And it's about $100, so it wouldn't even be unreasonable to buy 5 of them and have a small build farm, if you were super-serious about GHC-on-ARM (I'm not paid to do this, so that person isn't me. Well, I'm serious. But I'm not doing that. :) Seeing as I have a multitude of ARM boards (of various power-levels and specifications,) I'm probably better off saving my frustrations for the real deal. This also means it's possible to get realistic performance numbers from my devices, when the time comes. That said, I believe there are some people doing this.
It's to keep the type as general as possible. If you prefer, just think of it as `Pipe p a b m r` Edit: I wish there were a good way to hide unused type variables, but type synonyms are unsatisfactory because they induce higher-rank types.
Correct. I knew I could have applied the `Enum` typeclass to `Direction`, but it wouldn't have helped anything in this example. Since `Enum` gives us `succ`, `pred`, `toEnum`, and `fromEnum` (and a few others), these functions wouldn't have helped out. I said it was "an Enum by any other name" because it was serving the purpose of an enumerated type.
You should definitely check out [Learn You a Haskell](http://learnyouahaskell.com/chapters) and [Real World Haskell](http://book.realworldhaskell.org/read/), in that order (assuming you haven't already). LYAH is a fantastic, easy to understand tutorial on the majority of Haskell concepts, from the very basic to the complex. RWH isn't as much fun to read, but it goes into more depth with certain topics, and even covers how to write your own C libraries for Haskell.
You're right, it should have been "mzero" instead of "return ()" (func &gt;&gt;= \predicate -&gt; if predicate then doSomething else mzero) is the same as (func &gt;&gt;= guard &gt;&gt; doSomething), and yes this assumes you are using MonadPlus.
http://web.jaguarpaw.co.uk/~tom/blog/posts/2013-03-29-how-to-read-pipes.html
Another way you can remember the type variable conventions is: * Both of the a's (i.e. `a'` and `a`) refer to the upstream interface * Both of the b's (i.e. `b'` and `b`) refer to the downstream interface * The prime denotes information flowing upstream. No prime means information flowing downstream. * Sometimes I replace `a` with `x` or `b` with `y` to emphasize that an interface is unused, but I don't want to restrict it with a type synonym.
I'd have to recommend against Learn You a Haskell, or at least buying it - maybe the web version's better. I've got the dead-tree version and there are things in there that are driving me up the wall.
What specifically?
The most recent one that springs to mind is that his quick sort implementation on p57 doesn't compile. quicksort :: (Ord a) =&gt; [a] -&gt; [a] quicksort [] = [] quicksort (x:xs) = let smallerOrEqual = [a | a &lt;- xs, a &lt;= x] larger = [a | a &lt;- xs, a &gt; x] in quicksort smallerOrEqual ++ [x] ++ quicksort larger Gives a parse error on L5 There are others, even before that point - like claiming that a command prints something as a string when it actually makes it into a string. Can't recall what page that one's on but it does, at least in the dead-tree version. Stuff like that. Makes it difficult to learn with when you can't place a reasonable degree of confidence in the book.
I just copy/pasted it and it works with GHC 7.4.2 and Hugs Version September 2006 (latest from the Ubuntu 12.10 repos). However, I would say that if you run across a typo in the book, just like if you find a bug in a library, tell the author. The web version very likely has more errors fixed, but it is definitely not unusual for a book to have an errata.
It's admittedly a little deranged, or maybe maximally deranged, but the first signature: wrap :: (Monad m, Proxy p) =&gt; p a' a b' b m r -&gt; p a' a b' (Maybe b) m s can be legally written for example thus: wrap :: (Monad m, Proxy p) =&gt; p a' a -- ↑ ↓ b' b m r -&gt; p a' a -- ↑ ↓ b' (Maybe b) m s or just wrap :: (Monad m, Proxy p) =&gt; p a' a b' b m r -&gt; p a' a b' (Maybe b) m s That is, if you break `p a' a b' b m r` after the `a`s -- the 'upstream interface' -- and then indent, then the b's, the 'downstream interface', can come below them. This yields a decent geometrical image if you mentally insert the arrows I write explicitly above -- the upward 'flow' is on the left; downward on the right. So here you can see immediately the `wrap` takes anything that sends `bs` downstream, and makes it something that sends `Maybe b`s downstream. I found the use of a and b and ' of no use -- as it obviously isn't if they are replaced with concrete types -- but managed to imprint the business by repeating this daft procedure a few times when lost. It doesn't help that Tekmo thinks of upstream as to the left in his discussions; his picture is fighting against this one a little.
I put upstream on the left to match the direction of the composition operator.
Do you know how the Continuation monad works? I would say that its the fundamental mindblowing concept behind all the pipe libraries. If you know Python, an analogy would be that pipe libs basically implement "yield statements" (so you can write producers and consumers without turning the control flow inside out) and "with blocks" (so you can deterministically allocate resources such as file handles automatically free them when you are done or when an exception occurs) as well as lots of combinators and other glue to make it fit together.
There's an analogy that doesn't require thinking in terms of continuations. If a pipe is a list of instructions, then pipe composition is just a cooperative zipping of two lists, where 'request' and 'respond' tell you whether to merge in elements from the left or right. Edit: I meant "merge", it's a cooperative "merge" of two lists, not a zip.
But if you leave the continuations out you sorta go back to enumerators/iteratees don't you? Anyway, if /u/darkotter is looking for cool things to learn then he surely should take a deeper look into continuations if he hasn't already :)
I didn't interpret it as objecting at all! :) I actually thought it was neat and upvoted you, too.
The only two pipe categories that are actually continuation categories are the `request` and `respond` categories, but those are not the ones most people use. If you just mean continuation in the sense that pipes are suspendable, then teaching free monads is better preparation for understanding pipes.
What is the problem with higher rank types? Are you just trying to keep things simple (less language extentions / simpler error messages, etc) or do they have some other limitation (type inference, etc)?
Now that I think about it some more, this problem isn't as bad as it used to be. My original issue with them was the scenario where people write functions of pipes represented using type synonyms, such as: f :: Consumer a m r -&gt; ... ... but now that `pipes` has the new extra categories those functions become increasingly rare so maybe I will try the quantified type synonym approach similar to what `conduit` does right now.
YES! Look for a new hyperdrive (pipes based HTTP backend) soon!
Are you sure you didn't mix tabs and spaces? For better or worse, whitespace is significant; `smallerOrEqual` and `larger` have to be aligned properly.
Also, Renzo is releasing a `pipes-attoparsec` based on `pipes-parse` in the next few days which you might be able to use.
Nice, this makes a lot sense and is nicely backwards compatible. But it does require an extension. Is there any policy on using extensions in base? I can imagine this making adoption of other compilers harder.
Isn't there something wrong with Haskell's composition module if we need packages like pipes-everything, conduit-everything, io-streams-everything, enumerator-everything? This feels like one of the bigger problems in our Haskell ecosystem nowadays. Core libraries initially written to solve the problem of lazy IO now boil up in our entire dependency tree and are tightly coupled with stuff that used to be solvable with 'pure' libraries. And now our parsing libraries need seven type variables! Also most pipe-like packages still seem to be in flux. Does anyone else see this as a problem? How can we solve this? 
there is no problem, its just we've converged onto several nice solutions that have different styles. 2-4 years ago, it was a mess, there were no nice solutions that are easy to use, just iteratees and friends. Pipes, conduits et al allow much more direct style code, and are more general than what came before. And frankly are easier to learn and use. 
That looks like *exactly the same way* of understanding the type variable conventions!
As mentioned elsewhere, it's `#ifdef`'d out on non-GHC compilers, so `base` should remain usable for others. Clients will also not necessarily need to enable `PolyKinds` or anything else in their own source files to write new `Category` instances, the generalization occurs automatically (although then the instances are, by definition, bound to compilers supporting kind polymorphism, because `Category` itself will need it.)
I definitely agree that continuations are very important, but actually, I feel like I know them quite well, certainly I am familiar with using Codensity to avoid quadratic blowup, the simple Cont monad and it's use for terminating continuations, and two different ways to implement delimited continuations, both the monadic version using prompts and some other stuff to fiddle the type system a bit, and the indexed version (using either the two parameter or index-core style of indexed monads). I understand the old version of pipes (which was analagous to a free monad transformer using a functor along the lines of `data InOut i o x = In (i -&gt; x) | Out o x`), but the newer version has all these proxies and seemingly subtly different ways of composing pipes etc., which lead me to think that I don't really understand it very much.
The fact that there are more packages solving the same problem isn't an issue for me. The fact that we need special composition packages to use pure libraries (like parsers) on top of it feels like a problem, because this will split the package ecosystem. &gt; Pipes, conduits et al allow much more direct style code Can you elaborate? Much more direct compared to what? Although it comes with a lot of problems, writing code on top of lazy IO (hGetContents, etc) is way more direct if you ask me.
Also, REPA is an accelerate-like API (shallow rather than deeply embedded, and more expressive) that works on multicore CPU's. 
compared with Iteratees , pipes and modern conduit are much easier to understand. Lazy io has problems and requires extreme care to actually make sense. If you've some got a mental model that makes it work for you, great. Conduit and Pipes solve a much more general problem, incremental streams with feedback messages. Any two libraries that solve a general problem and have *different* APIs will need different adaptor modules to hook up with another lib. This is fundamentally *a fact* in any programming languages i'm aware of existing. What alternative would you suggest. I don't really see those wrappers as worrisome fragmentation, i see it as "its really really easy to tie diverse haskell libs together", which is GREAT. is that more clear? 
Looking at what one can write with ConstraintKinds, I see some potential for Haskell-style type classes and Agda-style implicit parameters to eventually converge.
That's because they are the dual of values.
FWIW, Hugs is considered to be deprecated. It is way out of date and no longer maintained. You'd be better of switching to GHC and ghci.
The way I understand it, pipes and conduits solve the same problem in almost the same way but differ on the philosophy of speed (and possibly features) vs correctness. It is not very nice that this grows two or more different ecosystems. If we could have a compatibility layer for the stuff they do agree on or perhaps "pluggable backends" for something like a webserver or attoparsec, this would reduce duplication. Just like in Haskell where we don't see what the underlying CPU architecture is.
Good question. NSFW?
There's nothing really to look at yet. I meant to say that if John is playing with Haskell, it cannot be so clearly out of question for games.
concatMap (filter (/= 'm')) $ replicate 3 "fmap"
What are the benefthis type of pushback vs having an upstream that handles the pushback for you?
One thing I'm curious about is that a few weeks ago, you [said](http://www.reddit.com/r/haskell/comments/1eatzy/comparison_of_enumerator_iteratee_io_libraries/c9yp1ui) that it would be possible to implement zero-copy support using `pipes-parse` and that you would talk more about it until its release. How can this be done?
I never saw that before—that's incredible.
Least appropriate use of the NSFW flag I have ever seen.
typo: section 5.1, paragraph 1: &gt; ... This free **Abalian** group has the structure: Should be "Abelian".
Yes! I think that would be great. I really like Conduit's solution of having two sets of synonyms; `Source` and `Sink` are pipes with closed ends (like `Producer` and `Consumer` from `pipes` are currently) and `Producer` and `Consumer` have open ends but just hide unused type variables.
Yeah, I switched to optparse-applicative. It was quite easy and now it's testable.
Ah, OK. I haven't tried it myself.
5.2.3: moddels
Actually, `pipes` has been as fast as conduit ever since version 2.5. I don't mind there being a `pipes` to `conduit` compatibility layer. The only reason I haven't released it myself is that I don't want to have to maintain that library because I don't have control over the evolution of the `conduit` library.
Also, you can just use a simple State type. Basically, we can make the observation that using state is similar to having functions that take in the current state and return the new state bundled with the answer: type State s a = s -&gt; (a, s) Since Haskell people like to abstract away implementation details like this, we can make a couple of useful 'primitive' functions of this form: get :: State s s get s = (s, s) -- (), pronounced unit, is something like void put :: s -&gt; State s () put news olds = ((), news) modify :: (s -&gt; s) -&gt; State s () modify f s = ((), f s) We can also define a few nice helper functions ("combinators") to combine functions of that form, too: -- flatten out nested states into a single level join :: State s (State s a) -&gt; State s a join s nestedS = state s' where (state, s') = nestedS s map :: (a -&gt; b) -&gt; State s a -&gt; State s b map f stateA s = (s', f a) let (s', a) = stateA s return :: a -&gt; State s a return a s = (s, a) &gt;&gt;= :: State s a -&gt; (a -&gt; State s b) -&gt; State s b stateA &gt;&gt;= aStateB = join $ map aStateB stateA &gt;&gt; :: State s a -&gt; State s b -&gt; State s b stateA &gt;&gt; stateB = stateA &gt;&gt;= _ -&gt; stateB runState :: s -&gt; State s a runState s state = state s Actual implementations in Haskell make use a few obsfuscations to be able to create typeclass instances. In particular, in Control.Monad.State.Lazy, it's declared as newtype State s a = State { runState :: s -&gt; (a, s) } At any rate, using this isn't hard. For example, we can say something like fib :: State (Int, Int) Int fib = do -- do notation is just syntactic sugar for &gt;&gt;=, map and return (a, b) &lt;- get -- grab the last two fib numbers put (b, (a+b)) -- modify the state return (a) -- since put returns () as the answer, use return to wrap the real answer. Return a so repeated applications of this return don't return the n+1th or n+2th fib nthFib n = fib &gt;&gt; nthFib (n - 1) tenthFib = runState (0,1) (nthFib 10) 
There's been some uncertainty about the exact semantics and laws that "pipes/conduits/iteratees/wacky inflatable fun tunnels" should uphold are, so it's difficult to have ADT semantics and laws for them all to adhere to. To my understanding, `pipes` is the best example of laying out some of these laws, but in the process it generalized the semantics of "these things" significantly. There might also be a simplified semantics that exists but follows fewer laws and matches fewer categories. These might be a more appropriate target for common usage, but I just don't think those things are well enough defined quite yet.
I kind of agree; both the ecosystem and the language (as defined by GHC) is becoming too complex and messy for my personal tastes. (for example I still like to use list of characters for strings when performance does not matter, even though we have Text and ByteStrings and everything. I also aim for absolutely minimal dependencies where possible) But this is probably the nature of language adoption...
It requires no cooperation from upstream.
Hmmm, okay. I guess what I had in mind was that it could somehow be possible to have an operation which opens a file and makes a Producer of ByteStrings (or whatever), which if you then composed it with a Consumer of ByteStrings made from a network socket, it would somehow magically know to use `sendfile()`, but if you had added extra transformations in the pipeline in-between, it would revert to copying. I couldn't conceive of how this could be possible, without at the very least breaking the identity law of composition. Just to clarify, is something like this what you had in mind?
Honestly, the only issue is the package naming convention (mentioning the underlying streaming type in the name). The fact that streaming libraries show up everywhere in dependencies doesn't bother me any more than the fact that `bytestring` is a dependency of half of Hackage. Also, `pipes` is actually the best dependency in this regard because it has the smallest dependency graph by far (I could even shed `mmorph` as a dependency, to be honest). As for the seven type variables, that should improve if I get the quantified type synonyms to work as I mentioned in another comment in this thread. Honestly, they are all necessary, both for theoretical and pragmatic reasons. To drive this point home, something like `zoom` is completely unimplementable using any less than 7 type variables. No other streaming library can implement anything remotely close to `zoom`. Edit: Also, `pipes` is extensible, and therefore future-proof. The other streaming libraries have basically painted themselves into a very narrow niche and I wouldn't really trust them to be the foundation of Haskell's streaming ecosystem.
I thought pipes was both faster and more correct than conduit now?
Check out my [`implicit-params` ](http://hackage.haskell.org/package/implicit-params) package on Hackage.
No, that's not what I had in mind, but that's an interesting idea, too. I don't think that I would mind that substitution as long as it produced the same behavior.
There's a really easy way to understand three of the pipe categories. Take a look at the `ProxyFast` type for a second: data ProxyFast a' a b' b m r = Request a' (a -&gt; ProxyFast a' a b' b m r ) | Respond b (b' -&gt; ProxyFast a' a b' b m r ) | M (m (ProxyFast a' a b' b m r)) | Pure r Three pipe categories are just glorified ways to replace those constructors with functions. All that request composition does is traverse the downstream proxy's free monad and replace all `Request` constructors with the upstream proxy. All that that respond composition does is traverse the upstream proxy's free monad and replace all `Respond` constructors with the upstream proxy. So what's the third substitution category? The free monad's kleisli category! It traverses the proxy and replaces all `Pure` constructors with the sequenced free monad.
Talking about "x times faster" when there's a time complexity change isn't really professional. In that regard, HLearn is in fact a billion times faster than Weka (for a certain n). 
I am becoming more and more irritated by all of the jokes about the "boob" operator. If I was ignorant of the societal context I would consider it just fine, but we are acting in a community that is already unwelcoming enough to women without this childish sniggering.
This library desperately needs something between a long tutorial and a book. I feel that most of Gabriel's efforts at documenting it so far haven't started with small concrete examples, or moved on to discuss particular problems that these libraries avoid and why they're useful. Instead, I get trapped under the ice of free monads and sevenfold parametric polymorphism, which I am frankly not going to make the effort to try to escape. And I'm supposedly an expert! Frankly, the current state of things feels to me like a disservice to the community: we're warned away from the bogeyman of lazy I/O, but presented with the impossibly baroque and intricate alternatives of conduits and pipes as if the only answers are to either embrace category theory or go back to Go. Not good.
Probably, but since Weka has no interfaces to let authors write algebraic operations for their models, it would take quite a bit of work to make it happen. Much of the code base would have to be restructured.
&gt; we're warned away from the bogeyman of lazy I/O, but presented with the impossibly baroque and intricate alternatives of conduits and pipes as if the only answers are to either embrace category theory or go back to Go. Not good. That's why there is [io-streams](http://hackage.haskell.org/package/io-streams).
Could it be done incrementally? Like, introduce "bool supportsOperationX()" for models, implement the operation for certain popular models one-by-one, and use it in learning algorithms whenever it is supported? (I haven't looked into Weka code for a long time, I don't know)
[Is this better?](http://hackage.haskell.org/packages/archive/pipes/3.3.0/doc/html/Control-Proxy-Tutorial.html)
Yeah. I'm pretty sure that's how they've added extensions to algorithms in the past.
The issue with simplifying the semantics is that the meaning of all four proxy categories are very deeply intertwined with each other. You can't define the semantics of any one category in isolation. This is in fact how I discovered all four categories, because I couldn't fully describe the behavior of just one category. This is true not only for laws and semantics, but for implementation purposes, too: most interesting features cannot be implemented with only one proxy category. For example, try implementing `fmapPull` without using `request` or `respond` composition.
That's because you read my blog posts and not the tutorials. Every tutorial has nothing but small and concrete examples. Even the main `pipes` tutorial does exactly what you describe: * It describes the problems that `pipes` solves * Presents several short and concrete examples Plus, it is the longest Haskell tutorial in existence. Not the best (I think the `pipes-2.1` tutorial was probably better), but I don't think the answer is making the tutorial even longer. Instead I should focus on making it clearer and simpler.
&gt; Instead I should focus on making it clearer and simpler. I kind of agree with this. It's not impenetrable by any means, but beneath its length I can sense a more concise — or at least more direct — introduction struggling to emerge. Perhaps I'm just a bit dim, but example code that employs constructions prior to definition ties my brain into knots …
Sure, but since it's all composed proxies I can always compose with an upstream that just handles pushback, and then the original upstream still doesn't have to care.
Part of the problem was that at the time I came out with a lot of the more advanced concepts (like monad morphisms, proxy morphisms, proxy transformers, and bidirectionality) one of the criticisms of the library was that there were too many mini-tutorials, so I went to the opposite extreme and condensed everything into one very large tutorial, which was probably just as big of a mistake. I think the main tutorial outline should probably go something like: * Brief introduction with one motivating example * Explain the unidirectional pipe type * Explain pull composition * Now a lot more examples of unidirectional pipes code * Extensions (but shorter than the current example, just briefly showing how to use them without going into a long discussion about their semantics or showing off their fancier features) * ListT (Producers only) * &lt;Warning to reader that everything after this point is advanced&gt; * Bidirectionality (brief description and reference a larger tutorial in another module) * Proxy transformers (brief description and reference a larger tutorial) * Monad morphisms and proxy morphisms (brief description and reference a larger tutorial)
What's the benefit of factoring out pushback into a separate proxy? Also, how would you manage multiple upstream leftovers buffers that way?
How would the pushback interface coexist with the request interface? What types did you have in mind?
Cabal [actually allows you to delegate the work of building the package to `make`](http://www.haskell.org/cabal/users-guide/developing-packages.html#more-complex-packages). It's called `build-type: Make`.
To my knowledge of `pipes` and Proxys I agree with that completely. I suppose I just don't know if I can rule out the existence of a simpler, less powerful ADT semantics that Proxys, Conduits, and Iterators could all instantiate. I generally agree with you philosophically in the power of the pursuit of a deepest unifying theory, but I can't say less comprehensive ones fail to exist. That said, given the need to generally handle things like leftovers and the like, it's hard for me to believe that if such a thing existed it would probably not be useful for many, many important uses.
Up until recently I hadn't dug into HLearn and felt that while it was making a great marriage between stats and algebra, I assumed it was stuff well inside of the community's folk knowledge better expressed with Haskell. Once I'd learned what a Monoid is, it was "obvious" to me that distributions formed one—they inherit it from regular multiplication. Apparently I was quite wrong. This is amazing. Keep beating Weka, Michael, please! I'm going to take a deep dive into HLearn at my next opportunity.
The relevant product for probability distributions is actually convolution. But granted, you can view this as multiplication, for instance on the level of generating functions.
I believe the `StateVar` package has a different objective than the `var` package. I believe `StateVar` provides more fine-grained type classes and some syntactic sugar. `var` provides unboxed variables (and mutable boxed and unboxed tuples). `var` and is meant to mimic `array` in structure and overall feel, and provide the `array` equivalent of mutable tuples.
I'd have to dig up my code (I wrote this back when proxies came out in the first place, which is most of where my interest comes from) -- IIRC I had a type: data Pushback a b = UnGet a | Get b and then the pushback proxy would pass through the `b` to the upstream and process the `UnGet a` itself.
[orly](http://therealkatie.net/blog/2012/mar/21/lighten-up/)
What would the response type be? How would you enforce that every `UnGet` returned nothing and every `Get` returned something?
There's a lot of things I don't understand yet about lenses and about your work, but your "S" made me think of the Cartesian Store Comonad in the paper [ Functor is to Lens as Applicative is to Biplate: Introducing Multiplate](http://arxiv.org/abs/1103.2841).
Interesting post! By the way, your blog doesn't seem to render that readably on mobile devices. 
Hmm, the copy pasted version works for me too. That's odd. ... Nevermind, I've figured it out, it doesn't matter whether you've aligned your variables into a column, as the book states, it matters that your variables are the same number of *characters* from the left as the variable above them - no more, no less. So, me sticking them all into a column by tabbing in kinda kills it. That's... horrible. Did they ever give a rationale as to why they'd set things up like that?
Most people don't mix tabs and spaces, and I prefer to use only spaces as it resolves problems like this. I think the rationale is the same as with Python and other similar languages that enforce whitespace. Well formatted code should have consistent whitespace anyway, and all editors support automatic whitespace. You can use curly braces and semicolons if you'd rather, though. For example, you could write code like main :: IO () main = do { input &lt;- getLine; putStrLn $ "You said: " ++ input; putStrLn "Now I'm going to use a different level of indention"; putStrLn "It is pretty flexible"; } And it will still compile.
Note on presentation, mathml support is essentially non existant on non-firefox browsers. You might want to use MathJax or just include an image at least on most browsers. 
except that asymptotic improvements don't imply *any* improvements *for any n* computable on any particular computer (theoretically inferior algorithms are used all the time for this reason). It isn't just an asymptotic improvement, it is a complexity improvement that manifests on real sized problems (and thus the title is reasonable). 
Distributions do because of the method of moments, so any model based on distributions (e.g. Bayesian classifiers) also do. Also, anything that has both an online and a parallel version probably is a monoid hom. My general approach for converting a model into a HomTrainer instance is to lookup papers discussion online and parallel algorithms, then see what they have in common. Edit: Also, in the ICML paper there is a technique on using "Free HomTrainers" to give algebraic structure to any model.
When talking about loops in Haskell, the "Essence of the Iterator Pattern" paper is essential.
Can you speak to compatibility between `Control.Lens.zoom` and `Control.Proxy.Parse.zoom`? It needs a type family instance for `Zoomed` at least... That all feels quite hairy.
Under what circumstances is something like this useful?
"There's a lot of things I don't understand yet about lenses and about your work" I just began to document this work, could you help me which part is hard to understand? I appreciate every feedback! "your "S" made me think of the Cartesian Store Comonad" I guess this is because I use the store comonad in the implementation. This is natural because I use the store-comonad-based representation of lenses. Maybe I should refer to "Functor is to Lens as Applicative is to Biplate" (but at the definition of lenses, not at the definition of S). 
No need for dependent types. Why don't you define data Resp = Resp1 | Resp2 | ... | RespN and convert your string to that before returning? Then the type would be getInput :: String -&gt; [ String ] -&gt; IO Resp 
The problem is that your function is "stringly typed". Create a data type for your options and return that instead. For example: data Option = Foo | Bar getInput :: String -&gt; [(String, Option)] -&gt; IO Option getInput prompt options = do putStrLn prompt resp &lt;- getLine case lookup resp options of Just option -&gt; return option Nothing -&gt; getInput "Invalid option. please choose again." options Now you can safely pattern match on the `Option` type in the rest of your program.
foldl doesn't fuse, as the base library uses build/foldr fusion only. Try using the `vector` package -- you'll have much better results. Generally you're not going to be doing numerics on linked lists anyway, so the point is rather moot. Just use vector.
the discussed article does use vector.unboxed. the list version is a (very slow) baseline.
see also the recent blog presentation https://twdkz.wordpress.com/2013/05/31/data-analysis-with-monoids/
Loop fusion isn't cheating.
Everyone gave very good comments that I agree very much, so I want to just add one thing to this in somewhat different point of view. Embrace haskell IO! Haskell IO is one of the best imperative state machine available, good separation of side effects, multiple level of concurrency and parallel operation support and most importantly, it's Monad! very good abstraction of stateful operation (and others, of course) Many tutorial, textbook, blog articles and etc on haskell emphasize functional design, so they first value pure functional approach, rather de-emphasize stateful approach and IO. That's for the purpose of triggering some kind of unlearning, which is necessary for widening programmer's view point and skills. One unfortunate side effect is that often outsiders tend to think stateful operation in haskell is very difficult, awkward and need to be suppressed, which is not necessarily true. I found that my friend concluded that haskell was not practical since he needed stateful operations after reading many articles and comments about haskell, and I needed to make him 'unlearn' about it again. Take what's available in the language and experiment many different options and get the reasons why common wisdom says so. Haskell is capable of making you express whatever you want in a simpler and better way! p.s. Admittedly, one quite messy part of haskell IO is lazy IO. But again, there are good ways to deal with it, especially recent iteratee and following advance. Ironically, this lazy IO problem teaches us to get to a better approach to real world problem in functional programming. 
Can anyone recommend a good article on this fusion concept? I haven't encountered it before. 
This is how people can do it manually http://www.randomhacks.net/articles/2007/02/10/map-fusion-and-haskell-performance of course you'd want the automatic approach.
There is still a loop after the loop fusion, isn't it?
If you want to block format you code, at least when posting a comment, you can indent it four spaces. Then it will be formated like hammar's or tomejaguar replies.
Weird, it looks just fine on the iPad's Safari. Which makes me think that normal Safari and Chrome also support it. Or maybe the OP saw your comment and changed it before now. 
It's all `jmp`s and machine code in the end, but that's not a useful perspective, even entirely ignoring Haskell. Indeed, even with a strictly C perspective, the C for loop is "no longer present" in the resulting compiled code; what literally comes out is more akin to C's goto than a loop. What matters is what a thing does. Things that can fuse are fundamentally different from things that can't fuse. The fact that they may all get compiled down to the same machine code is an incidental detail. Similarly, Haskell is a language heavy on immutability. It is implemented on machines heavily based on mutation. There's no contradiction there.
Safari (5+) has it, but chrome doesn't right now. Security issues disabled it, I think.
As dons says, it is that `foldl`s don't fuse with the standard ghc optimizations, but do well with the opposing stream fusion optimization scheme. It may help to see that the question 'list vs. vector' irrelevant in this case, since neither is actually built. Munging things arbitrarily to get uniformity: import qualified Data.Vector.Unboxed as V import qualified Data.List.Stream as S -- cabal install stream-fusion problemS = sqrt . S.sum . S.map (\x -&gt; x*x) . S.scanl (+) 0.0 . (`S.replicate` (1.0::Double)) problemV = sqrt . V.sum . V.map (\x -&gt; x*x) . V.scanl (+) 0.0 . (`V.replicate` (1.0::Double)) -- main = putStrLn "List" &gt;&gt; print (problemS 1000000000) main = putStrLn "Vector" &gt;&gt; print (problemV 1000000000) The functions marked with `S.` all have the types of the usual Data.List operations. Toggling between `main`s gives me (with `ghc -O2`): $ time ./forloop Vector 1.82574185971992e13 real 0m3.446s ... $ time ./forloop List 1.82574185971992e13 real 0m3.368s 
It will have different semantics. We may call the result fusion in both cases, but it will still be different. But that's not really the point I was trying to make. The point is that if you're willing to talk about a loop in C _at all_, you're obviously willing to agree that there can be a higher level construct that doesn't match the lower-level implementation, because there's no such thing as a loop in assembly. (Or at least not in any assembler I know; the fact that there is at least one such assembly language that C can compile to is sufficient for my point.) If there can be a loop in C, there can be a foldl in Haskell, and they don't have to be the same thing, even if in the end they must all end up assembler of one sort or another. If they aren't the same thing, it's no surprise they may act differently. (Arguably, that's the only useful definition of "not the same thing" in a domain covered by the Church-Turing thesis; what something "really" is, somehow independent of what it does, turns out to be far more slippery than it may first appear.)
http://www.mathemainzel.info/files/x86asmref.html#loop Now you are aware of at least one :)
Ah, that's much better - thanks! :)
in a way it is, but the linear model of stack overflow is not as nice to discuss problems. maybe /r/haskellquestions should be more active...
I'm fine with it being on reddit.
The short answer is yes.
Very Jealous. Won't be but since you're going to WWDC, can I ask have you had a look at reactive cocoa? FRP in general? 
Adding functionality is not refactoring. Still, I understand the problem although I have never come across it in practice. Do you have an explicit example? It sounds like you're doing too much pattern matching and not using intermediate functions enough.
Huh, that's pretty cool, thanks!
And ensure it is preceeded and followed by a blank line.
Not to impugn hammar, but if you don't know, it's a [reasonably well-established term](http://c2.com/cgi/wiki?StringlyTyped) (link intended as example, not proof; consult the Googles for more) that ever serious software engineer should be aware of as a serious antipattern in all languages.
I might have exagerated. Safari does have mathML support, and chrome has (afaik) the code to do it, it is just not currently enabled. But, although Safari's mathML support is better than chromes (or IE) for sure, but it is still pretty broken. Try rendering this page in both current ish safari and current ish firefox and you will see the difference--safari's spacing and sizing is all messed up by comparison and has some weird aliasing. IMO, mathml is readable in safari 6, but it kinda looks ugly.... MathJax is the current solution to this problem (and has been for years)--it requires enabling JS, but is otherwise universal across browsers. Even were the browser support problems to go away, since MathML is not generally known by mathematicians, MathJax or something similar would likely continue to be used for most professional math blogs/discussion sites for its latex support.
&gt; Aren't with blocks the same as haskell brackets? Sure, but if you pay too much attention to that then the analogy breaks :) The main point is that the pipe libs try to make it easy to allocate a resource (say, a file handle) and deterministically get rid of it as soon as possible (and in an exception-safe manner that doesn't necessarily happen if you write your brackets by hand without paying attention).
Is adding an if expression easier in another language?
Yes. For example, if you have inheritance, you could create a new _thing_ that behaves the same way and adds behavior without changing it on the old _thing_. Because you can easily do this, you'd design it in a more modular way. This particular frustration would not be a problem in Java, Go (probably), MLPolyR, or Ocaml (using polymorphic variants). In those languages, such a change doesn't require a refactoring, while in Haskell it does (unless you use one of the attempts to solve the expression problem, none of which is particularly appealing). [This](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.205.3123) paper talks about the different trade-offs e.g. Java makes from Haskell and how they relate to this and to the expression problem.
This *seems* like a bad way of doing things. I'm not very familiar with multithreaded haskell (or haskell at all, I'm dabbling at this point), but one of the main advantages of fp imho is referential transparency, and I do not consider blocking code (ie deadlock prone/capable) as RT. Does haskell have atomic compare and swap operations? In an attempt to write pure, RT multithreaded Scala code I've had a lot of success with using the State monad to operate on immutable structures (which give you transactional semantics) and then combining the application/run of the Monad with an atomic compare and swap operation to ensure both your actions along with any other concurrent operations on shared data structures succeed. This allows you to write most of the game logic in State, rather than IO, and only go to IO when you need actual IO. 
If you try to execute the last runnable example, this is what you get: Code failed to compile Compilation was terminated External exception: Server killed. Local exception: user error (too few bytes. Failed reading at byte position 1) It's just me?
When you add a new class that inherits from an old class you have to add a whole bunch of new implementation to that class. When you add a new constructor you have to add a small amount of functionality in a lot of places. Are you sure that latter is more work?
I'll gladly hang out with you. I'm free this weekend. I'll bet Jonathan Fischoff will be interested, too.
Yes. When you add a new class, you have to add a small amount of new functionality that never touches the old code; when you add a new constructor, you have to change the code everywhere it was used before. You add the same amount of new functionality / new code, but in the latter you also have to change existing code. Furthermore, the non-locality of the changes makes it more conceptually difficult, and the lack of tooling for Haskell does nothing to mitigate this.
hmm... good point. am exhausted fighting CSS though. Will try once batteries are a bit recharged :)
Yes. 
I don't know why, Tekmo, but I've always thought you were in Europe :)
Well, I seem to have solved this by specialising my [putLookup](https://github.com/perurbis/hfreeagent/blob/master/src/Web/FreeAgent/Types.hs#L108) function to only do anything with Objects and having [parseData](https://github.com/perurbis/hfreeagent/blob/master/src/Web/FreeAgent/Types.hs#L82) recurse into Objects and Arrays. Any comments still welcome though!
&gt; impossibly baroque and intricate alternatives of conduits and pipes as if the only answers are to either embrace category theory or go back to Go. That's a funny comment on conduit, considering the biggest criticism I've seen of it is that it *doesn't* embrace category theory, instead choosing to optimize for simplicity in common real-world use cases. It looks to me like the complexity in pipes is painting your perception of conduit as well. I'm obviously biased in this, but I'd recommend looking at conduit on its own, perhaps with the [School of Haskell tutorial](https://www.fpcomplete.com/user/snoyberg/library-documentation/conduit-overview).
Good bet, but I'll be with my son this weekend, for my last bit of paternity leave ;). Some other time for sure.
Do you really need all those strict bytestrings in there? Why not Text?
Why not Zoidberg?
fpcomplete.com/feed Sorry this wasn't easier to find. I'll get the ball rolling to make this visible from the blog page.
You may well have heard about these already, but... I recommend the index-core package to use for indexed monads (such as the one you have written here) in Haskell. I did have a go at writing one myself a while back, worth trying and not too difficult. And it gives some interesting effects with e.g. `foreverR` in the index-core package. Also, you can have a non-indexed delimited continuation monad if you cheat the type system a bit. Have a look at all the CC-delcont packages for that.
I might be able to come hang out with you guys, too. :)
It was in /r/programming. I don't know if you can crosspost an already submitted one. This is nice code. I am a wee Haskeller and it was very understandable.
Cool! Shall we combine it with dinner? Can you suggest any nice places where it's not too noisy to talk?
I don't eat out very often, but we can go to the place where John and I usually meet, which is La Taqueria: https://plus.google.com/115535773494508576119/about?gl=us&amp;hl=en Anybody else is welcome to come!
[Relevant Stack Overflow question and answer](http://stackoverflow.com/questions/14014683/how-to-implement-index-core-style-indexed-continuation-monad/14022777#14022777)
This blog keeps getting better. I'm learning something new from every post.
I'm in too.
Wadler actually talks about a whole family of indexed monads for composable continuations in [this paper](http://homepages.inf.ed.ac.uk/wadler/papers/composable/composable.ps).
That's a pretty awesome project. Like a tiny part of the ripple.com system, just for Bitcoin :)
Please nerd snipe remind me about this idea in a month or so. Exploring that will complement some of the other things ill be doing then. 
I'm the principal mentor on this project. The plan is to write a Haskell backend for swig using the swig meta data and associated machinery / tool chain. The first step goal is to have some semi automated general tool that could be used to make at least small c++ ffi bindings (like the snappy and double-conversion bindings by Bos ) super easy to write. After we get something to work for the simple / semi automated general case, we'll start working on a more fully automated specialized to qt sub tool using a lot of ideas from prior researchy experimentation in that space. Either way, it's a strong student and a high value project / goal for the community, and I'm excited/horrified to be in the position of mentoring this project. I suspect the project is going to succeed, and I'm going do my best to ensure it does! :-)
Nitpick: `Char8` is all of Latin-1.
Yeah. How about everybody meets there at 12 on Sunday?
Awesome. Does 12 on Sunday sound okay to you?
Hsenv all the way down!
&gt; Is compiling GHC from source going to produce a more efficient version than the version from the binary packages? I doubt it will make a meaningful difference.
The only upside of building from source is you can change the source and rebuild it again :-) Unless you're writing patches for ghc, or a distro maintainer, or you're doing some exotic thing I can't think of at 3am (well, aside from building a cross compiler build) Stick with the binaries. :-)
Indeed. The ghc prebuild binaries are heavily optimized. A local build will not be better optimize unless you do some pretty exotic work. 
The thing is I am on i686, but I have downloaded and running i386 build (since there is no i686 version). Is it ok?
... so it solves Bitcoin's problems by providing a central server?
Yes.
"Sell not virtue to purchase wealth, nor Liberty to purchase power." B.F.
hopefully w/o the scam part...
Building from source is great if you are tracking GHC HEAD or using it on a lesser supported architecture, otherwise it's pointless.
I understand that. However, decentralization is the key idea behind Bitcoin, and the hashing (or some other non-instantaneous proof of work) is a necessary consequence of this. Bittoll seems like being a bank you bring your Bitcoin cash to, I'm not sure this is a good idea (idealistically, at least). Another thing worth noting is that Bitcoin transactions are already pretty fast, but that depends on how much you trust your partner to not double-spend the coin, kind of like how you count the money when someone gives you cash. Bittoll solves this issue by attempting to provide a central authority, but in the process it eliminates *all* the benefits of having Bitcoin in the first place. Then there is the issue of "many small transactions", which is already possible in the current Bitcoin network: A tx in the Bitcoin sense consists of multiple inputs and outputs; the outputs are normally the one recipient of the transaction and a return address. However, there is no reason you can't use multiple outgoing recipients, so unless timing is crucial - i.e. you're trading on a sub-second scale - you can send "one cent per email" every couple of seconds, packed together in a single transaction.
"Sell not virtue to purchase wealth, nor Liberty to purchase power." B.F.
I think you are confusing me with Edward :)
Another nice use for `ScopedTypeVariables` is giving type signatures in a type class instance. For example: instance SomeClass f a where someMethod bla = g (Proxy :: Proxy a) bla
I don't know if I can make as early as 12 because I need to pick up my WWDC badge on Sunday morning. I haven't seen any news on what time Moscone opens on Sunday for this; when I do I'll post back here asap.
I think you may need to rethink your definition of "scam". Bittoll also involves counterparty risk.
Hell I'd take a job writing web apps if I could use Haskell...
Fay uses a strict subset of Haskell syntax and compiles to JS. You'd be the only one who understands the code, but it could work for freelance jobs. 
Do you have an alternate proposal for cons? If we used : for type declarations, what should we use for cons instead of ::?
Apparently they are part of Facebook now: http://spaceport.io/spaceport_joins_facebook Just wondering what will happen in terms of existing codebase, and if they will need to move away from Haskell.
This is really good article. 
Are people using Fay in production anywhere?
I doubt it will be a problem, Facebook is already using Haskell as a metaprogramming tool to apply transformations to its PHP codebase.
Haskell gives you more choice in how to model your game world. Too much choice makes things counter-intuitive. I prefer modeling the world as plain data with a function describing state transitions. worlds :: [[Event]] -&gt; [World] worlds input = scanl step initialWorld input where step :: World -&gt; [Event] -&gt; World step world events = execMiracle (processFrame events) world processFrame :: [Event] -&gt; Miracle () processFrame events = do mapM_ handleWorldEvent events handleHelmActionEvents events updateWorld The miracle thing is a state monad. World change = miracle... whatever. newtype Miracle a = Miracle { unwrapMiracle :: State World a } deriving (Monad, MonadState World) execMiracle = execState . unwrapMiracle Essentially, the world is a pure function of initial state and external input. I haven't muddied anything with IO yet. I use Control.Concurrent channels to give the pure world a stream interface, then that stream is fed and consumed in IO. Example (having some cruft): main :: IO () main = do controlChan &lt;- newChan controls &lt;- getChanContents controlChan (_, _) &lt;- getArgsAndInitialize window &lt;- initWindow "Main" dt &lt;- return 10 drifterShape &lt;- newCubeShape env &lt;- newIORef Environment { runState = Run, timeNext = 0, timeStep = dt, writeEvent = writeChan controlChan, window = window, drifterShape = drifterShape, model = worlds (parseMaybe controls), attachedEntity = Just 0, -- attach first thing.baeuthntuhe nearClip = 0.5, farClip = 50, aspectRatio = 1.2 } displayCallback $= displayProc env idleCallback $= Just (idleProc env) keyboardMouseCallback $= Just (keyboardMouseProc env) passiveMotionCallback $= Just (passiveMotionProc env) reshapeCallback $= Just (reshapeProc env) setRunState env Run mainLoop tl;dr - IMHO, **an interactive game has type InitialWorld -&gt; [Input] -&gt; [World]**. I model it that way, as a [difference equation](http://en.wikipedia.org/wiki/Difference_equation), and make IO a separate concern. 
Good point, it makes it possible for everyone to open his own bank easily. (By the way, Bitcoin already has taxes bult in: each transaction includes a tip for the miner. Leave away the tip and your transaction will take a long time to be confirmed.)
That's worse than C code because it is less composable. All C functions are IO but at least C doesn't force IO in the type of conceptually pure functions. IO is like heaven. You must go there eventually but once you do you can never come back (save for a dirty deal with unSATANperformIO). You can never come baaaaaaaaaaaaack........
Actually people *love* being teased :-)
Fay is used in production at FP Complete.
I recomend just doing cabal-dev install. That way you dodge the dependency problems. If you just want a node to play with you can do make test Which will bring up a test node. Also note that it is written for ghc 7.6 and won't compile on 7.4 AFAIK
&gt; After working with a roughly 10,000-line codebase for several weeks, I’m now convinced that Haskell is excellent for coding in the large. Ten thousand lines doesn't sound big to me - more like the low end of medium. I have personal projects that are significantly larger than that. I have a single cpp file with nearly 8000 lines, and more than 3000 in the header file, and that's just one library - admittedly that's just the line count in the margin of the text editor, so you can knock off 25% just deleting blank lines, but still 10,000 lines for a project just isn't that big. &gt; A Haskell codebase seems to remain readily comprehensible well beyond the size where, say, a C++ codebase starts becoming unwieldy; it’s maybe a 2:1 ratio. At present for me, it's somewhere between 5:1 and 20:1 in favor of obsolete-style C++, the reason being experience. I can still read Modula 2 and Ada more easily than Haskell - I haven't used either in anger for about 15 years, but they're not that different to C++ anyway. I want to believe that Haskell is better, and I have some reason to believe it, but I have some reason not to which I'll get to shortly. &gt; LOC is funny in that it’s simultaneously an important metric and a completely useless one. Actually no, LOC is the *only* metric I know of that's *proven* important. A lot of metrics that are claimed to matter are strongly correlated with total SLOC for a project and, once you control for total SLOC, the benefit from all those other metrics disappears completely. For example, the average size of your methods is predicted by the total SLOC of your project, and only predicts the number of errors because the total SLOC predicts the number of errors. Past studies that concluded that the size of a method was important hadn't controlled for that correlation with total SLOC and reached incorrect conclusions as a result. So if it seems natural to write a long method, go ahead - if breaking it up means you have more SLOC (extra lines to call the fragments, for example) you've not merely moved complexity into the call graph (which doesn't eliminate it) but also added extra complexity too. The supposed cure is worse than the disease because the supposed disease was a misdiagnosis. &gt; Language designers tend to be in agreement, more or less, that syntax matters—that good syntax does wonders for the effectiveness of language as a tool of thought. Naturally, they disagree about what actually constitutes “good syntax”. And as language designers, they are of course biassed about the value of language features anyway. If language features don't add value, what's the point in expending effort designing them. Actually, programmer productivity and reliability depends on the length of the program independent of the language level. High level languages are efficient and effective to the extent that they reduce the number of SLOC to solve a particular problem. That doesn't mean language features are irrelevant, of course - if they had no benefit, they wouldn't reduce the number of SLOC to solve a problem. But it does mean that the idea that one paradigm or feature-set is better than another is suspect, unless it allows problems to be solved with fewer SLOC. Also, the (non-primary) source concludes that (performance issues permitting) you should use the highest level language you can. I say that's not quite right - it assumes you can rank all languages in a total order of how high level they are, and the highest level language will be the best choice (performance and perhaps other trade-offs ignored) for everything. I say that's not true - even assembler can yield the shortest solution to some special-case problems. When you have a low-level problem to solve, you may need extra SLOC in a higher level language just to break through those high level abstractions. You should use the most appropriate language you can for the problem, that being objectively measurable (though sadly not until after the work is done) as the one that solves the problem with the least SLOC. I think the ability to write embedded domain specific languages is relevant to that - allowing a language to be specialized for a wide variety of specific applications for a reasonable cost in SLOC. Haskell does well from that. I would prefer Haskell over ML or Scheme mainly because Haskell is the one I've been spending time seriously learning, but I'd rationalise it with claims of static typing and the IO monad. However, as I said, it seems that what makes the difference is purely the SLOC. Does adding static type annotations mean you end up with more SLOC total, or does it allow you to express meaning more efficiently? I think the latter, but I have no evidence. My claims about evidence are all from a single non-primary source - a video by Greg Wilson "What We Actually Know About Software Development, and Why We Believe It's True" which is all about empirical research and evidence. * Video [on Vimeo](http://vimeo.com/9270320) * The [Reddit link](http://www.reddit.com/r/programming/comments/myo42/what_we_actually_empirically_know_about_software/) where I last found it The relevant claims... * 22:20 - Productivity and reliability depends on the length of the program independent of the language level - SLOC/hour is roughly constant. * 39:05 - Can code metrics predict post-release fault rates? It was thought yes from the 80s, but all the metrics correlated strongly with total SLOC for the project. It turns out that all those correlations are entirely explained by the correlation between total SLOC and post-release fault rates. 
Elm is a depature from the normal web programming paradigm, it integrates less easily with existing web code. Fay is aimed at being a drop-in replacement for JavaScript, its output is self-contained, binding to other libraries via FFI is extremely trivial and it is light-weight.
With your recent hires, im hoping you are doing way more than just using haskell to refactor your PHP code.
This seems very similar, conceptually, to the main idea of functional reactive programming.
Yes! I know people have been working on bindings for both. I'm not sure if anything is published yet, but I've seen it working nicely :)
since all the popular js libraries are side-effecty, its unclear to me if a pure language is at all suitable for browser development in 2013
&gt; it integrates less easily with existing web code. I know, sorry. I meant embed. Changed it.
holy wall of text. In response to your first sentence, i think you drastically underestimate how much functionality 10k lines of haskell can pack.
You can encapsulate side effects with a state monad or FRP with arrows or something like that. That's how all Haskell bindings for stateful libraries (like openGL) do it. Seems to work perfectly fine.
What is wrong with side effects in Haskell?
A state monad is just a monad that can make sure all statements are evaluated in the right order. The `State` monad is such a monad with functions to alter the "state" of an argument that you pass with it. The `IO` monad is a state monad without such functions, but with functions that act on "the state of the world". That's just theory vs. implementation. Sorry for the confusion. But yeah, most library bindings implement their own monad just to keep things separate, which evaluates to an `IO` action.
Haskell is too impractical. Its syntax and forced type system are a nightmare. 
To be fair, it's easy to do. :) My current personal goal is to also become indistinguishable from you and ekmett.
Indeed. I stole the IO parts from "Haskell School of Expression". In the book Hudak builds an FRP world of pure functions and then uses channels to facilitate IO. The difference between this and FRP isn't in how I handle IO, it's in how I model state. FRP is closer to traditional OOP in that state is encapsulated in each entity. The entities are wired together so the nodes are stateful and the arcs are streams/lists. This is ideal for some problems, like creating DSP models of musical instruments (arrows can make FRP even cleaner and there is a paper somewhere about modeling DSP with arrows), but I don't like it for games. I prefer a global world database because I like to think in terms of entity component systems. In my ideal game, "x + dx", would mean "for all entities with x, add dx". Using union and intersection these operations can be pretty expressive. type Entity = Int data Component a = Component (M.IntMap a) (First a) deriving (Eq, Ord, Read, Show) An Applicative Component would provide the notation I like. Then I can think of my world state as a database and my transition functions as update queries with nice notation and real union/intersection queries to be specific about what needs to be updated. This is totally different from FRP. It's a World Database facilitating an Entity Component System. Here's some building on the above definition. Can't remember where I got the 'First a' trick or why I used it?! Something tricky about the Monoid instance. Can't recall, but "First a" would just be "a" without it. newtype First a = First (Maybe a) deriving (Eq, Ord, Read, Show, Functor, Applicative) instance Monoid (First a) where mempty = First Nothing mappend l@(First (Just _)) _ = l mappend (First Nothing) r = r instance Monoid (Component a) where mempty = constant Nothing mappend = appendC instance Functor Component where fmap f (Component m d) = Component (fmap f m) (fmap f d) instance Applicative Component where pure v = constant (Just v) (&lt;*&gt;) = liftC2 ($) instance Num a =&gt; Num (Component a) where (+) = liftC2 (+) (-) = liftC2 (-) (*) = liftC2 (*) abs = fmap abs signum = fmap signum fromInteger = pure . fromInteger instance Fractional a =&gt; Fractional (Component a) where fromRational = pure . fromRational empty :: Component a empty = mempty
I only have learning examples for Haskell. The largest is a 6502 cross assembler which stalled because I needed (but for some time didn't realise I needed) the existential types extension. Whether I really need containers to hold multiple different types and similar tricks is a relevant question, but I've spent a long time designing software with that option as a given - it's hard to give it up. 6502 cross assemblers are basically an old next-step-after-hello-world habit that I used to apply to every new language at one time, but that was a long time ago. It was always essentially the same simplistic design, mainly a way to get used to string processing, file handling and basic data structures. When I decided to do that again for Haskell, that wasn't the reasoning. My goal was (and still is) a much higher standard for issues like choosing between long-form and short-form instructions. Anyway, for scanning and parsing, a basic model of the instruction set, but nothing close to code generation yet I have roughly 2000 lines. Why so much? First off, an explicit choice to reinvent the wheel where I felt it would help me learn. For example, my own non-deterministic state monad and generators library rather than using parsec, the state monad transformer or whatever. Unfair? No. In the C++ comparison, for example, I have my own multiway-tree container library where I could use STL containers, my own automata library rather than finding an open source one, etc etc. So I have at least some awareness of the density of functionality that Haskell can pack, though I definitely have more learning to do. However, in work, ~~I don't believe I've ever worked in a project with fewer than a million SLOC~~. Sure, a lot of that's because CRUD code is the fastest growing plague known to humanity, but if you start writing CRUD in Haskell the same thing will happen. The badness in large projects isn't mostly because of the language or style choices, it's because of unclear and changing requirements, communications failures, panics in the run up to deadlines, employees who moved on but left their technical debt behind, etc etc. It's often because someone *had* the perfect abstraction once, but the requirements changed and holes had to be smashed through that abstraction layer to cope. **EDIT** - That's wrong - I forgot that I started out with embedded systems. Also, let me repeat... &gt;&gt; Productivity and reliability depends on the length of the program independent of the language level - SLOC/hour is roughly constant. So size as in SLOC *is* the relevant measure of size for developers. It seems to measure the complexity the programmer is actually dealing with. Also... &gt; I think the ability to write embedded domain specific languages is relevant to that - allowing a language to be specialized for a wide variety of specific applications for a reasonable cost in SLOC. ... Does adding static type annotations mean you end up with more SLOC total, or does it allow you to express meaning more efficiently? I think the latter, but I have no evidence. An embedded domain specific language (for non-deterministic state handling) is exactly what seems to make my incomplete 6502 assembler project compact. For that up-front fixed cost of defining the eDSL, I get to benefit over and over again from a more concise and specialized notation elsewhere. Reality-check, though - that up-front cost is still there in my case (by choice, I could have used a standard library to do the job). I used to throw away all those old 6502 assemblers as soon as I wrote them - the only one that survived is working, complete, and written for Sozobon C for the Atari ST. I used it to derive some unit tests. Total SLOC - certainly less than 2000 lines. The reason it's complete, yet the Haskell one is bigger? It's not because of libraries - I wrote pretty much everything myself for that old C version too. As I said, I set the bar higher for the Haskell version. On some of my projects being 10,000 lines or more, you assume maybe that that's what Hello World takes in C++? Wrong. Those projects are domain-specific programming language code generators, and substantially more complex than a 6502 cross assembler. My guesstimate so far is that Haskell would be a few times smaller, but not hugely smaller, and there's some issues with that. The thing is that calling a function takes about the same amount of code in most programming languages - it's all give-or-take the odd paren or comma. Once you've implemented a generalised version of Hopcrofts algorithm to minimize a finite automaton, each call is just a call, irrespective of the language. Basically, libraries provide abstractions. They reduce the SLOC in your project providing those libraries were written separately. In my case (whether for C++ or Haskell) they're written by me for the project so they're in the SLOC count. But even when you count implementing your libraries for yourself, well, there's two issues... 1. A function call is still just a function call, even when it's the next-lower-down level of abstraction. Just because you call a function to add an item to a container in Haskell rather than C++ doesn't mean you get a huge automatic saving in SLOC. 2. About Hopcrofts algorithm for minimizing finite automata - it's an algorithm. The steps and the order in which they are done is important. The use of mutable data structures is important. In Haskell, if you ignore that, you're implementing the wrong algorithm and you won't get the performance and space bounds you expect. In C++, I cheated a bit - I implemented my generalised variant on top of the wrong container for convenience, so I'm a log factor out on both the time and space bounds. It hasn't been a big deal so I haven't fixed it. I'm not sure I know *how* to implement it in Haskell (even cheating) because laziness is in the way - my understanding of how to apply explicit strictness is improving but I just don't have the experience of dealing with this issue that just doesn't exist in most languages. As I said before, sometimes a low level issue needs more SLOC in a higher level language because the high level abstractions are in the way - in this case, laziness probably isn't *all* bad even within the algorithm, but keeping laziness in control so you get the algorithm you want is still part of the complexity of implementing algorithms in Haskell. Anyway, libraries are the key abstraction mechanism in any language, each abstraction is built on the lower ones, and the SLOC for the implementation either counts towards your project or doesn't. If you use a standard library you avoid a significant up-front development cost - always do that unless you have a good reason. Despite what I said about function calls, Haskell seems very concise. Concise core syntax does make a difference. Laziness (which you may think I hate) makes a very big difference as (along with reasonable trust in the compiler) it allows a relatively clutter-and-duplication free experience in gluing libraries together. But I'm not yet convinced that the functionality/SLOC advantage in Haskell is as large as you seem to think. A library function call is still a library function call in any language. 
nothing it just means half of your program is in I/O which is hardly idiomatic and restricts your ability to write the code the way you see it in your head
Yeah, that's true. However, I still feel his point is valid, even if his anecdote does not specifically back it up just yet. Haskell does scale well for projects because of its type system and purity by default.
~~All monads must be commutative over composition.~~ If you'd use evaluation as an operator, sure, it's not commutative over evaluation. (Please don't use evaluation as an operator, though.) A state monad does this by making each State action (each statement) dependent on a result of the one before it, and makes it produce a result where the next action depends on. You don't see these values; they're hidden in the `(&gt;&gt;)` and the type. That's basically what a monad is, it's a pattern to hide plumbing. See also: http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html The exercise with the random number generator tricks you in implementing a State monad, without even telling you what a monad it.
But it need not be so! Streaming libraries like `pipes` and `conduit` let you separate your effects from your business logic very easily. This lets you write a pure core that you can reason about and even quick-check, but then hook up to streaming effectful inputs and outputs when running in production.
Thanks, fixed!
My post isn't necessarily anti-Haskell - it's me thinking things out and hoping others will tell me where I'm wrong. You can learn a lot playing devils advocate - but probably not how to make friends. Haskell is 10 times more compact? I'm not yet convinced, but I'm not unconvinced either. You don't even need 10 times more compact to make a huge difference, though. The SLOC thing could even be strong evidence of Haskell superiority, but my concern about the validity of a lot of comparisons can also be explained referring back to that video... * 29:55- If you increase the problem complexity 25%, you double the complexity of the solution. Quite a few Haskell success stories are fully functional but not feature-for-feature comparable to existing alternatives, so comparing SLOC is misleading. Add all those missing features and you get a non-linear increase in SLOC. That 25% and 100% is probably invalid (limited scope of what has been studied) but you don't know how many SLOC it would take to clone Apache in Haskell until you write a full feature-complete clone. 
I'm no expert, but it sounds like the `pipes` method of implementing Behaviors is essentially applying `stepper` to an Event steam, which is the usual way to generate Behaviors in `reactive-banana` (well that and `accumB`, but I think you can define that with `stepper`).
Alas, it's not that easy. Fay implements only a subset of Haskell. Notably, it does not (currently) implement type classes. This means that using `pipes` on the client side would involve entirely rewriting the library (and any of its dependencies) to accommodate Fay's limitations.
What does "commutative over composition" mean, and what does "evaluation as an operator" mean?
thinking linearly, like "Haskell is N times more compact", is wrong. As you mentioned: as the number of possible states increases linearly, the amount of code needed to deal with it increases superlinearly. FP provides the tools needed to reduce the amount of interacting state. So the bigger the codebase, the more you need FP.
The set of integers are commutative over the addition operation. 1 + 2 == 2 + 1, but they are not commutative over the subtraction operation, 1 - 2 /= 2 - 1. ~~The former is a monoid, the latter is not.~~ 
&gt; The former is a monoid, the latter is not. That's true but monoids deal with associativity and identity, not commutativity.
The cloud Haskell / distributed computing story for Haskell is still young, and has a lot less man power being applied to it than some of these other tools. So it's hard to compare on that front! It's also worth noting that in some respects cloud Haskell is just one point in the space of distributed computing library design, albeit a choice that will likely have the broadest reach in terms of being suitable for most nonexotic workloads Edit: also it'd be much easier to compare CH vs the alternatives if you had a particular example application or work load in mind. The space of distributed computations people care about is infinite in its variety and complexity, even though many applications winds up being architected somewhat similarly 
&gt; Ten thousand lines doesn't sound big to me - more like the low end of medium. If one of my Haskell modules is over 500 lines long, I'm unhappy. It often means that I've conflated two (or more) concepts, or that I'm doing it wrong. Your source is perhaps right on imperative languages, but pure functional languages are very different. For instance, the problem of buggy action at a distance is mentioned a few times, but since the nature of that problem is very different in Haskell (e.g. space leaks due to unwanted sharing), it plays a different role in Haskell. The rationale behind the interpretation of the data falls apart. Especially since the data probably doesn't include any code written in Miranda or Haskell.
There is also the [`indexed`](https://github.com/ekmett/indexed) package from Edward Kmett, but sadly work on this has stopped until GHC can express more type level programming, iirc.
You can write Fortran in any language. The fact that it takes you how much ever code to do something means exactly fuck all when you don't know what you're doing.
The second part of that was sort of an aside and it felt wrong while I was typing it. Thanks for pointing it out.
It's okay. I messed up, too. Terminology in math is so confusing...
&gt; I guess you could make a monad that is commutative. It wouldn't be very interesting, though. `Maybe` and `Reader a` are both commutative.
Eeh, I'm having a hard time assessing your knowledge. I suppose they are. 
At a glance from my phone the API looks to sit in the IO monad. I suppose you mutate the ref for Latest instead of something more pure. Is this a concession or is it inevitable?
You're trying to use Haskell like you would use Python, and obviously that doesn't go too well. The "normal" way people use Aeson is to write a few types to represent the data hierarchy, and have them implement ToJSON / FromJSON. Once you have that in place, you can (de)serialize these types directly to and from JSON, and Aeson does much of the error handling for you; if the deserializing succeeds, stepping through the data structure can be as simple as `putStrLn $ city $ address content`. Oh, and the Haskell version is probably still going to be longer than the Python one, because you have all the overhead of setting up the typeclass instances and all that; you'd win those back if the project were larger though.
It seems like you want to write a high level interface to aeson. Try starting with one of these libraries: http://hackage.haskell.org/package/aeson-lens http://hackage.haskell.org/package/lens-aeson Oliver Charles describes something very similar to the approach you're taking: http://www.reddit.com/r/haskell/comments/14gule/24_days_of_hackage_aeson/c7da0or Perhaps you could build on his approach.
Yes, I used Aeson like this to parse Gist API. While it would perfectly work for my problem, it is not very nice to have to declare new types if I want to extract data from others source. Maybe Haskell does not fit well for this use case.
Referenced in HWN for my dumb joke? Better than nothing!
So say your use-case is that you get both the paths and the JSON data from an outside source, and that you don't know anything about the structure of said JSON data. Say, you're writing some sort of JSON inspection tool, for whatever purpose. In this case, you can't get much more general than the existing JSON datatype, so stick with just `Value`. I'd float out the `jsonGetter :: Text -&gt; Value -&gt; Value` function to the top-level. That seems like a useful function to keep around. Your path function now looks like this: path :: Text -&gt; Value -&gt; Value path pathString = foldl1 (.) getters where getters :: [Value -&gt; Value] getters = (map jsonGetter . reverse . Text.splitOn ".") pathString Now I'll do some tricks. `foldl1 (.)` is the same as `foldl (.) id`, except the former crashes on an empty list and the latter thinks that's just fine. Intuitively, an empty path will just return the top-level JSON data. Now, since `f . (g . h) === (f . g) . h` and `id . f === f === f . id` (the `Monoid` laws), `foldl (.) id` is the same as `foldr (.) id`. Now we have that, `foldr (.) id functions start == foldr ($) start functions`. We're not there yet. We inline `getters` and write the whole function as a pipeline of function composition: path pathString value = foldr ($) value . map jsonGetter . reverse . Text.splitOn "." $ pathString The next rewrite that we use is: `foldr f x . map g === foldr (f . g) x`, so we end up with `foldr (($) . jsonGetter) value` at the start. Finally `($) . f === (\x -&gt; f $ x) === f`. Finally, I put the bit that creates a list of path pieces from a text into its own function. path :: Text -&gt; Value -&gt; Value path pathString value = foldr jsonGetter value pathPieces where pathPieces :: [Text] pathPieces = reverse $ Text.splitOn "." pathString (Fans of point-free code will notice that `path = flip (foldr jsonGetter) . reverse . Text.splitOn "."`.) EDIT: Hah, it gets even better: `foldr f z . reverse` = `foldl (flip f) z`. Which turns your definition of `path` into: path pathString value = foldl (flip jsonGetter) value $ Text.splitOn "." pathString If you swap the arguments of `jsonGetter`, you can get rid of that ugly `flip`. So yeah. `(foldl1 (.) . map f . reverse) xs z === foldl (flip f) z xs` Look at that. (Fans of point-free code will turn this into `path = flip (flip (.) (Text.splitOn ".") . foldl (flip jsonGetter))`. This is definitely the point where things start getting ridiculous)
Do you really think I copied the idea of a non-determistic state monad from C++, Modula 2 or Ada? Maybe Basic? Or assembler perhaps? I stopped implementing 6502 cross assemblers as learning examples because it wasn't a good example (certainly not recycling the old design) for the languages and concepts I was interested in learning at the time, like ML, Scheme, Icon and Prolog. I didn't get a deep understanding of those languages, but I didn't learn anything at all about "how to write Fortran" in them - I was only interested in what made those languages unique. Likewise Haskell, only I'm taking that much more seriously. I'm not re-using the old assembler design, just the old specification, and even that with a lot of it's-just-for-learning limitations removed. Of course my early learning exercises won't show a decades experience of the Haskell way of doing things - I don't have a decades experience to show - but that doesn't mean I'm writing Fortran in Haskell. You don't like that I've chosen to use the existential types extension rather than learn the standard Haskell 98 way? Take a look around. Haskell has evolved in the last 15 years. And while the name "existential types" doesn't originate from Haskell, it certain doesn't originate from the imperative/OOP world. Even heterogenous collections - you think they were invented in the imperative or OOP paradigm? Nope - LISP - heterogenous collections through dynamic typing. Existential types are a part of the effort to close the gap between dynamic and static typing without losing the safety or making things insanely complicated - to minimise the class of designs that can't be used purely because of the limited expressiveness of the type system. It's interesting to note that Python advocates have sometimes accused me of trying to write C++ in Python, even though I started learning Python *before* C++. In a sense, it's not entirely wrong. A superior design is a superior design, irrespective of whether the concepts were invented here or not. I don't have a lot of respect for not-invented-here syndrome. You think I shouldn't use a list of instances of the same typeclass just because one of them is a different ADT to the others? I say that if Haskell 98 doesn't allow that level of polymorphism, it's an awkward limitation that reduces the expressiveness of the language. And it's not just me - modern Haskell hasn't had that limitation for quite a while. For what it's worth, here's the problem function with the problem list... scanAny :: NDState ScanState Token scanAny = do old &lt;- ndsGetState ndsElse (ndsgBest [ scanWS (ssPos old) , scanComment (ssPos old) , scanNewLine (ssPos old) , scanKeywords (ssPos old) , scanIdent (ssPos old) , scanDecimal (ssPos old) , scanHex (ssPos old) , scanBin (ssPos old) , scanStrLit (ssPos old) ]) (ndsgGet &gt;&gt;= \c -&gt; return (Token TokJunk [c] (ssPos old))) The problem is in the state - not the part you can see here, but a part that's that's hidden (the position includes a token count, bad names I should have changed by now aside, that's what all the `ssPos` stuff is dealing with). Specifically, the state is meant to keep track of how many new lines have been seen as the scanning progresses. The `scanNewLine` function handles that behind the scenes, but to do so it needs to deal with implementation details that the other scanner functions don't even know about. It's called elsewhere with no problem, but putting it in a list (so a higher-order function can pick the best-choice scan for a particular token) *is* a problem. The reason I hit this problem without seeing it coming is absolutely because I'm still learning Haskell, but IMO it's more interesting to think about how expressiveness is limited without supporting the polymorphism I need for this. And if you can't accept heterogenous collections because you think imperative/OOP programmers invented them, as I already said, Lisp was first. 
One token-scanner in a list of scanners (passed to another scanner function) holds a higher-order function that needs to know about the newline count in the hidden state inside the non-deterministic state monad. It handles that by wrapping the underlying scanner with a type that knows about that count, but passes on the calls to the "real" underlying scanner that, like all the other scanners, doesn't. Ad-hoc polymorphism used to wrap one value inside another, still implementing the same interface. That polymorphism worked perfectly well in a lot of places, right up until I needed a list. The way I see it, if I can't have a heterogenous list, polymorphism is arbitrarily restricted. Doing it unconventionally, probably. And running into the issue accidentally because I didn't have the experience to see it coming - absolutely. But personally, I'm calling that serendipity - just because the end result needs one existential type (and a couple of higher-rank functions) doesn't mean it's a bad design. There's plenty of bad things going on in the code, but that's the point - to experiment and get a feel for what works and what doesn't. People learn by making their own mistakes and experiencing the consequences. And FWIW, yes, I did work out how to change the design to not need that existential type - a second pass that counts already-scanned newline tokens isn't difficult, and it's not really a second pass given laziness, but it's *boring*. Of course I'd do that in the real world, but this is a learning example, and doing that doesn't teach me anything new. 
I have 15+ years of experience in the games industry using C/C++ and have release a number of successful commercial games. I am currently writing a commercial game in Haskell. Why? Because I am a lot more productive. Using Haskell has dramatically reduced the number of bugs in the code and the exe is more than fast enough to compete with C/C++. But most importantly: Haskell is just a great fun language to work with :-)
As an example of nontrivial "LOC" compression, I'll releasing a self contained Numerical Linear Algebra lib in haskell in about another month or so (with some C bits for a few primops because SIMD makes a difference), and it looks like i'm on track to be at least 10-50x faster than Atlas on "Level 3" style operations. My initial release is going to relatively feature minimal, just some basic lin alg + some general purpose solvers, will probably be under 3k LOC, and yet 10-50x faster than atlas over a robust range of input sizes (including substantially larger than the L3 cache on my machine). I ran the "cloc" util on the current release of atlas source yesterday, depending on how you count it, its 70-150k LOC. I'm getting a pretty good compression I think :) 
What is the nondeterministic concurrency in FRP?
If you simultaneously accept input from a mouse and keyboard there is going to be a non-deterministic component unless you are proposing that the mouse and keyboard cooperatively transfer control to each other.
Those are just inputs. Arguing that this implies that FRP is nondeterministic is like arguing that reading external inputs implies that Haskell is impure.
If you define FRP as deterministic concurrency, then `pipes` alone is already FRP because it is just deterministic concurrency.
It sounds like you are complicating something that is simple. :) I also don't understand why you don't want to use libraries. A strength of Haskell is that it has very reuseable libraries, if you're not leveraging that you're not really using the language in an effective way.
BTW, enabling and using existentials in Haskell is very easy. If you feel that you need that (you don't), just do it.
You can have whatever crazy instances you like. I'm just telling you what I (and I think many others) expect from the .. notation.
Continuous function of time? How do I manufacture a value of type `Time` to feed to my continuous FRP function? I feel that the brand of FRP you are referring to oversimplifies the problem and does not give a satisfactory answer. That's not to say that I don't think there is a denotational semantics, but I think that denotation is not the correct one.
True, but you're also not learning how to implement your own libraries, or the breaking strain of techniques that you'd otherwise only be using at a high level to glue things together, or what to do to fix things when you hit those limits. For example, I also used that same non-deterministic state monad to reverse engineer a couple of binary file formats. It worked rather well - it's very good at skipping over bits of file I don't understand yet and finding the next bit that seems to make sense, minimizing the noise in semi-parsed listings. But that kind of search shouldn't be expected to scale well to huge files. What I found is for the current *extremely* naive version (and complete knowledge of the file format, so non-determinism isn't *that* extreme), it scales to handle files in the megabyte range but not that much more. Performance is very hard to predict, but mostly surprisingly good until it stack-overflows. This has defined a follow-up challenge I have thought a bit about a few times, but not really worked on - how to make it more scalable, where and how to apply strictness, how general are the rules and how much depends on the context where things are used. Also, is laziness the real/whole problem? Should I be looking to pack my lists into another data structure at key points, for instance? Of course other people have already solved those problems in practical libraries, and of course I don't intend to invest the time to recreate that, but I still believe there are lessons worth learning. I *do* want to use libraries, just not for certain learning exercises. 
Indeed, specifically check out the `pathToLens` function and callers.
I admit I do not know when - but unlike windows it is no big deal to live without it - just apt-get GHC (see here: http://askubuntu.com/questions/286764/how-to-install-haskell-platform-for-ubuntu-13-04) For me everything works like a charm this way - no need for precompiled packages if you can compile everything in your OS without MinGW and other "stuff"
There is a typo in example: "perform GC" as a name of function Also this code from broadcasts seems to be not used -- Publisher (input , output ) &lt;- spawn Unbounded
&gt; One module isn't the whole project. A project is built on modules, or whatever you call the bricks you lay. As soon as the unit of construction becomes more complex than "brick", "wall (glued bricks)", "room (walls, ceiling, floor)", your units are chosen poorly, and the next programmer won't be able to figure out what you were doing. &gt; But if you really write code as lots of small functions, the convenience functions to reduce the complexity of calling the library in various cases must exist [...] Another issue - you really shouldn't be scared of genuinely very general concepts. There's no end of useful things you can do with a digraph. For me, these sentences are in direct conflict. The latter is already done in Haskell using the former. The `Functor` type class is perhaps the best example of this. Convenience functions (`&lt;$` for Functor, defined as `\v -&gt; fmap (const v)`) are very common in Haskell: they also live in the type class, meaning that they are just as available as the "core functions", and they are overridden when it helps performance (or elegance)(or whatever floats your boat). 
This. Also anyone know of any AWS AMIs out there that incorporate this as well as a working cabal installation?
Is `performGC` needed? I've seen it a couple of times in your examples, but never understood why you use it.
Text files are sequences of characters, not 2D character layouts. And the standard interpretation of a tab character is "advance to the next multiple-of-8 column". This is what GHC uses. You probably have your editor configured to use a tab indent of 4, but GHC has no way to know this. The simple solution is: don't use tabs for alignment.
I find that when it comes to quick and dirty extraction of data from JSON, a library like hjpath does very nicely. It allows taking a JSON string and getting a value by providing a path. For example: &gt; jPath "a/b" "{ \"a\": { \"b\": 3 } }" ["3"]
I defer to the inventors of FRP for whether a denotation is "correct" or not. You may have an opinion of what you would like FRP to be, but that is not necessarily what it is. Continuity and determinism are fundamental. http://stackoverflow.com/questions/1028250/what-is-functional-reactive-programming Edit: I forgot to answer your first question of how to manufacture a value of type `Time`, but to be honest, I don't know where you're coming from. I don't necessarily have to be able to do that at all (although it may be nice if I can).
How does it compare to Haxe? I have enjoyed using the type inference facilities of the language to produce JS code.
Well, it's basically Haskell. They're missing type classes, though. It's a work in progress.
If you haven't seen this talk yet, it gives a pretty decent overview of what's there: http://www.youtube.com/watch?v=1jJ2paFuErM I'm not sure it *quite* answers your question, but it's definitely worth seeing in any case.
Seems to be something wrong with the link, or the server is down :/ "Oops! Google Chrome could not find feuerbach.github.io" Seems to be a DNS problem: $ ping feuerbach.github.io ping: unknown host feuerbach.github.io
Here's the repo: https://github.com/feuerbach/standalone-haddock Don't know if the link was supposed to point there, though. 
Here's a backup link: http://documentup.com/feuerbach/standalone-haddock I don't know why you have a DNS problem, though. % dig feuerbach.github.io ; &lt;&lt;&gt;&gt; DiG 9.8.4-rpz2+rl005.12-P1 &lt;&lt;&gt;&gt; feuerbach.github.io ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 34570 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 4, ADDITIONAL: 0 ;; QUESTION SECTION: ;feuerbach.github.io. IN A ;; ANSWER SECTION: feuerbach.github.io. 2301 IN CNAME github.map.fastly.net. github.map.fastly.net. 18 IN A 199.27.76.133 ;; AUTHORITY SECTION: fastly.net. 78711 IN NS ns1.p04.dynect.net. fastly.net. 78711 IN NS ns2.p04.dynect.net. fastly.net. 78711 IN NS ns3.p04.dynect.net. fastly.net. 78711 IN NS ns4.p04.dynect.net. ;; Query time: 5 msec ;; SERVER: 192.168.1.1#53(192.168.1.1) ;; WHEN: Fri Jun 7 16:48:48 2013 ;; MSG SIZE rcvd: 171 
I believe it is optional but ensures prompt release of resources which otherwise wouldn't be released until the next GC cycle?
Thank you! Me neither, here's my dig output: $ dig feuerbach.github.io ; &lt;&lt;&gt;&gt; DiG 9.8.4-rpz2+rl005.12-P1 &lt;&lt;&gt;&gt; feuerbach.github.io ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 14774 ;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 0 ;; QUESTION SECTION: ;feuerbach.github.io. IN A ;; AUTHORITY SECTION: io. 261 IN SOA ns1.communitydns.net. nicadmin.nic.io. 1370607008 3600 1800 3600000 3600 ;; Query time: 0 msec ;; SERVER: 10.6.1.2#53(10.6.1.2) ;; WHEN: Fri Jun 7 16:09:04 2013 ;; MSG SIZE rcvd: 106 
This seems to be a problem currently with many .io name servers.
Yes, that's correct. It's there to ensure prompt notification of other readers or writers that the opposing end has gone dead rather than waiting for the next GC cycle.
Those are fixed in the Hackage documentation. I accidentally forgot to push the last two documentation commits to Github, so if you are consulting the Github documentation that is probably the reason you still see those.
I am reasonably sure no i386 package today is compiled using only 386 instructions anymore, probably not even 686 only these days on software where it means a significant performance difference. 
Just install GHC and any packages you want using APT, that's what I do and it works very well :D
Using performGC is rather drastic. It is totally unacceptable in many situations. 
I can't quite get my head around a standing haddock. 
&gt; The program only builds with (unreleased) Cabal 1.17 which you can get &gt; from github. &gt; I spent some time trying to make it compatible with Cabal 1.16 (see &gt; Cabal-1.16 branch), but the API seems to have changed too much. &gt; If you seriously care about this, feel free to send a patch, but it's really &gt; easier just to install Cabal HEAD. :(
Not really. You only call it once per stream and only at the end of the stream. Besides, you can just condense all the performGC calls to a single call to flush all of them if you really care or just rely on the background garbage collection to do it for you. The tutorial just explains the most common scenario where you have a mostly static graph of connections that all shutdown when the program is done, in which case the delay from garbage collection has no impact on performance.
I tried cabal-dev for another project but I couldn't figure out what to do with projects that were not canabalized, so I just used vagrant.
This is turning into one of those arguments about whether vocabulary is fundamentally static. I'd prefer not to continue this way. I admit that it is often the case that vocabulary evolves. My goal here is to keep the definition of a term that is still relatively obscure static because it, in the long run, prevents confusion later. It looks like I have already lost this battle, though. Your questions about `Time` are reasonable. I normally just abstract over time and just require some axioms about it.
**No, not necessarily.** This example only shows the main module, the implementation details might be *very* composable. For example, the 'MyWorldMap', 'MyEnv', and 'Player' might or might not be updated by completely pure functions. Yes, the "updateWorld" function asks for an updating function that evaluates to an IO monad, I wrote it that way so you have a choice: if you want to update the item using something from IO (say something stored in a file) you may do that, but if you want to update it using a pure function, you can simply write: updateWorld selector (return . myPureUpdate) 
Yes, Haskell has atomic compare and swap. http://www.haskell.org/ghc/docs/latest/html/libraries/base/Control-Concurrent-MVar.html#v:swapMVar Atomic swap is: Control.Concurrent.MVar.swapMVar To do an atomic compare, you would do something like this: \predicate -&gt; modifyMVar mvar (\ value -&gt; return (value, predicate value)) Compare is in the IO monad because locking the mutex requires IO. Furthermore, you can use the Monad Transformer Library (mtl package) in the standard Haskell platform to lift IO into a State transformer. A type like this: type WorldIO a = ReaderT IO World a updateMain :: (World -&gt; MVar a) -&gt; (a -&gt; IO (a, b)) -&gt; WorldIO b updateMain whichMVar updater = do mvar &lt;- fmap whichMVar ask liftIO (modifyMVar mvar updater) **You would then write entire modules in the 'WorldIO' monad,** every module being responsible for updating the MVar's in the world's shared object. If all of your updates are done with pure functions, or are atomic swaps, you are guaranteed never to have any deadlocks or race conditions. Then, your main program would look like this: main = initializeWorld &gt;&gt;= runReaderT mainWorldIOFunction 
I'd like to try this but the setup is still such a pain. For example, I've never used or heard of vagrant. Maybe you can maintain an AMI?
Vagrant automatically builds a VirtualBox virtual machine, see http://www.vagrantup.com/ Unfortunately it takes rather long to build (compiling GHC HEAD first, then GHCJS, then lots of libraries). I'm rebuilding the VM on my server at the moment. If everything works correctly I'll add a download link for the complete VM image. We're publishing this now already to get some feedback, we really want to make sure that we haven't missed any important functionality that has to go into the GHC patch. If we miss the 7.8 release (planned for Sept/Oct) we'd probably have to wait for the next major GHC release.
Yes we will have to see how it holds up in the real world. Synchronous threads were added pretty recently to help with event handling and to use in things like `requestAnimationFrame`. It's easy to disable the automatic yielding in the scheduler, then the scheduler only yields when it has no runnable threads, but still interleaves Haskell threads. Disabling automatic preemption of Haskell threads (so the scheduler only switches when a thread yields explicitly, or blocks on an FFI operation or MVar etc) is another one-line change. GHCJS itself only uses one setTimeout, for the next invocation of the scheduler, which then handles waking up Haskell threads if necessary.
It's not at all essential. We could even add a `critical section` field to the thread state objects or something similar, that indicates whether a thread may be preempted.
&gt; If all of your updates are done with pure functions, or are atomic swaps, you are guaranteed never to have any deadlocks or race conditions. Nice! I think the advantage of this route is you use the mvar as a way to communicate between threads, but the majority of your code that doesn't deal with communication can exist solely in State as a free function and later lifted/composed (not sure which is the right word) into IO when there needs to be cross-thread communication. Thanks for your response. 
Wow, amazing, I didn't know that GHCJS's development was already so far ahead! Congratulations! =) Some AngularJS-like library would be awesome... 
Very interesting! Looking forward to see how this plays out =).
You don't need streaming libraries to separate IO and pure code. There are plenty of other patterns available to keep IO to a minimum.
Yes, that's true. Streaming libraries are a common pattern but definitely not the only one.
Thanks for explaining that! I fixed both of them.
*hopes it's not about web applications*
Are you from the Gentoo world or something lolol
Alas, web applications encompass a large chunk of the work that needs to be done in "the Real World."
&gt; git pull https://github.com/ghcjs/ghcjs-build.git I believe you meant `git clone` there.
Neat. One thing that could drastically improve GHCJS performance is if it targeted asm.js (or does it already?), although I'm not sure of the current state of its JS generation in the first place.
thanks, fixed
Looks like I used a sum type for that too. Full code at: https://gist.github.com/singpolyma/5733724
Sorry people, I hadn't realized that building the VM took so long, it contains lots of extra things that aren't really needed for this post. To get up and running more quickly, use this: git clone https://github.com/ghcjs/ghcjs-build.git cd ghcjs-build git checkout prebuilt vagrant up This downloads prebuilt binaries for GHC HEAD and GHCJS, and just extracts them. Should take about 15 minutes, plus download time (about 1.2GB, GHC source trees are big). I hope this fixes most problems now, we'll look into a better solution later. If you have a 32 bit ubuntu 12.04 box, you can also download the binaries from [here](http://hdiff.luite.com/ghcjs/ghcjs-prebuilt.tar.gz), check [github](https://github.com/ghcjs/ghcjs-build/blob/prebuilt/manifests/base.pp#L26) to see which dirs need to be in the PATH. I've also uploaded the compiled examples from the post: * [hello](http://hdiff.luite.com/ghcjs/examples/hello/hello.jsexe/) * [ffi](http://hdiff.luite.com/ghcjs/examples/ffi/ffi.jsexe/) * [race](http://hdiff.luite.com/ghcjs/examples/race/race.jsexe/) * [fibonacci](http://hdiff.luite.com/ghcjs/examples/fibonacci/fibonacci.jsexe/) * [sync](http://hdiff.luite.com/ghcjs/examples/sync/sync.jsexe/) * [event](http://hdiff.luite.com/ghcjs/examples/event/event.jsexe/) And some older examples, compiled with older GHCJS builds, linked from the post: * [gloss examples](http://hdiff.luite.com/gloss/) * [realtime heap view](http://hdiff.luite.com/reduce/) 
AFAIK, asm.js doesn't support using JS objects, which GHCJS appears to use; so it wouldn't work (for now).
Thank you.
Hm! $1000 would cover my expensive inner-sydney rent for almost three weeks, I'm in!
I also write compilers in Haskell for my job, pretty nice!
Of course not, fixed. Thanks!
Seconded! Although I wonder if it would be easier to create bindings to angular (or derby or one of the other multitude) or use one of the existing haskell frp libraries. Would pipes work in the browser?
Wow.
The GHCJS devs are already well aware of asm.js and its limitations and why it's not appropriate for this use-case. luite can explain.
I've never seen this style before. I just do what emacs haskell-mode tells me to do.
I hope this will fix a problem haskell has suffered in the past. I'm an excited teenager who wants to build stuff, but haskell seemed dull and academic. Luckily I was weird and a tad autistic, so I took the bait anyway. Now that I'm writing haskell, I'm discovering stuff like Elm and articles on functional reactive programming that make it rather great to do sick user-interactive stuff in functional languages. It pays off to be dull though. Learning about dull fourier transforms is now allowing me to write this cool music recognition algorithm (similar to the one posted here before, that was written in java). Then there is the awesome tools (hpc, quickcheck, ghci). Concurrent Haskell is also amazing. Lightweight threads are cool and make me stop thinking in 'banana-callback-spaghetti-horror-style'. Software transactional memory is a great mechanism for sharing memory as well. Also, the ease of web development in haskell is crazy. The strong type system, acid-state and snap are such a good combination for a robust environment. So once you bite through it, it's extremely cool. But you can't just go "HEY I got a hackathon tomorrow, I'll use haskell because I never used it, and I will make the most awesome app ever" which I pulled off with a language or two before. And that is haskell's biggest weakness. 
Might be, but I don't believe web applications are where Haskell really shines compared to other languages &amp; their frameworks.
Sure, if you need to use the bleeding-edge of your libraries all the time, then you might want to install more from cabal. I default to installing less from cabal in order to keep compatibility of my code.
Bleeding edge? Not always. I use XMonad as my main WM, and I've got 0.11 installed because it has features I use that are missing in 0.10. The Ubuntu repos are still at 0.10. This is the same situation as it was four months ago. XMonad 0.11 was released over five months ago. An almost half year-old application is *not* bleeding edge anymore. Since I'm installing newer applications from cabal, I find it easier to just do everything through cabal so I don't end up with too-old library versions through apt wasting my time.
Yeah, the barrier is that Haskell goes against the grain in so many ways. It's not just a syntactic re-skinning of an existing language. Even other functional languages don't completely prepare you for it because Haskell uses enforced purity and laziness.
Yes, it is perfect for message passing. This is what I use it for in my own programs. It's actually great for reactive programming in general. Originally it was message-passing only, but the addition of the `Behavior`-like behavior generalizes its scope more. I will update the description in the next patch to reflect the most common use cases. I should probably just take the first paragraph from the tutorial's introduction and use that for the package description.
Thanks, I will document it. I'll check whether it supports a weaker put-put law described in http://journal.ub.tu-berlin.de/eceasst/article/view/729/0 
On the bright side, the pipes family is a shining example of good documentation, and I really appreciate all the packages.
Thanks! I really appreciate that. :)
If I were using streams to, e.g., read a lot of small files, I'd not want a multi-second pause after each one.
Multi-second pause? `performGC` takes about 100-200 microseconds on my computer for the cases I've benchmarked. Also, if your heap is so large that garbage collection takes that long then you aren't using `pipes` correctly. I rarely use more than 1 MB of memory. However, even if you have your reasons for having such a large heap, you can choose not to use `performGC` at all and just delay thread termination until the next garbage collection cycle. All that means is that you'll have a few extra threads lying around until the next major garbage collection harvests them, but if you have such a large heap to incur these huge garbage collection costs then these threads would be a very small fraction of your memory usage.
Indeed, put-put does not hold, I documented it. As I see the cited paper in my other reply deals with more complex cases where structures of the view transformation is taken into account, so it does not apply here. If I think about it, I interpret lenses as abstract editors, and listLens is an editor with a hidden state. I guess lenses representing editors with hidden states cannot satisfy put-put. I think I have no problem with that but of course I should be aware of that and think about the consequences. What do you think?
Nice t-shirt.
For my next post I want to write about FRP with GHCJS in a tutorial style. I'll probably use sodium (though reactive-banana also works last time I tested, and the other libs will probably also work, perhaps with minor tweaks). Of course the existing FRP libs will not give us the easy HTML templating that angular has, so we'll see how that gap can best be filled.
We rely too much on JavaScript objects and garbage collection to make that a feasible option. Unfortunately asm.js is really limited, it's not just a matter of annotating numeric variables. We would have to redo all memory management for this. I'll write a longer post with more details about the internals, but very briefly, GHCJS' memory is organized like this: * every heap object (thunks, data constructors etc) is a JS object * every thread has a stack, in a (untyped) JS array that grows dynamically * some global variables are used for passing function arguments and storing temp results This, together with eager blackholing, means that the JS GC will do most of the memory management work for us. A few things cannot be supported this way, they require the heap scanner (see gc.js in shims): * weak references * CAF resetting (a CAF is a global thunk, say you have a long global list primes :: [Integer], then you may want to have it deallocated when nothing refers to it anymore) * deadlock detection, unreachable MVars (currently unimplemented because of MVar event handling, so reachability checking is tricky) The heap scanner is usually run from the scheduler, but you can turn it off (or run it on demand)
So do you know when you will be free tomorrow?
What about putting a heap inside a typed array and letting the programmer manually manage allocation in a manner similar to the way we malloc and free on the C heap when we use foreign pointers? Edit: I guess we could just do this with a library or am I missing something?
pronounce it as 'fix' (carter's suggestion)
First thing that came to my mind as well.
So, Martijn said he wouldn't be available at 12, but he hasn't responded about rescheduling. Would you still be interested in meeting tomorrow at 12 if he still doesn't respond by then?
I was thinking more "eff fix", but same idea :-)
Yeah, definitely. I'll send you my number. Let me know if there's any further changes. I'm available until 5 anyway. Would love to talk about pipes, web frameworks, FRP and other things.
this and SFML would be so awesome ...
Yes, I agree with "eff fix" more now.
How big are the resulting minified JS files?
Excellent, I look forward to that post with great interest! Maybe bacon would be a good choice of library to use for your post as bacon.js is a port from a simple haskell frp library. Might make a good comparison to do both side by side?
It is a very interesting read. But, AFAIK, it was posted back in Feb, what have you been waiting for exactly.
I found the paper on complex numbers and semiring really interesting... are there similar things discussed elsewhere? 
I didn't solve many problems here. Since the purpose is to call C++ method from haskell preserving some hierarchy present in the original C++ model, treat typeclass as a interface works quite well as it is. fficxx is basically doing boilerplate of connecting C wrapper for C++ to the corresponding instances in typeclass. A given C++ OOP class (say Foo) is now represented as a type Foo and typeclass IFoo in haskell. So a subclass Bar can be benefitted from ad hoc polymorphism by being an instance of IFoo, which is boilerplated . C++ Template is not implemented at all at the current stage. Using parameterized type and appropriate type functions, I think I can model C++ templates in terms of haskell types. Again, boilerplate is quite necessary,but that's the job for a tool like fficxx. I am trying hard to implement it. I refered to many previous approaches. Maybe I got most influenced by the paper by Oleg Kiselyov and Ralf Lammel: Haskell's Overlooked Object System 
Thanks!
Pretty bad, since the values in the disk space are immutable. /s
`pipes` requires `RankNTypes` and `KindSignatures` and `pipes-concurrency` requires `STM`, so it pretty much depends on whether GHCJS supports those.
GHCJS uses STG, so RankNTypes and KindSignatures are handled for it by GHC. Luite has implemented the STM primitives so those should work too.
As well as the GC problems Luite descried in this thread. Emscripten's setjmp/longjmp support is too limited to do tail calls (at least the last time I looked). This problem may go away if JS gets tail calls.
Like I just said: &gt; However, even if you have your reasons for having such a large heap, you can choose not to use performGC at all and just delay thread termination until the next garbage collection cycle. All that means is that you'll have a few extra threads lying around until the next major garbage collection harvests them, but if you have such a large heap to incur these huge garbage collection costs then these threads would be a very small fraction of your memory usage.
GAHH, I though some of that looked familiar to me sleep addled brain yesterday evening but I though I dimly remembered if from the first installation! If I had glanced the right I would have seen this: http://chris-taylor.github.io/blog/2013/02/13/the-algebra-of-algebraic-data-types-part-iii/ Which I haven't read yet!
Discussing Seven Trees in One in this kind of article is a bit reaching. The 7trees paper is well above anyone who needs this blog post to help them understand the algebraic part of algebraic data types.
[Was last discussed here](http://www.reddit.com/r/haskell/comments/18bqth/the_algebra_of_algebraic_data_types_part_2/)
Also posted in February (still a good read, though!), and the "Next time" mentioned in the final paragraph doesn't seem to exist yet :(
code is pretty big at the moment, even though only actually reachable functions are included. Expect a few hundred kB minified+gzipped for an application that uses a few libs from hackage, it's likely to go down a bit before the first GHCJS release on hackage. some reasons: * we have long function names, that include the package name and the `h$` prefix, to avoid clashing names. They gzip rather well though. * our dataflow analyzer is still rather conservative rewriting expressions, so you often get things like `h$stack[(h$sp+4)-3]` (because it might be different when you are close to the precision limit of js numbers). We need to tweak the rewrite rules a bit, add some exceptions for really common situations like stack management where we know that this will not result in differences due to loss of precision. * We have some thunk metadata that describes the static data referenced. We use this for deallocating global thunks (for example if you have a long global list primes :: [Integer] that no one uses anymore) and finalizers for weak refs. This data is encoded really inefficiently unfortunately, which will be fixed soon. * We haven't spent any time yet making the code syntactically shorter by choosing shorter variable names or constructs. Really common names like `h$stack` are relatively long and can easily be changed, but i don't think the effect on code size after gzipping will be that big. The old code generator was compatible with closure-compiler's ADVANCED_OPTIMIZATIONS, which helps a lot with all the long names, and also removes unused functions from the RTS. The new one hasn't been checked yet, but it will be made compatible before release.
You need to edit gloss source files so that GLUT backend returns from mainloop when window closes. Unpack gloss source and edit file 'Graphics\Gloss\Internals\Interface\Backend\GLUT.hs': * line 30: "exitBackend = (\_ -&gt; GLUT.leaveMainLoop)" * line 39: "installWindowCloseCallback = (\_ -&gt; GLUT.actionOnWindowClose $= GLUT.MainLoopReturns)" I've only tested this on windows, but it should work on mac assuming GLUT was compiled with freeglut library.
I forgot to say that I compiled with GLFW. Using GLUT with Gloss works even worse than using GLFW – I can't even open a Gloss window from ghci when using GLUT. Thus your fix probably doesn't help (I will try it, though). But thank you anyway!
i assume he is referring to dynamic data types like the interface offered in [Data.Dynamic](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Dynamic.html)
To save people from clicking through three times, the tweets are: * The small size of scheme makes me feel that I could write a compiler for it in short order, which is definitely NOT the case for Haskell. * Haskell has the challenge of getting your program to compile, while Scheme has the challenge of getting your program to work. * The only real head-scratching, WTF-isn't-this-working? bit I have had in Haskell so far was while working on untyped data. Types Are Good.
* cabal-dev cannot create a new Yesod application, it can be used to install the Yesod platform. I prefer hsenv+cabal-install to cabal-dev, but that's another story. * What "development service" are you talking about? Is that `yesod devel`? * What did you deploy? The executable file? * Which version? On which OS? Sorry if I am wrong, but you don't look at ease with a basic workflow like: 1. run `yesod devel` in a terminal and keep it running 2. modify the application files =&gt; yesod recompiles the application in devel mode 3. stop `yesod devel` and compile the application (e.g. `cabal install myapp`) 4. copy the compiled file where you want to deploy, and run it (e.g. `./myapp --port 80 production`) Some month, I think had problems at step 2 (automatic re-compilation if templates changed), but I don't recall the exact conditions. I believe it was with Yesod 1.1 (or maybe somewhere into [1.1,1.2[). A [tutorial](http://yannesposito.com/Scratch/fr/blog/Yesod-tutorial-for-newbies/) could be helpful. I'm not sure the Yesod book is suitable for a first contact.
"Haskell has the challenge of getting your program to compile, while Scheme has the challenge of getting your program to work." I'm struggling with a Haskell program that compiles and crashes. He must have had a lucky day.
It's very easy to to make a Haskell program that crashes. Usually it's something that can be found with an appropriate compiler warning. Sometimes using a library function hides the warning. main = print $ Data.Maybe.fromJust Nothing
That's generally not considered a crash, I'd say.
Isn't that how you exit in a pure computation? I start every file with this definition: exit = Data.Maybe.fromJust $! Nothing j/k :P
Try finding a way with fix ... instead ;)
There are no warnings.
In light of the tweet just before, I believe he is referring to the fact that *writing* a Haskell compiler is more difficult than writing a Scheme interpreter.
You can store untyped data on the heap and access it through pointers. The Foreign.Ptr module provides tools for doing pointer arithmetic and casting. Needless to say, this lets you do all sorts of unsafe things with what is effectively raw memory.
did he give up on wolfstein haskell port yet?
With respect to my experience, you're the one having an unlucky day. While compiling isn't a guarantee that your program is going to work or not crash, the extent to which compiling implies working almost seems unreasonable at times. I've done refactorings involving changes to thousands of lines of code, and lots of hammering through type errors, only to come out on the other side with a program that worked the first time it compiled. It depends heavily on the extent to which your program leans on the type system though. If you're doing a ton of string manipulation, or other kinds of operations on data whose invariants aren't very fully expressed in the types, then it doesn't always work out so nicely.
&gt; It depends heavily on the extent to which your program leans on the type system though. I think that's the point in question. I think I'm yet to successfully type `tail` into a Haskell program that I actually intend to deploy without eventually seeing Ye Olde `Prelude.tail: empty list` as a result at some point. (I mean, you think you've learned your lesson, and then you're all like, "Oh, _this_ list can't ever be empty, I can make an exception this time..." and then, yeah, you've got an exception all right.)
&gt; I've done refactorings involving changes to thousands of lines of code, and lots of hammering through type errors, only to come out on the other side with a program that worked the first time it compiled. In my (admittedly limited) experience, this phenomenon seems tied to the number of functions you have on your types. Working with an AST that only has a few functions for manipulating it? If it types, it's probably correct. Primarily working with just Ints/Floats? There's quite possibly a logic error in there somewhere.
You should mention that you already asked this on Stack Overflow: http://stackoverflow.com/questions/16998181/hamlet-templates-doesnt-update
It's slow to respond on my Window's system (lag between click and pixel appearing, can't draw a line of pixels). Any thoughts on where the biggest bottlenecks are?
Well, the lesson there is, never ever use `tail`, use `drop 1` instead. Or better, use pattern matching (with -Wall so you don't miss a case).
please post your question on the yesodweb (google group) mail list
Based on my short and violent experience with OpenGL in Haskell I'm almost sure he did. You cannot just sit and hack a program in Haskell, at first you need to win a war with its type system. When program compiles you can finally run it and watch at it crash. I'd seen this many times and I'm starting to belive this compile-correctness in Haskell is the biggest hoax ever.
That's like saying to build a car you first have to win a war with physics so it doesn't fall apart.
that's exactly what I pointed out here : http://www.reddit.com/r/haskell/comments/1fvtbf/fp_complete_launches_haskell_in_real_world/caenl97 Haskell is _not_ "that language I'd like to use at that hackathon next month because I never used it before". When John made the bold statement of rewriting wolf3d in haskell, I feared the worst. You can't just 'start' writing haskell, it takes time and efford to adapt to the new paradigm. I've done java and C all my life (heh, from 14yrs to 17yrs old, young programmer here :P) and am in an On-again, off-again relationship with haskell and learning the ropes graduallly, and it really takes time and _especially_ efford to learn all this new stuff, I must say.
I think the "don't do that then" attitude leaves a bad tastes in Haskellers' mouths (as it should).
&gt; Based on my short and violent experience with OpenGL in Haskell I'm almost sure he did. Wolfenstein 3D would be rendered in software, wouldn't it?
Scala is slowly pointing me in the direction of Haskell, because with Scala I am learning what Monads or monoids, or other things mean w/o having to dive head first into category theory to even begin to understand the lingo.
&gt; I think the "don't do that then" attitude leaves a bad tastes in Haskellers' mouths (as it should). So … don't do that [have a "don't do that then" attitude] then? Meta.
`fix id` Just wait a while.
I think gauging how much rigidity or laxness to put in your types in something you learn over time. I certainly feel like I make better decisions about this now that I did when I was starting out.
There's an interesting discussion about Neil Mitchell's check I saw recently start around [here](http://www.haskell.org/pipermail/glasgow-haskell-users/2009-May/017226.html). It's an old thread, but interesting nontheless. 
The other problem is that very few tutorials teach effective use of pattern matching or present it as a fundamental skill that every Haskell programmer should use on a very regular basis.
My personal favourite `fix error`. Sadly, it has type `String`.
Haskell's compile-time guarantees goes out the window completely when you start doing OpenGL stuff, because OpenGL doesn't do any kind of static checking (it can't, everything's done at run-time). So Haskell, especially the raw bindings, can't do you any favors there. And OpenGL, in any language, will only begrudgingly work after you beat it into submission and sacrifice a goat, so there's no help there either. Hell, OpenGL isn't even typed. What type is a program handle in C? `GLuint`. Shader handle? `GLuint`. Vertex attribute location? `GLuint`. And don't even get me started on their one true God enum that encompasses everything from data types (because there's no types, so you have to tell OpenGL explicitly) to program link status. If it had some semblance of types then the Haskell bindings could leverage that and make things a little more safe, but a great type system can't do much when you've got no types to work with.
Is Haskell type system Turing complete? Is it possible to implement type that will check if given number is a prime number, for ex: sumPrimes :: Prime -&gt; Prime -&gt; Int sumPrimes a b = a + b sumPrimes 2 5 -- ok sumPrimes 4 5 -- error 
I know the feeling from using SDL in haskell, the only parts that ever failed me was the SDL internals that weren't that well represented.
I think because of type classes it is. Basically, type classes are Prolog. data Nil where data Cons a b where class Head a b where -- head/2 class Tail a b where -- tail/2 instance Head (Cons x xs) x where -- head([X|Xs], X). instance Tail (Cons x xs) xs where -- tail([X|Xs], Xs). data Zero where data Suc a where class Length a b where -- length/2 instance Length Nil Zero where -- length([], 0). instance (Length xs n) =&gt; Length (Cons x xs) (Suc n) where -- length([X|Xs], suc(N)) :- length(Xs,N). class Append a b c where -- append/3 instance Append Nil ys ys where -- append([], Ys, Ys). instance (Append xs ys zs) =&gt; Append (Cons x xs) ys (Cons x zs) where -- append([X|Xs], Ys, [X|Zs]) :- append(Xs, Ys, Zs). 
Well, in general it's undecidable. But even if we go for conservative approximations, the simplest thing that'd work reasonably well is to compute the call graph and then get the transitive closure of all the calls to `undefined`, `error`, `fail`, `throw`,... But just because you call those functions doesn't mean the function actually is unsafe. In the days before bang patterns we'd often see this pattern (and some still use it to avoid needing to rely on `-XBangPatterns`): foo x y... | x `seq` y `seq`... False = undefined | otherwise = ... And there are plenty of similar cases when using GADTs where there is a branch of a case-expression which is impossible, but GHC can't see that yet. And, of course, there are times where this is what we're doing, morally speaking, but we're not using actual GADTs, so the compiler has no hope of seeing it. This last one is the classic "I know it can't happen" scenario, which is usually untrustworthy, but when it's used in the containers package I have a pretty high level of faith that it was actually done correctly.
This is a bit more direct: main = undefined
I wish the site would allow itself to use more than half the width of my phone's (already quite small) screen.
are there any plans for integration into the Haskell ecosystem? will this become a part of the official GHC? or will it become a part of Haskell Platform? and when? anyway, very interesting, I'm sure this opens many possibilities for a lot of people. 
Have a type-level brainfuck interpreter: http://sillyskynet.blogspot.nl/2009/11/typefuck.html
Our short term plan is to get support in Cabal and get a minimal patch into GHC itself before the 7.8.1 release (after ICFP). The patch does not contain GHCJS itself, but adds the `foreign import javascript` FFI support and some extra things that help us set up the compiler for cross-compiling to JavaScript. When those have been merged, and GHC 7.8 is installed, you can install GHCJS with just `cabal install ghcjs`, and packages with `cabal install --ghcjs myPackage` 
I'm curious. Besides Erudify, are there any other companies using Haskell in Switzerland? 
this is a big problem. Also, `tail` and `head` should probably just not be in the prelude. I think people would be less inclined to use them if one had to say `import Data.List(tail,head)`. Sadly, that is a breaking change. 
But what is 'reasonable'? If you're guaranteed to have a non-empty list do you work with a non-empty list data type? Is it worth the hassle of not being able to use Data.List, and having to opt for more generic functions and/or having to write your own? 
That is wonderful..!
Under the circumstances, it was a bug. It just would have been advantageous for me to have given a better error message, and actually, error out somewhere else, sooner. I should have used [tailNote](http://hackage.haskell.org/packages/archive/safe/0.3.3/doc/html/Safe.html), at the very least. I wrote a bad state machine. It got into a state I wasn't expecting due to a straight-up logic error. I'm don't know that there was much more Haskell could have done for me; alas, for all its wonders, it can not surpass the fundamental GIGO limit.
That's true of a lot of things in Haskell. Learning the language is like learning how to program all over again and we learn the same way by first testing the extremes before we find a decent middle ground.
&gt; I'd seen this many times and I'm starting to belive this compile-correctness in Haskell is the biggest hoax ever. Nah, you just suck at programming. Better luck next time. Oh, and your comments in this subreddit are pretty close to trolling. If you have nothing interesting to say, please do us both a favor and kindly shut up.
I think you can double-tap on text to fill the whole screen. It works on the iPhone.
Also fwiw, that type is possible in most dependently typed languages. In Agda: sumPrimes : (m n : Nat) {_ : T (prime? m)} {_ : T (prime? n)} -&gt; Nat sumPrimes m n = m + n Such languages are often not Turing complete nor are their type systems.
Ideally, the Prelude would have "safe" variants of those (e.g: listToMaybe) as to further encourage totality.
You can use a wrapper like [DrawingCombinators](http://hackage.haskell.org/package/graphics-drawingcombinators) or [FieldTrip](http://hackage.haskell.org/package/FieldTrip) or if they aren't powerful enough, then you can extend/write one yourself. Such a wrapper is relatively small and easy to make, but makes things quite type-safe. Also, you don't use the GL-Raw package. The ordinary GL packages already use proper sum types to avoid the one-big-enum problems, etc.
You're right, you can use/make a wrapper to get rid of the untyped problem. The second half of my post was born from my experiences doing OpenGL stuff in C and doesn't really have any relevance to Haskell. The GL packages are nice, if quirky at times, but they can't shield you from the fact that your OpenGL calls are operating on a black box state machine. You end up with all your OpenGL calls sitting in the IO monad just as if you were writing it in C, and at that point there's little Haskell can do to help.
This is relatively old stuff. They mention buildpack for Haskell, here is my project that uses just that to get deployed on Heroku: https://github.com/Tener/deeplearning-thesis/tree/master/gui
Consider that the pure (+) function is implemented, under the hood with effectful register-add IO primitives. We should consider the black-box horrid GL interface similarly to the register-add operation. An unsafe primitive to be wrapped by a sane interface and never again used directly.
I implemented this a few years ago using functional dependencies: http://mortberg.wordpress.com/2010/08/30/zn/
Those are some exceptionally illustrative diagrams!
oleg facts? see http://ircbrowse.net/exports/quotes.csv related "fact": oleg doesn't write haskell. he writes prolog. he just happens to write it at the haskell type level. 
This is the best step by step introduction to F-Algebras I've seen. Having looked at other excellent documentation about this, if you find yourself needing a recommendation for this kind of thing to someone learning haskell, I'd say send them straight here!
Lovely, one of the best explanation seen so far! Strangely I understand the state monad way better than the reader, not sure why 
The reader monad makes no sense, that's why. fun = do a &lt;- ask return $ a + a vs fun a = a + a
Which package is Control.Functor.Algebra from? Google tells me it's from [Ed Kmett's obselete category-extras package](http://hackage.haskell.org/package/category-extras), which is obselete. I can't tell which of its successor packages Control.Functor.Algebra has moved to, or whether Ed Kmett just hasn't got around to it yet / no longer thinks it worthwhile / whatever. Then again, it's not clear to me that (if the only type of interest from the module is `Algebra`) Control.Functor.Algebra contains any useful functions (e.g. it doesn't contain `cata`). Perhaps it is enough for me to duplicate the definition in my own code?
Very very nice. This is just what the doctor ordered as I'm slowly gaining a foothold on monads.
Try it using: instance Monad ((-&gt;) a) where Without using extensions, class instances must be of the form (C a b c), where C is a type constructor `(-&gt;)`, and `a`, `b`, and `c` type variables (there are extensions to lift the requirement of them being type variables). It requires the constructor being in prefix position. It's really no different than `(a -&gt;)`, but it makes the parser happy.
I came to this by thinking.. hey if a function needs to read an argument to produce a result, in a sense its a 'reader' and oh my God the analogy worked o.o And wow now the official definition makes sense: newtype Reader a b = Reader {runReader :: a -&gt; b} What first looked like voodoo magic to me is now clearly a newtype wrapper around a function... Wow I need more of these ephiphanies. Hope this post helps other noobies as well!
`State s` without `put` is essentially `Reader r` if you don't modify your state; in this case, `get` is analogous to `ask`. In other words, `State s` is like `Reader r` but you can modify the initial environment. (Similarly, `Writer w` is `Monoid s =&gt; State s` where you can only (m)append things to the state, and not modify it in general.)
http://hackage.haskell.org/packages/archive/mtl/2.1.2/doc/html/Control-Monad-Error-Class.html#t:Error
That's because you're doing all your operations on `a`. If you sometimes want access to `a`, and if `a` is a large constant structure, it can be handy to pass it into the reader. The `a` could be a runtime configuration, gathered together from the command line, the shell environment and from a config file. You don't want the whole `a` all the time, you just want to get access to it when necessary: load :: [FilePath] -&gt; Reader Config [FileContent] load xs = do chatty &lt;- asks ((&gt; 3) . verbosity) when chatty (log "Opening files...") forM xs (\file -&gt; do override &lt;- asks format_override let inference = guess_file_format file case override of Just format -&gt; do when chatty (log "Over-riding format " ++ show inference) load_file format file Nothing -&gt; do when chatty (log "Guessed format " ++ show inference) load_file inference file) Please ignore the fact that the logic of this snippet is pretty much nonsense. I hope you get the idea. The reader monad can contain the logging verbosity, flags on the command line, or anything else that may persist for the duration of the execution.
In real applications, you really want to newtype something around that 'Reader Config' though, for sanity's sake.
Nice explanation! Might be worth mentioning the 'RWS' monad, which is a combination of Reader, Writer and State. I use it in Kontiki to model a state-machine: config in the Reader part, things-to-be-executed as a result of the transition are accumulated into the Writer part, and State is, well, the state. Reader &amp; State (and as such RWS) play really nice with Lens as well, see https://gist.github.com/NicolasT/5543593
&gt; I can reannotate functions that might fail using errors:Control.Error.catch to give more precise and informative error messages I suck at Haskell error handling beyond the basic `Maybe` and IO exceptions. Could you explain how this works in greater detail, or point to some kind of text on how to handle errors in Haskell?
The problem is that Haskell tutorials either go the "monads are burritos" route or "monads are just monoids in the category of endofunctors" route. Both of which sucks.
 &lt;interactive&gt;:3:12: Warning: Defaulting the following constraint(s) to type `()' (Show a0) arising from a use of `print' In the expression: print In the expression: print $ fromJust Nothing In an equation for `main': main = print $ fromJust Nothing
"in Haskell recursion is not completely first class because the compiler does a terrible job of optimizing recursive code. This is why F-algebras and F-coalgebras are pervasive in high-performance Haskell libraries like vector, because they transform recursive code to non-recursive code." if there is sense in this, I cannot find it.
Writer is just the monad instance of ((,) o) (requiring a Monoid constraint on o). That monad instance isn't in base, but I think it should be. (Your unwrapped reader monad *is* already in base, by the way). This may make less sense, but the State monad is the adjunction (for the purposes of saving time, I'll say just think of this as a kind of composition) of ((-&gt;) s) and ((,) s). All three of these monads are related! 
http://chris-taylor.github.io/blog/2013/02/13/the-algebra-of-algebraic-data-types-part-iii/
No no no! If you're expecting the list to be nonempty then `tail` is safer than `drop 1` (you need to catch your logic error, rather than have the program output a meaningless result). It scares me to see people upvoting this.
Imagine trying to write binary search with `safeDiv :: Int -&gt; Int -&gt; Either String Int`, it would be awful. Just because `` `div` `` isn't safe doesn't mean `` (`div` 2)`` isn't safe, and it's not worth distorting your code to satisfy some imaginary requirement of safeness.
Can you provide some citations or references here? I'm not sure I follow what you're getting at.
If you liked this post, you will probably also like sigfpe's post with a similar theme ("you want to do X, what do?"): [You Could Have Invented Monads! (And Maybe You Already Have.)](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html).
The problem is the compiled executable does the write then immediately terminates, so its already too late to catch the exception. This is because it is trying to do the write all at once, as the program terminates, _not_ when we call putStrLn. This is because we're block buffering instead of line buffering. This in turn is because we're handed a file descriptor instead of a terminal. So we're in BlockBuffering instead of LineBuffering mode. If I set an explicit LineBuffering mode on stdout, it works as expected: module Main where import System.IO main = do hSetBuffering stdout LineBuffering putStrLn "hello world" ------- machine# ./test 1&lt; /dev/null; echo Exit status: $? test: &lt;stdout&gt;: commitBuffer: invalid argument (Bad file descriptor) Exit status: 1 
I was just thinking about [monadic functors](http://ncatlab.org/nlab/show/monadic+functor). The relationship between monadic and algebraic categories is explained [here](http://ncatlab.org/nlab/show/algebraic+category). 
I'm saving that for an upcoming post on monad transformers :)
Notice in 'transformers', RWST (or RWS which is RWST on Identity, as most other non-transformer monads) is not a transformer-stack of Reader, Writer and State, but it's a completely independent implementation: newtype RWST r w s m a = RWST { runRWST :: r -&gt; s -&gt; m (a, s, w) } 
&gt; In turn I don't see the relation to the free monad directly? A monadic functor has as its codomain the category of algebras over a monad. No, a monadic functor is one such that the *comparison* functor (which is the one that has values in the category of algebras) is an equivalence. On sufficiently nice categories, free monads always exist, and you can prove that the category of algebras of an endofunctor is monadic over the base category (which is to say that the forgetful functor is monadic). So that implies that monad algebras subsume functor algebras (modulo some technical conditions). More details [here](http://ncatlab.org/nlab/show/free+monad). &gt; Just thinking aloud here a little. What I mean, I suppose, is that these constructs work a certain way in category theory, but they feel 'one level up' from the constructs we have in Haskell-land. So its not immediately obvious to me how to go from some notion of a monadic functor directly to a monad in Haskell, much less a free one, nor what going from Fix to Free buys us in this context directly. Well, you can get `Free` from `Fix` by adding a constant functor to `f`. This is sometimes relevant in practice. For example, in the original post, `Const` is hardcoded in the base functor `ExprF`, but you could take it out, and later use `Free` instead of `Fix`. Then the evaluation function is just the monad algebra corresponding to the functor algebra you started with (`alg`, in the example) through the equivalence given by the monadicity of `ExprF`-algebras.
&gt; f :: a -&gt; m a I think you mixed up the parameter and result type.
http://community.haskell.org/~ndm/catch/ I don't think it compiles with recent GHCs.
What? It is virtually nothing but examples.
Thank you! That clarified my confusion.
Very helpful. I've read (and bookmarked for re-reading) your blog post as well and I'm excited to try out Control.Error the next time I hack something together. As somewhat of a Haskell "newbie" you are correct in that there is a lack of social confirmation about using Maybe and Either. They have an aura of being "inferior" solutions to something that is never mentioned.
It's because Simon Marlow's exceptions sucked all the oxygen out of the room at the time so there was never sufficient exploration of using typed and pure alternatives.
It is also virtually nothing but theory. When I said light on the examples, I meant that there is very little in the way of "Here is how you use this very common monad. And here is how you use this very common monad again. And here is yet another use case of this common monad." It is more of "Here is one example of this theoretical concept. Here is an example of yet another theoretical concept."
What is theoretical about "debugging" or "random numbers"?
The (object) correspondence goes about as follows: to :: Functor f =&gt; (f a -&gt; a) -&gt; Free f a -&gt; a to _ (Pure x) = x to a (Roll y) = a (fmap (to a) y) fro :: Functor f =&gt; (Free f a -&gt; a) -&gt; f a -&gt; a fro a y = a (Roll $ fmap Pure y) Verifying that the same thing pops out through round trip in either direction, and that the various requirements are preserved is left as an exercise for the reader. In other words, the category of F functor algebras is equivalent to the category of Free F monad algebras. So an initial F functor algebra is an initial Free F monad algebra. But there are categories of algebras of monads that are not free monads. For instance, the category of list monad algebras is equivalent to the category of monoids and monoid homomorphisms. This is the sense in which monad algebras subsume functor algebras. This also explains (more or less) why Free F 0 is an initial F algebra.
The examples are mostly in the form of definitions and lemmas, which I consider typical for theoretical learning. ---- I don't mean to start a war here, this is just one guy's deliberately critical impression of a very good monad introduction. Don't take it too seriously.
Sorry, i forgot about this)
The environment doesn't even need to be global. Local state is useful, too. Consider, for example, a combinator based library for talking to a database and transforming the results, possibly to text or html or *something*. In the end, you might end up with a function like: generateUsersPage :: UserID -&gt; HTMLDoc Most of your functions don't need to know about the UserID; only the lowest level database functions actually care about it, so it's a perfect time to use the Reader monad.
Does it require any knowledge of category theory? 
&gt; One can show that groups, as well as all other "algebraic" categories, arise as a category of algebras for some monad. As an example: Recall that you can retrieve the monoid operations using `mconcat`; `mempty` is `mconcat []` while `mappend a b` is `mconcat [a, b]`. The reason for this is that `[]` is the monad that gives rise to the category of monoids. `mconcat` is then the monad algebra that functions as objects in said category. Exercise for the reader: Prove mconcat [mconcat [a, b], c] = mconcat [a, mconcat [b, c]] mconcat [mconcat [], x] = x = mconcat [x, mconcat []] from mconcat [x] = x mconcat [map mconcat xs] = mconcat (concat xs) and vice versa. (I hope I wrote this right...)
Yes! just after I move to Switzerland!
Which is useful for performance. For an extreme instance, [start with "And finally, we come to widgets." on this page](http://www.yesodweb.com/blog/2011/03/hamlet-lucius-widgets).
Right. But this is a correspondence between functor algebras and monad algebras. (which I now see is what was actually claimed.) My problem is that I can't relate this to _monadic functors_ and _algebraic functors_. Furthermore, given some signature f for group operations, I don't see how moving from `Fix f` to `Free f` will give me more power to express axioms. It seems to me what `Free f` gives me is that I can always take `a -&gt; Free f a` by return. So if I have some `f a -&gt; a`, then I have `Free f a -&gt; a`. Now if those two preserve certain equivalence relations, then I have a monadic functor. But if they do preserve these equivalence relations is a property of the `f a -&gt; a` I have to prove myself. Otherwise I still have an algebraic functor, just from a category with a little extra structure. Either I'm missing something, or everyone is talking past everyone, or both. I'll have to spend some time with TTT or the like to figure out more...
Oh, certainly :-) I didn't intend to doubt this implementation choice.
I didn't actually use this package. I just used it as an example. But it's okay to just duplicate the definition of Fix and cata in your own code. 
I love these guys explicitly comparing their project with the Java couterpart. /semi-sarcasm
Ooh ok thanks!
I'm wondering if author can also compare to python implementation: http://scikit-learn.org/stable/
Here's the source on github: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/naive_bayes.py Their implementation looks pretty compact. The main part (for continuous normal data) takes about 10 lines, with another 10 lines of initialization. Also, they have excellent documentation for the end user, much better than either HLearn or Weka. The downside to their approach is that they appear to be manually estimating distributions within the naive bayes classes, rather than in a separate class for each distribution. I'm not sure why they chose this approach, but it means they have a semi-complicated class hierarchy of different naive bayes types for each different distribution. Also, there are no online training methods that I can see. Disclaimer: I've never looked at the scikit source before now.
An invalid traversal.
I have no idea whether there is sense in that comment or not, but I like how you worded your response, so have an up-vote :)
This is the exact same as the FPComplete post submitted 11 hours before this and immediately below it on /r/haskell..
Interesting, maybe this will be a push for us to finally open source our rest API DSL...
You probably also need to start ghci with the -fno-ghci-sandbox flag. Pretty much all GUI libraries do not play well with the threading that ghci uses by default.
Code size comparisons can be done disingenuously, or can be deceptive on accident. I do not get either impression from the article however. Is there anything in particular that stood out as something that could lead to misunderstanding by the reader?
I think I may see the confusion. The issue is that there is no functor `F` such that groups are `F`-algebras. Functor algebras can only characterize algebras without equations. That works for data types, but not most algebraic structures. However, every algebraic signature containing generators _and_ equations _is_ covered by some monad, such that the algebraic structures are monad algebras. The easiest way to construct such a monad is to use the adjunction between sets and the category of algebraic structures in question. This gets you the free-foo monad, and foos are (monad) algebras of the free-foo monad. So, monoids are monad algebras of the free monoid monad (list). But there is no functor F such that monoids are exactly functor algebras of F. You need to talk about monad algebras to characterize monoids and groups, because functor algebras don't cut it. There is also no functor `F` such that `Free F` characterizes groups or monoids. But that wasn't the claim. The claim was that monad algebras _in general_ cover algebraic theories with equations, _and_ all the things you can characterize with `F`-algebras, via `Free F` monad algebras. Does that seem more sensible?
`error (fix error)` has the right type, though.
Last I checked most browsers still don't work with PUT/DELETE requests and Rails had to hack around this with "_method=PUT" additions to the request string. Do all browsers now happily and accurately support all REST-ful operations?
I'm sure there can be much more sophisticated solutions to this - I would love to see yours, especially if you built a full-blown DSL around the concept!
Yeah, we took this pattern pretty much directly from Rails but I don't see why it couldn't work with non-integer IDs. The objective was to put together something simple/practical that we could use immediately to tidy things up a bit and could potentially be useful for others out there familiar with the Rails pattern. We're always happy to take pull requests... :-)
Descriptions of the error reporting in things like ATS always make me think about how people complain that GHC's error reporting is bad because it sometimes suggests a wrong way to fix the error it's reporting.
Thanks! This gets me much closer. Here's another stab at something: List is given by the adjunction between Set and Mon, and the algebraic structure is all `type ListMonadAlgebra :: [] a -&gt; a`, subject to some sort of naturality property such that it commutes properly with the monad operations. Like `eval . join === eval . fmap eval` or similar. So for a given equational structure there are _fewer_ monad algebras than functor algebras, _but_ monad algebras can express certain things that functor algebras (which must of necessity reduce one step at a time, via cata) cannot. I.e. there are more monads than there are free monads. However, my intuition that there are "fewer" in some sense monad algebras than functor algebras is misleading, in that every functor algebra can be turned into a monad algebra by taking it over `Free`. It is just a different monad algebra than one directly over `f a -&gt; a`. If the above made sense, I follow the original argument. Thanks for your patience! (That said, making the connection to monadic and algebraic functors is something I'm going to work on. Additionally, I'm not sold on the claim that "If you want to express something like groups, you need algebras for monads." I'm thinking of Hyland and Power's "The Category Theoretic Understanding of Universal Algebra" and related, available here: https://www.dpmms.cam.ac.uk/~martin/Research/Publications/2007/hp07.pdf The argument as I understand it is of course you get monads on Set out of lawvere theories [since there are adjunctions], but the move towards thinking with them is not a one-sided victory. We can look at the nlab page for example, to see an algebraic formulation of Grp with lawvere theories that doesn't bring monads directly into the picture: http://ncatlab.org/nlab/show/Lawvere+theory#the_theory_of_groups_20 ).
It doesn't strictly need the constraint because you could specialize it for a particular monoid (like "always choose the second"), but it's more general if you allow it to work for any Monoid.
I disagree. I think we should tolerate these comments even if they are intentionally trolling. They reflect common criticisms of Haskell and we always benefit from answering trolls in good faith because: A) It educates people on the sidelines who are not as firmly convinced about the benefits of using Haskell B) It points out real flaws in our community that we do need to address, either through better tutorials or improvement of the standard libraries.
The behavior of flushing on exit without explicitly giving a good failure code is arguably a bug, or at least an infelicity in the RTS. We should probably get that patched :-)
Here's something you can do: Replace your `[(CRUD, Handler App App ())]` with `Map CRUD Handler`. Currently you can have two `RCNew`, which is weird. Using map lets you eliminate that possibility.
It is talking about Stream fusion and similar tech. And it is, imo, kind of a good point. GHC often can't optimize recursive functions over recursive tyes when it can optimize the equivalent non recursive function over the free co-algebra based encoding of that type. There are probably fundemental reasons for this: impredicative System F is strongly normalizing.
With this new knowledge I feel like Haskell can now *"read my thoughts as I type them"*
Well… a REST API can still be useful outside a web browser, and out there, you can use methods other than `GET` and `POST`.
We moved to Facebook as devs, but the product still exists independently, and apparently people are still using it. We get a trickle of signups/downloads every day, with minor spikes when I mention it online.
Well, you can always download the software and try it, if you have the desire to make a cross-platform game in Flash. What would you like to know? I’m still working on a personal compiler project in Haskell, so all is not lost, but I haven’t yet had the opportunity to use Haskell at Facebook.
Not as interesting as I would like, as we talked about just the other week...
What kind of compiler?
Well, for the first part, I think it's just a matter of being careful about what you're saying in each case. If `T` is a monad, then there exist functor `T`-algebras that are not monad `T`-algebras. But all functor `T`-algebras are monad `(Free T)`-algebras and vice versa. And there are categories that are equivalent to the category of monad `T`-algebras for some `T`, but are not equivalent to the category of functor `F`-algebras for any `F`. As for Lawvere theories, that's just another way to do some algebra. They are stronger for that purpose than functor algebras, because they can encode equations. But there are monads for which there is no equivalent Lawvere theory (like, `Cont r` is an obvious example, I think), unless you go to generalized Lawvere theories (the details of which I don't know), which are equivalent in expressive power to monads, but lose some of their nice properties (like being able to combine any two theories/signatures easily).
While HTML Forms only support GET and POST (according to the specification), all major browsers support PUT, DELETE, and even PATCH via XHR**. http://stackoverflow.com/a/166501/82898 ** PATCH is apparently a bit tricky for IE7&amp;8, but doable - https://github.com/documentcloud/backbone/issues/2152
For a file-system description DSL. It compiles to C and Isabelle specs, and synthesises a proof of correctness.
Why not use Data.Functor.Foldable from recursion-schemes instead, as that is still maintained?
Anyone know how to set that up so it work just as easily in homebrew and/or macports and/or fink?
are you talking about the same as in http://engineering.silkapp.com/post/31922845283/a-restful-api-with-automatically-generated-bindings?
I have never said Haskell requires CT. On the contrary, I'm a zealous advocate of the opposite. I do, in a tongue in cheek manner, say Monad tutorials all suck. Of course they all fall on a continuum where there are both better and worse, but I have yet to see one that I feel is close to perfect. I never claimed the tutorial in question has no examples. I just said I would have liked more of them. It seems as though you have learned a lot from some strawman.
If the list is guaranteed to be non-empty, don't use a list. Use a `NonEmpy` from the [semigroups](http://hackage.haskell.org/package/semigroups) library. If it's really a list `drop 1` is always safer. The only reason to use `tail` is for optimization. But it's extremely rare for `drop 1` vs. `tail` to be the true performance bottleneck; that optimization is almost always premature.
No problem. Just use `unsafeCoerce`.
I saw a demo of their dev tools last weekend, and I think you're selling them short. In addition to the IDE, they have a really nice deployment and testing system built in. They are building your programs on the fly and you can deploy with one click - locally for testing, or to AWS for production. If you've ever tried to deploy a Haskell web app, you'll recognize that this is a huge deal. It's certainly true they'll be hard pressed to beat the feature sets of existing IDEs - I still write my Haskell in emacs, after all. That's not really the point, though. The point is that they will maintain blessed, compatible versions of GHC+many libraries and provide extremely convenient tools for building and deploying your haskell software.
So, I suppose no one should ever build an IDE ever again. Please just stop.
It doesn't, snap-core compares to scotty.
I don't know anything about KDevelop/deploy.sh. Currently, deploying Haskell code to AWS typically involves setting up a virtual machine that looks right, getting your files on it, building your whole project again (don't forget to install all the dependencies!), and sending it to the server. This is a big pain, especially to set up the first time. And yes, an IDE is part of the fpcomplete dev tools. It seemed pretty nice to me. But if you're really concerned about another IDE, you can also interact with everything via github.
No one should ever build a language specific IDE ever again.
so why do we wrap these three monads in newtypes? my bet is easier to reason about, amiright?
Let us pretend there will be no impedance mismatches between existing IDEs and the needs of a functional programmer.
For `Reader` and `Writer`, it's usually just the transformers that have newtypes, which is necessary. `State` needs a newtype because its representation can't be expressed as a partial application, and if it could it would still probably conflict with `Reader` or something. Another argument for the newtypes is that some people find error messages easier to read, especially in the case of `Reader`.
Yes
I tend to save my source code in wordpress. Literal Haskell ftw.
This might be a silly suggestion, there are also quite a few security concerns with it too, but there's also RNG using random.org I recently needed to populate /dev/random with enough entropy to generate a key on a remote server and used a script that got its entropy from random.org over SSL. One would argue though that RNG over SSL is "scary" compared to entropy generated by the keyboard/mouse etc...
GHC allows for integrations that go beyond what is possible with JVM, that said it needs an actual attempt to create something that could be called an IDE in order to excavate all sweet opportunities that GHC presents to "IDE makers". Good luck with the IDE project FPC!
The review is okay. I was just concerned about their aggressiveness, and haters it might bring out. mehh, I'm kinda timid.
How about this: evalCont k = runCont k id evalContT k = runContT k return
That saves barely any characters and doesn't seem to be any more clear (the long form is pretty clear).
Yeah the author is probably trying to balance impact vs negative backlash form being perceived as hype regardless of the truth of the statements. I think the disclaimer at the start of the article did a pretty good job coving his intention and the inherent difficult in such comparisons. 
Inappropriate. Please take content like this elsewhere.
&gt; This is a side-effect, and monads are great at side effects! This is an effect, and monads are great at effects!
 flip runCont id $ do { ... } versus runContId $ do { ... }
Cool! A small _executable_ for genetic algorithms! ;-P Add a `library` section to your Cabal, [docs here](http://www.haskell.org/cabal/users-guide/developing-packages.html#library).
Hi mcandre! Welcome to Hackage! I have a couple of suggestions. First, it's too late to change it now, but you probably should have named your package hellogenetics, since genetics is a very generic term, and now you've claimed it for what looks like a toy project (no offense if that's not intended). Second, your cabal file's Build-Depends section needs to state that it depends on a packages that provide the modules Data.Random.Source.DevRandom, and Genetics. Of you can provide those modules in your package.
fair enough. i just feel as though dismissing fp complete's purported attempt at an IDE because other good solutions exist is a bit short sighted. they are pushing haskell the hardest of any organization/company i can think of (cloud haskell, the online learning program, etc) and i trust they've put thought into it. i've been a vim user since i started programming and don't plan on changing that, but i also believe that fp complete is going to make their ide unique.
The first one is a huge rush! And then you get users, oh no!
Thanks for the response. That's actually pretty in line with what I thought it would be. Not too bad! Keep up the good work!
I have tried a few pipes and pipes-concurrency examples out in ghcjs and they seem to work great.
Wow, that's pretty nice. This must be a pretty high-quality port of GHC to JS because `pipes-concurrency` exercise several `ghc` features.
Congrats. :)
Just follow standard Haskell practice: avoid success at all costs!
The browser isn't the be-all end-all for REST. That's kind of the point of it. I wish people would only use the word REST when they're actually implementing REST.
I actually did try that as well, since it was suggested in the bug report I linked to. Sorry for not specifying this as well. Unfortunately, adding that switch didn't help.
There're also other links in the **Learning material** section on the right-hand side.
I have a few packages, but I haven't seen these users you mention
Agreed, but some people prefer to spend money for this kind of thing as sort of a self motivator.
For the record, if you live in the bay area, I will beat their price and personally tutor you for three days for $1399. :)
It isn't solving Bitcoin's problems. It is using Bitcoin to solve a problem.
If you are willing to teach a course of people 15-20 you could probably set up a class. Part of the benefit of these corporate classes is the networking an shared war stories and who ever the boss is probably feels better when 15-20 other bosses are paying that much as well.
That is not unrealistic for other corporate training camps, in other words the pice seems comparable. It is also small compared to how much you pay(or pay for, health insurance etc) the employee. MY very rough lower end, not including those in finical markets and other well payed areas, guess would 1/40 to 1/90 of a yearly wage. Since a year is ~360 days the break even point is saving 9 to 4 days worth of work by the employee is the break even point. Plus I guess the differential cost of what the employee could doing instead of learning at the classes. So it does not look that hard to come out ahead if the class is well organized and transfers a reasonable amount of knowledge. It also provides networking and gaining shared experiences form the other employees at the event. Another benefit is that companies that provide payment for training often is more attractive to potential employees then just receiving that extra money as part of their pay check. &gt; Is it really realistic to be able to learn all those things in only three days? To know them in depth or be an expert, no that will probably not happen. They probably will walk a way some additional knowledge, summary of the field, considerably fewer unknown unknowns and will know where to go to look for more information and hopefully a network of people they can call upon for help as well however. I am not associated with link class or anything similar nor have I been historically.
If you're still there in 5 months I will fly to the bay area and take you up on this offer.
Hmmm. I never really took this seriously before. My initial plan was to first write a book, though. I'd like to complete that before setting up a course because it would organize my thoughts and presentation better.
From reading your blog posts I would have to say I consider that an unnecessary (but very welcome) first step! Your thoughts and presentation seem perfectly well prganised already.
Thanks! I appreciate that! :) I was referring more to the overall architecture and completeness. There are a lot of important topics that I haven't even touched on and I want to make sure I cover most topics necessary to go from zero to a working project.
I will definitely be here in 5 months.
I looked at it because I'd drive to Denver for a weekend or even 2-3 day trip and pay for it myself (because we probably won't use Haskell at work) but I'm no paying 1400 put of pocket for it. I was hoping more like 200-300 although I knew that was out of range
eh? I can teach you those for free.
Without testing it: evolve n pool = compete pool &gt;&gt;= evolve (n-1) seems to be the problem, given that n = 2^16 I would think that seq on the compete pool should fix it.
I don't like the extra overhead (although I agree that some benchmarks would be great) since it will slow down one of the most basic building blocks of Concurrent Haskell. 
What is this primarily used for?
It seems primarily useful when you want "wake all" semantics.
Thank you! Could you show me an improved snippet for this involving `seq`? Also, I suspect there's a better version using `foldM`, but I'm not quite sure how to use it.
Sweet, are you also going to implement `atomicTryReadMVar`? Would it be sufficient just to peek at the closure without modifying it?
I don't use `MVar`s that often, so you'll have to explain that part, too. Where would you want to use "wake all" semantics?
Suppose you have some thread which is doing some operation, and there are many other threads which are waiting on the result of the thread. When the thread completes, you'd like to wake up all of the threads at once, rather than continually take and put back to wake up another one. Another case where I've heard people wanting this is because readMVar will temporarily empty the MVar, which can cause a race if another thread is running tryPutMVar (because it would like to write to the MVar, but only if no one else has gotten there first.) So here, it's the atomicity that helps.
Javascript is the de facto "assembly of the web" nowadays, and it's unlikely to change for the forseeable future. I used to wish for a bytecode approach as well, but I got convinced that since bytecode representation usually follow the semantics of a given language anyway, you might as well target Javascript. And that's exactly what people are doing, which seems to work out OK (eg, GWT).
Something I've wished for is a slightly different kind of MVar, where readMVar never blocks, ever. If thread A does a takeMVar, and thread B does a readMVar, thread B gets a stale version of the contents, instead of blocking until someone does a putMVar. Maybe STM TVars have the semantics I want, I'm not sure. My use case is a web app that's backed by some in-memory data structure, with the root of that data structure stored in an MVar or TVar. The [M,T]Var would effectively serialize all the writes, but there's no reason why a read should ever be blocked by a write, assuming all your data structures are proper pure, functional, persistent data structures that are never modified in-place. (Bonus points if the run-time system implementation is completely lock-free in the read path.) You might end up responding with slightly stale data in some cases, but if the client wants to do a PUT and then a GET, it had better not be issuing that GET until the PUT returns successfully, if it cares about correct ordering.
I don't think the compiler is guaranteed to do common subexpression elimination on that first example. I think it'd be better to do evolve n pool = seq completePool $ completePool &gt;&gt;= evolve (n-1) where completePool = complete pool not sure if this has anything to do with the leak though
gdb, compile with debugging on.
I've pinned down at least two specific use cases that cause it, finally.
Instead of `MVar a`, you could use a `IORef a` along side an `MVar ()` as a lock. If `atomicModifyIORef` is not sufficient for your use case, this will be, allowing you to perform IO actions as part of the modification process.
I don't think it's that, since IO's `(&gt;&gt;=)` is left strict, and `compete` is spine strict and at least as strict on the elements as `fitness` is. In this typeclass instance, that is totally strict. Every `compete` fully forces `pool` first. So it must be somewhere else ...
You could try adding logging to your code, either full-on logging or taking the lazy route and using [Debug.Trace](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Debug-Trace.html). Then you can figure out which function is causing the segfault and try to figure out why.
This is also not a great type for `shift`. For instance, you receive a continuation with type `a -&gt; Cont i r`. But in fact, it's an often mentioned property of `shift` that the continuation you receive is just a function, with no control effects. So, a better type is: shift :: ((a -&gt; r) -&gt; Cont r r) -&gt; Cont r a But it's also the case that no control effects from the body of shift escape, and one can signal this by giving it the type: shift :: ((a -&gt; r) -&gt; r) -&gt; Cont r a shift = Cont This imposes a slight obligation on the caller, though. Similarly, `reset` makes a scope from which no control effects can escape. And I can easily tell from the _type_: reset :: Cont r r -&gt; r that this is the case. So, with the above types for the two combinators, it's easy to see the regions in which control effects can occur, because things are bouncing between `Cont r` where they may, and (effectively) `Id`, where they may not. This scheme of course stops working if you have multiple prompts that can jump past one another; but `Cont` doesn't have that. Other, less informative types can be gotten back from these versions, too. For instance: reset' :: Cont a a -&gt; Cont r a -- this type was slightly off reset' = return . reset shift' :: ((a -&gt; r) -&gt; Cont r r) -&gt; Cont r a shift' f = shift (reset . f) But, the more informative types also have other advantages. For instance, if you want to use delimited continuations to invert a loop with cursors, and you have the `Cont`-laden types, I expect you end up having to invent a type that plumbs the `Cont` through the cursor, and accessing the cursor happens in `Cont`. But, the ones that move between pure functions don't need that, and it becomes clear that lists are all the cursors you need, even if the inversion is couched in terms of delimited continuations. They end up letting you do some things closer to native delimited continuations, I think, despite not being them.
Thanks. When I run `make profile` with these suggested changes, I see the same memory graph. Maybe something else is leaking?
The other replies to my comment might tell you why perhaps
Why would you want isEmptyChan? That information can become stale almost immediately after you get it.
The anwer is [down there vv](http://www.reddit.com/r/haskell/comments/1gdh5u/help_tracking_down_a_possible_memory_leak_in/cajf6fw)!
Derive `Generic`, then implement `Binary` as usual without providing any member implementations. For example: {-# LANGUAGE DeriveGeneric #-} import GHC.Generics data MyData = NoValue | OneValue Int | TwoValues String Int deriving (Generic) instance Binary MyData EDIT: removed superfluous `where`
As I recall you can't currently get a fully atomic tryReadChan either, for similar reasons. That one is an obviously useful one. I don't think these primitives in themselves will get us there, but tryReadChan is one of the great advantages STM chans have over MVar ones.
atomicTryReadMVar has always been implementable, even before this patch (since the reader doesn't need to block, you don't a readers queue anymore.) So it is trivial. Also, I don't see how it helps for batching, could you sketch out the code that uses it?
Ah, I was using a too recent version of the `binary` package. Running `ghc-pkg unregister` on it fixed the problem.
Pull requests accepted :)
Cool example. Why does the `master` function take a `[NodeId]` as its last argument? It seems unused.
I know, right? Seems the API wants the master actor to know its peers at the time of starting, but that doesn't even work right, because sometimes the slaves are slow to connect, so you have to query with `findPeers` anyway. And it does make for uglier, boilerplate actor content code.
It seems to me like it would be easier to implement this using a TVar, where the waiters would retry until the TVar becomes true; the implementation of STM is smart enough that this is not a spin-lock in practice as they will be put to sleep until the TVar changes. This seems like a much more natural way to solve the problem and one that is 100% immune from issues of atomicity. On the other hand, I just discovered that there was a change in the STM implementation in GHC 7.6 that introduced a bug that causes a segfault under certain conditions that I have run into but unfortunately been unable to isolate, so I'm a bit disillusioned with STM at the moment... :-/
When I was working on a lib I was getting a segfault too, for quite some time. I was using gdb and all of the debug switches for GHC I could find. Turned out one of the functions I was calling was from another part of the lib someone had wrote and it was using unsafePerformIO. Turned out I wasn't using that function properly (it required some kind of contextual state) and *it was the source of my segfaults*. Hopefully that might help you, as it took me HOURS till I caught a little "strange wording" in the docs for that lib and realized what was going on.
I don't see how that's an excuse to use sloppy terminology.
Hmm, that is a good idea that I will put on my TODO list since I would like to help out with GHC development if I get the time, but otherwise it doesn't matter whether it works on HEAD or not because the fact that it doesn't on the version used in the Haskell Platform means that if I want to release a package that will work with that people have installed then I have to code around the GHC I have rather than the one I might want. :-) Thanks for the suggestion!
but if it is fixed in ghc head it might be easier to find the fix and backport it.
What sort of size might an average home desktop computer reach? (I don't have access to one right now - and wouldn't know how to test it if I did!)
The language standard defines the `Integer` type as: &gt; arbitrary precision [integers](http://en.wikipedia.org/wiki/Integer) In implementations on finite hardware we must choose a representation. GHC uses the [gmp](http://en.wikipedia.org/wiki/GNU_Multiple_Precision_Arithmetic_Library) library, which represents `Integer` as a [tree structure](http://gmplib.org/manual/Nomenclature-and-Types.html#Nomenclature-and-Types) of word sized values. &gt; There are no practical limits to the precision except the ones implied by the available memory in the machine in GMP. The implementation [has some limits though](http://gmplib.org/list-archives/gmp-bugs/2009-July/001538.html) - e.g. the use of *operands* (in limb format) of 2^37 bits (a 16GB integer in gmp representation) Other representations (e.g. [multivariate polynomials](http://www.cs.rit.edu/~swm/cs561/course/funMultiVar.html)...) can have different complexity and different limits on the maximum value they can represent. Simple take away: GHC's native `Integer` lets you represent very large numbers. For extremely large numbers you might need to think about other representations. 
Interesting stuff, thank you! I was hoping to get some rough scales, even just in order of magnitude, for the size of an integer Haskell could handle. Would 2 raised to the power of the number of bits in available RAM be a reasonable estimate? Or does Haskell ever use solid state memory?
So does this mean 2^(number of bits in 16 gigabytes) is the largest integer that can be handled? Or would if be half that, because of negative integers?
The largest number depends on hardware limits addressing structures inside the given representation. E.g. GMP's limb representation will have limits dictated by hardware. I think we'd need to dig into GMP internals to find the upper bounds.
No idea, but I guess that one bit isn't that significant here.
If we assume unlimited RAM, then yes, in the ballpark of what you say.
Have you looked at Text.HTML.TagSoup?
True, but the older versions of GHC that still have the bug will remain out in the wild for long enough that I can't assume that everyone or nearly everyone who might use my package will have a version of GHC without the bug. Also the concurrency was simple enough that converting from STM to MVars wasn't a big deal.
Where do you get the tree structure? (And I think there are other backends for arbitrary precision integers which GHC can use.)
Blimey - what a list! Better get my metaphorical reading glasses out :)