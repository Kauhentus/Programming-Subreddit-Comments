When I tried Yesod first time I couldn't understand it at all. But after writing a small application with Snap which is modular and very understandable for newcomer I could easily identify the same patterns in Yesod and write Yesod applications easily with greate help from Yesod book. 
- A server side application is a backtracking parser that read a stream of requests and returns a stream of responses - A route is a pointer to a closure (or defines it) - `&gt;&gt;=` bind a closure to a continuation - A parser is a particular usage case of a monad with state - Parallelism and indetermism are similar effects, have the structure of the list monad and can be modelized as such. - Concurrency is the effect resulting from the execution of an applicative expression in a parallel monad - Callbacks and loops destroy composability, but can be encapsulated within composable primitives in a monad that handle continuations and parallelism so they can be made composable. Reactive applications can be composed. - Errors, exceptions and resource recoveries can be considered analogous to parser state recovery trough backtracking. - Distributed computing can be made composable by using the above effects. - Applicative and alternative instances with these effects can modelize algebras for any domain problem that keep the invariants of the problem while performing any effect necessary. Examples are: a relational algebra or a non commutative, effectful algebra using the Num class. The monad binds algebraic expressions. - Therefore, an algebra of software components implementing each one an individual user requirement is thus possible. So the dream of writing requirements and making them run is possible with Haskell. Not only for mathematical, single threaded, pure applications, but for any kind of application 
This talk is about bidirectional parsing/printing libraries such as JsonGrammar, invertible-syntax, roundtrip, boomerang and many others. Using these, we can generate multiple programs from a common description. It's strongly related to generic programming, but (depending on your viewpoint) not quite the same. The talk discusses this relationship.
They're linked from the conference page for the talk: http://bobkonf.de/2017/loeh.html
 What is that `:&lt;`
link? my google-fu is failing to find it...
It's a trick to allow infix data constructors. Usually data constructors have to start with an uppercase character, as `Node` (of type `a -&gt; [Rose a] -&gt; Rose a`) data Rose a = Node a [Rose a] If you want to write `Node` infix it needs to start with a colon `:`, for example `(:&lt;)`, same type as `Node` data Rose a = (:&lt;) a [Rose a] ---- This is very common for defining mathematical operations data Exp = Lit Int | Add Exp Exp | Mul Exp Exp can be written data Exp = Lit Int | Exp :+ Exp | Exp :* Exp and then the evaluation looks quite pleasing eval :: Exp -&gt; Int eval (Lit i) = i eval (a :+ b) = eval a + eval b eval (a :* b) = eval a * eval b
Which can be made linear with a codensity representation. Don't remember off hand what that looks like for applicatives...
One thing that I've learned to do is `seq potentialInfiniteLoop True` in ghci.
If you want to get fancy you can define a general binary operator data Exp = Lit Int | Binop String Exp Exp and define pattern synonyms {-# Language PatternSynonyms #-} pattern (:+) :: Exp -&gt; Exp -&gt; Exp pattern a :+ b = Binop "add" a b pattern (:*) :: Exp -&gt; Exp -&gt; Exp pattern a :* b = Binop "mul" a b Now you can treat `(:+)`, `(:*)` as regular constructors &gt;&gt;&gt; Lit 1 :+ Lit 2 BinOp "add" (Lit 1) (Lit 2) and pattern match on them eval :: Exp -&gt; Int eval (Lit i) = i eval (a :+ b) = eval a + eval b eval (a :* b) = eval a * eval b eval (Binop op _ _) = error ("Operator '" ++ op ++ "' not supported.")
Agree, I think the Yesod book is great and cover everything you need get you started : Haskell , Web dev + databse. I would however, recommend trying to NOT use Yesod scaffolding (especially for small project) as I found it really hard to grasp the overall file structure. The good stuff about Yesod is , even if you don't like it, it's quite easy to remove and replace the bits you don't like. 
I *think* you can fold over it to reflect on things just fine. How you'd do that probably depends on what you're trying to accomplish, but I'd be surprised if you could find something you can't use the church encoded one for. In fact, you can always use it for fast construction and then convert it to the regular free Monad.
Here's a solution that uses the more "beginner" Haskell concepts of `map` and `lambda functions`. (Since you say you're new to Haskell, you may not be ready for Applicatives, cool as they are.) I found that the best way to approach this was to think of 2 phases: 1) apply the list of functions to a single value, and 2) map that function over the list of values. So: applyFuncs listOfFuncs val = map (\f-&gt;f val) listOfFuncs funcsAndVals fs vs = concat$ map (applyFuncs fs) vs 
Sure, the two representations are isomorphic. However, compare for example the list `[1..1000000]` with the closure `\c n -&gt; foldl' c n [1..1000000]`. What happens if you only want to access the head of the list? In the former case, you unwrap a single constructor, which if it's lazy in the tail results in no further evaluation. In the latter case, you have to evaluate the spine of the entire list, using an accumulator like `First` to throw away the remaining elements. So: equivalent in one sense, yes; but computationally equivalent in every sense, by no means.
But you can definitely get lazy head with `foldr` without evaluating the whole spine. You can do the same with the church encoded free monad.
Not sure what the final encoding has to do with his? That's a different thing. For one thing, you bother with the church encoded one because it's vastly more performant. Asymptotically, it solves the left associated bind problem, and it has constant time fmap. Secondly, you don't have to just provide the Free constructors to do useful matching. You can stop at whatever depth you like without converting to Free
Actually, lots of people think, that, for the fundamental functions, such as authentication or DB operations, `servant` is not that productive comparing to `yesod`.
I'm not sure we're talking about the same thing anymore. I'd recommend trying some code examples to compare. There is no way you can "have your cake and eat it too", by mixing ChurchFree and Free as suggested in your previous comment. That's why all these approaches exist: because there's a tradeoff associated with each method. Initially encoded gives you a value whose layers you can individually inspect: `Free (f (Free (f (Free (f (Pure _))))`. Finally encoded gives you a construction that elides these by "bundling" the traversal/fold into the closure itself: which can now be any computation, not just a value construction. Initial can be converted to final using an evaluator that builds such a closure; final can be converted to initial using `Free` and `Pure` as the functions you pass to the fold closure. Initial lets you step through the value, layer by layer, stopping whenever you want, applying whatever logic you want, very easily using regular case analysis and branching. Final requires folding through the entire structure, applying your provided functions at every point, meaning you have to "invert" any logic you want to apply in order to be selective about what you evaluate. Try writing "tail" for the Church version to see what I mean. This is covered in detail in the paper [Church Encoding of Data Types Considered Harmful for Implementations](https://ifl2014.github.io/submissions/ifl2014_submission_13.pdf) by Koopman. The end result is that you can't just "get good performance" by using Church, and then expect this to be equivalent to the reflective power offered by the non-Church approach. This is the exact point made in depth in the [Reflection without Remorse](http://okmij.org/ftp/Haskell/zseq.pdf) paper, where they address this tradeoff and then discuss their compromise solution.
Not sure why you're being downvoted as this certainly adds to discussion. O also find that API problematic in the manners in which you describe.
This implies that there was no reason for 'reflection without remorse'.
I think I need to reread that paper
that is a v nice explanation - thanks!
`(&gt;&gt;&gt;)` is `flip (.)` not `flip ($)`. Eg. `1 &gt;&gt;&gt; succ` is a compile error. Also the more "natural" place to import it would be `Control.Category` not `Control.Arrow` (if a bit more awkward as you will either have to hide `(.)` and `id`from `Control.Category` or `Prelude`).
For some reason I was thinking that the final encoding meant something other than the church encoding. newtype Final f a = Final (forall m. Monad m =&gt; (forall x. f x -&gt; m x) -&gt; m a) Anyway, you're right that you can't guarantee that you won't traverse the whole spine, but for all practical usages, this won't forcefully evaluate the next step in the computation. It is a very cheap linear cost though, whenever you choose to finally traverse the rest. But the cost of seeing the outermost `f` itself will be constant under typical usage of `liftF`, `(&gt;&gt;=)`, and `fmap`. tail' :: Functor f =&gt; F f a -&gt; Maybe (f (F f a)) tail' (F f) = justRight $ f Left (Right . fmap (either return wrap)) 
The author is a lady, though.
Given the constants involved, reflection without remorse can be quite difficult to exploit. If you can get away with a single inspection or multiple inspections that won't be followed by by subsequent binding, then using one of the CPS'd variants is better. If you need multiple inspection round-trips and will continue binding then there is a size at which reflection without remorse will break even. The constant factors make reaching that point difficult.
Your answer to this example looks incorrect to me, can you elaborate? &gt;apply [head.tail,last.init] ["abc","aaaa","cbbc","cbbca"] "abc" For starters, two functions over a list of length 4 should yield 8 answers. I expect either "babbbabc" or "bbabbbc"
I don't think that changes how professional or unprofessional that name is. I wouldn't want to explain what software I'm using under that name, for instance.
Every inspection of the church free monad involves running an arbitrary computation over from scratch. Let's look at something like Codensity: newtype Codensity f a = Codensity (forall r. (a -&gt; f r) -&gt; f r) /// lower . lift = id lower :: Monad m =&gt; Codensity m a -&gt; m a lower (Codensity m) = m return Now let's so something with say, Codensity Maybe The function (forall r. (a -&gt; r) -&gt; Maybe r) might compute some arbitrarily complicated thing that takes 2 hours to compute and spits back a final optional result. If you lower this thing a second time, nothing prevents it from taking another 2 hours to compute! It doesn't memoize the result when lowered multiple times. If you do something like foo m = case lower m of Just a -&gt; m &gt;&gt;= more stuff Nothing -&gt; ... where you carry on the computation, above, it'll recompute all the work that it took to compute the `Just a` to proceed. This is obviously a dumb idea with `Maybe`, as you can convert `Maybe a` back to Codensity Maybe a` cheaply. But what about `Free f`? We can convert ChurchFree -&gt; Free -&gt; ChurchFree readily, but each round trip pays to convert an entire syntax tree or O(n) work in the size of the tree. It touches all of the parts of the `Free f`. Laziness doesn't save you, it just defers the costs until later! Each round trip is adding more thunks between you and your data. If you just try to do the ChurchFree -&gt; Free, and then carry on binding the original ChurchFree you pay for the cost of that "potentially 2 hour computation" from scratch each time, and there is no way to peel off a layer and continue binding with just the inside without re-peeling every time you do the inspection. Reflection without remorse makes is so you can bind on the inside cheaply O(1) and inspect on the outside cheaply in O(1) multiple times. Before it came along I didn't believe it was possible.
I have no particular objection to using `$` for this (although I dislike `$` for simple argument application). I do have an objection to `ArgumentDo`.
Well Boehm–Berarducci (aka "Church") encodings are terrible for performance; i.e., they don't give the induction principle you want for asymptotically efficient programs. So, as long as that's the output of optimization, there's only so much you can do...
*all* things? Surely not
 instance A a =&gt; B (D a) is read "backwards" by the compiler. Say you are in the body of a function and need an instance for `B (D [Int])`. The compiler looks through the set of instances that were passed to the function and the stuff in global scope and tries to figure out if it can resolve that constraint. It has only a couple of tools for doing this. Given a class declaration it can walk up to the superclasses of the class. e.g. Given `Ord [[[[[a]]]]]` I can tell you `Eq [[[[[a]]]]]`. Given an instance declaration it pattern matches on the right hand side. instance ... =&gt; B Wat instance ... =&gt; B (C a) instance ... =&gt; B (D a) and basically pattern matches like you would when writing a function using these as the patterns. When it finds a match, the instance tells it what obligations it has yet to prove. So here if we're looking for B (D [Int]), and have a instance head B (D a) it fixes a = [Int] then looks at the left hand side of the =&gt;, finds out it needs an instance for `A [Int]` and carries on by looking for such instances in the current environment as a subgoal, trying to use new instance rules or the superclasses of stuff in the current environment to find it. This turns into straightforward code you wind up with the compiler having a list of instances that are in scope, and a 'goal' constraint to satisfy, which it does by 1.) looking at instance heads to pattern match against the goal, and simplifying the 'goal' into one or more "simpler" subgoals that can be discharged off the list of constraints. 2.) Add the superclasses of all of its instances in scope to the list of instances in scope, and trying again. 3.) Report failure of the current subgoal as a type error if this fails to add anything new to the environment and the instance matching step also failed. Otherwise loop and continue. i.e. if I need Ord a :- Eq [[[a]]] and have class Eq a =&gt; Ord a instance Eq a =&gt; Eq [a] `Ord a :- Eq [[[a]]]` incurs a subgoal `Ord a :- Eq [[a]]` by matching on the instance head, which incurs a subgoal `Ord a :- Eq [a]` by matching on the instance head, which incurs a subgoal `Ord a :- Eq a` by matching on the instance head, which is finally discharged by using the fact Eq a is a superclass of Ord a. 
Without the "no backtracks" then someone could later write an instance in a module you don't know about that should change the behavior of code you've already written and compiled. For example if you could say "Hey if this is Ord do this, if not, then if its Hashable do that" and then someone later showed up and defined an orphan Ord instance for your data type, the old code that was previously going to take the Hashable path now should switch over, changing semantics and requiring the entire program to be known before we know the correct behavior. This would violate the "open world" assumption where you can learn new facts later on, and folks can keep adding data types and classes to your program without changing existing behaviors.
Yeah, not *literally* everything.
A couple of people have already made this point, but I wanted to give a concrete example. Let's say you are implementing a backtracking monad transformer (i.e., ListT "done right"). There's a CPS version and an initial version, newtype BTC m a = BTC (forall b. (a -&gt; m b -&gt; m b) -&gt; m b -&gt; m b) newtype BTI m a = BTI (m (View m a)) data View m a = Nil | Cons a (m (View m a)) Ignoring some exotic terms, these are equivalent, but `BTC` is usually faster because it doesn't need to walk the `View` structure. The exceptions are operations like `interleave`. `interleave a b` evaluates `a` until it returns a result, then switches to `b` until its first result, and continues alternating until both are complete. Oleg showed how to write `interleave` for `BTC` using the `msplit` operation, msplit :: Monad m =&gt; BTC m a -&gt; BTC m (Maybe (a, BTC m a)) but `msplit a` creates overhead proportional to the length of `a`, and `interleave` will call it a linear number of times. In my experience, it's much more efficient to have `interleave` convert `BTC` to `BTI` once, do the interleaving, and then convert back. Now you have a linear amount of overhead, instead of quadratic. Note that this is distinct from laziness: neither implementation of `interleave` evaluates more of their inputs than is necessary to produce the demanded results. The difference is how much conversion bookkeeping they involve.
A bilinear map on A and B is a linear map on A ⊗ B, hence the notions of bi- and multilinear maps are redundant.
Teach them like kids learn an spoken language: the dollar is the dollar, is must be put where a dollar must be. You know by the examples. Make yourselves an intuition of where $ should be put. In the same way, period is a period and you know what I mean. Period If they continue asking: `left $ right` means to get all that is in the left side an pass to it the right side as argument
Reveal.js is very nice for code based slides, yes. Pandoc for reveal.js has several limitations though, because it doesn't allow you to have text in your horizontal slides (for interoperability reasons with beamer and latex presentations). I recommend [reveal-md](https://github.com/webpro/reveal-md), a wrapper around reveal.js that let's you write markdown without the limitations of Pandoc, plus nice features like speaker notes and live-reload
A lot of people not liking Yesod in this thread. Well: I like it :) Yesod is a complete solution for apps serving HTML. Thus not for the more modern "singe page" apps that render by JS in the browser and mostly only send JSON over the wire. These more old school apps are still very useful, as they "just work", and do not have to go with the latest-JS-fashion-du-jour. I think that apps supporting business processes are a good fit, they usually do not need flashy JS stuff. So when having/choosing to build such an app, Yesod is a great fit. It pretty much includes all you may need: i18n, templating languages, reasonable db abstraction, test framework, authentication plugins, etc. Quite some ecosystem of libraries exist. Your beginner questions are usually answered somewhere a simple Google query will take you. A reasonable community exists and is helpful. And it seems to be kept up-to-date (for instance it currently supports HTTP/2 through Warp). I do not think Yesod is soo hard to get started with. If you want to prepare yourself better I can vouch for the Haskell (from first principles) Book. Either way good luck!
I second this. You have access to a plethora of latex packages, like pgfplots for graphics, or algorithmic for pseudo code, plus you get pretty formatting for free.
I haven't explored or thought about this as much as I'd like to; take that syntax with a pile of salt. A static value is any value that is known at compile time, so it would have to be some sort of compiler magic (eg all top level terms and literals have an implicit Static constraint).
the tutorial explains that .~ is a lens, and it builds up to that program; but it should mention the verbose record namespacing (which isn't frp related btw). https://github.com/reflex-frp/reflex-platform/blob/develop/README.md
+1! You took the words right out of my mouth
haven't used gitlib, but make sure the documentation you're reading is the same version as the install package. also: do s &lt;- readFile "file.txt" putStrLn s is the same as: readFile "file.txt" &gt;&gt;= putStrLn i.e. everything to the right of the left-arrow has type IO. skimming the tutorial (whose docs are broken, sigh), if: repo &lt;- openOrCreateRepository path False works, then the expressipn `openOrCreateRepository path False` should have type `IO Repository`. or something more general like `MonadIO m =&gt; m Repository` or (if it exists) `MonadGit m =&gt; m Repository`, which needs `liftIO :: MonadIO m =&gt; IO a -&gt; m a` https://hackage.haskell.org/package/gitlib-3.1.1/docs/Git-Tutorial.html (apologies if these are unexplained concepts, some answer should be more helpful than no answer. i'd recommend any package by Gabriel Gonzalez, like pipes or turtle. they have tutorials that are accessible to beginning and advanced Haskell programmers alike, since they explain or at least mention what abstractions are being used by the API.) https://hackage.haskell.org/package/pipes-4.3.2/docs/Pipes-Tutorial.html but idk, i only glanced at this. 
Ah, good call! Yeah. I have no idea how any of this plays with anything, it just strikes me as a hill that may or may not have gold buried in there.
I confess to just using Keynote or Powerpoint. I'm boring.
Can you get `ZZ` for free by defining it as `Compose Z Z`? The comonad package has instances for `Sum` but not `Compose`, so I'm guessing the answer is no?
Rumor has it that that kind of functionality is in the works with Reflex.
I mean that we have modules like module PrettyShow.Text where import Data.Text as T class PrettyShow a where prettyShow :: a -&gt; Text default prettyShow :: Show a =&gt; a -&gt; Text prettyShow = T.pack . show
oh heck! i missed this!
[The optimal implementation of functional programming languages](http://dl.acm.org/citation.cfm?id=320040) is an excellent book on this subject
That seems like something that would easily be solved with blessed packages. Have the test package with the orphans be the blessed package for interaction with quickcheck of the production package.
https://hackage.haskell.org/package/pretty-display maybe?
It isn't. It renders in the PDF document so I'm not sure why that's happening.
&gt; this isn't going to help people learning Haskell &gt; Then when they go to look at real Haskell code in the wild, they still have to learn the $ and . business Certainly agree: if the goal is for the "slightly-more-artsy-than-mathy e-music coder" to "learn proper Haskell" and be able to read any "real Haskell code in the wild", then that's true enough. My interpretation was that this sort of audience is just mildly curious about the symbol, seeing that it's common semantics (US currency unit and curiously enough still global benchmark of currencies and asset denomination) ---while certainly being all about "flows"--- does not readily lend to an "adequate enough" intuition about "which side of this operator is fed into which other" =)
 import Control.Applicative import qualified Data.Set as Set apply :: Ord u =&gt; [v-&gt;u] -&gt; [v] -&gt; [u] apply fs vs = Set.toAscList . Set.fromList $ (fs &lt;*&gt; vs) why: Set is unique set of Ord(ered) values. Set.fromList feeding Set.toAscList will therefor construct/de-construct your list of unique items. `&lt;*&gt;` is the applicative operator, generally stated, it says 'Take from right feed to left' The applicative instance of list works like this: [(+1)] &lt;*&gt; [1] == [2] [(+1)] &lt;*&gt; [1,2] == [2,3] [(+1),(+2)] &lt;*&gt; [1] == [2,3] [(+1),(+3)] &lt;*&gt; [1,2] == [2,3,4,5] {-You can also chain the operator to feed n-arg functions It starts getting kind of hard to reason about the order stuff shows up in though. -} [(+),(*)] &lt;*&gt; [1] &lt;*&gt; [2] == [3,2] [(+),(*)] &lt;*&gt; [1] &lt;*&gt; [2,4] == [3,5,2,4] [(+),(*)] &lt;*&gt; [1,2] &lt;*&gt; [2,4] == [3,5,4,6,2,4,4,8] The applicative instance of list is a great way to generalize "do these things to these other things, and collect the results" `zipWith ($)` is also really great if the order of the two lists relative to each other is important. Be careful - Unlike `zipWith`, `&lt;*&gt;` doesn't "just stop" on the shorter of the two lists, so, you will get a non-terminating function on infinite lists. Don't get too scared about importing modules - The stuff in a lot of the `Data.Containers` module is pretty essential, and is usually much more idiomatic then sticking to the Prelude. `Data.Map.Strict`, `Data.Set`, and `Data.Sequence` are all pretty well documented and easy to work with, and are very useful in a great many simple scenarios. `Control.Applicative` is great, it'll probably be awhile before you use it for anything other than lists, but, just the list instance is extremely useful by itself.
Checking identity is enough. Composition law is a consequence of the identity.
/u/jaspervdj wrote [patat](https://jaspervdj.be/posts/2016-10-02-patat-myanmar.html)
Just keep programming stuff. The [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia) is a good reference for typeclasses in the standard library. If you don't have a background in abstract algebra, just treat `Category`, `Arrow`, et al as useful abstractions provided by the standard library and if they are a good fit for your programs, use them. The theory is really interesting (and useful), but not necessary at all to understand /u/jan_path's point. Their point was just that the types don't match: (&gt;&gt;&gt;) :: Category cat =&gt; cat a b -&gt; cat b c -&gt; cat a c -- Specialize to the (-&gt;) instance of Category (&gt;&gt;&gt;) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; (a -&gt; c) -- (-&gt;) is right-associative, so the last parens are redundant flip (.) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; c -- Not the same type, ergo, not the same function. flip ($) :: a -&gt; (a -&gt; c) -&gt; c In addition, since `(.)` is most generally defined in `Control.Category`, and `&gt;&gt;&gt;` [is implemented with](https://hackage.haskell.org/package/base-4.9.1.0/docs/src/Control.Category.html#%3E%3E%3E) `(.)`, it's not necessary import `Control.Arrow` to get to it and more things have `Category` instances than have `Arrow` instances, since there are fewer requirements on a `Category`. Since `Category` defines both `(.)` and `id`, it's ambiguous if you use the `(.)` and `id` provided by the Prelude as well. An easy fix if you just want `&gt;&gt;&gt;` is `import Control.Category ((&gt;&gt;&gt;))`. 
Haskell from first principles is great. I have just started Haskell half a year ago myself. I read HPffP watched a lot of talks, a bit of programming and also I like just reading the haddocks of interesting packages. :) That brought me pretty far I think.
Well, as the article points out, the version with comonads is faster and uses less memory and the function modeling the rules of GoF looks simpler too.
Thank you for the better explanation. And while I agree that the `Control.Arrow` vs `Control.Category` part was a bit unnecessary I don't think the `($)` vs `(.)` part was. Beginners often get that wrong, so I at least wanted to pointed out. I should of course have also explained why `(&gt;&gt;&gt;)` is analog to `(.)` and not to `($)`. `something &gt;&gt;&gt; operate &gt;&gt;&gt; operate` also made me think that he probably misunderstood `(&gt;&gt;&gt;)`, as it should really be more like `operate &gt;&gt;&gt; operate &gt;&gt;&gt; operate`.
The freer monad involves extensible effects. If you don't want that there's also [monad-skeleton](https://hackage.haskell.org/package/monad-skeleton): It's as fast as freer but has an interface similar to `operational`.
Yeah, I realized that after I was done and reread the question but I figured I might as well submit at that point. Sorry for that!
Keynote with a nice template is still a good tool.
&gt; What do you use for slide presentations Comic Sans
Fascinating. Do you have some examples for the last point?
I really liked using reveal.js, since I felt I got a huge amount of control over my slides. Here's the slides I made with it: http://maxgabriel.github.io/talks/objc-at-runtime/#/ Favorite thing was having a big code snippet on screen and highlighting specific portions of the code using fragments. For my presentation I also had a timer showing the duration of the video clips in my presentation that I wrote in javascript, which was part of my presentation (emphasizing how much longer it took to write code without autocomplete). Finally using SVG for illustrations was nice. 
/u/f0rgot I would agree with above regarding starting with a simpler server. I've used Scotty myself and it's a awesome! If you do go down the Scotty route there are a bunch of example projects listed here: https://github.com/scotty-web/scotty/wiki
Yea I've come to understand the point. You can get tail to *run* in constant time, but the DSL it produces will be linearly slower.
Can someone ELI5?
&gt; ccompiled JS size The JS is big, but it compresses quite well with gzip or zopfli--even better with the Google Closure Compiler. &gt; partial/on-demand loading of JS I don't know what you mean by this. &gt; Is there any benchmark comparing in-browser performance of reflex-dom with reactJS and NG2? Not that I know of.
[Landslide](https://github.com/adamzap/landslide) is quite nice.
Thanks for the solid response! By low level guy, I meant I primarily sit at the domain just above hardware (mostly C, sometimes Assembly) or doing hardware itself (mainly Verilog, Chisel sometimes)
couldn't stop laughing
It looks to me that list comprehensions do more harm than good, especially for learning purposes. There's rarely a simple learning exercise that can be written simpler and clearer with list comprehensions than with plain function application and / or recursion. 
I don't understand what you mean. Do you mean using the `newtype ZZ` instead of just `type Z (Z a)` ?
Fun stuff. This is very similar to my `fclabels` based/inspired `pickler`[1] library that I played around with a (long) while time ago. I used this for writing token parsers and pretty printers in one. Instead of using type level stacks, it uses an `Applicative` and `Alternative` instances (combined with some magic) to write down composable bidirectional code. This allows you to compose primitives 'almost' like parsers: many :: Monoid i =&gt; Pickler i a -&gt; Pickler i [a] many p = Pickler $ (:) &lt;$&gt; head &gt;- p &lt;*&gt; tail &gt;- many p &lt;|&gt; pure [] some :: Monoid i =&gt; Pickler i o -&gt; Pickler i [o] some p = Pickler $ (:) &lt;$&gt; head &gt;- p &lt;*&gt; tail &gt;- many p optional :: Monoid i =&gt; Pickler i a -&gt; Pickler i (Maybe a) optional v = Pickler $ Just &lt;$&gt; just &gt;- v &lt;|&gt; pure Nothing [1] https://github.com/sebastiaanvisser/pickler/blob/master/src/Pickler.hs
It doesn't actually solve the left associated bind problem. It makes bind take constant time, because all it has to do is construct a closure. It is also kind of friendly to the inliner. However, if it doesn't inline you still end up with this chain of closures that will need to be traversed when you actually run the thing.
I think this may be a labeling issue? Actions on zippers are a great example of a comonadic operation. To get the performance benefits of a "context", in this case being able to access your neighborhood with very efficient operations (in my case `O(1)`), you are going to end up "mapping" over your `Zipper a` but you are going to need more than just the `a` value to get the context efficiently. Whatever you do to have that context available while mapping is going to be a comonad, whether you write the instance or not. So, this post isn't really about "look at all the cool tooling I am able to leverage by recognizing a comonad", as you could probably do with monads, but more "let's recognize that the task at hand fits a well-defined algebraic notion that allows us to be more efficient than naively querying our data structure in `O(n^2)` time.
On the Mac, [Deckset](https://www.decksetapp.com). Has syntax highlighting, good themes, and LaTeX! (Been using for the past couple years. Very happy customer)
&gt; initial ... final would you mind clearing up (or pointing to a resource) the difference between initial/final encodings? I've studied cat theory and have a rough sense, but as it applies to free/mtl, I'm rather lost. 
To clarify, Yesod works fine for single page apps too, and the scaffolding now includes an API usage example (creating a comment from an Ajax request). If making a true single page app you should be able to just remove packages like yesod-form to slim things down a tad
ADTs vs typeclasses isn't realy related to ad hoc vs parametric polymorphism. Parametric polymorphism is where something works identically for every type. `length :: forall a. [a] -&gt; Int`is parametrically polymorphic: it doesn't care what kinds of elements are in the list; it works identically over all of them. Ad hoc polymorphism is about getting different implementations for a finite set of types. For example, `+` should work differently for `Int`s, `Double`s, and shouldn't compile when used on `ResultSet`s. ADTs vs typeclasses is related to the expression problem, though, as others have mentioned. It's easy to add new types to a typeclass, but difficult to add new functions to it. ADTs are the reverse.
Now you're stretching the definition of [side-]effect
Yeah once it got more usable to the point of potentially being used by people other than me it needed a new name. 
How is that implementation of `static` better than the one available via the [StaticPointers](https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/static-pointers.html) extension whose primitives live in [GHC.StaticPtr](http://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-StaticPtr.html)?
Nice, this _is_ the magic `static` from the Cloud Haskell paper! Looks like they did implement it after all. The `distributed-process` version was implemented before that, so its only advantage is that it doesn't require the extension. And also that it works with the rest of the `distributed-process` ecosystem.
Yesod is kind of its own thing. The book is definitely great, but it uses enough DSLs that's it's relatively foreign from Haskell proper and that can be off-putting.
I actually think the simple frameworks are best for experienced haskell web developers, ironically. Yesod is very opinionated and that makes for apps that "just work" out of the box. 
`Int` should only ever mean an arbitrary-precision integer. If you mean `64-bit integer`, write `Int64`.
Yes sorry, what is a general term for these diagrams? [great link by the way]
What do you believe helps? The syntax is not hard to learn, but it gets hard to find problems to solve.
For the last however many years I've been browsing this subreddit, there has always been a mix of questions in with the links / blog posts / etc..., and some of them seem to hold their own in terms of upvotes and the discussion they generate. I think that upvotes and discussion are a decent proxy for what the community considers to be on topic or off topic for the subreddit, and if that measure holds any weight then I think questions are on topic. There is a great non-beginner question about DSLs on the front page right now, that has been hovering around the top of the posts and generating discussion. It feels like the kind of post that this community tends to like - should it have appeared in /r/haskellquestions? Would it have been received as well over there? When folks get redirected to /r/haskellquestions, the phrasing really bugs me when it's an absolute statement about what this subreddit is for. If it was just a simple "you might have more luck over there" (although due to the relative number of subscribers I have no idea if that is true), I'd be fine with it - and if folks wanted to try to shift the community standards so that it was questions over there and other things in here, I'd have no problem with that either. With that said, I'd be happy to see a lot of the beginner questions moving over to /r/haskellquestions :) Sorry for the rant: none of it is personal, and I don't really know why this particular thing bothers me as much as it does :/ 
Absolutely! I think your post was totally reasonable. /u/grizwako just didn't have enough context with Haskell yet to see where you were coming from. I mostly wanted to make sure that /u/grizwako wasn't going off and learning Yoneda's lemma under the impression it was required to know how to use `(.)` and `($)` when some quality time with `:t` in ghci would be perfectly adequate. 
C'est un pénis!
Interesting idea. However, I tried the following but didn't get the expected result: &gt; 1 + omega == omega False 
why did you upload it to hackage under the name "clit" if it wasn't ready to be used by anyone? Unfortunately it can't be deleted (but perhaps you can mail the admins)
Yeah, noncommutativity of addition of transfinite ordinals is one of the reason i find ordinal arithmetic fascinating. :-) Thanks for having a go at implementing it! What other mathematical structures are you considering implementing, if any? 
What I mean is, that the type `Z (Z a)` —whether newtyped or not— only gives an approximation of the two-dimensional behavior you want. So, for example, the type doesn't know enough to know that going left then up is the same as going up then left. (Whereas, you can imagine flattening the two layers into a single one and phrasing things such that these sorts of identities fall out for free.) I bring it up just because I recall from when other folks were implementing GOL via comonads, they'd discovered the "not quite 2D" nature of nested zippers was a source of inefficiency for them. I don't remember all the exact details offhand, so their inefficiency may not apply to your version of GOL. Just something to think about.
I'm too lazy to find a link at the moment. But the canonical example is defining the predecessor function for Peano naturals, or the tail function for lists. Using catamorphisms (aka: what Boehm–Berarducci gives you; aka the "iterator" in discussions of Peano arithmetic) these end up taking *O(n)* time, because you have to walk through the whole number/list and reconstruct the whole thing sans one constructor. Whereas if you have *O(1)*-time case analysis, then you can implement these functions in constant time. Catamorphisms only give you *O(n)*-time case analysis. If, however, you were to encode things by their paramorphisms (aka: what the Mogensen–Scott encoding of data types gives you; aka the "recursor" in discussions of Peano arithmetic), then you get *O(1)*-time case analysis as part of the deal. Another canonical example for wanting paramorphisms rather than catamorphisms is the factorial function. Unlike predecessor/tail, this one actually uses the recursion in addition to the case analysis.
Isn't it also quite important for someone to curate a sizable collection of compatible packages? Stackage have done that for Haskell Stack.
This is even possible without stack, using just cabal and [cabal freeze files](https://www.haskell.org/cabal/users-guide/developing-packages.html#freezing-dependency-versions) but it doesn't specify the GHC version.
[removed]
&gt; This "just works" on all platforms and you always get the same thing. Unfortunately, it only works *most* of the time, but not always. There's plenty of ways this assumption can break, especially if you start using more low-level APIs which are not available on all platforms, or even compiler/language features like TemplateHaskell which are not available on all platforms GHC supports. Also, even on the same platform knowing the GHC version doesn't necessarily uniquely identify how your compiler was configured and which OS features were enabled at configure-time. You may also be interested to know that cabal allows for different levels of reproduciblity workflows, since using Stackage resolvers (which are basically huge freeze-files) can often be rather inconvenient to work with. So there's e.g. the new [`--index-state` freezing](http://cabal.readthedocs.io/en/latest/nix-local-build.html?highlight=index-state#cfg-field-index-state) mechanism (which I haven't seen in any other language yet) which provides a neat middle ground between accessing the latest available releases on Hackage while leveraging the cabal solver (which is afaik also quite unparalleled; except for maybe OPAM) to exercise maximal flexibility in composability and between the rigid/inflexible/non-composable resolver freeze file workflows. PS: Btw, you may also want to take a look at [NixOS](http://nixos.org/) which goes beyond the language/operating-system boundary for managing dependencies. 
Not one single language but related and I find it interesting for the lengths they go to, Debian has a [binary reproducibility project](https://wiki.debian.org/ReproducibleBuilds) where the output of builds should be bytewise reproducible.
Stackage resolvers are basically curated freeze files, a feature found in many package managers (e.g. pip, composer, ...), and some other language communities have developed a custom of always specifying exact dependency versions (e.g. clojure, where you can also fix the compiler version). I haven't seen anything like stack's pre-written freeze files though that allows you to switch out the compiler version and associated compatible library versions with one swift command argument flip.
The Javascript world has yarn, which does dependency locking. It's nice that good ideas occasionally trickle down even to those of us forced to use JS. Unfortunately this is Javascript and what you do to actually build and run varies. The most popular build/bundle thing right now is webpack, which works okay and is trendier than gulp/grunt/whatever. 
All of them! If you adapt them to work with [Nix](http://nixos.org) =P Nixpkgs has package sets for Haskell, Node, Python, and many others. [I set up PureScript as well](https://github.com/ElvishJerricco/nix-purescriptPackages), but I haven't been keeping it updated or anything. Point is, Nix lets you setup complex and reproducible build systems for anything you could imagine.
I've been wondering about this too. It seems to me like "reproducible build" has these two different meanings out there. The Debian/Gitian meaning is: You get *exactly* the same binary. The Stack meaning is only: You get a binary that does the same thing. If GHC/Stack could one day achieve the stronger Debian/Gitian "reproducible build", that would be really sweet.
By the way, if you're [this guy](https://www.youtube.com/watch?v=r_Enynu_TV0), thank you. Really helped me grasp parsers a few months ago.
With npm (Node.js' package manager), you can use [`npm shrinkwrap`](https://docs.npmjs.com/cli/shrinkwrap) if you are after absolute reproducibility of a build. However, in practice, what I find beautiful is that you quite rarely need it. Since most of the packages there faithfully use [SemVer](http://semver.org/) (Semantic Versioning) if you depend on a package in the following way: dependencies: {"socket.io": "1.7.*"} it means that you can still get bugfixes and minor improvements and additions to the package in question, but not removals, API-breaking changes, or changes to existing behavior. Edit: It's been some time since I've developped with node, but anything like cabal-hell is unheard of in that ecosystem with about [419k packages published vs. Hackage's 11k as of today](http://imgur.com/a/9XY3O) ([source](http://www.modulecounts.com/)). Which I consider to be quite a feat. Edit 2: Since I've started using `stack` when it was published (used `cabal-install` before) my frustration with Haskell's dependency management has gone down by a very large margin, but according to my memories it's still not quite where npm is in terms of ease and simplicity of use, and "just works"-factor. And I'd love if we could get there with Hackage/cabal-install/stack. Perhaps with Backpack we might? Or `cabal new-build` may still be a good step in that direction -- which I haven't tried yet, but heard some positive things about. Edit 3: I'd also love to hear other people's view on the comparison of the two ecosystems. Does your experience match what I am describing above? Does it differ?
Bundler for Ruby also does dependency locking.
This is not the reality. Int and Word are a fixed sized integer based roughly on native register size with ghc and have also a variable meaning in haskell standard (should be at least 29 bits IIRC)
How would `&lt;*&gt;` fit in the comparison with `fmap` and `extend` ?
Brouwer ordinals are usually represented as: `data Ord = Zero | Succ Ord | Limit (Nat -&gt; Ord)` in type theory. If you are interested in more details, search for "ordinals type theory peter hancock".
I think you might have missed my point. My comment was about how I think that languages *should* work: intuitive and "correct" by default, but with options for fast, less accurate code if necessary. Now of course I mean 'intuitive' and 'correct' from my mathematician's perspective: so arbitrary-precision integers (Integer), ratios of arbitrary-precision integers (Rational) and fast-converging Cauchy sequences of ratios of arbitrary-precision integers (Real). I strongly feel that types with names like `Int` - the obvious, default, most concise, shortest choice - should have simple, intuitive, 'correct' behaviour, while if you want something special (like fixed-width integers or floating point) you should have to write something a little more verbose like `Int32` or `Float64`. And no, I mean `Nat32`. They're called naturals. They're not called 'words'. Words are text. (I'm being facetious, I know why it's called `Word32` in Haskell, but ideally it would be called `Nat32`.) Again, this is idealism, and I totally recognise that it's idealism. EDIT: For example, in my opinion one of the worst parts of Haskell is that the default, easy choice for strings (`String`) is really stupid, slow and semantically wrong. Even really great, experienced, prominent Haskellers default to `String` in their code, in my experience.
Thank you. I needed that. I tend to hold myself to impossibly high standards, and then feel bad when I don't meet them. I try to do things perfectly, and I get close just often enough that it still hurts when I don't make it. But I don't need to bother you with my emotional issues. ...Maybe I should try to be flawed. It would make a nice change of pace. (Also, I think I was being self-deprecating, not negging myself. Negging is a combo insult/compliment thing, whereas I was just talking badly about myself. See https://xkcd.com/1027/.)
Together with rvm (via .ruby-version) for ruby version locking
In java you have nebula dependency lock, which does this.
Let's name each function by what it accepts and returns, e.g. `abc` accepts `a` and `b` and returns `c`. (So calling `abc a b` will give you `c`.) Also let's say `i` is `aa` and use an identity function for it. The code almost writes itself: act :: (a -&gt; d -&gt; c) -&gt; (b -&gt; a) -&gt; ((a -&gt; a) -&gt; b -&gt; d) -&gt; b -&gt; c act adc ba ibd b = c where i = Prelude.id a = ba b d = ibd i b c = adc a d Then you can smush everything into one line if you want: act adc ba ibd b = adc (ba b) (ibd Prelude.id b) And rename to make it look easy: act a b c d = a (b d) (c id d)
From the paper: &gt; As befits its roots in tactic-based interactive proof assistants, Idris’s reflected elaborator can be used as a tactic language for proof automation. Here, we present a simple yet powerful proof automation procedure, named mush as an homage to the much more capable tactic crush in Chlipala (2011).
&gt; affect the node module at runtime I'm not convinced of this. I don't remember running into runtime errors due to package upgrades (minor version number). Or even if I have, they were so easy to find/debug and apparent that it wasn't a cause for a headache. E.g. an exception at startup instead of something happening later. Have you developed with node? And have you run into any issues due to a minor version bump? I'd love to hear a concrete example.
Great work! You should be proud of yourself for getting this done, and consider putting it on Hackage. One note I'll make is that you're allowing more than just the ordinals, you also allow negative numbers and therefor negative powers of ω and algebraic combinations thereof. You could use smart constructors to fix this, but an alternate solution would be to change the definition of `InfOrdinal` to make illegal states impossible - encode the coefficients as naturals, not integers. Your definition can already encode `epsilonNaught` though: epsilonNaught = foldr1 (\_omega exp -&gt; InfOrd [(exp,1)]) (repeat omega) Feel free to argue about whether this is equivalent to epsilonNaught' = InfOrd [(epsilonNaught',1)] :)
+2
ooh even Simon Peyton Jones asks a question: &gt; Your elaboration language has a backtracking search aspect to it. That contrasts with the French school of elaboration, which is: to walk up your program syntax tree and generate constraints and then solve the constraints separately, which might involve lots of going to and fro and unifying here and knowing a bit there, it is very non-directional. &gt; But that somehow seems to be incompatible with backtracking. It is as if you have got a particular solve order. Do you think the two are compatible, could you take the French approach to elaboration and combine it with what you have shown?
you can fill an issue here if you have a proper way to reproduce: https://gitlab.com/vannnns/haskero/issues
Minor version upgrades by definition are supposed to not break things, so that's not really the subject of this conversation. Minor upgrades don't cause so-called "cabal hell" either. The most prominent example that stands out was upgrading to Angular 1.3 and then again to 1.4. Bugs in production both times. This is on top of the huge difficulty of doing the upgrade in the first place because of the lack of compiler type checking.
&gt; Minor version upgrades [are] not really the subject of this conversation. Are they not? I'd wager they are, in part. Since the original poster, /u/haskellgr8 mentioned their fondness of stackage's lts snapshots as a means for reproducible builds and asked us if we know if/how other ecosystems deal with this question, I thought to mention NPM. One can still use `npm shrinkwrap` if one prefers, but, in my experience just pinning down major/minor version and letting the patch number be `*`, one can already achieve reproducible builds in terms of API/behavior of a program. And what is less related to OP's question, but I thought to still mention in my original comment because it is very relevant to my encounters of using stack/cabal-install/npm: My experience so far has been more pleasant with node than with cabal-install, or even stack. I think the main reason often is that I want to use a dependency, and cabal-install/stack gives up and says, "sorry, no can do". NPM rarely if ever said that to me. So I hope that one day we can get to a place where using stack/cabal-install/backpack will be just as pleasant to use as npm, and perhaps even more so.
When you say "tooling", it's unclear what you mean. It sounds like you're talking about problems with package managers. If that's the case, I would encourage you to look around. Ruby, go, java, python, node, any language people are using to build web applications. Everyone has issues with their package managers, especially concerning reproducibility of builds. The untyped world bears the burden of finding out at runtime. Here, we find out more promptly if an incompatible dependency was used. Concerning stack and cabal-install, both of these are tools that will do what you tell them to. That being said, it's hard to figure out how to instruct them. This is the same experience I've had with package managers in any other language. You just have to keep picking up random bits of information from the Internet until your intuition for the tool gets good enough. The defaults used by stack are better for most application developers. Also, it's not really stack that gives us compatible dependencies, it's stackage. Stackage works with cabal-install as well (although stack integrates with it more cleanly). You can pin down most of your deps with a [cabal.config](https://www.stackage.org/lts-8.6/cabal.config) file and then use `cabal freeze` to lock down your deps that aren't on stackage. 
&gt; Are they not? I'd wager they are, in part. Nope, they're not. The vast majority of version bounds in hackage are of the form `&lt; a.b`. This means that minor bumps aren't going to cause cabal to say "sorry, no can do". I wasn't really addressing the OP's question. I was responding to your "I'm not convinced of this.", which was in response to "affect the node module at runtime". JavaScript most definitely doesn't alert you to type errors until runtime because it is dynamic and untyped. And I have personally been bitten by that problem. If you haven't experienced this, I suspect it's probably because you haven't done many major version bumps.
This is generally true of dynamically typed languages. Since there is no compile-time verification that your types will still line up after the upgrade, you basically have to upgrade and then start fixing bugs that occur in runtime. I've seen this happen lots of times in JavaScript, PHP, and Python. If you have a good test suite, then that essentially becomes your "ad-hoc type-checker" and it's certainly valuable. But do remember that a test suite is finding bugs that occur at runtime!
That's interesting! Can you elaborate? For the latter defn., what are the differences? 
Nice walkthrough - thanks!
Late to the thread but... If you want to incorporate "real world DSLs" in your code, I'd first consider whether you want a deep or shallow embedding. Deep - generating code for some foreign language (e.g. HTML, C, MATLAB...) Shallow - just a Haskell library but with a domain specific set of combinators (e.g. quickcheck, Parsec) Choosing Free, Freer, Operational... or none of them is a step to do later. At the initial stage there's no determination that your DSL even has to be monadic.
That's true. Much, much better. But the question was how to understand the given answer, not how to create a better one.
Sometimes when I need pen + scanner I use [diagrams](http://hackage.haskell.org/package/diagrams).
To answer 1), in the PureScript compiler, the function type constructor is just another type constructor, and there is a special case for it in the subsumption rule. Type application in general doesn't have a subsumption rule, and we fall back on unification, since we don't know the variance on the type on the left of the application in general.
The purpose of reproducible builds is often to produce byte-for-byte identical outputs from the build process, for example so that you can cryptographically (e.g. with a strong hash) pin the expected output. A binary may for example have build timestamps embedded into it by the compiler, which would lead to the relevant portion of the file being identical (i.e. the actual code being identical) but the file itself being different.
Definitely using Stackage LTS (stackage.org/lts) either via stack or cabal provides for a very stable experience. Upgrading LTS versions has always worked perfectly for me and often binaries of packages you have for the previous snapshot are directly reused if the version hasn't changed. What that means is if you have many projects on LTS-8.5, for example, stack maintains a binary cache of all packages all your projects use in its binary cache. So, you only build these dependencies for one project once and all the other projects just reuse them from the binary cache. Now, when you switch to LTS-8.6, only the packages that have been updated will be rebuilt and only once. E.g., see the difference between LTS-8.6 and 8.5: https://www.stackage.org/diff/lts-8.5/lts-8.6 This is absolutely brilliant: no more rebuilding each dependency for each project many times in a sandbox.
Personally, I don't think it offers much more perspective. I think it is worth looking at how `bind` and `extend` each differs from `fmap` because for both of them, the function argument `f` is necessarily *aware* of the structure on which it is performing, as opposed to the function argument to `fmap`, which by law *cannot be aware* of the outer functor structure. But with `&lt;*&gt;`, while the function argument `f :: w (a -&gt; b)` itself is within the structure `w`, any implementation of the inner function of type `a -&gt; b` is *not aware* of the structure. Does that make sense? I'm still pretty bad at translating Haskell &lt;-&gt; English :)
Based on the context ("rest services") I expect the parent is worried about reliability of the build process and not determinism of the output.
RemindMe! 18 days "Watch this Idris video"
This was pretty much what we spent the time talking about (the differences). It just wasn't work that was a high priority at the time.
To a certain degree Nix actually does let you deploy your entire dependency tree to a remote server that is not running with Nix or NixOS. On Linux or macOS, the Nix package manager can run side-by-side with the default one without ever harming anything. If you deploy, say, a webserver via Nix, you can use Nix to copy the *entire* dependency tree to the server and it will run entirely on its own tree (not using anything provided by the server or its package manager). In fact, this is pretty common.
People in the know are probably tired of hearing this, but just for people that don't know: Nix is *especially* good when your Haskell packages have external dependencies (e.g. they rely on C libs). Cabal and Stack do not handle this. (Actually, [Stack does handle a few build tools automatically](https://docs.haskellstack.org/en/stable/faq/?highlight=alex#how-do-i-get-extra-build-tools), but not C libs in general.)
Sure, but then is that timestamp field zeroed out, or given in a flag, or what? Where can I get the details on how stack or nix achieves this?
I was just dealing with this at work. Trying to run our legacy Ruby app on my local machine. I had to hunt down external deps and google solutions for this gem and that. Very tedious. But at least Ruby encourages programming with a "lockfile" (same idea as cabal.freeze or stack.yaml) which locks every dependency to a specific version, so it wasn't _too_ bad in the end. I definitely recommend using cabal.freeze, stack.yaml, or equivalent to lock dependencies when working on application code. Otherwise the chaos of version mix-n-match makes it very hard to reproduce issues across machines.
&gt; Stack on top of Nix Interesting. I would think that Nix solves most of the same problems solved by Stack. Out of curiosity: why Nix+Stack instead of Nix+Cabal?
++
From the linked docs: &gt; in order to also freeze the state the package index was in at the time the install-plan was frozen. I don't quite get it. What's the point of freezing the state of the package index at the point in time that you froze the install-plan? Freezing the install plan makes sense, because you're freezing the stuff you are actually using. But freezing everything else to that point in time... I don't see why.
&gt; Cabal and Stack do not handle this. (Actually, Stack does handle a few build tools automatically, but not C libs in general.) And so does `cabal new-build`... it can even handle different versions of `alex`/`happy` within the same install-plan if needed (since they're so-called [**qualified goals**](http://www.well-typed.com/blog/2015/03/qualified-goals/)). ...and starting with cabal 2 we can actually handle more than the small `build-tools` set that Stack currently supports, see also http://cabal.readthedocs.io/en/latest/developing-packages.html#pkg-field-build-tool-depends But handling non-Haskell dependencies will still not be possible. For that we need to interact with the system package management (Nix, deb, rpm, ...) EDIT: added link to qualified goals blogpost
Ahhh, didn't think of revisions. Makes sense.
&gt; Function composition is sort of like a railroad track with puzzle piece shapes at each end And monads are like burritos...
I'll throw into the mix since a lot of your answers have been lower level details. Haskell is great, and has a learning curve. Like, comparable learning curve than going from 0-&gt;programmer. But the tooling is great. I barely know what cabal and nix are, and just use stack. I recommend that too. Emacs (spacemacs if you're new) is the editor you will carry to your grave if you can devote a weekend to learning the key bindings. This is a major piece of your tooling, and, I can't laud it's name enough. Emacs! Intero is a great piece of the tooling puzzle. Since it sounds like you're new to haskell, I'd start with something like Learn You a Haskell, and then *don't worry about learning more haskell* (for now). Trust me, I got super distracted with profunctors and comonads etc. too early on. After you're done with LYaH, start building your REST service, and learn by necessity. Use Servant for building it. It is the most haskell, and least magic of the libraries (IMO), and feels like a joy to work with. There's a journey to learning haskell, and if you're buckled up and ready to enjoy the _journey_ (not just the your rest service), haskell will serve you well. (disclaimer: I basically just described _my_ journey, but I think it was a good one.)
oshit I just learned that this exists, thanks
Wow, this is impressive! Thanks for sharing :)
The same in spirit but technically different. 
Just an important caveat: Nix does not yet work on Windows. If you're like me, you solve that problem by...running a Linux VM in Windows. :P
 I just really enjoy the facts that (i) JS works everywhere so easily, (ii) JS's ecosystem is so much more enjoyable than Haskell, (iii) I don't need to wait for compile times, which I really hate. By writing that in JS, for example, now I have a **2.3kb** file that I can use to get this code to run on any browser, any desktop, on Android, on iOS, anywhere! In a sec I could put a live demo in a web app and show any friend. It gives me freedom. If I had written this in Haskell, my 1-2 hours coding it would be more enjoyable (because Haskell is so ridiculously better than JavaScript as a language in every imaginable sense), but then what? I'd be left with a .hs source which works on my own computer and that is it. Programming is not that hard, so I prefer to use a crappy language now, to have the freedom of running the stuff I build anywhere then.
There's also Haste, which should be plenty for the job, and makes smaller JS than GHCJS.
Do recall he hates waiting for compile time... ;)
I'd like to learn how to do that. Where do I start?
https://reproducible-builds.org/ describes the effort Debian has taken, and recommendations for developers. It's all fairly straightforward after a little bit of description I think. Nix does not have binary determinism. Neither does GHC (and, by association, neither does Stack or Cabal). Thanks to Debian, a lot of more 'ordinary' Unix/Linux software is reproducible, though. There are lots of sources of indeterminism. It's hard to separate sources of indeterminism from the design of an application itself, in some ways, but the basic ideas apply. GHC 8.0.x+ actually does have determinism *for an interface file*. That means that, should you compile a bunch of modules on two different NixOS/Ubuntu/whatever machines, barring *being haunted by ghosts*, the interface files should match and be reproducible on either. If you know what you're doing this can be leveraged for things like accelerating build times at large scale (e.g. when a CI system builds something successfully, it takes all the interfaces and objects and can put them in something like a temporary cache service. Developers then pull changes and run builds, but the build system can pull pre-built object files from the cache server. This is how systems like Google's Blaze &amp; some others work). However, this is not true of object files: the .o files produced by GHC (along with the interfaces) may vary between compiles/machines. So the work isn't done. It's also hard and tedious (interface file determinism was actually a crazy huge effort, almost multi-year long journey, by a single lone hero), but it can be useful.
btw, this has nothing to do with function composition, which is the dot operator in Haskell (an ascii period).
Are you saying that folks are over-engineering stuff in Haskell right now? Please say more about that.
Or PureScript, and then you (get?) to leverage the node ecosystem much as you can in the original JavaScript version.
Please tell me those are valid Haskell operators. I want to use them. Now.
those were my (polarized) Haskell sunglasses, through which all things in this world become either FullHaskell or invisible. 
Stack definitely solves many build problems by giving you repeatable build environments, but Stackage is also a semi-curated package list, as in they try to keep packages that have build issues out. As far as I'm aware, most of your core REST stack (things like Servant, Warp, ect.) are extremely stable and you would have to misuse the package or make an active effort to break the package. 
For next time if you can't be bothered using JavaScript, PureScript is a nice drop-in for it. With my [purify](https://github.com/chrisdone/purify) tool you basically just need `stack` installed and `stack build &amp;&amp; stack exec purify` builds your PS: https://github.com/chrisdone/purify-template 
The [new website](http://www.acceleratehs.org/index.html) is very nice, too.
monads are like branched railroad pieces https://fsharpforfunandprofit.com/rop/
Thanks! (:
Ain't nothing wrong with`&lt;|`, especially when you have the other direction defined as`|&gt;` (currently `&amp;`).
Thank you. I'd like to learn to use nix. This is a good, motivating exercise.
What does ANN mean in this context? Is Accelerate used for some neural net library?
Luke describes how to setup a hlint validation check on travis here: https://lwm.github.io/haskell-static/
Any chance you could show the code for river spirits?
well, then you can render it yourself and get the wav... I can show you the main drone wow cps = fmap (sum . zipWith3 (\f amp w -&gt; sig amp * w * osc' amp (f * cps)) [1,2,3,4,5,6,7,8] [1, 0.1, 0.6, 0.26, 0.5, 0.15, 0.2, 0.12, 0.1, 0.12]) $ sequence $ replicate 8 q q =urspline 0.15 0.37 drone = toSam $ mul 0.45 $ at (filt 2 mlp (1500 + 2000 * uosc 0.1) (0.15 + 0.25 * uosc' 0.25 0.15)) $ mul (fadeIn 2.5) $ (liftA2 (,) (wow 53.5) (wow 56.5)) + (fmap fromMono $ mul 0.25 (wow (110 * 3/2))) + (liftA2 (,) (wow 110.5) (wow 110.5)) and then dac $ cave 0.25 $ drone ~~~
I think Accelerate would be fairly easy to use for neural networks. If i recall correctly, they are often expressed as basic matrix operations. It's possible Accelerate would not have any advantage over hand-tuned matrix primitives (like CUBLAS), of course.
That's kind of hard to answer, and I don't think it's the right question to ask. But I can say that we did not have not compromise our hiring standards to get that many people.
&gt; ELI5: Function composition Instead of going from your house to Timmy's, and then the mall, just go straight to the mall! If Timmy needs to come with you, that's monadic composition! ;)
Yeah, but actually computing with this representation is hard, and equivalence is undecidable. Now we don't actually need the entire space of Nat-&gt;Ord , which inspires the binary tree representation. Ord = Z | Chi Ord Ord. Where Z = 0, Chi 0 0 = 1, Chi 0 = Succ, Chi 1 0 = omega. Ect.
It's because of the way they are parsed: (1 + 2 + 3 +) 4 becomes: ((1 + 2 + 3) +) 4 (((1 + 2) + 3) +) 4 and: (: 8 : []) becomes: (: (8 : []))
Thank you, very clear and well-written. To test your conjecture we could check how package managers for languages with a [structural type system](https://en.wikipedia.org/wiki/Structural_type_system) deal with the problem.
There is pre-stack Haskell and post-stack Haskell. The experience of working with language is significantly better since stack came out. Some minor issues remain (although some of the quirks of the first releases have been ironed out), but Haskell used to be much less comfortable to work with than wPython (where you have virtualenv, pip, conda, ...) and now it's even better than Python. (I bring up Python as Python &amp; Haskell are the languages I use the most on a day-to-day).
&gt; I want to write haskell in the browser AND mobile. u/mightybyte has hinted at something of that kind [here](https://www.reddit.com/r/haskell/comments/61y1ry/bob_2017_andres_l%C3%B6h_write_one_program_get_two_or/dfi9nk2/). Maybe he can tell you more…
More than once I have wished for foldl1 (++ " " ++) ["hello","world"] but that is a bit more problematic...
Ask around on IRC #reflex-frp. There might be some work being done for mobile support.
Isn't there an OpenCL backend? I mean how do I offload to non-nVidia GPUs?
They could have been designed that way, but the original intent as I understand it was to be able to think about them like working with `(():k)` or `[]`. Swapping the order of those two constructors has no impact. They are a stepping stone to thinking about (co)induction over more interesting data structures that students swiftly outgrow. Overall, they were scrapped because they were a confusing mess. Randomly changing their semantics and breaking code that previously worked would just make them more confusing.
&gt; Purescript, for all intents and purposes, is Haskell, but more DOM-centric. Strict vs lazy isn't exactly a trivial difference.
I kinda wish that the operator being sliced on was always given the correct precedence to just work. As I often find myself in situations where I want to do such a thing, and it shouldn't be too surprising since that is the only valid way to parse such a thing. EDIT: Hmm, actually perhaps it is not so unambiguous: (+ 2 + 3 + 4) Seems to have 2 different natural ways to be parsed: (\x -&gt; x + 2 + 3 + 4) (\x -&gt; ((x + 2) + 3) + 4) and: (+ ((2 + 3) + 4)) The former obeys the fixity of `+` properly, but it also isn't a particularly natural way for an operator section to work.
Al last a tutorial of Reflex... The composability is improved compared with the crappy MVC and react frameworks. I may hear cool kids crying now. The key element of this enhanced composability is the Monad, that many people hide due to atavistic pure-mongering prejudices (more cool kids crying) But there is a massive amount of ad hoc primitives due to lack of additional composability. I'm reading at the beginning. Primitives like `leftMost` is clearly an alternative. It can be solved with a chain of alternatives. In the other side, mergeWith is an applicative. Why this is not done right, using the proper operators &lt;|&gt; and &lt;*&gt; ? I don´t know. Recursive-do there is like needing a butler and resucitating Frankenstein for it Using records to configure fields and using a class `hasValue` to extract values instead of using a more articulated functional DSL is not good. Look verbose and rigid. Why don't use HTML attributes to configure the widgets? Again, having `el` and `elAttr` instead of a primitive `attr` and `el` , like in the case of blaze-html, to separate elements and attributes is more than rigid; it is transilvanian. (I will say nothing against the duality event-behaviours, since I don't want to have reactive cool-kid-zombies trying to drown me buried under a ton of papers, but that mumbo-jumbo look redundant to me) I will continue tomorrow with the tutorial.
Happy to see someone got some use out of my little `unique` package. =)
We hired a bunch of developers. I can't really make a meaningful generalization about the experience level of these developers beyond "we hired a range of experiences".
Read this guy's post in the voice of Comic Book Guy and it elevates it to entertaining.
This is great, awesome job! I wish I had had this half a year ago!
Maybe we can have a place somewhere on the reflex main Readme? I've just created an issue about that here: https://github.com/reflex-frp/reflex-dom/issues/143 Or is reflexfrp.org supposed to become that?
But even in that case my union package stuff seems fine to solve that. As far as I can tell there is really no reason at all for orphans that isn't covered by such a system.
&gt; I'm reading at the beginning. Primitives like leftMost is clearly an alternative. It can be solved with a chain of alternatives. While there may not be an `Alternative` instance (because there is no `Applicative` instance, which I clarify below), there is a `Monoid` instance. But `leftmost` exists because that monoid instance needs the event values to have a `Semigroup` instance in order to handle simultaneously firing events being `mappend`ed. `leftmost xs` is basically `fmap getFirst $ mconcat (fmap First xs)`. It's just a convenience. &gt; In the other side, mergeWith is an applicative. Why this is not done right, using the proper operators &lt;|&gt; and &lt;*&gt; ? I don´t know. This is a fundamental misunderstanding of `mergeWith`. It's not composing two events in Applicative style. It's just listening for all occurrences of either event, and allowing you to handle the case where both events are firing at the same time. Tying this back to my previous paragraph: `mappend = mergeWith (&lt;&gt;)` &gt; Recursive-do there is like needing a butler and resucitating Frankenstein for it GUIs are frequently self referencing. Writing FRP applications without it would be a real nightmare. &gt; (I will say nothing against the duality event-behaviours, since I don't want to have reactive cool-kid-zombies trying to drown me buried under a ton of papers, but that mumbo-jumbo look redundant to me) Excuse me? I can't imagine adequately representing reactive code without some notion of behaviors. It's not "cool-kid-zombies." It's just god damn useful.
If a package has to bless its own orphans in another union package, then there's no way every package I use will bless all the typeclasses I need. Someone will miss something, because the number of possible instances is enormous. And it would break the coherence of "blessing" if I could add my own blessed union independently.
is the LaTex tutorial up yet, or is it still a work in progress?
Simon Marlow's [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929) has a chapter on Accelerate. The API has changed a little since then, but it is still a good introduction. I agree there aren't many tutorials out there. I think part of that was that previously you needed a NVIDIA GPU to use it, so the barrier to entry is higher. This makes it more difficult to play around with it as a weekend project, make something cool, write a blog about it, etc., which can help people get started. As for who uses it, I honestly have almost no idea. People rarely contact me telling me what they use it for, what's missing, what works... (though I wish they would).
Yeah! Give it a go and let me know how it went (: You can use the [issue tracker](https://github.com/AccelerateHS/accelerate/issues) to ask questions too (as well as report bugs).
I've long intended to write a proper FFI layer to [cu]BLAS, but have yet to find the time. As far as I know CUBLAS uses inline SASS (or otherwise), and is not possible to beat with the publicly available APIs. On the other hand, I sometimes feel like this is another instance of express-every-problem-as-a-matrix-problem, because a fast matrix library is almost guaranteed to be available, but writing fast code yourself is hard.
It really doesn't seem that bad to me. And in cases where it is absolutely impossible to get both package maintainers to respond within a certain deadline then we could have something like `{-# ORPHAN -#}` that tells the compiler to allow it, but we just have to understand that in these cases you might causes problems with overlapping instances, and try and remove them ASAP. Just like `{-# INCOHERENT #-}`.
You seem quite eager to classify this as a bug rather than a result of misusing `StdGen`. It's also quite hard to tell what's wrong with no code to see. Consider using the MonadRandom library to not have to mess with passing around the generator and updating it.
&gt;the classic "Haskell is impractical" complaint particularly holds true when trying to use it outside of a server context. Unfortunately, in Haskell, if a suitable library doesn't exist, it's often best to just write it yourself. Although the minor upshot is that I've never been at a loss for projects. &gt;except maybe for extraordinary circumstances that required extremely high memory performance. Haskell ended up being slower than Rust for me on many occasions. It's definitely my favorite language that doesn't require manual memory management but it's also definitely slower (not just more memory intensive). &gt;there seems to be a massive gap when it comes to the mobile story. Yes indeed! [This](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md#mobile-apps) has details on some prominent libraries in all areas of haskell, which may be of interest.
Do you want unpredictability (cryptographic sense) or uniqueness ? If you want a random value that depends on the "document" then it might make sense to use something like a cryptographic hash of the document.
Because she wanted to? 
You can get extended debug information by running `cabal update --verbose` It should say something like Downloaded to /Users/yourname/.cabal/packages/hackage.haskell.org/00-index.tar.gz Updating index cache file /Users/yourname/.cabal/packages/hackage.haskell.org/00-index.cache Of course it won't if it thinks the files match though. So one thing you can try is to find those files and delete them to force them to redownload, or inspect them to make sure they have actual contents. You may also have a network connectivity issue, in which case using alternate choices of http transport may help. For example `cabal --http-transport=plain-http update` or `cabal --http-transport=wget update` You can also examine `~/.cabal/config` and see what it specifies as remote-repo. Mine, for example, has the line: `remote-repo: hackage.haskell.org:http://hackage.haskell.org/packages/archive` You may also want to run `curl` directly from your commandline to make sure it works (or more generally to make sure that the command-line tools installed with the apple developer toolkit are installed and work). 
Thank you very much, I deleted the file and ran 'cabal update' and everything works again! Not sure what was happening but that solved it. Thanks again for such a detailed answer :).
Thanks for your suggestion buddy :)
Might be worth reporting it as a bug: https://github.com/haskell/cabal/issues
That was valid (and funny) 6-8 years ago, but i haven't seen Dons do any public Haskell work since he joined Standard Charter.
Will do, thanks 😊
`mergeWith` just doesn't do what Applicatives do. To have an Applicative, the result event would need to fire depending on values from both of the input events. But `mergeWith` fires when *either* of the inputs fire, so it can't event have different types in the input events (which would be necessary for `Applicative`). The only way to come close to making an `Applicative` instance for `Event` is to have the output of `(&lt;*&gt;)` fire only when the inputs fire simultaneously, which would totally useless and isn't anything like `mergeWith`. Plus, there's no valid definition of `pure`. And of course this all means it can't have an `Alternative` instance, since `Applicative` is a superclass of that. And anyway, `(&lt;|&gt;)` is by no means more direct than `mappend`. In fact, if anything, I'd say the opposite.
I now see my parent comment sounds a little negative. I really didn't meant it to be that way. I wanted to say that accelerate is a great library and I believe it's adoption should increase. Apart from starting from Simon Marlow's book's chapter, I also found your Ph.D. thesis and your workshop slides to be really good for learning. Thanks for the great work @tmcdonell
Thanks for the detailed explanation! &gt;"In Haskell, we also get to write our final solution in a dialect which is close to our problem domain: a custom combinator library, such as a custom monad, applicative, or category." Would you be able to share something that I could read on how to get better at this? Have a few tasks that could benefit from this approach but having difficulty starting. 
Hmm, I'm sure there are better resources somewhere, but I'm afraid I don't remember where I learned how to do that. For now, [here](https://www.youtube.com/watch?v=85NwzB156Rg) is a presentation I gave about combinator libraries. A custom monad, applicative or category is simply a combinator library whose most important combinator is `(&lt;*&gt;)`, `(&gt;&gt;=)`, or `(.)`.
Then the thing you're talking about is completely different from `mergeWith`, and is a different model of FRP than Reflex.
This doesn't look like an April Fool's joke.
Stack itself doesn't recommend using it as a package manager heavily. I would recommend stack if the user wants to start developing, but cabal-install if they just want a few exes installed. They are installing hlint, so stack might be a good fit.
For anyone wondering what Idris is, it's pretty much Haskell with dependent types. That is, it's easy to encode thing like "vector of length n" in the type system, so you can go from concat :: Vec -&gt; Vec -&gt; Vec to concat : Vec n -&gt; Vec m -&gt; Vec (n+m).
why don't you do that?
Questionable timing IMHO.
Do not hesitate to give me feedback even from the auto-translated version. Again, the goal is to make people able to write LaTeX documents, I don't need them to understand what's under the hood. If you thinks it's a good read, I can think of a real translation, if not, I need to know what's wrong with it.
Alas, I have. Since I've not looked for any Bitcoin libraries I've listed all the ones I know. I'm sure the community is dying to serve the answer to you on a silver platter. /u/tekmo don't you have like a 7 page response all drafted up for OP.
That's not the best example of what Idris brings over Haskell as we have had length indexed vectors in Haskell for a while and there are now libraries that make using them really nice. My favourite is the vector-sized package, which is a rather thin wrapper around ordinary vectors but with static guarantees, so it compiles to very efficient code too. EDIT: [vector-sized](http://hackage.haskell.org/package/vector-sized) link, because I was on mobile earlier.
I think it is perfect timing to release a '1.0' version of software mostly due to the preconceived connotations associated with '1.0'. Oh and to be clear *this* release is not an April Fool's joke, or is it...
I like to think that Idris is not: &gt; pretty much Haskell with dependent types. I like to think of it as: If we had to design a language from scratch that supported dependent types: how would we do it; what would it look like; and what cool things can we do using such a language. The result is Idris. Of course other design decision's were made, for example strictness &amp; re-target-able code generators &amp; interfaces, that weren't necessarily related to dependent types. 
&gt; We can represent this in Haskell, since it’s a lazy language. You're going to have to do a lot of work to convince me that "since it's a lazy language" is the reason that we can do *x* in Haskell, for pretty much any value of *x*.
any real-world experience with reflex? How polished is the framework?
Ok, if you want to be pedantic, of course you can do it in any language. But what you *can't* do in other languages, is use the same representation. It's far more convenient and practical with laziness.
Yeah, you're right. I don't think I actually get what's going on here. Maybe you could say "We can represent this in Haskell, since it’s a lazy language which implies that a function can take its result as its own argument" or something.
Yea I'm sure the article could explain it better; I only spent like 20 minutes writing it =P
How is this parsed/evaluated? (+ (+ 1) 2) 3 Are partial functions the result of evaluating each '( ... )' ? How does GHC know that it is the second parameter that is being applied? Edit: I think I found the answers to my question: https://www.haskell.org/onlinereport/exps.html#sections
Edwin released Whitespace as an April Fool's joke, so this looked very suspicious at first.
It's cortex-a53 on the RPi 3, i believe. Cortex-a7 is the RPi 2
Highly recommend the Type-Driven Development with Idris book. Well written with good and practical examples. Syntax is basically Haskell, so it's not far to claim that Idris let's you write Haskell with dependent types e.g. you can use a regular function in the type signature!
It certainly would be if it worked in this case. 
Strict vs. non-strict (de facto lazy because of GHC) is a massive difference. SPJ in face gave a [talk](https://www.youtube.com/watch?v=06x8Wf2r2Mc) on how laziness informed other design decisions. 
| it is transilvanian. This actually sounds pretty cool.
I did update it. There is a comment saying that right now we still don't have stack 1.4.0. But we are getting there. Did I miss something? 
I don't think this is true. I just tried updating a project with 80+ dependencies from lts-8.5 to lts-8.6, and Stack had to rebuild all of them. I'm pretty sure binaries are cached within an LTS between projects, but each LTS gets its own distinct cache. Of course, I'm basing this on one example, so it's entirely possible I'm wrong.
Does Idris support top level type inference? 
Why does [the documentation](http://www.acceleratehs.org/get-started.html) use cabal rather than stack?
So close to the void? 
It wasn't there before. We could try a new spot. Let me build a smaller template.
I've had good luck with Atom and ghc-mod
Man we need dependent types.
Whenever something `fix`y is involved, I mostly try to understand it from the perspective when the fixed point is reached. If you look at `final` in the third (main) code block, it is exactly such a fixed-point (where the domain corresponds to a product of flat domains). What's the value of `final ^. b`? Looking at `overrideB`, it's probably `3`, since it's the last 'visible' write, not even involving recursion. `final ^. a` is more tricky: It isn't written by `overrideB` at all, so we look into `overrideA`. Looking at that definition, it mostly overwrites the value of `_a` for some incoming config `super`, with the value of `self ^. b`. `self` is exactly our fixed-point later bound to `final`, so we can again trace this back to `overrideB` to know `_a` will be bound to 3. Neat, but confusing.
Thx for the link, sounds interesting
(\ response -&gt; putStrLn (reverse response)) "!!waheeY"
[As Seen On Haskell Cafe!](https://mail.haskell.org/pipermail/haskell-cafe/2017-April/126656.html)
That would be: (+ (+ 1) 2) 3 (+ ((+ 1) 2)) 3
Idris has interfaces rather than type classes. They're superficially similar, but what Idris has is closer to ML modules in practice. One reason I changed the syntax was an attempt to make this more explicit. My feeling is that type class coherence is important in Haskell, but less valuable when you have full dependent types to express properties. Haskell folk like to look for similarities and differences, which is natural and a good way to understand what's going on at first, but it's more interesting to think about what fun new things you can do with first class types. Just as we tell people not to write Java (or whichever imperative language you want to pick) in Haskell, don't write Haskell in Idris... 
ha and SK seem to be on a different hight
It's just a constant battle with entropy. If you check the template we just need some repairs there
How would the semigroup class and instance look like for a future version of GHC which has full blown dependent types?
Is there a script?
I'll just leave this here, emphasis mine &gt; The short answer: GHC 8.4 (**2018**) **at the very earliest**. More likely 8.6 or 8.8 (**2019**-**20**). &gt; &gt; — Richard Eisenberg (author of `-XDependentTypes`) [Dependent types in Haskell: Progress Report](https://typesandkinds.wordpress.com/2016/07/24/dependent-types-in-haskell-progress-report/) ([reddit](https://www.reddit.com/r/haskell/comments/4uf2fv/dependent_types_in_haskell_progress_report/) discussion) as response to [this](https://www.reddit.com/r/haskell/comments/4uamzt/countdown_to_fully_dependent_types/) thread
Can you explain why the type of `z` can't be inferred here and why this is desirable?
Why does everyone use this example? I don't find it illustrative; perhaps because I don't know Haskell well enough to know why this would be useful. ELI5?
&gt; I hate April fools Me too. I upvoted this because of Haskell awesome.
1. Open `/r/place` 2. Copy this script into developer console https://grind086.github.io/PlaceBot/dist/placebot.min.js 3. Insert this config into developer console http://lpaste.net/raw/8073720046037237760 The config was made from the image source using [JuicyPixels](https://hackage.haskell.org/package/JuicyPixels)! EDIT: a less intrusive version of the config that will not overwrite the art of others with unnecessary white pixels: http://lpaste.net/raw/4408905038831812608 EDIT: new font + colors: http://lpaste.net/raw/2715401684581875712 (as discussed in other comment threads).
How much effort would it be to actually prove associativity and such of our own custom types that we make semigroups. It seems like if our type isn't too trivial then such a proof would be pretty hard. Like how would you prove that unioning red black trees is associative?
Hard, I would say extremely hard, but I am not the most knowledgeable resource. There is a reason why idris separated Monad and VerifiedMonad proving things in general is just hard. If you have a paper with a proof spelled out of red black tree unions I think it would be easier. When your data type is a composition of the standard function data types (functions, lists, maybes) it is quite easy to prove things because you can just stitch the proofs together.
I think dependent types are pretty cool, but honestly the sheer difficulty jump from quickcheck tests to actual proofs to me seems a bit too massive to justify having to deal with them much. I think if we just had testable functions (basically just predicates that should always return true) alongside classes that could be quickchecked against. And actually took it very seriously when an instance isn't lawful (basically just disallow it almost always), then I think we would be in a pretty darn good spot.
How well does specifying exact dependency versions work in practice? That sounds pretty neat for reproducible builds but it also seems like compatibility hell.
I mean the limitation is pretty reasonable considering how much of a pain typing lots of unicode stuff is. If symbols like `∀∃≥` and so on were easy to type then IMO a combination of ASCII and a restricted subset (due to the amount of identical looking letters and the difficulty of correctly implementing all of unicode) of unicode would be best.
I don't have enough experience, beyond listening to talks, with either topic enough yet to attempt it. Still trying to use type level DSL at a simple level in some toy project, when the opportunity arises. Haven't had an excuse to use reflex-dom yet.
Right, but it's a worthwhile comparison , since there's a bunch of practical Haskell features, like type classes, which are present in Idris but not in Agda or Coq.
To me, the draw to dependent types isn't that I'll be able to "prove all the things." It's just another tool that I'll have in my belt for the times that it's convenient. For example, I've been thinking about a type level DSL for describe SQL schemas, with function-like things on these types sort of representing DB migrations. This would be *much* easier to model with dependent types, despite that it's possible in GHC right now without them.
That seems like more of a problem with the syntax of type level programming in Haskell and the lack of unification between type and term level constructs, rather than a need for genuine dependent types. In that DSL at what point would you actually want to have a type that depends on a value?
&gt; lack of unification between type and term level construct That unification is called dependent types =P I wouldn't have types that depend on values. But I would want to write term level functions that can operate on these types, which needs dependent types
&gt; But I would want to write term level functions that can operate on these types, which needs dependent types Couldn't this be done by simply duplicating such functions on both type and term levels automatically (when using data kinds and such), again without true dependent types?
Book's in the mail. Grabbed it a few days ago with Manning's half price deal of the day. 🙂
Well no. The reason I want term level functions is to get GHC's term level inference and other features. I can already do this stuff type level, and my particular use case doesn't *need* term level functions for this stuff. They're just a shitload nicer and better implemented than type families are.
The best source on the topic is, unfortunately, [this](https://www.reddit.com/r/haskell/comments/2w4ctt/boston_haskell_edward_kmett_type_classes_vs_the/) reddit thread and the talk it's discussing. The main point (IMO) is that when you're doing complex type level things, e.g. `Compose`, `Sum`, and `Product`, and you want to use type classes, coherence is an invaluable reasoning principle. You do not want to be worrying about _which_ instance of `Monoid` the first argument of your second case used to construct the overall `Monoid` instance.
So in that case if you could define a term and type level function the same clean and easy way you can currently define a term level function would you be happy? So like: id x = x Would be both: id :: a -&gt; a and: id :: * -&gt; * Now of course allowing lowercase type families would mean that you would necessitate explicit foralls. But honestly having `ScopedTypeVariables` (and thus explicit foralls) be the default is better IMO, as long as we can find a replacement to actually writing out `forall`. Perhaps `\/` or `/\`.
&gt; with function-like things on these types sort of representing DB migrations I've been dreaming of a migration library like this, where each migration sequentially builds up the table and at the end you prove it's equal to the schema used in the app. I figured it would be possible to do in today's haskell type system but i'm not a singletons wizard. 
&gt; But that doesn't require proofs or a proof assistant of any kind. That's only a subset of what dependent types are. If you don't want to use that subset, don't. But the things I'm talking about are still dependent types.
Ah I see. From what I understand, system Fω already supports this, but it would be nice to have it in GHC. &gt; which is the part I am unconvinced about, as it seems to require lots of proofs and an actual proof assistant in the compiler For what it's worth, just having dependent types doesn't actually force you to write proofs, in the same way that having a value of `[a]` doesn't guarantee the list is non-empty. The proofs only need to be constructed if the rest of your program demands them. For example, you can simply define the unbounded list type as normal in a dependently typed language and write all your code with that, but you'll have to use safe `headMaybe` or `tailMaybe` functions and then pattern match at run-time to use it. On the other hand, using size-indexed lists would avoid that check, because the compiler would verify at compile time that the `head` operation was safe, but in that case, the `head` operation would require a 'proof' (in the form of a value of a size-indexed list) that the list was indeed non-empty, and you would have to provide that proof. We have a system like this in Haskell today. If you always use the `NonEmpty` type in Prelude, you have safe versions of `head` and `tail`. However, to convert a normal list to `NonEmpty` you must pattern match on the list at run-time. This run-time check is equivalent to a proof that the list is non-empty. But, I'm with you, I'm not sure I see the value of dependent types for general-use (although they could be really nice for DSLs).
For what it's worth, I'm working on a similar system in the next version of `beam`. It's in the `beam-0500` branch in the `beam-migrate` library (see [here](https://github.com/tathougies/beam/tree/travis/beam-0500/beam-migrate)). Basically, you would define two table types, one per version. data TableV1T f = TableV1T { tablev1Field1 :: Columnar f Int } data TableV2T f = TableV2T { tablev2Field1 :: Columnar f Int, tablev2Field2 :: Columnar f Int } Now a migration between the tables would be a function from `TableV1T MigratingField -&gt; Migration (TableV2T MigratingField)`. The only way to produce a new value of `MigratingField` would be to use an `addColumn` function, since the constructors of the type would be hidden by the module system. Thus, the straightforwards migration would be migrate (TableV1T field1) = TableV2T field1 &lt;$&gt; addColumn "field2" The only problem of course is that you could do something like migrate (TableV1T field1) = pure (TableV2T field1 field1) In the future, this could be solved easily with linear types, but we have to work with what we've got. I'm planning on checking that all transformations were correct at run-time. On the other hand, my intuition tells me there's some way to guarantee correct enough behavior without linear types using some fancy existentials, but I'm not sure how that would work exactly. The actual schema versions themselves are kept in a series of modules `&lt;top-level&gt;.Schema.V0001`, `&lt;top-level&gt;.Schema.V0002`, etc and there's a utility `beam-migrate` that will load these modules in interactively (using `hint`) and execute them against your database, tracking state to make sure only the migrations that need to be applied indeed are. I'm planning on adding verification and migration generation capabilities as well. Your application only ever loads the `&lt;top-level&gt;.Schema` module where the latest database schema is re-exported, or the individual schema versions if backwards-compatibility is necessary. This is mostly unimplemented as of now, but planning is the first step :)
&gt; but you'll have to use safe headMaybe or tailMaybe functions and then pattern match at run-time to use it. Hmm... to me that sounds a lot like having to use proofs. Because the thing is, sometimes I do want to use `head` on a list because I know it isn't empty, but I really don't want to prove it. I do think that avoiding partial functions is generally a good idea, but sometimes I kind of have to, to get the type I want and not have to throw in a `Maybe` on the output type because of an implementation detail. I would definitely not be happy if Haskell required you to actually prove totality, I have written plenty of functions that I would never be able to prove total, but that I know are.
Great, looking forward to further development. One immediate observation - you seem to have rolled your own TLS. That's a very deep rabbit hole. Vincent has already done all that work, and maintains it against the constant changes in the area of crypto, so I highly recommend using his [tls](http://hackage.haskell.org/package/tls) library. Or better yet, a higher level wrapper for that like [connection](http://hackage.haskell.org/package/connection), or an even higher level wrapper for that like [http-client-tls](http://hackage.haskell.org/package/http-client-tls), or an even higher level wrapper for that like [http-conduit](http://hackage.haskell.org/package/http-conduit). EDIT: http-client-tls of course, not http-client.
Do it!
The flood gates are open now ladies and gentlemen. Expect a huge influx of new Haskell programmers with all kinds of questions! Thanks to all. Here's how to get there: https://www.reddit.com/r/place#x=269&amp;y=573
What is this?
It's a reddit thing for April fools. http://www.reddit.com/r/place
We may be SOL :)
If by fought, you mean waited patiently, then yes :)
I mean an infinite loop / crash with error message is far less of a big deal than true undefined behavior. But I get your point. I can see the advantage of not being able to cheat and use undefined to prove contradictions. But as I said, for practical programming I really really just don't want to deal with lots of proofs. 
&gt; Is it necessary to annotate everything even in let bindings? Not necessarily. This, for example works fine; mirror : List a -&gt; List a mirror xs = let xs' = reverse xs in xs ++ xs' However, it only works because the type `xs` is known, and the type of `reverse` and `++` is so simple. Going back to my example, if we tried; z : List Integer z = let xy = [(x,y) | x &lt;- [1..10], y &lt;- [11..20]] in [y | (_,y) &lt;- xy] we would get a rather bizarre error telling us that there are type-variables that cannot be inferred. &gt; Cannot it just try to infer most simple (no dependent) type if there isn't type annotation? In the simplest cases, maybe. In general, setting all type-level functions to be constant won't type check. If you wanted to, you could stick to polymorphic-only functions, and it won't yell at you if you don't push it too far. For example, if we use the polymorphic elimination principal I mentioned before, the following works. z : List Integer z = let xy = [(x,y) | x &lt;- [1..10], y &lt;- [11..20]] in [tupRec (\a, b =&gt; b) y | y &lt;- xy] Note that this is only so because `tupRec` was declared before hand to have such a restricted type. Sticking only to polymorphic functions kind of defeats the purpose of using a language with dependent types, though. We will typically want vastly more general functions than what can be done polymorphically, it's why languages like Idris were made in the first place. Furthermore, having to annotate everything isn't that much of a hassle anyway. 
&gt;Sticking only to polymorphic functions kind of defeats the purpose of using a language with dependent types, though. I think there is a place as for polymorphic functions as for dependent functions in one code base. And both types could be used when they are needed. &gt; Furthermore, having to annotate everything isn't that much of a hassle anyway. I think for some complex algorithms or systems yes, but in something like webdev or gamedev there is so many code where you don't care what type it has. It is possible to annotate everything, but nowadays even Java and C++ have type inference and there is good reason for it.
I'm no expert here but here are some resources: https://www.fpcomplete.com/blog/2015/09/stack-more-binary-package-sharing https://nixos.org/nix/manual/#chap-package-management Also, a great series about Nix is Nix Pills: http://lethalman.blogspot.ca/2014/07/nix-pill-1-why-you-should-give-it-try.html
Here's a post about it https://www.fpcomplete.com/blog/2015/09/stack-more-binary-package-sharing
Petition to expand the logo 2 pixels down and use a readable 5x3 font? ## # # # # ## # # ### # # # # # # # # # # # # # # # ## # ### ### # ## ## # # # # # # # # # # # # # # # # # # # # # # ## # # ### ## ## EDIT: shortened the middle arm of E and made Ls 2px wide 
I'm passing on this open source maintainership of these pixels to you the community. I'll be on an airplane all day and I'm too cheap for the wifi. Take good care. The fate of the entire Haskell ecosystem is in your hands. Remember always photoshop your idea first... goodbye now everyone :{D
It looks like you're trying to mention another user, which only works if it's done in the comments like this (otherwise they don't receive a notification): - /u/biglambda --- ^I'm ^a ^bot. ^Bleep. ^Bloop. ^| ^Visit ^/r/mentionhelper ^for ^discussion/feedback ^| ^Want ^to ^be ^left ^alone? ^Reply ^to ^this ^message ^with ^"stop"
I'd rather expand it than recolor. This is a lot more wasted effort
Perhaps we can paint new logos on the least contested tiles? Like the ones around the edges?
Congratulations! Does the book have an explanation of elaboration?
I don't think it is /that/ worthwhile a comparison. Sure the surface syntax is similar and other features such as Idris' interfaces look similar to Haskell type classes, however fundamentally they are different languages underneath. The only comparison would be, in my mind, related to syntax, and then the comparison would be look at how we can do X in both Haskell and Idris, and see how the solutions compare. Which is something you can do with other languages such as OCaml, F#/*, Java, Agda et cetera. 
&gt; just don't want to deal with lots of proofs. Me too! I think more work needs to be done on proof inference. While an impossible problem in general, I imagine you ought to be able to figure out a system that can prove the most common access patterns
realistically it's going to be a ways off. We've barely got web apps down - ghcjs, reflex, servant are great technologies but they're still proving themselves one project at a time. There is almost no mobile development with haskell, and the things that do exist are pretty much at the "hello world" level. "In principle" you could derive multiple UIs from a spec and something like combining ghcjs w/ electron or react-native would work. But there's a big gap that still remains to use these technologies routinely. The challenge now is to move ideas and tools beyond the "proof of existence" level of capability which we've been stuck at for some time now.
Is there a nix docker image with reflex configured? or maybe stack.yaml with latest reflex? I cannot make reflex-platform work on my machine (Arch Linux)
So this is basically a virtual machine wrapped up inside a docker image?
&gt; This is a lot more wasted effort I mean, r/place is just pure wasted effort. So it's merely a matter of how we choose to waste our efforts.
Cross compilation is an area of much interest, and it is being worked on. It's something Nix is totally capable of, just takes some work.
&gt; you really ticked me off with this comment. I don't think there is any reason for ticking. It's just a conversation. Yes, I don't have much experience with dependent types. But when I use Scala or F# I really with they have type inference similar to Haskell. I try to split code as much as possible to small one line functions. And it's annoying when you want to move some code to new function you need to always annotate it. And especially when you need to refactor this functions later. I think if I start to use Idris I would have similar problem with it. But maybe I am wrong and somehow it would be different.
2 pixels is exactly as far as we can go without hitting the trans banner. If make a template for the readable font, you can probably convince /u/biglambda to edit their post. In preparation, I'll be trying to extend the blue-lightblue border downwards.
Haskell CERTAINLY is not a language where you can get by with superficial understanding. It's not like ruby or python.
In the case of one-symbol operators like `+`, I've seen it more often as `:+:`, to keep the symmetry I guess.
Yes tidalcycles is about making live music with code. As I said, it goes fine but I was looking for advice about how to make it easier.
We're talking about an EDSL.
Much more readable! EDIT: what are the dimensions?
You can often find the answers to your questions on stackoverflow (and might find it a nicer forum for Q&amp;A than reddit). Typing the first line of your type error into the search bar for instance yields this answer: http://stackoverflow.com/questions/36608918/haskell-couldnt-match-expected-type-io-t0-with-actual-type-integer
Aren't we going to conflict with people who have scripts with the current config open? EDIT: we could in theory fit like this: [http://tcpst.net/b2v_.png](http://tcpst.net/b2v_.png)
&gt; It won't matter unless it's actually enforced.. but can we really enforce it? Are we going to start banning people from the mailing lists? Agreed, a CoC that is not enforced (or is inconsistently enforced) is possibly worse than no CoC at all, since it provides a false sense of safety. But ultimately, that’s the real question here, isn’t it? Do we want to enforce a certain standard of civility or not? A CoC is just a way to codify that enforcement; it isn’t magic. So why is it important? The trouble is that “be polite” is a vague and relative promise of enforcement. Someone afraid of being hurt cannot depend on a “be polite” policy to protect them since it is not clear what is or isn’t “polite”. As /u/venju mentions, a CoC is a way to make proper and fair moderation easier, but it’s more than that, too. It’s also a promise to community members that they will not be harassed or attacked. It’s easy to look at our community and see a group of people who are relatively civil with each other, but be **extremely careful to not confuse homogeneity with inclusiveness**. Consider the sort of people we *don’t* have in our community in large numbers. I can tell you, firsthand, that there are lots of people who are afraid of the Haskell community for being harshly academic and intolerant of people who only want to get things done. *Whether or not this is actually true*, it is an impression people have, and a way to dispel that fear is to make a written promise that says that behavior is unacceptable (and somehow enforce it). Extend this to minorities who are generally treated with insensitivity in tech as a whole (or, more even more generally, in society as a whole), and realize those people are generally wary of participating in *any* community, since they are quite used to being harmed. A CoC can be a way to make those people feel safer and more willing to participate. If you believe our community will not hurt those people, then great! Might as well put those promises in a CoC, and it won’t change how we work as a community at all, it will just give people comfort that we are accepting. If you *don’t* believe that, then, well, obviously we need a code that states it isn’t okay so it can be fairly enforced. Either way, a CoC is pretty clearly a good thing unless you are for some reason uneasy with the idea of disallowing hate speech and harassment.
It worked! No idea what this sub is about, but I'm here
Yeah both are used and there are some tricks to get nice syntax. A cool example is `:::` for expressing ‘type of’ data Ty :: Type -&gt; Type where TInt :: Ty Int TBool :: Ty Bool (:×:) :: Ty a -&gt; Ty b -&gt; Ty (a, b) data Val :: Type where (:::) :: a -&gt; Ty a -&gt; Val allowing a notation similar to Haskell's double-colon: 42 ::: TInt (42, False) ::: (TInt :×: TBool) à la [`Dynamic`](https://github.com/bgamari/ghc-proposals/blob/typeable/proposals/0000-type-indexed-typeable.rst#implementing-datadynamic) from the proposed [type-indexed `Typeable`](https://github.com/ghc-proposals/ghc-proposals/pull/16). **Edit**: Same with `:=` for assignment data Decl a = String := a
Great, now just start teaching yourself Haskell. What could go wrong?
I really like Rust's [CoC](https://www.rust-lang.org/en-US/conduct.html) and I think it would have helped in this case. It seems like a tricky bit (at least for the Haskell community which doesn't have centralized leadership like Rust's) is not adopting but finding people who will call out and enforce. If people do decide to adopt Rust's CoC I suggest making it very clear where it will (and won't) be enforced. In this case the incident is within scope of the CoC because it happened on the official libraries mailing list but the linked Reddit [discussion](https://www.reddit.com/r/haskell/comments/5m678s/suggestion_code_of_conduct_for_haskell_community/) above points to a Twitter thread taking place on personal accounts. Another tricky issue is uniform enforcement: the mailing list thread above is bad but there is at least one person who is just as unpleasant on this subreddit. Their comments are downvoted pretty quickly so does that count as enforcement? Is it fair that they might have been outright banned from the mailing lists for the exact same comments vs. just downvoted here? The Rust CoC doesn't say anything about this so it might be worth elaborating.
Three objections: 1. The musicians using the OP's library are doing just fine. 2. Exactly how deep was your understanding when you wrote your first useful program in Haskell? 3. When applied to "understanding", "superficial" and "deep" are relative terms. Contrasting them as if they were absolutes isn't very helpful, as it tends to foment elitism and mystification.
Codes of conduct only provide value when they are enforcable. You can legally enforce certain standards of conduct in physical venues. IRC, mailing lists and other platforms can punish poor conduct with banning. Prescribing a code of conduct to a programming language just seems like virtue signalling. How would you enforce it? Ban someone's access to the technology? Ban their open source contributions? Maybe just forcibly take ownership of all their projects? This isn't a challenge to find better ways to police communities: at that level, *nothing* should be enforcable. In the case of the twitter thread linked in your proposal: how should that have been resolved? If it repeated, how then? One or both parties potentially removed from the community? Some people just don't get along. And that's okay, as long as they continue to contribute to the Haskell ecosystem. I am a proponent of meritocracy in tech, and I think the code of conduct movement directly opposes this. Some personal notes: - I've never seen a code of conduct that I completely agree with - I feel like the code of conduct movement encourages authoritarianism - The code of conduct movement seems to value diversity over ability and I disagree with this - I generally just have a "bad feeling" about the whole idea. Can't really articulate it. Something just seems "off"
Civility issues aside, I learned something that I guess I missed in the previous debate: maximum (3,2) =&gt; 2 minimum (4,5) =&gt; 5 sum (6,7) =&gt; 7 product (8,9) =&gt; 9 ~~That's nearly "wat" quality somewhat surprising. I get how it came about, but are there any cases in which calling `maximum` etc. on a tuple ever makes sense? There probably are and I'm just failing to come up with them. I'm more surprised that there isn't a compiler warning/a type error.~~ Edit: nvm, I obviously hadn't thought about it enough as expecting normally number-ish functions like `maximum` etc. to "just work" over Tuples doesn't make sense. It's still "wat"-ish because it's surprising but more in the vein of "because Haskell is awesome". 
I believe one compromise suggested was to enable compiler warnings for contentious Foldable instances.
A code of conduct is a statement that the community believes certain things are important. That's its primary value. It's more like a comment than a type declaration&amp;mdash;by itself it doesn't have power, but it sets clear expectations. It can be discussed and changed just like a comment, but if you're not changing it, contributors are expected to abide by the comment as it is, whether or not anyone enforces it. Of course, yes, if there isn't any interest in enforcement, it becomes like a comment that's ignored; you're better off having no comment at all. But that isn't to say that because a comment is ignor_able_ that it has no value. I do think that new contributors who are inclined to "behave badly" will see the CoC and know that their behavior won't be appreciated; there are different norms in different places. What flies on LKML won't fly on /r/rust, for instance. That has value. It also needs to be true that prominent, long-term contributors who violate the CoC get called out for it and their behavior isn't tacitly accepted. But if those same contributors started changing the behavior of code and not updating comments, and getting upset when someone tried to revert their change, they'd get called out for it too, right? The CoC should be no different.
Neither is the behavior on display, doesn't matter if the person is smart.
It is very difficult to retroactively enforce a code of conduct on a community. You're ultimately negotiating a new social contract, but not everyone is tuned in and at the table. Forming a splinter community where such a thing is strictly enforced can be done, e.g. on irc.freenode.net there is both the mainstream #haskell channel and #lambdanow, but look at what happened when a couple of members of the scalaz community tried to impose a CoC there without sufficient buy-in.
Thanks to the `Foldable` instance, you might write... GHCi&gt; (sum . Compose) ("foo", [1..10]) 55 ... which is entirely sensible. I don't think there is much to get from using those methods with monomorphic result types directly with tuples, though. On another note, I don't think `sum (6, 7)` is likely to happen in the wild. A more compelling example of this sort of accident is: suppose that you use functions from some library to produce... results :: [Int] ... and that you do a `length results` somewhere. Then, thanks to a change in the library, `results` becomes: results :: Either FooError [Int] That said, some of the `Foldable` methods can be actually useful with `Either` (`foldMap`, for instance, amounts to `either (const mempty)`); and the instances of the `Traversable` subclass for all of `Maybe`, `Either a` and `(,) a` are clearly useful. (That's one of the reasons why, as far as I'm concerned, removing the instances is simply not an option, and if something is to be done a softer compromise is necessary -- cf. for instance the [recent suggestion by Edward Z. Yang](http://blog.ezyang.com/2017/03/proposal-suggest-explicit-type-application-for-foldable-length/).) 
Turned out great. Nice work.
There hasn't been any new content to address.
I meant Edward Yang :) As far as I know that was a reddit-only thread, and I would have been interested to see what the folks on the libraries@ list thought of the idea. From the reddit thread on the type-applications-based idea, I understand your concerns / preferences for poisoning instances. Maybe you convinced Edward Yang and that's why we haven't seen it on the list :)
&gt; And it's annoying when you want to move some code to new function you need to always annotate it. And especially when you need to refactor this functions later. If you are commonly reusing a piece of code, then you should abstract it out into it's own function outside of a let or where binding. You'd only need to type it once. The only time I put anything in that kind of binding is if I'm certain it won't be used anywhere else. Most top-level functions that I write tend to be one-liners. I know that sort of code reuse is common in imperative settings, and Scala takes from those languages heavily, making that style attractive, but it doesn't have any real benefits in a purely-functional setting. This is part of designing your types properly. A Haskell or Idris program should be structured around your types. A module should define a small collection of related types, and any and all functions useful for manipulating that type. An example I've interacted with recently is a feed-forward neural network. I've gone through the process of implementing back propagation, so it's more or less complete at this point. When I was implementing it, I took full advantage of the fact that NNs are a special case of a free semi-category. About a third of my implementation was just functions for manipulating those. data FreeSemiCategory : (a -&gt; a -&gt; Type) -&gt; List a -&gt; Type where Out : f m n -&gt; FreeSemiCategory f [m, n] (&amp;:) : f m n -&gt; FreeSemiCategory f (n :: l) -&gt; FreeSemiCategory f (m :: n :: l) mapSemi : ({a, b : t} -&gt; f a b -&gt; g a b) -&gt; FreeSemiCategory f v -&gt; FreeSemiCategory g v mapSemi f (Out v) = Out $ f v mapSemi f (v &amp;: vl) = f v &amp;: mapSemi f vl zipWithSemi : ({a, b : t} -&gt; f a b -&gt; g a b -&gt; h a b) -&gt; FreeSemiCategory f v -&gt; FreeSemiCategory g v -&gt; FreeSemiCategory h v zipWithSemi c (Out f) (Out g) = Out $ c f g zipWithSemi c (f &amp;: fl) (g &amp;: gl) = c f g &amp;: zipSemi c fl gl ... Network : List Nat -&gt; Type Network = FreeSemiCategory Layer (Note that this type actually carries around more information than a free semicategory. This is to an actual semicategory what a vector is to a list) At various points, I wanted to extract out other pieces of information, such as the weights of each layer, and the action of each weight. I didn't need to make anything new. wFun : FreeSemiCategory (\m, n =&gt; Vect n Double -&gt; Vect m Double) (i :: v) wFun = mapSemi {f = Layer} ((*.) . transpose . layerWeights) net and the gradient itself is just stored as a neural network. To combine them, I can just zip them together using a function that combines individual layers; applyGrad : Double -&gt; Network v -&gt; Network v -&gt; Network v applyGrad rate n g = zipWithSemi (applyLayer rate) n g As an aside, non-empty lists (free semigroups) are a trivial special case of a free semicategory, and, in theory, the map, zip, etc functions could be replaced with something like my `mapSemi` and `zipWithSemi `, so that these functions designed originally for manipulating neural networks, in their true generality, are a generalization of the ubiquitous `map` and `zip` functions. There is a correct style to programming in any paradigm. More often than not, frustration with one aspect is a sign of inexperience, and you yourself have to change to fix it. Edit: I should re-mention [this post](https://izbicki.me/blog/gausian-distributions-are-monoids.html) and [this presentation](https://www.youtube.com/watch?v=WsA7GtUQeB8&amp;ab_channel=LambdaConf). A good Haskell or Idris programmer should take the perspectives expressed there very seriously.
There are two things that you might mean here: how Idris is translated to the core language, or how that machinery is exposed to Idris code for metaprogramming. Neither is really covered, but you can learn about the former from [Edwin's JFP paper](http://eb.host.cs.st-andrews.ac.uk/drafts/impldtp.pdf), and the latter from either our [ICFP paper](http://davidchristiansen.dk/pubs/elab-reflection.pdf), [my PhD](http://davidchristiansen.dk/david-christiansen-phd.pdf), or by watching [the talk](https://www.youtube.com/watch?v=pqFgYCdiYz4). There's also built-in API docs for every symbol exposed from the library that you can browse from the Idris REPL, but that doesn't give much of an overview of the whole system.
It does not support it in the style of Haskell or ML, where the top-level annotation can be omitted. However, limited type inference is available as an editor feature. I can, for instance, write the following in my editor: dog : ?canineType dog = "puppy" and then run the "Idris fill this hole" command on `?canineType`, which results in: dog : String dog = "puppy" This can be a nice way to save typing on large, complex, and obvious types.
It's at least a very, very elaborate April Fool's joke....
Yeah, looks like we got outnumbered very fast. We'll look for somewhere else. Thanks for the response!
Thank you. For everything. For what you do for the community, for the libraries you write, and for the time you take to explain things in such approachable detail... even in response to my rather dim question (sorry about that). Last time I got to enjoy an "Ed drops knowledge" moment it was for lattices at Compose, which was sweet! I feel like I wasted my question this time (and your time in replying). How/why this was happening always made sense to me--though not in the depth that it does after reading your post--but `maximum (10,2) == 2` seemed so obviously weird at first glance that I didn't stop to ask "does calling `maximum`/etc. on a numeric tuple, to get the max/etc. value, make sense?" No. Of course it doesn't. I was expecting it to be "Foldable-but-nice" because I hadn't thought about it. This clicked about a minute after I initially posted... and by the time I came back I realized that I had confidently yet unintentionally walked into an old hornet's nest. I'll see myself out. 
Incredible threat, 10/10.
I'm not sure we have a common understanding of the word *threat*.
Guess not. Sadly you chose to be the julienchurch that cried wolf so I'm having a hard time taking your concerns seriously now.
It'd be nice for EDSLs. Hopefully we'll have learned from our mistake with OverloadedLists and not require both the embedding and projection to be included in the same typeclass. This ruined OverloadedLists for EDSLs. =(
Disclaimer: the co-maintainer of `these` here. I see that the package documentation can be improved, can we use your examples to put them in as doctests? Btw: if we'd add a `lens` dependency to `these`, the dependency footprint won't increase much: ../these master % diff without-lens.txt with-lens.txt 30a31 &gt; lens 4.15.1 31a33 &gt; parallel 3.2.1.0 39a42 &gt; reflection 2.1.2 
Global type inference of Rank-3 (or higher) types is "impossible" (I can't remember if just turing complete, or actually non-computable). Dependent types subsume all ranks. That said, it should be *possible* to have the elaborator attempt a Rank-1 or Rank-2 type inference and then fail to inference (i.e. requst a type annotation) when that fails, but it's not done in the current elaborator.
This would go nicely with bringing some form of homogeneous tuple types into common parlance. A quick look at Hackage brings up [*fixed-vector*](https://hackage.haskell.org/package/fixed-vector-0.9.0.0/docs/Data-Vector-Fixed.html#t:Index) and [*vector-sized*](http://hackage.haskell.org/package/vector-sized-0.5.1.0/docs/Data-Vector-Sized.html) (which aim at a *vector*-like infrastructure), [*tuples-homogeneous-h98*](https://hackage.haskell.org/package/tuples-homogenous-h98-0.1.1.0/docs/Data-Tuple-Homogenous.html) (a simplest-thing-that-could-possibly-work alternative) and [*fixed-length*](http://hackage.haskell.org/package/fixed-length-0.2/docs/Data-FixedLength.html) (which fits somewhere in between, I guess). None of them seem to be particularly well-known as of now, though.
BTW, I'm really glad to see that someone is using `merge` and `mergeA` for `Data.Map`! I actually wrote `mergeA` more because I *could* than because I saw an obvious need, but thankfully it seems to have found one. You might be interested in the fact that Wren Romano has now implemented this API for `Data.IntMap` as well. Would it be okay if we linked to your blog post in the documentation?
&gt; 32nd/10000th edition So, what happens after May, Year 5001?
A process I've seen elsewhere (e.g. in Python's PEP process) is to make decisions and then capture the points brought up before the decision was made. The idea is then to not revisit decisions if the circumstances didn't change since the decision was made.
Turn backslash into lambda maybe?
Well, we need a something more artistic in general. Just this white block with our logo is kind of offensive to the rest of /r/place
Right now we're conflicting with some pink head that I unfortunately don't know what is, but they're all over our upper left corner.
I think it's worth saying that `These` is to `Either` what `or` is to `xor`.
Agreed. One point worth noting is that the Haskell community may already have a code of conduct, in a formalized form that you can cite: [SPJs letter][1]. Sure, the title is not "code of conduct", but from an operational perspective, it fulfils the same role. In fact, the OP has referenced it in his email. [1]: https://mail.haskell.org/pipermail/haskell/2016-September/024995.html
`these` already depends on quite a bit due to the needs of Chronicle and Bicrosswalk etc, but I regularly run up against smaller packages depending on `lens` just to provide simple lenses and traversals. I thought `these` was a nice example of how to provide those without imposing irrelevant constraints on downstream users. For example, `lens` currently can't be used with `containers-5.9.2` (for `mergeA`) due to upper bounds, so depending on it for no reason really riles me up. Thanks for maintaining the package! Feel free to use my examples however you like.
I'm interested by the same thing in France ;) (Edit: I don't want to relocate from Lyon because of my family and I'm not really happy with my current remote situation, so I'm looking for a job opportunity in Lyon.)
"Unfortunately" I'm "far" (2 hours of train) from Paris. I'll follow that meeting, perhaps one will happen when I'm close to Paris. Thank you.
Respect for the Bald forties. We want to establish a base camp near the Haskell logo to express our discomfort for the invasion of cool kids in the Haskell community. We would really appreciate if haskoolers don't expand. That would add humiliation to torture.
Thank you! I was indeed looking for elaborator reflection. Your work is very interesting. 
The guy you replied to is a well-known troll over here. `servant-everything` isn't a thing.
**Here's a sneak peek of [/r/KamenRider](https://np.reddit.com/r/KamenRider) using the [top posts](https://np.reddit.com/r/KamenRider/top/?sort=top&amp;t=year) of the year!** \#1: [Just sharing my genderbent Hidari Shotarou cosplay](http://imgur.com/a/FhmY4) | [36 comments](https://np.reddit.com/r/KamenRider/comments/4gqgrg/just_sharing_my_genderbent_hidari_shotarou_cosplay/) \#2: [The cast of Ex Aid having some fun](https://i.redd.it/9w3vy2d3wdmy.jpg) | [16 comments](https://np.reddit.com/r/KamenRider/comments/60alip/the_cast_of_ex_aid_having_some_fun/) \#3: [So I recently cosplayed a Genderbent Eiji Hino. I hope you all like it!](http://imgur.com/a/aO3n1) | [19 comments](https://np.reddit.com/r/KamenRider/comments/51445i/so_i_recently_cosplayed_a_genderbent_eiji_hino_i/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
It's not for everyone. I'm sure many frequenters of /r/haskell can attest to the various ups and downs of remote work. I personally found the lack of social interaction, asynchronous communication, and potential work hours due to time zone difference (with say, the US) to be the biggest challenges when starting out. But these can be overcome with time/patience, and the flexibility and distraction-free environment I can create for myself tends to make up for it in terms of sheer productivity now. Also having first-hand experience with Czech salaries - you can probably find a role with much better compensation if you have either the experience or a relevant academic background, by looking abroad.
2 hours isn't very far. I was in a similar position (2.5 hours outside a major city) and got a job opportunity through attending a Meetup. I say go for it, do a bit of reading on the train and the time will fly.
Two hours in return for a potentially fulfilling lifetime through networking :)
&gt; Point being that there are always going to be some bad apples. While I don't necessarily disagree with your thesis, the original idiom you're referencing here reads (many variations on) "a few bad apples spoil the barrel", and means "you need to be really vigilant and get rid of bad behavior (or badly behaving people) immediately". I don't really know how we came to treat it as if it were "a few bad apples are no biggie, deal with them if you happen to notice". Not, of course, that reasoning from tenuous analogy and traditional sayings is particularly rigorous in the first place - but it's probably marginally better than reasoning from their inverse...
Python trying to takeover the haskell logo! https://www.reddit.com/r/place/#x=269&amp;y=573
Have some empathy. That person thinks python is a good language.
Thanks! Sorry I haven't written proper programmer docs yet.
Whether this makes sense or not depends on what you think a tuple is supposed to represent. One point of view is to see a tuple `(a, b)` as a container that contains exactly one `b` and has an additional label `a`. An `a`-labeled `b`, so to speak.
Woah, thanks for that SO link! I happened to need exactly that today for something completely unrelated at work. :)
foldr fuck 0 ["functional programming","bullshit","your place"]
Travis does have Nix support now. https://github.com/dmjio/google-translate/blob/master/.travis.yml Using something like this in hydra would be as trivial as making your own release.nix with an attribute set that contains your project built with different GHCs. Like this: https://github.com/dmjio/stripe/blob/master/release.nix
&gt; only to find out there are not that many jobs in europe Really! Before finding my current Haskell job, I was considering moving to Europe because it seemed to me like there was a lot more FP jobs there. The grass always looks greener on the other side of the fence!
[](/winkiepie)Parallelism, too, amirite? ;D
But then there is this phrase in the rust CoC: &gt; Remarks that moderators find inappropriate, whether listed in the code of conduct or not, are also not allowed. (2. under moderation) Frankly, I don't understand at all what the purpose of this phrase is. It seems to put the opinion of a "moderator" above that of a non-"moderator", unconditionally, before the fact. If the purpose of the CoC is to make it clear where the community "draws the lines" between behaviour that is tolerated and behaviour that will incur repercussions of some form, this phrase erases that line completely. The purpose seems to become to solely make moderation easier, by pretending to give the moral upper hand to the moderators. Which would be political/rhetorical nonsense, to say it politely. Perhaps the phrase has another purpose that I fail to spot. But given my best current interpretation, the phrase conflicts with the very purpose of a CoC as discussed here. Such an inconsistency leaves a rather bad taste.
This helps; thanks. 
Running this, but I trimmed the tiles to just the text, since that seems higher priority.
I remember my friend working in Seznam.cz (a local google-like company) telling me that they use Scala in some of their products. I'm not sure if you consider Scala to be good enough :-)
It's the same 4 or 5 accounts, which I suspect are all alts running a bot. I keep changing bits back while I wait for code to compile. I also moved the logo 4 pixels to the right to avoid the purple head thing, but it looks like people are running the old script for the logo and it is changing back (/u/dllthomas). Not sure if the purple head will be rebuilt but they are also running scripts, it's probably best to stick with the shifted location. Mexico are also creeping into the top border.
This is a good read. My favorite sentence (to be taken in jest of course): &gt; The `Applicative` instance could easily continue collecting every `This`, but that would make `ap` different to `(&lt;*&gt;)`, likely offending lambda man or whoever.
I was thinking of something in that direction with respect to release.nix.... But one needs to handle the built in ghc library versions though, or does haskellpackages et al already know to change out those library versions when you change ghc? (https://www.fpcomplete.com/blog/2014/05/lenient-lower-bounds) I might use travis in the interim until I have more time to install and configure Hydra server.
Nope, the purple creature is not us, it's these guys: https://www.reddit.com/r/KamenRider/comments/634flo/our_last_stand_on_inserting_mighty_onto_the/
Just in general, there is never going to be a "master" language that saves you from having to ever learn another. 30 years ago, people would have said that LISP was the only language you would ever need. If Haskell works for your applications, then great. But the lingua franca of the programming world is unlikely to be a functional language in the near future - people are just too comfortable with declarative and object languages. 
My notes on running C/C++/Haskell on Lambda: https://github.com/chadbrewbaker/introToAWSLambda Basically an exercise in statically compiling or bringing any shared libraries you need with you in the zip file. 
It is probably easier to bolt dependent types onto Haskell (this is well underway) than it is to write a competitive RTS and library ecosystem for Idris. I kinda hope that dependent typing becomes common and useful enough that Haskell's clunkier approach is outmoded, but I'd be surprised to see it happen in the next 5-10 years.
If [dependent types](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell) and [linear types](https://ghc.haskell.org/trac/ghc/wiki/LinearTypes) land in time and get significant adoption in time, I can't really think of any other way another language can take over Haskell as for now. &gt; I was hoping that with Haskell, I would now finally be set for life. I do hope you're joking. I take that as 'Ah so *that* should be the last model of programming to see that isn't designed to be esoteric' in the joking sense.
This is really good timing - I was recently working with a collection of Seqs and wanted to zip them. They aren't the same length, and i don't want to drop things on the floor - align is exactly the right tool for the job
As a French guy who (recently) found a Haskell job: don't hesitate to contact random people who do things you find interesting and ask for advice / ideas. Even if they're not hiring, they may still give you some good leads. That worked for me. My personal opinion is that there are far more Haskell jobs than people think ; most of them are hidden though, you have to fight to find them, especially if you don't have a strong FP resume.
&gt; But I do wonder whether the growth of such abstractions will taper off as time goes by. As long as complexity of programs increase, abstractions will grow. I don't expect the end of that growth of complexity to happen soon enough to be significant to talk about here.
Apparently the submission of this to the reddit got caught in the spam filter. Since its well past April 1, I highlighted when it was made, even though that ruins some of the fun.
I'm happy to see this, even it ended up late. Made me chuckle.
lel, deal with it botter
It's over, we got a spot, I don't really care. You guys are greedy treacherous fucks, though.
From that meetup, I have heard some people are starting something at Vente Privée. I also know some people are using Haskell at BNP. There are some more, but unfortunately Haskell is probably not as lively in Paris as it is in London :/. This meetup is intended to be mensual.
Well, Idris also has strictness going for it. I have not developed any large scale Haskell project myself, but laziness is often cited as a source of pain for Haskell programmers.
&gt; Edward Kmett Caught Writing Monad Tutorial Under Pseudonym Hahaha :D
Once we get an ML with ad-hoc polymorphism and "true" first-class modules (i.e., not just packed modules), that would be my language of choice.
COBOL, once it gets lambdas.
&gt; [...] but lazyness is often cited as a source of pain for Haskell programmers. Yes, it is. I feel this pain every time I see someone outright dismiss the entire language based on hearsay and folklore on lazy evaluation.
Yes - I meant imperative - mea culpa.
Well, as long as I can't write math formulas onto my screen and expect them to turn into computer programs, math being the master language isn't of much help to me. The question is then: what is the next best thing?
Haskell lacks homoiconicity. As it stands I doubt that will affect things, because the competitors are missing out on static typing, monads, etc. but it's honestly a huge feature in my mind. 
It's slightly more difficult to understand from a performance perspective. Virtually always, this means it is faster/more efficient than an equivalent strict algorithm. It sometimes leads to performance issues due to thunk buildup, but I have literally never had this happen in the ~2 years of professional Haskell development, so :shrug:
Yes, it already knows how to change out library versions. If you want hydra, use NixOS 16.09 and do: services.hydra = { enable = true; package = pkgs.hydra; }
I'm one of those weirdos that really likes laziness. Strictness is a surprisingly annoying thing to deal with if you're trying to write idiomatic and performant functional code.
I suppose it's because if you want pure functional then laziness makes sense, but pure functional doesn't really make sense for people ready to dismiss Haskell...
Haskell functions are math formulas. 
There's a really simple Haskell-ish language reimplemented on top of Racket [here](https://github.com/lexi-lambda/hackett/), homoiconicity gives some _huge_ boosts. Lots of things that are syntax in Haskell can be reimplemented as macros. I totally agree that if Haskell was homoiconic (ideally with a some homoiconicity-preserving surface syntax sugar like the "sweet expressions" concept put forward as an SRFI), and had a solid macro system to go with it, it would be my favourite language
A lot of the reason I want dependent types it to prove the correctness of my program in the same language as the program. Dependently typed Haskell doesn't actually let me do that, since the logic it corresponds to is inconsistent. All types are inhabited, and all propositions are true. :/ That said, Idris' approach to laziness is not what I prefer. And, Dependent GHC will certainly be of value; you "just" have to audit your code for \_|\_ by hand or with a linter, since I believe the logic is "morally" consistent. I haven't actually contributed to either ecosystem in many months, but I'm refocusing what little efforts I can make at Idris.
Well, they don't really feel like math formulas when you are writing a long equational reasoning demonstration in pen-and-paper Haskell :)
You have to do that in Idris too, `believe_me`!
&gt; you "just" have to audit your code for _|_ by hand or with a linter A literal `undefined` is easy to find, but I worry about nontermination and nonproductive corecursion. Much proof code doesn't ever get run, so I would expect errors lurking in code that looks correct at a glance -- especially because proofs for complicated stuff tend to grow unwieldy. Then there is the issue that apparently Dependent Haskell will assume data types to be inductively defined, which opens another can of worms. With all that said, correctness is very much a continuum in ordinary programming, so we'll see how the endeavour shakes out in practice. If the bug count is reduced with sensible effort, that's a win.
[can this just be the standard response to any FTP related discussion and the thread closed](http://hackage.haskell.org/package/simpleprelude-1.0.1.3/docs/Prelude.html)
Reddit is obviously a member of the AntiFunctional conspiracy.
tweag.io is exactly the kind of place I'll be happy to work at. However for family reasons, I cannot relocate from Lyon and I really want to try something else than remote work.
Will Fancher recently wrote a [blog post](http://elvishjerricco.github.io/2017/03/23/applicative-sorting.html) (see also the [Reddit thread](https://www.reddit.com/r/haskell/comments/610sa1/applicative_sorting/)) about sorting arbitrary `Traversable` containers without any of the ugly incomplete pattern matches that accompany the well-known technique of dumping all the entries into a list and then sucking them back out in `State`. Fancher used a custom applicative based on the usual [free applicative](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Applicative-Free.html). Unfortunately, this type is rather hard to work with, and Fancher was not immediately able to find a way to use anything better than insertion sort. This gist demonstrates an asymptotically optimal heap sort using a heap-merging applicative.
I wrote [Extra](https://github.com/evincarofautumn/Extra) a while back, inspired by `These`, but never did anything with it. Maybe it exists somewhere already, or someone can find a use for it.
This hinges on identifying what the "normal intuition for using them" is. I, for one, think of `(a, b)` as a contanier of a single `b` element labelled with an `a` about as often as I think of it as a symmetrical container of two elements with different types. One reason why this debate never seems to advance is that if you are sufficiently convinced of the illegitimacy of the assymetric instances you can simply judge my intuition for them as abnormal (or say it is not, to quote a message from the mailing list, "normal people's intuition") -- and conversely, if I regularly use the instances and think in terms of them, I will likely consider the suggestion that my intiution is somehow abnormal as a perplexing claim. (By the way, yet another way of framing the matter is saying that the assymetry does not lie in the `(,)` or `Either` ADTs, but in classes such as `Functor` and `Foldable`, when applied to types parameterised in more than one position. From that perspective, `(,)` and `Either` can be handled symmetrically by switching typeclasses: cf. `Bifunctor`, [`Bifoldable`](https://hackage.haskell.org/package/bifunctors-5.4.1/docs/Data-Bifoldable.html), etc.) 
Please forgive me if I'm not in a hurry to go clutter my code with newtypes and lose the ability to talk about "the" product type or "the" sum type in Haskell when I need to pick one with a type family. Adding "strictly unbiased" versions of everything just makes it so that now there are two things to use, and one of them -- the one you want -- is crippled to the point where you can't do anything with it with any of the standard machinery. New combinator names have to be made for everything, cluttering up the global namespace. Folks learn less about the relationship between all of their classes. There's now a tax of converting to between these forms whenever you guess wrong or have to deal with the impedence mismatch between libraries as there are now two settings. These aren't small prices to pay. Overall, it seems to me to be quite a loss for the language to lose those insights and connections and a complete non-starter to break a decade of code to get to a new equilibrium where everyone's code is more verbose and less stuff works together. Scala has an unbiased Either type and two useless "projections". All that serves to do is make it so you can't write straightforward code using flatMap and that pattern matching on the projections is a pain in the neck, so nobody uses it.
Seems like Rust follows a fair amount of those goals 
It does. I need to learn it some time. At least enough to read the [Pijul](https://pijul.org/) source and see if I can write an importer from Git.
TCO?
Does template Haskell not allow for macros and self writing code? (I might be missing something...)
imo the point is, don't "learn Haskell", learn math. The thing that one ups haskell will simply be more convenient and powerful at allowing you to do math.
Yes but TH tends to be verbose to author with, and more restrictive than macro systems in, say, lisps.
I was not dismissing the language. I was just pointing out that there are other reasons someone might choose Idris over Haskell.
It went out on time. Unfortunately, my attempt to cross-post the post from the mailing list to here wound up stuck in the spam filter and went undetected. I think it was because I linked directly to the image, and then included a comment with a blurb like "As Seen On Haskell Cafe!" with the link to the mailing list post.
 type Lens s t a b = forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t is used internally. Depending on the version of GHC it may or may not require the extension to be turned on to deal with code that uses that type alias, because that alias could be unfolded in negative position, leading to a rank-n type. Here makeLenses ''Pong produces several lenses with the explicit 'Lens' type above, this causes the alias to have to expand when type checking. If we didn't use the alias in the generated code then when you looked at the haddocks, you'd see the big expanded form to the right and have to figure out that it was a lens, etc. yourself.
This is awesome! I love `Applicative` =P
What is "homoiconicity"? Any references or examples?
Yes. It is fun to imagine all those QuickCheck properties out there that pass random input to identical functions and then compare the output. This has a Zen-like feel to it.
I have been eyeing Amazon Lambda for a while, but I wonder if this way of using it - firing up a new Haskell executable on every invocation - does it service. Does anyone have some serious experience as to how that performs, compare to JS functions? Ideally, Amazon Lambda would define an interface that would let it start new executables, keep them running if useful and discharge them when no longer needed, all automatically. Or does it do that already?
| MTL ... it's more that meets the eye. I see what you did there.
I'm curious why you chose the argument order `(m + n)` in the `Sort` constructor. Since `(+)` is defined by induction on the left argument, it seems like the proofs should work out more cleanly with it phrased as `(n + m)`... Though, on further inspection, it looks like it the only place this comes up is removing the need for `plusZero` in `runSort`
lol
I haven't used aws lambda yet, is docker not a thing there?
'/r/haskell' is diagonally to the bottom left of the Mona Lisa
Once you see it, it's always there! "avoid success at at all costs"!
The canonical example is LISP, and clojure is the most popular modern dialect. It basically just means that data structures/code are treated on the same level. For one thing, it means you can write macros to generate code incredibly easily. 
Nope, you get to select from a few different runtimes and then can either edit an inline script, upload a zip file (with some included dependencies), or if you need more than 10MB you can point it to an S3 bucket. 
There's just as much of a tax to pay if you get your elements the wrong way round in a tuple. Given the consequences, I don't see the insights that you can use `Functor ((,) a)` etc as useful ones.
Yes, I think that's the root of this, though my intuition doesn't lead to `maximum (5,3) = 3` and yours does :-) You could also say that the asymmetry lies in Haskell itself by giving special treatment to the last type parameter. But I think we can be careful about propagating that asymmetry.
I was curious so i tried it out. The final result, after some cleanup (on ghc 8.0.2) seems to look like this after desugaring foo :: Bool -&gt; Bool -&gt; Bool -&gt; Bool -&gt; Bool -&gt; Bool -&gt; It foo a b c d e f = let fail :: Void# -&gt; It fail _ = let fail1 :: Void# -&gt; It fail1 _ = Qux in case d of _ False -&gt; fail1 void# True -&gt; case e of _ False -&gt; fail1 void# True -&gt; case f of _ False -&gt; fail1 void# True -&gt; Baz in case a of _ False -&gt; fail void# True -&gt; case b of _ False -&gt; fail void# True -&gt; case c of _ False -&gt; fail void# True -&gt; Bar Note: you can add `{-# OPTIONS_GHC -ddump-ds #-}` To the top of a module to see how it desugars, or you can type `:set -ddump-ds` in ghci and `:load MODULE` The actual output + a few cleanup steps included below [gist](https://gist.github.com/merc1031/637269d8b669892016931f82d3245925)
No, but I've tried Idris and Agda is on my to-do list. In my Haskell code I've never used Unicode symbols outside of comments, and it seems not many in the community like Unicode code.
Ah ok great, thanks! So I guess it is the second option then. I guess that works. Do you know if there are any downsides to my backtracking guard approach?
Laziness is a lot like garbage collection. You trust the runtime to automatically do what you'd otherwise do by yourself (memory management/evaluation management) and trade slightly decreased performance and indeterminism for a better programming experience.
&gt; (15:18) No. That's a nice thing. Even if the compiler is *incorrect*, it's still the same because it's the compiler's behaviour I cared. Oh. Thanks.
You can find [style guides such as this](http://snapframework.com/docs/style-guide) which promote option 1. Personally I like having the `,` in front as they form a nicely aligned vertical line and the space after the `,` helps to have the actual payload be visually separated from the `,` syntax element. Whereas by postfixing the `,` you end up either glueing it to the preceding token (e.g. `Int,`) or having to arbitrarily insert whitespace/alignment which looks more awkward (imho :-) )
Dang, the use of rewrite rules to eliminate the proofs is smart!
Looks like I succeeded in griefing. The final version is messed up haha. I've seen a bunch of printouts that don't spell Haskell as well. Cheers 
I'm really tempted to try this out in the context of building UI widgets. There, you often want some "configuration" system (in HTML that might be tweaking CSS classes) - I wonder how this works out. Nicely done!
To be fair I think as type get advanced, inference serves you iff you know what your doing
*I* don't format data, I just use a tool that does! In my case, I just feed this to `hindent` :)
I use: data MyData = MyData { a :: Int , b :: String , c :: Bool } 
1, of course!
+1. Ya. I was using a stack project, and passing GHC options down on the command line is sorta ugly in that case.
I recently had a nasty bug, and tracked it down to `(-1) &lt;$&gt; Just 1`, which evaluates to `Just (-1)` instead of `Just 0`. This should have been a type error, but an Num instance for (-&gt;) was in scope, which let this pass. The solution was ~~obviously~~ `(subtract 1) &lt;$&gt; Just 1`. EDIT: removed the word obviously.
\#1 for product types. For sums: data MySum = ConstructorA | ConstructorB | ConstructorC
See [Haskell Report chapter 3.17.3](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-610003.17.3).
But Haskell can natively do one of the major things one used LISP macros for (i.e. delayed evaluation). In practice I'm not sure how much the rest of it is necessary. Of course there are cases but they seem rare enough that it's ok to make it inconvenient to do. Otherwise it gets used to much (for some circles, it may already be too easy!).
https://github.com/LuxLang/lux
Please don't use the word "obviously". It's not, since you made the mistake in the first place.
Conor McBride once said something sort of like, "if your type system is so stupid the compiler can write all your types for you, it's not very useful." I took this to mean that when you have an expressive type system, you really should use it to express what you intend, then write the code which proves your type is inhabited.
\#1, but with the fields less indented to leave more room for postfix haddock comments.
Very nice idea!
Edit (edited): for reliable (excellent!) results arising from discussion below see the benchmarks included in the repo https://github.com/treeowl/sort-traversable . `traversableSort` is pretty much equivalent to the library `sort`s for Data.List and Data.Sequence --------------------- Here's one I made http://sprunge.us/YAeT with these results http://sprunge.us/eVRG At first it was faster than `Data.Sequence.sort` for sequences, which was exciting, but in the end there is some asymptotic advantage emerging. The code got a little out of hand as I was following this up.
Richard Eisenberg demonstrated the technique, I think in a video. You do have to be careful to verify termination.
This is almost exactly what I do.
It was obvious in hindsight, when I saw the offending code, I knew why. But you're right, it's an easy mistake to make. The point I was making is that it shouldn't have passed the typechecker, and a Num instance for (-&gt;) is evil.
Was `Num` instance of (-&gt;) used to implement something like `2 seconds` etc etc. That is pure evil. In the rush of coding one often does not give such things much thought. I find this dangerous. 
i think option 1 is generally preferred, I don't like it, option 2 is more readable imho (coming from an imperative Java-background).
I figure if you put the "=" on the 1st line it'll look better.
doesn't matter. which style does it use?
NumInstances, which is a dependency of vector-space.
I think for record types, you're right; putting the single constructor on the same line as the `data` is more common. But for sum types, what he described is most common I think. 
How's hindent working since the unification of styles? I stopped using it a long time ago because of strange results, do you recommend trying it?
I remember seeing this hack in a blog post (cannot locate it). It is to write things like `2 minutes`. For example say you have a type Secs that captures time in seconds but you want to provide ways to say things like `2 minutes`. Then define `minutes = Secs 60` and give a Num instance for `Secs -&gt; Secs` where the number `n` denotes the function `(Secs n*)`. 
It's not perfect, but I can deal with the odd bit of weirdness. It's good enough almost all of the time. The only thing I wish it could be taught to do is preserve newlines within monadic `do` blocks.
That is clearly not a fault on your part. The Num instance (-&gt;) I think is pure evil. Particularly when done at that generality. I am guessing that the author did something like instance Num n =&gt; Num (n -&gt; n) where fromInteger x = (*) $ fromInteger x 
The most sensible instance is instance Num n =&gt; Num (a -&gt; n) where fromIntegral x = const (fromIntegral x) ...
I do not get this. Why is this even required. You mean 2 will denote the function that gives 2 ?
Yes the instance is similar to the reader monad. For example `f * g = \x -&gt; f x * g x`. In actual math we use this notation all the time. Sometimes I wish it was included in base, but I do recognise that it may lead to some hard to track down bugs. It is just so neat and concise. :/ 
[removed]
Sounds like you wanted the equivalent of -XOverloadedStrings for integer literals so you could say `(42 :: BLOCKS)`, but unfortunately `fromInteger`'s `Num` class comes with a multiplicative algebraic structure which makes only sense for dimensionless quantities.
Last evening I took some time to browse the modules of *profunctors* that I had never looked at; after I was done, I went back to your post to compare our notes. A pleasant discovery was that there actually is an `arr` equivalent in there. It is very well hidden, though! *profunctors* [calls it `eta`](https://hackage.haskell.org/package/profunctors-5.2/docs/Data-Profunctor-Composition.html#v:eta), and it lives in `Data.Profunctor.Composition`. Another thing which surprised me is that your `Const` profunctor doesn't seem to be available in that form, neither in *profunctors* nor anywhere else (there used to be [a `Const2` in *category-extras*](https://hackage.haskell.org/package/category-extras-0.53.5/docs/Control-Functor-Combinators-Const.html#t:Const2), but that is in the remote past). The closest we can get with the *profunctors* machinery is `Cayley (Const m)` (where `Cayley` is the `AppArr` you mention in the notes), which is not convenient enough to be used directly -- specially because, without extra annotations, this... getConst . runCayley $ Cayley (Const "foo") &lt;&lt;&lt; Cayley (Const "bar") ... will make the typechecker complain about ambiguity of the profunctor type variable we are trying to ignore.
I was planning on putting all this stuff into a package (I've been putting this off for a while). But I'm not sure what to call it =P
*free-arrows*, perhaps? No one seems to have claimed that piece of real estate yet.
The Qt stuff is probably inescapably C/C++, and so I assume will have to be shoved in an IO monad in some way (using inline-C/C++). The timeline is flexible to some degree. I assumed since much of the rewriting is wrapping C/C++ it wouldn't take as long as if it would were a pure rewrite from scratch. But I might be way off and it will ultimately depend on reality :) If you're interested PM me and we can talk.
Yea that's probably true. Then I think the rest seems fine for a `free-arrows` package
I would generally prefer for this sort of thing to wait until after the binary distributions themselves are announced. While I do upload a few of the binary distributions alongside the source distributions, all of these tarballs are intended for the binary builders and are subject to change until the final announcement which will take place on Sunday at the latest.
This is because the binaries have yet to be built. This is a candidate rc1, not even rc1. The email which was circulated only to GHC developers was for the purpose of getting binary maintainers to try and build the binaries for the different platforms. Once this is done and the issues are solved then would rc1 binaries be provided. As it stands this public release was premature and is using a version of the compiler that will likely soon be superseded. It's equivalent to providing just a random build of the tree.
Ok, I'll delete the post and wait until the binaries are released.
That uses `whnf`. Since `Data.List.sort` is incremental, that means its clock stops after O(n) time! If you use `nf`, things look different. Also marking stuff `INLINABLE` as /u/ElvishJerricco suggests, and making the `Sort` constructor strict in its `Heap` are very helpful. With these changes, I'm getting similar times for `Data.List.sort` and `sortTraversable`, and I'm seeing `sortTraversable` consistently beat `Data.Sequence.sort`. But the sort to beat in `Data.Sequence` is really `unstableSort`, which I haven't tested against.
Haha, no idea.
Yes, it's equivalent to asking for the head of the sorted sequence, at least as I was thinking. For a sort this means the input must be completely consumed (as I was thinking). Here are the results for complete evaluation which seem the same for the `Data.Sequence`operations http://sprunge.us/iIGC (Data.List.sort takes a hit) I did mark the recursive functions in the modules as `inlinable` and long non-recursive functions `inline` thus: [HSTrav.hs](http://sprunge.us/bSeb) , [HVec.hs](http://sprunge.us/ATKP) `unstableSort` is supposed to be better for partly sorted material, but I was using random ints which are not likely to give it any help.
Sum types should not have record accessors.
Ooooh. I thought you meant **T**otal **C**ost of **O**wnership.
Gratuitous punning reflects poorly upon you. Tsk tsk.
 instance Num Block where fromInteger n= n/blocksize +1 ... This consider bytes well. If the programmer ever think in terms of bytes when he writes a number in the context of block aritmethic
Where are you getting $2000 from? All we know about the fee is "the budget measured in 4 digits", isn't it? That means &gt;= $1000, i.e. at least $83 per day. Equally it could be $830 per day, but you think they would have made that explicit if so.
Very cool! I noticed that if you erase the size annotations, you end up with an applicative functor that looks isomorphic to a product of `Writer (Heap a)` and `State (Heap a) r`, the first of which builds up the heap by merging, and the second breaking it down by repeatedly extracting the minimum element. Running heapsort then amounts to feeding the output of the first into the input of the second.
As for why the proof has to exist at runtime, you can think of it in terms of the curry-howard correspondence: An inhabitant of type `a` is a witness to the "truth" of `a`. So, *proving* `a` is the same as *writing* a function of type `a`. For example, if I wanted to prove that (P AND Q =&gt; P) (If both P and Q are true, then P is true), or `(a, b) -&gt; a`, is logically true, I would have to write a function of type `(a, b) -&gt; a`. *The fact that the function is possible* IS the proof. project :: (a, b) -&gt; a project (x, _) = x Our types show what we *want* to prove (the "proposition"), and the *implementation* (which works on values, at runtime) is the *witness* that what we want to prove is actually true. Once we have actually *written* the function, of course, we never actually have to "run" it. If we want to prove that `P and Q =&gt; P`, then we don't actually have to ever use the function `project`....just the fact that we *wrote* it is the proof itself. `P and Q =&gt; P` is the proposition, and the implementation is the proof. So, we have `plusZero :: Natty m -&gt; (m + 'Z) :~: m`, which is a statement that, for all `m`, `m + 'Z = m`. But, is this true? We can write the type signature, but is it true? We can write any type signature, after all. We can even write type signatures for "untrue things". So, no, just writing the type signature itself isn't a proof of the type's truth. You can't just write `(a, b) -&gt; c` and take it as truth. The type signature `Natty m -&gt; (m + 'Z) :~: m` is the *proposition*. The *implementation* is the proof. ------- For the implementation itself, we have some nice tools. The basic mechanism is the GADT: data (:~:) :: k -&gt; k -&gt; Type where Refl :: a :~: a You can only *use* `Refl` to create a value of type `a :~: b` when `a` and `b` are the same. So, pattern matching on the case statement brings that fact into scope. For example, I can have a value of type `a :~: b`, and I'm not sure what `a` and `b` are, or if they are equal or not. Pattern matching on `Refl` means that, yes, the context in which that `a :~: b` was created is one where `a` and `b` are equal. (In Haskell, it's actually a bit different even than in terminating languages. Because it's actually possible in Haskell to construct values of type `Int :~: Bool` -- just use `undefined`. So pattern matching on `Refl` also ensures that the `Int :~: Bool` was "actually created" using `Refl`, instead of `let x = x in x` or something silly.) Remember that you can also only *use* `Refl` to *create* an `a :~: b` in the case where GHC knows that `a` and `b` are equal. So something like: proveSame :: [a] -&gt; [b] -&gt; (a :~: b) proveSame _ _ = Refl Doesn't work. You have to do work to bring to light the fact that `a` and `b` are the same before GHC allows you to use `Refl. And that work is what the proofs are. For example, proving `n + 0 = n`: plusZero :: Natty m -&gt; (m + 'Z) :~: m plusZero Zy = ??? In the first case, `Zy :: Natty 'Z`, so the fact that we matched on `Zy` means that `m` is `'Z`. Now we can evaluate the type family `+`: type family (+) m n where 'Z + n = n We know that if the LHS of `+` is `'Z`, then `'Z + n = n`. So GHC knows that `m + 'Z = m`, just because it was able to apply the type family definition to get `'Z + 'Z = 'Z` =&gt; `'Z = 'Z`. So, because `m = m` is in scope, you can use `Refl`: plusZero :: Natty m -&gt; (m + 'Z) :~: m plusZero Zy = Refl The second case is not as simple: plusZero :: Natty m -&gt; (m + 'Z) :~: m plusZero Zy = Refl plusZero (Sy n) = ??? This is becuase `Sy (n :: Natty n) :: Natty (S n)`. So, is it true that `'S n + 'Z = 'Z`? Directly applying the type family, we get: 'S n + 'Z = 'S (n + Z) So `m + 'Z = m`, when applied, is: m + 'Z = m ('S n) + 'Z = 'S n 'S (n + 'Z) = 'S n -- application of the type family Now, GHC will *only* let us use `Refl` if `'S (n + 'Z) = 'S n`. GHC assumes that constructors (like `'S`) are injective, so this is the same as saying that GHC will only let us use `Refl` if `n + 'Z = n`. So, until we know that `n + 'Z = n`, we can't use `Refl` to prove that `m + 'Z = m`. But, we can bring this equality into scope using `plusZero`! plusZero :: Natty m -&gt; (m + 'Z) :~: m plusZero Zy = Refl plusZero (Sy (n :: Natty n)) = case plusZero (n :: Natty n) of (Refl :: n + 'Z :~: n) -&gt; -- in here, 'n + 'Z = 'n is in scope So, `plusZero n` brings `n + 'Z = n` into scope, so GHC knows that `'S (n + 'Z) = 'S n`, so GHC knows that `'S n + 'Z = 'S n`, so GHC knows that `m + 'Z = m`. But, it wouldn't have known that *unless* we explicitly brought `n + 'Z = n` into scope, because that's not an immediate conclusion from applying the type family, or popping on/off a constructor. 
That is very good intuition, but it's not quite `Writer (Heap a)`, because it can't produce a result. It's actually a different standard `Applicative`, `Const (Heap a)`. So Sort a r ~= Product (State (Heap a)) (Const (Heap a)) r which automatically provides the otherwise messy proof that the `Sort a` `Applicative` instance is valid. Thanks!
I think /u/mstksg covered a lot of the key concepts, but I figure I should mention a thing or two. For several reasons, Haskell's type system is inconsistent. The most important reason is that it's possible to write non-terminating programs, but there are apparently other reasons too. In order to maintain type safety in the face of inconsistency, GHC has to greatly restrict the sort of evidence it "believes" and the circumstances under which it believes it. Basically, GHC only believes evidence of equality between two types, and only believes it under a pattern match on a GADT witnessing the fact. Suppose you write a proof leftDistributive :: Natty m -&gt; Natty n -&gt; Natty o -&gt; (m :* (n + o)) :~: ((m :* n) + (m :* o)) GHC will not believe it. But if you actually apply it to three arguments and pattern match on the result, then (under the pattern match) GHC will believe the equality. So the main thing the rewrite rules do is to assert that if GHC were to run the proof, it would successfully produce equality evidence. This is playing a bit fast and loose: if we pass the proof undefined or insufficiently defined values, it *won't* produce evidence. So we have to be careful to rely on the proof only when we either know that the values are sufficiently defined or are going to be forcing those values before the validity of the proof becomes a type safety problem. Indeed, undefined values even allow us to find counterexamples to the stated theorems. For example, we could write type family AnyNat :: Nat where {} oops :: Natty AnyNat oops = undefined This breaks all sorts of rules. For example, `'S 'Z + AnyNat = 'S AnyNat`, but `AnyNat + 'S 'Z` does not equal `'S AnyNat`. Commutativity is violated! In a language like Coq, Agda, or Idris, the type checker can (try to) erase proofs that aren't needed at run-time. In Haskell we have to do it manually and a bit dangerously.
Since `Data.List.sort` is specially optimized for ascending and descending runs (it's O(n) for a descending list), that doesn't really look like a very fair comparison on its own. Of course, it's a perfectly good *piece* of a fair comparison.
Ah, that makes sense. I don't know if there's ever a good reason for a type like this, but if there was, would this be reasonable? data Foo a = Bar { baz :: a } | Bif { baz :: a } 
I've often come across `These` and never had a use for it. Reading this blog post must have set some gears turning in the back of my mind because when I woke up this morning I realised I could use `These` to nicely solve a little problem I've been stuck on! --- **The problem:** I have a simple representation of expressions, which may have free variables. I'd like to compare expressions, but the free variables are essentially namespaced to an expression: `x` in one expression is in no way related to `x` in another (they may not even have the same type). So in order to think thoughts like "when the `x` in the first expression is equal to the `y` in the second expression, they have the same result regardless of the values of the other variables", I need to be able to project the expressions into a common namespace. `These` gives a nice way to express these projections: - `This v` means a variable called `v` in the first expression, which is distinct from all variables in the second. - `That w` means a variable called `w` in the second expression, which is distinct from all variables in the first. - `These v w` means that the variable `v` in the first expression gets identified with the variable `w` in the second. So a projection is a list `[(These Name Name, Type)]`, with one entry for each variable across both terms. We can produce all projections, ordered from most general (most `This` and `That` values) to least general (most `These` values): projections :: [(Name, Type)] -&gt; [(Name, Type)] -&gt; [[(These Name Name, Type)]] projections t1 [] = [[(This v, ty) | (v, ty) &lt;- t1]] projections [] t2 = [[(That v, ty) | (v, ty) &lt;- t2]] projections ((vL, tyL):t1) t2 = map ((This vL, tyL) :) (projections t1 t2) ++ concat [map ((These vL vR, tyL) :) (projections t1 (filter (/=x) t2)) | x@(vR, tyR) &lt;- t2, tyL == tyR] Given a projection we can produce a consistent renaming of variables in both terms, mapping them into a common namespace: renaming :: (Type -&gt; Name) -&gt; [(These Name Name, Type)] -&gt; ([(Name, Name)], [(Name, Name)]) renaming varf = go [] ([], []) where go e x ((these, ty):rest) = let name = varf ty in rename e x name (maybe 0 (+1) $ lookup name e) these rest go e x [] = x rename e ~(l, r) name n = let name' = if n == 0 then name else name ++ show n in \case This vL -&gt; go ((name, n):e) ((vL, name'):l, r) That vR -&gt; go ((name, n):e) (l, (vR, name'):r) These vL vR -&gt; go ((name, n):e) ((vL, name'):l, (vR, name'):r) My only regret is that I found no use for the fancy functions in the `these` package, only the type itself.
I don't know javascript, but I tried this in an online interpreter: const greet = name =&gt; name ? 'Hello ' + name : null const exclaim = s =&gt; s + "!" With these defined, `greet(0)` raises a `TypeError`, but `exclaim(greet(0))` returns `"null!"`. Since the result of composing `greet` and `exclaim` is different to the result of applying `greet`, collecting the output, then applying `exclaim` to that output, I'd say that `greet` is an impure function. (A key property of pure functions is that they compose in a simple way.) I now see that returning `null` and raising exceptions are indeed different in Javascript. However, the above represents inconsistent and unpredictable behavior in the language.
Factis research gmbh
[There you go.](https://www.reddit.com/place?webview=true#x=273&amp;y=566)
I cabalized a version. The readme has the results I just got. It's quite possible I'm making some dumb mistake as I was with the small lists. See what you think. Edit: benchmarks are included in https://github.com/treeowl/sort-traversable now that it exists
So that you can write `f + g` for `\x -&gt; f(x) + g(x)`, just like in our mother tongue J.
I have to admit, I don’t quite understand what the advantage of waiting is. It seems like detecting issues as early as possible is a good thing? I guess I’m missing something here. Anyway, thanks to everyone who works on GHC, I’m exited about playing with the new release.
I would personally argue that, in practice, a lot of that stuff would be better not done at all. I think that in many of those cases, there would have been superior non-macro solutions possible.
I'm not sure what you mean by "subtracting from the right", but my point was more about the material effects of when various variables take known values vs when they remain unknown, rather than being about aesthetics. That is, because of the way `(+)` is defined, if we know the constructor of the left argument then the `(+)` can reduce, whereas if the left argument is unknown then the `(+)` remains stuck. This is why proving `('Z + n) ~ n` is free whereas proving `(n + 'Z) ~ n` requires an explicit proof by induction on `n` (which in Haskell has to be materialized as term-level induction on the value of type `Natty n`). On first glance at the code, my intuition was that the arguments to the use of `(+)` in the definition of the `Sort` constructor wasn't ideal. But then I haven't read the references so I was guessing at what `m` and `n` actually mean (i.e., what the stored `Heap m a` represents vs what the `Heap (m + n) a` passed to the continuation represents). After poking around with it a bit more, I think the `(m + n)` order actually is the cleaner one. Of course, for things like this it's always a tradeoff between the proofs you see and the ones you don't; which is why I was curious whether the argument order was intentionally chosen, vs if you just happened to pick the cleaner one
I like the idea of Haskell programs as glass palaces. It's difficult to lose your way on a glass building, even if you don't have a guide, because you can always catch a glimpse of the overall structure. Also, I suspect tearing down and rebuilding sections of a glass palace is easier than with a brick-and-mortar one, provided you have enough lenses and prisms at hand.
It also has a string type built in, and optional laziness. Honestly *if* Idris continues to improve, I could see it displacing Haskell usage. I do love Haskell but there is still a lot of pain to using it that I hope a newer language such as Idris can alleviate. Time will tell. 
Schmidhuber once did a [search in program space](http://people.idsia.ch/~juergen/oops/sld015.htm). He used forth though. I wondered how this could turn out on a more structured language.
You're correct, but it took me all of 30 seconds to find both [the example](https://github.com/ThoughtWorksInc/DeepDarkFantasy/blob/master/DDF/Poly.lhs) and [the blog](http://marisa.moe/2017/DLPL/).
I realize this is off topic, but the work that tweag.io has been doing is simply amazing (don't forget about your work on Cloud Haskell). For a small team, every single contribution you guys make is huge. I know of no other software company that has such high quality contributions in so many areas while being this small. Please keep it up! Also, do you have any plans of eventually opening an office in North America? 
I read a lot about spock, snap, happstack and yesod before I started a real project. I chose yesod because its very good book and all the batteries included. Since then I use yesod exclusively. All problems could be solved with the help of the yesod google group. Sure there are some harder topics like monadic forms or writing your own input fields, but you won't need these in the beginning. So thanks for the yesod team for keeping up this nice and polished framework.
It`s time to change that perception. Haskell can be made easy
Related docs: [1](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#hascallstack) [2](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#stack-traces-in-ghci) [3](https://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-Stack.html) So, with no experience on this, I think I have some relevant questions: What flags are you using during compiling? And did you try it on ghci? Are you on windows?
I've read those three documents. &gt; What flags are you using during compiling? Flags are `-Wall -threaded -O2` . &gt; And did you try it on ghci? Nope, only on the compiled binary. To reproduce this I need to have four instances of this multi-threaded program running at once on a 16 core machine for about an hour before I see the problem. &gt; Are you on windows? Hell no! :)
Then, for you as well as for most haskellers the reason why Haskell has not taken the industry by storm is because non haskellers are stupid. right? Since the responses will include obviously, an elaborated list of excuses, I say to you that please don't waste your time. I will not believe you. This pseudo-reasoning (Haskell is the best for programming, Haskell is used by a minority, I'm in that minority, therefore I'm the best and the others are stupid) is the reason why many mediocre programmers become haskellers and produce piles of shit in haskell and invest a inordinate effort into learning CT and other abstractions: because they have the urgent need to demonstrate others and themselves that are intelligent enough. But no matter the number of lenses and abstractions that they pile to do useless container massaging, they are mediocre programmers and mediocre haskellers. The reason why Haskell has not taken industry is just the opposite: because the haskell community is full of mediocre programmers who think that they are brilliant. I will never say this enough times. 
It's just so many people simply see Haskell, 'well, that's quite a lot of mirrors, maybe I should put my mirror on it?' and make things worse. (Edit: I moved the rest of my comment out to [this](https://www.reddit.com/r/haskell/comments/63k5t8/the_pretentious_haskell_phase/dfuxr0x/))
Well I don't think OP and the Github author are the same person, and it looks like the repo is under heavy development.
Currently CallStacks are not attached to exceptions (I think there is a discussion going on there). But since you already hacked up the `network` package you could hack up all those `throwSocketError*` functions to take a `CallStack` and print it in case an exception occurs. Edit: Here are the links for the discussion: https://ghc.haskell.org/trac/ghc/ticket/12096 https://ghc.haskell.org/trac/ghc/ticket/13372
Try it while compiling with profiling enabled (or running ghci with `-prof`)
Author here. Thank you.
Fixed. Please note that I have not refactor the code yet, so https://github.com/ThoughtWorksInc/DeepDarkFantasy/blob/master/src/Xor.lhs seems very complex to do such a trivial test. Once I move all the common infrastructure out it will look much nicer.
Author here. Sorry for the confusion, but it is the right thing. It was originally written in scala, hence the examples are in scala. I will redo it soon.
Try compiling it with `-rtsopts -fprof-auto -fprof-caf` and running it with `+RTS -xc`, this should print a stack trace whenever an exception is thrown, as explained on https://wiki.haskell.org/Debugging
Try harder to hurt people with truth *not* words next time.
Mirrors attract trolls as well, it would seem.
Oh! That would be annoying. Or maybe it would force me to write simpler monadic code.
Thank you
https://www.youtube.com/watch?v=Qiw3vVy_eN8
I was just ball-parking it with the "4 digits" comment. Assume I mean ~$5K but not higher than $9,999. Many people have responded to me with comments like "I can do this for this much, and the deadline has to be extended to this date". Fair enough. I was just trying to start the conversation :)
I don't think I see the point here. Is it that people get pretentious and make Haskell code that's cool but not practical? Because that's true in any language, and isn't really indicative of how good Haskell is in production.
The last edit on that page is january 2016, GHC 8.0.1 was released in may, is this the same mechanism of HasStackTrace? edit: well the [reddit post linked on the page](https://www.reddit.com/r/haskell/comments/2fwwx3/how_to_get_approx_stack_traces_with_profiled/) uses a deprecated function from 7.8, which could be good enough for OP if the flags don't work.
&gt; Haskell is a beautiful crystalline palace. Oh look, he thinks he knows Haskell
I think the point is that upfront time investment with regards to normal tasks you've done plenty before in other languages is huge, compared to something like ruby, python, javascript or even new languages like clojure and clojure-script. There's also "excuse our dust, we are under construction" phase which in haskell ecosystem goes on for years. Just look at how much work it is to simply setup reflex-platform. And the confusion between Text and String, and pains of record syntax. All of this combined together leaves the impression of wasted time. Especially in web development it is true that you can build something much quicker with plain reactjs than with either reflex or purescript. Of course the advantages of strong static typing and null handling payoff much later in the game, so they are not apparent to people who just try things to see how the effort level compares to similar tasks they've done before. Then they conclude that if someone still works with haskell after some time, then they must be pretentious FP purists who never get anything done. 
urge to write `The pretentious Ruby phase` rising
-1 It's a short read that has no meat to it. Just a vague analogy without concrete explanation. I was hoping it would at least dig into some details.
&gt; Stop being racist to try to combat imaginary racism. How is "you should be welcoming and polite to people ... even if they're not as technically adept as you" racist? This is a part of the Rust CoC? The Rust CoC also says you can't treat someone poorly just because they're straight. Is that racist? Incidentally the Rust CoC also says you can't treat someone differently for being gay either. Now the idea that someone should be treated in a polite and friendly manner regardless of their race, gender, appearance or ability is a belief, that is true. It is also the rule by which most kindergartens operate. Surely if a four-year-old can manage it, so can we.
Maybe. I think there are a lot of good reasons for TH (e.g. lens, recursion-schemes). I also think you're right that it's best to have a language that doesn't *need* a ton of macros, but there are definitely reasons it's necessary. And it makes the language totally extensible which is also very nice. 
Eta v0.1.0 milestone is set for April 30th. https://twitter.com/eta_lang/status/842987827373768704
&gt; Prescribing a code of conduct to a programming language just seems like virtue signalling. In a sense virtue-signalling is a good thing. It means that people know they're guaranteed to be treated well by a community, and may make them more willing to participate. Rust's for example explicitly says people who aren't very technically adept should still be welcomed and treated politely. This lowers the bar quite a bit compared to the LKML, where mistakes are brutally criticised. Note this doesn't mean poor work is accepted, it means good efforts are coached along. For a language like Haskell, where the potential Haskell developers vastly outnumber the actual Haskell developers, this seems like a good thing. &gt; How would you enforce it? Ban someone's access to the technology? Ban their open source contributions? Maybe just forcibly take ownership of all their projects? Ban them from the mailing list, and from making contributions. The latter is hard, as sometimes the people who most fall foul of this stuff are great developers. But given that even great developers have finite capacity, it's better to ban them and gain a dozen they deterred, than retain them and lose contributors. It's not feasible or necessary to forcibly take ownership of a project. &gt; Some people just don't get along. And that's okay, as long as they continue to contribute to the Haskell ecosystem But what if one person's bad social habits encourage many developers to leave Haskell. Between F-Sharp, F-Star, PureScript, Idris, Elm and Rust, there are a lot of alternatives. &gt; I am a proponent of meritocracy in tech As am I, but not myopically. It's only good enough to be a good coder if you are the only one working on a project. In a multiparty project it's necessary that everyone be good and productive coders, and that means each should not do anything that impedes others' work, or deters them from working at all. In multiparty project it's not just programming merit that matters, social-merit (politeness, level-headedness) matters too. &gt; I think the code of conduct movement directly opposes this No it doesn't: merit is still rewarded, but not merely merit in coding alone. One must be a good coder, and a good team-worker, to work in a team. &gt; I've never seen a code of conduct that I completely agree with It would be good if you could (1) provide a particular example of a code of conduct that you didn't like (2) explain what in particular affects you and (3) describe someone who was unfairly excluded as a result. &gt; I feel like the code of conduct movement encourages authoritarianism Haskell is an authoritarian project: we don't all have commit privileges. Codes of conduct protect users from authoritarianism, by placing limits on its abuse. Commit privileges don't give you the right to abuse people (LKML is a good example again). &gt; The code of conduct movement seems to value diversity over ability In nowhere has this been the case. The code of conduct movement is a constrained maximisation problem: developers excel by maximising their technical output for a fixed level of politeness. Developers that are unable to meet that politeness constraint fail. But like I said, in a multiparty project, how your affect others' work is at least as important as the work you do yourself. 
&gt; look up the definition of racism ... I need special treatment to participate in tech because I am a minority, so I am used to being "harmed" by people arguing about Foldable instances. Minorities are not limited to races: gay people are a minority in general; women are a minority in tech. No single code of conduct that I know of demands people be given special treatment _because_ of their race. Most demand people be given equal treatment _in spite of_ their race. That's the _opposite_ of racism. As such it protects white people from being criticised for being white just as much as asian people being criticised for being asian. &gt; we need to single out minorities and make them feel uncomfortable so we can feel better about our misplaced white guilt As I mentioned, must codes of conduct consider a half dozen classes of minority (gays, women, newbies, religious, atheist, etc), so it's not about white guilt. It's all about setting rules so that people know what they can expect if they join the Haskell club. Saying you'll be treated nicely no matter how you look, who you date, what you worship, and how clever you are (or aren't), is just being nice to folks.
I'm really torn about this. On the one hand, having super complex and involved generic types can eliminate the need for Template Haskell in a lot of places. On the other hand, Template Haskell can be used to generate monomorphic variants of those types. I've found that the TH versions have much better error messages and are a lot easier to use, and typically have less boilerplate.
Are you able to share info what kind of compositor it is and what use case it serves?
Sad to see this being down voted. I think it is a fine witty poke that Haskellers should welcome, and I say this as an original Haskeller who has instigated many projects over the years, including a current Haskell/Ruby team that has realised some serious tech. Haskellers really should not be so sensitive!
&gt; It seems like detecting issues as early as possible is a good thing? There are a few considerations here, * I reserve (and sometimes exercise) the right to change the binary tarballs up until the announcement. If they went into use earlier it would be ambiguous which binaries a user using "8.2.1-rc1" was actually using. This complicates issue tracking and generally runs against the principles of a "release" being a well-defined set of artifacts. * we have intentionally put this system in place in deference to binary build contributors, who dedicate their time to bring GHC to a wider range of platforms than we would otherwise enjoy. They have expressed a desire to have their builds placed on an equal footing to the builds I prepare. Granting this wish is the least we can do to recognize their efforts. Of course, I recognize that this builds in a fair amount of latency into the release process, but at the moment it's the system that we have.
Ish. You can use AWSCodebuild as a fat Lambda, and it supports docker containers. You have to go into the special config to get the 15G RAM instance.
Why so serious batman? It is totally obvious what he was driving at -- would have provoked this response otherwise. Some of the critique is definitely dated. The day that it can be posted on Reddit and Haskellers ignore it we will know it is all dated.
Something I've experimented with (with mixed results) is rather than encoding the data-type name (or a representative acronym) in the field dame, I import the data-type fields with a descriptive qualifier (although, I almost always give my qualified imports descriptive qualifiers; I cannot stand the single-or-double-letter qualifiers). This way I only need the qualification when there's ambiguity. But, I do end up with a lot more imports. So, without the `_myData_` prefix, I would use your `MyData` as import qualified SomeModule as MyData (MyData(..)) -- ... foos = map MyData.foo 
The state transformer takes a heap of size `m + n` and performs `m` `minView` operations to reduce the heap to size `n`. It therefore seemed natural to me to perform the induction on `m`, leading to that argument order. What I meant about "subtracting from the right" is that it seems easier to to talk about how many items you're going to pull off the left side than about how many you're going to leave behind on the right side. Which I think amounts to the same thing.
This may be a silly question, as I don't really understand what you're trying to do here, but why not use Data.IntMap?
Wait a few weeks. The conclusions are provisional in the sense that common sense is provisional . I'm writing a paper on that. when I finish it I will post it to Arxiv. I have to dissect some haskeellers to examine their brains, so be patient. 
So the *code* should be ugly so the diffs can be nice? That doesn't make sense to me. :) But maybe some people have workflows where they read diffs more often than code?
This is great! Let's get that extra student slot. I just sent an extra donation for 1/8 of it.
Each key represents a monomial of Grassmann variables. Since a Grassmann squares to 0 so in a given monomial its exponent is either 0 or 1. n-th bit of the key represents the exponent of n-th Grassmann variable. So using Intmap would restrict to a maximum of 64 Grassmann variables. 
My bad, sorry for that. 
**Parallel and Concurrent Programming in Haskell** (mentioned by [@Lokathor](https://www.reddit.com/user/Lokathor)) * http://chimera.labs.oreilly.com/books/1230000000929/index.html **Game programming in Haskell** (preview in [this video](https://www.youtube.com/watch?v=1MNTerD8IuI); haven't read this book but seems good and rather advanced) * https://leanpub.com/gameinhaskell **The Joy of Haskell** aims to cover some advanced Haskell topics and it seems promising to me but far from release (so keep eye on it) * https://joyofhaskell.com/ **Type Driven Development** (yes, I know, it's about *Idris* but this book is sooooo good and I really like *Idris*; such things as Dependent types, programming with effects, Linear types, etc. are going to land in *Haskell* as well and some are already supported in some way so reading this book really will leverage your programming skills) * https://www.manning.com/books/type-driven-development-with-idris **Haskell Data Analysis Cookbook** (clear from topic: about Machine Learning and Data Science in *Haskell*) * http://haskelldata.com/ **GHC User Guide** (not quite a book but still worth reading) * https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ You can find more books here: https://wiki.haskell.org/Books But personally I'm not familiar with all of them. Also friend of mine is going to write a book about advanced topics in *Haskell* and I'm going to help him but that's just an idea. After learning *Haskell* for already 4 years I've realized that reading books is probably not the best way to understand some advanced topics (especially for me, but some books are really useful). Sometimes it can be better just to dive into one topic and read everything about it that you can find. You can take this list of topics as starting point: * http://dev.stephendiehl.com/hask/
Yep, adding an explicit `prettyCallStack callStack` allowed me to find the calling function. I'm now working my way back up the call stack adding `HasCallStack` on the way.
I know it's crazy, but I've started doing this as a general default. data MyData = forall x y z. ( Show x , C y z ) =&gt; MkMyData { a :: Int , b :: String , c :: T x y z }
I think it was determined that the strain of managing the feedback while also writing new portions of the book was very high. So we will probably not have previews of this new book until it is closer to complete.
&gt; Finally, Davean has volunteered to fund a student expressly to work on the Hadrian build system for GHC Seems weird given [this exchange](https://www.reddit.com/r/haskell/comments/5wpwkq/summer_of_haskell_2017/dej6sep/?context=1). Not that I'm bothered or anything: when it comes to funding for students, more is better.
Haskell Data Analysis Cookbook is one of the Packet books I was referring to. Everything I've heard/seen suggests staying away.
I was about to send a PR with an idea to work on Hadrian, but to my surprise it's already in the list, and moreover someone (thank you, mysterious Davean!) has offered to fund it -- wow! I hope we'll find a student willing and resilient enough to help bring Hadrian closer to the point it can replace the current GHC build system. 
We have a fortress of solitude?
Man I am so happy about this - and I'm excited for user level functionality that can convince me that `tAp f g = (,) &lt;$&gt; f . fst &lt;*&gt; g . snd` is the same as `tAp f g (x,y) = (f x, g y)`
There's a lot more results from research than listed here, but don't make it into mainstream languages, which is dismaying. Almost all of them are some form of type theory. The reality is that software development is dominated by social factors, and the community never really recovered from the influx of "career programmers" during the Dot Com boom. /unpopularopinion
&gt; If I need multiple arguments, I introduce a record and then use RecordWildCards to expand it out later. Wtf. That sounds like a major downgrade IMO.
**Haskell Tutorial and Cookbook** - https://leanpub.com/haskell-cookbook (&amp; **Haskell Cookbook, Volume 2** - https://leanpub.com/haskellcookbook2 in progress)
How / when were the mentors chosen? I was considering applying as a mentor this year.
http://i.imgur.com/JjrksGc.jpg I wrote a long detailed comment and then lost it trying to post a picture. Brief version: connected set = memory location, Compact set = binary values in memory, Continuous function = move to next memory location All types utilizing binary values in memory are fibrations forming a homotopy. Think of these fibrations as parallel 2 dimensional arrays. Functors are then 2 dimensional arrays running across the homotopy we just defined. They map values between the fibrations. I think this visualization gives an intuitive understanding of functors without getting too deep into topology/type theory/set theory. Please correct or clarify anything that needs it.
Eh. If it was supposed to be funny, as you claim, then it failed. Telling people they *should* have found something clever and amusing when they did not is a fool's errand. If they didn't laugh, it wasn't funny. To be honest, though, it didn't sound to me like it was even intended that way in the first place. If it was supposed to say something of substance, it apparently also failed.
Can anyone comment on the stipend given to students? This year, GSoC's stipend has now been changed to depend on the country of the student. I can't find anything about stipend on the HSoC's website.
You could probably start tcpdump and record the traffic (tcpdump doesn't record whole packets, just the headers by default). Unless it causes some performance degradation that would cause the problem to disappear, you may see the reset packet in the dump.
Obviously you don't find it funny, but many Rubyists will laughing themselves silly -- and who could blame them.
stack supports two types of docker containers - building in containers and creation of images.
It is evident that the haskell community in general look the outside with contempt. Every other software community agree on that and the article that originates this thread is a proof of it. I can not stand when someone, like the author of the article make a self critic of his own attitude as haskeller and a critic of the haskell community and a critic of the state of the haskell ecosystem in a wonderful and sweet tone and another haskeller learn nothing from it. I see a lot of responses that demonstrate that the community as a whole react denying the most mild critic. This reaction is rooted in the same absurd superiority complex: "Haskell is beautiful, we are the best. Some wish it, we work for it" What can I do to help the community but saying bluntly the same, exposing all the implications? In this time of irreligion, everything becomes a religion and everything is taken religiously, since people are like lonely dogs in search of a herd. This makes any kind of constructive criticism impossible. The alternatives are isolation from reality or inevitable confrontation. Haskell is suffering a lot from isolation. "To be in" has become the only important thing rather that "to be for" serving the rest. I can not permit myself to allow this situation. Haskell should be a useful tool for the world, not a therapy group or self affirmation group for some kind of people. That kind of people need to look somewhere else. perhaps above them. 
This is a nice list, thank you. I'm really looking forward to Joy of Haskell now, especially if it is as good as Haskell Programming. I know reading stuff isn't as beneficial as doing stuff, but I usually can't think of anything to do. I'm going through some simple algo-challenges on CodeWars, but that's it. Do you have any recommendations about this as well?
(IANAL disclaimer) Almost all countries are eligible. The only exceptions I know of are Iran, North Korea, Syria, Sudan and Cuba -- countries that are currently embargoed by the United States. Due to the fact that Haskell.org is incorporated in the state of New York, it is not legally possible for us to sponsor students in those countries.
Mind elaborating on why? And are there any that really do have to be monads?
I wouldn't call generics a problem caused by strong typing but rather a desirable consequence of it that had to be massaged a bit in order to get it right.
I thought you would just record all traffic without mocking anything, however it occurred to me that you are interested in a place where the exception is being thrown in order to catch it, seeing a RST in a tcpdump would probably not be any helpful.
That's because it's very hard to draw a line there. If you are looking for a reference, try looking at _past_ projects and ask yourself if you could've been able to do something like that in three months (given appropriate mentorship): - https://summer.haskell.org/news/2016-12-08-2016-wrapup.html Or you can look at the current set of ideas and guesstimate if you could take those on: - https://summer.haskell.org/ideas.html In general, it's not bad to pick a slightly less ambitious project if that means there is a bigger change of completing it successfully. Feel free to contact me if you want me to try and give you some guidance based on your personal situation.
Reddit is full of people peddling the lie that Rust's CoC drove developers away. The truth is the opposite: Rust launched _with_ a CoC; the early developers that left did so mostly due to burn-out and Mozilla politics; and only one significant developer (as opposed to IRC troll) has ever been asked to leave. Graydon Hoare, who invented Rust initially, has written about this. He has specifically said [he'd written the code of conduct before he first released the Rust code to the public](https://www.reddit.com/r/rust/comments/450zy7/blog_code_of_heat_conductivity/czuixjc/) Graydon and other early contributors did leave. Graydon [has explained that he was burnt out](https://www.reddit.com/r/rust/comments/2u1dme/daniel_micay/co4uurq/). Tim Chevalier [has stated poor Mozilla management frustrated developers](http://slash-r-slash-rust.github.io/archived/2u1dme.html#co4f2wp). More recently Apple has been hoovering up Rust developers to work on Swift instead, which is a matter of income, not politics. Swift also has a CoC Only one significant developer (as opposed to casual troll) has, to my knowledge, been asked to leave because of CoC violations. Working under the name "strcat" ([reddit discussion](http://slash-r-slash-rust.github.io/archived/2u1dme.html)), he was a bit like Snoyman: excellent technically, but [too eager to call other people dishonest or shameful if they disagreed with them](http://slash-r-slash-rust.github.io/archived/2u1dme.html#co4uurq) [[2]](http://slash-r-slash-rust.github.io/archived/2u1dme.html#co4h3bv) [[3]](http://slash-r-slash-rust.github.io/archived/2u1dme.html#co4vfhe). They worked around his outbursts for several years. In the end he was burning everyone else out and so they stopped interacting with him and refused to accept or discuss pull requests to maximise the productivity of the other contributors
Thanks. I don't know how I missed that.
Why does your `mailParts` have empty string? Can't you design you `Mail` type itself to avoid those. If you cannot, you might split up functions that clean up the `Mail` data type. Also instead of doing all the lookups for subjects, Can't your Mail data type be defined as data Mail = Mail { mailSubject :: Maybe String -- other blah fields } If the form in which you get is different (say as a string from user), you might be better of writing a `parse :: String -&gt; Mail` (with enough quick checks for testing correctness). For example, if you have a render function an obvious test is `parse . render = id`. 
Yesterday I put in some time to finally figure out how to link with GNU `gold`, which is much faster than `ld`. I found the topic quite underdocumented. Thought I'd share it for others who hate waiting on `Linking dist/build/myapp/myapp ...`. Thanks to the helpful people on `#ghc`, especially rwbarton.
If you are using lenses, `over _1 (decodeUtf8With onError)` can be a good option too.
Having to introduce a record whenever you want to return multiple values is exactly the sort of thing tuples were meant to avoid. Coming from a Java background where this is your only option, is really rather avoid it. That said, I would probably agree with it in scenarios with more than a few values to return. But this is where anonymous record types would be great =P
I wouldn't skip Stack until I really knew how to take advantage of Nix. You can use Cabal just fine without either of them, but it's the worst of all worlds. That said, I *would* highly recommend Nix over Docker. I think it's a huge improvement.
I don't really like tuples, because they lose the association of what a value means - I want to name them! If anything, I would prefer `labels`.
So... what's the catch?
You say that, but I didn't come up with this guideline out of thin air ;)
Indeed, I think the pattern we use in the SDL2 library is the way to go: https://hackage.haskell.org/package/sdl2-2.2.0/docs/SDL-Event.html#t:EventPayload
I checked the api of HashMap from unordered-containers and couldn't find an equivalent of mapKeys for Map. I am using Map mostly because keys are unique and I don't need logarithmic indexing. Is there a better data structure which has an equivalent of unionWith but has better asymptotics for operations on keys?
An interesting aspect of the post and your reply is that the post title mentions "Results", but a lot of it and your answer is about "Concepts". It's amusing to think about which of those concepts can be presented as "Results" (I'm not sure about "Modularity", for example), or which can be attached to a particular result that is also of interest. I would say that generational garbage collection is a result (the result being that it is essential to GC performance in practice), type soundness has also been presented as a shape-changing result ("well-typed programs don't go wrong", etc.). For polymorphism, one could claim that the result is Girard's strong normalization proof for System F; admittedly, this is not what most people think about when they think "polymorphism is great", but it also had a large impact on PL research in the way it showed how Tait's accessibility method scaled to richer settings (later we had step-indexing, etc.; how to make logical relations do useful thing is a workhorse of PL research). For DFA/NFA it's not completely clear to me which result I would choose. Not determinization. Maybe decidability of emptyness checking. I think I am most excited about minimization and equivalence checking, which predates a lot of the following work on (bi)simulations etc. Something about them that is 70% a result and 30% a concept and had major influence on PL research is "the algebraic, denotational and operational semantics coincide so nicely".
Anything like this for Windows?
ok, what if all constructors have records?
We are in violent agreement
Yes, it's possible for a consistent system to express productivity like that. In formal settings, that seems to go by the name "coinduction". I don't know enough about type theory to go into any details, but I know that proofy languages like Coq, Agda, and even Idris carefully distinguish inductive types from coinductive ones, and restrict what you can and can't do with each.
In the sense I was talking about when I posted that, many things "need" to be monads. (In another sense, nothing at all needs to have a Monad instance, but that's not the sense we're discussing here.) There are actually very few things that have the same expressiveness via Applicative as the do via Monad. Reader (or trivial variations on it) is the only example I can think of offhand that has the same power at Applicative and Monad. In a hand-wavey sense, this happens because Reader doesn't encapsulate any effect. Reader consists only of a read-only environment. Nothing you can do with a Reader action have any effects that are visible to any other Reader action it is composed with. This means that the difference between Applicative and Monad (that Monad lets you control what effects are used where Applicative does not) is irrelevant. Nothing using Reader has effects anyway, For a more formal proof, you would need to write `(&gt;&gt;=)` or `join` in terms of `pure` and `(&lt;*&gt;)`. The type system will prevent that with Reader itself, but that's only a temporary setback. That's because `Reader r a` is defined as `r -&gt; a`, and they actually have the exact same Functor/Applicative/Monad instances. Without that newtype wrapping in the way, you can actually write those functions in terms of each other. I'm not going to go into the details because I'm answering from my phone, and never learned enough SK calculus to know the result offhand. But if you want a hint.. `pure` is K, `(&lt;*&gt;) ` is S, and it's probably easier to use them to implement `join :: (a -&gt; a - &gt; b) -&gt; (a -&gt; b)` with them. 
*Woah*, that’s a cool feature list – especially the renaming and inlining/extraction functions. Make sure to grab 0.6 from Github – Stackage 8 has only version 0.5, which is unusably broken. Edit: 0.6 as well, all I get is one message a second telling me the server has crashed. :-(
No container can offer you a "mapKeys" cheaper than converting back and forth toList and applying the map. mapKeys f = HashMap.fromList . map (first f) . HashMap.toList
Yet Another Haskell Tool (
I don’t think more tooling – especially tooling that offers some of the most basic of IDE features – is something to frown upon.
Windows doesn't run ELF binaries, which is the only thing `gold` produces. 
~~150~~
Not as helpful for Hakyll as I was hoping... Compiling my Hakyll config goes from 29s to 23s. But I seem to have gold installed by default anyway, so might as well!
~~150 seconds~~ ^Does ^this ^form ^a ^group?
I will update that stackoverflow once I have figured out how to use LLD. If you happen to know it, please let me know here.
&gt; A commenter said the following, but unfortunately didn't expand on his comment: &gt; &gt; Tail recursion isn't necessary or sufficient for haskell code to run in constant space. &gt; I'd be interested in reading more about how this works. Tail call recursion/optimization removes stack space usage. If we allocate new HEAP objects in each recursion step without freeing them we can still leak memory. For a pretty silly example see the function below, while it's tail recursive it still takes up linear space since every new recursion step will have a reference to the list elements allocated from all calls before. f :: [Int] -&gt; Int f (0:xs) = 0 f l@(x:xs) = f ((x-1):l) Edit: There is more detail on the Details [here](https://wiki.haskell.org/Tail_recursion), but to really understand the details one has to dive somewhat into GHC's execution Model.
If you are only using `ghc-options`, and not also `ld-options` as described in my answer, you might still be accidentally using standard `ld` in the final executable linking step. So there might be even more speedup to gain for you. I have not yet looked into whether stack is smarter than `Setup.hs` here and does this automatically. **Edit:** No, stack just uses the Cabal library for that, so you are probably out on big improvement that is final executable linking time (same for tests and benchmarks of course). I recommend you to do a run with `strace -f -e execve -s 10000 stack build ... | grep execc 2&gt;&amp;1 &gt; strace.log` into your Travis artifact dir to check it.
I have added a note to the answer to address your concern.
Terminological quibble -- fix is sound, as it doesn't cause a type error at runtime. That said, a typical sort of type based criterion one might put on fix would be to add a "measure" function on `a` that yielded a natural number and require a proof passed in that for all `a`, the `a -&gt; a` function yielded a strict decrease in the measure.
Fair point, I'd be happy to see an exploration. I don't know of a good packaging system for Windows or how to get `gold` installed but that's probably the easiest problem.
Why not make gold the default and required-by-default linker on Linux?
It's not just how the diffs look, the commas are a source of unnecessary automatic merge conflicts too. In some workflows, it's much nicer when merges just work.
Ha! That's great to see. Thanks!
If you read the links I've provided you'll see the Mozilla politics mentioned were actually to do with poor management: e.g. strcat's interview was delayed, moved around, and then happened with people forgetting to reimburse him for his transport. As an organisation, Mozilla would anyway have been forced by employment law to ensure people were treated equally regardless of race, gender etc and protected from bullying, so the Rust project's code of conduct would have had no internal effect. The funny thing about codes of conduct like Rust's is that largely they just extend the fairness rules of employment law into the online volunteer space. This is why I'm always a little surprised when people claim they're peculiarly burdensome. As for git history, I don't know what metric you're referring to (commit count, diff sizes, pull requests), what time period you're considering, and which projects you're referring to. A lot of once core Rust features are now in external libraries, so the workload is further spread (e.g. green threading moved from Rust core to mio &amp; tokio). Also mature projects see less flux. What I can say is Rust churns out a release every six weeks, and is adding features at a steady pace such as Macros 1.1, better error messages, and lots of library improvements including regex, async (tokio) and rustc-serialise. 
-optl-fuse-ld=lld obviously. (If your default c compiler is gcc, it might not know about lld, so you'd need to tell ghc to use clang to link. On FreeBSD, cc is clang, so it just worked for me.)
Awesome! Haskell needs this
Your detailed reply is much appreciated. If the answer is that Monad let's you control what effects are used, whereas Applicative doesn't, I guess that's a pretty straightforward answer and I will try to get an intuition for why. I feel like I've bugged you enough but if I could trouble you for an example it would be super useful.
Here's a bit more informal discussion of that topic: https://stackoverflow.com/questions/17409260/what-advantage-does-monad-give-us-over-an-applicative/17412969#17412969
I've never heard of the book before and did some digging, it seems rather that it was finished but became out of date: https://twitter.com/elise_huard/status/844197018373423105 I guess game development in Haskell is not an especially big niche and the ecosystem is not terribly stable. It's rather sad that frequently many of the often recommended and seemingly promising libraries get abandoned after an initial push. I sometimes find myself thinking 'whatever happened to XYZ', only to see that there's only minor maintenance releases on Hackage for the last few years.
But the homepage from op links to https://github.com/haskell-tools/haskell-tools ...
LLD is probably the way forward for faster linking on Windows.
Can't say what is more common. My personal style is to use same alignment for records and sum types (with exception to single constructor datatypes). data MyData = SingleConstructor Foo Bar deriving (Eq, Ord, Show) data MyData = Case1 A | Case2 B deriving (Eq, Ord, Show) data MyData = MyData { a :: X , b :: Y } deriving (Eq, Ord, Show) data MyData = MyData { a :: X , b :: Y } | MyDataCase2 | MyDataCase3 { c :: Y , d :: Z } deriving (Eq, Ord, Show) And for records I usually prefix field names with abbreviated data type name: data MyDataName = MyDataName { mdnX :: X , mdnY :: Y } 
Because for a long time Gold wasn't always assured to be there (it was optional as part of `binutils`) although the situation is much better, today. In general GHC demanding specific things like this from core packages like `binutils`, especially when it's optional, feels like bad style (at least, IMO). Also, because the way various distributions layout their packages/distros, GHC generally never knows what `ld` is anyway, so it has to check -- because there are various ways to install and configure different default linkers in several systems. So do you just pick `ld.gold` or `ld` and try both to see? You have to demangle the output to check it, etc... GHC already has to detect this to an extent, at least, but there are some annoying corner cases to think about, I think. This is something that might be better left to distros themselves with a bit of coordination on the GHC side, i.e. recommending they configure their GHC packages to use Gold instead by default. They also have to integrate GHC across their packages so it's "easier" to ferret out incompatibilities (e.g. if there's a discrepancy between `ld` and `gold`), instead of letting the user hurt themselves.
Thanks! I'll try it out in a VM. Compile times at work have gotten a bit tedious :).
(Old post, sorry) I did really just say "of course they do". I mean, I might have been a bit sarky with the "cite needed" bit, but I think your accusation of "hiding" is absurd. To the point: To quote yourself "outside of a few languages like Haskell". That's my exact point. Mr. Bracha surely *should* know about Haskell... and I'm pretty sure he did. I'm not sure why he didn't bring it up. Of course Haskell in those days wasn't quite the Haskell that we know today (which is de-facto GHC Haskell, let's face it), so he may have an excuse there. ... but my general impression is that Mr. Bracha just prefers to remain insulated/pampered (e.g. Dart) rather than actually try to keep up with what's actually happening currently. (Yeah, I know that's harsh, but that's my honest impression of him from lectures, writings, etc.)
i have some experience with this. The problem is that GHCs-parser/typechecker is not usable with incomplete code. If you think of Intellij, you can refactor code etc. while the file itself has syntax errors, no problem. This allows the IDE/Editor to assist you while you are writing the code, providing a much more dynamic experience. Unfortunately i have not found a parser/typechecker (parsers alone would be great!) that can cope with this! For proper assisting syntax-highlighting/refactoring etc. we would need a parser that can do this. I don't think this would be super difficult to do (if we tolerate unparsed-sections), because the indentation rules help us notice when the "broken" sections stop (we don't have count brackets!). 
Yeah, it's definitely a historical artifact. Much like Applicative/Monad, I think the original library design missed the mark a little bit and it was really unclear how gigantic the momentum behind standard typeclass hierarchy would be. It's stuff Haskell will be stuck with forever.
Cool! Hopefully this makes its way to Sublime soon
FYI: bos = Bryan O'Sullivan. I should mention that there are certainly some companies you didn't mention that contribute back, including Microsoft (very important!), Galois, Well-Typed, and surely others I don't know about.
If you're hiding some of the anecdotical information with notations and not mentioning implicit arguments, you can basically give `fix` this type: fix : [ Acc R ] → [ □^ R P ⟶ P ] → [ P ] where * `[ Acc R ]` means that it is impossible to define an infinite chain of values which are decreasing according to the relation `R` (in other words: any call graph which has decreasing arguments according to `R` defines a terminating function) * `[ □^ R P ⟶ P ]` means that you have to define a function and you may only use the recursive calls at smaller (with respect to `R`) values * Finally the return type indicates you get a proper value out. This `□^ R` is fairly well-behaved: it's a functor, you can write `(&lt;*&gt;)` for it (but not `pure`) as well as `duplicate` (but not `extract`). The whole Agda code: module Accessibility where data Acc {A : Set} (R : A → A → Set) (a : A) : Set where step : (∀ b → R b a → Acc R b) → Acc R a □^ : ∀ {A : Set} (R : A → A → Set) → (A → Set) → (A → Set) □^ R P a = ∀ b → R b a → P b [_] : {A : Set} → (A → Set) → Set [ P ] = ∀ {a} → P a infixr 5 _⟶_ _⟶_ : {A : Set} → (A → Set) → (A → Set) → (A → Set) (P ⟶ Q) a = P a → Q a fix : ∀ {A : Set} {R : A → A → Set} → [ Acc R ] → ∀ P → [ □^ R P ⟶ P ] → [ P ] fix {A} {R} acc P f = go acc where go : [ Acc R ⟶ P ] go (step acc) = f (λ b Rba → go (acc b Rba)) 
Thank you for that suggestion!
You can't do that currently, but it is on the roadmap.
Since you don't give your location, I assume you're looking for something remote. That being the case, you should surely check out the many free community resources. /r/haskellquestions, StackOverflow, the Haskell channel on FreeNode IRC, the haskell-beginners mailing list, etc. Edit: I should mention that IRC is probably your best bet for quick general help, or help with really small problems, or help getting a sense of where to start on large problems. SO and the mailing list tend to be best when you have a small to medium-sized problem that you can explain reasonably well. If you actually do want something in person, I'm often available in Bethesda, Maryland, or Arlington, Virginia. You can contact me for a rate.
&gt; we don't even have anything approaching the LSP at this point What about https://github.com/haskell/haskell-ide-engine ?
Sorry, but I don't understand what problem you'd like to solve or how \_\_init\_\_.hs would solve it. Can you explain more?
while I use 2 spaces in python and java and other languages when everyone else uses 4, in Haskell I think in the case of "data" 4 spaces is better, isn't it? 4 for "data", 2 for "where"
Final link times for a Haskell project with 130 modules and 8 executables: ld 124 seconds gold 36 seconds lld 11 seconds That is a 10x speedup from the default. Linking seems a solved problem now. Caveats: You could link even faster, but cabal bugs prevent it (see linked README). [`gold` thread from earlier today](https://www.reddit.com/r/haskell/comments/63shj7/link_with_the_gold_linker_for_faster_build_times/); I have also updated the [StackOverflow answer](http://stackoverflow.com/a/43243323/263061)
I've now figured out how to use `lld` and made an example. See https://www.reddit.com/r/haskell/comments/63y43y/liked_linking_3x_faster_with_gold_link_10x_faster/
If you're interested in using Docker for deployment you can do much better than using the containers Stack provides. Using [Alpine Linux](https://alpinelinux.org/) you can create some really small images. A simple hello world type application starts around 8MB. You can see what I rolled for myself [here](https://github.com/mgreenly/dockerimages/tree/master/alpine-stack). Be warned my scripts may be a bit out of date. The author of the [project](https://github.com/mitchty/alpine-linux-ghc-bootstrap) I based this on was working to get GHC up-streamed into Alpine repositories. Since that work was done I haven't had time to update my scripts yet. So I'm not necessarily recommending anyone use them. Just sharing them as an example. 
I'm not proposing removing the current way. I'm just proposing adding this as an alternative option. Also I really think the downside of a one time rename is negligible.
I don't understand how `__init__.py` files would help with that? Is it the `__all__` you're after? I'd suggest that the desire for this is an artifact of how you structure your packages based on python habits, rather than something that makes sense with idiomatic Haskell code. Generally, we have some level of hiding in place, so if there is a module `AST` and a subdirectory of modules that are used to build it, the `AST.hs` module itself won't export all the names from them, but just a portion of them. The `__all__` stuff won't cope with that. Furthermore, you don't even save on files or typing really, since you have a `module.hs` per module subdirectory in this system, and an `__init__.py` inside each subdirectory in the other. So I suppose the tab completion and file listing style is all you really get out of it. And you don't have to even worry about that if you're willing to name your modules e.g. `AST.AST` and place them inside their respective subdirectories anyway :-)
The great linker shootout
You're right, `howMany` is a better name. It's clever :-) 
&gt; Lets first try to imagine what we have on the heap after a couple iterations: &gt; (((go 3 &lt;|&gt; return 2) &lt;|&gt; return 1) &lt;|&gt; return 0) This is really helpful to visualize what is going on. It makes sense how this could cause data to build up on the heap. &gt; To circumvent that we have to move the decision into go so it is dispatched before recursing. Here is a different way to write your second solution with which it might be easier to spot the difference ... I don't think I have the intuitive understanding required to spot this difference. If I rewrite your example without the `do` syntactic sure, it might look like this: go !i = (&gt;&gt;=) (optionMaybe p) (maybe (pure i) (go (i + 1))) If I inline `go` this three times (like you did with the previous example), it might look something like this: go !i = (&gt;&gt;=) (optionMaybe p) (maybe (pure i) ((&gt;&gt;=) (optionMaybe p) (maybe (pure i) ((&gt;&gt;=) (optionMaybe p) (maybe (pure i) (go (i + 3))))))) It's not obvious to me why this runs in constant space. I can now clearly see why the previous example uses a lot of heap space, but I think I'm still missing an intuitive understanding of why this runs in constant space. Any idea what I could be missing?
Was "reflects" too vague, or am I not getting your joke? :/
 agree that `data` as a variable name is seldom useful. How about a third option: groupBy :: (Ord groupKey) =&gt; (record -&gt; key) -&gt; (record -&gt; otherRecord) -&gt; ([otherRecord] -&gt; finalRecord) -&gt; [record] -&gt; [(groupKey, finalRecord)] The original reason for replacing `record` with `data` was that the groupBy function doesn't really care if it's a record or a tuple or anything else. But our dominant use-case is with records. 
I mean all I want. And I really mean all I want. Is the tab completion and file listing style. And the `AST.AST` style won't fully work. As I have an `AST.AST` already for the `AST` type, and `AST` that exports both the type and the parser. Although for now I have just omitted such a file, as so far I don't seem to get much benefit from a re-exporting module, since most files only import just the AST or just the Parser, but surprisingly usually not both. So I guess for now this suggestion isn't really that important. 
A good lower level example of exactly that would be how in math there is one notation for powers and exponentiation whereas Haskell's got three: `^`, `^^` and `**`.
Depending on the size of your graph and purpose of your program, a test case may be more efficient. Although it is not compile-time, it is still a build-time check.
I prefer this. Using `k` for "key" is conventional in the `containers` library.
Question: what refactorings are on people's wish lists? Mine would be one to extract a local binding to module scope, adding extra arguments for other local bindings. So foo x = bar where bar = x+1 becomes foo x = bar x bar x = x + 1
I am running into issues with the assembler in Windows: &gt; package-0.2: configure (lib) &gt; Configuring package-0.2... &gt; package-0.2: build (lib) &gt; Preprocessing library package-0.2... &gt; [1 of 4] Compiling Module.Base ( src\Module\Base.hs, .stack-work\dist\ca59d0ab\build\Module\Base.o ) &gt; C:\Users\username\AppData\Local\Temp\ghc11572_0\ghc_2.s:55567:1: error: unknown directive &gt; .ident "GHC 8.0.2" &gt; ^ &gt; &gt; &lt;no location info&gt;: error: &gt; `clang' failed in phase `Assembler'. (Exit code: 1) &gt; &gt; -- While building package package-0.2 using: &gt; C:\sr\setup-exe-cache\x86_64-windows\Cabal-simple_Z6RU0evB_1.24.2.0_ghc-8.0.2.exe --builddir=.stack-work\dist\ca59d0ab build lib:package --ghc-options " -ddump-hi -ddump-to-file" &gt; Process exited with code: ExitFailure 1 
&gt;Not a Haskell comment, but you (or your employer) should worry about you becoming the only person who can make the software work. Producing something that works sort-of-mostly is easy. Producing something that integrates well with the office workflow and that can be used by other people without having to be trained by you personally is another matter. As things change your scripts will need to evolve, so maintenance of this stuff is another issue that needs to be considered. Our company currently doesn't have resources to start IT department and in fact it is not needed. You see, I kinda come here as a young guy to bring light like prometeus and freshen stuff. So if anything I should not generate more costs but look how can we increase productivity. I already made many things better thanks to my financial background. Many of the decision before my arrival where based on intuition. I try to bring some data centralization so we can decide on facts. See when there is downtime there I can only look at Excel spreadsheet and think how much better it would be if i could transform it into database with proper queries. Of course it would be better to have IT team working with me. I could just explain them our needs and have them build interface for me so I could only input data. But I have to work under constrains that i have and programming is something that i enjoy anyways and would like to come back to. Maybe in future if we continue to grow we can hire some IT but i think just about short term for now. When we order simple website and overpay for it significantly it is also something that i could take over given how much time I still have. This is my job. Help us make better decisions cause it was shitty before. And I just realized that computers can help me with this.
So would you say that Haskell is bad choice for this tasks ? I know i will sound provocative but i just want to learn more on the topic - so Haskell is not good for website, not good for database, servers, not really for scripts also I heard pretty bad for GUI apps and bad for games. It paints pretty bad picture overall. So probably only very niche stuff is where it is useful ?
Personally, I would use Haskell for scripting (syntax/type errors in long-running scripts are a personal bane of mine), command-line tools, servers, rest clients, and for access to well supported databases like postgresql and mysql. Also for parsing stuff, of course. But it is a less used and less known language, so it can be harder to justify to your boss/coworkers.
I do not know if or how this can be made work on Windows. But if you want to look into it, I recommend first building as usual and passing `-v` to ghc to see how Windows usually links. On Linux, ghc uses the C compiler (so gcc or clang) for linking and also for assembling, so i could simply replace gcc with clang using my options. If Windows doesn't use gcc for assmbling, you may need to use a different option. Try leaving away the `pgma` flags to check if the default assembler works.
I'd love to know if there are established names for these. I tend to call box "box", or "guarded", or "later" (which is probably more appropriate in a coinductive setting, cf. [Productive coprogramming with guarded recursion](http://dl.acm.org/citation.cfm?id=2500597)) but it's also behaving a bit like the [modal operator "necessarily"](https://en.wikipedia.org/wiki/Modal_logic) so that could work too. The brackets are a dependent product. I guess you could see it as the forgetful functor from indexed set to set but that does not bring much to the table. The long arrow is the appropriate notion of arrow for indexed sets: `[ P ⟶ Q ]` precisely corresponds to index respecting morphisms between `P` and `Q`.
Turns out the flickering has something to do with how rxvt-* interacts with some internal buffer of NCurses or something along those lines. Replacing wclear with werase where appropriate appears to have solved that issue. Resizing the terminal window still causes flickering however.
For anyone wondering how to install lld on linux: * [StackExchange: What's the name of ubuntu package contains llvm linker lld?](http://unix.stackexchange.com/questions/184014/whats-the-name-of-ubuntu-package-contains-llvm-linker-lld) * http://apt.llvm.org/
How about this? groupBy :: (Ord k, Semigroup v) =&gt; (a -&gt; k) -&gt; (a -&gt; v) -&gt; [a] -&gt; [(k, v)] It follows the single-letter convention, it uses more meaningful names (k and v for the key and value of the resulting assoc-list), and most importantly, it reduces the the number of cognitive pieces by moving one of the arguments to a constraint. It is now clear from the type signature that the function constructs an assoc-list from a list by computing a key and a value from each list element, and merging the values which have the same key. The API encourages the use of precise types like `First Int` or `Sum Int` with a built-in notion of merging, but if you really need to merge with an `f :: [v] -&gt; v'`, you can always use `[v]` for the Semigroup and post-compose with `map (second f)`.
 *Quark.Types&gt; :set -XOverloadedStrings *Quark.Types&gt; import qualified Data.ByteString.UTF8 as U *Quark.Types U&gt; putStrLn $ U.toString "\226\135\165" ⇥ *Quark.Types U&gt; U.toString "\226\135\165" "\8677" *Quark.Types U&gt; 
Personally I never got it to work in my emacs setup, despite trying multiple times. For me it's kind of similar to ghc-mod: a nice enough idea in theory but just not stable enough for production use.
haskell is so advanced that a inordinate amount of literature is devoted to what in any standard language would be simple getters setters and loops. But standard language programmers feed their children with the money they earn satisfying the needs of the client with their programs so they have other problems more important than wasting the time on aesthetics.
I don't like using TH for these purposes because I like my compile times short. I typically have some `LensClasses` module with hand-crafted MPTCs, like the ones generated by `makeFields`: class HasFooL s a | s -&gt; a where fooL :: Lens' s a class HasBarL s a | s -&gt; a where barL :: Lens' s a Sucks to write, but I only add classes as I need them, and I tend to rarely actually use lenses, as awesome as they are. I also don't like the underscore convention or the type-prefix convention, so my data types look like data User = User { name :: Text } Yeah, `name` is a nice variable name, but if ever there's a clash I'll just use `n` or something. Plus, with `NamedFieldPuns`, you can get out your fields with nice names as User{name} Overall, it seems our approaches are very similar, though I've yet to experiment with some of the newer extensions, or any record library. If anything exists today that can scrap some/all of this boilerplate I'd love to hear about it.
Sorry I don't. However it's quite popular if you compare it to other secure messaging applications.
I'm not sure if this is the type of joke that we can be telling around here...
Haskell is very good at all of those things (really, it is). In fact, for a lot of us, it's the ideal language. Unfortunately, Haskell is not as straightforward as other languages, and can require a substantially greater up-front investment to be productive. Dovetailing with this is the fact that client-side scripting in any language other than vanilla JavaScript means build tools, transpilers, and often new design patterns. But with vanilla JavaScript, mess of a language that it is, it's as simple as including a `&lt;script&gt;` tag, and as intuitive as mutating the right DOM node. The story is better on the back end, but you're still going to have to learn a *ton* of theory upfront. People say this is easier to do if you're a beginner, but I don't think it's a cinch either way. Haskell is hard in a way that can't be pinned only on bad pedagogy or paradigm-shifts. So many of the benefits of Haskell are only obvious if you've been bitten before by less rigorous approaches. It's a beautiful language, and you can be very productive in it in many domains. It's telling, though, that something as fundamental and necessary as IO is often discussed in Haskell books only after many chapters of theory have been introduced. On the other hand, Python might as well be optimized for beginners. Like Haskell, it's consistent and powerful. Unlike Haskell, it's incredibly intuitive as both a language and an ecosystem. Want to write a function that prints something to the console? Just include `print` anywhere in it; monads not necessary. (So why use anything but Python if it's so easy? You'll have to take my word that there are plenty of reasons to prefer Haskell, but these don't rear up nearly so much when you're first learning.) None of this is to discourage you from using Haskell, but the language will demand a lot from you before you become efficient with it. In contrast, I'd say that many Python or Ruby programs could be deciphered even by non-programmers (with a little help).
I wonder if Nixpkgs could default to using `lld` for Haskell builds somehow, since it can be sure that `lld` is available during the build.
My thoughts on this are summarized here: http://softwaresimply.blogspot.com/2011/12/whats-in-name-lot.html
I think we can use `shake` to rewrite `makedeb`.
Suggestion: Emacs IDO like fast and comfortable fuzzy selection for anything but most importantly files.
That's encouraging! With intero coming out it seemed like most effort went that way.
Also codementor, where there are some haskellers
Is there a way to make this work with stack?
First we should try to make it optional ([see effort here](https://github.com/NixOS/nixpkgs/pull/24692)), then if that works, one could try with it being the default.
I would say probably an error. I guess for total backwards compatibility `AST.hs` should take precedence. But I am guessing that in practice this should not break anything. 
Not currently because you can't pass `--with-ld` to stack, but it's on our roadmap (https://github.com/commercialhaskell/stack/issues/2369).
Here is the official blog post about it: [Open sourcing Wire server code ](https://medium.com/@wireapp/open-sourcing-wire-server-code-ef7866a731d5).
I am curious about this too, especially with Haskell for Mac's bindings. 
As far as I understand this can only handle the first requirement: &gt; The user specifies the graph in the code by defining each vertex (name &amp; properties) together with its predecessor vertices (by name) (implying incoming arcs). But what about the second requirement? &gt; The user can also explicitly define arcs; an arc can be explicitly added after its vertices have been defined. So, if I have a `g :: DAG Int` and would like to add a dependency `3 -&gt; 5` to `g`, how do I do this? Looks impossible, as the type of `g` should somehow encode the fact that there is no reverse dependency `5 -&gt; 3` in `g`. 
Off top of my head, we had a couple of reasons for breaking up the project into multiple packages: - better distinguish between the responsibilities of the modules and different parts of the system, - perhaps helps reduce the amount of stuff that has to be re-compiled due to a small change, - make potentially useful modules available in form of standalone libraries and put them on hackage, and - enable code reuse in other parts of the project. More specifically, about the last point, we were working on a few different parts of the system at the same time, and we thought it'd be nice to be able to use parts of each other's work in our own branches without necessarily having to merge everything into `master` first. What's your situation like?
I just want to chime in and echo this sentiment. We moved to a monolithic repo last year after we realized we had a couple circular dependencies between some haskell and non-haskell git repos for the same product. We set up a Shake build system to handle our monolithic repo build and now the situation is much nicer. We can build individual components or the whole thing, we can build in parallel, and caching means we only need to build what has changed. A git repo should be a unit of development, not a unit of deployment. Prematurely splitting things into different repos is akin to prematurely sharding your database, or making a microservices architecture on day 1. You are just introducing an operational headache and a lot of overhead. The idea of one repo per build artifact by default is pervasive in open source and most CI build systems, so I think that's why people do it in companies by default.
So because of the bug mentioned in the README, are the ghc-options in the .cabal file being ignored, and the original `ld` linker still being used in part of the build?
Yes, you are correct that it doesn't allow after-the-fact edge insertions -- that is part of what I meant by the fact that it "requires the DAG to be topologically sorted before it's even constructed." If you are seriously considering a data structure like this (would could possibly be made more tractable using `DataKinds` in a way that allows you to use the same increment-by-one-at-the-type-level trick without needing the crazy unary representation, at least at runtime), then its main use would probably be as an intermediate step in an ultra-safe sanity check for a graph whose type doesn't express its invariants. E.g., you would start with a data structure like whatever you already have, *convert* it into the above `DAG` programatically (maybe at compile time using Template Haskell!), and not have to worry anymore about whether your code keeps the invariants under all cases, since the type system checks that for you! I agree that hand-writing DAGs directly in the above encoding is probably impractical.
From my own experience on two larger projects, I highly recommend the mono-repo approach. I've had difficulties using `stack` on projects which have many `packages` entries which point to Git repos. Furthermore, in my first non-trivial commercial app I did the same and broke the package into many different packages. I probably went overboard but there is definitely overhead with cabal, stack, and other tools to doing this. In a more recent project I've been working on, I have kept all my modules in a single cabal project (&gt;150 modules) and only move packages out when there is a clear benefit. 
Checkout https://github.com/michaelt/structured-graphs I haven't looked at that repo, but the paper it's based on is what I'm recommending. With the PHOAS approach, you can enforce your desired properties *by construction*: it's effectively impossible to express an ill-formed graph. HTH. (Another way to think about it: you want a tiny DSL that has lets but not letrecs. And you either want no name shadowing or a sensible semantics for scoping. And static name resolution. That's sort of where PHOAS originated.)
&gt; perhaps helps reduce the amount of stuff that has to be re-compiled due to a small change, This is a big feature of breaking up packages. You can recompile only your (smaller) package, and once it compiles / tests pass you can start compiling the packages that use this package. Huge boost for productivity!
That's true, and I apologize. I forgot that Lennart and Neil both work at SC.
Why is `ld` so slow?
Fail again :). I can only offer you [this](https://en.wikipedia.org/wiki/Wilhelm_scream).
I don't really know but as I recall, the last I looked (which was long ago, before intero existed), no.
Exactly.
Doesn't seem to be working... also, not sure which package to get liftIO from? It comes in Network.Wai, but apparently also in some other libraries. I have a cross-post [here](https://www.reddit.com/r/haskellquestions/comments/641t0u/how_to_respond_to_a_wai_request_while_also/) with my full code, if that might help.
Regarding the learning curve of vim, you could give https://vim-adventures.com a try. I haven't paid for the full version, but found the intro legitimately fun. (Not to take anything away from your project!)
I somehow feel like we're caught in an in an infinite loop. :)
Still not working :( Here's my imports: import Control.Monad.IO.Class (liftIO) import Data.String.Conversions (cs) import Data.Text.Internal (Text) import Network.Wai hiding (liftIO) import Network.HTTP.Types (status200, status400) import Network.Wai.Handler.Warp (run) And here's the code still giving trouble: scoreResp s = do file &lt;- liftIO $ readFile "txt/tower_scores" liftIO $ writeFile "txt/tower_scores" (unlines ([s] : lines file)) responseFile s t "txt/response" Nothing where (s,t) = (status200, [("Content-Type", "text/plain")])
And here's the error from GHC: server.hs:34:36: error: * Couldn't match expected type `Response' with actual type `m0 b0' * In the expression: scoreResp s In a case alternative: ["put-tower-score", s] -&gt; scoreResp s In the second argument of `($)', namely `case pathInfo req of { [] -&gt; index ["martin"] -&gt; martin ["dot-martin"] -&gt; dot_martin ["balls"] -&gt; balls ["fast-balls"] -&gt; fastballs ["space"] -&gt; space ["towers"] -&gt; towers ["tower-scores"] -&gt; towerScores ["tanks"] -&gt; tanks ["graham"] -&gt; graham ["jarvis"] -&gt; jarvis ["jQuery"] -&gt; jQuery ["jqmJS"] -&gt; jqmJS ["jqmCSS"] -&gt; jqmCSS ["images", "ajax-loader.gif"] -&gt; ajaxLoadImg ["snd", "torpedo"] -&gt; snd_torpedo ["snd", "bomb"] -&gt; snd_bomb ["snd", "death"] -&gt; snd_death ["snd", "orb"] -&gt; snd_orb ["favicon.ico"] -&gt; favIcon ["put-tower-score", s] -&gt; scoreResp s }' server.hs:120:51: error: * Couldn't match type `Network.HTTP.Types.Status.Status' with `Char' Expected type: [String] Actual type: [[Network.HTTP.Types.Status.Status]] * In the first argument of `unlines', namely `([s] : lines file)' In the second argument of `writeFile', namely `(unlines ([s] : lines file))' In the second argument of `($)', namely `writeFile "txt/tower_scores" (unlines ([s] : lines file))' server.hs:120:57: error: * Couldn't match type `Char' with `Network.HTTP.Types.Status.Status' Expected type: [[Network.HTTP.Types.Status.Status]] Actual type: [String] * In the second argument of `(:)', namely `lines file' In the first argument of `unlines', namely `([s] : lines file)' In the second argument of `writeFile', namely `(unlines ([s] : lines file))' server.hs:121:3: error: * Couldn't match expected type `m b' with actual type `Response' * In a stmt of a 'do' block: responseFile s t "txt/response" Nothing In the expression: do { file &lt;- liftIO $ readFile "txt/tower_scores"; liftIO $ writeFile "txt/tower_scores" (unlines ([s] : lines file)); responseFile s t "txt/response" Nothing } In an equation for `scoreResp': scoreResp s = do { file &lt;- liftIO $ readFile "txt/tower_scores"; liftIO $ writeFile "txt/tower_scores" (unlines ([s] : lines file)); responseFile s t "txt/response" Nothing } where (s, t) = (status200, [("Content-Type", "text/plain")]) * Relevant bindings include scoreResp :: t -&gt; m b (bound at server.hs:118:1) 
&gt; I don't like using TH for these purposes because I like my compile times short. Does anyone actually have profile data showing that using TH for simple tasks drives compile times dramatically higher? I don't understand why TH would be so dramatically expensive to run. It's running a program that generates code, which is then compiled. So the metaprogram must be compiled, generate its code, and then compile again. Doesn't seem so massively horrible. Indeed, the times I use TH to generate lenses, my compile times don't shoot way up. I have written some TH that takes a long time to compile. But that's because it's generating a massive amount of code. I can also write massive modules by hand and they will take a long time to compile. It's just easier to generate a massive amount of code (and not know you're doing it) when using TH. However, other Haskell constructs--such as `deriving`--can also generate massive amounts of code that take a long time to compile. TH just seems to get disproportionate hate for compile times and I wonder if anyone has anything to back this up.
Thanks for the tip on the [s] - it's indeed a string, not a char, so I fixed it. But that didn't solve everything. I apologize for just not knowing enough.. I'm in an emergency and can't study monad transformations at length. I've tried various combinations of liftIO, return, pure, liftM... I understand the general problem is that my function is returning IO Response when it ought to be Response. But I don't know how to change that. Once I've got a type stuck in the IO Monad, I never know how to get it out. And I feel like surely there must be an easy solution I just don't know. All I want is for my server to take a request, write a string from that request to a file on the server, and send the client a generic response string. If I could just see one working example... That said, I appreciate everyone's help for a newbie. I'm a big fan of Haskell for purely-functional algorithms; it's my tool of choice for testing them. But every time I try to write a larger, stateful application, with monad stacks, crazy type hierarchies, lots of IO... I get nowhere. It's like having to learn a new DSL for every kind of application, which is great for code correctness but causes a never-ending learning curve.
Unfortunately, STLens fails to pass the lens laws when you use it in non-trivial situations. Consider when I give you something like newtype Loop s = Loop (STRef s (Loop s)) If I give you a Loop tied back upon itself. then compose two lenses deep and change it what answer do I get? I have to make two changes, but only have one mutable pointer controlled place to store the result! If I do this in two passes instead, do I get the same answer? Sadly no.
You could just make the lens itself polymorphic and do different things depending on if it's operating on a `STRef` or a `ST (STRef)`.
&gt; I'm in an emergency and can't study monad transformations at length. I don't think you need monad transformers at all. But, in Haskell, you do yourself a disservice not being familiar with the functions and operators in `Control.Monad`, especially the ones that are in `Prelude`. &gt; Once I've got a type stuck in the IO Monad, I never know how to get it out. You don't. However, if you have an `IO a` and an `a -&gt; IO b` you can use `(&gt;&gt;=)` or `do` notation to produce a `IO b`. For example in the `wai` example, `respond :: Response -&gt; IO ResponseReceived` and this is applied directly; i.e. `respond expr` where `expr :: Response`. But, if you have a `expr :: IO Response`, you can't apply it directly; the types don't match. In that case you *can* write `expr &gt;&gt;= response` or `do { resp &lt;- expr; respond resp }` both of which have type `IO ResponseReceived`. Don't try and get out of `IO` until you are very much more experienced. `main :: IO ()` is in `IO`, so you certainly don't *need* to get out of `IO`.
just sayin, I kinda like these newsletters. I follow one for clojure too and, well, it keeps me up to speed :) thanks!
&gt; What's your situation like? Huge app with common DB models that can be split up multiple ways. For example: common models, task-specific models, end-user facing code, admin-facing code, etc. If split into multiple packages, what does the refactoring story look like (of a common data model is changed)?
A couple more adjustments I noticed you made, and it works! Thank you so much; I understand the points of that 'resp' better now.
So now we're quoting /u/metafunctor? :)
It might be good to avoid the name `groupBy` as there is a function with the same name in `Data.List`. For functions with type like `a1 -&gt; a2 -&gt; ... -&gt; a100` I prefer multiline signature foo :: a1 -&gt; a2 -- ^ In case you want to document this type. . . . 
I will second the `monorepo` approach. At work we have a big repo with all internal Haskell projects. We separate concerns, so there's a `foo-models`, `foo-api`, `foo-admin`, `foo-worker`, packages (self descriptive) along with `foo` which is a general purpose library shared among them. The stack.yaml refers to these by location, so any change immediately propagates, which is awesome. This has done a ton to reduce compile times, since the `foo-models` or `internal-prelude` modules rarely change, and we're usually only recompiling ~20-30 modules at a time in a single project. This keeps things very reasonable.
If you use stack's paths to specify package locations, it is pretty seamless. `stack build --fast --file-watch`, make some changes, fix some errors, eventually everything is green and awesome.
Yeah, I know there's a lot of movement in this area, both by LLVM and Apple itself. I think they recently moved (or are in the process of moving?) to some sort of intermediate bytecode which will be uploaded to the Appstore, then optimized and compiled by Apple for specific devices. Not sure if that will simplify or complicate things. Probably the latter.
What does it provides over what [the Elisp wrapper for HaRe already does](https://github.com/RefactoringTools/HaRe/tree/master/elisp)? 
holy shit this guys comment history
It's just a simple wrapper over that for people who use Spacemacs layers instead of MELPA packages.
LLD is not only faster, it's also *massively* simpler: &gt; We are using LLVM libObject library to read from object files, so it is not completely a fair comparison, but as of February 2017, LLD/ELF consists only of 21k lines of C++ code while GNU gold consists of 198k lines of C++ code. And it's always a cross-linker.
The people on #haskell have probably told you this, but aren't these ~~similar~~ related to monoidal functors? I'm rambling here: for a monoidal functor, one wants F(A) * F(B) = F(A * B), so F respects the monoid/tensor operations of the source and target categories. `pm a` seems to be an instance of `Monoid` in the "image" category of `pm : Hask -&gt; Hask` (if talking about images is even sensible here).
Would [hakyll](https://jaspervdj.be/hakyll/) serve your needs?
That's not encouraging to read, but I think you're right. Without a sponsor of some sort willing to put someone on following LLVM and maintaining the GHC backend, the prospects aren't good. One other concern that I've come across while investigating this is that the GHC GC doesn't seem to be geared towards real-time applications (like running a UI on an iPhone!), so LLVM may be far from the only difficulty.
I like that golang has integrated the linker and made it possible to cross-compile trivially. This doesn't prevent you from optionally using an external linker but see golang and D's dmd for fast compile times in contemporary compilers. Is it correct that the separation of `cc` and `ld` is a Unix'ism or gcc'ism?
hm. These seem quite dissimilar to monoidal functors because the "structural monoid" instance does not preserve any monoidal structure on the original category, does it? This is why the currently available Monoid (Map a) instance is often not what people are expecting. I'm not familiar with images. Will read around a bit...
Yeah, it's sort of a unix-ism, but it makes sense in general. Linking has nothing to do with compiling C, it just links binary objects regardless of where they came from. Go is possible to cross-compile trivially because *it does not use libc or any other system libraries* (it uses system calls directly), not because the linker is integrated.
Thanks, looks good. I have to experiment though.
As for a hosting platform, am I right to assume you can host this at github pages?
&gt; Linking has nothing to do with compiling C, it just links binary objects regardless of where they came from. True but there's efficiency (aka speed) to be gained with an integrated linker. The inefficiency here might become more negligible with the proliferation of persistent RAM storage getting mainstream in the near future, although `dmd` like `go` is fast because it's tightly integrated. C and C++ already waste a lot of time with the inefficient design of CPP so any improvement is welcome when building Firefox of LibreOffice. &gt; Go is possible to cross-compile trivially because it does not use libc or any other system libraries (it uses system calls directly), not because the linker is integrated. Nit-pick: if your go is built with `CGO_ENABLED=0` and is at least 1.8.
This seems really nice. I've been confused by how to make realistic scenarios within the QuickCheck `Arbitrary` paradigm, which now that I see your library seems pretty obviously like a typical misuse of typeclasses. One thing I wonder about property-based testing in general is how to ensure coverage of edge cases, for example `Int` overflow...
I'm not sure if hedgehog can guarantee coverage of such edge cases, but I think being explicit about the values you're generating certainly helps. In hedgehog you'd say `x &lt;- forAll $ Gen.int Range.linearBounded` and it would be obvious you're exercising the entire range of `Int`. One thing which I have seen bite people over and over again with QuickCheck is that the `Arbitrary` for `Int` only has a range of `-100` to `100`. This makes sense in many cases, but not always.
I'm pretty sure the main Go compiler (not gccgo) always produced static binaries that use syscalls directly. Certainly waaaay before 1.8. You don't have to disable cgo for the entire compiler, just don't use any cgo in your projects. I've cross-compiled go programs from darwin/amd64 to freebsd/armv6 back in the day. Certainly the libc for freebsd/armv6 was not present anywhere on my computer!
Yea that's true. I guess that matters a a lot if you use Intero or other things like that
There is a more formal write up in-progress, but nothing useful I can point you to yet I'm afraid. The main difference is that QuickCheck uses type directed shrinking via the `Arbitrary` type class: data Gen a = Gen (Seed -&gt; a) class Arbitrary a where arbitrary :: Gen a shrink :: a -&gt; [a] where as in Hedgehog, the generator produces a lazy tree containing the initial generated value and all of the possible shrinks: data Gen a = Gen (Seed -&gt; Tree a) The advantage of this is that `Gen` in Hedgehog is a covariant functor, while `Arbitrary` in QuickCheck, even if it was changed to be a data type, is invariant. This advantage makes it possible to combine `Gen` using the familiar `Functor` / `Applicative` / `Monad` interface, while still offering shrinking. *Edit: I thought of another interesting aspect to the QuickCheck comparison.* The above example is actually a simplification. In Hedgehog, `Gen` is a monad transformer, so in reality it is more like: data Tree m a = Tree (m (Node m a) data Node m a = Node a [Tree m a] data Gen m a = Gen (Seed -&gt; Tree m a) This is used to good effect in the [simply typed lambda calculus example](https://github.com/hedgehogqa/haskell-hedgehog/blob/master/hedgehog-example/test/Test/Example/STLC.hs) where a `Reader` monad keeps track of which available bindings have a particular type. I think that this could also be achieved if an `mtl` style type class was created for `Gen` however. I'm not sure what will end up being the best approach in this space.
Yes -- there's information on how to do this in the docs: https://jaspervdj.be/hakyll/tutorials/github-pages-tutorial.html
I've heard various complaints about the GC over the years, but the problem cases were a lot more demanding and realtime than a typical GUI application. Unless you're doing something especially demanding and need rock-solid &gt;60Hz at all times you're probably going to be fine there.
Any way to generate random functions?
Why wouldn't there be? The quickest idea that comes to mind is, since you usually are interested in a subset of functions, write an EDSL in the form of an ADT and an interpreter function, then `Gen` the EDSL and `fmap` the result.
That makes sense; I've heard that the standard `ld` is especially slow with `-split-sections`, but not tested that myself yet.
Make z a type parameter. Then your types are `Point ()`, `Point Double`, and `Point (Maybe Double)`.
Considering QuickCheck features a way of generating and showing completely arbitrary functions this sounds like a really poor way of doing it. I think the question that was asked is along the lines of, is there anything comparable to `CoArbitrary` and `Fun` from QuickCheck in this framework?
Please tell your people to stop posting these links to /r/haskell. It comes up all the time, and it's annoying as hell.
That doesn't change the fundamental problem which how to convert between those types ( which in that case are just examples).
I'm sorry, I think my pun thread intents got lost in translation... "depends on" and "type of" were meant to continue your joke, not confuse you. Sorry about that.
The general solution is to find the common structure of the types you want to convert to/from, insert type parameters to make the general versions, and specialize the type parameters to get the exact structures you want. Alternatively, with `RecordWildcards`, foo Point {..} = Point2 {.., z=0.0} can also work, though it's not quite as nice.
I have no idea what shrinking means I'm this context.
Though really, just directly using applicative notation is not that bad: bikeMsg city bikes = "there are " |# bikes #| " in " |# city #| "." printBikeMsg = print (bikeMsg &lt;$&gt; getCity &lt;$&gt; getBikes) 
The section on template haskell was brilliantly concise =P
I've never found myself reaching for `label` and friends when using QuickCheck, but maybe I should. I didn't implement them yet because I felt like I haven't used them enough to really understand what would make a good implementation. I'd love to see some examples of their use in the wild.
Hedgehog's love eating bugs! *Sorry, I know, I'm a terrible person :)*
I see. However i am interested in lenses going the other way : from `PointM` to `Point3` or `Maybe Point3` as it's not always possible to get a `Point3` `PointM`. In the stab semantic does a `Maybe Point3` contains a `Double` or a `Maybe Double` and which point view works better in the lens universe. Do i need a prism or something else instead of a lens to go in this direction (from PointM tobPoint3) ? (There are just made up example to simplify the problem and have nothing to do with my real problem).
I have a nagging sense that there's some kind of highly relevant abstraction for this—anyone have any idea what I might be thinking of? Some kind of class or type family relating ID types to object types, or maybe a class for things that can be parameterized by either?
&gt; or maybe a class for things that can be parameterized by either? That would be `Functor` =P But more along the lines of what you're thinking of, maybe `Traversable`? Since it allows you to turn `User OrganizationId` into `User Organization` with just `traverse getOrganizationById`.
Awesome, thanks! I've been using disorder-jack and I like it a lot. I agree that Arbitrary is superfluous, I don't miss using it.
I think the relationship is `OrganizationId ~&gt; Organization`, for some definition of `~&gt;`. The most straightforward candidate is monadic functions for fetching something by its ID, which can be plugged into `Traversable`. So again, I think the abstraction here is `Traversable`.
If you have 17 parameters just use lenses, as the final example says. Parameterizing the types gives you more flexibility there whether you use typeclasses or not. But a thing that appears often in more abstract things (Of course something is abstract. Why do you call it abstraction otherwise?) is that one or two types are actually pretty generic and you want them to be usable for more purposes.
No, I get what you mean, it's just that I think my "nagging sense" was about something else. I guess maybe I was thinking of something along the lines of data LoadState = HaveKey | HaveValue type family ILS ls a where ILS HaveKey a = Key a ILS HaveValue a = a data Post userls = Post { postTitle :: Text , postBody :: Text , postAuthor :: ILS userls User } , which may or may not be an improvement! On the one hand, with some tweaks it could let you do ad-hoc polymorphism over loadedness, but on the other hand, it makes stuff like your `Traversable` usage much more complicated.
I just think that's contrived. It doesn't seem to accomplish more than `Traversable`. And if you care about abstracting keys, your type family is unnecessarily complicated type family Key a data Post user = Post { postTitle :: Text , postBody :: Text , postAuthor :: user } type PostWithId = Post (Key User) type PostWithUser = Post User getPostUser :: MonadDB m =&gt; Post (Key User) -&gt; m (Post User) getPostUser = traverse getUserById
I know of at least two people working on it (myself included) 
As someone who's currently hard at work to make TH a reality for iOS/arm64, android/armv7 and android/arm64, I'm quite confident we are getting there. As for backporting, I don't think that will happen. Realistically I see this to be part of 8.4, assuming there won't be any further big roadblocks. Regarding bitcode (it really is a bit based format), if apple ever restricts us to bitcode for iOS, we'll have another set of challenges ahead. It's not impossible to get this right, however there's quite a bit of work needed to make this work. ghc would need to be tweaked a bit (e.g. the mangler would need to go as that operates on the assembly level), all tooling would need to learn that bitcode is an acceptable final object format. This of course would be valuable outside of the goal to produce bitcode apps, it would also allow to use llvm bitcode lto. These are all interesting challenges, however as noted elsewhere, funding is certainly an issue.
Ah. That's pretty straight forward indeed. Thanks for expanding that macro definition.
If z has type `a` where `a` is a type variable, you could derive a functor instance. Therefore you could just use `fmap` to convert between the different types.
&gt; pointM3 id :: PointM -&gt; Maybe Point3 Of course, I think that's what i was looking for ... 
I agree with you, in fact that is exactly the point of my post [there](https://www.reddit.com/r/haskell/comments/647o1l/new_question_about_lenses)
The natural conclusion of this is a fully polymorphic product type which I recommend[1] for Opaleye. It turns out that it gives a lot of people an allergic reaction. [1] but do not require, contrary to popular belief
Not quite, because you also want `Point3 -&gt; PointM`, at which point you use `Control.Lens.Prism.prism'` to get yourself a `Prism`.
Could you explain please, because i don't really understand prism.
&gt; You do not need Monads to perform IO in Haskell [...] You can combine expressions of type `IO` by other means than `&gt;&gt;=`, for example as monoids. ghci&gt; putStrLn "Hello" &lt;&gt; putStrLn "World" ghci&gt; getLine &lt;&gt; getLine Or as categories and arrows by wrapping them in the `Kleisli` newtype, to name just a few. &gt; The instance of return that does the work is defined elsewhere. [...] Can someone please comment where the instance is defined. Here is what the documentation for [GHC.IO](https://hackage.haskell.org/package/base-4.8.0.0/docs/src/GHC-IO.html) says: &gt; The IO Monad is just an instance of the ST monad, where the state is the real world. In GHC.Base you will find `return` to be an alias for `pure`. What you are looking for is defined in the `Applicative` type class in [GHC.ST](https://hackage.haskell.org/package/base-4.9.1.0/docs/src/GHC.ST.html).
Everything in this block of code is illegal from a lens law perspective. Notably the 'Lens'es supplied aren't. That said, its cute that it typechecks. 
I agree with this, but I'll just add that you also need the monad laws to hold (and return obviously).
Well, 8.4 is still about a year out. However, I do have iOS/arm64 working, and believe I'm almost done with android/armv7. android/arm64 might in principle already work with my code base. What is left is mostly resolving minor bugs (which for armv7 are almost all due to word size differences between the host and target). If you want to keep up to date, you might want to follow https://twitter.com/zw3rktec, where I intent to post updates shortly. ghc does not have a real notion of bitcode. I did some work on giving ghc a proper bitcode backend (via https://github.com/angerman/data-bitcode-llvm, and the other data-bitcode-* repositories that can be found there), however that has not been completed yet. What ghc (with -fllvm) currently does is: cmm -&gt; textual llvm ir -&gt; opt -&gt; llc -&gt; mangler -&gt; assembler it *does* use the llvm intermediate representation, and opt turns this into bitcode, which llc then turns into assembly, upon which the mangler does some, transformations (avx alignment, function/object rewrites for elf, dropping .subsections_via_symbols for mach-o) and finally turning the assembly into object code. Thus there is bitcode in there, but it's not really of interest to ghc, it's just an intermediate format used between opt and llc as far as ghc is concerned. Regarding backends, ghc has four backends I can think of right now: the ncg, llvm, ghcjs, and well eta; ghcjs and eta not being part of ghc. ~~I don't think ghc was designed with multiple backends in mind. Changing this design would certainly be helpful. I don't know of anyone who's working on this though.~~ I think the way backends are currently supported in ghc could be improved substantially by redesigning the way backends are handled. GHC is basically a cascade of intermediate languages until you end up in cmm and then either the ncg or the llvm backend. From which level you want to start your own code gen pretty much depends on your codegens needs. If it's rather C like, cmm may be a good option. If you need information that might have been erased at the cmm level already, you might want to consider basing your own code gen upon an earlier stage. ghcjs startes the codegen from stg I believe. I don't know much about the java codegen in eta. And that is only code gen. You'll need to deal with linking and cabal integration as well. All in all I would say it's approachable quite ok. I personally would prefer cleaner interfaces and better isolation than what we have right now. Then again, as I mentioned this all ends up being a question of funding. I'm very grateful to obsidian.systems, who have invested into improving ghcs cross compilation template haskell situation, and are funding part of the cross compiler template haskell work I do. --- Edit: As rwbarton kindly pointed out, GHC has had the ncg and c backend since the 90s.
Nitpick, this is all there is: (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b return :: a -&gt; m a (&gt;&gt;= return) = id return x &gt;&gt;= f = f x a &gt;&gt;= (\x -&gt; b x &gt;&gt;= c) = (a &gt;&gt;= b) &gt;&gt;= c 
I still think these kinds of representations (or lack thereof) are nothing fundamental and just an engineering trade-off. Everyone likes simple monomorphic datatypes that just perfectly model your domain and nothing else, but sometimes in Haskell it's just easier to open them up and leverage some generic machinery — like Opaleye. Getting allergic reactions to it (which I fully understand), isn't always a great argument when engineering an easy to maintain piece software.
&gt; I'm not sold on the typeclasses. Isn't it too arbitrary? It might just be useful. Things don't always need to have some fundamental underlying theory. Sometimes things just accidentally work, but might not work anymore in a more general settings. And that's fine. Many people like Haskell because it allows them to use fundamental abstractions from math and model things 'just right'. For me the biggest power of Haskell is to be able to model ad-hoc abstractions / arbitrary things as well and still have a sense of control over what's going on. Haskell makes it super easy to model something ad-hoc that just works, find out later that it doens't exactly scales to what you want now, change it, and *never feel a bit of regret* for picking the ad-hoc abstraction in the first place.
I'm a little unclear about this myself. Why would lazy IO require monads? My understanding was that Monad is a convenient way to nest function calls and then join the results. Nesting function calls forces sequential evaluation that can be evaluated lazily, but that does not require `join`.
Ok, I got a `_Point2` and `_Point3`. I can get from `PointM` to `Maybe Point3` using `^? _Point3`. I can extract a `PointM` from a `Point3` using `^. re _Point3`, but `re _Point3` is not a lens. Is it possible to set a `PointM` inside a `Point3` ? Also, when writting `_Point2` I realized I could always return a `Point2` even when z is Just. In a way `_Point2` could be a lens instead of prism. I guess both have different usage but is it possible to convert from `_Point2Lens` to `_Point2Prism` (or vice et versa ) ?
Honestly the hardest part for me was because several of my professors and peers did not understand that a) Monad was a typeclass and b) it was useful for things other than IO. I remember one of my professors saying, "Haskell is amazing if you can get your head around those Monads to do IO".
Yes. Proper API, well-aligned for legibility, arguments in an expected order, variable names following good practices. All these things signal something about this code. I haven't seen this function ever before, but know exactly what it does.
You need IO because otherwise evaluating some innocuous value could cause arbitrary side-effects. The monadic operations are simply what you need to operate on values within this IO construct. One gives you a way to inject an actual pure value into the IO framework. The other lets you touch the value that will be the result of running the side-effects, while remaining save and keeping the whole thing still inside IO.
I mean I guess those lenses dont deserve their own names, I just included them because of the parent post. But would anything be broken by saying that specializing lawful lenses preserves lawfulness?
What's broken with PointM3 for example ?
If you're talking about &gt; Is there any way to allow IO inside #| |# brackets? (With the result being an IO action that prints the string.) then this is not what meant in the question. You can already use formatted string as an IO action that should be printed and this is done [here](https://github.com/aelve/fmt/blob/c3b8dbb79f883ac1cdfeccbd8c27ee25e2ca56a0/lib/Fmt/Internal.hs#L87). See examples on [hackage](https://hackage.haskell.org/package/fmt-0.0.0.4/docs/Fmt.html) inside `main` function. This question is about allowing and understanding how to support something like this: "File exists: "#|doesFileExist foo|#""
Instead of sucking out happiness, they suck in knowledge. 
It somewhat reminds me of the ["Trees that grow"](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf) approach for extensible ASTs.
I think his point was that the Haskell report doesn't define the implementation of `IO`, so you can only do this in GHC, not Haskell in general.
Yes, that's the function in question.
That's basically what I want to say.
Edit: the answer below is not so much about mathematicians/logicians/type theorists, but rather about the people that develop the abstractions that are used when programming in a way that uses abstract concepts (typically Haskell's type-class hierarchy). I would say that it's a combination of all of the above, happening as a conversation between many people and on the span of many years. Many of the algebraic objects that are in use in programming today come from the development of abstract algebra in mathematics at the end of the 19th century, beginning of the 20th (most of the objects that started gathering interest among mathematicians, such as groups, are a bit less common than later concepts like monoids in programming). Monoids where heavily used in mathematical approaches to automata theory, so they were familiar to computer scientists before they were introduced as such in programming. Monads come from the second half of the 20th century, and were only introduced in programming in 1989/1990. The reason why abstract algebra and category are fruitful tools to find "good abstractions" is that they provide a catalogue of concepts that are known to have already been fruitful in other domains; if you are working on your own specific definition, and you find that some tweaks let you formulate it as a combination of concepts existing in these theories, it is likely that those tweaks are good tweaks that you should do. This generalizes the idea of "taking from other domains" beyond Curry-Howard. Note that while Curry-Howard had a large impact on the research on type systems and programming languages, it plays a comparatively smaller roles in the development of abstractions relevant to programming (in both its proof- and category-theoretic forms). Abstractions that are most useful in programming are those that occur the domains that our program is manipulating, rather than those that characterize our programming language itself. For a concrete example, look at Arrows: there were quite a few papers on Arrows and its properties (the question of the laws is crucial when proposing a new abstraction), but they did not all agree on each other. The first proposal was in 2000, but there were many laws and they were messy -- they did not look right. People played with arrows, understood them better, and in [2008-2010](http://homepages.inf.ed.ac.uk/wadler/papers/arrows-jfp/arrows-jfp.pdf) we started getting better formulations with nicer laws. Edit: in a more general sense I would say that "aesthetics" play a strong role in designing new abstractions, maybe surprisingly more than people that are not familiar with that process realize. Some people have a view of mathematical objects and programming abstractions strongly influenced by platonism, where the general idea is that those objects exist in an eternal (timeless) universe of concepts, and that we *discover* them as we go along thinking about stuff -- at any given point in time humans have only mapped a certain part of this universe. But it is important to realize that concepts are also, at the same time, *inventions*, they bear the mark of their authors. A theory developed by you or by me on the same domain may have many differences. A part of what happens in the year-long conversation that successively refines those abstractions is to attenuate those authorship aspects, ironing out some of the idiosyncrasies towards a more common presentation (but that's not all that happpens, there are also genuine breakthrough in simplifications and reformulations; the process of teaching is a large provider of such improvements) -- we also find out links/relations with other concepts from other communities that come with difference nuances and the mark of different authors (in a personal sense or as a group/community that shares a different perspective). The community's perspective, values and taste evolve with time, and the presentation of the concepts also changes. 
Elm and Purescript both kept Monads, despite being strict
I admit that `Arbitrary` is questionable *lawless* instance. For finite `a` the good choice for the law would be that `Gen a` provides uniform distribution of values `a`; the infinite (e.g. `Integer` and `[a]`) are difficult; there the good candidate would be to "provide uniform distribution of values under the size". We do "stringly" typed programming, so we do "inty" typed programming, and thus the definitions in `QuickCheck` are instance Arbitrary Int where arbitrary = arbitrarySizedIntegral shrink = shrinkIntegral instance Arbitrary Int64 where arbitrary = arbitrarySizedBoundedIntegral shrink = shrinkIntegral The `Int` instance is compromise. We have `length :: Foldable f =&gt; f a -&gt; Int` because it's practical, where the more correct type would be `... -&gt; Natural`. OTOH, `Arbitrary` is super convenient when I don't need to be explicit about generators; i.e. any `Enum`. Also quite handy to generic deriving of the instance for domain-specific types (e.g. records of 10+ fields). Also e.g. when testing `aeson`, it would be quite PITA to define generators for everything `aeson` supports. And then duplicate that work in e.g. `hedn`. What I'm trying to say; whether `QuickCheck`-like library has or hasn't `Arbitrary` class shouldn't affect your decision which one to use. One can write `Hedgedog-class` package; one don't need to use `Arbitrary` from `QuickCheck`: there is [`forallShrink`](https://www.stackage.org/haddock/lts-8.8/QuickCheck-2.9.2/Test-QuickCheck.html#v:forAllShrink), and in the next release there will be [`Arbitrary1` and `Arbitrary2`](https://github.com/nick8325/quickcheck/blob/e41506b3fb5835c777cadc8d150878466d835ac9/Test/QuickCheck/Arbitrary.hs#L269) so we can write generators for tuples with non-`Arbitrary` generators for tuple parts. I personally believe that there is uses for both: explicit generators and type-class based convenience. That for I'll do my best to remove &gt; Since all of these instances are provided as orphans, I recommend that you do not use this library within another library module, so that you don't impose these instances on down-stream consumers of your code. disclaimer from [`quickcheck-instances`](http://hackage.haskell.org/package/quickcheck-instances) so people writing data-structure packages could depend on `QuickCheck` and provide handy `Arbitrary` instances. [this issue is related](https://github.com/nick8325/quickcheck/issues/158). For example there is `Arbitrary (These a b)` instance in `these` package; 
I'm sure there are only so many good library names to go around for functionality like this, so I don't envy anyone who needs to come up with a name for a new formatting library. Anyway, there is a widely-used formatting library for C++ with exactly the same name: https://github.com/fmtlib/fmt.
I don't have an allergic reaction to it, but a reaction still. I haved used it in the past and even though it has some advantages u prefer to avoid it as much as possible. Doing a full polymorphic data doesn't convey much information and is only a language trick. Also sometimes, you are given a non polymorphic or non parametric data and it would be nice to go back and forth between similar type. For example I use Yesod and persistent. The data type have been generated for me so it's a bit awkward to parameterize them. On the other hand I need to be able to validate and display "invalid" data (for example having a text instead of a Int). Having everything parameterized by a functor is really helpful but doesn't work well with yesod and doesn't make sense outside of the validation part. Instead of having to chose between plain Vs parametric/polymorphic types I'm working at the moment on a little project using Template Haskell to generate types from another and it's conversions.
Implementing the abstraction in a theorem prover like Coq or Agda comes to mind. The interactivity of the environment lends itself to hacking and fooling around. Might not be the most rigorous approach, but a lot of scientific discovery got started with people just farting around :)
The final generalization is realizing you should've been using `linear` all along.
It was hard for me simply because it took forever to understand that a Monad was not a noun, but a qualified collection of verbs. Every piece of writing I found on monads referred to the individual data as "being a Monad"- like, this list is a Monad, this maybe is a Monad, etc. That obfuscated that the whole "point" was to talk about a way to generalize a category of operations, and not to try to describe a commonality of structure in the underlying data.
Right. But those are derivatives of Haskell. They mean we probably wouldn't have developed this pattern in the first place if we didn't have to due to the insanity of mixing IO with laziness.
How does Elm have Monads? It doesn't even have typeclasses. There are some operations that are monadic, but AFAIK you can't define Monads.
Your comment has absolutely nothing to do with Haskell. I suggest you to read both the read me in the repo and the medium post if you are wondering about what the road map for opening up the source of the other components looks like. Note that also the clients' code (ios, Android, web, desktop) is open. You can take your concerns with Wire directly too, @wire on Twitter. 
Lots of ways! My favorites are probably analogy and generalization. If there is a theory for a concrete thing, say lists, one might notice it behaves similarly to something else, like functions, and ideally a good analogy crystallizes the connection motivating the abstraction. Then you can use either special case as a model for the abstraction. I find that fun. You might also have a complete argument for something. Integrating over a sphere surface in a gravitational field counts the number of objects inside. That's neat, but maybe your argument required spherical coordinates, or perhaps your not so interested in physics in particular. But this argument seems to connect a couple of reasonably simple things, it can "count" things in a region somehow. One might see if they can generalize the argument, to "get at" what's really going on. Such an abstraction, if it exists, could be very useful. But that's only the tip of a very large iceberg! In general, certain arguments are discovered to be useful for "something", when this happens it's prudent to push those arguments as far as they go. Most really nice generalizations, we're found in a process similar to this I think. 
&gt; Ok, I was just trying to say that there isn't much content to be sought in monad after all. Please be kind with my English and accept the spirit not the form of my argument :P FWIW I think your English was good and your point was good.
Yeah, I think it mainly happens with Monads.
&gt; The state transformer takes a heap of size m + n and performs m minView operations to reduce the heap to size n. Yes, given that semantics, it does indeed make the most sense to have `m` be the first argument (since you're doing an induction on `m` in that semantics).
I've always thought of it as the state of the art at any given time is never perfect, but over time it approaches perfection.
It depends a lot on what exactly you have in mind by "has a correspondence". Mathematics can be applied in (at least) two different ways: expressive vs canonical. When using mathematics expressively, we're using the tools of maths in order to clean things up and make them look pretty. To do this, you first notice that whatever you're working on has some nice properties (e.g., "hey, this operator is associative"), and then choose to rephrase your work in terms of those abstractions (e.g., semigroups). There's an art to this, since you want to make sure that the abstractions pull their weight and actually help clarify things, rather than just sounding fancy but actually making everything worse. In this way, mathematics is just another "library" for programming. When using mathematics canonically, we're trying to show that some mathematical representation is canonical for our task. That is, it's not enough that the abstraction seems to be useful, we want to find out whether every conceivable X must be a Y, or whether every Y can be expressed by some X. This sort of work is more in line with denotational semantics, or with proving the soundness and completeness of a logic for a given model theory. In addition to looking for canonicity of mathematical objects, this is also what (at least certain areas of) canonical mathematics looks like. That is, this is the sort of work mathematicians do, more than what programmers do. (Type theorists, especially in academia, tend to do a bit of both.) But also, being the sort of work mathematicians do, we haven't figured out "the trick" to it. Sure, mathematicians have tricks and techniques they use to try to ferret these things out, but ultimately "it" is the human activity of discovering/creating maths. (One final note, it's often valuable to be able to express what cannot be said. That is, capturing what is forbidden is also a form of expressivity. So expressivity isn't just about what can be said, it's also about what can be unsayable. Thus, the expressivity vs canonicity distinction should not be conflated with dichotomies like soundness vs completeness, or necessity vs sufficiency.)
Seems [inevitable](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) that we would have reinvented them eventually.
I *think* it can mostly be proven if you impose two interchange laws: lmap l f . rmap r g = lmap (l . r) f . g lmap l f . rmap r g = f . rmap (l . r) g Proving the laws with these would probably require much deeper knowledge of parametricity than I have, and I wouldn't be surprised if those two laws can also be proven with parametricity. EDIT: You might need a nesting law too rmap r f . lmap l g = dimap l r (f . g)
Yea I agree that there's no reason a beginner needs to fully understand monads to move on in Haskell. I just meant that it's better to say "you'll understand later" than it is to say "look how simple that signature is!"
That's due to purity, not laziness. Any pure language will need monads as much as Haskell needs monads, even if it's strict like PureScript.
For Shake I wrote a custom pre processor that takes all markdown files and all haddock comments and puts them in a Haskell file and uses runhaskell on it. All custom code at https://github.com/ndmitchell/shake/blob/master/src/Test/Docs.hs. Writing something custom gives you more freedom and isn't too terrible (although is a pain and not reusable). 
Yes, you are right. I did not find it when i first looked. I think i will delete my root comment, because it is misleading. I am sorry.
If there's a negligible upside, then there is a strong argument against it, in that someone would have to implement and maintain the feature, and that it adds more edge cases to the language for everyone. You may not be looking for those kind of arguments, but I suspect that you would find them pretty quickly nonetheless :) 
Cabal invoke's GHC for you. For example, `cabal install` or `cabal repl`.
And everything else! I once stayed in a cabin with serious hedge hog damage. They'll literally eat a log.
Arguably, Haskell isn't pure. Any function could have `unsafePerformIO` or an FFI call hiding inside. Haskell's laziness makes the order of evaluation difficult to predict, which discourages people from writing impure functions.
Hi Koen, thanks for your comment. Firstly, I just want to say that I'm a huge fan of your work. QuickCheck has completely changed the way I approach writing robust software, so it's an absolute honour to hear your thoughts on the approach taken by Hedgehog. I haven't tried to implement generating functions yet, so I was not aware of (1), thanks for the heads-up. The downsides of the monadic style you mention in (2) are definitely problems which I have encountered with this encoding of shrinking, but I think it is a trade-off. The reason I decided to experiment with this structure in the first place was that I would often find my colleagues hunched over a laptop screen, with hundreds of lines of counterexample pasted in to an editor, trying to figure out why a property was failing. The problem was that they weren't writing `shrink` functions for their `Arbitrary` instances at all, and weren't interested in doing so. This is because you only get the payoff when a test starts failing, and my colleagues didn't have the penchant that I do for minimal counterexamples. Another common idiom I have come across is colleagues only writing `Gen` functions, and not `Arbitrary` instances, in order to avoid orphans. With these issues in mind, having a potentially sub-optimal shrink which you get for free, has out-weighed the benefits of the post-hoc shrinking which `Arbitrary` provides, at least for our team of lazy programmers :) Not to bad mouth the quality of the shrinks which Hedgehog can provide, in many cases they are better because of the invariants which can be encoded directly in the generator. &gt; In Quviq-style QuickCheck in Erlang, generators are specified as &gt; a mixture between monadic style and applicative style! This really seems like the ideal, I originally had an `Applicative` instance which had different semantics to `Monad`, but it was a little confusing and as you note, people tend to just write in the monadic style instead. Are you aware of any papers which discuss the problems with monadic style and integrated shrinking? &gt; This is because Erlang is untyped, so ANY structure with a generator &gt; embedded in it somewhere can be interpreted as a generator: A pair of &gt; (independent) generators is the same as a generator of pairs. Pretty &gt; neat, and a clear advantage of untyped languages over typed ones. Wow, this is really cool. I have noticed that the state machine testing relies on something similar, which is challenging to replicate in Haskell. &gt; Anyway, I just wanted to share my reasons for not making this choice &gt; when I wrote QuickCheck2 (in 2003). I think it's perfectly fine choice, and there are good reasons to choose either approach depending on what you're optimising for.
Is it easier to give an estimate based on a benchmark? (eg "I have completed the final project of the Haskell from first​ principles book")
&gt;This got me curious - what techniques do mathematicians/logicians/type theorists use to test if some new abstraction they thought of has a correspondence to theory? Mathematicians use examples just like anyone else! And honestly if you know the theory well enough, then you'll notice when the things you stumble upon are part of a broader class. &gt;Obviously for the well-known ones like functors and monads, you check if they obey the laws of their corresponding mathematical structures. Monads aren't particularly well-known outside of computer programming. But pretty much any mathematical concept that is published in academic journals will have a precise definition. &gt;the Curry Howard Isomorphism where you can check if an idea from one field has a correspondence in another. That only works on proofs/programs. If you want to do this in math, model theory is probably your best bet. &gt;If you invent an ad-hoc abstraction and it starts reoccurring in some interesting contexts, how would you check if there's something fundamental about it? If there were a general procedure math would be a lot more boring! It's basically mathematicians' job to do this, so well, they get plenty of practice. 
Haskell from first principles is a great book, I love it so far. I'm currently on chapter 11 out of 31 (algebraic data types) and only started about 2 weeks ago. However I did go through the first 5 chapters in a single day, so there's definitely a ton of slow down as the concepts get more difficult. I'm hoping to finish the textbook by June, to give an idea of how long it might take. (Being a full time student slows things down a bit)
I'm happy to see this officially announced! If you want to test GHC 8.2.1-rc1 with Stack, use this `stack.yaml`: https://gist.github.com/tfausak/a36862c53a2cc53029cab18a05788b95. Please note that it uses the `lts-8.8` resolver, which means you'll probably need `allow-newer: true` to get stuff to compile with `base-4.10.*`. Alternatively you can set the resolver to `ghc-8.2.0.20170404` and explicitly add packages to `extra-deps` (or use `stack solver`).
The windows64 variant seems broken though. A plain `stack build` fails with: &lt;no location info&gt;: error: Warning: Couldn't figure out C compiler information! Make sure you're using GNU gcc, or clang ghc.EXE: could not execute: C:/msys64/home/drydock/bin-dist-8.2.0.20170404-MINGW64_NT-6.3/ghc/inplace/mingw/bin/gcc.exe Seems the tarball uploader mis-configured something and made `ghc` look for `gcc` in a hard-coded path. After manually setting up a symlink for `mingw`, `stack build` fails with another message: Linking C:\Users\astro\AppData\Roaming\stack\setup-exe-cache\x86_64-windows\tmp-Cabal-simple_Z6RU0evB_2.0.0.0_ghc-8.2.0.20170404.exe ... C:\Users\astro\AppData\Roaming\stack\setup-exe-src\setup-shim-Z6RU0evB.o:fake:(.text+0x12d): undefined reference to `Cabalzm1zi24zi2zi0_DistributionziSimpleziSetup_replVerbosity_closure' ... collect2.exe: error: ld returned 1 exit status `gcc.exe' failed in phase `Linker'. (Exit code: 1) My system is Windows 10 build 16170. I resorted to firing up a docker container and using ghc-head provided by /u/hvr_ :)
I'm pretty sure that once you go lazy, you have to go pure.
Backpack is actually fairly orthogonal to most of the stuff in GHC. If you don't use it, you shouldn't notice it (perf or feature wise) at all.
That is unlikely the reason, as 8.2 includes the fix whereas 8.0 doesn't (yet).
Right. I see now that referring you to the ST implementation was wrong. You can find the Monad instance of IO in [GHC.Base](https://hackage.haskell.org/package/base-4.8.0.0/docs/src/GHC-Base.html) somewhere near the bottom of the page. bindIO :: IO a -&gt; (a -&gt; IO b) -&gt; IO b bindIO (IO m) k = IO $ \ s -&gt; case m s of (# new_s, a #) -&gt; unIO (k a) new_s ... unIO :: IO a -&gt; (State# RealWorld -&gt; (# State# RealWorld, a #)) unIO (IO a) = a &gt; What exactly is going on here? The entire point of `IO` and `ST` is to ensure that side-effects happen *once and in order*. It does so by passing a state token `s`. The state token is [magic](https://hackage.haskell.org/package/ghc-prim-0.5.0.0/docs/GHC-Prim.html#t:State-35-). The compiler isn't allowed to optimize it away, even tough it doesn't actually contain anything. Think of it like this: impure functions run their side effects once they get their hands on the state token, then pass it on to the next impure function. Many [GHC prim-ops](https://hackage.haskell.org/package/ghc-prim-0.5.0.0/docs/GHC-Prim.html) like `writeIntArray#` ([link](https://hackage.haskell.org/package/ghc-prim-0.5.0.0/docs/GHC-Prim.html#v:writeIntArray-35-)) are essentially just strict, impure functions. But without ensuring proper ordering of side effects these would often be too dangerous to be used directly, because laziness may change the evaluation order, in its never ending search to find the fastest evaluation order dynamically at runtime. In summary, `IO` doesn't run any side effects by itself. It's only there to keep the evaluation order of potentially impure functions safe and sane.
I think these should be enough? rmap r f = arr r . f lmap l f = f . arr l 
&gt; That only works on proofs/programs. If you want to do this in math, model theory is probably your best bet. How would you use model theory in this way? I've mostly focused my studies on proof theory, so am not very versed in the other. 
&gt; (of course after adding this you must nuke all your built snapshots and rebuild everything) Ie. remove all of ~/.stack first? Does this also mean the above won't work with `system-ghc: true`? 
Arguably the *join points* optimisation is the biggest change: https://ghc.haskell.org/trac/ghc/wiki/SequentCore - it represents a modest change to Core. - it results in some significantly reduced allocations: https://twitter.com/nomeata/status/826901568821850113 My understanding of *performance improvements* when it was first touted for 8.2, was shortening compile times, rather than improving code generation. So perhaps join points is not a consolatory feature, but certainly a welcome one! I for one would be really interested in how the join points optimisation improves the GC and runtime performance of real world Haskell code.
I've written a couple of scripts for pandoc (AKA "filters"), one of which can pipe the contents of code blocks through a shell command: http://chriswarbo.net/projects/activecode (forgive the broken "View Source" links; I'm in the middle of migrating the site to IPFS! EDIT: The links should all work now! The site is now available on IPFS at [/ipns/chriswarbo.net](https://ipfs.io/ipns/chriswarbo.net/projects/activecode) too :) ) I use this on my Web site, to do things like generating actual output from code snippets, generating graphs, testing whether code snippets actually work, etc. For example, I could type-check some Haskell via something like: ```{.haskell pipe="tee -a MyFile.hs"} module MyModule where import Foo define some = things ``` ```{.haskell pipe="tee -a MyFile.hs"} defineSome (More things) = here ``` ```{pipe="sh &gt; /dev/null"} echo "Testing whether we can compile MyFile.hs" 1&gt;&amp;2 ghc --make MyFile.hs ```
This is a known issue, see https://github.com/Microsoft/BashOnWindows/issues/1671.
Could you ELI5 this? Reading the wiki post it seems to be related to compiler details, which I'm not well versed on. 
[markdown-unlit](https://github.com/sol/markdown-unlit) lets you use a Markdown readme with Literate Haskell.
I mean what is a pure language if not a language with an IO construct? You could take every instance of IO and replace it with the word 'purity' and my point would still stand.
thanks! seems to work :)
The first one indicates that `$TopDir` was hardcoded into the config file instead of using the `$TopDir` variable as it's supposed to for release builds. During dev builds the paths are hardcoded. So this seems to indicate the binaries were incorrectly packaged. Second one is a tooling rather than a compiler bug. Does it happen with just cabal?
I'm fairly sure Giovanni implement compact normal forms and Ryan implemented deriving strategies. 
&gt; So perhaps join points is not a consolatory feature, but certainly a welcome one! Surely, if it is welcome or not in a "consolidation release" depend on to what extent it has any drawbacks, destabilize or increase the risk in any way - we should not just look at the positive side. 
Thanks for your efforts :)
Sandy! Just wanted to say how great this talk was. It presents the challenges very well, and what Eff offers to solve them. It made me excited to use it in my next project!
I think a talk about `Eff` should at least mention the downsides, namely poor type inference and performance. It's necessary to evaluating it for a real project, and it's better to not have unrealistic expectations.
That's a shame, but thank you
Really interesting project, but it seems that only about 50% of this video is real useful content. Much of it is personal things. I get that it's a stream, but not what I was hoping for.
For a while now I've been mulling over a system to tie tests to footnotes in documentation, such that when the test fails I automatically surface the relevant bits of documentation, letting tests serve as citations and helping catch stale docs. Tangential, but seemed worth mentioning.
If I'm reading [the source](http://git.haskell.org/ghc.git/blob/b1acb167b93f62eefab3f8cb24518eb0ce410d8c:/configure.ac#l1046) correctly, you can build a version of GHC without large address space support by running `./configure --enable-large-address-space=no`. Disclaimer: I haven't tried this.
It looks like you're trying to mention another user, which only works if it's done in the comments like this (otherwise they don't receive a notification): - /u/goldfirere --- ^I'm ^a ^bot. ^Bleep. ^Bloop. ^| ^Visit ^/r/mentionhelper ^for ^discussion/feedback ^| ^Want ^to ^be ^left ^alone? ^Reply ^to ^this ^message ^with ^"stop"
I don't think that's true. I don't think people would go wild with `unsafePerformIO` just because they could. They don't in PureScript, for example, which is strict
I'm fairly sure we can thank C for this. As a beginner, dealing with overloadedStrings and the 5 different string types in Haskell makes me want to pull my hair out sometimes figuring out where I need the :: type added in. (I've gotten much better at it, and it's not that bad). C on the other hand is beautifully​ terrifying with this; want this to be an int? Just kidding, it's either an int, a double, a float, or some other thing depending on where you used it and how you wrote things out. Then of course, there's Java and oop in general which provide inelegant solutions​ to already solved problems. I took a low level programming course and the majority of the course was "security vunerabilities created by casting, overflow errors, and illegal memory usage"; in both C and assembly of course. And then, we learned a lot about optimization hacks in assembly and C; loop unrolling, implicit tricks we can do that the compiler probably won't do for us, etc. If someone already learned those sorts of things and spent a career doing that, I can't imagine trying to convince them that type verification and strong type systems are a good thing...
Is there a writeup somewhere that discusses how this was implemented efficiently?
I'm confused why you want to quantify over the `r` as well as the `ty` in your `CodTy`? 
Check out the paper "Reflection Without Remorse."
Basically, model theory gives you tools to lift statements that make sense *logically* to another domain, ignoring semantics. Which is a good part of abstraction. 
[removed]
Get better at reading and implementing research papers. 
I love this series. Remarkably funny and insightful 
Indeed the Windows build has unfortunately regressed. See [#13560](https://ghc.haskell.org/trac/ghc/ticket/13560). I'll try to put a fix together and then perhaps I can release another `rc1` tarball for Windows to ensure that Windows users are able to test.
The relevant ticket is [#13411](https://ghc.haskell.org/trac/ghc/ticket/13411).
Indeed something went wrong here; I'm investigating.
&gt; Get better /s? There are a lot of "unknown unknowns" for me here, which is why I'm curious where to look to find what I need to study to build the foundation for papers to carry meaning.
Try building something moderately big in Haskell. You will learn advanced techniques as you run into problems and learn the Haskell solutions. Alternately, get involved in a large open source Haskell project. You'll end up reading through code, asking "what the heck is this doing", and learning in the process. Many Haskell papers are more relevant to those building GHC than those using it. Functional Pearls are probably the papers that will be most useful for learning the language itself.
Tons of well-known Haskellers have great blogs. Off the top of my head, * Dan Piponi / sigfpe (http://blog.sigfpe.com/) * Gabriel Gonzalez / Tekmo (http://haskellforall.com/) * Matt Parsons (http://www.parsonsmatt.org) * Oleg "from Finland" Grenrus (http://oleg.fi/gists/) * Oleg "liboleg" Kiselyov (http://okmij.org/ftp/Haskell) Learning category theory is definitely helpful. Sadly I don't (really) know any text that does anything nontrivial without (morally) assuming you know some abstract algebra.
Your username suggests that you're some sort of Bayesian artificial intelligence. The thought of a recursively-self-improving AI becoming as good as *Edward Kmett* terrifies me. 
I'm not entirely sure I understand. I installed GHC 7.10 with `apt-get` and ghc-mod with cabal and it was fast enough. Installing GHC 8.0.2 through Stack took 4 hours and ghc-mod is still churning at 59/60, 17 hours in, (but I'm getting there!) Maybe the time difference is caused by GHC 7.10 (`apt-get`and cabal) vs 8.02 (stack)
https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/ As someone with next to no math background I would say this is a decent, albeit steep, intro. Definitely still a lot I don't grok, but it's slowly helped me understand why we do certain things in Haskell the way we do.
Conor McBride's "Kleisli Arrows of Outrageous Fortune" discusses functors and monads over indexed types. In short, type p ~&gt; q = forall i. p i -&gt; q i class IFunctor (f :: (i -&gt; Type) -&gt; (j -&gt; Type)) where imap :: (p ~&gt; q) -&gt; (f p ~&gt; f q) class IFunctor f =&gt; IMonad (f :: (i -&gt; Type) -&gt; (i -&gt; Type)) where iskip :: p ~&gt; f p iextend :: (p ~&gt; f q) -&gt; (f p ~&gt; f q) You can define an applicative-like interface over a subset of this, data (:=) :: Type -&gt; i -&gt; i -&gt; Type where V :: a -&gt; (a := i) i ireturn :: IMonad f =&gt; a -&gt; f (a := i) i ireturn = iskip . V (=&gt;=) :: IMonad f =&gt; f (a := j) i -&gt; (a -&gt; f p j) -&gt; f p i m =&gt;= f = iextend (\(V a) -&gt; f a) m iap :: IMonad f =&gt; f ((a -&gt; b) := j) i -&gt; m (a := k) j -&gt; m (b := k) i x `iap` y = x =&gt;= \f -&gt; y =&gt;= \a -&gt; ireturn (f a) In general, applicative is only implied by strong monads, and `IMonad` does not generally have a strength.
He's got a great youtube series he's putting out regularly too: https://www.youtube.com/user/DrBartosz
This! As far as writing "useful" code, I find I'm falling into this trap frequently. (Granted, once I know i've been boilerplating something for which an elegant solution exists, I learn. But, what mistakes do I not even _know_ Im' making?) Plus, as mentioned in main post, I'm interested in solving abstractions, not just big commercial projects.
This talk inspired me to work on a way to decouple “handlers” (interpreters, sorta) from “permissions” (effects and coeffects) in a programming language I’m working on. :)
The definitive guide is "Towards no side effects" and is subtitled "From writing industry programs to massaging abstract lists without running any program and earning a life with it" Someday I will write it.
I'd suggest reading some category theory text books for mathematicians, e.g. Category Theory for the Working Mathematician. If you understand German, I'd recommend Martin Brandenburg's book.
https://yow.eventer.com/yow-2014-1222/stop-treading-water-learning-to-learn-by-edward-kmett-1750
In response to this I've [tweaked](https://github.com/hedgehogqa/haskell-hedgehog/pull/35) the way properties are run to make a macro free workflow a little nicer. Now you can do automatic discovery with macros: tests :: IO Bool tests = checkConcurrent $$(discover) Or specify your properties manually: tests :: IO Bool tests = checkConcurrent $ Group "Test.Example" [ ("prop_reverse", prop_reverse) ]
&gt; The GHC "API" is really an up-hill struggle, I'm pretty stubborn but I can't stand working with it anymore. Has there been any discussion about improving this? I keep reading stories of editor tooling people making valiant assaults on getting things working, but it seems like it's always very difficult.
Yes, but I will guarantee you that you will not be using Backpack unless you explicitly go about deciding to use it, given that Hackage doesn't currently accept package uploads of packages using Backpack ;) (Put them on http://next.hackage.haskell.org:8080/ for now!)
&gt; You can see how Foo depends on Bar, and Bar depends on Foo. Even UndecidableInstances won’t help us here. [...] we need to turn on UndecidableInstances to make GHC happy. This is backwards. UndecidableInstances isn't a flag like FlexibleContexts or RankNTypes which you turn on in order to lift an unnecessary restriction or to enable new syntax. Turning it on doesn't "help" you to write fancier code, and turning it on isn't a hoop which you need to jump through in order to make GHC "happy" when you use some advanced feature. Quite simply, UndecidableInstances turns off one of GHC's safety checks, and so you should use it if and only if you know that GHC is being too conservative. And it probably is, because this particular safety check is *very* conservative. The safety check is there to make sure you don't accidentally write non-terminating type-level functions. Unfortunately this check currently isn't very sophisticated, so if you do fancy things at the type level, you are likely to encounter an error message which mentions it. If you do, this does not mean that you accidentally wrote type-level code which doesn't terminate, it only means that GHC could not prove that your code terminates. If you want to proceed anyway, you must take on the responsibility of making sure that your type-level code terminates, and you must turn on UndecidableInstances to indicate your understanding of this new responsibility. Or you could just turn it on anyway to see what happens, because even if you mess up, GHC isn't really going to get stuck in a type-level loop, it's just going to give up with a "Reduction stack overflow" error. Here is a not-that-fancy type-level program which nevertheless requires UndecidableInstances: {-# LANGUAGE TypeFamilies, UndecidableInstances #-} type family IdUnit1 a where IdUnit1 () = () type family IdUnit2 a where IdUnit2 a = IdUnit1 a unit :: IdUnit2 () unit = () I implement a specialized type-level identity function which converts the type `()` into the type `()`, I implement a second specialized type-level identity function which delegates to the first, and then I apply this second type-level identity function to the type `()`. The resulting type, obviously, is `()`. And so `unit` type checks. But in order to determine that it type checks, GHC first has to expand `IdUnit2 ()` to `IdUnit1 ()`, and then it has to expand `IdUnit1 ()` to `()`. That's just two steps, so it doesn't looks like a big deal, but the problem is that you don't know in advance that evaluation is going to stop after just two steps. If each function delegates to the other, for example, they get stuck into a loop and I get the aforementioned reduction stack error: type family IdUnit1 a where IdUnit1 a = IdUnit2 a type family IdUnit2 a where IdUnit2 a = IdUnit1 a -- Reduction stack overflow when simplifying the following type: IdUnit1 () unit :: IdUnit2 () unit = () One final note: while GHC's safety check sometimes seems overly-conservative, it actually works quite well with the kind of type-level recursion which Haskell programs most often encounter, like `instance Monoid a =&gt; Monoid (Maybe a)` and other cases in which the recursive call is on a strictly smaller type. It even handles mutually-recursive type-level functions! {-# LANGUAGE DataKinds, TypeFamilies #-} data Nat = Zero | Succ Nat type Three = 'Succ ('Succ ('Succ 'Zero)) type family Even n where Even 'Zero = 'True Even ('Succ n) = Odd n type family Odd n where Odd 'Zero = 'False Odd ('Succ n) = Even n unit :: Odd Three ~ 'True =&gt; () unit = () 
I'm not seeing base-4.10.* on the base hackage ( https://hackage.haskell.org/package/base ). Any idea when it will hit?
What makes it so bad?
Thanks for that one, I saw it in Brisbane, and it contributed to my decision to stop being in management and go back to pure technical work, including going back to university part-time to do an MCompSci, which now has me writing a (relatively small, since it's one course in a coursework master's) thesis on event sourcing, a deeply rewarding experience.
This is some high quality hasochism!
It won't be on Hackage until 8.2.1 is officially released. 
Nice one too. I thought about the idea of a mobile app + web API, but might keep it as a little side thing and move on to something else.
"Learning Category theory" is precisely one place where /u/edwardkmett 's bi-directional (i.e feynmann + co-feynmann algo) approach I think makes most sense. The problem with vanilla category theory is that it is just a bunch of definitions and has a tendency to evaporate out of the brain without the stickiness of a concrete application. Functional programming gives this stickiness. For mathematicians they have their own sticky glue - algebraic geometry for example.
I tried to use these bindings a month or two ago. It seems to be tied to an old version of GHC, and our project is already on 8.x, but even aside from that, I couldn't get it to build and run in the first place. To be fair, getting TF to build at all even aside from Haskell is a nightmare. As much as I want it to be good, I simply cannot report having any positive experiences with anything related to TF.
*her, she's a witch
I think that what's wrong is that you cannot "lift" a value level function to type level. I'm wondering if there is a theoretical limitation for that or if it is simply not implemented (yet?).
(and you get a static solution)
Well... That's insane. Thank you ;)
No problem. Just put the whole thing in a shell script that interpolates N into the program string and pipes it into GHC.
That's strange. Do the box-drawing characters (e.g. \226\149\159) also work in ghci?
RemindMe! 10hr
How I missed this thread? This kind of insubstantial poetry that sings a particular kind of technology, that repetitive white noise that is sterile and produces nothing useful is the essence of the sectarian environment in which the Haskell community is being dying. Less and less practical results are produced while more and more bad technobabble, white noise or bad poetry, call it as you like, visiting the same common topics, even repeating the same ritual phrases. "Haskell for the shake of itself" is a sign of gnosticism, a form of primitive cult. Gnosticism is ever made in each age with the accretion of stuff from his time. In this time, technologism is the form of gnosticism made with the stuff of the current technological age. The purpose of gnosticism is the creation of a primitive, informal, but strong hierarchy of power based on esoteric (hidden) knowledge that has been "revealed" to some special people, the priests, who occupy the top level of the hierarchy and are worshipped. That "Knowledge" is proclaimed, but hidden by means of the pronunciation of strange words in obscure phrases. But in the deep hides a trivial meaning or no meaning at all. it's all in the imagination of the deceived. The deceived occupy the bottom level and spend his efforts working for the sect with the promise of ascending in the hierarchy of power and knowledge under the promise of having access to the true meaning of the secret. There's little knowledge, if any, but certainly there may be much power to gain, depending on the engineering of the whole invention. However that cult can be engineered, like in the case of scientology, a techno-gnosticism brilliantly confectioned by R. Hubbard, an Sci-Fi writer, gnosticism appears in most cases spontaneously as a consequence of the combination of the attributes human nature operating over the current unsatisfactory worldview of the time, when it is distorted by an ideology made of pure desire for personal emancipation, that pretend the transformation of the status quo and is made respectable and profitable as a tool for power seeking by ambitious individuals. some ideologies comes to mind, but it is not my purpose to being explicit about that. In fact Scientology can be also considered in the second case, since Hubbard created it at a time where OVNI abductions were at the peak, and some spontaneous sects were being spontaneously appearing. He made use of this stuff to create his own gnostic religion. Raelianism is a parallel case. In this sense Haskellism is in the spontaneous phase, but this does not mean that it is harmless: The first consequence is the encryption and sclerotization of the knowledge, and the lack of advancement since it no longer is a tool subject to improvement, but a ritual element that is repeated with reverence. Incidentally, a sign and a consequence of gnosticism of all ages, due to the belief in the promise of the transformative power of the hidden knowledge, is the despise of the real world -as it is made currently- since it is corrupt and flawed, and a longing for purity. Does it sound familiar in the Haskell community? The author frames it perfectly in the introduction when he mention that kind of primitive religiosity in which implicitly he includes itself. I can not find better evidence, and the match of the facts with the theory is almost laughable. This case will be in the textbooks of a future history of religion. I will write soon an article about the gnostic cult called technologism and, in particular, Haskellism. This article is a perfect piece of it, and the warm receipt in this thread is very, very enlightening
That's not what is meant by free monad. It's just a normal monad, and a pretty boring one too. The free monad is one that lifts any Functor to a Monad, just like how lists imbue any type with a Monoid instance. Lists are `* -&gt; *`; "give me any type, and i'll return a Monoid (*)". The free monad is `(* -&gt; *) -&gt; (* -&gt; *)`: "give me any Functor (`* -&gt; *`) and i will return a Monad". `FileCmd` is in no way related to the free monad, any more than any other monad is. It isn't a function from Functor to Monad....it's the wrong kind, and wrong everything else, really, heh.
Haskell is pretty limited in dependently typed development, even if you could "techically" implement a lot of it. Idris-the-language has a lot of tools for making proof-writing and automated proof-searching and proof generation very easy, where Haskell doesn't, so the language and compiler don't really help you at all. This is probably the single biggest fundamental issue to me. In addition, working with type-level numerals in haskell is a bit of a pain; if you want inductive proofs, you can use inductive Nats, but those have pretty high performance costs for most operations (whereas Idris optimizes it away by using Integer internally). For performance, you can use TypeLits, which basically creates a new type for every numeric literal, but the types aren't related to each other in any structural way, so inductive proofs don't really work. Idris also optimizes a lot of proofs away at runtime; GHC isn't smart enough to do that. I suppose this isn't a fundamental issue. Those are sort of the main fundamental issues...with singletons, you can sort of have a disciplined (yet ugly) system of having types-as-values and manipulating types as values, but even with singletons, those issues remain.
One mans abstraction is another mans boilerplate. All large enough projects contain boilerplate, it's OK if it is at the right level. Aren't we worrying about boilerplate too much? Correctness, usability and efficiency are more important, imho.
What the fuck did I just read?
Well you can still run algorithms via instance search in dependently typed languages, and it still has the very odd untyped-feel of being able to perform decision procedures on types. [Here's a product normalisation algorithm I wrote running via instance search in Agda](https://identicalsnowflake.github.io/ProductNormalisation.html). Granted, that's like second-year Slytherin tier compared to the Bellatrix in the OP above, but it's using some of the same principles.
&gt; repetitive white noise that is sterile and produces nothing useful 
Fair enough. But even if we ignore that part of it, whether by accepting instance search as a widespread and necessary evil, or by admitting that it's actually a good thing for... reasons, that leaves enough ickiness in Haskell's treatment of type-level computations to make my point still stand. The fact that the problem is resolved logic programming-style is tangential to that, imo.
\&gt; 2017 \&gt; fundeps
I immanentized the eschaton and all I got was this lousy compilation error.
You mean hidden knowledge, except for dozens of book, hundreds of blog posts, fora, university courses, lectures, videos, irc, and a friendly community always happy to help beginners and advanced alike?
I cite myself: That "Knowledge" is proclaimed (this means, publicized), but hidden by means of the pronunciation of strange words in obscure phrases. But in the deep hides a trivial meaning or no meaning at all
the metaness of this is truly ironic.
Sooo, you no longer need to define your "cute" peano numerals because you have built in type level Nats. You don't need to define type level lists, because they're built in and use almost the identical syntax to the value level ones. You don't need to use classes with fundeps for type level computation because you have type families that read exactly like value level functions. You don't need to deal with the "dynamic language" because you have DataKinds and PolyKinds which let you arbitrarily constrain your types. You can even write type classes that operate on kinds. Hard to read error messages? Guess what, you can use custom type errors now. Basically everything you mentioned is about a million times nicer now (still no currying tho). Of course, you're still right about it being a separate language and dependently typed languages showed us this doesn't have to be the case, but your argument becomes SO much weaker when you take into account modern Haskell. As for the type system constraining values, this all lets us constrain our values more precisely. Basically, while the core Haskell language is neat, it's now true that the core Haskell type-language is also pretty neat. Although still a separate thing from Haskell itself. 
I think you might be able to do that by coming down to the term-level with singletons.
Nice markov chain bot.
What is it about Scala's partial functions that your friend misses in Haskell? Where does `a -&gt; Maybe b` fall short? As tomjaguarpaw comments on the gist, is it not easier to use `a -&gt; Maybe b` directly? 
Ah ok. Thanks! I was having a similar idea after writing the original post (after all it feels the same trick as with `printf`), but it was not clear in my head. 
&gt; But in the deep hides a trivial meaning I agree. haskell isnt that hard.
Having the audience be "people with prior reflex experience" makes this not for me. Cool though. I'd consider adding intro chapters so that you can target people without prior reflex experience as well though.
&gt; Consultant is a co-Genius I could only find this: https://twitter.com/alios/status/575053997133926400 what are you referring to? is it in the video? (about to watch...)
nothing is more practical than a good theory
&gt; CRDTs ... Lattices Your post is motivating since I wasn't sure if the math was something I needed to dive into before this problem would seem more intuitive. I have the gist, but I think it's time now I buckled in and gave myself a math education :) 
Wow. I thought I understood Agda, but I guess I'm only a first-year Slytherin then! Can you explain this black magic?? *edit*: I should note that I understood the "Belatrix-level" stuff perfectly well, so no need to re-explain that part of instance search. My main questions with your code are the use of `_` as types (I usually see it for filling in a value with a known type, not for filling in a type with an unknown kind), and where the `x` comes from in the `i {x}` instance.
In limited situations, it may be (asymptotically) less work to determine if a value is in the domain without also calculating the result if the value is in the domain. If the `Maybe` / `Option` type is strict, this means that it there may be real operational advantage in exposing an `isDefinedAt :: a -&gt; Bool` function rather than pattern-matching on the `Maybe b` result. Even in Haskell, depending on how you write your code, reducing the `Maybe b` to WHNF might involve more work than is strictly necessary compared to reducing the `Bool` to WHNF. But, because of pervasive laziness it's actually really easy to write code where that is not the case; i.e. where composing `isJust` with your `a -&gt; Maybe b` is exactly the same operationally as having a separately defined `isDefinedAt` function. --- Note that having a separately defined `isDefinedAt` function also introduces the possibility of having semantically invalid partial functions. I.e. it's certainly possible in Scala to have a partial function is says it is defined where it is not, or vice versa, or both. In a dependently typed language you could carry around a proof term that ensured `isDefinedAt` and your `a -&gt; Maybe b` where consistent, but last time I was looking, the Scala standard library didn't even attempt to do that.
Maybe is a collection of zero or one elements. Why is strange to be using traverse? 
my first thought too, I faced a similar problem at the boundary between my types in a servant-backed app, and Persistent (_and_ what I sent to over the wire, _and_ what they sent to me for example for PUT operations where they might only want to update one out of many fields, so now all those fields are Maybe?)
Not to spoil the joke or anything, but the entire prompt of this series is "answer a mundane interview question in a magical way." It's supposed to be arcane, bizarre, and hard to follow. That's why the previous one found a cycle in a linked list by writing JVM bytecode directly, and the first reversed a linked list by defining a Church encoded variant. The techniques used in this post are a bit dated, as well -- a more modern implementation would be a lot cleaner. I think everyone agrees that dependently typed programming in Haskell is painful -- thus the term "Hasochism."
I have a couple of talks on the CRDT front about propagators: https://www.youtube.com/watch?v=acZkF6Q2XKs https://www.youtube.com/watch?v=DyPzPeOPgUE The former is far more accessible, but "obvious." The latter dives down into a ton of Haskell tricks and is more of a stream of consciousness.
&gt;Is there any way to get the "short-circuiting" behaviour of a Maybe without resorting to a `traverse` `forM` or `mapM`? It just seems **very weird** to me to use these functions on a Maybe which is not really a collection type! A rose by any other name... I think it would be very unfortunate to lose extremely useful functions such as `traverse` due to a difference of terminology. You might find it helpful to think of `traverse` not as a way of walking across a collection (though it is *also* that, of course), but as another kind of mapping, which given an `a -&gt; F b` function for some `Applicative F` can lift it through some other functor.
I wrote a unification algorithm in my AI course in college in Haskell, and it was able to determine that there was a match before doing any variable matches. I have no idea how, just that trace debugging happened *after* it printed out `Right`, and no traces were printed in the `Left` case.
The biggest obstacle for me trying out reflex is how ridiculously hard it is to install it. The only way is nix platform. And it does not work on my distro (Arch) I wish there was a stack configuration instead or at least a docker image. 
Thanks, that is useful. I was thinking of something simple like moving objects within a bounded grid and having some types that can be in multiple states, so I'll give it a go and see what problems I hit. Out of interest I'm assuming that ugly and unperformant types in Haskell is more practical (in the short term) than trying to port my hobby site over to idris (or scala :o ) 
*please downvote so this collapses* - Richard Feynman - Feynman's Algorithm 1. Right down the problem 2. Think really hard 3. Write down the solution - What is the cost of using the _wrong_ solution over your entire career? - we don't _know_ what we don't _know_ - The point of the talk is _how do we learn what we don't know, and what we don't know we don't know_ - How to be a Genius &gt; Keep a dozen of your favorite problems [...] present in your &gt; mind. Every time you hear a new [solution], test it against each &gt; of your twelve problems to see whether it helps. - Richard &gt; Feynman - Dual of "How to be a genius" - = consultant, the exact opposite of a genius - Development vs Research - search from both ends - if your job is searching through a space for a solution, start from the dev side and research side and meet in the middle. - a developer is paid to solve a problem - a researcher has a solution they need to raise money for (...?) - Hold on to half a dozen solutions and half a dozen problems - this optimizes you for solving problems, just not any specific ones - Memory has an exponential decay - decay is slower if you review, keep revisiting things you've learned - Spaced Repetition - Instead of using a Spaced Rep. app, Kmett maintains a ton of libs. Bug reports induce his spaced repetition, since he needs to go back and relearn stuff. "If you're willing to maintain, you can't forget it." - "Iterative Deepening" - Phd is very much a depth first search, hopefully you picked the right topic to study! - BFS means, you've gotta be able to remember everything which is infeasible - Imagine a chess AI, more time = better move, but time is bounded. It must make sure it always has _some_ move to offer. That's a good way to explore the search space, go one tier and find the best options, then go the next tier, etc. &gt; Whenever you come back to a topic, go deeper. -Edward Kmett - A dumb person who can decompose problems is worth more than an intuitively smart one who can't. - Tips - Learn the vocabulary &gt; The purpose of abstraction is not to be vague, but to create &gt; a new semantic level in which one can be absolutely &gt; precise. - Djikstra - be willing to break down what the jargon _means_ so that others can enter the domain space - Alexander Grothendieck took Serre's math and expanded/digested it for everyone else (IE 2 page proofs -&gt; thousands of pages of cat. theory) - Grothendieck's method can perhaps be taught, where as Serre's, and Feynman's cannot - Interestingly, Grothendieck was only active for ~4 yrs. - 3 Approaches, as analogies - Serre could crack any walnut to get at the meat. (He was super crazy smart and could just see which the right tool was for the job) - Grothedieck would "soak" a walnut to soften it up until he could get at the meat. (Analagous to taking a problem/solution and running with it's basics until the structure unfolded itself). - Feynman would throw his walnuts on the shore, and the tide would soak em all up simultaneously, until he could find which ones were yielding. (Kmett gave a quote from Grothendieck offering this advice, but to me, it seemed to apply more to the Feynman approach, so, artistic liberty ;) ) - Lens: an example of soaking - in order to solve getters/setters without mutation, a whole panoply of concepts is derived (Prism, Traversal, Review, ...) - Zen and the Art of Library Maintenance - see note about Spaced Rep, Kmett's way of achieving spaced repetition - Walk away - Saunders Mac Lane, highest marks at Yale, tried his hand at Logic, sucked, walked away and into Cat. Theory, huge success! - Walk away not when it's _hard_, but when it's not _productive_.
How would that work? Let's look at a simpler version of the problem: {-# LANGUAGE DataKinds, FlexibleInstances, FunctionalDependencies, GADTs, KindSignatures, MultiParamTypeClasses, UndecidableInstances #-} import Data.Proxy data Nat = Z | S Nat deriving Show class Twice (n :: Nat) (r :: Nat) | n -&gt; r instance Twice 'Z 'Z instance Twice n r =&gt; Twice ('S n) ('S ('S r)) How can we run Twice on a value-level Nat? We can use singletons to obtain an equivalent type-level `n`: {-# LANGUAGE RankNTypes #-} data SNat n where SZ :: SNat 'Z SS :: SNat n -&gt; SNat ('S n) raise :: (forall n. SNat n -&gt; a) -&gt; Nat -&gt; a raise cc Z = cc SZ raise cc (S n) = raise (cc . SS) n And if you can somehow produce a `SNat m` from that `SNat n`, you can get back a value-level answer: -- TODO: instead of producing an (SNat ('S n)), -- produce an (SNat m) such that (Twice n m) holds twice :: SNat n -&gt; SNat ('S n) twice = SS -- | -- &gt;&gt;&gt; raise (lower . twice) (S (S Z)) -- S (S (S Z)) lower :: SNat n -&gt; Nat lower SZ = Z lower (SS sn) = S (lower sn) But how do you write a proper version of `twice`? What is even its proper type? *edit*: I've figured out the proper type, I need to use continuation-passing-style in `twice` as well. twice :: (forall r. Twice n r =&gt; SNat r -&gt; a) -&gt; SNat n -&gt; a twice cc SZ = cc SZ twice cc (SS sn) = twice (cc . SS . SS) sn -- | -- &gt;&gt;&gt; raise (twice lower) (S (S Z)) -- S (S (S (S Z))) lower :: SNat n -&gt; Nat lower SZ = Z lower (SS sn) = S (lower sn) But this solution merely duplicates the type-level implementation at the value-level, in a way which guarantees that it computes the same thing. It does not lift a value to the type level, execute a computation there, and then bring the result back to the value level. I don't think doing so would even make sense: the type-level computation is executed at compile-time, so there's no way to send the runtime value back in time in order for the compiler to re-run its compile-time computation with the correct input.
The core of Reflex itself is not hard to install and get going, it's the DOM and GHCJS related parts which require nix-shell. I have an example [native Reflex app](https://github.com/deech/fltkhs-reflex-host) that avoids those dependencies and it's pretty easy to install. That said the `try-reflex` shell script in `reflex-platform` just worked for me as well and I'm running Arch same as you.
[removed]
I have miniscule reflex experience and found it valuable. I feel a lot more comfortable reading the intro material now too
Kind classes, too, though type in type makes them just type classes again 
Arch nix installation is currently broken: https://github.com/NixOS/nix/issues/879 It did work for me more than a year ago. But since then things have changed. &gt; it's the DOM and GHCJS related parts which require nix-shell. Which is weird considering it was worked out quite some time ago: https://docs.haskellstack.org/en/stable/ghcjs/ There are recent stack configs both for 7.10.3 and 8.0.1 ghcjs branches. 
cabal-newbuild and stack are tools for building Haskell projects. These have been discussed at length, for example in this thread: &lt;https://www.reddit.com/r/haskell/comments/4hiwi0/announcing_cabal_newbuild_nixstyle_local_builds/&gt; Hadrian, however, is a project to replace the build system for _GHC itself_. GHC currently uses a custom build system by putting together different tools. This makes it fairly complicated, see e.g.: &lt;https://ghc.haskell.org/trac/ghc/wiki/Building/Using&gt; You (probably) won't be able to use Hadrian for your own projects.
I very much concur. So far I've only managed to incorporate it into one of my projects, after much difficulty. IIRC first I was using it through `./try-reflex` and later on I started using `./work-on`. I'm not on Arch though, but rather a Debian derivative, so that may make it easier. And that leaves me to use `threepenny-gui` in one of my other projects, just because I couldn't make `reflex-dom` be compatible with `haskell-stack` for the life of me. And even that `work-on` workflow is flawed in my case: I have two dependencies that I need to comment out of my cabal file every time I invoke ``~/sandbox/reflex-platform/work-on ghc `pwd` ``. One is a local dependency on another package added through `cabal sandbox add-source`. The other one is a package that doesnt seem to be part of nixpkgs: [haquery](https://hackage.haskell.org/package/haquery). Or at least not the version of nixpkgs work-on is trying to use. error: anonymous function at /nix/store/nm4nh3sx7qnyk6f22p6h6pgk1cp82rd0-cabal2nixResult:1:1 called without required argument ‘haquery, at /nix/store/5kqp52f7p2mbhlpp8m7bs83j8wz0czw4-nixpkgs-channels-09c3d04b0e53d65f60569fc01698decff3a657a3-src/lib/customisation.nix:56:12 So I'd like to take this chance to ask: - Has anyone run into this or a similar issue with reflex-platform/nix/cabal2nix? - And/or might anyone know a way to get around it? 
Hmm. If that is the case I will delve deeper when I get the chance. It's just that the "for people with prior reflex experience" part was right at the top of the readme.
I remember reading something about how it increases compilation times significantly though, which does not fit with the original plans. Has this been resolved already?
work-on allows local dependencies: https://github.com/reflex-frp/reflex-platform/blob/develop/HACKING.md An example `overrides-ghc.nix` file: https://github.com/imalsogreg/servant-reflex/blob/master/overrides-ghc.nix You would run `.../work-on ./overrides-ghc.nix ./.` to get into a nix shell with the ghc dependencies. Another file in that project (`overrides.nix`) has the ghcjs-specific overrides.
Wow, the Arch issue seems like a huge pain! I can't believe it's been open for so long. I added [a note](https://github.com/reflex-frp/reflex-platform/blob/develop/notes/ArchLinux.md) to reflex-platform. We've also begun work on reflex-frp.org, and I think it would be a great idea to include docker images or other VM-based solutions there. I've created [a ticket for that](https://github.com/reflex-frp/reflex-frp.org/issues/7). I also think we should provide better support for stack-based workflows. I don't use stack personally (everything I do is nix-based), so if anyone is willing to spend some time helping me out with this stuff, that would be really appreciated. Ticket [here](https://github.com/reflex-frp/reflex-platform/issues/116).
The best way to deal with this currently is to create a .nix file with your overrides in it, as described [here](https://github.com/reflex-frp/reflex-platform/blob/develop/HACKING.md). In the `overrides` section, you can write something like: myPackage = self.callPackage (reflex-platform.cabal2nixResult ./path/to/your/package) {}; haquery = self.callPackage ({ mkDerivation, base, containers, parsec, split, stdenv, tagsoup , text, transformers }: mkDerivation { pname = "haquery"; version = "0.1.1.3"; sha256 = "15ihh10whhzcv0i7vm3aj4g5zdf818w06wix2vp8yqp6pragc1va"; libraryHaskellDepends = [ base containers parsec split tagsoup text transformers ]; homepage = "https://github.com/crufter/haquery"; description = "jQuery for Haskell"; license = stdenv.lib.licenses.bsd3; }) {}; I got the first argument to self.callPackage for haquery by running `cabal2nix cabal://haquery`. This isn't nearly as smooth as I'd like it to be, but I believe it should get the job done in this case. (/u/imalsogreg's [comment](https://www.reddit.com/r/haskell/comments/64r5on/realworldreflex/dg4rc2n/) also covers this well)
Awesome! Thank you!. I'll try it ASAP. 
&gt; For performance, you can use TypeLits, which basically creates a new type for every numeric literal, but the types aren't related to each other in any structural way, so inductive proofs don't really work. Apparently, there are [typechecker plugins](http://hackage.haskell.org/package/ghc-typelits-natnormalise) that add some bells and whistles to TypeLits.
Ed has contributed more practical libraries than most other Haskellers. His code is in basically every other non-trivial Haskell project.
&gt; you would like generators to be monadic Why would you like this? Can you explain what you mean? I don't think I understand what you're saying.
Not sure. I think it's just a prebuilt python distributable
And that's why I'm saying intuition and analogies shouldn't be trusted blindly. They might not match the mathematical definitions.
I have a project setup to simply use stack to bootstrap GHCJS and pull in pinned commits of reflex/reflex-dom if anyone is curious - https://github.com/alasconnect/reflex-material
&gt; When to use each one? &gt; - If you need to know when something happens...use Event &gt; - If you need to be able to get a value at arbitrary points in time...use Behavior &gt; - If you need to see something's value at arbitrary times AND know when it changes...use Dynamic &gt; It really is exactly that In which cases you need to know a value at arbitrary times when programming a web application? 
It ends up coming up a lot; for instance, consider `dynText :: MonadWidget t m =&gt; Dynamic t Text -&gt; m ()`. When it is first constructed, this widget needs to get the initial text it should display - that comes from the Behavior aspect of the Dynamic. Later, if the Dynamic changes, it needs to get notified so that it can update the DOM - that comes from the Event aspect.
I agree with all of this, to the extent that I'd be fine with linters warning about `type` when the definition is just a single type.
You dislike them because sometimes they're necessary?