&gt;I focussed to much on the sexual connotations of "attractive" Yes. And so did probably Tim wrt Doaitse's use of the word. And that's what I'm talking about, some people are too easily offended and see sexism or other unwanted things nearly everywhere, they start with unfavorable interpretations of what people said or did and end with accusations based on their speculations.
...and now I'm looking forward to the xmas-2012/early-2013 GHC 7.8.1 release :-)
People like *what* have a problem? People on a mission to make the community more inclusive, who are looking out for harmful behaviors, and that don't mind if perpetrators of harm wind up with hurt fee-fees? They sound like good people to have around.
There's no reason not to do this in GHC now. So why don't we? 
AFAICT, both Simons use almost exclusively Emacs (for coding).
They're working on it. I thought it was planned for 7.6.
The nub function is O(n^(2)), I wouldn't use it unless absolutely necessary. I think you're using it to clean up sloppiness in the mid/midL/midR concept. Add a `| a &gt; b = []` case, get rid of nub, midL and midR, and make sure each index is produced only once, and I think you'll be set. 
I wonder what the singing was all about in the next room. http://www.youtube.com/watch?v=2Pucbf8_hGQ&amp;t=21m50s 
I don't think this is entirely true. 
They're playing thought police and accuse people of things like "subtle sexism". They don't make the community more inclusive but homogeneous according to their narrow idea of "ethical behavior".
If it's not, then the internals of both the `IO` and `ST` monads in GHC need to be rethought, but I don't think that's the case. Perhaps `RealWorld` should be renamed to `Global`, in order to better reflect its purpose.
Yeah, that's the same answer I got. I thought maybe there was a different answer due to the 'bonus' bit where it claimed the 'forall r' was required. I changed to 'known return type C r m a' and the only difference was that the type of `abs` had to get specialized to `C a m a -&gt; m a`.
 {-# LANGUAGE DataKinds, KindSignatures #-} data Region = RealWorld newtype ST (s :: Region) a = ... type IO = ST RealWorld
I suspect that this algorithm has been independently discovered by many people. Consider problem 204 of Project [Famous Mathematician]. The Dijkstra algorithm would be slaughtered by duplicates.
There was a meeting for teachers of danish supplementary education for children in the middle school age (that the children attend in their free time). The song is classic danish folk song called "Marken er mejet", that celebrates the harvest. Here is a short version: http://www.youtube.com/watch?v=BBfZiCYqPZg
_Hit it Freddie!_ It's a kind for regions, It's a kind for regions, A kind for regions, One dream, one scope, one type, One goal, one golden glance of what should be, It's a kind for regions... (To be fair I ran out of steam at this point.)
The only thing I can think of is ghc 7.6.1's `Nat` kind and use of `sing`.
I'm moderately unhappy about Safe Haskell. As a library author, it's a new, not-quite-finished thing I'm being asked to pay attention to and understand, for which it honestly seems to me that nobody except its developers have any demand. As a person who is somewhat interested in security, I look at the landscape of smoking ruins that is all the language-level security mechanisms that have gone before (I'm looking at you, Java and Safe Tcl!), and wonder what Safe Haskell is supposed to be giving us that really can be relied upon. Worse, I don't understand or trust Safe Haskell, but if I accept a patch to a library I maintain that frobnicates the diddlyknobs Safe Haskell needs, and a user of my library runs into a security problem, I'll feel responsible for fixing it even though I've no idea what the machinery is doing. So while I like to be encouraging to people by accepting patches, I kinda don't want these. But mostly I want to file it into my box of things I don't care about or pay any attention to (which is how I treat most new Haskell extensions), but I don't feel like I can, even though it doesn't seem to actually merit my attention.
Hah! I forgot that. So it's actually not much of a change at all.
Thank you, you're absolutely right!
Hahaha. That's pretty funny coming from the fake account spending all this effort to come here and whine. :] Pretty much everything you've said here boils down to "people are mean to me and I'm upset about that" so you complain about everyone else being easily offended. Man. Project your own insecurities much?
My impression of Safe Haskell--which nothing in this article seems to contradict--has always been that it essentially locks down all the usual "escape hatches" that are otherwise available and blocks Template Haskell and certain potentially sketchy things with instances. In other words, it's fundamentally *not* a security mechanism, except insofar as you might consider pure functions to be a form of security. Using `fireZeMissiles :: IO ()` would be perfectly "Safe" by these standards, whereas `Debug.Trace.trace` is not.
&gt; I don't understand or trust Safe Haskell... Yeah, I have no zarking clue what this does either. Maybe it would be useful for implementing tryhaskell? But there's already a system for that.
But he said "I find several people in the Haskell community attractive. Don't you?", that's not "the Haskell community is appealing because it shares my values."
I won’t (yet) comment on Zippo itself, but: How in the world is this “beautiful code”? zops = \ w_s2uY -&gt; case w_s2uY of _ { Br ww_s2v0 ww1_s2v1 -&gt; Br (NodeBr (case ww_s2v0 of _ { NodeBr ds_d16N -&gt; plusInteger ds_d16N incNode2 })) (NodeBr (case ww1_s2v1 of _ { NodeBr ds_d16N -&gt; plusInteger ds_d16N incNode2 })) } It looks worse than C++. (Probably because it’s not meant for humans to read. But it’s still really really not “beautiful”.)
&gt; see if I can convince ekmett to modify lens in some clever way to support my use case (not optimistic about this) What is it about `lens` that does not currently support your use case?
If only there was a paper or something that explained and motivated it! Oh, right: http://www.scs.stanford.edu/~davidt/papers.html
The article said beautiful co**r**e, not beatutiful co**d**e
Good question! Future research.... :-P
I'm aware of that - I was just playing the devils advocate with respect to the methodology of the comparison. I think that despite the various caveats and addendums that the post was still claiming a bit too much of the comparison as it stands. I don't know what kind of scrutiny Masters come under in different parts of the world, and was kind of hoping that pointing out what stood out as a red flag for me now might save the OP hassles later if the justification needed massaging for the dissertation. Since the official reason for the decision is explictly noted as being without evidence/data, and gathering convincing evidence probably has a bad cost/benefit ratio - especially if it there's a good guess at how a detailed comparison would go down - why not just change the official reason? Maybe something like "we want to explore the challenges of carrying out JIT compilation with a typed functional language"? I guess it'd be just as easy to spend a paragraph or two in the Masters explaining the logic behind the decision, since, you know, LLVM is huge.
BTW, what was the reason already that we don't have: type IO a = ST RealWorld a I assume it's harder for beginners when error messages pop up, or maybe for historical reasons (IO came up before ST), but I see a lot of code having to deal with duplication (PrimMonad for conduits, for instance) instead of just putting everything into ST and specifying the region RealWorld in case a function needs IO.
I think that most of the confusion is due to the fact that SafeHaskell is a misnomer. This name has a variety of connotations that are not intended. A much better name would probably be ReferentiallyTransparentHaskell, at which point you wonder how Haskell could ever be *not* referentially transparent, but then you realize that we have `unsafePerformIO` and that accessing arrays without bounds checking may not be entirely safe either. That's all there is to it. (And there I have used the word "safe" again, as I can't come up with a better one. Conor, help?)
&gt; But there's still hope for vim, he may repent! ??
Then we can also introduce `Europe :: Region`.
Yes, at least for all `F` that you can successfully encode in the given system.
I wonder what's going on there. It's not at all a given that two successive `send` calls will turn into two separate TCP segments. I tried to look at the warp sources to see, but that code has suboptimal import hygiene, so it's impossible to figure out where almost anything not defined in a given file really lives.
Wow, that was like the evolution of a "functional thinker." I really liked the progression that guy (you?) gave; it showed a lot of the reformatting capabilities of Haskell.
Assuming that's only for one specific benchmark where the header and body fits in one packet, it sounds plausible.
Who's stopping you?
Seems to cause crashes when I try to import. Gets to about Data.L (for instance) and then crashes. I also can't get the commands to work. I have all the correct packages installed and in my path, so I'm not sure what's up. Glad to see plugin development for ST2, though.
&gt; A much better name would probably be ReferentiallyTransparentHaskell Or just TypeSafeHaskell, given as the only goal is to eliminate loopholes in type safety.
Or, following the standard nomenclature: NoUnsafeHaskell. Then in the future we could make that the default and require enabling UnsafeHaskell... y'know, if the idea works out in practice.
&gt; There are proportionally very few women in CS or the IT industry for reasons that are clearly social Clearly? Is this opinion based on any academic study?
+RTS -xc, and take it from there ;-)
Does that print out a stack trace if you ctrl-c?
The problem is that IO does a whole lot more than just mutable references. So we'd need to have some way of including those, e.g. newtype IO a = IO (IO# (ST RealWorld a)) But the problem is that the effects of `IO#` and the effects of `ST` can interleave, whereas the above formulation suggests that we can run all the `IO#` effects and be left with just `ST` effects (assuming `unsafePerformIO# :: IO# a -&gt; a`). There are other approaches to decomposing IO, but none of the ones I've seen look much better than the current situation.
So it would have been acceptable if he had tweeted it? Good to know!
&gt; They are not asymptotically faster for everything. Well sure, they're the same for some operations. I'm not aware of any operation that Church encodings can do faster than Scott encodings, however. The speed of the codensity transform comes from the fact that it allows avoiding traversal of fixed tree structure on your way down to the leaves you're interested in changing. However, this relies on having some notion of structure outside of the `Codensity` constructor--- i.e., Church-encoding `Free` is all well and good, but you still need the `T`. I don't see how you can provide an analogous speedup for a pure (i.e., single-level) Church encoding, which would not also be a speedup for Scott encoding. If you have a specific example, I'd love to hear about it.
You're deliberately missing the point, so I'm not going to bother anymore.
What if I say yes? Will you expect me to waste my time finding citations for you when you'll just ignore them anyway? Objective fact has nothing to do with your current opinions, so why should I expect that facts would persuade you to reconsider? I've had enough arguing with people completely immune to reason to last me for a while, thanks.
In general, when the issue is "Thing X is off-putting to many people of Group Y", having someone not in Group Y drop by to say something like "well, I don't think Thing X is inherently off-putting from my perspective" doesn't really add anything. It's also one of those tediously predictable things that happens in almost any conversation like this one, simply because so many people will do it without even thinking. Basically, your personal interpretation is not really at issue, as such it can come across as a little bit self-centered to show up and inject your experience, and when your comment is approximately the ωth such comment someone has seen it's not surprising they get annoyed.
Well, here's another, I guess. You know how people like `DList`, because nesting of concatenation doesn't matter, because it's just composition? For a Church encoding concatenation is: (Church l +++ Church r) = Church $ \f -&gt; l f . r f That is, it has the same properties. It's even constant time in the same way as `DList`. This means that if you write: nil = Church $ \_ z -&gt; z single x = Church $ \f z -&gt; f x z toList (Church l) = l (:) [] revved :: [a] -&gt; Church a revved [] = nil revved (x:xs) = revved xs ++ single x Then `length . toList . revved` will be linear time (I tested this). There are other examples: cmap :: (a -&gt; b) -&gt; Church a -&gt; Church b cmap h (Church l) = Church $ \f -&gt; l (f . h) This collects up mapped functions via composition in the same way as `Codensity`, and is constant time in the same way (only eventually applying the functions when you eventually ask for the elements; and of course it does, `Church` = `Codensity Endo`). So really, people should (probably) ditch `DList` and use Church encodings. It accelerates more operations, has the same weaknesses, and is good if you want to do some accelerated stuff and then dump to a more inspectible form (Scott-like).
Thank you for your pointers. I will read them.
"faster" was misleading. Sorry. I meant that throughput measured with httperf became 100+. For more information, see http://www.haskell.org/pipermail/web-devel/2012/002481.html
You've obviously never experienced being told by a feminist that your opinion doesn't matter because you're a man. I was kind of trying to satirise that. Also, I didn't actually say I wouldn't listen to men, but to men's opinions. I mean if a man says "I think some hypothetical women would find this offensive", and another man says "My friend, who is a woman, was there and yes she DID find it offensive"... who do you think is going to have the edge in convincing people? How do we actually know that anyone would have complained if Tim hadn't blogged about it in the first place? I should stop giving out free advice on how to communicate more effectively, no-one ever appreciates it.
Based on Chris's blog, I don't think he reads reddit, but I emailed him the same earlier. Most of the info about fay seems to be here anyway: http://github.com/faylang
What do you mean "suboptimal import hygiene1"? 
Curious why you're not using snaplet-fay?
Wow... definitely impressive! The [examples](http://fay-lang.org/#section-4) on the fay website show some really cool stuff. amazing how verbose the javascript equivalent can get! but how should 3rd party javascript libraries be used... write a fay wrapper? 
The scope is a bit larger, actually. Accessing arrays without a bounds check is type safe, but not SafeHaskell.
;-)
&gt; It'd just be a newtype over ST RealWorld instead of duplicating the implementation. That's close to what it is currenty: IO and ST have exactly the same implementation. stToIO simply unwraps the ST inner function and re-wraps it in IO. So making IO a newtype of ST won't change anything to the API, we still won't be able to have the more general functions (those which only deals with references mutation) work in IO and in ST. In some sense they're is no point in having IORefs, because you can handle safely STRefs within IO, but you _have to_ wrap every action on them with stToIO.
&gt; The problem is that IO does a whole lot more than just mutable references. Yes, that is the point of the RealWorld region. a ```forall r. ST r x``` action can't do anything but mutate references. As it must work _for all regions_, it cannot perform but the most general operations. To give it more power, you have to change its type to ```ST RealWorld x```, which indicates we're no longer dealing with the regular restricted ST. We already have it now through stToIO which is perfectly safe, as it instanciates the region of the ST action to RealWorld, but it requires explicit casting.
That's the most politically-correct comment I ever heard of. Author writes what *some* woman might have thought if she understood meetings in very *specific* way. Then he's sure that normal laughter meant hell knows what. He decided to ignore all doubt that author simply might have meant that women civilize environments that were men only. He should touch the fundamental topic. Why are women interested in CS far less than mathematics. As far as I understand from what Thomas Sowell written woman tend to go to industries where 2-3 years long gap won't mean they'll be far behind their colleagues. It just so happens that CS/programming changes rapidly and other jobs don't have such rapidly deteriorating knowledge. Statistically, women that choose CS or any similar job anyway tend to have shorter maternity leave.
Why? Why should gender ratio in CS be close to 1:1 if ratio for school teachers, nurses and so on isn't?
There are a few examples on the [faylang](https://github.com/faylang) git hub organization and Chris' github. For example https://github.com/chrisdone/fay-three. The way snaplet-fay does it is also instructive.
It's just a result of being in work environment with few women that'd civilize the behavior.
Those that "confront" "injustice" like particular hates against women, blacks, single women and so on usually tend to simply enforce a new, different kind of injustice on different people. But that's whole another discussion.
Very well said.
But it hasn't.
is there anything more to read about this? 
As Ywen said, here is the code: http://code.joyridelabs.de/nikki/ (It's a darcs repo.) Some information on the development can be found on our blog: http://joyridelabs.de/blog/ 
Is it possible to ask some question about the design? (Genuine questions, not "meh, wouldn't've done it this way..." questions) - Why not FRP? (even if I think I roughly know the answer) - Why Qt instead of a GUI library which already has a Haskell binding (like wxWidgets)? Necessity to re-write OpenGL boilerplate? (Yay, you're using my old 'binary-communicator' package \^\^. It took me about 10 minutes to write it back then so it's close to nothing, but I'm happy.) **EDIT:** Oh, and of course the most important: if you should re-do it, would you design it likewise or would you change things?
On your order form, the amounts in the first question should be independently selectable so you can choose how much to pay by adding up the appropriate powers of €2.
After installing the profiling libraries, recompiling with profiling and -caf-all and -auto-all, running the code with +RTS -xc, and sending SIGINT during the loop, I got a stack trace that probably would have help me narrow down the source of an infinite loop I had just fixed. It will be my first debugging step for the next one, and I'll see how useful it is when I don't already know where the divergence is.
I'm quite glad MonadTrans doesn't require `Rank2Types` :)
I type-check and generate the client-side code from Emacs, and [`cabal build` also generates the code](http://hpaste.org/75013). It's just not a use-case I personally have. 
Yeah, JavaScript libraries should be wrapped up. The modules under “Libraries” contain a few wrappers to third party libraries. It's quite satisfying to create type-safe interfaces to useful but otherwise untyped libraries.
Yep, fixed: http://fay-lang.org/
I would appreciate it if you avoided gendered slurs, even when they're not being used to insult.
Because a gender disparity in CS (and in teaching and in nursing) indicates a deeper societal problem and because increased diversity has an increased utility.
It seems like you're saying that 't' is an *indexed monad*, to use Conor McBride's terminology. There is one implementation in [indexed-core](http://hackage.haskell.org/packages/archive/index-core/1.0.1/doc/html/Control-IMonad-Core.html), but I'm sure there are more already. I know I've defined my own. In that case, I think if would make more sense to define the whole hierarchy: class IFunctor f where fmap :: (forall a. g a -&gt; h a) -&gt; f g a -&gt; f g b class IFuntor f =&gt; IApplicative f where -- etc class IApplicative f =&gt; IMonad f where -- etc
I wrote both index-core and the blog post in question. The index-core classes don't work for this purpose because they don't impose a Monad constraint, and this constraint is necessary for writing these instances. You could also weaken these type classes to only use a Functor constraint and then you would be working with more general natural transformations. I'm not sure whether this class belongs in index-core or its own package. The fact that you see the same pattern arise, except with Functor/Monad constraints suggests that perhaps indexed types can be viewed as functors of a different sort.
I do agree with you. I was just making the connection with the contributor's nickname. &gt; gendered slurs **EDIT:** Now that I think about it, it may also come from the fact that "victory" is a feminine noun in French.
If it's easier to write definitions for mapT and squash, why not make them methods? (I'm one of those people who also think join should be a method of Monad.)
Just because something isn't widely recognized as a problem outside your culture doesn't mean it's not a problem.
`ioToST` would be fine, as long as it only went back to `ST RealWorld`. It isn't there now so that you can reason about `ST RealWorld` as a form of deterministic STArray-only IO, but thats mostly a quirk of not exposing `ioToST`.
Just curious. Have you looked at Monatron package on whether it satisfies you more?
Ah; I had inferred you to be the contributor in the picture.
It's not stupid at all. When I started coding, QML was very young and somehow immature. But I thought about Haskell-bindings for QML, too. Sounds interesting, though I don't know a lot about QML.
I added a simple example of fay-three to the modules. Written by [Jason Hickner.](https://github.com/jhickner)
JSON is not untyped, but it has a very limited number of types (string, array, map, int, float)
If the goal is to work with things other than Haskell, embedding a sort of DSL into a standardized JSON language dodges the whole parser issue in a rather nice way. aeson makes working with JSON in Haskell fairly nice, IMHO, you'd barely know it was untyped to start with.
One thing to consider is encoding, both in the UTF-8 vs. anything else sense, and the HTML sense, where you set of legal characters at any given point is restricted and some have to be escaped. A lot of these text templating languages just do straight-up string interpolation and have the same basic problems that you get when you do that in HTML, though rather than having a security impact usually it's just the user starting out with a syntax error at exactly the time when the template author and user do not want the user (who may be a rank novice after all) getting a bizarre syntax error. For example, if the project owner is going in an apostrophe-delimited string and I enter O'Brian, what happens? Arguably one could just document that you don't do anything about this and just punt, because it sure will ugly up your approach. Even just documenting prominently this sort of thing can be an issue would be a big step forward over most similar approaches...
The `Monad` constraints you use can all be derived from the fact that, in your setting, `:~&gt;` only make sense between two monads, as you're studying natural transformations between monads. It would be nice if there was a way to tell the Haskell type system, at `a :~&gt; b` definition site, that "this type only make sense under the conditions that `Monad a` and `Monad b`", and have the type-checker therefore infer those constraints in signatures using `:~&gt;` instead of asking you to write them down explicitly. This also raises a small suspicion. When using natural transformations between monads, are there not some coherence conditions that you should require? Natural transformations between functor already have a coherence condition that they should commute with the application of a mapped morphism (for `f : x → y` you want `η : F → G` to verify `η[y].F(f) = G(f).η[x]`), but I think this is given for free by parametricity. Are there corresponding conditions on bind and return? Could the fact that they don't appear in your interface suggest that it is not yet complete? 
I just tried MaybeT (after ReaderT, StateT and WriterT) and indeed needed a Functor constraint on almost all (only ReaderT didn't) but no Monad constraints. My code is in [this gist](https://gist.github.com/3757658).
It does in at least certain cases. For example, if you are designing a game to appeal to girls, it might help to have some women on the team.
Not sure about that explanation. Law and medicine are also rapidly changing fields. I guess the difference is, in those fields, you can refuse to learn, you'll just be worse at your job but maybe no-one will notice. Well, it depends on how demanding your customers are I suppose.
Well, no, not really. You can't dismiss all actual and perceived injustice against all those groups just like that. That's offensive!
Well, that's not the issue. I actually wanted the constraint on the type synonym itself so that people would not need to declare `Monad` constraints, but it does not work. I believe the only way to get it right is to use a GADT style definition that wraps the constraint, but that extra indirection from using a non-`newtype` wrapper would incur a performance penalty and also make it very inconvenient to use the type class.
But do teenage girls really think like that when thinking about what field they'd like to go into? I mean it's plausible, but I've never heard of it. Again, some women chiming in here would help - but this isn't exactly the best venue to ask for that!
Yes, require some sort of law, but which laws?
If I understand this correctly, the user could only pass monad morphisms that were true natural transformations. However, I think you were right that parametricity guarantees that they are correct. I don't know of any other laws that the monad morphisms or the type class operations would need to satisfy.
The fact that there's no sensible instance for `StateT s` makes me skeptical. We have StateT s m a ~ s -&gt; m (a,s) joinI :: StateT s (StateT s m) a -&gt; StateT s m a ~ (s -&gt; s -&gt; m ((a,s),s)) -&gt; (s -&gt; m (a,s)) So the only implementation of joinI that can typecheck is joinI s's'mass = s'mas where s'mas s = q1 {- or q2 -} &lt;$&gt; s's'mass s s q1 ((a,s1),s2) = (a,s1) q2 ((a,s1),s2) = (a,s2) Either way you throw away one of the states. EDIT: If the inner monad has MonadFix, something like this might make sense? joinI s's'mass = s'mas where s'mas s = do ((a,_),s2) &lt;- mfix $ \~(~(_,s1), _) -&gt; s's'mass s s1 return (a,s2) But that seems sketchy to me. I haven't thought through the implications of what this actually means for possible StateT-of-StateT values.
Was that complicated much by the fact that Qt normally uses a nonstandard preprocessor for stuff and is in C++ ?
Then I will weaken the constraint to `Functor`. I think `StateT` does not have a proper instance and I am beginning to believe that this theoretical concept is distinct from the notion of a monad transformer.
Xpost to gaming
So here's a question. foo :: StateT Int (StateT Int Identity) Int foo = do x &lt;- get lift $ put (x+1) y &lt;- get put (y+1) What should `joinI foo`'s semantics be? What makes the most sense to me is that joinI is an interleaving of the effects, that is, that the two states get aliased somehow and the `lift $ put _` just becomes `put _`. I'm not sure it's even possible to make that work.
I really like the simplicity and consistency. This is kind of how I expected a Haskell-ish approach to look like, by pushing as much functionality into the host language and away from the template language. Also, the attribute trick is a clever touch.
When/why was support for OSX 10.5 dropped? I'd love to support ye, but see...
A monad morphism should (in addition to being a natural transformation) preserve the monad structure. I.e., it should be a homomorphism in the obvious way.
Yeah, but based on my discussion with others, I'm wondering if the overlap of `returnM` with `lift` might just be incidental and that really these should only be natural transformations as opposed to monad morphisms.
Don't go to Tianjin if you value your lungs.
Regardless, mad props on raising it to a form of art.
For the html sense, would it be enough to throw regular expressions at this? One regex that a given input value must match in order to validate, and then a bunch of substitutions for each context that an output value might appear in?
I have updated the document with a discussion of HSP. I'd be interested in hearing from people more familiar with Yesod, why Hamlet uses separate constructs to substitute URLs, templates, and other Haskell expressions. It seems easy to provide the necessary instances to use HSX with blaze-html. With such instances, HSX could be used in Yesod without sacrificing the speed or type safety provided by Hamlet. Has somebody linked HSX to blazy-html by providing the necessary instances for the types defined in the xmlhtml package?
There is no evidence these template languages will integrate more and more features. It is even stated the exact opposite: &gt; As little interference to the underlying language as possible, while providing conveniences where unobtrusive. [^1] [^1]: http://www.yesodweb.com/book/shakespearean-templates
Technically the (forall a. m a -&gt; n a) (or m a -&gt; t n a or whatever) should be a monad homomorphism rather than just a natural transformation for embed to make sense, which is just a slightly stronger condition and is the coherence condition on bind and return. You can't encode that coherence condition in the type, but we commonly state it in the ocumentation.
&gt;Note that we construct the URL to the link manually which is error prone and may be difficult to maintain when routes change. But, you are generating the URL yourself so you can do it any way you please. If you want to do it in a type safe manner, then do so. You could say, use web-routes for example.
Certified Haskell Programmer? I bet writing a bad monad tutorial is a requirement.
Just create a wrapper around blaze-html that has a proper Monad definition. Use operational (http://hackage.haskell.org/package/operational) this would be very straightforward.
Presumably an implementation of my &lt;ca&gt; splice would be looking up HomeR in a some table that was statically generated from the types. If you change HomeR to RootR, then you won't find HomeR in your table and you can throw an error. It is true right now that as a splice that happens at runtime. But Heist does provide an onLoad hook that allows you do do things at load time which would give you the same kind of help the compiler gives you. In the next release of Heist this will get even easier.
In case someone is curious, I just re-worked out the rules required by "monad homomorphisms" and they are the following, for a natural transformation `η : ∀a. m a → n a`, writing `return{m}` the return of the monad `m`: - `η . return{m} = return{n}` - `η . join{m} = join{n} . η . map{m} η` (Note that in the latter rule one could write `map{n} η . η` rather than `η . map{m} η`, and they are equal by naturality.) 
To be fair, you do have a point because there is an impedance mismatch between the splice implementation and the onLoad hook. But that deficiency will be gone in the next major release which should be coming out soon.
I wrote in [this reply](http://www.reddit.com/r/haskell/comments/106xwf/the_monadtrans_class_is_missing_a_method/c6behz8) my best bet on the monad morphisms laws.
Pffft -- worthless dead tree shavings. It's dogfood man, we don't expect others to eat what we don't ourselves. 
Your `hoist` is my `mapT`, which is an `IFunctor` except with the monad morphism constraints. You could reasonably split my abstraction into two separate classes, `MonadM` and `FunctorM` (a super-class of `MonadM`), where `FunctorM` implement your `hoist` method.
&gt;it does not help with links that are written in Heist templates directly "If I do it the dynamic way, it isn't static". Well then if you want it to be static, do it the static way.
There is a more fundamental difference between a dynamic check that could be done in Heist at load time and the static checks done by HXT and Hamlet: the latter can use type information even when the concrete values are not known yet. For example, Hamlet will ensure that the link &lt;a href="@{PersonR ident}"&gt;Person with identifier #{ident} corresponds to a well-formed route, while Heist cannot check the same for &lt;ca href="PersonR ${ident}"&gt;Person with identifier &lt;ident /&gt;&lt;/ca&gt; if `ident` is a template argument, because it does not know the type of `ident`. In fact, it seems nothing can be checked in Heist before actually rendering the URL for a concrete `ident`, even with `onLoad` hooks.
Isn't it possible to choose the escape method based on the type of the substituted expression? In HXT strings are html-escaped, and the type class instance for a route type could (should) do url-escaping. As Yesod also uses type classes for the substitution, I don't see how different escape methods are a good reason for having different escape syntax.
&gt; I'd be interested in hearing from people more familiar with Yesod, why Hamlet uses separate constructs to substitute URLs, templates, and other Haskell expressions. I'm hardly an expert on such matters, but one reason might be "safety". Because each of those three substitutions uses different syntax, it is harder to accidentally use one where you meant to use the other.
&gt; If only GHC could provide a monoid do notation, that would solve the issue. Just use indentation rules and lots of `&lt;&gt;` :) mempty &lt;&gt; foo &lt;&gt; bar &lt;&gt; baz You could, of course, do the same thing if you are only using `&gt;&gt;` with your monad return () &gt;&gt; foo &gt;&gt; bar &gt;&gt; baz
My rule of thumb is that if you complete a non-trivial project in Haskell, you know "enough", whatever that means. :)
I do: mconcat [ thing, thing2 ]
I stopped reading your post after the first paragraph. JPMoresmau has done an amazing job, EclipseFP is very useful. If you're not happy with it, don't use it. Otherwise, file a bug report!
found this report from haste side : http://ekblad.cc/hastereport.pdf 
It seems like the best attempt at a consolidation can be found at ["The Javascript Problem"](http://www.haskell.org/haskellwiki/The_JavaScript_Problem) on the Haskellwiki. It is woefully out of date however. My suggestion would be to draw up a standard list of features for comparison and ask each maintainer to update their entry to reflect the current status of their progress, as well as any additional features they feel are worth mentioning. We should also probably rename "attacks" to methods/tools/platforms/whatever, and then sub head based on the type of approach each method employs e.g. written in Haskell, uses functional approach but not Haskell, compiler, interpreter, DSL or whatever makes the most sense. We can even use this thread as the equivalent of the wiki talk page. What should the standard of comparison be? Wave mentioned &gt;(goal, strategy, difference, performance, usability.. ) What else?
Here's a start, off the top of my head. Feel free to correct me. We can make a bigger table on the wiki. | System | Lazy | Pure | FFI | Haskell syntax | Haskell type system | Performance | TCO | Demos | Easy Setup | Compression | Maintenance | Threads | FRP library | Client-side Libraries | Package support | Approach |:------------|-----:|:----:|:---:|:---------------:|:----------:|:----------:|:------------:|:-----:|:----:|:-------:|:------:|:----:|:---:|:--:|:---:|: | GHCJS | ✓ | ✓ | ✓ | ✓ | ✓ | Good | Full (optionally limited) | Some | No (?) | Poor | Active | Yes | Yes (existing libs) | Few (?) | Cabal | Support as much as GHC as possible, and be correct and be fast. | UHC | ✓ | ✓ | ✓ | ✓ | ✓ | Poor | Full | Some | Moderate | Very poor | Active (?) | No (?) | Yes | Some | None (yet) | Support UHC, comprehensive library set, take advantage of JS-specific compilation. | Haste | ✓ | ✓ | ✓ | ✓ | ✓ | Good | Full (optionally limited) | ? | Yes (?) | Decent | Active | No | Yes | Few (?) | None (?) | Simple runtime, produce small code and be useful. | Roy | ✗ | ✗ | ? | ✗ | ✗ | ? | ? | Few | Yes | ? | Active | No | No (?) | Few | ? | Provide a statically typed alternative to JavaScript. Structural typing included. | Elm | ? | ? | ✓ | ✗ | ? | ? | ? | Many | Yes | Good | Active | No | Yes (built-in) | None (?) | ? | Take the FRP-on-the-web concept to its logical conclusion and re-invent the HTML-CSS-JS paradigm as one language. (?) | Fay | ✓ | ✓ | ✓ | ✓* | ✓* | Good | Limited | Many | Yes | Totally sweet | Active | No | No | Some | None (planned) | Provide a subset of Haskell as an alternative to JS with a simple runtime, corresponding generated code and small output. *Subset. 
I believe I have the solution for how to do this using an abstract type-class with nice theoretical properties but have not released the code yet because I spend most of my time working on the next `pipes` release. I can briefly describe it and you can judge if it suits your solution or not. The basic idea is outline in this hpaste: http://hpaste.org/73329 In it I define an `EitherLike` type-class which encompasses the ability to throw errors and change the error type. I define the capability as a category with certain laws it must obey. The second thing I do is define an `EitherTrans` class which provides an abstract way to lift an `EitherLike` capability from one type to a transformed type. The `EitherTrans` lifting operations behaves like a functor that preserves two categories. The first category it preserves is the Kleisli category. The second category it preserves is the `EitherLike` category (i.e. `returnE`/`&gt;|&gt;`). This provides a set of laws you can use to verify that you wrote an instance correctly. The hpaste also shows an example lifting over the `StateT` transformer with proofs of almost all the laws that I had time for when I wrote that up. Formulating it this way gives several nice benefits: 1. You can equally prove a lifting is correct by verifying the functor laws. 2. You don't need to lift throw and catch, similar to the `mtl` style of programming. 3. It scales well (only a linear number of instances). However, the huge disadvantage is that it only works with types that have the type variables aligned just the right way. I still consider it an improvement over the `mtl` because you can fix this using newtypes and only a linear number of instances instead of the quadratic blowup you usually get, but this still requires some work to do and it's the reason I haven't released this yet. The other problem is that it doesn't work with other liftings formulated this way.
Well Fay is not currently on the page, would you like me to add a blurb from fay-lang? You can add it yourself, but if you don't have an account readily available then I would be glad to add whatever you want.
Lexicographic ordering is fine but sometime you want a different ordering. If you wish to reverse order for, let say, the second element, you can't use `compare` anymore. However, a `&gt;?` operator could be added to the suggested DSL to reverse order of some fields: foo3 :: (Ord a, Ord b, Ord c) =&gt; (a, b, c) -&gt; (a, b, c) -&gt; Ord foo3 (a1, b1, c1) (a2, b2, c2) = runChain $ (a1 &lt;? a2) &gt;&gt; (b1 &gt;? b2) &gt;&gt; (c1 &lt;? c2) &gt;&gt;&gt; foo3 ("abc", 3, 1.99) ("abc", 5, 1.99) GT 
If you have accessor functions defined, it's as simple as: import Data.Monoid import Data.Ord fst3 (a, b, c) = a snd3 (a, b, c) = b trd3 (a, b, c) = c foo3 :: (Ord a, Ord b, Ord c) =&gt; (a, b, c) -&gt; (a, b, c) -&gt; Ordering foo3 = comparing fst3 &lt;&gt; flip (comparing snd3) &lt;&gt; comparing trd3
Let me preface this by saying, I don't mean to jump on anyone who is actually getting Real Work done in their code--this is really cool and I'm impressed with the results so far! That said, reading the code (and associated blog post), it definitely feels 'exploratory', like the author doesn't know exactly what he's trying to make, or how it's going to fit together. The `at'` function used in the `Signal &lt;*&gt; Sequence` declaration is kind of the perfect example of that, and it's a "code smell" that makes me think the abstractions chosen aren't quite right for the problem being solved. Maybe `Signal` and `Sequence` shouldn't be the same type; maybe you convert a `Sequence` into a `Signal` and then apply functions to it; the type `Sequence (Signal a)` seems like a useful one to use for most cases where you want the full `Pattern` type, but maybe that's not good enough and you need both in a different way.
GHCJS stuff you have looks fair. I'll be putting up some new examples today. Please add columns for... * Stdio (GHCJS uses are using HTerm) * File IO (GHCJS is still read only and text only) * Tree Shaking * Forest Shaking (tree shaking based on multiple entry points) * Native DOM (you can run your GHCJS apps using using GHC and WebKitGtk) * JS DOM (GHCJS simply replaces the WebKitGtk with JS for this and this) Perhaps a DOM column with "Native &amp; JS" for GHCJS I would characterise FFI in GHCJS as "Difficult" at this point. We kind of hoped you might help us with that :-)
Hi Ryan, always great to have your insightful feedback! Yes this is very much exploratory. I will write a better post about the motivations behind it soon though. As for at' in particular, I can replace that by making arc with duration of 0 do the same job. On reflection that makes perfect sense. Otherwise this abstraction is working well for me so far.
We shouldn't "police" everything. I've changed my mind on the original comment, but there has to be a line drawn somewhere. Some things are too trivial. Maybe "police" is the wrong word. I would be happier with "challenge". On the other hand, this: "Even though the vast majority of the men don't have a sexist bone in their body" is a rather male view of what sexism is!
Well, actually...
Yes, the dynamic and static approaches have different performance. Static checks cost time when compiling, dynamic loads cost time at run time. But if templates are loaded when the webapp starts (rather than when a page is requested) dynamic loading does not have a negative impact on "delivery speed", right? &gt; You can construct an higher level abstraction than PersonR that knows how to do the ident part. I think you mean something like a splice `&lt;personLinkForGivenIdent /&gt;`. Indeed, the Heist way to do things seems to be: do everything that should be checked statically in splices. This fits well with the philosophy to not put logic in templates. Constructing links can be seen as part of the logic..
I still need to analyze your paste, and am only posting this to illustrate the progress I have made: http://hpaste.org/75124 It hopefully works with `mtl`, though it does unfortunately require that `MonadCatch` involve transformers. EDIT: http://hpaste.org/75134 - not sure how I feel about the PolyKinds, but I guess more flexibility doesn't hurt (assuming it doesn't mess with the functional dependencies..)
It looks correct to me so far.
I don't like Template Haskell either, but I also don't like appeals to emotion. I think your efforts would be better served coming up with an alternative solution within the language.
Very nice! Great overview.
What about mtl-2.0.1.0 having Control.Monad.Error. mapErrorT ?
Strings are lists. You just need to use list operations. sub list from to = take (to - from) (drop from list) --Not the best, probably, but it works. Or if you want to check if one string is inside another: check a b@(_:z) |take (length a) b == a = True |otherwise = check a z check _ _ = False Which starts at the beginning, checking if the first few elements are in the list and if so, stops with a true. Otherwise it pops off the first character and tries again.
You can use it to let library users write one block of imperative-style AI code that runs once for the entire game and asks what the state of the world is and gives orders for trying to change it and says when it's done with its turn and so on. Then, on the functional-style simulation side, you just need to deal with two lazily-generated lists (which Haskell is very well suited for), taking each order given by the AI and lazily generating the next state of the world. You need the state monad to carry the list of game states forward in time. You need the reverse state monad to carry the tail of the list of moves the AI is making backward in time to be consed onto. You can't just use functional-style programming like `theRest` in `thisTurn:theRest` because `theRest` is essentially generated by a list of instructions in a monad, not by just one function. [I did this a few years ago (wow, to the day, apparently) for an AI contest.](http://forums.aichallenge.org/viewtopic.php?p=4611#p4611)
Your string question aside, show(1/x) with Haskell's floating point types is not going to give you the answer because they are limited precision, and so cut off after a small number of digits. You need to think more mathematically about how long division with unlimited precision numbers works, and why the cycles repeat.
Goodm stuff Others multiplaform GUIs 
For example, consider this interesting correspondence: 3 divides 9, and 1/3 has cycle length 1 11 divides 99, and 1/11 has cycle length 2 27 divides 999, and 1/27 has cycle length 3
This is cool! Thanks for this nice example.
You mention that it is important to look at the hom types, and not just the objects. Is there ever a time where it is more important to look at the objects over the hom types?
Here's a link to [Down on Hackage][1]. [1]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/GHC-Exts.html#g:8
I am using Arch Linux (updated in early Sep) I just don't get how it work. When it compiles to javascript, how does webkit part work? (runWebGUI and other functions defined in webkit library.) I thought that webkit package just wraps FFI to corresponding c function call in libwebkitgtk. How does this work with javascript? Would you elaborate what's the magic here?
I got it. Thanks! I imagined that you made a magical transformation of c function ffi to javascript ffi ;-) Your hidden game is cool!!! by the way, what's the main obstacle in implementing generic javascript FFI in ghcjs now?
Blackh deserves the credit for the game. Once it works in Firefox I think we will try to post a description of how it was done to r/programming. The guys working on the old yhc JavaScript back end hacked something up with HDirect to process WC3 IDL files. I got it working with the WebKit IDL files and changed it to output Gtk2Hs style .chs files and once that worked I added code to generate the rts_webkit.js file. Source code is here... https://github.com/ghcjs/domconv-webkit It might be useful if we decide to build a higher level DOM interface. I think we should make the JavaScript FFI work for native Haskell code first too. I have added an issue... https://github.com/ghcjs/ghcjs/issues/41 If you have time to help hop on #ghcjs
I'm no haskell expert, but : If you're trying to be close to the haskell spirit, you should separate game data and logic from the IO stuff. Game data and logic should be purely functional. I feel like it's the whole point of functional programming, make it testable, reusable, easy to reason about.
That would actually be an easy thing to do if the prelude were factored a bit differently, because `-1` is not a literal. All literal numbers are positive, and the prefix negation is applied to write negative numbers. So if we had a `Num` (or some such) without `negate`, and then a subclass with `negate`, `Natural` could be an instance of the former and not the latter, and that would be a type error.
Small tips: You can use `putStrLn` instead of `print` in `print "Thanks for playing."`, `print "You are dead."`. Here: monsters &lt;- forM [1 .. monstersNum] (\_ -&gt; do builderNum &lt;- randVal $ length monsterBuilders monsterBuilders !! (builderNum - 1)) you can write monsters &lt;- replicateM monstersNum (do builderNum &lt;- randVal $ length monsterBuilders monsterBuilders !! (builderNum - 1)) and also `forM_ [0..times] (\_ -&gt; ...` is equivalent to `replicateM (times+1) (...` Here: newPark &lt;- liftIO $ forM [ 0 .. (length m - 1)] (\i -&gt; if i == idx then return (hitInstance (m !! idx) pow) else return (m !! i)) you don't need a monad: let newPark = map (\i -&gt; if i == idx then hitInstance (m !! idx) pow else m !! i) [0 .. length m - 1] even better: let newPark = take (idx-1) m ++ hitInstance (m !! idx) pow ++ drop idx m 
Nope, -1 is a literal. Try this in ghci: &gt; let f (-1) = 1 Btw, how do you make inline text monospace?
Surround the text with backticks.
That is only the case for patterns, which must be handled magically anyway, and still isn't difficult. Edit: In fact, asking for the type of `f (-1) = 1` gives: (Eq m, Num m, Num n) =&gt; m -&gt; n (since `Eq` is no longer a prerequisite for `Num`) which suggests this is already implemented via translation to: f m | m == (-1) = 1 in which case it would automatically work.
I think it might be nice if, instead of partitioning the `ByteString` modules into `Char8` and `Word8` versions, `ByteString` was parameterized on the type it is intended to be treated as storing, so there might be reason to not see this as such a great example. However, I do agree with you in principle. I overlooked this aspect of the proposal.
Yes, I overlooked some of these cases of the original proposal because I skimmed it too quickly. In my haste, I said something barely relevant.
A parameterised `ByteString` is essentially a `Vector`. There has been some effort to make ByteString a synonym for `Vector Word8`: http://hackage.haskell.org/package/vector-bytestring. I don't know whether Bas managed to make it as fast as existing `ByteString`.
Would it be possible to constrain the target to be a monoid? Then you have an easy law to verify an instance is correct: fromList mempty = mempty fromList (as &lt;&gt; bs) = fromList as &lt;&gt; fromList bs However, I think a better solution is to simply standardize on an isomorphism dictionary: data Iso (~&gt;) a b = Iso { fw :: a ~&gt; b, bw :: b ~&gt; a } instance (Category (~&gt;)) =&gt; Category (Iso (~&gt;)) where ... The dictionary is valid here since there are no coherency issues. Also, it allows composing isomorphisms, which is awful handy. Then you would just write a bunch of isomorphisms between the types you want to convert: listIsVector :: Iso (-&gt;) [a] (Vector a) bytestringIsString :: Iso (-&gt;) String ByteString And if you still want type-inference, you can always create a ListLike type-class that automatically selects the right isomorphism as a convenience for users. If somebody wants, I can quickly write up tiny library with this dictionary.
&gt; mapIsAssocs :: Iso (-&gt;) (Map a b) [(a, b)] This is not an isomorphism. Consider `[("a", 1), ("a", 2)]`, or `[("a", 1), ("b", 2)]` vs. `[("b", 2), ("a", 1)]`. Also, you need a `Category (~&gt;)` constraint on the `Category` instance.
BTW this is not the game that is hidden in the [GHCJS Hello World](http://ghcjs.github.com/bin/ghcjs-hello.jsexe)
Thanks everyone for feedback and tips. I was sure my program wasn't perfect, and was more a style exercise to teach myself transformers, but you gave me food for thoughts :)
I would rather a simpler extension with just a straight typeclass. Making list syntax overloadable to sets and similar is not intuitive for me. In my view, list syntax should be reserved for stuff where the order of the elements you list out actually matters, which includes vectors, lists, sequences, and so on.
I cannot agree with you any more. But after developing a rather big-size real world application in haskell, I would like to give OP another practical advice. If avoiding IO monad is easily possible, just do it early. But if it is not very clear, do not feel too hasty about purifying functional part too much, I mean 'do not be too much dogmatic from the beginning.' IO monad itself is a good prototype DSL. You can program in IO monad, and that's just a valid haskell program. What is great in haskell is that IO monad action has its IO tag all the time. Later, when you refactor by making your function into smaller functions, you will find reusable pure functions and IO monad actions and their separation is clear in type. That just gives you automatic separation. Even after this, you will see IO action can be refactored into generic monad actions and real IO actions if you parameterize your IO monad by (Monad m)=&gt; m . Monadic action without MonadIO instance is pure. This kind of simple function shredding and reorganizing is a basic method of purification. At some point, maybe you will need like using Free monad or other method for further purification. If you already shred your IO actions well, your effort in modification for purification using advanced technique can be minimized. All these refactoring processes are doable by pleasant conversation with ghc typecheckers. 
`forM` needs a list to iterate over; it is similar to `flip map`, only it's monadic. When you don't really have need for the list, `replicateM` is a better choice because it simply takes an `Int` specifying how many times the action should be repeated. If you find yourself doing something like monsters &lt;- forM [1 .. monstersNum] (\_ -&gt; ...) most likely you should replace `forM` with `replicateM` instead.
I like your first idea of requiring monoid morphism laws because list is the free monoid. The second idea of using isomorphisms is too restrictive for some of the use cases in the proposal. For example, Set and Map are both commutative monoids so there can't be isomorphisms to lists. Your remark gives rise to a funny (conceptual) implementation: fromList = mconcat . map pure
I've thought a lot about: cons x xs = (pure x) `mappend` xs But the problem is that things like Set and Map are not Applicative, and can't even define `pure` well (since it does not have any constraints).
but you only need the fundep if you want to support, e.g., `Map`.
True. If `Applicative` was split into two classes we could use the one for `pure` to define "pointed monoids" which my "definition" would require. edit: doh! I misread your comment, thinking you were referring to difficulties defining `&lt;*&gt;`. The splitting I mention would not help with the constraints, of course, which are a separate issue.
&gt; But doesn't ghc just kind of optimise the list away as a counter anyways? Surprise! They're pretty much the same in terms of performance, but not because GHC optimizes away `forM`'s counter: it's because `replicateM` uses a list internally, too.
Additionally, this will help dramatically reduce instance lists that have lots of type classes representing varying levels of granularity. I'm looking at you, Edward. :)
Hey, at least he's trying to do it right! ;)
Fundeps are indeed notoriously difficult to implement correctly. A big part of this is the fact that they are relational rather than functional, so there are some unexpected corner cases. There was a paper on these difficulties a while back, though I can't seem to find it at the moment. For this reason JHC is planning to avoid them entirely and go straight for type families; I'm not sure what the progress looks like on that front however. Then again, JHC is also hoping to avoid MPTCs due to technical issues about how instance resolution is implemented; the hope being that without fundeps MPTCs are "unnecessary". Except that, in truth, we do occasionally need unconstrained MPTCs; so I'm not sure how JHC plans to address the issue in the long run. However, in spite of the difficulties, because they are older fundeps are better understood. They're implemented in (the now defunct) Hugs, for instance.
Ok, now I got it what you mean by «literal» and why it matters for your proposal. You are right indeed; sorry for the noise.
I know. I love Edward's libraries.
Is this what you want? http://www.haskell.org/haskellwiki/Quasiquotation
Is there a way of checking if the result has already been cached for either of them in order to avoid the overhead of spawning threads? Or does it do that automatically?
I'd just whip out orc (a concurrent orchestration EDSL) and do it with that.
It would be great if it could be merged back in as a kind of cross compiler. In the mean time time perhaps we should try to build Haskell Platform binaries with the integrated GHCJS instead of GHC.
Yes, just allowing class aliases isn't very powerful. It is just the logical counterpart to the use of ConstraintKinds. Proposals like InstanceTemplates and ClassDefaults are way more ambitious, and therefore would require way more discussion before being adopted. I was hoping for something I'd be able to actually use within a year :-) InstanceTemplates does look very nice, though, and I can see places in my code where I could use the additional power for extra code clarity. Most of the bang for the buck would still come from a simple ClassAliases extension, though.
What kind of threads? The heavyweight OS ones, or the extremely lightweight Haskell ones? Because the latter ones are nearly exactly like what OP wants.
Backspace does not work. It lacks the abbreviations text adventures used to have (e.g. Look -&gt; l).
Curly braces for overloaded lists would likely be in conflict with their other uses. While set notation borrows the intuition (from mathematics) that order is irrelevant, it also borrows the intuition of no duplicates, so it is similarly misleading for some instances. Also, list notation may be justified when thinking of overloaded lists as applying an implicit monoid morphism because list is the free monoid. 
Does anyone else find the hidden game very memory leaky? After about 10 minutes in the game, it's eating up more than 1GB of memory and climbing. I'm running Chrome linux. 
The moc (the mentioned preprocessor) made our buildsystem a little bit more complicated. We wrote a C-wrapper for the stuff we needed from Qt, so it was a bit more effort than it would have been to interface with a C-library. Not much, though.
https://gist.github.com/3775816
&gt;&gt; If cache timeout is fast enough (say 10 seconds), the risk of cache inconsistency problem is not serious. Does it mean that within 10 seconds after updating file the server may send the client truncated file using the old size? Perhaps Warp could use inotify to keep the information up-to-date.
Ah. Perhaps I'll take a whack at compiling it later this week
Sounds good. Is there a way to compile code conditionally on the JS backend? I.e. like a `__GLASGOW_HASKELL_JS__` flag. Some types like `ByteString` are heavily optimized with an x86 architecture in mind, and it would be very useful to provide an alternate implementation for a JS backend. I'm currently trying the [haste compiler][1] and that's where we have trouble at the moment. [1]: https://github.com/valderman/haste-compiler 
Isn't this just `Free Identity`?
Then you love most of my writings ;)
What joeyh is talking about is choosing an escaping rule based on the HTML syntactic context where the template value is expanded. I.e.: &lt;p&gt;Expanding &lt;% foo %&gt; into a paragraph is different from doing so in a script. &lt;script&gt; var foo = '&lt;% foo %&gt;'; &lt;/script&gt;
Look up Claessen's poor man's concurrency monad. Seems to me it fills the bill perfectly.
&gt; Without creating threads every single time Is there any particular reason for this restriction? GHC's lightweight threads really are lightweight, so performance will probably not be a problem.
Since this solution just produces the result of the algorithm with the fewest steps (instead of doing wall-clock timings) what is the reason for the `seq` call? Especially since `u` is not used again?
Your applicative instance is still pretty sketchy, for example pure id &lt;*&gt; Sequence anything != Sequence anything. Also the `Sequence &lt;*&gt; Signal` and `Signal &lt;*&gt; Sequence` cases seem too different to be correct; `id &lt;$&gt; fs &lt;*&gt; xs` should generally be "mostly" equal to `flip id &lt;$&gt; xs &lt;*&gt; fs`. (This isn't true for all applicatives, for example, it doesn't hold for IO or other noncommutative monads, but seems to be true for most instances that aren't instances of Monad)
As I wrote in my preface, I don't want to jump on anyone who is actually getting Real Work done, but rather am seeing it from a stylistic/mathematical point of view. I think you would really enjoy reading [Conal Elliott's FRP papers](http://conal.net/blog/tag/frp), since I'm seeing a lot of parallels between Signal and Behavior and Sequence and Event. The ranges on your events is novel and interesting, however! I also suggest taking a look at apfelmus's fascinating [reactive-banana library](http://www.haskell.org/haskellwiki/Reactive-banana). But not before your gig... have fun and keeping writing music with code! 
This is also known as the Partiality monad in Agda land, and is how we can embed non-terminating computations into a total language (with codata).
The phrase "doable by pleasant conversation with ghc typecheckers" has been added to my list of favorite adjectives. Thank you for this gem.
The problem with this approach is that all of those packages are in the platform anyways AND this makes it harder for every user AND you are stick with needless orphan instances AND I already need at least the `containers` and `unordered-containers` instances in order to define the `Data.Data.Lens` uniplate combinators and the `Control.Lens.TH` code. Plus this lets me define a class for accessing members of a `Map`, which becomes an even worse problem with the proposed orphan instance approach. I realize it makes the library less minimal, but a stated design goal of lens was to come batteries included with stuff you need to work with `base` and the platform. There are other far more minimal compatible lens libraries, see `lens-family`, which already breaks things out into 3 packages to avoid extensions. I would point to the (rather overwhelming) surge of support I've seen for this library compared to the somewhat anemic response I had for `data-lens`, that in hindsight I'd do it the exact same way. My experience with making a ton of small packages is it just leads to end user confusion, and my experience with orphan instances is even worse.
OK, here's a proposed syntax and translation for class aliases: class alias SuperConstraint =&gt; ClassName tvars = AliasConstraint where defaultMethodDeclarationsForClassesInConstraint instance ClassName SomeTypes where methodDeclarationsForClassesInConstraint Desugaring to type ClassName tvars :: Constraint = (AliasConstraint, SuperConstraint) -- This datatype is used to force the compiler to validate the superclass constraints -- and can be omitted if there are no superclasses in the class definition. data CheckSuperclassesClassName tvars = SuperConstraint =&gt; DefineCheckSuperclassesClassName -- force the compiler to verify the instance satisfies the superclass constraints -- by wrapping the dictionaries for the superclass in an existential. checkSuperclassesClassName_SomeTypes :: CheckSuperclassesClassName SomeTypes checkSuperclassesClassName_SomeTypes = DefineCheckSuperClassesClassName -- declare the instance methods instance Class1FromConstraint tvarsFromConstraint where methodsFromInstance anyUnspecifiedMethodsThatHadDefaultsInAlias -- repeat for each class in the constraint For example class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b class Pointed p where return :: a -&gt; p a class Apply f where (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b class alias Applicative f = (Functor f, Pointed f, Pure f) where fmap h f = pure h &lt;*&gt; f pure :: Applicative f =&gt; a -&gt; f a pure = return class Bind m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b ap mf mx = mf &gt;&gt;= \f -&gt; mx &gt;&gt;= \x -&gt; return (f x) -- inferred type: (Pointed f, Bind f) =&gt; f (a -&gt; b) -&gt; f a -&gt; f b class alias Monad m = (Bind m, Applicative m) where (&lt;*&gt;) = ap instance Monad [] where return x = [x] (&gt;&gt;=) = flip concatMap {- defines type Monad m :: Constraint = (Functor m, Pointed m, Apply m, Bind m) type Applicative f :: Constraint = (Functor f, Pointed f, Apply f) instance Functor [] where fmap f x = pure f &lt;*&gt; x instance Pointed [] where return x = [x] instance Bind [] where (&gt;&gt;=) = flip concatMap instance Apply [] where (&lt;*&gt;) = ap -}
If we were converting not from an ordinary list, but from a list with its length encoded into its type, that would not seem to be that complicated. (And it's worth emphasising that although the compiler does not in general know what any of the individual run-time values in a list literal are, it certainly knows what the run-time length of a list literal will be.) class IsList a n where type IsListInput a fromIndexedList :: IndexedList n (IsListInput a) -&gt; a instance IsList [b] n where type IsListInput [b] = b fromIndexedList = listFromIndexedList instance IsList (NonEmpty b) (Succ n) where type IsListInput (NonEmpty b) = b fromIndexedList = nonEmptyListFromIndexedList `n` is a [type level Peano natural number](http://www.haskell.org/haskellwiki/Peano_numbers#Peano_number_types). I don't know whether it would be possible (or better, in terms of kind-safety) to do this using GHC's new type-level values instead, although that would raise the implementation complexity for other compilers. This is certainly more complicated than before, but it doesn't seem to me to be way more complicated, and it would be a useful extension.
Would it be an entirely new implementation or just a wrapper around existing (i.e. C-based) libraries? What would really help is a well thought-out API to these algorithms that took advantage of Haskell's unique language features. After that, whether the algorithms are implemented in Haskell or in another language doesn't matter so much. 
It's a useful data structure for probabilistic set membership. I wrote it to run in constant memory (in Haskell on a server, for generation), and to be fast for querying (in Javascript, in the browser). (I used it to test for passphrase safety, at http://www.leebutterman.com/passphrase-safety/ .) Interestingly, I hand-translated the tail-call-optimized code into a Javascript while{} loop and got something vastly more efficient than I could've written myself from scratch.
Re non-empty list types, [see my comments in this thread below](http://www.reddit.com/r/haskell/comments/10c8ld/call_for_discussion_overloadedlists_extension/c6c77b6).
There's plenty of room for books at all levels, you don't agree? People write what they can, for people both vocal and silent. [Here's a PDF](http://okmij.org/ftp/Haskell/Iteratee/describe.pdf) Oleg wrote on iteratee IO.
I agree with tr0lltherapy. We have plenty of introductory books. In addition to what he is looking for in a Modern Haskell book, I would add Functional Reactive Programming as it's very own chapter. On the other hand, being that it's an ongoing area of research maybe things haven't settled down enough to go in a book like this?
Keep writing, if only for yourself. Even if we already have many introductory books, one more cannot really hurt. On another note, anybody know how RWH 2.0 is coming along? If at all?
I hope so, that'd be nice.
Bryan O'Sullivan did put up a survey a few months ago asking what additional things people might like to see covered. Assuming O'Reilly wants it updated, and Brian wants to do it, you're still probably looking at a couple of years minimum before there's a new edition. Incidentally, if you purchase it directly through O'Reilly, you can get non-DRM copies in multiple formats; perhaps you were already planning to do this, but just in case you were going to get it through Amazon, who might or might not give you a DRM'd version, but almost certainly won't give you the multiple formats option, nor the upgrade-to-new-version price break, etc..
I think there is interest out there. However, for people who want to use it for actual work, efficiency is crucial, which unfortunately goes against readability. Also, if you look at existing systems like Matlab, Maple, etc. they have lots of tricky fine tuning, lots of variants of similar algorithms, etc. because of this. It's really a *huge* amount of work, and requires a *lot* of domain expertise.
I'm thinking of making higher-order functions the last "introductory" chapter and using the first two parts as a "springboard" for real-life examples. As [BeforeTime wrote](http://www.reddit.com/r/programming/comments/10fx3n/im_writing_a_haskell_programming_book_id_love/c6d56kk) in the other thread, I think the chapters following the sixth will be covering real-world examples almost exclusively. And yes, I won't forget modern Haskell -- that's where the cool stuff is!
No, [automatic differentiation](http://en.wikipedia.org/wiki/Automatic_differentiation) is not [symbolic integration](http://en.wikipedia.org/wiki/Symbolic_differentiation) nor is it [numerical differentiation](http://en.wikipedia.org/wiki/Numerical_differentiation). With AD what we do is recognize that every numeric quantity was build using a number of primitive operations that have known partial derivatives. So instead of storing an expression tree i can store a tape of just what the answer was ad what its partial derivative with respect to each of its input was, and then work backwards from the result: this is reverse mode. Or I can just pass an extra bit of information to the function with each numeric argument, that consists of its (partial) derivative information. *e.g.* instead of calling `f x`, call `f (AD x 1)`, and treat constants x as `(AD x 0)` instead. This approach is called forward mode. The `ad` package represents both forward and reverse mode, along with other modes more suitable for computing derivative towers, or complex sparse derivatives. Vectors are represented in the `ad` package by using a `Traversable` container, (and in some places it can permit an arbitrary `Functor`, where infinite dimensional vectors don't cause problems). &gt;&gt;&gt; diff (\x -&gt; x * sin x) 12 9.58967458678947 &gt;&gt;&gt; grad (\[x,y,z] -&gt; x * sin y + exp z) [1,2,3] [0.9092974268256817,-0.4161468365471424,20.085536923187668] You can also use safer finite-sized vector types if you'd like.
I just googled the problem. It seems to be some sort of bug. I'll go ahead and separately attach it on google sites. Maybe this way it'll work. edit: try it now.
Ahh. Interesting. Never heard of that before. I think my goal would be to have symbolic differentiation to calculate gradients and hessians. But that would come later after implementing some popular algorithms.
More than a few months ago, bos tweeted it a year ago yesterday: https://twitter.com/bos31337/status/106496536434851841 dons retweeted it more recently, and I retweeted and posted it to this subreddit four months ago (not realizing at the time that the original tweet was quite old). http://www.reddit.com/r/haskell/comments/u7lsk/interested_in_rwh_2e_rt_take_survey/
I was thinking about something this morning, and seeing your book has just prompted me to write it down. (Though let me confess that I've only just skimmed through for 5 minutes right now, so the following is not meant as a comment on it specifically.) One of the things that's bugging me is that Haskell instructional material seems to be pretty light on **evaluation model** and **execution model** issues. They do a good job of explaining what the programs *mean*, but not of explaining how a program's result is computed. This is problematic because Haskell's non-strict semantics and its implementations' lazy evaluation are unfamiliar to most programmers, and there just aren't many good resources for learning how to reason about it. My favorite comparison here is to compare Haskell instructional material with Lisp material. Several Lisp books are famous for demonstrating how to write a simple Lisp interpreter in Lisp. While such interpreters are necessarily simplistic, this does have a huge virtue: it teaches the student an explicit model of how the language's programs are evaluated. I can look at a Scheme program and mentally picture how its evaluation is going to proceed. I can reason about time and space usage that way, which subexpressions will be evaluated and which not, etc. Since Haskell lacks Lisp's homoiconicity, we can't quite reproduce this, but it should be possible to do a lot better in this regard. Some ideas would be: * The most basic thing: what is forced when? Existing materials do a poor job of teaching this, I feel. * Teach hand-evaluation of Haskell programs by equational reasoning and reduction rules. Explain how this can serve as a useful model of space complexity of program evaluation. (It's also sometimes useful for understanding libraries. For example, monad transformers didn't really "click" for me until I once did the exercise of hand-evaluating a call to `runMaybeT`, and saw that what it does is to translate a program in the `Monad m =&gt; MaybeT m` monad into an "equivalent" program into one in `Monad m =&gt; m`.) * Teach a "translation to thunkese" semantics for pure lazy evaluation of Haskell programs. By "thunkese" I mean stuff like "this code creates a thunk for `foo` that contains code X and required pointers to the thunks for `bar`, `baz` and `foo` itself, then passes that to..." This can help clarify circular-looking programs to people, like those that use the so-called "credit-card transform." Again, these things don't have to actually be presented at the level of complexity that the compiler uses. They just have to be useful as reasoning tools for the programmer.
In case you weren't already aware of it; sounds like you're describing [mezzohaskell](https://github.com/mezzohaskell/mezzohaskell). Sadly that effort seems to have died?
http://www.haskell.org/haskellwiki/Typeclassopedia
The closest thing that exists, to the best of my knowledge, is [The Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia).
And then, for the brave, there's the docs for edwardk's latest lens library: https://github.com/ekmett/lens/wiki/Derivation
It would be a while until I even started working on it. But I will have lull time in the coming year. If I get anything worth mentioning I will certainly post it here to receive feedback and criticism. I am certainly a better mathematician than Haskell programmer so I'll probably seek help regarding the code much more than the underlining mathematics.
I'll definitely consider it when the time comes, but I won't be able to seriously start entertaining these decisions for another couple of months now. I'm not saying no. Just want to hear of the advantages and I would have to look into each method first. also, I wouldn't implement any differentiation for a while I would just solve problems of simple following forms: min &lt;c, x&gt; subject to Ax = b, or min &lt;x, Ax&gt;. Then get into generalizing it later.
I assume that there's no dearth of fast and efficient C implementations that you've no hope of competing with without immense effort. If you do implement this library may I suggest you focus on ease of use and readability over speed and efficiency, like how parsec is supposed to be the easy to understand version of the speedy attoparsec? Maybe if you consider the library user to be an engineer type person doing quick and dirty calculations in ghci instead of an academic performing long-running simulations? That might be a group of people who don't yet have a good selection of open source solutions.
To me list notation strongly implies *collection*. It does not (to me) imply *monoid*. So we have different intuitions of what the syntax implies / should imply semantically. I'm going to appeal to your sense of pragmatism instead: the `IsString` class has been abused; there are instances that define `fromString` as a partial function. This is clearly not an ideal state of affairs. I predict that the `IsList` class will be abused in the same way, with implementations defining `fromList` as a partial function, whatever design we settle on. But it will be abused less if literals can be rejected at compile-time on the grounds of length.
... and forward AD is *O(n)* slower than reverse mode AD for the common usecase of multiple inputs with a single output, which is why nobody serious uses it.
On that note, I think it's best to avoid the word "thunk" and "pointer" when talking about lazy evaluation, because they refer to representations on a machine. Equational reasoning and graph reduction are perfectly fine for understanding lazy evaluation, including circular data structures, in particular because you can do them by hand.
A bit off topic, but the code in those slides is typeset horribly. Please use either [lhs2TeX](http://www.andres-loeh.de/lhs2tex/) or just \verbatim in Tex.
Kenichi Asai and Oleg Kiselyov gave a [tutorial on delimited continuations at the Continuation Workshop 2011](http://pllab.is.ocha.ac.jp/~asai/cw2011tutorial/). From the linked page, you can obtain [tutorial notes](http://pllab.is.ocha.ac.jp/~asai/cw2011tutorial/main-e.pdf), notes on [delimited continuations in Haskell](http://okmij.org/ftp/continuations/Haskell-tutorial.pdf), as well as the [tutorial slides](http://pllab.is.ocha.ac.jp/~asai/cw2011tutorial/slides.pdf).
And as a counter-point, I think we certainly do need more work like this. LYAH is not the be-all and end-all of haskell introductions. Lots of people I've worked with have had serious difficulties with trying to learn haskell using LYAH, and alternatives are welcome. Different people find different approaches to teaching things more/less effective, and the more options that are available, the more likely it is there will be something to help everyone.
I've thought about doing a convex optimization library before. But alas, the real world demanded results, so I just used the Python library. So yes, I'd love one in Haskell.
I agree, I don't think I could compete with even immense effort, especially with the proprietary software. But if it could provide other advantages, such as ease of use, reduced development time, and ability to understand the algorithms it may have a niche.
Tiny, tiny little thing, but a pet peeve of mine — very common with code typeset in LaTeX — is the use of curly single quotation marks instead of straight apostrophes and backticks. This makes copying and pasting code more difficult than it could be. I believe you can use the upquote package for this.
I'd be curious to see the Haskell benchmark. I'm somewhat dubious that it has all the strictness annotations necessary to be fair.
Yes, but you didn't say "backward AD is faster". You said "AD is faster". By the I think forward AD can be still useful when performance is not critical, since it is so easy to implement. In any case, in practice all kind of hybrid symbolic/AD system can be done. For example imagine the (completely realistic) situation when the computation is globally regular but locally irregular. You can use backward AD for the global structure and symbolic diff. locally.
I confess I was cherry picking reverse mode AD vs. tree-like symbolic differentiation without custom sharing nodes, but any naively written symbolic implementation using the stock haskell typeclasses is likely to be the latter. ;) I do like symbolic approaches when you can precompute a new more efficient calculation with better constant folding, etc, but for the vast majority of usecases it is just a bigger and more bloated approach that has to do a ton of additional case analysis rather than working with the partials directly. As for forward mode, I support it in the AD package and I use it for a number of cases, for instance to calculate `diff`, or to build the chain of all derivatives of a unary function, and I mix forward-on-reverse to compute Hessians. Moreover, my `Sparse` mode is designed as a particularly efficient forward mode for when you need all of the kth partial derivatives of an n-ary function, though even there I use Sparse-on-Reverse. =)
&gt; but any naively written symbolic implementation using the stock haskell typeclasses is likely to be the latter. ;) and any naively written AD implementation using stock haskell typeclasses will be most likely the forward one :)) But yeah, you are right that the symbolic approach is much more complicated to implement. But it has some advantages.
&gt; Also doing the CSE after forward AD is too late. You mean after symbolic diff? Because I meant that. Also, if I put (x\*x)\*(x\*x) into the CSE (which I won't, because if I have the trouble to write symbolic code, I will implement powers properly), I damn well expect it to return "let y = x\*x in y\*y". x is symbolic variable here, after all.
Personally, I prefer list comprehensions to both.
The problem is doing so with operators in Haskell. You don't have control over the implementation of `(^)`. You _can_ build such a specific DSL, but it doesn't work with third party code you didn't write.
Their benchmark pisses away the pointer tagging benefit by using 100 cases.
That's right. I didn't consider working with existing code. In that case, AD wins, but it's not easy (maybe even impossible) to make it completely automatic even for AD. edit: though if you give me a mathematical function which is polymorphic in Num, and uses only the basic operations, I think it's possible to make it symbolic. You overload everything and just feed (Var "x1") etc into the input arguments. But this is of course not a realistic example of third-party code.
100 constructors is not a realistic case, but more of a stress test. A more realistic benchmark would use a range of enum sizes, from 1 to 1000s. Accessing branches in order is also highly unrealistic. The GHC HQ do a much better job explaining the practical concerns here, http://research.microsoft.com/en-us/um/people/simonpj/papers/ptr-tag/ptr-tagging.pdf 
Fixing the first stage to properly strictify the list with Control.Seq gets us a 18-19% speedup. Fixing the `k` to fit in the dynamic pointer range speeds it 42%, which makes the Haskell version the fastest kid on the block.
I get similar results. I'll send a mail to the authors.
And good for them. If I ever have to program in C++, I'd certainly love to have *some* form of pattern matching at my fingertips. And their dedication to "zero cost abstractions" is admirable, even if it makes the language definition very, very complicated.
Amusing, but not surprising. This is something so fundamental to Haskell and ML; we've had decades to get it right, so it *is* impressive that they are in the same ball park with a brand new library.
Looks about right.
Thanks for telling us what you're interested in! I'm interested in black holes! :D
How about Cale's running example of taking one function `withCString :: String -&gt; (CString -&gt; IO a) -&gt; IO a` and getting one that operates over a collection of strings so you get `[String] -&gt; ([CString] -&gt; IO a) -&gt; IO a`? It's not that hard to figure out without continuations, but it's a lot simpler with :)
Can you put your code online? It's a bit hard to understand what exactly you changed from your terse description.
Here's what I submitted to the authors: http://code.haskell.org/~dons/code/patterns/ The benchmark results are in [this graph](http://i.imgur.com/7dpbl.png). I also show how the speed varies with the constructor count (so pointer tagging can kick in).
https://gist.github.com/3790954#gistcomment-579888
yep.
So where's your modified benchmark for the rest of us to play with? :)
It seems to be largely dead. The last episode was published at the beginning of August.
That is merely two months ago. That does not necessarily mean it's dead. Work/school/life may have come in the way, etc.
Sure.
Typechecking is useful, but only the insane think it is sufficient. That's why we have QuickCheck :)
Cool [hedcut](https://en.wikipedia.org/wiki/Hedcut) portrait.
This is the (accurate) logic behind why a majority of a codebase's tests should be unit tests. However, unfortunately "correct" is not context free. Instead a component is "correct" in your codebase if it matches the expectations those depending on it. Hence the need for some amount of integration testing. Also, we want to verify invariants from the user's perspective while using the product. This requires nailing down this behavior regardless of which components are currently used, and regardless of how they are currently wired together. Hence the need for some amount of acceptance testing. Lastly, there are other examples of things you want to verify beyond individual units, like system wide extra-functional attributes (scalability, performance, reliability, etc.)
Since when is stuffing your Haskell code with "low-level" evaluation order consideration "necessary to be fair"? I'm a bit surprised by that idea. I think everyone recognize that it's possible to gain important performance benefits in Haskell by making carefully chosen part of your program more strict than the compiler would infer (just as it's possible to make C faster by using inline ASM in carefully chosen place), but has this been redefined as the idiomatic, expected way to program in Haskell? I somehow resent the idea that one would need to jump through hoops to write Haskell considered "fair" for comparison with other languages. There is a use-case for benchmarking idiomatic code rather than optimized-by-compiler-experts code. In any case, I don't think this concern is relevant here, as it is essentially only testing the performance of pattern matching. If strict vs. lazy made any performance different in the Haskell case, it wouldn't mean that the benchmark is "not optimized enough", but that it failed to actually exercise the thing supposed to be tested, pattern matching.
&gt; I suppose one can point to the huge number of FRP success stories. Oh, wait. ;) This was what I meant by "(even if I think I roughly know the answer)". &gt; The iPwn guys wound up spending 2 years fighting with FRP rather than shipping their last game. Link to their feedback?
What he says about the usual TDD approach encouraging writing programs incrementally by handling the different cases separately, vs. functional programming encouraging the construction of complete (all-case-encompassing) solution by composition of transformations is interesting. The obvious way to test such functions is to write a specification for it, and then translate it in the test language at hand (eg. Quickcheck). If your testing tools supports rich specifications (more or less first-order formulas), then a single spec/test can be enough to exercise the problem, otherwise you may again split the specification in different cases to test them separately -- but that doesn't mean the code must reproduce this splitted structure. This suggests a different way to write code incrementally: instead of handling first this case then this other case, you should build first this part of the pipeline, then this other, as two separate functions that can be tested independently, then composed.
The phrase "doable by pleasant conversation with ghc typecheckers" has been added to my list of favorite *euphemisms*.
&gt; Whilst applicative functors are standard, again as "design" they are an odd proposition - being functors that don't have lawful monad instance I fail to see why. Applicatives show great applicability (pun) to problems like command-line parsing (see Paolo Capriotti's blog http://paolocapriotti.com/blog/2012/04/27/applicative-option-parser/ for an example). And a lot can be done with Parsec by simply relying on the Applicative/Alternative instances of Parser.
The benchmark is constructed in such a way that it should nominally testing the cost of solely the *pattern matching* cost. It was not doing this. The way it was actually constructed we were also paying for the cost of the construction, while ocaml, etc. were not. Not surprisingly this rendered us slower than the 'competition', which had strict phase separation between construction and matching. They had a strictness annotation that they thought would fix it, but it only forced the head of the list due to a misunderstanding. Forcing the _head_ of a list is not the same as forcing the entire list. It was an obvious and honest mistake, and one I *expected* to find given the relative performance between Haskell and Ocaml, benchmark largely unseen and when dons supplied the link to the actual code. No strictness annotations in data structures were necessary. Before I saw the benchmarks I assumed it was either that or they were pattern matching on a syntax tree of some other sort that would have unforced members. We have dynamic pointer tagging, ocaml does not, so I would expect under average circumstances to get a rather significant speed boost based on my past experiences with benchmarking both, so when their benchmarks showed us not only losing, but losing badly it struck me that there was a secondary effect confounding the results. If you have an evaluated thunk with a low enough constructor id that dynamic pointer tagging kicks in, then its a direct jump. ~62% of thunks are already evaluated on average, though the highs and lows vary quite a bit from benchmark to benchmark. 99.2% of references found in haskell code are evaluated and tagged in real world code. In their benchmark, &lt;8% get tagged, so while it is an interesting stress test of an unrealistic condition, under more realistic conditions performance differs by a factor of 2. My problem wasn't that the benchmarks weren't designed by experts. My problem is that a.) the benchmarks aren't testing the same thing from language to language and b.) that measurably they aren't realistic. I do not consider proper use of strictness annotations to be an 'expert-only' concern for Haskell. You _need_ strictness annotations on things like size annotations to avoid building up towers of thunks and leaking space like mad. This isn't the domain of the expert. It is very much an everyman concern. If you wanted to live in a version of Haskell where this isn't an operant concern, someone would need to build a version of Haskell based on Rob Ennals' thesis, which can evaluate rather naive Haskell code quite efficiently. Nobody has stepped up to do so. Fixing the fact that we were paying for lots of extra set up makes a 20% difference. Fixing the compilation flags and distribution of the arguments cuts the runtime in half. This takes us from near the back of the pack with the c++ guys who have to pay for all sorts of crazy RTTI nonsense to so far out in front there its not funny.
&gt; so it is impressive that they are in the same ball park with a brand new library. Not that much if they were reusing all this experience, which would just be a Normal Thing. They don't tell how much time it took them to achieve it, maybe it took them a while to extract and adapt the already existing pattern matching techniques to C++.
This was just a brief survey of the problematic nature of the web application space, and then scratched the surface of using Snap and Heist. Pretty lightweight compared to the normally type-theory heavy stuff at the monthly Functional Programming SIG in Sydney, but we'd had a few discussions about web stuff of late and I wanted to show my perspective. Doing slides using an HTML based presentation system was a bit of an experiment. On the upside, pretty amazing this can be done with CSS! On the other hand, no PDF export, boo. Apologies if it doesn't render quite right for you.
Lots of plugins and customizations, but nothing Haskell-related as of now... I am currently thinking of which haskell plugins to integrate in a "haskell" github branch. On GitHub: http://github.com/joaopizani/modular-xplatform-vim-cfg Blog post with some background: http://joaopizani.hopto.org/en/2012/06/modular-xplatform-vim-cfg/
One you've matched patterns, you never turn back.
I've used phantomjs in the past with an appropriate stylesheet and a little coffeescript.
hastache seems to have some good adoption. The mustache style never quite tickled my fancy, but as I noted templating is as much a question of personal taste as anything else. When I gave the talk I went to some pain to point out there's no right answer, nor does there need to be. The nice thing about Haskell is how easy it is to switch from one templating engine to another regardless of the web handling framework you might be using.
&gt; The nice thing about Haskell is how easy it is to switch from one templating engine to another regardless of the web handling framework you might be using. Since everybody seems to have settled on backing to Blaze, you can even mix-and-match freely on the same page. Heist + Hamlet is probably silly, but I've got a couple places where I take the result of rendering a "real template" and just smack a `div` in front of it using standard Blaze, without having to "template". (Also, the factoring in the template is better; I can inline it into an element or not, and the decision is easy and elsewhere, as it should be.) It's really quite nice. My Django projects inevitably end up with about half the templates being just these little compositional glue pieces that have just one or two tags and an inclusion like that, it really clutters up the template directory.
I'm sure you could write a Haskell compiler using template code. The ultimate in template metaprogramming.
C++ is a car built by taking parts from 20 different cars, hitting them with a sledge hammer, throwing them in a pile then dumping a tonne of glue on top of it. As a result of this it has 5 steering wheels and no brakes. Everyone gets where they want to go with C++. Few survive the trip.
Love ekmett's comments on that thread, I wish there were a way to link to a specific comment on facebook. 
fay is looking increasingly nice :-)
So one of my upcoming blog posts will discuss the specific advantages of the identity and associativity laws and why they give the nice context-free properties, among other things. The best answer I can give at the moment is that associativity and identity and functors go a really long way and most software will see a considerable improvement in quality just by structuring itself along those lines. You don't have to use advanced category theory abstractions to benefit from category theory.
Your edited solution is actually perfect. I'm stealing that. :)
Click the timestamp.
Thank you!
There might also be some confusion over the meaning of well typed programs not "going wrong". 
 having dependencies you don't use also makes it harder to use...
Certainly, applicatives are useful and the applicative style is nicer for parsers if you don't need the power of monads. My argument was they don't really work as design patterns or a "design repertoire" like monads / monad transformers do. Particular applicatives seem like *one-off designs* (the Swierstra parsers, or the ones in your link) rather than *reusable solutions*.
I found his description slightly vague, or maybe I am just not versed enough in the TDD vernacular. Particularly I am confused about the tension between decomposition and composition. Is he saying that he typically writes a test for a large function first and uses the process of writing tests to aid his decomposition? If so I agree with his observation that he should start with small components verify them and compose them, but I'm not sure what that has anything to do with Haskell. 
Isn't mustache essentially the same as heist, but with special syntax instead of normal tags? I mean the underlying principle is the same, no logic exists in the templates, you use placeholders that get replaced with the real content.
It's pretty much those three categories most of the time. They cover the vast majority of my use cases. However, using functions is not the same thing as using function composition and using monads is not the same thing as using Kleisli composition. I specifically emphasize use the composition operator of each category. For example, I would not consider the following code compositional: \a -&gt; do b &lt;- f a c &lt;- g a b h a b c I cannot take any chunk of that code and split it off and factor it into its own component, because it is not written using Kleisli composition. However, if I refactor it to use Kleisli composition, I get: (\a -&gt; fmap (a ,) $ f a) &gt;=&gt; (\(a, b) -&gt; fmap (a, b,) $ g a b) &gt;=&gt; (\(a, b, c) -&gt; h a b c) By doing so, I've made the dependence of each stage on the previous stages explicit, which means I can now factor each step into its own logical unit, i.e: -- Some contrived partitioning scheme phase1 =(\a -&gt; fmap (a ,) $ f a) phase2a = (\(a, b) -&gt; fmap (a, b,) $ g a b) phase2b =(\(a, b, c) -&gt; h a b c) phase2 = phase2a &gt;=&gt; phase2b This now lets me reason about and test each portion of code independently instead of tightly integrating them together. Also, because they are now morphisms in the Kleisli category, I can apply functors to these morphisms transparently and easily reason about what these functors do, such as: -- mapM = A functor between two Kleisli categories -- that transforms the objects of the category mapM :: (Monad m) =&gt; (a -&gt; m b) -&gt; ([a] -&gt; m [b]) phase2a :: (Monad m) =&gt; (a, b) -&gt; m (a, b, c) phase2b :: (Monad m) =&gt; (a, b, c) -&gt; m d mapM phase2a :: (Monad m) =&gt; [(a, b)] -&gt; m [(a, b, c)] mapM phase2b :: (Monad m) =&gt; [(a, b, c)] -&gt; m d mapM phase2 :: (Monad m) =&gt; [(a, b)] -&gt; m [d] mapM phase2 = mapM phase2a &gt;=&gt; mapM phase2b I don't even really have to invoke Kleisli categories to show this. The same principle applies to functions. Just translate the above example to functions to get: myFunc a = let b = f a c = g a b in h a b c Again, this creates a function whose internals are difficult to decompose and are tightly integrated together. So what I'm trying to say is that it's not enough to just use something that has a category. You have to specifically use the composition operator as much as possible, which forces you to write in a decomposable style. It's not enough to use functions; you must use function composition as much as possible. Similarly, it's not enough to use monads, you should Kleisli composition as much as possible. I mean, these are just rules of thumb. It's not like 100% of my code is composition and even I make exceptions. It's like the guideline for writing pure code: we always tell people to factor as much code as possible into pure functions and leave as little `IO` residue behind as possible, but we acknowledge that there are reasonable exceptions to be made. Similarly, I advocate that you should factor as much code as possible into the composition operator and leave as little non-compositional residue as possible, acknowledging that there are reasonable exceptions to be made.
Ahh! Thx.
Excellent answer. This expresses something that I have been discovering recently firstly doing fairly dumb datastructure manipulation in Haskell and secondly doing stream processing in Python. "Prefer composition over application" is what I've been calling it. 
I hoped this was a link to a new episode.
Yes, but in this case the dependencies are all in the platform, or are relatively small and needed for other things within the package itself. I find this particular complaint to get more traction when the obligation is gratuitous and falls outside of the platform. If I started needing some random unicode symbols library, then I'd be the first in line to agree with you. =P
Note that I haven't suggested ```(MonadState WorldState m) =&gt; m ()``` so that I wouldn't hurt your feelings. Thoughtful, huh? \^\^
 = let b = f a c = g a b in h a b c --- = let fa = f a; ga = g a; ha = h a in ha fa (ga fa) --- = let fa = f a; ga = g a; ha = h a haf = ha fa; gaf = ga fa in haf gaf ? *edit* and for the monadic version \a -&gt; do b &lt;- f a c &lt;- g a b h a b c --- foo a = hg =&lt;&lt; f a where hg x = h a x =&lt;&lt; g a x
Simplicity is relative. Next to the problem of composable, extensible interpreters, the EDLS solution at the link is quite simple (Edit: as compared to a monad transformer stack).
I think that's just syntax sugar for `f x = case x of ...`
Sigh. Posts like this do not encourage me in my seemingly eternal "charlie-brown-and-football" recurring attempts to master this language. 
It's cool that category theory has inspired you to write more compositional programs and that you enjoy waxing poetic about it, but I still don't think your observations have much to do with category theory per se. :) Compositionality is a general property of programs that is nice to have - it is nice to be able to assemble more complex functionality from smaller components which can be written independently and reused in different contexts. Associative operations with identities facilitate this, yes. But this observation has little to do with category theory. It's quite common to have domains in which there is more than one operator--there are still laws relating the operators to each other and to various distinguished values, the programs that can be assembled are compositional, but there isn't composition in the categorical sense.
:) I actually don't mind that much. It's more of a "I won't use it in my code or explanations" kind of thing. I appreciate the thought, though!
Don't worry about it. It's cool stuff, but not necessary at all to learning Haskell. I pretended `bottom` didn't exist for a long time when I was getting started, and it was fine. Even when you do have bottom values, you can ignore the issues dealt with here until you start to do very particular types of equational reasoning. (and the paper itself is basically about when/how you can ignore it even then :-) )
Of course, I'm sure you didn't mean to belittle anyone, it was more like a remark to keep in mind for future submissions and handling of research papers -- in this domain, attribution is important. Plus you get to complain about reddit not allowing to edit titles.
I'll go out on a limb and guess that what it has to do with category theory specifically is that Tekmo gained much of his appreciation for more formal notions of compositionality (rather than the hand-wavy definitions in common use) from the specific example of what `(-&gt;)` and pipes have in common, which is a pretty generic sort of category. It should not be terribly surprising that material about categories as a mathematical structure would be expressed in terms of category theory, and as a Haskell programmer with that as a starting point it's natural to see things from a category theory-influenced perspective and branch out along those lines. Which I suspect is not too far from a point you were trying to make. His observations seem to have more to do with categories as algebra-flavored things than with category theory itself, so I suspect that mining algebraic structures directly for useful abstractions would be at least as productive, if not moreso...
&gt;&gt; Do you have any idea how many times a day women hear that comment? &gt; This I honestly was ignorant of Not to belabor the point, but a friend of mine just wrote [a good blog post](http://lederhosen.dreamwidth.org/795054.html) about this topic. You may be interested in reading it.
I think it's actually encouraging. What it's saying is that ignoring the differences caused by seq, partial values and unrestricted recursion is the morally right thing to do. In other words it's morally right to come at haskell with a great deal of simplifications. Or at least that's how I interpreted it.
I don't remember what exactly I did but wx works for me with GHC 7.6.1, Windows 8 RP, mingw 4.5.2. I haven't used 4.7 because it creates sections that GHC doesn't understand. You have to recompile WX using mingw yourself. I used mingw32-make way and WX 2.9. I had to get wxconfig code and fix it myself for it to link anything properly. wxhaskell project has a git repository where they have fixed wxconfig (Windows one). I can run various examples. Linking takes forever to do but executable doesn't crash.
What are you envisioning? My first thought is something like demo = do objectiveFn $ \(x,y) -&gt; x^2 + y^2 equalityConstraint $ \(x,y) -&gt; a * x + b * y == c inequalityConstraint $ \(x,y) -&gt; y &lt; 3 minimize Thinking about it for more than two seconds, I don't think you'd be able to supply arbitrary functions as your constraints. Perhaps you could supply a matrix A and vector b, specifying a linear constraint that Ax = b.
What do you mean by installing from binary? Network is easy to build, wx is hard, gtk is doable (I haven't checked recently). You just have to get MinGW and MSYS. Mingw is bundled with GHC. I get mingw-get-install, download mingw and msys into d:\mingw and then make symbolic link from d:\ghc\mingw\ to d:\mingw. Lately it got a bit more involved, GCC 4.6 and 4.7 generate PE/COFF sections GHC doesn't like so what I do instead is copying msys on top of d:\ghc\mingw\ and run msys shell from there. I use msys shell only to compile 'network'. That way I don't have to use Haskell Platform at all. You might also want to get cabal-install in binary form and put it in PATH. I use Rapid environment editor to handle all those tweaks to PATH.
You're not alone. Running Ubuntu 12.04 with GHC 7.4 here, couldn't install last wxHaskell branch (0.90.*): whichever way I tried, either wxHaskell compilation fails, or that of wxWidgets 2.9 (development branch, ergo unavailable in the Ubuntu repositories) does.
Actually, not directly. XAML is a domain specific language (DSL) for declaring GUI layouts. But remember that wxHaskell ang Gtk2Hs are, in essence, DSLs for GUI layouts as well, it's just that they are *embedded* in Haskell (aka EDSL). From that point of view, we already have XAML. What I mean is that both the semantics and the implementation of our GUI EDSLs would greatly benefit from additional efforts. I have a few ideas of how to make them slicker and more pleasant, but I don't want to go into the business of maintaining a GUI framework all by myself.
I will write list of things I did in order I did them later today.
I installed Gtk on Windows XP once, and all I remember is being impressed at how easy it was. I normally expect anything on Windows to be a giant pain :)
That's normal. Windows only looks for dlls in the current directory and in `%PATH%`
Someone tried to create /r/haskell-beginners I think. Or maybe it's /r/haskell-questions. Anyway, have you checked out the haskell-beginners mailing list? It's more bang for your lurking buck. E.g. **Hask** to a first approximation is cartesian closed, and an accessible question just asked yesterday referenced exactly that. 
I just wrote my own OpenGL-ES-based widget set. Seemed a lot easier. And it probably was. There are just two widgets anyway: View and Choice. (Where Choice is View with a Controller pseudo-widget.) Done. the rest is all part of the general nonsense we consider normal applications nowadays.
(I am the author of this blog post.)
The mechanism for enumerating in feat is through Applicative, and with QuickChecks monadic generators you can branch among and there are probably other limitations (but you can enumerate mutually recursive datatypes without writing any code, which is pretty cool.) Yeah SmallCheck doesn't get mentioned enough. 
I just want to say thanks for all the work you guys do, and I agree that instructive material for Haskell is something profoundly lacking, especially practical code examples with commentary from the author. Is there any interest in Haskell for GUI applications? I always assumed this was a large untapped user base that gets discouraged by the poor state of Haskell GUI libraries. I get the impression that the Haskell community puts too many of its eggs in the server side web development basket and should diversify a little.
I think better bindings for GUI frameworks GTK, QT, WX and I guess the MS stack would be really useful for little apps. However I think haskell is a bit far behind the curve as it is for larger apps. Perhaps a more productive effort in this area would be to enhance cross language and platform client server libraries instead. This would enable for example a front office team building apps for trading to layer the latest and greatest GUI client over their bullet proof, high performance core server app. I also think that the ubiquity of the web makes it a very attractive target, so I'd love to see better ways of building web apps in haskell (that's why I'm contributing to fay). Here's how I structure most of my stuff: A whole bunch of different languages all talking over a common layer of zeromq and protocol buffers. We have the "glue" in python, various stuff in C++ and C# and web based GUIs in modern coffeescript/backbone/bootstrap. I think haskell could fit very well into the glue and web GUIS area, and the separation of communications layer give you the option of hooking in things like excel, WCF is needs be. Also, the perception of haskell as arcane and hard to learn is basically scuppering any possibility of getting it in the door at my small team.
 avoid success at all costs has been revised. The new code is: avoid $ success at all costs
I'm working - very, very, very slowly, and non-exclusively - on bindings for Qt. I have most of the big ideas mapped out in my head, but implementing and ironing and polishing them takes a long time, especially after a day job and trying to keep up a social life. I'm not mentioning this for any particular reason except to show that yes, there is some interest, at least on my part.
But it's not native on the browser.
We have no problems making library contributions but currently we are not in a position to make the types of big investments that haskell needs, notably stack traces, tools for visualizing strictness and better debuggers. I for one am excited at the possibility of getting these types of tools even if we have to pay for them. 
This list is great, and FP Complete's promised contributions will surely be appreciated. I hope to see someone take up the interoperability banner soon, too. It seems like more reasonable embedability into other programs would lessen the problem of missing bindings to essential libraries. Unfortunately, every time I work on integrating Haskell into someone else's workflow based on another language(s), I age 5 years in a week and eventually am forced to hide Haskell behind a file system- or socket-mediated barrier.
No, that was last month. Now it's this: avoid success $ at all costs
&gt; I get the impression that the Haskell community puts too many of its eggs in the server side web development basket and should diversify a little. I don't know about that. I know that there are several Haskell web frameworks, but how many public websites written in Haskell are out there?
This is why I made [Elm](http://elm-lang.org/)! I remember jumping through the same hoops before getting tired of it all. Not sure if Elm suits your purpose, but it makes graphics a whole lot easier!
FRP was actually the motivation for me to try to install wx. The first thing I tried was actually to write a bare-bones GUI application with the Win32 API which calls into Haskell (through a `foreign export`-ed entry point) for most of the logic. This worked but was somewhat tedious.
&gt; I also think that the ubiquity of the web makes it a very attractive target, so I'd love to see better ways of building web apps in haskell (that's why I'm contributing to fay). My bet is on the [haste compiler][1]; it shows [very impressive results][2]. At the moment, the most pressing problem with all Haskell -&gt; JS approaches is that we need the ability to install cabal packages and do compilation conditional on the JS backend. [1]: https://github.com/valderman/haste-compiler [2]: http://jshaskell.blogspot.de/2012/09/breakout.html
I'm happy to port my reactive-banana-wx examples to any other GUI framework, but the selection is somewhat unsatisfactory at the moment. I think a binary installation of wxHaskell would alleviate much pain. Gtk is difficult to install on Mac. Tcl/Tk doesn't have nice Haskell bindings. My current bet is on the Haskell -&gt; JavaScript approach and the [haste compiler][1]. [1]: https://github.com/valderman/haste-compiler [2]: http://jshaskell.blogspot.de/2012/09/breakout.html
What do you mean? It's already possible to write an entire application in QML and JavaScript if that floats your boat. But it's not pleasant. Usually what you do is you write the UI logic in QML and the rest of it in C++ using the Qt APIs. The only difference here would be that you would write the UI logic in QML and the rest of it in Haskell using the Qt APIs. (Unless it turns out that a nice DSL for QWidgets and FRP are preferable to QML, and you write all of it in Haskell. ;)
The latter kind is probably one application of the work. The only way (that I can think of) to define a Leaf that can be an optional child of a Node would be to make both of them part of an inheritance hierarchy, where, for example, Leaf might derive from Node. Then when traversing the Node left and right pointers, the proposed pattern matching would choose behavior based on whether the next Node is a Node or a Leaf. (If I understand the article correctly.)
I understand that and there's probably a way of doing that but not when you're using incompatible GCC versions. At the moment there are too many things that can break or be out of sync for it to be reliable. There's also too small wxhaskell community at the moment. GHC 7.6 is also quite new so some tiny fixes are needed here and there. It'd be awesome if compiling gtk/wx along with Haskell bindings was possible from cabal. I just got used to the fact that you have to recompile half of Haskell pacakages due to version constraints and almost all of their C/C++ dependencies.
You know about [qtHaskell](http://qthaskell.berlios.de/)? I have no idea how good that is, just trying to avoid duplicate effort if possible. 
Yeah, I saw that and immediately disagreed with almost every choice that they made. ;) I mean, look at the Haddocks! It's horrid.
Thanks, good luck with your work.
`'a void' : success at all costs` ?
imho the best thing you can do for that project is throw it on github or [darcshub](http://hub.darcs.net/), state some clear principles and goals for the project, and throw anything you've got up there, and let other people help.
I agree with you, A simple/stable FFI for python (writing the glue/gui in python and the extension in haskell) would be a very efficient way to get into many code shop. From a project manager perspective, it would a low risk scheme and give ample opportunities to prove the value of the language and to ramp the team into haskell mastery.
&gt; Facebook, Google, Citations ? 
I have a few comments and questions regarding some of these points. &gt; The community resources, especially the component libraries, are incredibly valuable, but need to be strengthened and curated ... online resources should be reliably up, accurate, and secure. What is the status of Hackage2? Are there any concrete plans to shift responsibility of Hackage from Galios to the Haskell.org Committee? &gt; An easy cross-platform IDE for learning and for daily productivity is a must. If we want this feature *now* then the obvious solution is to bolster EclipseFP. I have a hard time believing that effort is well-spent creating a brand new IDE, but I'm hardly an expert in such things. &gt; Error messages could more clearly pinpoint the source of a mistake and how to fix it. I think it's important to note that we're probably talking about GHC. Are there any other compilers that the Haskell community deems "worthy"? What about the numerous HS-&gt;JS compilers? Should the recommended approach to these be to leverage GHC for type checking &amp; error messages? &gt; Educational resources online are very good, but need to be very much better I will again draw attention to [Mezzo Haskell](https://github.com/mezzohaskell/mezzohaskell). Has FPComplete considered paying people (possibly including, but not limited to, Michael Snoyman) to work on that project as well? Perhaps it would be beneficial to supplement "hackathons" and "code sprints" with comparable get-togethers focused on enhancing documentation.
Sure: `endPoints = endPoints' :: [ Edge ] -&gt; [ Node ]`
There's a difference between defining f as a different function when given two different types of the same base and--in line with what this is being proposed--extending the syntax of the switch statement so that you can define f as working differently in both cases. Typically, what will be done is f will be a virtual function and Node and Leaf will inherit from NodeType, which will at least declared f. Which f to call gets deduced at run-time. That is how you solve *that* problem in C++, the proposal solves a different problem.
the first isn't, the second is. and you *always* have to pick a semantic domain, otherwise you don't have a semantics :-)
Some of this stuff is partly a research problem, though--not just a matter of implementation. Like, okay--you want a debugger? GHC has one. Except that stepping through lazy evaluation is like listening to a stream of consciousness narrative told by a 5-year-old with ADHD. A conventional IDE debugger is not really going to make this less disorienting. Creating a Haskell debugger that's up to expected levels of usability for industry tools will likely require a fundamentally different UI approach.
You can :) (not that I really suggest doing this) {-# LANGUAGE TypeFamilies #-} endPoints :: (Eq node, edge ~ (node, node)) =&gt; [edge] -&gt; [node] 
Not having read the article (since the link is broken) I have to say this title comes off as pretty snide. Why not just say "MIT introduces new declarative programming language for computer vision"? Unless of course the article actually indicates that the researchers were until recently ignorant of declarative programming; if that turns out to be the case I'll happily retract this comment.
I would write type Edge a = (a, a) endPoints :: Eq a =&gt; [Edge a] -&gt; [a]
Write the function with the more parametric signature, then write a helper that re-exports the specific case or just put a comment specifying what the intended use-case is.
 Debugging can be split into correctness debugging and performance debugging. For the former, there's no need to consider laziness, just allow expanding subexpressions arbitrarily, and stepping through io code. For the latter, use a profiler. In any case I don't see why you'd want to step through laziness. 
I'm not under the impression that Haskell is used much in AI, but from a language perspective it would be great. It lacks libraries and an AI community though. More common are languages like C, Java, Python, and various Lisp dialects. Still, Haskell is probably the best language for learning the functional paradigm, which would be very useful in AI.
There's a paper from 2004 that answers that question in the positive: [Escape from Zurg: An Exercise in Logic Programming](http://web.engr.oregonstate.edu/~erwig/papers/Zurg_JFP04.pdf). It was also my experience, having implemented different AI Planning algorithms in Haskell (classical, hierarchical, non-deterministic, probabilistic) and participating in the Google AI Challenges (minimax with alpha-beta pruning has never been so easy to implement). Back in 2007 there was also a proposal to implement many AI algorithms (and organize a proper module hierarchy), called the "AI Strike Force". Although it didn't get traction, many people contributed with libraries, but it's never too late to get that going again. **TL;DR** yes, go for it!
&gt; It was, however, quickly adapted for __. Thought that quote was worth pointing out/A good description of Lisp =]
It would be nice if we had a TH library for specifying this sort of thing, without needing to resort to wx.
Very cool. I particularly enjoyed hearing someone opining about the benefits of signal-function FRP. Performance considerations and the state of maintenance have kept me from AFRP libs. So far I have considered Arrows a smell, preferring Kleisli. I will have to take another gander at an AFRP lib, and perhaps finally experience a real fit for them. Good luck on your thesis. I look forward to the final publication. 
We did the research on plumbing dwarf symbols all the way through. It was estimated at 6 developer months so figure a bit more than that. I figured that there would be two call stacks one for creation and one for evaluation. Understanding how laziness is effecting your program is one of the biggest pain points for shipping big apps. It is something that must be solved and I see very little motivation for solving it in the traditional community. Today we are in a bad spot where if I get a blocked on STM exception the runtime has already thrown away the context before throwing the exception. I have apps with dozens of different transactions. Who is blocked where? No one can say. 
This is unlikely to resonate with commercial or potential commercial users. What is desired is a more polished GHC, all the features with more shine.
I'm glad you enjoyed it. There was general consensus in the smaller-group post-discussion that Arrows are in fact a "smell" as you put it. Once the thesis defense is behind me, I fully intend to release and maintain my library, so there's something to look forward to. If you want to toy around with it now, the code is on GitHub: http://github.com/eamsden/pushbasedFRP
From my limited experience with machine learning in Haskell (I have mostly done ML in C and Prolog): there aren't many machine learning packages available in Haskell. There are bindings for libraries such as libsvm, liblinear, and liblbfgs. Some of them are ok, some of them are not really great. **But:** most machine learning software use a very simple input format. So, it's usually trivial to write extracted training data to a file and train using an external utility. For feature extraction, on the other hand, Haskell is an exceptional language! It's trivial in Haskell to walk through most data structures and extract features. Things like the list monad allow you to simulate backtracking, to declaratively write feature extractors, etc. What Haskell also excelled at for me is in the preparation of training data. We train models for parse disambiguation and fluency ranking, but before using a parameter estimator, we massage the data a little. E.g. we select random samples of training instances in a context to reduce training time and normalize quality scores. For this, I wrote a small package and a couple of command-line utilities that use the conduit package (and enumerator before). I have never seen a language that allows you to do I/O in such a compact fashion. E.g., the main expression of a the program that normalizes quality scores is: runResourceT (CB.sourceHandle stdin $= CB.lines $= bsToTrainingInstance $= groupByKey $= score $= concat $= trainingInstanceToBS $= addNewLine $$ CB.sinkHandle stdout) It almost resembles a UNIX pipeline ;). Regardless the state of AI packages, I would learn Haskell anyway. It makes you a better programmer in general. And it is really a lot of fun!
GHCi isn't what I would consider a debugger in the production sense. Can it debug compiled applications? Nope. How about showing evaluation history when an exception is thrown? In simple cases. Can it debug mixed compiled/interpreted sessions? Until you need to step into compiled code. GHCi's loader precludes using C++ FFI on OSX (and probably other operating systems), which also limits the usefulness it *could* have. Would a Haskell debugger absolutely require a different UI approach? I'm not so sure. Concepts from native debuggers are still useful, even in the traditional debugger form factor. Stepping through code could be avoided entirely by setting breakpoints in the past (see Microsoft's Time Travel Tracing (quite fast) and gdb's reverse debugging (slow ime)). Even without stepping, native debuggers can be used to: * Inspect a value, with a Show instance (if one is present) * Break when a value/function/thunk is evaluated * Show the type, values and function bound to a thunk (particularly useful for blackhole/loop exceptions) * Launch missiles * Display the evaluation stack (for IO heavy code this can approximate a call stack) * Find GC roots for a value Mix in some [line numbers](https://github.com/scpmw/ghc/commits/profiling) and local symbols and it solves real problems ~now while we wait for something better... even if "better" is GHCi married to LLDB and proper DWARF symbols :-)
Haskell works very well for AI work. I use Haskell to do neural networks. [Automatic differentiation](http://hackage.haskell.org/package/ad) makes that trivial. I use Haskell to do support vector machines, regression, conjugate gradient descent, graphical models, metropolis hastings, hamiltonian monte carlo, etc. You can use it for genetic programming by defining an AST for what you want do mutation in, and there, unlike the lisp version, you can have good assurance that you've covered all of the cases in your AST. Now, the question is, is it popular in that field? Sadly, it is nowhere near as prevalent in AI as lisp/scheme.
...how does this fit into the ongoing iteratee-design-space (iteratee, enumerator, pipes, conduit, iterIO, et al.) exploration? What's its relation to the existing approaches?
Sorry for the late reply, was away for a while. The stack you mention could only happen, given the line numbers, in code that was changed approx. a year ago. So if you had notified me of the error it could have been fixed a long time ago, and would have benefited maybe others (though nobody else ever reported that problem). From what I can see, this error could only happen if cabal is not in your path. I have now coded more defensively so you'll won't get the error anymore, and you won't be asked to install anything if we can't find cabal anyway. Which brings me to the second point. Cabal is not in your Path, you may have a non standard install or what not, and that's exactly why I'm asking people before installing a haskell package on their system. Yes it's required for EclipseFP to work, but maybe you have a carefully managed cabal install and you don't want to willy nilly install packages with long dependencies. Maybe you have installed the required executables in non standard locations and want to point to them. And anyway, it's good form I think to warn users that you're going to install something extra on their system and give them the chance to abort. Frankly, it's one extra click each time you install EclipseFP, I don't think it's really so demanding. I also don't see what's wrong with the dev page (apart from a bit about scion that is outdated, that's true). It tells you where to get the source, how to configure Eclipse to run from source and trace errors. Once you're there, making changes in source code and seeing what happens when you relaunch the project is easy. You get the source, you write a Java class, reference it in your plugin.xml file and launch your project. Done. And this page's mission is not to explain to you how to write a Eclipse plugin, there's plenty of content on that on the web already. Maybe writing for Eclipse is more complex than writing for Emacs, but there's not much I can do about that. you're free to prefer Emacs and the Haskell plugins that go with it!
&gt; wasn't sure if OP gets notified of top level comments. (They do on self-posts. Not sure on regular posts.)
Regarding the two function implementations on page 5, left to the reader to determine which doesn't diverge: walk mn (Leaf n) = (n, if n &gt; 2 * mn then Leaf mn else Leaf n) walk mn (Leaf n) = (n, Leaf (if n &gt; 2 * mn then mn else n)) is the answer (spoiler) the second one? I'm thinking this because you can deconstruct on a Leaf without finishing the computation. If that is the case then yeah, that is a pretty sneaky gotcha. Edit: So I've gotten to the section where they present code. It doesn't read very nice, even if it allows them to compose transducers. To the people who regularly work with iteratees is this how code typically looks, and if so do you get used to it?
&gt; Today we are in a bad spot where if I get a blocked on STM exception the runtime has already thrown away the context before throwing the exception. I have apps with dozens of different transactions. Who is blocked where? No one can say. Then again, debugging a concurrent program is kind of a hopeless task anyway. How do you know that you've caught all the Heisenbugs? 
I'm new to Haskell - how do you get (and more importantly avoid) memory leaks? Edit - http://www.haskell.org/haskellwiki/Memory_leak pretty much covers it.
Using monads/applicatives/categories/you name it to solve real world problems seems at first sight overwhelming and overrated (the so-called "OMG why can't they use mutable objects like everybody with a normal brain?!!"-effect). But after some training, the practical advantages of those unusual and seemingly esoteric constructs become more clear: the safety they bring in, the expressivity, composability, the way they make testing and parallelizing easy, etc. Hence the saying (which, this time, I do not invent): "Haskell is an excellent imperative language".
Now, complementary question: is there other ways to get memory leaks than by having too much laziness or value sharing? (I of course exclude improper use of foreign functions. Managing allocations and de-allocations yourself _is_ the #1 source of memory leaks)
Why not?
&gt; If we want this feature now then the obvious solution is to bolster EclipseFP. That's right. &gt; Michael Snoyman is taking the lead on this issue for FP Complete, working with several contributors. Contact him if you want to help. Why should we help someone who willfully chose not to help the currently existing IDEs without any real good explanation? Sorry if this feels rude, but it feels like a calculating move, sort of "We do that to get all the credit in the end".
Those reason sound exactly like the reasons I use S5 (HTML-based) slides :)
From first glance, this seems significantly more limited than most iteratee-style designs. Where more advanced stream processors reify the data flow to some degree, this seems to restrict its attention to programs that are essentially iterative loops in a monad. Such iterative loops fit into the larger debate over iteratees, lazy IO, and whatever else as the sentiment "hey, why not just do things with regular old strict IO in a simple, boring way?" What the design in the paper seems to do is pry open what would normally be a closed, non-compositional loop and provide points where functionality can be spliced in at the production or consumption of a value. As far as I can tell, none of this is a new *concept*. In fact, I'm pretty sure I've implemented similar designs. What the paper does offer is an implementation of the design that's strikingly simple without sacrificing much generality, with apparently very low performance overhead vs. simply writing the whole thing as a direct monadic loop. I've only had time to skim the paper, though, so I may be off the mark to some extent.
It's been widely acknowledged for a long time that lazy evaluation is undesirable in some contexts--lazy evaluation that does I/O is particularly obnoxious, which is why the dirty hack that is "lazy IO" is frowned on, and in languages with lazy sequences and unrestricted side effects you can create an unbounded amount of confusion that way. It's already possible to use Haskell in a fully "imperative" style with strict IO, but you sacrifice certain kinds of compositionality in the process, which is what this paper is addressing. In terms of an imperative language, think of being able to take typical while loops and interleave them, splice them together, let one supply the stop condition for the other, &amp;c.--all without modifying the code of the original loops. That's (a vague example of) the kind of thing motivating research like this paper and lots of related work from the past few years.
I'm envisaging something like qp Maximise objective constraints where objective = x * x + y * y + 2 * (x + y + k) constraints = [x + 5 * y &lt;= 12 * k, k * y == 6] for `x,y` of some polymorphic variable type, `+,*,&lt;=,==` which exist to construct the data structure, and `k` probably just an integer. There are many details to think through, but that's the kind of format I'd aim for I think.
The second one, for sure. It's not very sneaky if you've studied lazy evaluation for a while, though.
To be fair, the "only" point where lazy IO does badly is that it cannot release a resource immediately after it falls out of use. Rather, you have to wait for garbage collection to reclaim it. If you're fine with garbage collection, then you can transform every iteratee program into an equivalent (and likely simpler) lazy program.
Yeah, I didn't really think about that, but their 'stream of open, close, node data' representation of trees is much too large, containing lots of streams that don't correspond to valid trees. So they've weakened something that can't possibly go wrong to something that needs to be checked by hand (and don't suggest a stream that statically ensures that opens and closes are correctly balanced, and that the trees represented are binary).
Advantages: * It might be faster, but there need to be comparison benchmarks to prove this. Disadvantages: * No bidirectionality (compared to pipes) * Transducers and consumers are not monads (so you cannot use do notation to build them or vertically concatenate them) * Nothing is a monad transformer. * You can't resource manage consumers (example, a file writer) * Consumers can only loop. I feel like they failed to acknowledge everybody else's contributions, and then proceed to not compare efficiency, which is the only advantage they claimed in the paper. I was a bit disappointed, especially since they apparently did not reach out at all to the rest of the Iteratee community.
Just check out all the Iteratee libraries they mention in the Background section (Iteratee, enumerator, monad-coroutine, iterIO, conduit, pipes). Particularly, Iteratee already uses the codensity transformation to remove the auxiliary data structure, so I don't know what they are claiming.
They have upwards of 20 citations. I think they acknowledged the core contributions from published, peer-reviewed work quite adequately.
Thanks.
For the record, I do plan on publishing my work, but I have to wait until Paolo begins his PhD soon since he will be a coauthor. Also, I don't believe they chose a worse implementation just not to step on our toes. That would be a little silly. :)
The problem is that the strict IO actions are called by lazy actions. So you could demand some resource (like a database connection) to pull some rows out and close the connection once you've finished mapping over them. But if due to lazy evaluation you never finish mapping over them then the database connection is never cancelled. Do this a few times too many and your database starts to run out of available connections. 
Well, it's true that with iteratees, you cannot write a program that reads from a file in an unpredictable manner. However, you can structure a lazy IO program in the same way, i.e. as a strict left fold and reason about it in the same way that you would reason about the iteratee version. The idea is definitely not that one is allowed to use `hGetContents` and hope for the best, but that doesn't mean that you have to put on a straightjacket either. Iteratees do incur the high cost that list functions cannot be used anymore, which breaks the modularity that laziness was designed for. What I like about lazy IO is that it puts the resource usage in the evaluation model, which is a natural place to put it. At least for me, it is natural to view files and network sockets as "memory", and that's why I do not perceive reading from a file as much of a side effect.
_inventing_ iteratees.
I would not say that. Concurrent debugging is just more difficult. I have certainly done my fair share of it. Part of the reason for using the STM is to avoid the traditional hard to replicate issues that arise in concurrent development. The issue is that after the runtime knows an exception will be raised it collects all the state and then creates and throws the exception.
Monadic style for effects treats your base types as something like the types-as-sets in call-by-value, and then all effects are forced outwards. So for instance, if we have `f :: a -&gt; M b` and `g :: b -&gt; M c`, Kleisli composition gives you an `a -&gt; M c` that does all the effects up front before yielding anything. Or, for a more direct example, consider a language where the function space is total. We can extend a data type `T` with a notion of possible non-termination in two ways. The first way is to use a partiality monad: codata D a = Now a | Later (D a) But then writing in monadic style corresponds to call-by-value. `D T` contains delayed `T` values---which are themselves still the set defined in the total language---plus a non-termination value. Kleisli composing functions and whatnot results in functions that terminate only if all their constituents terminate. The other way is that if we know that `T = mu F`, we can construct: codata DT = Later DT | Step (F DT) where we have added delays to T itself. Programming with `DT` allows partial inspection and interleaving of delays with constructors of `T` and whatnot. This corresponds to something non-strict. If you're dealing with effects other than partiality, interleaving monads with your data type mimics non-strict evaluation with effects in a similar way.
You could be a pioneer with Haskell but in you don't go lisp
You can inline the equality and get the same type as Tekmo showed above. It's just a fancy way to write the same thing.
Yes, but the difference is small. type Iteratee a = FreeT ((-&gt;) a) type Generator a = FreeT ((,) a) That distinction is hardly groundbreaking and the linked article reverts back to using generators anyway. What was groundbreaking was how Oleg used them to solve the lazy IO problem.
Not all of them. Gradient descent is in the `ad` package. Graphical models can be done with `passage`. But on the other hand, my metropolis hastings and HMC code is still commercial, and some of the others haven't been packaged up for public consumption.
I feel for you. My Haskell code at work is not concurrent, but it is still frustrating when a bug shows up, and I have to explain to my boss, that I can't just drop a view breakpoints to understand help track down the problem.
We definitely should make the AI strike force resurrect guys. We have nice libraries (hmatrix, ad and others) to build cool stuffs upon.
For me that was the least significant contribution of their work since you are just switching from FreeT ((,) a) m r to FreeT ((-&gt;) a) m r. However, I suppose significance is in the eye of the beholder. Also, don't forget that the paper we are discussing reverts to generators anyway.
All Iteratee libraries are fundamentally stream processors. Anything that is not a stream requires either linearizing it or using an abstraction other than an iteratee.
That's [Time and Space Profiling for Non-Strict, *Higher-Order* Functional Languages](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.6277) btw. Higher-orderness is a biggie here.
cool, where's he starting his grad program at? :) 
Here an example of the equivalence I was referring to: a [lazy IO combinator][1] that provides similar guarantees to iteratees. As soon as the function `withFile` function returns, the file is closed. [1]: http://article.gmane.org/gmane.comp.lang.haskell.beginners/7548
Are you talking about the style of the page or about the anthology of not-necessarily-relevant typeclasses?
Heh, *"Colocated with"* in small font nobody'll notice. **Second International Conference on Certified Programs and Proofs** http://cpp12.kuis.kyoto-u.ac.jp/accepted.html
Seeing as zippers can be viewed as a suspended traversal computation, that sounds like continuations, yeah.
Depending on how you measure, the majority of software right now may well be written for mobile (rather than desktop) use. I'm carefully keeping servers out of the equation, of course, but even then... For one thing, there are already I believe more smartphones in use that desktops and laptops combined. With a big market to target, a lot of people are rushing to target it. Of course a lot of that software is rubbish, and even some of the good stuff is just a good gimmick that people will tire of pretty quickly, and most isn't commercial, but the same has always been true of desktop software too. 
Nottingham, with Graham Hutton
&gt; Did anyone else have trouble with this? I stumbled quickly upon foldr/foldl's folding functions arguments being swapped, and I admit I found it odd and unnecessarily confusing... ...but when you realize what really are foldl and foldr it makes perfect sense, and actually it would have been confusing if foldr and foldl have had the same signature. But you're right, the _naming_ of the type variables could be improved... ...or else is it again due to some greater arcane ~~magic~~ logic for which my epiphany is still to be awaited?
By the way, when it comes to folds I recommend reading [A tutorial on the universality and expressiveness of fold](http://www.cs.nott.ac.uk/~gmh/fold.pdf).
Agree, absolutely.
`embed f = (f .); recover f = f id` is the same transformation as the one used for difference lists (`embed l = (l ++); recover dl = dl []`). It should work for anything with an associative operation and an identity (and also ensures that the operation itself is used in a right-associative way, no matter how the composition is used). This has all the advantages and disadvantages of difference lists. This isn't really the same thing as `lens`'s `(.)`, which is CPSed so composition happens backwards (if `SimpleLens` was a `newtype` with a `Category` instance, `(.)` would compose in the normal order, of course). You can probably do a CPS transformation with any type to get the backwards composition, though. Edit: Actually, to get `embed` to compose backwards, you just need to define `embed f = (. f)` (this is the obvious `Category` generalization of `cps :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; c`).
Additionally, there is an argument to be made for the arguments of foldr to be swapped, i.e. foldr :: (a -&gt; r -&gt; r) -&gt; [a] -&gt; r -&gt; r foldl :: (r -&gt; a -&gt; r) -&gt; r -&gt; [a] -&gt; r Now both of the folds lift a function from type `a` to work on lists of `a` instead.
That also matches the constructor order for `[]`, making `foldr` into a function that would be `list` by analogy to `maybe` and `either`. :] (By the same reasoning, the `False` case would come first in an `if` expression... but for some reason that's never caught on.)
It's also implicit, given type inference. By your reasoning, why talk about a+b vs a+r at all? Also, technically you mean partial application. Partial evaluation is a [more general concept](http://blog.sigfpe.com/2009/05/three-projections-of-doctor-futamura.html).
haha, i noticed that myself as I was writing it, and I decided that both interpretations are constructive / valid readings! 
awesome! sounds like a fun match! 
I admit that we have not yet explained very well why we had to do something different, but in short, the scenarios we're trying to serve are truly quite different due to our strong focus on "expert beginners" (language-switchers who may be pretty new to Haskell, yet need to get serious work done). We're starting with some private conversations with willing collaborators, but will explain ourselves in much more detail in the near future. Our IDE is not going to work the same as what you're used to. As with most tools, we believe core IDE support infrastructure should be able to serve multiple user interfaces with different designs and priorities. In this life I have found that it seldom pays to try and paste motivations onto other people or guess what they are thinking. We really don't care much about credit, we just need something that works differently, and I promise we'll make those differences quite clear.
I don't know that I find them confusing, but I do prefer an algebraic viewpoint on this issue of folds -- that's how I came to property understand what folds were doing in the first place. So instead of `foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b` you'd have data ListF a b = Nil' | Cons' a b lfOut :: [a] -&gt; ListF a [a] lfOut [] = Nil' lfOut (x:xs) = Cons' x xs lfMap :: (b -&gt; c) -&gt; ListF a b -&gt; ListF a c lfMap f Nil' = Nil' lfMap f (Cons' a b) = Cons' a (f b) foldr :: (ListF a b -&gt; b) -&gt; [a] -&gt; b foldr f xs = f (lfMap (foldr f) (lfOut xs))
One-letter identifiers are a way of thinking that comes from math and purely theoretical CS. Every programmer learns, that you should use *descriptive* identifiers for a very good reason. It’s sad that we still have people acting as if code as obfuscated and unintuitive as the above is supposed to be normal. I would write it like this: foldl :: (result -&gt; element -&gt; result) -&gt; result -&gt; [element] -&gt; result foldr :: (element -&gt; result -&gt; result) -&gt; result -&gt; [element] -&gt; result **Try it with other code and type signatures. Suddenly everything becomes clear. So *why not do it like that in the first place*?**
I dont think that's quite as elegant as [this](http://www.reddit.com/r/haskell/comments/10q2ls/the_usual_type_signatures_for_foldl_and_foldr_are/c6fqyog), where you can use partial application with both styles to achieve a function xxx :: [a] -&gt; r
Right, so I talk about reversing the order using the "contravariant Yoneda embedding" in the "Lenses" section. I don't see what this has to do with CPS though.
But then you run into Church vs. Scott. (; My preference would be to use `list` for the Scott encoding and `foldList` for Church, and similarly for other types.
By the way, to get down to the precise shade of the bike shed, what do you think about `r` versus `b` in cases like this?
Yes, you can. Just scroll to the bottom of Control.Proxy where I provide an alternate implementation of unidirectional pipes for backwards compatibility, and also as a proof that the bidirectional implementation is a strict superset of the unidirectional one. However, I believe out of all the criticisms I listed, the most significant one is the inability to write a consumer that does anything other than loop. This abstracts nothing over just hand-writing the effect full loop.
In what way are result and element uninformative? 
In what way *are* they informative? They don't tell you a damn thing about what the function does that isn't already obvious just from looking at the type. ...or, rather, that *would be* obvious if I could just read the type at a glance like the original version. It's easy to recognize letters by shape, so the original can be comprehended without having to mentally parse text. With the verbose version, on the other hand, I'd have stop and read a whole bunch of words. It's not even obvious at a glance that there are only two types being used (yet another benefit to the `a`, `b`, &amp;c. convention, I note). And what is gained by the extra verbosity? It tells me that some type called `element` is a list element, and that some type called `result` is the final result. Hooray! Seriously. Don't do this.
For a functor `f` (not any, but in this case it works), the type `Mu f` is the initial algebra of the functor, with an injection `inj :: f (Mu f) -&gt; Mu f`, and any other algebra is of type `alg :: f b -&gt; b`. This induces a unique morphism from `inj` to `alg`, namely, `fold alg :: Mu f -&gt; a`. For lists, the functor is `ListF a` for your value type `a`, so you get `fold alg :: [a] -&gt; b` where `alg :: ListF b -&gt; b`. Since `ListF a b` is basically the same thing as `Either Unit (a,b)` you can do some fiddling with the type to find that `alg :: (a -&gt; b -&gt; b, b)` so you have `fold :: (a -&gt; b -&gt; b, b) -&gt; [a] -&gt; b` or, after currying, `fold :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b`, which we all know and love. :) Delightfully, the more general definition lets you get folds for arbitrary functors of the right sort, so with appropriate generic programming tools (like dependent types, or just really smart backends) you can define `fold` once and for all, and every type of the right sort just automatically gets `fold` for free (and `map` too, since `map` is just a special `fold`).
On a related issue: when trying to implement some slightly more advanced game search algorithms (namely, scout and it's parallel version, jamboree) I found that the existing imperative descriptions are actually *not* particularly accessible. For Jamboree, I literally had "reverse engineer" the imperative pseudo-code to get a cleaner and higher-level pure functional version with parallelism expressed using strategies. I think parallelism is a major reason to argue that purely functional code is actually much more accessible! 
&gt; I think parallelism is a major reason to argue that purely functional code is actually much more accessible! Are we talking about functional in general or about Haskell? Because if I agree that programming with values is a big boon for making parallelism easy, how Haskell specifically helps it is by lazy evaluation: letting you specify with strategies how a result should be computed.
By "If your library insists on providing bidirectionality" I meant not allowing the purely simpler case. Obviously you can create extensions of things - it's done all the time. Plenty of type-classes have a superset of the interface of some other typeclass and so on. Though of course this still makes my point completely pointless. 
&gt; As with most tools, we believe core IDE support infrastructure should be able to serve multiple user interfaces with different designs and priorities. If this is what you're heading to, then I couldn't agree more.
&gt; give me a binary function, and a **starting** value Nope, only for foldl. For foldr it's not a starting value (that's a common misconception that Haskell tutorials are riddled with, it leads beginners to wrongly think that foldr processes the list _from the right_, i.e. the _end_). &gt; I can switch to the other with a simple s/foldl/foldr/ You can't switch as you please from one fold to another. They don't do the same thing and are fundamentally different: one does tail-recursion, the other guarded recursion. I agree switching from one another is something we've all done while learning the language, but a good heuristic is to consider that the type of fold to use stems directly from the answer to a simple question: "To get the foldX result, do I necessarily have to process the whole list, or can I get a result, even a partial one, without evaluating the whole input?" If you can, it's a foldr, else it's a foldl'. You really seldom want the regular foldl. **EDIT:** Canonical example: to get the last element of a list, you want foldl: safeLast = foldl (flip const) -- "safe" as it doesn't let you away without handling the empty list case. Just like: safeHead = foldr const -- no need to rant anymore about standard 'head' being unsafe! Just use foldr ;) **EDIT 2:** Ok, forget what I said: even in that case, foldl' is better than foldr which still accumulates a thunk. _Without optimizations_ (i.e. without strictness analysis): foldl (flip const) 0 [1..100000] consumes 18MB at most on my laptop whereas the same with foldl' consumes only 1MB. So even in that simple case regular foldl causes a memory leak. So if it occurs even when the folding function is not strict in its first argument, I definitely fail to see where foldl would be preferable to foldl'...
&gt; Nope, only for foldl. For foldr it's not a starting value (that's a common misconception that Haskell tutorials are riddled with, it leads beginners to wrongly think that foldr processes the list from the right, i.e. the end). Yes, I understand that we start from one end or the other, that doesn't change the fact that the *first* 'accumulator' is this value. EDIT: Sorry for pariting exactly what you were trying to refute. I had to re-read your words. I'm looking into this more, I have to say this is very worrying to me if it's true. EDIT: from the docs "foldr, applied to a binary operator, a starting value (typically the right-identity of the operator), and a list, reduces the list using the binary operator, from right to left." What exactly are you saying here? &gt; They don't do the same thing and are fundamentally different: one does tail-recursion, the other guarded recursion. Certainly. I don't mean you get the same semantics, just that you might find yourself saying to yourself "Self, I thought I wanted to foldl for this, but now I'd probably better foldr." Presumably this change in choice will be motivated by the very semantic differences. Tail-recursion vs guarded-recursion isn't the end of the story, given your operator isn't associative. I still think it's valuable to have that be a cheap change. 
&gt; from the docs "foldr, applied to a binary operator, a starting value (typically the right-identity of the operator), and a list, reduces the list using the binary operator, from right to left." What exactly are you saying here? You're right, the docs say that. They are so to follow the same pattern that's used for foldl's doc. What is said is true, but IMHO misleading. Indeed, it does actually _reduce_ the list from the right (as it associates the folding operator, the 'f' function, to the right) and in that sense 'z' is a _starting_ value for the _reduction_, but it does not _process_ the list from the right (or else we couldn't write lazy functions with foldr), because this depends on the way foldr's result is evaluated. That's the property of guarded recursion, whereas tail-recursion forces the entire spine of the list to be evaluated before _any_ result, even partial, can be returned. That is a subtle distinction, I would understand if I wasn't making myself clear. To me, the only good way to present the differences between foldr/foldl is this one: foldr f z (a:b:c:d:e:[]) = a `f` (b `f` (c `f` (d `f` (e `f` z)))) foldl f z (a:b:c:d:e:[]) = ((((z `f` a) `f` b) `f` c) `f` d) `f` e Which is that 'f' and 'z' are only used to _replace_ list constructors. Nothing else. This is true and doesn't build wrong intuitions. Don't burden yourself with stuff like _"reduce"_, or _"from left to right"_. Just think of it in terms of _list transformation_.
By convention, `r` is the result and `a` is some generic type.
I don't think church encoding has order of constructors/destructors specified.
done!
I am very interested in parallelism and not at all interested in concurrency. I like race conditions.
Be stealthy, say "type families".
I would accept either ordering
So I've argued this on /r/haskell before, but I always disclaim things like fold or map, because the letters have meaning by convention. If one is unfortunate enough to be stuck in a language that still does explicit array index iteration, one uses `i` and nobody complains. On the other hand, when I'm reading an intro to Pipes or something and the very first mention of the Pipe type is `Pipe a r m h` or whatever, it puts an unnecessary barrier up to understanding exactly what the type is when you're not already an expert. At least throw me the bone of spelling it out in the Haddock docs at the point of declaration (and nowhere else, that's cool, all further references link back anyhow), or the first time you use it in the tutorial. To me, use of very short types and variables is the same general tradeoff as the general consensus on point-free code. Used judiciously, it's a fantastic aid to comprehensibility. But it certainly can be overused.
I'm surprised AI and topics in that family didn't make your list of interesting subjects :)
I thought about learning Prolog once, but then I learned Haskell and the list monad :)
That example made me think about the difference between using documentation to get acquainted with a library, versus as a reference once you're familiar enough to start programming. Each case probably has its own optimal presentation. Repetition, expanded definitions, and verbosity all tend to be useful to the learning process. Later, though, I think people want to be able to most effectively skim and absorb things at a glance. Long type variables serve as the kind of expanded definition that you want right in front of you while you're learning what a library does. Letters, on the other hand, are great when you want to visually search over a list of functions, looking for patterns in the type signatures. `sideOne^2 + sideTwo^2 = hypotenuse^2` is more useful the first time you read it, but once you've used it a few times, it's easier to recognize or manipulate `a^2 + b^2 = c^2` in various contexts. That's the process that I see myself going through with libraries. So if you agree that there are two diverging optimums, then you probably start asking which use case is more important. Personally, I'd guess that most people eventually spend more time with those type signatures while using the library than while learning it. But learning is hard and necessary, so maybe library authors who produce types with many parameters that they want to keep concise just need to be careful to produce a little extra documentation that caters explicitly to the getting-acquainted phase.
One more step for `foldr`: foldr :: (a -&gt; Endo r) -&gt; [a] -&gt; Endo r And then it's clear it is just a specific instance of `foldMap`: foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; [a] -&gt; m
They are not the same. They are fundamentally different in their associativity and their laziness. e.g.: http://en.wikipedia.org/wiki/Fold_%28higher-order_function%29 You will have to grapple with this for a long time until you understand it but unfortunately it does not boil down to a simple "foldl and foldr are the same thing but with some letters flipped around." Actually you are right when you say "they are different in some fundamental way."
In your pattern-matching variants, `(x:xs)` as a pattern in better than `head`
I was the question asker, and, for the record, I was still a little bit unsatisfied with the answer, even though I accepted it. I discussed it with `copumpkin` again just a week ago to try to clarify my somewhat vague question. What I was really searching for was a way to reduce universally quantified expressions to some sort of normal form so that you could programmatically compute how many expressions inhabit that type. For example, given the type: f :: forall a . a -&gt; a -&gt; a ... I'd want some algorithm that proves it has exactly two inhabitants (ignoring bottom and other misbehaving things). The "algorithm" part is key, because I know how to do it by hand, but I don't know how I would write a program to do it for me. The whole "algebraic" stuff was just my intuition of how I thought I might go about it, because I thought it might be similar to how I can use addition for sum type and multiplication for product types to similarly deduce the number of inhabitants. If anybody knows whether or not this is possible, I'd appreciate it.
`FreeT` differs from `WriterT` in two important ways. You already noted the first difference: streaming. `WriterT` doesn't generate a result until the computation is done, whereas with `FreeT` you can consume the tree as you generate it. All the iteratee libraries use control structures isomorphic to `FreeT` to achieve their streaming behavior (and one of my links gives concrete examples of this). The second gain is more subtle, which is that `Free f`/`FreeT f` generate "lists of functors", whereas `Writer [a]`/`WriterT [a]` generate lists of values. They overlap in the specific case when the functor happens to be `(a ,)`, but the ability to use functors instead of values gives you a considerably richer set of control structures. To see this, think of the difference between a syntax tree (a list of functors) and a stack machine (a list of values). A stack machine can only store a flat list of instructions, whereas a syntax tree can store more complicated program structures since it uses functors instead of values.
You might find [this](http://hackage.haskell.org/package/djinn) interesting. Djinn is able to deduce Haskell functions from type signatures, assuming that such a deduction is possible. It uses the Curry-Howard correspondence to treat the types as statements in intuitionistic logic and then tries to prove them. It can then use the steps of the proof to construct a program. At least, that is my understanding. In addition, it is fun to play with.
I think this is more the inverse of `intercalate` and would be [`splitWhen`](http://hackage.haskell.org/packages/archive/split/0.2.1.1/doc/html/Data-List-Split.html#v:splitWhen). (I may be wrong; I haven't made extensive use of the split library.) If you're looking for creative ways to implement this: import Control.Monad (guard) import Data.List (unfoldr) splitWhen f = unfoldr (\xs -&gt; guard ((not.null) xs) &gt;&gt; Just (break f (tail xs))) . (undefined:) I have some sort of irrational attraction to `unfoldr` + `guard`... Pushing towards unreasonable opulence, using the `Monad` instance for `r -&gt;`: import Control.Monad (guard, liftM2) import Control.Monad.Instances import Data.List (unfoldr) splitWhen f = unfoldr (liftM2 (&gt;&gt;) (guard . not . null) (Just . break f . tail)) . (undefined:)
&gt; On the other hand, when I'm reading an intro to Pipes or something and the very first mention of the Pipe type is `Pipe a r m h` or whatever, it puts an unnecessary barrier up to understanding exactly what the type is when you're not already an expert. Right, the difference here is exactly that something like `Pipe` is completely opaque at first, whereas something like `(-&gt;)` is entirely clear to anyone who knows Haskell. And, speaking of which, I would at minimum define `Pipe` as either `Pipe i o m a` or `Pipe a b m r`, either of which adds some immediate mnemonic value, given even very rough knowledge of what the library does. Using common naming conventions, standard abbreviations, and helpful mnemonics is important; so is having a clear definition for people to start with. Names should also be detailed in proportion to how specific the thing they name is--using `p` or `pred` for the argument to `filter` makes sense. Defining a specific predicate and calling it `p` is kinda terrible. What I take umbrage with is the idea that longer names are *always* better, without regard for context, especially when (as often happens) the example used to illustrate the "better" way is a generic combinator in the `Prelude`, like `foldr`, which is exactly where the stuff like `a` and `b` for types and `f` or `(x:xs)` for arguments makes the most sense.
Yes, that's exactly it. The only thing I'd dispute is that I see no contest in which is more important--the purpose of a library *is* to use it, so optimizing for that case makes sense. A library should certainly be *consistent* in its concision and explain the conventions used; that's what documentation is for.
So this is a bit off topic, but can paper authors please start putting the date of their paper in the header? I'm getting kind of tired of scrolling down to the references and manually doing `maximum . map date` in my head.
That looks pretty idiomatic to me.
I thought about both, actually, and I was really surprised to not find it already implemented. I searched for the type signature on hoogle and came up empty. rabidcow pointed out that splitWhen from Data.List.Split is what I wanted, and 5outh explains why it doesn't show up on hoogle. I contemplated using Parsec, which would have been relatively straightforward, but implementing it myself seemed like a good learning exercise.
Cool, thanks.
&gt; We’ll be building up this web site to prove that even Piggies can write clean, maintainable code. ... &gt; {-# LANGUAGE TypeFamilies, QuasiQuotes, MultiParamTypeClasses, &gt; TemplateHaskell, OverloadedStrings #-} lol
Code golfing is fun, but one thing you may want to ask yourself is if you can come back in several months and understand that version just from looking at it briefly. If so, well and good. If not, then sometimes spelling things out in a bit more wordy way has its advantages.
&gt; ...could programmatically compute how many expressions inhabit that type. For any type, it's either inhabited by zero proofs or infinitely many proofs. That is, given any expression of type `T`, we can always construct another expression of type `T` by introducing a new redex; e.g., adding the identity function somewhere. What you're probably interested in instead is the number of *observably distinct* proofs, which is a much harder problem. Indeed, it's impossible in the general case--- since it requires deciding the equality of functions. Parametricity helps here because, in virtue of being parametric, the only thing we can do is shuttle things around, duplicate them, or drop them on the floor. So you can probably find solutions for the fully polymorphic cases, but you'll be hosed once you allow any non-finite ground types.
It is amusing, but to be fair none of those extensions are particularly insane, except maybe `TemplateHaskell`.
With the amount of TH used by yesod, there isn't much of Haskell left.
I think your first definition is just fine, actually. Not clunky at all. If I was going to make this more advanced I'd look into parser combinators. Parsec's `sepBy` combinator is, in a sense, the inverse of `intersperse`: import Text.ParserCombinators.Parsec unintersperse :: Char -&gt; String -&gt; Either ParseError [String] unintersperse sep = parse (unintersperseP sep) "input" unintersperseP sep = many (noneOf [sep]) `sepBy` char sep
An actual problem is that Haddock doesn't preserve superfluous parentheses in signatures.
That was just an oversight. I knew I would overlook something; hence, the choice of "Other" :)
Thanks.
He's complaining about the variable names.
maybe `forall a. a -&gt; a` becomes `min_a. a^a`, which has minimum 1 for a=0 or a=1. The intuition is that you want the most general, i.e. smallest, thing that fits. Let's see how this works for other types flip :: forall a b c. (a -&gt; b -&gt; c) -&gt; (b -&gt; a -&gt; c) ~ min a b c. ((c^a)^b)^((c^b)^a) = 1 undefined :: forall a. a ~ min a. a = 0 This seems to work for these simple cases. Let's try some harder ones pick :: forall a. (a,a) -&gt; a ~ min a. a^(a*a) = 1 and here it fails, because there are two possible implementations for pick. So much for that idea. It was too simple to work anyway. To get the 2 in the pick example it looks like you could perhaps use the derivative somehow, i.e. `d/da a*a = 2a`. But I have no idea why that would make sense.
If you've ever seen ruby on rails you'd notice a lot of what you're doing isn't ruby coding, even if it is valid ruby code. There's so much that can be "optimized" in an MVC framework (due to it being a well known pattern) that I expect all MVC web servers eventually end up like this. DSLs for routes, preprocessors for templates/views and ORMs everywhere. If your code doesn't look like a DSL it'll be twice as long as necessary. Although I'm open to counter claims.
It's certainly a complicated issue. Being anti-competitive is bad for everyone in the long run but the superficial alternative is to deliberately not aid yesod in the name of fairness. I think the root problem here is that there aren't more companies like fpcomplete. If there were then one of them could back Snap, one Heist, etc, resolving this issue capitalist style. But until then this is an open issue to be resolved. NB: I say 'superficial alternative' because I don't wish to imply it is the only alternative, or that this was what Chris was implying.
I want to thank you for the work you've done on this. I'm trying to rework my website with Snap but I wouldn't have been able to get over the hump of getting started without this boilerplate (version 1). I almost packaged this up and sent it to you, but alas, I didn't, and don't have time this week. (Next week I can talk.) If you add a Text field to the website state, you can add a form that takes in Pandoc Markdown and then renders that as a page, which also nicely demos a whole lot of things at once, such as actually using a form, using Pandoc, etc.
A safe way to replace ```head``` is to use ```foldr```: invIntersperse2 p = filter (foldr (const . not . p) True) . groupBy ((==) `on` p) It also allows you, simply by replacing ```True``` by ```False```, to filter out empty lists. *EDIT:* Whoops, that's also what does ```any``` when fed an empty list: return ```False```. Nevermind, in that case, yours is simpler.
DSL != code generation. Most of the time you can handle it with combinators. [Web-routes-boomerang](http://hackage.haskell.org/package/web-routes-boomerang-0.27.0) does type safe routing, using TH only to generate PrinterParsers for top ADT. Heist does templates without any logic at all. And HSX is template embedded in haskell (not haskell embedded in template) implemented with combinators. I do use that shakesperean-i18 to generate messages though.
That was the answer I was looking for. Thank you so much.
You know, when I thought of using "any'", it didn't occur to me how it would perform on empty lists, so I really ended up making the exact same think-o as when I picked "head". Just going by the name, though, I think I assumed it would indeed return False on an empty list. Thanks for pointing that out.
Aha! Tricksy! Thanks.
Hmm. This is really interesting. My initial reaction was that introducing Either would be needlessly annoying, but that's due to my assumption that the input would always be well-formed. Hmmm indeed! I'd also like to start looking into evaluating performance of different implementations, because I would have assumed that throwing Parsec into the mix would result in some overhead. Not that it matters for this toy example, but it's something I want to learn more about. [ D'oh! The "Debugging/profiling/optimization" section of the "Learning Haskell" page on haskell.org is empty! I can only imagine there are other resources available. In my copious spare time, I'll poke around and see what I can find. ] Thanks!
Amazing stuff. More Span examples is certainly welcome! I would really love to see a simple example with just snap and acid-state, in the most direct way possible. But this looks like its doing some acid stuff, so I'll dig into that for now!
Out of the box, Snap encourages the use of more composable building blocks than yesod. Therefore, you have more chance with snap to succeed to build your "chartMyDataset" function and have it easily embeddable in all your snap websites.
&gt; Its easy to get going, especially now, but then you need to do something a bit complicated and you have to dive into the details of the framework. When choosing framework myself, I looked at yesod and it felt like - "Hey, we are going to do all the work for you". I already had some experience with such tightly integrated environments, and got scared away because with them it's easy to do "right way". But once you need to do something different - at best such frameworks provide no help, and at worst - become hindrance. Snap and happstack felt more like - we help you do it, but you have to do it yourself the way you want. So I chose snap because it looked to have more active developers. This is just an opinion, and I may be wrong about yesod/snap/happstack. Their communities are helpful and ready to chat, so check them all in case of starting with haskell webdev. Also snaplets are not widgets, they may have splices for heist, but mostly help with managing state and configuration for some reusable code. That's why there are mostly database snapplets - they are the easiest to reuse. And there are built-in auth and session snaplets. As about lack of easy to use widgets/controls in haskell frameworks: Today web is a bunch of different technologies, and everyone have their preferred way of doing storage/javascript/ui/css. Though I'm sure you can write such library for any of the popular Haskell frameworks, but seems no-one needs bad enough to start coding.
There are really good framework in javascript for charting, tables and other controls. Snap excels at digging data of out a complex back end, formatting it and serializing it out over the wire. I've been building internal tools in Snap under a similar use case to Alpha Heavy and I've found that type safety and the concurrency primitives are a huge win when sucking data out of the backend but controls really belong in javascript frameworks like d3.
You are pretty close regarding the Happstack philosophy. We have gone out of our way to make it easy to do things your way. We have made sure that the low-level components are separate and can be combined in many different ways. Additionally, we have gone out of our way to document the various different options, provide glue code, send bug patches, etc. The long term plan has always been to then also provide some 'best pratices' solutions that are more integrated. Unfortunately, we spent too long working on the low-level stuff without every getting around to the mid and high level stuff. In retrospect, that was a mistake. So, we now have two additional products in progress -- though still pretty early development. One is happstack-foundation, which provides a type-safe integrated environment which is very similar in features to Yesod. (Though, we obviously think it is better, more flexible, etc). It provides type-safe routing, compile time HTML templates, integrated Haskell database layer, i18n, type-safe forms, etc. At an even higher-level is clckwrks, which aims to provide a complete CMS solution, which one-click install of themes, plugins, etc. Obviously, the more integrated you make the system, the less choice the end user gets about how best to do things. So, we think there is value in supportting multiple levels of integration from practically none (giving you the most flexible) to highly integrated (giving you the most off-the-shelf funcutionality). (Note: the current happstack-server is still too integrated for our tastes. The next release will split out the components into a low-level backend, routing, query/form-data, etc.)
The above example is a trimmed down version of Edward Yang's [Codensity problem set](http://blog.ezyang.com/2012/01/problem-set-the-codensity-transformation/) with an example hole (line 29.) GHC output is at the very bottom in comments. Most of the bottom half of the code is undefined because I wanted to go through all the hard exercises (I did them a few weeks ago) one-by-one using holes, to see what they offered. They made the work a lot easier. :) If you haven't done this problem set yet, get GHC HEAD and try it now! If you don't want to build GHC HEAD, just try writing the `Monad (Free f)` instance with the information there - it's quite easy to intuit. Who's going to be the first to write up the haskell-mode and/or ghc-mod support, so I can break stuff down Agda-style? (OT: I tend to spam new GHC features here RIGHT when they go in it seems, and I'm sorry about that, but I know everyone here doesn't watch GHC development closely like me, but love to see what's going in, hence posts like this.) **EDIT**: and, for the *really* interested parties, the commit [is here](https://github.com/ghc/ghc/commit/8a9a7a8c42da3adb603f319a74e304af5e1b2128), and [this is the trac ticket](http://hackage.haskell.org/trac/ghc/ticket/5910). **EDIT #2**: and for those who are wondering, attempting to evaluate a hole results in a deferred type error as you would expect with `-fdefer-type-errors` (although only for holes, so type-checking failures elsewhere will happen at compile time, unless you explicitly also say `-fdefer-type-errors`.) This is, I think, a nice approach that's easily possible by piggybacking off deference, which seemed like an awfully controversial feature. :) I think these two features (Holes + deferred errors) will greatly help Haskell tooling in the future, where we often work with incomplete programs and information.
Well, there goes *my* only reason for using `ImplicitParams`...
The idea here is to force use of good error handling with `EitherT`. A real library would include (at least) everything from `System.IO` wrapped up with `tryIO`. The `main`/`safeMain` split enables errors that bubble all the way to the top to be exploded by the runtime system just like they would be with normal exceptions. Write `main` differently to force different handling.
I've built them before, it's just more work than a server side control in ASP.NET.
I think it's still a bit experimental for that, but if it proves useful / becomes more mature that might be cool :)
It is my hope that holes will allow splices to receive their type during reification. So If have functions aToB :: A -&gt; B bToC :: B -&gt; C cToD :: C -&gt; D I could write magic :: A -&gt; D magic = cToD . $(to) where "to" would be able to call env :: Q Type and then construct (aToB . bToC). Is anyone else interested in having this capability?
I used them occasionally to pass around an IORef to a lot of IO actions... I've also used them to simulate type classes when I needed to generate an instance at runtime.
To me, situations like that always seem better suited to just using `Reader` or `ReaderT`. What's the advantage of implicit params? I guess for the `IORef`s it means not scattering `liftIO` all over the place like with `ReaderT`.
Mostly just convenience, yeah.
I wrote a dumb hack once that did a limited version of this--had a splice find its location, parse the file itself, look for a type signature or such, then used djinn to wish for an implementation. It was incredibly silly. Having direct access to the expected type of a spliced expression without all the shenanigans I used might be interesting, but I suspect anything worthwhile is going to need enough interaction to require editor support rather than letting a TH splice fabricate whatever nonsense it comes up with. EDIT: I later attempted to extend my hack via the GHC API instead of using `haskell-src-exts` and relying on type signatures. Alas, GHC did not appreciate my attempt to use the GHC API from inside a TH splice, and that's probably for the best, all things considered...
I mostly disagree. I simply want to take an arbitrary set of data and have it make charts. While d3 is powerful it takes considerably longer than 15 seconds to make a chart. 
Right, I figured the lack was due to there not being a reasonable enumeration algorithm. &gt; Can you give me a reference rank-2 propositional logic being decidable? Sorry, I meant the typeability problem is decidable (e.g., [Kfoury and Wells](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.144.1202)), not the inhabitation problem; I've corrected my previous post. I forget off-hand where the subtle boundary is for the inhabitation problem.
Or just define: l = liftIO My reasoning is that such a package would clash with the `Prelude` anyway, so you'd have to namespace it with a two character prefix, so you could equally well just define a one character function for lifting.
I think you mean `f :[a] -&gt; [a]` implies `f [] = []` as `f : [a] -&gt; a` doesn't make sense to have [] as a value since you don't know that a is a list type.
There are no type checkers in faux holes. 
The given example shows a warning, not an error message; the module Main is loaded.
As the author of this extension, I don't think even I could have come up with such a good explanation of this feature. Thanks! :)
That's a really good overview, thanks. It was quite readable to me, and I don't know a lot of the theory behind all this stuff. Given how much easier this can make things, can we start to write search algorithms on top of it to try and find a workable result? I wonder how many of these things could be solved automatically. EDIT - Just saw this: &gt; In particular, Agda can literally write your code based on holes. Sounds like it might be possible then, particularly if it's an interactive thing. 
Thanks for saying that - I thought I must be misunderstanding something crucial.
This is really cool! But why is the extension called "TypeHoles"? With that name I would expect to be able to make holes in types, which is also a very useful feature! These holes are expression holes.
Ah, I see (also trying my suggestion gives a compiler error). I think I'm getting confused with cons lists by thinking about the `[]` at the end being an element of the list. Thanks for the correction :)
You can put the declaration `_ = undefined`, but it doesn't bind anything. `_` is a special pattern, not an identifier.
Well, I would just import Prelude hiding all name clashes, since the MonadIO versions can be used anywhere the originals can.
It should have been "TypedHoles".
Very interesting. The annoying thing about Haskell's lack of subtyping &amp; untagged union types is that (in this example) now we have to `liftIO` anything that was formerly just an `IO` action. And there are problems with just providing a library with everything lifted to `MonadIO`: then it can become overly polymorphic. This problem can sometimes be dealt with via context. safeReadFile :: MonadIO m =&gt; m ByteString putStrLn :: ByteString -&gt; IO () runSafeMain :: EitherIO a -&gt; IO a safeMain = do s &lt;- safeReadFile "foo.txt" liftIO $ putStrLn s main = runSafeMain safeMain `safeMain` can be correctly inferred as `EitherIO ()` because it is used as the argument to `runSafeMain`. ---- I have been working on "monads in racket", so to speak. While it's currently too raw to share right now, here's a sample of what you could do with it. (define (either-t m r?) (make-instance monad (define (return x) (cond [(r? x) x] [else (... contract violation ...)])) (define (bind x f) (cond [(r? x) (gen-apply m bind x f)] [else x]))) (struct io-action-pure (val)) (struct io-action-impure (...)) (define (io-action? x) (or (io-action-pure? x) (io-action-impure? x))) (struct io-error ...) (define io-monad (make-instance monad (define return ...) (define bind ...))) (define safe-main (do-with-monad (either-t io-monad io-action?) (s &lt;- (safe-read-file "foo.txt")) (put-str-ln s))) (run-safe-main safe-main) There is no need to "lift" the `put-str-ln`, due to the way i chose to encode the `either-t` monad transformer.
\^\^ I used wrong types on purpose (like ```undefined :: Int```) where I wanted to know what was some intricate type. Then I switched to ghc-mod.
Yes, using `liftIO` instead of a type-specified function does in general mean that the type could be inferred to be just `IO`, instead of `EitherIO`, which would defeat the purpose of this. This basically necessitates an explicit type signature somewhere (either on a utility function as you've shown, on on your toplevel, as in my `safeMain`).
One question: the SipHash paper says: &gt; We propose that hash tables switch to SipHash as a hash function. On startup a program reads a secret SipHash key from the operating system’s cryptographic random-number generator; the program then uses SipHash for all of its hash tables. Of course your current test code does not read a random number from the OS, but is one of the params to `hashByteString` this seed arg? Obviously we need it to be pure, and it's a bit of a pita if we have to pass in an extra key/seed. Is the resistance to collision attacks dependent on using a secret key? If so it would not fit the simple `hash :: a -&gt; Int` interface.
I half-agree. The most famous frameworks are certainly like this. Annoying when all you want is a clean interface on top of CGI and session handling. Database and templating should be separate modules. However, I do think that micro-frameworks, like Python's Flask, are very useful. I have never tried scotty for Haskell, but I suspect that it does what I want.
Built by a company that makes tools for the finance industry no less: https://www.novus.com/ :-)
Snap is great, it just needs more (newbie) people to start poking at it and documentation needs to be a more newbie level as well. In particular, a few beginner "blog" or "todo list" tutorials, approaching the topics from slightly different directions would help things a lot. Regarding widgets and such - why do you even need a web-framework for that? Pick a Javascript lib, hook it up to rest/any-other-kind of api, and you'r done.
So would it be possible to have the same feature as a compiler error and still conform to Haskell 98?
The quasiquoted DSL pattern used frequently in Yesod sacrifices composability and flexibility in exchange for stronger compile-time type safety and custom syntax that might be a little more concise. Developers on both sides of the web framework fence have acknowledged this tradeoff. As for the OP, first he says: &gt; I feel the same in Yesod. Its easy to get going, especially now, but then you need to do something a bit complicated and you have to dive into the details of the framework. Then in the rest of the article I didn't get the feeling he actually tried to do anything complicated with Snap. So the "it's not" conclusion rings hollow. &gt; What I really think either framework could benefit from would be a layer on top of what exists now. A set of full featured controls for charts, data grids, calendars and things I have never thought of. This conclusion doesn't have as much to do with the frameworks as it does their age and still relatively small community.
&gt; Of course your current test code does not read a random number from the OS, but is one of the params to hashByteString this seed arg? Yes, it is. &gt; Obviously we need it to be pure, and it's a bit of a pita if we have to pass in an extra key/seed. That's right. One of the constraints within which I'm working is to not change the API of hashable. As such, the guts of SipHash implementation will be available, so that people who need to write new Hashable implementations can do so easily, but end users will still use the simple API. (The simple API already provides a `hashWithSalt` function, of course.) &gt; Is the resistance to collision attacks dependent on using a secret key? Somewhat. SipHash doesn't yet have any seed-independent attacks (of course, since it's so new), and it is not as obviously susceptible to trivial collision attacks as FNV-1. But keeping the key secret certainly helps to an extent. Here's how I am currently planning to meet that need: https://github.com/bos/hashable/commit/49694d7b95c24b24fc4e1366414a984dde04b2b4 &gt; If so it would not fit the simple hash :: a -&gt; Int interface. Except for the `unsafePerformIO` sin I commit above :-)
I thought about it, but this seems like the sort of open-ended question that gets promptly shut down on SO. Of course, there's no harm in trying (AFAIK). I'll give this question a day to see if he sees it here first.
Thats what I have done. Writing lots of javascript is not something that I want to be doing. 
There's also http://codereview.stackexchange.com/ where these sort of questions are more welcome than on SO.
Ah, I was not aware of that. Thanks!
This seemed to be part of the original design, being able to put holes inside types. For now, it's terms only, and putting holes in types is a straight up parse error. I imagine as they're fleshed out a bit, features like this will come in due time. I do prefer the original name of the extension though, which was `-XHoles`.
I've wanted/thought about something like this for a while, but the implementation of ConstraintKinds and associated type defaults in GHC 7.4.x actually caused the code you linked to fail to compile, IIRC (you had to make an empty proxy typeclass, for the default implementation, which was gross.) It's much better in 7.6.x. It also allows you to make Functor/Monad instances for .e.g `Set`. Ultimately I'd love this to be more widely adopted, although what I'd really like to see is this kind of stuff go in, along with the whole Functor/Applicative/Monad hierarchy refactor, if that were possible. That wouldn't be completely drop-in compatible though. I'm also generally not sure about how I feel about packages depending on 'enhanced interfaces to base', especially libraries since that could be infectuous (and it would potentially require users to learn about such extensions, and why they can/cannot write code certain ways for what reasons, which are fairly advanced,) but maybe I can't see all the possible outcomes.
What exactly is stopping us from adding this to *base* itself? Of course we need to put: #ifdef CONSTRAINT_KINDS ... #endif (or something similar) around these associated constraints and also around instances that rely on this. 
You'll probably want to use blaze-builder for uwsgiReq.
I'm on 7.6.1, where it works. I agree that this should happen at the same time as the refactor. I know there's lots of implementations of the refactor floating around, and I think it would be best if one of those authors added the CK stuff as well.
Speaking as someone who has implemented ConstraintKinds in his own compiler, it is actually easier than the alternative.
It isn't a drop-in replacement for base, as almost any polymorphically recursive function that recurses through a functor or monad breaks under this scheme.
That's not a bad idea. I might tweak what's there now so that it always uses an environment variable if supplied, otherwise \/dev\/urandom. (Of course if you're doing cluster computation, you're almost assuredly not interacting with the public internet.)
`-XHoleyTermsBatman`
I don't think this is the case, as `roche` noted below. But not intrinsically, just because of the current implementation. Indeed, in Haskell 98, you cannot bind the identifier `_` in a meaningful way, because it is actually a pattern. As a result, if you turned off `TypeHoles`, it would be *impossible* to define the `_` pattern as meaning something else (like regular `undefined`,) without special compiler support. Basically, once you've used `_` in non-pattern position, you can't be Haskell98/Haskell2010, because you can't define it as an identifier in vanilla Haskell. If you were to turn `undefined` into a 'hole-form' however, and make it the only form (removing `_`) I think it would conform perfectly to Haskell98, just being a compiler warning, albeit an advanced one. GHC treats holes a bit special at runtime (by treating it as a type error, and deferring that error to runtime a la `-fdefer-type-errors`,) but other compilers could just stick with the runtime error being `error "hole was evaluated"` or something. EDIT: also, though, I think in the future actual holes *in* types could be supported by this extension. As well as named holes. And both of those would be really nice features, but I don't think there's any possible way to do that in a Haskell-98 compatible way. Only with regular unnamed holes in terms does it work.
I don't think making `undefined` a whole is a good idea. There are legitimate reasons of using `undefined` even in production code and I don't want lots of compiler output telling me about all of those!
http://semver.org/
&gt; Except for the unsafePerformIO sin I commit above :-) Heh, that's the kind of worry I had in mind :-) So it hadn't clicked that `hashWithSalt` does give us the opportunity to pass in a secret key. So if we wanted to have network-facing servers do the random seeding properly, we'd need to make the seed be an (optional) input to things like hash table constructors (e.g. in `unordered-containers`), and then the hash table would remember that seed and pass it to `hashWithSalt`. That'd be workable wouldn't it? Then we can avoid implicit (and impure) parameters like `unsafePerformIO getRandomBytes`. tibbe: could you see that working in `unordered-containers`, to store the key/salt and have an extra constructor function or two that take the salt param? I realise there's lots of functions that make containers, but transformers are ok, it's just ones that construct from nothing, so that's `empty` and `fromList` that would need "with salt" variants. Container transformers can keep or merge the salt of the input containers.
Probably the reason why Haskell does not have this in base yet is that an environment variable is a global mutable variable. Generally they should be used in a read-only fashion or changed just for a spawned process (which is already supported by System.Process). I am not against including this in base, I just hope everyone avoids using it as much as possible.
Yep, when I added ConstraintKinds to GHC I was actually able to remove one of the constructors from the "Type" datatype, which lead to quite a bit of simplification across the whole compiler.
Libraries get reused. If a finished app can be reused then there's probably some code that could be factored out into a library.
This proposal *extends* our ability to *write* instances. Does it also *limit* our ability to *reason* about them? I expect free theorems to brake if formerly polymorphic types can be constrained..
You will notice that great authors sometime use TH, like bos in the aeson package ( http://hackage.haskell.org/packages/archive/aeson/0.6.0.0/doc/html/Data-Aeson-TH.html ). But there is a pattern in this use case : a single line of TH is usually required, which is not invasive, and does not requires the user to express any type of application logic here. There are mainly two things that make experienced haskell programmers avoid TH as much as possible. First, functional programming is all about composability : take a couple of little block, and make a larger one with them. See Control.Category : (.) :: cat b c -&gt; cat a b -&gt; cat a c (.) is the canonical way to compose a couple of blocks to form a new one. Laws associated to Categories are simple, intuitive, and give strong confidence on the properties of the composition. If some library gives you a `cat b c` and an other a `cat a b`, you will immediately know how to compose them ! However, given a couple of pieces (excerpt ? Lines ? I don't even know how to call a such a unit) of template language, and ... you won't do anything useful with them. Have a look at Yesod routing syntax : you can't compose a couple of lexical routing rules (a line of the QQ language) to make a new rule. The other reason is even simpler : great people work hard to standardize the Haskell language, so that other people can express a wide variety of logic using the same language, and have all that wonderful libraries work together. The abuse of TH (see yesod routing, or hamlet and its relatives ...) encourages users to write thousands of lines of something that is not Haskell. Let me stress that again : the content of a QQ expression is not haskell, is not standardized in any way, and will not work with anything else than the mini-interpreter designed to understand it. If you decide to change your lib, you throw away every bytes you typed in this environment, just like if you decided to re-implement the whole thing in an other language. So all these lines spent in QQ : * are not standardized or documented by any committee * can be broken by a simple update of some lib on hackage * are not readable by other haskellers * don't compose * gives obscure error messages when something goes wrong For all these reasons, good libraries avoid TH and QQ.
&gt; `WriterT` doesn't generate a result until the computation is done, Not entirely true: ghci&gt; take 10 . execWriter $ forever (tell "hello") "hellohello" 
Performance isn't the only relevant issue here: with `replicateM` it's immediately obvious that all the actions are identical, whereas `forM` is more idiomatically used for actions which use the list items in some way.
Reminds me of [What's so bad about Template Haskell?](http://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell)
It's the monomorphism restriction. Prelude&gt; let f = uncurry (&gt;) Prelude&gt; :t f f :: ((), ()) -&gt; Bool Prelude&gt; :set -XNoMonomorphismRestriction Prelude&gt; let f = uncurry (&gt;) Prelude&gt; :t f f :: Ord b =&gt; (b, b) -&gt; Bool 
Ah, it's our good old monomorphism restriction in action. And defaulting, though I'm not sure why the default gets to be () (see the [report](http://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-790004.3.4) for more info). In any case it's not particularly related to *uncurry*. You can turn it off with: &gt; :set -XNoMonomorphismRestriction I'm pretty sure there's also a command line option. If you're not aware of the monomorphism restriction, check out the Haskell Wiki and Stackoverflow, there are multiple questions there related to that.
Yes, it's a tradeoff. For our simple cases the limited and poorly designed controls work fine and often look good enough. My experience with D3 (which we do use) hasn't been opinion-changing.
Note that he uses the whole list, too. This can be done with `s@(x:xs)` so you can use `s` in stead of reconstructing the list later on. This is called an As-pattern.
This is very promising. Reading through the source I'm extremely pleased to see that functions are commented in a way that makes reading much easier. Given my deep experience with Erlang it's also fairly easy for me to see precisely what they are doing, but it's also pretty cool to see how *well* they are doing it in Haskell.
No one says this is a bad thing, in most cases this is probably a very good thing. But you should be aware that putting constraints on polymorphic types *can* make it harder to reason about things. Free theorems become less free.
The point of `IO` is that you can do the dangerous stuff.
As I say in the post, we think it is ready for serious experiments. That doesn't mean you should immediately base your next mission critical project on it. So we don't claim it is polished or that you will have no issues, but we think it's a very promising technology and we are keen to help any customers that are interested in using it. There's lots of directions in which we could improve it, depending on what people need. And of course it's also open source so if you want to hack on it yourself that's also great.
Of course! Their gut-feeling is that their bullet-proof cloud project synergizes the resources, while breakout improvement prioritizes the key people from the get-go. Their perspective empowers the thought leader within the community, whereas their verifiable cloud footprints transfer the cloud standard-setters. They’ve got to build a bullet-proof cloud convergence; this is why the group 24/7 delivers. The thought leader pre-prepares their versatile, controlled, cloud industries, and a strong self-awareness cautiously targets the cloud resources. So cloud development models boost the client-oriented cloudportunity, while the community-wide cloudnovation promotes the key people. And because of this, the enablers learn leveraged and/or high-performing cloud interdependencies. At the same time, the partners 200% whiteboard their growing on-boarding cloud processes. So as a result, the cloud developers engineer a full-scale cloud value creation cloud. Why do you have so many doubts? ;)
The type `forall a. a` is indeed a sort of bottom type, in that you can "convert" a value of that type to a value of any other type, with the caveat that no defined value can have that type. The obvious duality here would be `exists a. a` as the top type, which works similarly--you can turn a value of any type into a value of that type, but you can't actually *do* anything with that value afterwards. So you have an uninhabited type that lets you do anything you want, and a universally inhabited type that doesn't let you do anything other than say "yup, it sure does exist".
I would prefer, if we’d still call clusters clusters, and online file storage services online file storage or something. The problem is that nobody knows what “cloud” means, and so it means *everything*. Except of course actual clouds, because that would still make some fuckin’ sense. ;)
The transformation between them can be seen as just (un)currying, too.
To clarify this comment for those who might not get it, godofpumpkins is referring to the fact that the first version of that function could be expressed in a dependently typed language like this, where the first component of the argument determines the type of the second component of the argument: (A : Set, A) -&gt; T And the second could be expressed like this, where the first argument determines the type of the second argument: (A : Set) -&gt; A -&gt; T
This is awesome stuff, congrats! It would also be interesting to see how this compares to the Hadoop stack, which is the de-facto platform for doing cloud computing. I guess this could function as a base for building a Hadoop-like stack in Haskell? One of the awesome things they do is just distribute the entire code across the platform (as copying the code is cheap, and copying the data generally isn't cheap). Building something like Pregel on top of this would be awesome, too: http://googleresearch.blogspot.de/2009/06/large-scale-graph-computing-at-google.html
How about we call them "VMs wot you can rent"?
&gt; Haskell does not have subtyping You just showed that ```(forall a. a)``` was the _subtype_ of every type. And an equivalent of Data.Dynamic.Dynamic but _without_ the Typeable constraint would be a workaround to express ```(exists a. a)```, and then could be the Top you're looking for. So I wouldn't say Haskell does not have subtyping, just that it does not work the way we traditionaly do subtyping (which is called _nominal subtyping_ [1]). "```&lt;--```" meaning "is a subtype of", in OO you can have: ```ClassA &lt;-- ClassB &lt;-- ClassC &lt;-- ClassD &lt;-- ...``` And in Haskell (with the right adjustments to circumvent the 'exists' that does not... well, exist), that would be for instance: (exists a. a) &lt;-- (exists a. (Num a) =&gt; a) &lt;-- (exists a. (Num a, Ord a) =&gt; a) &lt;-- Int &lt;-- (forall a. (Num a, Ord a) =&gt; a) &lt;-- (forall a. (Num a) =&gt; a) &lt;-- (forall a. a) I see it as a different way to express subtyping [1], not as "no subtyping", because bottomline it does provide you a way to give some type where another was specified. [1] Just like some languages feature _structural subtyping_ (OCaml does).
It's been said that one doesn't understand an algorithm until one has implemented it. There are tons of algorithms for which we don't have Haskell code. What rocks your mathematical boat? Suggestion: finite fields are ripe with algorithms of every stripe. Bonus: lots of practical utility (people might actually appreciate your work). Also: it won't be difficult to find a buddy or two for the adventure. 
LCF-style proof assistant for HOL.
GAP is probably a good source for algorithms from abstract algebra that would benefit from an implementation in a proper language.
Interesting, but I'm not sure why it's hard to make this extensible in C without appealing to higher order functions. Every problem can be solved by adding one level of indirection. In this case, we just reify the program as data: typedef struct fizzbuzz { int i; char* format; } fizzbuzz; static int order(const void *elem1, const void* elem2) { return ((fizzbuzz*)elem1)-&gt;i &lt; ((fizzbuzz*)elem2)-&gt;i; } static int main() { fizzbuzz* cases = { { 3, "Fizz" }, { 5, "Buzz"} }; int length = sizeof(cases) / sizeof(fizzbuzz); qsort(cases, length, sizeof(fizzbuzz), order); for (int i = 0; i &lt; 101; ++i) { int found = 0; for (int j = 0; j &lt; length; ++i) { int x = !(i % cases[i].i); if (x) printf(cases[i].format, i); found |= x; } if (!found) printf("%d", i); } return 0; } qsort is only used to sort the cases so we don't have to track it manually, but as long as we add cases in order we can skip the sorting. Edit: forgot the not found case. Edit: corrected invalid use of outer state 'found'.
Re: combinatorial species, that "someone" would be me. But I haven't worked on that library in a while and it needs some love, and there are still lots more things that could be added. If you'd be interested in helping out with it, let's talk!
 fmap (maybe 100 read . listToMaybe) No. fmap (foldr (const . read) 100) Yes. Why translate a list to a Maybe when you can directly process the list?
I actually wrote such an thing. https://github.com/Mgccl/mgccl-haskell/blob/master/random/ContinuedFraction.hs. I should complete and document it. 
That's great! I can start looking into it in 2 weeks. What are some things you want but still not in the library?
Or without partial functions: `fromMaybe 100 . (listToMaybe &gt;=&gt; readMay)` As I suspected, no intermediate maybes are actually allocated in GHC. It pattern matches directly on the lists. This code: import Safe import Data.Maybe import Control.Monad import System.Environment main = getArgs &gt;&gt;= print . go where go = fromMaybe 100 . (listToMaybe &gt;=&gt; readMay) Compiles to this core: %module Main main3 = zdwshowSignedInt 0 100 ZMZN main2 = zdfReadInt5 minPrec zdfMonadPzuzdcreturn main01 = \etaB1 -&gt; case getArgs1z etaB1 of Z2H realworld aazzn -&gt; hPutStr2 stdout -- If the list is… (case aazzn of -- … empty, then return 100: ZMZN -&gt; main3 -- … otherwise take the head: ZC a1awf _ -&gt; -- Read it: case readMay1 (run main2 a1awf) of -- If it's empty list then return 100 ZMZN -&gt; main3 -- Otherwise open that list… ZC xa1Yb dsa1Yc -&gt; case dsa1Yc of -- And make sure the remainder is empty: -- (E.g. case reads x of [(x,"")] -&gt; Just x; _ -&gt; Nothing) ZC ipva1Yi ipv1a1Yj -&gt; main3 -- If it's empty then we're good to go, unpack the int: ZMZN -&gt; case ZMZN xa1Yb of Izh wwa1Xzz -&gt; zdwshowSignedInt 0 wwa1Xzz ZMZN) True realworld I stripped out noisy package/type annotations, and added comments to explain what's going on. If I rewrite the core in plain Haskell it's like this: do args &lt;- getArgs putStrLn (case args of [] -&gt; 100 (s:_) -&gt; case reads s of [] -&gt; 100 (x:xs) -&gt; case xs of (_:_) -&gt; 100 [] -&gt; case x of I# i -&gt; showSignedInt 0 i []) 
2 nitpics: 1) Monoids are from Abstract Algebra 2) mappend and mempty are terrible names; they have nothing to do with appening or being empty (other than that appending strings is one of many things that forms a monoid). mappend isn't an 'append operation', it's the 'monoid operation'.
I think your `found` handling is still off: for (int i = 0; i &lt; 101; ++i) { int found = 0; for (int j = 0; j &lt; length; ++i) { if((i % cases[i].i) == 0) { found = 1; printf(cases[i].format, i); } } if (!found) printf("%d", i); } You are quite right though. The generalisation addressed by the ruby example calls for paramaterisation by arbitrary divisors; reaching immediately for `lambda` seems a bit trigger-happy.
Halfways through the defective examples I thought I knew what he was going for and typed up (ruby): (1..100).each do |i| line = "" if i % 3 == 0 then line &lt;&lt; "Fizz" end if i % 5 == 0 then line &lt;&lt; "Buzz" end if line.empty? then line &lt;&lt; i.to_s end puts line end Seems pretty similar, I didn't reify the table, but I also thought the lambdas in the later solution were kinda silly.
Well, I'd say that Haskell "doesn't have subtyping" in the same way that Java "doesn't have first-class functions". In both cases, there are sort of workaround ways to emulate it. Is there a separate name for systems that include subtyping via untagged type unions? Haskell doesn't have this form of subtyping either, so if it's not that, it's notnominal subtyping, and it's not structural subtyping, then what would you call it?
Have you ever tried hspec.
Isn't this just a special case of a DFA regular expression parser?
There are several kinds of cohomology it would be reasonable to compute: group cohomology, Lie algebra cohomology, Hochschild cohomology...
Using implicit params, really?
Messages sent to other processes are always fully evaluated. In implementation terms, this is because they're fully serialised, sent over the wire and deserialised.
I see, so this is not a problem for lazy *distributed* concurrency. Cool! Separately, I still wonder how to solve the problem in a non-distributed setting.
What compiler is this? Is there something published about it?
Can you explain the memory leak to me?
This is optimal... basically everything else on that page is just silly overcomplicated.
Sure. The problem comes from the fact that the xs list cannot be garbage collected early, because we still need it when ~~last xs~~ ```tail xs``` has returned to compute ```head xs``` (and as evaluation order is up to GHC, this is not even a predictable behavior). So the list xs will be wholly kept in memory even if the only thing we'll eventually do to it is to get its first element. To fix it, we need to explicitely force the element retrieval to be computed before. This calls to the use of ```seq``` (or the BangPatterns extension, which is convenient syntax sugar): cyclicOneShiftLeft xs = let x = head xs in seq x (tail xs ++ [x]) Then, ```tail xs``` being the last thing we do to xs, no reference is kept and it can be garbage collected progressively.
Sure, that builds but the logic's still off - multiples of three are printed.
&gt; We’ve written our concatenation in terms of a monoid, so it turns out any suitable monoid will do. Technically, any suitable semigroup will do.
Which part of maths do you find most interesting (eg abstract algebra, number theory, combinatorics, ...)? My [HaskellForMaths](http://hackage.haskell.org/package/HaskellForMaths) package has simple commutative algebra (Groebner bases for ideals plus elementary applications), but real commutative algebra packages such as [Macaulay2](http://www.math.uiuc.edu/Macaulay2/) go much further (eg Groebner bases for modules, free resolutions). An implementation of the LLL algorithm would be useful for number theory.
Death to the reader monad!
Um... I started playing with profiler and memory management only recently, so I guess I don't see them. I am aware of inefficiency of (++) - is that what you mean? If so what solution would be better? Difference list?
So one thing I love about erlang is the way epmd and DNS are used to create ad-hoc clusters. I think Epstein's initial Cloud Haskell had two options: config-file based clusters, or multicast discovery, and I haven't seen much that has differed since then. What I'd love is to be able to have an interface as simple as "send a message to (dns_name, registered_process_name)". Maybe the message will be sent, maybe not. I haven't seen any Cloud Haskell implementation where it would even be possible to write something like this. Can this implementation support ad-hoc DNS-based networks? 
We have a Haskell-like language we use here at S&amp;P Capital IQ that we call Ermine that we use for reporting that is basically a Haskell compiler extended with row types (and some more common extensions such as RankNTypes, PolyKinds, etc.) with minor syntactic and structural differences that is designed to run on the JVM.
It amazes me to see how any mention of FizzBuzz results in programmers coming out of the woodwork to show that they too can implement what is supposed to be a trivial problem. It's always funny to see how many of them get it wrong.
Well in fairness, `maybe` and `listToMaybe` are more approachable to newcomers than `foldr` and `const`. Const especially is a function that the initiated take for granted; it's a very unusual function that takes time to get used to, even though it is so simple.
Polymorphism conceptually induces a subtyping relation, but that doesn't mean GHC actually makes use of it at all. And it doesn't. So for instance: (forall a. a) &lt; Int So we would also expect: Int -&gt; Int &lt; (forall a. a) -&gt; Int So if we have `f :: Int -&gt; Int` and `g :: ((forall a. a) -&gt; Int) -&gt; Int`, we would expect that `g f` is well typed. But, in fact, it is not; one must write `g (\x -&gt; f x)` to get the instantiations to work out. So if all you mean by 'has subtyping' is that you can think about things being subtypes of one another in your head, then GHC has subtyping. But that's a pretty low bar, and could apply to a lot of things. Edit: I guess it works out okay if you only care about the first-order cases, but it's kind of weak that way.
Just because I feel compelled to imitate everything in Haskell import Control.Monad.Trans.State (runState, modify, get, gets) import Control.Monad.Trans.Class (lift) import Control.Monad (forM_, when) main = forM_ [1 .. 100] $ \i -&gt; flip runStateT "" $ do when (i `mod` 3 == 0) (modify (++ "Fizz")) when (i `mod` 5 == 0) (modify (++ "Buzz")) gets null &gt;&gt;= flip when (modify (++ (show i)) get &gt;&gt;= lift . putStrLn I thought the `gets test &gt;&gt;= flip when expr` was cute. :)
The [`strict-concurrency`](http://hackage.haskell.org/package/strict-concurrency-0.2.4.1) package is one solution to this problem.
Nice try \^\^. But nope, the problem is not with (++). See my answer to schellsan for the explanation, but it boils down to: cyclicOneShiftLeft (x:xs) = xs ++ [x]
&gt; So if we have f :: Int -&gt; Int and g :: ((forall a. a) -&gt; Int) -&gt; Int, we would expect that g f is well typed. But, in fact, it is not; one must write g (\x -&gt; f x) to get the instantiations to work out. You are right, I wasn't aware of that case. Normally, ```((forall a. a) -&gt; Int) -&gt; Int``` is the same as ```forall a. (a -&gt; Int) -&gt; Int```, right? Is that to be considered a limitation of GHC?
Oh, well then I think: fmap (foldr (\x _ -&gt; read x) 100) I've no problem with lambdas used in place of combinators if it eases the readability, I was bothered by unnecessary conversions. You may think I'm nitpicking, but it doesn't feel logical, and I think going this way ends up cluttering the code. Plus it feels like a misunderstanding of the fact that the functions ```maybe``` and ```foldr``` serve the same purpose, deconstructing, only for different types, which actually have very much in common: Maybe acts as a 0 or 1-element list. It's as you say something the beginners don't get at first, but I think it's important to eventually show them where common patterns are to be found.
LLL would be great.
I took a look at this. It depends on the `DeepSeq` library, but it doesn't actually use `($!!)` or `deepseq` as far as I can tell (see [definition of `writeChan`](http://hackage.haskell.org/packages/archive/strict-concurrency/0.2.4.1/doc/html/src/Control-Concurrent-Chan-Strict.html#writeChan)). Am I missing something? Even if it did, you'd still have the problem of calling `deepseq` every time you passed a message, a cost that does not exist in a strict languages like [Concurrent ML](http://people.cs.uchicago.edu/~jhr/papers/cml.html) (sadly CML is not really maintained these days).
Okay, I'm stupid. No, really. My solution works, but I overcomplicated everything \^\^. The good solution was simple: cyclicOneShiftLeft (x:xs) = xs ++ [x] And it's actually better, as I don't force the evaluation of x, the list first element, which I forced with my first solution.
I'm perfectly comfortable with calling your Azure implementation a cloud backend, much less so with the Cloud Haskell name in general. The model you're implementing proved its worth in Erlang decades before "cloud computing". Naming aside, I'm extremely exited about the project and its recent progress.
It took me a while to figure that out too, its `Chan` type is built using its own `MVar` type which does call `rnf` on stuff that gets put into the `MVar`. http://hackage.haskell.org/packages/archive/strict-concurrency/0.2.4.1/doc/html/src/Control-Concurrent-MVar-Strict.html#putMVar And yeah, `deepseq` is overkill and arguably a bad idea in general. That's why I hesitated to endorse `strict-concurrency` more strongly. Maybe a "less strict" version that just evaluates with `seq` would be the way to go.
Or just use a pattern match instead of head/tail...
Is it bad that stopped reading as soon as I saw that?
`[ fizzbuzz i | i &lt;- [1..100] ]` What? Why not `map fizbuzz [1..100]`?
You don't have to be a newcomer to find `maybe 100 read . listToMaybe` easier to read than `foldr (const . read) 100`.
Yes. Another way to look at it is: (forall a. a) &lt; Int (Int -&gt; Int) &lt; ((forall a. a) -&gt; Int) (((forall a. a) -&gt; Int) -&gt; Int) &lt; ((Int -&gt; Int) -&gt; Int) So we should be able to use `g` at type `(Int -&gt; Int) -&gt; Int`. Also, the bottom is at a double-negative = positive position.
I am a ways :P
Came here to post something that looked like this. Thank you.
Thanks! This is so obvious it makes mi think why didn't I figure it out :)
Both are technical; one is even snug!
I fail to see why [[Char]] would be better ...
Yes, yes \^\^, I felt stupid when I stumbled upon that...
I find `map` much easier to read and type than the eye-sore, `&lt;$&gt;`. To me, its only use is in combination with `&lt;*&gt;`.
This is where we need the list function. No need to go into recursion with fold when you only need to inspect one level of the list. list :: a -&gt; ((x, [x]) -&gt; a) -&gt; [x] -&gt; a list nil _ [] = nil list _ cons (x:xs) = cons (x, xs) fmap (list 100 fst) I always miss the list function in base. Luckily we do have the maybe function.
That is subject to taste, I guess. I suggested that exactly because I found the ```foldr``` version more readable.
&gt; after this you tell it not to This is the whole point of lazy evaluation: making functions that can do _more_ to afterwards being able to _limit_ their action in specific cases. That was my rationale while proposing ```foldr```, because IMO it's what follows the most Haskell's spirit. &gt; An datatype eliminator like list (or maybe) is even more fundamental For a _recursive_ datatype, the datatype eliminator has to be _recursive_, or else you're not eliminating the whole datatype. Your vision is valid, too, I'm not arguing, but what I read the most was ```foldr``` being the standard eliminator for lists. Or else you don't _eliminate_ a list, but **a list cell**, and everytime in Haskell we speak about _lists_ we think of them _as a whole_, so departing from that philosophy would bring an incoherence. Whatever, this all is just a big nitpick ;-)
So pretty, and so obvious in hindsight!
Well, let's see... virtual species is one big thing. Maybe asymmetry series (?). Boltzmann sampling. Probably lots of other things I don't remember (since I haven't looked at it in so long). When you're ready to start looking into it send me an email (my reddit username at gmail). I'll see if I can at least get it updated and compiling again by then.
String is not suitable for fast text processing. There's the `text` package for that.
Lovely! It might look nice of you rotate the coordinate system as you go around the polygon by tau * n / p.
Yeah, I'd had `readMaybe` and `readEither` in my alternate `Prelude` for a while. Always seemed like such a weird oversight in `base`...
Oh, because I had already introduced the monad comprehensions above and thought it was nice symmetry. Either works. I did my best to try and maximize reuse of ideas in the Haskell code so as to minimize the number of new concepts for readers unfamiliar with the language.
Ooooh, good idea!
I applaud, but I hope you appreciate why this should NEVER be shown to fp novices hoping to learn haskell. :) And I agree, flip when is cute.
I'm not sure how avoiding "appealing" to higher order functions is bad in this case. Do you really think this approach is less complex than the linear lambda dispatch method? You had to refine it at least once to get it right. 
&gt; I hope you appreciate why this should NEVER be shown to fp novices hoping to learn haskell. Indeed. However, if anyone wants to write typical-imperative-language in Haskell (rather than learning Haskell), I'd be happy to demonstrate how the `StateT localState IO` monad can do it. ;)
I didn't mean to put down the guy who wrote the bit with the lambdas! It's a perfectly reasonable, readable and high-level solution, and any possible critique of a fizzbuzz program that isn't leading up to some sort of more general argument is just nitpicking anyway. My version was written before I even got to that part of your post, though, mostly because I saw that it would fit a pattern I'd just used in a bit of javascript, and I wanted to get it on paper before getting "spoiled" by your ultimate solution. Later, I also spent a bunch of time messing around with haskell and got [something](http://hpaste.org/75885) that involved `foldl mappend` and `foldl (&lt;|&gt;)` and a bunch of `Maybe`s, but I gave up because I couldn't get the `Monoid` operations to be clear enough to make it worth the cognitive overhead over my ruby version (for me personally, anyway--I had some clear idea of what I wanted to write, but I can't quite recognize it in the `Data.Maybe` functions that I ended up writing). Of course, that's all irrelevant to your point :)
I wouldn't feel too bad about giving it a try and failing that way. I certainly was confused at first too. Part of the reason I'm writing about all this is to help me force myself to really understand it. Nothing gets shot down faster than an incorrect Haskell blog post. :) A common criticism of c_wraith's version is that it actually makes an unnecessary use of `Maybe a` to avoid doing an explicit comparison. A fair sum of people (myself included) didn't realize that Maybe String could be used the way it was there.
You're right. I plan to play around a bit more with ordering. A way to see all the orderings would be cool.
&gt; I wish I knew how to make a website where you could enter a number and have it show you the factorization diagram… maybe eventually. Well here's [hello world in Yesod](http://www.yesodweb.com/book/basics); from there I'm sure it's fairly trivial to put the pieces together. * Create a route that accepts an integer * Have the corresponding resource function run your diagram program with the provided integer * Create a simple HTML template that includes the freshly-created image * Consider deleting the images at periodic intervals, or caching common ones
But does that make it any less clear or useful as an explanation of an interesting algorithm? Implicit params do; they make information flow less obvious. Strings really don't.
This is very subjective, but I find test-framework to be more concise and more suited to my style of organizing tests. I guess I don't see any technical reason why test-framework should be considered better than hspec and vice versa.
 master n slaves = do [...] -- Wait for the result sumIntegers Isn't there a mistake? Shouldn't it be instead: master n slaves = do [...] -- Wait for the result sumIntegers n
Yeah, I'm sure it's not too hard, although there's also the problem of a server to host it on...
So, proxy transformers free us from having to unwrap the client and the server proxies before composing them, when one or both of the proxies come wrapped in an "outer" monad transformer like StateT or ErrorT. But is unwrapping-before-composing really that bad, so bad as to require that whole new machinery to avoid it? Do proxy transformers handle the case in which the client and the server come wrapped in different monad transformers? Say, a client wrapped in a StateT and a server wrapped in an EitherT? Also, while playing with pipes/proxies, sometimes I find myself in situations in which I have a client and a server over slightly different base monads (say, StateT IO as the base monad for the client and WriterT IO as the base monad for the server). How to best handle these cases?
Yeah I kind of got that feeling too. It seems like a lot of work to avoid putting a few seemingly sensible calls here and there.
&gt; But is unwrapping-before-composing really that bad, so bad as to require that whole new machinery to avoid it? Yes. The simplest way to prove it to yourself is to just try mixing extended pipes with the non-extended pipes by manually unwrapping the extended ones. That is the fastest way to convince yourself. The example I gave in the tutorial made it look easy because both pipes had the exact same feature set, but when there is a mismatch in feature sets then getting the unwrapped types to match up is a royal pain and non-standard. Any attempt you make to provide a uniform interface to "regularize" the types ends up just recapitulating proxy transformers. Also, you are only considering the case of **composing** base proxies with extended proxies. Don't also forget using base pipes within the same do block as extended pipes. I'll use the specific example of StateP. If you were to manually unwrap StateT to sequence it with an unextended base pipe, you'd be in charge of manually threading the leftover state across unextended pipes, like `conduit` forces you to do with `($$+)` and friends. `StateP` handles all of that transparently so that you don't have to do this manual state threading. &gt; Do proxy transformers handle the case in which the client and the server come wrapped in different monad transformers? Say, a client wrapped in a StateT and a server wrapped in an EitherT? Not yet. This was a feature I was meaning to add to this release but did not have the time. The short answer is that the solution is to define a PFunctor class (a functor in the category of `Proxy`-like type constructors) so that you can embed missing layers of a proxy transformer stack to fix feature set mismatches like that. &gt; Also, while playing with pipes/proxies, sometimes I find myself in situations in which I have a client and a server over slightly different base monads (say, StateT IO as the base monad for the client and WriterT IO as the base monad for the server). How to best handle these cases? I actually DID implement this feature but FORGOT to write about it in my blog post. `Proxy` and all its transformers implement the `MFunctor` class (a functor in the category of monads), which lets you do this. It takes any monad morphism and applies it to the base monad. So, for example, if proxy `p1` has the base monad `IO`, and proxy `p2` has the base monad `StateT IO`, you would just use: mapT lift p1 &lt;-&lt; p2 However, if neither base monad is "greater" than the other (like the example you gave of `StateT IO` versus `WriterT IO`), then you need a more sophisticated monad morphism. However, beyond that point it's entirely an issue with monad transformers and not proxies, since then it is simply a matter of finding compatibility with `StateT IO` and `WriterT IO`, a problem that has nothing to do with proxies. Again, you could define something like a `TFunctor` class (a functor in the category of monad transformers) that lets you fix monad transformer compatibility problems, but that is WAY outside the scope of my library.
These allow you to intrusively modify a proxy without pattern matching on the type. Unlike composing a proxy upstream or downstream, these kinds of modifications give you access to both interfaces of the underlying proxy simultaneously, allowing you to transmit information between them where there previously was no transmission at all. For example, you could use (`/&lt;/`) to convert the `discard` proxy into the `idT` proxy, something that's not possible using composition. The reason I even care about not matching on the proxy type is that I actually want to type class all the utilities in the proxy prelude. All those utilities in there are actually agnostic of the base type and use the type classes exclusively, but for reasons related to the broken constraint system, I was unable to fully generalize many of them, so I just said "screw it" and specialized them to `Proxy` for this release. This will be important later on since I plan on releasing a variation on the Proxy type for resource management that will also implement those same type classes and I would like that utility library to work with that extended type transparently. As for `(/&gt;/)` and `(\&gt;\)` specifically, none of the utilities require those currently, but I had in mind some other utility function (i.e. zips and interleavings), which would require that abstract interface.
You're quite right, that's how it is in the code in the `distributed-process-demos` package. We'll fix the blog post.
Also, mempty is not used in the solution. Semigroup only plz.
The other question I was meaning to ask is what your troubles were with the constraint system :). Only not being able to write `forall a. Foo a` as a constraint (as I saw on StackOverflow), or also others? Anything to justify calling it "broken" instead of "not as powerful as I would like"?
There's a good chance I'm misinterpreting you, but if I'm not: In the first case the instance declaration is saying, "Proxy a' a b' b m is a Monad, as long as m is a Monad". With the constraint you're saying, "I need p a' a b' b m to be a Monad". The compiler unifies p with Proxy, and sees that it is a Monad, but only as long as m is a Monad. So then it needs evidence that m is a Monad, but in the example there's nothing to deduce that from. If you had a situation where you already knew that Proxy a' a b' b m is a Monad, then in that case Monad m would also be implied, but here we're still in the process of figuring out whether it is one. In the second case I'm not clear what you want, but if a universal isn't working, then maybe you want an existential?
So, I'm pretty beginnerish in Haskell as well and looking for something to hone in on some skills and build something that isn't a trivial code snippet. I've been using Haskell for about a year now -- I'm no expert, but I would actually be pretty interested in helping you out with this if you could explain the project to me a little more in depth. Not sure if you're looking for someone to help you out, but I have been getting increasingly more interested in a collaborative project, and this looks simple enough at face-value for me to actually impact it, if you're interested. For an idea of how skilled I am at Haskell, you can take a look at my only real project on GitHub to date... [My brainfuck interpreter](https://github.com/5outh/fhck) (it's nothing special and could be improved in many places). As far as math goes, I'm still relatively new, but I really like it and I'm ready and willing to learn whatever's necessary. Send me a PM if you're interested in some help! :) 
Sorry, I meant to say it does not reduce the constraint to (Monad m) which I can supply but it never gets that far. I can give a simple example of what I mean by the latter. Let's imagine that I wanted a generic way to compose monad transformers. I'm typing this on my phone so there are probably mistakes: newtype Compose t1 t2 m r = C (t1 (t2 m) r) instance (MonadTrans t1, MonadTrans t2) =&gt; MonadTrans (Compose t1 t2) where lift = C . lift . lift That won't type check because it (correctly) cannot deduce from the type signature of lift that it produces a monad as its result. However, even if I could change the type signature of lift from MonadTrans to something like: lift :: (MonadTrans t, Monad m, Monad (t m)) =&gt; m r -&gt; t m r ... that still does not work. I have no way of proving to the compiler that the output of lift is also a monad suitable for a second invocation of lift. Now, that is not what I actually wanted to implement, but I think it is the simplest illustrative example. 
Thanks for taking the time to explain that. You just dramatically improved my intuition for forall and exists.
Yay! \o/
I like the look of the proxy library. Do you plan on getting rid of pipes and/or implement them in terms of Proxy, as in Control.Proxy.Pipe? One thing I do not like about eh pipes library is that you pick new terminology for every type. There are pipes that yield and await, with sources and sinks; frames with stacks; and proxies with responses, requests, servers and clients. Wouldn't it be better to just stick to one set of terms for everything? Something similar goes for names like `discard` and `ignore`. Without looking at the documentation, how am I supposed to know which one ignores/discards requests and which one ignores/discards responses? It might be clearer to rename the functions to `discardRequests` and `discardResponses`. Why is there a `() -&gt; ...` in the type signature of `runProxy`? runProxy :: Monad m =&gt; (() -&gt; Proxy a () () b m r) -&gt; m r I assume it has something to do with the idea that each proxy/pipe/frame/whatever always gets an input to represent its first request. But that is not true for clients/sinks and sessions/pipelines, i.e. things at the bottom of a stack. I am also still not entirely happy about the termination of an upstream pipe killing the entire stack, but we have discussed that enough already. Edit: one more thing: should `(\&gt;\)` and friends be in the same type class as `request` and `respond`? I can imagine types for which it is possible to implement the latter but not the former. And on a related note, why are there no instances of the `Interact` type class for some of the proxy transformers?
The short answer is "It doesn't affect the time complexity of composition or of long-running proxies, which are still linear, but it does affect the constant factors of binds and proxy composition". I don't know the exact impact on the constant factors because it is very awkward to implement composition for the codensity transformed version, although it can be done. The reason is that if you treat the free monads as lists, then proxy composition is (more or less) interleaving a bunch of lists, which involves a bunch of operations at the head of the list, something that *might* be inefficient for the codensity transformation (but I don't know for sure), and that is also awkward to implement. I have implemented subsets of proxy/pipe functionality without composition and I see a speed-up in the monad bind's constant factors of about 30% for entirely pure code, which is low compared to the other enhancements I could do, but these benchmarks are not entirely representative since composition is usually the biggest source of inefficiency and they don't test that. However, my guess is that composition in general is probably slower for the codensity transformation because of all the operations at the "head"s of the proxies. One of the reasons I switched to the `free` library was to keep open the possibility of using Edward's codensity transformations, however fully porting composition to use that is still tricky regardless, although it can be done. The codensity transformation should be completely invisible to proxy transformers. They completely abstract over the underlying type. All they require is that the base proxy type implements the `Channel` type class (i.e. proxy composition) and `Monad`. This means that if I do implement the codensity transformed version, it's invisible to users of the library. However, I want to emphasize that it does not affect the time-complexity of long-running proxies. The existing implementation still has linear time complexity because `forever` right-associates all binds. I've also empirically proven this by just leaving proxies running for days with trillions of binds and no perceptiple slowdown.
Well map makes sense for far more data types than just list for a start, at least theoretically if not in Haskell.
I won't get rid of the original `Pipe` implementation, because I think it is just a plain useful reference implementation that belongs somewhere on Hackage, but I am planning on making the `Proxy`-based reimplementation work nicely with proxy extensions. Proxy transformers play very nicely with `Pipe`s as `Proxy`s (just `liftP` and `runXXXP` instead) and all the `D`/`B`-suffixed utilities (i.e. the ones that make sense for pipes), will work okay as `Pipe`s if you just apply them to `()`. In the long run, I will add `P`-suffixed utilities specialized to `Pipe`s so that you don't need to add the extra `()` parameter. The first big main remaining problem is `Server` and `Client` utilities, which don't play nice with `runPipe`, so they definitely need `Pipe`-specialized versions or generalizations of their types. The other big remaining problem is getting `Pipe`s to play nice with `runProxy` so that you can more easily mix `Pipe` and `Proxy` code. I don't have strong opinions about naming conventions for each abstraction. If I were to keep `Frame`s in the long-run, I would agree that it should share terminology with `Pipe`s and they should share a type-class for common operations (and I know what the laws for that type-class should be now). However, I'm eventually deprecating `Frame`s, so that's low on my list of priorities. I think `Proxy`'s bidirectionality is a significant enough difference in semantics from `Pipe`s that its warrant the name change for its operations just to signal to users that their mental model needs to change to use them. However, now that `respond`/`request` are type-classed you don't need to worry about further extensions to the `Proxy` type introducing new operations. They will all still use `request`/`respond`, so at least that terminology is stable for all bidirectional abstractions. For `discard` and `ignore`, I can follow the naming conventions I use for utilities by renaming them to `discardS`/`discardC` (one is a server that discards all input, and the other is a client that discards all input). They predate my utility naming convention, which is why it never occurred to me to also rename them to follow that convention, but I like your idea and I will do that for the next release. `runProxy` takes a `()` so that you can run things without first applying them to `()`. Otherwise you would have: runProxy $ (p1 &lt;-&lt; p2) () More philosophically, the `()` belongs there because Kleisli arrows of proxies are actually the more central and fundamental unit of abstraction than just the proxies themselves. You see this the more you study the structure and laws of the proxy type. In fact, `runProxyK` is actually the more fundamental operation and `runProxy` is just a convenience operation for users, so that they don't have to type: runProxyK (p1 &lt;-&lt; p2) () Regarding upstream termination, this is something I want to address using an extended `Proxy` type that still implements all the `Proxy`-based type classes. It will be backward compatible with everything I've released so far since there will be a functor from `Proxy` to it, and all proxy transformers will layer just fine on top of it. The main thing I have to work out is how guarding against upstream interception plays nicely with resource management without using indexed types.
I understand, syntax is highly personal. The discussion seemed to be taking a code-golf direction; hence, my remark. 
It has been turned into a type class somewhere, I think, but honestly it's just not that useful. Having it not be in a type class is convenient because it's often the one place you and the compiler both *do* know the type, so it prevents you from having to write an extra type annotation.
We're #7! Got my eye on you /r/cpp. It'd be really cool to see the 'very large communities' charted over time so we could see growth rates.
I love Haskell. I don't have the first clue what I'm doing, but i enjoy it. 
Indeed. It's a bit more work, but there's also this: [Deploying Yesod apps to Heroku](https://github.com/yesodweb/yesod/wiki/Deploying-Yesod-Apps-to-Heroku) (Heroku provides a free tier of service)
Forgive me if I have misinterpreted your comment but I think you are raising two issues: 1) why is functional programming better (or not) than imperative programming for solving PDEs and 2) why use a comonad. The arguments for and against (1) have been well rehearsed (although not specifically for PDEs) so I don't propose to make what I am sure would be a poor summary here. With regard to comonads, the argument for using them is exactly the same as using monads: that they abstract a particular pattern of control. In more detail, we use e.g. error monads so that we can abstract away from having to say "if errorCondition then handleError else carryOnProcessing"; much more detail on the rationale for using monads is contained in this: http://research.microsoft.com/en-us/um/people/emeijer/publications/meijer95merging.pdf and other papers of the same vintage. Of course comonads are rarer beasts than monads much in the same way as unfolds seem to be rarer than folds. One my intentions in writing this was to show one of my colleagues that PDEs could be solved as well in Haskell as in C++. I had intended to move on to show how solvers could be composed to solve path-dependent options but sadly got side-tracked on to other things.
Thank you.
I think all those functions are sensible and I will try to include them. I have to specifically mention that right now I'm limiting to utilities that use the type classes exclusively so that when I add other variations on the `Proxy` type they can share the same standard library. Right now that means limiting things to the `Channel` and `Interact` classes, which can implement `up` and `down` just fine, but I may need to find some additional structure to the `Proxy` type that I can encode in a type class with laws before I can implement `flipP`. I still want to keep the Proxy prelude suffixed versions because the point of the Prelude is a safe function set to import by default along with `Control.Proxy` to keep import lists shorter when possible. Also, there is a (slight) performance advantage to specializing them to a particular direction. However, I don't mind keeping the more elegant unsuffixed versions you propose in a separate module that is not imported by default so that users can do it your way if they prefer.
I've been playing around with hand-drawn diagrams representing Proxy, and I think there is something interesting about extending the Proxy concept to allow closing/opening one "side". The basic Proxy can be thought of as a state machine* with 3 states: waiting for a request, in control, or waiting for a response. inl outr |--------| --------&gt; |--------| --------&gt; |--------| | wait 4 | | in | request | wait 4 | | request| respond | control| |response| |--------| &lt;-------- |--------| &lt;-------- |--------| outl inr It's easy to "compose" two of these diagrams, by overlapping 2 of the 3 states from each of two separate diagrams. If you consider "wait for request" the "starting state" of every proxy, then the diagrams show that composition is associative. But when you try to add "close-left" and "close-right" operations into the mix, it becomes *much* more complicated to hook up the diagrams. Every "close" operation infects the entire chain of diagrams, putting them all on a completely different state machine. I suspect it is still associative, but the diagrams are so hairy, I haven't waded through the whole affair yet. *Ok, not really a state machine. But still a machine... that can be in different... states.
That is assuming that a Church-style calculus is somehow privileged as the 'true' basis for Haskell. But one can have a Curry-style calculus, where terms are independent of the types assigned to them, with rules such as: Γ, a type |- e : T(a) --------------------- Γ |- e : ∀a. T(a) And: Γ |- e : T(U) ----------------- Γ |- e : ∃a. T(a) Etc. Type checking is going to be undecidable without assistance, which is what you can view the annotations and such required in GHC as. But the Curry-style view is that System F is a system of judgments on untyped lambda (or other) terms. And then, the type `∀a. T(a)` is the intersection of the types `T(U)` for particular `U`, while `∃a. T(a)` is the union. The subtyping relationship is fairly realistic there; universal types classify the same terms as their instantiations, and instantiations classify the same terms as existential types.
Might help if you put up a minimal code fragment that replicates the issue on hpaste or one of the other free pastebin sites. If `MState` has a `MonadIO` instance then `liftIO` should work. So something else is probably going on here, like the type of the monad you're calling `liftIO` in isn't what you expect it to be.
I've made some code to parse the string as a string table with each numbers and operators as a single elements in the string table import Data.Char data AST = Leaf Int | Sum AST AST | Min AST | Mult AST AST parseCharacter :: String -&gt; [String] parseCharacter [] = [] parseCharacter (' ':xs) = parseCharacter xs parseCharacter ('(':xs) = "(": parseCharacter xs parseCharacter (')':xs) = ")": parseCharacter xs parseCharacter ('+':xs) = "+": parseCharacter xs parseCharacter ('-':xs) = "-": parseCharacter xs parseCharacter ('*':xs) = "*": parseCharacter xs parseCharacter (x:xs) = if isDigit x then (takeWhile isDigit (x:xs)) : parseCharacter (dropWhile isDigit xs) else parseCharacter xs 
This works okay; you presumably have something more complicated? import Control.Concurrent.MState import Control.Monad.IO.Class import System.IO newtype MyState = MyState Int getline :: Handle -&gt; MState MyState IO String getline handle = liftIO $ hGetLine handle
I added a bit of the code, hopefully that clarifies things a bit. At this point, I don't want to post all of it online.
&gt; Did you mean "harder to reason about performance?" Yes, sorry I didn't say that clearly. It's harder to reason about time and space usage, because laziness makes it practically unknowable beforehand. &gt; Hopefully the insistence on laziness will eventually force us to come up with debugging tools / methodologies that are fundamentally more "correct" than stepping. I welcome such a tool, but find it hard to believe we'll see anything like this anytime soon. I think the REPL, combined with stuff like QuickCheck, go a long way towards understanding and debugging code, but it's hard to imagine anything more useful than a stepper.
What exactly is the point of using `(` and `)` if you have prefix operators? The main power of prefix operators is that you don't need to worry about precedence. The way I'd handle this (probably not the best, though probably one of the simplest) is to make a function `parseTree :: [String] -&gt; (AST,[String])` which parses a tree, starting with the beginning of the token list, and then spits out the rest back. It recursively calls itself, then. If the string has valid syntax then your leftover will be `(tree,[])` and it won't enter the edge-case of an empty list passed to parseTree. parseTree (o : z) |o == "+" = let (oa,ra) = parseTree z; (ob,rb) = parseTree ra in (Sum oa ob,rb) |o == "*" = let (oa,ra) = parseTree z; (ob,rb) = parseTree ra in (Mult oa ob,rb) |otherwise = (Leaf (read o),z) Computing it is probably a lot simpler: compute (Leaf a) = a compute (Sum a b) = compute a + compute b compute (Mult a b) = compute a * compute b
Well, if you *insist*... data WithRefsF ref next = NewRef a (ref a -&gt; next) | ReadRef (ref a) (a -&gt; next) | ModifyRef (ref a) (a -&gt; a) next deriving Functor type WithRefsT ref = FreeT (WithRefsF ref) type WithRefs ref = WithRefsT ref Identity newRef :: Monad m =&gt; a -&gt; WithRefsT ref m (ref a) newRef a = liftF $ NewRef a id readRef :: Monad m =&gt; ref a -&gt; WithRefsT ref m a readRef r = liftF $ ReadRef r id modifyRef :: Monad m =&gt; ref a -&gt; (a -&gt; a) -&gt; WithRefsT ref m () modifyRef r f = liftF $ ModifyRef r f () data Ref a = ... runWithRefsT :: Monad m =&gt; WithRefsT Ref m r -&gt; m r runWithRefsT = ... someFunc :: Int -&gt; Int -&gt; IO Int someFunc a0 b0 = runWithRefsT $ do -- first we manually allocate our stack a &lt;- newRef a0 b &lt;- newRef b0 flag &lt;- newRef False -- then we move on to the actual function a1 &lt;- readRef a b1 &lt;- readRef b when (a &lt; b) $ do modifyRef a (+ b1) modifyRef flag (const True) flag1 &lt;- readRef flag when flag1 (lift $ putStrLn "Hello, world!") a2 &lt;- readRef a return a2 `Ref` and `runWithRefsT` left as exercise to the reader. See these excellent resources if you are not sure how to implement an interpreter on top of FreeT: * http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html * http://www.haskellforall.com/2012/07/purify-code-using-free-monads.html
It feels that it already has a lot of killer libraries. Problem *seems* to me rather that it's still a bit cumbersome to do the basics, graphics and gui with debugging. Since it's mostly interfacing non-functional libraries when doing this debugging has to work. I admit I might not have given it an honest chance but I feel my opengl experiments mostly end in some cryptic run time crash. I probably lack knowledge in how to handle such situations efficiently.
Yes. Haskell's GUI libraries are in an atrocious state. It's a vicious cycle because people interested in GUI programming get turned off from Haskell and then we end up with a community biased towards non-GUI programming, so very few people have a motivation to fix it.
If anyone have issues with this release. Please let me know by e.g. filing a bug at https://github.com/haskell/cabal. Please include as much information as possible (e.g. steps to reproduce).
It's a terrible instance that only sort of makes sense. I'd much prefer Maybe be First and Dual Maybe be last.
It's overcomplicated because unless you're shipping around new printing lambdas at runtime then you might as well write the conditionals statically... Monkey patching is overcomplicated, yeah. I wouldn't suggest it either.
On second thought, laziness can cause difficulties in reasoning, because termination is part of semantics :) &gt; I welcome such a tool, but find it hard to believe we'll see anything like this anytime soon. I think the REPL, combined with stuff like QuickCheck, go a long way towards understanding and debugging code, but it's hard to imagine anything more useful than a stepper. Yeah, it may be a while out.. One example of significant enhancement to a stepper is being able to do "reverse debugging", or at least having enough metadata to divine what happened, historically. It seems like this desire to debug in reverse is rooted in a desire to see where your values came from. I don't think it's too far fetched to say that we might be able to instrument code in order to trace data-dependencies backwards, ignoring all of the irrelevant intermediary steps. There's also a lot of academic literature about the idea of using constraint solvers / model checkers for test generation - sometimes using the information from running the code to perturb the input to generate different results / coverage. As far as I know this hasn't gained much traction in most programming practice, but I could see such techniques being adopted by Haskellers, if there were good tools for it. This might allow us to ask our tools questions like "Why wasn't x == True? What would need to be different to make it so?" An interesting way to make Haskell more steppable might be to enter into a "mostly eager" mode, making the execution a little less "ADD". You might even be able to have "stepping out" effect evaluation order (though never in a way that would introduce puzzling behavior in safe code).
@KirinDave : Could you increase the font size of your essay somewhat? My eyes hurt trying to read the font, and browser zoom in (Chrome on Mac) does not work on your page for some reason. 
if you send me "ghc-pkg list" output from inside virthualenv, I'd take a look at it.
Also, roughly (I think, IANA expert) parseSum = Sum &lt;$&gt; (char "+" *&gt; skipSpace *&gt; parseAST) &lt;*&gt; parseAST :-) I generally prefer the applicative style 
Thanks cabal-team (or should I say cabal cabal?)! The parallel package installation option (-j) is a real timesaver.
A lot of people have asked me how they can transition into working on Haskell. This position is right for someone who has deep skills at Web UI engineering (especially strong on the client side with JavaScript) but wants to work with an expert Haskell team and gradually get deeper into Haskell, and into how these tools can work together. As with most FP Complete positions, you can live anywhere in the world as long as you have good Internet access. We live in the 21st century. :-)
I have tons of experience with QML and binding to QObjects. Let me know if you need some input. 
What about going the other way? Are you interested in someone who is already good at Haskell but with little professional experience with Web UI &amp; client-side JavaScript? I imagine those are the sorts of people you are more likely to find on /r/haskell.
Are you stating that here in Italy we have rubbish internet connection, Chris? Oh wait, you're right :P greetings from Rome, though.
What is the intended task for a js ui dev ? Yesod widgets ? Snaplets ? ghcjs frp framework ? I do not see how he fits your recent announcements, especially toolchain/IDE support. 
Yeah, Telecom Italia is murder, eh? Che palle. Greetings from Trento! If you're ever up here we should definitely go for a drink. Maybe we should organize some Italian Haskell hackathons? Are there more of you down there?
It's Polish Notation. Just reverse the string, then process it as RPN using a stack in the usual fashion!
More people than I anticipated decided that this question was not a good fit for SO, so I'm opening it for discussion here instead.
Back when dons submitted a load of Haskell articles to proggit every day.
&gt; There's also a lot of academic literature about the idea of using model checkers for test generation Do you know of any particular papers? I'd be interested to read them, and a UNSW undergraduate I know doing a thesis on this very topic would also be interested.
Oh, I was hoping to actually start using it. Proof-script style theorem provers grate on me (I'm hired to use one, Isabelle). I really like Agda's approach of actually writing the proof object -- if only the reflection API was more mature and more people wrote useful tactics.
[You mean this?](https://docs.google.com/viewer?a=v&amp;q=cache:t9iFJN8DDbQJ:research.microsoft.com/en-us/um/people/simonpj/papers/constraints/jfp-outsidein.pdf+&amp;hl=en&amp;gl=hu&amp;pid=bl&amp;srcid=ADGEESjJh1Ffyt5IwYX4TnyaFNhMFu4xZnyLfGXcgtqsfA8WEoAq9RfN3G9zXPlExwIJcm2CkEvL4C2LE1T-MT5Iaob-rw-dSRfC5gvUrEVU6-HacSTeC8hz-XqVtbchZ7muEH8KN700&amp;sig=AHIEtbQR_Rsdog6MQDegxWwMJ_NUEmvaCQ)
That's very strange. I think if the question was "What's wrong with Java's type system?" there'd be more opinion, debate, etc. But I think the problems with Haskell's typesystem are largely theoretical. People could mention things like the decidability of the type system under extensions to GHC, or ways you can mess up the type system, or how rigorous it is, etc. It's not so much a debate because Haskell does aspire to have a very well theoretically grounded typesystem.
Hey! I did search around a bit to see if I could find the things I vaguely remember seeing, before posting the above comment. Instead I just left it at "a lot" because the googling did turn up quite a bit of related material. I graduated from UW undergrad recently, but remembered that there was a somewhat relevant project [here](https://docs.google.com/viewer?a=v&amp;q=cache:jovpYdTRH9gJ:homes.cs.washington.edu/~mernst/pubs/palus-testgen-issta2011.pdf+&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESi8O4A1NNlIcTp3GOU7uvI3AvkWR71UIPwwf6T7x6IjcYmq6fV7z3C7auoILJTYyEoSSjOfGaJ8WYKHXIVejMGl829JA96IdXPnCUMZe3I5TKu3v7TXWVmySkI7GH-HjAr8g5A_&amp;sig=AHIEtbQ-h7v2blZuGhFm99Hj6cZYvx7nZA). It's not too heavy on explicit static analysis / abstract interpretation, but it does infer models that it then uses. In general the group there is very into mixing dynamic information with traditionally static techniques - exactly the sort of stuff that lends itself to debugging (though largely imperative focused and testing focused, rather than interactive tools). From that paper there are a few good references for symbolic execution for test generation: http://www.cis.upenn.edu/~alur/CIS670/cute.pdf http://users.ece.utexas.edu/~khurshid/papers/JPF-issta04.pdf I'm sure there are more! I don't think these are the ones I remember being blown away by. Wish I still had the reference!
[I twiddled your code](https://gist.github.com/3862632) a bit to help make the point. One change made was to exploit the point-wise semigroup (`Semigroup b =&gt; Semigroup (a -&gt; b)`) which is used on the (`Maybe a`) semigroup. Importantly, `mempty` is never used (it isn't in your code either). In this case, the benefit is not really exploited because it specialises anyway, but in practice, there are many operations and data structures that only require `Semigroup` (and in many of those cases, `Monoid` is not even possible). I am just encouraging to think in semigroups first, even though this use-case does not clearly demonstrate the reasons why. `Semigroup` is more appropriate for this use-case because `Monoid` is unnecessary specialisation. Perhaps the biggest benefit in this case is code readability -- by examining the absence of the use of `Monoid`, you can restrict the set of possible operations on the data. Not the biggest benefit overall, but hopefully gives a nudge in the direction :) HTH.
Ah, thanks. I've learnt something today :) You mean you are working on the constraint system? At Microsoft Research? I have absolutely no line of sight on these things, so excuse me if I'm asking silly questions.
You need to be more precise about what a "crash" means. For example, it *shouldn't* be possible to dump core under normal circumstances; however there can be bugs in GHC runtime or code generation, there can be bugs in libraries that call out to C, and of course you can cause core dumps via unsafeCoerce and friends. So there are still a few caveats.
I don't know about a tautology--it's still important that something of type `A` is never a value of type `B`, even if we take this for granted in Haskell. It might be any value of type `A`, which for lifted types includes ⊥, but without resorting to horrifically malevolent stuff like `unsafeCoerce` we still have that guarantee of type safety, along the lines of Milner's statement that "well-typed programs cannot 'go wrong'".
I think it's reasonable to say that if you don't use partial functions, exercise caution to avoid infinite loops, and don't subvert the type system, you can have a reasonably high expectation that your program won't crash short of hardware malfunction.
|choosing to evaluate our program on a finite register machine, rather than in our brain. I would just like to point out that the probability that a human brain could evaluate any reasonably large computer program without error is exceedingly low. We make mistakes on trivial four step problems, let alone thousand or million step problems. Also, the suggestion that our brain has unlimited memory is almost certainly false. It has a finite size, thus a finite storage capacity. 
I'm annoyed at StackOverflow crowded thought police, that decides with often frankly dubious reason that a question is "not a good fit" for SO. It often happens for questions that could generate very interesting answers, and in the fairly specialized tags I follow (OCaml, Haskel, FP) I have always seen it used for the worse. If you look at the people that voted to close this question, two of them aren't even Haskell programmers at all. They just saw a question of the form "What's wrong with" and voilà, let's use those moderation powers, the question is closed. I don't know if there is something to blame for this (the individuals doing the moderation are individually doing their best and trying to be helpful), but the result is a crowd mentality that is often just annoying. I suspect this is a fundamental problem with SO that was purposefully designed this way to become a reference on answers for easy questions about mainstream technologies, at the expense of the few interesting questions about more sophisticated topics. 
We, too, assume that all nodes in the network run the same code. As regards a Hadoop-like stack, this is something that could be implemented on top of Cloud Haskell.
&gt; There were several times where I wanted to write something like: (forall a . SomeClass (f a)) =&gt; as a constraint, but could not. This seems like this is something that is fixable since this is clearly implementable using the dictionary equivalent. Just going from gut instict, I would say that while this is certainly possible as part of System Fc (or System F pro or whatever we're using now), it would likely substantially complicate inference. In particular, it would make universal quantification part of the base constraint language (called Q in the OI(X) paper, if you're interested). The base constraint language by default introduces *no new type variables*, with all quantifiers being added in the extended phase. I suspect adding quantifiers into the base constraint language Q would make constraint solving very difficult, if not undecidable in general, and make type inference substantially harder to do.
&gt; doesn't allow them to be "results" of functions that can be passed to other functions to satisfy downstream constraints. I'm puzzled as to how this would look. Are you sure you can't get by with a top level type class implication constraint? e.g Show a =&gt; Show [a]? You could also capture a type class constraint in a GADT: data IsC :: * -&gt; * where IsC :: (C a) =&gt; IsC a This gives you an explicit witness that (C a), allowing you to "return" proofs of that with functions: foo :: (a, IsC a) 
Ahh, now it's clear. Thanks for the answer. And for contributing to my favourite language/compiler :D
&gt; Comfortable working with senior colleagues Hmm, as opposed to?
heh
Yeah, that GADT trick is the workaround that `illisius` mentioned. I'm not entirely sure why my `MonadTrans` composition example does not work, but I believe the problem I encountered with `pipes` is similar in nature to that. Here's the full example code: {-# LANGUAGE KindSignatures #-} class MonadTrans t where lift :: (Monad m, Monad (t m)) =&gt; m r -&gt; t m r data Compose (t1 :: (* -&gt; *) -&gt; (* -&gt; *)) (t2 :: (* -&gt; *) -&gt; (* -&gt; *)) m r = C (t1 (t2 m) r) instance (MonadTrans t1, MonadTrans t2) =&gt; MonadTrans (Compose t1 t2) where lift = C . lift . lift
&gt; notably, if you write a recursive function, you must recurse on a [subterm] of the input And heaven help you if that subterm is in an indexed type.
This position requires someone who's ready to do hardcore client-side Web programming immediately. As we continue to grow, we'll need other mixtures of skills for other positions. 
I was just in Firenze and Siena a couple of weeks ago, and my Internet access was quite good. Guess I was lucky?
StackOverflow is explicitly designed for ["practical, answerable questions"](http://stackoverflow.com/faq#dontask). Problems that have solutions, like "how do I do X" or "why is this code so slow". It is not designed for questions that generate discussion. "What's wrong with X" is not an answerable question, at least not in that form. Closing a question on SO does not represent a judgment of the intrinsic value of that question. By the same token, whether a question would generate interesting responses is not a criterion for whether to leave a question open. As far as SO is concerned, open-ended questions that generate a fruitful discussion are perfectly good questions—for some other site. It's hardly fair to be upset with SO moderators for enforcing the rules of SO. Whether you (or I) feel that the SO question policy is for the best is another matter; personally I too find that it can be annoying, but given the success of SO, I am inclined to give them credit for knowing what they are doing. Anyways, by now the question has been reopened. (Finally, this bit of your post misses the mark: &gt; [SO was designed as] a reference on answers for easy questions about mainstream technologies, at the expense of the few interesting questions about more sophisticated topics There are plenty of perfectly good easy and not-so-easy Haskell questions on SO. Being about "mainstream technologies" has absolutely nothing to do with whether a question is suitable for SO.)
&gt; That's very strange. I think if the question was "What's wrong with Java's type system?" there'd be more opinion, debate, etc. That's exactly the point—StackOverflow is specifically *not* a site for opinion, debate, etc.
Thanks for the explanation. So really, every one of my problems boiled down to the polymorphic constraints issue! So I guess I will take the time to try out the GADT workaround that people have been suggesting.
assuming: no partial functions, no infinite loops, no IO, no compiler bugs why "reasonably high" expectation that the program won't crash? why not absolutely certain? 
Space leaks!
Woah, all the proxy transformers require `KindSignatures`? :(
Doesn't 'converge' to a return value :)
&gt; but GHC Haskell also has to deal with GADTs, so we're well beyond F to begin with. That's true, but in terms of inference, the only new thing is the existential/implication constraint Exists a. Q =&gt; C for local assumptions. This constraint form is only ever introduced by the inference algorithm itself and never by the user. Whereas allowing quantified constraints in the base constraint language Q seems like a far more scary addition. 
I'll paraphrase a question from the post here. Are there any generic functions you're using on — or have wished you could use on — indexed datatypes? Thanks.
With the caveat that non-termination detection is hard so ignore that, I would like a program that scans your Haskell codebase and finds definitions and uses of partial functions. It's easy to lure yourself into a false sense of security, thinking you're using total functions but then you go and use `maximum` or something and you get a lovely exception.
I'd be interested in some insight into this too. I'm trying to write an arrows based FRP library for fay at the moment, but I can't quite get the arrow loop right. Anything that could help be develop my intuition for loop and fixed points would be deeply appreciated!
I'm actually really surprised at the amount of people who use Haskell at work. I thought that basically nobody was able to land jobs like that. Encouraging. Thanks for the survey and data analysis!
I just tried it, and MonadTrans works quite nicely with explicit dictionaries. See https://gist.github.com/3873221
I use haskell at work but only for small, internal software. It's sadly not my main programming language at work, but I'm definitely trying to use it more. 
Nice!
I didn't try to look for a haskell job, I just found a job I liked and then when some projects came up where haskell made sense, I used it.
&gt; Basically you can have either type inference or program inference, but not both at the same time. Fingers crossed someone eventually lifts this restriction so that I just have to write `main =` and then have my program filled in for me.
What are your considerations for when other people need to look at your code? (I'm asking this because I'd like to do the same, not to get a rise.)
[Spurred by chrisdoner](http://www.reddit.com/r/haskell/comments/118gm4/i_wrote_this_trying_to_understand_if_the_type/c6ku66p), I decided to go ahead and create a wiki page for this.
 (def targets (sorted-map 3 "fizz" 5 "buzz")) ;; match any applicable factors (defn match [n, factors] (filter #(= 0 (mod n %)) factors)) (defn fizzbuzz [n] (let [factors (keys targets) matches (match n factors)] (if (= 0 (count matches)) (str n) (reduce str (map targets matches))))) (join ", " (map fizzbuzz (range 1 20)))
&gt; Evaluating head [] is a bug in virtually every Haskell program. (In languages like Python, this is less true because you can catch it anywhere. In Haskell, it's an asynchronous exception that can only be caught in IO). I would make the much stronger statement that evaluating `head []` is *always* a programmer bug. The *only* reason you should even be *able* to catch it is for long-running processes (servers) that need to be able to log errors and continue *but in such a case it's still a bug*, just one that does not break the rest of the service. Unfortunately, in Haskell-land we have a concept of "Exceptions" that includes `IO` exceptions (and in GHC-land even possibly *user defined exceptions*) that *might actually be used for errors you want to catch*. This is a very bad situation.
They do a "darcs get" and then look at it? I'm not sure what you mean beyond that.
I added some. But I suppose Chris meant something that would work in an automatic fashion? And I think the haddocks of each module would be a better place for that info. 
I think he's asking about whether your co-workers are able to understand Haskell code.
Well, you can't implement them in Haskell. Any fixpoint combinator you do implement in (safe) Haskell cannot be the Y combinator.
Look for smaller companies, where the owner(s) are directly involved and care about the success of the company. I've never worked anywhere where I didn't choose my programming language, operating system, database, etc, etc. The whole point of IT staff is to make such decisions.
That's exactly what happens. You store a constraint in the GADT and take it out again by pattern matching. Just like with normal data, except the compiler handles the plumbing.
&gt; the type ∀a. T(a) is the intersection of the types T(U) for particular U, while ∃a. T(a) is the union. Good summary.
`map` and `filter` are not partial.
Thanks! I've been getting blank stares when I mention Haskell, which is pushing me toward working on the projects during my own time.
mostly, that it loops forever.
So it doesn't work if you simply pattern match against '_' or any irrefutable pattern?
That's right.
Correct. The constraint is inside the constructor, it is just a hidden argument. So you have to use an actually pattern match to get access to the constraint.
How did you get your coworkers excited about learning haskell?
where do you look for these companies?
Your first program is wrong because the loop combinator is meant for constructing a loop from output of an arrow to its own input, not for defining a recursively composed arrow. The latter can only be achieved through let. If you want to define an arrow that can act as a fibonacci number generator in the setting of stream programming, then you should look into arrow FRP. The paper [Causal Commutative Arrows](http://www.thev.net/PaulLiu/download/jfp092011.pdf) section 9.3 gives a definition of the fibonacci arrow you wanted. (*Disclaimer: I'm the first author*) The gist is that you need more than loop combinator for stream programming, namely, an init (or delay as called in other places) combinator is the enabler.
(Added, together with `mod`, etc.)
Depends on the type. ∀x. x ⊢ 5 / 0 :: Rational *** Exception: Ratio has zero denominator I'm pretty sure that division being a total function was intentional in the design of IEEE floats.
Most list functions are partial if you consider non-terminating functions to be partial. length, map, filter, last, init, sum, etc.
I'm waiting for Duncan's Cloud Haskell talk to show up.
That should have been school, then :) The result is consistent with what we have seen at ICFP this year. For example, 40% of the over 450 attendees did not attend the main (academically focused event). Instead, they went to the workshops and to CUFP (Commercial Users of Functional Programming). I conjecture that most of these 40% were practioners, not academics or PhD students. Moreover, looking at affiliations on name tags, there were also many non-academics at the main event.
In practice, but not really in theory.
I think you'd have to make that concession. If `anyFunction` was non-terminating you'd be in the same boat, and then you'd have to include functions like map in your list of partial functions.
Considering tools had the least amount of very interested people I'd say haskell is still at the 'avoid success at all costs' stage. :)
*because they mark the beginning of the week!* Oh wait, Monads. Not Mondays.
Isn't this way more complicated than necessary? I just proved this without crazy type-classes or generalised induction principles: {-# LANGUAGE PolyKinds, DataKinds, TypeFamilies, TypeOperators, KindSignatures, GADTs #-} data (:==:) a b where Refl :: x :==: x cong :: a :==: b -&gt; f a :==: f b cong Refl = Refl trans :: a :==: b -&gt; b :==: c -&gt; a :==: c trans Refl Refl = Refl data Nat = Zero | Suc Nat type family (:+:) (a :: Nat) (b :: Nat) :: Nat type instance (:+:) Zero x = x type instance (:+:) (Suc n) x = Suc (n :+: x) data ForallNat :: Nat -&gt; * where SZero :: ForallNat Zero SSuc :: ForallNat n -&gt; ForallNat (Suc n) nPlusZero :: ForallNat n -&gt; (n :+: Zero) :==: n nPlusZero SZero = Refl nPlusZero (SSuc n) = cong (nPlusZero n) sucDistrib :: ForallNat n -&gt; ForallNat m -&gt; n :+: (Suc m) :==: Suc (n :+: m) sucDistrib SZero m = Refl sucDistrib (SSuc n) m = cong (sucDistrib n m) plusComm :: ForallNat n -&gt; ForallNat m -&gt; (n :+: m) :==: (m :+: n) plusComm n (SZero) = nPlusZero n plusComm n (SSuc m) = trans (sucDistrib n m) (cong (plusComm n m)) 
Why do all the cool haskell things happen where I'm not? In any case, it sounds like an interesting talk. Any chance there will be a video after?
Correct: the techniques I used are certainly not the simplest way to prove commutativity of addition. My intention was an approachable sketch of some connections to constructive dependent type theory.
Yet another simple demo? I've been off the reddit for a while, so if that's what you meant, would you be willing to drudge up whatever links you had in mind? I'd like to read them. Thanks. If you meant yet another blog post, then: I recently graduated and am temporarily unemployed. :)
I suppose so. So I'll ask this instead: what's crazy about them? They're no crazier than the ranges of your sucDistrib and plusComm functions, right? They're just "constraint synonyms"; I could have used a Constraint-kinded type synonym instead, but I think those are sometimes jarring to see.
In arrow based FRP, loops are for behaviors that depend on themselves somehow. If you don't mind me plugging my own article, I have an example [here](https://github.com/leonidas/codeblog/blob/master/2012/2012-01-17-declarative-game-logic-afrp.md) which demonstrates why loop is required and shows how it's implemented in my AFRP library.
Also, in what sense are you saying this proof is "simpler"? This one doesn't use ConstraintKinds or RankNTypes — I'll agree that the evidence for implications was a bit exotic. But this one explicitly invokes congruence and transitivity, whereas the proof in the post leverage GHC's type equality to get such basic equational reasoning implicitly. Did you have some concrete notion of "simpler" in mind? And thanks for the dialogue! I wasn't aiming for simplicity, but should definitely have considered the issue.
&gt; Also, in what sense are you saying this proof is "simpler"? This one doesn't use ConstraintKinds or RankNTypes — I'll agree that the evidence for implications was a bit exotic. But this one explicitly invokes congruence and transitivity, whereas the proof in the post leverage GHC's type equality to get such basic equational reasoning implicitly. I would call that less simple than explicitly invoking such concepts. If you want to write a proof of this in martin-lof type theory then you have no automatic equational reasoning given to you by a compiler. Hell, even in Agda you still need to invoke cong and trans explicitly. &gt; And thanks for the dialogue! I wasn't aiming for simplicity, but should definitely have considered the issue. Sure, it's just that the title was "simple demo" on the post, I was puzzled because to me it looked far from simple.
It's not "overcomplicated." It's "slow." The big complaint about that ruby code is that it's only slightly faster than the worst conditional expression. A linear ruleset like that should not present any complexity to anyone used to Ruby. It's _not_ complicated.
I'm working on giant carbon fiber kites with little wind turbines on them to save the planet. There's a nice animation [here](http://homes.esat.kuleuven.be/%7ehighwind/). Right now I use my library [not-gloss](http://hackage.haskell.org/package/not-gloss) (not affiliated with gloss, thus the name) to do live 3d visualization of my simulations and trajectory optimizations, using [zeromq-haskell](http://hackage.haskell.org/package/zeromq-haskell) and [protocol-buffers](http://hackage.haskell.org/package/protocol-buffers) for message passing from python/C++. I'd love to use haskell for much more but many in the group aren't programmers and don't have time or will to learn haskell. I'm chipping away at some of them though :)
Interesting. Is there any reason why GHC doesn't do this by default?
I've shamelessly stolen your code for my fay library https://github.com/boothead/fay-frp and your write up on arrows is the best I've seen - so a big thank you! I understand what loop does and (almost) how it does it. My sticking point right now it allowing the the generated javascript to be sufficiently lazy to express it. What I end up with is a thunked expression several thunks deep that doesn't get properly unwrapped when it's evalled. I'll keep plugging away I guess :-)
Does it really count when you include bizarreness like NaN? I can understand the infinities but something not equal to itself is just silly: Prelude&gt; let x = 0 / 0 in x == x False 
When I added `fromJust`, I put it in its own Data.Maybe subsection. Someone moved it. Tsk. I've moved `fromJust` to the Data.Maybe section, and `genericLength` and `genericIndex` to the Data.List section. Any more? (Do edit the page yourself if you can.)
Is it worth drawing a distinction between functions that are total on finite data but partial on infinite data -v- functions that aren't total even on finite data? Intuitively I feel that I should be warier when using `head` than when using `length`. Is my intuition wrong?
What I find encouraging about this is how the GHC plugin framework makes one feel less overwhelmed by complexity. This doesn't look too hard. Hacking GHC is scary by comparison.
&gt; Isn't this way more complicated than necessary? &gt; ... &gt; {-# LANGUAGE PolyKinds, DataKinds, TypeFamilies, TypeOperators, KindSignatures, GADTs #-} 
Who put foldl/foldl'/reverse? These are not really partial in any meaningful sense. An infinite loop in not necessarily a failure.
both nonterminating and partial functions are ⊥
Right. That's a more clear case I forgot to mention where the compiler cannot do it automatically, but the user will know ahead of time if that is the case.
You can easily quite and instantly splice a part of code using TH.
&gt; unary RealFloat functions Even a simple "x + y" for floats may return different values depending on what processor it's run, which you can't deduce at compile-time. It's not a valid optimisation probably.
This article is to FRP what a laundry detergent advertising is to good house keeping. The "[interactive page source](http://elm-lang.org/edit/learn/What-is-FRP.elm)" is quite impressive, regardless.
That is a pretty serious restriction. Also this doesn't seem to work: {-# LANGUAGE TemplateHaskell #-} module CompileTime where import Network.URI import Language.Haskell.TH.Syntax (lift) value = $( lift (parseAbsoluteURI "http://localhost") ) It seems like it should be possible (though probably not easy) to add this to GHC as a pragma-able thing like INLINE, etc.
Mmh, indeed. So, is there some TH way to do this generally? I think I read the claim a lot of times that there is, so…
If someone would like to use the ideas in here for a production quality MapReduce implementation, I would start by sending file names of files containing the key-value pairs, instead of the key-value pairs themselves, so each worker can pull the key-value pairs themselves. This is much more efficient than having the master read and send them.
Isn't it strange to list functions like `toEnum` that have a type class constraint? Or, if you list them, you should list for which instances they are partial. For example, I think the `toEnum` for `Integer` (and `Int` :)) is total.
Rereading that blog post, I'm now curious about what a Freyd category is and what it's useful for. (A quick search online yields only articles aimed at mathematicians, which alas I'm not.)
The problem is that there is no Lift instance for URI. But, since URI has a Data instance, you can use `dataToExpQ`: import Language.Haskell.TH.Quote (dataToExpQ) value = $(dataToExpQ (const Nothing) (parseAbsoluteURI "http://localhost")) 
Yes, definitely. ATM the list doesn't look very credible because of this — there's a huge difference between foldl and foldl'. I'm not sure the list of non-corecursive (foldl-like) functions is very useful at all.
The emacs mode at least already includes a convenient way to get the core output, but it could of course be so much nicer.
Aaaa great You should make a tutorial how to setup everything the basics etc..
If understood SPJ right in his recent *Haskell Exchange* talk, the contracts are a complementary approach to dependent types, providing something also called [refinement types](http://en.wikipedia.org/wiki/Program_refinement)...
I know I've seen some people talking about FRP having problems with e.g. [time leaks](http://blog.edwardamsden.com/2011/03/demonstrating-time-leak-in-arrowized.html) or space leaks. Have you noticed any problems in Elm with leaks or other performance problems?
Refinement types are just subset types/dependent pairs! :)
Well... floating point values aren't really "numbers" in the usual sense, since things like arithmetic, equality, and ordering don't in general behave in the expected way. So at least NaN is *honest* about it. :]
[This page](https://github.com/evancz/Elm/blob/master/README.md) covers basic set up, but it is pretty bare bones. I like your idea :) I'll put something together that's not hampered by github's 4-to-5 inches of totally unnecessary header. What sort of things would you want to see in a "getting started post" besides installation? Maybe how to get a project going!
Some interesting new slides about adding contracts to haskell in SPJ's talk..
Learning the language something close to learn you a Haskell syntax operations JavaScript relation etc
From the author's work: {-# LANGUAGE RankNTypes, TypeFamilies, DataKinds, TypeOperators, ScopedTypeVariables, PolyKinds, ConstraintKinds, GADTs, MultiParamTypeClasses, FlexibleInstances, UndecidableInstances, FlexibleContexts #-} 
Only a little. My (limited) understanding is that I can call a function in the IO Monad to get some input, and if I call it a second time I'll get a different answer, but that seems different from having a state variable that changes.
Ah, okay. Signals are not the same as stateful variables (although I see why you make that connection). Get the idea of stateful variables out of your mind. If this was *Dune*, then stateful variables would be the mind killer. Signals can change but they cannot *be* changed. There is no such thing as destructively changing the value of a signal; signals just *have* a value (like radio waves or waves in the ocean). If you want to work with `Mouse.position`, a value that changes, you have to explicitly create a new signal by `lift`ing a pure function onto it. This only messes with the current value of the signal. In other words something of type `Int` will never ever change. It is immutable. But something of type `Signal Int` can change. But if you want to do anything with it, you need to create a new signal with something like: lift :: (a -&gt; b) -&gt; Signal a -&gt; Signal b So the types make sure that the changes happen in safe ways. So you cannot say `(1 + Mouse.x)` and get a snap-shot at the time that code executed. In fact, that code does not type check. Maybe that's clearer? If it is still confusing, tell me what *you* think FRP does and I can help figure everything out. There is maybe some assumption that is making things extra complicated.
Hmm, ok, that is making some more sense (also I should mention that I am a Haskell newbie so I'm not entirely comfortable with lift, and I don't know anything about FRP). The way I'm picturing it is the program you write is some big long pipeline transforming these inputs into some output. As the inputs change the outputs change but the pipeline stays the same. Am I close?
I'll check them both out, thanks!
Thanks for the interesting articles! I have a question about Cloud Haskell in general: is there any way to limit the number of processes that a node can run simultaneously? Doing so would be interesting from the perspective of resource management. E.g. we have an application that is relatively heavy and that uses forking to handle requests. We want to restrict things such that that application can only run N instances/forks simultaneously to preserve CPUs and memory. It is so not trivial to modify the application to impose such limits. So, what we do now is using N [Celery](http://celeryproject.org/) workers on each machine. In that manner Celery acts as a proxy between clients and the application, that distributes queues jobs such that at most N instances of the application are working on each node. I wondered if the same can be achieved easily in Cloud Haskell. Another thing that bothered me a bit is that processes are spawned on specific nodes. I think for many use cases, it would be nicer if you could just spawn a process and let Cloud Haskell figure out which nodes are available and distribute it to a node that is not busy. I know that this could be implemented on top of Cloud Haskell, so I guess that this is a feature request ;). Again, thanks a lot for the awesome work!
Since this is actually a new language and isn't embedded within Haskell, what happens if some of the computations you want to do "live" require the support of the full Haskell Prelude or other third-party Haskell libraries not available in Elm? Is there a way to separate execute Haskell scripts from Elm? I see there is something similar to this for Javascript, like in your Pong example. I hope this question makes sense. I haven't actually gotten my hands dirty with any of this stuff yet so I fully admit I might be misunderstanding something here.
Elm compiles to JavaScript so having a FFI for JS is fairly easy. If you want to interact with Haskell libraries you'd currently have to compile them to JS and use them with the existing FFI. This is the big trade off of going for a new language instead of an embedded one. In the case of FRP, I think it is the best choice ([Why Elm?](http://www.testblogpleaseignore.com/2012/06/21/why-elm/) and [The trouble with FRP and laziness](http://www.testblogpleaseignore.com/2012/06/22/the-trouble-with-frp-and-laziness/) and laziness+concurrency is not a good mix) So for FRP there are certain semantic changes that are really vital to creating a workable system, so in the short term it means Elm's libraries are not as well developed as Haskell's, but in the long run it will be a much better language for it. I am working to make Elm+JS interop easier, so hopefully the approach I outlined in the beginning of this post is a viable way to go.
I really like the interactive page source. Perhaps this is a bit much to ask, how do you go about setting up your interactive editor? Do you have the code for your editor somewhere? I feel like that might actually help people understand how to set up something fairly complicated using Elm. By the way, I'm super excited about elm and FRP. Fantastic job.
I used OCaml and Haskell in my last job and Haskell in my current job. My last job was a small startup (&lt; 20 people) where the engineering manager grew to trust my opinions over the first couple of years. We had a task where I considered C, C++ and Python a poor fit. I suggested OCaml and it worked out really well. From there I developed other code in OCaml and later added Haskell to the mix. In my current job (fortune 200 US company) I'm using Haskell to make a new internal web site (using Yesod). In the interview I mentioned my use of OCaml and Haskell and that was received with great interest. For the first 6 month in the new job I worked on an existing code base written in Python. I did well even though I find Python a pretty poor language for programming in the large. I helped with improving the development process, refactoring, code reviews, debugging performance issues etc. When a new project came up my manager gave it to me and was already expecting me to suggest doing it in Haskell. 
But this means you have to cache the values of `in2` and `f2`, right? This takes up more memory than it would do when it would fully recompute everything.
Try the DAAD's RISE programme, it's the only undergraduate research programme I know of in Europe, outside of just begging professors personally. That said, this is really specific work you're looking for.
Monomorphisms are things that algebraically mean the same, but at a lower level aren't. E.g. 1/2 == 0.5 == 0.5f == 5/10 but we can clearly see the difference. The monomorphism restriction prohibits ghc from autocasting obvious numeric literals, even when they can't cause any problems at all.
Based on a little experimentation it looks like GHC has a built-in assumption that `-x` has the same type as `x`. {-# LANGUAGE RebindableSyntax, NoImplicitPrelude #-} n negate x = - x {- *Main&gt; :t n n :: (t -&gt; t) -&gt; t -&gt; t -} This seems like a bug, at least in the sense that it does not agree with the documentation for RebindableSyntax as far as I can tell.
One can draw a distinction between "essentially partial" functions and "inessentially partial" functions, the latter being those `f` which can be written `f = fromJust . f'` for a total function `f'`. For example `head = fromJust . listToMaybe` is inessentially partial while `length` is not because there is no way to write a function `l` such that `l xs` is `Nothing` if and only if `xs` is an infinite list.
In the Haskell menu, choose Load tidy core. I use it infrequently enough that I haven't bothered adding a convenient keybinding for it, but frequently enough to remember the keystrokes for quickly choosing it from the menu. M-` h L
So the idea would be to experiment with a plugin and then port it into GHC once it's working acceptably?
It assumes that `negate :: t -&gt; t`.
I'd say it's a bug. The whole idea of `RebindableSyntax` is to enable the programmer to apply her semantics to the syntax, without restrictions. Requiring `negate` to unify with `forall t . t -&gt; t` is a restriction. What's also missing from the extension is a way to rebind arithmetic sequences (`[x..]` and friends). It should be trivial, seeing how they're already sugar for function applications (`enumFrom` and friends).
To be clear, you cannot call a function in the IO monad twice and get different answers, but when the runtime evaluates the related IO actions at the edge, those action will do/produce different things. In other words: what you get back from the function is always the same IO action, so it is always the same, but the effect of running that action may be different.
A good trick is to start off easy and slowly increase in difficulty. People will read as far as they can handle, and when they hit a part they don't understand they will be motivated to learn more so they can get further through the material.
True, it is definitely a trade off! In traditional formulations of FRP, everything was updated as quickly as possible. As soon as one output was computed, they started working on the next one. In this environment, latency is the big problem. You want things to update *fast*. Right now Elm does not offer a way to turn off caching, but it is totally possible. If you do not want to pay the memory cost, Elm's theoretical model is totally okay with that.
Without recursive signals, things look a little less like the math that they model. Specifically for physics stuff where you have acceleration, velocity, and position that are all mutually dependent. The AFRP people get these cool equations where you see the integral in math and then you see the code and they are pretty much the same. With Elm's signals, you can't do that. That said, all of those things are totally possible to write in Elm and I think they actually come out making a lot more sense. It is easier to read at least. [This post](http://elm-lang.org/blog/games-in-elm/part-0/Making-Pong.html) implicitly goes into how to do "recursive" things. The trick is to model your state very explicitly. Relatedly, Signals in Elm and Signal Functions in AFRP are really closely related (there's a bit about this in chapter 3 of my thesis). In fact it is entirely possible to embed AFRP in Elm, and the next release will begin this process with [Automaton library](https://groups.google.com/forum/?fromgroups=#!topic/elm-discuss/9R-sld6eQ78) (be sure to check out the example in there!) I actually already submitted my thesis and graduated! It was an undergraduate thesis and no one gave me any trouble about copyright stuff. In fact, one one of my advisers recommended that I release it under BSD which is exactly what I did.
&gt; Is there a connection between allowing loops in Signals and allowing laziness? That is the crux of the problem. The "loop" is how you can depend on the past, so it lets you build up large computations. Strict languages will do everything as they come. So when you use `foldp` in Elm (like `foldl` but you are folding from the past instead of the left) it takes every value that flows through a signal and does some computation *at the time the value is recieved*. Whereas a lazy language may save the whole computation for when it is observed. That means you could end up with minutes of unevaluated computations that you suddenly have to perform for the next frame. This will produce long delays if not stack-overflows.
Ah, thank you :) Up til now, I mostly did teaching in person, and I don't have a good gauge on how to present things when I cannot sense the readers confusion and correct as I go. I really like your strategy!
I am going to work on a more comprehensive intro: knowing nothing to having Elm set up and having some programs running. I am not sure if this should happen in one giant linear article though. Maybe have smaller pieces so you can just read as necessary? Do you have a preference?
&gt; I am going to work on a more comprehensive intro: knowing nothing to having Elm set up and having some programs running. That's great, I'm looking forward to it! As to "giant article vs many small pieces": Hard to say as it boils down to personal choice. Personally, I tend to forget smaller articles after a few days. Longer articles require a higher threshold of available free time, but once I start, there is a high chance I'll make it to the end -- and I will keep thinking about the content for a longer time. Largish but well partitioned articles hit the sweet spot, at least for me.
That's close to what I had in mind. It would be even nicer if I could use my browser's 'find' capabilities to search all parts at once; or to open the page on my phone while being at the station and not having to load another page with unreliable net access. As a side-note, I just realized just how immensely Wikipedia has influenced my learning habits. Anyway, thanks for the time and effort you put into this. It's rare for someone to write good software *and* good documentation. It is highly appreciated.
this actually isn't about Haskell at all, but the topic still is IMHO extremely relevant. (in case you wondered, I found about this on [HN](https://news.ycombinator.com/item?id=4649308))
Yeah die java
... Java 8 has lambdas and a functional collections library (filter, map, reduce...) with optimized multicore support. There are also a whole bunch of libraries for thread safety and the new features will make them even easier to use. How about "Yeah go with the time, Java."? Because that's what they're doing. 
Yeah, I hate it when slow software is slow. No really, are you admitting that you didn't read the article but just saw "Java" and decided to bash on Java like a complete tool?
Moores law aw waiting faster hardware know I think he are hitting the roof so software has to be efficient In order to achieve required speeds
Think it was a well-chosen post, it's very relevant and was rather interesting.
I actually read the Pong tutorial before I posted. Your pedagogical material is excellent. Is AFRP what you meant to reference in your first paragraph? Arrowized FRP is associated in my head with crazy schematics, not elegant equations, but maybe I've been looking in the wrong places. I don't really grok arrows (*especially* proc notation), admittedly I also haven't tried very hard. (Is there a distinguished name for non-arrowized FRP, like reactive and reactive-banana? Isn't that the kind more likely to have math-mimicking equations?)
Extremist is a bad word.
Thank you :) Teaching is a major passion of mine! Yeah, behind all of the notation and weird concepts AFRP is a really brilliant and simple model for FRP. I actually hated it for most of the time I was writing my thesis for the reasons you list (which is why Elm does not use that model), but by the end of the process I finally understood why AFRP is a great idea. I think they mainly have an image problem. They just present things in a really abstract way (which is why I call Elm's AFRP-inspired library Automaton instead of some random category-theoretic word). Signals-of-signals are a semantically messed up idea, especially when some signals are stateful. If you plug a stateful signal into a GUI 10 minutes in, should it have been doing computations the whole time? When you take it out should it stop? Should it keep going? These questions sort of break the abstraction of "signals". And this is why Elm does not let you have signals-of-signals. But then how do you have code that is dynamic *and* modular? AFRP is the answer. If you read the section of my thesis on "signal graphs" (3.1 I think) you'll see that you can think of Elm as a bunch of nodes that talk to each other in a controlled way. AFRP is the same thing at its core. They also solved the time and space leak problems of earlier FRP implementations (closely related to solving the semantic issue I mention in the previous paragraph). There are three major eras of FRP: * Traditional FRP * Signal based FRP * Arrowized FRP (or signal function based FRP) I would call reactive-banana "Traditional FRP" because it has a notion of Events and Behaviors, just like in the [original paper](http://conal.net/papers/icfp97/) on the topic. He also limits signals-of-signals to keep things efficient, like I do. I have a lot of respect for his project because I think we are working towards a very similar goal, and I think he is doing it in a cool way. I do not know anything about reactive though. After Traditional FRP (with events and behaviors) people realized that `Event a` is equivalent to `Behavior (Maybe a)` so the two concepts were combined and called Signals. This spawned [Real-Time FRP](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.1676) and [Event-Driven FRP](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.7720), both being really influential for Elm. Elm follows in the tradition of Signal based FRP, and it's probably best classified as "event-driven FRP" or "discrete FRP". 
Ah, "Traditional" is the word I was looking for. Thanks for the explanation and references!
I loved this answer Conor! Although I admit to being continually frustrated that whenever I realize something cool about the algebra of types, I find that either you, Brent Yorgey or Dan Piponi have already noticed and written about it 3-5 years ago.
It's dangerous to go alone! Here, take a downvote!
There is no problem! The "foo = -1" means "foo = fromInteger (-1)" and the fromInteger in scope is Prelude.fromInteger :: Num a =&gt; Integer -&gt; a (fromInteger _ :: String) this requires "Num String" which is the error. 
To be clear, this compiles: {-# LANGUAGE RebindableSyntax #-} import Prelude hiding (fromInteger,negate) fromInteger :: Integer -&gt; String fromInteger _ = "bar" negate :: a -&gt; String negate _ = "foo" foo :: String foo = -1 
I'll have a shot. One way you could limit processes is to have a shared variable where you store the number of processes. Instead of using the native spawn process function you'd use your own which checked the variable before forking. Alternatively you could use the work stealing method. Spawn N processes, where N is the max you're willing to allow, and have them block until work is available. Since ghc implements non-blocking IO this will be cheap.
All my thanks
So, what's the story morale? I kinda lost it after a benchmark slide. "LLVM is complicated and slow, stick with shallow EDSLs."? Am i missed something?
Does this mean that at least part of machinery is in place for GHC JIT? 
The morale is that there is no single best answer. 
That static contracts stuff was new to me (around 40minutes in). Here's some relevant papers: http://research.microsoft.com/en-us/um/people/simonpj/papers/verify/ I wonder which Christmas we'll be getting these presents?
Thanks for posting this, I was very curious to watch. 
If you define a JIT as creating machine code in memory then no. This is compiling the code to disk.
If you want to do a service, just post the link to the main page with all the videos on it. Don't spam the subreddit by posting every link on the page.
karma ****ing is the most childish concept to ever come out of reddit.
FRP can be divided into two categories: discrete-time and continuous-time. I'll try to explain discrete-time FRP, which is easier to implement and so more FRP implementations tend to use that model. The typical way to define reactive systems (in an imperative framework) is "push-based". That is, sample some input, and when an event occurs, explicitly call whoever needs to be updated as a result of that event. Also, it is typical for these handlers to communicate via mutable variables. This means that if you want to know the meaning of a particular variable, you cannot look at the definition to see. Instead, you have to read all the code that updates it. FRP reverses the "push-based" idea to "pull-based". Instead of having a mutable variable and updating it upon events, you define the variable as a function/composition of things that represent the event streams. The major benefit this gives, for which FRP was originally designed, is giving [Denotational Semantics](http://en.wikipedia.org/wiki/Denotational_semantics) for reactive systems. This means that to understand the meaning of a reactive variable or system -- you just need to understand the meaning of the components of their definition and nothing else! So FRP is a way to define reactive systems that have a denotational semantics, and tends to inverse control from push-based to pull-based.
I notice that for a lot of people, when you search for “Cloud Haskell”, you get Jeff's prototype implementation, rather than the newer distributed-process which Duncan is talking about. See the [Cloud Haskell wiki page](http://www.haskell.org/haskellwiki/Cloud_Haskell) for more details.
And yet it exists!
No, no it doesn't. No one cares about imaginary internet points.
&gt; Just trying to be servicey, so you don't have to dig through/click through to find a talk that's up your alley. Take it or leave it, I s'pose. Given the ratio of up to down votes, the community consensus appears to be "take it". I'd say that settles the matter well enough.
I'd go with "fanatic", personally. Mixed connotations, enough to be self-deprecating but not excessive, and no implied negatives about people lacking sufficient fanaticism for whatever abstract arcanity one might be peddling.
I imagine my tone didn't help much. :)
Ah, perhaps then *fanatic* would be the most apropos term without too much extra connotation? Natural language is muddled with so many implicit decorations.
Interesting comments on performance and laziness at the end. The question is always 'is it worth it?' All these thunks. For me, laziness has these advantages: * Evaluation abstraction: Order often does not matter. As Lennart said, I don't want to have to re-order my code to satisfy a strict evaluation model of pure code. * Suspended computations: No having to write ugly anonymous functions that take unit just to suspend a computation. When I write "print 1" I'm just talking *about* the action, not demanding it. In this sense all Haskell code is pure, it's a language for describing things. * Control structures compose: Looping, reducing, conditions, etc. embodied in map, foldr/l, zip, take, infinite lists, and so on form a mini-language of control structures that is far superior to custom language constructs, or in the case of Lisp, a monolithic LOOP macro that you can't extend. Laziness enables these to work properly. * Laziness is better than macros: with the exception of generating boilerplate, laziness is almost always better than macros a la Lisp. There's no separate namespace or level of evaluation, the names are first-class things that you can pass to `map` or whatever, you don't get abstraction breakage when you want to change how you pass something to the 'macro'. You can refactor and move code around and it won't break because of some hidden macrology. This is why Haskell is good at "lots of little functions" like the UNIX philosophy, whereas Lisp will have one monolithic function that takes a bunch of options. (Yeah, I currently have to maintain a Lisp codebase presently and it is painful.) I think the purity and static typing of Haskell are the main selling points, but laziness doesn't get the same kind of love as I think it should. [Lennart](http://augustss.blogspot.it/2011/05/more-points-for-lazy-evaluation-in.html) wrote a great article about it that rings true for me in my experience. You have to look at laziness not just as a novelty that lets you write `[1..]` but compare it to what other languages are doing to solve the same problems, you'll probably find a disparate set of not-very-composable special cases. Don't have time to express myself fully but I agree more teaching about laziness esp. in regard to performance and strictness would be good.
My impression was rather of something even more restrictive than Haskell, with an attempt to present it as just another imperative language in hopes of winning over mainstream programmers. Like I said, marketing, and I doubt it will work. I do think ideas like this have a lot of potential, though, if they can get past the mainstream adoption hurdle.
&gt; Thanks for the interesting articles! I have a question about Cloud Haskell in general: is there any way to limit the number of processes that a node can run simultaneously? You have explicit control over spawning processes. Spawning processes is not automatic, it's totally manual, so you can take whatever strategy you like. &gt; E.g. we have an application that is relatively heavy and that uses forking to handle requests. Do you mean forking OS processes, or lightweight threads? Cloud Haskell processes are GHC lightweight threads so you can have 10's of 1000s of them without problem. &gt; Another thing that bothered me a bit is that processes are spawned on specific nodes. I think for many use cases, it would be nicer if you could just spawn a process and let Cloud Haskell figure out which nodes are available and distribute it to a node that is not busy. I know that this could be implemented on top of Cloud Haskell, so I guess that this is a feature request ;). Right, it's manual, but of course you can build some reusable process management code to decide what node to spawn the next process on.
Thanks for writing this. We need people to lay things out like this every few months to really call out how different programming in Haskell is.
This comment thread is really stupid. Knock it off, both of you.
but...but... you're mod! you're supposed to use your powers to push arbitrary measures against the consensus!
Ok, 100% official totally legit mod announcement here, from now on all comments posted on /r/haskell between the hours of 6 AM and 3:50 PM UTC on the third Wednesday of each month must be written in strict poetic meter. Rhyming is optional, but breaking meter for the sake of a particularly nice rhyming scheme will be allowed on a case-by-case basis. How's that?
This is not the only example of such assumptions. Last time I checked, the documentation explicitly noted that desugaring `proc` notation requires type signatures very similar to the standard versions. Nobody seems to mind that (perhaps because nobody actually uses `proc` notation?). So I'm not sure it's technically a *bug*, as I don't think the docs specify that every desugaring which typechecks is valid. It does seem counter to expectation, though.
I once proposed something similar on IRC, auto-kicking anyone who gave an ill-typed expression to lambdabot. If memory serves me, the primary objection to this was the idea of giving lambdabot ops...
Does GHC expose libraries for "JITting"? If so, wouldn't they eliminate haskell-c interop bottleneck? Or, GHC only knows how to statically generate code? 
But of course. &gt; The most important thing in the programming language is the name. A language will not succeed without a good name. I have recently invented a very good name and now I am looking for a suitable language. (The above quote is often attributed to Knuth, but I don't know of a reliable source.)
We discovered that we could get some easy optimizations by doing these kind of replacements by hand, but that might sometimes involve modifying library code; it was an obvious candidate for implementing in a compiler pass. Who would benefit from it? Anyone doing ordinary math with Doubles, Floats and Ints. It probably won't help you much for things you should be doing on a GPU, and it still has some limitations like not folding into lazy structures. Some other people here at Tsuru are keen to make plugins for optimizing parsing and serialization. We use increasing amounts of TH, and it's interesting that we could mix expressive compile-time DSLs with transparent optimizations. It'll be interesting to get this kind of thing into GHC. One advantage of doing it there rather than in a plugin would be more access to the full type dictionaries, so that (perhaps) these optimizations could be done on eg. any Num type rather than just a known set of primitives. In any case, doing it in a plugin has allowed us to go from zero to ready-for-production within a week, and it works with our existing ghc-7.4 and ghc-7.6 installations.
Maniac? Madpeople? Aficionado?
This is not spamming.
I'm aware of the irony, but even a sense of "narrow fixation" makes it the wrong word.
I'm only noticing this because I've been playing with Gabriel's Pipes library recently but there's an un-ignorable parallel between send/expect and yield/await. The only difference in the kinds seems to be that send needs a processId and yield doesn't. Doesn't this mean that distributed-haskell should/could be depending on conduits/pipes?
No. The communication pattern that you can create using pipes is much more limited (linear) and moreover local (same machine only). Of course, you could do the opposite thing, and implement pipes on top of Cloud Haskell (as in, implement a pipe that expects to await and sends to yield). That might certainly be beneficial.
&gt; At the Network.Transport layer we support, at least in principle, Unreliable, ReliableUnordered, and ReliableOrdered connections. (In practice, the only serious transport implementation that currently exists, the TCP transport, only supports ReliableOrdered connections). Does this carry over to the upper layers? So if I send a message over a backend that uses a transport that is Unreliable, I will receive messages in any order? Can I have (or does the design support) two channels - one Unreliable and one ReliableOrdered at the same time between two nodes (say one for heartbeat / consensus and one for RPC)? &gt; - The guarantees that are given re retransmits come from the underlying Network layer; this should not really be visible at the Cloud Haskell layer. This is generally true actually: Cloud Haskell programs are not really aware of the underlying network transport at all. &gt; - You never need to "check for connectivity every second" -- it suffices to monitor the other process (or link to it). You will then receive a monitor message (or die) when connectivity is broken. I am worried about the other state change - from non-connectivity to connectivity. My understanding of 'reconnect' is that it is a "one-shot" request that tries to reconnect on the next message send, but that it is not a continuous "polling" that will reconnect at the earliest possible time. If we try to implement "connectivity detection" on top of the 'reconnect' primitive, we cannot detect connectivity quicker than what the retransmit and timeout-behavior of the underlying transport permits, and this is essentially random with some upper bound which is the timeouts in the TCP implementation. So instead of actually using the 'reconnect' primitive the way it was meant to be used, I would probably create a new Connection every X seconds, thereby *guaranteeing* that a packet was going to be sent at regular intervals, and let the old Connection objects be garbage-collected. Btw, I am probably wrong in assuming that this works out-of-the-box in Erlang. Another issue that just occurred to me is that from the documentation it seems like 'reconnect' also could be called 'previousMessagesUnreliable' - i.e. it specifies retroactively that previously sent messages were unreliable. Then another approach would be to have send, and sendUnreliable, or simply to have the TCP backend support UnreliableOrdered as well as ReliableOrdered. UnreliableOrdered could be implemented on top of reconnect + ReliableOrdered, but whether to use 'send/sendUnreliable' or 'reconnect' more depends on which part of the application is better suited to make that decision. 
I apparently guessed wrong that it was present in Data.List.
No the bottleneck is not about code generation, it's about thread safety when moving between the C world and the world inside the RTS. It involves taking locks etc, to make sure that only one OS thread is evaluating Haskell code per capability. We can so fast "unsafe" calls to C code, but in Lennart's example he needs to allow the C code to call back into Haskell land, and that's only allowed for the "safe" calls, which are the ones that have to take locks etc. Normally this is not a problem, it's just that in Lennart's example he's doing lots and lots of small calls that have to cross the C/Hs boundary. You'd face a similar (probably worse) issue with calls between C and JVM.
One might consider using Ceph for this.
Well... calculus is really about the change of one parameter vs. another. It is frequently convenient and useful for "dt" to be on the bottom, but it is _only_ frequently convenient; it is not the definition of calculus or anything. Making time first class and embedding calculus seem to me to be two independent considerations, and conflating them to be only confusing. That said, if you're interested in the idea, the term you are looking for is "automatic differentiation", such as [this article](http://conal.net/blog/posts/beautiful-differentiation). You may also be interested in the [Haskell data structure case](http://en.wikibooks.org/wiki/Haskell/Zippers#Mechanical_Differentiation).
I'm aware of automatic differentiation, hence "Remember we can find derivatives of data structures!".
That's not the same thing as automatic differentiation.
This idea has been beaten around for quite a while. The problem is that nobody really *knows* how to make this generalization in any practical sort of way.
Value- and type-level differentiation are analogous, look at http://conal.net/blog/posts/differentiation-of-higher-order-types. This analogy is a nice use case for level polymorphism or datatype promotion. In a sufficiently powerful language a definition such as `id x = x` could work both as a value-level identity function and a type-level one. Likewise, you could have single `diff` function that differentiates both value- and type-level functions. I [attempted](http://hpaste.org/40138/level_polymorphism_in_omega) to express this in Omega, and could unify declaration of objects, but not the differentiation itself.
I guess technically you’re just offering us the ability to give you money; but in practice that comes across as asking for money. And while it’s not a shit idea by any means, it’s not an original one — it’s (at least in the theoretical aspects) a question that other people have considered before, and not made much progress on; and at least in what you say here, you’re not showing us that you have any new ideas on how to make progress on it. So what you say comes across as asking for money, without clearly offering anything in return apart from vague optimism and enthusiasm.
I don't know what that means yet, but thank you.
Does that mean we need dependent Haskell to express this? I am totally willing to go there!
Time-bending programs: e.g. turn an arbitrary piece of audiovisual material into a song by messing with timing and stress and pitch.
Again, I have no idea what that means, but thank you.
Ok. Allow me to shamelessly self-plug: [An ODE to Arrows (PADL'10)](http://www.thev.net/PaulLiu/download/padl012-liu.pdf) *We study a number of embedded DSLs for autonomous ordinary differential equations (autonomous ODEs) in Haskell. A naive implementation based on the lazy tower of derivatives is straightforward but has serious time and space leaks due to the loss of sharing when handling cyclic and infinite data structures. In seeking a solution to fix this problem, we explore a number of DSLs ranging from shallow to deep embeddings, and middle-grounds in between. We advocate a solution based on arrows, an abstract notion of computation that offers both a succinct representation and an effective implementation. Arrows are ubiquitous in their combinator style that happens to capture both sharing and recursion elegantly. We further relate our arrow-based DSL to a more constrained form of arrows called causal commutative arrows, the normalization of which leads to a staged compilation technique improving ODE performance by orders of magnitude.*
I'm discretizing time as a Word64. :) see rdtsc-enolan on hackage.
Well, they are related in the same way that Z_p and the real numbers are related. They are both instances of the same algebraic structure -- in that case, a field -- but one is discrete, and the other continuous. They have profoundly different characteristics (er, pun intended I guess), and having both implemented somewhere in a programming language doesn't really let you use one to do anything with the other. Here they are literally a universe (level) apart. ;)
Why is Edward getting all the imaginary internet points in this thread? Where are my points?!?!!?
You will get upvotes when you contribute something.
&gt; &gt; At the Network.Transport layer we support, at least in principle, Unreliable, ReliableUnordered, and ReliableOrdered connections. (In practice, the only serious transport implementation that currently exists, the TCP transport, only supports ReliableOrdered connections). &gt; &gt; Does this carry over to the upper layers? So if I send a message over a backend that uses a transport that is Unreliable, I will receive messages in any order? Just to clarify: the Transport interface lets us pick the relability guarantee on a per-lightweight connection basis. See http://hackage.haskell.org/packages/archive/network-transport/0.3.0/doc/html/Network-Transport.html#v:connect So it's not that one transport impl is reliable and another could be unreliable. All of them support the user choosing the reliability. Now what Edsko says is also true: our TCP transport implementation is overly generous in the sense that if you ask it for `Unreliable` or `ReliableUnordered` it always gives you `ReliableOrdered`. OBviously that satisfies the semantics and if you are using a TCP connection, you can't actually do worse. To be faster and take advantage of the weaker guarantees we'd need to use UDP or something here, which would be great but we've not done that extra work. On similar lines, the transport supports unreliable multicast, but not all transports are required to support it (and our TCP one does not). Now at the cloud haskell / process layer, we only use `ReliableOrdered` connections. As Edsko said, we could possibly provide typed channels with weaker reliability, and similarly we could provide unreliable multicast "channels". We would not want to make the general send/expect stuff be weaker, but it's not completely implausible to have a special send variant that has weaker reliability.
Right, because creating this discussion has zero value.
I tend to `ghc-pkg list | grep haskell-platform`
Couldn't you just do something like (pseudo-code since I don't have the time to look up the necessary function names right now): pow n b | isPowerOf 2 n = let x = pow (n/2) b in x * x pow n b = b ^ n or would the guard have too much overhead? 
&gt; I thought some of you would be interested in this :). Yes, thank you for posting this.
Unfortunately, the trend is the other way round and most universities use "real" programming languages in their courses ... \*sigh\* Glad to see there are exceptions.
Loading this blog post (from a blog running Warp, the headers say) is really slow for me. A simple time curl -s -q http://www.yesodweb.com/blog/2012/10/measuring-warp &gt; /dev/null takes an average of about 8 seconds here!
I agree that this is a good trend but I would feel more comfortable if they also would teach lamda expressions in C++, F# and C#. I find that I use haskell to re-write alot of production c# code. Corporations are not ready to bet the farm on yesod yet.
 data Burrito a = Tortilla a instance Functor Burrito where fmap f (Tortilla a) = Tortilla (f a) instance Monad Burrito where return = Tortilla a &gt;&gt;= f = (\(Tortilla b) -&gt; b) $ fmap f a 
These monads are making me hungry...
University is supposed to provide an education, not provide job training for specific corporate interests.
Sure, real-time CG is pretty much about faking it. After all, it only has to look good, not physically correct. In any case, there’s nothing preventing LambdaCube from being used for offline rendering either.
&gt; Essential CS concepts such as data structures and algorithms are introduced It was my impression that a lot of the traditional data structures are quite hard to work with in haskell since they rely on mutable state. How would that be handled?
Welcome to the Haskellverse! If you or your professor want any Haskell-related help, you'll find many in this community that are eager to lend a hand. StackOverflow, /r/haskell, and the #haskell irc channel are my favorite sources for enhancing my Haskell experience. Best of luck; as a mathy person I'm sure you'll find a lot to love about Haskell.
glad you found this interesting :)
Thank you :)
And given 90% of languages are not pure, the Haskell structures your learn may not be applicable.
And 1.033 from here with just curl. [Pingdom reports](http://tools.pingdom.com/fpt/#!/N2H7gdugv/http://www.yesodweb.com/blog/2012/10/measuring-warp) the site as being in the 90% percentile, and looking over the reason for that drop, it's mostly disqus. That's not a valid way of measuring site speed.
Isn't Disqus usually loaded async, with Javascript? That way it wouldn't impact the curl time. The reason I measured it was not to bash anyone, on the contrary. I noticed the post was very slow, and thought people might want to know (and possibly investigate). So you might think it's not a valid benchmark, but it's a real issue I ran into. 
Is pingdom broken? Surely 90% of the internet is not even slower than a full second.
Or just `ghc-pkg list haskell-platform`.
&gt; which is significantly more difficult and mind-bending in Haskell due to monadic IO and dealing with the logistics of keeping most of the program pure. Monadic IO isn't any more complicated than normal IO. Keeping most of the program pure is entirely optional. Good design is harder than hacks, but you can code hacks in Haskell and good design in Java, the difference is the Haskell community is full of people who love good design and the Java community is full of hacks ;)
Haha, that's actually a really good point. It's a little harder to throw stuff together in Haskell that you feel good about, I guess (at least that's how I feel -- separating IO from the pure bits is important to me), but the hard part is that separation.
Looks like a fun user group. I've explained monads to a few JavaScripters in the past in a similar manner. The problem I describe is lookup in an object structure. var struct = { foo:{ bar:{ baz: "hello world" } } } I want to get the 3rd char of baz. The shortest code I could write is ... console.log(struct.foo.bar.baz[2]) But if any of the structure is missing then I'll get an infamous "Can't access prop of undefined". So to be safe, I have to say ... console.log(struct &amp;&amp; stuct.foo &amp;&amp; stuct.foo.bar &amp;&amp; stuct.foo.bar.baz &amp;&amp; stuct.foo.bar.baz[2]) I've also written a similar "macro" to solve the problem but it's still ugly IMO var Z = function() { var len = arguments.length, i; var obj = arguments[0]; for (i=1; i&lt;len &amp;&amp; obj; i++) obj = obj[arguments[i]]; return obj; }; var foo = {bar:{baz:{a:"hello world"}}}; var success = Z(foo,"bar","baz","a",2); var fail = Z(foo,"barFFF","baz","a",2); console.log("pass", success &amp;&amp; !fail) I think the big difference between my JS demo and haskell is that haskell is lazy and you're calling functions which are giving a value to pass to the next function. If at any point of that function chaining, you're typical case fails, the monad pattern will give you a graceful way to fail.
Back in 2001, when I started at university in Groningen (Netherlands), my first course was also in Haskell. I believe the main motivation was to level the playing field, as some freshmen had already plenty of programming experience while others had next to none. It worked quite well.
I commented on this on yesodweb.com a while back. My analysis at the time was that this was caused by loading fonts from the Google content network.
Also would not affect curl :)
Ooo, that's much better! Thanks, mifrai. By the way, for those that don't have `grep` in Windows, I've uploaded a simple [grep MSI installer](http://www.yellosoft.us/helpers#grep).
Good to know that I'm not the only one in this course who uses reddit ;-) See you on Friday!
Am I the only one who stands mitigated? Do we want Haskell to be seen as a "good for teaching purposes" language? Do we want it to acquire the same reputation that, sadly, have now Caml and Scheme? Java and C++ are not concerned by that, because they're presented to students like "what we teach you so that you can get a job in the real world", but Haskell haven't acquired this status right now, it might be dangerous to present it too widely as a language that's "good for teaching", because people are prone to classifying quickly and sticking labels. (Which brings us niceties like "Lisp is just for AI stuff", uttered by people who don't know jack about it) **tl;dr** To me, Haskell is better sold as "that one language that allows you to make the same that in Java, only with 5 times less code and 5 times more trust in your code" than "that one language that's good for teaching purposes, like Pascal or good ol' Caml".
Alternatively, given the increasing move to multi-core computing, tomorrow's graduates may appreciate being equipped with pure data structures which behave well in the face of concurrency.
It is a good idea to separate pure code from `IO`, because: * It is easier to understand pure code because of referential transparency * The compiler optimizes pure code much better because it has more liberty to transform the code * Your code is more likely to be correct if you can't "cheat" and use `IO` to solve your problems. Generally, if you are in a hurry and it's a small program then doing it the quick and dirty way is fine. However, if you care more about quality, lack of bugs, and maintainability, then it helps to keep your program pure. However, even purity is not completely the gold standard for program quality. I actually believe compositionality is what matters more, where purity is a close approximation for compositionality. This is why I promote the use of iteratees to keep the ability to interleave effects while still retaining the ability to reason about code easily.
That error is even more trivially disproved by the identity monad :-)
Honestly after working in the 'industry' for ~15 years, I'm glad my run-of-the-mill university taught with C and Java. Haskell and functional languages have very rarely had a chance to come to my rescue when doing day to day coding. Sorry.
Or just `ghc-pkg field haskell-platform version`. 
Not a Haskell guru, but I have two comments: The language is a little unclear - "simply turn the recursive solution into tail recursion" makes sense, I think the reference to TCO is a red herring (can you rely on your implementation language of choice performing TCO? It's not really relevant to the exercise). Also, I don't think I'd do this problem in Haskell. Making the implementation obviously tail recursive (the point of the exercise) is going to move away from idiomatic code and probably eliminating laziness, making it all rather pointless. I imagine building it in monad notation using `ListM` could satisfy the requirement, but it's a bit silly imo. Do it in Scheme - or if you want to be really hip, [Tcl](http://wiki.tcl.tk/14011). *EDIT: Also, talk to your professor. Seeking ideas on here is cool, but he wrote the question and probably has a better idea what he wants you to get out of it. And you didn't speak to him after a similar situation on the exam??*
Its been 10 days before this late reply. From what I see you have learned how to overload operators by declare instances. Good job. Also once you are not happy with the standard prelude, you can try numeric-prelude. 
University of Pennsylvania switched from Java to OCaml a few years ago
Tail recursive means that the 'head function' in the WHNF of the 'fixed point' version of the expression in the recursive call. In your case, something like: sort list = fix (\rec params -&gt; if cond_exp then base_case else rec param_next_exps) param_exps where `rec` isn't free in `cond_exp`, `base_case`, or any of the `param_next_exps`. That is, you need to find a way that does it in some sort of accumulator-passing-style. Now, that said, is that useful in Haskell? Tail recursion often has surprisingly bad results in Haskell, because laziness means that the recursive `param_next_exps` may not get evaluated until the termination of the recursion. Here's a simpler example: -- not tail recursive sum1 [] = 0 sum1 (x:xs) = x + sum1 xs Here the head function in the WHNF of sum1 is (+), not sum1. -- tail recursive sum2 xs = go xs 0 where -- non recursive -- tail recursive helper go [] acc = acc go (x:xs) acc = go xs (x + acc) But in Haskell, due to laziness, sum2 evaluates like this: sum2 [1,2,3] = go [1,2,3] 0 = go [2,3] (1+0) -- tail recursive call = go [3] (2+(1+0)) -- tail recursive call = go [] (3+(2+(1+0))) -- tail recursive call = 3+(2+(1+0)) -- evaluation of result = 3+(2+1) = 3+3 = 6 Note the giant thunk (3+(2+(1+0))) that isn't getting evaluated until late, building up a big waste of memory. Here's a better form: sum3 xs = go xs 0 where go [] acc = acc go (x:xs) acc = go xs $! (x+acc) -- strict-application helper function f $! x = seq x (f x) Now this evaluates: sum3 [1,2,3] = go [1,2,3] 0 = go [2,3] $! (1+0) = let x = (1+0) in seq x (go [2,3] x) = let x = 1 in seq x (go [2,3] x) = go [2,3] 1 -- tail recursive call ... = let x = 6 in seq x (go [] x) = go [] 6 -- tail recursive call = 6 -- no additional evaluation required
Are you sure that sum2 will produce a giant thunk in GHC? The [source](http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/Data-List.html#sum) for Prelude.sum is almost exactly sum2, and I find it hard to believe that such an important function would be given an inefficient definition. Might GHC's strictness analysis optimize the generated code?
Guess I'll have to read up on continuations. I'm not sure what that is. Thanks for the reply though. I have a lot to learn about Haskell's idioms.
Heh, I had a blog post sitting in my queue with a short pitch about this project (which comes from the lab at Stanford I'm currently working with). If you don't like video, here is a somewhat liberal adaptation of the talk into textual form http://blog.ezyang.com/2012/10/hails-protecting-data-privacy-in-untrusted-web-applications/
If you're still checking this thread, I had some time to play around with Elm today and discovered that if I try to build a web app that uses Elm, nothing gets rendered. I initially tried doing a custom web server with Snap, but I figured I must have been doing something wrong when I kept getting a blank white page, so I downloaded the source to the elm-lang.org site and built it. But lo and behold it also doesn't render anything, just a blank white page. It will show an error message if I try to load an Elm file that it can't find, but that's it. If it actually can find the Elm file, nothing happens. The actual elm-lang.org site works fine for me, of course. Any ideas what could be causing this? Edit: Running elm-server does work, though.
Doesn't look like Warp can scale beyond 2 or 3 threads. Is that a limitation of Haskell? This is somewhat of a surprise to me as Haskell is famous for its concurrency ability. Has anyone got good concurrency performance from Haskell in production?
Is LYAH the official textbook for this course?
&gt; In any case, there’s nothing preventing LambdaCube from being used for offline rendering either. Do you mean that it's possible to make different backend to LC dsl? Like generating renderman shaders instead of glsl and RIB instead of opengl?
With optimizations on, GHC's strictness analysis will indeed find that sum2 is strict in its second argument, and therefore it's safe to eagerly evaluate. According to the Haskell standard, all nonterminating/crashing values are equivalent, so an implementation is allowed to choose eager or lazy or even call-by-name evaluation for sum2. Effectively, GHC knows how to automatically transform sum2 to sum3. However, I find it very useful to use the "graph reduction" model of lazy evaluation; you can always perform the optimization by hand (as sum3 does) to force it to happen, and GHC, in my experience, will never generate code that is *worse* than this model, and often it will be equivalent up to constant factors. Strictness analysis on sum2 is one of the cases where the optimizer beats constant factors since it could potentially turn a crashing program (out of memory) into a successfully terminating one. Note that this changes the behavior of the following program: ignore x = do evaluate x return () main = do catch (ignore $ sum2 (1 : 2 : error "xxx" : 3 : error "yyy")) $ \(ErrorCall e) -&gt; putStrLn e With optimizations off, this prints "yyy"; with them on, it prints "xxx". Also, looking at the source for sum you linked to in GHC, it's actually *not* always strict in the second argument, so even with strictness analysis, GHC won't do this transformation in general. Consider: data PeanoNat = Z | S PeanoNat deriving (Show, Eq) instance Num PeanoNat where -- note: not strict in first argument! a + Z = a a + S x = S (a + x) fromInteger _ = error "unimplemented" instance Ord PeanoNat where -- lazy comparison escapes evaluation -- as soon as we know the result compare Z Z = EQ compare Z (S _) = LT compare (S _) Z = GT compare (S x) (S y) = compare x y main = print (compare Z (sum [S Z])) -- GHC source for 'sum' sum l = sum' l 0 where sum' [] a = a sum' (x:xs) a = sum' xs (a+x) Reduction: compare Z (sum [S Z]) -- compare pattern match, reduce 2nd argument to WHNF -- sum compare Z (sum' [S Z] (fromInteger 0)) -- sum' case 2 compare Z (sum' [] (fromInteger 0 + S Z)) -- sum' case 1 compare Z (fromInteger 0 + S Z) -- (+) case 2 compare Z (S (fromInteger 0 + Z)) -- compare case 2 LT The strict version would hit the bottom in `fromInteger`. However, GHC specializes `sum` to `Int` and `Integer`, which happily are known to be strict in both arguments; in those specialized versions strictness analysis kicks in and fixes the laziness problem.
Using a "suspend" implementation (based on closures and memoization), why wouldn't you be able to implement laziness?
If we wanted to prepare people for day-to-day coding, we wouldn't be sending them to University at all :)
I think the practical difference is that contracts are simpler to use. And I think the difference in principle is that dependant types forbid programs that will potentially fail and contracts use theorem proving to prove that the program won't fail. The money lying on the ground, or free lunch, here is that if we can transform the contracts into theorem prover readable logic we get to import many years of research into our haskell programs.
As far as the TCO is concerned, we talked about how some languages support it and some do not. And he wanted us to specifically write this algorithm in a way in such that it was tail-call optimized. Given the time constraints of the exam (3 hrs.) and the fact that my programming partner and I have concentrated the most on Haskell and Erlang, it made sense for us to attempt this problem in Haskell. I asked him if I could post it here to get y'all's thoughts. He said go for it, as he's also interested to see the responses. In addition, my professor has a teaching style where he doesn't really tell us answers... he encourages us to go explore and find out stuff on our own. So I spent a long time researching that Hanoi problem, and actually found a solution that worked. However, the method I used to solve that one could not be applied to this problem, which is why I posted it here. Thanks for the response.
In case anyone is interested, there's an implementation of MergeSort in StandardML [here](https://github.com/mortenbp/mylib/blob/master/Extensions/List.sml) (mylib), which I believe to be tail-recursive (I implemented it). Comments are welcome ;) Edit: [here's a quick rewrite to Haskell](https://gist.github.com/3905696).
(You mean cores, not threads.) No it's not a limitation of Haskell, nor of GHC concurrency in general. It's quite specific to the situation where you have lots of Haskell threads spread across multiple cores and lots of them waiting on I/O operations. So of course that's just what you get in a web server. The problem is that the I/O manager in GHC is single threaded[*]. The author of the article, Kazu, has been looking at exactly this issue for the last year or so. As he says at the end of the article, he and Andreas have been working on the I/O manager to parallelise it more. I've seen their early results and they look very promising indeed. So watch out for their next article. [*] actually it's not quite that simple, Bryan and Johan tried to make it parallel previously but it didn't run any faster. The difficulty is that we have not had the profiling tools to tell exactly where the bottlneck is and so it's a bit of guesswork. Despite that, Kazu and Andreas have made good progress, and on the profiling front we're now also getting better tools so we can see what's going on with I/O and system calls.
Using sed seems like a cop out in a tutorial like this, especially since it means you have to process the input at least twice (5 times in your case). How would you go about using iteratee to process the input just once? 
As a matter of fact it was an excuse to get acquainted with sed, too :)
Lazy evaluation has a lot of advantages indeed. It makes many things much cleaner. Though they are possible without built-in laziness, of course.
Yep. That works for a parsec-style parser. It is a bit more problematic when you want a more direct style or observable sharing though. 
Fair enough :) But I still want to see how you'd solve this problem with iteratees alone!
Any hackage links? edit: what I mean is, even in Haskell the most widely used libraries (parsec, attoparsec, uu-parsinglib) do not use these additional possibilities. So it turns out to be not such a serious limitation of eager/strict languages.
&gt; simply turn the recursive solution into a tail recursion I don't see how this is possible in any language, given **there are *two* recursive calls**. &amp;#3232;\_&amp;#3232;. TCO only works with *one* recursive call at the *tail* position of the function evaluation. ~~Any divide-and-conquer algorithm that divides the problem into more than one subproblem cannot be written in tail call fashion. (Well it can be, but it would be meaningless, because one of the *other* recursive calls would not be in tail position, so you lose the benefit of TCO. Perhaps your professor expects you to rewrite the algorithm in this meaningless fashion.)~~ [edit: wrong. By implementing your own stack you can do anything "tail-recursively".] Also, due to laziness, tail recursion is often irrelevant in Haskell. ---- Often, but not always. Consider the recursive factorial function: factorial :: Integer -&gt; Integer factorial 0 = 1 factorial n = n * factorial (n - 1) This builds up a large chain of thunks, n * (n - 1) * (n - 2) ... without evaluating the multiplications along the way (assuming GHC doesn't do any deep magic to optimize it). We can manually optimize this by adding strictness annotations and converting to a tall call version of the same algorithm: factorial' :: Integer -&gt; Integer -&gt; Integer factorial' !acc 0 = acc factorial' !acc n = factorial (acc * n) (n - 1) The built-in numbers are easy to force, because they are opaque values that are unevaluated or evaluated. Lists, on the other hand, are not so easy; strictness annotations will only guarantee that the list is forced to reveal only its beginning structure: whether it is an empty list, or a list with at least one element. If you want more control over when evaluation happens on a sequence of elements, consider using some sort of array (e.g. the `vector` package). This would be a non-trivial learning pursuit, so if you are short on time or interest, look into other options.
Chris Okasaki has a paper implementing parser combinators in SML using CPS. I can't remember 'many' being a problem conceptually, maybe it has performance problems. Paulson's book "ML for the Working Programmer" has a direct (not-CPS) implementation of parser combinators in SML, if I remember correctly the implementation is a bit ugly as you have to eta-expand (?) the primitive combinators (which I think 'many' was one of).
Yes, one of the things we have on the [roadmap](http://lambdacube3d.wordpress.com/2012/09/16/a-few-words-about-the-lambdacube-stack/) is the separation of the platform independent front-end functionality from the OpenGL specific parts. In the next iteration, these will be two separate Haskell libraries, which will also force us to define a clean interface for back-end implementers.
There was a dissertation that used them, but which I can't dig up at the moment, and I had some applicative parsing combinators that used this approach, but I never shipped them. http://comonad.com/haskell/parsimony/Text/Parsimony/Prim.hs is from an early version. I observed sharing in an AST like that to get a nice bottom up parser with sharing.
&gt; People go to university to get jobs Then they're going to the wrong place. Just go get a job. Or maybe a community college or vocational training school if you really must. I'm the only person in my company with a CS-related degree of any kind.
&gt; If I came out of college having used only Haskell or Scheme, I would be pissed. Shouldn't a decent college program get enough of the concepts into you that (in conjuction with tinkering on actual things) you can pick up whatever else you need? I think I would have been fine if my entire program had never used a programming language as long as they found another way to illustrate concepts. That said, I didn't like my program at all, and am fairly biased against the whole system.
The point I was making is that sum2 is nonterminating if one of the elements of the list, or the list itself, is nonterminating. And from the point of view of the standard, those two things are the same. This is why `sum (1 : error "xxx" : 2 : error "yyy")` can throw either error depending on optimization settings. We have `error "xxx" :: Int` which will be thrown if the accumulator is eagerly evaluated, and `error "yyy" :: [Int]` which will be thrown if it's lazily evaluated.
Just to be clear, the spacesuit analogy was in earnest, but the burrito analogy was [a joke](http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/).
Looks like something comonadic to me.
Continuations are not particularly idiomatic for Haskell. You see them more often in strict functional languages. Monads and lazy evaluation mitigate much of their necessity. You can apply the Continuation Passing Style (CPS) transformation to any recursive function that is not tail-recursive to yield a tail-recursive one. Here is example with a friendly function, `map`: map _ [] = [] map f (x:xs) = f x : map f xs In Haskell, the above is just fine, because we are not evaluating the list all at once, only building a series of thunks. However, if we had a strict semantics, we might want to transform it to the following^2. map f xs = go id xs where go k [] = k [] go k (x:xs) = go k' xs where k' rest = k $ f x : rest In a strict semantics, either way we are building a "stack" as we walk down the list, and then folding^1 it up to our final value. In the first example, each descent holds `f x` on THE STACK while we wait for the recursive call `map f xs` to complete. In the second example, each descent allocates a closure on THE HEAP (our friend `k'`). Alternatively, we could reify the "stack" as a list, pushing `f x` on at each descent and then reversing at the end. map f xs = reverse $ go [] xs where go acc [] = acc go acc (x:xs) = go (f x : acc) xs Or, to phrase it as a fold: map f xs = reverse $ foldl go [] xs where go acc x = f x : acc I found the best thing to really get CPS to stick was to practice implementing tree traversals, and then CPS transform them. ^1 [EDIT] this is probably not the best word. I suppose it's more of "evaluating." ^2 [EDIT] just for fun, I implemented this as a fold too: map f xs = foldl go id xs [] where go k x = \ rest -&gt; k $ f x : rest 
&gt; Any divide-and-conquer algorithm that divides the problem into more than one subproblem cannot be written in tail call fashion. As the other responses above prove, this is false. You simply need some extra machinery to keep track of "which subproblem" you're currently working on, and what other subproblems have yet to be solved. Two common instances of such machinery include continuations and [dissection types](http://strictlypositive.org/CJ.pdf).
Nope, our reading list includes 1)Simon Thompson: Haskell: the craft of functional programming, Third Edition, Addison-Wesley 2011 and 2) Graham Hutton: Programming in Haskell, Cambridge University Press, 2007. In the additional materials that we could find useful though, one of the suggestions is LYAH.
Ok, thanks for the info. Thompson's is a good one — I use it for my course too.
Use the `filter` enumeratee from `Data.Enumerator.List`: filter :: Monad m =&gt; (a -&gt; Bool) -&gt; Enumeratee a a m b All of his sed passes were just line deletions, so you could just add those to his iteratee pipeline as four filter stages.
I'm a first year at the University of Edinburgh and I'm currently getting taught haskell too.. Normally Philip Wadler takes the course but hes on a sabbatical this year.
You're right; my mistake.