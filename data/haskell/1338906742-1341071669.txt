Hmm, shouldn't those functions that use the random generators pass out the new generator? I guess using the state monad might be more convenient than doing it manually though.
Don't use 'otherwise' for a case patten. Use an underscore instead. 'otherwise' is meant for use in pattern guards.
The monad involved here is very similar to the one provided by the .NET Reactive framework, and in https://github.com/ekmett/reactor/blob/master/Reactor/Observable.hs#L70 I'm leery that without explicit 'i'm done' messages, his implementation will hold onto far too many event sources that have been spent.
For the curious, `flip concatMap = &gt;&gt;=` for list's monad instance, so you could also write `unlines . map (&gt;&gt;= show)` if you were so inclined.
The singletons package relies on many recent GHC extensions. You'll either need the HEAD version (or to wait for GHC 7.6 to be released.) 
I actually started with that, but it was real cumbersome to have all those functions passing around generators and returning new ones. It seemed much simpler to use split instead.
I am confused as to why you would reply in order to add nothing to the discussion. What context? What gives it worth, being the only language embedded in browsers? I already responded to "but you have no choice and have to use it so I want to pretend that is valuable". I think you are confusing "where the language is available" with "the language". The language javascript is a bad language. It is worse than virtually all comparable languages (PHP being the only competitor to its crown), and has no benefits or upsides compared to those other languages. If you could just wholesale replace javascript with perl, python, ruby, lua, arena, pike, tcl, etc, etc, etc then you would end up in a better position. It would be entirely a gain, no loss of any kind. That is a strong indication that javascript is in fact, wholly and entirely worthless.
This will overlap with normal list instance.
I thought there was something fishy there. I suppose you could use `-XOverlappingInstances`, but I've never been too comfortable with the prospect. Guess it's best to just leave it how it is.
I think that the uses of `INLINE` that exists only to remove dictionary parameters can be replaced by `INLINABLE`, which would decrease the amount of code duplication.
Why doesn't Num come with Eq any more?
Yeah. And you would have to treat removal in a special way. You don't want to cut out 20 lines to paste them further down and all your imports are removed and you have to do the whole picking thing over again.
Floats not coming with an Eq instance is a good idea, though keeping Cmp is probably sensible (even though it contains equality). Then have eqEpsilon :: Float -&gt; Float -&gt; Float -&gt; Bool and (=~=) = eqEpsilon 0.0001 {- or somesuch -}
thanks for your help! I've heard that before about avoiding fail. but im not quite sure why it doesnt belong there. is it because it makes the function partial?
Yeah, provided you didn't want any fusion. Using INLINABLE would be ok for function calls that do much more work than the function call overhead.
The GHC native backend doesn't do the Global Value Numbering (GVN) optimisation. We rely on this for good performance with stencil convolutions. I'd expect vanilla GHC to be 2-5x slower than using LLVM. The role of GVN is discussed in our earlier [stencils paper]( http://www.cse.unsw.edu.au/~benl/papers/stencil/stencil-haskell2011.pdf) 
please post a bug over at the haskell-platform trac, or find me on IRC (mzero), and we can discuss the details off-reddit. That said, the 2012 installer does not remove any files, though it will overwrite symlinks. When you said it "remove my 7.0 install" what was actually deleted? /LibraryFrameworks/GHC.Framework/Versions/7.0.4-i386 was gone?
&gt; a better proposal is to split off fail into its own class (i.e. MonadFail or something like that) Which, IIRC, used to be the case, except it was called `MonadZero`, until it was removed because of the perceived complexity in introducing yet another type class.
I like that alot. 
&gt; I'd strongly recommend against this style of making things functions as a way of indicating that they will be duplicated. Huh? Functions perform computations; what's wrong with that? Functions written in an imperative language typically perform *effectful* computations, whereas functions written in the pure subset of Haskell perform *pure* computations. At issue here is whether something with type "Num z =&gt; z" is really a number (as opposed to a function) in the way that an "Int" or better yet an "Int#" is a number. Given an "Int#", you can be assured that you've a string of bits, and therefore memoizable. Given an "Int", modulo unsafeness, you could have a computation that diverges. But if it doesn't, it's memoizable, like in the first case. Given a "Num z =&gt; z", you have something that is explicitly contingent on a dictionary and may have no resemblance at all to a string-of-bits, and therefore not even remotely memoizable. All ryani is pointing out is that the last case behaves much more like a function, as opposed to a concrete (string-of-bits-like) value, than the first one. The whole business of MR arose because let-bindings enable saving work by allowing explicit sharing annotations. One may be forgiven for being seduced into believing that using let is enough to obtain work-sharing. Enabling MR (the default) is like turning on the alarm that warns you when that belief is mistaken. Of course, most beginners don't care that much about sharing work (assuming they understand the issue in the first place) and just want to run their programs, if only they could just get past the dang typechecker. (I might also add that Ryan Ingram has proven his considerable haskell chops way back already so he knows what he's writing about.) EDIT: I really should say something about how functions /are/ values in Haskell, that "value-oriented" programming language, for some meaning of "value", pace Harper. But this comment is verbose enough as it is already.
Not the whole directory: usr/lib inside it was still there, but all other things were gone. I noticed because usr/bin (and thus the ghc executable) were missing. I'll get on IRC this afternoon, to see if you're there.
No, each capability manages its own run queue of Haskell threads, with occasional load balancing between caps. At most one OS thread holds each capability at once, so at most one OS thread per cap is actually running Haskell code.
Yet another free monad? :)
This is great work. The approach taken seems obvious in retrospect -- a sure sign that things are on the right track :-)
If I understand you correctly, Operational is a way to implement any(?) monad with primitives which guarantee that the monad laws hold? 
Everything that can be implemented in prompt can also implemented in operational. The same goes the other way round, though I think there is a remote caveat.
I think Template Haskell is a pretty good 'first attempt'. It can usually get the job done. But.. now that we have a lot more real world usage of it, hopefully we can design something much better.
Although that does remind me of a question that's been in the back of my mind for a while; namely, are Haskell threads assigned to a given capability at any given point in time, and does blocking the capability then block all those other Haskell threads? Or will blocking a capability block only that capability, and let any other Haskell thread execute on some other capability? **Edit** after reading dcoutt's reply and re-reading your comment, I think I understand now, that Haskell threads are assigned to a capability and that blocking the capability blocks all those threads.
I'm not sure why this post got voted down - but perhaps I shouldn't self-promote my comments on another site! I posted it because it turned into quite an essay (when I really should have been doing something else..), and the previous post is 4 days old. I think it brings up some interesting (if polarized) ideas, and I'm curious to hear responses, particularly regarding the end's criticism of default methods. I don't go into it at all in the comments, but it seems like it would be very possible to use TH to mock up a system for the specification and utilization of defaultings.
Note that you can only block all the other threads on the the capability if you make an unsafe FFI C call that itself blocks (obviously you should be using a safe call in that situation).
It's an eternal truth that humans learn by example, and here again, just by seeing the concept specialized to the example of requesting that a file be read so that computation can continue, I got a much better intuitive understanding of what the Prompt monad is about than I had previously gained from multiple occasions of staring at the documentation. Thanks!
I don't know how detailed a "bug report" I can file about this, but for some reason I found (and now looking at the package again, still find) MonadPrompt more approachable than operational. I'm not really sure why. The separation of Program and ProgramView is probably part of it, and the formulation of the simpler Program monad in terms of the more complicated ProgramT monad transformer another. Precise technical language like "the type constructor instr :: * -&gt; * indexes the primitive instructions" might also stand to be more human-friendly... what does "indexes" mean? what are the primitive instructions? I understand that there's an entire separate tutorial we should be reading instead (and I did, once), but perhaps it wouldn't take much to put a little more connective tissue into the documentation of the package itself.
Speaking of it, gloss-fluid is very slow on my computer (Core2 Duo E7400, 2,8GHz), less than 1 frame per second (I'm running Kubuntu 12.04 32 bits). And running it with +RTS -N2 or -N1 doesn't change anything. If that's relevant, other examples using gloss-raster (i.e. llvm) run smoother (gloss-ray is fine, gloss-crystal is laggy, but far less than gloss-fluid...) Examples not using gloss-raster all run fine. Just to know: is that normal?
&gt; but perhaps it wouldn't take much to put a little more connective tissue into the documentation of the package itself. Sure. But what would you suggest I should write? The problem is that technical terms cut both ways. What is a "prompt"? How can I put "all of its effects as terms in a GADT" when there are infinitely many effects? (Solution: only put the primitive effects, aka the primitive instructions into the monad.) What's a "recursive prompt computation"? How do I figure out which terms are beginner friendly, i.e. already known, and which are not? That said, the word "indexes" is indeed confusing. Also, I should move some parts from the Documentation.md file -- which documents changes with respect to the tutorial -- back to the Haddocks. Can you file a [bug report][1] and maybe offer a few suggestions? [1]: https://github.com/HeinrichApfelmus/operational/issues 
I voted it down because SO is not the venue to have back-and-forth discussions. It is for questions and answers. The original question squeaked by SO guidelines (after a close and and edit) because it asked why TH is looked down on, which can be answered. To debate whether the reasons given (which are genuine reasons people have) are *good* reasons really doesn't belong on SO, and to direct people to this debate is a bad idea. I've resisted downvoting on SO itself because I like to reserve downvotes for things that are not just going afield but are actually wrong. In any case, what you wrote on SO doesn't belong there, since SO is not a place for debates. It might belong on reddit, or in a blogpost. But I limited my reaction to downvoting here. Thanks for encouraging me to explain myself -- hopefully this will prove useful. *Edit*: In fact, if I were you, I'd delete both my answers on SO and put them in a more suitable venue for discussion and debate, and provide a link to that new location in comments to the original post. I'm not passing value judgments on what you argue -- it simply doesn't belong.
&gt;is the kind of question you sarcastically ask a child It is the kind of question you ask someone who says "I'm not sure why this post got voted down" and then gets upset and starts trying to argue when someone gives a reason. &gt;also the pattern of downvotes here is very suspect Your paranoia isn't helping you. The post claims to have 6 downvotes right now. Reddit makes up fake downvotes and upvotes and plays with numbers to try to make bots gaming the system less effective, we have no idea how many people actually downvoted. But acting like I am personally responsible for your downvotes is both childish and petty. &gt;Unless you would like to specify what elements of the accepted answer I did not address? Your response to template haskell being unsafe was simply "only if you do unsafe things". This is not a real argument, and looks absurd when discussing a statically typed language. Why have static typing at all, just don't use the wrong types!
Now that is a good reason! I saw that the original question was closed, but due to popular demand, reopened. You're right that this isn't the venue for it, but that is due to the opinion-related nature of the question. I couldn't leave my favorite tool of late undefended in the answers, after all!
TH+Cabal also used to be pretty buggy. When I saw a valid Lambdabot codebase fail during TH, I would groan because I knew I had no idea why it was breaking and I would have no idea why it would start spontaneously working again later.
&gt; Maybe it's not actually necessary to understand the transformer in order to understand the monad That was my intention, yes. If I don't make them synonyms, then people can't mix `Program` and `ProgramT` freely; the situation is similar to `State` being a synonym for `StateT` in the latest versions of the transformers library. &gt; Maybe it would be best to start the module with some kind of practical example (not an abstract one like a stack machine!) You are asking for the impossible. :-) If an example is small enough to be understood immediately, then it's likely impractical. What would you say is a practical example? The state monad? &gt; I could file a bug report with all the questions that arise in me as I read the module haddock (questions I'm otherwise loathe to raise because I could figure them out with more effort), if that would help. That would help, yes. If you could also take a look at the operational monad tutorial and tell me how smooth the transition to the "real" package is, that would be great.
The other commenters were right: the tutorial didn't work anymore because the type of Map.lookup changed. I have just uploaded a new version of the tutorial which runs with GHC 7.4.1. You can find both the PDF and the literate Haskell source here: http://www.grabmueller.de/martin/www/pub/Transformers.en.html
I wonder if an approach similar to that of the Prompt monad could be employed for resource allocation and deallocation in iteratee-like libraries. An inner IO layer which performed only reads and writes could defer resource allocation (opening files and the like) to an outer "more impure" IO layer. Unlike with the Prompt monad, we wouldn't be segregating pure from impure code, but impure code which doesn't allocate resources from impure code which does. Would that be useful?
FRP is intended to be useful for time-dependent systems, which is a rather large set of applications.
No, it wouldn't be, because true isn't a number.
&gt; Functions perform computations; what's wrong with that? Pedagogically speaking, the reason I warned against that association for new Haskell programmers is that many people coming to Haskell from other environments think too operationally already. Indeed, the very point here is an example of a place where the "computation = function application" mental model breaks down. IMO, the answer isn't to stick to the broken mental model and throw in a pointless and unnecessary function application, but rather to adopt a new and more accurate model that doesn't conflate the ideas of applying a function and performing a computation.
Well, the reopening wasn't due to "popular demand" per se, it was simply that first, 5 people with the power to vote to close did so, and then subsequently, 5 people with the power to vote to reopen did so. I knew that the way I had to ask the question to fit StackOverflow guidelines would lead to a negatively biased presentation of Template Haskell, but I felt the question important enough to ask it anyways. I've contemplated asking the inverse question, "What can Template Haskell do that regular Haskell can't?", or something like that, intended to draw out positive summaries of Template Haskell's features. However, I don't think it would be a successful StackOverflow question. A blog post or wiki page or some other form of documentation would be more appropriate. (Anyone should feel free to ask such a question if they think they can phrase it appropriately for SO. I just don't think I can pull it off for that particular question right now... maybe later.) The great thing about SO is that it provides incentive for people to answer, in the form of magical internet points, which are surprisingly irresistible; I often use SO questions to get more "documentation" out of people, and I think it is quite effective. Of course, in the end, I'm just using SO as intended: I search on my own for answers to a question I have, and if I can't find sufficient info on the subject, I create a SO question. Consolidated expositions of the *downsides* of something rarely exist, and so negatively biased "what's so bad about X" questions tend to make good SO fodder, for better or worse.
Huh?
I've gone ahead and done that. Thanks for the tip! Sucks that stackoverflow is such a restricted communication medium. I haven't used it in ages, so wasn't aware of these guidelines.
`lazyCompareLists xs ys = map (const ()) xs &lt;= map (const ()) ys` Or, more idiomatically: ((&lt;=) `on` map (const ())) And in modern ghc, `map (const ())` can be replaced with `Control.Monad.void` yielding: (&lt;=) `on` void
Definitely. I asked the question knowing it would cast a negative light on TH, but negative light is better than vague FUD; my ultimate goal was to promote better understanding and less "fear" of "big, bad, scary" TH. I'd like to keep promoting discussion of meta-programming in Haskell, as this is an area that has a lot of potential, yet seems under-appreciated amongst Haskellers. I'm curious to see how [MetaHaskell](http://www.eecs.harvard.edu/~mainland/projects/metahaskell/) will fare (a paper on it was [accepted by ICFP 2012](http://icfpconference.org/icfp2012/accepted.html)).
I'm using lenses because record syntax is broken. FTFY
Sweet!
Here's a small file that demonstrates what godofpumpkins is talking about import Data.List (genericLength) data Nat = O | S Nat deriving (Eq, Ord, Show) instance Num Nat where (S O) + m = S m fromInteger 0 = O fromInteger 1 = S O natLength :: [a] -&gt; Nat natLength = genericLength foo = natLength [] &lt;= natLength [1 .. ] Of course, `Nat`s deserve a better `Num` instance than that, but I just hacked together only the things that `genericLength` would need. ghci&gt; foo True ghci&gt; natLength [1,2,3] S (S (S O)) 
And on unmodern GHC, you can replace void with its definition: (&lt;=) `on` (() &lt;$) Or more useful-y comparing void comparing (() &lt;$) I find `comparing void` to be especially awesome; it's _literally_ saying 'compare the structure of the data, ignoring the contents.'
Also known as the [Peano Number System](http://www.haskell.org/haskellwiki/Peano_numbers)
 mind == blown
I'd love feedback of this form; if you have examples that you found particularly enlightening I want to include them (or references to them) in the documentation.
I think of the magic as `[()]` being isomorphic to a lazy nat, the resulting Ord instance being defined reasonably on this type, and `void` providing the unique (up to iso) morphism from a functor to its shape. Now that I think about it, there's probably a neat way to characterize `void` categorically, but I'm not quite sure what the most elegant formulation is. I'd also be interested in a concise categorical formulation of `on`, since I'm sure there's something cool lurking there as well.
[The original formulation of prompt](http://www.haskell.org/pipermail/haskell-cafe/2007-November/034830.html) used exactly this representation. It was refined over the next couple of weeks to fix the left-recursion problem (the same one lists have with ++): ((Ask p Answer &gt;&gt;= \x -&gt; Ask (f x) Answer) &gt;&gt;= \y -&gt; Ask (g y) Answer) &gt;&gt;= \z -&gt; Ask (h z) Answer -- 3 steps in &gt;&gt;= to find the first Ask constructor, evaluate that &gt;&gt;= (Ask p (\x -&gt; Ask (f x) Answer &gt;&gt;= Answer) &gt;&gt;= \y -&gt; Ask (g y) Answer) &gt;&gt;= \z -&gt; Ask (h z) Answer -- evaluate 2nd &gt;&gt;= Ask p (\x -&gt; (Ask (f x) Answer &gt;&gt;= Answer) &gt;&gt;= \y -&gt; Ask (g y) Answer) &gt;&gt;= \z -&gt; Ask (h z) Answer -- evaluate 3rd &gt;&gt;= Ask p (\x -&gt; ((Ask (f x) Answer &gt;&gt;= Answer) &gt;&gt;= \y -&gt; Ask (g y) Answer) &gt;&gt;= \z -&gt; Ask (h z) Answer) and now we can finally run that ask, but we still have the 'left recursive pattern' on the inside of the "Ask", leading to O( n^2 ) behavior when interpreting n left-recursive `&gt;&gt;=`s. The current implementation is basically [the same technique used to solve this problem for lists](http://hackage.haskell.org/packages/archive/dlist/0.5/doc/html/Data-DList.html) except for this monadic type. After discussion and examination, it looks like the original Prompt is the Free Monad for the "Ask" functor, and the optimized prompt is the ["Free Monads for Less"](http://comonad.com/reader/2011/free-monads-for-less/) implementation of the same free monad. I might modify the implementation to make that connection more clear, since Ask is really simple to understand: data Ask question a = forall x. Ask (question x) (x -&gt; a) 
`void` is just taking the morphism map of the functor `f` and applying it to the unique morphism to a terminal object like `()`. The definition of `()` being terminal is that `forall a. a -&gt; ()`, so "just apply fmap" to that :) I'm glossing over the polymorphism in that `forall a`, of course...
You want [Data.List.Extras.LazyLength](http://hackage.haskell.org/packages/archive/list-extras/0.4.0.1/doc/html/Data-List-Extras-LazyLength.html)
I guess you're saying `Codensity (Free f) a`? That works, but I bet there is a way to fuse &amp; eliminate the intermediate construction of the "Free" data structure, that generalizes to any functor f. I think runPromptC is certainly an instance of the right pattern. EDIT: got it, and while the types are different, the code for the monad instance is identical to that in MonadPrompt! newtype Free f a = Free { unFree :: forall r. (a -&gt; r) -&gt; (f r -&gt; r) -&gt; r } instance Monad (Free f) where return a = Free $ \ret alg -&gt; ret a m &gt;&gt;= k = Free $ \ret alg -&gt; unFree m (\a -&gt; unFree (k a) ret alg) alg liftFree :: Functor f =&gt; f a -&gt; Free f a liftFree f = Free $ \ret alg -&gt; alg $ fmap ret f data Ask p a where Ask :: p t -&gt; (t -&gt; a) -&gt; Ask p a instance Functor (Ask p) where fmap f (Ask p k) = Ask p (k . f) -- or (f . k)? I'm bad at doing type inference in my head. type Prompt = Free Ask prompt p = liftFree (Ask p id) runPromptC :: (a -&gt; r) -&gt; (forall x. p x -&gt; (x -&gt; r) -&gt; r) -&gt; Prompt p a -&gt; r runPromptC ret prm m = unFree m ret (\(Ask p k) -&gt; prm p k) Like the Codensity transform, Monad (Free f) doesn't even rely on f being a functor! EDIT 2: Aha, found that exact form in part 2 of the linked series, where it's simply called `F`.
Well, you could see `void` as a natural transformation from `f` to the composition `f . const ()` :) Not sure where bitcategories would come into play though.
Thinking some more about `on`, if you rephrase it a bit from on :: (b -&gt; b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; a -&gt; c to on' :: (a -&gt; b) -&gt; (b -&gt; b -&gt; c) -&gt; (a -&gt; a -&gt; c) it might start looking more promising. More specifically, if you define a contravariant functor: type F r a = (a -&gt; a -&gt; r) then you'd define instance Contravariant (F r) where contramap = on' Well, that's what I have off the top of my head, at least :) I haven't tried making sure this all works, but it seems like it should.
I'm confused why anyone upvoted this. You can't do that for infinite lists. Length loops for infinite lists. You can write a comparison that works as long as one of the two lists is finite. But it loops when both are infinite. There is no algorithm for determining if a list is infinitely long. 
But you can't compare conats. Where is your sense of totality, man!
But you can compare a Nat with a Conat, which is really what the situation would be here if everything were typed as precisely as we'd like it to be :)
 codata Nat = O | S Nat codata Eventually a = Now a | Wait (Eventually a) coleq :: Nat -&gt; Nat -&gt; Eventually Bool coleq O (S _) = Now True coleq (S _) O = Now False coleq (S a) (S b) = Wait (coleq a b) 
 &gt; but something that you would write for its own sake because it does something. But I fully empathize, coming up with realistic yet simple examples is the hardest ever. If nothing else the web server example from the OP might work if fleshed out a bit further (what else might a 'serve' function need to do besides read in a file? The problem I have with this seemingly practical example is that it doesn't actually do anything. If you look closely, it just maps every effect to a plain `IO`, solving none of the problems mentioned in the introduction. The only benefit it has is that is *talks* about functionality that you would write for its own sake ("web server", "open a file"). I think it is largely a "faux example". In contrast, the stack machine or the state monad are not something that you may want to implement for their own sake, but you can actually learn how to use the library by studying how they function. When given the choice, I always opt for the latter type of example. (Not to mention that what you would write for your own sake differs from reader to reader.) But maybe I should reconsider? What do you think is more "approachable": an example that talks about things you are interested in (but doesn't deliver), or an example that teaches how to do things (but they are not necessarily interesting)? &gt; I'll get on to the bug report shortly. Thanks!
ghci&gt; null $ quotesOf (hwn 230) True :(
It is extremely useful pattern. I found it in gtk sources (http://hackage.haskell.org/packages/archive/gtk/0.12.3/doc/html/src/Graphics-UI-Gtk-General-General.html#postGUISync) a year ago, and used several times in my code. Would be nice to have a library on hackage.
Looks interesting. I am just learning haskell coming from statistics and normally using R. R leaves a lot to be desired as far as the language goes. It would be nice to see haskell supply a lot of what R offers. It seems it would be a good match.
Wish I had more. In this case it was just seeing an actual, practical problem [the 'serve' function from the article] that MonadPrompt-like things are a solution to, and how they solve it.
R is a special purpose language, it just has a bunch of built in stuff for statistics and plotting. Haskell is general purpose. You don't need statistics to make server program or a calendar or whatever, so statistical functions are not included in the standard library. There are, however, a whole bunch of special purpose packages for Haskell. They can be found on [hackageDB](http://hackage.haskell.org/packages/archive/pkg-list.html). There's a section with packages for all kinds of standard statistical stuff. (And even one that seems to be built for interfacing with R programs.) There are also a bunch of packages for plotting, but they're a bit scattered between categories.
Indeed.
The proper link this time, thanks for pointing that out drb226.
=)
&gt; GHC will spit out all six type parameters, in all their glory. This can be a bit confusing. Perhaps it's time we start contemplating ways of alleviating this problem from the GHC end, rather than just leaving it to each library developer to solve it in an ad-hoc fashion. Haskell's type system can do some wicked awesome stuff; it's rather silly to not take advantage of it simply for the sake of the default way that error messages are presented.
This looks like another great improvement. I'm not sure if the old restricted conduit types are still that useful. But then, I am not in the intended audience for them. Now, if you want to be able to connect these pipes to your kitchen sink, maybe you need a couple more type parameter. You could add a parameter e for errors, then await can get type await :: Pipe l i o u e m i Though you can probably do this manually with EitherT. It might also be possible to recover a more general identity for composition by propagating leftovers upstream? I am thinking of data Pipe l j i o u m e a (&gt;+&gt;) :: Monad m =&gt; Pipe l1 l2 a b r0 r1 m r1 -&gt; Pipe l2 l3 b c r1 r2 m r2 -&gt; Pipe l1 l3 a c r0 r2 m r2 Gabriel will probably correct me on this, and I'm not sure if I can actually get it to work. And even if I could, I don't think it's all that useful. Maybe with some more type parameters? We still have 18 to go :) At the very least, you could generalize composition to (&gt;+&gt;) :: Monad m =&gt; Pipe l a b r0 m r1 -&gt; Pipe Void b c r1 m r2 -&gt; Pipe l a c r0 m r2
&gt; Move the Pipe constructors to a separate module (Data.Conduit.Internal), and recommend people avoid using them directly. I will say that I think this is good practice. Can't remember the number of times I've wanted to make legitimate extensions to a library but was unable to due to unexported constructors or unexported methods, without a good reason for not exporting them. I had to go and patch the library itself. So I feel the .Internal approach is best, it's a way for the API developer to say “hey, you can use it, but don't rely on it”, but also doesn't block the developer from extending/experimenting. IME such “encapsulation” is almost always premature and just annoying. For example: class PrintfType t The PrintfType class provides the variable argument magic for printf. Its implementation is intentionally not visible from this module. *Why?*
There's still one problem, namely that to get a proper upstream identity you cannot receive a `Right` after a `Left`, however if you separate the category of `u` values from the return value `r` then I think you can then implement this correctly and form a strong upstream identity. The lack of a strong upstream identity manifests itself in the inability to return a value directly from upstream. I know this is not as important to `conduits` as to `pipes`, but I briefly mentioned the benefits of being able to do so here: http://www.reddit.com/r/haskell/comments/uav9d/pipes_20_vs_pipescore/c4u4npt
I think you were confused by this: &gt; GHC and Cabal for example both invoke GCC for compilation I should have been more clear. As it stands, from what I saw of the source last time I looked, GHC really only invokes GCC for C code compilation (passing in a `.c` on the command line) or, compiling the .s files containing assembly code from the code generators. LLVM itself has a built in object-code emission framework, but I don't believe we use that. We just invoke `opt` and `llc` to optimize and generate assembly, then compile it with GCC. But you still need some means of making GHC use the cross compiler in these cases (aside from generating correct code.) I think the ultimate goal some people have is to make GHC a 'Universal' cross compiler, where it can always generate code for all architectures, and you can just specify at compilation time what toolchain to use.
I hope all the major changes in conduit are tested with benchmarks to avoid performance degradation ? 
Why do a strong type system and immutability make implementation hiding any less relevant? The reason to do implementation hiding, besides enforcing invariants, is so that people don't wind up depending on your implementation, so you can change it without causing breakage. It's to preserve your freedom as a library author and to protect users of the library. It's possible to do compromises like making implementation details public while stating that "this can change at any time and if you use it you're on your own", but in practice, if something important ends up depending on it, it can wind up being impractical to change all the same. If some Haskell package authors are overzealous with their implementation hiding, leaving the public interfaces insufficiently expressive, that seems like a human problem, not anything to do with the language. Or - I think I just now got your meaning - if in languages like Python and Perl due to the nature of the language it's possible to subvert the module system, I would call that a bug, not a feature. But if that's what you want to do, then Template Haskell might be able to do it. (As an aside, do mutability and a lack of static typing have anything to do with it in even Perl and Python's case? I remember there being a big internet debate between the Python and Ruby camps about how in Python private members were only private by convention but could actually be accessed, which seems to imply that Ruby does manage to enforce implementation hiding even though it's dynamically typed and mutable.)
&gt; Can you clarify what you mean by "separate the category of u values from the turn value r"? I mean something like: Pipe l i o u1 u2 m r I guess the part I'm not sure about is how you write something like `awaitF` from `Frame`s that automatically handles upstream termination (i.e. returns `a` instead of `Maybe a`) without bottoming. I find the lack of a proper `awaitF` makes conduit consumers more difficult to write, as I noted in my link. In other words, I'm looking for something with the type: Pipe l i o u m i However, you are right that this does make folds easier to write. Perhaps these should be considered two separate categories with separate purposes (i.e. the `pipes` one is not biased towards any pipe in the chain returning, whereas yours is a fold-like category that biases towards the downstream pipe). There is still the issue of the weaker upstream identity that requires invariants, too, and that will always bug me, but you and twanvl are really making some interesting points and I need to think more about this.
I already benchmarked conduits, pipes, and frames, and conduits 0.4 is slower than pipes, but faster than frames, and the changes he's proposing should not affect performance. The majority of performance degradation is from the finalizer caching, which this doesn't impact.
Thinking about it a little, I think the situation might be a result of Haskell programmers placing a higher priority on enforcing invariants than programmers of other languages. So if they have some things in the library which can be used to break invariants but which could also be necessary for some purposes, they will opt to hide those parts and preserve the invariants. In that case the Internal module is a good compromise (though maybe Unsafe would be a more appropriate name?): the implication is that you can do bad things with it so you better watch out, not that it might be changed without warning.
Ah, I see. So, the practical example is meant to provide context and to explain how the library is, in principle, capable of solving this kind of problem. The "toy examples" can then be used to show how to create something executable. By the way, I have a [couple of interesting examples][1], but I see that I need to make them more prominent. [1]: https://github.com/HeinrichApfelmus/operational/tree/master/doc/examples#readme
The one that I'm unconvinced about is the leftover bit. I think there may be less intrusive ways of handling this. Your type is: Pipe l i o u m r Is there ever any useful option besides: (a) `l = i`, or (b) `l` is unconstrained?
My issue is not the lack of type-level lambdas, but rather the awkward use of newtypes to select the appropriate type-class instance.
Something like this might improve error messages: data Pipe' l i o u m a type Input i = i type Output o = o type Leftover l = l type UpstreamResult u = u type Pipe l i o u = Pipe' (Leftover l) (Input i) (Output o) (UpstreamResult u) On the other hand, it might just make things worse. [](/twismile "Here is a value-level unicorn, is that good enough?")
With monad transformers: awaitForever :: Monad m =&gt; (i -&gt; Pipe l i o r m r') -&gt; Pipe l i o r m r awaitForever f = mergeEitherT $ forever (EitherT awaitE &gt;&gt;= lift . f) mergeEitherT :: Monad m =&gt; EitherT a m a -&gt; m a mergeEitherT = liftM (either id id) . runEitherT It would be even nicer if yield and await lived in a type class: class Monad m =&gt; MonadYield o m | m -&gt; o where yield :: o -&gt; m () class Monad m =&gt; MonadAwait i m | m -&gt; i where await :: m i instance MonadYield o (Pipe l i o u m) where ... instance MonadAwait (Either u i) (Pipe l i o u m) where ... instance MonadYield o m =&gt; MonadYield o (EitherT m) where yield = lift . yield instance MonadAwait (Either e i) m =&gt; MonadAwait i (EitherT e m) where await = EitherT await Now inside `EitherT u (Pipe l i o u m)`, upstream termination automatically terminates the current pipe. mapM_ :: Monad m =&gt; (i -&gt; m ()) -&gt; Pipe l i o r m r mapM_ f = mergeEitherT $ forever (await &gt;&gt;= lift . f) On an unrelated note, type classes might also improve the error messages. And they would also allow you to stick in Source and Sink newtypes, if you so desire.
As tibbe points out in that discussion it can be done in C++ and C#. C++ basically does what your template Haskell code does. C++'s template system is basically just a weird and awkward code generation system (it does avoid generating the same thing twice, though). I assume C# uses the JIT compiler to do this somehow. I'd be interested what the constraints are for doing this in C#.
Solving this problem well is really hard. We discussed it with the Simons and a bunch of other people at ICFP last year. I really want to solve it though, it's one of the last remaining road blocks to great Haskell performance. The problem with a template Haskell (or CPP) code generation approach is that you either end up with orphan instances, if you use associated data types, or incompatible types, if you simply generate new top-level types for each key-value combination you use. To make this code generation approach work you'd have to pre-generate common combinations of key-value pairs and stick them in some basic package where everyone can reuse them. If you don't, and `IntIntMap` generate in one module can't be used with another module that has its own generated `IntIntMap`. 
The problem often is that while we have a strong type system, it isn't strong enough. For example, I often use newtypes to implement weak dependent sums in Haskell. I.e., given `newtype PFoo = PFoo Foo`, the type `PFoo` represents `Foo`s which satisfy some additional predicate `P` (e.g., non-negative, non-zero, etc). The whole point of using newtypes is to ensure that being strongly type correct does not actually impair performance. However, if I expose the constructor for these newtypes, then the type safety goes out the window--- since Haskell lacks dependent types and so there is no way to force the standard typechecker to respect the invariants I'm encoding. I understand that the subject of your ire isn't newtypes like these but rather data structures etc. However, I see no principal distinction between those cases. Often with data structures the constructors must be hidden for exactly the same reason that the newtype constructors are hidden: the structure uses powerful type invariants which Haskell's type system cannot express (or cannot express efficiently). And even if we did have full dependent types, there are still reasons for implementation hiding--- or else Coq, Agda, and everyone else wouldn't be doing it. One reason is simply a pragmatic one: users of, say, IntMap shouldn't care how it's proved correct; they just want to know that it is. That is, while dependent types are needed locally within the implementation in order to demonstrate its correctness, the dependencies don't often escape into the API. But another reason for hiding is that it appears to be necessary for fundamental reasons. That is, by hiding implementations you tell the typechecker where it is not allowed to unfold things, and this is necessary to prevent type checking from taking unacceptably long. Fundamentally there is always a difference between the thing being implemented and the content of the implementation. The implementation lives at the level of syntax, whereas the thing being implemented lives at the level of semantics. Computers understand structure/syntax, but they cannot comprehend meaning/semantics. Thus, this distinction is firmly in the realm of human thought and, much as we try to codify it into syntax, there will always be a gap between the two. Thus, without implementation hiding there is no way for the computer to ensure that clients are not depending on implementation details.
We've been looking at this [since 2009](http://donsbot.wordpress.com/2009/10/11/self-optimizing-data-structures-using-types-to-make-lists-faster/), there's just no good soln yet for the explosion of instances. Increasing data density, and removing pointers, is a huge win for performance nonetheless. (I'm actually a bit surprised we haven't made more progress since Utrecht 09. )
I'm still not convinced it's insurmountable (at least for a restricted set of lambdas/combinators). Consider again the example you give: solve Monad m =&gt; m String ~ (,) String String With possible answers: m = \x. (,) String String m = \x. (,) String x m = \x. (,) x String m = \x. (,) x x While there is indeterminacy here, these unifications are only *potential* solutions to the system of constraints we're trying to solve. We have to push the question back one step: these potential solutions are only actual solutions if there are instances for them in scope. But, notably, there cannot be instances for more than one of those unless we also have OverlappingInstances (and possibly IncoherentInstances). Thus, in the absence of those extensions, the actual solution (i.e., the instance dictionary required) is guaranteed to be unique, whatever it is. And even with OverlappingInstances, in the absence of IncoherentInstances we are still guaranteed to have a subsumption lattice, and therefore we can always uniquely choose the most specific instance. Yes, it would greatly complicate the inferencer. And, sure, there may be uglier cases which can refute this argument. But the presence of the argument suggests that the problem is not, in fact, known to be impossible. I agree that it's hardly a "not implemented yet" ---there's a lot of theory to be done yet to prove confluence of type-class resolution--- but I'm not convinced that it's impossible.
Alright, I'm coming around to this point of view. I'd much prefer to see a clearer type parameter there, though. It took me some nontrivial thought to realize that there are really only two options there, and that `l` is only intended to be unified with `a` if anything. If GHC extensions are not an issue, then a GADT with phantom types `Leftovers` and `NoLeftovers` could be much cleaner. A new kind as well would be even clearer yet, but I'm afraid that it's a bit premature to expect any kind of core package to require GHC 7.4.
Yes, I too am surprised it has not been done already. It's not that difficult to do. You just specialize all constructors and functions to the types that they are actually used at (with some escape hatch for the few cases where the program cannot be monomorphised), and then you collect the specialized functions and data types into groups where all the types have an isomorphic representation (like, Word64 and Double can go together) and just generate code for one function in each group.
Exactly what I thought at the first place. Another solution is the reification of the dictionnaries (passing them explicitely).
I think the type-level records approach is nicer anyways. Wonder what the problems with that are. Apparently Ur has it already, but things are never simple.
Your "groups" suspiciously look like kinds to me. `*` for one-word types, `**` for Word64 and Double? 
I am very interested in all the ways serious haskell programmers would improve this program ;o) One specifc point that "needs to be improved" is that I am dumbfounded as to how this program behaves on Linux/OS X and Windows XP - see http://stackoverflow.com/questions/10960264/haskell-opengl-texture-rendering-fine-unter-linux-windows-shows-white-os-x-bla
Well, after a quick look I say it's mostly imperative, which isn't your fault because GL just happens to be imperative. That `IORef Game -&gt; IO ()` thing is definitely ugly. Try to use update :: Game -&gt; Game draw :: Game -&gt; IO () , instead: It's apparent from the signatures whether the state is only used or also written, and that `update` does no drawing and `draw` no updating. If you really need to thread the gamestate through the IO Monad there's `Game -&gt; IO Game` or `StateT Game IO a`. You probably don't, though. If you need to push input into `update`, use `update :: Input -&gt; Game -&gt; Game` or something to that effect. As to your SO question: Maybe you're using a kind of texture (non-square? non-power of two? strange in other ways?) that isn't supported by OSX and XP, and the two just like to fail differently. Or you need to enable an extension explicitly.
I've used records and -XImplicitParams in some situations to replace type-classes where I want to select an instance, or the functions are sometimes generated from data I read at run-time.
If you haven't, [get the Haskell Platform for Windows.](http://hackage.haskell.org/platform/windows.html) [Learn You a Haskell for Great Good!](http://learnyouahaskell.com/chapters) is a pretty good tutorial, though it's pretty lengthy, it covers pretty much everything I imagine you'd need to get started.
Many thanks for your hints... I will look into separating concerns to keep functions simpler and cleaner. You may have hit the mark with the non-power of two idea... I didn't know that arbitrary-size textures are only supported from OpenGL 2.0 on up and that the Windows Software Renderer uses 1.1. 
The devil is in the details. We tried exactly this and ran into corner cases. What do you do if someone takes your monomorphised type and wraps it in an exestential for example? 
I suggest using `hs-Source-Dirs` and move your sources to their own subdirectory. I almost missed the existence of `Recogs.hs` in a glance, because it's thrown there with the rest of the top-level files. `getImageData` should not print an error (and definitely not to stdout), it should return an `Either` instead of a `Maybe` and allow its caller to print an error. Also, you can extract all the `Right` cases into its own `case` and then you'd be able to extract the `return . Just` part outside of the case and not repeat it 5 times. Also, use qualified imports or name lists when you import. https://github.com/FlashKorten/recogs/blob/master/Recogs/Util/Config.hs has 7 open unqualified imports. Every name might come from any of them. Any new name added to these modules' export lists may conflict with names you give in your own module. This may cause compilation breakage later for no good reason. Use `ghc -ddump-minimal-imports` to get the list of symbols you actually use. Use `hlint` to get style hints and how to have idiomatic Haskell style. Use `GLFW-b` instead of `GLUT`, it is a much better library, with better input handling. If you use the [graphics-drawingcombinators](http://hackage.haskell.org/package/graphics-drawingcombinators) packages you can avoid the imperative nature of OpenGL, mostly. Try not to indent your entire program so far to the right. In `createAdjustedWindow`, for example, put a newline before the `|` to move everything to a sane indent. 
 drawSegments width height = foldr ((&gt;&gt;) . drawSegment width height) (return ()) can be replaced with: drawSegments width height = mapM_ (drawSegment width height) and inlined, does not seem to be worth a separate function. eol :: Parser () eol = do _ &lt;- oneOf "\n\r" return () &lt;?&gt; "end of line" you can use: eol = (oneOf "\n\r" *&gt; return ()) &lt;?&gt; "end of line" or: eol = void (oneOf "\n\r") &lt;?&gt; "end of line" The following pattern: case field of Just n -&gt; Just n Nothing -&gt; fmap read (Map.lookup fieldName confFromFile) can be replaced with: fromMaybe (fmap read (Map.lookup fieldName confFromFile)) field I would replace merge' (_ , a) _ = return a -- This will not happen... only to suppress warnings. by: merge' (_ , a) _ = error "Impossible happened"
That sounds like quite a wide area for application of the technique. As for Elm, I was thinking more along lines of FRP Gtk, but this looks nice. Even if web GUIs are quite different to the classic stuff.
I'm a visual learner so I like to watch lectures and actual live coding. I've found these videos to be extremely helpful in learning haskell. The first link below is in german so you'll need to translate it to english unless you can read and understand german. :-) http://videoag.fsmpi.rwth-aachen.de/?course=12ss-funkprog Google tech talk -&gt; also good for just starting out. http://www.youtube.com/watch?v=b9FagOVqxmI Also, spend some time navigating around haskell.org lots of great links to other video presentations. Good luck and have fun with haskell! 
This has already been solved. The way you do this is to use a free monad to represent the code to be threaded, and then you interpret the free monad using the scheduler of your choice, allowing the threaded code to be described completely independent of the scheduler (and consequently, independent of the base monad). This technique is described in [A Language-based Approach to Unifying Events and Threads](http://www.cis.upenn.edu/~stevez/papers/LZ06b.pdf), where they use a CPS-style free monad to implement it. I also demonstrated the ordinary free monad version [here](http://stackoverflow.com/questions/10364549/monad-friendly-event-based-io/10365692#10365692), which is handy since the author's original code is hard to follow and has some mistakes.
I didn't know of hs-Source-Dirs, -ddump-minimal-imports and GLFW-b yet... Moved sourcefiles into src dir, explicitly named imports... Will look at GLFW-b tomorrow... Your help is greatly appreciated, thank you very much for your time and your hints ;o)
And I thought my code was concise already ;o) I don't think the fromMaybe is applicable here, since Just n results in Just n, not n... but I found another way to improve that part Thanks for your time and hints! (you'll find all your other advices taken)
You are right - that behaviour was not really what was intended... I used your insights to greatly improve those parts in the Config-Module. (using a single Function getParameter instead of getInt, getBoolean...) Still not perfect, but much better. I have to think a little more about a consistent error handling style... Thank you very much for your time and effort, I am deeply impressed by how nice, well-intended and passionate the haskell user base is... bless you!
Is there any benefit to having a functional GL rather than having bindings to imperative C?
&gt; Do you really need to repeat it? You do. StateOf is just a type function (as far as I can tell you can use it just fine without providing a type class constraint), you need the MonadState constraint to gain access to the type class dictionary and methods. &gt; If StateOf m must be 's', then it means that the guy declaring an instance must write: &gt; &gt; instance MonadWithState Foo Bar where &gt; type StateOf Bar = Foo &gt; &gt; Which is kinda redundant: it had to be Foo, there could not have been anything else, due to the equality constraint. This is confusing things a bit: you do not declare instances of MonadWithState. You declare instances of MonadState, those are automatically instances of MonadWithState, and you use the latter when the syntax is more convenient. However, in other cases (like the 'Three' class above) you do in fact need to redundantly declare the type instances as you say even though they could not have been anything else, which is kind of annoying. I was hoping associated type defaults (available since I think 7.4) would fix that, but it turns out that in exactly these kinds of writing-FDs-using-TFs situations you can't use them, for the same type-variables-on-the-right-must-be-bound-on-the-left reason as the other things you can't do. See also Simon P.J.'s comments here: http://hackage.haskell.org/trac/ghc/ticket/5481
&gt; You do. StateOf is just a type function, you need the MonadState constraint to gain access to the type class dictionary and methods. Okay. You're right, but what is the point then in putting the type family declaration *inside* the class declaration? It's misleading if the type instance doesn't imply the class instance (because that's what you would expect, at least that's what I expected), even more if you call those types "*associated* types". &gt; This is confusing things a bit: you do not declare instances of MonadWithState. You declare instances of MonadState, those are automatically instances of MonadWithState, and you use the latter when the syntax is more convenient. Sorry, I had that part wrong.
Ah, you are right. Just for the sake of it, you can combine maybes using mplus, so previous version would be: field `mplus` fmap read (Map.lookup fieldName confFromFile) and current version... getParameter fieldName field confFile fallback = case field of Just n -&gt; n Nothing -&gt; case confFile of Just file -&gt; fromMaybe fallback $ fmap read (Map.lookup fieldName file) Nothing -&gt; fallback can use msum: getParameter fieldName field confFile fallback = fromJust $ msum [field, fmap read (confFile &gt;&gt;= Map.lookup fieldName), Just fallback] so it's clear you choose in order of preference: `field`, then configuration file, then the fallback. Other things: `(flip (-) 1)` can be written as `(subtract 1)`; `subtract` was created as a replacement for missing section `(-1)`. I noticed the `getSizeFromMaybeImage` function is not needed, as you always call it on `Right`, never `Left`. You can change: Right img -&gt; do let (w, h) = getSizeFromMaybeImage image (configWidth c) (configHeight c) ... to Right img -&gt; do let Image w h _ = img ... You can also change `(putStrLn $ show err)` to `print err`, but it's better to refactor errors as in Tekmo's comment.
 &gt; :t map isUpper map isUpper :: [Char] -&gt; [Bool] &gt; map isUpper "testString" [False,False,False,False,True,False,False,False,False,False] 
I would suggest you not do this OP. Trying to learn haskell is challenging enough already. Learning haskell by learning yesod is like learning ruby by learning rails: you end up a rails programmer, not a ruby programmer.
Are you suggesting that this is because of Yesod's Template-Haskell/QQ-use? The OP is apparently interested in web development, so why not suggest the most feature rich Haskell web framework for him to check out? There's also Happstack and Snap of course (I've never used Snap personally), with Happstack being slightly more low-level in all regards and thus "closer" to "raw" Haskell. I started with Happstack and I'm using both Yesod and Happstack productively, at work. Sometimes I feel like starting with Yesod would've made learning Happstack later on a bit easier, but that of course differs from person to person. That was my reasoning behind suggesting Yesod.
How can you say that about LYAH ? It's just a brilliant book, what bad habits could you possibly get from it ? Further more OP only has a week and you could easily read it in a week. RWH has great stuff in it and it's true that once you understand it you'll have a great feel for how to write real world code in haskell. But that book took me months to understand and frustrated the hell out of me at times.
As dmalikov proposed, the idiomatic way is to use a map. But if you prefer to get to the bare bone of the stuff if you feel you will understand better, here is a simple function, in the spirit of what I think you've attempted so far: import Data.Char (isUpper) uppercased :: [Char] -&gt; [Bool] uppercased [] = [] -- When the input string is empty uppercased (c:cs) = isUpper c : uppercased cs -- When the input string is not empty: -- c and cs are mere variables, -- c is the first element (character) of it, cs is the rest It's a common pattern in Haskell to have a recursive function whose body is laid out like that: f (something headOfTheList) (recursiveCall restOfTheList). In that case, f is but the (:) infix operator, which constructs lists. If the function f can return something without evaluating its second parameter (the recursive call), then we call the whole thing "guarded recursion", because the recursion will be done only if it's needed. It's so common a pattern that we actually very seldom write it like that ourselves: we have functions (like map) which encompass it and which are generic enough. Those are simple haskell functions, and that reduces the need for specific syntax ('for' loops...). Now, if what you want is actually specific syntax ("syntactic sugar"), Haskell has comprehension lists (like Python), so you example can be written more closely to your pseudo code as: uppercased input = [isUpper character | character &lt;- input] But here it really is the same than using 'map'.
I appriciate the reply, but I know what isUpper does. How I write a function to check if the character is upper or lower and spit it out is the question at hand. Thanks.
Seems my reply was not submitted. Thank you very much! That was a very good explanation. Better then most of the books i've read through. Is there a way to contact you with more questions? Let me know Thanks again!
I am indeed interested in web development, but I'd like to build a foundation of Haskell knowledge before I dabble in webdev, to avoid "rails programmer" syndrome. Yesod goes on the list.
This smells a lot like a homework problem to me. Anyway: you're passing a single argument to filter, but it requires two. The second argument is the list you want to filter, the first argument is a function which returns a boolean value for a single element. "True" is not a function, I think you mean (\x -&gt; x == True), but the one you should be looking for is the id (identity) function. You should also read about applying functions in Haskell, you don't need that many parens in your example. EDIT: Applying filter to the string directly would be the most straightforward: countupper = length . filter isUpper
Woah, great point! Even more awkward to write, but that's a whole lot better than not being able to write it at all. It's basically using the same trick as my second nugget (hiding parameters), by using a type function from in-scope variables which evaluates to the same type as the out-of-scope one.
And wow, almost simultaneously. :)
&gt;Are you suggesting that this is because of Yesod's Template-Haskell/QQ-use? Not specifically, more just the general fact that learning haskell by using yesod will teach you to write very non-idiomatic code that lots of people will not want to touch. &gt;The OP is apparently interested in web development, so why not suggest the most feature rich Haskell web framework for him to check out? You didn't, you suggested he learn by playing with yesod. I think it is better to learn the language first, then play with a framework. &gt;with Happstack being slightly more low-level in all regards and thus "closer" to "raw" Haskell. Sounds a lot more like what someone who is learning would want to use. It is also much more flexible, I would characterize it as "the most feature rich", given that it works with pretty much every library around. &gt;I've never used Snap personally Happstack - documentation = snap
&gt; How I write a function to check if the character is upper or lower and spit it out That is what toUpper does.
[Edit: Fixed] 
I'd be interested in seeing those benchmarks. In the benchmark in the conduit repo, `pipes` is 20x slower than `conduit` on pure code, and 2x as slow on file writing code. This devel branch has been thoroughly performance tested, and AFAICT there are no regressions. I had to add a few rewrite rules to avoid intermediate data structures (e.g., a rewrite rule for awaitE == flip NeedInput), but as those seem to fire reliably there doesn't seem to be a problem.
Oh yeah, on pure code conduits is faster because it skips the bind like pipes 1.0 used to. I test typical IO functions like reading and writing files.
I agree that in general it's meaningless how fast these things perform in completely pure code: if you can express it in pure code, just use laziness and don't bother with any streaming library! The reason pure code *does* matter though is that it's very common to need to combine some pure Pipes (like map or fold) with IO pipes. I should figure out a good benchmark to measure the effect of adding some pure pipes to an IO pipeline like file copy.
Yeah. Also, the only reason pipes go faster than conduits is because they are not doing any finalization work. However right now I'm working on implementing a proper parametrized monad, which is why I've been a bit silent on pipes.
You need the escape hatch. Sometimes you need to use a pointer representation that guarantees uniformity, existentials and higher ranked universals are examples (they are really two sides of the same coin). I don't think the approach I'm suggesting can be done without compiler support. I want a language that allows me the same fine control over data representation as, e.g., C or C++ gives me. For instance, in Bluespec we had a Bits class that described how to convert a value to and from a bit string. With that you could have full control over data representation, but usually you just used deriving to get the instance of the Bits class.
I don't think they're necessarily architecture-dependent. There are some types for which you can make reasonable representation choices in a portable way: for example, that `*` types are one-word, and that unboxed products of `n` elements of kind `*` are `n`-words. Now, some other types have portably-unknown width, for example `Word64` and `Double` (which may moreover need to be passed on FP registers in some architecture, etc.). They should each have their own kind: what's not portable is unifying this kind with one of the known-length ones. You can type the surface program in a system where those kind-level-equalities are not known (guaranteeing portability), or reserve them to marked-unportable fragments of code, and then later reveal them in low-level stages of the compiler. This would justify the transformation you were mentioning (quotienting specialized functions over representation isomorphisms) as a well-typed source-to-source (or core-to-core) translation: elimination of duplicate code.
Nice! Note that free monads are not necessarily the free-est possible object in the class of monads, though. The approach presented my ["The Operational Monad Tutorial"][1] is even more general. If I remember correctly, free monads have trouble implementing `get` and `set` from the state monad. [1]: http://apfelmus.nfshost.com/articles/operational-monad.html
`reactive-banana` is probably one of the most well-documented libraries out there.
&gt; there's a haskell-beginners mailing list that may help you as well. The problem with haskell-beginners... it that there are mainly beginners. &gt; gtFive num = if num &gt; 5 then True else False Ahem: gtFive num = num &gt; 5 Please.
&gt; Better then most of the books i've read through. What are the books that you read? Have you tried RealWorldHaskell online? It covers the definitions of the basic functions (map, foldr...). I'm don't remember if it tells explicitely the difference between tail and guarded recursion (which is very important) since I learnt the latter term from Haskell-Café (the mailing list). Those are things I have struggled a bit with while learning the language \^\^, so yeah if you have more questions you may PM me on reddit.
&gt; Maybe he's trying to do it with the list monad Actually, list comprehension is just another wrapper around the do notation for the list monad. (Which explains why now with the extension MonadComprehensions you can use this syntax with every monad) But List comprehension are easier to grasp than the do notation for lists for beginners IMO (due to the fact that they're not totally new: they're already used in Python for instance).
In my opinion, Yampa's arrow style is very clunky to use, though. Note that [sodium][1] supports dynamic event switching ("dynamic collections of behaviors"). The next version of reactive-banana will do so as well. [1]: http://hackage.haskell.org/package/sodium
Mmmh... no, either you make you own recursive function *or* you use a function from the standard library, not both. 'filter' is defined like that: filter f [] = [] filter f (x:xs) = if f x then x : r else r where r = filter f xs You see it shares a common recursion pattern with map, that's because it also uses guarded recursion. But it already does the recursion, so you don't have to do it yourself in your 'countupper' function. keepupper :: [Char] -&gt; [Char] keepupper input = filter isUpper input No more. And if you want to count them, that's what 'length' function is for: countupper input = length (keepupper input)
 data Stateish s a = Get (s -&gt; a) | Put s a
Personally, I love the arrow-notation in Yampa. It makes a huge sense there. So yeah, I would recommend Yampa, too.
&gt; In the List' a monad, (&gt;=&gt;) behaves did you mean (&gt;&gt;=) ?
If you are interested in using FRP for web-based projects, take a look at [Elm](http://elm-lang.org/). As [announced](http://www.reddit.com/r/haskell/comments/uugne/announcing_elm_02_haskell_integration_yesod/) today, Elm is now pretty well integrated with Haskell, so you can serve Elm code with Yesod, Snap, or HAppStack without too much trouble ([examples](https://github.com/evancz/Elm/tree/master/Examples)). Warning: I am the main developer of Elm, so I *may* be biased :)
Thanks :) There's not a full-fledged FFI for JavaScript yet, but some mild forms of integration are possible: **Elm in Html** Elm code can currently be embedded in a larger HTML application using iframes (linked to the compiled html). You can add as many Elm components as you'd like with this method. The Yesod-specific libraries make it easier to embed Elm as a component more directly (in a span), but right now, you can only embed one Elm component per page with this method (adding more will cause problems). I singled out Yesod, but this is possible in any of the web-frameworks. **Html in Elm** I have been thinking about providing something like `(fromHtml :: Html -&gt; Element)` or just making it possible to embed pages via iframe `(iframe :: Int -&gt; Int -&gt; String -&gt; Element)`. Adding `iframe` should be pretty straight forward, and I can't really see it causing terrible problems that I regret forever. Although I guess that's how it *would* seem before you made that kind of mistake :P Would something like `iframe` be helpful to you?
Ah, right. get :: Free (Stateish s) s get = Free (Get Pure)
If you can acclimate to arrow notation (which I really hated at first), the theoretical benefits of arrows (Signal Functions) are really great. It does impose a bit of an extra burden on the programmer because every component must explicitly state its inputs. Nonetheless, I have not been able to come up with an unproblematic alternative to signal functions when it comes to dynamic collections and dynamic switching. How are you going to do it in reactive-banana?
&gt; How are you going to do it in reactive-banana? My solution is going to be similar to the approach first described by Gergely Patai. [Summary][1]. That's also how sodium does it. Basically, you do have to distinguish events with different starting times. Yampa solves this by deferring this problem to the input stream. ("Different inputs = different input starting times -&gt; different outputs"). &gt; the theoretical benefits of arrows (Signal Functions) are really great I don't perceive them as great. The main feature of arrows is that they restrict the flow of information (keyword: no strong monad), which is necessary for an efficient implementation of FRP, but the terrible price is that you have to do explicit plumbing. They don't solve the problem of restricting information flow in the usual applicative style, but that's exactly the problem I want to solve when I embed FRP into ordinary Haskell. [1]: http://apfelmus.nfshost.com/blog/2011/05/15-frp-dynamic-event-switching.html#solution-1-context-dependence
Wasn't 'sodium' a joke name coming from the fact that bananas contain potassium and that it is less *reactive* than sodium? \^\^
Sorry about that. Can you try a more recent browser? Elm needs some HTML5 features, so the major *modern* browsers are supported.
1.1 is really ancient and will probably give you bad habbits. If you plan to do something more demanding like games you should really stay away from it and do it the hard (and fast) way of VBO and all. On the other hand if all you plan to do is goofing for fun (and there's nothing wrong with it) then 1.1/2.0 will probably be enough, just keep in mind it kills performance.
`&gt;&gt;`
No, he meant (&gt;=&gt;), with (&gt;&gt;=) types wouldn't match. You get two "lists" and return a "list".
I think the joke was that bananas contain potassium, which is actually [more reactive than sodium][1]. I wager that Stephen chose the name "sodium" because it is a very reactive metal, not because it is yellow or otherwise related to bananas. [1]: https://en.wikipedia.org/wiki/Reactivity_series
I believe they are completely equivalent, except the free monad approach doesn't require GADTs to permit input from the interpreter. Basically, if you have a monadic action of the signature: action :: a -&gt; m b Then the term in the free monad's functor would be: Action a (b -&gt; x) ... whereas the term in the operational GADT would be: Action :: a -&gt; Instruction b The advantage of the operational approach is that the signature for the base type's terms match the signatures of the corresponding singleton actions. The advantage of the free monad approach is that it doesn't rely on GADTs and is closer to the category theory concepts of the base functor and free monad. As a result, I find that the concepts and intuitions developed by the free monad are more easily generalizable. For example, right now I'm working on using an indexed free monad (the one in Conor McBride's "Kleisli Arrows of Outrageous Fortune") to implement the parametrized monad for frames, and the free monad trivially generalizes to the indexed type. On the contrary, doing the same for the operational monad would be much more difficult. Also, the free monad type more closely resembles a list (the corresponding free monoid for values) than the operational type (which does resemble it, but not as well). Edit: Actually, now that I think of it, I'm not sure how you would implement the following functor term in operational: Action (a -&gt; (b, x))
yeah, it works with iceweasel 10.0.5 from squeeze-backports.
I've always wondered if when someone states such a definition, he includes the word "just" just to make fun of the other people.
You're right, that is a better way to write it. I was just trying to focus on showing how filter worked.
Usually you can do without a GADT, but how would you implement such type (from http://joeysmandatory.blogspot.fr/2012/06/explaining-prompt-monad-with-simpler.html) as a functor for Free? data Request a where GetLine :: Request String GetChar :: Request Char PutStrLn :: String -&gt; Request () Print :: Show a =&gt; a -&gt; Request ()
Actually, while most people seem to discuss GUI's and such my interests are in controlling autonomous agents. Reactive programming seems like a good way to encode the rules of action languages. Maybe this changes things. 
That would be data Request a = GetLine (String -&gt; a) | GetChar (String -&gt; a) | PutStrLn String a | forall p. Show p =&gt; Print p a
I'm pretty sure that stating the definition is meant as a joke (look how simple monads are when modeled as something that sounds more confusing!) Free monoids are just lists, and the category of endofunctors just represents effects, so free monads are just lists of effects.
 data Instruction a b x = Action (a -&gt; (b, x)) is this in operational: data Instruction a b r where Action :: (a -&gt; b) -&gt; Instruction a b a as far as I can tell.
Well, let's try `uncurry`ing `\a -&gt; \b -&gt; a + b`. uncurry (\a -&gt; \b -&gt; a + b) (x, y) = (\a -&gt; \b -&gt; a + b) x y uncurry (\a -&gt; \b -&gt; a + b) (x, y) = (\b -&gt; x + b) y (β-red.) uncurry (\a -&gt; \b -&gt; a + b) (x, y) = x + y (β-red.) uncurry (\a -&gt; \b -&gt; a + b) = \(x, y) -&gt; x + y (removed sugar) So the uncurried form of `(\a -&gt; \b -&gt; a + b)` is `\(x, y) -&gt; x + y`. Let's look at your examples: &gt; So add (x, y) = x + y in curried form would be (\y -&gt; x + y) Well, `\(x, y) -&gt; x + y` would curried be `(\a -&gt; \b -&gt; a + b)`, yes. &gt; curry f x y = f (x,y) &gt; uncurry f (x, y) = f x y Yeah, that's the definition of curry/uncurry I used before. &gt; curry's first argument must be a function which accepts a pair. It applies that function to its next two arguments. uncurry is the inverse of curry. Its first argument must be a function taking two values. uncurry then applies that function to the components of the pair which is the second argument. Well, the type of the `curry` we used was curry :: ((a, b) -&gt; c) -&gt; a -&gt; b -&gt; c
Aaaah, yes, that makes a lot more sense to me now. I think you're right about where I was getting confused. Many thanks!
Thanks for the help!
[Event-Driven FRP](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.7720) is a great example of FRP for robotics. The Arrowized FRP folks (who did Yampa) also did some work on robotics [[1](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.66.4431)] and (in case your interested) on music synthesis [[2](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.5312)]. Event-Driven FRP (E-FRP) is a domain specific language, designed with tight processor and memory limitations in mind. To get better performance, it used a hybrid event/behavior model for signals (just like Elm!) and compiled to a subset of C that happened to work with their robots (if I remember correctly). E-FRP stuck out to me in all the papers I read on FRP and it influenced Elm quite a bit. E-FRP and its predecessor [Real-Time FRP](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.1676) are both really cool, but it may be hard/impossible to get them running today. In any case, I bet robotics and music synthesis don't get as much attention because its a lot harder to get cool and repeatable results because people are all working on different hardware that runs in different ways. Being cross-browser is a decent challenge, and I imagine it's much worse if you want to make a general purpose tool for robots. If you want more info on the different FRP frameworks that have cropped up, see chapter 2 of [my thesis](http://www.seas.harvard.edu/academics/undergraduate/computer-science/thesis/Czaplicki.pdf). For info on how Elm relates to E-FRP and Arrowized FRP, see chapter 3. As soon as they make JavaScript-based robots, Elm will be all over that! :P
Looks like reactive-banana will soon pass your requirements (when the switch is implemented)
Not that I know of. That's the whole point of this post, I think.
Thanks for the links! I read through some tutorials but admittedly the one I found kept changing metaphors every paragraph (formula boxes, event boxes, then just events, etc IIRC). However reactive does look solid - the examples work! Is there a good paper or article which describes its semantics concisely?
Yep, the [experimental branch already contains switch](https://github.com/HeinrichApfelmus/reactive-banana/blob/develop/reactive-banana/src/Reactive/Banana/Experimental/Switch.hs) (allegedly non-working, and, anyways, reliant on the otherwise hidden construction monad). *Mostly* construction-monad less programming I can already do with sodium (apart from step, switch and similar, but it still leaks over all my code, as it's supposed to be uniform to be composable.
&gt; explicit multithreading Yeah, I remember having seen that now. That's a big asset.
Yeah, existentials suffice and are more appropriate, to signify that `Print` may hold any type of value without us specifying up front what kind. Notice that you can still remove the `Show constraint from tailcalled's type, giving: data Request a = GetLine (String -&gt; a) | GetChar (String -&gt; a) | PutStrLn String a | forall p. Print p a This would be more in the spirit of the free monad in that it doesn't specify what `p` is to be used for.
My plan is to not even expose that special kind system to the normal user. I think it's an implementation detail. Even if it's not all architecture dependent, it's certainly compiler dependent. 
Given the way Tiobe works probably gonna be more people asking why should they learn Haskell than people actually using it...
Oh yeah, you would need the constraint at the point of the data type otherwise there would be no way to infer from the free monad's type that the value stored in the Print was showable. I stand corrected.
If you are wondering what a free monad is, I post here the full description from the free package on hackage: Monads for free
&gt; I wonder if an approach similar to that of the Prompt monad could be employed for resource allocation and deallocation in iteratee-like libraries. Tekmo would talk about this better, but his library pipes (in the spirit of iteratees) uses internally free monads, which are computationally equivalent (but more formal) to Prompt.
&gt; Indeed, if I'd been as experienced now as when I came up with Prompt, I would have probably realized it was the free monad over the functor Would you have developped Prompt then? If yes, would you by basing it on a free monad?
&gt; There's the codensity transformation... Which is pretty much like the ContT monad. (But with universal quantification over the final result, usually called 'r', that must be important but I can't understand why)
Arrows: http://www.haskell.org/arrows/ http://www.ittc.ku.edu/Projects/SLDG/filing_cabinet/Hughes_Generalizing_Monads_to_Arrows.pdf http://www.cse.chalmers.se/~rjmh/afp-arrows.pdf
So [this](https://github.com/colah/Haskell-Re-Syntaxed) is what I'm thinking. I'd love feedback on it specifically as well. :)
I have a strong suspicion that this collection of criteria cannot be satisfied by an EDSL.
I like your suggestions. The question is: Can they be implemented without problems like collisions with other syntax?
Could you elaborate on the construction monad thing? I found a way to avoid the construction monad in `stepper`, `accumE` and probably `switch`, but I don't think it's possible to avoid it in `trimE` and `trimB`. Would that be enough for your purposes?
I think so and have mental ways to resolve the obvious collisions, but I haven't written a parser yet. So take it with a grain of salt.
&gt; Typeclasses? No, explicit dictionary passing from now on. Hm. What are the advantages of that? It seems pretty tedious to do... &gt; Add mixfix operators ala Agda That is a very cool feature.
Two things. I'd like a short syntax for record updates. Given some type like `data X = X { i :: Int }`, I'd like a shorter syntax for the update function `\x i' -&gt; x{ i = i' }`. Early-exit in `do` notation without extra indenting. main = do args &lt;- getArgs if length args == 0 then mquit else return () putStrLn (show . length $ args) putStrLn (show $ args) as a less-indent-y alternative to: main = do args &lt;- getArgs if length args == 0 then return () else do putStrLn (show . length $ args) putStrLn (show $ args) You can already do this with `ContT IO` instead of `IO`, like: main = runContT . callCC $ main' where main' quit = do args &lt;- lift getArgs if length args == 0 then quit () else return () lift $ putStrLn (show . length $ args) lift $ putStrLn (show $ args) but y'know, `ContT` is a bit scary and there's `lift` everywhere.
Nice! Native Android applications written in Haskell next?
Use `when` and `unless` instead of `if ... then return () ...` Even without those you do not have to indent that much: main = do args &lt;- getArgs if length args == 0 then return () else do putStrLn (show . length $ args) putStrLn (show $ args) Futhermore GHC allows you to write main = do args &lt;- getLine unless (length args == 0) $ do putStrLn (show . length $ args) putStrLn (show $ args)
You can use the Android NDK (http://developer.android.com/sdk/ndk/index.html) for developing native apps in C.
fyi I've been compiling 7.4.2r1 (obviously will switch to release) on the beaglebone. It still requires some patches to be applied which I've taken from the debian package builds. Looking forward to getting ghci working on the arm! Also as is mentioned in the link, the cross compiler is missing. This is really too bad since ghc is _slow_ on the beaglebone. Hope it makes it into 7.6!
(I guess that means you can have as many of them as you want?)
I don't like your use of `_` to mean "another argument on the LHS". It would appear to make it harder to work out how many arguments the function took.
Sorry, I had a brainfart, the example given is clearer now.
Oh wow I did not know that was there at all. That's a significant improvement, thank you. Does that work in H98 or is it GHC-specific?
&gt; I like writing type functions as, well, type functions. :) &gt; ... Starting datatypes with the 'data' keyword has the advantage that it is very easy to see what is going on. That's a good point. I'd only thought of it from the giving useful messages for screwed up programs perspective before. :P That said, I'm not sure how big an issue it is, especially since this isn't providing full inline type functions... &gt; Writing constraints as guards is also not really consistent with what guards usually mean. Could you elaborate? If type synonyms/families are type level functions, the constraints seem fairly analagous to guards. &gt; The most sensible thing is if _ scopes as tight as possible, i.e. to the next parentheses. Indeed. But a smart parser will think of the brackets in a tuple as different from normal brackets. So my mental implementation is that _ (_,_) = \a b c -&gt; a (b,c) where as _ ((_,_)) = \a -&gt; a (\b c -&gt; (b, c)) 
Wow. FP getting some recognition on the TIOBE blog.
&gt;That said, I'm not sure how big an issue it is, especially since this isn't providing full inline type functions... I am thinking from the point of a human reading or skimming a piece of code. If it looks like "data this; type that; class foo; instance bar" that gives me a quick overview of what is going on. At the start of the line all these things line up, and make the code look pretty. &gt; Could you elaborate? If type synonyms/families are type level functions, the constraints seem fairly analagous to guards. A guard means "if" you can have a function `f | guard = x; | otherwise = y`. On the other hand for a constraint there can not be an alternative. There already is a way to write constraints, namely as `(Constraint) =&gt; stuff`. You still need that one for type signatures. 
I should have been more careful in phrasing this: By "fully supported" I do not mean supported by GHC HQ. ARM is still not one of the Tier 1 platforms prioritized by the GHC developers. There remains a great deal of infrastructure work (e.g. getting LLVM patches upstream, setting up reliable buildbots, finding a qualified sponsor) which must be done before ARM can be considered a Tier 1 platform (and even then will only happen if/when GHC HQ feels appropriate). That being said, It Works For Me(TM). (or rather, did. As has been pointed out on the mailing list, the LLVM patch needs some forward porting love). Regardless, it is quite exciting.
But will *humans* think of the brackets in a tuple as different from normal brackets?
1. http://www.haskell.org/haskellwiki/Comparison_chain 2. if x \`elem\` [1,3] then ...
Not to mention *DoAbles are just monoids in the category of endoMapAbles* just doesn't quite have the same ring to it.
&gt;If you write native code, your applications are still packaged into an .apk file and they still run inside of a virtual machine on the device. I wonder how this would compare to doing a bytecode back-end for GHC.
DoAbles are List-y's in the function-y collection of self-MapAbles!
You might want to consider just using Agda. :p
Elaborate on your last point?
&gt; Different editors. I see much the same. It's all about twiddling 2D grids of ascii characters, then the weaving together of various static text files, then arguing over tabs and spaces. People forget that this is just an abstraction and none of it really exists. Something as essential as line numbering is a construct. It is just a count of invisible carriage returns after all. We have semicolons for the compliers and returns for people. Indentation is a big hack to get over the static grid structure of the abstraction. Scrolling around is struggling with some UI, as is having VIM impose line lengths. Or having to do a 'find all and replace'. We spray comments through the code just like visual formatting. We then impose style guides. The programmer has to deal with all manner of namespaces - a module system, os environment, file system. Going back to ~~line~~ the line numbering, if the 'atom' of the language was the function then the problem component can be referenced internally by name: Math.Geometry.foo. I think it is ridiculous that we still have programs broken due to spelling mistakes, a missed tab or typos. I shouldn't have to care about the geography in a virtual world (don't get me started on how the OS forces a user to be a pruner of cyclic directory trees and weeder of desktops). &gt; Version control. Version control is again one of those things that tries to over come text files. We have to shuffle file state around which could potentially affect many functions other than the target, when really we want to perhaps see the evolution of a particular function. Computers deal with source code and most of the time a programmer is in this other place with lots of good stuff layered on. The other point is discoverability and a lack of conveyance of program structure (indentation is a 'cure' for the latter). We have type systems as well that try and define the interfaces between components, but there is very little in the way of visual cues. I feel we are answering the wrong question most of the time, what's the best logical representation of programming. I think that is pretty much solved. Really though it's more about how to visualise, organise and map how everything fits together. Rather than showing us a text file, show us a visual representation constructed out of the individual parts. &gt; Fractal It would be nice to literally dive through a tree in a fractal fashion where the boundaries of a particular element have some conveyance, but I slide through the different scopes of the program. I'm not really talking about getting rid of the text file (defined as convient one dimensional serialisations of text), I just shouldn't have to see it.
It is a measurement which can be interpreted in many ways. Still not a random number. 
I second this!!! They are usually called Applicative Functors, and sometimes Idiom in the context of Haskell. But you can add effects to your program in an intuitive way that doesn't harm the elegance of your pure code. Maybe this is outdated, but the typeclassopedia is a good place to start.
I've commented on a few details, but as a general point, I don't think there's much of a motivation for a CoffeeScript-like project in Haskell. Unlike JavaScript, which can hardly change because it's a political minefield of compatibility between Microsoft and Apple and Google and Mozilla and whatnot, Haskell and GHC can and do change frequently and sometimes radically when good ideas come along. If enough people agree on a specific improvement, it's not too hard to get it included as an extension in GHC. Of course, that does requires getting some agreement. But I think you'll find the Haskell community to be (a) very open to positive changes, and (b) smart enough to point out when those changes are problematic for some reason.
While an easy problem to hit it is also usually easy to fix. This is the first time it made it past my basic sanity check line and out "into the wild" of actual use and not just something I bump into during development. Probably due to my reliance on the stream-replay testing which *did* effectively protect the user-visible portion of the app. I think the worst part is I had discounted that happening because I used a strict data structure - of course it never actually made it INTO the data structure ... I'm blaming 0200 on April 1st for that one. It was rather embarrassing.
Also, it implements an old version of the spec which is less pleasant to work with. Not decisive in it's self but it helped drive me off. (This is the sort of template system I've always preferred and gone out of my way to select.) For example, "To include another template, just reference that template like a function call with any arguments you need. Note that expr is either a template name or a parenthesized expression that evaluates to the name of a template. E.g., (templateName)()." - [ST4 spec](http://www.antlr.org/wiki/display/ST4/Templates#Templates-include) That last part is very beneficial. With HStringTemplate you end up having to write a long "if" chain where ever you include a large number of templates IIRC. This can add a fairly high overhead to using it. Of course HStringTemplate only attempts to implement the 3.1 spec.
In the common case where you expect a type function to be defined for all types in a class, putting the type family declaration inside the class declaration lets the compiler warn you when you forget to define the type function for an instance.
Multiline strings and string interpolation. I can basically get this with templating, but it's not as pretty as Ruby and feels like a very basic feature that any language should have.
I'm definitely a fan of switching over to a more Agda-like way of just admitting that type aliases, type functions, etc are all just functions. One of the things I love about official Haskell (H98, H2010,...) is that it did away with most of the keyword nonsense in SML/OCaml. Unfortunately that's all started to creep back in with the type families et al. We need to quash it before it spreads!
Yes.
&gt; Answer: The expression cannot be parsed, Why not? Python has it as well.
It's not entirely frivolous. The thing I find most problematic isn't just repeating myself by duplicating `x`, it's that in so doing I've broken sharing! Which means that I can't just desugar: 0 &lt;= e &lt;= 1 into: 0 &lt;= e &amp;&amp; e &lt;= 1 but rather, what I really want is: let x = e in 0 &lt;= x &amp;&amp; x &lt;= 1 And as if that weren't obnoxious enough, when I have a lot of this sort of thing going on then I end up floating the `x=e` out to the top (via `let...in...`) or the bottom (via `...where...`) to put it with other local variables, and now I've ruined the logical flow of reading the function. The wiki page g__ posted demonstrates that this could be solved in a library rather than in the language--- we just need a decent library for it already (ideally one which doesn't require sentinel values nor type signatures). Of course, it'd be a lot easier to make sure it's optimized if we had mixfix syntax like Agda...
So you basically like to type more?
Yes, the universal quantification is important (the name `r` isn't :) Notably, with the universal quantification in place, you can't define things like `callCC`. This is important for optimization and whatnot. Just like how list-fusion relies crucially on the rank-2 polymorphism of [build](http://www.haskell.org/ghc/docs/latest/html/libraries/base/GHC-Exts.html#v:build).
You can compile Agda to Javascript, if that's any "help". lol
[Javascript is not Java](http://kb.mozillazine.org/Javascript_is_not_Java)
And this one has quite a precedent! https://www.google.com/search?sugexp=chrome,mod=4&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=Free+monads+are+just+free+monoids+in+the+category+of+endofunctors.
What do you mean by `Thing`? The base functor of a free monad? A monad?
Documentation!!!
Python has [a dedicated set of comparison operators](http://docs.python.org/reference/grammar.html): comparison: expr (comp_op expr)* comp_op: '&lt;'|'&gt;'|'=='|'&gt;='|'&lt;='|'&lt;&gt;'|'!='|'in'|'not' 'in'|'is'|'is' 'not' 
Well, "re-arranging" is less powerful than "lambdas", because with lambdas you can duplicate or ignore your input. This messes with the kinds a little bit TakesTwo :: * -&gt; * -&gt; * TakesOne :: * -&gt; * TakesNone :: * Lambda X. TakesTwo X X :: * -&gt; * Lambda X. TakesNone :: * -&gt; * Not sure if the absence of dup/ignore is significant in terms of determinability.
If you ever get around to open sourcing digestive-functors-wai, I'd be happy to help with maintenance. I'm also pretty curious to hear what you've done with Hakyll, but I guess time will tell :-)
Yes, but you'd expect more. I discovered recently that using a type family not instanciated does not result in a error at the compilation: type family F a f :: F Int -&gt; F Int f = undefined I've never declared an instance for F Int, none is accessible from another module, but still it compiles (without even a warning). That's also a misleading behaviour. (turning F into a data family does not change it)
They just found that Logo is also a name of a TV channel; the improved search will be available from next month. I don't mind that Pascal is 10 places before Haskell, what I mind is that PHP and Perl is ~16 places before it :)
&gt; Would something like iframe be helpful to you? Nope. :) I asked about JS integration not because I had a particular application in mind, but because I don't trust you and your newfangled technology. :D I'm only half joking. Look, I found elm a few weeks ago via the list at [altjs.org](http://altjs.org/). I played around with some small changes to the demos, poked around in the docs, skimmed over your thesis, glanced at the source code, and came away quite impressed with the whole thing. Then I moved on to the next language on the list... What I'm saying is that Elm is just one of many tools competing for my attention. And it's still a young project, which leads to two problems: * there will surely be rough spots using it for real work * there's not much of a community yet to ask for help Now, add the fact that elm builds the entire application. This is one of its biggest potential strengths, but also a serious obstacle to adoption... At least it is for me. What can I make in elm that isn't a toy? I would be completely stuck attempting to make a simple guestbook application. I see how to add input boxes, but no button to post to a backend. No way to use the new HTML 5 local storage tools. Even inspecting one of your iframes in the dom is tricky, because there's nothing to uniquely identify any of the components. For example: the [animated clock demo](http://elm-lang.org/edit/examples/Intermediate/Clock.elm) appears to create a completely new canvas element with a fresh id attribute on each tick. As far as I can tell, an elm app is a black box, and nothing can get in or out. Which means that all elm appears to be good for *at the moment* is making presentations or acting as a teaching language. Now suppose I want to do more than that. I basically have three options: * write my extensions directly in elm * find some way to get it to talk to javascript * edit the haskell source code The first and last options have the highest costs. In elm, I've got to write everything I want from scratch. If the community grows, then this cost will start to shrink... But it's going to come from lots of people in little bits and pieces over time. If I extend elm in haskell, I have to pay the cost of understanding how your compiler works. I assume your thesis will be helpful there, and I'm the kind of guy that would actually sit down and read it... But it's still a big investment of time and energy. But: if you allow javascript integration, you give potential the ability to draw from all kinds of different tools and languages that already work with browser technology. You'd be minimizing my learning curve because I could route around any problem I have with elm (either because a feature is missing or because I'm not yet educated enough to understand it) by falling back to some other tool. If you do *that* for your potential users then we don't have to put nearly as much trust in you *or* your newfangled technology.... Which means we're that much more able to trust *ourselves* to find a solution to our problems through elm. Anyway, that's why I asked about the JS FFI. Sorry for writing such a long post. Does any of that make sense? :) PS: I hope it doesn't seem like I'm putting your work down, because I'm not. You've got a fantastic piece of technology here, and it seems like you have the intelligence, enthusiasm, and interest to make something out of this. I don't have a lot of time on my hands, but if there's anything you could use some help with, feel free to ask.
I learned programming before university/college, and in high school they never used chains of comparison like that. Upon first seeing it, I thought _that makes no sense, you're comparing a bool and a float!_, and it took me a surprisingly long time to be able to quickly read 1 &lt; x &lt; 5 as "x is between 1 and 5". That may be mostly humorous. But in general, I think Haskell is going far enough in adopting mathematicians' customary ways of expression, even when programmers' ways are obviously more suitable. (At least we aren't using greek letters for identifiers).
Right, because a -&gt; (b, x) = (a -&gt; b, a -&gt; x)
Nice answer from Daniel Fischer. I did not know about Debug.Trace.trace before now.
self reply, got it built (with above mentioned patches applied), but ghci segfaulting now, will have to look around to see if I did something wrong. *edit* I was able to build ghc, with working ghci using the newly updated debian 7.4.2 from darcs. http://anonscm.debian.org/cgi-bin/darcsweb.cgi?r=pkg-haskell/ghc;a=tree The compile took over a day, and the haddock generation failed (possibly out of memory). I had to change the build.mk to disable that portion. This board is only 256mb of ram, all in all pretty impressive that this all worked. 
Sorry for the delay :) I just added keyboard support ([Keyboard.Raw](http://elm-lang.org/docs/KeyboardRaw.elm)). The char-codes of `charPressed` can be converted to chars with `Data.Char.fromCode` ([Data.Char)](http://elm-lang.org/docs/Data/Char.elm). If you have the latest version of the compiler, just download the latest elm-mini.js ([here](https://raw.github.com/evancz/Elm/master/elm-mini.js)), and you should all set. I am working on Elm's module system and an FFI for JavaScript right now. When these are done I'm going to ask people to help provide Elm libraries that hook up to useful JS projects, extend the standard lib of Elm (Map, Set, etc.), and whatever else people want to do. I'm working as fast as I can, but there is always tons more stuff to do. On the topic of asking for help, if you end up giving the game another try now that you've got the keyboard input, I'd be extremely grateful if you posted it online (even if it is really small or simple). Maybe with some explanation or documentation. I know Elm is limited in some aspects, but I think a Elm-based game could be a really cool example. I'm sure people would like to see it in /r/haskell. /r/programming and /r/javascript would probably be interested as well. I am getting more and more people asking to see larger applications, so this would be *extremely* helpful!
Sure, I'll send it in your direction. Probably take me a few days though. Mostly I haven't because I wrote it and then you changed the base library and I've been meaning to look at the changes and consider updating.
No, that is definitely better. Feel free to do code review. Not my best code to review but I'm sure I'd still end up better for it.
Yeah. For xkcd's numbers, I'd probably not want to do any template rendering on the fly. I might imagine that the latency issues in hstringtemplate itself would have to do with nested template calls and similar logic more than just plugging in a few holes? If so, I imagine moving to more modern data structures could be a real win there. But even then, the template engine is an interpreter and not a compiler (although a stringtemplate compiler *would* be pretty sweet), and so there's some necessary and unavoidable overhead. w/r/t the syntax, if you have any concrete proposals to make (for template calls or otherwise) please let me know, and I'll be happy to look into it.
&gt; Typeclasses? No, explicit dictionary passing from now on. Explicit dictionary passing is not equivalent. Type classes guarantee (as long as you don't use orphan instances, which the compiler warns you about) that the instance is unique in the whole program. This means that e.g. we can implicit efficient set union, knowing that both sets use the same Ord instance for Ints.
&gt; without Template Haskell Errr... It uses template haskell through data-lens-template, doesn't it? Plus it links with it. But it's really great, though! Applicative make such a lovely and simple syntax for that job ;) It becomes now my library of choice for that task. My plea with TH is that it grows a lot the size of the executable. (The example with Sample from README.md weighs 2,5Mo once compiled) *EDIT:* After a little check, every option parsing library from hackage seem to use TH.
I would make it a lot more Agda-like.. but with less unicode. Though, a lot of things things I like go a bit beyond mere syntax, such as being able to open a module over a limited scoped.
yes. Section 7: http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/history.pdf
One major problem with the first three, I think, is that it makes optimizations that factor out repeated expressions unsafe. E.g. the compiler could no longer factor out two uses of `[0..]` in case they are compared for identity. This problem doesn't affect the last optimization though, which would maintain current Haskell semantics AFAICS.
This is not specifically related to Haskell at all...
Oh, you're right. Fixed.
Haskell hackers don't hate parentheses, they just broke their parenthesis keys when they were still using Lisp. You tend to avoid them when you need to copy &amp; paste the buggers every time.
The (==) operator is an algorithm, not a proposition. Object identity is something Haskell actively avoids. In its absence, the compiler is free to futz around with things, substituting equal things (equal the proposition, not via the (==) operator) with equal things. The problem with your idea is that it's too weak an optimization with too great a cost. Consider that "let a = [0..] in a == a" is actually a loop. Maybe looping is the behavior you want. (Maybe compare it to "while (1) { }" in C). Optimizing it to True (or in the C comparison, a no op statement) may change the behavior of the program. But even if programmers use a function "loop :: a" to loop when they need it, this optimization is still anemic. Consider the statement "let a = [0..] in a == map id a". During evaluation, "map id a" will create an equal object (equal in the proposition sense) with a *different* identity. Your optimization is useless. Or maybe it's "map (f . g) a" where f and g are inverses. Or maybe they are inverses for all the inputs you're interested in. Haskell doesn't know these things. And so, for optimization purposes, they might as well be false. Don't think about object identity. That's your brain using procedural logic. You're above that now. 
They don't hate parentheses. They just wrote a language that doesn't need as many. Why? Currying makes it so requiring any parentheses requires a LOT. Compare: -- with parens map(double)(numberList) -- without map double numberList Operators bulldoze them. -- no operator, with parens plus(2)(3) -- operator, with parens 2 `plus` 3 The REAL problems with reading Haskell are: * Record types suck really bad. * The module system sucks equally really bad. * Hindley-Milner is a double-edged sword. (Leaving off top-level type signatures is assholish). * Haskellers choosing stupid names for global things. * Haskellers abusing operator overloading. * There are a million and one extensions to standard Haskell. * Fancy types lead to split hairs. (Sometimes you just need a concrete implementation, not the most general solution). * Having three ways to define types (data, newtype, and type) means you have to grep for all three to find a declaration. 
Do note, however, that [GHC.Prim](http://www.haskell.org/ghc/docs/7.2.2/html/libraries/ghc-prim-0.2.0.0/GHC-Prim.html#g:19) provides `reallyUnsafePtrEquality# :: a -&gt; a -&gt; Int#` Presumably, it might produce false negatives, but never false positives. I myself have never actually used it, but that's what I'd expect. So yes, some level of optimization might be possible if you limit yourself to GHC only, but you still can't *rely* on pointers being the same in any situation where you think it should be true, because optimizations and garbage collection have free reign to muck about with such things.
That's legit, I think. This sort of thing actually comes up with mapping newtypes and extractors, and I could imagine some rules in core might improve the situation a bit. It feels a bit ad-hoc and at the wrong level for that sort of thing though...
&gt; Having three ways to define types (data, newtype, and type) means you have to grep for all three to find a declaration. Perhaps we should change `data` to `datatype` so that all you have to grep for is `type[space]`
The $ operator is useful in situations where you want to apply something to a function i.e: map ($ value) xs --Very useful with partial application zipWith ($) fs xs --Or just as another function
&gt; Another example is the $ operator, which apparently serves no purpose except to avoid having to type parens. It is often used to avoid parens, but it also serves an interesting purpose when used in a [section](http://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-300003.5): applyFour :: (Int -&gt; Int) -&gt; Int applyFour = ($ 4) -- alternate definition applyFour f = f 4 The latter defines `applyFour` as taking in a function, and then passing 4 to that function. The former defines it as the section representing, quite literally, "apply four", in much the same way that `(/ 2)` is the section "divide by two". Back to the "avoid parens" concept, I've seen this a lot in Yesod example code: countWidget &lt;- textOutput $ jsShowInt $ jslength todos ----vs---- countWidget &lt;- textOutput (jsShowInt (jsLength todos)) After a little while of using `$`, you just get used to it meaning "gather everything to the right, form an expression, and then apply it to this function". Parens, commas, and square brackets can limit this, but again, these are things that you simply get used to, just like you got used to the rules about + * and ^ in math. It is also common when used with functions that require a monadic input foo $ do ... It is convenient in this particular case to let indentation handle the job of indicating where the do block ends, rather than having to put a close paren foo (do ... )
As usual, when this sort of thing comes up, I wish for less noisy syntax for GADTs: data Foo a where Bar Int Char Whatever :: Foo Int instead of Bar :: Int -&gt; Char -&gt; Whatever -&gt; Foo Int which introduces a lot of extra noise and gets little benefit from doing so. (It does allow type classes to be inserted, so that's something.) But the proposal would actually solve that: Foo a = data Bar Int Char a | Monad a :: Foo Int works quite well, I think. EDIT: I can do better. The type parameter's name serves no purpose for GADTs. For regular ADTs, it is in scope for each branch and prevents the need for a type signature. For GADTs, it can be replaced with the parameter's kind, but since I think ADTs and GADTs should use the same syntax, an underscore should be fine. Kind signatures shouldn't be needed in most cases, especially if Conor McBride's "suspenders of upwards mobility" are used: Baz = data Baz Foo _ _ = data Bar Int Char a | Monad a :: Foo Int {| Baz |} -- Here we mean Baz the constructor, not Baz the type. 
This will start a whole new debate, but I'll also point out that when you're repeating the same thing 15 times in a row (for example, writing instances of a type class for all of the standard mtl monad transformers, which I did recently), keeping everything on one line and aligning common pieces of structure can make it far easier to visually verify that the same thing is happening each time. In those cases, I think there's justification for using some point-free stuff that would be excessive in a single definition.
The problem is that kind of change is 1) highly non-backwards compatible and 2) its usefulness lies outside of the language's type system most Haskellers wouldn't be able to recognize it as an actual problem.
&gt; parentheses in `f x x' ^! y` are no more helpful to me (even without knowing the `^!` operator) than they would be in `1 + 1 == 2` to you Without knowing the `^!` operator, how do you know whether that expression is equivalent to `f x (x' ^! y)` or `(f x x') ^! y`?
Yep. Unary function application always has higher precedence.
Or simply: application has the highest precedence. This way you cover both term and type applications.
&gt; The problem with a template Haskell (or CPP) code generation approach is that you either end up with orphan instances, if you use associated data types How about an `instantiateUnlessInstanceExists` TH function (using `reify` to look for an existing instance)? That leaves it indeterministic (depending on compilation order) in which module the instance will actually live, though. Edit: Nevermind; this doesn't actually work if one module imports two independent modules defining the same instance.
How do you "apply four"? What does 4 do? :)
&gt; That's your brain using procedural logic. You're above that now. Love this!
&gt; Maybe compare it to "while (1) { }" in C Funny you should mention that -- nonterminating loops invoke undefined behavior in C and C++. Removing the loop in its entirety is a legal optimization (which, IIRC, gcc might do on certain loops, at certain optimization settings)
It's legal in a few languages. However, it lives in a moral grey area of compiler optimizations. It is legitimately a semantic-altering optimization, since it might widen the domain of the function it's in. But my second point is more weighted. It's an extremely weak optimization. You could spend months bloating the compiler with special-casing, but the compiler size (and runtime) will grow faster than its usefulness.
Of course, there are few optimizations that get you much that would be so effected. I think it's funny that you say it changes the semantics of the program. In practice, on some particular compiler build, yes. According to the spec, there is no change in semantics... undefined behavior has arbitrary semantics, widening already included. This might be comparable to the more practical optimization or removing dead stores, whereby the program `void f(int x){int* p = 0; *p = x; }` accepts nothing (crashes) normally, but accepts everything in optimized builds (nop, empty procedure).
This would be be why we call C "unsafe". It's only a partial spec :P
No one hates them. Their disappearance is just a natural consequence of precedence rules. Function Application has highest precedence. If you remember that then its easy to read your example: f x x', y Apply till the first operator: (f x x'), y
There are costs and benefits to every syntax decision. It's complex. But I think the real reason is simply that Haskell is a descendant of ML. And ML took parentheses to the opposite extreme, in reaction to its ancestor LISP.
There were no specific reasons for chosing JuicyPixels... I read about it somewhere and it fit my requirements... since this is my first 'real' haskell project I played with the code and thought I could make it work, so I did not really research alternative libraries.
The point is that you can use the tail to build new sequences, or take a look at other elements than the next one. It is crucial that ViewL/ViewR return a Seq again.
Not being a big user of if then else, I come at this from a different direction; I want pointfree case statements. Or in other words, the ability to feed any value through pattern matching and guards without first binding it to a name.
Infix is also application... It's less ambiguous to say "prefix application" maybe?
Very nice. I really like this approach.
&gt; Perhaps the better phrase would be "apply to four". It would. Scala's notation (*disclaimer: I don't actually know scala*) looks like a method call on a function object, so I'm going to reject that outright as a different paradigm from Haskell. We do not inhabit the "kingdom of nouns". Haskell is much closer to mathematics, or to lisps. In both you would say: (f 4) Or "eff four", perhaps "eff of four". A mathematician would describe this as "applying (the function) f *to* (the value) 4". Never the other way around. `apply` in Lisp and Scheme is used like this: (apply f 4) So "apply *xxx*" implies that *xxx* is a function. Direct and indirect objects are distinct for an important reason, even if English doesn't make a big deal about them (by changing articles and pronouns in these positions, as say German), it is *vital* to talk with precision. You're not the first person I've seen using the direct and indirect objects of `apply` the wrong way around, but this is the first time I've ranted about it. Please stop! *not trying to put you back in your box; your explanation to psyker was quick and excellent. I liked your use and explication of "left/right sections". But language is important, and you gave me an opportunity to say my piece.*
Great, please let us see it!
Well, the point is that the security flaw stems from two problems shared by most current "mainstream" languages, both of which are addressed directly by Haskell. One is that "equality" is defined as comparison of pointers and physical memory contents, rather than actual semantic equality. The other is type system weaknesses that allow automatic casting and other infidelities.
I find myself gravitating slightly away from do and slightly towards the operator-filled stuff over the years. The main reason is fear of burning my fingers with some variable name. I think what happens is that your idea of what is “readable” shifts a bit as you grow accustomed to things. For example, I used to roll my eyes at code containing `&lt;$&gt;` and `&lt;*&gt;` thinking it was just being clever and show-off-y. Then I had to write some JSON parsing code and I realised how much nicer it is to say `f &lt;$&gt; mx &lt;*&gt; my &lt;*&gt; mz` rather than `do { x &lt;- mx; y &lt;- my; z &lt;- mz; return (f x y z)}` So sometimes it's just show-off-golf… but other times, it may be legitimately trying to improve code quality (at the cost of unfamiliarity to less experienced Haskellers)
In the elm-happstack and elm-yesod examples I'm using a CloudApp hosted version at http://f.cl.ly/items/2e3Z3r3v29263U393c3x/elm-min.js
Object identity is more his brain thinking about machines, than about procedural logic. It's perfectly sensible to have references in a purely functional language and do comparison along references as well as along structure. It's just highly atypical.
&gt; I believe that for this issue, all developers currently agree the best solution for output resource management is to use a monadic region, such as that provided by regions or ResourceT . This is currently the only approach guaranteed to run cleanup code with complicated monad stacks in the presence of exceptions. I disagree. In pipes-core, if the base monad `m` supports exception handling, then `Pipe a b m` has guaranteed finalization. 
Scott-encoding plus newtypes?
No, a thousand times no. &gt; because in some sense, four is a function in this context. `($ 4)` is a function. `\f -&gt; f 4` is a function. `4` is four. Saying "four is a function" is not only wrong, it's ridiculously stupid-headed and the kind of thing that makes people run screaming thinking Haskell is complicated. I'm sorry if I come off abrasive, but you really are making words meaningless.
I thought "enumerators" was deprecated in favor of conduits?
&gt; iteratees solve the resource management problem for input, but don't address it for output. `conduit` already solves it for output, too, assuming the invariants are maintained. `pipes-2.0` also solves bidirectional resource management (but not yet exception safety). &gt; I believe that for this issue, all developers currently agree the best solution for output resource management is to use a monadic region, such as that provided by regions or ResourceT . While I have not completed my solution to this, I do not agree. Also, the only reason `conduit` uses ResourceT is for exception safety, not resource management. `conduit` doesn't rely on `ResourceT` to properly finalize resources under premature termination. &gt; And iteratees are quite composable (perhaps too much so!), solving the other issue as well. No. I can safely say that only Paolo (last time I checked) and I have completed `Category` proofs for our libraries' composition operators (`pipes-core` and `pipes` respectively), although Paolo's requires certain invariants. Declaring your library compositional does not make it so, and I can say with certainty from the type signatures of your library's composition operators that they do not form a category (no downstream identity, at a minimum): (&gt;&lt;&gt;) :: (Nullable s1, Monad m) =&gt; (forall x. Enumeratee s1 s2 m x) -&gt; Enumeratee s2 s3 m a -&gt; Enumeratee s1 s3 m a Notice how there is no pipe you can set as the second argument to form an identity, since you can't generally unify `forall x . x` with `a`. As a side note, this is yet another reason why I love Haskell, since there is so much you can learn about a function just by looking at its type. My insistence on a `Category` isn't trivial. For example, the identity laws of category theory are what guarantee correct behavior at the "ends" of the composed argument. The way I like to think of it is that the identity laws make the notion of finiteness "sensible" by filling in the all gaps with `id`. So when you compose a "finite" number of arguments like: f . g . h You can think of the "empty" space to the left and the right as being filled with `id`s: ... id . id . id . f . g . h . id . id . id ... ... but moreover, you can also think of the space between `f`, `g`, and `h` as being filled with an arbitrary number `id`s, too: ... f . id . id . g . id . h ... So `id` makes the problems of finite-ness and "punctate-ness" completely disappear, leaving one infinitely large object with no distinguishable internal or external boundaries. Finiteness is particularly important in your case. The lack of a downstream identity means that you will encounter corner cases at the boundary between the downstream end (i.e. the iteratee) and the "emptiness" beyond it since there is no longer a meaningful `id` to define what exists "beyond" the downstream end. Anywhere you have a boundary like that where you can't dissolve it with an endless stream of `ids`, boundary cases begin to appear. From a computer science standpoint, categories exist precisely to eliminate boundary cases, since any object that forms a category is guaranteed to have no external or internal boundaries. You noted this yourself where you have difficulty getting iteratees to handle finalization as well as enumerators, but perhaps you don't appreciate that this stems directly from the lack of a downstream identity.
I don't think the definition of exception safety is really up for debate. If it's possible to leak resources in the presence of exceptions *somehow*, the code isn't exception safe. This was one of the major motivating cases for moving to iteratees in the first place: in those libraries, it's absolutely impossible to leak a file handle acquired via `enumFile` (or the equivalent). And yes, that means the code you've copied in is actually a bug in `resourcet`. It's an extreme corner case (which is why it's never been caught), but I'll include a patch soon (it's very easy to fix, just need a `seq`). Thanks for the report. __Edit__: Relevant patch: https://github.com/snoyberg/conduit/commit/95527c11019b770bf9f806eb821087b0ddae011d
&gt; conduit already solves it for output, too, assuming the invariants are maintained. pipes-2.0 also solves bidirectional resource management (but not yet exception safety). Resource management without exception safety is easy. I can implement that in `enumerator` right now, and I'd be shocked if `iteratee` was any different. It's only in the world of auto-termination where non-exception-safe resource management is a challenge. &gt;&gt; And iteratees are quite composable (perhaps too much so!), solving the other issue as well. &gt; No. I can safely say that only Paolo (last time I checked) and I have completed Category proofs for our libraries' composition operators (pipes-core and pipes respectively), although Paolo's requires certain invariants. I don't know where this definition of "composable" as "it has a Category instance" came from, but I think a lot of people (myself included, and I'm guessing John as well) disagree. It's very easy to take two components from `iteratee` or `enumerator` and combine them together. We're not speaking of strict mathematical composition, we're speaking of composition in the "good programming" sense. &gt; You noted this yourself where you have difficulty getting iteratees to handle finalization as well as enumerators, but perhaps you don't appreciate that this stems directly from the lack of a downstream identity. Again, this has everything to do with *exceptions*, and nothing to do with prompt finalization: `iteratee` can handle that just fine. You're actually missing the point of why this is impossible: it's because in the iteratee paradigm, the enumerator controls the flow of execution, so it has the ability to wrap an entire code block in `bracket`. The `iteratee`, on the other hand, is the callee, and therefore has no such ability. This has nothing to do with any `Category` laws.
Of course it is possible to leak a file handle, in any language that is Turing complete. Just enter into an endless loop and that is an asynchronous exception no library can recover from. More practically, segmentation faults, power loss and memory corruption are all asynchronous exceptions that cannot be caught. So while it is nice to be able to catch some classes of asynchronous exceptions, it is not possible to catch them all, nor should we depend on being able to do so.
Can you go into a bit more detail, for those of us who haven't used a language with metavariables? I think the auto-lambda interpretation is more motivated by some kind of linguistic intuition than by consistency. It would be nice to be able to say both take each ingredient and (add it to the bowl) take each dish and (add salt to it) with equal ease. Currying makes things nice when the noun you're abstracting from a phrase is at the end, but in other cases you need to resort to `flip` (or more involved gymnastics) to avoid single-use names. Of course, natural languages can get away with leaving the the scope up to the listener. Having to pick a particular rule might make this an unintuitive feature in a programming language. 
Haskell wiki: [Applicative functor](http://www.haskell.org/haskellwiki/Applicative_functor)
so, that is the secret of chinese cyberwarriors!
If you modify my library you're not depending on my library, you're depending on your fork of my library, and it's up to you to maintain it. That's fine with me. But we still seem to be talking past each other: I'm saying the language should not allow implementation hiding to be circumvented; you're responding that implementation hiding can be circumvented from outside the language, therefore libraries should export more of their internals. And I agree! Libraries in many cases should export more of their internals. But that doesn't touch the question of why implementation hiding should be easier to circumvent. (As an aside, Template Haskell really can see unexported constructors, though some people consider it a bug. Have you considered using it?)
Yeah, I will have to come up with a better way to articulate my intuition. The concept that is foremost in my mind when I think of identity is the lack of internal and external boundaries (and thus no boundary cases). However, I can't clearly articulate why the identity laws guarantee that, yet, which is why I struggle to explain it. What you are saying is completely right, though.
&gt; You might also want to kill threads doing pure computations. Although cooperation is not strictly impossible in that case, it makes the code impure and less composable. On the other hand, asynchronous exceptions make MVar code more error-prone (and less composable?). It seems like there should be a separate combinator for forking pure computations. Killing pure computations at any point is not a problem, so that would be allowed, while things actually doing IO would be forced to cooperate. But I don't have a good answer to blocking IO (which was part of the rationale for concurrent haskell in the first place). Thanks for the link, I'll take a look.
&gt; Application is always higher precedence than a binary operator, so it must be `(f x x') ^! y`. Thanks. That's very good to know. FWIW: despite over 20 years of experience with C, I actually do find this: (1 + 1) == 2 easier to read than this: 1 + 1 == 2
Yeah, pretty much. They're *necessary* for interrupting other threads' blocking IO, but also useful for interrupting big pure computations without refactoring all the loops to poll something. The [paper](http://community.haskell.org/~simonmar/papers/async.pdf) also mentions stack and heap exhaustion, but that seems like a thing that almost nobody can or should bother to deal with other than by dying. *(edit: this is what I get for leaving a tab open and not refreshing before responding.)*
Did you know that you can write everything in Haskell without ever using parentheses, even in helper functions? You can! http://www.usma.edu/eecs/SitePages/Chris%20Okasaki.aspx#jfp03 Edit: drat. it seems the pdf has disappeared from the internet.
Sounds like apples vs oranges? Or I don't understand. Yesod is a web framework for Haskell, and .NET is a general software framework/platform, with many languages and many web frameworks.
I use undefined. Holes are still a nicer solution.
&gt; I think it says something that order of operations are defined in math, and there is obviously a reason for it. Saying "that's the way math does it" is kind of funny when you consider the fact that in math function application has parens around the parameters. &gt; In my opinion, its more meaningful to train the eyes to look for certain operations first (knowing that they bind stronger) than counting parens, which I believe you can only get so good wtih. If there were only a handful of operators to worry about, OK. In mathematics you generally only need to know the precedence of five operators: addition, subtraction, multiplication, division and exponentiation. Beyond that either parens, bracketing operators (eg: floor and ceiling) or typographical cues (eg: the horizontal rule for division, or font size/placement for subscripts and superscripts) are generally used if there's any possibility of confusion. Even when I don't know many of the operators involved or their precedence I can generally at least parse most mathematical expressions into an expression tree. The same is not true of most Haskell code I've seen. It's usually riddled with unfamiliar operators, and because parens are rarely used the reader needs to know the precedence/associativity of a lot more than just five operators.
&gt; Your first example makes it very clear that foo is a function of one argument without even reading the right hand side. If a better name than x was chosen I might even have some idea what kind of argument foo takes. If it's a top level declaration, then there should be an accompanying type signature that gives you most of the information you are asking for. If you see `foo x = bar (baz x)`, you're not guaranteed that it's a function that takes only one argument: it depends on how many arguments `bar` takes. &gt; The second definition tells me nothing about what foo is unless I read the right hand side, know the types of bar and baz, and how . operates on those types of arguments. Entirely true. My point is that sometimes it is simple to read the right-hand side, the types of bar and baz are well-known, and `.` is categorical composition, which intuitively "composes" things in almost the same way no matter what sort of thing it is. (and 99% of the time, it's function composition.) I'm not saying that you should *always* seek to eliminate temporary names (indeed, entirely pointfree code can often be dreadful to read), but *often* there are these little situations where I believe it actually improves readability to remove them.
I read in some category theory book that category theorists have the unfortunate reputation of going to conferences listening to a talk and then at the end saying: "Isn't that just &lt;category theory term&gt; in the category of &lt;_&gt; though?" So I always thought it was a play on this reputation.
Oh, I absolutely agree. I just hadn't quite wrapped my brain around the English language side of things. I think of `($)` as "apply" because, in prefix form: (($) f 4) it's just like the lisp/scheme example. I suppose if we have a code phrase of the form "[command] foo [preposition] bar", then the correct way to explain the right section would be "[command] [preposition] bar", e.g. "divide X by 2" =&gt; "divide by 2", "apply F to 4" =&gt; "apply to 4".
Example please.
No, I'm not suggesting that Haskell should emulate the shell. Only trying to point out that there are other computer environments that also eschew parentheses. As for "improve clarity" I think that is far too subjective a criteria. Improve for who? By who's tastes? For who, at what level of exposure or facility? There is no real objective stance one can take on parenthesis: You must get used to the domain of discourse, and spend a long time with the notation, and then get a feel for what conveys meaning best - and that is dependent on what you are trying to express, and to whom. This is true in algebra, programming languages, physics, or any field that must extensively notate.
Yep, just to amplify this comment, the comma is indeed not an operator but part of the syntax `(x', y)` for tuples (and various other syntaxes such as list literal syntax). The surrounding parentheses are also part of the syntax, and not being used in their regular role of demarcating a subexpression. So it's quite impossible to mistake `f x x', y` for `f x (x', y)` because the former has no parentheses at all. (By the same token, `f x x', y` is not a valid expression on its own--it needs to be contained in parentheses or square brackets or something like that.) You might be accustomed to Python, which is different in this regard (I'm not sure whether the comma would be regarded as an "operator", but in any event you can write `x = 2, 3` and it means the same as `x = (2, 3)`).
A cool thing about haskell is that if you forget the precedence, you can always ask (in GHCi): Prelude&gt; :info (+) class (Eq a, Show a) =&gt; Num a where (+) :: a -&gt; a -&gt; a ... -- Defined in GHC.Num infixl 6 + Prelude&gt; :info (==) class Eq a where (==) :: a -&gt; a -&gt; Bool ... -- Defined in GHC.Classes infix 4 == Prelude&gt;
I find the former more difficult to read.
From the Haskell 98 report: &gt; A string may include a "gap"---two backslants enclosing white characters---which is ignored. This allows one to write long strings on more than one line by writing a backslant at the end of one line and at the start of the next. For example, "Here is a backslant \\ as well as \137, \ \a numeric escape character, and \^X, a control character."
Would you call the odd numbers "even" just because they are isomorphic?
The point is that when you see a start-parenthesis, the matching end-parenthesis might be anywhere. When you see a dollar, it can only mean one thing. The situation with data-lens can be a little confusing at first because you have basically a 3-ary operator. I don't think this is something which is commonly done. It *can* also be written without dollar and with parenthesis: `((height ^= 180) . (width ^= 260)) window`.
Here's my shot at cli arg parsing (using arrows): https://github.com/Paczesiowa/hsenv/blob/9c4871f669dd515ad636afb712af637c24f7afeb/src/Args.hs
Haskell is not category theory.
And if the base monad m is a "complicated monad stack", do you know that it supports exception handling properly? It's easy to wrap a cursor safely when the base monad provides the correct primitives, but what if you can't rely upon that? That's what a monadic region gives you.
&gt; Step 3) I would combine type declarations together under a single keyword. Not "data", "type", and "newtype". I would probably use "new datatype" for newtype, "datatype" for "data", and ax "type" all together. Now grep works to find type declarations. Eh, what's wrong with `egrep '^(data|(new)?type)' Foo.hs`? &gt; Step 7) All top-level terms must be declared with a type signature. Yes, Hindley-Milner is great. But just because total type reconstruction of your program is decidable does NOT mean I want to spend my time deciding it. I don't want to have to load your Git Hub source code into my REPL to get the types of shit. I want it there in front of me. Maybe this means the language comes with a tool that will annotate it all for you. Maybe you just need to man up and do it. It's not that hard. I'd support the tool solution, but I'd say two things: 1. Writing type signatures for code that uses methods from complicated class hierarchies (e.g., `Num` and friends) is annoying as hell. It's usually quicker to let the compiler infer it. 2. Even in the "simple" cases, requiring the programmer to write type signatures will often result in them writing types that are less general than they could be. This sounds innocuous at first, but [more general types have subtle advantages](http://blog.malde.org/posts/polymorphic-types-are-safer.html).
This is one reason I hate language or compiler specific build tools. Real build tools can build in parallel. Use them.
&gt; Formal semantics often models bivalent propositions as functions that range over truth values, and there is such a thing as trivalent logic, so it's not wrong to see a == a as a proposition. This model seems needlessly weak. Propositions ought to be supported by proofs or refutations (or the open pursuit thereof). To call a proposition "True" or "False" is to discard that vital information. To include a tertiary state is to speak sternly about the abilities of the logic in question. &gt; undefined == undefined does not denote True Did you mean 'undefined' in the codomain or the domain? The problem still remains, even if the inputs are guaranteed not to be bottom-valued. It's an issue of deciding equality of infinite objects. 
clckwrks, taking the OO out of web development ;)
There is some Template Haskell I've seen for string interpolation. Still, some flat out sugar for "lorem ipsum #{ foo $ bar baz } etc" would be nice (precise syntax up for debate). Could just desugar into "lorem ipsum " ++ (foo $ bar baz) ++ " etc" Or some sugar on top of printf: "I ate %d{ someNum } apples" could desugar to printf "I ate %d apples" someNum
I want this too. Dumb example: x &lt;- foo case x of Just x' -&gt; bar Nothing -&gt; baz ==&gt; foo &gt;&gt;= lambdacase Just x -&gt; bar Nothing -&gt; baz
In that case, wouldn't you need 3 `'` in front of `f` to indicate 3 applications?
I could easily imagine a hlint-like tool that rejects code that uses List sugar, typeclasses, records, parens, and deriving. I could not easily imagine many Haskellers subjecting themselves to these restrictions voluntarily.
Honest question: is there some technical difficulty I am missing that has been making it hard for **ghc ---make** and **cabal** to build in parallel? It doesn't seem to me like this should be hard to implement, but since it hasn't been done despite demand for it for several years, and I know that the people working on GHC are incredibly smart, there is obviously some factor that I am missing. :-) (Heck, I've even rolled together my own build system that had no problem at all building Haskell modules in parallel; the main problem that I ran into was that I got too fatigued from jumping all of the hoops that you have to go through *after* the compilation process to package everything up nicely so that it fits into the Haskell package system to finish it --- compared to that parallel builds involving multiple languages were incredibly easy! :-) )
GHC has some state that's hard to parallelize at this point. Cabal is about to get parallel builds. I believe Duncan has merged some of the patches already.
See the SO question I asked millennia ago. http://stackoverflow.com/questions/1513020/how-do-i-do-python-style-indent-dedent-tokens-with-alex-haskell
What would you do if your private work is different but it has a related work at your current workplace? I don't say that this is the situation in my case, it's just a question to think about. 
There is a problem with your post. It is not on top of the page yet.
You can also do it with Shake (http://hackage.haskell.org/package/shake) however unless you get &gt; 3 times parallelism it will go slower, since GHC has various in-memory caches (package metadata etc) that need reloading on each fresh GHC call.
But that would also reject do notation, which is too useful to just remove.
Whoops, fixed.
If you're learning programming for the first time, yeah, you've got no choice. But if you're coming from another language you should be able to build on that, in as far as that's possible with something as different as Haskell. But we shouldn't make it harder than it needs to be. &gt; One of the biggest pitfalls of Monad tutorials is trying to compare Monad to things that the reader is familiar with; this often leads to confusion and misunderstanding. Yeah, maybe I should have specified that things should be named so that people who are encountering them for the first time can relate them to something they already know *in a way that is accurate*. Obviously if you mislead people you don't help them learn faster. That's about the worst thing you can do. But I thought that was obvious.
I always wonder why in-memory caches are built from some slow/raw disk format. Why not build the correct data structures on disk and use them directly from there (e.g: via mmap) and let the OS do the caching part?
First of all, your first example is still not valid: you have to use `mdo` instead of `do rec`. The answer to your question is that not only there are performance implications, but the semantics will actually change in general. For example, if you print `baz` inside the rec block, it's likely that the computation will diverge. However, `rec` is now meant for advanced use, when you really know what you are doing. Normally, you should use `mdo`, which will do automatic segmentation (i.e. split into minimal mutually recursive blocks), so you need not worry whether you write statements inside or outside the block.
Yeah, assertion failures are generally not considered uncovered.. It is meaningful to measure 100% coverage of code that is *supposed* to run. If you expect that code to actually run, ever, you'd really really want it to run in your testing sandboxes before it runs at some super-important site.
I imagine that verifying that the test suite is of sufficient quality will be difficult to make “an entirely automated process” (this seems to be an important goal of the first two levels) unless *some* arbitrary benchmark – 100 % coverage or other – is chosen.
Well the bugtracker they're using is a plugin for the framework. So I imagine that as things progress, their tracker will become much nicer as well. When possible, it's always better to dogfood.
&gt; I did read the thread in the ticket, so I know the answer, This is all a bit over my head — I’d never encountered the recursive do construction before — but does the name refer to “minimal”? Still feels rather strange to me.
Right, that's exactly what I meant in my first reply.
&gt;Why would you prefer to avoid creating a temporary name? for the same reason that in C++ you would write this-&gt;foo()-&gt;bar()-&gt;baz() and not: a = this-&gt;foo(); b = a-&gt;bar(); c = b-&gt;baz(); &gt;The second definition tells me nothing about what foo is unless I read the right hand side Of course you have to read the right hand side. &gt;know the types of bar and baz, and how . operates on those types of arguments. (.) is function composition a well known concept. it says foo takes what baz takes and outputs what bar outputs
It gives a warning here: {-# LANGUAGE TypeFamilies #-} type family F a f :: F Int -&gt; F Int f = undefined main :: IO () main = putStrLn "Hello, world!" crabgrass:~/programming% ghc -Wall test [1 of 1] Compiling Main ( test.hs, test.o ) test.hs:6:5: Warning: Defined but not used: `f' Linking test ... If you try to fix this warning by referencing `f`, then it really turns into an error: {-# LANGUAGE TypeFamilies #-} type family F a f :: F Int -&gt; F Int f = undefined main :: IO () main = putStrLn (f "Hello, world!") crabgrass:~/programming% ghc -Wall test [1 of 1] Compiling Main ( test.hs, test.o ) test.hs:9:22: Couldn't match type `F Int' with `[Char]' In the return type of a call of `f' In the first argument of `putStrLn', namely `(f "Hello, world!")' In the expression: putStrLn (f "Hello, world!") zsh: exit 1 ghc -Wall test
tl;dr Haskell programmers don't hate parentheses, they hate having to implement a stack in wetware.
Wow! We got three paragraphs! Apparently, I gave the impression that the whole of Haskell was imperiled with the fate of the Hacker Dojo. But I like that the article stated it was a powerful tool.
Yeah, 100% coverage is definitely not the same as no bugs. For example this reverse function is almost completely broken and has 100% test coverage: reverse :: [a] -&gt; [a] reverse _ = [] reverse_test :: Test reverse_test = "reverse_test" ~: ([] @=? reverse []) The level specifications are definitely not set in stone. For the lower levels, I think it is important that it be something we can do in an entirely automated way. wordpress, for example, currently has 19,831 plugins. At that level the metric for 'sufficient quality' has to be fairly objective, even if it is a bit arbitrary. But, perhaps the bar can be much lower -- like at least 30% coverage. Or, something else not based on coverage at all? Or maybe it is used as a weight value between 0 and 1? We face the same issue with hackage already. Any can upload a package to hackage -- which is great! The barrier to entry should be low. But, then where do we go from there? Some things we can measure objectively like: 1. does it compile? 2. on average, how many days does it take for the author to fix compilation failures 3. does it have a test suite? 4. how good is the coverage? and do the tests pass? 5. how many releases have there been? 6. does it cost money? There can also be some self-selection measures such as: 1. does the author allows us to notify them when their plugin stops building? 2. has the author marked it as unsupported, deprecated, etc? 3. did the author take time to include screenshots and other optional material? 4. has the author made any sort of opt-in (though, non-binding) commitment to supporting the project for some period of time? There can be user-supplied feedback: 1. number of downloads 2. how long the average user uses the plugin (assuming there is a way to measure that) 3. updown/votes So, the common thread to all these metrics is that it does not require the clckwrks team to invest any of their own time. Hence, it can scale to 20,000 plugins. At the same time, automation can only get you so far. Which is why the higher levels with more human judgement are valuable. As I mentioned, the problems faced here are not really any different than the growing pains hackage is facing. So hopefully we can come up with some techniques that can be applied for hackage as well. 
So...many...acronyms...
I would at least consider rather than having a hierarchy, having simply yes or no for various attributes. Also, 90% code coverage or any particular 9x% coverage metric might be better. 100% always makes me nervous because it disincentives me to create good error messages for the "this can't happen, I don't even know how to test it" cases, which, admittedly, is a significantly lesser problem in Haskell than it is in my usual professional languages. Leave a bit of slop in there for those and you're probably still doing well. On the topic of "difference from my usual languages", 100%ish coverage in Haskell probably also implies a lot of tests that are just testing things the type system is testing. Is there anybody here who can show a non-trivial module they've achieved 100% test coverage for, and have good reason to see it as a net gain? (Oh, and I like where you're going with this. This is definitely in the spirit of trying to find a good solution and not nitpicking you unto demoralization.)
mdo is ASCII for $\mu$do; $\mu$ suggesting the least-fixed-point operator from domain theory. (As opposed to $\nu$, which typically signifies a greatest-fixed-point operator.)
When you'll have a large number of plugins, it would be very interesting to see the number of modules that compile with the new version, but their functionality is broken. That would be a good real-world quantification of the saying "if it compiles, it works" :)
I'm a bit unimpressed that after the effort to remove an ugly keyword, and one which affects layout too, it now seems to have snuck back in. I've read the ticket and see the desire for syntax which supports segmentation; I don't have an opinion on the appropriate semantics, but couldn't a way have been found that keeps "do" as the monadic layout signifier? I was happy with "do rec". 
"do rec" is still available, if that's what you want. But it no longer does segmentation; so you have to be careful in putting rec blocks.(In fact, the whole reason "mdo" is back is because people weren't happy "rec" was doing segmentation.) I don't understand your comment about "affects layout"; "mdo" is subject to the precise same layout rules as "do"; what's the impact?
It is, but only if {-# Language RecursiveDo #-} is at the top of the file. Will tools/editors etc know about this? Does it make explaining layout rules to new users (or computer algorithms) reading Haskell code easier? I have to say I've used neither "do rec" nor mdo in anger myself, and I'm very happy for people who want both auto-segmenting and manually segmenting fix points to get their features; I would just prefer things if a way could be found such that language extensions don't change the list of keywords which trigger layout subject to whether they're enabled (and that in general the list should remain as short as is feasible).
&gt; Another example is the $ operator, which apparently serves no purpose except to avoid having to type parens. That's not entirely true. You can use the right section as the type-lifting combinator **T**, i.e.: f x ===&gt; ($x) f This is especially helpful for things like passing `($x)` to `fmap` and the like.
&gt; The interaction between the juxtaposition for application and the infix operators with who-knows-what precedence/associativity is the main thing that drives me crazy about reading Haskell expressions. Why? it's so simple. Function application via juxtaposition has precedence higher than any possible infix operator. (Juxtaposition goes up to 11.) Therefore infix ops can't interfere with juxtaposition in any way. This is far simpler than what you get in other languages (Perl, SML, OCaml,...)
Same is true for many other things already: magic-hash, pattern guards, view patterns, proc (arrow notation), etc.; all becoming available when some pragma/flag is appropriately given. Some tools know about some of these, and some don't. I agree that the an inflation in the number of special syntactic patterns/keywords is undesirable, but GHC also serves as a research language where many ideas are tried out and experimented with. Strictly speaking "do" should take care of recursive bindings; just like we don't have a separate letrec. But that's a whole another can of worms.
The big place I include extraneous parentheses is for mathematical formulae with uncommon operators (or with operators that have no commonly agreed precedence). The reason is exactly one of clarity. Yes, I know what the associativity is, and anyone familiar with the code base does too, but it often helps to be excruciatingly clear exactly what is meant. But then this is, in part, a reaction against other mathematical software which is unclear about what it really means (especially in areas like statistics and linear algebra, mathematicians are horrible at actually saying what they mean with exactness and without overloaded syntax). But for everyday coding, there's not much need for this sort of thing. I agree about record syntax's implicit parentheses being bizarre, though. Worse than pattern matching is the use on the right hand side. f Foo { bar = b } just looks wrong on so many levels.
The sets of functions O( n*(n+1)/2 ) and O( n^2 ) are the same. [Wikipedia](http://en.wikipedia.org/wiki/Big_O_notation).
When everyone is familiar with the conventions then implicit is always clearer than explicit. This is just a fact about human perception and linguistic practice. Clutter is confusing, and if someone goes out of their way to say what doesn't need to be said then clearly they have some hidden message. Trying to discern hidden messages, especially from strangers, is always fraught with difficulty. The problems arise when everyone is not familiar with the conventions. Considering that Haskell follows the typical grade-school conventions for arithmetic operators, and the Boolean operators seem the same as in any sane language, the question is: which infix operators do you find so confusing?
*Asymptotically* the same ;) Alright, I'm nitpicking.
The story refers to a Haskell event at Hacker Dojo that had its wings clipped by the problems the Dojo is having with the city of Mtn. View. The Dojo is open for business and working through its issues. BTW I am coding in Haskell at the Dojo nearly every day. 
The problem is, this is the way mathematicians often talk. It's a common form of syntactic overloading, albeit one I find dubious in most circumstances. Just google for "group action" (hint: group elements are silently lifted to mean multiplying by that element), "linear operator" (hint: matrices and the operation of multiplying by a matrix are willfully confused), or anything of that ilk.
Except that most of the standard library for ML is uncurried
I would say nitpicking poorly, since the whole point of big O notation is to describe the asymptotic behavior of functions. EDIT: This reply seems a bit snarkier than I intended, so I'll clarify. O( f(x) ) is a *set* of functions. For example, the functions g(x) = 1, h(x) = 18380924, and j(x) = 0 are all in the set O(1), and equivalently, O(2), O(12309852), etc. Also, O(1) is a subset of O(x), and O(x) is a subset of O(x log x), and O(x log x) is a subset of O( x^2 ). So when I say O( n*(n+1)/2 ) and O( n^2 ) are the same, I mean exactly that--they are sets that contain exactly the same functions.
Also, why are you even checking for changedness of sprites? It'd be better to have a queue of the things which have changed, and then whenever you make a change you add the sprite to that queue. No need to check for anything; you know a-priori that everything in the queue has changed (or is in need of proving that it has not). Moreover, in lieu of a global queue, you can have separate queues for each locality (a la quadtrees and octtrees) so that you can (a) notice when an entire region doesn't need updating, and (b) compute the updates in parallel. You should check out [hashlife](http://en.wikipedia.org/wiki/Hashlife) to see why tricks like these are such a good idea.
Nice work!
A strictly accurate way to put would be "amortized O(1), assuming you look at every instruction exactly once", right?
Not with Codensity! newtype Free f a = Free { unFree :: forall r. (a -&gt; r) -&gt; (f r -&gt; r) -&gt; r } instance Monad (Free f a) where return a = Free (\ret alg -&gt; ret a) m &gt;&gt;= k = Free (\ret alg -&gt; unFree m (\a -&gt; unFree (k a) ret alg) alg You do need Functor to implement liftFree though... liftFree :: Functor f =&gt; f a -&gt; Free f a liftFree f = Free (\ret alg -&gt; alg (fmap ret f))
Yes, indeed.
Excellent!
Huh? Sjoerd has it right, I think.
Free monads don't impose any laws either (or rather, the laws are completely defined by the base functor). I'm pretty sure I can construct an example where free monads are strictly more powerful.
People make the same statements about beginner v experienced in all languages. I still avoid depending on operator precedence in other languages no matter how much experience. The more concise forms aren't more readable. With experience you'd just be more practised at reading them. With practice the explicit forms are fine as well. I don't want to think about precedence at all. It is a pure distraction and irrelevance. Every moment spent thinking about it is a pure waste for something I can permanently postpone at no cost. I don't want to get experience at learning an arbitrary set of rules. I want to not have to think about it at all because it really doesn't matter.
In Java yes I tend to write like that. It isn't a matter of preference to me. I cannot slip up by being specific. I see people accidentally slip up when depending on precedence all the time. It might be once a year or something but the cost of chasing it down is much greater than the cost of being explicit and learning to read it. Though this might be less of an issue outside of the madness of say C. If things nest so much that the parens get in the way I break down the code. In Haskell I tend to write a lot of "let x = this and y = that in z" if I'm struggling to navigate the parens and x and y are meaningful concepts. Generally my opinion is your function is too big if it becomes a debate about whether parens or operator precedence is superior.
ooh TIL, thanks
Let's try to work at it from both ends, my friend :) But let's be fair to inferior toolchains. Flat text is the lowest common denomenator of programming languages. The "lowest" part gives it the "inferior" label. But the "common" part is important too. Grep works just as well for finding Java declarations as it does for Python declarations: not amazingly, but often decently. Often, when downloading a new code from Github, I want to poke around a bit before I decide to install it and run it (since for many projects, setup is very difficult). I might want to look at some key parts of the code to see how they are laid out or how they work. For this, grepping is just well-suited for the job.
There is now an experimental mirror on github: https://github.com/clckwrks/clckwrks
I actually find the github page insanely busy and hard to grok. Of course, the clckwrks bug system is currently far less powerful. And, the theme for it is pretty much non-existant at the moment. So there is plenty of work to do there.
&gt; When everyone is familiar with the conventions then implicit is always clearer than explicit. If you ever have to maintain other people's code you quickly learn that "everyone is familiar" pretty much never happens. Even reading your own code 6 months later can be difficult if to much is implicit. &gt; Clutter is confusing, and if someone goes out of their way to say what doesn't need to be said then clearly they have some hidden message. That seems like an argument for requiring parens in Haskell. Then people won't try to second guess why they're there... :-) I agree that clutter isn't good, but I don't think making the structure of your code explicit is clutter. &gt; Trying to discern hidden messages, especially from strangers, is always fraught with difficulty. That's exactly my point. Fixity rules *are* "hidden messages". &gt; The problems arise when everyone is not familiar with the conventions. Considering that Haskell follows the typical grade-school conventions for arithmetic operators, and the Boolean operators seem the same as in any sane language, the question is: which infix operators do you find so confusing? What grade school did you go to that taught you about `.`, `^^`, `⋆⋆`, `:`, `++`, `&gt;&gt;`, `&gt;&gt;=`, `$`, `$!`, `&lt;|&gt;`, `&lt;$&gt;`, `&lt;$`, `&lt;*&gt;`, `&lt;*`, `*&gt;`, `&lt;**&gt;`, `&lt;=&lt;`, `&gt;=&gt;` and `=&lt;&lt;`? These come from Prelude, Control.Monad and Control.Applicative, apparently required knowledge for Haskell programmers. And since the fixity rules for these are so *obvious*, other libraries must avoid these operators, or at the very least use the same fixity rules, for operators that look the same, right? Like Parsec would *never* have an infixr 1 `&lt;|&gt;` when Control.Applicative has the same looking operator as infixl 3.
&gt; You might be accustomed to Python Yes, good catch. I am indeed accustomed to Python (where comma *is* an operator, though a kind of weird n-ary one), and I think that threw me off when trying to read that Haskell code. I guess I should have realized that the parens in the original code must've been required, or they would have been left out... ;-)
I wonder if this is because back then they didn't fully "grok" currying. The very early Java libraries similarly show a lack of understanding of object-oriented programming. It's pretty clear that some of those early libraries were examples of "writing FORTRAN in any language".
That is the plan. I started clckwrks-plugin-darcs today. The first piece will be source browsing with syntax highlighting.
Since I'm not on G+: ekg!
Wow I was wondering on "haskellers have bad naming skills"(somedays ago this was around), but this is pretty clever.
Web frameworks are probably the most obvious solution to these things. Sure, there are a million and one web frameworks out in the wild, but Haskell frameworks have some "killer features" that I think will attract users. They also have the potential to fill the "brain-dead framework" requirement, though some more massaging may be necessary to get them to that point. The only thing I think is really lacking right now is some more solid tools to facilitate hosting and deployment.
Woohoo! This is an awesome project!
The OP is the Director of Client Projects at FP Complete. 
&gt; what does :t or :info not provide? I want to be able to look at top level things in my editor and figure out what they do without reading their implementations, and without having to load them into some other tool.
I think Haskell source with extensions is the only non obvious TLA http://hackage.haskell.org/package/haskell-src-exts
&gt; Brain-dead frameworks that a novice straight out of college can use for their projects with very little investment (including learning the language). If this is a requirement, why does Java's and C#'s web frameworks have such success? IME, it's got to be the hardest way of developing a web application ever invented, and I was certainly not qualified for it straight out of college.
I don't know that it's the best idea to encourage anyone to use Haskell without really learning it. Of course there are various definitions of what "really learning it" means but if you're going to use Haskell, it would be good if you met at least one of them.
I agree with what you say about getting to the mainstream (i.e. level of adoption on the scale of C/C++, Java/C#, Javascript, PHP). However, is that really the best metric? I'd much prefer increased adoption among people who produce useful stuff and put it on hackage. In that vein, adoption more like Python and Ruby have looks more interesting. I think that in the most popular languages there is a comparatively high percentage of people who don't produce anything useful to other programmers.
They were released by well-established organizations who already had networks set up to promote their product, money to fund it, and a pre-existing market share (even Mozilla had Netscape). Haskell isn't like them; i don't think its road to success will be down the same path.
1. I don't think you can learn Haskell with little investment. For one thing, monads cause a lot of drop-outs and they're a very necessary part of the language to do anything non-trivial. Frameworks that in some way require little knowledge of the language just seems unlikely to me. 2. It does, but mainstream programmers still don't turn their heads from what they're accustomed to. Perfectly safe multi-core usage, good developement environment, nice typing system, very proactive community, etc. Haskell needs more than to *be* superior, it has cultural prejuduice to counter. Most popular languages right now have C-based syntax. I'd imagine that a programmer who has only been exposed to C-languages would find anything else wierd (i used to). I didn't really groc Haskell until i learned LISP first. 
&gt; Step 12) I would bring over the concept of "holes" to the language (see Agda). Being able to write a bit of your program with holes where you need to "think about it", type check it, then fill in the holes -- is the greatest thing since the cons cell. This is the best thing about Matita. Once you're used to putting question marks everywhere and having the refiner figure out what you mean (or just open a goal because it can't work it out) it's impossible to give them up.
I had more luck developing web applications in Perl (which I didn't really know at all), than in Java. I think there's got to be a reason the Perl, Python, and Ruby frameworks/web development libraries are so much more popular than their Enterprise Java equivalents.
But you have to use a language to learn it, no...? I presume you meant something along the lines of "don't bet a company on it until you're skilled," which is fair. But if someone is *going* to bet a company on a language they're not expert in, at least in Haskell they'll have a much harder time writing the sort of blow-up-on-you-later mess that beginners in other languages write.
Google Chrome got there by being advertised on Google's home page. Every project you mentioned owes its success to Google sponsorship.
I think what he means is the situation for Rails, where you have tons of Rails developers who don't really understand Ruby at all.
I had really bad experiences the last time I used attoparsec (roughly a year ago). Not only were the error messages incredibly useless, but in many cases the behavior was flat out incorrect (i.e. backtracking worked completely wrong, despite an incredible effort I made to get it to work at all costs). Contrast this with parsec where the exact same parser worked correctly. It got to the point where if I wanted a high-speed parser I would just hand-roll it using ByteString/Text primitives. The way I saw it, this didn't give useful error messages and wasn't composable, but neither was attoparsec, and at least this way worked.
Many languages are large enough that people rarely learn them fully (e.g, C++). They limit themselves to features they need and understand. Take Java, for instance. Java generics are nice, but if you're writing idiomatic Java (i.e, not something like Functional Java), you typically won't have to _write_ a generic class of your own at all. Their main use is in collections, and Java has excellent default collections. The skills you need to master a language in the _problem domain_ is different from the ones needed to master it in the library domain. Sure, it's nice to be able to roll a few useful libraries of your own, but most programming jobs in the world are with specific libraries and frameworks - domain specific languages in a sense. You can see it as a disaster for Ruby that the domain specific language/framework/library Rails spawned so un-idiomatic code, but from the perspective of those flocking Rails users it isn't - as long as they're productive. If they aren't, that is arguably a problem with Rails. So, is it true of e.g. Haskell monads what is true of Java generics? That you can use them productively without necessarily knowing (well) how to roll your own? Can you use Yesod to build a webapp without being remotely able to develop your own reactive programming framework? I think so. If having a deep understanding of all the babies of category-extras is necessary to be productive in Haskell, then it will never be a mainstream language.
I don't know what this does either, but it compiles: data Contish a c = Contish ((c -&gt; a) -&gt; c) eval :: Free (Contish a) a -&gt; a eval (Pure a) = a eval (Free (Contish f)) = eval (f (\c -&gt; eval c)) evalOp :: Program (Contish a) a -&gt; a evalOp (Return a) = a evalOp (Contish f `Then` k) = evalOp (k (f (\c -&gt; evalOp (k c))))
I have had the opposite experience with back-tracking: it just works in attoparsec, whereas in Parsec I have to constantly think about where to tell it to back-track. Is there a parsing library with automatic back tracking and good position information?
Cool. Anyway, is `Free (Contish a) a` isomorphic (yes, isomorphic, not bijectible) to `Program (Contish a) a`? How about `Free (Contish a) b` vs `Program (Contish a) b`?
For some cases, getting position can be even easier. Since attoparsec returns the tail of the input, you know how far you are from the end and only need to subtract, then scan to count characters. \* heh, as noted by the first comment on the OP...
I think it is by design. -- This combinator is provided for compatibility with Parsec. -- Attoparsec parsers always backtrack on failure. try :: Parser a -&gt; Parser a try p = p
We're going to start using the TPS Monad, so instead of returning a Report, your function should return a TPS Report.
Just don't rise him from the grave. He had enough of world domination in his time.
I still believe that corporate backing is big in IT. IBM, Oracle, Sun and others pushed the Java platform. They sold it to other large corporations and now a lot of large corporations use Java. I don't even think they looked at the technology. And to be fair, Java is a complete solution for a lot of different things. For example, Open Forth isn't a complete solution because it doesn't have a lot of libraries and no one is using it. Java is stable in many different areas. So, since Haskell is stable and complete and can be used for general purpose development, you could have large corporations pushing the Haskell platform. And the corporations might need to tweak parts of the Haskell image. I see a lot anti-windows among Haskell users. Well, in the corporate world, you need to push Windows, especially windows, Mac and Linux. ... Every large corporation I have worked for (I am a enterprise type guy) is the same: Mostly Windows and some Linux/Mac for desktop development. Mostly Java and .NET web-application/business technology. And the occasional hackers doing side projects that use python, ruby, something-else (plug in haskell here). There is no real reason why Java or .NET is king except that the momentum is with those technologies. And the occasional IBM or Oracle software rep goes across the globe selling those technologies. 
You *could* define an `(==)` operator that behaves like C's, but you'd have to hide the Prelude to do so. As for `1 + 1 &lt;&lt; 2`, that's exactly the sort of thing I mentioned elsewhere on this thread about operators which don't have a common precedence outside of the language in question. The issue here is that you're mixing paradigms: arithmetic vs bitvectors. While that's common in C, it's hardly something with a universally agreed upon resolution.
The beauty of being "commercial" is that until you "go to market" you get to ask more questions than you have to answer. I will say that you are asking the right questions.
i discovered the sinatra-like wai library scotty through that link. wonder why i haven't seen it before, it looks great!
I've hacked extensively on a happy parser (in language-javascript) and a Parsec parser (in the disciplined disciple compiler and other projects) and I find Parsec parsers: * easier to get going * easier to work on * easier to debug * easier to tweak to give good parser errors. YMMV. 
&gt; For IO, can't you just use some low level unix tricks to grab on to stdout and stdin and do something particular with them? Yes, but which UNIX tricks would that be precisely?
GHC.IO has a bunch of stuff to duplicate file handles, allows you to redirect Haskell stdin and stdout to files while running a Haskell program. I've used it with hint before and it works.
Yes. freeToOp :: Free (Contish b) a -&gt; Program (Contish b) a freeToOp (Pure a) = Return a freeToOp (Free (Contish f)) = Contish f `Then` freeToOp opToFree :: Program (Contish b) a -&gt; Free (Contish b) a opToFree (Return a) = Pure a opToFree (Contish f `Then` k) = Free (Contish (opToFree . k . f . (. k) . (. opToFree))) `freeToOp . opToFree = id = opToFree . freeToOp`, `eval . opToFree = evalOp` and `evalOp . freeToOp = eval`.
Isn't that the LISP folks?
I have a few doubts about this test. I haven't had the chance to discuss with ericmortiz, so I'll say them here for public discussion. * If a new connection is spawned every 1ms, shouldn't there be 300k connections at the end of 5 minutes instead of 10k? I don't know Erlang but I scanned for the number 10000 as a hard-wired cap somewhere in the client application and did not find it. * EC2 is said to have a 100k packets/sec limit on its network adapter, in/out combined. This would mean it couldn't hold more than 50k connections at the most between 2 instances. Relevant: http://blog.rightscale.com/2010/04/01/benchmarking-load-balancers-in-the-cloud/ * What kind of scenario could result in snap being very fast in connection time, but totally failing in establishing half the connections? It doesn't really make sense to me - you would expect a more linear starvation profile. Edit: I see now that the 10000 limit is indeed there. So perhaps my first 2 questions are already answered.
nitpick: (forall a. (a -&gt; b) -&gt; f b) ~ f a Your `forall a.` is in the wrong place (latter `a` is out of scope). Does this equivalence hold even if `b` and `f` are both monomorphic?
I'm bit sceptical. How would you define data Eq a b where Eq :: Eq a a
Fixed it. I meant the following: (forall b . (a -&gt; b) -&gt; f b) ~ f a ... which I think answers your latter question.
This topic is already pushing the limits of my understanding, so I'm probably going to get this wrong, but my first guess would be: data Eq a b = Eq (forall x . (x -&gt; a, x -&gt; b)) Edit: Actually, I like this [other answer](http://www.reddit.com/r/haskell/comments/v5ewa/gadts/c51iy8z) better.
Let's consider two empty datatypes data True data False You should be able to define `Eq True True` but not `Eq True False`. It is impossible to do this by looking only at values of type True and False. Edit: perhaps `forall f. f a -&gt; f b` should work. But it's a different approach than the Yoneda lemma.
I agree, but in a programming language without `undefined` you still should be able to define a function `absurd :: Void -&gt; a` by empty pattern matching. Therefore a and b are isomorphic `Iso absurd absurd :: Iso a b`, but not equal.
Yeah, at best you can do isomorphism and not equality, unless you somehow forbid `absurd` or empty pattern matching.
I can almost do it. Would you consider this solution acceptable? data S x = S x -- The only difference data Equal a b = forall m. Refl ((m, m) -&gt; (a, b)) symm (Refl f) = Refl ((\(a, b) -&gt; (b, a)) . f) inj (Refl f) = Refl ((\(S x, S y) -&gt; (x, y)) . f) Again, the same caveats apply as I mentioned in the post. The existence of undefined or empty pattern matching ruins the constraints. Edit: Or alternatively, lose the constructor for `S` and provide some means to define a function of type: pred :: S n -&gt; n And use that function in the implementation of `inj`. Edit #2: Or define pred within the module where `S` is defined and then hide the constructor for `S` so you can't pattern match against it. I don't know if that would count, either.
It's reasonable to consider Eq as a GADT with one parameter and one index. Consider `(Eq a)` to be the predicate which captures "being `a`". It's a bit like defining vectors with no cons, then abstracting over the type level Z{-ero-}. From that viewpoint, you get data Eq a b = Refl (a -&gt; b)
I meant analogs of `absurd` for `True` and `False`. Of course you can join them like this: class Initial e where absurd :: e -&gt; a data True data False instance Initial True where absurd x = case x of {} -- pseudo-Haskell instance Initial False where absurd x = case x of {}
This looks like an incomplete/inaccurate rephrasing of the fact that GADTs are equivalent in power to type equivalence constraints combined with existential quantification. The Yoneda-inspired transformation looks like just an awkward way of expressing the existential (and in fact, I think for your length-indexed list you said it used `Rank2Types` but actually used `ExistentialQuantification` without realizing?). Without the equivalence constraints, I think you can't express `Equal` with your technique at all. Here is a way of doing it with equivalence constraints: data Equal a b = (a ~ b) =&gt; Refl Here's a length-indexed list type: data List a n = (n ~ Z) =&gt; Nil | forall m. (n ~ S m) =&gt; Cons a (List a m)
It seems to me that with this encoding, any two inhabited types are equal. This is certainly not a faithful rendering of the corresponding GADT. Why do you think these functional presentations of equality constraints make sense? GADTs are not endofunctors on *. If I define data OnlyBool b where That's :: OnlyBool Bool it is not sensible to expect that `OnlyBool` has a sensible definition of `fmap`: as I can define a function of type `Bool -&gt; ()`, I'd expect to get an element of `OnlyBool ()`, which really shouldn't happen. Correspondingly, the conditions necessary for deploying the Yoneda lemma may not always be in place. GADTs are functors, but from the discrete category |*| (ie, same objects as *, but the morphisms are only the equations, not the functions) to *. See the POPL 2008 paper by m'colleagues Ghani and Johann for more detail on this way of seeing things.
Ok, what about types whose constructors are not exposed? In other words: module S (succ, pred) where data S n = Succ { pred :: n } succ = Succ For types with no exposed constructors (like the above `S` example) and no `undefine` or empty pattern matching then you can restrict the set of morphisms from the given type, however you are still correct that a type like `Bool` where the constructors are exposed cannot be restricted, at all, in which case the GADT is strictly more powerful. However, the convention with dependently-typed programming is to use empty types without constructors anyway. Edit: So my point is, yes you can't do the Bool example, but since it is an index inhabited by only one type, it is isomorphic to defining it to be a different index with no constructors exposed for that index.
&gt; Without the equivalence constraints, I think you can't express Equal with your technique at all. If I restrict `Equal` to types with no exposed constructors, no `undefined`, and no empty pattern matching (I know that's a long list of caveats; humor me), what would be wrong with: data Equal a b = forall m. Refl ((m, m) -&gt; (a, b)) Edit: And yeah, I meant `ExistentialQuantification`.
Let me try to understand the plan. Use the function space in place of a notion of equality, but restrict the availability of functions so that only those corresponding to valid equations are definable. It does make sense to treat the type-level things that we use for indexing as morally a different kind of thing to actual types of values, so we can stop worrying about (Bool -&gt; ()) as somehow meaning an equation. The idea is to construct a subcategory D (for "data" or "discrete") of * such that D and |D| coincide. It's perfectly fine to say that types in D must be constructed in a particular way, hiding constructors, etc. If you're going to try to make that work, you need to stop asking "what about..?" questions and trading this or that example, and give a systematic treatment. Check that for your construction of D, the functions really do give you the discrete category, and you're sorted. Make sure that the equality GADT works out sensibly for types drawn from D, because really, if GADTs offer anything extra, it's equality. But I don't think it'll be easy (in fact, I rather suspect it won't work). The function space is readily reflexive and transitive, but symmetry is tricky. And if you expose more functions, like succ and pred, in order to deliver the necessary injectivity of constructors and substitutivity of equal for equal, you may make more things equal than you intend or can handle. Remember, if you're the one claiming that "GADTs are just such-and-such by translation", you're the one who needs to give a general translation scheme for datatypes and pattern matching programs. And then it's up to the sceptics to find a counterexample.
I understand. I obviously don't think my treatment is realistic (because of the requirement to avoid `undefined` and empty pattern matches, which I consider impractical for typical Haskell programming). I was just excited because I had my first major leap in developing an intuition for GADTs and thought it might help other people understand them better, too. It's not like I'm trying to publish a paper. I just try to write my blog posts at the moment I learn a subject and the freshness of it is still on my mind so I can speak like a beginner to another beginner, because I often find that the more comfortable and familiar I am with a subject the greater difficulty I have in formulating it in terms a novice might understand.
As I've just said in a different reply, I'd be quite happy if you could show me a GADT translation scheme which yielded an equality that worked just for types specially constructed to represent indices.
Yeah, it's mostly useful for understanding GADTs that are intended to be endofunctors (like the `operational` example I gave). Some people brought up the `Refl` example in the previous reddit thread where I asked about GADTs but only now do I appreciate what they meant when they said that `Refl` was the nub of what a GADT was for.
why no exposed constructors?
&gt; You are just using different syntax to write the same thing. Correct me if I'm wrong. I think this is correct. &gt; While we're on the subject of things you can do with equality constraints though, with Rank2Types and equality constraints you can encode GADTs as functions with a simple CPS transform Right. Higher rank types can encode existential types.
This, combined with disallowing empty pattern matches, is the only way I know of to restrict the set of permissible morphisms.
&gt; GADTs are functors, but from the discrete category |*| (ie, same objects as *, but the morphisms are only the equations, not the functions) to *. Right, and if we apply the Yoneda lemma in this category, which I'll write `Disc`, it tells us there is an isomorphism (forall a. (a `Disc` b) -&gt; f b) = f a, so then we write down types like data List a n where Nil :: (Z `Disc` n) -&gt; List a n Cons :: a -&gt; List a m -&gt; (S m `Disc` n) -&gt; List a n ... but of course `Disc` is just `Equal` so we are back where we started.
Why am I allowed to write `Nil id :: List a Z`, but not for example `Nil diag :: List a (Z, Z)` where `diag x = (x, x)`? That `diag` function doesn't know anything about `Z`.
Good catch. That would definitely be a huge problem, so this means the category of Haskell functions is useless for restricting morphisms.
I didn't read much of the article, but the very first type declaration achieves the opposite of its stated intent (and alas, this is where I stopped). &gt; For efficiency reason, I will not use the default Haskell `Complex` data type. newtype Complex = C (Float,Float) deriving (Show,Eq) That's the *opposite* of efficient, as it makes the `Complex` type lazy in both parameters. Also, it's idiomatically strange: declaring a `newtype` around a tuple is cryptic shorthand for "I don't understand the difference between `newtype` and `data`". Oh, and he also uses `Float` instead of `Double`. It was certainly the case a few years ago that `Float` was substantially *slower* than `Double`. I would be surprised if that's changed. (Yes, this is unfortunate.) In all likelihood, then, this `Complex` type is asymptotically far *less* efficient than the built-in one: data Complex a = !a :+ !a Here's what a more-efficient-than-the-default `Complex` type would look like: data Complex = Complex !Double !Double And probably a substantial additional win: data Complex = Complex {-# UNPACK #-} !Double {-# UNPACK #-} !Double These aren't criticisms of the article (which I haven't read) or of the author (who I don't know) or even quite of the line of code I'm fixing (these are common beginner errors). But it's best to have tutorial-style articles not get basic details wrong, and I hope the author goes back and fixes his article up.
Yeah, that argument of mine has been torn apart. Not only is there no way to rule out `undefined` and friends (because of undecidability), but the category of Haskell functions permits all sort of shenanigans that make it impossible to restrict morphisms between types.
Also note that this is available online for free at http://yesodweb.com/book, for those that like to try before they buy.
Injectivity works for Leibnizian equality in Haskell with type families. ;) Is pointing to https://github.com/ekmett/eq/blob/master/Data/Eq/Type.hs cheating? =) Of course, since type families use equality coercion constraints behind the scenes, this is effectively appealing to the solution. ;)
Unfortunately this code is using immediate mode rendering a depreciated and inefficient method of rendering. OpenGL has vertex-buffer objects in core since GL 2.0/1 use them. Also using a fixed-function pipeline and avoiding shaders is not really the way to go because you could (and should) be pushing more of this on to the GPU to do. 
You could always imagine going the way of Agda: enforcing termination checks for these kinds of functions. The only problem is that it is quite difficult to check termination of *part* of a program without enforcing termination of all of it. The fine folks that develop [Trellys](http://code.google.com/p/trellys/source/checkout) have tried to give a type system in which some terms are terminating without enforcing the whole system to be.
I'm no fan of that terminology, and have no respect for the presumption moral authority that goes with using it, but that is my point. The essence of typed programming is to introduce conceptual distinctions between copies of the natural numbers.
The point is that the evilness prevents it, even if `Equal` was a functor. His method only works up to isomorphism, which doesn't work well with evil stuff.
Thanks. TIL. :) When I wrote my post I was more interested in GADTs-as-functors, but I still learned a lot from everybody here about the other half for GADTs as Equal and this was really illuminating.
Well, I was thinking of possibly just using a different value-level category than Haskell functions for expression reachability relationships, one in which you can properly restrict the set of permissible morphisms, however I'm not sure what that category would be.
I don't mind losing the evil-ness, which I don't consider that useful. For all the applications of Equal that I can think of, equalness up to isomorphism is sufficient, but maybe I'm missing an important class of applications.
This is the well-known encoding of type equalities using the Leibniz equality. See for example the [work of Oleg](http://okmij.org/ftp/ML/first-class-modules/index.html) (and Jeremy Yallop) on encoding leibniz equality, and therefore GADTs, with OCaml first-class modules.
What you really want, I think, is to be able to use the right category for these arrows. If you could use the discrete category ℕ for the numbers instead of Hask, you'd be fine: there would be an arrow from n to n and nowhere else, i.e. you'd have an equality type. This gives you equational constraints. So for vectors, you would get (using Agda): data Vec (A : Set) : ℕ → Set where [] : ∀ {n} → n ≡ 0 → Vec A n _∷_ : ∀ {m n} → n ≡ suc m → A → Vec A m → Vec A n `n ≡ 0` and `n ≡ suc m` are exactly the arrows of the discrete category, which is defined as Ob(ℕ) = ℕ Hom(m,n) = if m ≡ n then {*} else {} So what you really want is to be able to use whatever category you want with your indices, instead of Hask. Which you can't do in Haskell, but you may be able to do in Agda. :)
Fortunately for you, arrows in C form a set: Hom(X,Y) for X, Y in C is a set in Set! And I do believe the whole point of the Yoneda embedding is to embed a category into Set. I'm not entirely sure, but this is interesting enough to me because it's almost exactly what I've been looking at recently. Come to #agda on freenode and lets chat with edwardk some more. :)
(can't tell if Qiita is agreeable to content generators, or if they're more like iframers, cause i can read about.. 3 Kanji) 
Alright. I'll check it out. I'm still very new to languages like Agda and more advanced concepts, but I will do my best.
Yes. I recommend you check out this link: http://www.haskell.org/haskellwiki/Modern_array_libraries `STArray` lets you confine mutable operations in a referentially transparent code block to avoid unnecessary duplication and garbage collection
You're perfectly right, the problem is a bit more subtle than I thought...
Also, what is your name on IRC?
I haven't finished reading the post, but I don't think it's quite fair to call the first example a failure of equational reasoning. You're not allowed to cancel infinity from both sides of an equation, unless you're Paul Dirac.
Slight typo. Cobind should be: cobind :: (w a -&gt; b) -&gt; w a -&gt; w b I loved the article, though.
&gt; a = true &gt; if a then b else c =&gt; c ^
Michael; congratulations on the book release!
Constructor names must begin with a capital letter (or a colon, which makes them infix operators).
(lsat summer) http://www.cl.cam.ac.uk/~dao29/talks/comonads-and-codo-talk-dorchard-2011.pdf
`Int` isn't codata. It's just lazy. You can write loop in a strict language too, and run into the exact same problems. With unbounded recursion, you necessarily have to work over a lifted domain, and denotational semantics becomes sort of a necessity w/r/t equational reasoning.
You can't distinguish the two in Haskell since it's abstract. The point is you *don't* run into any problems if you define it as codata, regardless of whether your language is lazy or strict.: loop :: CoNat -&gt; CoNat loop n = S (loop n) Here, loop is total. The problem with the equational reasoning is that either you're dealing with data and loop becomes partial or you're dealing with codata and your subtraction becomes partial. Denotational semantics doesn't even play into the picture, so I don't get what your point is.
 data Bit = O | I This might be what you want.
This is a great introduction to Monads for beginners looking to use and somewhat understand `do` notation without having to step too deeply into theory land. The role reversal of explaining `&gt;&gt;=` and `&gt;&gt;` in terms of their `do` notation counterparts is amusing; I suppose the interested learner can discover elsewhere that the de-sugaring actually goes the other way.
I met Dominic at CamHac. Honestly, he talked about comonads the whole time. It was pretty funny. So I'm interested to read this paper.
Thanks for the link! I hadn't read anything about ypnos before, so i dug up a relevant paper: http://www.cl.cam.ac.uk/~dao29/publ/ypnos-damp10.pdf
But you should buy it anyway, to support Michael and the project.
Of course. His name is "Comonad" permuted with a few vowel switches i.e. a very small Levenshtein
Is there a trac ticket for adding `{-# LANGUAGE CoDo #-}` to ghc yet? I wish Haskell came with a macro system that would allow us to define sugar like this without having to turn to the compiler implementation or some other preprocessor. Imagine if you could simply `cabal install codo` and then `import syntax Sugar.CoDo`
Thanks, I tried this but still came up with the same issue. Great tip though as I don't have X installed and don't plan on doing any graphics intensive operations, so the extra 32mb of memory is welcome
What OS did you use for QEMU? I'm trying the arch linux ARM image (for the raspberry pi) but it restricts the memory available to it to 256mb and I'm still trying to work out how to lift that
I thought the totality constraints the OP talks about do allow for ordinary arithmetic on programs?
 data Equal a b where Refl :: Equal a a is evil; the definition of evil is basically when isomorphic elements don't share the property. For example, `2 -&gt; a` and `(a, a)` are isomorphic, so if there is a value of type `Equal (a, a) (a, a)` then there should also be a value of type `Equal (a, a) (2 -&gt; a)`, unless `Equal` is evil.
The problem with adding CoDo is that there is no "blessed" comonad package yet.
Exactly: https://github.com/dorchard/codo-notation
Nice work guys! Is there any chance the snap-0.9 series will be able to build with the containers-0.5 series? Would help us out in some projects. 
So if I understand correctly, the reason they are more complicated is because comonads in general do not necessarily have a "costrength". Perhaps one solution would be to define simpler rewrite rules for the ones that do.
Or more simpler: b 0 = F b 1 = T
Indeed. There is a question of whether Turing completeness is really all that great, which is the purpose of the article. I actually think simple totality/productivity as propose is insufficient, and one would need to have dependent typing to make programming bearable (otherwise one would need some very clever contortions to write certain algorithms; I would like to be proven wrong!). Examples of research languages which move in this direction are Agda and (the long awaited 2nd version of) Epigram.
I will say that I know from experience, certain types of algorithms which we know to be terminating (e.g. efficient union-find) are nonetheless very tricky to prove to be terminating, even (especially?) with dependent typing. We need to get better both at proving termination and at automating termination proofs (or nontermination proofs :-)) to really begin taking advantage of this sort of stuff in earnest.
There's no "anti-windows" feelings in the Haskell community (not significantly), that would be strange given the huge boost from Microsoft Research to GHC development and the amicable relationship with F# people. I think you're referring more to the fact that many people in the community develop on other OS (mainly Linux) and don't go the extra miles to ensures their packages work on Windows, especially in the beginning...
You seem to have a misunderstanding of evil. You say earlier that it is _impossible to implement_ because it is evil, which is not what evil is about at all. The evil distinction is a preference in category theory, which encourages people to think "up to isomorphism" rather than about equality. Even in a proof assistant, we _can implement_ category theory and use evil all we want. It just feels kind of ugly. Even apart from that, as dolio was explaining above, propositional equality is not inherently evil. Some definitions of functor equality might be, but that's a different story.
Awesome work you guys. I love how fast an issue I raised on github has made it to a production release :)
The claim that 'Haskell types are all codata' unlike ML datatypes seems to me a bit of an academic nicety. In an MLish language you can introduce functions that loop on certain arguments, if I remember, butwhere the expression, read as Haskell, might be taken to designate the something like `inf` above; it is just that the customary way of looking at the matter just declares the function to be partial. (I seem to remember some form of ML that would accept a pair of interdefined terms of that sort, but would loop on opening the file -- you couldn't get it open in the repl; if this is true of all of them, then this is a rather strange ground for saying that such terms are excluded by the type system and that the datatypes are thus genuine 'data'.) However that may be, this whole typically academic and type-theoretic approach overlooks the role of the programmer, the users, the speakers, in enforcing things that cannot be enforced by the compiler. This I think is not a matter of principle but a real lack; an approach closer to that of linguistics is conceivable. Does the fact that some known `Monad` instances violate the monad laws entail that `Monad` doesn't really express the idea of a monad on type system? If I purposefully introduce a `class Schmonad (m::*-&gt;*)` with similarly-typed 'methods' but announce no laws, or less restrictive laws, the type checker would act exactly as it does with `Monad` instances. Does this mean that a Monad-in-the-Haskell-sense is just a typeclass with two associated functions that could do anything consistent with their signatures? Not at all. *This* case is especially clear: though the compiler can't reject the instance, the community of users in unrelenting in its rejection and vituperation; on any rational interpretation of the word "language" the latter is a far deeper clue to the semantic facts. But existing programming language theory has no tools for registering such facts. This is simple a theoretical deficit and not something for the theorist to take pride in. Thus one is familiar with another way of looking at things. And it can be brought to bear on the data/codata matter: whether a Haskell declaration of the form `data Blah ...` introduces a kind of data or codata depends on wider purposes and involves the commitment of the user of the type to a particular programming style. It's just like classes, which note, do not enter into the usual ways of opposing ML -ish languages and Haskell. It is actually a deep difference between the languages as media for human thought. This is why the `she` pre-processor announced a separate `codata` keyword "documenting an infinitary intention".[*] The idea was evidently that in Haskell+she one is supposed to maintain appropriate canons of definition. It is true that norms pertaining to this are not much developed in the Haskell community, but mostly unconsciously maintained. If a library introduces `data N = Z | S N` as a form of data, not codata, its principles will preclude your definition of `inf` even if the compiler doesn't. This situation is of course annoying, especially since it seems plain that suitable forms of check on recursion are possible, and they are difficult to maintain 'by hand' in complex definitions. Everyone thinks that a new Haskell with a Turner-like ancillary system, or a Haskell-like language of that general sort would be far better than Haskell. The hope of this seems to have been dashed by the enthusiasm for dependently typed systems, which really speak to different objectives. [*] : It seems not to have been implemented, but it's trivial: https://github.com/michaelt/she-codata.git
I really liked the way you presented the Maybe monad as the list monad with either no element or a single element. That was quite clever and I had never thought of it that way myself.
So do you think their version is more awkward because of the lack of a general costrength or do you think the sigfpe comment version handles all the use cases they mentioned?
For http://hackage.haskell.org/package/acme-realworld-0.1.1 I see: &gt; Joey Adams • 15 minutes ago Testing Comments userscript For http://hackage.haskell.org/package/acme-realworld I see: &gt; Joey Adams • 8 minutes ago Testing comments userscript &gt; &gt; Joey Adams • 9 minutes ago Testing Comments userscript (not using a version-specific link) It didn't appear for me initially, but now I see it. I guess you just have to be patient with it, they're serving hundreds of thousands of sites after all!
I asked my communities about getting GHC7.4.2 on a Raspberry Pi. Most recently I heard that debian/unstable has debs for 7.4.x, and most likely that would just work. That is, if you're using debian/testing or debian/unstable, you can likely just install the ghc packages from debian/unstable and you'll be good to go! I can't test this as I loaned out my Raspberry Pi for the summer.
I have to go through their examples in more detail to make sure, but that is where I was going with it. ;)
Perhaps what we need for the [monad tutorials timeline](http://www.haskell.org/haskellwiki/Monad_tutorials_timeline) is some kind of depiction of the parameter space of things you could try when writing a monad tutorial: terse vs wordy? bind vs join? etc
Not-quite-relevant but humorous quote from IRC logs via an old Haskell Weekly News post: &gt; edwardkmett: Most monad tutorials are written by people who barely understand monads, if at all, but unfortunately nothing can stop someone from writing a monad tutorial. We've tried, there was blood everywhere.
I totally agree, but f.e. Dominic Orchard does not seem to be aware of this, since he uses his own implementation with completely different names.
Well, it's impossible to implement evil things without something more powerful than what Tekmo allows
What you are looking for used to be called [`HFunctor`](http://hackage.haskell.org/packages/archive/category-extras/0.53.5/doc/html/Control-Functor-HigherOrder.html) by category-extras. Also in practice, unless you want to do everything through left kan extensions you probably want to permit the use of a Functor instance for t and s or some reasonable transformations can't work. The HFunctor class is useful when you go to model GADTs through initial algebra semantics. (There the inability to use the Functor doesn't matter because its used to model a functor from the discrete category of Hask rather than Hask proper.) I've broken up category-extras into lots of little packages, but I haven't replicated this part of its functionality. That said, your attempted generalization of StateT etc using it is subtly incorrect. What (forall b. t b -&gt; s b) is is a natural transformation from t to s. What you need for a correct mapState, etc. is a monad homomorphism, which is a stronger condition. I call this operator 'hoist', but it is sadly not Haskell 98, because of the Rank 2 Types, and the mtl doesn't currently use higher rank types (even where they would help, in for instance the signature of callCC). What you want is class MonadHoist t where hoist :: (Monad m, Monad n) =&gt; (forall a. m a -&gt; n a) -&gt; t m a -&gt; t n a where the use of hoist is subject to the monad homomorphism laws (sensible conditions about the mapping of return and join).
I've just installed GHC7.4.1 using the 'wheezy' branch of the debian installation and it seems to work OK so far on the QEMU VM I'm running I'm hoping that the Arch guys get a similar Arch binary going soon (I'm running Arch on my Pi as the size of the OS image is a lot smaller than debian) Thanks for the tip!
Yes, GHC is evil.
Your `Paired` type is (using the `TypeCompose` package's `Control.Compose` operator `(:.)`): ((,) w) :. f You get automatic instances of `Functor`, `Applicative`, and other goodies.
Add this to the top of your file: {-# LANGUAGE DeriveDataTypeable #-} In the future, stackoverflow is probably a better venue for these sorts of questions, since answers there are readily searchable by others in the future and neatly categorized :-)
Thanks for the comprehensive response and all your work on those libraries. Cool notation in HFunctor. My knowledge of category theory is insufficient to appreciate all of your response, but there is one thing that i find a bit strange. None of mapStateT, mapReaderT, etc require Monad constraints, so f2map should not have to either. f2map for the StateT case is a bit weaker that mapStateT because it cannot change the 'a' to a 'b', and because it requires a stronger natural transformation instead of a function with fixed types. So it cannot replace mapStateT, but it does part of its job, it changes just the second type parameter. It seems that it can do so for all those monad transformers... am I missing something? 
Keep in mind some monad transformers will perform more exotic combinations of actions, and may have to get 'under the monad' to do some of their work. Consider f (Either b (f a)) -- this is a monad transformer, but to transform from f -&gt; g you need to get inside of the both the first f and the Either to reach the second f to exchange it for a g. The fact that the stock mtl transformers can be transformed in this way isn't indicative of the general properties of monad homomorphisms.
The error message was pretty clear about that and as sclv answered.
Disqus has some automatic spam detection and a “mark as spam” button for moderators, it also has blacklists/whitelists. I can add more moderators by email, too, if anyone wants to be one (provided I can verify that you're legit). There's also “mark as inappropriate”, not sure what that does but I presume flags to the moderators.
Wow, great work Stephen, can't wait to try this out!
Nice! For some reason I never thought to use Writer. The "Bonus: commas are implicit!" statement is a bit of a misnomer, because all you've really done with comments is replace them with the word 'tell'. But nonetheless, the ability to use other constructs to form lists is very useful.
Hmm: json = fromList ([ ("title",JSONString "Document title") , ("date",JSONString "2012-06-12") ] ++ (if notesPresent then [("notes",JSONString "These are notes")] else [])) To be fair on the list syntax: json = fromList (concat [[("title",JSONString "Document title")] ,[("date",JSONString "2012-06-12")] ,[("notes",JSONString "These are notes") | notesPresent]]) And with value name val = (name, toJSON val) it's even elegant: json = fromList (concat [[value "title" "Document title"] ,[value "date" "2012-06-12"] ,[value "notes" "These are notes" | notesPresent]) But yeah, the writer monad is a much more robust solution, visually and expressively. Expanding on that, maybe you could emulate local bindings for additional nice syntax, so have some "here comes an object" function, object = fromList . execWriter . ($ value) where value name val = tell [(name, toJSON val)] Which passes in the constructor: json = object $ \(:) -&gt; do "title" : "Document title" "date" : "2012-06-12" when notesPresent $ "notes" : "These are notes" Now it even looks like JSON, but has all the extensibility of a monad. The type system will warn you of accidental uses of (:), and you can even use normal (:) outside of the lambda: json = object $ \(:) -&gt; do "title" : title "date" : "2012-06-12" when notesPresent $ "notes" : "These are notes" where title = 'D' : "ocument title" Thoughts? **EDIT**: Okay so `:` is special so we can't use that. There's always the Ruby-style object literal: json = object $ \(=&gt;) -&gt; do "title" =&gt; "Document title" "date" =&gt; "2012-06-12" when notesPresent $ "notes" =&gt; "These are notes" But yeah, I preferred the colons as it looked like JSON.
It doesnt work for me in FF13. It just shows the javascript code. Should I install an extension for this?
I'd make a few tweaks: -- nothing wrong with the Seq implementation -- I just stuck to [] for simplicity of demonstration newtype JsonMonad a = JsonMonad { unJsonMonad :: Writer [(String, JSValue)] a } deriving Monad -- use an infix operator for convenience (:=) :: ToJSValue v =&gt; String -&gt; v -&gt; JsonMonad () k := v = JsonMonad $ tell [(k, toJSValue v)] -- add a ToJSValue instance for convenience -- the newtype makes this safe &amp; easy instance ToJSValue JsonMonad where toJSValue = JSObject . toJSObject . execWriter . unJsonMonad -- showing off the changes -- no need to explicitly distinguish between `value` and `object` json = toJSValue $ do "title" := "Document title" "date" := "2012-06-12" "sub" := do "nested" := "yep" "cool" := "that too" when notesPresent $ do "notes" := "These are notes" [edit: ugh Haskell doesn't like identifiers that start with `:` being regular functions. But you can dream up any old operator.]
They are stored somewhere for sure, but I don't think it's a publicly accessible location. See this post: http://haskellwebnews.wordpress.com/2011/03/16/hackage-stats-the-past-year/
You need Greasemonkey. I'll add that to the page.
There is problem with url's that mean the same. For example: http://hackage.haskell.org/package/kibro-0.4.3 and http://hackage.haskell.org/package/kibro . These point to the same actual package version (0.4.3 is the latest one). Yet disqus thinks they are different. I don't know how to deal with it. The comments are also a bit hard to browse. I have no idea how to fix that problem either. 
Well, they are different. The latest package will always change. Disqus *should* think they're different. package/kibro is the package in general terms, package/kibro-0.4.3 is that version of the package.
How do you make your version of `Expr` or `Decl` into a comonad, when their kind doesn't even lend itself to making them `Functor`s?
Or just use something like `^=`, `%=` or `#=`.
Thanks for the counterexample. It is probably better to have a more general class like MonadHoist, even if the monad contraints are often unused. Your observation about the need to comply with monad homomorphism laws, that also applies to mapStateT doesn't it? 
&gt; Technically this isn't directly using a comonad, but you can get to this construction by 'rolling' the definition of the cofree comonad, like how you can get from [...]
Good point. That is pretty annoying. At the very least they should expose a censor-like operation. Also, I don't know why people are so gung-ho about encapsulation, in cases where it is not required to enforce invariants. A simple newtype with the constructor exposed should be sufficient.
Actually I find the approach of making something a Comonad over a Kleisli category to be quite nice. You can often get away with even making them an instance of 'Extend' in Hask, despite not being able to write extract with the same signature.
My personal choice would be `=:=`, but it doesn't really matter.
I can't wait until there's a 64-bit Haskell Platform for Windows!
Maybe you could add the `cfix` from the slides: cfix :: (Stream a -&gt; a) -&gt; Stream a cfix f = fix (cobind f)
.___. I frankly think using the Writer monad here just to obtain that list is a lot uglier than the original list code. You have that extra execWriter, and then a bunch of tells in your do-block. The syntactic noise is going way up, at least as I see it. The main thing which was wrong with the list code as far as I can see was that it was indented poorly and had extraneous parens. As Chris points out, a list comprehension can replace the if/then/else, if the problem was the explicit mention of [], which is probably the nicest way to clean that bit up. You could also opt to use the list monad: guard notesPresent &gt;&gt; [("notes",JSONString "These are notes")] or filter: filter (const notesPresent) [("notes",JSONString "These are notes")]
Hrmm. I have to admit that is a rather odd variation of the operator. The version I'm familiar with is Menendez's original: wfix :: Comonad w =&gt; w (w a -&gt; a) -&gt; a wfix w = extract w (extend wfix w) [Edit: Added, Orchard's is a damn sight more convenient]
I've just started using netwire for a 2D game. It's much easier than Yampa, though I have a weird bug (after a few sedonds, If I click with the mouse, the rendering goes off...), but not sure if it has something to do with netwire.
Great initiative! Although, I can't help but wonder if we should be using general web annotation tools rather than re-inventing wheels on a per-site basis.
I've never seen censor function used anywhere. Tekmo, can you give a usage example?
Any sequence of more than one hyphen starts a comment. (unless part of a longer lexeme)
Oh, now that's strange. I can use `--|` as an operator. I can use `--&gt;` as an operator. But not `---`. You learn something new every day.
I've personally never used it, but here's the simplest contrived example I can think of: w = do tell [1] tell [2] tell [3] execWriter w == [1, 2, 3] execWriter $ censor (map (+3)) w == [4, 5, 6]
Yeah, and now I have to wrap every primitive exposed by the library just to get the underlying functionality back...
I love encapsulation when it intentionally hides implementation details. In this case, however, the underlying representation is the same as the model it exposes, so there's just little point in "re-representing" the same thing in a different way.
The advantage of the `tell` is that it goes in front of *every* line, not every line but the first one. I find this convenient for text-level transformations. Is there some good reason why Haskell doesn't allow trailing commas in list literals, as e.g. C# does in array initializer expressions? (Doesn't appear to make parsing ambiguous, at first glance.)
Agreed that termination proofs are hard. But the OP's suggestion of total non-dependent programming relies on expressing everything as structural recursion, which is clearly a subset of the proof techniques available to a dependently typed language. Furthermore, it would be much harder in the OP's proposal to implement proof-irrelevance. Personally, I think these problems will only become even tackle-able once we have something like Epigram on which to experiment.
Pretty crazy slides!
"to argue for undocumented risk is to insist on ignorance of safety"
I usually use `censor` when the output is tree-like, to implement grouping combinators; e.g. if you're generating a `[Tree a]`, you can have something like group lab = censor ((:[]) . Node lab)
I have a few real `censor` examples in [a LaTeX generation module](http://hpaste.org/70200) I wrote some time ago.
You'd typically declare a completely distinct type using `data` instead. Doing so gives you the ability to control strictness, whereas tuples are always lazy.
My apologies for the way this document currently ends suddenly, quite some distance from the end. I'm afraid that other thunks are being forced right now.
I'm actually a bit surprised that `Complex Double` does so badly. To make a guess in the dark, I'd imagine that its methods are not being inlined and specialized, so you're being killed by pointer indirection due to class dictionary overhead, and possibly dictionary construction too. Try this: data MyComplex = C {-# UNPACK #-} !Double {-# UNPACK #-} !Double This should perform best of all. `-funbox-strict-fields` has no effect on polymorphic types, because GHC can't unbox a type if it doesn't know what it is.
Cool! Have you had a look at the effect system based on generalized arrows in the work for scala? 
Oh yes, sorry, I misread your code. But to me, 'F Int is not defined' and 'F Int cannot be matched with Char' should be two different error messages, for clarity purposes. For instance, when you use a type class, you don't have to *use* an erroneous function to spawn an error, you just have to declare it: class Something a where foo :: a -&gt; a f :: Int -&gt; Int f x = foo x It directly fails because Int is not declared as an instance of Something, even if 'f' is not referenced anywhere. Okay, it's maybe not totally the same, so the same logic may not be fully appliable, but you see my point concerning error messages.
Can you perhaps elaborate the role of `abort` in the typechecker signature? I can't see how any guarantees don't get thrown out the window the moment you allow for `abort` in the signature, since then you can just use bottom elimination in the continuation (the right-hand side of `&lt;|`)... or am I missing something here? Or is that the whole point, that `abort` is basically a reified version of divergence, divergence which you could only do if `check` was defined in terms of `F∞`, not `F*`?
Any chance of getting the rest of the panels in the highres directory?
The role of abort in the typechecker is the finite failure that you need to signal when the input doesn't typecheck. I talk a bit more about the method [here](http://www.e-pig.org/epilogue/?p=914) and give the actual typechecker [here](http://www.e-pig.org/epilogue/?p=955) if you're interested. The source of divergence is that it's a dependent typechecker with Set : Set, so it's defined mutually with evaluation, which is partial (in both senses). Note, however, that it makes perfect sense to define what the typechecker (or any other general recursive program) *is* using a suitable free monad, F-star, generated by the signature of possible recursive calls, because it's enough to explain how to expand the call graph one node at a time, in a bounded way. The coinductive delay monad then provides one way to *run* the program. Being and doing are separable.
Thanks, I'll check those links. Thanks for mentioning the *is*/*does* separation, that makes the big picture much clearer for me.
I've just seen these [slides](http://www.slideshare.net/akuklev/scala-circuitries). No paper yet AFAIK.
FWIW, I love his handwriting. If I wrote like that I may *never type again*.
In practice, almost all code has very simple logic. There is this 1% where something actually happens, but many bugs happen outside of this core.
We use types extensively to prevent both logic errors as well as simple but potentially difficult to track down errors. For instance we use types for type safe times that are in different time zones and also for currencies. Dependent typing would make some logic typing much easier in some of our applications and I have to make some time to look at how Agda might be able to help us. I more and more believe that safety outweighs near term productivity. It enhances long term productivity because bugs are expensive, especially in my business. Software is too complex to say things like, 'Well this is probably a bug but it won't impact customers.' That may be true until it does at some point in the future. I hope to see Haskell continue to implement more and more type features to enable my team to build applications that I can feel good about their correctness.
Num instance allows you to write things like 0 :: Bit
What's happening with epigram(-2) ? The blog has been quiet for some time.
There's not a ton of software written in it, but I can't help but feel he missed a great opportunity to give Racket -&gt; Typed Racket a spin.
I'm too busy to work on it. Nobody else is paid to work on it. But observational equality and first class datatype descriptions will be coming your way in Coq, Agda and Idris any day now (only joking).
Apologies to Edward. I am familiar with his package but for the purposes of my examples I wanted to a). put all operations into one class as opposed to splitting them between Extend and Comonad b). use different naming for the intuitions in the paper. I would be happy to fork my prototype and use Edward's Control.Comonad package. 
I think having a blessed package is a good idea for encouraging the use of this pattern more seriously. Do you think that it might be simpler to present just one class (as with Monads) or is there a good reason to split them as is the case at the moment? 
This is just a copy-paste of the mailing list announcement. http://www.reddit.com/r/haskell/comments/va5ao/ann_gencheck_a_generalized_propertybased_testing/ http://permalink.gmane.org/gmane.comp.lang.haskell.general/19371
This looks a little like Disciple's effect discipline. Nice!
Oh Tekmo, look at you preaching the dependent types line. :)
If the Agda is readable and there are proofs that the Haskell code is equivalent, and if the generated Haskell can be given haddock comments easily, then it hardly matters what the resulting Haskell looks like. I'm not at all familiar with Agda though, and unsure whether this is the case.
The reason I split them is because in particular when working with comonads you find that you want the ability to work with the semigroup-like (&lt;*&gt;) of Applicative without pure since you often want a strong lax symmetric semimonoidal comonad. (Yes, I know this implies a stronger connection to extract than just associativity of (&lt;*&gt;)) I split them out in part because only 2 Prelude types can become instances of Comonad proper, ((,)e) and Monoid m =&gt; ((-&gt;)m). But, with Extend you can make instances for many more structures, including Maybe. The principled explanation is that an instance of Extend offers a co-Kleisli semigroupoid rather than co-Kleisli category. A semigroupoid is simply a category that doesn't necessarily have identity arrows. Semigroupoids arise a lot in Haskell, because there are many such semigroupoids you can build that you cannot build the associated category. Try building a product/coproduct Category instances for instance -- they are just now becoming implementable with polymorphic kinds, without those, you can't provide 'id'. This (&lt;*&gt;) only Applicative operation is already captured in the semigroupoids package as the Apply class, which indicates that static arrow composition forms a semigroupoid. If I wanted full pedantry, I'd make a subclass of Apply for 'ComonadApply' to state that (&lt;.&gt;) interacts properly with extract, but I've tried to avoid making classes that only provide laws without providing extra methods. I went with the f (a -&gt; b) -&gt; f a -&gt; f b Applicative-like signature because it avoids creating a needless constructor and wasting time uncurrying inside of a container under the common usage patterns. In particular, Extend is often uses for comonads over other categories. I can build comonads that happen to embed kleisli arrows for some monad, that can be made comonads over that kleisli category, but not over Hask proper. This happens with many memoizing, task and event oriented comonads. I already have many packages built in this fashion, and removing it would cripple a fair bit of existing code, hurting existing useful functionality in a misguided quest for simplicity. * http://hackage.haskell.org/package/comonad-transformers * http://hackage.haskell.org/package/comonad-extras * http://hackage.haskell.org/package/comonads-fd * http://hackage.haskell.org/package/compressed * http://hackage.haskell.org/package/free * http://hackage.haskell.org/package/streams * http://hackage.haskell.org/package/kan-extensions * http://hackage.haskell.org/package/representable-functors * http://hackage.haskell.org/package/adjunctions * http://hackage.haskell.org/package/trifecta * http://hackage.haskell.org/package/ad And finally, (and an interesing exercise) every comonad transformer in Haskell can be 'lowered' with just Extend, but not every monad transformer can be 'lifted' without return.
All that said, if it was a choice between getting blessed package without the superclass and never getting anywhere with standardization, i'd be willing to rip it out and spend a week rewriting packages. The major problem with doing so is that you still need to get Apply from somewhere, which brings back in all the semigroup dependencies, because some comonads need a semigroup to define ComonadZip/ComonadApply, so you don't really get to shed dependencies in the long run. Rather, you can't even avoid having the use of the comonad package for prelude types depend on semigroups, since the symmetric definition for Apply for ((,)e) needs it right away. =/
I've gone ahead and put together a branch of comonad in the repository labeled 'simplified': https://github.com/ekmett/comonad/blob/simplified/Control/Comonad.hs This adds a ComonadApply class, avoids the use of the less correct Apply class outright, and makes the above changes. I haven't yet pushed it to hackage or committed that its a good idea though.
This is simply awesome and shows why is all the fuss about haskell, monads and stuff.
Just watched a Common Lisper colleague for two days try to figure out why a JSON encoding library was throwing cryptic exceptions on a certain value in production. In this case it actually took the whole server down due to an overflowed max exception depth. Just now it was discovered that the library is incapable of encoding a cons. Static types would have caught this weeks ago. I see the same kind of things over and over. But here, I am indeed preaching to the choir.
You can actually do this. But the problem is the actual record syntax is far from elegant when you need a lot of fields and record types. First problem I see is the lack of namespace. I would like to be able to give: data Pacman = Pacman { position :: Loc, nbLifes :: Int, ... } data Ghost = Ghost { position :: Loc, agressive :: Bool, ... } data GameState = { pacman :: Pacman, ghosts :: [Ghost], ... } But this is actually not possible directly, because I can't use position for both Pacman and Ghost. I must precise pacmanPosition and ghostPosition or put my data declaration in another file to use the namespace provided by the import system. But even using the import trick, I cannot use this to call elements of records inside another record. I'd like to know if there is a way to organize your code in such a way you don't have to manipulate complex records. 
&gt; How would you manage this case? Lenses. (See the packages data-accessor, fclabels, data-lens) Also, specifically for games, I would use functional reactive programming. (See my reactive-banana library.) It's not concerned with records directly, but gives you a different perspective on state.
JavaScript is the worst offender. Every damned typo is turned into a cryptic run-time bug. Gah! How do you deal with this?
If you use this: https://github.com/Peaker/bottle/blob/master/bottlelib/Data/AtFieldTH.hs You can do: AtFieldTH.make ''GameState and then: move :: GameState -&gt; GameState move = (atPacMan . atPacmanPosition) (+pacmanSpeed) . (atGhost . atGhostPosition) (+ghostSpeed) 
One of his first examples is Contracts, and suddenly I wondered: are there any libraries that add "Contracts" to Haskell? I mean, usually the solution is to encode invariants in the type system, but sometimes this is overly cumbersome. Wouldn't it be great if we could add a language feature like this: foo :: Int -&gt; Int foo x y ~ x &gt; 0, y &gt; 0 foo x y = blahblah And then be able to choose at compilation time whether or not to enable contracts?
&gt; How do you deal with this? If that's a non-rhetorical question, one answer is the Google Closure compiler which adds a simple static type system, with inference, that knows about ints, strings, functions, arrays and structural typing of objects. So it stops typos, type mismatches, null/undefined are not values of all types, etc. The syntax is lame (comments), the system is a bit quirky and the closure compiler is slow, but it will save you from dynamic errors like that.
What's nice about Agda is that it reopens the question "how far can I get, just by writing a program which typechecks?". The answer is unlikely to be "all the way to smiling end users", but you can surely achieve an improved standard of basic hygiene by (a) baking simple invariants into your data structures and (b) ensuring that functions which check a condition deliver useful evidence rather than true-or-false. Of course, both of these require good design: there are plenty of logically equivalent ways to specify the same invariant, but they won't necessarily be equally easy to work with. It may well be that designing good dependent data structures takes more skill than designing good tests. My point is just that it's not "tests versus proofs". The goal of dependently typed programming is to reduce (I said "reduce", not "eliminate") the need for proofs or tests, by supporting the translation into code of more of the *design*. The effort in expressing more of the design in types is repaid by type-directed (I said "type-directed", not "syntactic structure") editing, which pulls you towards an implementation that elaborates your design. As ever, measure a static type system by what you don't.
IMHO that is a small sacrifice to make to have same labels in different records. map unFoo does not happen in my code very often and I'm OK with (\r -&gt;r.unFoo). On the wiki one of proposals allows map (.unFoo) as kind of section syntax for label selectors. Anything that will allow two different types of records to have same field label will be OK with me. Currently we use modules for that purpose, can anything be worse? I do not know much about lenses. Tell me: would that solution create conflicts between function and label names?
http://en.wikipedia.org/wiki/Partial_function
Hello trapxvi, meet problem domain. Problem domain, trapxvi. If the object conceptually has many attributes, what are you do do? "Compositional approach?" The best you can do with that is records containing subrecords containing subrecords etc, and Haskell's record update syntax strongly punishes this (but yes I know, lenses).
My post mentions static typing but it doesn't focus on static typing. The title is simply "Not enough" and goes on to talk about the specific strengths or weaknesses of certain validation strategies (and as a bonus, I do talk up Agda). The altered title "Static types are not enough" misses the point and is set to enrage those unlikely to read more than the title.
&gt; I really really really don't want subtyping. Why? It clearly allows very useful polymorphic programming. It's a very simple addition, only standard polymorphism (no type-classes or row types) is needed. Where's the downside?
There was, see ESC/Haskell.
I've never worked in a statically typed inferencing language with subtyping. Sounds like it's very problematic to work with. What are some examples of parametric polymorphism breaking and contra/covariance problems? What does mutable state have to do with this? Cheers!
I think most people agree on the easy stuff. I said something similar [on Google+](https://plus.google.com/116312471061608346570/posts/BTovxQDa1kJ) recently as well. But things get stalled because, as elaborated in that thread, there are things that can be done with records now that are difficult to recover in an approach like this: 1. When field references and updates are now polymorphic, type inference has a lot less to go on, and it's possible that this will have to happen in concert with some serious defaulting reform. 2. Fields with higher order types cause some problems when used with type classes (either with MPTCs or type families), and SPJ says these restrictions are HARD to remove. 3. Type-changing updates no longer work. For example, consider: data Foo a = Foo { x :: a } f :: Foo String -&gt; Foo Text f r = r { x = T.pack (x r) } I'm not so concerned with this last one, since (a) type-changing updates are actually impossible to do in a way that preserves module boundaries and information-hiding, and (b) if you just know the constructor (which you must to do this safely) it would be easy to provide sugar for copying most fields to a new record via some kind of record wildcards. But other people seem to be very concerned about this.
Yeah. FWIW there's [a wiki page I wrote about JS](http://www.haskell.org/haskellwiki/The_JavaScript_Problem). Might be helpful to liberate you from the hell of JS. GHCJS and UHC are both very promising and actively developed. I also wrote a limited compiler for Haskell to JS [which I'm already using](https://dl.dropbox.com/u/62227452/Screenshots/Screenshot%20from%202012-06-21%2015%3A27%3A36.png) to generate [some pages](https://dl.dropbox.com/u/62227452/Screenshots/Screenshot%20from%202012-06-21%2015%3A27%3A57.png) in production. I am slowly rewriting pages in Haskell when the time comes to improve them. The compiler's not public yet, as it sucks.
What about immutability? 
I see that you have expected declaring records, also called product types, with creating a nested namespace. The way to introduce namespaces in Haskell is with modules or by nesting lexical scopes (let a=b in c). The field-names are syntactic sugar for positional pattern matching. The syntax for record updates is a wart, and there are various "lens" libraries that usually have template Haskell to help define better syntax. There are GHC extensions to help with what you want: http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html 7.3.15. Record field disambiguation 7.3.16. Record puns 7.3.17. Record wildcards 
That's not a very idiomatic approach to it.
There's also clojurescript, which from it's [github page](https://github.com/clojure/clojurescript) says: &gt; ClojureScript is a new compiler for Clojure that targets JavaScript. It is designed to emit JavaScript code which is compatible with the advanced compilation mode of the Google Closure optimizing compiler. I'm not certain, but I think that means you get type checking and compile. I'm very uncertain about that though.
OCaml
Presumably a copy of the paper is not yet available?
I know this is a longshot, but this is currently the most highly rated unanswered question on Stack Overflow, so I'm hoping someone here can help out. Thanks. (It's not my question, I'm just trying to get help for the OP.)
Lenses provide composable setters without breaking immutability. Record update syntax also provides "setters" for immutable data, only it's syntax and not composable. The way both work is that you get a copy, with the relevant field updated. (Actually, there's problably "sharing" involved; not a complete copy of all the memory.)
The other problem is that Haskell compiling (let alone programming) on Windows is very awkward. Windows' lack of a fully featured command line environment does not mix well with ghc and cabal. The Haskell platform cannot really be considered to be batteries-included on windows until that is fixed.
Killed
Interesting. Maybe it is because a lot of monads come from an adjunction between a free functor and a forgetful functor, where the forgetful functor forgets some type class constraint. The the monad lives in Hask, but the comonad lives in the category of types that have an instance for that type class. It turns out that you can implement this in Haskell in a generic way: https://gist.github.com/2965235 And `extend` can then be implemented without constraints, but `extract` cannot.
My take on the fixed-point operators is that: wfix :: Comonad w =&gt; w (w a -&gt; a) -&gt; a is needed in situations where comonadic computations themselves produce comonadic functions on which a fixed-point is required. The alternate: cfix :: Comonad w =&gt; (w a -&gt; a) -&gt; w a is more useful on the "outside" of a comonadic computation (i.e. not within some other comonadic context).
What's the argument against making data types a namespace? e.g. data Foo = Foo { name :: String } where we can say "Foo.name" when "name" is ambiguous.
&gt; if a &lt;: b, then IORef b &lt;: IORef a for writing (contravariant - any a can be written into an IORef b, but you can't write any old b into an IORef a), but IORef a &lt;: IORef b for reading (covariant - anything you get from an IORef a is also a b, but not all values read from an IORef b are guaranteed to be an IORef a). So I follow that, but not where it becomes problematic. Is it actually about mutability? It seems you have the same contra/covariance with any data parametrized data structure? I mean, `a &lt;: b` also has the same implications for `Maybe a` and `Maybe b` when you construct/deconstruct? Sorry if I'm being dense. &gt; Also, adding subtyping to a type-inferred language makes complete type inference undecidable. Seeing as Haskell's type inference is already incomplete, and limited structural subtyping is supported in OCaml, I guess this isn't a big deal in practice. Hm, I see. 
&gt;Look at the extensions DisambiguateRecordFields and NamedFieldPuns. Maybe I am missing something, but I don't see how those solve the problem. You still have to create every single record in its own module do you not?
Namespace identifiers have to be globally unique, so if you associate every type with a namespace identifier of the same name, then type names must be globally unique. Unless, of course, you could nest namespaces, which would be interesting and probably useful.
Not that I've found yet :-)
That would be the same thing as lenses, except that lenses are first-class.
Agda has arbitrary namespacing. It just chokes when its ambiguous. You use the record name. When that fails, you add the module name. When that fails you just change the name of the thing during import.
The point about Agda is, I think, very correct. Contracts will, or at least should, be eliminated in favor of dependent types. The benefit of this is that the language becomes drastically more expression, because dependent types are going for way more than just contract-y things.
Not recommended. Your fromIntegral is a partial function, and will fail at runtime from a typo or an incorrect input. Furthermore, bits won't conform to the contract specified by the Num class. Better to just use the constructors (T|F or I|O).
My concern was more to do with non-haskellers running a shelly script and finding they needed to download 100mb for it to work. If you wanted to distribute a small script for common use you'd then be restricted to the regular scripting languages which are already installed. Although I'm not sure if those languages would be installed on a windows system, bring the haskell compiled exe files ahead of other scripting languages. However I'm not entirely sure if that's a fair complaint, since shelly seems mostly aimed for personal use, in which case you can have any library as a dependency, no matter how large.
In C or Java, it's common to have one or two "god structs/objects" that have a lot of fields and basically serve as a substrate for multiple modules to interact. I grew up in the 90s so I've never had a chance to play Pacman, but my GameState for something like Asteroids would look something like this: data GameState = GameState WorldState PlayerState [Asteroid] The Asteroid type is something simple like (Pos, AstData) where AstData tracks how much health remains on this asteroid and perhaps if a powerup will drop. WorldState consists of things like the current level, how many more asteroids until the level is done, etc, and maybe some seeds for powerups to drop. The point is this is all composed again as these are different functions. PlayerState consists of things like lives, current attitude of the ship, powerups, etc. The intent is that the updateOneFrame function is composed of smaller functions that independently update the various components: map a numerical integration across the asteroids to compute new positions, update the world state, update the player state, check for collisions and perhaps reject the new state. The record syntax serves a problem I've rarely encountered in Haskell. In C I'll admit to making structs with 20-30 fields but I've never had to pull off that kind of abomination in Haskell. It's just too easy to pull those isolated concerns into independent units, something I'll generally attempt to do anyways in order to make the code more generic.
They've just had their will to resist broken.
Somehow I don't want any proposal to impede possibilities like this: http://research.microsoft.com/en-us/um/people/simonpj/Papers/first-class-modules/first_class_modules.pdf which was once actually implemented . I wish more people were militating for something like it.
 data A a b = A (a -&gt; Int) (B b a) data B a b = B (a -&gt; b) (B b a) how do we figure out if A a b &lt;: A a' b'? Its not impossible, just harder, and would require serious alterations to standard unification. It becomes even less obvious when you have higher order types. [I have a partially coded solution to this here](https://github.com/mmirman/Korma/blob/master/src/KKindOrderInference.hs) &gt; if a &lt;: b, then IORef b &lt;: IORef a for writing (contravariant - any a can be written into an IORef b, but you can't write any old b into an IORef a), but IORef a &lt;: IORef b for reading (covariant - anything you get from an IORef a is also a b, but not all values read from an IORef b are guaranteed to be an IORef a). I think IORef b &lt;: IORef a should result in a &lt;: b &amp; b &lt;: a &gt; Also, adding subtyping to a type-inferred language makes complete type inference undecidable. Can you provide a link to the proof please? 
Clearly it's because [Haskell's on a horse](http://hackage.haskell.org/package/on-a-horse) :).
The problem with your post is that you are stating useless truisms. It's like saying "wearing seatbelt is not enough, the meteorite may fall on your car." In other words you are not taking into account the Point of Diminishing Return. Static typing offers huge and visible return for performed work. Anything else though quickly drops to the level of statistical error rate. 
Right. They are. The problem is that they are file-based, meaning that if I want to namespace multiple types with shared lens names, then they must go into different files.
Just add your MinGW\bin to PATH. EDIT: Err. I actually thought of msys' bin, but I think it's a part of mingw (optional?) distribution.
Technically, that's an implementation detail. It's quite possible to have a standards-compliant Haskell implementation that allows multiple modules per file, or isn't even file based at all. JHC's looking for module `Foo.Bar.Baz` in both `Foo/Bar/Baz.hs` and `Foo.Bar.Baz.hs` could easily be extended to looking for both `Foo.Bar.Baz` and `Foo.Bar.Quux` in `Foo.Bar.hs`. (And in existing implementations, another file really should not be that significant of a barrier. You do have an editor that can open multiple buffers, right?)
Because the idea of a "contract" is independent to the mechanism used to encode it. Most languages provide the capability at runtime. I only mention Agda because AFAIK it can move many of the same checks into the static analysis phase (excuse my awkward terminology). Regardless, I'm still talking about a contract-like-thing.
I am professionally using clojure (and haskell) in my everyday work. And i've been using lisp for years. I think i understand the "functional perspective" better than you do. Do not hide behind authoritative names. If you have something to say, say it.
Except that current unFoo is not a lens. I'm after namespacing problem here. Thanks for the lenses tip. I'm sure I'll use them soon. But they solve a problem that I view as an orthogonal problem. 
Exactly.
That's a pretty long way of saying "you're right, Haskell *does* have a lot of operators not taught in grade school".
I may have misremembered the example. I do recall that the last line was just of naked type 'a' though.
That's fine. See my remark to the protocol buffer package above. I just think that a lot of the calls for reform to Haskell's record system try to make it serve the same purpose as structure fields in C which is like trying to reform the type class system to better support object orientated programming. Similar word, different purposes. Outside of dealing with the FFI (where I declare records in their own module so that I can use qualified import to tag the structure labels --- I prefer this for "big data" records) I rarely use the record system for anything but creating newtype projections, i.e.: newtype Foo = Foo { unFoo :: Bar } If you want simple projection functions, then the existing record syntax is great. The idea of removing record fields as functions defeats this usage forcing more boiler plate: newtype Foo = Foo Bar unFoo :: Foo -&gt; Bar unFoo (Foo bar) = bar On the other hand, if you want record fields because you want to store a lot of fields, then lenses are a vastly superior solution to the problem. Lenses give you *first class selectors* on fields, so that you can select which field you want an algorithm to update by simply passing it in as a parameter. You can't do that with record syntax unless you pass in an anonymous function that does the update using record syntax, which is just more verbose.
I hate that approach. When someone adds some innocent new exported name to a module, it should not break user code... I think qualified names or exhaustive unqualified import lists are so much better for the reader, and for robustness against future changes.
Lenses are too restrictive with regard to polymorphic record updates. 
It seems like Elm is a growing subset of Haskell... Why is it a separate language rather than some formal subset? Or am I totally missing the point?
I asked the same question, of all these Haskellish languages. I don't see the point in the custom parser, the slight syntax differences.
&gt; The reason people prefer dynamic typing + unit tests to static typing + * is because it gives them the ability to iterate quickly, while ostensibly covering their asses for the use-cases they care about. [...] [Y]ou should be talking about how you'll spend less time getting the same amount of functionality done and stable in Haskell + quickcheck / whatever than one would in Ruby + Rspec. AND PROVE IT You are beating up a straw man. I never claimed that static typing is better than dynamic typing at all, only that it is wrong to believe that static typing does not give you any extra safety because unit testing makes it redundant. If you are okay with giving up the additional safety that comes from static typing because you find that unit tests are good enough for you and you find that the speed of development in a dynamically typed language more than makes up for it anyway, then more power to you; just don't claim that static typing is completely offset by unit tests, because that is wrong. Edit: On a computer without RES so I didn't notice some glitches and choices of wording I didn't like until it was too late.
Speaking as someone who's working on developing a subtype constraint based language with mutable state (position paper [here](http://wrigstad.com/stop12/); it's called Big Bang), inference is tricky, but so far, I think it's simpler than Haskell's inference and working pretty well. It still needs a lot of work, but we're optimistic. The real difficulty we're going to face is not type inference but error reporting (we have a story as to how we're going to do it, but we've gotten a lot of "I'll believe it when I see it."). If you thought Haskell's errors were incomprehensible...
&gt; Can you provide a link to the proof please? It's pretty well known, take a look at Pierce's Types and Programming languages &gt; I think IORef b &lt;: IORef a should result in a &lt;: b &amp; b &lt;: a That would imply a ~ b which is almost certainly not what you want. Also, this is about applying a type constructor to both sides of a subtyping inequality, not removing one (which sounds like generally the wrong way to do typechecking). In practice, you'll already know that a &lt;: b or b &lt;: a (or both), and you'll want to determine IORef a ? IORef b, using some known contravariance and covariance rules. The actual solution to this is to have two type variables: type IORef in- out+ = &lt;implementation&gt; -- + means co, - means contra read :: IORef in out -&gt; IO out write :: IORef in out -&gt; in -&gt; IO () create :: IO (IORef x x) -- then we have, if a &lt;: b -- IORef in a &lt;: IORef in b -- IORef b out &lt;: IORef a out Of course, this represents a substantial complication to the type system.
Most of the differences are behind the scenes. There are already semantic differences (e.g. Elm is call-by-value), and when adding new features I am free to think about what is best for Elm (which may not always align with the choices made in Haskell). **Full explanation:** Elm began with frustration with current web technology, so the goal was to create a functional programming language for the web that was easy to use and understand. FRP quickly became the desired model of interaction, and this choice eventually transformed Elm into my [senior thesis](http://www.seas.harvard.edu/academics/undergraduate/computer-science/thesis/Czaplicki.pdf). I considered three ways to create this language: * Create a DSEL. It's generally a good idea because it often saves a lot of work [[1](http://www.cis.uab.edu/courses/cs593/spring2010/dsel-Hudak.pdf)]. Unfortunately, producing JS with this method is not really practical because Haskell functions cannot be translated to JS unless you reify *everything* (i.e. how can `(\x -&gt; x + 1)` be translated to JS at the source level?). Writing code where everything is reified is annoying and pretty darn close to creating a new language. * Get some JS backend working for Haskell. Given the complexity of GHC and my lack of familiarity with it, I did not think this was a tractable project for one person, especially considering that my goal was not Haskell in browsers but "a FP language for the web that is easy to use and understand". * Start from scratch. This gives total freedom over the semantics of the language and avoids the major problems of the other two approaches. The downside is that it requires a lot of infrastructure work (e.g. creating a module system), but this work is generally well understood (i.e. you won't run into any fundamental problems unless you try to). Obviously I chose the third choice. This allowed me to make the major semantic choices I wanted: Elm is call-by-value and Signal replaces IO as the primary means of interaction with the outside world. Using Signals simplifies the mental model for programmers; a GUI is a time-varying Element, not a sequence of mutations here and there. I don't think a truly *functional* GUI framework is going to rely on the IO monad. Call-by-value is a win for FRP, making it extremely difficult/impossible to accidentally introduce a space leak. More details on these issues and more can be found in my [thesis](http://www.seas.harvard.edu/academics/undergraduate/computer-science/thesis/Czaplicki.pdf). The other benefit is that Elm actually works in browsers *right now* and it's relatively painless to write client side code. Neither would be the case if I chose a different way to implement Elm. AFAIK the Haskell-to-JS projects are not close in terms of ease of use (both in writing code and getting it running). So why do modules look the same? Well, I just wanted to add a dead-simple system to make it possible for people to work on larger projects with less hassle. Haskell has this and it works, so I don't see a great reason to do something else right now. In some ways I prefer the ML module system [[2](http://caml.inria.fr/pub/docs/manual-ocaml/manual004.html)] because functors could make it easy to create more flexible GUI libraries (e.g. the color scheme is passed in as a module), but I do not expect this to be incompatible with dead-simple modules. So why does the syntax look the same? You have to make a lot of syntactic choices when you are designing a language. Haskell's syntax is quite nice when compared with other FP languages I've seen, and I didn't see any point in arbitrarily inventing new reserved words or idioms (i.e. changing the things that aren't broken). Sorry this is so long, but it's an important question. Hope that all made sense :) edit: [more readable version](http://www.testblogpleaseignore.com/2012/06/21/why-elm/)
&gt; Is it actually about mutability? No, actually it's about anything which is a function in disguise, which includes mutability, but obviously also functions, and state monads, etc. &gt; It seems you have the same contra/covariance with any data parametrized data structure? I mean, a &lt;: b also has the same implications for Maybe a and Maybe b when you construct/deconstruct? Sorry if I'm being dense. Yes, Maybe is covariant though, which is easy to understand. The problem is that not all constructors are (most specifically, functions are *contravariant* in their arguments, which means that IORefs have weird behaviour when you have subtyping which means that the only correct way to implement them is to make them have an input and output variable; because writing and then reading from an IORef in analogous to a function from a -&gt; a, but in subtyping land those two type variables may no longer be the same).
Yeah, I didn't say it was impossible (although it is impossible to do *completely*, you'll still need the occasional type annotation). I would shudder to think of the complexity it would add to Haskell's OutsideIn(X), though. We've already got GADTs and local assumptions, throw subtyping into the mix and you've got a melting pot of horrible crap.
I'm pretty sure the problem isn't that subtype inference in general gives you bad results, but that entirely structural subtyping gives you bad results. I think just like how explicit nominal infinite types give fine results, explicit nominal subtyping would better results.
You're basically describing bottom-up learning. Notice how you said that you came across all the examples first, were somewhat mystified by them, then came across the formal definition which cleared everything up. Maybe you benefited from being exposed to the examples, so that when you finally came to the definitions, you had the experience you needed to wrap your head around them.
That's certainly plausible. On the other hand, I learned about arrows the opposite way: I first saw a definition and only later saw examples (in JavaScript, of all languages!). I found learning about and using arrows much easier than monads, but that could be because I was already familiar with similar concepts. That said, I *think* (with no concrete evidence) that learning about the definition first and the use second really helped.
The problem is even with lenses you lose existing functionality. Consider: data Foo a = Foo { bar :: a, baz :: Int } instance Functor Foo where fmap f t = t { bar = f (bar t) } None of the lens based proposals for records correctly allow that to continue working. The nice thing with that version of the code is that it is robust against the addition of extra fields to Foo. record field assignment can change the type of the record, it really is only sugar for pattern matching and putting together a new value with all the fields replaced. You can't do this in general through any sane combination of MPTCs, fundeps, etc. if you start overloading fields across types. It only works in haskell because when you name one field you've determined at least which datatype/newtype you're talking about.
Right, so my position is that we should 1) scrap record syntax, keeping the "getter" functionality, and solving the namespace issue (not something I care about), and 2) include a blessed data-lens -like lens implementation in the base libraries (something I care very much about). It's frustrating to see a slew of ugly, complicated language-level solutions that don't even have the power of lenses (an elegant solution we can implement as a library). W/r/t first class solutions like lenses, I think we don't quite know what we've been missing in terms of what can be built from powerful abstractions. Just one instance I've noticed are that lenses make a heterogenous, generic zipper trivial to implement.
Interesting point. My instinct is that this isn't so appealing unless you've managed to write all your code without pattern matching on that type. Would this be an important trade off to you, or we're you just pointing out an interesting downside? As an aside, has anyone done a survey of hackage to see how often people use the special record setting syntax? I meant to do that some time ago.
Sure, same as every other language. Maybe "really fast" is slower for languages that impose more restrictions (like Haskell), but it still happens everywhere. This user adding example is kinda a bad example. I can imagine adding features like: * including the user's ssh authorized_keys * maintaining group membership * including the user's dot files (.bashrc, .vimrc etc) Considering adding those features, I still don't see any benefit from Haskell the language. I'm purposely excluding issues where the shell based solution might be easier to "port" across multiple OSes or whatever. With shell, it's possible to improve maintainability by breaking things into separate scripts, where each one does something specific. And along those lines, if you had some functionality that was trivial to implement in another language, it'd be (probably) trivial to integrate with your shell script (I think this is evident from the example I gave, where I'm using tr and grep (obv all of the usual *nix tools are at hand, I think there was/is a project to implement many of them in Haskell, I suspect they'd need a slight revamp to switch to using Text, but it might be interesting to combine with Shelly)). I'd really like to know what the author of the article (or anyone who "gets it") sees as his reason for reinventing things. To me it seems like a case of "well, I know and love Haskell, so I'll use it for everything" but it's kinda pointless otherwise. I think I'd like to have the option of using a shell (or a *nix) that supported other datatypes between processes than streams of bytes, but I don't think that Shelly is achieving that goal (obviously? doesn't seem it's part of the design).
Interesting, is there some expansion that sticks pipes in the appropriate places?
What would that be?
Right... but arguably the current behavior is just broken. At the very least, it is incompatible with hiding (i.e., not exporting) portions of a record-based API, since it exposes information that is global about the type. Consider: module Foo (Foo, x) where data Foo a = Foo { x :: a, y :: Int } And we have: Prelude Foo&gt; :k Foo Foo :: * -&gt; * Prelude Foo&gt; :t x x :: Foo a -&gt; a Prelude Foo&gt; :t \r f -&gt; r {x = f} \r f -&gt; r {x = f} :: Foo t -&gt; a -&gt; Foo a But make a change to an unexported field: module Foo (Foo, x) where data Foo a = Foo { x :: a, y :: a } And now: Prelude Foo&gt; :k Foo Foo :: * -&gt; * Prelude Foo&gt; :t x x :: Foo a -&gt; a Prelude Foo&gt; :t \r f -&gt; r {x = f} \r f -&gt; r {x = f} :: Foo a -&gt; a -&gt; Foo a In other words, all the types and kinds in the exported types and terms from Foo are the same, but somehow a term in `Main` is given a different type. Information about the field `y` is leaking out of this module, despite its being hidden. In order to know that performing an update like that is safe, you have to know all of the constructors and all the fields of the entire type. Haskell currently just looks, even if the information wasn't *supposed* to be exported in the API. I'd be in favor of replacing this IMO broken feature with a variation on record wildcarding... something like: \r f -&gt; Foo { x = f, _ &lt;- r } which would explicitly require (like any construction) that the constructor and all of its named fields are exported. This would be more limited than the current system, where presumably you could have a variant type where all the constructors just happen to share one field, and where it's the only field using a given type parameter... but frankly such cases are crying out for some improvement anyway.
Indeed. And clever construction can eliminate the proof terms entirely, at least in many useful cases, making it clear what is meant without resort to extra proving. The obvious definition of typed lambda terms, for instance, makes no use of well-typedness, it just enforces well-typedness inherently, in a way that makes it absolutely clear precisely how the constraints are to be satisfied. Which, I think, makes later changes easier to implement.
GHC is not a cross compiler.
I thank you for pointing out there are some pain points in developing Haskell on Windows. I'm not certain if the command line environment is really the blame for why Haskell is so difficult. I think it is more of a philosophical difference between the platforms. Common libraries in standard locations for *nix type systems versus local libraries in windows. I don't like it, I was looking the other day at how many different copies of OpenSSL I actually have on my Windows instance. And as far as gcc goes, between the versions of gcc I have that come with cygwin, Haskell Platform, Ada, Python, the standalone instance of mingw32 and whatever other languages I have installed. It's almost a guarantee that dependent libs aren't going to be jacked up, and you'll be in for a fight.
&gt; It's not an argument, it's a fact about human cognition and communication. I'm not sure what you're having trouble understanding here. The argument I'm referring to is the one you re making about what constitutes good language design. You've argued that languages should be as terse as possible. I contend that this is not always optimal. The "terse is always better" argument applies to automatic coercion just as well as it applies to custom infix operators, juxtaposition for application and return type overloading. So if you believe that terseness is *always* better then you must believe that automatic coercion is a good thing. If you don't believe automatic coercion is good, then you must admit that it's possible to be *too* terse. In fact, you go on to admit this when you say: &gt; when the stuff to content ratio is bad this leads to a bad reading experience. (That ratio can fail either in the direction of Java, or in the direction of APL, mind.) In other words, being concise is good, but it's possible to go too far. You give APL as an example. I'd argue that Haskell's syntax isn't much better than APL's in this respect. I guess the fact that it sticks to ASCII is an improvement. &gt; The more conventions there are, the more people have to learn them and therefore the harder the learning curve is. Right. This is one of Haskell's problems. The learning curve for learning to even parse an expression is way too steep. Worse, you can't even learn the precedence for all of the operators, because every library can introduce a bunch more. I predict that your objection is going to be that the same problem exists with libraries introducing new functions (or types, for that matter). The difference is that even if I don't know what an unfamiliar function does, I can still at least parse expressions it appears in. I can work out what the subexpressions are, and I can ignore or gloss over the parts that aren't important to understanding what I need to understand at the moment. This is not true of infix operators. An expression with infix operators cannot be parsed without knowing the fixity rules of all of the operators involved, so any expression with unknown infix operators is a "syntax error" as far as the reader is concerned. This is why a well designed language will have a small set of infix operators (so readers have some hope of learning all of them), and when using a language that doesn't do this, people who care about maintainability should insert parens to disambiguate whenever "unusual" operators are used. (In Haskell this would mean anything outside of Prelude, and perhaps even for some of those.) I am increasingly under the impression that most Haskell programmers don't actually care about making their code readable, however, and are instead more interested in playing golf.
 data A = A { b :: B } data B = B { c :: C } cb :: A -&gt; C cb = c . b
I think it ended up taking many seconds to loads all of the DLLs for every command, and was discontinued..
Wikipedia was my first stop indeed, but it remained abstract to me until I read the introduction of this thesis.
"How about distributing as an LLVM binary though?" Do you mean something like compile on runtime or generating byte code and then to bundle it with the platform specific interpreter? 
Without static types, very abstract code becomes much harder to read, write and maintain.
Though there has been some recent effort to make GHC a cross-compiler, I don't think there's anything functioning in in 7.4. Since you have qemu-arm running I would try building GHC 7.4 in an ARM VM and using that to compile code for your R-Pi.
No, I missed to point that out but the pipes are explicit in the alias definitions alias -g G="| grep -P"
Repeating a rumor without checking, very stupid of me :-) Thanks for the correction...
I think "foldp" should probably be named "scanlp" or "scanp"? It seems like more of a scan than a fold... If you treat the input signal as a list, then the output signal is a list too...
I'm not proposing changing semantics, so it should not cause any leak or such. I just think the meaning of a Signal-&gt;Signal function is more a "scan" than a "fold" because the result signal is in a sense (denotationally) a "list" of values.
If simply using a strict language was sufficient to fix the FRP space leak then we would have a bunch of strict FRP libraries on Hackage. The space leak is much deeper than that.
Well hopefully people are using it successfully, since I'm getting requests for new features... If nobody else I'm using it (-: And yes, we have stopped bundling some Haskell code with it. Now the Haskell side comes straight from Hackage, hence the mention of "companion packages". But as with any non trivial package you CAN get into dependency hell. I always check the packages build after a clean HP install and a cabal update...
B-but... in handwritten code, are we supposed to use tabs, or spaces!?
`foldl` in conjunction with (almost all) monads are worthless on big chunks of data, even when using random-access structures, but `foldr` without lazyness is worthless on big chunks of data too. Take your pick, but I'm *not* going to use monads in a strict language unless it has some *smart* optimizations.
First, I think I wrote the post a bit wrong before, I'll fix that first, but that shouldn't change your post. Basically, monads don't need lazyness alone, but you it for `foldr`, and many monads are worthless with `foldl` as that usually gives either stack overflow or quadratic performance. So basically, monads with folds need lazyness because... * `foldl` and monads gives stack overflow or quadratic performance * `foldr` and no lazyness gives stack overflow when using `[]`, and a huge delay of constructing the action when using something random access. Actually, now I'm thinking of it, folding over huge data when using `[]` is a bad idea. I found out about this when trying to make a pure functional library in Scala.
Take a list of all the numbers from 0 to 2000000000. Use `map print`. `foldl (&gt;&gt;)` or `foldr (&gt;&gt;)`. Watch it crash.
I should probably mention how or whether this relates to the reactive-banana FRP library. While using call-by-value semantics for FRP does have merits, it is not mandatory for a leak-free FRP experience. The first-order part of reactive-banana handles everything correctly: `accumE` is strict by default and Hudak's described loss of sharing does not apply. The real trouble only starts when you add dynamic event switching (which Elm excluded from the start). It is possible to rule out time leaks statically (elerea, sodium, reactive-banana-0.7), but I already anticipate a few other issues. Dynamic event switching will stay a bit experimental for some time, but I do not think that the issues are fundamentally insurmountable. In any case, they are definitely not related to strictness/laziness. 
It sounds like your implementation is not leaky, but can users still create their own leaks (as in [this example](http://blog.edwardamsden.com/2011/03/demonstrating-time-leak-in-arrowized.html) and the general approach outlined in my post)? I think `accumE` would need to use `deepseq` to avoid leaks entirely, or is that what you mean by "strict by default"? On the separate issue of dynamic switching and dynamic collections, yes, strictness/laziness are not directly relevant. I'll do a post on AFRP in Elm soon to clarify how Elm will likely handle these issues.
I'd make a distinction between adding laziness ([OCaml style](http://caml.inria.fr/pub/docs/manual-ocaml/libref/Lazy.html)) rather than adding strictness (Haskell style). Using something like `(lazy :: a -&gt; Delayed a)` and `(force :: Delayed a -&gt; a)` is more verbose, but I know exactly what I am getting. Do you think OCaml's explicit laziness is insufficient? Laziness needs to be at the language level? I am unfamiliar with your example as I have not worked with pipes, so I'd like to know more about the distinction you make between push and pull and their relation to categories. &gt; It would be interesting if Haskell exposed an evaluation monad where binding values evaluated them. Agree, that could be really cool :) This may be a clearer way to deal with strictness and laziness.
Yes, users can happily create their own leaks if they so wish. This cannot be avoided in *any* language, be it lazy (`foldl (+) 0 [1..10^6]` leaks) or strict (`foldl' (+) 0 [1..10^6]` leaks in a call-by-value language). Rather, the point is that events and behavior should have the memory semantics that you would "expect" from an ordinary first-class value. The `accumE` combinator behaves like `foldl'`. It does not use `deepseq` for the same reasons that `foldl'` does not use `deepseq`.
PDF paper by Gonthier, Ziliani, Nanevski, Dreyer http://www.mpi-sws.org/~beta/lessadhoc/lessadhoc.pdf
The main reason I've never tried EclipseFP, despite installing it once or twice, is that it doesn't let me say "Build a project from this .cabal file." Lacking that makes it extremely impractical for occasional use.
&gt; It would be interesting if Haskell exposed an evaluation monad where binding values evaluated them. Like the strict identity monad? Or you mean something different here?
Can't the RTS smartly evaluate how much it can before filling the memory?
Ah OK, my mistake, thanks
I'm sure you can easily build them out of lenses. I still don't think they're terribly useful. The small subset of cases where programmatically varying which label I assign or read from that can't easily be done by passing functions around really doesn't seem worth catering to.
How does this compare to optparse-applicative?
I did not intend my statement to be a provable fact about the universe. Here is a more precise way of phrasing my statement: If simply using a strict language was sufficient to fix the FRP space leak then *it is highly likely that* we would have a bunch of strict FRP libraries on Hackage. &gt; in a lazy language, no matter how clever you are you can still have leaks This is a pretty wild claim. Just because you have an example of somebody getting it wrong doesn't mean it is unavoidable. I do agree that Yampa does not make this particularly easy to reason about, however. It may even be the case that Yampa simply doesn't provide a reasonable interface to be able to control it in all reasonable circumstances. &gt; The paper I linked on space and time leaks [1] discusses one issue that is caused by laziness (recursive functions building up useless thunks) which is not really an issue with FRP itself. I've read the paper many times, and I think you misunderstood it. Your blog post also reflects this: &gt; “We show that recursive signals in standard implementations using streams and continuations lead to potentially serious time and space leaks under conventional call-by-need evaluation.” In other words, FRP had serious space leaks in its implementation due to laziness, and tons of work went into figuring out how to avoid this. The paper only said that this does indeed occur under call-by-need evaluation. Your interpretation is wrong in two ways: The paper does not say anything about this problem going away in a strict language. In fact, the problem *does not* go away in a strict language. The problem can be characterized by the implementation creating a chain of functions. Neither call-by-need nor call-by-value can evaluate inside the body of an unapplied lambda. The paper's solution was to change the interface in such a way as to disallow certain kinds of computations. There are other lazy evaluation orders besides call-by-need. In fact, the paper explicitly mentions optimal evaluation as an example of an evaluation order that gets it right. Also, in [the corresponding slides](http://www.cs.ox.ac.uk/ralf.hinze/WG2.8/24/slides/paul.pdf), they claim that completely lazy evaluation solves the problem as well. In other words, not only is laziness not the problem, more laziness is a *solution*.
I was at the end of development of this first release when I saw optparse-applicative, so I have not yet played around with it enough to make more than shallow comparisons. If I say anything misleading I would appreciate it being pointed out. The largest differences in my view are 1. CmdTheLine uses a type class to represent values that can be parsed from the command line. This has the advantage that converters are inferred from context instead of being built in by hand as is the case in the combinator approach. Contrariwise, if you want to convert a value multiple different ways you have to use `newtype`. 2. CmdTheLine seems to favor records over combinators for building information about arguments and commands. 3. CmdTheLine sacrifices type-safety for a what I consider a cleaner interface. Options and Positionals are only distinguished by the values of their fields, not by types, so there is a (guaranteed every-run)run-time error if you treat them incorrectly. 4. OptParse-Applicative doesn't produce full man-page documentation. [EDIT]: and CmdTheLine comes with oodles of examples.
Eclipse has the notion of projects built in. EclipseFP use the Cabal file as the main source of information. When you have a Haskell project in EclipseFP, Eclipse builds your project as cabal build would. So I'm not sure what you're missing here. If you have an existing project, you can import it into EclipseFP. You can even create an EclipseFP Haskell project in the location of an existing project, existing files will not get overwritten. Feel free to contact me or create a bug report if you feel a usage pattern is not correctly addressed.
You can always install packages from within Eclipse with the Cabal Package view. But are you sure you're not using the proper ones? The path won't change since the executables will be installed in the same folder. Please describe your problems on the sourceforge forum, giving as much info as possible, and I'll try to help.
&gt; This is a pretty wild claim. Just because you have an example of somebody getting it wrong doesn't mean it is unavoidable. I don't think it is so wild in context. I claimed that space leaks can occur when using a lazy `foldp` (or its equivalent) and that happens to be in the straightforward usage. I did not claim that it must *always* be a space leak, which would just not be accurate thanks to `deepseq` (i.e. strictness). I do not know another way to get around the leak described. Thank you for your thorough comments on the linked paper! I think you are correct on both accounts, and the slides you mentioned were quite helpful. I misunderstood the problem that cropped up in the implementations they described. I saw big computations building up and assumed laziness was their cause. I did not realize that this was a design issue :/ It looks like they needed *more* sharing because computations were getting duplicated inside and outside of a continuation, thus making completely lazy evaluation a good choice. I don't know enough about the "optimal evaluation" strategy to say whether it is practical. I just wanted to note (for people who didn't look through the slides) that the slides say more laziness is the alternate solution, but neither the solution they spend the majority of the time discussing nor the solution they ultimately use. I guess laziness ends up being orthogonal to the problem described in the paper and to the solution they end up using. In any case, I'll correct my misunderstanding in the post. Thanks again for the thorough response :)
On my iPad, the site caused safari to crash :'(
Are there any option parsers that also do shell completion?
Hey, this looks familiar :). 
Very nice! One quibble on metaphors. "Glue" is how other web frameworks combine components. When you glue something, it and you are stuck; if you want to rearrange the parts later it'll take a sharp object and leverage, and something will probably get broken. But Snap is modular. Parts just fit together, and connect with a satisfying ... um, click. Or some other sound.
&gt; This [C++] code doesn’t look as imperative as its Haskell counterpart This made me smile.
I think your original interpretation is correct (or at least how I am reading it too). The problem is writing `[0..10^9]` in a strict language. More generally, the problem is building up huge data structures in memory, which can be a problem whether you are lazy or strict. Important trade-offs between laziness and strictness is how easy it is to tell when you are building large data structures, and the cases when you can avoid it.
Thank you. I consider this enough of a problem to have uploaded a minor revision package. Bad documentation sucks.
I think the ultimate answer to this is a proper system for both codata and data and totality checking.
I have not touched Snap in a while, but compared to Rails, there still seems to be a fair amount of boilerplate.. How important is it to the community to make simple sites as easy as it is in Rails?
I have some code that generated bash completions for an early compiler design of mine by making an extra command line argument that provided the expansions that could be invoked by the shell to complete for you, but I never got around to packaging it up as a separate package -- I got bogged down in adding l10n and i18n support to and early version of cmdargs, because obviously it is more important to have en@lolcat be a usable locale than to ship a compiler or package my tools. The idea was that I could supply a bash script to put in the completions folder for any executable i wanted to install this way, and perhaps hook the installer to load it.
I have heard an anaogy that Snap is to Sinatra as Yesod is to Rails. Perhaps that is what you are thinking about?
I'm not a web developer, but even I would be interested in concrete examples of differences between the two frameworks.
...plus CmdTheLine doesn't use template-haskell. Might make it more suitable if you don't want to add 2MB to your executable "just" for command line parsing. (At least as long as the package zeroth is unusable with GHC 7)
Another problem is that, when you declare each data type in a module of its own you quickly bump into http://www.haskell.org/haskellwiki/Mutually_recursive_modules Maintaining hs-boot files is no fun.
 exec :: (Term a, TermInfo) -&gt; IO a run :: (Term (IO a), TermInfo) -&gt; IO a Thanks for providing both: I don't like being stuck with IoC, and providing only "run" would have kind of resulted in that. (BTW: why tuples as arguments?) I noticed one thing: the applicative syntax is good for letting you choose between IoC and normal workflow. Either you use a datatype to store everything or you directly build the IO action that'll run your program. Nice. Well thought.
Because of the dependency: optparse-applicative (even without providing a TH-dependent API) uses it internally and then has a cabal dependency on the 'template-haskell' package, which is heavy. So it links with it, because there's no way (currently?) in cabal config to express the concept of a "compile-time only dependency" i.e. to give it the information that this link is unnecessary. But it might be even more intricate.
And most of the code that is very complex logically speaking is simple in other ways, e.g. it is often referentially transparent.
Personally, I always felt very uneasy about all the magic that was going on behind the scenes with Rails. I was always worried, that I would trigger some "convention over configuration" behavior, that I didn't actually want. And sure enough, if you look at the history of Rails security incidents, they are often related to some of that magic going wrong and having unintended effects. If I'm writing an Internet-facing application, I want to see how all the parts fit together. So I rather have some boilerplate, then too much automatic stuff happening.
&gt; Sure, same as every other language. No. In shell the lack of proper data types really hurts you. Ever tried to extend a shell script that stored record-like data in columns in a string to handle an additional column?
Well, as far as I know none of the common scripting languages are installed by default on Windows either, not any of the standard shells, not Perl, Python, Ruby,...
Wow. Nice slideshow presentation. Is it custom?
What specific "boilerplate" are you referring to? I've never used Rails, so I'm curious to hear your thoughts.
Why do I need to cabal install my projects to run them? Can't I build and run in place? Especially in development mode?
boo!
It uses deck.js, a pretty cool css/js library for doing presentations 
Short version: you don't need to. In our tutorial materials, we use "cabal install" fairly frequently in places where "cabal configure; cabal build" would work mainly because it's more concise, not because you can't build and run them in place. Also, as of Snap 0.9 the default project template has dependencies that none of the other snap packages have, so you'll need to do a cabal install the very first time to get those dependencies installed. But once you have those, then building in place is fine.
Awesome, I am looking forward too it. Maybe this can make it into the next major version of CmdTheLine.
Yes, we're only using "cabal install" for it's install dependencies behavior. The starter project depends on the hint dynamic loader for development mode. This code used to be included in the snap project, but in 0.9 we factored it out into two separate projects snap-loader-static and snap-loader-dynamic. This lets people use the loader without having to install the snap package (which is kind of an integration package that requires some extra things like Heist). It also makes it so you don't have to build snap with -fhint if you want to get development mode.
Are Heist templates XML? If so there are some problems in the slides: Template on slide 13: &lt;year&gt; isn't closed. On slide 21: &lt;flavor&gt; isn't closed. On the other hand, if Heist templates are HTML, then how does it know which user create tag names are self-closing? 
Heist templates are a hybrid of XML and HTML, hence the reason we wrote the xmlhtml library (see http://cdsmith.wordpress.com/2011/02/05/html-5-in-haskell/ for more details). That being said, you should always close tags that you define yourself. That's a mistake in the presentation.
We could offer non-command-line alternatives, like GUI versions of cabal and mature IDEs.
Is there a package that provides these?
Can somebody explain why scheme makes such a big deal about continuations?
Generally speaking those tend to not work very well, I mean the GUI tool "for the less experienced users", judging by e.g. GUIs for VCS tools or graphical programming languages. Mostly the problem seems to be that all the experienced users use the command line version so when the new users have questions about the GUI workflow nobody can answer them (not to mention the fact that it is a lot easier to write "type a and then b" then to guide a new user through a GUI).
ah, thanks for pointing that out, i've fixed the slides!
I don't think so, unless the pipes themselves are made lazy explicitly.
yeah, like mightybyte said, it's for dependencies. i suggest using virthualenv (a great tool!) to keep the dependencies for each snap project nicely contained.
Alternatively, you could add an option to dump usage information in a well defined format and then use something like [compleat](https://github.com/mbrubeck/compleat)^1 to transform it into shell specific completion functions. compleat only supports bash and zsh at the moment, but it could be extended to other shells I am sure. ^1 I haven't actually used compleat nor do I know if it actually compiles, but it's an awesome idea.
Yea, I have tried things like that. But that's when I switch to another language, typically Perl, sometimes Python. As long as it uses stdin and stdout, there's a good chance that my original shell script can use the new functionality seamlessly. Maybe it's possible to replace Perl or Python with Haskell and Shelly, but it's probably not going to happen in my use since it's a lot of management overhead. The whole point of *sh isn't that it's a do everything, solve all problems language, it's that it's often the right tool for the job.
I'm doing my part to help increase the count! :-)
This looks great. I may try it out! (or at least mull over doing so)
&gt; 1. (&gt;&gt;=) m k = \w0 -&gt; ... is the same as (m &gt;&gt;= k) w0 = ... Correct. The second version looks very weird to me, but it's valid haskell syntax and equivalent to the first version. &gt; 2. IO b is really \w -&gt; (b, w) You're mixing notation for types and values. Write it like this: `IO b :: w -&gt; (b, w)` where `w` and `b` are both types. (**EDIT:** As a completely separate issue, this is not what an `IO b` is. I discuss a beter way to think about `IO` below.) &gt; the call to k with r then w' will yield the tuple (b, w'), I don't think so. Yes, `k r w'` is a function call with two parameters, but here `k`,`r`, `w`, and `w'` are values, whereas it's not clear what `b` is... And it's wrong to have `w'` on both sides of the equation. The `'` is pronounced "prime", and it's a convention used in math to describe a new value of a changed type. The idea is that `w` ("double-u") goes in, and `w'` ("double-u prime") comes out. I suspect the letter `w` was chosen as an abbreviation for the word *world*. Think of it like this: every time an `IO` function is called, *something changes in the world*. But in haskell, variables can't actually change, so the only way to change a world is to pass the world in to the function and get a new world out. You can think of `IO b` as a synonym for `(w,b)`. Now you can translate: (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b as: (&gt;&gt;=) :: (w, a) -&gt; (a -&gt; (w, b)) -&gt; (w, b) Remember, when you're using `::`, these are *types*, so here, `w` means "type of the world". The two parameters of `(&gt;&gt;=)` are a value of type `a` wrapped in some kind of container, and a function that takes values of type `a` and returns a value of type `b` wrapped in the same kind of container... The `(&gt;&gt;=)` simply unwraps the first value and passes it to the function so it can be transformed and re-wrapped. The idea of unwrapping something, transforming it, and re-wrapping the result in the same kind of container is so general that we have reason to do it all the time. So `(&gt;&gt;=)` is defined not just for the `IO` type, but for a whole class of container types - the class of Monads. With the `IO` monad in particular, part of what may be confusing you is that you can't actually see the `IO` constructor. It's not defined in the Haskell prelude, or in any module... It's just a special type used for everything that's outside the haskell's purely mathematical universe. The "secret" nature of IO's definition is *why* you don't see that "third parameter." But: if you search for `instance Monad` in the [standard prelude](http://www.haskell.org/onlinereport/standard-prelude.html) you can see how to define `(&gt;&gt;=)` for other types (`[]` and `Maybe`), and that might help you understand it a little better. 
Wait, it's a primitive? I thought you could implement it within the language the same way Haskell does.
Don't you mean: (&gt;&gt;=) :: (a -&gt; (w, a)) -&gt; (a -&gt; (b -&gt; (w, b))) -&gt; (b -&gt; (w, b))
Yes, all of your observations are correct. It's important to understand that this is an example of the `State` monad, (basically) defined as: type State s a = s -&gt; (a, s) So Simon is pretending that `IO` behaves like the `State` monad: type IO a = State RealWorld a ... which is *sort of* true (i.e. that's how GHC represents the type under the hood), but it's less clear cut than it sounds, since obviously we don't observe or control the state of the real world. The following tutorial on the state monad might help you: http://brandon.si/code/the-state-monad-a-tutorial-for-the-confused/
How does this differ from: EitherT ExceptionType IO () Edit: Oh wait, didn't see that it was a type synonym.
I can't speak for the whole community, but as one tiny little part of it: it is not important at all to me. The only aspects of rails that are quicker to prototype with are scaffolding, which you have to throw away and do it all for real anyways, so it doesn't actually save any time. And the ORM layer, which can save a bit of time on simple tasks, but makes normal tasks more difficult and has been a constant source of security holes.
Why do you have: type Throws t m a = m a Rather than the simpler: type Throws t x = x 
I thought I understood Applicative functors for a long time, and I did, but I had a nice "aha" moment about them: Basically applicative functors lift application to inside a functor, right? &lt;*&gt; :: f (a -&gt; b) -&gt; f a -&gt; f b However, this requires that the function inside the functor on the LHS of `&lt;*&gt;` be *pure*. If we pass a function of type `a -&gt; f b` (i.e an "effectful" function) into `&lt;*&gt;`, we get this: &lt;*&gt; :: f (a -&gt; f b) -&gt; f a -&gt; f (f b) So, this was an "aha" moment, because in order to get that `f (f b)` into an `f b`, you need *join* - a *monadic* operation. So, this really crystallised for me exactly what monads have that applicative functors do not. 
Hmm, I don't know that such an optimization could even be done in Haskell code itself, but I'd be surprised if GHC weren't able to recognize when multiple field modifications were taking place (with the intermediates being immediately discarded, of course) so it can fuse them together.
Tools could analyze the haddocks just as easily, I imagine.
Because that's not usable. It would have to be type Throws t (x :: * -&gt; *) = x which requires `KindSignatures`.
I don't know if I ever had a real aha moment with Haskell, but I do remember that when I first read about the language, the type signatures made *no* sense to me. I don't remember what point it was that where they became second nature, but I'm very glad they did.
Super-excited about this because unlike GWT, it's not tied to a particular backend and it's not Java.
Yeah, I was thinking about making these changes and sending a pull request on github, but I didn't get around to it. All instances of a route with the form ("/foo", render "foo") should already be handled by heistServe and should be able to be removed. The only thing that needs to be changed to make that work is to change: nestSnaplet "heist" heist $ heistInit "templates" to nestSnaplet "" heist $ heistInit "templates"
How ContT makes anything into a monad. newtype ContT r f a = ContT { runContT :: (a -&gt; f r) -&gt; f r } instance Monad (ContT r f) where return x = ContT (\k -&gt; k x) m &gt;&gt;= f = ContT (\k -&gt; runContT m (\a -&gt; runContT (f a) k)) Look ma, no `instance Monad m =&gt; Monad (ContT r m)`! This along with the Codensity variation `newtype Codensity f a = Cod { runCoD :: forall r. (a -&gt; f r) -&gt; f r }` magically turn all the binds in a computation to be right-associative, which is a great thing for free monads that have a painfully slow left-associative bind.
And how glad we all are that you did :) Any word on the revision that was tweeted about a while ago?
* Understanding the State monad instance * Pattern-matching is a better/safer basic operation than boolean "if" statements * Maybe type -- such a simple way to get rid of null dereferences * Difference in power between Applicative/Monad * Covariance and how it relates to Functors/Applicatives/Monads * Catamorphisms, church encodings * The "From lists to streams to nothing at all" gave me a big a-ha about how low-level functional code can be almost assembly-like and very easy to optimize * Rank2 types and that type-classes sneak rank2 types into Haskell98 * [SECs](http://conal.net/blog/posts/semantic-editor-combinators) * Algebraicness of data types, how sums, products and exponents work I'm probably forgetting many more. Needless to say, studying Haskell has been very very pedagogical to say the least :)
The "State RealWorld" representation is really just a GHC hack, and is not what IO really is...
Monads are just monoids in the category of endofunctors!
Well, I was talking about optimizations like reusing the old copy if it knows it is no longer referenced after the update. For the lenses I meant a manual fusing.
How to implement context reduction was my biggest aha moment (I couldn't really get any aha moment from Haskell code since I couldn't run it; there were no Haskell implementations).
First aha moment: Touch huge amount of code / refactoring, then run compiler, then fix compiler found errors, then run smoothly. Second aha moment: Try to do the same on Java project.
&gt; (&gt;&gt;=) :: (a -&gt; (w, a)) -&gt; (a -&gt; (b -&gt; (w, b))) -&gt; (b -&gt; (w, b)) Hmm. I can kind of see why you might say that, and I really had to think about this before answering, but... No. An `IO a` isn't a function... It's an `a` wrapped in a special container that represents the state of the world. For the sake of argument, I was saying "just pretend it's a tuple". The first argument (left side) of `&gt;&gt;=` is always a wrapped value. The second argument is always a function that takes an *unwrapped* value and re-wraps it. I just typed this in GHCi: Prelude&gt; let w0 = return "hello world" :: IO [Char] Prelude&gt; :t w0 w0 :: IO Char Prelude&gt; let w1 s = putStrLn s Prelude&gt; w0 &gt;&gt;= w1 hello world Here, `w0` doesn't do anything and takes no arguments so it can't be `a -&gt; (w, a)`... In fact it would be `(w, [Char])` in this hypothetical notation, so after your rewrite, it would become `[Char] -&gt; (w, [Char])`... And that just can't be right. I think the reason your version looks so plausible is because you're reading the `m a` type constructor as if it were a function. So, here's another example using something that's much more obviously a value: Prelude&gt; [1, 2, 3] &gt;&gt;= \x -&gt; [x, 0, x] [1,0,1,2,0,2,3,0,3] Here, `&gt;&gt;=` took each number out of the list on the left, unwrapped it, and passed it to some arbitrary function that creates a new list. Since `&gt;&gt;=` happens to be defined in terms of `concat` for lists, the result is one giant list that concatenates all the generated lists. 
Oh... I didn't mean to imply that I accepted his definition. I was making a point about how the OP was confusing types and values... And then as a separate point later on, I suggested thinking of `IO x` as if it were a tuple. Thanks for pointing that out. I edited the original.
People are mean with you ;) That was well put, that made me laugh.
They could do it with macros, but they have just one ambient monad with built-in "do" notation, so you can't overload the semicolon but you can rewrite it with a macro.
I don't get why people are downvoting you. For me it was a *real* "aha" moment. Before I could understand instances of monads, but didn't feel what they actually are.
Well, that requires `TypeOperators`. I'm quite happy to just write standard Haskell for the purposes of a trivial one-liner.
* The interaction between Arbitrary and CoArbitrary: function instance for Arbitrary uses CoArbitrary, and for CoArbitrary uses Arbitrary [see section 3.3. in [quickcheck paper](http://www.eecs.northwestern.edu/~robby/courses/395-495-2009-fall/quick.pdf)] * ST uses rank-2-polymorphism to avoid leaking of references * Seemingly impossible functional programs * Understanding monads in general
That's orthogonal to my point. If you don't like the type operators: foo :: Throws Exception (IO x) Works just fine without kind sigs.
&gt; I don't get why people are downvoting you. Maybe they believed it was a troll ;)
&gt; isAlphaNum x = isAlpha x &amp;&amp; isNum x I think you got that wrong...
Huh. I've never seen a context where people used monads in a way that doesn't represent effects. Could you give an example?
I just did: &gt; An example, ASTs as functors (paramaterised by a variable name type) don't really represent effects per se, Kleisli arrows in an AST monad represent name substitution. If you haven't heard the jargon before, an AST is an abstract syntax tree, used for representing terms in a programming language. Edit: I'll update this post with a more detailed explanation of this example. Edit: Okay, so imagine we have lambda calculus, here's a grammar. Most of it should be familiar to you, from Haskell: term ::= (term) (term) | λ(var). (term) | (var) var ::= a,b,c, ... A nicer way to represent terms is with de bruijn indices, which assign numbers to binders instead of names: term ::= (term) (term) | λ. (term) | (var) var ::= 1,2,3,4,5... So, if we have our Haskell encoding of this representation: data AST = App AST AST | Lambda AST | Var Int The problem is, this lets us refer to invalid (free) variables that don't have binders, so let's restrict the type representation of our terms a bit, by making it take the type of available variables as a parameter. Lambdas introduce a new variable, so we need to use a name type that has exactly one more element, namely a Maybe type. data AST n = App (AST n) (AST n) | Lambda (AST (Maybe n)) | Var n And introducing a type with no values: data Bottom Therefore, closed terms are AST Bottom, and terms with three free variables might be represented as AST (Maybe (Maybe (Maybe Bottom))). Note we have a monad instance here, bind: AST n -&gt; (n -&gt; AST m) -&gt; AST m Is applying a substitution function from names to terms to a term.
Apparently not, although given current record syntax, generating each lens in TH should be fairly straightforward. data Pair a b = Pair { _pi1 :: a, _pi2 :: b} deriving Show pi1 f p = (\x -&gt; p { _pi1 = x } ) &lt;$&gt; f (_pi1 p) pi2 f p = (\x -&gt; p { _pi2 = x } ) &lt;$&gt; f (_pi2 p) Building this off of an existing lens library (e.g. [fclabels](http://hackage.haskell.org/packages/archive/fclabels/1.1.3/doc/html/Data-Label.html)) feels like it *ought* to be straightforward import Data.Label import Control.Applicative data Pair a b = Pair { __pi1 :: a, __pi2 :: b } $(mkLabels [''Pair]) fcToVanLaarhoven label f p = set l p &lt;$&gt; f (get l p) pi1 = fcToVanLaarhoven _pi1 pi2 = fcToVanLaarhoven _pi2 but there are some type issues with this, and my type-foo isn't strong enough to give `fcToVanLaarhoven` the correct type.
Understanding how the STG machine worked.
Don't you mean `ContT r m a` is just `Cont (m r) a`?
I don't know why you got downvoted. I think you're right, both philosophically and practically. I'm not even sure that `State` is the right metaphor for `IO`. I still think the correct solution might be some form of a free monad.
Oops. I typed my own signature wrong. I actually meant: (&gt;&gt;=) :: (w -&gt; (a, w)) -&gt; (a -&gt; (w -&gt; (b, w))) -&gt; (w -&gt; (b, w)) This has the correct behavior. And to answer your question about return: return x = \w -&gt; (x, w)
How do you mean? I realize `isNum` is actually `isDigit` in `Data.Char`. I don't think that ruins the example. You'll have to forgive me I am sick and wrangling my niece around. A few things are bound to slip :P.
The Haskell platform is the current approach to reducing dependency hell by providing a set of core libraries that all have the correct dependencies. It's a short-term fix until we can solve the longer term issue of dependency management, which is as bad as you describe. I think the main issue is that Haskell developers tend to prefer bleeding edge libraries, but this generally conflicts with the dependency stability of a rolling release system. The Haskell platform is a compromise between those two tendencies by providing a rolling release system for the global core libraries that are very commonly used, and then leaving the user-space libraries for you to sort things out.
Because this specific "aha" was about the function instance, I wanted the types to illustrate that specific concrete example. I also wanted to say something about `pure = k` and `&lt;*&gt; = s` from combinatorial calculus, but I'm still trying to wrap my head around that one.
Thanks for that. =)
Oh, I thought you were suggesting &gt; type Throws t (m :: * -&gt; *) = m &gt; foo :: Throws Exception IO x because that's the format I proposed. But you meant put the whole action type in there [as someone else proposed five hours earlier](http://www.reddit.com/r/haskell/comments/vhnvv/an_idea_for_annotating_io_actions_that_throw/c54l0zx). I'm just acknowledging that we're on the same terms now. I don't have any opinion about your/their suggestion. Requires more parens, less with infix operators. Meh. Doesn't really matter in the type-alias-vs-documentation discussion.
Something alphannumeric is EITHER alphabetic or numeric, not both.
The thing is that I'd like an example where bind is used for the ADTs.
In the same way that lists are the best monoids.
Exactly.
I really hope its compiler is named Siegfried.
I'm not saying they're *side*-effects, I'm saying they are effects. This is probably very vague, but IMO `Maybe`, `List`, `State`, `Free` and `Cont` are obviously effects.
Right, but my point is that your definition of effects is so general that encompasses everything and thus isn't very meaningful.
&gt; Ryan Ingram's observation in the original post that you can do without the type class still goes through at a more liberal type signature. &gt; &gt; data AnyF a a' b' = AnyF a (a' -&gt; b') &gt; anyRef :: (b -&gt; AnyF a a' b') -&gt; (forall f . Functor f =&gt; (a -&gt; f a') -&gt; b -&gt; f b') &gt; anyRef k m a = fmap f $ m b &gt; where AnyF b f = k a Indeed you are correct. Ryan Ingram's [original `AnyF`](http://www.twanvl.nl/blog/haskell/cps-functional-references#comment-comment8) is the [Store comonad](http://hackage.haskell.org/packages/archive/comonads-fd/2.0.1/doc/html/Control-Comonad-Store-Memo.html#t:Store), and your `AnyF` is the [indexed store comonad](http://comonad.com/reader/2011/free-monads-for-less-3/). Unfortunately getting `(.)` to work with the indexed store coalgebra version is tricky.
I just tried it and got up to 'Another senior Haskell programmer' easily but stumbled on the next one. I got long ways to go ... 
My definition of effects is something along the lines of "endofunctors that are not constant". I have yet to see an example where this is too general. For example, the free monad of `a^2` or `a x a` is an endofunctor which represents values dependent on streams of bits. And sure, constant functors can be given monad instances too, but those monad instances aren't used (to my knowledge). Here are some examples of functors that I don't think represent effects: A^x A Set (it would if we used mathematical sets) x^x
Ok, what about the Reader monad. That is a commonly used Monad that has no effects.
Ah. That makes perfect sense, and I guess that's how GHC actually implements it, but my answer is still no. It may happen to be a function in GHC, but it could just as easily be a tuple. Unlike anything else in haskell, the world changes on its own and doesn't require a function to do so. The important thing is just that the two values are different (or could be different) so the sequence can be established.
And thus my ruse has been uncovered. I came here wanting to write comment akin to crt32's, but seeing how he has been downwoted for suspected trolling, I tried to make him seem serious. Looks like it worked.
Then what exactly isn't an effect? I'm not sure you've got something that has a well-formed definition there. I think it's actually important to understand that these really are just refactorings, exactly equivalent to their manually expanded (via equational reasoning) form. If threading state manually isn't an effect, then neither is the State monad, and so on for all the others I mentioned. Whether something is an "effect" shouldn't depend on how it is spelled. It's important to understand that the monad typeclass isn't magical or special or the gateway to "effects", it's just a rather simple little refactoring. (Which also provides a nice cutting point for verifying some properties via the type system, as IO does, but that's another issue.)
I know, but part of the point is that the threading happens in the background. I think a good way to think about it is that values wrapped in functors that represent effects "can" something; `Maybe` *can* fail, `List` *can* be nondeterministic, `State` *can* "change" stuff, `Cont` *can* use the continuation, `Reader` *can* look at the environment. This fits with constant functors not being effectful (while still having pretty much unused monads); they can't do stuff.
Can anyone give a concise summary of Roy as compared to elm or other similar languages?
[Obligatory link](http://james-iry.blogspot.com.br/2009/05/brief-incomplete-and-mostly-wrong.html)
Alright, fine. Just make sure that you clarify your concept of effects, since it differs from the conventional meaning of "side-effects" or state.
This does not work, precisely for the reason mentioned in the article. First, you need to change `set l p` to `flip (set l) p`, since `set` takes the record to update last. But then, you need a `set` to be a function with type `Pair a b :-&gt; a -&gt; a' -&gt; Pair a b -&gt; Pair a' b`. But `set` cannot change the type of the record, which was precisely the motivation for the van Laarhoven lenses.
I use `Applicative` when possible because it carries less mental baggage with it. Why make code look more context-dependent than it actually is?
Yeah, I think that's ok, sort of like denoting a capability.
My feeling about that is that most people will parse the equivalent `Monad` code about as easily as the `Applicative` code. Also, there are some times where the `Applicative` style is brittle. For example, if I want to deserialize something it works if the order of the data in the file matches the order of the arguments: Constructor &lt;$&gt; get &lt;*&gt; get &lt;*&gt; get But if the order is different, then I have to do a bit of gymnastics: (\z y x -&gt; Constructor x y z) &lt;$&gt; get &lt;*&gt; get &lt;*&gt; get Similarly, if the data has additional elements in between (i.e. maybe I'm parsing some larger record and I'm not interested in extracting every field), then those start to make it worse, too. For example, if there is an extra field in the middle that I have to skip over using "getInt", then it starts to look like: (((\z y x -&gt; Constructor x y z) &lt;$&gt; get &lt;*&gt; get) &lt;* getInt) &lt;*&gt; get ... and that is not very easy to mentally parse (at least for me). However, the equivalent monad code is rather straightforward: do z &lt;- get y &lt;- get getInt x &lt;- get return $ Constructor x y z
Thanks. So there is no need to use Applicative instance solely for the fact that it is possible to use it instead of the Monad instance? My most recent example, monad style vs applicative style: vectors = do skipTill A.anyWord8 $ A.string "Eigenvectors:" line &gt;&gt; line &gt;&gt; line between (symbol '{') (symbol '}') $ vector `A.sepBy` symbol ';' where vector = between (symbol '(') (symbol ')') $ lexeme number `A.sepBy` symbol ',' number = A.takeWhile1 (\w -&gt; AC.isDigit_w8 w || w `BS.elem` ".+-e") vs. vectors = (skipTill A.anyWord8 $ A.string "Eigenvectors:") *&gt; line *&gt; line *&gt; line *&gt; between (symbol '{') (symbol '}') (vector `A.sepBy` symbol ';') where vector = between (symbol '(') (symbol ')') $ lexeme number `A.sepBy` symbol ',' number = A.takeWhile1 (\w -&gt; AC.isDigit_w8 w || w `BS.elem` ".+-e") I find the monad version much easier to read. &gt; Then I can create a parser for points as: &gt; pPoint = Point &lt;$&gt; pDouble &lt;*&gt; pDouble &lt;*&gt; pDouble Couldn't you also use this instead: pPoint = return Point `ap` pDouble `ap` pDouble `ap` pDouble 
This doesn't seem so bad to me: liftA3 (\z y x -&gt; Constructor x y z) get get $ getInt *&gt; get
They should add "discrete math programmer" who uses matrix squaring to write an O(log N) implementation.
As I said, Kleisli arrows are substitutions, so, if you wanted to implement beta reduction for example, you could go: beta :: AST (Maybe n) -&gt; AST n -&gt; AST n beta lambdaBody argument = do v &lt;- lambdaBody case v of Just v' -&gt; return v' Nothing -&gt; argument
Check out the Reverse State Monad.
For me it was the moment when I realized that zygohistomorphic prepromorphisms are better suited for my task
&gt; Now, every weird-ass extension in Haskell looks like a furious attempt at mimicking programming in Agda. Amusing how you can say the same thing about C#/Java/C++ and Haskell. So the question is, what do weird extensions in Agda attempt to mimick?
And your take on first encountering **iteratee monad**....?
Creating the "pipes" library, of course. ;)
Yes. DuckDuckGo is very Haskell-unfriendly.
Your next aha moment is when you need to do this in the IO monad. That's when transformers will hit you hard :) 
&gt; Any error involving a null. Like I said, show some code that is semantically equivalent in Java and Haskell but which the Haskell compiler refuses to compile because the code will crash. You will see that what you just said doesn't answer the question I asked. 
For me, it was: "Aha, nobody will pay me to use Haskell for work"
Well, it *can* build on an AST.
To be honest I have no such an example. Problem with refactoring of Java code was mostly related to explicit state everywhere but that doesn't necessary make Java weaker language or something.
Disciple's type inference is essentially the one described by "The Type and Effect Discipline", Talpin and Jouvelot, 1993. It suffers from "the poisoning problem" when used with higher-order functions. I suspect that a better approach is to use flow typing as described in "Polyvariant flow analysis with higher-ranked polymorphic types and higher-order effect operators", Holdermans, Hage, ICFP 2010. That paper explicitly states that they solve the poisoning problem, though I haven't had time to rewrite the inferencer yet. I'm going to properly bake the DDC core language before going back to the source language. 
I don't think it's any worse on Ubuntu. That's what I'm running, and I haven't noticed any more issues than when I was using Fedora or Mint (or Windows for that matter).
Well, I defined "effect" as something which you can say "can" something; State can "modify" some state, Reader can take information from the environment, etc. AST can fill the holes in an AST.
Alright, but that feels like a very stretched definition of "effect", to me. Edit: Also, that sort of definition isn't quite right. AST isn't just describing functions that can populate ASTs, it's describing actual ASTs themselves.
Really? Say you have something like Handle aFancyMethodReturningAHandleOrNull(); and a method `void open()` on `Handle` then you use that nice method: Handle myHandle = myObj.aFancyMethodReturningAHandleOrNull(); myHandle.open(); // oh shit, NPE at runtime Now let's look at how Haskell would approach the same problem: aFancyActionReturningAHandleOrNothing :: IO (Maybe Handle) open :: Handle -&gt; IO () and then you use it: myHandle &lt;- aFancyActionReturningAHandleOrNothing open myHandle -- type error. You need to check that you got a Just first Does that not count? While you could argue that the types don't match up, the point is that they're both idiomatic approaches in the respective languages, and Java doesn't have convenient sum types that allow `Maybe` to work with low syntactic overhead. The fundamental distinction is that `Maybe A` is a distinct type from `A`, which is not the case with nullable types in languages like Java.
The two main "extensions" I can think of to Agda's type system are: `--without-K` behaves more like Coq and turns off an axiom that lets you eliminate more equality proofs than you might normally expect. This then allows you to start talking about things that the homotopy type theory people care about. Axiom K allows you to prove that all equality proofs are equal, and the HTT people explicitly want to allow more than one distinct equality proof in many cases (between types, or even between values sometimes). Unfortunately it doesn't work very well right now, was recently found to only go up to dimension 3: you can't prove that two equalities of non-equalities are equal, but you can still prove that two equalities of two equalities are equal, which should not be possible. `--type-in-type` allows to write impredicative definitions (`Set : Set`) and prove things like Russell's paradox or (more traditional in type theory) Girard's/Hurkens's paradox. 
Probably my biggest 'aha!' moment took place on stack overflow after I responded telling someone it wasn't possible to implement idiomatic reverse mode automatic differentiation in Haskell -- then I went back and did it. I'd done some work with observable sharing before but this really stretched my brain, and showed me lots of neat opportunities for DSL designs. http://stackoverflow.com/questions/2744973/is-there-any-working-implementation-of-reverse-mode-automatic-differentiation-fo/2842605#2842605 The second package that I released was a library of type level 2s and 16s complement integer arithmetic. That made me learn a lot about type level programming in Haskell and about template haskell in particular. Along the way I finally understood MTPCs, functional dependencies and the way typeclasses get dispatched, and wrote my first class that had a fundep like: class Closed a | -&gt; a Most of my other 'a ha!' moments have involved category theory. There is the usual big 'a ha!' moment from the first time I grokked monads. Then there is the first time I understood exactly what was happening with the codensity monad, and when i finally understood how all Haskell monads and monad transformers can be implemented with right Kan extensions and right Kan extension transformers. Realizing that comonad-transformers lower rather than lift. Working with Elliott to make the 'reflection' code as fast as possible really bent my brain inside out, leaving these 4 lines of code that still to this day give me the heebie jeebies. class Reifies s a | s -&gt; a where reflect :: p s -&gt; a newtype Magic a r = Magic (forall s. Reifies s a =&gt; Proxy s -&gt; r) reify :: a -&gt; (forall s. Reifies s a =&gt; Proxy s -&gt; r) -&gt; r reify a k = unsafeCoerce (Magic k) (const a) Proxy Realizing that covectors form a monad, and linear maps between vector spaces form an arrow, but with the opposite argument order you'd expect, and that if you want to work with vector spaces in Haskell you're better off just working with infinite free modules, and that mathematicians push the infinities in the wrong places on free modules to make a nice constructive definition that generalizes to the infinite dimension cases. Realizing that recursion schemes are largely a waste of time beyond the basic cata/ana/hylo-morphisms. Knowing the ontology of the remainder of them is largely useless, though you can still derive some benefits from knowing how they all arise from distributive laws. When Brent Yorgey and company added polymorphic kinds to GHC, and all of a sudden it became obvious why I hadn't been able to make certain constructions like product and sum categories work in Haskell, but could make them work in scala. Realizing you can make a monad out of revision control. Haskell has been a never-ending supply of revelations to me, and they keep coming. Heck, just the other day, when Russell O'Connor popped up and mentioned that van Laarhoven lenses could be used for polymorphic updates, I broke a mental block. 
I'll explain when I get to a computer, but there are a few reasons for my idea.
When, after years of studying the language, I wrote my first real piece of software on it. I haven't touched the language ever since. I don't even know why -- I wasn't thoroughly disgusted with the experience and the application worked well (and still does). Perhaps the complexity ultimately did not seem worth the trouble. Perhaps I was put down by my inability to make *anyone* where I work (and where the application was) interested in the language.
 module Test where test z = putStr $ show $ case z of Just x -&gt; x test.hs:2:26: Warning: Pattern match(es) are non-exhaustive In a case alternative: Patterns not matched: Nothing
Roy is like Coffescript, whereas Elm tries to abstract away HTML as well; not just Javascript.
&gt;Like I said, show some code that is semantically equivalent in Java and Haskell but which the Haskell compiler refuses to compile because the code will crash. You're clearly talking about runtime errors in java that are transformed to compile time errors by haskell. The example fits.
Yes: http://www.reddit.com/r/haskell/comments/vhbf4/polymorphic_update_with_van_laarhoven_lenses/c54vu0g
Twan already explored what happens if you replace `Functor` with different constraints, see the last slide in this talk: http://twanvl.nl/blog/news/2011-05-19-lenses-talk, and see my comment there if you're curious what happens for `Alternative` and `Monad`. If you use `Applicative`, you get a lens that updates 0 or more values in a container. When gettings values, you get to choose what monoid to use. When using list you get all the values. Here's an example: https://gist.github.com/984392
Searching on google "real world haskell bos" bolds out Brian O'Sullivan. Extremely smart from Google to know that bos stands for it.
Cool indeed!
When I found a software that was written in Haskell in real world use. ... Oh ... wait.
In all seriousness, your comment was *correct*. For me, researching what that phrase *meant* turned out to be very illuminating, and not as hard as it looks.
Your next aha moment comes from "why is that?"
The problem with using the entire Traversable to get a list of elements back out is that you can't actually get something that meets the lens laws in general. You get back a variable number of elements, and there is no guarantee that the end user will put exactly that many back.
Maybe this is a completely stupid idea, and I'm sure you already realize it, but this feels similar to the distinction between Hoare / Separation logic...
I was not suggesting that we try to get a list of elements back -- I'm not familiar with the `Multiplate` work at all. Only that this (admittedly restricted) formulation of lenses resembles the Traversable definition. This definition is parametric in all `Functor f`, but actually you use only two of them `Identity` (for `set`/`modify`) and `Const b` (for `get`). It seems natural to wonder what other functors you would need, what this polymorphism brings you (parametricity?), and if weaker quantification would be possible (indeed, that would still satisfy the lens laws). `Applicative` is not quite possible because `Const b` doesn't fit in general. PS: thanks for your comments, they're interesting.
&gt; But their laws are rather weak as they make no attempt to say "... and nothing else changes" They do. put l (get l a) a = a says that putting something you got out doesn't change any other part of the structure and put l b1 (put l b2 a) = put l b1 a is sufficient to disallow any other edits in the same sense that once you know fmap id = id, the fmap f . fmap g = fmap (f . g) law follows from parametricity. The remaining law get l (put l b a) = b just only sure that you actually store something you are given, so two of the laws are very much explicitly to address your concern. The fact that you can't edit anything else in the structure is clearer with an isomorphism lens: newtype Lens a b = forall c. Lens (Iso a (b, c)) there its obvious that you aren't editing the rest of the structure, because you know nothing about c to edit it with!
I underestimated the lens laws, but it still seems like there is a hole: A lens could perform an idempotent destructive change on other fields when asked to actually modify (but not when it is asked to put the same value that is already in place). 
I have used Ynot, and can indeed recommend it. Their ICFP paper is a really great read, for learning separation theory and the concepts involved in parameterized monads!
Really awesome, I cannot think of anything you can do better beside upload more videos. Thank you very much for these.
Looks nice. I'm a fan of the single class comonad ;)
Yeah, I figured that was your motivation. It looks like you still have routes that you'd want to define manually though. So you could do those and just leave the simple render proxies to heistServe.
This was what I was doing before. That is [Apply](http://hackage.haskell.org/packages/archive/semigroupoids/1.3.2.1/doc/html/Data-Functor-Apply.html) However, ComonadApply is a stronger claim. In addition to associative composition: (.) &lt;$&gt; u &lt;.&gt; v &lt;.&gt; w = u &lt;.&gt; (v &lt;.&gt; w) You get that extract and duplicate are symmetric monoidal natural transformations: extract p (extract q) = extract (p &lt;@&gt; q) duplicate (p &lt;*&gt; q) = (\r s -&gt; fmap (r &lt;@&gt; s)) &lt;@&gt; duplicate q &lt;*&gt; duplicate q These are rather strong restrictions I was previously omitting.
thank you so much for the praise. I am uploading the second part "as we speak" and intend to upload regular sessions. But in future I will tackle smaller projects, because I think that 3 hours is a bit excessive to watch ;)
This is the first video(s) for my new project "Haskell Uncut". "Haskell Uncut" has the goal of providing "unscripted, unedited, uncut" live coding sessions in haskell to give an impression how actual programming in haskell may look, with all its pitfalls, errors, mistakes and cul-de-sacs. I don't consider myself a haskell master, so enjoy watching me make a balls of it ;)
Nice initiative! What would even be more awesome would be live sessions. People may comment directly on the chan (or vocally, even better). Just a remark: when you started with your file with just empty tests, I hoped you were going for test driven development, which would have been really nice. I just regret this hasn't been more followed (I didn't see you touch the tests as you were expanding your code during the rest of the video).
and does this like a boss!
yes, test driven development was the plan. I just got into the flow of data driven development that I never got to use the tests. I promise that I'll use TDD in one of my next sessions. I love the idea of a live session. Maybe I'll advertise one of my next sessions on #haskell and open a new channel for it. It's definitely worth a go. Any idea what tool I'd use to stream my screen live to spectators?
Thank you both for spelling it out. I hope I wasn't the only one not grasping the full effect the lens laws. I'll feel more confident using lenses now. Re-reading (http://twanvl.nl/files/lenses-talk-2011-05-17.pdf) I understand that the residual representation of lenses (isomorphism lens above) is stronger/safer than the store-comonad one (for example) because you can prove the lens laws from it (assuming you trust Iso). Is that also true of the functor transformer representation (assuming you trust the Functor instances) ? Because both the laws for Iso and Functor seem easier to understand.
Yes. Unfortunetely we cannot compose field lenses to get such composite lenses, because they might overlap. It might be nice if fields were represented by something stronger than lenses that we could then use to turn into a lens: fieldsToLens (hCons fieldA $ hCons fieldB hNil) I guess type-level naturals could do the trick, if we had a bijection between a datatype and a corresponding Data.HList.HArray.
You do lose information about separability of fields, and the inability to take a (&amp;&amp;&amp;) of two lenses f :: Lens a b, g :: Lens a c to get a composite lens f &amp;&amp;&amp; g :: Lens a (b, c), yes. One problem is that you can make lenses that say, access a key in a map, and there isn't a good way to reveal what key is accessed at the type level. But even if you do make an HList of the fields, and we restrict our vocabulary to fields of records, I don't currently believe that you can come up with a scheme for field access where you can polymorphically update with the composite lens something like: data Foo a b = Foo { foo :: a, bar :: a, baz :: b } to change both the foo and bar field to a different type at the same time using such a (&amp;&amp;&amp;)'d lens anyways.
 (unsafeCoerce &lt;*&gt; unsafeCoerce) (unsafeCoerce &lt;*&gt; unsafeCoerce) 
sure. anything that interests you particularly? I just did it this way because I have no working computer with linux on it and found that native windows firefox had better resolution. Apart from that I just dragged the VirtualBox window to half screen size ;)
Ultimately trusting iso is relying on another uncheckable assumption. The benefits of the store comonad coalgebra are that you never have to explicitly describe the 'residue', which can be quite complex. In the end all 3 sets of laws are interchangeable. The benefit of the Store comonad coalgebra version is that unlike the other two it doesn't require any extensions and gives nice type errors.
This type wankery rather reminds me of the pipes discussions. The idea seems to be: use a single type to represent the various scenarios, and make synonyms with carefully restricted types to clarify each scenario. Then you have various composition/usage functions that only work in the absence of certain type restrictions.
nonsense?
That is (SII) (SII), the usual fixed point from the SKI calculus.
Cool, I got distracted by unsafeCoerce' type signature. It's sort of a self-fulfilling process.
Just to chime in, I much prefer it the way you did it now. I don't want to watch you waste time messing with TDD. The haskell part is interesting, silly fad "methodologies" aren't.
thank you. I got carried away a bit with the programming. quite proud how well Sokoban (incl. GUI) turned out in 3 hours. But all in all, definitely too long. My next session will be 10-30 minutes and hopefully introduce a couple basic haskell concepts "en passant"
well, I am a big fan of TDD. If you want to refactor code, do regression tests and generally have a good feeling about making changes in a big code base, tests are the way to go. they can also save you a lot of time. I agree though, that they wouldn't be all too interesting to watch ;)
Really? Because I've seen tons of blog posts for C++ about exactly when the return value optimization does and does not apply, and how to use the new C++11 rvalue references in order to implement move semantics so that compilers can elide or eliminate copies. Performance-critical code in any language requires an understanding of the semantics of object allocation and copying, there's no getting around that. And yes, I've seen lots of C++ code that jumps through hoops in order to save heap allocations or copies. This isn't unique to Haskell.
Great idea! In an ideal world it should be doable in a short time, because it's only 140 lines of code (for the console version). I could have cut a few corners and generally have been more focussed. But to be honest, I am quite happy with the speed, quality and structure of this implementation (for my standards), and think that it's not too far from a realistic implementation time. I am convinced that some programming gods can program this in less than 45 minutes... Not me under pressure of a running and unstopped recording, though ;) Glad you enjoyed it.
The second part is showing up as private though, I was looking forward to watching it. :(
I am working on a medium-sized Haskell project (~6000 lines) and I dare change a whole lot all the time. I don't have tests. There are a few parts that would benefit from tests, but all in all, the type checker gives me enough assurance, and tests would make refactorings harder, not easier, due to having to maintain the tests. After I make large changes, when it compiles, it almost always works anyway.
I basically used as I to let us dodge the fact that you can't write (SII) (SII) in a typed lambda calculus, so it adjusts the types so we can. (unsafeCoerce &lt;*&gt; id) (unsafeCoerce &lt;*&gt; id) doesn't have quite the same ring to it. ;)
For me, it was: "Aha, if I don't tell anyone they'll pay me to use Haskell for work"
I think "IO is a free monad over some side-effecting functor" is the 'most true' answer. See [this post](http://comonad.com/reader/2011/free-monads-for-less-3/) and its predecessors. An implementation in prompt: data FFI a b -- opaque, representing an effectful "function" from a to b. unsafeFFI :: a -&gt; FFI a b -&gt; b -- executes the effects data IOP o = forall i. Effect i (FFI i o) type IO = Prompt IOP liftFFI :: FFI a b -&gt; a -&gt; IO b liftFFI f a = prompt (Effect a f) unsafePerformIO :: IO a -&gt; a unsafePerformIO = runPrompt (\(Effect i f) -&gt; unsafeFFI i f) -- an example ffiPutChar :: FFI Char () -- given by runtime system putChar :: Char -&gt; IO () putChar = liftFFI ffiPutChar
&gt; Only that this (admittedly restricted) formulation of lenses resembles the Traversable definition. Won't that be true of any implementation of lenses? Traversable allows to walk over a structure and change things "in place", and the Foldable allows to walk over a structure and observe its components; lenses allow to observe the components of a structure and to change them "in place".
I've not seen your video yet (busy) but I want to thank you for doing this. The streaming culture in starcraft 2 has greatly helped improve the quality of the game and I expect it could do the same for programming (although it might not become as much of a sport).
So, I have had no direct experience in this area, but it sounds to me like what you are doing can be classified as stream processing, and the most popular way to do stream processing nowadays is to use one of the many libraries that implement an abstraction called [iteratees](http://www.haskell.org/haskellwiki/Iteratee_I/O) in which you (very roughly speaking) implement a left fold on the input data with the ability to terminate before all of the input is read to report a result. Unfortunately for the beginner, there are a large number of libraries that implement this pattern using slightly different ways: * [iteratee (Oleg's original library)](http://hackage.haskell.org/package/iteratee) * [enumerator (a reimplementation of iteratee)](http://hackage.haskell.org/package/enumerator) * [pipes](http://hackage.haskell.org/package/pipes-1.0.1) * [conduits](http://www.yesodweb.com/book/conduits) (I have tried to link to the best documentation I could find with a quick search, so if someone here has a better link then I would appreciate it if you corrected me.) So which of the above libraries should you use? Honestly, I have no idea. :-) *However*, [this linked conversation on StackExchange](http://stackoverflow.com/questions/9983840/what-are-the-pros-and-cons-of-enumerators-vs-conduits-vs-pipes) should hopefully help you get started. Again, I am not an expert on any of the linked libraries above so my words should be taken with a grain of salt and used as a starting point rather than a solid recommendation towards any action in particular.
You might like the `(&lt;|&gt;)` function: example = runMaybeT $ do x &lt;- proc &lt;|&gt; return something y &lt;- proc2 x &lt;|&gt; fail "procedure2: Nothing" ... Also works with the Either transformer and many other similar error-like monads.
You don't need to do TDD to have tests.
This is a brilliant starting point, though. I keep thinking I want a fold with something extra, but had no idea what. Thank you very much for writing this.
This monoid instance can be implemented for any Monad: mempty = return () mappend = (&gt;&gt;) It violates one identity law if you take into account bottom, but otherwise it works and most people don't mind. I personally prefer the Kleisli endomorphism version: mempty = return mappend = (&gt;=&gt;)
I've looked into live streaming. And while it will take me a while to set it up in the required quality, I'd love to do a coding live stream. I hope anyone would watch ;)
It impresses me how little the abstract machine underlying GHC seems to have changed over the last 20 years.
There's going to be less of an audience for live streaming, given the current lack of demand. Nevertheless if you're interested you should totally try and organise a time and date to have a stream with a chat channel on the side. Just make sure the entire thing is recorded for the people who don't live in the same timezone as you. =p
&gt; silly fad "methodologies" Explain why. I'm also completely against using a method just because it's trendy. But I believe TDDs make a point. And I'm not speaking about silly testing. I'm speaking about just the intelligent and relevant tests. Plus yesod team has made a good point regarding them: they help you design your API, an API that's hard to test might be an API that's hard to use. And they make a good documentation for your invariants (if for instance they can be expressed as QuickCheck properties).
&gt; "Oh, it's that language for if you don't care how long it takes you to code things." It's true if you're in your learning phase, though. The point is not to stay in that phase.
Use two different functions: unNum :: Type -&gt; Int unNum (Num n) = n unText :: Type -&gt; String unText (Text s) = s But in general, partial functions are a bad idea (they move bugs from compile-time to run-time). Better to use something like this: untype :: (Int -&gt; a) -&gt; (String -&gt; a) -&gt; Type -&gt; a untype unN _ (Num n) = unN n untype _ unT (Text s) = unT s (Compare with `maybe` and `fromJust` in Data.Maybe.)
Note: * this is only parallelism at the level of independent packages * at the moment all the output from concurrent builds are interleaved in the console output. Mikhail is also working on a patch to cache the setup binary which will speed things up a bit. On the second point, for the moment you can use the --build-log='$pkgid.log' or similar. The plan is to do something like this automatically, so you'll get a summary of what packages are building at the moment, but have the details go to log files. For example this is what Gentoo does if you build a bunch of packages in parallel.
I think I've been in your shoes. It isn't a simple "Request -&gt; Response" problem. Instead, you need to be able to, while waiting for input on a connection, independently send output on the same connection. I recommend doing something like this: * Send and receive with different threads. * Use STM for communication among threads. You could, for example, define a "global state" record type whose fields are TVars and TChans. The rest of this post will talk about how to do this without contaminating large amounts of your code with concerns such as IO, concurrency, and exceptions. The best piece of advice I can give you is: **don't mix up your business logic with impure implementation details**. By "impure implementation details", I mean things like: * IO, especially concurrent IO. * Exception safety. * Access to application-global state. So consider code like the following: data ClientEnv = ClientEnv { globalEnv :: GlobalEnv , clientConnection :: Connection , clientCert :: X509 } serveClient :: ReaderT ClientEnv IO () serveClient = do ClientEnv{..} &lt;- ask ... First the good: `ReaderT` and the [RecordWildCards extension][1] can make your life easier. If you aren't familiar with these, take some time to learn about them. Now here's the bad: * `serveClient` has to worry about the possibility of receiving an asynchronous exception (perhaps some master thread will kill unresponsive client threads). * When `serveClient` needs to access some global environment info, it has to look at `GlobalEnv` directly. This can muddle the business logic. You might end up with code like this: m &lt;- liftIO $ atomically $ readTVar $ widgetCache globalEnv case Map.lookup widget_id m of instead of this: m &lt;- getWidget widget_id case m of Here are a couple, essentially equivalent ways to avoid this problem: * Define a typeclass whose instance (implemented elsewhere) will provide the desired functionality: class Monad m =&gt; MonadServe m where getWidget :: Id Widget -&gt; m Widget recvMessage :: m ServerMessage sendMessage :: ClientMessage -&gt; m () log :: Priority -&gt; String -&gt; m () serveClient :: MonadServe m =&gt; m () * Use a [Prompt monad][2]: data PromptServe a where getWidget :: Id Widget -&gt; PromptServe Widget recvMessage :: PromptServe ServerMessage sendMessage :: ClientMessage -&gt; PromptServe () log :: Priority -&gt; String -&gt; PromptServe () serveClient :: MonadPrompt PromptServe m =&gt; m () Both of these delegate implementation details to "somewhere else", letting your code focus on the business logic. Yes, the instance of `MonadServe` might be full of `liftIO`s and such, but now: * The source code of `serveClient` will be much easier to understand. * It clearly defines what `serveClient` is allowed to do. * If there are any IO-related bugs, the blame can be placed solely on whoever implements `MonadServe`. [1]: http://www.haskell.org/ghc/docs/latest/html/users_guide/syntax-extns.html#record-wildcards [2]: http://hackage.haskell.org/packages/archive/MonadPrompt/latest/doc/html/Control-Monad-Prompt.html
&gt;And I want to write a function that extracts the Num or Text, such that &gt; &gt; untype :: Type -&gt; a If that's the signature of your function then both the pattern matched results (n &amp; s) need to have the same type. Clearly that isn't possible since one is an Int and one is String. 
&gt; untype :: Type -&gt; a This type says that given a `Text` value you promise to return a value of type `a` which can be **any type whatsoever**. Clearly this is impossible. Not having an `a` value in scope, you have nothing you can hand back. Since you don't know what the type `a` actually is, you can't possibly use that knowledge to construct one yourself. The only thing you can do is loop forever (or throw an exception).
Indeed, the type signature forall a. Something -&gt; a is the same as to `False` (the type, not the Bool), the uninhabited data type in proof systems.
Note that `untype` is equivalent to: untyp unN unT t = case t of Num n -&gt; unN n Text t -&gt; unT t The only reason I reformulate it that way is that sometimes case statements make it more obvious that the two code paths must return the same type of value.
Can anyone explain the benefit of the Functor lenses over this simple definition? I guess (.) composability is one, but it is also far less straightforward... data Lens oldrec newrec oldfield newfield = Lens { get :: oldrec -&gt; oldfield , set :: newfield -&gt; oldrec -&gt; newrec } compose :: Lens oldouter newouter oldmiddle newmiddle -&gt; Lens oldmiddle newmiddle oldinner newinner -&gt; Lens oldouter newouter oldinner newinner compose (Lens outerget outerset) (Lens innerget innerset) = Lens (innerget . outerget) setter where setter field rec = outerset (innerset field (outerget rec)) rec 
`foldMap`!
 sortBy (compare) files Firstly, the parens around `compare` are unnecessary. Secondly, `sort = sortBy compare`, so just use `sort`. foldl1 (&amp;lt;&amp;gt;) Firstly, I believe HTML escaping has struck again. Secondly, very clever! Thirdly, as Tekmo pointed out, you could use the Monadic `&gt;&gt;` in place of Monoid's `&lt;&gt;` and get the same result. `&gt;&gt;` would be a bit more canonical in Tekmo's terms, since he describes monad sequencing of pipes as [vertical concatenation](http://hackage.haskell.org/packages/archive/pipes/2.0.0/doc/html/Control-Pipe.html#g:4).
I use &lt;|&gt;. but not in this context. As i said, I don´t like the MaybeT transformer:(. In your cod all the procs in the do must return Maybe values and every intermediate result must be so. In mine they do not. 
So that's how I got those broken ribs.
I've done Eulering and have been reading TMR. It's just hard to fit in any sizable projects after a long day of writing C++ ;-) Thanks for the haskell_proposals link! I will check some out.
GADTs provide the power of Existential Quantification, and then some more. I "blogged" about why GADTs are cool a few months ago: http://www.reddit.com/r/haskell/comments/rk0uf/why_gadts_are_awesome_implementing_system_f_using/ I believe it's true that for my example code in the comment above, existential quantification would have been sufficient. But I just love GADTs so much more.
&gt; this is only parallelism at the level of independent packages What would it take to get parallel builds going at the module level? Obviously, some modules depend on others, and so there will still be *some* amount of sequential compilation. Are there any packages that would actually benefit from their modules being built in parallel?
 type Ref a a' b b' = b -&gt; (a, a' -&gt; b') anyRef :: Ref a a' b b' -&gt; (forall f. Functor f =&gt; (a -&gt; f a') -&gt; (b -&gt; f b')) anyRef k m b = fmap f $ m a where (a, f) = k b compose :: Ref a a' b b' -&gt; Ref b b' c c' -&gt; Ref a a' c c' compose f g c = (a, a'c') where (b, b'c') = g c (a, a'b') = f b a'c' = b'c' . a'b' edit: other direction for isomorphism newtype IndexedStoreW a a' b' = IndexedStoreW (a, a' -&gt; b') deriving Functor refAny :: (forall f. Functor f =&gt; (a -&gt; f a') -&gt; (b -&gt; f b')) -&gt; Ref a a' b b' refAny k b = (a, a'b') where afa' :: forall a a'. a -&gt; IndexedStoreW a a' a' afa' a = IndexedStoreW (a, id) IndexedStoreW (a, a'b') = k afa' b 
and that would get in the way of actually composing the lens.
If you've ever tried to compile Haskell Platform... some of the packages take a while :-)
Yes, wordpress keeps replacing the &lt;&gt; with &amp;lt;&amp;gt;. Yuck. I removed the parens and switched to sort as suggested. I don't get the Monadic &gt;&gt; stuff. Would I just use foldl1 (&gt;&gt;) to build a chain of actions? What is the difference? Is there an advantage or disadvantage for either? I just thought that people should know about this undocumented, and to me, non-obvious feature.
0\. :(
[Go read some papers](https://github.com/haskell-distributed/distributed-process)^tm
I guess using `foldl1 (&gt;&gt;)` is a bit more explicit about what is happening. I probably should explain my foldMap comment a bit. Check out [Data.Foldable](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Foldable.html). It has `fold` which is the same as `foldl (&lt;&gt;) mempty`, but even better, it has `foldMap f` which is the same as `fold . fmap f`. So your code becomes: let source = foldMap sourceFile $ sort files 
I can't seem to reach arxiv right now, but the same article is [here](http://cgi.cse.unsw.edu.au/~rvg/eptcs/Published/MSFP2012/Papers/13/paper/FormalComparisonGP.pdf).
and vim2hs: https://github.com/dag/vim2hs
Thanks for the link. Okay, I was not specific enough, I spoke about my own opinion: having see a lot of people too lazy to test their code, I think that forcing them to do the tests before the code is written might be helpful in two ways: - The code is tested early. - It helps you design your API (see what yesod people have said about that, I think it's a good point) It is sure that if you work with people who have a good discipline, the use is diminished. I'm reading the study.
To all those people with the idea that it is somehow bad that a developer picks a language he is comfortable with for small one person projects just because that makes it harder to replace him I would like to say that with that mindset we would never make any progress at all. You do not get ahead by doing everything in the most traditional, most risk-averse way. You also don't get anything done by asking the managers about every detail.
Thanks! added to the post.
Naaaaa, Vim is an old friend ;-)
Are there different implementations of Cloud Haskell? Is this the "golden standard" to achieve distributed programming in Haskell?
I try it. Every few years I have tried a new vim emulation mode for emacs and been disappointed. Definitely worth a try though.
TIL Tagbar exists. ( http://github.com/majutsushi/tagbar ) +1 for lushtags if it integrates with it. The CommandT plugin is great for tags/files/jumplist/buffers navigation.
Arf... lushtags only tags the functions which have a type signature. *EDIT:* And some advanced features like GADTs are not recognized.
It's only me, or ghc-mod is a bit too slow to perform checking and linting? 
But it is important to understand that now `Num 3` and `Text "hello"` have two different types `Type Int` and `Type String`, you can't put them in the same list for example. It still might be the right thing to do in aHumbleMonad case but since we don't have **any** context, we can't tell... (Also I persist in thinking that answering this kind of question is not reddit vocation but rather StackOverflow, IRC #haskell or even the haskell-beginners mailing list)
For tags, I am very happy with [hothasktags](http://hackage.haskell.org/package/hothasktags), although I haven't tried lushtags. Another recent plugin I'm very fond of is [CtrlP](https://github.com/kien/ctrlp.vim). It's a fuzzy file finder a la TextMate. Not Haskell specific, but too useful not to mention.
Conceptually, perhaps, but practically, you can't compose them at all if you put newtype noise in the middle. =P
Make this a blog post and you're pretty much there. 
Actually I believed the package 'remote' is too. That's why I asked.
Of course, if you want to put them into the same list, you could always pair them up with the function which is to operate on them. data GType a where Num :: Int -&gt; GType Int Text :: String -&gt; GType String data Type = forall a. Type (GType a) (GType a -&gt; Type) listOfTypes :: [Type] listOfTypes = [ Type (Num 1) myIntFunc , Type (Text "Bob") myStringFunc ] myIntFunc :: GType Int -&gt; Type myIntFunc (Num i) = Type (Num (i + 1)) myIntFunc myStringFunc :: GType String -&gt; Type myStringFunc (Text s) = Type (Text (s ++ "!")) myStringFunc runFuncs :: [Type] -&gt; [Type] runFuncs ts = map runFunc ts where runFunc (Type gtype f) = f gtype I don't know if any of that is of any use to you. Perhaps you could provide more context of your problem, either here, or on StackOverflow, etc.. 
But... but... *friendship*. With a text editor &gt;.&gt;
&gt; the FSINCOS instruction that modern cpus have. And indeed not-so-modern cpus like the 80387 released in 1987 :-) A good example here is the Haskell `divmod` function which calculates both the `div` and the `mod` and in principle (depending on the hardware) may be faster than running both `div` and `mod`. So the answer is the -fasm backend does not emit `fsincos`. The LLVM backend might be able to optimise two nearby uses of `sin` and `cos` to use the single instruction, you'd have to check. There's no problem in principle with there being a sincos function that does both, and in principle that could be implemented (or optimised) in some backends to use the `fsincos` instruction on x86 hardware and to use a combination of sin and cos on other cpus. Laziness does not play a role here (except in the sense that nobody has been bothered to implement it!). Have you seen any benchmarks? Is it really that much faster?
`remote` is pretty much the first incantation of Cloud Haskell for doing distributed processing; `distributed-process` is more like the successor to `remote` which not only has a better name, but also decouples transport layers/mechanisms from distribution mechanisms (so you could concievably have a TCP based transport layer, and transparently swap that out with say, a CurveCP based transport layer, or just raw UDP layer, etc.) In this sense it's a lot more flexible for arbitrary distributed services.
thank you for your feedback. Yes, scope is an issue. Name hiding is frowned upon, so it makes sense to give things a new name, unless one can use the first one. Yes, i can get a bit carried away with whitespace and alignment. but it also makes it easier on the eye and easier to spot mistakes. "myfunc :: String -&gt; Int -&gt; String" is an optional type declaration. It means: myfunc takes an Int and a String and returns a String. ' is just a legal letter to name variables. It often signified a slightly altered value. Edit: is* could certainly be refactored. If the data type looked differently I could even match on constructor. I agree, there is a bit of code duplication. I could have written "isWall coord = elem coord . wWalls". Not sure what you mean by general purpose function.
FWIW on x86, div and mod have been a single instruction div/idiv which always gives you the remainder too. I don't know if this has changed but there used to be no mod instruction.
&gt; hasItem :: ItemType -&gt; Maybe ItemType Does this make anyone else sad?
and this, which uses vim 7.3's features to display some operators in unicode! https://github.com/Twinside/vim-haskellConceal
I know who you are. Was actually hoping for a funnier response.
The whole API makes me sad. Looks like it needs to be recompiled every time the game state changes.
Just... wtf?!
I looked at sublime text for Haskell programming, and even made a [cabal syntax file](https://github.com/Twinside/sublime-haskell-cabal) for it, but I'm not convinced right now. The build output wasn't fitting my tastes, and the syntax files are... clumsy to write.
As usual: downvoters, explain. I don't see where my rationale is wrong, and I just hope you're not the kind of people who are exasperated when people say something like "nah, I'll just stick to Java, it's an old friend" after being spoken about the benefits of Haskell. Would be a _flagrante delicto_ of "double standard". Because should you be, you'd be no better than them. *EDIT:* Still have downvotes, but no explanations. It's too easy to hide behind a downvote and not to stand up for one's opinions. I'm not trolling or playing against my team, just arguing.
I did look at it a little while ago following this: http://www.reddit.com/r/haskell/comments/ts8fi/haskell_ides_emacs_vim_and_sublime_oh_my_opinions/ and due to me being quite new to emacs. Since then however Chris Done has folded his work in to haskell-mode and it's available through emacs' package management system. I now happily use emacs fro haskell, but I'd certainly like to hear how others get on with sublime! Se also: https://github.com/wuub/SublimeREPL https://bitbucket.org/holmak/sublime-text-haskell
&gt;No separation between built in (like foldl) and user defined functions are confusing. Not sure what you mean here; foldl is just a function in the standard library, not a built-in.
I use sublime for my haskell fu. As for plugins, just use Iterm2 for repl and read package docs :-). 
I use Sublime with the SublimeHaskell plugin, it integrates with Cabal projects (saving a file triggers a reload), marks errors in your code and adds a color scheme which looks like Hackage's source code viewer. I'm very content with it and I rarely ever use vim for anything anymore. (Especially because you can use Sublime's CLI, subl, and set it as your $EDITOR)
In case anyone has similar problems, the issue is an uncodified dependency on a prior version of jison. This has since been corrected by Roy’s author … but that version of jison doesn’t work with Node 0.8. Oh, well. A cute little language anyway.
pattern for almost homomorphic functions: http://www.ittc.ku.edu/csdl/fpg/node/122 sorting: http://www.cs.ox.ac.uk/people/daniel.james/sorting.html the right kind of generic programming: http://dreixel.net/research/pdf/trkgp_draft.pdf (looks particularly awesome/practical!)
Since SSE does not give you sine or cosine, I don't exactly see how you can avoid it? Or do you mean FSINCOS is worse than FSIN and FCOS separately? (which would be rather strange) 
[Links to draft papers.](http://www.haskell.org/haskellwiki/HaskellSymposium/2012)
I've skimmed through the syntax extensions paper and I think it can be a great tool, but I have a much less substantial question. SHE is mentioned several times in that paper, and one thing I noticed (here and other papers not written by Conor where SHE gets referenced) is that they don't do the usual pun/word-play that Conor employs; for example, in the syntax extensions paper, they write &gt; For example, it is not possible to use SHE to enable idiom brackets Surely Conor would have written &gt; For example, SHE cannot be used to enable idiom brackets My question is, is it because of some universially known disapproval by Conor for others adopting his notation? If not, why don't others' papers usually follow his convention?
Some people might feel such puns to be beneath them. Personally is say, more power to Conor!
Does it handle TH correctly? Github gets it wrong :(
I don't know. I don't really use TH.
For me, the point of using editors like Emacs (and Vim) is that we can extend them, to our liking, and don't have to beg others. I realise there are two kinds of Emacs user; the extender and the “user.” The latter could be using any editor really, and these are the ones that move to other editors at the drop of a hat, for some nice feature. The former will just change how Emacs behaves.
I like puns, but I think that Conor uses them so much that the readability of his papers suffers. A little humor is great, but in my opinion it should always be subservient to readability.
The almost homomorphic functions paper (generics workshop) has a really fantastic pun. Conor needs to watch out for the competition.
Seems a lot like CommandT. Have you tried it? If yes, do you have a quick comparison?
Pretty much.
I have a great time learning Haskell while practicing on codeforces.com. It's just like Topcoder SRMs, but they support Haskell submissions.
Well, if you want to keep it Haskell 98, then I'd advocate the splitting into polylens and polylens-th, and you could then put the decent aliases in the polylens-th module. ;)
you can implement sin and cos using vector operations and the Taylor series...
Well, it's the same as Something -&gt; False which is *provable* if and only if `False` is provable. However, equiprovability isn't all there is to semantics. The above type also asserts the claim that `Something` is an empty type (or else you have a proof of `False` laying around, and so you're already inconsistent).
and then do you will have either much lower precision or much slower code...
Well, I didn't downvote you, but I'll explain why I think your argument is flawed. Perhaps it may shed light on why the downvoters felt as they did. I, for one, dislike emacs. I don't care how fully-featured it is or whathaveyou; I don't like it. This isn't a matter of familiarity or experience, it's a matter of taste. There are plenty of programming languages I dislike as well--- both functional and dysfunctional. And there are plenty of dysfunctional languages I do like (Perl) or am not especially hateful of (Bash, C); this isn't a matter of religion, it's a matter of taste. And when it comes to matters of taste, while not a fan of emacs, I am especially hateful of the pro-emacs proselytizing that's out there these days. For as much as I love Agda, it actively disapproves of non-emacs usage. For as much as I endure the horrific syntax of Coq, the only real way to have an interactive session is if you use proof-general. And if I am to believe your argument, the only way to Haskell is with emacs as well. This all smacks of the "Eclipse is the only way" religion in the Java world. And, frankly, there are only two responses to that kind of argument. Either, (a) yes, that single editor is the only way of using the language--- in which case the language is broken and should be discarded; or (b) no, that editor is not the only way--- in which case I'd much rather hear about what alternatives are out there. To put a finer point on it: Just because someone does not come to agree with you does not mean they have not heard you. I've heard the mantras of emacs religion. That I am not swayed by them is not because I have failed to understand; it is because I have rejected them. To repeat the mantra once more, hoping I'll get it this time, rather than to accept the rejection is to deny my agency as a thinking human being who can make her own decisions.
The rationale for using either emacs or vim even when a single language comes along that has better support in the other one is that there is limited time to learn text editors and both emacs and vim are quite good at being a text edior for many languages and provide more than enough to learn for a lifetime. That said I recently did switch from emacs to vim after watching some videos on the vim movement modes which are quite a bit superior to emacs bindings. I do still prefer the emacs extensibility though. I wish there was an editor combining the two, preferably with a nice modern, thread-safe approach unlike elisp.
I know Brian. He is open to working out appropriate language-integrated support for lenses -- what the Haskell record system could have been?
&gt; For as much as I love Agda, it actively disapproves of non-emacs usage. For as much as I endure the horrific syntax of Coq, the only real way to have an interactive session is if you use proof-general. There's only so many hours in the day, and writing integration scripts for several different editors isn't on the exciting end of the spectrum of proof assistant/dependently typed language implementation. This is especially limiting for Agda, with its three or four person workforce. It doesn't require active disapproval for people to not want to bother writing this stuff. I prefer vim to emacs, but I'd rather just use the latter than reimplement the whole of agda-mode.
I demand cabal and/or ghc integration! This is very awesome; it *must* be integrated into the Haskell ecosystem as soon as possible.
I'll take a stab at answering a bit. &gt; When defining a datatype, and function body, and when overloading, why does the name have to be repeated? Are reasons historic or practical? I assume you mean, why the redundancy between the type declaration and the function declaration. One possible reason is that you don't have to provide a type declaration: function declarations can stand on their own. Another possible reason is that type declarations can be grouped, e.g. car, cadr :: [a] -&gt; a car (x:_) = x cadr (_:x:_) = x I'm not quite sure what you mean when you say "overloading". I suspect you mean supplying a different body for different possible matches, though this is not actually overloading at all.
Constructor and function/method overloading, in computer science, a type of polymorphism where different functions with the same name are invoked based on the data types of the parameters passed. (From Wikipedia.) It sounds at least very similar. Yeah I didn't think about the implicit type declarations, probably because I have unconsciously made them mandatory. I've never seen the grouping before. I don't like it. I like to have 'atomicity' at the function level, and have that property reinforced by visual separation. If Light Box ever gets adopted and files become that convenient form of data serialisation, then grouping would be discouraged. I want to see a function in its entirety, rather than performing a type of metal Template Haskell on the source file. Having said that about the implicit declarations: head :: [] -&gt; Nothing x:_ -&gt; Just x I guess that must mean grouping is the answer.
For people who are confused, this is what he means. You can write a function like: f True = 1 f False = 0 ... or: f x = case x of True -&gt; 1 False -&gt; 0 What he wants to know is why we use `=` in the first version for each case statement but use `-&gt;` in the second version.
I know the 'correct' way(s). Haskell likes to pride itself on its syntax but (admittedly from a naive position) it seems efficiencies could easily be made.
To clarify regarding overloading, regular functions in Haskell do not support overloading. They do, however, allow you to discriminate between *different constructors* of the *same datatype*. `[]` and `:` are two different constructors that both belong to the "list" data type, so a function like `head` is not considered to be "overloaded" because it is merely handling different shapes that values of the list type can take. True overloading in Haskell is accomplished with typeclasses. If you don't mind silly pictures and laid back language, allow me to point you to [Learn You a Haskell &gt; Types and Typeclasses](http://learnyouahaskell.com/types-and-typeclasses/) for an introduction to typeclasses, if you are not yet familiar with them.
I suppose I should have called it 'overloading'. I've read at least one direct comparison to overloading in Java. Okay, so can you condense that into one word like overloading? (Haskell seems to have an inexhaustible nomenclature, so there must be one somewhere.) I understand that true polymorphism is derived from the type classes. During the construct it does indeed look like we are defining multiple functions that share the same name and type definition (*which is very close to the meaning in english, one thing supporting many**). That kind of gets us back to the start, if it's not overloading then why make it look like it is?
Here's one possibility: if I'm defining a function that has multiple cases, and each case's value is some 10-line expression, then that means I have to look up dozens of lines just to figure out the name of the function that I'm defining.
It's called "pattern matching", not "overloading". What it looks like to me is a series of equations. It looks like overloading to you because you are thinking in terms of a different programming tradition.
It's not overloading because it is not defining multiple functions. You are giving multiple equations that, together, define a single function. I can't think of a better term than "defining a function by pattern-matching", since each equation serves to define what the function does when the argument matches the respective pattern.
It is though, in the construct (*text editor**) the user is literally defining multiple functions that presumably become a unified/sophisticated switch-like statement later. It's strictly not overloading because the function doesn't branch on different datatypes, but on patterns of the same datatype. I deliberately didn't use pattern matching because it can be found in multiple places. 'Named pattern matching' was probably the term I wanted to use. Calling it overloading wasn't meant to literally mean the same thing as how Java handles different types, but how they share the same pattern of visually stacked function definitions sharing the same name. Part of my thinking stems from an SPJ talk where he remarked how he suspects that there might be a tighter language with in Haskell, but he doesn't know how to trim it down and he was ~'happy with all that good stuff' anyway.
In a nutshell: ctrlP is 100% VimL, while CommandT requires Ruby, so CtrlP is a great deal more portable across systems and with less dependencies :)
This is entirely equivalent to the original definition of "head" you gave: head :: [a] -&gt; Maybe a head = \n -&gt; case n of [] -&gt; Nothing x:_ -&gt; Just x If you like, you can think of function definition as *assigning* a name to an anonymous function. In that way, it makes perfect sense to use an equals sign.
I am the author of the blog post. First of all, thanks for your comment. However, I am afraid I am missing the point you're trying to make. Note that: *Main&gt; let xs = [ exp (-t) | t &lt;- [0,0.1..]] is not that problematic for I will extract only the first eleven elements of the list. Note also that: *Main&gt; take 11 [0,0.1..] [0.0,0.1,0.2,0.30000000000000004, 0.4000000000000001,0.5000000000000001, 0.6000000000000001,0.7000000000000001,0.8,0.9,1.0] Though there is numerical error in the middle elements, the first three and the last three are indeed correct. If I follow your suggestion, I end up with the exact same problem, though the numerical error is corrupting less samples: *Main&gt; let ts = [ 0.1 * k | k &lt;- [0..10]] *Main&gt; take 11 ts [0.0,0.1,0.2,0.30000000000000004,0.4,0.5, 0.6000000000000001,0.7000000000000001,0.8,0.9,1.0] I do not understand why you're so worried about the last element. My examples above show that the numerical error is corrupting the middle elements, not the last one. 
&gt; That kind of gets us back to the start, if it's not overloading then why make it look like it is? I honestly don't know the answer why they chose to use equals in the first version. It's not overloading in the sense you were describing, in that the function still takes the same type of argument and still produces the same type of result. The only difference is that the specific value of the argument is changing, and we are just defining what our function does for each value the argument might have. To get an idea of the difference between overloading and pattern matching, overloading means that it has different behaviors based on the type of its arguments. For example, let's say I defined the `(+)` function to add if you give it `Int`s and concatenate if you give it `String`s result2 = (1 :: Int) + (1 :: Int) result2 :: Int result1 = ("Hello, " :: String) + ("world!" :: String) result1 :: String That would be overloading. On the contrary, what I was doing with the contrived `f` function is simply defining how it behaves for different inputs of the same type: result1 = f (True :: Bool) result1 :: Int result2 = f (False :: Bool) result2 :: Int
There's an awful lot to like about conduit 0.5. The [haddock docs](http://hackage.haskell.org/packages/archive/conduit/0.5.0/doc/html/Data-Conduit.html) contain the sort of detailed explanation/examples that all packages should have. And the whole things is based on a Pipe type, the constructors of which are in an Internals module, unneeded in normal use.
There isn't a strong reason, in that I doubt these two designs were ever directly compared and someone thought "that's nice, but we'll leave in the two delimiters `=` and `-&gt;` because..." But I suspect that the reason `=` was chosen in one place, and `-&gt;` in the other, is because one of them can be read as an equation, while the other can't. Consider: head [] = Nothing You can read that as a symmetric statement that the two expressions `head []` and `Nothing` really mean the same thing... i.e., that anywhere you have *either* one, you can substitute the other without changing the meaning. This is the beginning of so-called "equational reasoning" that you'll hear a lot about in the Haskell community. Now contrast that with: head xs = case xs of [] -&gt; Nothing If you had tried to view that last line as an equation, it really would make no sense at all... clearly `[]` and `Nothing` are not equal to each other. They don't even have the same type! Incidentally, the reason you can't leave out the parentheses is that `head x:xs` would be parsed as `(head x) : xs`, which is not what you mean. Function application always binds more tightly than operators. Again, this is part of viewing the function definition as an equation; the left-hand side (except for some pattern matching features) is seen as an expression that is equivalent to the right-hand side.
In Agda we have `\ { x -&gt; y ; z -&gt; w }`