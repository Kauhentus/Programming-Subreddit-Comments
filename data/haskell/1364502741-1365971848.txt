You mean the technicalities like "because the system actually does stuff, and takes time, so it's a side effect"? That's not the same as *capturing* outside world interactions. The only way to ask the outside world anything, or directly manipulate it on purpose, are through `IO`.
&gt; Perhaps now is the time to look for something more effective. *groan* oh the puns...
what about fitting ghc in with the ndk. or jhc to gcc, etc. have not looked at android at all. 
US only, I assume
If you ask around in #haskell-mobile on irc ... eventually you will find (a person|people) who have compiled Haskell to android. A current sticking point is you will want to generate marshaling code with TH, but TH and cross-compiling are not trivially compatible. Also you will want bindings to all the libraries you expect to have access to you. Going the Vital route might make interop with Android easier, but without GHC you will have to port most of favorite libraries. 
This sounds approximately correct, but it only applies to the subexpression at the precedence level of + and - (i.e. 6). E.g. you can have the expression 2 == -(-2) and both -'s are treated as unary, because == has a lower precedence.
I'm going to pretend that one was an accident.
Yeah, there are two obvious approaches here: Option 1: Compile Haskell to Dalvik bytecode (possibly via Java and/or JVM bytecode). Discussed here. It's pretty strightforward as a strategy, but the details are hairy. The Dalvik virtual machine doesn't have the same runtime characteristics, like calling conventions and garbage collection, that make Haskell fast with GHC. There's a *huge* unsolved problem making Haskell work well in Dalvik. Option 2: Go in via the NDK. The advantage is that we're a lot closer to the result we want: GHC can already compile for ARM, and so it's just a porting job. But then you just have Haskell in NDK, meaning there's not such a clear path to interacting with the Android system or using the APIs. In other words, this really only buys you command line stuff that can be run in the Terminal app, and back end libraries that implement pieces of applications. But the user-facing code in the application still has to be written in Java. There is an option 3, which is to do option 2, and then build the Android API in Haskell as a free monad, and link with a universal Java front end that just proxies API calls and results back and forth. You end up writing your entire Android application in continuation style; but Haskell is the only language where that might be reasonable!
The first question is “Which country do you live in?”
I'm not sure I can commit to much, but I am happy to help. I'll send a PM.
&gt; It doesn't try to solve the problem in general, and explicitly says so - it is already known that some effects can't be modelled algebraically, like continuations. Do you mean undelimited continuations? It was my impression that [algebraic effects are equivalent in power to delimited continuations](http://lambda-the-ultimate.org/node/4481).
Thereby hangs a tale. Rather than "hangs a tale".
I think finding a tutor is a fine idea. I would also utilize #haskell on irc and Code Review on Stack Exchange
You say you're looking for people capable of helping with an intro class to Haskell - I don't suppose you'd have some examples of the sort of problems involved? I wouldn't personally have the time to help at the moment (and probably not enough expertise anyway), as I'm in the middle of my dissertation, but I would be interested to see what an intro to Haskell class is like, as all my Haskell so far is more of the self-taught variety.
&gt; Only IO itself (and ST) actually capture outside world interactions. This seems like a nonsensical distinction then. On this particular count, there really is no difference between ST and State, just that the implementation is different.
And it is worth noting that the pure function approach is generally going to be faster than using mutation because it doesn't involve a write barrier to play nicely with the garbage collector. (The write barrier is needed because IO/STRefs can create pointers from older generations to newer generations and thus require special attention.)
It is actually better if you learn by submitting questions to Stack Overflow or here. That way other people can learn along with you. Don't worry about asking dumb questions, because half of learning is figuring out what are the right questions to ask. However, you should always make an honest effort to solve things yourself before asking for help, at least if you want to become a good programmer. Paying other people to help is a more valuable skill for a manager. ;)
I can't believe people actually have Haskell classes. I'm in grad school and the whole thing is like a fucking Oracle vendor presentation.
Could you tell us a little about who you are first, and how you'll be using the results of this survey? Also, will you be publishing the raw stats?
Asking haskell questions: lots of forums, you'll get high quality responses and you won't wear out your welcome https://plus.google.com/100165496075034135269/posts/UztvNpAJfow The 2nd G+ is devoted to beginner questions, as is http://www.haskell.org/mailman/listinfo/beginners --------- my single favorite resource besides RWH, the Stanford lecture notes http://www.scs.stanford.edu/11au-cs240h/notes/ ----- the trees: a seemingly huge number of syntax features and libs the forest: a finite number of skills to pick up - loading and testing small bits of code in GHCi - pattern matching syntax - understanding type signatures and compiler error messages - IO monad, and develop intuition about lazy evaluation - Num class, String/ByteString/Text, basic data structures: list, tuple, record etc ----- also look into http://www.haskell.org/haskellwiki/User_groups http://haskell.meetup.com/ -------------- 
It's [Chad Scherrer](https://plus.google.com/108260226117268131144) of [FP Complete](https://www.fpcomplete.com/), a company that does Haskell advocacy.
Cool, I was recently looking for something like that. It's sad, however, that you have to encode precedence rules manually, rather than using annotations supported by happy.
Why is that? I probably know enough of Haskell to tutor OP. But I don't earn $100/hour. It's not like I can go to http://ElanceForHaskellProgrammers.com and get a $100/h (or even $50/h) job using Haskell.
Teachers for musical instruments get ~$30 per 30-45 minute lesson. Private Haskell tutoring should probably be a similar ball park.
Good point, I've added it. Thanks.
I'm quite interested to see how Linux containers (LXC) and [docker](http://docker.io) will change the landscape...
This looks quite interesting, unfortunately, I am not sure about the construction of the product of two algebras. Can somebody give me a hint, perhaps?
I'm not talking about State monad per se, but its usage. do f; g where f :: State Foo and g :: State Foo . You don't know what f is going to change in the state. So if you have a big state because you are going to use one big RWS monad, you're going into troubles as well as using IO everywhere (except IO has also real world side effects).
You're not offering enough money or the right currency to make it worth my while, but here's some more questions it would in general be helpful to know the answers to: * Are you looking for in-person tutoring, or remote tutoring? * What times of day / days of the week are you looking to have the lessons take place at? If you're looking for in-person tutoring, then * Where are you based? If you're looking for remote tutoring, then * Specify a time zone when giving times. * Via what media do you want the lessons to take place? Google Hangout? Skype + ssh to shared tmux/screen session? Etc.
At least it is only semi-manual, since the coercions macro does most of the work.
Yes, Android lib interoperability is the goal. I'd love to write my Android apps in Haskell. One thing I've noticed is that there are too many small projects that "one day" could bring GHC to Android. I'd like to see one big project that the community spends enough time on that it succeeds. [Fay](https://github.com/faylang/fay) is in this category.
Sure, but then what is the benefit of using the browser as a platform rather than just compiling a separate *native* binary for each platform? If you don't have portability in the browser, then it doesn't seem like it would be a particularly appealing target for application development.
When I answered the question, I said that there exists a Haskell group, and I was referring to fp-syd.
Wow, I think I actually know who that is. I've never actually found anyone I knew on reddit before.
Personal message me any questions you have, and I'll take a look. No charge :) **Communities** If you can generalize questions (don't post homework questions verbatim), [StackOverflow](http://stackoverflow.com/) is a good resource. Feel free to ask basic Haskell questions here on [r/haskell](http://www.reddit.com/r/haskell/). There's an active mailing list for Haskell talk, [Haskell Cafe](http://www.haskell.org/mailman/listinfo/haskell-cafe). Visit the [#haskell](http://webchat.freenode.net/) channel on [IRC](http://webchat.freenode.net/). **Tutorials** I'm not sure which textbooks you're using, but I'd highly recommend you supplement them with: * O'Reilly provides an excellent introductory book on Haskell, [Real World Haskell](http://book.realworldhaskell.org/), and provides it legally for free online. * Same goes for Bonus500's creative book, [Learn You A Haskell](http://learnyouahaskell.com/) * There's also an online, interactive tutorial, [Try Haskell](http://tryhaskell.org/) **Tools** * [Haskell Platform](http://www.haskell.org/platform/) is by far the easiest way to get up and running with Haskell. It comes with a compiler (`ghc`), interpreter (`ghci`), and a package manager (`cabal`). * It's a good idea to backup your code, even your homework assignments, on [GitHub](https://github.com/). When you ask others for help debugging your code, they'll appreciate a convenient way to copy your code to their computer and refer to lines in your code with permalinks. You can use GitHub Issues as a form of TODO list, and use [Gist](https://gist.github.com/) for one-shot code posts. * I'm not sure which text editor you prefer. I can recommend a few, but just know that the major ones have some form of extensible syntax highlight support for Haskell ([haskell-mode](http://marmalade-repo.org/packages/haskell-mode), [haskell.vim](http://projects.haskell.org/haskellmode-vim/), [haskell.tmbundle](https://github.com/textmate/haskell.tmbundle), [haskell.nanorc](https://github.com/serialhex/nano-highlight/blob/master/haskell.nanorc), [haskell.syn](http://www.textpad.com/add-ons/synh2m.html)) * You won't need many external packages to learn Haskell, but if you ever do, they're on [Hackage](http://hackage.haskell.org/packages/archive/pkg-list.html). * [Hoogle](http://www.haskell.org/hoogle/) is an experimental search engine for looking up Haskell functions. Supply the desired function in the form input `=&gt;` output, and Hoogle will list functions that do that.
working on 3. Would you be able to do it as a comonad, vs freemonad. I'm still trying to get a handle on when comonads are appropriate. extracting methods/values from the api. you'd still have to trigger the extract function. edit inserted http://comonad.com/reader/2011/free-monads-for-less-3/ for my own future ref
Could any of this be embedded in Haskell through the use of promoted data types? Or is it essential that types and kinds be unified? I'm taking a crack at it here: https://gist.github.com/johnpmayer/5271503 edit: I think I need to be able to promote GADTs. There is work being done in this space by weirich@cis.upenn
Please make sure that you come to the Haskell IRC channel (#haskell on irc.freenode.net). Beginner questions are always welcome there, and usually get plenty of attention.
In fact, you don't even need extensions: f :: Show a =&gt; a -&gt; Integer -&gt; String f x 0 = show x f x n = f (x,x) (n-1) main = mapM (putStrLn . f 0) [0..] This code is Haskell 98, and uses infinitely many instances of Show at runtime. Of course, there's a fairly small finite amount of work to do at compile time, because the instance (Show a, Show b) =&gt; Show (a,b) is just one (polymorphic) thing to compile, even though it applies equally at many different choices of types a and b. In fact, Haskell has polymorphic recursion even without typeclasses. We can write code like this horribly inefficient way to compute powers of 2: data Nested a = N a (Nested [a]) mapNest :: (a -&gt; b) -&gt; Nested a -&gt; Nested b mapNest f (N x ns) = N (f x) (mapNest (map f) ns) aNest = N 1 (mapNest (\x -&gt; [x,x]) aNest) sums :: Nested Integer -&gt; [Integer] sums (N x ns) = x : sums (mapNest sum ns) main = print (sums aNest) Any fully defined value of type Nested a will necessarily contain a value of type Nested [a] as well, necessitating that the code uses "infinitely many" types at runtime. Of course, this isn't so much of a concern, being that the types don't exist at runtime, and even if they did, only finitely many of them would be needed up to any given point in the execution.
1) It looks like two circles, with an overlapping bit. 2) Not really. 3) Yes.
Most of the questions relate to user groups. But we'd also like to get an idea what people are using to track what's going on with Haskell, what packages most need better learning materials, and what domains people see the strongest potential growth for the language.
Ok, I was exaggerating, but a $50 gift card isn't worth $50, and he's not asking for an hour of your time. He asked for someone with a flexible schedule and sufficient free time. That sounds like he wants a few hours of tutoring over the semester, perhaps 30 minutes per week or something. What he's offering is out of touch with economic realities. Second, if you are qualified to tutor him, you COULD get a job as a professional programmer (not necessarily in Haskell), which is currently a highly in demand field. Even if you are still a student, I knew many during my undergraduate days that worked part-time, programming, and made good money.
Good point, and flamingspinach_ is correct. Upvotes all around!
I would think that the market rate would be significantly higher, given that the work alternatives for someone who tutors music are substantially worse than the work alternatives for someone who tutors programming.
OOP in Haskell: nothing to do with monads. * an OOP interface (Java `interface`) is just a record * an OOP constructor is just an ordinary function that results in a value of that record type * an OOP method is just one of the fields from that record (and is usually a function) This gives you immutable objects (c.f. `java.lang.String`). You'll note there are no OOP classes (although they're perhaps implied by the constructors). Alternatively, if you want a more message-passing-based OOP, then an object is a thread, with read access to a `Control.Concurrent.Chan.Chan` that other threads can write messages to.
Your example with twice x y = Cont $ \k -&gt; (k x, k y) does not typecheck because of infinite types. Too bad, as my biggest problem with understanding the weird things Cont can do (and where its superpowers come from) is exactly using the continuation multiple times. From what I understand, if you use Cont in a do-block like this: x &lt;- Cont $ \k -&gt; k 7 the variable x will be bound to 7, or whatever else it is you pass to k. When you don't use k at all, the whole thing will be similar to mzero from the MonadPlus world, in that anything after that line will never be evaluated. But when using the k multiple times... is it like [] in that x will take on all values that k is used with? Perhaps I should not try to understand what it's doing in terms of things I already know, and accept it as something entirely new.
I know OOP and Haskell monads are incomparable. I'm asking if OOP could be implemented *by* monads.
Not using monads, but with type-class magic it appears you can: http://code.haskell.org/OOHaskell/
fwiw, I'm working on a finite domain solver for Haskell based on a particular Prolog implementation. Its definitely very possible to implement constraint logic programming. In this particular case, I do use monads. http://github.com/sonyandy/fd (still some work to do, including examples and documentation)
Oh, right. I changed that part from using `(+)` to `(,)` because I thought at one would be more general yet easier. I think the problem with this is that `(,)` changes the eventual result of the entire thing, `r`, to a tuple, and continuations aren't polymorphic in `r` but only in `a` (i.e. you can't change the type of the final result along the way), so the typechecker complains. If you change it back to `(+)` the example works: flip runCont id $ do x &lt;- cont $ \k -&gt; k 1 + k 2 return (x^2) -- result: 5 The first line forks two "square" computations, `2^2` and `1^2`, which are then added up in the end.
Monads aren't really a "thing" that can do something. A thing can be monadic, just as a thing can be "red". Some implementation of an OO system may have a useful implementation of the Monad interface, but the OO-ness would come from whatever OO you implemented, not the fact you gave it a Monad instance. Monad is a typeclass, and it's really important not to forget that.
At most Universities I have been at in the USA it is cheaper to get tutored in calculus, differential equations, or intro programming/comp sci than it would be to get a drumming lesson or similar instrument. This might be due to the fact that it is easier for a non-expert to judge the quality of music produced by a potential tutor than it is to judge the quality of a math/science tutor.
Each time k is run, everything from that point happens, with x bound to what k was called with. So yes, in a way the binding behaviour is like [] here, as x will take on all values that k is used with. But to be honest, I don't think the Cont monad is particularly useful for running a computation multiple times. For doing that, you really need delimited continuations for it to be useful/easy, but to get those you either need some relatively complex hackery to make it a monad, or alternatively to use an indexed monad.
That seems a different direction than we are headed. We are going to a more interconnected set of applications, and towards the always online systems. Also on your previous point, I still see backend applications as the way to go. We are just starting to develop "big data" applications, and distributed systems, barely scratching what new things we can do with this much power at lower and lower prices.
Well, I'm not talking about business applications but rather consumer applications. People don't like having to be always online all the time and they also like owning their software.
That's missing implementation inheritance, where base class is allowed to call subclass and vice versa (also called "open recursion"). I don't find it useful but maybe it is. 
I've seen talk about delimited continuations and had a look at the CC-delcont package, but that stuff is way over my head right now. So the main novelty of Cont is that it can abort the computation in a more fine-grained way than is usually done with Maybe or Either? That is, you can abort to any enclosing callCC, but it won't stop the entire computation.
This is a good article that you might find interesting: http://www.haskellforall.com/2013/02/you-could-have-invented-comonads.html
You might be interested in tekmo's article titled [Comonads are objects](http://www.haskellforall.com/2013/02/you-could-have-invented-comonads.html).
Thanks! I for just looking at Johan's 2011 survey the other day, but hadn't seen Manuel's. I'm surprised to hear that you think zip/postal code is intrusive, but I don't see how it makes any sense to leave this out. The original purpose of the survey was to get some measure of involvement, interest, and location relative to Haskell user groups, in order to gauge demand for new user groups and help connect users with existing groups. It's not clear the information is of any use unless some aspect of location is included. If you have suggestions for other ways we can help the community grow, please let me know.
Interesting I was just digging into the Haskell Cont to write one in Javascript, which leads me to a comment. I think a good use case your ContT is reverting inversion of control when working with callback heavy libraries. There is Tekmo's and Sigfpe's blog post which are great, but I think there could be something more detailed at a simpler level (Tekmo's is too theory oriented, Sigfpe's is too complicated for pedagogically purposes if you are new to Cont). I think you could add an asyc example that would fill the gap ... or maybe I should actually .... Looks good though! 
Here's an easy I trick I use to remember what `ContT` does: do x &lt;- ContT f y &lt;- ContT g z &lt;- ContT h lift m ... is equivalent to: f $ \x -&gt; g $ \y -&gt; h $ \z -&gt; m So, for example, you could just write: x &lt;- ContT (forM [1..3]) y &lt;- ContT (forM [4..6]) lift (print (x, y)) ... which is equivalent to: forM [1..3] $ \x -&gt; forM [4..6] $ \y -&gt; print (x, y)
Thanks a lot :D! I actually read a lot of blogs about Parsec a month ago or so, and it didn't really make a lot of sense. But with this, I think I got the idea. (This is so great about Haskell - and mathematics: You don't understand a concept, you stop thinking about it for some time, and when you get back to it, everything makes sense.) So, guess I'm off to writing my own parser :).
Curiously enough, the type of (&gt;&gt;=) for any Monad m is isomorphic to m a -&gt; ContT b m a, which is the type of lift. Something's going on here.
Much back-end work does not even make sense to run on a user's computer. Either * Their computer is resource deprived. * Your application requires a network topology. * Your data is sensitive and needs mediation. I am sure I could think of more, but my build has finished so it's back to work.
This is a nice introduction to parsec, but the parsers presented here are also very slow. If you have lots of floats to parse, I can recommend the bytestring-lexing package. Some time ago I had to parse a textual 3d model format. If you are parsing a gigabyte of floats it gets quite noticeable. Also, going through read feels a bit like cheating. At least for the integer case, a variant which does some arithmetic to get to the final result is doable. And is possibly faster than parsing twice.
In fact, that's the definition of `lift` for `ContT` (if you ignore the newtype): lift m = ContT (m &gt;&gt;=)
Trivia: Endo does to Monoid's &lt;&gt; what Codensity does to Monad's bind. Endo (List a) is a difference list.
Maybe Parsec should provide a primitive integer parser. It does provide one if you "instantiate" the Token module - but people often call out to Haskell's `read` for simple parsers away which means parsing twice (once with Parsec , once with `read`).
In layman's terms, what this contest consist of? :)
Also, if you then use ContT Endo (or CodensityT Endo), what you get is list, but in CPS form.
Nice! I didn't know you could do GPU-computing in any reasonable fashion in Haskell. This language is full of surprises. My amateur would be that guess GPU computing profits heavily from the fact that functional languages are more easily parallelizable. Is that correct?
I felt like that about delimited continuations for a long time and put off learning about them for longer than I should have. They are actually pretty simple, at least from a bird's eye point of view. Surely this has happened to most programmers. You're writing some exception handling code and you think "wouldn't it be cool if I could catch some exception here, clean up, and then restore control *to the point where the exception originally occurred*, perhaps with some extra information necessary to avoid the exception next time?". Delimited continuations are basically that. `callCC` allows you to exit, like a typical exception, but delimited continuations also allow you to pass control back from the exception handler to the point where you had exited. Of course, both undelimited and delimited continuations are more than just exception handling, but this is a reasonable enough analogy to inspire one into learning delimited continuations, I hope.
Also related is the fact that `ContT`'s `Monad` instance doesn't require the underlying `m` type constructor to be an instance of `Monad` at all.
OO could be implemented in a number of ways using many different abstractions. I don't see what the ultimate goal of your question is.
It's a programming problem that's posed on Friday, and the solution is due the same time on Monday. Anyone can participate (although I'm sure there are normal restrictions for people closely related to contest officiating), and the solution can be prepared in any programming language. It's typically a challenging problem; many teams don't end up even submitting an answer. The problem usually has (sometimes subtle, sometimes less so) connections to traditional topics in theory of computation, formal languages, etc. But it's also unique enough that you won't be able to look up the answer in a book! More information and links to past tasks are at http://en.wikipedia.org/wiki/ICFP_Programming_Contest
You might want to look at our 2013 ICFP submission about the constrained monad problem, and its generalizations. http://www.ittc.ku.edu/csdl/fpg/papers/Sculthorpe-13-ConstrainedMonad.html It does not directly solve your problem, but does give ways to be creative about where constraints go using normalization and GADTs. We can give a Monad with constraints on its arguments as a standard monad instance definition, for example. The paper also contains an extensive survey of other techniques you might want to look at using. So rather than generate a GADT without constraints, and then dynamically bind the constraints, you could try generate your GADT with constraints directly, using normalization to box the constraints. 
Frege is not a strict subset of Haskell.
Yes, it is easy to show that ST plus a single STRef is isomorphic to State.
Speaking of bytestring-lexing, I was considering taking another look at the float parsing sometime this summer to see if things could be improved further. You wouldn't happen to still have all those floats laying around would ye? I prefer to benchmark on real data whenever possible, and that sounds like a helpful corpus to work with. (Of course, a few megs would be more than sufficient for testing.) If so, feel free to email me.
cool, cheers :)
&gt; 13:21 &lt; Iceland_jack&gt; xQuasar: We are cooperating with you, you're just not &gt; aware that your goal is learning Haskell
Edit: In case it wasn't clear, this post is in reference to the claim that you can code games in haskell and then implying it can be done in FRP. I think it's a little disingenuous to tell new comers that they can code *anything* in Haskell. While *technically* true it obviously isn't practically true. A more honest way to argue why one should learn haskell would be to tell the *indirect* benefits to one's day job. That is, learning haskell arguably broadens horizons enough to advantage you over your otherwise equal peers.
I know this isn't what you're asking, but... you do know you've written theversion of fib that's used as an example of what *not* to do, right? I don't know why the primitive performace differs, but the easiest performance improvement would be to use a better algorithm. Have you tried using some more idiomatic code (like replacing the if expression with pattern matching)? That might give the compiler some hints.
[Why not Haskell?](https://dl.dropboxusercontent.com/u/62227452/Internets/trollinghaskell.html)
That's a really bad way of calculating Fibonacci numbers. Also, the very short running time gives C an advantage because it has a smaller runtime environment to load, with a longer running program, C will have less of an advantage. 2x the running time of C is also about the best you can generally expect from Haskell without excellent knowledge of optimization.
Increasing to fib 45 gives the same result (Haskell &gt; 2x C speed) and takes about 8s. Haskell code in my experience starts as fast as C when linked statically. And in this case definitely so given the tiny size of the .exe files. I do not believe "advanced optimization woodo" is need in this case. The algorithm is trivially simple. The ghc generated intermediate core algorithm is optimum (basically identical to the C code). So the question is: what goes wrong when the core code is converted to machine code? Again, I don't care about the absolute performance of the generated code. The point is to *compare* the code generated *for the same* algorithm using different compilers. 
As I said, 2x the running time of C is usually about as good as it gets. GHC's optimizer just isn't as good as the one in a good C compiler for low-level optimizations.
fib (n + 2) = ... doesn't parse in ghc. The f (n + ..) = ... syntax was removed some time ago if I remember correctly. 
&gt; &lt; ChongLi&gt; xQuasar: even if you decide not to use Haskell in the future, having learnt it you may find new things to apply in your other languages While I'm still only writing trivial programs (like Project Euler solutions) in Haskell, it forced me to start thinking about mutation as creating a new instance. I learned to focus on how my data types were composed, to reduce the cost of a copy, instead of dismissing the pure functional style as wasteful. I applied what I learned to a document-oriented iOS app written in Objective-C, storing my document data as a list of immutable chunks. Creating an immutable snapshot of the data was as simple as a shallow list copy. I could save in a background thread while the user continued to edit, without adding locks or worrying about data race bugs. Clean code, great performance, no data loss issues or save-related crashes despite widespread use. Learning a bit of Haskell has paid for itself.
GHC has pretty cool high level optimizations but is sorely lacking in great low level optimizations. GHC will typically fail at low level microbenchmarks. It's the big stuff where GHC really shines because you don't have to write low level code to get that so-desired "within 2x of C" (I find that this is typically under 1.5x, myself) performance.
You might be right. It is a shame though because those micro benchmarks do have a real impact on how Haskell (and functional programming in general) is perceived by individuals and businesses that are currently using C/C++. The knee jerk reaction to functional programming is always "it is too slow for practical usage". Which is obviously incorrect. Haskell is very fast for real applications. However perceptions are hard to change. Imagine a world where the fastest practical programming language was a functional language? What would make a huge difference and remove the last "C/C++ is faster" barrier.
From my experience, GHC does way less low-level optimizations than any of ocamlopt, MLton and GCC (I don't know how Gambit-C does). but, imho, that should be the reason why the x86/64 code generated by C or ML compilers usually outperform those generated by GHC.
Haskell has always been infinitely fast for me. That's right, **infinitely**. There are things I need to get done that are impossibly buggy, convoluted, and complex in imperative languages. The speed of a program not written is zero. Finite divided by zero is ....
Gambit-C compiles via C so that one is easy. Which makes me wonder why the GHC maintainers spend time maintaining a native code generator when compiling via C is faster *and* makes the generated code portable? I would love to compile via GHC and build the generated C code on an Xbox 360 for example :-)
Hey Haskell wins hands down when it comes to productivity, quality, maintainability etc. No argument there. That's why Haskell is by far my favourite language. The problem is when the execution speed *is* important. For example, crew optimization algorithms that take 24 hours to run using highly optimized C++. If using Haskell =&gt; it takes 36 hours instead (1.5x C++) then it is simply not an acceptable alternative. However if Haskell *could* match C++ for speed then there is suddenly a real business case for starting new projects in Haskell.
The main advantage Haskell has over C++ for high performance computing is not that it's fast already but that it's significantly easier to optimize and refactor. Against a tuned and polished C++ program that pulls out all the good tricks it might be a hard sell. Against a mess that has little hope of significant improvement in terms of quality or performance, it should fare a lot better.
Sometimes the LLVM backend has a better constant factor
But Haskell *is* my day job... D:
I would be much more interested in a comparison of crew optimization algorithms in Haskell and C++ than I am in fib.
pigworker had his iI ... Ii idiom brackets around then too, if not before.
Whether haskell can be used in a day job depends on the job. For the specific example of game programming then coding in haskell for anything other than scripts would be an uphill battle. I applaud the people who fight that battle but I wouldn't wish it upon newcomers.
This is particularly useful for working out where to split modules in two. For example, System.IO.Streams.Internal seems to have a neat split available: http://honeycomb-grouper.jaguarpaw.co.uk/upload/downloader?location=https%3A%2F%2Fraw.github.com%2Fsnapframework%2Fio-streams%2F04b2632e7a20728aa3a9052842e62c0369a086c8%2Fsrc%2FSystem%2FIO%2FStreams%2FInternal.hs https://github.com/snapframework/io-streams/blob/master/src/System/IO/Streams/Internal.hs
I did try to compile JHC for comparison but it failed to build (something about not knowing which Prelude to use). I haven't investigated further.
&gt; I think the hope is that the LLVM backend will fill the gap it was intended for. I’m totally out of my element here, but FWIW, I’m getting even worse times using the LLVM backend (GHC 7.6.2 64bit, LLVM 3.2, OS X 10.8). About 1.7s vs. 1.95s, give or take.
Unfortunately industry strength crew optimization algorithms is not something you throw together in an afternoon. Even with Haskell :-) *However* getting GHC to compile trivial recursive calls effectively after 15+ years of work on the compiler shouldn't be a huge ask? Recursive calls (even trivial ones) is kinda useful for a functional language :-)
There are a bunch of options when execution speed is important: * increase strictness by adding annotations, etc. * FFI * data parallelism * distributed computing &gt; there is suddenly a real business case for starting new projects in Haskell There already is. You just need to give up the assumption that real business can only happen incrementalizing what already exists.
It depends on the program. The LLVM backend particularly excels at arithmetic-heavy stuff.
&gt; Finite divided by zero is .... ... undefined.
I expect the OP is getting bored of responding to this objection. Yes it's a bad algorithm, but so what? The versions being compared all use the same bad algorithm.
The reg-to-reg optimizations sounds promising Simon. A "don't stack check" GHC compiler option would also help. Unless of course GHC needs the stack check to trigger a GC when the stack gets too big. In which case a #NoStackCheck might be more useful for individual functions where we know we are not going to run out of stack space.
Yes and yes :-) Thanks for answering on my behalf.
FYI: I did my test (see above) using GHC 7.6.2, 64bit, LLVM 3.1, Ubuntu 12.10. So a slightly older version of LLVM.
The thing is that if eliding recursive calls makes it faster, haskell should be doing it too.
Yep precisely.
It is testable, I have a list of things that learning functional programming helped me do in an imperative language, and many of them are arguably non-obvious. I should write a blogpost about that...
I'm sorry, which part of his conclusion was aimless?
Some influential "gamedevers" have already put some thoughts on that direction though: see Tim Sweeney's [The Next Mainstream Programming Language](http://www.st.cs.uni-saarland.de/edu/seminare/2005/advanced-fp/docs/sweeny.pdf) (PDF, 2005)
 &gt; So the question is: what goes wrong when the core code is converted to machine code? Well, have you looked?
Until his retirement, my father used Comic Sans for his teaching material. The extra glyph feature (slant) seemed to help dyslectics to distinguish the letters. That's perhaps a rational reason **for** using Comic Sans.
Not what we all want to hear, but even compiling the C example with `-O0` (with either gcc or clang) is still faster than either Gambit or GHC with optimization on. To clarify I just tried: gcc -O0 -o fib-c fib.c
Rogan?
I just recently discovered infoq and I have to say, what a great collection they have there.
Seems to me you are really comparing apples and oranges here. But setting aside the question if your benchmark is reasonable, the short story is, highly optimized C and C++ will in most cases be faster than Haskell, ML or other functional languages. The reason i still prefer Haskell though, is that it is much less effort to write high quality code that is as correct as possible and algorithms are easily distributed over many cores/cpus. I like that the time i invest in cleaning up compile time errors is returned to me tenfold for having to deal with far less run time errors. I like that there is a sound mathematical theory underlying all of this, that lets me reason about my code in a sane way and not having to deal with some made up OOP bullshit. In a time where every day is 0day somwhere, sacrificing correctness on the altar of performance is foolish at best.
I uploaded a source file and got Server error: Ambiguous infix expression ?
If you use mutable array or mutable vector, use them in a relevant monad (ST or IO mainly), then yes. The only weak point is the lack of syntactic sugar to work cleanly with them, but that's not much of a problem.
I wish this had a content warning. Some people don't like to be blindsided by some rather despicable slurs in all-caps.
Haskell will never beat C at C's own game. The best you will ever do is come close to C and then people will say "So what, I already know how to program in C/C++/Java" and then move along. If we really want to attract people we need to focus on features that cannot be easily replicated in other languages. This is one of the reasons that I spend a lot of time working on `pipes` because it is a very useful feature that is incredibly difficult to implement or use in any other language so it forces people to switch to Haskell if they want to use it.
Wrong: Prelude&gt; 1 / 0 Infinity
I did a [naive fib microbenchmark](https://gist.github.com/wting/77c9742fa1169179235f#file-results-md) a while back. Haskell is faster than all the dynamic languages, but still a few orders of magnitude slower than C-family languages. To be honest the biggest surprise (to me at least) is LuaJIT's performance. In the end, you have to understand that the advantages of Haskell is expressiveness, correctness, parallelization, and *pretty good performance*. ghc is also pretty damn good. I would hazard a guess that gcc/icc/clang/jvm get significantly more developer time and corporate sponsorship as well.
I think it depends on the kind of game you are trying to program. If you are doing a first person shooter where frame-rate and graphics are king then Haskell is not for you. More generally, if you are pushing the limits of hardware then you won't want Haskell and you will prefer C++. However, many successful games these days don't push the limits of hardware and Haskell would be an excellent choice for those games.
Thanks for the bug report. Can you show me the source file that causes this error?
Yes. Haskell arrays have O(1) mutation and lookup. You should check out the `vector` package which provides tons of high-performance operations on arrays, both pure and impure. I highly recommend that you try the pure higher-level functions first because they compile incredibly efficient code thanks to stream fusion.
Statement's understood as a limit.
That's a) anecdotical evidence and b) single cases don't make good statistics and c) it doesn't give a comparison to whether spending the time on something else wouldn't have been more efficient.
Also, modern CPUs will sniff the stack pointer and aggressively precache it.
True, but it at least gives a sense of what benefits to expect, and some evidence that they are achievable.
Can you imagine an internet where everyone did that? What a wonderful world it would be...
Monads are a nice thing, but they're not always the best thing. The following is much cleaner and easier to follow: fib :: (Num a, Eq a) =&gt; a -&gt; a fib 0 = 1 fib n = loop 1 1 (n-1) where loop x y n | x `seq` y `seq` n `seq` False = undefined loop x y 0 = y loop x y n = loop y (x+y) (n-1) You can add the basis cases for `fib 1` and `fib 2` if you really want; the former seems reasonable enough, though the latter seems superfluous to me. And you can use `-XBangPatterns` if desired, though that seems marginally slower for some reason. (I mostly avoid `-XBangPatterns` just because I still support some older GHCs.) *Edit:* Of course, if you want it to actually be type correct, it'd be better to do: fib :: (Num a, Ord a) =&gt; a -&gt; Maybe a fib n | n &lt; 0 = Nothing | otherwise = Just $ loop 1 1 (n-1) where loop x y n | x `seq` y `seq` n `seq` False = undefined | n &lt;= 0 = y | otherwise = loop y (x+y) (n-1)
If only governments were as smart as #haskell folks ... this world would be such a better place.
... you win this time.
GHC has always focused more on high-level optimizations rather than low-level ones. Thus, as others've mentioned, if you're only looking at microbenchmarks, then C will almost surely outperform Haskell. If you'd like to help change that, I'm sure the GHC devs wouldn't mind. The real strength of GHC/Haskell when it comes to optimization is when you move beyond microbenchmarks and start looking at real programs. In the real world, it's not possible to eek out every last ounce of performance, because we have other concerns as well (e.g., legibility, maintainability, extensibility,...). In languages like C, the compiler is crippled against making very many high-level optimizations because there are too many corner cases to deal with; this is why C compilers have always focused on microbenchmarking in order to eek out what performance they can. Whereas in Haskell, you can write code which is modular, extensible, etc, and the semantics of the language permit high-level optimizations (e.g., deforestation, aggressive inlining, rewrite rules). By making use of these high-level optimizations, I have written many real-world programs in Haskell which are as fast or faster than the C programs they were meant to replace. TL;DR: optimization is non-compositional. Looking at code that's "as basic as it gets" is not a good way to gain insight into general performance characteristics.
The reason for this is, basically, that every monad can be the composition of two adjoint (basically means "almost opposite") functors. In the case of `State s`, that is `(-&gt;) s` (i. e. `Reader s`) and `(,) s`. `Codensity f` is `f` composed with its adjoint whenever it exists, and since the adjoint exists in the case of `(-&gt;) s`, and that that leaves `State s`, we get `State s`.
Transcript: &gt; This is a very funny question, why I use Comic Sans. So. All my talks use Comic Sans, and I frequently see little remarks, “Simon Peyton Jones, great talk about Haskell, but why did he use Comic Sans?” But nobody’s ever been able to tell me what’s wrong with it. I think it’s a nice, legible font, I like it. So until someone explains to me — I understand that it’s meant to be naff, but I don't care about naff stuff, it’s meant to be able to read it. So if you’ve got some rational reasons why I should not, then I’ll listen to them. But just being unfashionable, I don’t care. That’s a new word for me, “naff”. (Comic Sans being ugly is a good enough reason not to use it, but until I’m Simon Peyton Jones I’ll keep my mouth shut.)
"Naff" is in common use in the UK, although it's probably a bit old-fashioned (or naff, if you will). It's from [Polari](https://en.wikipedia.org/wiki/Polari), which is fascinating in its own right.
Bona!
 λ ➔ 1 `div` 0 *** Exception: divide by zero 
No, you just redefined the type. That’s cheating. Otherwise, I could just go Prelude&gt; let divB x y = if x /= y then "OVER 9000" else "True" Prelude&gt; True `divB` False "OVER 9000!”
Transcript of latter question: &gt; So […] Barbara is calling for a programming language that is fantastically simple and easy to learn but wonderfully powerful and doesn't have any glass ceilings in it. So, umm… [pause, followed by growing laughter]
As usual, great presentation from SPJ (w/ a blue background). I can't stop the video, he don't let me interrupt.
Thanks!
Maybe he just doesn't care about types.
This is documented in two posts on my blog. More recently, Dan Doel talked about why Codensity is "bigger" than you might expect naively: http://comonad.com/reader/2012/unnatural-transformations-and-quantifiers/ And much longer ago, I talked about how adjunctions combine with Kan Extensions (including Codensity). http://comonad.com/reader/2008/kan-extensions-ii/ The http://github.com/ekmett/monad-ran package also witnesses this equivalence.
It's basically a fun way to spend a weekend with other programmers. The problems have traditionally been very interesting and the contest committees have clearly spent lots of time on the problems and the architecture around them. But don't take my word for it--several of the past contests can still be tried (although without competing, of course). I suggest the [2006 contest](http://www.boundvariable.org/task.shtml)--implement a simple VM in your language of choice and start playing.
I'm not particularly attached to the Twan-styled free applicative, it was just the first that came to mind as obviously free. If you want another order or to make a strong case for switching to another, I'd highly recommend creating an [issue](http://github.com/ekmett/free/issues) so people have a place to debate it.
I think micro-benchmark such as this one have little value to asses the high-level performance of a programming language, but for what it's worth OCaml: let rec fib n = if n &lt; 2 then n else fib (n - 1) + fib (n - 2) let () = print_int (fib 40); print_newline () On my machine: $ ghc -O -O2 -optc-O3 -o fib-ghc.exe fib-ghc.hs $ time ./fib-ghc.exe 102334155 real 0m1.327s user 0m1.316s sys 0m0.000s $ ocamlopt fib.ml -o fib-ml.exe $ time ./fib-ml.exe 102334155 real 0m0.674s user 0m0.672s sys 0m0.000s $ g++ -O3 -o fib-c.exe fib.c $ time ./fib-c.exe 102334155 real 0m0.607s user 0m0.604s sys 0m0.000s That's coherent with Simon's remark that using the native stack is advantageous over allocating frames on the heap (ocamlopt uses the native stack). Well, advantageous for speed at least, as having OS-bounded stack usage is sometimes a pain (which is why SML/NJ also uses a heap-allocated stack). Note that this is not a matter of "low-level optimization" (the OCaml compiler performs little optimization), rather of efficiency-oriented design choices for even a simple compiler. 
[Done](https://github.com/ekmett/free/issues/15)
Integers aren't the problem, integer division is. Rational numbers *are* more acceptable.
I love it when I get to know about these Haskell related events in japan AFTER they already happened.
True, but is not the intersection Frege/Haskell "bigger" than Vital?
Within a Haskell application, there are no pointers. Shared mutable objects are referred to by way of their container. For example, an IORef is a kind of container that can "point to" a mutable value. The only time we use "real pointers" is when we call C code directly.
There is [Foreign.Ptr](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Foreign-Ptr.html) when you really do need to use pointers. IIRC ByteString uses them for performance reasons.
Do you speak Japanese ekmett?
Thanks for this Roman! It's good to see the different versions presented together. I came up with my version specifically in order to avoid "denormalisation", that is, equal values being represented by distinct terms. However the other implementations you mention do this better than mine. 
If you have to ask, you don't need them.
huad, I'm glad that you are trying to learn Haskell, but it's really hard to answer your questions properly because your questions are all very short and very vague. In the current question, for example, you say that you already know that Haskell has pointers, but that doesn't tell us much. A more precise description of what you already know, such as "I know that there is Foreign.Ptr in Haskell" or "I know that Haskell uses pointers to implement its linked lists" would make it crystal clear which pointers you are talking about. Another thing which might help us answer your questions better is if you gave examples of what you are asking for. In the current question, for example, you are asking if Haskell's pointers are like C's. The answer will obviously be that they are similar in some respects (e.g. you can follow the pointer to the thing it points to) and different in other respects (e.g. the syntax is different). Which aspects are you interested in? If you gave an example of something you would do with pointer in C, we could tell you whether you can also do that with Haskell's pointers. For example, in C, we can use pointer manipulations to find the length of a null-terminated string: int strlen(char* str) { char* p; for(p=str; *p; ++p); return p - str; } You can also do pointer arithmetic on Foreign.Ptr, but not on the pointers Haskell uses for its linked lists. The type String is implemented using a linked list, so we would not compute the length of a []-terminated string using pointer arithmetic, but using recursion: length [] = 0 length (x:xs) = 1 + length xs And we would probably hide this recursion inside a higher-order function such as "foldr": length = foldr (+) 0 
Firstly, those results show Haskell as **less than one** order of magnitude slower than C. Secondly, how was Haskell ran, and with what compiler? Thirdly, your Haskell code is not at all analogous to either the C, Java or Go code. Your Haskell code uses infinite-precision integers, whereas the others all use machine Ints. This'll slow things down *substantially*, and I suspect is the biggest problem here. You also do two comparisons in the Haskell code (== 0 and == 1) rather than (&lt; 2) in most of the others.
I see. Most functions depend on a few data types, and this clutters the graph. I wonder what layout would best take that into account. 
There is also [free applicative of Sjoerd Visscher](http://hackage.haskell.org/packages/archive/free-functors/0.1.2/doc/html/Data-Functor-HFree.html). It's very different from the ones listed in the blog post and is totally mind-blowing, IMO.
Another use is when you have variables that can, or cannot point to a value. In Haskell, you should use the Maybe type. E.g: Searching a index in a map will return Nothing or (Just val). find :: Map Int a -&gt; Int -&gt; Maybe a 
Not at all very well. I used to run an import/export business named otakuland.com that would dropship anime and models from Japan back around 1994-96, but I basically let the little bit of Japanese I acquired back then fade away. =( It is now down to the point where I can mostly use it to pick out lines in anime and do little more than read the 'sound effects' drawn into manga. That said, we were able to work around the language barrier quite well!
Sorry about that. FWIW I think they filled their available space this time. =) If you follow the partake.in link from the post, it should have another upcoming workshop or two listed.
I am sort of curious how 6 am. edwardk explanations sound compared to 6 pm. edwardk explanations. :)
There's nothing wrong about asking questions.
http://partake.in/events/search → カテゴリ "コンピューター" → Search works pretty well, though I heard about it from my cow-orker.
Availability of infrastructure aside, I wouldn't wish anything else upon newcomers.
"this post was submitted on 01 Apr 2013"
Clearly I'll need to advertise harder next time. ;)
That's cruel. ;_;
Certainly. :(
Oh dear.
We all did. And then Tekmo made this thread.
My hopes... so high...
nice - thank you for sharing
I'm stunned.
&gt; The trouble is that GUI widgets need to know their parents to receive events. Do they? I've seen event bubbling happen both ways, ie. bottom-up and top-down. Bottom-up bubbling seems consistent with FRP. Unless you're referring to something else here? &gt; But this cannot be implemented on top of pure FRP, because in pure FRP an event stream has to be fully determined before it is used. Isn't this dynamism satisfied by containers/parents using a signal for its widget lists? I know you've covered this topic on LtU before, so I don't want to take up too much of your time, but perhaps you can explain why the Haskell implementations of FRP, like Reactive, reactive-banana, Fran, etc. do not satisfy what you consider "pure FRP". I suspect my confusion stems from this.
What would the equivalent of profunctor be on a trifunctor ... multifunctor etc?
This is great! Another cool thing about (co/contra)variance with respect to functions: each time you nest back a function as the argument to a function, the argument to the nested function flips it's variance. f :: a -&gt; b -- f is contravariant in a g :: (a -&gt; b) -&gt; c -- g is covariant in a This provides a good path for deriving the type of Cont.
If it had been *Applicative is now a superclass of Monad in GHC HEAD*, I would have fallen for it.
Very nice work! Choosing this for FP Complete's Pick of the Week.
Well there isn't **a** equivalent... there have to be multiple equivalents. That is, a multifunctor can be considered as taking a set of arguments, of which some are covariant and some are contravariant. Thus, there are two variances of 1-functors (co, contra); three variances of 2-functor (co/co, co/contra, contra/contra); four variances of 3-functors (co/co/co, co/co/contra, co/contra/contra, contra/contra/contra); etc. Of course, these numbers are collapsing over the order of the arguments, since we're considering arguments as a set. Also, we could collapse things further if we identify things with their duals--- thus, co-1-functors and contra-1-functors are essentially the same; and while co/co-2-functors and contra/contra-2-functors are essentially the same (co- and contravariant bifunctors), co/contra-2-functors are different (profunctors). That sort of collapsing would yield two varieties of 3-functors (co/co/co, co/co/contra), similar to the two varieties of 2-functors--- with the notable difference that 2-profunctors are self-dual (co/contra = contra/co), whereas 3-profunctors are not (co/co/contra /= co/contra/contra).
I put another comment up, which describes, lyah, building -&gt; from partial application, of one typeconstructor, just wondering how you would alternate, say you have a tri functor, and would like to be covariant in the first 2 parameters, contra ,in the third. etc. It would like to see the step, where you switch from co to contra. 
I type constructor can be partially applied to be a functor, but it doesn't have to be. `Maybe` does note have to be applied to anything to be a functor. A functor requires that the type constructor has one argument left to be filled. A bifunctor requires there to be two arguments. It helps to look at the Kinds of types. Kinds are like types for types, and indeed type constructors look much like functions in Kind space. (-&gt;) :: * -&gt; * -&gt; * -- The function type constructor takes two types and makes a type. Maybe :: * -&gt; * -- Maybe takes a type and makes a type. Perhaps we should have a separate "hastype" for kinds, but so far we just re-use `::`.
I'm sad now. 
&gt;When we look at it as (-&gt;) r a, we can see (-&gt;) in a slighty different light, because we see that it's just a type constructor that takes two type parameters, just like Either. But remember, we said that a type constructor has to take exactly one type parameter so that it can be made an instance of Functor. That's why we can't make (-&gt;) an instance of Functor, but if we partially apply it to (-&gt;) r, it doesn't pose any problems. Hopefully the above shows up as a quote, just the relevant section, from lyah. I was just wondering if you could make multiparameter functors, instances of the typeclass of functors, from this partial application method. And if so, how would you put in the switch, from co to contra? I understand, the lyah, was probably just a digression, to help teach, and I might be pushing it too far. 
Point taken, though the OP framed the question (and a few others in recent days) in such vague terms that a useful answer is essentially impossible to give (though gelisam gave it a hell of a go).
You're thinking of `&lt;=&gt;`
Applyril.
I knew that today was April fools, but figured that Tekmo wasn't the kind of person who would post a joke in such horrible taste on this subject; I got excited, and now I am just sad. :-(
For you; for me all of this is happening on April 2nd, which lowers one's guard a bit...
In all seriousness though, would this not be possible? Make it so that GHC warns about monad instances that do not have a functor instance with a message that explains that in some future version Functor will be made a superclass of Monad? Obviously this would require a little bit of special casing, but it's not that bad. I know what we really want is a way to specifiy default instances for superclasses, but there's no consensus on what the best way to do that is; looking at the comments here it seems like there absolutely is a consensus in the community that Functor should be made a superclass of Monad. 
"Mon Apr 1 15:16:24 CEST 2013"
That's it, thanks!
After some more experiments, I came to the same conclusion (i7 here, and the difference is even wider). Despite executing about the same number of instructions, the C verison achieves 3 IPC, whereas GHC gets just 1. Removing the stack check and doing a few micro optimizations by hand made almost no difference. You know, back in the good old days when we used Sparcs, GHC used to wipe the floor with C in this benchmark, due to the Sparc's register windows which are terrible for recursion.
This one did make heart heart jump into my throat -- and then I suffered the chilling fear that the title was accurate and it was `Functor` and not `Applicative` that might have been added as a superclass and then I looked and I found myself relieved to find at least that it at least hadn't been screwed up that badly. =)
Oh, hey, I've actually kinda done this before, all on my own: import Control.Monad.Reader -- The function and argument names are for reasons I won't explain... reshape :: (r -&gt; r') -&gt; (a -&gt; b) -&gt; Reader r' a -&gt; Reader r b reshape reframe recube = fmap recube . withReader reframe This operation gets more interesting when you use types for `r`, `r'`, `a` and `b` that share some type variables. For example if you have something like this (which I do!): f :: Key t q -&gt; Key s p g :: Reader (Key s p) a -&gt; Reader (Key t q) b reshape (nub . map f) g :: Reader [Key s p] (Reader (Key s p) a) -&gt; Reader [Key t q] (Reader (Key t q) b) -- Yeah, I know, those nested Readers are isomorphic to ReaderT [Key s p] (Reader (Key s p) a) Now I have to figure out whether there's something like a `Proapplicative`...
Pretty much the same -- with slightly longer pauses while I collect my thoughts.
I thought this would be the best way to bring attention to this long-outstanding problem that plagues us all.
Sure. If you can take an n-ary type constructor and turn it into a unary type constructor that has a meaningfull interpritation as a functor. One way to do this is to apply it to n-1 arguments. Or you could wrap it in a newtype if you want to "focus" on some argument that isn't the last. I am not sure what you are asking about "switch" ing from co to contra. Are you saying if you have a type constructor that is covariant in some arguments and contravariant in others, how do you switch from using it as a contravariant functor or a covariant functor, this again is a good case for newtypes.
I tried to implement a indexed map for lists, where you can index with any Integral. So far I can define go :: (Integral i, Indexable i p) =&gt; i -&gt; p a b -&gt; [a] -&gt; [b] go i f (x:xr) = indexed f i x : go (i+1) f xr go _ f [] = [] which indexes starting from a given index `i', but defining imap = go 0 gives me Indexed.hs:28:1: Ambiguous type variable `i0' in the constraints: (Integral i0) arising from the ambiguity check for `imap' at Indexed.hs:28:1-11 (Indexable i0 p) arising from the ambiguity check for `imap' at Indexed.hs:28:1-11 Probable fix: add a type signature that fixes these type variable(s) When checking that `imap' has the inferred type `forall i (p :: * -&gt; * -&gt; *) a b. (Integral i, Indexable i p) =&gt; p a b -&gt; [a] -&gt; [b]' Probable cause: the inferred type is ambiguous Said fix doesn't work for me of course. Does anyone have an idea how to implement `imap'? 
There are many, and there is even a compilation of IT related events called the "IT勉強会カレンダー", which I follow religiously, but I didn't see there anything about this particular workshop.
Great, I hope I can get there next time.
It would be great to have the technical reasons documented in the commentary or on stackoverflow. http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution/CallingConvention 
So Haskell can be as fast as C. Even for silly micro benchmarks. That's more like what I expected.
No that is not correct. I am explicitly using Int in the type signature and not Integer =&gt; GHC compiles to Int# (primitive int types). There is a post below showing the GHC intermediate core result.
Yes absolutely. Then more "eyes and minds" might be able to help. Diving into the GHC source code is a huge task.
Yes, the problem is that if that now there are monads witout a Functior instance, and they would be broken.
The *whole* profunctors tutorial? Why did the Haskeller's head asplode? Because somebody functoriality.
&gt; Pro­ductiv­ity means that, even if a pro­gram gen­er­ates an in­fin­ite amount of data, each piece will gen­er­ated in fi­nite time. This is an odd sentence.
So maybe the compilation strategy currently being used by GHC worked well in the past (beating Sparcs as you say) but less so for modern/future CPU's? My guess is that modern CPU's are super optimized for "C stack style" machine code and will be even more so in the future. So JHC's strategy of compiling "C stack style" gives it a huge advantage, making it run not just a bit faster but twice as fast as GHC.
Simon has looked (read above). He knows what he is doing.
The definition of `Contravariant` just seems so wrong to me I can't get over it. How?! Where does the `b` come from?? class Contravariant f where contramap ∷ (b → a) → f a → f b
Then fix them.
`a` and `b` are type variables. The definition in plain English states: Let `f` be a type taking another type as argument, example, Maybe, [], etc... `f` is a `Contravariant` if and only if there is a function `contramap` of type `(b -&gt; a) -&gt; f a -&gt; f b` Meaning, contramap takes a function from `b` to `a` and a returns a function from `f a` to `f b` (reverse the arrow).
For reference, I get this Core (ghc 7.6.2, -O2): Rec { $wfib :: Int# -&gt; Int# $wfib = \ (ww :: Int#) -&gt; case &lt;# ww 2 of _ { False -&gt; case $wfib (-# ww 1) of ww1 { __DEFAULT -&gt; case $wfib (-# ww 2) of ww2 { __DEFAULT -&gt; +# ww1 ww2 } }; True -&gt; ww } end Rec } fib :: Int -&gt; Int fib = \ (w :: Int) -&gt; case w of _ { I# ww -&gt; case $wfib ww of ww1 { __DEFAULT -&gt; I# ww1 } }
Working at the type level makes me feel so nostalgic for the days when I wrote in languages using callbacks (functors I suppose) and you had to write wrappers that basically shuffled arguments around because you didn't have lambda abstractions. (Is it still April 1st somewhere?) You want type-level lambdas. Not that there is such a distinction in Agda.
You can answer the Proapplicative question by a simple application of the "edwardk's already done it" law.
I can see that's what it says but it doesn't really help me with what it means. If I have some `g :: b -&gt; a` and `f a` how do I get `f b` out of that? Unless the `f a` is hiding a `b`-typed something inside it I seem to be on a hiding to nothing. For example, if `g = length` then somehow given `f Int` I can get `f String` back?
Perhaps you find it contraintuitive because you're too used to thinking of functors as *data containers* of some kind. Flush any concrete types out of your system, and start by staring at the `Predicate` example: try to write a `Functor` instance for it. Then slowly realise you can't… but that you *can* write `Contravariant`. Read `contramap`'s type as: *you tell me how to convert `b`s to `a`s, along with something that operates on `a`s, and I'll tell you how to do the same with `b`s*. [Relax as it kicks in](https://fbcdn-sphotos-d-a.akamaihd.net/hphotos-ak-ash3/13173_10100474770233810_885210003_n.jpg).
Ah! Yes, it *is* kicking in :-) :-( :-) This will take some time...
Well, if it was implemented at the same time as auto-generated functor instances(all functor instances are unique) then this wouldn't be a problem.
Very nice! I hope that your work on Agda highlighting not only results in more blog posts from you but in ones from other people (including me :P) &gt; Let’s prove that ≡ is an equivalent relation, and a congruence law which will be useful later: You probably meant equivalence relation there! I'd also end it off with sorting an actual list and showing the result :) I know the running joke is that Agda users don't run their code, but it's still nice to see output every so often! If you want to take it to a painfully thorough conclusion (or follow-up), you might also want to prove that the output is a permutation of the input. And make the whole thing keep track of how much work it's doing so you can show how long the algorithms take. I'm on my phone now but also, is your treesort trying to keep the tree balanced? If not, evil input will still make it take quadratic time, I'd expect.
I'll make a blog post for the highlighting as soon as it's in a nice state, there are a couple of things to iron out (for example the metadata showing up...). Thanks for the correction (equivalent vs equivalence). Every other suggestion is good, but the good thing about that example is that is very short. Proving with permutation and complexity would be nice but I'm not aware of any sweet implementation, I'll think about it (suggestions welcome!). Note also footnote 6 regarding this. And yes, the treesort is still quadratic, worse case. that is probably easy to fix but again I wanted to keep things short and sweet.
Thank you for that, it was brilliant :) When I started using Haskell a few years ago, I really did not think that this kind of minor design wart would settle for so long in the language. In my opinion, if it was fixed in GHC and in the platform today, popular packages would be fixed in a matter of days, and all that would quickly belong to the past. Maybe, with recent versions of GHC, we can even generate automatically a list of hackage packages holding datatypes that have a Monad instance but no functor/pointed/applicative instances.
I think that comment was not directed at you.
One thing I noticed is that any Agda or Coq post starts from defining own lists or numbers. Isn't there a standard library or something? I'm missing a sense of how people write real agda code. Do you always start from the lowest level of abstraction?
Example: newtype b :&lt;- a = Op (a -&gt; b) instance Contra ((:&lt;-) r) where -- contramap :: (b -&gt; a) -&gt; (r :&lt;- a) -&gt; (r :&lt;- b) -- contramap :: (b -&gt; a) -&gt; (a -&gt; r) -&gt; b -&gt; r contramap f (Op g) = Op (g . f)
It's probably best just to spell `(:&lt;-)` as `Op`, as `Data.Functor.Contravariant` already does.
If it's balance that you want, the same pivoting method works rather sweetly with 2-3 trees. Or, for nlogn the easy way, just write mergesort. I've always wanted to prove that sorting functions permute by observing that they obey a linearity discipline. I mean, look at the code! The pattern variables from the input are each used exactly once in constructing the output: that's got to be a clue! As for complexity, that requires some thought about how to impose a cost model. Counting comparisons might be one approach (although that clearly does not tell the whole story). Fascinating also to consider a primitive recursion operator for trees which at each node permits a recursive call on at most one child. Then you get a relationship between the complexity of recursion and the depth of the tree. Make up your own dodgy pun about getting log factors from balanced trees...
Would you mind answering some questions for me? I tried to follow along, but there's some concepts that still baffle me. infix 3 ¬_ ¬_ : Set → Set ¬ X = X → Empty Doesn't this mean that `¬Empty` is `Empty`? Rel : Set → Set₁ Rel X = X → X → Set So is the fact that `(∀ {X : Set} → X → X → Set): Set₁` known to the Agda compiler, or is this an assertion we're making with this definition? Decidable : ∀ {X} → Rel X → Set Decidable R = ∀ x y → Either (R x y) (¬ (R x y)) I'm not really sure which `Set` `∀ x y → Either (R x y) (¬ (R x y))` is. This might stem from my earlier confusion about `¬`. total : ∀ x y → Either (x ≤ y) (y ≤ x) Is there any reason this can't be written `total: Decidable _≤_`?
My first question when glancing at the abstract was: "do they use copatterns?". I've been strongly impressed by the copatterns work, a kind of "wow, that is the right way to do codata" moment -- but note that I'm not an expert in the field and may be a tad too gullible for that reason. The answer, it quickly turns out, is "no", so I decided to have a look again at the copattern work to get an idea of if your approach could be ported to such a setting. No need, as I quickly found that Andreas Abel and Brigitte Pientka apparently also have submitted to ICFP, with a different approach, [Wellfounded Recursion with Copatterns](http://www2.tcs.ifi.lmu.de/~abel/icfp13-long.pdf). After having given a glance to both, I like both approaches (I found the big idea that productivity becomes termination checking in a copattern setting really cool, and on your side the regionish feeling of finite clocks smells definitely interesting) and think they are rather orthogonal / complementary. There is a remark on the ST-like local polymorphism being "more local" than sized types, but I'll admit it flew over my head for now; I need to understand the clocks approach better.
It's normal to be baffled :). &gt; Doesn't this mean that ¬Empty is Empty? Luckily, it doesn't. For example, using the ≡ and the numbers in the blog post, I can write foo : ¬ (1 ≡ 2) foo () Since Agda knows that the indices of ≡ must be definitionally equal, and also knows that 1 and 2 can't possibly be unified. Edit: I read that comment wrong! Let's try ¬ Empty foo : ¬ Empty foo () Note that the identity also works foo : ¬ Empty foo theSkyIsFalling = theSkyIsFalling &gt; So is the fact that (∀ {X : Set} → X → X → Set): Set₁ known to the Agda compiler, or is this an assertion we're making with this definition? The type hierarchy (the fact that 1 : ℕ : Set₀ : Set₁ : Set₂ ...) is at the very core of Agda's type system, so yes Agda knows it. You could see it as a generalization of Haskell's `1 :: Integer :: *`. &gt; I'm not really sure which Set ∀ x y → Either (R x y) (¬ (R x y)) is. This might stem from my earlier confusion about ¬. Well, the type of that term is the type of `Decidable` applied to a `Rel X`---`Set`. As said `Set` is a shorthand to `Set₀`. Why is this the case? Well, what's the type of `Either`? `Set → Set → Set`. What's the type of `R`? `X → X → Set`. What's the type of `x` and `y`? `X`. So we are applying `Either` to two `Set`s, and we get a `Set` back. It's very much like with values, but now types are in the game as well. &gt; Is there any reason this can't be written total: Decidable _≤_? Decidable _≤_ = ∀ x y → Either (x ≤ y) (¬ (x ≤ y)) Which is different from ∀ x y → Either (x ≤ y) (y ≤ x) If you have any other questions please ask, or better come to #agda!
&gt; Agda or Coq posts/books tend to focus on teaching a language and a way of expressing things, and so they prefer to build up from more or less 0. Right, I understand that, but we need both types! If every Haskell post introduced its own State monad, we would never learn how to use mtl. &gt; There is also a consider that Agda is very much in its infancy, so nobody really has an idea of what 'real' Agda code looks like because there aren't any big, 'real world' code bases I don't necessarily mean 'big'. I mean the sort of code you write not for a blog post, but for a library. (I assume agda has its own little hackage?) Anyway, thanks for your link, it definitely helps.
&gt; If it's balance that you want, the same pivoting method works rather sweetly with 2-3 trees. Or, for nlogn the easy way, just write mergesort. Yes, I might take inspiration from your Epigram balanced trees and merge the two when I have time. &gt; I've always wanted to prove that sorting functions permute by observing that they obey a linearity discipline. I mean, look at the code! The pattern variables from the input are each used exactly once in constructing the output: that's got to be a clue! Absolutely, as mentioned in the post proving that length is preserved is super easy (with `Vec`) and at that stage it ought to be even easier to prove permutations since we are not fabricating any new elements! But the reality is a bit harsher.
&gt; Right, I understand that, but we need both types! If every Haskell post introduced its own State monad, we would never learn how to use mtl. I think you should imagine what Haskell was in 1995. There still isn't a solid ecosystem in which to program into, and the education that is really needed now is something that introduces the broad ideas, since advice focused on practicality will probably be out to date soon. Moreover, Agda/Coq and their community are somewhat oriented towards theorem proving rather than software, so that influences the material as well... Idris is a programming languages with a more 'practical' mindset, but that's in even earlier stages. &gt; I don't necessarily mean 'big'. I mean the sort of code you write not for a blog post, but for a library. (I assume agda has its own little hackage?) That's an incautious assumption :). By the way, don't get me wrong, something like an introduction to Agda's stdlib would be *great*, but nobody has the time to write it and it can easily end up being a wasted effort, given how rapidly things change.
&gt;Doesn't this mean that ¬Empty is Empty? `¬Empty` is `Empty → Empty`, `id : Empty → Empty` inhabits that type. &gt;So is the fact that (∀ {X : Set} → X → X → Set): Set₁ known to the Agda compiler, Yes. If `A : Set a` and `B : A → Set b`, then `((x : A) -&gt; B x) : Set (max a b)` &gt;I'm not really sure which Set ∀ x y → Either (R x y) (¬ (R x y)) is. This might stem from my earlier confusion about ¬. It's a `Set`: `Either (R x y) (¬ (R x y))` is a `Set`, `X` is also a `Set`, so `((x y : X) → Either (R x y) (¬ (R x y))) : Set (max 0 0)` &gt;Is there any reason this can't be written total: Decidable _≤_? I think `∀ x y → Either (x ≤ y) (y ≤ x)` is a weaker condition than decidability, but I might be wrong on this one.
/u/geezusfreeek is correct, my post was not aimed at you, but at /u/AncientPC's microbenchmark link.
Right. It takes only finite work to be ready for one level of inspection.
They are functorial, from the category |Hask| of types and the only morphisms being id.
Oh, I added a small demonstration of the function :).
Thanks for the link to Abel and Pientka's work, I hadn't seen that. I (and, I think, we) haven't thought at all about integrating the clocks and guards approach with copatterns yet. Ideally, I think that it would be great to be able to elaborate corecursive definitions made with copatterns into our system, similar to how [Eliminating Dependent Pattern Matching](http://www.cs.st-andrews.ac.uk/~james/RESEARCH/pattern-elimination-final.pdf) does for recursive definitions over inductive types. A nice property of copatterns is that they give you a way to understand the execution of a corecursive program in terms of rewriting. I have an idea for how to come up with a rewriting system for guarded recursion, based on not doing reduction under a 'pure', and eliminating 'pure's when 'force's are applied to them, but this is future work. Neel Krishnaswami and Nick Benton have defined notion of reduction and strong normalisation for guarded recursion, which was extended to dependent types by Severi and de Vries at last year's ICFP. What we were trying to get at with the "more local" comment with clocks was that once you have closed over a clock, you can effectively ignore it. The head and tail deconstructors for streams do not mention clocks at all. The locality also applies to our example with replaceMin, which doesn't make any use of coinductively defined data structures at all. It doesn't obviously fit into any scheme using copatterns or sized types. Abel and Pientka's sized types are great because they integrate checking termination of recursive and and productivity of corecursive programs into a unified system. We just shunted the problem of proving that recursive programs terminate off to the user by making them write every recursive program as a primitive recursion. Also, since sized types appear to be very precise, it is possible that there are corecursive programs that are more naturally expressed in their system than ours (although it would be surprising if there was a *function* that the clocks and guards couldn't express). Purely subjectively, I think that our types are easier to read, but then I would say that. One thing about guarded recursion that we didn't push very hard in the abstract is that, if you don't quantify over the clocks you get the initial-final coincidence like in Haskell/domain theory. I think this is really cool, even though type theory people seem to think that domain theory is a bit icky. I know that Rasmus Møgelberg and Lars Birkedal have been looking at using guarded recursion to do synthetic domain theory.
&gt; But every time I try it out, I get depressed with the amount of boilerplate code that I have to type in order to get to the stuff I care about. &gt; &gt; And it may not even be a big deal when you are comfortable with the syntax and emacs mode. But, as a novice, I'd rather learn and experiment with the things that are interesting to me, not some standard theorems about naturals and lists. These are very understandable feelings and the dependent types community has a long way to go before we have a programming language that I can reasonably use to write the applications that I write now in Haskell. What I try to show is how useful these things are, and I guess you have a point in saying that proving 'standard theorems' about lists might not be the best way since the 'practical' programmer does not relate to much to them. But behind the expressiveness needed to prove things like in the blog posts there is the potential for a lot of safe and expressive practical programming, even if not obvious. I'll try to think of other examples :).
Thank you! It seems, that I haven't used ScopedTypeVariables correctly :) The use of Enum seems to very fitting. I don't like that class in particular though, because many instances have a partial 'succ' definition, for example Bool.
&gt; I think `∀ x y → Either (x ≤ y) (y ≤ x)` is a weaker condition than decidability, but I might be wrong on this one. I think so too. Suppose I have `x` and `y` and `decide : Decidable _≤_`. Then `decide x y` and `decide y x` together with antisymmetry can show me that `x = y` if indeed that's the case. But `total : ∀ x y → Either (x ≤ y) (y ≤ x)` might just tell me that `x ≤ y` whichever way round I put them.
At one point you claim: &gt; Every number is less or equal than zero Possibly you mean the other way around?
&gt; Decidable _≤_ = ∀ x y → Either (x ≤ y) (¬ (x ≤ y)) &gt; &gt; Which is different from &gt; &gt; ∀ x y → Either (x ≤ y) (y ≤ x) If you have `Decidable _≈_` or when `_≈_ = _≡_`, then they are equivalent.
Very nice! A couple of nitpicks: * In `sumSqrV`, it's not clear what `U` is. * I think there's a typo in sumSqrPOp. The last line should be `foldM' add 0 vec`, with an apostrophe instead of a backtick. * Very few people would write know to use the approach from Waters's paper for the C version. It would be helpful to also include performance results for "idiomatic C",
Yes, thanks!
Yeah, see my followup to myself.
Based on my work on Kleene Stores, I eventually settled on the Applicative instance using `flip` as the "forward" version and the one with compose was "backwards". This is in part corroborated by the fact that using the compose version with the `Backwards` applicative is not as lazy as using the `flip` version directly. Assuming the Free Applicative is analogous to Kleene Store, then I'd argue that Twan's version is the natural direction.
A detailed comparison of the two approaches (guarded recursion and sized types) is definitely something that needs to be done in future work. I think between our papers we've exhausted the usual collection of standard examples of small corecursive programs, so someone actually needs to get on with the task of writing a server coinductively. Hopefully, it will turn out that copatterns are orthogonal to whether guarded recursion or sized types are used, and everyone will be able to enjoy copatterns.
Given that Eq and Show were removed from the Num superclasses I don't see why this couldn't be done for real (btw, Functor used to be a superclass of Monad, way back when).
You're right. I tested that and it produces the code that has exactly the same efficiency. However not all algorithms that I work on can be modified like that, which means that I probably could have chosen better example for my post.
Certainly. I thought about mentioning invariance as well, but dropped it for simplicity
OK
I maybe miscomprehending something but shouldn't this line have an extra y: insert y (cons x xs (l≤x , x≤u)) (l≤y , y≤u) | right y&gt;x = cons x (insert x xs (x≤̂y (reflexive refl) , x≤u)) (l≤x , x≤u) shouldn't (insert x xs (x≤̂y (reflexive refl) , x≤u)) be (insert y xs (x≤̂y (reflexive refl) , x≤u)) ??? 
Categorically speaking, functors don't curry per se. In category theory it's important to bear in mind the distinction between morphisms and objects. That is, in Haskell etc, when we write "`-&gt;`" there are a number of different things that could mean; in category theory we must be explicit about which meaning we intend. In particular, there are at least three major notions of "`-&gt;`": * As a metatheoretic mapping * As a hom-set * As an exponential object Just because we can metatheoretically map from one collection to another, doesn't mean we have some theory-internal way of defining that mapping. That is: not all functions can be written in Haskell (or whatever theory of choice). Metatheoretic mappings are akin to the horizontal bar in inference rules. Hom-sets are also a metatheoretical notion, namely the class of morphisms between two objects. When we say `f :: A -&gt; B` in Haskell, we generally mean `f \in hom(A,B)`. Hom-sets are, to some extent, akin to the turnstiles in inference rules. Exponential objects are something that only certain categories have, and they give us a way of internalizing the metatheoretic class `hom(A,B)` into the theoretic object `B^A`. Whenever we have higher-order functions in Haskell, e.g. `g :: (A -&gt; B) -&gt; C` or `h :: A -&gt; (B -&gt; C)`, what we really mean is `g \in hom(B^A,C)` or `h \in hom(A,C^B)`. Exponential objects are akin to the actual "`-&gt;`" that shows up in the type-level expression in inference rules. Notably, while standard lambda calculi allow us to internalize the turnstile as an arrow via the lambda introduction rule, not all languages or logics would allow you to do this. While the idea behind currying could be applied to a category of categories (provided it has the necessary structure), it's not something that just comes for free. As far as the functorality of "`-&gt;`" goes: from the fact that `(-&gt;)` is a profunctor, it follows that `(A-&gt;)` is a covariant functor and that `(-&gt;B)` is a contravariant functor, for any choice of `A` and `B`. These two families arise from the slice categories (aka, comma categories) and are known as the co-/contravariant hom-functors. Every category has these functor families, even if the category does not support exponential objects.
The data parallelism paper he refers to: http://www.cs.cmu.edu/~blelloch/papers/Ble90.pdf
If a C programmer reads this, he might object due to the fact you've used the FFI to call C in Haskell, the performance comparison may not be fair. Maybe add a short sentence on how the FFI works, i.e. that it's not "absorbing" the C, but does an actual call out of Haskell?
There was a german guy, who used haskell to build a "functional drawing language". I forgot the thesis name and which uni it was. But you still can find it in the Internet.
Since all functor instances really are provably unique, then we have no need for a deriving clause, since no one will ever wish to replace such an instance, in fact they never can(except in the case that they would want to use some unsafePerformWriteDebugInfo ?) I think we could get away with disallowing such a flexibility in this case.
Here is a nice german BC thesis in haskell ;) http://felsin9.de/nnis/ghc-vis/ Not suggesting you do something so hard, guys a freekin genius... And I wrote a good part of the code for my own(yet to be completed thesis) in a DSL for haskell: https://github.com/timthelion/anonGraph
&gt;On unix-like systems, you do this by editing ~/.bashrc I realize it isn't a unix shell tutorial, but I suspect you could be concise while still being correct.
Oh damn, you are very right, I was refactoring and I screwed up! This confirms that we definitely should check for permutations! Thanks for spotting that.
Why is it no longer a superclass then?
I don't think it matters that `sumSqrPOp` is not idiomatic Haskell. As you point out, `sumSqrV` *is* and you can convince observers that they are the same by using Quickcheck. It's like a by-hand rewrite rule!
This is similar to Edwin Brady's Idris paper [posted the other day](http://www.reddit.com/r/haskell/comments/1b64xh/programming_and_reasoning_with_algebraic_effects/), except without the dependent types, and with a Haskell implementation. There is a comparison with the Pipes library too. There is a github repository with the code from the paper too: [effect-handlers](https://github.com/slindley/effect-handlers). 
That C code looks a bit questionable to me. The branch to check if the int is odd could be replaced by a multiplication with the result of element &amp; 1. Having a branch in the loop stops automatic vectorization, so this could be a lot faster. Also I am not sure if the autovectorizer works if you write it using a bunch of gotos instead of a traditional loop. for(int i = 0; i &lt; xn; i++) { result += xs[i] * xs[i] * (xs[i] &amp; 1); } This also has the advantage of not looking like a garbled mess. Edit: I checked and gcc 4.7.2 produced fully vectorized AVX code (~~8 operations~~ 4 operations per instruction) from the above code. Edit 2: I got consistent results of +- 2.37 seconds for the new code, vs. +- 3.55 seconds for the code in the article. Testcase was the first 2 billion positive integers. These timings include the time to initialize the memory, i'll test again with more precise timing. Edit 3: New timings (only calculation, excluding setup time): +- 0.50 seconds new code, +- 1.67 seconds code from article. ~~The reason for the less than 8x speedup from AVX is the reduction sum~~ This would be even faster on a CPU with AVX2 (Haswell) because those have ~~horizontal integer additions~~ 256-bit wide integer vector instructions (AVX only has that for floating point instructions).
Nice! Monads aren't the last word when it comes to managing effects. Letting alternatives fall through the cracks and become forgotten would be a crying shame.
I don't want to seem overly negative as I love bloggers and the work they do to teach the world but I can't read or understand the sumSqrPOp version. It looks like a kid using instagram with all the #s. The fact that it still isn't as fast as the C code just makes me give up reading it. By the way isn't it true that because you've added a fold means that this code wouldn't scale with larger data due to the fact that fold can't easily be split up and run concurrently.
Related: [spoon](http://hackage.haskell.org/package/spoon).
Thanks everyone for the comments. I'm looking in combinatorial functors by Crossly and Nerode. And then my brain melted. They bump it up to product categories, so that they can " reduce the study of functors of several variables to functors of one variable." Then they index co-ordinatewise, on the product category. Don't know how hom-sets fit in. Just asked on stack overflow if |hask| is locally small, and a presentation shows the category hask, is likely a version of cpo. Wonder how hom-sets work for cpo, is it an enriched category, or something. I'm just reading off of http://ncatlab.org/nlab/show/hom-set This takes me a while, and the reason I don't fully address parts of comments is because I don't understand them yet. -- edit I'm also beginning to suspect that it's tarskian turtles all the way down, everywhere. 
Thanks for your links, I will check them out and look if I can find something useful or inspiring. :)
That's quite orthogonal to the unnecessary unboxing and strictness annotations in the code. let add a x = do let odd = v .&amp;. 1 return $ a + (odd * v * v) will be just as fast (or will be after changing Bits Int to use the new primop, if it's a big win here).
I think this trend of posting highly optimized, unreable to anyone who isn't a haskell expert, haskell code to demonstrate that it is still slower than C is doing the language a great disservice.
It isn't clear to me what the win is in transforming \_|\_ to `Nothing` as it can cause you to lose information about what happened (e.g., if an exception was thrown). Furthermore, in general it seems to me that if something is evaluating to \_|\_ then the underlying problem is not something that you'd want to fix at runtime anyway *unless* it is an exception that you were expecting. For example, when you use the `head` function you are implicitly asserting that the list *must* be non-empty, and if this assertion fails then there is a bug in you code that shouldn't be swept under `Nothing`. Having said that, it is often better to instead use a total function and map the unexpected results to an exception so that if something goes wrong you have an easier time tracking the place where the bug triggered a problem (as when `head` fails it doesn't give you any information about which call failed).
Nifty, but most of the time I would much rather have a library that maps \_|\_ to something like an exception giving a source file and line number rather a library that erases information about what happened when an error occurs.
That may be a feature, not a bug.
Also you named the function `sumSqrPOp` but you refer to it as `sumSqrOp` when showing the core output.
When I try running the first code block, I get this error: test.hs:8:26: No instance for (Stream s0 m0 Char) arising from a use of `digit' Possible fix: add an instance declaration for (Stream s0 m0 Char) In the first argument of `many1', namely `digit' In the expression: many1 digit In an equation for `number': number = many1 digit Failed, modules loaded: none. What does this mean?
I love fully baked papers! They not only implement their idea but provide reproducible benchmarks, too, in a Github repository no less! I could only find one flaw with the paper, which was the performance comparison to `pipes`. They compare to the old unidirectional implementation in `Control.Pipe`, which doesn't have any of the rewrite rules that make `pipes` fast. I [forked their implementation](https://github.com/Gabriel439/effect-handlers) and only made two changes (here's the [diff](https://github.com/Gabriel439/effect-handlers/commit/7f0679c34dfecc692ef8e22bdd9c6dc9a0b614d9)): * import `Control.Proxy.Pipe` instead of `Control.Pipe` * fixed the type signatures just to add the extra `ProxyFast` type parameter ... and `pipes` is now faster on all benchmarks, being at least 50% faster when benchmarking the monad and at least 3x faster when benchmarking composition. I've hpasted the [raw benchmark data here](http://hpaste.org/85123). I had to stop on the nested composition benchmark at 2^12 pipe because I didn't have enough time to run the full benchmark. However, I really love their implementation of effect handlers, which is one of the best ones I've ever seen and I'm sure that if they spent as much time optimizing it as I have spent on `pipes` they could close the performance gap. It's amazing that they come as close as they do with no special considerations. Edit: Fixed the hpaste.
I wonder how far you could get trying to do algebraic effects using promoted data types, which is at least in the direction of dependent typing.
This is neat! I wish it had a text format for messages too, so you could have readable "source" formats that get compiled to binary messages. This would be useful for using this as configuration/data formats, where you have a process for compiling the data into the efficient message format for shipping to customers, but you can use text editors for manipulating the data at authoring time.
the cpp code is generated with haskell, but is it cpp code thats easy to ffi out to from haskell? (as the cpp ffi seems a lot more fiddly to use correctly than the c ffi)
There is a difference between int and long int when interfacing with Haskell, at least on my machine. Long int turns out to have the same precision as Haskell's Int, while int is twice smaller.
I wish I could upvote this comment more than once. :&lt;
The win is that foo :: Int -&gt; Maybe String foo 0 = "zero" foo 1 = "unity" foo x | 100 &lt; x &amp;&amp; x &lt; 200 = "bus number " ++ show x is simpler than foo :: Int -&gt; Maybe String foo 0 = Just "zero" foo 1 = Just "unity" foo x | 100 &lt; x &amp;&amp; x &lt; 200 = Just $ "bus number " ++ show x foo _ = Nothing I think singpolyma was slightly sloppy when posing their question: if I understand rightly, they do not want to blindly replace ⊥ with `Nothing`; they want to replace *pattern match failure* with `Nothing` (and not just any pattern match failure, just failure to match at the top level: `foo` in my example, but not in a `case` expression that might appear in one of the equations). And `head []`, `error "Oops"` etc would continue to produce ⊥. It might be best to consider this an extension to `case` expressions: foo :: Int -&gt; Maybe String foo x = maybe_case x of 0 -&gt; "zero" 1 -&gt; "unity" _ | 100 &lt; x &amp;&amp; x &lt; 200 -&gt; "bus number " ++ show x 
Okay, that makes a lot more sense. Thanks! :-)
You have just shown exactly why such blog posts can be quite misleading. The right way to do this is to put two experts together, and let them do their job then compare the results. C wins for performance every time because it is extremely important to write code for the architecture, and you need absolute control to do that.
It looks like you are correct about the size of `long int`, don't know why I thought otherwise. This sort of thing is why I usually only use `int32_t` and friends. I changed everything to `long int` in both versions and now they both hit the memory bandwidth limit (because the old code is now also processing in bigger chunks), so it's not possible to say which one is fastest. Interestingly, the AVX code for this new version was actually slower than when I disabled the autovectorizer, possibly because the compiler assumed the memory bandwidth limit wouldn't be hit.
I am sure it is possible for Haskell to be equally fast as C, I just wanted to demonstrate that C code doesn't have to be complicated and ugly to be fast.
You are also using `int` in your original C code, because `element` and `result` both have type `int`, only the input is `long int`.
&gt; I'm not going to write a library providing imap :P No need: [Edward Kmett has already done it](http://hackage.haskell.org/packages/archive/lens/latest/doc/html/Control-Lens-Indexed.html#v:imap). &gt; succ should be of type `Enum a =&gt; a -&gt; Maybe a` What do you think `maxBound + 1` should be? Should `(+)` be `Num a =&gt; a -&gt; a -&gt; Maybe a` too? There are many problems with the numeric hierarchy in Haskell's standard libraries, sadly.
FYI: [partiality is a side-effect](http://www.cs.ox.ac.uk/ralf.hinze/WG2.8/22/slides/tarmo.pdf). We can [reason about such side-effects with monads](http://www.cse.chalmers.se/~nad/publications/danielsson-semantics-partiality-monad.pdf). [Monads are also applicative](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#t:WrappedMonad). Have you come across [Conor's idiom/applicative brackets](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/idiom.html)? There seem to be a few [alternative implementations](http://new-hackage.haskell.org/package/applicative-quoters) of it flying about.
Holy crap, I didn't notice that :/ But I just checked and it doesn't affect neither correctness nor the performance of the solution.
&gt; if you want to compare performance, you have to at least compare the 2 fastest versions, ugly or not I’m unsure. Consider a hypothetical comparison between languages A and B. Extreme difficulty deters literally everyone from writing high-performance A code *except* in benchmarks, and all other A code runs an order of magnitude more slowly than commonly-written B code. It’s not clear to me that a fair comparison should use the high-performance A code in that case. Writing high-performance Haskell is by no means as arduous as all that, but it nevertheless seems important to consider the costs involved in eking out performance (and they seem rather higher for Haskell).
I recently started a new screencast targeted at beginner Haskellers. Feedback is very much appreciated and I hope this screencast will help people get started with Haskell. If you spot any errors or have suggestions please post them, thank you very much!
 foo 0 = Just "zero" foo 1 = Just "one" foo x = ["bus number " ++ show x | 100 &lt; x &amp;&amp; x &lt; 200] 
Not from the looks of it. Though even if it did, would you want to write foreign imports for every field accessor by hand? And pay the ffi overhead for each call?
 foo x = ["0"| x == 0] &lt;|&gt; ["1"| x == 1] &lt;|&gt; ["bus number " ++ show x | x &gt;= 100, x &lt; 200] :: Maybe String
The "GHC pipes library", eh? ;)
Yeah, I thought that was funny, especially considering they use many more GHC extensions than I do. I had to install ghc-7.6 for the first time yesterday just to run the benchmark. I also realized that I didn't upload the hpaste correctly and accidentally linked to the home page. I will fix that tonight since my home computer does not have an SSH server installed.
That Fig. 8 is pretty funny =).
Great work, liking the videos so far. I would like to see one where you walkthrough your emacs haskell setup
I'm of the opposite opinion for the reason I mentioned here: http://www.reddit.com/r/haskell/comments/1bikvs/yet_another_lambda_blog_haskell_as_fast_as_c_a/c976u1k
I kind of feel that it doesn't matter if `sumSqrPOp` is unreadable because you can use QuickCheck (or whatever) to convince yourself that it is the same as `sumSqrV` which *is* readable.
[`pure`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#v:pure) and [`empty`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Applicative.html#v:empty).
&gt; Do they? I've seen event bubbling happen both ways, ie. bottom-up and top-down. Bottom-up bubbling seems consistent with FRP. Unless you're referring to something else here? Well, even if event bubbling is bottom up a widget still needs to know about its parent to know where it is positioned on the screen so that it can filter the mouse events from the right rectangle. So even with events bubbling up, `b = new Button(); b.clicks` doesn't make sense in a purely functional API, because `b.clicks` has to change depending on how `b` will eventually be used. &gt; but perhaps you can explain why the Haskell implementations of FRP, like Reactive, reactive-banana, Fran, etc. do not satisfy what you consider "pure FRP". These FRP systems themselves are pure FRP. Also the examples of simple graphical applications, like a rotating rectangle or a pong game, are done with pure FRP. But once you go into GUI development, they step out of pure FRP. Reactive I don't know, I haven't seen a GUI toolkit on top of it. Do you have a link? The FRP part of reactive-banana is pure FRP, though because its GUI part is a thin wrapper around the underlying imperative GUI toolkit it's GUI part is not pure FRP. There is one example provided of a dynamic GUI: https://github.com/HeinrichApfelmus/reactive-banana/blob/master/reactive-banana-wx/src/BarTab.hs You can see a screenshot on this page under the heading BarTab.hs: http://www.haskell.org/haskellwiki/Reactive-banana/Examples I find this code very hard to understand, but everything does suggest that the dynamic GUIs are achieved by imperatively mutating the widget tree. This is the code for adding a new textbox to the GUI: newEntry = do wentry &lt;- liftIO $ entry f [] bentry &lt;- trimB =&lt;&lt; behaviorText wentry "" return (wentry, bentry) So `entry f []` imperatively adds a textbox to parent widget f, and returns that widget. If I'm not mistaken this is the code for deleting one of the textboxes: reactimate $ ((\w -&gt; set w [ visible := False]) . fst . last) &lt;$&gt; bEntries &lt;@ eDoRemove So it's not actually deleting the textbox from the GUI, it's just (imperatively) setting the visibility to false. FranTk is certainly imperative too. I think it's a great API that allows you to create compound widgets and build GUIs in a nice way, but the key functionality why it is able to achieve this is because you can imperatively hook up event streams to event listeners (with addEventListener and wires). So while the FRP parts of these libraries may be pure FRP, the way you build GUIs isn't. This is in contrast to the simple examples that the papers on FRP have, like a pong game or an animation. Those are truly pure FRP, because they take an event stream of OS events (keyboard, mouse, etc.) and define the visual output as a function of that. FRP is supposed to be great for composition, so it's unfortunate that nobody has so far been able to extend the same approach to compositionally building GUIs. You'd expect that the widget tree would be defined as a time varying tree of widgets. But somehow, in the FRP GUI frameworks, the widget tree is mutated imperatively. First I thought that this was just an artifact or wrapping an imperative GUI toolkit, but now I think it's not an accident that there has not been a pure FRP interface to a GUI library. It's an unsolved problem how to do pure FRP compositional GUIs, as far as I can see. If you do embrace mutation you can get a really nice GUI toolkit interface though. But you lose the nice properties of pure FRP.
&gt; Isn't there a standard library or something? [Yes](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=Libraries.StandardLibrary) (and I like it), but some people object to it on Unicode grounds.
Thanks a lot! I might make a somewhat more casual video about my emacs-haskell setup, but for now im just making more episodes.
Thanks I really appreciate it :) I'm writing scripts for the next episodes right now.
I guess eclipse is great at everything but being an editor :P
Interesting take on this problem. I note that the type of `foldNM`, and thus the underlying structure of their main innovation, is basically equivalent to `runPromptC` (with the addition of a class constraint--a feature not available at the time `Prompt` was designed): foldNM :: (a -&gt; r) -&gt; (forall x. c x =&gt; t x -&gt; (x -&gt; r) -&gt; r) -&gt; NM c t a -&gt; r runPromptC :: (a -&gt; r) -&gt; (forall x. p x -&gt; (x -&gt; r) -&gt; r) -&gt; Prompt p a -&gt; r And you could always encode the `c x` constraint into the underlying `p` data type, so they are equivalent. So it's Free Monads With Codensity all over again, I guess? EDIT: equivalency proof: {-# LANGUAGE RankNTypes, ExistentialQuantification, ConstraintKinds #-} data ConstrainedPrompt c p a = c a =&gt; MkCPrompt (p a) type NM c t = Prompt (ConstrainedPrompt c t) foldNM :: (a -&gt; r) -&gt; (forall x. c x =&gt; t x -&gt; (x -&gt; r) -&gt; r) -&gt; NM c t a -&gt; r foldNM ret prm = runPromptC ret (\(MkCPrompt p) -&gt; prm p) The opposite direction is trivial, just use the null constraint.
Yeah, I noticed that I didn't even upload it correctly, so I can't even fix the link until I get back home and upload the results correctly since my desktop computer doesn't have an SSH server installed. I will let you know when I fix it.
That doesn't work, but this would: import Control.Applicative select :: Alternative f =&gt; [(Bool, a)] -&gt; f a select [] = empty select ((True, a):_) = pure a select (_:xs) = select xs foo :: Int -&gt; Maybe String foo x = select [(x == 0 , "0"), (x == 1 , "1"), (x &gt;= 100 &amp;&amp; x &lt; 200, "bus number " ++ show x)] 
&gt; Well, even if event bubbling is bottom up a widget still needs to know about its parent to know where it is positioned on the screen so that it can filter the mouse events from the right rectangle. So even with events bubbling up, b = new Button(); b.clicks doesn't make sense in a purely functional API, because b.clicks has to change depending on how b will eventually be used. Ok, I think I finally understand where you're coming from. By "pure FRP", you mean literally an FRP expression built using only pure primitives. You're assuming a container-flow based layout like HTML, so you're thinking you need some mutation to hook up events. We can do a purely functional Bling-like UI pretty easily: interface ICanvas { React&lt;Point&gt; MousePosition {get;} React&lt;Click&gt; Clicks {get;} React&lt;int&gt; Width {get;} React&lt;int&gt; Height {get;} } struct Click { public int X; public int Y; } class Button { public Button(ICanvas canvas) { Clicks = canvas.Clicks.Where(z =&gt; X.Value &lt;= z.X &amp;&amp; z.X &lt; X.Value + Width.Value &amp;&amp; Y.Value &lt;= z.Y &amp;&amp; z.Y &lt; Y.Value + Height.Value); } public React&lt;Clicks&gt; Clicks { get; } public React&lt;int&gt; X { get; } public React&lt;int&gt; Y { get; } public React&lt;int&gt; Width { get; } public React&lt;int&gt; Height { get; } } Not much use for containers here though, except perhaps for binding lists and tables of controls. If I wanted a purely functional container/flow-based FRP UI I'd start with the above and simply delay evaluation with first-class functions: struct Click { public int X; public int Y; } abstract class UI { } class Container : UI, ICanvas { public Container(ICanvas canvas, Func&lt;Container, React&lt;IEnumerable&lt;UI&gt;&gt;&gt; bind) { // initialize what you need to Children = bind(this); } public React&lt;Point&gt; MousePosition { get; } public React&lt;Click&gt; Clicks { get; } public React&lt;IEnumerable&lt;UI&gt;&gt; Children { get; } } class Button : UI { public Button(Container parent) { Clicks = parent.Clicks.Where(z =&gt; X.Value &lt;= z.X &amp;&amp; z.X &lt; X.Value + Width.Value &amp;&amp; Y.Value &lt;= z.Y &amp;&amp; z.Y &lt; Y.Value + Height.Value); } public React&lt;Clicks&gt; Clicks { get; } public React&lt;int&gt; X { get; } public React&lt;int&gt; Y { get; } public React&lt;int&gt; Width { get; } public React&lt;int&gt; Height { get; } } Creating static UI expressions is then something like: return new Container(canvas, x =&gt; new UI[] { new TextBox(x), new Button(x), ... }); Dynamic expressions would be something like: return new Container(canvas, x =&gt; x.Clicks.Select(z =&gt; z.X &gt; 1000 ? new UI[] { new Button(x) } : new UI[] { new Label(x) })); That's one simple solution anyway. No mutation needed, though mutation is more efficient than creating all those lambdas.
Perhaps skim the list of proposed GSoC projects: http://hackage.haskell.org/trac/summer-of-code/report/1
Exactly! That's what I mean, and this is how you ideally would be able to build a UI toolkit on top of FRP. The difficulty with doing this arises when you consider stateful widgets, in dynamically changing UIs. How would you subscribe to the button's click stream from outside the lambda? (for example in a UI where clicking the button has to affect something outside of that container) What if you wanted to have a textbox instead of a button, and you want to maintain the state of the textbox even if you change to a label and then back to the textbox? Existing systems get around this problem by secretly or not so secretly using mutation. Then you can do this: b = new Button(); ... do something with b.Clicks ... return new Container(canvas, x.Clicks.Select(z =&gt; z.X &gt; 1000 ? new UI[] { b } : new UI[] { new Label() })); Same for a textbox. When you want a textbox that maintains state when switching back and forth to the label: t = new TextBox(); return new Container(canvas, x.Clicks.Select(z =&gt; z.X &gt; 1000 ? new UI[] { t } : new UI[] { new Label() })); When you want a new textbox every time: return new Container(canvas, x.Clicks.Select(z =&gt; z.X &gt; 1000 ? new UI[] { new TextBox() } : new UI[] { new Label() })); One solution may be to at each point return a pair of (widget, data) so that you can get data out of each container, but this gets very hairy very quick with circular data flows and packing and unpacking of data.
&gt; How would you subscribe to the button's click stream from outside the lambda? (for example in a UI where clicking the button has to affect something outside of that container) Button foo; var c = new Container(canvas, x =&gt; new UI[] { new TextBox(x), foo = new Button(x), ... }); foo.Clicks.Select(z =&gt; ...) The limitation here is that it's a collection of UI elements, so it's hard to assign usable names to the controls. &gt; What if you wanted to have a textbox instead of a button, and you want to maintain the state of the textbox even if you change to a label and then back to the textbox? Part of the appeal of FRP is that you have declare all your state upfront. In this case, I would simply toggle the visibility on the textbox and labels, ie. Label label; ... label = new Label(x) { Visible = somecondition }, new TextBox(x) { Visible = label.Visible.Select(z =&gt; !z), ... Or maintain the TextBox as a captured variable and just reuse it when needed as you demonstrate. Easy to use mutation in these cases, so I see your point, but I'm sure lazy evaluation can be extended to handle this as well. Just too complicated in C#.
Well, to get the button out of the lambda you used mutation ;-). Even lazy evaluation cannot do that. You can of course return what you want to get out of the lambda, and let the Container() call return that as well, but that's one instance of what I mean by it gets hairy ;-) &gt; In this case, I would simply toggle the visibility on the textbox and labels, ie. Right, though in general the problem is not so easy. It's not just a problem of hiding and then unhiding. What type signature would you suggest for new TextBox(...)? How does a textbox maintain its text content and a cursor position? How would you do a button and a variable length list of textboxes, every time you click the button a new textbox gets added. The cursor position in the textbox that's currently in focus should be maintained when a textbox gets added. You can use Haskell syntax &amp; lazy evaluation too.
Oops! Thanks for the feedback. We'll make sure that the final paper includes figures for the pipes benchmarks with the rewrite rules enabled.
That's good advice, thank you. I will definitely check it out. 
It's [straightforward](https://github.com/slindley/effect-handlers/blob/master/ix/HandlerIx.hs) to adapt the core of our effect-handlers library to use Conor McBride's notion of a monad over an indexed set (monadix :-)) as described in [Kleisli arrows of outrageous fortune](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf). What I'm not so sure about is what exactly is the right way to extend this to support forwarding of unhandled effects.
It does work: http://ideone.com/1acYMl
It does work if you use `MonadComprehensions`.
&gt; The OP's OList definition requires a little more comparison in the cons case, demanding that l &lt;= x &lt;= u, as well as taking xs as an OList from x to u. The corresponding slide "we are very powerful - take two" from my talk requires only l &lt;= x, and that's quite deliberate on my part. Oh, when writing OList for some reason I complicated it like that---I amended it now to follow the presentation more closely, which makes things much nicer!
I don't think this addresses my problem with this trend. So what if once you have this hand optimized unreadable code you can check that it does what it is supposed to. You still have to write it. This is not a good slogan, "Write code that is harder than C and still have it run slower than C!" The tenants that I think attractive people to Haskell is that it is supposedly writes shorter programs that are more likely to be correct and easier to reason about. This example demonstrates none of that.
I wish the code samples would use foo a = do b &lt;- bar a return (b + 1) or something along those lines instead of the semicolon/braces syntax which I think might scare or 'look ugly' to some newcomers.
Doesn't it? For transitive orderings, it makes not the slightest logical difference. And yet the practical difference is really quite noticeable. One good definition is worth a cartload of theorems!
I think Simon wanted the code to look more familiar to imperative programmers. It only "looks ugly" to Haskell programmers.
Alright, I fixed the [hpaste](http://hpaste.org/85123).
Braces and semicolons are fine, but why not put semicolons on the end of the line, instead of the beginning of the next line? :P
I've been trying out BNFC but have a problem: I want a data structure like this: data BinaryOp = Add | Sub | Mul | Div | And | Or | GT | LT | LE | GE | EQ data UnaryOp = Neg | Not data Exp = Literal Value | Unary UnaryOp Exp | Binary BinaryOp Exp Exp | If Exp Exp Exp | Variable String | Let String Exp Exp I can't figure out how to get BNFC to make this work, while at the same time properly handling precedence and associativity. Any ideas?
Links seems to be broken
Agreed. The plan is to support JSON as the preferred text format.
http://www.reddit.com/r/programming/comments/1bm16t/simon_peyton_jones_explains_how_to_deal_with/
Or extend the [protobuf](https://github.com/alphaHeavy/protobuf) library to support more encoding formats and compiler plugins ;-). I have some simple examples up here: http://breaks.for.alienz.org/blog/2013/02/28/generics-and-protocol-buffers-the-hackage-years/... and I'll be giving a talk about some of the ideas in the library in a few weeks at the [Hacker Dojo](http://www.meetup.com/haskellhackersathackerdojo/events/106624712/). Feel free to stop by if you're interested. I'd recommend porting your ideas to Haskell, not necessarily the design. Though it is the world's ﬁnest imperative programming language some concepts have a different natural representations in Haskell than C++. I find it easier to make a well designed Haskell library fast enough than to introduce good design to a fast library. It's definitely a good first project, welcome to Haskell.
Is it available for download from somewhere?
It's on my github profile, but it's still a work in progress.
Sorry, I should have known better, that most likely I'm just one GHC extension away.
Overall a great article but one criticism nagging in my mind is the unfortunate confusion in the introduction between concurrency and parallelism. IMHO it would be worth adding a paragraph clarifying that Haskell can do deterministic parallelism without locks/STM and that concurrency is useful without parallelism (e.g. for servers). 
I'm currently working on an embedded DSL in Haskell for my undergraduate dissertation, and I've found it to be a very quick way to get something that looks really impressive. :P My project is about hardware description, but there are plenty of other topics that might benefit from the embedded DSL approach. It might be a nice way to turn something else you're interested in into a Haskell project.
more discussion happening over at http://www.reddit.com/r/programming/comments/1bneoe/dependent_types_a_new_paradigm/
I tried mixing a new non-terminal into the coercions tower for Exp, but seems this is not allowed (coercion rules are named _., and only work between the same non-terminal it seems). You might try the bnfc mailing list though.
The abstract: &gt; Applicative functors ([9]) are a generalisation of monads. Both allow expressing effectful computations into an otherwise pure language, like Haskell ([8]). Applicative functors are to be preferred to monads when the structure of a computation is ﬁxed a priori. That makes it possible to perform certain kinds of static analysis on applicative values. We deﬁne a notion offree applicative functor, prove that it satisﬁes the appropriate laws, and that the construction is left adjoint to a suitable forgetful functor. We show how free applicative functors can be used to implement embedded DSLs which can be statically analysed. After reading the abstract I've no idea what the contribution is. You define something (new) and prove the appropriate laws. Doesn't sounds very useful an sich. What is a free functor and what benefits does it have? My `Applicative`s don't have to be free in order to make statically analysable DSLs, right?
Simon's use of brace and semicolons is different from C/C++/Java use so I'm not sure it helps. Also, lots of imperative programmers come from whitespac sensitive languages nowadays (I.e. Python and Ruby).
Yes, exactly. This is the same encoding we use to be compatible with the Operational library (the "Box" type in Section 4.3.1) , which similarly doesn't have an explicit constraint. I wasn't aware of MonadPrompt so thanks for pointing it out; I'll mention it in the next revision of the paper. In practice, I expect people will use existing libraries such as Operational or MonadPrompt when using this technique. We presented it using a GADT with an explicit constraint parameter in the paper as it's the most direct way to explain the idea.
Partial functions are very well-understood. Haskell just happens to be a very poor arena for annotating totality.... The *proper* place to keep partial functions is the Partial monad, not the Maybe monad: data Maybe a = Nothing | Just a codata Partial a = Later (Partial a) | Now a The "codata" declaration defines a coalgebraic data type. To constrast, algebraic data types must be finite. (You can't have "infinite lists" if List is an algebraic data type). Codata types, however, can be infinite. The distinction between data and codata doesn't exist in Haskell. It only arises when a language is total. If a function would not be defined for a value, you instead return the (infinite) value: never :: Partial a never = Later never You might wonder, "why don't we just use Nothing" and ignore Partial entirely? The answer is because you have to *decide* when to return Nothing. For many problems, you can decide when to return Nothing (and it's much preferred). However, many problems are undecidable. You can't know you're *ever* going to return a value until after you already have. So what you do is wrap your return values in Later after Later after Later. If you never end up returning a value, you end up with "never" above. But if you do end up returning a value, you can "crush" all the layers of Later (because it's a monad) to just one layer. You then end up with convenience functions: step :: Partial a -&gt; Int -&gt; Maybe a step (Now x) = Just x step _ 0 = Nothing step (Later l) n = step l (n-1) run :: Partial a -&gt; IO a run (Now x) = return x run (Later l) = run l (run works because, in a total language, IO *must* be defined as a coalgebraic type, and is "lazy" just like Partial is). Just just to reiterate, none of this is very useful in a partial language like Haskell. Check out Idris (which has a total sublanguage). You can do this kind of thing and it's a lot of fun not having to worry about undefined nonsense.
Well, every monad blog starts with defining monads. Isn't there a standard library in haskell or something?
Sure, this sort of thing is interesting. In my original question I was more interested in convenient ways of writing functions that produce `Maybe` than in "avoiding bottom" (because I already have a way to avoid bottom in all of these cases, which is to produce `Maybe`). That said, this material on the `Partial` type is interesting. I'm not sure how you would ever get something (instead of `Nothing`) and the `never` you have written is equivalent to bottom.
Cool, nice to see two articles on free applicatives in one week! (The other is here: http://www.reddit.com/r/haskell/comments/1be2iv/flavours_of_free_applicative_functors/)
I wish the code samples would use foo = plusOne &lt;=&lt; bar where plusOne = return . (+1) but I'm weird like that.
Abstracts don't get much love these days. No one seriously critiques abstracts. When was the last time that a PC review gave it serious thought? The abstract in question is at least succinctly vacuous. [Here's an arguably worse abstract proposing a similarly abstract structure.](http://www.cs.umd.edu/~mwh/papers/hicks12polymonad.html) 
Yeah, it's a funny coincidence :)
There's a difference between explaining what a monad (or list) is, and something that uses monads or lists. A post explaining the continuation monad probably wouldn't define the monad class.
 foo = (return . succ) &lt;=&lt; bar FTFY.
Thanks, though I still prefer use the form foo = succ' &lt;=&lt; bar where succ' = return . succ
Adding syntax in an ad hoc way is almost definitely not the way to go. There is a lot to be learned about syntax extensions for languages. Again, see Idris for some interesting work in that area. The difference between bottom and `never` is that never is actually a value. Forcing it to weak-head normal form does not cause the program to explode (unlike in Haskell, which raises an IO exception). Partial a is also a distinct type from a, so you can't do something silly like `head never`. It forces you to consider the case that your computation might not terminate. And to "get" the data, you either have to peel off n layers (using step) or run it in IO, peeling an infinite number of layers... and in that case, it might cause the IO computation to loop.
&gt; Adding syntax in an ad hoc way is almost definitely not the way to go. Again, I would never suggest this as an actual language feature (I avoid all extensions). I was contemplating how hard a code-generator for this would be.
&gt; A Haskell newbie would not be advised to adopt such a big project until they've mastered the language a bit. Well... their code won't be very well if their new, but I would never discourage someone from building terrible piles of hacks! That's the best way to learn.
protobuf is a fucking slick library, I've sadly been too busy to have time use it properly from some side projects yet. Also a really neat point in the design space for representing these sorts of protocol data structurees
Dunno if it's a coincidence. From what I can see interest has really been swelling.
Ruby is white space sensitive? 
It emphasizes the uniformity, makes it easier to see what spans one vs two lines, where a semicolon might be missing, etc. 
From my [own haphazard efforts](http://hackage.haskell.org/packages/archive/pianola/0.1.0/doc/html/Pianola-Pianola.html) in using Haskell to monitor and control external systems, I gleaned two "patterns": * In interactions with external systems, try to distinguish purely observational actions from those which do actually cause changes. This even if both kinds of actions happen through the IO monad. Use a newtype wrapper if necessary. * You can use a coroutine-like library (like pipes) to dissociate when an action is requested from when it is executed. If you have a "producer" of action values you can add logging, notification or ralentization effects very easily, before composing with a "consumer" that executes the action. 
I generally favor the point-free (&lt;=&lt;) over (=&lt;&lt;), but your example seems overly verbose. I would write this simply as foo = liftM (+1) . bar Ideally this would be fmap rather than liftM. Any time I see a bind and return near each other, I can't help but look for a way for them to cancel each other out. Sometimes the result is more readable. Anyway, for such a simple expression I still prefer your style over do-notation.
The StackOverflow website is probably a better venue for this question.
The lines are not terminated by semicolons, they are separated by them. That is why they do not come at the end of the line. I like to think of the semicolon as a polymorphic associative operation on lines of do-syntax. I agree that they do not offer much of a benefit to those coming from other languages.
Definitely. Also, mek0da, this should probably be at least two questions and each question should have some (possibly not working) code that you've tried.
I'm afraid I've not used Idris. What does it allow that's similar?
Ok, change of mind. Simon actually agreed to switch to braceless format. Check it now.
&gt; The lines are not terminated by semicolons, they are separated by them. In particular, note how when there are three lines, there are 2 semicolons { line one ; line two ; line three }
I'll give you a hall pass this time ;-) I think it would be interesting to try to extend the DSL to support other encodings like Thrift and Cap'n Proto. They share quite similar languages so it should be possible... perhaps a weekend project.
thankee. Been a bit mega nerd sniped getting my past years work core lib ready for alpha released, so other projects are languishing. Huh, thats a neat / nice idea!
i should also add that the pending libs to be released soon are perhaps just as handy for you maybe as the stuff i've got on the back burner
Cool, I'll watch out for em.
Anyway, the point remains. No mandatory curly braces or semi colons.
[Google Calendar link](https://www.google.com/calendar/embed?src=fvijvohm91uifvd9hratehf65k@group.calendar.google.com) for aforementioned.
I might get around to mentioning that eventually…
The little I have seen so far looks great! A few small comments: When you say an acronym, please pronounce the letters just a tiny bit more slowly and distinctly. Don't waste time apologizing for what you are leaving out and explaining why. Just do it. For example, the very most you'd want to do in episode #1 is to say at the very end something like "GHCi has many more features; we'll discuss some of them later." The comment that appeared on the screen in episode #1 about the indentation of the "in" clause was confusing. How about just: &lt;-- any non-zero indentation --&gt; Good luck with this series!
Sure, and this is all wonderful, but without knowing this the abstract doesn't give any motivation for reading the paper. And having this knowledge a priori is very unlikely, because *they* are the ones that 'deﬁne a notion of free applicative functor'.
Thanks a lot for the feedback, it's really helpful! I'll be sure to apply these tips in my next episodes.
I was glancing over ‘[A Semantics for Imprecise Exceptions](http://research.microsoft.com/en-us/um/people/simonpj/papers/imprecise-exn.htm)’ earlier today, and it reminded me of your comment. I think that dealing with `succ maxBound` definitely falls into the category of ‘disaster recovery’ as defined in §2, &gt; succ maxBound :: Bool *** Exception: Prelude.Enum.Bool.succ: bad argument so the above seems to be perfectly reasonable behaviour. If you wanted a `Fin 2` with wraparound `Enum` semantics, I don't see any reason why you couldn't define it yourself…
Nice paper, and thanks for reviewing my `free-functors` package so thoroughly in the related work section! I agree with you that it is harder to use, but your claim that `matchOpt` is not applicative is false. It does take a bit of thought to find the right applicative functor, which is `Product (Const Any) (FreeA Option)`. See https://gist.github.com/sjoerdvisscher/5319039 Why do you say that the `Maybe` `Alternative` instance is not a valid monoid? It does not match its `Monoid` instance, but it still is a proper monoid (equal to the `First a` `Monoid` instance), or am I missing something?
GHC has an experimental "Rebindable Syntax" extension which lets monadic notation be bound to other functions if the Prelude versions aren't in scope. But it doesn't really achieve overloading as such.
&gt; There is some cleverness involved in going from a straightforward definition using pattern matching like the one in the paper, to yours. You could say the same thing if you had to convert my definition to your definition. In your definition you need cleverness in combining the result with the tail `x`, which disappears completely in my version. For the `Alternative` laws, since the class seems to be defined with parsing in mind, I would expect the laws to match those of a semiring (but without commutativity), i.e.: empty &lt;*&gt; a = a &lt;*&gt; empty = empty (f &lt;|&gt; g) &lt;*&gt; a = (f &lt;*&gt; a) &lt;|&gt; (g &lt;*&gt; a) f &lt;*&gt; (a &lt;|&gt; b) = (f &lt;*&gt; a) &lt;|&gt; (f &lt;*&gt; b) It seems that in CT that would be called a [bimonoidal category](http://ncatlab.org/nlab/show/bimonoidal+category)?
&gt; An attraction of this approach is that this type class could replace the existing Monad class in the standard libraries, without breaking any existing code. This is unfortunately just not true. Any code that relies on polymorphic recursion is irredeemably broken by this change. It doesn't break simple code, but it makes harder cases impossible to write.
Assuming this is the app, I assume it is spending all the time in random number generation: https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/yesod/bench/Application.hs Is randomness part of the spec? a serial choice of numbers here will be loads better. Absent that, a more performant generator like mersinne-twister. On the db side i'd check if you're maintaining a nice connection pool. and remember, profile profile profile. also i'd imagine aeson will be faster than yesod's json, plus you can write a datatype and give it a proper ToJson instance. but all that pales in comparsion to random generation, i suspect.
&gt; On the db side i'd check if you're maintaining a nice connection pool. See: &gt; p &lt;- Database.Persist.Store.createPoolConfig (dbconf :: Settings.PersistConfig)
that says you're creating _a_ pool, but not the size of the pool or its properties! here's the settings file: https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/yesod/bench/config/mysql.yml so are they running it with 10 connections, 100, neither? which is the best size for this benchmark? all things to check.
Ah, so here “nice” means “efficient”. Gotcha.
Yes, randomness is part of the spec. It's also only in the DB portion of the tests, the straight JSON portion has no randomness and Yesod and Snap both still retain roughly the same ranks, so I'd be surprised if that were a bottleneck. I'll definitely look into aeson, though.
It is worth making sure decent entries are made here -- really, both snap and yesod should be near the top. 
I looked at the Snap one. I seriously doubt aeson will make a measurable difference for such a tiny piece of data. For what it's worth, the “Hello, World!” test is most probably a no-op because GHC's -O2 moves constant expressions to the top-level so that they will be evaluated once, you'd be just testing how fast Snap can write some bytes onto a socket. HDBC is known for being not fast. [mysql-simple](http://hackage.haskell.org/package/mysql-simple) is faster.
Yeah on the snap code I notice its using the galois json package which was/is pretty good, but aeson is more modern and _scads_ more efficient. the other bottleneck (and profiling is the way to tell) may be in db serialization/unserialization where HDBC isn't great as i recall and I have no idea about Persistent performance but you could move to mysql-simple, which is another bos package written with efficiency first. The thing i hate about these benchmarks is in real world production code that I've done, things like json vs. aeson or even slow vs. fast random generation or db serial/unserialization would almost _never_ be the bottleneck. the faster choices aren't less idiomatic or anything, mind you.
You should profile them and post the profiling results. That will give us a much better clue of what true bottleneck actually is.
For work with pdf files make use how to repair adobe reader pdf files [http://www.adobereaderrepair.acrobatrepairtoolbox.com](http://www.adobereaderrepair.acrobatrepairtoolbox.com) must help you to learn more about features of pdf files 
Yesod has clientsession running, probably. I don't know if it should be part of the benchmark or not, but it does slow down the req/s. Also, a plain wai-over-warp benchmark should be contributed. 
I swear, every time I want to investigate a new package on hackage, edward kmett wrote it...
What are the runtime parameters for the benchmark? At the very least, the resulting executables for both snap and yesod should be run with: +RTS -N -A4M -qg1 -I0 
I seem to be too lazy for blogging, but I just sent a mail to the Haskell Cafe that includes some bullet points: http://www.haskell.org/pipermail/haskell-cafe/2013-April/107428.html Nothing like the serious comparison and analysis you were hoping for though. :$
It was a great class indeed.
thanks, btw there's also /r/functionalprogramming 
The [pseudo code](https://github.com/TechEmpower/FrameworkBenchmarks) in the instructions does not need any clientsession and does not seem to be mentioned anywhere else so it looks like it can be taken out.
What I find most interesting in this paper is the discussion of C++. As the authors say, comparing with C is a "straw man" since no C compiler can perform fusion. But, C++ expression templates outperform the Haskell version presented here. The authors argue that C++ is harder to use &gt; Clearly “properly”-written C++ can outperform Haskell. The challenge is in ﬁguring out what “proper” means. A Haskell programmer can write straightforward, declarative code and expect the compiler to handle fusion. The C++ programmer must worry about the performance implications of abstraction. I am inclined (as a Haskell programmer) to agree with them, but I am not sure that this is true. Getting fusion to work in Haskell does take some knowledge (inline pragmas!), and the expression template based approach is quite popular.
I didn't realize newSTDGen had such a reputation for being slow. I'll definitely switch it to what you linked when I get the chance. Thanks!
Will there be something in the near future called stream fission that I'll have to learn about?
C compilers *can* perform fusion. They just call it loop optimization. They can also do automatic vectorization and parallelization. I find it surprising that this paper did not compare the performance against a simple C baseline.
http://www.cs.ox.ac.uk/jeremy.gibbons/publications/fission.pdf
I knew it!
They do say in the next paragraph: &gt; Furthermore, unlike Eigen, our implementation is immature, and there are several straightforward changes that will further im- prove the absolute performance of Haskell code. There is room for improvement in our use of prefetching. More importantly, we currently cannot rely on memory allocated by GHC to be prop- erly aligned for SSE move-aligned instructions, so all SSE move instructions are unaligned. Differentiating between unaligned and aligned memory will allow us to avoid the large performance hit incurred by our current implementation If it were another set of authors I might dismiss this as an empty promise to attract interest, but not so with these guys.
&gt; I find it surprising that this paper did not compare the performance against a simple C baseline. Did you look at Sec 5.3, in particular Fig. 9? They did exactly that!
Mmmm, reverse engineering possibilities! EDIT: Ahhh, author admits as much: code recomprehension a.k.a. *Software Renovation* on page 15.
This seems like a perfect book for the [School of Haskell](https://fpcomplete.com). You could make all code samples in-place runnable without the need to install Haskell Platform. It could be a real hit.
Loop unrolling/vectorization/parallelization are *not* the same thing as fusion. C compilers can perform primative *loop fusion* but not the kinds of data flow driven fusion that Haskell fusion frameworks rely on or that is enabled via expression templates. Which is to say, you can get fast code in C--but you don't get *reusable* fast code in the same way you can with other techniques.
I think those tests say more about the skills the users have in their languages, than the quality of the compiler (or whatever). I often see people saying “Why is Haskell so slow?”, when really they should ask “Why is my Haskell code so bad?”. (I don’t blame them though. Haskell is hard to learn. My code is bad too. :)
In the intro motivating the problem: wurble :: (Int -&gt; Int) -&gt; Int And yet there is a call wurble (\k -&gt; map (k+) ys) The argument has type `Int -&gt; [Int]`. What gives???
I don't post much here so excuse any faux pas plz... I used apache benchmark to compare just the code from Netty with a slimmed down version of the json serializer: screenshots of the results: https://plus.google.com/photos/101584221360796182765/albums/5863499036194796865 I didn't test the rest but this was satisfying to me, netty does a better job of serializing the data but not anything drastic. I am sure if I didn't suck at haskell it would have been even better. One other thing to note is when I was using one concurrent with one request, netty was way slower. Not sure what that was about. Meanwhile netty code looked icky and the yesod code is so pretty (FACT) ran with: +RTS -A10M -qg2 -N -IO -G2 -s --Here is the code I used {-# LANGUAGE TypeFamilies, QuasiQuotes, MultiParamTypeClasses, TemplateHaskell, OverloadedStrings #-} import Yesod import Data.Text data HelloWorld = HelloWorld mkYesod "HelloWorld" [parseRoutes| / HomeR GET |] instance Yesod HelloWorld getHomeR :: Handler RepJson getHomeR = jsonToRepJson $ object ["message" .= ("Hello, World!" :: Text)] main :: IO () main = warp 3000 HelloWorld 
&gt; that says you're creating a pool, but not the size of the pool or its properties! indeed! and it should be the same for all participant in the test. i think all will supply some default out of the box. some defaults, for some frameworks, perform better on some machines. tricky.
There is a small error in the `dotp` function on page 2 dotp ≡ sum (zipWith (∗) v w) ≡ foldl s 0 (+) (stream (unstream (zipWiths (+) (stream v) (stream w)))) ≡ foldl s 0 (+) (zipWiths (+) (stream v) (stream w)) these should be `zipWiths (*)`, not `zipWiths (+)`.
That is the best part about the benchmark game.
Brent has posted most of these on School of Haskell [here](https://www.fpcomplete.com/school/introduction-to-haskell) with active code, so you can play with it from within the browser. More on the way soon!
I realize the gains of this approach are significant, but I was a little saddened the first time I saw this that the approach is basically a sledgehammer. We haven't found any one optimal representation for streams, so let's just slam all the ones we can think of together.
I only saw a single "universal" representation for streams at a low-level --- namely, the type `Stream`. There were other types mentioned but they were higher-level, being designed to be convenient for the user rather than the compiler.
Options: * use simple-integer: this requires recompiling GHC * statically link GMP: [apparently one can only do this by statically linking everything](http://stackoverflow.com/questions/10539857/statically-link-gmp-to-an-haskell-application-using-ghc-llvm). Is this true? * copy over libgmp.so.10 as part of the install
libc version doesn't match either. Probably I should do what you suggest.
I don't see what they have to do with being convenient for the user. They are all still hidden behind the same abstraction, aren't they?
Happy you liked it :) first examples are a bit rough maybe, but overall I think it could be a nice groundwork for a small cookbook, and I hope to get a lot ot feedback from you all :)
They are convenient because the user can express his or her algorithm in terms of transformations applied to vectors of data, and the low-level work of translating this into operations on `Stream`s for the sake of helping the compiler see how to optimize it is done behind the scenes.
Abstract: Generic programming (GP) is a form of abstraction in programming languages that serves to reduce code duplication by exploiting the regular structure of algebraic datatypes. Over the years, several different approaches to GP in Haskell have surfaced. These approaches are often very similar, but have minor variations that make them particularly well-suited for one particular domain or application. As such, there is a lot of code duplication across GP libraries, which is rather unfortunate, given the original goals of GP. To address this problem, we introduce yet another library for GP in Haskell... from which we can automatically derive representations for the most popular other GP libraries. Our work unifies many approaches to GP, and simplifies the life of both library writers and users. Library writers can define their approach as a conversion from our library, obviating the need for writing meta-programming code for generation of conversions to and from the generic representation. Users of GP, who often struggle to find ``the right approach'' to use, can now mix and match functionality from different libraries with ease, and need not worry about having multiple (potentially inefficient and large) code blocks for generic representations in different approaches. 
Abstract The most widely used generic-programming system in the Haskell community, Scrap Your Boilerplate (SYB), also happens to be one of the slowest. Generic traversals in SYB are about an order of magnitude slower than equivalent handwritten, non-generic traversals. Thus while SYB allows the concise expression of many traversals, its use incurs a significant runtime cost. Existing techniques for optimizing other generic-programming systems are not able to eliminate this overhead. This paper presents an optimization that completely eliminates this cost. The optimization takes advantage of domain-specific knowledge about the structure of SYB and in so doing can optimize SYB-style traversals to be as fast as handwritten, non-generic code. This paper presents both the formal structure of the optimization and the results of benchmarking the optimized SYB code against both unoptimized SYB code and handwritten, non-generic code. In these benchmarks, the optimized SYB code matches the performance of handwritten code even when the unoptimized SYB code is an order of magnitude or more slower.
This is great! I loved Ollies series,mand this is exactly the "haskell is practical for real stuff damnit" advocacy that we need. Can't wait to read the blog posts and code!
For those who do know what all these names mean: contrary to what the title of the post wants you to believe they are not all FWs. Apache's Wicket seems to be the winning FW on the list, and it is what I consider to be the upper-barrier for FWs with this benchmark. Gemini seems to be really fast across the board on the less-then-FW; I hope we can soon put Warp(/WAI) against that. :-P
I've just checked the DB config: PHP is configured to use sockets (localhost), while Yesod is told to use TCP (127.0.0.1). I don't know if it has a big incidence on the results, but it's clearly not fair on Yesod.
BLAS is C, no? Also, calling into BLAS or IMSL or ... is exactly the C way of rolling a number-crunching app. Do you have a link to what you mean by "a simple C baseline"?
Found a typo: "are often **worht** specializing".
I wonder if the same approach can be used to work around the `concatMap` problem that list fusion suffers from.
Yes, but they are making multiple calls to BLAS for a simple vector norm, which is obviously not something anybody would do in practice (they also mention this). A simple C baseline for the problem in the paper: for(int i=0; i&lt;n; i++) s += pow(a[i]-b[i],2);
This is pretty awesome and seems really useful when doing parallel builds with the new cabal.
Please keep up the good work, that may be very important for newbie haskellers like me to be more interested in/contribute to haskell itself :-)
2006 was a fantastic year. Probably the most fun since then.
I made a quick example [here](https://gist.github.com/DarkOtter/5326095). It's the same type problem that tchakkazulu noticed, where infinite type errors happen if you try to modify the answer type in part of the computation. The approach I used here is an indexed monad, although I wrote it in a quick and dirty way rather than using [index-core](http://hackage.haskell.org/package/index-core), for which you can write an equivalent (but neater) delimited continuation indexed monad.
I made this post myself (it is my own blog). The issues in this blog post took some time to solve, and was not obvious at the outset. I hope it can help others new to Haskell.
The new Cabal? Is there anything concrete yet?
you could have made your life easier switching to linux. i made the switch a while ago to arch linux, defintely recommend. then installing / uninstalling / upgrading is easy.
I don't think of these records of streams as being high level. They are just another part of the optimization. The only user-facing part is the vector interface, which hides almost everything about streams. My complaint wasn't about the user-facing part or about the streams but about the rather inelegant globbing of various streams together. I want to reiterate that I don't object to the results or claim that I could improve them, only that I suspect there is a more elegant formulation with the same benefits.
Too much work-related .NET and Office-stuff chains me to Windows. I have a virtualbox on my desktop with PC-BSD, and we use some FreeBSD on the server side, but "the big switch" won't happen anytime soon.
Guess what, `getCurrentTime` also "breaks equational reasoning"! Let's remove that, too.
You're missing the point. You can easily construct a `counterex_ctx` using `getCurrentTime` for which `main = counterex_ctx $ x` and `main = counterex_ctx $ y` produce different output even when `x` and `y` are equationally the same.
I don't get it. His point is that lazy IO is bad because side effects break equational reasoning, but isn't that true for *all* IO? Isn't it, like, the *definition* of IO? Without an example of how other IO models don't do that I don't think this makes sense.
Totally rad. Is there functionality for configuring the size and location of each head? This would be awesome for putting together cl chat apps, text-based adventures, and any other kind of GUI in which you want to see interacting textual data sets.
Give me a concrete example and I will show your error.
That's why we have the IO monad, but lazy IO can 'escape' the IO monad.
It's not true that \ str -&gt; (length . take 2 $ (' ':str)) &gt; 0 is the same as const True All it takes to prove that is to take apply these to undefined. 
I personally consider lazy IO to be a weak form of concurrency, albeit one with a relatively deterministic scheduler in GHC. In my view, what we are seeing here a consequence of non-determinism, which just goes to show that you shouldn't count on the scheduling details of lazy IO, or any other concurrency operative, for the semantic correctness of one's programs. In particular, I'm of the opinion that both results are valid output of both programs because there is a race condition in str1 &lt;- hGetContents h1 str2 &lt;- hGetContents h2 I think that different Haskell runtimes could give different results from GHC and still be considered "correct" here.
Seriously? import Control.Exception import Data.Time.Clock counterex_ctx a = do start &lt;- getCurrentTime evaluate a stop &lt;- getCurrentTime print $ diffUTCTime stop start x = 0 y = length [ (a,b,c) | a &lt;- [3..1000], b &lt;- [3..1000], c &lt;- [3..1000], a^3 + b^3 == c^3 ] 
The function counterex_ctx uses IO. Any function in Haskell that uses IO must from a formal point of view be regarded as doing something random (since the IO has no semantics). So it's not strange that the function can do different things with different arguments. It would be allowed to different things even if the two arguments were equationally equal. That's the power of IO; it can do things that cannot be done in pure code. I've yet to see an example where lazy IO actually breaks equational reasoning. (Assuming the result of all IO actions can be random.)
How? It's true that you can observe the effects lazy IO has when you're in the IO monad, but how do you observe these effects when you're not in the IO monad?
True, but given that we have `\str -&gt; (length . take 2 $ (' ':str)) &gt; 0` ⊑ `const True` (I believe that is the case), then by monotonicity we should have that `f $ \str -&gt; (length . take 2 $ (' ':str)) &gt; 0` ⊑ `f $ const True`. and thus only the strictness of the `IO ()` value should be affected. Unless I'm mistaken.
Why is that so? Isn't localhost translated to relevant IP?
What?
I think it's implicitly assumed that he meant in the absence of `undefined` arguments.
Good point. So currently, the equational properties of lazy IO are undetermined.
No, I'm sorry. What I meant to say was: the current cabal with the relatively recent addition of building packages in parallel.
That's not true if f is in the IO monad. In the IO monad you can observe all kinds of things.
It's trivial, but tedious, to show that y=0 with equational reasoning (it just takes evaluation). (Or you can appeal to Abu-Mahmud Khojandi.)
The referential transparency violation is not the fact that you use `getCurrentTime`, but rather the fact that `ghc` does not precompute all pure expressions at compile time, which it could do in theory if I wave my hand a lot and ignore all sorts of theoretical and practical considerations. :)
I thought that too, but realized that the only example I can come up with where lazy IO breaks equational reasoning is based on bottoms.
And? Are you saying that `State` violates equational reasoning too? Just because you get "different results from different binds" doesn't mean you are violating equational reasoning.
But as pointed out by rwbarton, substituting equal for equal may well result in different behavior in the IO monad, since in IO we can observe non-extensional properties of code.
I'll allow you to handwave the practical considerations, but not the theoretical ones. :)
You are confusing the program with its result. A value of type `IO a` is a description of program behavior, and it is that description which is referentially transparent and unchanging. A random number generator is an `IO Double`. A random number produced by that generator is a `Double`. These two things are not the same thing or we wouldn't give them two separate types! It really boggles me that you don't see how lazy `IO` does not break equational reasoning. To quote your last sentence: &gt; I've yet to see an example where lazy IO actually breaks equational reasoning. (Assuming the result of all IO actions can be random.) The problem with lazy `IO` is that ALL values in the program are potentially `IO` actions, therefore all values in my program can be random. This places me back at square one where I was with imperative languages where I cannot assume any equations are true about my program.
Well, I believe that `y=0`. Now use that to prove that `getCurrentTime` violates equational reasoning.
Read rwbarton's original comment. See the quotes? getCurrentTime violates equational reasoning in the same way (i.e. "not") as Oleg's example. counterex_ctx provides a function (in IO) that can distinguish equationally equal arguments.
Here's an easy example that does not use bottoms. line = unsafePerformIO getLine main = putStrLn (line ++ line) That will only prompt the user for one line and then duplicate the result (I compiled with `-O2`, in case you want to try it yourself). Now let's substitute in `line` with its definition: main = putStrLn (unsafePerformIO getLine ++ unsafePerformIO getLine) That prompts the user for a line twice and then concatenates the results. This is why unsafePerformIO violates referential transparency. These kinds of substitutions of equal for equal are no longer valid. 
I might be wrong but I believe OOM throws an asynchronous exception in Haskell.
Here's a much simpler example that shows why unsafePerformIO violates referential transparency: import System.IO.Unsafe line = unsafePerformIO getLine main1 = putStrLn (line ++ line) main2 = putStrLn (unsafePerformIO getLine ++ unsafePerformIO getLine) Those two `main`s give entirely different behaviors, despite substituting equals for equals. I cannot trust any substitution to be safe once people start using `unsafePerformIO`, which is further compounded by the fact that I cannot even tell which functions use it from the types. I'd have to visually inspect the source code of every library I use to ensure that any code refactorings are safe.
It's not guaranteed to work, as the current time is, to my knowledge, allowed to behave quite unpredictably. If you can make a function that distinguishes equationally equal arguments *and* are willing to thoroughly prove that it works, I will believe you.
You do what Oleg did. You write the two programs, one with the original expression, and one using `0`. Then, like Oleg did, you run the two programs and observe the two different results of execution. Now you claim that equational reasoning has been violated even though the two `IO ()` values are clearly the same.
That is a violation from the runtime system, not from Haskell's treatment of `IO`.
I believe this just shows the old problem of giving IO denotational semantics in the presence of concurrency. It's related, but not limited, to lazy IO.
Or even two different executions of identical programs can result in different behaviour.
I don't agree with the last paragraph. Show me an example where lazy IO breaks pure code. Oleg's `counterex_ctx` is not an example. The execution is consistent with `prepare` returning two random strings.
I don't think it is a violation. IO does things that are outside the scope of pure code. That's the point of it.
I agree with that much. I'm just saying that it does not justify using `unsafePerformIO`, because then I can't prove things about my program any more. I know that there are already some cracks where that information leaks like you and `rwbarton` pointed out, but I don't think those minor violations justify widening the cracks even further, especially when we do derive a great deal of benefit from the referential transparency that we still manage to preserve, the same referential transparency that `unsafePerformIO` jeopardizes.
The problem is that Oleg only shows that his two programs produce two different *executions*. But we all already know that two different executions of the same program can produce different results. In order to prove a violation of equational reasoning, Oleg needs to demonstrate that the his two programs are different *values* of type `IO ()`. This cannot be demonstrated by simply executing them.
Lazy IO undoubtedly wreaks havoc with IO semantics. But as long as the best semantics we have for IO is that it performs random actions, I don't think lazy IO makes it any worse. Scramble random and you get random. :)
I'm with you on `unsafePerformIO`. In our Haskell at work we don't have `unsafePerformIO` (nor lazy IO, for that matter).
I think Conal Elliot said it best: &gt; What meaning do we want String to have? ... The most compelling simple candidates I know of are that [] denotes lists (sequences), and Char denotes characters, and consequently that String denotes sequences of characters, i.e., "strings". &gt; &gt; **Therefore, we have to either choose a different type, such as (but not necessarily) IO String, or we have to choose a more complicated meaning for String**. The latter route might at first seem appealing, until you consider the deep implications. Purely functional (more accurately "denotative") programming supports practical, rigorous reasoning thanks to **using simple meanings like sequences rather than complex meanings like functions from some sort of environment including operating system, machine execution context**. (emphasis mine) Using Conal's example, when you use `unsafePerformIO` then all `Strings` mean a complicated function of execution environment rather than a sequence of characters that we can evaluate statically at compile time. This is true not just for strings, but for all program values. When I allow people to use unsafePerformIO I must implicitly assume that every value in my program is an implicit function of execution environment and cannot be reasoned about at compile time. I had to trim his answer because it was a long-winded answer, but you can find the original here: http://stackoverflow.com/questions/14295582/why-are-getargs-and-getprogname-io-actions/14296888#14296888
Handling OOM isn't really possible in Haskell. You never know what code will allocate, so there isn't really anything you can do safely once you hit OOM. If Haskell has a non-allocating subset, then it would be possible to do something.
Lazy IO makes it worse because now all pure values in my program can be random, too. We're not saying that unsafePerformIO makes `IO` actions more difficult to reason about. We're saying that it makes all the other pure actions more difficult to reason about, too, since we can't prove that they don't use `unsafePerformIO` under the hood.
Oh, hah!
That doesn't prove that it is not referentially transparent. Referential transparency is about equating two program descriptions as equal, not about equating bound values as equal to their originating program.
He's still correct in principle. The reason equational reasoning breaks down is precisely because when you bind `str`: str &lt;- hGetContents someFile ... the type of `str` is `String`, meaning that at that point in the program I should be able to point to a precise linked list of characters that `str` equals. If you say that this equation is not permissible within `IO`, then you are basically invalidating the referential transparency of all `let` bindings within `IO` code.
There's some interesting experimentation going on by Edward Z Yang on having support for resource limits on threads. Some of the preliminary info in his icfp submission on the topic is intriguing and may pave the path to having strong resource usage guarantees on a workload edit: heres a link http://ezyang.com/papers/ezyang13-rlimits.pdf and http://hackage.haskell.org/trac/ghc/wiki/Commentary/ResourceLimits
I thought Linux would never warn you about out of memory errors. Don't all memory allocations succeed and then only fail later when you actually try to use it? It's been a while since I checked, so I may be wrong.
A lot of this is a result of the ugliness of the underlying hardware, I found the idea of multiple stream representation quite elegant in fact.
Lazy I/O is not the same as `unsafePerformIO`.
Because at the moment the original maintainer is not maintaing it: https://github.com/Paczesiowa/hsenv/issues/40
I think what I find inelegant about it is that it's clearly not a scalable approach. Every time you find another recursion pattern that a consumer might care about for performance, you have to add another field to the record.
I think the only thing you get to assume about getCurrentTime is that it returns a number, and (if you are lucky) that the numbers are non-decreasing between subsequent calls. The exact behavior depends on the environment, which is why the result is wrapped in `IO`. The runtime of the program has itself an influence on the environment, but there are no guarantees. A sufficiently smart compiler or sufficiently fast CPU could evaluate `y` in less than one millisecond. Or by suspending the program at just the right point, evaluating x may take an arbitrary amount of time.
Optimizations in C are considered well-known, well-researched territory (read: "boring"). It's only a matter of implementing them. GHC didn't even do a trivial form of constant folding until recently [1]. So in principle Haskell optimizes every which way just like C. And more, much more. [1] http://hackage.haskell.org/trac/ghc/ticket/6121
Yes, this is how it works. The process gets killed by the Linux OOM killer on access to such pages. Especially on 32-bit systems, malloc() returning NULL usually means you ran out of virtual address space in that process - while actual running out of memory probably happened already earlier.
It's his paper.
cabal-install would have to be modified to support this mode of output. Currently it just writes the build log to a file.
&gt; meaning that at that point in the program I should be able to point to a precise linked list of characters that str equals. What do you mean by "at that point in the program"? I view the run-time system as traversing a path through a tree of "IO actions", and I personally have no problem with the run-time system operationally deferring a choice of which branch to follow when encountering a non-deterministic choice (ie, what the particular value of `str` is "at that point") as long as semantically the final traversal is consistent with the available choices. (As a consequence I view a strict implementation of Lazy IO as consistent with the unwritten semantics of Lazy IO) Of course, I'm the one who thinks Oleg's example is simply a program that contains a race condition. Clearly this is not a universally accepted viewpoint.
of course.
Think of it this way. When I write a function of type: myFunction :: String -&gt; Int I want to be able to statically reason about what that function does, in the same way that Oleg tried to statically reason about his example function. When I write such a function, I have no guarantee that people will not use it on the result of a lazy IO action. So what am I supposed to do? I would have to tell people in the documentation: "Don't use this function on the result of a lazy `IO` action if you want any of my equational guarantees to hold". Isn't the whole point of Haskell that we use types to enforce that kind of thing rather than documentation?
Yeah, this could easily be a Bob Harper blog post about how lazy evaluation breaks equational reasoning (except he doesn't care about equational reasoning). Evidence: same functions, but use the list `ones = 1:ones`. Now one answer is `True` and the other is bottom. And all we did is innocently use the Peano axioms to a situation they don't apply to.
If you mean [1] and [2], then there's nothing in there that's inherently imperative. In fact the Data Parallelism folks would easily scoop up the results like fish to water because there's less of an impedance mismatch than with C. Do you have concrete evidence against: *So in principle Haskell optimizes every which way just like C. And more, much more.* ? [1] http://en.wikipedia.org/wiki/Polytope_model [2] http://en.wikipedia.org/wiki/Loop_nest_optimization 
I wish every numerical library would stop taking vector dot product as the canonical example. It's boring and it doesn't even make sense from a language comparison point of view. Vector product in C is as readable as it gets. for (i = 0; i &lt; n; ++i) c += a[i] * b[i]; It's not like `sum (zipWith (*) as bs)` is an enormous improvement syntactically (composability notwithstanding). If the argument is that we can write **complex** algorithms cleanly *and* get high performance out of them, then show that. 
That's not what I disagreed with: read the first sentence of my comment. Although what you're saying here may be true, it's far from clear. If you see a way to apply polyhedral optimization to DPH programs with less of a mismatch than to C programs, write a paper about it :)
&gt; That's not what I disagreed with: read the first sentence of my comment. Would you like to point out the disagreement? Here's what I said: Classic optimizations are dead. Period. Not all have made it into GHC. Because there are bigger fish to fry. &gt; If you see a way to apply polyhedral optimization to DPH programs, write a paper about it. And take food away from the nice DPH folks? Why would I want to do that? Really now, do you see anything inherently imperative in P.O.? Such a paper would be far more interesting and publish-worthy, agreed? 
`getCurrentTime` breaks reasoning, but we don't have an alternative. We do have alternatives for IO, as far as I know they do not break equational reasoning. In a distant future, if they will be mature enough and commonly used, I will support deprecating lazy IO or requiring -XUnsafe to use it.
Absence of proof is not proof of absence. The point is not that the program might in some corner of space produce the same result for `x` and `y`. It's that `x` and `y` are no longer the same, for precisely the meaning of *same* in dispute.
&gt; Now, I would guess lazy IO has purely observable effects, but I'm not sure. That's what Oleg has been trying to prove all along, but this last attempt is clearly a dud.
You can disable memory overcommit and the OOM killer on Linux.
I said IO can distinguish equationally equal arguments, not that there is a guarantee that it always can.
Show me a pure function `f` that does such an observation. I claim only function of IO type will be able to make those observations.
Sure, but can you write a function `f :: A -&gt; IO Bool` for some type `A`, such that there exists `x y :: A` where `x` and `y` are definitionally equal and where the only IO `f` uses is `getCurrentTime` (and `evaluate`, since it can be written in terms of non-IO things), such that it is guaranteed by Haskell that `f x` is not interchangeable with `f y`? If not, `getCurrentTime` is not (to my knowledge) itself a problem, though it might be a problem in conjunction with other features.
My point is that `getCurrentTime` doesn't break equational reasoning any more than `readFile` does. For both of these functions the result depends on the environment, so the result can change between different runs. If I understand /u/rwbarton's argument correctly, then he argues that since changes to other parts that are equationally equal can change the behavior of the `getCurrentTime`, that this makes `getCurrentTime` violate equational reasoning. But I claim that this is not the case. You can not write a context that can distinguish two equal expressions by using `getCurrentTime`. You can certainly try, by measuring the time it takes to evaluate the two different expressions. But a different time is no *guarantee* that they are different or the same. On the other hand, I don't really care. Stuff in the IO monad can do whatever it wants. You could read the executable file of the program, for all I care, and distinguish equivalent expressions that way. This shouldn't inhibit the compiler from optimizing some expressions to other equivalent ones.
This is what I disagreed with: &gt; Optimizations in C are considered well-known, well-researched territory (read: "boring"). As a counter-example, I provided polyhedral optimization. Haskell is not the only place where people are doing this kind of research. There is great compiler research on vectorization and parallelization in the imperative world, and they're not "boring" nor "dead". Polyhedral optimization in particular goes quite far, since it tackles nested loops too, and restructures nested loops not only to achieve vectorization and parallelization, but also to improve cache locality. I'm not saying that the same can't possibly be done for Haskell, obviously it can, and it would be great. &gt; Really now, do you see anything inherently imperative in P.O.? Where did I say that? Nothing inherently imperative, but polyhedral optimization *is* based on representations of nested for loops. Your claim was that there is **less** of an impedance mismatch than with C. This is a non trivial claim, given that in C one generally writes nested for loops for this kind of code, whereas it's much less clear how to get DPH code into the polyhedral model.
Really nice! This feels very similar to staging of type class dictionaries + memoization to handle recursive definitions. Would it be possible to let the programmer mark what should be evaluated at compile time to make this generally applicable to type class dictionaries? Or some maybe even do it automatically?
The Repa and Accelerate papers tend to use Black-Scholes or n-body simulation as their examples. When discussing a library, there's a tradeoff between giving simple examples that shed light on the implementation and giving larger examples that show the library interface in action. 
Note that lazy IO is built on `unsafeInterleaveIO`, which is very different from `unsafePerformIO`.
Yeah, I just realized that. My mistake.
BTW, from the pipes tutorial: &gt; pipes never uses unsafePerformIO and never violates referential transparency. Is this meant to be a reference to lazy IO, or the (past?) use of unsafePerformIO in conduit (I don't remember for certain if it ever was used)? If the former, perhaps change it to `unsafeInterleaveIO` for clarity.
I'm sure I'm missing something, but what do you mean? &gt;&gt;&gt; let ones = 1:ones in (length . take 2 $ (1:ones)) &gt; 0 True
It was a reference to lazy IO. Yeah, I need to change that.
As well, it seems like to me the dotp should be: dotp ≡ sum (zipWith (∗) v w) ≡ foldl'_s (+) 0 (stream (unstream (zipWith_s (+) (stream v) (stream w)))) ≡ foldl'_s (+) 0 (zipWith_s (+) (stream v) (stream w)) As the following page has the definition of: foldl'_s :: (a -&gt; b -&gt; a) -&gt; a -&gt; Stream b -&gt; a
&gt; given that in C one generally writes nested for loops for this kind of code, There's your problem. &gt; whereas it's much less clear how to get DPH code into the polyhedral model. You don't write DPH code using nested loops. What you do is take the salient ideas from P.O. and apply them in a declarative context. And it's obviously easier because not knowing anything about P.O. until you mentioned it (thanks!), just from [1] alone, I see that it's about analyzing data dependencies and shaking them around for a better fit, something that Haskell has always done for a living. (I'm obviously being facile about the whole business, and you obviously should pin me down on "put up or shut up.") It's only about stencils, but if you're interested, this work-in-progress seems related [2]. [1] http://en.wikipedia.org/wiki/Polytope_model [2] http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/Stencil.pdf 
&gt; On the other hand, I don't really care. Stuff in the IO monad can do whatever it wants. Based on [1], allow me to play devil's advocate in the C camp. I claim that if you have written and executed any kind of Haskell program at all, then your statement of indifference is a lie. Because if you truly believed IO is utterly and mind-blowingly random, you wouldn't feed anything to `main`. So Oleg has a point. (Although he does himself disservice by being so shrill about this, verging on hysteria.) [1] http://www.reddit.com/r/haskell/comments/1bsitm/lazy_io_breaks_equational_reasoning/c99tv63
It's odd that mighty oleg thinks you can execute proofs straightforwardly with Haskell `Int`s. A similar proof would of course show that `length xs &gt;= 0` -- but try that on `[1..maxBound :: Int] ++ [1..10]`. His choice of examples shows awareness of this, but he can't have thought it through. It is clear that in this example the trouble arises from strict evaluation not lazy io. In fact if you use the only number concept suitable for `length` and `take`, as acting on lazy lists, the example fails completely. Insert the following in http://okmij.org/ftp/Haskell/LazyIONotTrue.hs : data N = Z | S N deriving Show _length [] = Z -- plz note _awesome oleg stylings_ _length (x:xs) = S (_length xs) -- I should call it real_haskell_length _take Z s = [] -- and real_haskell_take _take n [] = [] _take (S n) (x:xs) = x : _take n xs cex1b = _counterex_ctx $ \str -&gt; case _length (_take (S (S Z)) (' ':str) ) of Z -&gt; False _ -&gt; True cex2b = _counterex_ctx $ \str -&gt; const True str _counterex_ctx :: (String -&gt; Bool) -&gt; IO () _counterex_ctx hole = do (str1,str2) &lt;- prepare if hole str1 then print (_length str2) else print "impossible" Then, replacing `cex1` and `cex2` in oleg's `main`, the program runs as follows: -- Demonstrating that LazyIO breaks equational reasoning -- cex1 -- producer start -- producer end -- S (S (S (S Z))) -- cex2, a provably equal expression prints: -- producer start -- producer end -- S (S (S (S Z))) yours ever, *Revolutionary Committee for the Defense of Lazy IO*
Oh, I'd forgotten about the `take 2`. But, he applied the peano axiom to: 1 + length (take 1 ...) &gt; 0 So, drop the `take`. Then we get the reasoning: length (' ':ones) &gt; 0 = (definition of length) 1 + length ones &gt; 0 = (Peano axiom) True Bob Harper has to use slightly different code.
FWIW, that agrees with my understanding.
Well, a simple but a bit hackish solution might be, to set a hard limit for the RTS’s memory usage, in a way that gives you a exception that you can still catch and handle… and then either configure a fixed size to be reserved for that program at start, or to run a function every n milliseconds, that updates the RTS’s memory limit to the free space minus a safe space (so you can still actually run that exception handling function). With the size of that safe space set to the maximum other programs can grow in that amount of time between two size updates.
&gt; Now, I would guess lazy IO has purely observable effects The only impurity in the provided program is what it takes to call `prepare`. The fact that `counterex_ctx` prints stuff is a red herring; we could redefine `counterex_ctx` to simply return the answer rather than print it. counterex_ctx2 :: (String -&gt; Bool) -&gt; String -&gt; String -&gt; Maybe Int counterex_ctx2 hole str1 str2 | hole str1 = Just (length str2) | otherwise = Nothing This is as pure as it gets, and yet, `counterex_ctx2 (\_ -&gt; True)` is not guaranteed to behave the same as `counterex_ctx2 (\s -&gt; length (take 2 (' ':s)) &gt; 0)` on all inputs. The problem is that `unsafeInterleaveIO` allows us to form entangled values, from which pure actions on one of those values will cause spooky action at a distance on the other value. Of course, without using `unsafePerformIO` or equivalent, there's no way to exit `IO` and therefore no way to gain direct access to the entangled values--- we can only access them under the `IO` monad. But this is little consolation, since ultimately all values are under the `IO` monad, and therefore all "pure" values could in fact be entangled. Ultimately, the problem of `unsafeInterleaveIO` is identical to the problem of `unsafePerformIO`. While Oleg is targeting Lazy IO, there's nothing special about Lazy IO here. You can perform the same shenanigans with `IORef` and `unsafeInterleaveIO`. As far as `getCurrentTime` goes, the only way that's relevant is because we can use it to observe the side-effect of using call-by-need instead of call-by-name. If we ignore the ability to introspect on the computation process to observe how long it takes or how much memory, etc, then call-by-need does not break referential transparency. However, as soon as we admit introspection on the computation process, then the fact of whether something has been forced or not also becomes spooky action at a distance. **Edit**: Just to be clear, even though `counterex_ctx2 (\s -&gt; seq s True)` and `counterex_ctx2 (\s -&gt; length (take 2 (' ':s)) &gt; 0)` are equivalent and therefore we should be able to substitute one for the other, in fact we can only substitute equals in pure code: we cannot do so under `IO`. However, because `main::IO()` is the entry point to the program, everything is under `IO`; therefore, we cannot substitute equals anywhere in actual programs. I don't see a way to resolve this issue, since programs always have `IO` and therefore our semantics are always embedded under a layer of meaningless randomness; we can give meaning to pure *expressions*, but we cannot give (non-trivial) meaning to *programs*.
Oleg could have used cex1 = counterex_ctx $ \str -&gt; case str of _:_ -&gt; True _ -&gt; True and saved me a lot of puzzlement, since that's all cex1 is doing. (PS I don't understand why you used cex2b = _counterex_ctx $ \str -&gt; const True str rather than cex2b = _counterex_ctx $ \str -&gt; True or cex2b = _counterex_ctx $ const True).
yeah I tried a few variants of cex2b in order to convince myself that nothing about eta equivalence was somehow coming in, but left that one. All the business is in the other function; it isn't true as is said again and again elsewhere on this page, that what matters is that cex1 doesn't have to 'observe' anything, but cex2 must examine something, since that distinction applies to cex1b and cex2b.
DPH uses very similar techniques for vectorization of nested data parallelism. I wouldn't be surprised if most sophisticated imperative optimisations can also be applied in a functional setting. The "nested loops" simply become "nested map/folds".
I can't remember where I read this, but it is no longer the case, even though the out of memory exception continues to exist. I also can also confirm that this is true in my personal experience as I once had out of memory problems and tried catching the exception in tests that I wrote to pin down what was going on but it turned out that the exception was never actually thrown.
I discovered Haskell had stack traces while trying to debug an OOM, only to discover they don't work for OOM precicely because they only trigger on exceptions and there is no OOM exception :(.
Before getting too fancy with the UI configurability, I was planning to just enable a tmux front-end and let people use an established UI for navigating panes. However, for the purpose of improving "cabal install -j" I wanted something that didn't have tmux as a dependency, hence my rudimentary NCurses interface. Still, rudimentary as it is, I'm surprised I couldn't find anything to do this for me out of the box! (Or that make -j or cabal install -j weren't doing something similar already...)
Note that oleg's 'main' also seems to fall flat if we skip `System.IO` and introduce our own 'visibly genuinely lazy io' . The essence of so-called lazy IO resides in the connection between `hGetChar` and `hGetContents` on the one hand and `hPutChar` and `hPutStr` on the other. So if e.g. we keep `hGetChar` but redefine the essential concepts as one might in an elementary tutorial: hGetContents h = catch loop rest where loop = hGetChar h &gt;&gt;= \c -&gt; fmap (c:) loop rest = const (return "") :: SomeException -&gt; IO String hPutStr h [] = return () hPutStr h (x:xs) = hPutChar h x &gt;&gt; hPutStr h xs hPutStrLn h xs = hPutStr h xs &gt;&gt; hPutChar h '\n' then, with this genuinely lazy IO, oleg's original example yields: Demonstrating that LazyIO breaks equational reasoning cex1 producer start producer end 0 cex2, a provably equal expression prints: producer start producer end 0 If we use 'lazy length' we get `Z`. (The difference from the result above is a standard sort of `counter-intuitiveness' of lazy io). The real result of oleg's researches seems to be that 'kinda-lazy kinda-strict io breaks equational reasoning', which one might have expected. But who knows, he's presumably always right.
&gt; I'm not sure any is disputing that lazy IO messes up the intuitive semantics of IO. Well, there is one thing that is non-intuitive about lazy IO, which is that you can't reason about the order of operations. In the absence of lazy IO you can assume that if you sequence two IO actions their effects occur in order, but with lazy IO you cannot assume that. Edit: Oops, I misread your statement. I thought you were arguing that lazy IO was not unintuitive. Just ignore me!
Oleg's attempt seems to be to try to "entangle" two lazy I/O operations, such that evaluating one of them changes the other one. So it's a bit more convincing than just using some more IO to observe the effects of lazy IO: it's using some lazy IO to observe the effects of some other lazy IO, so there's your pure observation. I'm not sure I'm convinced it counts, but I think it's a lot of fun nevertheless :)
Now try OpenGL T.T
I was wondering about that, I was messing with that Netty and it did not seem to be an FW at all... At least I wouldn't want to write web apps in it. 
But what the pure code is "observing" is a value that comes from an IO operation, so it's random anyway, so there are no inconsistent observations. 
It was a mistake. I didn't realize that `unsafeInterleaveIO` was the culprit. Several other people corrected me elsewhere in this thraed.
Ya know, I never had a problem getting OpenGL working with Haskell. Once I had OpenGL/GLUT working with c++ I just installed the haskell packages and it worked fine.
In the end I did it in a chroot environment, using debootstrap.
I actually don't remember if I had that too much hassle with OpenGL, but everything that depends on something outside of Haskell (that doesn't work with a simple cabal install), like GTK or such, was always difficult for me. It always gives weird compilation errors (Compiler exit 0) and I have no Idea what's missing. It's kinda of a let down having such a hassle to try some new libraries when it takes (at least for me) a big effort to figure out how to install. Easier to work on Linux.
from the course wiki: *Tips for presenting and being presented to* smile.. pay attention .. nod.. praise.. know their name.. ask questions
I'm afraid this is some leftover from previous versions that we forgot to remove.
You can, but not many people do that and it's a system-wide setting, so it can't be used for one application only. Many programs overcommit a lot.
Isn't it worrying that the semantics of IO is random, since `main :: IO ()`?
If it were truly random it would be worrying. :) But it has an informal semantics. Lazy IO plays funny tricks with this informal semantics, so if you use lazy IO you have to be extra careful. Using lazy IO is similar to using `forkIO`; it introduces actions that will happen asynchronously.
I agree with all you said, I'd just like to emphasize that the observed difference can be explained by `prepare` returning different values for different uses. Of course, to do this `prepare` has to use time travel, quantum entanglement, or something similar. If we had a semantics for IO, we could show where things break down, now we have to shout and wave arms.
related: http://stackoverflow.com/questions/257433/postgresql-unix-domain-sockets-vs-tcp-sockets from one of the answers: &gt; Momjian states, "Unix-domain socket communication is measurably faster." He measured query network performance showing that the local domain socket was 33% faster than using the TCP/IP stack. 
Neat! Definitely going to look under the hood to see how you did some of this (and whether field access is O(1)).
&gt; It seems especially tricky because even in code that you might not think of as allocating memory, there may be thunks getting forced. ??? To force a thunk means to evaluate it. Typically that's a good thing for memory: the thunk ends up ready for garbage collection. Possibly, at least. It's thunk *creation* that's the bee's knees.
Interesting... am I correct in inferring that there is only a single global namespace for field names? That is, if I define two fields with the same name in different modules then they will be equivalent?
The namespace is for name/type tuples; `"rank" ::: Int` and `"rank" ::: String` are distinct types and can coexist.
Ah, [it looks like it's O(n)](https://github.com/jonsterling/Vinyl/blob/master/Data/Vinyl/Lens.hs): rGet = view . rLens rPut = set . rLens rMod = over . rLens -- Records have lenses rLens' :: (r ~ (sy ::: t)) =&gt; r -&gt; Elem r rs -&gt; SimpleLens (PlainRec rs) t rLens' _ Here = lens (\(x :&amp; xs) -&gt; runIdentity x) (\(_ :&amp; xs) x -&gt; Identity x :&amp; xs) rLens' f (There p) = rLensPrepend $ rLens' f p rLensPrepend :: SimpleLens (PlainRec rs) t -&gt; SimpleLens (PlainRec (l ': rs)) t rLensPrepend l = lens (\(x :&amp; xs) -&gt; view l xs) (\(a :&amp; xs) x -&gt; a :&amp; (set l x xs)) Well, for most records, hopefully, ```n``` should be pretty small, so this isn't a huge deal. But if anyone thinks of an O(1) way of doing it, it looks like it'd be possible to change the implementation without changing the library interface, since none of the outwardly facing stuff seems to spill this implementation detail. So good design! 
Yes, but my point was that if a field with name "rank" and type `Int` appears in two different modules, then even though each of the two modules might have a completely different interpretation for what the field means and how it should be used, the two fields will nonetheless be treated by the library as being identical --- that is, there is no way to prevent a field that was declared in one module from being accidentally used in another module where one of the same name and type just so happened to be declared.
It would be nice to have a comparison table for this sort of thing. What's possible with `Vinyl` but not in others, and most importantly, why?
Think about it this way: there are two common kinds of approaches used to improve on Haskell records: one kind starts with Haskell records and then build on top of them, and the other kind provides a new way to define records. `Vinyl` is the second kind, whereas most other lens libraries that I've seen are the first kind. Specifically, `Vinyl` uses the new datatype kinds to allow you to treat records as a collection of fields each of which has a String name and a type. Furthermore whereas standard Haskell records use nominal subtyping (i.e., types are distinguished by their name), `Vinyl` records use structural subtyping (i.e., types are distinguished by what fields they have). `Vinyl` is not the first library in this spirit, but I can't remember the name of the last library that I saw which did something like this.
I don't exactly know this library, but I think you're right. The type level strings don't introduce a new name in the namespace. So, it's not really different from defining something like (Bool, Int), which is the same for every module in your system, independent of meaning. You can always use `"rank" ::: MyRankNewtype`
You could limit processes which are known to cause OOM with memory cgroups though so they don't eat up all the memory for the rest of the system.
At the very least you should in theory be able to have an Erlang OTP style supervisor/worker hierarchy where the OOM kills the workers and the supervisors restart them.
Good post. I had the same problem when trying to upgrade cabal from haskell platform.
Do we have type level comparison for type level strings yet? Then it could at least be made O(log n).
I forgot for the moment that aside from The Pragmatic Programmer (Addison-Wesley) none of The Pragmatic Bookshelf's titles actually follow the template "The Pragmatic ...". So I was honestly confused, and read this announcement as a The Pragmatic Bookshelf contribution to Haskell. Whether or not The Pragmatic Bookshelf or Addison-Wesley has legal protection or would seek to enforce it, I am baffled why anyone would want to crowd an existing namespace to the point of confusion, when there are so many original possibilities out there.
I like these "objects"—collections of fields. This is a feature often missing in Haskell—extensible, open world records. It feels like row typing [1] though I haven't thought it through yet. [1] http://www.cs.cmu.edu/~aldrich/courses/819/slides/rows.pdf
Yes, though if you do the DPH transformation you pretty much lose any hope of doing this, since it flattens loops. Without the nested loop structure it will be almost impossible to do locality optimizations. For example polyhedral optimization can automatically turn a naive matrix multiply into a cache friendly one with strip mining. Perhaps a better approach is to define combinators that directly express the structure that polyhedral optimization can handle.
You underestimate how hard things are. The data dependencies that PO can handle are of a specific form. Haskell code is not in such a form. DPH code isn't either. A class of C loops are in that exact form. It may be the case that (a subset of) Repa code is in that form. That could be a promising avenue to apply this to Haskell.
So are you looking for something like the stream fusion equivalent of cache-oblivious algorithms?
Correct. One of the things I've considered doing is allowing users to provide their own key kind, instead of just using Symbol everywhere. Of course, real modules would make it a bit easier to do, but it should still be possible.
But if I got it right, then the examples of the article *would* be equivalent, right? I mean, both programs return True when run properly: one of them just is more prone to not being run properly. Is that the idea?
Hi, I'm the creator. 1. Records as a collection of fields: if one is going to have multiple fields in a record, then we must have a "collection" of fields. Further, the point of extensible records is to be able to do things to them, like add and remove fields. Further, any diagnostics we apply (such as the subtyping relation, which I'll discuss below) require information about what fields a record has. 2. The structural subtyping is necessary and useful because it lets us treat two records as the same, when they share some common _intersection_ of fields. For instance, `{day:Int, month:Int, year:Int}` is a inherently (structurally) subtype of `{month:Int, year:Int}`, and there may be times when we'd want to convert the former into the latter. In addition, what the subtyping relation gives us is the ability to treat two records which are not the same, but which are _congruent_ (they have the same fields, but in different order) as the same.
So the main issue with lazy `IO` is that you can't reason about the order of side effects. Usually when you run an `IO` action like: do io1 io2 io3 ... we can guarantee that `io1`'s side effects occur first, `io2`'s side effects occur second, and `io3`'s side effects occur third. Using a concrete example, if you use the strict `readFile`, then in the following program you can guarantee that the file is read before `print` executes: do str &lt;- readFile print 1 ... With lazy `IO`, you lose that total ordering of `IO` side effects, which makes it more difficult to understand when side effects actually occur. So if you use the lazy `readFile`, you can't guarantee when exactly the file is read. All you know is that it won't be read any later than when the result is demanded by some function. So lazy `IO` behaves sort of like a future/promise where it's as if you had forked a concurrent thread to read the file and then promise to return the result when it is needed, except that instead of evaluating speculatively to retrieve the contents as early as possible you evaluate lazily. Also, unlike a future/promise there is no "type-ful" distinction between the promise and its result. This is why people in this thread make a lot of analogies to concurrency when describing why this behaves weirdly. Oleg's example basically shows that you can have two separate functions which should evaluate to the same thing, but they evaluate their arguments with slightly different strictness properties. lazy `IO`'s behavior is very sensitive to strictness properties, so he tricks the lazy `IO` into reordering side effects differently as a result of that different strictness to give different results. So the objection to lazy `IO` is that we don't really like it when our program has different side effects as a result of different strictness properties because strictness is already pretty difficult to reason about in Haskell, and tying side effects to the evaluation model just worsens thing. One of Haskell's defining properties is that we separate side effects from the evaluation model, and lazy `IO` undoes all of that.
That's a pretty nice analogy, I guess.
Thanks! It was actually those slides the first inspired me.
You have not demonstrated how myFunction can break under lazy IO?
Awesome explanation!
It's not so much that it breaks as you can't easily statically reason about what it does in the presence of lazy `IO`. In the absence of lazy `IO`, the only denotation I need is that a `String` is just a sequence of characters and I can reason very simply about how `myFunction` behaves. With lazy `IO`, a `String` becomes a function of the execution environment and other processes, which makes it very hard to reason about what it will do. Oleg's example highlights that where the `String`'s value is changing over time because it is a function of the execution environment (in this case because he is piping data from another operation to the lazy handle), making it very difficult to reason about what any function of `String`s will do when passed those lazy `String`s representing file IO.
I have a suggestion for the `concatMap` problem in my thesis. It'd be great if anyone had the time to follow it through.
That's very cool! I can't wait to play with it more.
The first example is compared with a simple (Figure 3) and somewhat less simple (Figure 4) C baseline, as "Hand Written C" lines in Fig. 5.
could the subtyping relation be generalized so that not only A is a subtype of B if A has an extra field but also A is a subtype of B if A replaces a field of type T in B by a field whose type is a subtype of T ?
The reason why Netty is slower with on one thread is because it's only using one thread. Netty is very good at saturating all cores if you let it. How did you test this by the way? Did you warm up both before doing your tests? Netty is at least 2 to 3 times more performant than your code on my machine even with your parameters passed when benchmarking with weighttp on a crappy Core 2 Duo. 
That's a really cool idea! I don't know if I have time to work that out at the moment, but I'm happily accepting pull requests! :)
Reports of the death of lazy IO are greatly exaggerated. The linked abstract was all fine until near the very end. But it is not disturbing. And lazy IO, which for me is still clearly the best way of doing incremental file processing in Haskell for most programs, should certainly not be banished. I do believe that soon we will achieve IO in Haskell that is as intuitive and easy to use for simple programs as lazy IO, yet preserves pure-style equational reasoning and is compositional. When we get there, then yes, lazy IO will be finished. We are getting very close. But we are not there yet. In the meantime, Haskell's type system allows us to separate clearly between the pure and impure portions of our programs, and we endeavor to keep as much of it as possible pure. For the impure sections, yes, we know that equational reasoning holds only modulo the implied operational semantics. And yes, we also know that a pure sub-expression of an impure expression is actually impure. In cases where the complexity costs of those caveats are high, then yes, we must use some other more complex IO paradigm. But for most programs - at least most of the programs I come across - lazy IO still wins.
Another option is to use [cde](http://www.pgbovine.net/cde.html). That's what we do.
Yes, and in that case C is roughly the same speed as Haskell, despite the fact that Haskell is using AVX instructions whereas the C code is only allowed using SSE (although the Haskell code is using the AVX instructions on SSE registers, but still). However, I'm talking about the second benchmark, the one where Haskell is supposedly 2x faster than C.
Well, I have a virtualbox Win7 VM on Linux. Works great that way, too.
But when you have a function `String -&gt; Int` which, say, counts the number of 1s in the sequence of 0s and 1s, regardless of how the input string behaves, the answer will be correct for whatever the lazily returned string *ends up being*. It doesn't seem to me to break equational reasoning because within the block of code that takes that string, there's *only exactly one way* the lazy thunks can be evaluated, you just can't determine which way in advance (but, I mean, that's no different than any function whose domain is "all strings" and range is "all non-negative integers". There's no equational reasoning for "string x for all x should have output 5". That doesn't obey your range. Let me put it this way, and maybe this will help you point out how I'm reasoning about this incorrectly. If I have `myFunction` as below and I pass it a string returned from lazy IO, then the first character will either be, per the definition of a list, either [] or a : [a], right? And what values "a" and "[a]" take on are, like *any other lazy value* not yet known at the time you have merely pulled the first character off. So, for example: myFunction :: String -&gt; Int myFunction str = case str of [] -&gt; 0 '1':xs -&gt; 1 + myFunction xs x:xs -&gt; 0 + myFunction xs Even though the lazy string might have values pulled lazily through IO, it still *must* be a string, the only caveat is that it could take arbitrary lengths of time to evaluate the next character. So if the string is defined based off a series of characters the user inputs and finally hits enter, no matter what, if they type "01001&lt;enter&gt;" it will return 2. Right? There's no way it can return "5" if that's the string I enter. Likewise, myFunction can be composed with a function that consumes that integer and returns a string of 1s of that length. And it'll return "11" always. Now, if it takes me 33 days to hit "enter" after typing "01001", then the program will not return a value for 33 days. But when I hit enter, it'll behave correctly, right? What's confusing me is that you seem to be saying that you cannot reason about code with lazy IO ever, period, at all, there's no way to understand what it will do, that you could have a function that takes a string and it'll be passed a banana, and you just have to suck it up and deal with it. But I can't imagine a situation in which anything as hyperbolic as the way you seem to be suggesting it affects understanding the program. It can't ever be passed a banana, and it's not like lazy IO allows the string to go back in time and retroactively change 0s to 1s or vice versa. A string passed to `myFunction` will behave soundly *no matter how that string is obtained or how long it takes to evaluate the thunks involved*. Or at least, it won't behave any more strangely than it can already behave in Haskell with bottom and `error` and `undefined`. Is this more or less correct? Where have I slipped up?
Right. The problem, though, is that we need a complete semantics for IO. Otherwise we'll be left with `TheKnownPartOfIO` and `TheRestOfIO` where we'll still have `main :: TheRestOfIO ()`. Hopefully, the reason this is a problem should be evident from the edit to my previous: `TheRestOfIO` (like current `IO`) is a referentially opaque context, and therefore we cannot substitute equals underneath it.
&gt; What's confusing me is that you seem to be saying that you cannot reason about code with lazy IO ever, period, at all, there's no way to understand what it will do Yes. &gt; ... that you could have a function that takes a string and it'll be passed a banana, and you just have to suck it up and deal with it. No. I'm not saying that lazy `IO` is some sort of type error. When I say that you can't "statically reason" about lazy `IO`, I mean that it interferes with equational reasoning. Again, going back to Oleg's example, he has a function which he shows should be equivalent to some other function, but his equational reasoning is rendered invalid by lazy `IO`. I'm not saying you won't get a `String`. You will. You just can't guarantee that the `String` doesn't randomly change as a result of refactoring pure parts that have nothing to do with `IO`.
I still don't follow at all. Equational reasoning doesn't work *in* IO, but that's because in IO the order in which things *actually occur* matters, which doesn't matter for pure code, but equational reasoning *outside* of IO is still valid. That is, if Oleg performed his equational reasoning outside of IO, it's okay. Inside IO, it's not. Your `myFunction` is still safe. And if it's not, I'd like you to show me a proof that it isn't? That should be a lot easier than asking for a proof that it is, I just need a counter-example where you're not fiddling with equations *inside* IO, which allows you to observe effects like Oleg's example shows?
The function *f* isn't what's causing a problem here though, it's this: counterex_ctx :: (String -&gt; Bool) -&gt; IO () counterex_ctx hole = do (str1,str2) &lt;- prepare if hole str1 then print (length str2) else print "impossible" Which does something *crazy* with IO by making a choice of which string whose first character must be evaluated first. The problem is the *crazy* involved. i.e.: f never returns a value other than what it should, but it has a side effect that's visible in IO. What's the big deal?
Ok, I understand your point now. You're saying that it's not bad that `IO` can distinguish between two equationally equal functions because it can do so already, such as by using the `getTime` example already given elsewhere in this thread, or perhaps by loading its own source code and reading the original function definition. I guess my point of view on that is that while `IO` can distinguish to some extent these kinds of transformations, we shouldn't make it worse than it already is. Haskell is one of the few languages where equational reasoning is mostly valid, and it would be a shame to weaken the strength of equational reasoning just for a little convenience.
using apache benchmark ab -c 360 n 60000 &lt;target&gt; Should I have used more reuqests? played around with some other concurrency and instance settings but that is what is in the pics. That was it, that and whatever the default settings are from the git site.
No. You need to warm both of them before executing your actual requests. So execute your test run and then measure the second time. Reason being, the JVM does some jitting at runtime over the course of an application to speed up execution itself. Warming up a JVM app before executing your tests gives the true speed of the app. I have a feeling you haven't done that at all. In my opinion though, it's best to run that test over and over again until the numbers stabilize and then take that measurement. Also, try out different benchmarking tools. ab is regarded as a pretty poor and outdated one. Weighttp, siege, wrk, and httperf are considered superior. 
I ran the test about 30 times. those were the best scores I got for each. I didn't see a huge variance from run to run but am completely ready to admit non scientific, ad hoc and probably poor. I'll keep weight http in mind, thanks! 
Thanks, that clarified what going on with `RecursiveDo`/`DoRec`.
But we can never get a complete semantics for IO unless we have a complete semantics for the world, so it will never happen. It would be nice to carve out pieces of IO which can be given semantics (e.g., `IORef`) and then be able to reason about those. Then we could make accurate statements about what happens when you combine, e.g., `unsafeInterleaveIO` and `IORef` (kaboom!).
Off-topic: Pretty annoying that spacebar is bound to "scroll to the top", when every browser uses it as an alternative page down button.
Not in Firefox or Epiphany for me?
Be wary of `MultiWayIf`, lack of layout [screws up nesting](http://hackage.haskell.org/trac/ghc/ticket/4359#comment:90).
I just tested it again - it seems that the "email or username" form field is selected as default, but also hidden away. So basically any button would scroll you to the top, to make the "Log In" form visible (though it was not actually visible)
Just a remark, Michael Snoyman has just given a shorter and better version: https://gist.github.com/snoyberg/5335726 Only one file, no session backend, no log, and use System.Random.MWC for random number generation. Also, there is a very few usage of template haskell magic. I am very curious to see the new results.
Your is a good point, but it's unlikely "The Pragmatic ..." could be a copyrighted one. In case Addison-Wesley will enforce the restriction I will change the name, but is not a commercial product, is just a github repo and a bunch of tutorials, at the end of the day.
This is the Haskell methodology in a nutshell! (half joking)
Correct me if I'm wrong, but it seems to me like vinyl would allow shadowing fields, right? (currently updating the ancient version of GHC on this box, so I can't test it out). name = Field :: "name" ::: String age = Field :: "age" ::: Int sleeping = Field :: "sleeping" ::: Bool jon = name =: "jon" &lt;+&gt; age =: 20 &lt;+&gt; sleeping =: False steve = name =: "steve" &lt;+&gt; jon And `steve` would be of type `Rec [ "name" ::: String, "name" ::: String, "age" ::: Int, "sleeping" ::: Bool ] f`
Actually, I'd rather seem a comparison table of uses. That is, I'd like to see two or three sample real-world-ish programs that make use of records, and see how they "feel" in several record systems. For the record (ahem), I'm dubious that real world programs benefit from this sort of structural subtyping (static or no). Years of OO programming have left many with the feeling that structural inheritance was a mistake from the start.
Very nice work! Setting this for [Pick of the Week](https://www.fpcomplete.com/school/pick-of-the-week) at School of Haskell.
Do you happen to know what the RTS options should be for this? I can package it up and make a pull request if he wouldn't mind.
OK cool. I think it was an older version, maybe pre-CPS-rewrite? If I remember correctly at all. :)
Is using `aeson` to parse JSON safe from an untrusted source? I assume it is, but it never hurts to ask.
Here's an example. The function `f s = take 2 s`. If `s` is backed by lazy IO, the expression `fmap f $ readFile path` leaves the file handle open indefinitely. If on the other hand `readFile` read the entire string strictly, then that same program closes the file handle immediately. Basically, if you consider the resource usage of your program to be part of its meaning, then lazy IO violates RT because it means that supposedly pure values will be allocating and freeing resources. 
I agree that class hierarchies ala C++/Java/C#/etc are undesirable. I feel that structural typing is a bit different. I think what is going on here is much more like statically checked duck-typing, which I think is a real win. The Go language is a very good example of an OO language that does not have class hierarchies, but does support structural typing, and I think it benefits from it greatly.
&gt; I'd like to see two or three sample real-world-ish programs that make use of records, and see how they "feel" in several record systems. Exactly my thinking! &gt; For the record (ahem), I'm dubious that real world programs benefit from this sort of structural subtyping (static or no). Years of OO programming have left many with the feeling that structural inheritance was a mistake from the start. I'm willing to give it a spin, provided I have some idea where the sharp edges are. Specifically, does this play well with type inference?
&gt; I think it benefits from it greatly. Not having any experience with Go, could you provide some snippets illustrating this benefit?
Great! Glad to here success stories, and I'd be interested in a link to your code or (even better) if you can blog about how it went.
Thanks. I'm notoriously bad at finishing blog posts so don't hold your breath. But I might give it a shot since you ask so nicely.
Here's an example that uses [the upcoming `pipes-stm` library](https://github.com/Gabriel439/pipes-stm): **EDIT**: You want the [fork branch](https://github.com/Gabriel439/pipes-stm/tree/fork) of the repository which has my latest changes. import Control.Concurrent import Control.Monad import Control.Proxy import Control.Proxy.STM import Control.Proxy.Trans.Maybe import Control.Proxy.Trans.State data Event = Regenerate Integer | Damage Integer type Health = Integer acidRain :: (Proxy p) =&gt; () -&gt; Producer p Event IO () acidRain () = runIdentityP $ replicateM_ 3 $ do lift $ threadDelay 1000000 respond (Damage 1) commands :: (Proxy p) =&gt; () -&gt; Producer (MaybeP p) Event IO r commands () = forever $ do s &lt;- lift getLine case s of "potion" -&gt; respond (Regenerate 20) "quit" -&gt; mzero _ -&gt; lift $ putStrLn "Unrecognized command" gameState :: (Proxy p) =&gt; () -&gt; Consumer (StateP Health p) Event IO r gameState () = forever $ do e &lt;- request () case e of Regenerate n -&gt; modify (+ n) Damage n -&gt; modify (subtract n) health &lt;- get lift $ putStrLn $ "Health = " ++ show health main = do (input, output) &lt;- spawn Unbounded forkIO $ runProxy $ evalStateK 0 $ recvS output &gt;-&gt; gameState forM_ [0, 10, 30, 60] $ \delay -&gt; forkIO $ do threadDelay (delay * 1000000) runProxy $ acidRain &gt;-&gt; sendD input runProxy $ runMaybeK $ commands &gt;-&gt; sendD input The idea is simple. For every concurrent behavior in the game you fork a pipeline and use ordinary stream processing to work with it. You communicate betwee pipelines using the `pipes-stm` convenience functions. It's very easy to dynamically add new behavior this way. In fact, it greatly resembles the actor model for concurrent programming where the communication channels resemble mailboxes, and each session processes the mailbox as a stream. Any time you need a new behavior you just fork a new pipeline and communicate using thesse mailboxes. `pipes-stm` is already ready to be released. The only reason I haven't done so already is that Eric Jones first started the library and I built on his initial design. I've been submitting pull requests and e-mailing him lately but he doesn't respond, and I don't want to release the library without making a good faith effort to get his approval since he did begin it. That's the only reason it's not on Hackage today. If you download pipes-stm from Github and compile and run the above example, it will spawn acid rains at predefined times that last for three seconds and slowly drain health. You can type "potion" to raise your health or "quit" when you are done. All of these processes generate events which in this example get fed to a master game state process which processes the events and reports on your current health after each update. I'd show even cooler FRP tricks you can do with pipes using only `pipes-stm`, but that's the quickest thing I can put together while stealing a few minutes from work.
Righto. The problem is, so matter how much of IO we can develop semantics for, `main` will still live in the part we don't understand; so we'll never be immune to the sort of complaints Oleg is making. The only way to fully address the complaint is if we can somehow prove that there exist referentially transparent windows into otherwise referentially opaque contexts, and that we can locate them. This could be marginally more tractable than giving semantics to the world, but only marginally. Which is not to say I'm opposed to the idea of giving semantics to as much of IO as we can manage. Even if we can never clarify everything, that's no excuse for not clarifying the things we can. Haskell is so much nicer than flagrantly impure languages in large part because we refused to give in to that impossibility.
This looks promising, I'm going to toy around with it and see what I can come up with -- I haven't used `Pipes` yet, but I'm glad that I now have a reason to play around with it. Thanks again for another detailed response!
Thanks, fixed!
Your example sounds like a perfect fit for FRP. Whether you'll want to use FRP in the long-run once your game increases in scope...I don't know. I would take a look into Yampa first because there were a number of games implemented with it and papers available that explain them. Your game loop could be reactimate, you could generate events from the keyboard and from within the arrows that define the entities' behaviors, and you could use switches to alter their states.
great! this looks very similar to my approach (coroutine-object) to underlying event processing in hoodle. I was thinking to replace my own coroutine by pipes soon, but found that suspend/resume in pipes was not exactly replaceable for my purpose yet. Since this is clearly more similar and address basically the same problem, I will look into pipes-stm library and give it a try for replacing my coroutine-object library completely. 
Do you know the definition of covariant functors? It is in the standard prelude, defined as follows: &gt; class Functor f where &gt; fmap :: (a -&gt; b) -&gt; f a -&gt; f b The difference between covariant and contravariant functors is just that contravariant ones reverse the direction of functions.
This example demonstrates that in case performance is the first priority, you can still use Haskell. In other words, you just need to change 1% of codes (where the bottleneck is) to ugly but fast implementation instead of wrapping C codes or change the language. I think this is important.
Thank you for a great question! I think I'll even reuse this for the `pipes-stm` tutorial. If you have any more questions feel free to ask!
That was surprisingly comprehensible.
I think the problem you are facing is that there is a piece of software that is a singleton in your architecture. Take a step back and you see that this is an architecture issue that should be solved at a higher level. You can potentially make your database server distributed, and in fact almost all modern storage systems are distributed to avoid a number of other catastropic events. Electricity outage, software upgrades, or DoS are all availability and scalability issues that someone somewhere has to deal with. So even if the database server could somehow deal with OOM it wouldn't solve the other issues. So instead, don't worry about this issue and try to make a collection of haskell processes robust, not a single process. OTOH, there are cases where you want to set effective limits to how failure of a component can affect other parts of the system. The OS can do that, and as noted elsewhere ezyang has some GHC-internal machinery for this as well.
That explanation was wherev all the confusion came from. 
I would be very annoyed if the `main` context was so referentially transparent that I could no longer do benchmarking. :)
That's not an `f` that makes such an observation. That's an `f` that causes different things to happen. I asked for a pure `f` that actually observes the difference and behaves differently. Once you start saying that RT should preserve resource usage you are on a very slippery slope. That means that it also have to preserve memory resource usage. Is that really what you want? It's not what I want.
Good catch, thanks! A fix for this will be on the site within the next few days.
Me too. Since I discovered this feature on my iPod, listening to TED talks without double speed is like listening in slow motion. For those wondering how to do this: $ wget -q 'http://pdl.vimeocdn.com/78409/730/156821234.mp4?token=1365499949_4efff56fc1c9c8450ff5438057ecf02d' $ mplayer -speed 1.5 -af scaletempo 156821234.mp4?token=1365499949_4efff56fc1c9c8450ff5438057ecf02d Adjust speed as you prefer.
Where is the 1.5x/2.0x speed up feature in YouTube?
Deleted because I answered the wrong comment.
Now that I read the rest of the thread I think that maybe *I* was confused. I thought you didn't understand what the definition of contravariant meant but instead now I think you did understand the definition but couldn't imagine any examples and not only that but actually may have thought it was impossible to have such a thing.
/me cries
I'm interested in this topic, but there's no way I'm watching a tech talk video or whatever this is. Slides or transcript, please. Internet video tip: if it's more than 30 seconds long and doesn't include a dog doing something funny, you've lost 99% of your audience before you start.
Hopefully we'll have the slides up soon as well.
Even those in bed with a broken leg?
I've always wondered if their are any patents on this....
I have a greater aversion to ads, but I agree that would be a sweet feature to have.
Let's just say the relevance of my advice only increases when an audience member is on drugs :-)
I seriously have not been aware that there're any ads on youtube, because I'm using since ever the firefox addon 'Adblock Plus', which filters even these ads. 
Did you see where someone mined twitter tweets for tweets in iamib pentameter? Maybe write, in Haskell, a program to mine tweets for Haikus? 
Has anyone actually watched the whole thing? I skimmed as best I could, but I didn't find any part where there's actually any talk about infrastructure at all? The bulk of the talk was "we need an IDE, so we made school of haskell instead", and then at the end it mentions some infrastructure related things that are "on the roadmap"?
why are free monads special here?
One actual important infrastructure that Haskell misses is dynamic linking. While ghc, of course, builds shared libraries, these do not, and cannot, behave as proper shared libraries at least due to cross-library inlining. This voids client-side use of haskell: you will never be able to write a Haskell desktop or embedded environment, or operating system, or plugin-based IDE, or use haskell as a scripting language for your app. All these require either working dynamic linking or a fast interpreter/JIT. This also puts strain on haskell development, since everything has to be rebuild every time an upstream library updates, and discourages said upstream packages from releasing often. Some packages can't even be updated without updating ghc, which halts their development.
The problem is also that, many frameworks don't manage session by default (for example compojure apparently). Apparently sessions cost a lot. So the actual benchmark has a strong bias toward framework without session management by default. So it would seems a good idea to benchmark with and without session for all framework as the majority of web applications need session management. 
Why do you need free monads - "regular" DSLs are perfectly suited.
I think FRP would be a really good way to go. Here are some quick examples from [Elm](http://elm-lang.org/) to get a feel for how your game might look if you use FRP: * [Mario](http://elm-lang.org/edit/examples/Intermediate/Mario.elm) * [Zelda](http://elm-lang.org/edit/examples/Intermediate/Walk.elm) In both examples, you use an infinite stream of input events to update the game (these are called Signals in Elm). In these cases, the inputs are all from keyboard input and time input. It sounds like you'd want to use a similar set up in which some [Random](http://elm-lang.org/docs/Signal/Random.elm) action is taken after a given [Time](http://elm-lang.org/docs/Signal/Time.elm) interval. One nice benefit of Elm is that it is really easy to do graphics and you can start [programming online](http://elm-lang.org/try) without downloading or setting anything up. There are more [examples](http://elm-lang.org/Examples.elm) here if you are interested in this approach.
I would also be very interested in seeing this, or even just the code.
Goldman-Sachs has some patents related to this, I think. But I don't know any details. Patents they got after the paper was published. 
Here's the slides: http://gbaz.github.io/slides/FPCompleteTalk.pdf
I made a simple static pdf website builder for hoodle using excellent pdf-toolbox (https://github.com/Yuras/pdf-toolbox) I claim that this is the simplest way of making a website ;-) hoodle-publish + hakyll is a very good combination, too. 
&gt; For the record (ahem), I'm dubious that real world programs benefit from this sort of structural subtyping (static or no). Years of OO programming have left many with the feeling that structural inheritance was a mistake from the start. What's your take on relational databases? Because the relational algebra can be seen as a functional language over relations parametrized by structurally-subtyped record types: type Relation a -- `a` is the type of the tuples Now, SQL `SELECT` can be seen as a sort of `map`, `WHERE` can be seen as a sort of `filter`: instance Functor Relation where -- fmap :: (a -&gt; b) -&gt; Relation a -&gt; Relation b fmap = ... instance MonadPlus Relation where ... -- now we have mfilter :: (a -&gt; Bool) -&gt; Relation a -&gt; Relation a So how do joins work? Well, you can think of Cartesian joins as being an `Applicative`: instance Applicative Relation where pure :: a -&gt; Relation a rf &lt;*&gt; ra :: Relation (a -&gt; b) -&gt; Relation a -&gt; Relation b Now we can do a very simple kind of Cartesian join with `liftA2 (,)`—but multiple joins become syntactically very annoying if you're using pairs of pairs of pairs as your tuple type. What would be more convenient is to get the union of the fields of the types `a` and `b`—which is a kind of structural subtyping relationship. Another related example is the [natural join operation](http://en.wikipedia.org/wiki/Relational_algebra#Natural_join_.28.E2.8B.88.29): &gt; The result of the natural join is the set of all combinations of tuples in R and S that are equal on their common attribute names. Schematically it would be something like this, using `Combine` to mean an appropriate type function: natJoin :: Relation a -&gt; Relation b -&gt; Relation (Combine a b) The type `Combine a b` is a structural subtype of both `a` and `b`.
I would be inclined to agree with you but... Wicket and tapestry does sessions and still manages to post results way higher than yesod. Finagle (whose results haven't been posted yet but going from my unofficial run of their bench) also does sessions and is almost as fast as Netty. If you are going to do it, at least publish it as a stripped framework like the other frameworks that have done so for speed. Edit: My mistake. Finagle doesn't do sessions but only cookies out the box. My memory is apparently failing me. I'm leaving my post as is to point out my mistake. Regardless, my point still stands. Double edit: :-(
Here's an example implementation that Well-Typed did for a client https://github.com/netrium/Netrium As dons says, you don't need anything fancy, an ordinary shallow or deep embedding works just fine.
Excited by "Next Steps"
Yes, you can very easily implement this using a free monad, and it even produces a logical behavior! {-# LANGUAGE DeriveFunctor #-} import Control.Monad import Control.Monad.Free data Currency = USD | GBP | EUR | ZAR | KYD | CHF deriving (Eq, Show) type Date = (CalendarTime, TimeStep) type TimeStep = Int type CalendarTime = () newtype PR a = PR { unPr :: [RV a] } deriving Show type RV a = [a] newtype Obs a = Obs (Date -&gt; PR a) mkDate :: TimeStep -&gt; Date mkDate s = ((),s) time0 :: Date time0 = mkDate 0 instance Show a =&gt; Show (Obs a) where show (Obs o) = let (PR (rv:_)) = o time0 in "(Obs " ++ show rv ++ ")" data ContractF x = Zero | One Currency | Give x | And x x | Or x x | Cond (Obs Bool) x x | Scale (Obs Double) x | When (Obs Bool) x | Anytime (Obs Bool) x | Until (Obs Bool) x deriving (Show, Functor) type Contract = Free ContractF zero :: Contract a zero = liftF Zero one :: Currency -&gt; Contract a one currency = liftF (One currency) give :: Contract () give = liftF (Give ()) cAnd :: Contract Bool cAnd = liftF (And False True) cOr :: Contract Bool cOr = liftF (Or False True) cond :: Obs Bool -&gt; Contract Bool cond obs = liftF (Cond obs False True) scale :: Obs Double -&gt; Contract () scale obs = liftF (Scale obs ()) cWhen :: Obs Bool -&gt; Contract () cWhen obs = liftF (When obs ()) anytime :: Obs Bool -&gt; Contract () anytime obs = liftF (Anytime obs ()) cUntil :: Obs Bool -&gt; Contract () cUntil obs = liftF (Until obs ()) Then you can assemble derived primitives using `do` notation. The `Bool` that bifurcating contracts return corresponds to which branch it took (`False` if you are currently observing the left branch and `True` if you are currently observing the right branch): andGive :: Contract Bool andGive = do isRightBranch &lt;- cAnd when isRightBranch give return isRightBranch This then compiles to the correct pure value, as if we had written the contract by hand: &gt;&gt;&gt; andGive Free (And (Pure False) (Free (Give (Pure True)))) Since it is a monad, we can take advantage of the combinators in `Control.Monad`, too: &gt;&gt;&gt; replicateM_ 3 give Free (Give (Free (Give (Free (Give (Pure ())))))) Now imagine writing a combinator equivalent to `replicateM_` for the `Contract` implementation given in the linked article. Not fun! Don asked why you need free monad when a regular DSL suffices. The answer is that not all of us can afford to hire Don to write deep DSLs for us. Don is expensive, whereas a free monad is free!
Can you help me understanding the semantics of that free monad vs. the combinatorial approach from the original paper? Your last example, slightly simplified, is: do give give give So (regardless of any free monad you might be using behind the scenes to implement this monadic API) what would be the semantics of that contract, in terms of the combinatorial contracts? 
The semantics of the free monad are entirely in the interpreter. The interpreter can choose to do whatever it wants when it encounters a `give`, including firing missiles or ignoring it. The free monad is purely syntactic.
Free monads -- you get what you pay for!
Thank you very much for this series of lectures. I think they're a great resource for the Haskell community.
Why do you want a monadic interface?
Very impressive low level pdf-fu! I was not aware that cross-document links are possible.
I pretty much agree with that. Sometime we care about the memory resource, if we do, then don't use Haskell. Sometimes we care about file handles, if we do, then don't use lazy IO. But sometimes we don't care about either of those resources, and Haskell with lazy IO works fine.
They're just URLs aren't they?
Yes, they are link annotations. I was sure they work only inside the document. I have no idea why I had such the (obviously wrong) impression.
He means maybe you don't need a monad here. Not everything has to be a monad.
Thanks! I kept my promise to try your library after a new version came out ;-) Since hoodle now treats pdf as the first class element, your library will become indispensible for me. It would be even greater for me if annotation is implemented as a high level construct in pdf-toolbox-document some time. (I saw it is already in your todolist. ) 
Thanks! Haskell is definitely a winner when maintaining bigger projects. Type system helps me a lot for finding lots of stupid potential bugs. When it compiles, it just works. That's amazing. Another important factor 'now' is expanding scale of good quality haskell libraries. I found in recent two or three years, haskell library space is getting filled very rapidly. And if a library is there, it is usually very good in many cases. Since most of them are open-source and codes are relatively very short, it is even easier to look into the detail and extend some necessary features and communicate with the library authors. That's an enabling factor for fast development. 
That is one of my favorite features of the Haskell ecosystem, if a library exists it is usually relatively high quality. I can actually rely on other people's code without constantly looking for bugs in there. Even relatively large Haskell projects are surprisingly small too which helps keeping the code flexible when requirements change too.
That's slightly misleading. With a free monad you get exactly one thing: a `Monad` instance. This means that you get `do` notation for free, you can layer on monad transformers for free, and you get `Control.Monad` combinators for free, which can be valuable for many applications, if not necessarily this one.
Well, I was just answering his question. It really depends on whether or not you want to program using `do` notation or `Control.Monad` combinators, or monad transformers. If you do, then you want a monadic interface. If you don't, then you don't.
Some kinds of orders can also be seen as monoids.
Yeah. I proposed adding `Maximum` and `Minimum` to `Data.Monoid`, but at the time we couldn't decide whether to use semigroups or `Maybe`s to deal with unbounded ranges.
This is true. This is why I frown at the `Default` type-class, which to me feels much too ad-hoc. Every instance I've seen is also a perfectly good `Monoid`, and that actually gives us some usable structure. 
Presenting the monad laws in terms of join is really unintuitive to me. The most intuitive version to me is the version that shows that the monad laws are just category laws for the kleisli category.
I usually like defining both `Default` and `Monoid` since even if `Default` is almost always actually a `Monoid` it makes a lot more semantic sense in locations where all you need is to be able to produce an element from no input. `Default` seems to show up all the time in configuration. As usual, it'd be nice is `Default` was a superclass of `Monoid`. And maybe was called `Pointed`.
I didn't even see that tekmo wrote that. I should start reading stuff more carefully.
Yes, unsafeInterleaveST really is unsafe. But unsafeInterleaveIO does not allow doing the same trick in a pure setting. Still, there's a reason it has the name it has. :)
Alright, alright. I'll change it.
Free monads help with creating DSLs where the AST can be extended without modifying existing code. Data type a la carte is the example I am thinking of. There are probably other uses of free monads that I am unaware of. 
This thread is a continuation of [this one](http://www.reddit.com/r/haskell/comments/1bsitm/lazy_io_breaks_equational_reasoning/).
I'm not sure what the point of the example is with regards to `IO`. Implementation isn't the only thing that matters. The reason that `unsafeInterleaveST` is significantly worse than its `IO` counterpart is what each of its types are designed to represent. `ST` is for representing computations with local mutation, such that `\a -&gt; forall s. ST s a` is actually the identity monad. This is why `runST` is justified. `unsafeInterleaveST` allows you to write nondeterministic things in `ST`, and such nondeterminism breaks the specification that led us to declaring `runST` acceptable. But, since the whole point of `ST` is to be able to `runST` at the end, it makes sense to lay the blame on `unsafeInterleaveST`. `IO` on the other hand, has all sorts of nondeterminism (etc.) thrown into it regardless of `unsafeInterleaveIO`, which is why it's not sensible to have a 'safe' `IO a -&gt; a`. So interleaving isn't breaking any contracts that weren't already broken a dozen times over. There are a bunch of other `IO` things that can't be added to `ST` without breaking referential transparency for this same reason (`forkST`, for instance). But these, I suppose, are fine, despite the fact that their implementation in `ST` would probably look about the same as for `IO` (since implementation-wise, in GHC, `IO` is approximately `ST RealWorld`).
&gt; And unsafeInterleaveST is really unsafe -- not just mildly or somewhat or vaguely unsafe. Or if you prefer, there's a difference between "this could theoretically break your code under some circumstances, which you probably won't encounter" and "this will almost certainly break your code". Compare and contrast with reddit post about running out of memory; in theory, at any moment you might run out of memory and there's not much you can do to handle it gracefully if that happens. In practice, this is not something you worry about on a line-by-line basis. Obviously, memory problems can occur, it's just a rather more vague threat for most code.
The latter (a "fast" or "fast enough" interpreter) is looking increasingly feasible, with the progress on haskell-suite. I think we're only a few years away from a decent, lightweight embedded interpreter. (With a dedicated dev it would take far less time, but work progresses based on interest and availability).
That's already in a library somewhere. I feel like `Pointed` as a pointed set stands out more to me, though.
If you call a function that is unknown except for its output type and that it has side effects, then there is a small but finite chance the side effects will include the destruction of humanity, which is infinitely bad. An infinitely bad outcome times a small but finite probability is still infinitely bad. Therefore, you should not write functions with side effects, just to be safe.
I've heard of [Pascal's wager](http://en.wikipedia.org/wiki/Pascal%27s_wager), but not Haskell's wager. What is it?
I might be insterested. Just to hang out as I'm not very proficient with Haskell yet.
Thanks! :-)
It isn't a valid contract. It's a contract expression in one free variable. Only values of type `Contract Void`, in the above formulation, are (correspond to) sensible contracts (meaning calling that type 'Contract' is something of an over-sell). Why would you want this? No idea.
Wow, great, interested! I can bring some friends probably. I suggest organizing same place (mailing, G+ community, IRC, whatever) to exchange info about group and potential meetings. Łukasz PS. I'm not that proficient aswell.
Upvote for the underappreciated genius Emmy Noether.
Stream fusion merges loops--it does not flatten them (that stream fusion does not optimize `concatMap` is the reason it is not used in the Base list library). I don't know enough about polyhedral optimization, but the code emited by stream fusion ends up looking like nesteded for loops, so I really don't understand why you can't do the exact same optimization then. 
Now this is genuinely bothersome from the perspective of a developer and consumer of Haskell libraries. I gave Lazy IO a pass in the previous thread (and argued with tekmo about equational reasoning, something I was quite brave to do) but the idea that allegedly pure code can cause observable effects in this way is a little scary. I am curious though, there was a post from the previous thread that seemed to suggest the problem with lazy IO lay in that it wasn't lazy enough, that there was a mixing of strengths of laziness and this somehow resulted in visible effects outside of IO. Is this also true of ST? I quote **codensity** here: &gt; Note that oleg's 'main' also seems to fall flat if we skip `System.IO` and introduce our own 'visibly genuinely lazy io' . The essence of so-called lazy IO resides in the connection between `hGetChar` and `hGetContents` on the one hand and `hPutChar` and `hPutStr` on the other. So if e.g. we keep `hGetChar` but redefine the essential concepts as one might in an elementary tutorial: &gt; &gt; hGetContents h = catch loop rest where &gt; loop = hGetChar h &gt;&gt;= \c -&gt; fmap (c:) loop &gt; rest = const (return "") :: SomeException -&gt; IO String &gt; &gt; hPutStr h [] = return () &gt; hPutStr h (x:xs) = hPutChar h x &gt;&gt; hPutStr h xs &gt; hPutStrLn h xs = hPutStr h xs &gt;&gt; hPutChar h '\n' &gt; &gt; then, with this genuinely lazy IO, oleg's original example yields: &gt; &gt; Demonstrating that LazyIO breaks equational reasoning &gt; cex1 &gt; producer start &gt; producer end &gt; 0 &gt; cex2, a provably equal expression prints: &gt; producer start &gt; producer end &gt; 0 &gt; &gt; If we use 'lazy length' we get `Z`. (The difference from the result above is a standard sort of `counter-intuitiveness' of lazy io). &gt; &gt; The real result of oleg's researches seems to be that 'kinda-lazy kinda-strict io breaks equational reasoning', which one might have expected. But who knows, he's presumably always right. It appears that this example is so simple that this cannot be the case here. It seems to me that one solution is that there should be a "strictness barrier" between IO and ST and all other computation that involves `unsafeInterleave..`. That a consequence of using ST code is that all inputs must be in normal form and that all outputs be in normal form. Does that check with anyone else's intuition here?
One hundred percent read that in Jerry's voice.
OK, the G+ community is here: https://plus.google.com/u/0/communities/103183708602453146804 Will edit it to look better in spare time ;)
Don't worry, me neither ;)
Learn you a pun for great good!
If would be cool if we could derive or typeably get Monoid instances for records whose fields are Monoid instances.
Your `count` function looks it could be rewritten in terms of a `fold`, which may be faster than repeatedly doing `head` and `tail`; just carry the `res` with you as you go.
Stating the obvious - have you compiled with -O2 option
the problem I have with a fold is that I want to terminate early (i.e. not process the whole thing but just until I say I'm done). 
&gt; And unsafeInterleaveST is really unsafe -- not just mildly or somewhat or vaguely unsafe. &gt;&gt; Or if you prefer, there's a difference between "this could theoretically break your code under some circumstances, which you probably won't encounter" and "this will almost certainly break your code". So then that can't be what Oleg meant. Because lazy IO is in the "theoretically" category, *not* the "almost certainly" category. The mental effort I need to exert to avoid Oleg's contrived cases in everyday programming is near zero. Of course, I am talking about using lazy IO responsibly. You do have to know how to recognize potential trouble - complex interleaving of IO operations, lots of open files, etc. Nowadays, that's really all you have to know. At the first sign of potential trouble, you use conduits or whatever. It's not *that* much harder than lazy IO anymore. But for ordinary programs without any complex IO requirements, just use lazy IO.
It sounds like you want an apomorphism then. Have a look at origami programming [1] and recursion schemes by example [2] [1] http://www.cs.ox.ac.uk/jeremy.gibbons/publications/origami.pdf [2] https://github.com/willtim/recursion-schemes/raw/master/slides-final.pdf
sure, compiled with -O2 using ghc 7.4.2 ...also tried to compile using the llvm backend, but that didn't show any improvements
Use a better algorithm. You only need to do 1 pass over the string to find the answer. When iterating over the string, keep track of two numbers n and k, and two characters a and b. Supposing we are at index i: * `n`: the number of identical characters `a` before index `i`. * `k`: the number of at most two different characters `a` and `b` before `i` Example. If we are at the indicated position in the string "caabbaa|a", then the values are: a = 'a', b = 'b', n = 2, k = 6. Now when you see a new character `x`, you can update these 4 values based on `x`. Namely: step x (n,k,a,b) = | x==a = (n+1,k+1,a,b) | x==b = (1,k+1,b,a) | otherwise = (1,n+1,x,a) Fold this operation over the string, and additionally keep track of the longest sequence seen so far (i.e. the maximum k with associated index i). Voilà, you have an efficient solution. Edit: with some extra ingenuity you can make the operation associative, with would allow you to do a parallel reduce.
If you haven't already, I highly recommend you read [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html). It shows some practical examples for very different monads to help exercise your brain and show how they solve diverse problems.
Also, if you do this using the `foldl` bytestring primitive then you can take advantage of high-performance traversals of the bytestring internal array.
I can believe there's a single stream representation that's asymptotically optimal - within a constant factor of optimal for all cases - but constant factors can be important. "Twice as fast" *is* a constant factor. A single stream representation that's optimal for all cases? That's the specialised-for-everything fallacy. For some things you get lucky, either because there's only one case to consider or because all the cases are equivalent in some key sense - after all, the tendency is to define the problem that way - but the fact is that real-world hardware defines special cases for its own optimisations and you either deal with that or you won't get the best from the hardware. Actually, one "problem" I see with generalised stream fusion is that there can never be enough different stream representations to ensure that everything that *could* be optimised by some extension of stream fusion will be implemented near-optimally. The reason - you can potentially extend stream fusion to cover all computation. If an optimiser could deterministically transform all possible computations into near-optimal implementations, it would effectively have solved the halting problem. This isn't the first time that an optimisation has resorted to try-many-possibilities-and-keep-the-best, and it won't be the last. And I predict it won't be long before someone finds a special case that can't be optimised well without adding another stream representation. In reality, of course, that's not really a problem - it's just the decades-old story of optimisation progressing to a new chapter. There will always be special cases where programmer optimisation is needed, and where lower-level tools (even a lower-level language) may be needed to handle that. As long as that set of special cases continues to shrink, compiler optimisation is still making useful progress. 
A probability monad is always fun. distFromList :: [(a,Rational)] -&gt; Dist a uniformDist :: [a] -&gt; Dist a normalizeDist :: Ord a =&gt; Dist a -&gt; Dist a die6 :: Dist Int die6 = uniformDist [1..6] twoDie6 :: Dist Int twoDie6 = do d1 &lt;- d6 d2 &lt;- d6 return $ d1 + d2 show $ normalizeDist $ twoDie6 "distFromList [(2,1 % 36),(3,1 % 18),(4,1 % 12),(5,1 % 9),(6,5 % 36),(7,1 % 6),(8,5 % 36),(9,1 % 9),(10,1 % 12),(11,1 % 18),(12,1 % 36)]"
Thank you! I appreciate the reference.
Can't we? I'm pretty sure that's being done with defaults in AlphaHeavy's protobuf library.
Would it be possible to post the C++ solution as well? It'd be nice to have a point of reference for our own systems.
It'd be nice if the `module ... where` bit could be detected automatically.
have you built a lisp interpreter yet? start with a monadic parser, then build a monadic interpreter, that will get you through error, reader, writer, state, &amp; cont.
http://blog.tmorris.net/posts/20-intermediate-haskell-exercises/
Without looking at any definitions except the monad typeclass definition, reimplement the Maybe, Either, List, Reader, and State monads. I have to admit I had to consult the definition for the last one, but I saw it much more clearly once I'd been struggling with it for an hour. There's a bit of a trick to it that turns out to be an important functional technique.
I wrote a blog post about some basic monads and their uses a little while ago. It's not the best resource, but it's a resource, and I found that just reading and watching stuff about monads, and doing random exercises trying to use all of the monads I knew about was helpful. The more material I used, the deeper I understood the concept. [Here's the article I wrote, if you're interested.](http://5outh.blogspot.com/2013/01/introductory-monads.html) Good luck! 
This is awesome. A few additions might be nice though: - The "about" page could talk about how the core is generated, so people can reproduce it without looking at the source. (`src/GHCCore.hs` says it's compiled with `-ddump-simpl -dsuppress-idinfo -dsuppress-coercions -dsuppress-type-applications -dsuppress-uniques -dsuppress-module-prefixes`.) - Docs would be nice, as this seems to be a decent example of a website built with Happstack and Blaze. I couldn't spot a single comment in the source files. (Also, think of yourself in 3 months) :-)
There was some work a while back on database-backed Haskell, but I can't for the life of me find it anymore. It might be useful for your use case, so if someone else remembers what I'm talking about and knows how to find it, that'd be helpful!
When you strip out the constructors and uncurry the resulting functions, State actually turns out to be defined by (&lt;=&lt;) = (.), which is really cool. 
For me, this example drove home how powerful the monadic concept is. You just have to implement a simple imperative routine determining an individual outcome, and suddenly you have a whole distribution of them.
Yes :) The end result should be really good. It's hard to predict such things but probably GHC can eliminate the tuple construction, so it will end up being a tight loop with all values in registers, so it will be just as fast as the equivalent algorithm written in C++.
But for what you're doing an applicative functor is perfectly sufficient. It's the influencing of the further computation by analysing values that's the sauce that's special to monads: twoDie :: (Num b, Applicative f) =&gt; f b -&gt; f b -&gt; f b twoDie x y = (+) &lt;$&gt; x &lt;*&gt; y twoDie6 = twoDie d6 d6 Contrast that with: die6Die6 = do count &lt;- d6 rolls &lt;- replicateM count d6 return $ sum rolls
Hmmm, that'd be very interesting. It'd certainly be possible, you'd just have to save the relevant bits of RAM to disk, then figure out a way to copy those back to the right locations in RAM and tell the executable to use it. Sounds similar to pickling in Python, but I bet it'd be a lot harder in haskell due to the nature of the language.
&gt; Is there a way of saving the current state of an entire Haskell program (along with all thunks, possibly infinite data structures, etc) to a file, and resume it later? I can't absolutely say no but, if it *is* possible, I will be seriously shocked. For a start, what does "all thunks" mean? That's an implementation detail - specific to a particular compiler. Worse, thunks can be optimised away, so it's probably dependent on optimisation settings and the exact version of the compiler. So although it's useful at times to think about the state of the computation in Haskell, that's a pretty vague thing. Your only realistic option is to make the state you need to save explicit in your program so your program can serialize it for itself. 
this isn't intended to be made public yet, it has security issues. i don't know who posted this but you'll have to wait my friend. I've cut down the app, for now.
Could you comment on the applicability of this approach to implementing GADTs? And if it is applicable, could you comment on how it might compare to how GADTs are currently implemented in GHC?
And return = id. Whoa. That had never occurred to me at all!
So `runST $ foo bar` would not work, but `runST (foo bar)` would? What about with an annotation on `foo bar`? Or are you saying you would not be able to define the function `runST` at all?
wow
TimTravel, are you rejecting the solution of: &gt;Your only realistic option is to make the state you need to save explicit in your program so your program can serialize it for itself. I remember a blog post or discussion on this a while back and it is the solution used by industrial users of Haskell that is the comment I remember donsbot make a while back at least. I do not think it would be a trivial addition to GHC to suppor the features you are looking for.
This sounds like what you're looking for: http://hackage.haskell.org/package/Workflow although I don't have experience using it.
There's this, but I don't think it relates: http://db.inf.uni-tuebingen.de/research/dsh
Do you mean [DSH](http://db.inf.uni-tuebingen.de/research/dsh)? &gt;Database-Supported Haskell, DSH for short, is a Haskell library for database-supported program execution. Using the DSH library, a relational database management system (RDBMS) can be used as a coprocessor for the Haskell programming language, especially for those program fragments that carry out data-intensive and data-parallel computations. Rather than embedding a relational language into Haskell, DSH turns idiomatic Haskell programs into SQL queries. I've read their paper, and have even talked to one of the guys, and it isn't really what's being discussed here -- it's more about offloading some list comprehensions etc. to a DB engine so that you don't have to marshall the input from the DB into the Haskell runtime and then back.
Clean had a pretty cool feature which did something basically like serializing the running program to a file, but I don't know that it ever got implemented in a usable way for Haskell. (There were some network Haskell implementations which would serialize a function, send it over the network to another node, and the node would continue running the function, but those involved hacking on GHC and so would be many years bitrotten by now.)
IIRC it really is just about the type inference (meaning a declaration should fix any problems). For example, the Core language Haskell compiles down to features second order polymorphism but with explicit typing on all variables.
It doesn't look like there's a way that would allow this in their declarative specification. We have: ($) :: forall a b. (a -&gt; b) -&gt; a -&gt; b runST :: forall r. (forall s. ST s r) -&gt; r And we want to check the application `($) runST` There's one rule for application, and it says: G |- e1 =&gt; A G |- A * e2 =&gt;=&gt; C --------------------------------- G |- e1 e2 =&gt; C `A` is the type for `($)`. So we need to check that application judgment. There are two rules that pertain to that. We're trying to get something like: ... |- (forall a b. (a -&gt; b) -&gt; a -&gt; b) * runST =&gt;=&gt; ... The only thing we can do with that is instantiate `a` and `b` with _monotypes_. That gives us: ... |- ((σ -&gt; τ) -&gt; σ -&gt; τ) * runST =&gt;=&gt; ... and then the only way to progress is to check: ... |- runST &lt;= σ -&gt; τ But, `runST :: (forall s. ST s r) -&gt; r`. So we get `τ = r`, but there is no monotype `σ` that works, because runST's argument isn't a monotype. The same type of thing happens when you try to push the wanted type into `($)`. The rules for subtyping don't accomodate that. They have to not accomodate that, because as the paper says, it's undecidable in general. The only way to make `runST $ ...` work in a predicative system is to define a different `($)` with a sufficiently polymorphic type. But it probably won't work in a lot of the boring cases you usually want.
Ah, now I see that you were talking about remaining in an impredicative system. If all you care about is the regular higher rank polymorphism extentions in GHC then they turn on "impredicative mode" and its perfectly fine to instantiate type variables to polytypes (therefore making `runST $` a matter of type inferencing / manual type declarations)
Oh, I misremembered what they were trying to do with it then. Thanks! :)
Without more context, I don't understand the purpose of those exercises. E.g. class Fluffy f where furry :: (a -&gt; b) -&gt; f a -&gt; f b -- Exercise 1 -- Relative Difficulty: 1 instance Fluffy [] where furry = error "todo" What's Fluffy about? Does it have a contract? How about this solution? furry f [] = [] furry f (x:xs) = [f x] 
I may tend to agree (I think an equally important point is that it is "impredicative" in allowing instantiation by polymorphic types; neelk, how natural would feel an extension of your system with annotations for polymorphic applications?), but besides the generic argument that it's good to understand the rest of the design space as well, I think the argument about complexity of implementation should not be taken lightly. Yakobowski's graphic formalization of MLF is helpful and makes understanding easier, but it's still difficult to implement (and also has some surprising edge behaviors when you're used to more syntactic systems). I would be interested in how the complexity of the bidir implementation compares to the one of HMF.
Generally for these exercises, you are going for the "most elegant" answer. A simple heuristic for elegance is that your answer uses every bit of data passed in at least once, and as close to once as possible. Another good heuristic for higher order functions is that they behave "nicely" (again, subjective) with `id` as an argument. 
Serializing thunks shouldn't be hard given that serializing closures is possible. Distributed haskell does serialized closures, but the use-case for serialized thunks isn't there. I don't think it is unreasonable to get serialized thunks by building on what is done in distributed haskell.
Where?
It's very easy to extend the system with annotations for polymorphic applications. However, this makes it more complicated to explain which program transformations preserve typability. Basically, you get to pick any two of: (a) impredicativity, (b) preservation of typability under eta, and (c) the usual type language of System F. We chose (b) and (c) in this paper, MLF picks (a) and (b), and HMF/FPH pick (a) and (c). Joshua and I were interested in working this out because the applications of bidirectionality have vastly outpaced its proof-theoretic foundations -- it's used all over the place, in all kinds of fancy type systems, but even the story of how bidirectionality and polymorphism interact has not been fully laid out. (Simon Peyton-Jones, Dimitrios Vytiniotis, Stephanie Weirich, and Mark Shields have a JFP paper on this, but they don't prove their implementation matches their spec. Even so, they remain well ahead of the pack, since they have a spec!)
Since you're there, another related remark (thanks for your answer!). I'm never sure what I'm being sold when I see "predicative System F"; in your case, and it is also quite common in the litterature, it means that type application only uses monomorphic types, but there is also a "stratified System F" where abstractions are marked with a maximal rank at which they may be instantiated. It is more powerful that what you have, easier to work with meta-theoretically than impredicate System F, but a kind of pain to program in imho (like universe-stratified theories without inference or level polymorphism). Do you think you could extend bidirectional techniques towards this setting? I'm under the impression that it is not the case, but I'm not sure. I even don't remember whether the decidability problem of System F-eta has been solved (positively or negatively) in the stratified case.
&gt; Because lazy IO is in the "theoretically" category, not the "almost certainly" category. Lazy IO bites many beginners immediately, when they read a file, process the contents, and then try to write to the same file again.
You mean something like http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours?
I don't have any experience with category theory and certainly not with categorical type theory. I had to look up some terms, like category and monoidal category. Unfortunately wikipedia is not very helpful. It says: "A monoidal category is a category C equipped with a bifunctor (+) : C x C -&gt; C" When you go through to the page on categories, it says that a category a tuple `(ob, hom)` of objects and morphisms. If that is so, I find the type given for (+) quite strange. Probably what is meant in the definition of (+) is that C is an element of `ob`? And a monoidal category is always a category in which tuples of those objects are also objects, and where you have an operation to construct a tuple out of a pair of objects (otherwise, how could you use (+)). If that is the case then I find this tutorial quite clear, though it's certainly possible that I think I understood it but really didn't understand it at all :) One question that did come to mind is what this change of terminology buys us? tuples -&gt; products, functions -&gt; morphisms, objects -&gt; sets, bifunctor -&gt; function of two arguments, etc. Why not "interpreting proof derivations as functions", instead of "categorical semantics"? You lose nothing, since you can always use functions `f(x) = a (*) x` and then use function composition instead of custom composition `(*)`? (so like you can interpret a matrix as the function that multiplies by it on the left, then function composition becomes matrix multiplication) In my mind this makes everything much clearer and concrete, and then the paper is almost like writing a functional program. Am I missing something important?
this is neat! I tried a solution using your step function [here](https://gist.github.com/marcmo/5363788). It seems to be correct but I got a space leak somewhere so it's not as fast as it should be...
fixed the space leak...now it runs at 2x the speed of my original version. still 5x slower than the C++ solution...
A few comments: * I think it would be clearer if you included the typing rule 'hyp' for variables. * In the typing rule 'unit' the typing context is empty. Since you don't have a weakening rule to explicitly introduce redundant variables, and the 'mul' rule requires that the two contexts mentioned in the premises are equal and the same as the one in the conclusion, then, just following the rules, it is impossible to form any term that involves 'e' and a variable. Also, even though you haven't explicitly stated the 'hyp' rule, the semantic definition at the bottom of page 2 indicates that the context for this rule has exactly one variable. Again, without weakening and with the current form of the 'mul' rule, this means that you can only form terms that have exactly one variable in. You have three choices: (a) reformulate the 'hyp' and 'unit' rules so that they have an arbitrary context, though this would mean that your semantics would have to be stated in a cartesian category (i.e., a category with finite products) (though see below); (b) change the 'mul' rule so that the two premises take separate contexts, and the conclusion merges them somehow (possibly with an additional exchange rule) (this would make the theory linear); (c) add a weakening rule (this would complicate the categorical semantics because terms+types+contexts would not uniquely determine typing derivations). * I assume that by 'the usual rules for equality', you mean reflexivity, transitivity, symmetry, congruence and substitution instances? For a tutorial, I think it would be clearer if you included the rules explicitly. Also, instead of using variables in the equality rules + an equality substitution rule, it is technically easier to just state the monoid equalities with terms. * Your presentation of the substitution rule is a bit strange. Normally, I would expect to see this rule presented as a lemma (you hint at this by saying it is admissible). Then, in the categorical semantics, it would be *proved* that [[ N[M/x] ]] = [[N]] \circ (id \times [[M]] \times id). The key point is that the interpretation of each of the typing rules is natural in the context. * In Section 3, you state that you require a strict monoidal category to interpret the monoid theory. However, in the clause for interpreting 'mul', you seem to use the standard notation for 'pairing' two morphisms in a category with finite products (or you haven't defined what you mean by &lt;m,n&gt;). Also, using \times is fairly standard for cartesian product, while \otimes is more often used for a general monoidal product. I think you want to state that the category you are working in is a category with chosen finite products and a monoid object. If you want to interpret a linear theory of monoids, then use a (strict) monoidal category, but this means a reformulation of the 'mul' rule. * In Section 4, both your examples are in the category of Sets. This makes the use of category theory seem like a bit of a waste of time: why not just use a more straightforward set-theoretic semantics? Perhaps an example of a semantics not in Set? e.g., topological monoids. * Section 4 could be made sharper by pointing out that you can make a finite product category with a monoid object from the syntax defined in Section 2 (and this category is initial in the (large) category of models (and semantics then becomes a functor)). Though I see you mention this briefly in Section 6. * Section 5 seems a bit sketchy at the moment. I think, for a tutorial, you need to carefully define both what the simply-typed lambda calculus is (especially what the equations are), and what a bicartesian closed category is. The clauses on page 5 use a whole load of undefined symbols. For a tutorial, I'd just drop the coproducts part, just to keep things simple. * Section 6: you say "the objects in the category ... are generated from some base objects ... by some operations that code up the structural properties of the logic". I think "connectives" is more precise than "structural properties" (though obviously there is a close relationship between connectives and their structural properties).
Maybe GHC is not eliminating the tuples? You can also speed it up by giving concrete type signatures instead of type classes (though maybe GHC already optimizes that out, but it's worth trying). You can further simplify the code. Try something like this: step (n,k,a,b) x | x==a = (n+1,k+1,x,b) | x==b = (1,k+1,x,a) | otherwise = (1,n+1,x,a) stepmax (i,s,kmax,imax) x = (_,k_,_)@s' = step s x if k &gt; kmax then (i+1,s',k,i) else (i+1,s',kmax,imax) findLongest s = extractString s $ B.foldl' stepmax (0,(0,0,'a','a'),0,0) s You'll have to change the extractString function to extract from index `imax - kmax` to `imax`. Or maybe this: stepmax (i,n,k,a,b,kmax,imax) x = | x==a = cont (n+1) (k+1) x b | x==b = cont 1 (k+1) x a | otherwise = cont 1 (n+1) x a where cont n k a b = if k &gt; kmax then (i+1,n,k,a,b,k,i) else (i+1,n,k,a,b,kmax,imax) findLongest s = extractString s $ B.foldl' stepmax (0,0,0,'a','a',0,0) s You might need to make the tuple components strict to avoid space leaks. I don't have GHC at hand at the moment, so I can't test it. Perhaps it helps to inspect the Core it generates to see if it eliminates the tuples?
Hello OP, I work at [humane software](http://humane-software.com), a Warsaw-based Haskell shop. We have tried in the past to start up a Haskell group in Warsaw. It was called Warszawskie Towarzystwo Funkcyjne (wtf). We held a couple of meetups, but--unfortunately--ran out of steam. If you'd like to use the name and our wiki at (http://wtf.humane-software.com) then please do! I don't live in Warsaw anymore, so it's unlikely that I'd attend, but the other WTF-ers may be interested. 
This is actually a second iteration which builds up on experience of five distinct SOAP server vendors each with its unique «features». The initial version started as a WSDL code generator but was scraped shortly afterwards in favor of making the damn thing work. You don't get mechanical correctness this way, but still can implement methods you need with a limited amount of work.
https://github.com/alphaHeavy/protobuf/ I'm not sure where in the code, I haven't done a deep dive yet, but I know that the types which can be derived for `Decode` have to be records with `Monoid` fields and that the general Protobuf protocol ensures that messages are interpreted as `Monoid`s.
Yes.
Yes, that's basically right. The only minor terminology thing is that in this case, typability is not preserved by an eta-*expansion*, since `f = runST` is okay but `f x = runST x` is not. 
Yes, `runST' (Poly st) = runST st`. Records don't have much to do with this btw, `newtype Poly r = Poly {poly :: forall s. ST s r}` works equally well.
&gt; Unfortunately wikipedia is not very helpful. You probably won't get anywhere far using Wikipedia math articles. They all suck. Some very, very badly. &gt; what [does] this change of terminology buys us? It's all standard terminology in category theory. As I understand it, the names of things are weird on purpose in CT. It is to prevent you from abusing your intuition. In almost all the categories we care about, objects are some kind of special set, type, or space, morphisms are special functions, products are some kind of pair, etc. However, this is not necessarily the case. One important class of category is called a poset category, where objects are just any old mathematical object and morphisms tell you which objects are "less than or equal" to others. You also have the category of relations, where objects are sets, and a morphism A -&gt; B is actually a relation on A and B (a subset of the set A x B). Furthermore, sometimes the closely related notions can turn against you! Monomorphisms and epimorphisms both tell you exactly when a set is injective or surjective respectively (in the category of sets). However, in the category of rings, there are non-surjective epics. And while the coproduct in sets is the disjoint (or "tagged") union, in the category of abelian groups, they are actually ordered pairs (and they coincide with products). &gt; Am I missing something important? Yes and no. You're right that there are simpler ways to present this. However, you are missing the big picture of category theory. (Not that I blame you. Most people have no clue what it is or what it's good for). Category theory is a big hairy complicated issue. In a single phrase, you can think of category theory as a "grand unified model" of modern mathematics, or perhaps a "rosetta stone" between distant fields of math, logic, and physics (and philosophy, for what it's worth). Category theory has brought insight into many previously untamed problems in math. What does it mean for two things to be isomorphic? Why do so many similar constructs seem to appear in every field?: Sets, subsets, cartesian products.... types, subtypes, pair types..... spaces, subspaces, topological product spaces..... manifolds, submanifolds, products of manifolds.... groups, subgroups, free products..... etc etc etc. It also has a great amount of symmetry. You can take any category, flip around the domain and codomain of every morphism, and you get another category where every true statement has a dual true statement. The dual of a kernel (in algebra) is a cokernel. The dual of a "set of solutions to an equation" (an equalizer) is a quotient set (a coequalizer). The dual of an epimorphism (often surjections) is a monomorphism (often injections). Isomorphisms are self-dual. The dual of algebraic data types (which include the natural numbers, the integers, and finite lists) are coalgebraic data types (which include the conatural numbers, potentially infinite lists, and IO commands). Basically, if you can phrase your problem in terms of category theory, it's obvious if your solution is a good one. You can also use it to translate your problems to other fields. An open set in topology "is just" a semi-decidable problem in computation theory. Equality types in type theory "are just" path homotopies in homotopy theory. Even our beloved Curry-Howard Correspondence is a categorically-flavored claim: formal logic "is just" functional programming. Lastly, category theory makes a very succinct language for talking about a wide range of subjects. If you check out the nLab online, you will see what I mean. "The kernel of φ is a pullback of φ and the zero morphism". What's a pullback? "A pullback is the limit over two morphisms with a common codomain". What's a limit? "A limit is a terminal object in the cone over a diagram". While these terms are unfamiliar to you, it should be clear that these are simple definitions. It's pretty rare to see a categorical definition with any real complexity to it. So there's your category theory primer. Please note 90% of what Reddit posts about CT is garbage, and it really does require a pretty solid grasp of math to do anything with. 
As pointed-out, you can maybe do a low-level suspend of your process. But if you look for an applicative solution, you will have to use explicit state. For example, you can probably model your loop (computing the digits of e) as a function `narrow :: s -&gt; s` where `s` is the state of the calculus. Running the computation will be like : compute s = if (enoughDigits s) then (result s) else (compute $ narrow s) Serializing state `s` to a file will allow you to suspend and resume the computation.
Try this version of `findSequenceCustom`: https://gist.github.com/tibbe/5373035 It's probably not as good as just using indicies though.
The types themselves give it a bit of a contract. One fun exercise might be to try to enumerate/classify the (total) functions that fit those types.
It could've been nice, but it's extremely over- and under- specified at the same time making true-to-the-letter implementation very complex. Still the majority of its features aren't used in most services and they should've been using plain-old-XML/REST anyway. And when the stuff is used as intended (e.g. WS-* enterpriseville) it's a horrible mess. The biggest trouble with hand-rolling the requests is that the servers are very, *very* nitpicky and crash with some nonsense excuse. There's the [shoap](http://hackage.haskell.org/package/shoap) package which merely flings wrapped strings. Mine is just a fancier wrapping for the same style with some request bells and response whistles piggybacking on the renderer and parsers of the [xml-conduit](http://hackage.haskell.org/package/xml-conduit) package.
Thank you, this is very useful. :)
Thanks! Great reply! What should I read to get an appreciation for category theory? What is a great application of category theory where you really see that it shines compared to a simple model without category theory? For example in other branches of math, you have some powerful thing that it lets you do or something where it gives you great insight that would be very hard to get without it. For example calculus lets you do many cool things like calculate volumes, solve minimization/maximization problems, etc. Linear algebra lets you solve systems of equations, is invaluable for numerical methods, etc. Same for control theory, graph theory, probability, combinatorics, differential geometry, etc. Each of these is like acquiring a superpower for thinking about certain types of problems. What is a good resource that shows the "superpower" of category theory?
For an opposite example: foo :: ((forall a. a -&gt; a) -&gt; Int) -&gt; Int foo f = f id bar :: (Int -&gt; Int) -&gt; Int bar g = g 1 baz :: Int baz = foo bar quux :: Int quux = foo (\f -&gt; bar f) In GHC, `quux` type checks, but `baz` does not.
it doesn't have to be that hard. I took [Peter Norvig's Lispy](http://norvig.com/lis.py) (which is like 100 LOC) and [refactored it to be pure functional](https://github.com/dustingetz/monadic-interpreter). Its a toy and not very good code but it helped me grok monads in about a weeks worth of work. [Here's some code snippets (python)](http://www.dustingetz.com/2012/10/02/reader-writer-state-monad-in-python.html), though this post has a few errors in it.
I have some doubts about that special case. Looking at figure 6 and figure 7 in http://research.microsoft.com/en-us/um/people/simonpj/papers/higher-rank/putting.pdf, it looks dsk should succeed for `($) runST` without a special case. Also, apply f a = f a \ m -&gt; runST `apply` m succeeds.
I for one was looking for this a few months back. Thanks!
The problem with the Poly record is that you can no longer refer to the variable @s@ in other places, which is the entire point of that type variable. How would you write newSTRef :: a -&gt; ST s (STRef s a) with Poly instead of ST?
I actually don't know. Category Theory is a tough cookie to crack. Being a Rosetta Stone, as I said, means you have to have a pretty solid understanding of at least two fields where it is known to excellent at (otherwise, you're just coming up with funny names for things). It's super power, if it had one, is in unifying different concepts into just a few. Unions, intersections, the cartesean product, disjoint unions, quotient sets, solutions sets for equations, kernels for groups, rings, and modules, cokernels, products for groups, rings, modules, product topologies for spaces, subsets, subspaces, subrings, subgroups, submanifolds, currying and partial application (in funcitonal programming), universal quantifiers, existential quantifiers, tensor products, tangent maps, galois connetion, the empty set, trivial groups, rings, and modules, the unit type (in Haskell)...... It turns out that every single one of those things listed about can be described using a technique called a universal mapping property. If there's one take home lesson about category theory, it's what a universal mapping property is. I'll just sketch out the idea and leave it to you to fill in the details from Wikipedia or wherever. Basically, we have a bunch of objects (sets, spaces, groups, whatever) and a bunch of morphisms (functions, continuous or linear maps, homomorphisms, whatever). These form a diagram. It's like a little directed graph (possibly infinite). A diagram might have the special property that every path/walk starting at the same object and ending at the same object is equal -- we say such a diagram commutes. Commutative diagrams play the role of equations do in the rest of math. Much like how we factor integers, one of the most basic motivations in category theory is to factor morphisms. That is, if you know h : A -&gt; C and g : B -&gt; C, you might be tasked with looking for a morphism f : A -&gt; B such that h = g . f (where the . is morphism composition). More often than not, you are looking to prove there is a UNIQUE factor f. A universal mapping property is a setup where you start with a commutative diagram. You extend that diagram with an object Z and a set of morphisms, then you need to prove that for any other extension of that diagram Z', there is a unique factorization through Z such that everything commutes. That's all very handwavy. And while I don't plan on going into any real detail, I'll give one example: the product of two objects A and B. Let's just work with sets to keep things simple. Suppose A and B are sets. Then AxB is the unique (see below) object having two morphisms fst : AxB -&gt; A and snd : AxB -&gt; B such that for any other object P with maps fst' : P -&gt; A and snd' : P -&gt; B, there is a unique factorization f : P -&gt; AxB such that fst' = fst . f and snd' = snd . f. If you draw the right picture, it's very clear what's going on. Anyway, one thing to note is that, (in sets at least), we have defined the cartesean product. In fact, we have done so without using the word "pair" at all. Nor did we talk about any element in the statement of the problem (though we talk about them in the proof). We defined it entirely by talking about how to "factorize" morphisms (again.... it's very similar to how you might factorize a composite integer in number theory). And *because* we didn't mention any elements, nor any other things besides morphisms, it will work in any category. Many (if not most) categories we're interested in have products. A note on uniqueness, and one of the central issues with mixing category theory and classical mathematics.... I lied earlier. We can't actually *prove* that there is a UNIQUE product of A and B. In fact, that's a lie. BxA is also a product of A and B. In fact, there are others, too. Infinitely many. However, by taking P = BxA above, and then flipping the situation.... taking P = AxB and taking BxA to be the "true" product of A and B, we find that the two are isomorphic. (You can freely swap between isomorphic objects without loss of information). It turns out that most constructions in category theory are only defined "up to isomorphism". This causes foundational issues with set theory, as objects must be constructed exactly in all cases. However, categorically, we know how a thing is constructed is immateral as long as we stick to thinking only about how objects related via morphisms. It's all interesting stuff. 
&gt; Is there a way of saving the current state of an entire Haskell program (along with all thunks, possibly infinite data structures, etc) to a file, and resume it later? You're save the state of an entire process. So, VM or paravirtualization.
I guess that instead of a special case for this $... runST $ do ... ^ you would need a special case for this $: runST' $ Poly $ do ... ^
What do you mean? If we *always* wrap `ST` with a `Poly`-like thing, then `s` has no reason to exist anymore. If you want `runST` to get `Poly`, you need to wrap your `ST` manually and you'll incur in the same inference problems.
By any chance, are you trying to suspend an ST computation and restore it later? Don't do that. Recently, I tried to implement a queue in an imperative way, by hiding the state mutations inside an ST monad. The code compiled and ran fine, but benchmarks showed that it was much slower than my immutable list-based implementation! I used something like your Poly datatype to to hide the ST monad from its pure callers. I thought that a value of type (Poly a) contained a hidden piece of state, that an operation of type (Poly a-&gt; Poly a) could update that state, and that runST' allowed me to extract a pure value from a stateful object. I was wrong. In reality, a value of type (Poly a) is the description of a computation which has not taken place yet. An operation of type (Poly a -&gt; Poly a) modifies that computation, usually making it a bit longer by appending a few more computation steps at the end. And runST' runs the entire computation, from the beginning. This explains why my benchmark was so slow. Each time through the loop, I was updating the state using an operation of type (Poly a -&gt; Poly a), and then extracting a pure value from the state using runST'. I was expecting each loop iteration to take a constant amount of time, but what I measured was that each loop iteration was running slower and slower! The reason is that I wasn't performing a state update, I was extending the description of a stateful computation. Thus, each time I ran runST', the program had to run a longer and longer stateful computation in order to obtain the result!
I tried to write a wsdl generator with template haskell, but I didn't realize how hard it would be. If I redo it I might consider just copying some other implementation. To have a typed web service that actually interacts with other languages sounds like a godsend to someone who likes haskell types, but haskell just got popular a bit too late to have one.
Interesting, but I'm not trying to *do* anything in particular, I'm just trying to understand why the complexity of rankN polymorphism is necessary. 
With the exception of the extra term for the return value, the free monad is not adding any other power. So, for example, the following would be an invalid term introduced by the extra power of the free monad: do give give BUT, the following would have been well-typed even in the absence of the free monad: do give give one USD What do two gives mean? All I have to go on is their `eval` function, which says that `give` negates whatever it wraps, so presumably the above would negate the USD twice to gain a dollar. Seems ridiculous, right, but that was valid even under their existing contract scheme, so that's not a flaw of the free monad but rather a flaw of their `Contract` data type. And who am I to say that some weird client might not want to write: replicateM_ 2 give &gt;&gt; one USD Same thing with `cond`. If you use the free monad version that returns a `Bool`, then you can use the `monad-loops` package to define a loop within the contract that tests the `cond` at each step. That's a perfectly reasonable thing to want to do, and the free monad gives you that for free by virtue of the monad interface. Without the monad interface you'd have to write the looping combinators by hand, repeating a lot of unnecessary work. Or what about `scale`? Maybe the client wants to read in a text file containing multiple scaling values and then translate them to the `Contract` DSL. If you generalized the free monad to the free monad transformer (trivially, just by importing `Control.Monad.Trans.Free` and inferring the new types), then you could read in the scaling values using `pipes`: type ContractT = FreeT ContractF toDSL :: ContractT SafeIO () toDSL = runProxy $ runEitherK $ readFileS "scalingValues.txt" &gt;-&gt; handler where handler () = forever $ do val &lt;- request () lift $ scale val I mean, use your imagination! Think creatively!
Not yet! We did work out (but left out of the paper) how to elaborate into ordinary System F, so I would guess it would follow along those lines. How is Daan Leijen's translation different, though? 
Wait, newSTRef is really rank 1 right? So there's no need to change it at all: runST' (Poly (do ref &lt;- newSTRef 42 modifySTRef ref (*2) ))
So with your help, some code to demonstrate that **yes, it works** [(run it here)](http://codepad.org/6SMov1TE): import Data.STRef import Control.Monad.ST newtype Poly r = Poly (forall s. ST s r) runST' :: Poly r -&gt; r runST' (Poly a) = runST a main = print $ runST' $ Poly (do ref &lt;- newSTRef 42 modifySTRef ref (*2) readSTRef ref ) I'm still wondering what all the fuss about rankN types is then?
I'm immensely looking forward to giving this a try.
OK, I understand that, but then I'm already familiar with Functor and its most common instances. I'm not sure those principles/heuristics would make sense for someone who doesn't know the basic typeclasses to start with.
This is giving me horrific flashbacks to the pain involved with dealing with multiple SOAP implementations from *Python*; I imagine if I'd tried to do what you've done I'd be babbling in the corner of a mental institution right now.
To clarify: `$` is a special case, `runST` is not. GHC types `f $ x` as if you'd written `f x`, so it works even when predicativity would prevent `$`'s type from being instantiated into `f`'s type.
Me too. I currently am hacking on a project which uses FFI to gsoap. It works, but FFI. 
Having polymorphic fields already introduces functions with rank-2 polymorphic types. &gt; :t Poly Poly :: (forall s. ST s r) -&gt; Poly r Once this cat is out of the bag, you may as well also allow user-defined functions other than record constructors to have such types.
I'm actually following through on that now! (after our talk at ICFP) I formulated your suggested rule as a HERMIT rewrite and tested it on lists... worked like a charm... now seeing what other mileage I can get. Would you be interested in reading a draft when I get that far? I'm hoping to target the Haskell Symposium.
`g__` had an excellent reply to my exact same question: http://www.reddit.com/r/haskell/comments/1c7hiq/undestanding_runst_and_impredicativity/c9dyer5
I gave that page to a complete Haskell newbie a year ago and he had no problem deriving the proper Functor and Monad instances in those exercises by 'following the types'. If anything, it's too easy to just win by following the types without gaining any real understanding of what you are writing. Also his code was terribly unidiomatic, but that's fine/expected of someone new, I think.
GHC is not eliminating the tuples, so unfortunately you have to write this as: loop !i n k a b kmax imax !s | B.null s = (kmax,imax) loop i n k a b kmax imax s | B.head s == a = cont (n+1) (k+1) a b | B.head s == b = cont 1 (k+1) b a | otherwise = cont 1 (n+1) (B.head s) a where cont !n !k !a b = if k &gt; kmax then loop (i+1) n k a b k i (B.tail s) else loop (i+1) n k a b kmax imax (B.tail s) findLongest2 b = case loop 0 0 0 0 0 0 0 b of (k,i) -&gt; B.take k . B.drop (i-k+1) $ b The overhead of packing/unpacking tuples in the inner loop makes it 6-8x slower. I would be curious if there is any way to persuade GHC to do it. 
No. not bad at all. 
I think Haskell is a fine first programming language. (In my opinion, it would be a better first language than Python, but the Internet would slap me if this thought spread beyond /r/haskell.) The problem with choosing a first language is that you don't know enough to make an educated decision. However, rest assured that it's not the most important decision you will make in this journey. (But please learn Haskell at some point even if it's not what you choose to learn first!)
Well I have started Haskell as my first language ( I did quick CAML-light for a year in school but we only learned like 10 commands :D ) and I love it. I hate imperatives languages and Haskell is really intuitive for me compared to things like Matlab... But I'm a math student so there's that
I think you could learn Python and Javascript, etc. easily enough going from Haskell anyway. Haskell will give you amazing recursion and functional skills. But focus on learning algorithms, not languages.
I think it's a great idea to start with Haskell, that way you don't have to unlearn all the bad habits you pick up in imperative languages.
There was a [Google+ post](https://plus.google.com/111778690342006065958/posts/hGBi2FAaeGj) recently asking a similar question. I posted an answer there. In summary, it will be harder than other languages which are more popular, because they are more popular and have more infrastructure support. This is not Visual Basic or Java where you have an IDE that just works and guides you along with point and click (yet: there are [various](https://www.fpcomplete.com/) [companies](http://www.well-typed.com/) working towards removing friction for Haskell programmers). If you're not deterred by that--and the Haskell community is filled with such thick-skinned/stubborn optimists--you'll do well. The language itself shouldn't present you with any problems, if you are an absolute beginner.
After learning Haskell the quality of my JavaScript improved immensely. Haskell forced me to think in terms of values and how those values should be transformed and returned. It really helped me to focus on the "one thing" that a function should do.
Of course /r/haskell is going to tell you it's a great idea. /r/python will tell you that Python is a good first language, /r/ruby will tell you to use Ruby, etc. However, one thing to keep in mind about Haskell is that it's quite different to traditional languages like Java, C++, Python etc. More than likely, most of the jobs near you are going to be in a traditional language. To go from just Haskell to an imperative language will require you to learn concepts (What's a for loop? Where should I store shared state? OOP, etc) not just the somewhat differing syntax. The same can be said about going from imperative languages to Haskell (What's a monad? What's a pure function?). If you're looking for something to learn for a future job, a traditional language would be more helpful. It's very easy to go between those languages as the differences are very small when compared to the difference between imperative (most common languages) and functional languages (like Haskell). For example, job listings in one of the top job sites in my country for: * Haskell: [0](http://www.jobs.ie/Jobs.aspx?Categories=Job+Type&amp;Regions=Job+Location&amp;Keywords=Java) * Java: [~70](http://www.jobs.ie/Jobs.aspx?Categories=Job+Type&amp;Regions=Job+Location&amp;Keywords=Java) * Python: [24](http://www.jobs.ie/Jobs.aspx?Categories=Job+Type&amp;Regions=Job+Location&amp;Keywords=Python) * Ruby: [10](http://www.jobs.ie/Jobs.aspx?Categories=Job+Type&amp;Regions=Job+Location&amp;Keywords=Java) * C++: [23](http://www.jobs.ie/Jobs.aspx?Categories=Job+Type&amp;Regions=Job+Location&amp;Keywords=C%2B%2B) This doesn't mean that there aren't Haskell jobs out there, just that they're a lot harder to find. On the other hand, if you're looking for something just to interest you, pick whatever you like. As far as starting, [Learn You a Haskell](http://learnyouahaskell.com/) in the sidebar might help you, but it does assume some programming knowledge, so you will have to put in a bit of work to find things out (e.g. the very first section assumes you know what a boolean is - while that might be obvious from the context, it happens a few times throughout the text). The other book listed in the sidebar, Real World Haskell is definitely not for people who don't already know how to program.
I generally would recommend Haskell, but I will compare the two languages so you can understand why. The first major difference between the two languages is when you discover mistakes in your code. In Python, you only discover mistakes when you run the code and they manifest themselves as backtrace exceptions. In Haskell, you catch most mistakes at compile time. This difference accounts for the perception that Python is beginner-friendly and Haskell is expert-friendly. Python is very forgiving of mistakes, waiting until the last possible moment to crash, where Haskell is not, refusing to even run the program until you fix all the obvious problems in it. (**NOTE**: You can recapitulate Python-like behavior in `ghc` 7.6 using `-fdefer-type-errors`). Python therefore often seems more inviting when you are first learning to program because it lets you run something even if you make mistakes, something you are very prone to do when first learning to program. However, the better you get and the more complex the project gets the more this becomes a misfeature. You end up having to write lots of tests to exercise all of your code regularly to try and detect latent bugs, a process which is very tedious and difficult to do comprehensively. Without these large arrays of tests nobody feels safe modifying the large code base because they are afraid they will break something. With Haskell, the compiler is really good at catching bugs automatically, so you have to write much fewer tests, if any. Moreover, generally the code size is smaller and easier to reason about which also makes it scale well to more complex tasks. This is why most people who like Haskell are those who come to it from another language, because they've experienced first-hand the kind of problems that Haskell was designed to solve. If you've never programmed outside of Haskell, it's difficult to appreciate how many problems it solves. The second major difference between Haskell and Python, which is sort of related to the first difference, is that Haskell has a strong static type system, whereas Python has a ~~weak~~ (see correction below), dynamic type system. Generally, training on a static type system is much better for your education as a programmer, because it helps you organize your thoughts more clearly about what is going on. For example, if you ask Python what the type of a function is, it will be totally unhelpful: &gt;&gt;&gt; def f(g, x): ... return g(g(x)); ... &gt;&gt;&gt; type(f) &lt;type 'function'&gt; ... whereas if you ask Haskell, it will give you very detailed information because it has a much clearer and powerful type system: &gt;&gt;&gt; let f g x = g (g x) &gt;&gt;&gt; :type f f :: (t -&gt; t) -&gt; t -&gt; t That's much more helpful, because now we know we can see clearly from the type that `f` takes a function from `t` to `t` as its first argument, an initial argument of type `t`, and then returns a new `t` as a result. This is called type inference and the compiler uses information like this to find all sorts of bugs whenever the type it infers from a function definition doesn't match the way in which it is used. Haskell's type system is much clearer and elegant than other type systems, which teaches you to think much more clearly about how things connect together. You would eventually learn these kinds of things in Python, but it would take you a while and a lot of frustration before you developed a consistent mental model of what is going on, whereas Haskell has a very consistent and uniform approach to the way everything interrelates so you learn much faster, and you will take this coherent mental model with you when you learn other languages after learning Haskell. Python, on the other hand, is very inconsistent, and the more you learn it the more you will just end up learning Python-specific quirks which do not carry over well to other languages. Haskell, on the other hand, teaches very elegant and general concepts which are widely applicable in all languages.
Your list doesn't show "any language will do as long as you can learn ours in 3 weeks" kind of jobs. Those are the best ones, and they do exist.
They offer no extra expressivity compared to types with polymorphic fields. You can always rewrite a rank-2 type using an additional datatype. If your type has deeply nested "forall"s (rank-n) you can do it gradually, in layers, using several auxiliary datatypes. Essentially, the two features are equivalent. However, the version with datatypes is more verbose, you need to run it with an extra `Poly` constructor. (Trivia: GHC has a special flag -XPolymorphicComponents, whose purpose was to allow polymorphic fields like yours only, but it's deprecated in the development version; now you should use -XRankNTypes in both applications.) All the fuss about the higher-rank type of runST is that it forbids leaking references. If you were able to create a reference `newSTRef []` and run it with `runST` (i.e. remove the ST monad), you'd have a value `x :: STRef s [a]`. This is a polymorphic reference: it can work both as a reference to a a list of integers or a list of characters. Using read and write functions, you could write there a single integer and then read it as if it was a character. Therefore, you could coerce between two _arbitrary_ types, which is the ultimate failure of the type system: its job is to prevent mixing types, whether accidentally or maliciously. Even if we somehow forbid polymorphic references (for example, if you could build references to Ints only), having references outside ST allows side-effects, which Haskell attempts to dodge. For example, assume `x :: STRef s Int`, then a value `runST (writeSTRef 5 &gt;&gt; return 1)` is an integer with side effects. With several such values, the result of a computation can depend on order of evaluation - what a nuisance. By giving runST the rank-2-type, you are not allowed to return references out of runST, even if you attempt to smuggle them hidden somewhere in a list of maybes of tuples. References have that "s" parameter which controls their scope: if you attempt `runST x` where `x :: ST s ...` and inside `...` there's `s` somewhere, it will not unify with `forall s. ST s a`. It's really clever and in my opinion it's one of the most beautiful parts of Haskell. Even more, you can nest it, and in those cases references cannot escape their corresponding runST calls. For example, you can do: import Control.Monad.ST import Data.STRef x = runST $ do x &lt;- newSTRef 0 y &lt;- newSTRef 6 let z = runST (do a &lt;- newSTRef 0 modifySTRef a (+1) k &lt;- readSTRef a return $ [x,y] !! k) readSTRef z Here the inner runST returns a reference that is bound to the outer runST.
I wouldn't hire someone who had never touched anything but Java before for *anything*, not even for a Java gig. If one programming language is all you've ever bothered learning, then I'm sorry, you don't have what it takes. This, incidentally, is also why I think the choice of first programming language isn't all that important. You'll learn more languages, and eventually settle for a selection that works well for you.
What fuss are you referring to? Rank N types and records with polymorphic fields are equally difficult to implement.
I think Haskell is a great first language - people who have coded before can find it difficult, but only because they have a ton of preconceptions about programming from imperative languages, and Haskell isn't one of them. However, if you get good at Haskell, you get a really good mindset for solving problems, and you can learn other languages with ease. This is the approach my university (Imperial College London) takes.
I more meant in thinking about when it is applicable and how you can use it. You are right, of course.
I recommend joining the haskell-beginners mailing list.
I think it's brilliant if you can get away with it. People, having learned an imperative language, often avoid learning about the functional style at all, akin to never switching from qwerty. By contrast, imperative is easy to pick up, and should you transition into python it'll leave you with some great habits. That said, python is probably easier as a first language, and nice in general. So there's no shame if you end up doing that either.
I was under the impression that polymorphic components were easier to implement, since each wrapping/unwrapping refers to a name in the environment, and thus can be implemented by a standard rank generalize/instantiate? And doesn't it remove the need for explicit forall syntax? (because as in HM, it's only allowed in the environment)
I would say it depends on what you would like to do. For example, making games in Haskell if you haven't programmed in anything else will be very hard. For your second question: I don't know if there is any good beginners guide yet, but I have a friend who is working on one right now. http://hasp.xkqr.org/ Eleven chapters done, the early ones also have exercises so I would recommend you to check that out.
Mmm. While that's your opinion and one I agree with in principles, it's not the common case.
No, I don't want to go into software development. I'm studying economics. So I'll be dealing with a lot of mathematics. I'm only hoping that knowing programming may come in handy later on. Will Haskell be a better companion than Python (or any other language)?
Thank you for such a detailed reply. I have never done any programming but as a student of mathematics, the second piece of codes seems much more clear and intuitive. 
Thanks, I corrected it.
Can you write a rewrite rule to force ghc to do it?
Do you mean like what http://hackage.haskell.org/packages/archive/generic-deriving/1.5.0/doc/html/Generics-Deriving-Monoid.html gives you?
Depending on what mathematics you're interested in doing, Python's numpy package may be more useful to you. 
Seconded. I prefer Haskell to python, but as a first language I think a regular old imperative mutable state language would be an easier to take on. That said, I'd love to hear how a native Haskeller would feel about learning the other kinds o languages. 
If you need a notion of variable binding, you might want a monad. Countless DSLs don't and thus are not monads. You lose introspection abilities once you start throwing functions in the mix.
I would first use one of Haskell's many json libraries to convert the json string to a json object. It will be much easier to extract the values you need from that json object than to extract them from the json string.
If you are a math student ~~python~~ Haskell would be wonderful for you. There are certainly parts that confuse a lot of people, but those tend to be the math like abstractions which you shouldn't have too much trouble with. Edit: Thanks for pointing out that my brain was off cdsmith.
I use aeson library for JSON thing. I usually do not care much about fast performance for JSON conversion, so I just use Data.Aeson.Generic and make my haskell data types as an instance of Data.Data using DeriveDataTypeable extension. Then, you don't actually have to write any JSON data structure, parser and builder. This scraps your boilerplate ;-)
Yeah, I like candidates who are able to choose the best tool for the job from a large toolbox of existing knowledge and who are able to rationally determine (from hearsay or whatever) when their toolbox could benefit from something new. Somebody who knows only Java is either inexperienced (and might become this kind of person someday) or experienced but not this kind of person; either way, they are not the kind of person I'd prefer to work with.
Imperative and functional programming are entirely different paradigms, and I honestly believe an understanding of imperative qualities will suit you much better. Come back to Haskell once you have a strong understanding of imperative programming, you'll get a lot more out of it.
Here's an anecdote that I've told several times already in other contexts. My wife learned a little Haskell and then looked at imperative languages and didn't understand why somebody would ever want to "change" a value or how they would keep track of all the things they change.
Correct - polymorphic components are theoretically easier but putting the wrappers and unwrappers everywhere might be too verbose in practice. It's just a design decision.
Since you're parsing into several different types that don't match up nicely to the JSON representation, here's what I'd recommend: 1.) Use the aeson package to parse the JSON into an Aeson.Value[1]. This is an intermediate type that just represents JSON. You don't need to make a FromJSON instance to do this, just: ```decode json :: Maybe Value``` 2.) Then pull out the specific fields you need and use them to assemble your types. aeson-lens[2] is a big help with this if you don't mind taking the time to understand the lens library a bit (a very valuable investment). [1]http://hackage.haskell.org/packages/archive/aeson/0.6.1.0/doc/html/Data-Aeson-Types.html#t:Value [2]http://hackage.haskell.org/package/aeson-lens **EDIT:** A full example using ```FromJSON``` instances: https://gist.github.com/jhickner/5379928 
&gt; Seems ridiculous, right, but that was valid even under their existing contract scheme, so that's not a flaw of the free monad but rather a flaw of their Contract data type. So you've shown that the monad interface not only preserves junk in the original but also adds to it. Horrors! What would be far more interesting is if you could eliminate that junk, with or without free monads. Even a trimming achievement is worthy of mention. &gt; And who am I to say that some weird client might not want to write: replicateM_ 2 give &gt;&gt; one USD That's really a stretch. First of all, you already concede that consecutive "give"s don't make sense in the domain being modeled. "Never mind that, let's just pretend it does!" you answer. Are you focused on the domain, or something entirely different altogether? You next two examples with `cond` and `scale` also bring in monads gratuitously. (You can always tell when someone's clutching at straws when they invoke the code-reuse argument over easily rewritten combinators of uncertain domain-relevance kept in obscure packages.) &gt; I mean, use your imagination! Think creatively! As others have pointed out, it does seem you're proposing a solution in search of a problem. What's NOT creative is to get hung-up on the laughable idea that all DSLs must use free monads. "If there's no free monad in there, it's not a DSL" is a slogan you don't want to be caught singing. Designing a DSL requires domain expertise, not just facility with hipster tool à la mode. 
I'm going to give a different answer here than most people. I think [Scheme](http://en.wikipedia.org/wiki/Scheme_(programming_language\)) is a better introductory language than Haskell: * It's a very simple language that is nonetheless extremely powerful * It has a gentler learning curve than Haskell. * It's been used for education for very long so that it has better learning materials. * Scheme is multiparadigm, so you can do both functional and imperative programming easily. Python is similar to Scheme, except more complicated, less powerful and enormously more popular. The two popular Scheme for beginners books are both online: * Abelson and Sussman, [*Structure and Interpretation of Computer Programs*](http://mitpress.mit.edu/sicp/). * Felleisen, Findler, Flatt and Krishnamurthi, [*How to Design Programs*](http://htdp.org/). There's [a paper by the authors of HtDP (PDF)](http://www.ccs.neu.edu/racket/pubs/jfp2004-fffk.pdf) explaining their approach compared to SICP's, which is worth reading if you want to decide which of them you'd like to tackle first. (There's also [an older, pre-Haskell paper by Wadler criticizing the 1st edition of SICP and the use of Scheme for intro CS, and recommending some of Haskell's predecessors instead (PDF again)](http://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf)). SICP is nonetheless such a classic book that it's worth reading it in any case—the main question is now vs. later.) For a Scheme system I'd recommend [Racket](http://racket-lang.org/), which is what the HtDP book uses (note that Racket was formerly called "PLT Scheme," and the HtDP book might refer to it like that, or to the old names of its components: the `mzscheme` interpreter/compiler and the DrScheme IDE). There are a some Racket modules designed for the SICP book, but I don't know how good they are. What about Haskell? Well, it's a great language, but the learning curve is very, very steep, and it won't teach you very much about imperative programming, which you do need to know. (Or object oriented programming; garbage as though OOP is, you do need to know it as well.) The two main difficulties for a beginner are, in my mind: 1. The syntax is hellishly complex. Sections, partial application (which leads to "what?" moments such as encountering `foldr f z xs i`), partial application of infix type constructors, etc. 2. The error messages have a steep learning curve. Haskell doesn't usually just tell you "you violated the syntax of the language" or "your function doesn't make sense because it tries to add a string to an integer." Oh no, it too often tells you that you violated the monomorphism restriction, or that you tried to construct an infinitely recursive type, or that it found no crazy Num instance for the crazy thing that it thinks your typo meant. Another language worth considering—though I suspect there isn't much beginner's didactic material for it—is Microsoft's F#. This is a variant of CAML, so design-wise it sits in between Scheme and Haskell.
There are a ton of jobs--like being a grad student--where programming is really useful but not directly part of the job. For these, you can use whatever language you feel like. (I'm assuming the poster is going to be a math/science grad student.) Also, if you start out with Haskell and then learn about PL theory--semantics and type theory--you will be learn enough about imperative programming to go over to such a language fairly easily. Most treatments of type theory and semantics I've seen take this approach: start out with just the lambda calculus and then add in imperative features. I think this makes for a very good way to learn the relevant concepts. Essentially, you can use functional programming as a base to figure out how imperative constructs work. Of course, this won't give you much experience using those imperative ideas in real life, but it should be enough to put you on your way. I think it's easier to start with functional programming and use that to get to imperative programming than vice-versa.
I know Python and use it in work and for fun. I'm still learning Haskell and finding it very slow going. I'm very open-minded so I'd be interested to hear why you'd suggest Haskell over Python as a first language (I simply don't know enough about Haskell to work it out myself).
Haskell has some advantages and disadvantages as other people have written about in this thread; it's certainly something to consider. Another option--one that I think is strictly better than Python--is Scheme. It's an *incredibly* simple language that manages to neatly cover both basic functional programming and basic imperative/OO programming. It also comes with some wonderful free books like [Simply Scheme](http://www.eecs.berkeley.edu/~bh/ss-toc2.html) and one of the most famous CS books of all, [SICP (Structure and Interpretation of Computer Programs)](http://mitpress.mit.edu/sicp/). Most of the arguments for Python also apply for Scheme; if you find yourself swayed by those and decide not to start with Haskell, I definitely suggest picking Scheme over Python.
Yes, I had already put my eye in aeson. I hadn't seen aeson-lenses however: I was thinking of parsing the structure, but this seems much better, thank you. I think I understand lenses enough for this, and if I don't, it's even better(if I started doing this is because I wanted an actual project that would make me learn in the progress: it already made my brain click a bit about free monads and learn about JSON, so the next thing is lenses and how HTTP requests and such work). Thank you, I'll start looking into it ASAP.
Mainly because it teaches you a more disciplined, systematic way of thinking, which is a skill that carries over to other languages, and it's easier to reason about your code (equational reasoning!). If you get used to thinking this way early it's more likely to stick. Also, it's harder to shoot yourself in the foot, which is especially important when you're still learning.
That's what I did, as an inveterate non-'programmer', though I use it all the time in my work (admittedly I'm not a model to emulate in anything). I think I see again and again that the wisdom once preached about Lisp holds of Haskell: that with prior programming experience you can learn it in a week; without prior experience it takes an afternoon ... (expertise takes a bit longer in either case...) I know from experience that learning Python is harder than Haskell, if you know nothing; or maybe it's just that I found it so disgusting, but then chanced upon Haskell, and was stunned by its beauty - principally due to the genius of D. Turner - and the immediately intuitive character of 'programming' as simply *defining ones terms*. There are few things in the world as beautiful as the Haskell language, it is like hearing music for the first time -- and few human inventions compare with the Glasgow Haskell Compiler which inexplicably makes all of this as 'practical' as you like. Of course none of this is a good idea if you are planning to become a professional programmer, which is largely a matter of learning other quite different ways of thinking, well illustrated by the likes of Python. I think learning both Haskell and Python at the same time might be a good idea, as e.g. Tekmo seems to be saying -- also not a programmer by profession, I think, and thus maybe the wisest head here speaking -- maybe wavewave and some others are the same . Since learning Haskell I've learned a number of other languages. But maybe I would have gotten further in Haskell quicker if I had started with C earlier, or maybe Python or something. The best beginning tutorial is Learn You a Haskell. The best 10 minute abcs tutorial is http://tryhaskell.org/ Also signing in to freenode #haskell under a suitably ludicrous nick, as soon as trouble arises, pasting code on hpaste.org is a very good idea. I can't emphasize this enough. Who needs a tutorial when a teacher is always available? They can also point you to suitable things to read for your specific problem. There will always be someone to explain what's going wrong and the related principles. The 'sillier' the mistake the better, since it draws 'intermediate' level users into the discussion. One major problem will be installing ghc, the Haskell Platform and so on; these questions can be answered by #haskell people; you should make clear that you are not a computer person so they dont take certain 'obvious' things for granted. (If you are using Windows they will not be able to restrain themselves from offering rude unneeded but of course correct advice to stop using it.) StackOverflow also gives more or less instant responses, but is maybe better after you have a few months' experience; not sure.
Aeson is definitely what you want. If you specify the FromJSON instance for your types (if you can that is), it gets much much easier.
I think the problem with python and other non-lambdas is that you actually have to unlearn something to learn something in order to learn lambdas... If you think about it, language with these properties aren't promising at all, they aren't serious deal. I recommend you scheme or Haskell, scheme is an uniquely good learning material because it is "unbloated" but somewhat fatally trade productivity (if you plan to use the core only) so it is really cool to start with, Haskell adheres many many serious discipline from mathematics i believe, and it is already usable by now. Of course, read [these good old classics](http://library.readscheme.org/page1.html) to get your hand hot first!
Thank you, very helpful.
&gt; What would be far more interesting is if you could eliminate that junk, with or without free monads. Even a trimming achievement is worthy of mention. I didn't want to trim anything because I didn't understand the DSL, even after reading the linked article. &gt; That's really a stretch. First of all, you already concede that consecutive "give"s don't make sense in the domain being modeled. "Never mind that, let's just pretend it does!" you answer. Are you focused on the domain, or something entirely different altogether? Look, you are really taking advantage of my ignorance of the financial domain. I can't suggest useful abstractions because I'm not their target client. All I did was answer the question that the OP asked to the best of my ability. &gt; You next two examples with cond and scale also bring in monads gratuitously. (You can always tell when someone's clutching at straws when they invoke the code-reuse argument over easily rewritten combinators of uncertain domain-relevance kept in obscure packages.) This is not true. `pipes` is not an easily-rewritten combinators and you never properly debated this point. That is a perfectly legitimate mixture of `pipes` and this DSL and you know that hand-crafting an equivalently elegant solution without the free monad would be very laborious and be incredibly prone to the common classes of mistakes that all newly minted iteratee libraries make. I think this is a perfectly valid example of code reuse via the `Monad` interface and you haven't yet provided a compelling argument against it other than an appeal to ridicule. &gt; As others have pointed out, it does seem you're proposing a solution in search of a problem. No. The OP very specifically prompted the issue whether or not to use free monads: &gt; This would seem to be something that would be well suited to being implemented with Free monads as a DSL and interpreter. ... and I answered it to the best of my ability. If you don't like the topic of free monads then you have an issue with the OP, not me. &gt; What's NOT creative is to get hung-up on the laughable idea that all DSLs must use free monads. "If there's no free monad in there, it's not a DSL" is a slogan you don't want to be caught singing. Please don't put words in my mouth. I'm just saying that there are perfectly legitimate uses of free monads and this example is no exception. We can debate all day long about whether or not the client needs these uses, but the truth is that neither you nor I are the customer/client. I only speculated about possible uses because I don't work in the financial industry and that was the best I could do given the question. &gt; Designing a DSL requires domain expertise, not just facility with hipster tool à la mode. You are being too aggressive about this. Please tone it down and keep it friendly, in the best spirit of this community. I think this whole topic has been educational for everybody to point out the strengths and weaknesses of free monads.
Well there's a nice resource to happen upon.
I'm not sure if I'll go the FromJSON way: it forces the `Parser` type on you, and if I end up using lenses that will turn out messy.
I'll gladly answer your question, but first allow me to allocate some memory for my answer. Now, I will declare a variable named `i`, which I initialize to `0`. This number will enumerate which deficiency of imperative languages I am referring to, but keep in mind that `0` refers to the first deficiency, not `1`. Now I must check if `i` is less than the number of responses that I plan to give. I see that it is not, so I will now dereference the answer indexed by the variable `i` and assign it to the variable reply. Now I will print the contents of the variable reply: &gt; Imperative languages do not actually model the the way we do things in real life Now I must initialize a container, named `answer`. I will initialize it empty and then append the answer I just gave to this container, for safe keeping. Now I will increase the value of `i` by one. `i` still remains less than the number of answers that I plan to give, `numAnswers`, and fortunately `numAnswers` did not change since the last time I reference it, which would not make sense now, would it? I will now dereference the answer pointed to by the variable `i` and store this in the variable reply, overwriting the previous contents. Fortunately, I will not need the previous contents because I stored them in my container and I am reasonably certain no other portion of my algorithm references my old answer. I will now print out the contents of the variable reply: &gt; Simulating a state machine in your head does not scale well to complex problems I will now append this value to the container that I init`Segmentation Fault`. Now compare this to the equivalent Haskell solution: runProxy $ fromListS answers &gt;-&gt; raiseK printD &gt;-&gt; toListD ... which pretty closely matches how you would describe the problem in plain English if you were trying to tell me how to respond to you: "Take all answers, print them out, and then store them in a list."
I was answering this part: &gt; That said, I'd love to hear how a native Haskeller would feel about learning the other kinds o languages. The post reflects my view of other kinds of languages.
The sum types are modeled using several constructor tables which reference main table with id and discriminator. Despite being experimental, they work robustly. If want to have more flexible control over schema, you can use plain records. Groundhog migration mechanism supports composite keys, indexes, triggers, and functions.
&gt; how they would keep track of all the things they change. Based on my experience in industry, working on large applications written in an imperative language, the answer to that is all too often "they don't".
I suspect that doing anything interesting with F# will quickly bog down in dealing with the .NET libraries, which are not exactly designed for ML-style functional programming. For practical use that's great because you have all those libraries available, but for learning not so much. No argument about Scheme as an introductory language, though. It's very nice.
Frankly, after years of experience in industry using mainstream languages, I'd honestly say that Haskell makes a much better imperative language than most imperative languages do and will teach you more about writing good imperative code than they will. What Haskell won't teach you is OOP, but that's another matter.
&gt; regular old imperative mutable state language This assumes that there's something normal about mutable state. To a beginner, there is not.
And if you use Aeson's typeclasses, your code will act as though it was extracted directly from the string.
&gt; Then, you don't actually have to write any JSON data structure, parser and builder. Assuming your JSON happens to follow the model of that generator.
In that case, may I suggest using Text.JSON instead of aeson? Implementing its readJSON is just as easy as implementing FromJSON, but its type is much simpler to understand than Parser. data Nullable a = Null | Value a value :: Nullable a -&gt; Maybe a value Null = Nothing value (Value x) = Just x instance JSON a =&gt; JSON (Nullable a) where readJSON JSNull = return Null readJSON v = Value &lt;$&gt; readJSON v instance JSON Votes where readJSON (JSObject o) = Votes &lt;$&gt; valFromObj "ups" o &lt;*&gt; valFromObj "downs" o &lt;*&gt; (value &lt;$&gt; valFromObj "likes" o) 
Does Text.JSON have lenses? I didn't find them. I will probably end up trying both methods and finding out which one I like better. Maybe I end up just using the parser. Maybe the parser just makes most sense, knowing that I don't really want to set anything, just get.
Meta: I'm continuing the discussion for the benefit of the silent, albeit largely departed, gallery. &gt; pipes is not an easily-rewritten combinators and you never properly debated this point. The easily rewritten combinator refers to a non-monadic version of your `replicateM_` example, i.e. a `replicateEndo` of type `Int -&gt; (a -&gt; a) -&gt; (a -&gt; a)` [1]. Recall the context: * As an example of a monadic benefit that you yourself acknowledged as dubious, you give: `replicateM_ 2 give &gt;&gt; one USD` * I claim: (1) an easily rewritten combinator, (2) uncertain domain relevance (is all of `monad-loops` relevant? are monads even relevant?), (3) obscure package (`monad-loops`). Introducing IO is orthogonal to the discussion, and it's not at all clear that an iteratee-style solution, much less a specific variant, is by default the *best thing*, whatever that means [2]. And given that the pipes example is at the end of a list of far-fetched examples, it's not clear that (1) you've finally got a use-case that's reasonably common in the domain, and (2) you've solved it in an elegant, composable, scalable way that's sufficiently so relative to the alternatives. &gt; Please don't put words in my mouth. I'm just saying that there are perfectly legitimate uses of free monads and this example is no exception. It's interesting to examine your reaction to this particular sentence: &gt; "If there's no free monad in there, it's not a DSL" is a slogan you don't want to be caught singing. There's nothing in there that says you've embraced the slogan. What's evident is that a peculiar gung-ho-ness about free monads won't help others from falling under its spell. See, the entire discussion on this page illustrates the danger of over-investment in a particular tool and outlook. Way to go illustrating *perfectly legitimate uses of free monads*, but a toolsmith also knows the boundaries of a tool. And &gt; You are being too aggressive about this. Please tone it down and keep it friendly, in the best spirit of this community. Bringing people outside of their comfort zone isn't always to anyone's disadvantage. Y'know, a frightfully effective way to let *community* go to pieces is to not do anything. And that includes not speaking up when giddiness of advocacy (or dis-advocacy in the case of Oleg) runs amok over a sense of balance. [1] Morally (because `Int` is not `Nat`), it's one half of a church numeral iso. [2] I might add that the whole case for monad transformers isn't open-and-shut either. 
&gt; This doesn't mean that there aren't Haskell jobs out there, just that they're a lot harder to find. This is true ... In my country at least, companies in sectors ( defense, finance, research ) where Haskell is used tend not to advertise on channels like open job listings sites as much. That said all of the Haskell programmers that I know that have had the wherewithal to pursue it as a professional career are doing /very/ well in this market.
Thank you for the effort! However, a good part of that code covers things I already done (in the post I just put the relevant ones), and my approach is a bit different, specially how to get the actual link from the listing(that listing actually returns always a single link only, so it's not that complicated).
&gt; The easily rewritten combinator refers to a non-monadic version of your replicateM_ example As far as I can tell, all you've proven is that your particular use case does not require free monads, but who are you to say that somebody else might not need them? I've posited some examples that I thought I would find useful if I were using this software. I still feel that the `scale` example is compelling enough circumstance to warrant the `Monad` instance, an example which you haven't refuted to my satisfaction. &gt; Introducing IO is orthogonal to the discussion, and it's not at all clear that an iteratee-style solution, much less a specific variant, is by default the best thing, whatever that means It is very relevant. I've shown how having a ` Monad` instance gives you streaming for free as a result of having the `Monad` interface. That's the text-book definition of code reuse. One of Haskell's main distinctions over other languages is that it promotes this kind of code reuse through theoretically-disciplined type classes and abstractions that the community agrees on. If you're not interested in that theoretical discipline and you are satisfied with redesigning DSLs from scratch every single time, then what's even the advantage of doing this all in Haskell versus, say, C#? Fewer parentheses? &gt; What's evident is that a peculiar gung-ho-ness about free monads won't help others from falling under its spell. Your arguing from the premise is that free monads are intrinsically bad, a premise which I do not share. What is the disadvantage of having a `Monad` interface? The absolute worst that happens is that you don't use it. &gt; Bringing people outside of their comfort zone isn't always to anyone's disadvantage. Y'know, a frightfully effective way to let community go to pieces is to not do anything. There's a fine line between debating in good faith and bad faith... &gt; And that includes not speaking up when giddiness of advocacy ... runs amok over a sense of balance ... and you crossed it.
No, but you won't miss them! From what I read about them so far, a lens combines a getter and a setter for an element nested inside a bigger datatype. So if you don't want to set anything, they probably won't buy you much.
I like haskell, but python is great too. Python is great because of its community which is huge. There are libraries in python for just about everything. Plus, PIP is much easier to use than Cabal. Python is a better first language than Haskell. That said Haskell is much faster than python, has a great web framework in Yesod, and teaches you many new ideas. Learn both. 
I haven't looked at the GHC generics solution at all. Now I noticed it explained in the haddock documentation in aeson package. Thanks for the info!
I can't agree enough. Haskell would be a fine first language, but the beginner accessible documentation just isn't there. Even as an experienced programmer, I found it difficult to find the information I needed to learn Haskell. A beginner would find it very frustrating. Python isn't the world's greatest language, but at least it has a lot of information available for beginners.
IMHO, the problem with Haskell as a first language is that in order to do IO (which is necessary to do anything interesting at all) you first need to understand monads, or you probably won't really get it. It's a pretty steep learning curve to do the simplest things--even if it is worth it when you come out the other end. Granted, when I got my first exposure to Haskell, I was already an experienced imperative programmer, who had to un-learn a bunch of things. It *might* be easier to start with a blank slate. I don't know.
The only problem I've had with learning Haskell is that I program embedded systems for a living in C. I now often find myself doing crazy recursive things, and forgetting that C is not a lazy language. As a result, sometimes my C code doesn't play well with memory (particularly the stack) which can be problematic on embedded systems.
Whoa, this workaround significantly raises the barrier to casual benchmarking: &gt; You should close busy tabs in a web browser (or preferably quit it entirely), kill your mail client and antivirus software, and try to eliminate other sources of system noise. You’ll be surprised by how big a difference these can make; anywhere from a handful to a few hundred percent. Since 12% variance in runtime is quite a big deal, is there any low-lying fruit at all? What if the heap was initialized at 300% size? 
Hmm, I thought I made it clear in the blog post, but maybe not: what Alexey did was very unrealistic, and you should not expect to see anything like that behaviour in practice. People don't have thousands (or hundreds, or usually even dozens) of benchmarks in their criterion programs. You will not be affected by this *especially* if you're just getting casual numbers. I also covered what you should expect if you change the heap size: it will affect your measurements in some way.
Whew, thanks for the clarify! EDIT: relevant graf, halfway down (emphasis mine): &gt; The obvious next question is: how realistic a concern is this? A normal criterion program consists of a handful of benchmarks, usually all very different in what they do. I have not seen any cases of more than a few dozen benchmarks in a single suite. **If only a few benchmarks get run, then there is essentially no opportunity for this inflation effect to become noticeable.** 
While we're on this topic, how does `criterion` deal with garbage collection produced by the benchmarked function? The reason I ask is that I've seen several instances where certain benchmarks give heavy outliers and I suspected that these were due to infrequent triggerings of the garbage collector, but I never was able to figure out how to prove it. However, I did have a clean system background, and I always run several known trustworthy benchmarks in the same run or independently which give clean results as a control.
I agree with Tekmo that this is a misconception. You do *not* need to understand monads to use `IO`.
A former coworker of mine put it best, I feel: "Every time I code in Java, I end up asking myself: 'Why am I still typing?'"
I don't know how it works elsewhere, but in Japan those kind of jobs are very rare. Most jobs require very specific experience, e.g. not only "Java" but "Java+Spring+BlablaSQL". And there are 0 Haskell jobs. So I agree with MachaHack, if you intend to work in software development don't choose Haskell as a first language. Python, Ruby, even Javascript would be better.
If you are going to compare funktional vs. imperative programming, at least don't create a strawman by doing one in spoken english, and the other in code. 
it almost doubled the performance...wow, amazing what a little more strictness can do. though not sure what you intended to use the PairS for...
&gt;You need to understand monads to do IO Just like you need to understand disk drivers to write to a file in python... that is to say, you don't. You can easily use `putStrLn "hello world"` without understanding the entire backend, then learn it when you're ready.
To throw a different anecdote in: I tried teaching some high school students with *no* programming experience Java. This was well *before* I knew anything about functional programming, coincidentally. They all had problems with the idea of a mutable variable. This actually surprised me, because I found the concept very intuitive when I first learned it. I suspect that this was probably the beginning of my gradual move towards functional programming. If you're starting out with functional programming, "variables" as such simply won't come up. So "variables as labels" is a bit of a straw man. My first CS class--based on SICP--started out with functional programming. Except it didn't say this; it just taught you programming which happened to be functional. So we never thought of variables as such; instead, we had functions with names and function arguments, with names. It doesn't make much sense to mutate either one! This is basically how variables tend to behave in other fields that beginners are familiar with like basic math and physics, so it shouldn't be surprising that it makes sense. Mutability was introduced later on in the semester (we were using Scheme after all), but I think most of the people with no prior experience actually found the functional introduction rather intuitive.
Meh, this is a matter of philosophy. There are really two ends you can approach programming and CS from--the low-level, EE end of hardware and stuff and the high-level end of abstract mathematics and logic. I personally much prefer starting from the high level and answering questions like "what does my code mean" rather than "how is my code run". Conal Elliot had a great way to explain the distinction: for some people, code exists to be run on a computer; for others, the computer exists to run their code. Haskell is entirely in the second camp, and I think it's a much better place to start.
Oh yeah, so I fixed the problem, even though it won't affect real benchmarks. Reload the blog entry for an update.
As [geezusfreeek](http://www.reddit.com/r/haskell/comments/1c7hiq/undestanding_runst_and_impredicativity/c9edrdr) pointed out, the whole point of the `s` variable is to introduce a correlation between `STRefs` and their `ST` context. How do you suggest making `newSTRef` make sense with `Poly`?
I like that take on it, very interesting. I came from a hardware background so I can definitely see how the "code exists to be run on a computer" philosophy may have placed a bias in my early software learning.
If you're familiar with operational semantics, it would help to write out the rule for sequencing two statements. The State monad is just a Haskell version of that rule. Something like: 〈S₁, σ〉 ⇒ 〈r₁, σ″〉 〈S₂, σ″〉 ⇒ 〈r₂, σ′〉 ————————————————————————————————————— 〈S₁; S₂, σ〉 ⇒ 〈r₂, σ′〉 It's not *exactly* the same, but this should give you the correct idea. If you're *not* familiar with operational semantics, then this probably won't help very much :P. I actually learned the two in the opposite order, and the state monad helped me with understanding this rule.
As opposed to Agda, where a lot of times, I end up asking myself: "Why is it still typing?"
Well that's exactly what I meant when I said &gt; then `s` has no reason to exist Although I should have probably said something like 'then you can't make the type machinery relying on `s` work'
Sure... you *can* do IO without understanding monads, but then the do block, and when to use &lt;- vs. when to use let becomes much less clear.
Thanks so much for this! SOAP is no fun, so it's not surprising that not very much has been available in an open source ecosystem like Haskell's. But SOAP is an essential ingredient of any complete set of libraries. Lots of heartfelt upvotes from me!
For some reason my first image was that putting a suit and tie on would make things better. :)
From the README, I understood that Sunroof is an EDSL for creating JavaScript code from Haskell. But it is a "deep embedding" EDSL, meaning that instead of just providing an AST that directly represents JavaScript code, Sunroof represents some JavaScript constructs by types that have a more Haskell-like feel to them. This places Sunroof somewhere between a conventional EDSL and a Haskell-to-JavaScript compiler. Is that a correct assessment?
These are great slides containing simple examples from a variety of useful Haskell libraries. Are these slides from a talk? It seems like you couldn't fit all that into one lecture.
Does anyone have the link to the talk?
I didn't recognise the acronym [HOAS](http://en.wikipedia.org/wiki/Higher-order_abstract_syntax) from slide 14.
This link goes crazy for me on an iPhone. The text changes size rapidly...
The slides mention llvm-pretty, is that still active? What's the best way to use LLVM from Haskell?
This was nice but the presentation is no good. I wish everything didn't have to be the flashiest, latest javascript explosion. Reading this on a phone was a disaster. Turning off JS fixes everything. Great stuff!
&gt; This link goes crazy for me on an iPhone. Yeah. I had trouble with it on an iPhone, too: occasional frantic jitteriness; it kept scrolling back to the top before I was done reading; the table of contents would pop up randomly.
Correct. Its is not a complete Haskell-to-JavaScript complier solution, but comes remarkably close. The biggest restriction is the use of ifB for conditionals.
If anyone wants to work on combining the two systems, we (KU) would be happy to help. We already plan to submit a Google summer of code to enhance Sunroof further.
But you can get around that by using thing like the Data.Boolean.Overload (http://hackage.haskell.org/packages/archive/Boolean/0.2/doc/html/Data-Boolean-Overload.html) together with rebindable syntax (http://www.haskell.org/ghc/docs/6.8.1/html/users_guide/syntax-extns.html). Though I don't know how cumbersome it is to actually use it right now.
Thanks! Can you still call the things you generate directly, or do you have to put it into a file and compile it with command line LLVM utilities? Otherwise perhaps the easiest way to target LLVM is to generate C?
It could be. Or it could be something else. Generally there is nothing to reduce run time and plenty of factor which could inflate run time: system scheduler, GC...
What are the things that you can do in LLVM IR that you can't (easily) do in C? I'm thinking of using C because then you don't have to learn LLVM IR, it's more readable and you probably get better error messages from the C frontend than from the LLVM IR frontend, and it's probably a lot easier to call C++ libraries because then you just compile with a C++ compiler and call C++ code directly that way.
LLVM IR lets you annotate the resultant IR code with metadata about aliasing that may not be expressible in C, for example. (Disciple emits LLVM IR partially for that reason).
Everything in C that you could do in LLVM and vice-versa. But then you buy into C's type system, pointer aliasing, and stack. EpicVM for instance generates C and is very pleasant to target ( https://github.com/edwinb/EpiVM ).
Same on ipad. 
Also, try to to break problems into simpler steps that you can then combine. It's a lot easier to reason about small functions than large functions.
The Fay example is wrong (`ffi` needs a top-level type-signature), but no matter, here's a page explaining [generated code](https://github.com/faylang/fay/wiki/Generated-code) and, just for fun, [reducing output size](https://github.com/faylang/fay/wiki/Reducing-output-size). If you want to see the pascal function [it's like this](http://hpaste.org/85784). And the whole version could [be like this.](http://hpaste.org/85785)
I've found the fastest way to debug in Haskell is write QuickCheck tests. If a piece of code fails without an obvious reason encode all your assumptions about the behaviour of the code as QuickCheck properties and let QuickCheck debug for you. As a nice side-effect you will end with tests for you code. 
It's probably not a good first language, because it's too difficult for most people. That may easily lead to thinking that coding is too difficult, when it's just that one language that is.
Well, NumPy and SciPy obviously aren't written in pure Python, and if memory serves me there are Haskell bindings for the same underlying libraries NumPy and SciPy use. But I'm sure it's much easier with the Python libraries to just get started crunching numbers.
What kind of semi formal methods?
http://dev.stephendiehl.com/hask/flat.html
I don't see why it's any more difficult than any other language. This seems to be an unfounded argument.
I'm not talking about you nor indeed anyone else reading this. I'm talking about 99% of people who will never be able to grok Haskell but may perfectly well be able to code a bit. But I agree with your first sentence: I also cannot quite put my finger on *why* Haskell is so hard.
The best way to debug a parsing error is to be able to see the whole program. The only obvious parsing error that I see is that a do block must end with an expression, and 'let' isn't an expression. Perhaps you want 'return (person : personList)' or something like that? This snippet has other problems, most importantly "let personList = person : personList" makes an infinite list of [person, person, …] since it refers to itself, and the pattern matches on value are not exhaustive. Note that if there were a previous binding for personList, it would *still* be an infinite list since this binding would shadow the previous binding.
"let ... in ..." is not an assigment, but scoping. to return something in an IO action you just... return it i.e. apply your data to a function named "return". The question is what are you trying to do with the result of this expression. So, yes, without more code context we will not be able to help you out.
Now that I've thought about it, I think using do notation (in this way) is an actively bad idea here. For instance: giveOne = do give ; one giveTwo = do giveOne ; giveOne What is `giveTwo`? Exactly the same thing as `giveOne`. But that's probably not what someone's going to expect.
Congrats Tekmo on the 1.0.0 release. I've been eagerly looking forward to this library!
Thanks! Also, I think you accidentally linked the mobile version, but it's okay. I'll include the link for the original here: http://www.haskellforall.com/2013/04/pipes-concurrency-100-reactive.html
The compiler thinks it's part of the previous do block. Indent the body of the previous do block (including the case expression). The return to normal indentation will tell the compiler that the do block has ended.
Why do you use Nullable? Couldn't you just do this? instance JSON a =&gt; JSON (Maybe a) where readJSON JSNull = return Nothing readJSON v = Just &lt;$&gt; readJSON v
This is awesome. Are you going to do a write up at some point about how you handled deadlocks?
This is nice, but it really isn't at all reactive programming, much less functional reactive programming. It is just, as it is named, a library for concurrent streams. The way to see this easily is to imagine the same library without deadlock detection. Now you just have channels. Is the lack of deadlock detection unfortunate? yes. However, do channels + deadlock detection = reactive programming by any stretch of the imagination? Not really.
I've had good luck with the existing LLVM bindings on os x. They aren't the very latest, but I managed to get everything up and running with the exception that dynamic linking doesn't interact properly with ghci sessions. I know GHC is generating IR directly, which makes sense because the llvm bindings introduce a bunch of requirements and dependencies. But what are the other packages avoiding the bindings? If you want to use all the nice features of llvm above and beyond just generating code (e.g., dynamic linking), then some form of bindings is a must. I wouldn't give up on them so easily!
I think that the ones [linked from Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia#Further_reading_3) are all quite good.
No, it's not the common case. The common case is a pretty sad act though if you look close enough.
You probably should if you want to know what using a real type system is like.
No. Because y'd have figured out "why" you "should" if you _really_ "should". Yes. Ditch the JVM, compile to native, unbrace your code and have a HM-typesystem. 
What's your goal? To pass exams? To get a (better paying) job (in a particular industry)? To expand your mind? You know Clojure. You don't know Haskell. Do you know Ocaml/ML? F#/Scala? Ruby/Python/Perl? Javascript?