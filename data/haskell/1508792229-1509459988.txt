It seems you are correct, I am unable to create a proper `instance Monad (IO [a])` that follows the associativity law. So I guess `IOLA` violates the `ArrowApply` laws most likely.
His main point though is right on the money. I wish reflex allowed to create event hooks and then link them to arbitrary widget event handlers instead of laying down the html in the monad. Would eliminate the need for all those rec monads and make the actual html creation compositional. 
I would love to see this in purescript :)
I for one is really excited about concur, I am not a web developer and every time I tried doing some very simple apps everything I tried feels clunky and hard for me. I feel like concur is fairly straightforward and that I could probably implement the small apps I want to make fairly easily and quickly. One thing that would really drive me to start using it is if it were on something like hackage and if there were simple instructions on how to get started with it and compile stuff with it. If I could use GHC and make desktop apps that's even better, but GHCJS could probably also work. Even if it's not ready to production, I'm guessing you'd still like it if people started experimenting with it, right?
When compared to Agda, Idris' syntax is easier to type, and its type system has just a tad more simplicity, especially in the case of `Type` being of type `Type`. Agda doesn't do this because of Russel's paradox, but Idris isn't as proof-oriented as Agda. In comparison to Coq, well there's theorem proving as a start; Coq's system is more specific for proving things, but Idris' type checker handles things on its own quite well - it's not what I have in mind when I'm writing code in Idris. Also, Coq's syntax is super difficult to read. As a whole, Idris is in my opinion terser and easier to read, though I admit that this might be my Haskell bias. 
It is because GHC is used to build GHC, and so it also accepts +RTS arguments. Note that `-with-rtsopts` is probably not doing what you're expecting, rather than setting the RTS that GHC should use when compiling, it is setting the rts opts that your compiled program will use by default. For the original report, it looks like you are running into this issue - https://github.com/commercialhaskell/stack/issues/3353 - which only occurred in unreleased stack, and I fixed on master a couple days ago. I'm guessing you're either: * On arch linux, which started distributing master versions of stack. This was a surprise to stack developers.. Particularly since the distributed versions did not include git SHAs and so they looked a lot like stable releases. * On some master version of stack that you built.
&gt; Idris' syntax is easier to type You can write Agda without using any unicode characters. `forall`, `-&gt;`, `\`, etc. are all valid &gt; its type system has just a tad more simplicity, especially in the case of Type being of type Type AFAIK it's not actually the case: under the hood you do have universe level and will a get a universe inconsistency error if you try to do something illegal. It's similar to Coq in that respect. If you do want `--type-in-type` in Agda, you can always use the corresponding flag.
Hey thanks for the response. I'm using 1.5.1 (I did `stack upgrade --force-download` too). I will double check tomorrow at work. Are you suggesting that my original attempt should be working?
Funny, I was interested in deca before I came to Haskell. I was a little sad to see it being abandoned.
/u/haskman: on the [GitHub README](https://github.com/ajnsit/concur), you write: &gt; Reflex/FRP [by /u/ryantrinkle] has brilliant ideas at its core but ultimately suffers from annoying flaws due to its flexibility and complexity. FRP is NOT the easiest model to think and reason about. **I've also found it difficult to refactor. For example, if you have to extract a bunch of events from deep within the DOM (though that may just be my bad coding practices).** [bold emphasis mine] This is not an uncommon complaint people have with Haskell programs in general. To make a modification, you may have to modify code in more parts of the program than you're used to - and battle the type system in the process. Many consider this *safety* to be a *benefit* of Haskell. I'm wondering, could your criticism of FRP be similar in nature? Namely, to modify parts of an FRP program may require relatively substantial refactoring because of the *safety* that the type system provides you with. If so, maybe that's not such a bad thing.
&gt; Are you suggesting that my original attempt should be working? Yes! However, not in the stack.yaml unfortunately. When looking into that, I noticed it's also splitting up the arguments and passing them individually to `--ghc-options`. This causes the rts of the cabal executable to see +RTS as rts options. So, part of the complexity here is making --ghc-options work the same as the ghc-options in stack.yaml, while also supporting both the usecase in #3353 and in #3315
Thank you for the summary. I feel that your two-sentence summary is clearer than the entire article.
It also avoids the hacker-mythologising that esr is so fond of.
Yup absolutely! I would love for people to use Concur and provide feedback. The aim is to make common things easy and uncommon things possible. Feedback from people using it in real world apps would really help!
Ooh, these thoughts had been rattling around in my head, too. Interesting ways to interpret the dependant+linear implication ("commemoration"), and dependant+linear pairs (??, maybe "consumptive existential"). Thanks for the links!
These sorts of questions typically get asked on IRC. I don’t think they belong on /r/Haskell. Does anyone agree?
Idris is already a nice dependently typed Haskell-like language. For Disciple we're more interested in computational features than program verification -- it's more fun to compute. 
It sounds like you think that a lot of your previous knowledge is transferable and given the things you listed it may well be true. There's a good way to find out if it's the truth and accomplish your goal I think. Skim through the chapters very quickly, and then do the exercises at the end. Try to complete the exercises, but be honest with yourself if you're struggling too much and go back and thoroughly the chapter. At worst, at the end of this process you'll have a big list of things you need to improve on most and know whether or not your goal of creating a scalable website is achievable as quickly as you want. I said all this with speech to text, so if it sounds weird or reads weird I'm sorry.
Right, thanks for that!
This isn't PureScript, you can't have two fields in a `newtype` :P
Same here. "I don't want to be programming with the hoi polloi" is a motto that would attract some and repel some :)
&gt; (I guess this is the right moment to mention Raphaël Proust's PhD thesis that was going in sort of the same direction: ASAP: As Static As Possible memory management, Raphaël Proust, 2017.) Thanks for this link! I skimmed through it, and as far as I can tell, it seems they've unfortunately provided high-level overviews of how each strategy works *except* for ASAP itself, for which they only give the in-depth, detailed explanation, which I don't have the time+energy to try to digest. Do you know if there's a paragraph-level summary of the basic idea somewhere, or maybe if there's a section of the thesis that qualifies as such a high-level description which I had missed, and should look at again? (I got as far as "the compiler inserts code at runtime in a granular fashion to check when something is safe to deallocate, if it couldn't be determined statically", but not really what the checks are checking (something to do with "paths") or why they're sufficient to determine what property.)
I'd argue with that. Depending on a specific database means *actually taking advantage* of that database. If you're not going to depend on database specifics, why choose a specific database at all? Rather than trying to swap between SQL backends, it's probably better to make a highly domain specific data access module that can be swapped over.
I gave a talk on this at LambdaConf titled "[I command you to be free!](https://www.youtube.com/watch?v=Ej5FQtEgTBw)". I should probably write up the Ruby code as a library, but then I'd have to write Ruby.
I appreciate the link to a similar post. I wasn't aware of ESR's post and perhaps Snoyman wasn't either. Even if he was, there's nothing wrong with writing your own take on something! 
Yeah. Monoids and Semigroups are phenomenally useful ideas and it's completely bonkers that such a simple, powerful concept hasn't saturated the programming world. 
You'll need to describe `ID3v2` to get a better answer, but I doubt that you'll need `StateT` to handle it. Also, since this seems to be something that you want to perform well, there's a good chance you'll want to make the fields in your data type strict and unpack them (except for the `Maybe`, which cannot be unpacked): data ID3v1= ID3v1 { title :: {-# UNPACK #-} !C.ByteString , artist :: {-# UNPACK #-} !C.ByteString ... , genre :: {-# UNPACK #-} !Word8 } I know absolutely nothing about the format you're parsing, but your code looks good to me. 
There is `EventWriter` in `reflex` that can be pretty useful for gathering `Event`s from deep within the DOM. I found `reflex` pretty easy to reason about after I spent quite a bit of time learning `reflex`. I'd put `EventWriter` into the pile of things that would correspond pretty strongly with "has learned `reflex` well". It could certainly be easier to learn about FRP though. I know of three different kind-of-FRP-like-but-less-composable-and-missing-features libraries that were partly created as reactions against FRP / through dissatisfaction with FRP, and I asked a few questions when each of them was announced and found something between zero to shallow understanding of what FRP is for / how it works. Two of them didn't think FRP offered anything at all for state management, which is a good chunk of what Behaviors are for. They're written by smart folks who took a look at FRP libraries first, but they didn't find anything they could get a handle on and then get on with the task.
I'm not sure presence of documentation has _ever_ been a problem in the Haskell community -- on the contrary, it's always the lack of documentation that's been a problem. I personally would prefer a alpha-quality library have production-quality documentation than production-quality library have alpha-quality documentation.
&gt; it will foster a deep sense of longing for a static type system during subsequent use of R This is so unbelievably true that it hurts. Sometimes, R feels like it's this weird, confusing hodgepodge where god only knows what type a function is asking for and what type it will put out. 
You have a small typo: "Yes, because I carefully definied `Class`" should be "defined". I [opened a PR](https://github.com/tomjaguarpaw/H2/pull/1).
With regard to 1, you're correct, except that Reflex is a pure FRP library, while Reflex-DOM is the corresponding widget library built on top of it. It's safe to say that the better comparison here is with Reflex-DOM, but at the moment the two are somewhat synonymous for most people, since Reflex-DOM is by far the most heavily developed Reflex host. For 2, I think you're missing the point of Reflex's widget-building monad to some extent. The point is that individual composable widgets can both produce Dynamic results which vary over time, expressing their current state, or which otherwise explain the manner in which the user is interacting with them via Events. So while I would agree that the monad structure in something like Lucid is trivial, that's not what we have here. Each widget which is built may produce results that describe its output (over all time). It's actually really nice that stuff like XML HTTP requests or websocket communication happens via events that are defined locally, and random stuff doesn't just happen everywhere. If you're debugging an app and you see "hey, this request is being made and it shouldn't be", you have an immediate way to find out what things could possibly influence that decision: you look at the definition of the Event which is firing off the request, and that provides you with a description of all the times that it will occur, in terms of other Events, and you have a clear game plan for tracking everything down. Now, you might go ahead and say "I don't care about being able to look at a concise definition, I just want to be able to fire this Event from anywhere in the app", in which case, I would say perhaps look into using EventWriterT, which allows you to 'tellEvent' from anywhere, and will merge and collect up all those event sources from throughout a widget. I would usually hesitate to employ that though, because you're losing out on one of the major benefits of FRP, which is that locality of definition, and being able to understand things in an equational way. For 3, I'm not sure I agree -- most of the time, this hasn't been too onerous. You only have to provide as much of an FRP interface as you really want to. You can, inside a performEvent, or during widget construction, carry out arbitrary Javascript if you really want to. It's good practice though, to wrap your uses of external libraries a bit. For example, at one point I needed to use the Google Maps geolocation API, and built a function: geolocating :: (MonadWidget t m) =&gt; Event t a -&gt; m (Event t (Coordinates, a)) That is, it's an invisible widget which takes an arbitrary Event, and tags each occurrence with the coordinates of the user's device at each moment when the Event occurs. We could then use that to tag an Event corresponding to a form submission, so as to be able to attach the coordinates and know where the user's phone was at the time they submitted the form. Schematically, the code for this just looks like: geolocating e = performEvent . ffor e $ \v ret -&gt; do p &lt;- getGeolocationCurrentPosition return (p,v) This little bit of impure code will run each time the Event e fires, it will execute the bit of foreign imported javascript for getGeolocationCurrentPosition (specifically, "navigator['geolocation']['getCurrentPosition'](this[0])"), and pair that up with the original value that the Event fired with. There's also performEventAsync, which will give you, in addition to the Event's value, an action that you use to fire the resulting Event explicitly. This really helps to disentangle the ~~horrible~~ marvellously Goldbergesque continuation-passing-style Javascript libraries out there, since you can just stick that in the handler for the Promise you get back, and now you don't have to think about all the nested continuations and results you're going to get back via handlers -- you can just always work with Events of tangible values, and the code remains flat. So I don't know, it's obviously a matter of taste, but personally, I find FRP actually makes many external libraries *easier to use*. I've been at it longer than most people though. As for 4, I'm not sure this is a bad thing about Reflex? The tradeoffs are not always 100% clear. Sometimes you *do* want to keep something around in the DOM and show/hide it via attributes so that the browser doesn't have to work as hard to re-render it, and other times, it would be silly to leave a complex and infrequently used part of your user interface lying around in the DOM. Control over rendering is very important. We actually recently added a bunch of stuff to allow you to hook the rendering of DOM changes to a widget, so that you can wrap it into a requestAnimationFrame if you like. One of our client projects makes some pretty fancy use of this to ensure that when loading new messages at the top of an infinite scrollback widget, they all get added to the DOM in one frame, and the viewport visually remains completely steady, despite the actual scroll position moving downward due to new content at the top. While it's important to promote doing things the right way, we'd also like it to be *possible* to do everything that needs doing, so being able to exert control over the exact contents of the DOM when you need to is pretty essential. I suppose I could talk more about other monads and monoids we can build on top of Reflex-DOM, but this message is getting pretty long. :) It's totally possible (and easy!) to build a Monad where each of the actions is a widget that fires an Event to finish, and that causes the next widget in the sequence to take its place. But this message is getting pretty long.
I had good experiences using it for a small utility website at a previous job. I remember it handling the case of not switching over if the new site wasn't working, but I didn't use it at any scale where I could confirm that it offers zero-downtime deployment.
&gt; highly domain specific data access module I'm certainly fine with a custom API, or an eDSL for data access. Something generic like HDBC is the first thing I look for but an eDSL is probably even better. I just wouldn't want to commit to a particular SQL database in the core application. I'm still fine committing to something SQL-like, but even that is questionable these NoSQL (not-only SQL) days.
At a glance, fingertree looks pretty nice. It's unfortunate that the implementation details of `IntervalMap` are hidden. IMO packages should expose `Internal` modules to make it easier to hack in missing features. If you can somehow open up the package, here's how to get what you want. An `IntervalMap` is represented as a sequence of intervals `[a, b]` ordered by the two bounds lexicographically. Furthermore every node maintains a measure of the intervals in its subtree, which is a triple `(a, b, c)`, where `[a, b]` is the smallest interval, and `c` the greatest right bound in the subtree. Thus, by comparing our key `k` to `a`, it is easy to first get all intervals that start *after* `k` to the right side, and the others to the left side. startAfter :: Ord v =&gt; v -&gt; IntervalMap v a -&gt; (IntervalMap v a, IntervalMap v a) startAfter k (IntervalMap t) = let (before, after) = FT.split (greater k) t in (IntervalMap before, IntervalMap after) where greater k (IntInterval (Interval a _) _) = k &lt; a So the closest interval to the right of `k` is just at the front of the right sequence. startsFirst :: Ord v =&gt; IntervalMap v a -&gt; Maybe (Interval v, a) startsFirst (IntervalMap t) = case FT.viewl t of Node i a FT.:&lt; _ -&gt; Just (i, a) FT.EmptyL -&gt; Nothing The closest interval to the left of `k` is slightly trickier. In constant time, we can get the measure at the root of the left sequence, in particular we get the greatest right bound, i.e., the point closest to `k`. Then we can just keep searching in subtrees where that upper bound is still reached. The library implements a generic `search` procedure. We just need to provide the "comparison" predicate, and the rest of the code handles the corner cases. endsLast :: Ord v =&gt; IntervalMap v a -&gt; Maybe (Interval v, a) endsLast (IntervalMap t) = case measure t of NoInterval -&gt; Nothing IntInterval _ high -&gt; Just $ -- "search nohigher" finds the first interval such that -- the right bound of the intervals to its right are less than the greatest one. let nohigher _ NoInterval = False nohigher _ (IntInterval _ high') = high /= high' in case FT.search nohigher t of FT.Position _ (Node i a) _ -&gt; (i, a) FT.OnRight | _ FT.:&gt; Node i a &lt;- FT.viewr t -&gt; (i, a) _ -&gt; error "Impossible. There must be an interval reaching the high bound" Finally, we put everything together. around :: Ord v =&gt; v -&gt; IntervalMap v a -&gt; (Maybe (Interval v, a), Maybe (Interval v, a)) around k t = let (lt, rt) = startAfter k t in (endsLast lt, startsFirst rt) Somewhere in `ghci` thirdAndFourth (Just (Interval 15 25,"Third"),Just (Interval 30 40,"Fourth")) 
I've used it a bit. It's just generally raw and under polished. Not to say it's bad, but definitely not my first choice for critical apps at this point. I use nginx and nix underneath so that updating the app simply changes the nginx config and runs a reload.
I use it for http://zoomhub.net/ which had about 12k page views last month. I never analyzed the graceful re-deployment but otherwise it has worked well for me. ZoomHub is open source, so you can dig into its CircleCI + Keter setup: https://github.com/zoomhub/zoomhub Hat tip to Michael Snoyman for the super useful library :)
I've always thought something like type class dictionaries describing how to GC each type might be promising, in that it might be amenable to a lot of aggressive compile-time optimizations which we can't benefit from with a typical written-by-hand-in-C one-size-fits-all GC.
Do you know if the memory usage is stable over multiple deployments? Any cases of log file-handles disappearing under load? 
Any specifics to support the "raw and underpolished" bit? Did it crash? Or does it lack features? 
Wow, thanks. Would you be interested in submitting this as a patch to the package maintainer? If not, would you mind if I did using your implementation? It seems like a useful function to me.
the `State` monad one is basically "roll your own in-memory database" that you can use for unit testing. However, you might also be interested with [monad-mock](https://hackage.haskell.org/package/monad-mock) for testing. I have not personally used it myself, but the usage looks promising.
This looks really cool, but unfortunately I don't understand what most of these features do (eg. what is "higher rank polymorphism with bidirectional type inference"?) :/ Where should I go/look to learn about this stuff?
Wait for the blog post. Just racing to deploy our first module built in Haskell. Then all these experience reports will follow. 
I have a couple issues open on it regarding documentation, and migrations: https://github.com/snoyberg/keter/issues/129, https://github.com/snoyberg/keter/issues/141 I recall trying it out and getting it to work pretty well. However, I had a hard time figuring out how to configure it as the docs were all but missing. My intention was to avoid the magnitude of nginx, but I'm pretty sure that nginx is easier to pick up simply because it has so much better support.
Thank you, could you please mention a in-depth article on how to use state monad for testing if do you have any! 
&gt; If, instead, you say "Linux sucks, you can't even get a f*&amp;$ing WiFi driver working!" thousands of people will solve the problem for you. http://thecodelesscode.com/case/170 
[A variant of that technique](https://ro-che.info/ccc/images/shootout.png) has been known to work in Haskell as well!
So, how does keter really work? Is it like tomcat where the Haskell app-servers are in the same RTS/OS-process as keter? Or is keter a replacement for haproxy/nginx acting as a proxy?
Pappaperm well spent? :]
this is a longer post on why and how from the library author: https://lexi-lambda.github.io/blog/2017/06/29/unit-testing-effectful-haskell-with-monad-mock/
Thanks! Fixed.
Living the dream!
The point of the book isn't that your core application should not be bound to any particular implementation ? You could say I need to persist some data but I don't know yet how I will do it. So your application knows only it needs to use this function to save and this function to retrieve datas. The details about it is done are in the function, but for your tests for example you can swap the method called without affecting your application behavior. I did not read the book yet, but I think that's what they mean by deferring the technical choices.
Useful complementary information, useless insult. Downvoted.
Shall I create an issue from this on Github or are you on top of this?
What about [Clojure](https://clojure.org/about/clojureclr)?
It sort of reminds me of a library that was once called mflow, later turned into transient. The libs I've mentioned had the ambitions to not only be for UI layer, but to structure your whole app, but I think the 'widget in time' notion was similar. It's extremaly appealing at first, but once going into the gritty details and various UI requirements (basing this experience solely on oldish mflow version), it might get tricky to get what you want.
Oh no! :)
I should have known that writing code without loading it in ghci first was a bad idea.
I think [quickcheck state machine](https://github.com/advancedtelematic/quickcheck-state-machine) also targets this kind of use case.
Keter works as a reverse proxy. It is very different than an nginx reverse proxy though. Some of its features: * Deploy entire application as a single file, just by copying (or rsync'ing) into an "incoming" folder. * Hot-swapping app versions - existing connections to the previous version keep running even after the new version is deployed. * Auto creation of new DB on first deployment, and auto connection to an existing DB (PostgreSQL only). * Auto forwarding of selected environment variables from keter's environment to the app environment.
Yes. We are using it for multiple high-load enterprise sites. It's very stable, and works fine.
[unpack](https://github.com/sam-barr/haskell-gui-snake/blob/master/Display.hs#L25) could be replaced by [concat](https://hackage.haskell.org/package/base-4.10.0.0/docs/Prelude.html#v:concat) 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [sam-barr/haskell-gui-snake/.../**Display.hs#L25** (master → 0534020)](https://github.com/sam-barr/haskell-gui-snake/blob/0534020b20cf221adf21cf2266918977b12490c4/Display.hs#L25) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dot7yj5.)^.
May I request you to please add a testimonial with some load stats to the keter repo/README?
Starts the incoming app in the same process as Keter, or a different process?
MFlow seems to be for writing servers, not compiled to JS for single-page client apps. I think agocorona started out by writing TCache and Workflow which were pretty nifty ideas when I checked them out years ago. Concur actually started out similar to Workflow and an even earlier framework called HaskellOnAHorse, where you capture all the user events and then pass them down a widget tree. I later replaced all that machinery with STM. Concur is still experimental, but so far has worked out pretty well. I've built a bunch of UIs with it and have found the implementation to be clearer and easier than other UI libs (Haskell or JS or whatever) regardless of the size of the UI spec. If you have any particular GUI spec that was cumbersome in some other framework then I'd give it a shot at implementing it in Concur. I did that once before with [Kirby Super Star Ultra Splits Timer GUI Challenge](https://gist.github.com/lexi-lambda/701f1f1282401059f13a4220e8178ba4) - [Source](https://github.com/ajnsit/concur/blob/master/concur-vdom/examples/KirbySuperStarUltra.hs) - [Demo](https://ajnsit.github.io/concur/examples/kssu.jsexe/index.html).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ajnsit/concur/.../**KirbySuperStarUltra.hs** (master → b68f9cd)](https://github.com/ajnsit/concur/blob/b68f9cd414bdaf66cdb4a5abe352438f19b2030d/concur-vdom/examples/KirbySuperStarUltra.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dot8rlo.)^.
How do i write a function which counts the number of occurences of a given operator in an expression? Does anyone have a good and simple solution that kinda explains for itself? data Expr = Num Double | BinOp Op Expr Expr deriving (Eq,Show) data Op = Add | Mul deriving (Eq,Show) countOp :: Op -&gt; Expr -&gt; Int (continue here)
Different process. Keter only runs applications that are native binary executables, so it wouldn't make sense to run them in the same process.
Got it, thanks. Don't know why I was expecting it to behave like Java app-servers, like Tomcat.
&gt; Even if he was, there's nothing wrong with writing your own take on something! It's often a good thing to paraphrase in order to write your own take on something. However, *if* Michael was aware of ESR's work there would be would be something questionable about it as it's generally frowned upon to appropriate another persons' ideas without giving appropriate credit. That being said, I'll just assume for Michael's benefit he was ignorant of ESR's work and consequently wasn't capable to willingly plagiarize ESR's work.
Thanks for detailed response Cale! I certainly meant Reflex-dom, and not Reflex. I am also one of those people who conflate the two! To distinguish, IMO - 1. Reflex (FRP + plethora of combinators) is absolutely fine. 2. The view layer parts of Reflex-dom (Monadic DSL for DOM layout + FRP combinators which let you replace parts of the DOM by consuming some Events) are fine (but can perhaps be more user friendly). The problem only starts when you have to start worrying about where you are getting those events from. 3. Extracting Events from DOM elements, and then threading them through the layers of the DOM Layout DSL, leaves a LOT to be desired. I haven't worked with Reflex as long as you have, but it got hairy for me very fast. It feels too low level and there is a distinct lack of application architecture. There is too much bookkeeping and many times supposedly simple things turned out to be complex. With enough effort I don't think that there is anything that Reflex cannot do and Concur can. But I wanted a balance of power. Something midway between the unconstrained but hard(-er) to use world of Reflex and the limited but easy to use world of Elm. I'll address the last part of your message, as it's slightly misleading. You certainly can have a Monad where each widget action leads to the whole widget being replaced by another. But as I understand, there is no way to do that without killing performance, since Reflex has no dom-diffing? Please let me know if that is incorrect. As I said earlier, with Concur, the easy way is the correct way and the performant way. 
Well in a way *all* compile errors are due to type safety, since syntax and types are all a compiler can check. The point is how easy is it to construct a semantically valid program, and how often the obvious way to write something is also the correct way. I did work with Reflex long enough to (I hope) not have fallen into basic beginner traps.
EventWriter must have emerged recently since I didn't see anything like it when I looked at Reflex. It seems interesting, I'll take a look.
Many thanks
Writing it in the simplest way might look like this: countOp :: Op -&gt; Expr -&gt; Int countOp op (Num _) = 0 countOp op (BinOp op' l r) = if op == op' then countOp op l + countOp op r + 1 else countOp op l + countOp op r Although for this kind of thing I'd usually write a fold and use that: countOp :: Op -&gt; Expr -&gt; Int countOp op = foldExpr (const 0) (\op' l r -&gt; if op' == op then l + r + 1 else l + r) foldExpr :: (Double -&gt; a) -&gt; (Op -&gt; a -&gt; a -&gt; a) -&gt; Expr -&gt; a foldExpr b f = go where go (Num n) = b n go (BinOp op l r) = f op (go l) (go r) 
Higher-rank polymorphism refers to type systems that allow you to express types like data Foo a = Foo (forall b. a -&gt; b) or `foldFree` from [`free`](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Free.html): foldFree :: Monad m =&gt; (forall x. f x -&gt; m x) -&gt; Free f a -&gt; m a Source That `forall x. f x -&gt; m x` term has a "rank-2 type". Bidirectional type inference refers to a certain way of writing typecheckers in which instead of only checking that a term/value has a type, we can also infer types from values. Information flows in the typechecker from terms to types and vice versa, hence the "bidirectional". Often, people refer to a fairly well-known paper coauthored by /u/neelk in this context, titled [Complete and easy bidirectional typechecking for higher-rank polymorphism ](http://www.cs.cmu.edu/%7Ejoshuad/papers/bidir/). A nice implementation of it by /u/lexi-lambda can be found here: https://github.com/lexi-lambda/higher-rank
I actually edited out a reference to row polymorphism there, replacing it by linear types. I have no idea what "dependent linear types" (h/t /u/vaibhavsagar) would be.
I've looked at Disciple more and I see what you're saying (and the OpenGL examples are very nice to see from an "it's fun to compute" perspective!). Have you thought about linear types, or, at least, some way to work without a GC?
Sure, [I opened an issue](https://hub.darcs.net/ross/fingertree/issue/4) to ask their opinion while I figure out this darcs thing.
I understood the first version you wrote. But I haven't seen the term "fold" before, will look into it further. Cheers!~
I use it in production with great success. Just be aware of common gotchas: http://sillybytes.net/posts/keter_tutorial.html
Thank for mentioning.
May I request you to please add a testimonial with some load stats to the keter repo/README?
How does this compare to the classic [ranged-sets](https://hackage.haskell.org/package/Ranged-sets) library?
Ranged sets are non-overlapping lists of ranges, so they can't represent arbitrary sets of intervals and don't have the efficient slicing enabled by fingertrees.
It's actually not quite Haskell, but a syntax inspired by it. It's really neat though!
It would be nice to have an intermediate language with a node layout/representation description algebra.
&gt; AUTOINCREMENT and RETURNING means I don't have to know anything about the current IDs before I create a new row. how could I keep track for the lasted or current id in order to generate the next one which is the current one + 1? I might missing a point or something
Interesting article, but from the title I expected a discussion about using type classes where a class doesn't make sense.
Hey, check out this repository: https://github.com/exallium/CleanHaskell I'm not a pro haskell developer, so it might be a bit basic but it's a rough idea of the architecture specified.
Ranged sets can be constructed from arbitrary ranges, including overlapping ranges. These are normalized to a non-overlapping set of ranges to support operations such as testing whether a point is a member of the ranges, or finding the nearest boundary point to a point that is not a member. It has a simple yet very general semantic interface, based on laws, and comes with a nice set of quickcheck properties for the interface. The internal interface uses lists, not finger trees, but I have used it extensively with large sets of data and have never run into any issues. Do you have an application where the difference between a simple list-based implementation and a finger-tree-base implementation is definitely the performance bottleneck? Otherwise finger-trees, while nice, sound like a premature optimization.
&gt; Finally, Num requires that multiplication is commutative. I don't think that's accurate. &gt; to implement &lt;*&gt;, we would require a function of type: `forall a b r. (forall s. ((a -&gt; b) -&gt; s) -&gt; f s) -&gt; (forall t. (a -&gt; t) -&gt; f t) -&gt; (b -&gt; r) -&gt; f r` It's not obvious that this type is uninhabited to me. I haven't found an inhabitant and I don't expect to, but an assertion isn't a counterexample, no matter how much *I* believe it.
https://www.postgresql.org/docs/9.5/static/datatype-numeric.html#DATATYPE-SERIAL. If you don't specify a value (or specify DEFAULT), the database generates one. --- https://www.postgresql.org/docs/9.5/static/sql-insert.html &gt; Insert a single row into table distributors, returning the sequence number generated by the DEFAULT clause: &gt; INSERT INTO distributors (did, dname) VALUES (DEFAULT, 'XYZ Widgets') &gt; RETURNING did; --- A good database doesn't depend on application logic to keep unique identifiers unique.
&gt; I don't think that's accurate. This is in PureScript where `Num` represents a field. &gt; It's not obvious that this type is uninhabited to me. To construct an `f r` (without some additional assumptions about `f`), you would need to either use the first function or the second, so you'd need an `(a -&gt; b) -&gt; r`, or an `a -&gt; r`, but all you have is a `b -&gt; r`. If `f` itself were `Applicative` then you could construct an `f (a -&gt; b)` and an `f a`, and combine them with `&lt;*&gt;`, but in general, without some sort of assumption about `f`, we can't produce such a term. So I'm not saying this is _never_ `Applicative`, but that if I want a counterexample, then I can construct one this way by choosing `f`. Or looking at it as a consumer, if all I have is `Coyoneda f`, then I don't for sure know I have an `Applicative`.
Well, the performance would depend on how often those switches happen, and how complicated the replacement is. If you want to update at 60Hz, rebuilding the entire contents of the DOM usually isn't a good idea. If you know which parts change specifically at the outset, you'll always save time by only having those parts be the ones affected by the Dynamics that are changing. It's by no means hard to do that in Reflex-DOM, it just means passing the Events and Dynamics defining how things ought to change down to the places where they're actually needed, and using them there. We *could* do DOM diffing in Reflex, and the only reason it doesn't exist is that we haven't really run into a situation where it was difficult to avoid needing it. I can imagine weird scenarios where you're constructing a lot of DOM in a way where it's hard to predict the outcome of that process, and what might be shared between subsequent variations of the subtree. Doing some work to diff the tree might save the browser from rendering effort in a case like that. However, in real-world GUIs, it's hard to find many cases where that really happens. Usually you either have things changing in a fairly predictable and well-structured fashion, or you're effectively changing between entire screens with essentially no shared structure. The most complicated examples tend to break down into dynamically adjusted collections of various sorts, where you're going to keep some items and add and remove them over time (and then the individual items might themselves change). But we have some dynamic list/map widgets, and they mostly take care of that. There's probably still a fair bit of room for improvement there, but it's really not so hard to use at present. I think the thing about Reflex-DOM which might be daunting at first is that indeed, it doesn't impose a rigid architecture on you, and instead, leaves you free to make the appropriate decisions for different parts of your application. It takes some time to learn how to make those decisions well, and they aren't always the same in every situation. To some extent, I think our problem is mainly one of education -- figuring out how to teach the most common patterns of usage, and pair those with our goals for modularity or extensibility. One thing which you certainly can do, for example, is to design all your widgets such that they produce Events reflecting user input, and consume Dynamics controlling their appearance. The limit of that approach is effectively a nice version of Elm's old model for FRP, where you define all the state (the holds defining the Dynamics) at the top level of your application, and only have to pass Events upward and merge them together into something that's going to manipulate your big Dynamic state, and then that will get refined and passed back down through the app to where it affects how things look. From a reuse of *presentation* standpoint, this is optimal. However, from a reuse of logic and local reasoning standpoint, it's pretty bad. You end up globally needing to know about too much of your application's state in one place, and having everything potentially affect everything else, when it would be nice if various reusable widgets could manage their state locally, and only inform the rest of the application about what really needs to be communicated elsewhere. So that's a choice that Reflex gives you: you get to decide for yourself where the Events get translated into Dynamics, which is usually right at the boundary of modular components. Then only part of that state may get shared via the result of the widget. Of course, this is a compromise in terms of reusing presentation: once you have a component like this, from the outside, you no longer have absolute control over how the user input influences its presentation, and it may choose to hide some user input from the rest of the application. But at the same time, that also absolves the rest of the application from worrying about it, and that's a key to modularity -- being able to make some decisions locally about how input is tied to output that the rest of the program doesn't need to worry about.
Thanks! (founder here). You are right that it is Haskell-inspired and not quite true Haskell. We can export to idiomatic Haskell, though.
Can't I compose the b-&gt;r with the a-&gt;b to get an a-&gt;r ?
How would you represent OP's `times` map as a ranged set? This normalization loses information about the original overlapping intervals. Interval maps also associate data to these intervals. A typical application of interval maps is in scheduling problems, where you may want to query what events are happening around a given point in time. The question of performance is secondary, but I also don't see a premature optimization problem. fingertrees can be used to implement the same interface as ranged-set. Then from a user's point of view they just need to change the import statement to get one or the other if they care.
In my reading of the comment it is not not "lowering the barrier to entry" that may end up harming GHC, it is opening up a further impedance mismatch with two PR paths or moving as a whole to a solution less suitable for core contributors that I think is being discussed.
Nothing I can give the details for - it was couple of years ago when I toyed with MFlow. But my intuition which I'm carrying and applying to Concur (which might be completely wrong, I haven't looked into it in details), is that it makes it easier to lay out "sequential" (in time) dependencies for your UI. And it made a bit harder to express the dependencies that would allow user to modify things later on. Suppose I've got some dropdown, then another dropdown with values that depend on the first one. Then, after selection is done, you're able to add some more details (entries to an array, just like in TodoMVCs), of which one field in each entry depends on the selection. So, if I get this right, the entries widget "starts" after selection is made, what if user gets back and selects something different? What would happen to the "entries" state?
The point you're missing is that, regardless of -which- DB you choose, the incrementing of the ID is a persistence layer problem. So if your application code needs to be aware of how the ID gets incremented, you've broken encapsulation and you've failed. So the question you are asking above is 'how do I write a mock persistence layer' - Which is an extremely open ended question. The high level answer to the question though is basically "don't be more specific with your types than you absolutely have to" - For example: data PurchaseOrder = PurchaseOrder {orderNum :: Int, items :: [Item]} deriving Show newtype RandomThing a = RandomThing {getRand ::IO a} deriving (Functor, Applicative, Monad ) class (Monad m) =&gt; OrderSource m where genOrder :: [Item] -&gt; m PurchaseOrder instance OrderSource RandomThing where genOrder stuff = let idNum = randomIO in RandomThing $ (\n -&gt; (PurchaseOrder n stuff)) &lt;$&gt; idNum You could easily define whatever instance you want to for 'OrderSource'. If you want to make it some flavor of an IO or state operation, use a newtype, like I've done here, and you can define whatever logic you want to generate your mock data. In Haskell, if you don't want to specify a specific implementation so that you're not tied to a specific implementation ... Then just don't specify it. That's it, that's the whole trick. You are never tied to a specific implementation of anything at all unless you wrote your type constraints incorrectly.
The title was meant to be a reference to Counterexamples in Topology. Maybe I should have called it Counterexamples in _Typology_ :)
I'd encourage you to try writing out the instance for a general `Functor f`.
There is [react-hs](https://github.com/liqula/react-hs) but it's pretty experimental at the moment. We are happy to have more people using it and contributing to it though!
The equivalent of reflex is hplayground, that run under the Haste compiler. http://tryplayg.herokuapp.com/try/todo.hs/edit The GHCi version is called Axiom, but the latter is more complex since the client run as a node in a cloud, using transient and websockets. https://github.com/transient-haskell/transient-examples
Do concur has algebraic composablity? I mean, if W and W' are widgets his composition wth `&lt;&gt;` are widgets? I do not see it in the examples.
Just because I can't figure out how to do it, isn't a non-existence proof.
As with http://comonad.com/reader/2015/free-monoids-in-haskell/, `NonEmpty` is not the free `Semigroup` - consider `fix (flip mappend ())`.
forgive me for asking a lot. I was checking the code base. however the term `slug` confused me, is it like a tag?
That's not how I read it at all. Even so, that's still an interesting statement that I would like to know more about. How would transitioning from Phabricator to GitHub (or whatever) harm GHC in the long run? 
One additional note: it is not "inversion of control" what concur uses, but "de-inversion of control" . ELM, react, Javascript callbacks etc do use inversion of control
Yes it does (the docs are a mess). You can use either `u &lt;|&gt; v` or `orr [u, v, ...]`to put widgets at the same place in the dom hierarchy. You can also use `never` as an algebraic identity. You can create dom elements and compose things in a hierarchy using the `el` and `el_` combinators. So for example a div with a heading - `el "div" [] $ el "h1"[] $ text "This is a heading"`. Finally you get to make `u` appear after `v` finishes, by using monadic bind, i.e. `u &gt;&gt; v`, or `u &gt;&gt;= v` if `v` depends on the value generated by `u`.
This* is such an important part of type classes and type classes are a crucial part of Haskell, I can't recommend this enough. \* An intuition for the boundaries of our abstractions.
Ah that's an interesting use case! The answer is simply, whatever you want! The point of Concur is flexibility. I went ahead and [implemented a rudimentary version of your spec](https://github.com/ajnsit/concur/blob/83cf5e31d8de6f32014bb7c5becfe98683053a5e/concur-react/examples/MultiEntry.hs). [Demo here](https://ajnsit.github.io/concur/examples/concur-react-multi-entry.jsexe/index.html). Here's the entirety of the widget code, sans the few lines it took to write a custom menu/doubleMenu control from scratch. I added some shouty comments to point out where the business logic could be changed - main :: IO () main = void $ runWidgetInBody $ flip execStateT (EntriesState "Red" []) $ forever $ el "div" [vattr "className" "main"] [ lift $ el_ "h1" [] $ text "Select a color" , selColor , lift $ el_ "h1" [] $ text "Make entries" , newEntry , lift $ el_ "h1" [] $ text "Current entries" , entriesList ] where selColor = do c &lt;- lift $ doubleMenu "Fruits" "Color" itemsFruit itemsColor -- HERE YOU MODIFY THE STATE TO BE CONSISTENT WITH THE CURRENTLY CHOSEN COLOR. -- KEEP THE OLD LIST OF ITEMS, DISCARD THEM, FILTER THEM, OR MODIFY THEM. -- I CHOSE TO KEEP THE OLD ITEMS AS IS modify $ \s -&gt; s { color = c } newEntry = do EntriesState {..} &lt;- get e &lt;- lift $ menu ("New Entry for " ++ color ++ " fruit") $ itemsFruitColor color modify $ \s -&gt; s { entries = e : entries } entriesList = do EntriesState {..} &lt;- get lift $ orr $ map (el_ "div" [] . text) entries 
Well, this post is in the context of PureScript, which is strict. Point taken, but does the same apply there?
Wow, I've been using the term wrong all this time then! Thanks for pointing it out!
Another example of a type that can be made an `Extend` but not a `Comonad` is [FoldM](http://hackage.haskell.org/package/foldl-1.3.3/docs/Control-Foldl.html#t:FoldM) from the foldl package. You can write `extract` because the result comes wrapped in a monad.
I'm guessing the point was to try and do it which is the usual method of proving something doesn't exist.
Hm, this looks very easy, indeed :) Thanks for having a go at this! I just don't have great intuition now for the `forever` widgets. Here it seems it's top level thing, because, well, at top level this UI interacts with user "endlessly". However, what would happen if I wanted to refactor it to a widget that I can then use somewhere else in UI? Let's say I want to be able to prepare two lists, one after another (I guess if I was to allow to do this in parallel, then I could just combine those 'forever' widgets with &lt;|&gt;. So let's complicate it, and introduce a sequence (forcing us to drop the forever)...
You need either (I) `(a -&gt; b) -&gt; r` or (II) `a -&gt; r`. Case I: To obtain `(a -&gt; b) -&gt; r`, you assume `a -&gt; b` and try to obtain `r`. Well you have `b -&gt; r`, which means you can compose that with `a -&gt; b` to obtain `a -&gt; r`. However, you still need an `a` somehow to get `r` out of it. Dead end. Case II: To obtain `a -&gt; r`, you assume `a` and try to obtain `r`. The only way to do that is through `b -&gt; r`. But then you still need `b` to get `r`. Dead end again.
Highly recommended!
Got any details on the actual product and the problem you are trying to solve?
I think that's the way on convincing *yourself* that it doesn't exist. It might give inspiration into a non-existence proof, but it's not how I've ever constructed one. I usually start a non-existence proof by writing down it's type, in this case: `(forall a b f r. (forall s. ((a -&gt; b) -&gt; s) -&gt; f s) -&gt; (forall t. (a -&gt; t) -&gt; f t) -&gt; (b -&gt; r) -&gt; f r) -&gt; Void, then proceeding to do case analysis, but that's difficult with the lambdas.
Why if I try both at the same time? Set my goal as both, assume `(a -&gt; b)` for the first, then compose, then assume `a` for my second, get a `r` by applying the composition, done. *waves hands* ... and fixpoint!
Thanks @tomejaguar!
[Here is a proof](http://try.purescript.org/?backend=core&amp;gist=83a683d0990753eec3917fb5612229fa) that if you have a `Functor f` and `Yoneda f` is `Applicative` then `f` must have been `Applicative` as well. I'll leave the laws as an exercise ;)
Ah yes, I can see how this can be turned into a non-existence proof. Thanks!
Apparently it's about 11 months old, so it's nowhere near as old as `reflex`. I guess the thing before that which would mark someone as having dug into `reflex` a whole heap would be understanding the "Dynamics in, Events out" starting point and where to evolve from there. Just to be clear (because text based communication is pretty lossy) - none of this is meant to be implying that you missed anything / didn't properly evaluate `reflex`. My point was more along the lines that there might be some answers to your concerns / frustrations that just aren't visible unless you spend a crazy amount of time digging in the code / chatting on IRC / iterating on various designs over and over. I'm trying to turn some of the stuff I've learned into blog posts, hopefully more of that will help somewhat :)
I'm not sure my brain could handle the move to NYC. I spend my first 18 years in a town of 300, and [NWA](https://en.wikipedia.org/wiki/Northwest_Arkansas) still feels like a big city to me. When I go to ICFP, I cower in my hotel between sessions. That said, I have plenty of Haskell, Java, and Python experience over the last 5 years, and 20 years experience in the IT field in general. I am very comfortable with flexible roles and constant communication via IM (Slack or similar) and email. I'm familiar with both engineering and non-technical tradeoffs in order to deliver MVP, and deliver value quickly, while still using process (CI/CD, review) and technology (static analysis, predictive log analysis) to ensure quality. My ideal role is cutting-edge language development or delivering verified code bases for use in environments where software faults are literally fatal. My current role is with POS (point of sale) software that, while it is undergoing modernization, is still mostly older than my career; I have no problem dealing with the quirks of old systems or low-level technology details. Moving to a more modern or open (as in open-source) environment at the same pay point would, I feel, be a good step toward my ideal role. If you become flexible about work location, please PM me and I can send you a link to my resume on Google Docs. I am also publicly [listed on LinkedIn](https://www.linkedin.com/in/boydstephensmithjr/) where my profile can serve as a CV; much of the information is the same. 
On top of it, it's tracked by #3315
Vote here for Servant, we use it quite a lot at work.
I put this in production around 2.5 years ago at a previous position. This was a startup which had on the magnitude of 100k hits/month if I remember correctly. We initially deployed on FreeBSD 10.1, though last I know of, everything was running on later versions in the 10.X series. Internal development at the company was halted, but I know the applications handled by keter are still scaled up/down by the automated process. Further, there, we had an Erlang application that I got to work with Keter, but had to abandon it because of the nginx reverse proxy issue. At the time I couldn't get around it while using cloudflare and didn't have a choice on using the service or not. That issue might be resolved now though. 
Karamaan is a great place to work. It's not at all your typical finance gig. The team is fantastic and very stimulating to work with.
Those three aren't mutually exclusive. The [servant-snap package](https://github.com/haskell-servant/servant-snap) allows you to use both Servant and Snap.
[removed]
I've been using Concur (the vdom variety) to build a dashboard for our database and it's been really wonderful. It's very easy to make general-purpose widgets that would be horrid to do in Elm. For example, here's a "loading" widget that you can wrap around any IO call to display a loading gif until the call completes: loading :: IO a -&gt; Widget HTML a loading action = el E.img [ A.src "/images/loading.gif"] [] &lt;|&gt; liftIO action To use it in some other widget: someWidget :: Widget HTML () someWidget = do r &lt;- loading $ someIOAction displayResult r This will display a loading graphic until `someIOAction` returns `r`, then it will run the `displayResult` widget on `r`, because Concur runs/displays monadic actions sequentially. I haven't used Reflex much so I can't make a good comparison, but I definitely enjoy Concur more than Elm and am much faster with it.
Seriously look into [nix](nixos.org) he total solves the devops "problem".
When I was new to Haskell yesod is the only thing that made sense to me (YMMV) because there was a lot of crap I don't care about that got generated for me and I just had to learn the format and where I needed to put things and I was ready to go. I later learned when all the code gen was doing but before that is was put this line there and that line over here then write a function to handle the request and BAM things worked. Sure the error message would sometimes confuse me, but I eventually got over it. A lot of the other Web frame works felt like they required me to know much more before I could be productive which was discouraging as a _mostly_ PHP and JS professional dev. I was always playing with other languages on the side though. Again YMMV
If you pick Scotty, pick Spock instead :-) The API is very similar but it offers a better safety net in terms of type safety and also a few more useful utils. (Disclaimer: I'm one of the Spock authors) Apart from that - if you are going for an API and you want to examine the API's type to derive other things like documentation, client libraries or mock servers then servant is a solid choice. Only caveat is that it uses lot's of type hackery which may or may not be a deal breaker for someone new to Haskell.
Just the person I wanted to reach. Do Spock and Servant play well together? Is that a question that even makes sense given that Spock looks to do type safe routing and that is a subset of what servant does?
Well, there's always x = x : T
I don't think that's much of a first-class citizen of Clojure-world. Maybe use F*?
no problem. slug is a SEO friendly string. so usually people turn an article with title like "Haskell Web Development Tutorial" to "haskell-web-development-tutorial" and embed it in URL (example: mysite.com/articles/haskell-web-development-tutorial) to rank better in search engine. from the case above, "haskell-web-development-tutorial" is called slug.
&gt;I don't think that TH is a good idea to rely on. Correct me if this is wrong (I personally haven't used much TH, so I'm going off of some other things I've heard). TH is plenty reliable, the issues start to arise when you want to cross-compile. &gt;at first glance it seems like it is pretty advanced type level hacking. Well for it for an API, in my opinion. Also, you can use servant for your API while using something else for the rest of the app.
You can definitely write an API using servant and then write a web app using something else.
I had the exact same experience. I understand why people avoid Yesod, but if you don't have any expertise with web, it'll be much easier to understand. 
That's cool. What do you think about using the OverloadedStrings and type aliases to notate the proper types of your arguments instead of using comments to do so? That'd yield safer extensibility without limiting your usage of direct strings/text in constructing instances of the data types.
Thank you for this great work. A small typo in the last code snippet of "Controlling the Future with Futumorphisms": futu f = In &lt;&lt;&lt; fmap worker &lt;&lt;&lt; h where Should be: futu f = In &lt;&lt;&lt; fmap worker &lt;&lt;&lt; f where 
&gt; I think that's the way on convincing yourself that it doesn't exist. Sadly, that is not universally true, or there would not be people trying to build perpetual motion machines.
I'm quite a fan of `servant-snap`; I use it in https://matrix.hackage.haskell.org/'s backend. Besides being easily combined with non-servant Snap handlers it also allows you to easily bypass limitations in what `servant` can currently express, like if you want to implement a specific form of Etag handling.
Technically a `Semiring` because an empty plate is definitely not Lasagna.
&gt; TH is plenty reliable, the issues start to arise when you want to cross-compile. ...or when you need to target a platform which doesn't support TH even when non-crosscompiling :-)
That first comment was far too entertaining to me.
First to clarify one thing, wrapping a widget in `forever` means that that widget never finishes, but it isn't necessarily at the top level. It is a regular widget with type `forall a. Widget HTML a`. You can still use the widget in other places but you can never get information **out** of it. Thanks to its type it can be `&lt;|&gt;` with any other widget at all, so you can stick it pretty much anywhere on the page and be guaranteed that it will not impact any logic, or will not require making any other changes to other code. It's the same situation with display-only widgets like `text "Hello"`. It has the same type `forall a. Widget HTML a` and can be put anywhere on the page without impacting anything else. When nesting a widget, the type of the child widget becomes the type of the whole widget, so for example a heading `el_ "h1" [] (text "This is a heading")` has the same type as `text` i.e. `forall a. Widget HTML a`. The types usually just work out without thinking. However, in reusable widgets, we would typically not have a top level `forever`, because we would like the widget to impact the rest of the program at some point. So coming back to the entry list example, we can easily extract it into a reusable widget, something like - entryWidget :: EntryState -&gt; Widget HTML EntryState entryWidget (EntryState {..}) = go color where go col = el "div" [vattr "className" "main"] [ elLeaf "hr" [] , heading "Select a color" , Left &lt;$&gt; selColor , heading "Make entries" , Right &lt;$&gt; newEntry col , heading "Current entries" , entriesList ] &gt;&gt;= either go (\e -&gt; return (EntryState col (e:items))) heading = el_ "h4" [] . text selColor = doubleMenu "Fruits" "Color" itemsFruit itemsColor newEntry col = menu ("New Entry for " ++ col ++ " fruit") (itemsFruitColor col) entriesList = orr $ map (el_ "div" [] . text) items Typically Concur widgets perform one action and then exit with the results. Here, the widget takes in the current state of the Entry, and then returns with the modified state as soon as a new entry has been added. Now we can use it as many times as we need. In this main function I am displaying a list of EntryWidgets, all of which can be used independently - -- Main main :: IO () main = void $ runWidgetInBody $ flip execStateT (entriesStateInit 5) $ forever $ do EntriesState {..} &lt;- get (i, e') &lt;- lift $ orr (renderEntry &lt;$&gt; zip [0..] entries) put $ EntriesState (take i entries ++ [e'] ++ drop (i+1) entries) where renderEntry (i,e) = (i,) &lt;$&gt; entryWidget e I have uploaded the full [Code here](https://github.com/ajnsit/concur/blob/master/concur-react/examples/MultiEntry.hs). And updated the [Demo here](https://ajnsit.github.io/concur/examples/concur-react-multi-entry.jsexe/index.html). 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ajnsit/concur/.../**MultiEntry.hs** (master → f2c14d9)](https://github.com/ajnsit/concur/blob/f2c14d9201ade26c714c4bdd35a6818aa1b56673/concur-react/examples/MultiEntry.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
I actually wrote a type I called "Lasagna" once. But it wasn't a monoid. My lasagna was either a single layer `L`, or a number of layers separated by a `S` value: `LSLSLSL`. This type is not a `Monoid` unless the layer type `L` is itself a `Monoid`. But it is a `Bifunctor`, `Bifoldable`, `Bitraversable` and also a `Comonad`. Ultimatedly the type grew in complexity to be recursive, so the simple lasagna metafor didn't work that well anymore. I split it into a type called [Steps](http://hackage.haskell.org/package/plan-applicative-2.0.0.1/docs/Control-Plan-Core.html#t:Steps) and another called [Timeline](http://hackage.haskell.org/package/plan-applicative-2.0.0.1/docs/Control-Plan-Core.html#t:Timeline).
Is this true though? I can't remember the layering rules for lasagna but most people cap it with grated cheese which doesn't appear anywhere in the other layers. 
Oh fair enough. I've gotten it to work on ARM so so far I've not had too many worries, but I guess that's not always the case :)
It depends on what you mean by "play well together". Spock can support embedding a Servant app via the `middleware` combinator, and as far as I know the other way around works too. Spock and Servant can't really be compared like that because they work differently. For Servant you specify the type for your API up front and them implement your server or client against that, where as for Spock you spend most of your time at value level and simply have the type checker make sure you are doing the right thing. Servant is also geared towards Rest-APIs, I've found dealing with raw POST Data and File Uploads really awkward. They may have changed something since the last time I looked though.
Glad to hear it! Also, thanks for submitting issues and providing feedback on github - it really helps me improve the library!
Cool, many thanks for answering my (not so well written) questions in such depth. Just one thing. When you extracted the `entryWidget`, you've peeled off the 'forever', and it required the state to be pushed one level up, so now somewhere higher in hierarcy we run `execStateT`. This means, we follow what Elm is doing. What if I don't want to be fussed about my children state? PS. I checked the source code, it's surprisingly simple :)
If they're going to the trouble of removing all implied boxing, I suspect that they would also want to avoid pervasive GC. You can do away with GC by "just" making sure that in-memory objects, e.g. closure bindings do not outlive anything that they point to. Rust does this with compile-time checking as far as possible, and with lightweight, special-cased runtime bookkeeping ("reference" pointers and refcounting) when necessary. In principle, you could also have lightweight 'gc' that only ever gets applied to the parts of the program that really need it (cycle-prone, spaghetti-like reference graphs), but so far the Rust folks have gotten away with simple refcounting as a practical matter.
Take the one with the best documentation. This probably means servant.
Like /u/agrafix said, it's not really that they play well together, you can't really "combine" them. But you can embed a servant app in a spock one and the other way around, yes. This is true of any pair of libraries that end up compiled as a `wai` `Application` and which allow users to embed other `Application`s. This is true for servant, scotty, spock, yesod and maybe others.
Have to add my voice in agreeing here. I made a whole CMS in Yesod as my first ever Haskell project, it is quite easy to get started with, especially with the Yesod Book (which is free online). &gt; Since most of my other teammates will be new to Haskell as a whole, I don't think that TH is a good idea to rely on Honestly, the TH that Yesod uses, it does so for a very good reason. I only ever encountered it in Routes and Models (I guess you can count Shakesperean templates too), where there are so many peculiarities that can go wrong, that it just adds so a nice safety net for you. Take for example your DB models, Person name String age Int Maybe deriving Show BlogPost title String authorId PersonId deriving Show and you are done. Couldn't really be cleaner than that, and all the neat functions you need are generated for you. As a last remark, Yesod doesn't force you to use any of the TH at all, neither its default templating language, so you can really go your own way with this.
That's yet another one for the *Counterexamples of Typeclasses*! 
&gt; Seeing as the authors are going to the trouble of removing all implied boxing, I suspect that they would also want to avoid pervasive GC. I don't see things this way. I can perfectly see why you would want fine-grained control over the value representation and minimize boxing, yet be willing to afford a GC. If you look at the code examples shown in the Readme, they are very close to standard functional programs (with dependent types). Moving to a system of static memory ownership tracking could add significant complexity and change the code style a lot. I would also be careful with ideas such as "Rust has demonstrated that you can do just fine without a GC". Maybe the kind of programs written by the Rust community are specialized enough, for now, that the costs of using refcounting instead of a tracing GC have not been made apparent yet. One reason why there is no pluggable GC in Rust is that it is very difficult to combine the Rust approach with an efficient (precise) GC (those tend to want to have their say in the choice of value representation, which is precisely the question raised here) -- this may be the sign of a problem rather than of a solution.
Now this is shitposting. I love it.
If you liked the paradigm shift from Haskell, you will *love* Idris. There's a whole new universe of types waiting to be discovered over there! 
Really? They're wildly different beasts, but you say the leading criteria should be the quality of the docs?
If you can use a generated client-side API wrapper that is generated by a servant library (or the docs), then that is a big reason for going with servant. I'm no fan of Snap. And the case for Spock over Scotty has already been made in another comment.
&gt; ... Maybe the kind of programs written by the Rust community are specialized enough, for now, that the costs of using refcounting instead of a tracing GC have not been made apparent yet. ... Well, this is exactly what I meant when I said that the Rust folks have "gotten away" with simple refcounting so far, with no practical need for 'pluggable GC'. (Though it's not clear why one would describe C-like system programming as excessively 'specialized', and lightweight refcounting as being *less* costly than pervasive tracing-GC!) The Rust approach is not the same as 'static memory ownership tracking' either, AIUI. It's memory ownership tracking that's "static" as far as practical, but then switches to increasing levels of "dynamic" runtime support as needed. Of course some sort of static analysis is involved, but the constrained in-memory representation that most pervasive GC's use (e.g. 63-bit integers only, because the LSB is needed to tell pointers apart!) is a sort of de-facto static analysis, too.
&gt; ... Maybe the kind of programs written by the Rust community are specialized enough, for now, that the costs of using refcounting instead of a tracing GC have not been made apparent yet. ... Well, this is exactly what I meant when I said that the Rust folks have "gotten away" with simple refcounting so far, with no practical need for 'pluggable GC'. (Though it's not clear why one would describe C-like system programming as excessively 'specialized', and lightweight refcounting as being *less* costly than pervasive tracing-GC!) The Rust approach is not the same as 'static memory ownership tracking' either, AIUI. It's memory ownership tracking that's "static" as far as practical, but then switches to increasing levels of "dynamic" runtime support as needed. Of course some sort of static analysis is involved, but the constrained in-memory representation that most pervasive GC's use (e.g. 63-bit integers only, because the LSB is needed to tell pointers apart!) is a sort of de-facto static analysis, too.
How about a monad which is not an applicative? :) For example, let `Gph` be the category of (directed, multi-) graphs, and `T : Gph -&gt; Gph` be the monad that maps a graph to the underlying graph of the category it generates. Clearly, this is a monad, but if it were applicative (i.e. lax monoidal with respect to cartesian products), then in particular you would get a morphism `TX × TY → T(X × Y)` for all graphs `X` and `Y`, but there is no such morphism for say when `X` is two vertices and an edge between them, and `Y` is a single vertex. 
Another +1 vote for Yesod. We use it for a major web application deployed by our enterprise customers on some of the largest sites on the web. The TH is a big plus of Yesod - the simple type-safe DSLs built on TH make setting up and maintaining sites easy and reliable, and enable smooth interaction with other engineering teams that are non Haskell programmers. If you already have some familiarity with MVC frameworks like Rails, Django, etc., Yesod is probably the easiest and quickest way to get up and running even for a simple site.
Although if there *was* grated cheese somewhere in there, I would still call it lasagna. So I think it works. 
&gt; issues start to arise when you want to cross-compile What's the use case for cross-compiling a web app? You generally run a web app on a platform that fully supports TH. In the modern VM ecosystem, there is rarely a case where you would want to cross-compile to deploy to any common platform.
What platform that does not support TH would you conceivably be targeting for a web app?
It's not like Elm at all despite superficial similarities, because in Elm you are *forced* to handle all State. In Concur you can do what you want. You are only forced to handle the state you actually want to use at the current level. So while I managed state for each `entryWidget` inside `main`, I could have simply ignored it altogether, by forcing the child to manage its own state - loop :: Monad m =&gt; (a -&gt; m a) -&gt; a -&gt; forall b. m b loop w a = w a &gt;&gt;= loop w encapsulatedEntryWidget :: Widget HTML a encapsulatedEntryWidget = loop entryWidget entryStateInit And then use it without caring about the internal state of any children in the main function - main :: IO () main = void $ runWidgetInBody $ orr $ replicate 5 encapsulatedEntryWidget Here, `encapsulatedEntryWidget` is a completely self encapsulated widget. Logically equivalent to the first version with `forever`. If however you want to do something with the state in the parent widget you will need to extract it and manage it yourself in the parent widget. Does that answer your question? If not, I think you may be trying to think of Concur as a widget graph where child widgets retain their identity, when it's really a widget algebra. You combine widgets to create new widgets, *and lose their identity in the process*. In this algebra it's not possible to have a `widgetA` that is composed of a `widgetB`, but oblivious to the internal state of `widgetB`, and then have the internal state of `widgetB` be accessible from some third `widgetC`. I mean it's possible with IO/STM effects and/or black magic, but you want to avoid that as much as possible. 
&gt; but the constrained in-memory representation that most pervasive GC's use (e.g. 63-bit integers only, because the LSB is needed to tell apart pointers that might keep some object alive, i.e. 'own' it!) is a sort of de-facto "static analysis of memory ownership", too. I'm not sure what you mean by "static analysis" in the context of data representation choice. My point was that tracing-GC-friendly data representation choices let you *not mention memory* at all in the source of the program, and gives a very lightweight programming style. This is not something you can get in any of the proposals to "avoid pervasive GC" that I know of. Ideally you want to be able to write parts of your program in this "I don't speak about memory" style, and then some other parts of your program in a memory-minded style (with lifetimes etc.). This may be attainable with some explicit-memory-regions schemes, but my understanding is that nobody quite knows how to do this without either (1) globally using a GC-friendly memory representation, or (2) making the GC-ed parts of the program less efficient, or (3) making it costly for the non-GCed region to point to GCed data (negating some of the expected benefits of the non-GCed part).
I've been implementing web-services which run on IBM's AIX (not my choice tbh).
Can someone check if my understanding is correct? :/ `Comonad`s define an _extend_ and _extract_. A `Maybe a` could not be a comonad because it cannot satisfy `extract :: w a -&gt; a`, because for example while you can do `Just a -&gt; a` in a pattern match, you cannot do `Nothing -&gt; a` since it contains no element, a.
What's the value of mempty?
&gt; My point was that tracing-GC-friendly data representation choices let you not mention memory at all in the source of the program, and gives a very lightweight programming style. This is only true if you do *not* regard the tracing GC itself as being part of the program - other than that, it is just as true of tracing GC as of any other sort of runtime support, including explicit `free()`, lifetime annotations etc. All of these memory-management strategies are self-contained; the only way in which they "mention" memory is as part of the strategy itself. If anything, the user of tracing GC does in fact need to 'mention' memory inasmuch as she need ensure that the way data is represented/layed out in memory is suitable for the garbage collecting code. This is not different in principle from the requirement of a user of explicit `free()` to ensure that each call to `free()` is paired at runtine with its corresponding `malloc()`.
My vote goes to Scotty. It's easy and clean. Not going with Yesod because I don't really like many decisions made within the framework. Not going with Spock because it cares about Database &amp; Session. Not "minimal" enough for me. Not going with Servant because it's hard to grok how it works. There was something that I wanted to do but I'm unable to do it with Servant and that was a deal breaker. Unfortunately, I forgot what that was :(
We’re not your normal startup: we’re not building one product targeting a specific problem. Instead, you’ll build a bunch of different internal products to solve internal problems. All of these products will be addressing problems in economics, finance, markets, or, usually, a combination of the three. You’ll initially be working on problems concerning organization and access to data—data loading, verification, and normalization—so other members of the organization can easily access this data. Later on you’ll probably be working on data processing pipelines—machine learning pipelines, MapReduce calculation engines, etc. I’m sorry I can’t be more specific, but we haven’t figured out how to solve all of these problems; that’s something you’ll be helping with. The type of person who will be most successful in this position will have a bit of a consultants mentality: comfortable working on a variety of different types of problems and sometimes not knowing what they will be working on tomorrow. Let me know if you have more questions or I can provide additional details.
Mathematically I think you can argue that an empty plate is the trivial instance of any serving. Like an empty set. Although if you're a cook your clients may disagree.
&gt; Though it's not clear why one would describe C-like system programming as excessively 'specialized', and lightweight refcounting as being less costly than pervasive tracing-GC! Reference counting traces dead data. As a result, if you have code with a high allocation rate of data with short lifetimes, reference counting tends to be much higher overhead than copying GC (which only traces live data). The Rust/C++ approach is to ask programmers to explicitly choose between stack- and heap-allocation, with the idea that you stack-allocate anything with a short lifetime. The intent is to ensure that the total amount of reference-counted data is small, so that the overheads aren't too bad. Obviously, this doesn't work well if you have a lot of data with very complex lifetimes (eg, the DOM or scene graphs). Less obviously, the Rust/C++ style costs you tail-call optimization, since destructors have to be invoked before a function returns. This can make writing programs with interesting control flow much harder. 
I don’t think they care, it sounds like the empty plate is _free_
I have been interested in trying yesod without TH, or reading more tutorials that demonstrate using Yesod that way. 
A library is not useful to those that can't learn how to use it.
Oh, I never realized that Monad does not imply Applicative in the general case. TIL.
This is interesting. By my calculation `TX x TY` has an edge between its two vertices and `T(X x Y)` doesn't. (There is actually a morphism from the latter to the former but presumably there's not a natural transformation in general.)
I usually do sauce, (pasta, sauce, cheese)*, pasta, cheese
OK, but how does anyone take any value out of such widget if it just loops and never returns?
Regarding DB models, [here's an example of servant using Yesod's persistent library](https://github.com/parsonsmatt/servant-persistent).
Do you offer relocation, sponsoring for a visa or do you have any other legal constructions in place to get a well educated and experienced European to New York? 
Okay well let me ask you, what would the ideal API for this look like? Perhaps we can add this feature :)
What are the morphisms of `Gph`? is-a-subgraph-of?
Under the GitHub model, each of my commits would likely be an independent pull request. However, then what I am to do when I rebase (which, as I mention above, happens rather often)? I will lose my review history on every revision of my branch.
You can just embed the semigroup in a monoid by adjoining the empty plate.
Hell if I know! :) At this point I haven't yet spend enough time with Reflex to see whether it would be optimal, or whether I'd (much like you) want to pursue something simpler. I was just trying to wrap my head around the differences, because it looked like it just have to be some trade-off.
Nice! Monads are monoid objects for `Compose`, and applicatives are monoid objects for `Day`. So in Hask, I think we have a natural transformation `Day f f ~&gt; Compose f f` which corresponds to `ap` essentially. This turns any `Compose`-monoid into a `Day`-monoid, but I'm not sure if this mapping can be expressed in any category, so maybe that's the issue there?
&gt; Reference counting traces dead data. As a result, if you have code with a high allocation rate of data with short lifetimes, reference counting tends to be much higher overhead than copying GC (which only traces live data) Rust does not use reference-counting exclusively; it is quite unlike, e.g. Swift. It supports stack allocation, direct "ownership" (for tree-like allocation patterns) and also arenas; the latter can be used to free a whole subtree-like graph of "dead" data in a single operation, without chasing any references. This should significantly mitigate the issue you mention. &gt; Obviously, this doesn't work well if you have a lot of data with very complex lifetimes (eg, the DOM or scene graphs). Yes, but pluggable GC should work fairly well for these sorts of easily-identified "graphs": the graph object might "own" its contents and manage these allocations with a tracing-like strategy. Some tradeoffs are required as gasche mentions, but overall it should be workable.
Interesting that you pick servant over yesod for docs, are they much better? I've only really dived into yesod (book and docs) and skimmed others briefly 
What is isomorf written in? Super cool project and good job on the execution!
I'm curious what decisions you don't like that yesod did? Do you realize you can create a "minimal" yesod project that basically only has routing and then do whatever else you want? You are not forced to use one of the templates (though it does help) Yesod is what I know best but I'm always willing to learn if there are better things out there.
Curious if you have people at work that use servant but have little or no Haskell experience? I'm wondering if the types used blow their mind ;) it messes with mine a bit haha Very cool though, it's on my list of things to understand.
I think if you have a CCC, and `Day` is built from the categorical product for that CCC, then the statement that every Monad is Applicative might be true.
I would argue that, as the absence of a layer of lasagna is keenly felt, it must exist, and that should it exist, it bears most in common with the presence of a layer of lasagna. Therefor, the absence of lasagna itself must be considered lasagna. 
I think the morphisms in question consist of a map `f` from vertices to vertices along with a map from edges to edges `g` such that if `e` is an edge from `v1` to `v2` then `g e` is an edge from `f v1` to `f v2`. Graphs under subgraph inclusion would also form a category but I suspect it would have less interesting structure. 
&gt; This is only true if you do not regard the tracing GC itself as being part of the program Indeed, because the calls to the Tracing GC can be automatically and safely inserted by a compiler that needs no information from me. GC can be made just as transparent as register allocation. &gt; his is not different in principle from the requirement to ensure that each call to `free()` is paired at runtime with its corresponding `malloc()`, and that no pointers to an object are live across a free() of such object. There is a huge different, which is memory safety. With a GC (either called manually, the way you see things, or with compiler-inserted calls the way I see them), checking that GC calls are correctly made is a purely local check (each allocation point needs to protect its local roots). This can be checked by a local analysis that requires no annotation from the programmer, whereas checking that free/malloc are used well requires reasoning about the global flow of values, and a scalable/modular analysis requires annotations on functions, and in general on abstraction boundaries. This is why I think one can claim that GC make lighter/simpler source languages possible. (Refcounting is sort of half/between, with in theory a simple scheme to insert refcounting operations and check their safety (even if you allow local refcount elisions), but the issue of cycle handling that either requires an extra tracing GC or source-level annotations of weak edges, which are similarly harder to statically verify.) 
Which works great until there isn't a library for your task :)
Also Agda (which is more of a proof assistant, but transpiles to Haskell).
Sauce at the bottom? Heretic! 
I wanted to run it on a raspberry Pi. It's a very specific use case but it was notable how much simpler the Rust tooling was to use. 
So if I want to use Servant as an API specification and Spock as an API implementation I could just use Servant as a first layer middleware for Spock and use Spock for the rest? I'm leaning towards including Servant no matter what at this point, even though the types are tough. The question at this point I think is what to use to do the actual implementation, and it sounds like Spock is one of the better choices here, unless theres some incompatibility I'm unaware of.
I'm not saying Scotty is objectively better. This is actually subjective: In general, I prefer writing as many things as possible with the host language (in this case Haskell) compared to using special syntax. So, I prefer writing routes using Haskell syntax instead of special route syntax in TH that Yesod use. Scotty routes are just basic Haskell. I also prefer if each route definition is nearby controller implementation. Scotty does this. Yesod splits route definitions and controller implementations. I like writing HTML using `blaze-html` in Haskell better compared to using special syntax (hamlet). Because I don't need to learn new syntax and I can reuse the power of host language. I prefer not to use ORM which means I'm not using Persistent. &gt; Do you realize you can create a "minimal" yesod project that basically only has routing and then do whatever else you want? Yeah, I heard Yesod framework is pretty modular. But since I don't like most of it, there's not much for me to use. I would as well pick another framework that suits me better.
I'm digging projectional editing. I'm working on a similar editor for my [Duet](http://chrisdone.com/toys/duet-delta/) language. Here's a short vid [gif](https://i.imgur.com/IDyTJu1.gif) on writing expressions, this builds up a Duet AST ready for type-checking as in the link above. I whipped this up last weekend. My goal is that you can type like you would normally do and it creates structure accordingly. If you want to write `f x y` you just type it. If you want `f (x y)` you type `f SPC ( x SPC y`. If you want `case x of p -&gt; q` then you just type `case SPC x TAB p TAB q`, or `\` to get a lambda. If you want to change `y` to `x` just hit backspace and type `x`. Additionally, when in an expression, or a pattern, w/e it'll show a dropdown of `case/if/lambda` for working on an iPad where keyboard work is arduous. I had a go at writing some code in isomorf and it's very "builder" oriented: you type `(` to make a function call which pops up a dialog with completion, or hit `RET` to type a variable which opens a dialog, or `?` to produce a case then hit `→` to type the case expression. If you want to edit what you're looking at you have to hit `RET` and type in a popup box and then hit RET again. It doesn't feel "fluid" yet. That might improve with time. 
Sounds really neat!
&gt; I'm a beginner at haskell &gt; &gt; {-# LANGUAGE NoImplicitPrelude #-} &gt; {-# LANGUAGE OverloadedStrings #-} &gt; {-# LANGUAGE QuasiQuotes #-} &gt; {-# LANGUAGE TemplateHaskell #-} &gt; {-# LANGUAGE MultiParamTypeClasses #-} &gt; {-# LANGUAGE TypeFamilies #-} Woah, those are quite advanced extensions for a self-professed beginner. [Are you sure you want to begin with a web app](http://bitemyapp.com/posts/2015-08-23-why-we-dont-chuck-readers-into-web-apps.html)?
But then wouldn't that also make a stack of two empty plates lasagna? It'd also mean you could put empty plates on top of lasagna, then put more lasagna on top of that.
Of course an empty plate isn't a lasagna. It's a plate with a lasagna on top. Otherwise, you could stack a plate on top of a lasagna and call it a lasagna. I'm not going to eat *that*.
We address this situation on a case-by-case basis. So, depending on the candidate, we're flexible and will try to work something out that is fair and equitable for all parties.
Lasagna is a semigroup. Adjoining the empty plate embeds that semigroup in the monoid (lasagna and the empty plate). A better structure might be to make a semigroup with only a left (or WLOG right) identity of empty plate, then plates can't be stacked on top of lasagna. Then you can have a stack of plates still be an element of the semigroup with left identity (lasagna and the empty plate), but not of itself an element of the semigroup (lasagna).
Not really. In the example I gave `Gph` is a CCC (it's even a presheaf category). Also, Day convolution does not really work in a general CCC. Even in Set, Day convolution of arbitrary functors does not exist in general, but at least you can restrict yourself to accessible functors (which you can approximate in haskell as "strictly positive functors"). The secret missing ingredient here that makes things work in Haskell is the notion of [tensorial strength](https://en.wikipedia.org/wiki/Strong_monad), which is automatic for Haskell functors. Indeed, you can prove that any strong monad is an applicative (on a category where that makes sense). 
This is correct. In general, the endofunctors of monads need not preserve tensorial structure such that they're monoidal functors, but in the case of an endofunctor that preserves strength they do, and so in a setting where endofunctors must preserve strength, such as all endofunctors of a CCC, then you do get such a result. This is folklore that goes back to the 70s in the enriched categories school, but I'm not sure exactly where its written down. I've been meaning to find an occasion to give a longer presentation on this, but would love it if someone else did instead :-)
https://www.reddit.com/r/haskell/comments/78p3me/looking_for_a_haskell_dev_in_london/
Can we reason about the [Empty Lasagna]?(imgur.com/a/ySlyi)
Because of all the drawbacks to github as opposed to phab in workflow that many contributors have discussed all over this thread.
ok i'm confused a bit. i thought that every ccc had strength. in particular, because they are self-enriched. in my notes from the last time someone tried to explain the details to me (i could have sworn they were more detailed -- i should write and ask them to spell it out!) i have "tensoring v-cat and monoidal cat together give enrichment = strength" which is a bit cryptic for me to unpack, sadly.
Which is a million times more likely to happen in Haskell.
It's mostly Scala right now. We hope to bootstrap parts of it on the platform eventually.
OP said they were leaning away from yesod so didn't pick it.
Helps prevent the bottom noodles from drying / sticking to the pan.
Semigroup
If I have zero layers of lasagna and tell my wife we have lasagna, she'll be upset with me and tell me we don't have lasagna, so I think the empty pan is something you have to adjoin to a proper lasagna. Then in GHC 8.4 you'll finally be able to say that you simply have Nothing.
Thanks for the feedback! We'd love to have you in the beta. What would make it feel more fluid? Would a cursor model feel more fluid than the selection-based model? In your example with typing `x`, how would you want to convey the difference between a value called `x` versus a value called `xray` versus wanting to call a function called `xify`? Or typing the string "x"? If you have a whole match expression selected and type `x`, do you just replace it immediately? We are still grappling with these questions and would love to hear your thoughts!
I would agree - specially for beginners. Scotty has a lot (relative to other projects I've checked) of documentation / example projects too: https://github.com/scotty-web/scotty/wiki/Scotty-Tutorials-&amp;-Examples
Thank you for sharing stats as well. May I request you to add this testimonial to the Keter repo/README? Btw, the reverse-proxy issue is still there - https://github.com/snoyberg/keter#known-issues
Do you remember how much memory/CPU the Keter process used to consume?
In what way is it more type safe?
(servant developer here) I've talked with haskell beginners who were working through the servant tutorial, on the #servant IRC channel. Many of them, with a bit of help from people from #servant, have managed to build non trivial web apps, learning some haskell concepts along the way. Others have had a much harder time. It all depends on the person's motivation and whether the tutorial's approach suits that person and whether that person reaches out to us when in trouble, and surely on other factors.
I guess a free lasagna would be just the list of ingredients. Bring your own `fold`.
Thanks, I think I now have enough information to unpack /u/pcapriotti's statement into a form I can understand: `T` maps each graph `X` to another graph which contains the same vertices as `X`, but contains many more edges, one for every directed path through `X`. In particular, each edge of `X` is a directed path of length 1, so every edge of `X` also appears in `T X`. `T` is a functor, so it also needs to map the morphisms of `Gph`. We've said that such a morphism `(f, g) : X -&gt; Y` maps vertices to vertices and edges to edges. Well, `T X` and `T Y` have the same vertices as `X` and `Y`, so `f` doesn't need to change either. `g`, however, now has a lot more edges it needs to map. We know how to map a single edge, but how will we map an entire directed path? Oh, now that I spell it out, it's actually quite easy, simply map each component of the path to obtain a path in `T Y`. Okay. `T` is also a monad, which means it also defines natural transformations `return : x -&gt; T x` and `join : T (T x) -&gt; T x`. `return` is easy because `T x` contains all of `x`'s vertices and edges, so `f` and `g` are both identity functions. `join` is harder. `T (T x)` has the same vertices as `x` and `T x`, but it has a lot more edges: one for every directed path whose steps themselves consist of directed paths through edges of `x`! And we have to map all those edges to a much smaller graph, one which only has the directed paths through the edges of `x`. Oh wait, I can just concatenate the directed-path components of the path-of-path in order to obtain a single directed path! Easier than it looked. Next, cartesian products. `X * Y`'s vertices are probably pairs of vertices, and its edges are probably pairs of edges. Interestingly we don't have identity edges, so we can't first go through an edge in `X` and then an edge in `Y`, we have to advance in both graphs at the same time. That is why, if `Y` is a graph with a single vertex and no edges, then `X * Y` has no edges and thus neither does `T (X * Y)`. `T X` and `T Y` do have identity edges though, and thus so does `T X * T Y`. So there can't be a morphism from `T X * T Y` to `T (X * Y)`, as we would have to map those identity edges to some edge in `T (X * Y)`, but there are no edges at all in there. 
As far as I know "strength" is something that functors can have, not monoidal categories, so I don't know what you mean with your first paragraph. As for Day convolution, yes, it is a left Kan extension of something involving the monoidal structure, but that doesn't mean that it always exists, just like Kan extensions don't always exist. If you take the formula for a pointwise Kan extension as a coend, the domain of the coend is large (Set itself), so even though Set is cocomplete, you can't conclude that it exists.
Depends on the domain. Haskell is far superior for compilers, C# is far superior for GUIs. At my day job, Haskell's library situation is much more suitable. 
https://github.com/sboosali/speech-recognition provides a thin wrapper around an unofficial google API IIRC. I don't know of anything actually doing the speech recognition directly in Haskell, though. Usually you build some kind of [VUI](https://en.wikipedia.org/wiki/Voice_user_interface) using a set of carefully planned prompts unless you are a Google or Apple with a ton of data to allow unstructured speech to parse at a high enough acceptance rate to be acceptable to user. In this era of "big data" folks have gotten more ambitious, though.
Don't rebase. Always do a squash merge to create one commit per pull request. If you want to create a more granular commit history then create one pull request per smaller commit Let me make an analogy to the GHC release process. If you don't cut GHC releases often and on a schedule then people will keep trying to squeak in yet another change before the cutoff, which delays the release further and creates a vicious cycle. Pull requests have the same vicious cycle. If pull requests are too large and take too long to review then you incentivize people to keep adding new commits to an existing pull request instead of opening a new pull request. Keep pull requests small and focused so that you only need one final commit per pull request instead of playing commit Tetris within a large pull request.
No, the monoid operation is not just stacking. The monoid operation is removing lasagna from one plate and stacking it on top of another plate that may or may not have lasagna on it.
Is there a reason that chunks of the internet persist in managing communities via mailing list? This is not a gripe - I ask out of honest curiosity, as I have not attempted any form of community management in the past, and so hold no informed opinion as to the relative value of any given approach. 
Sure, because C# is less expressive in ways that matter for writing compilers. Doesn't mean there aren't libraries for writing compilers in C# (it's a bootstrapped language after all), and my point still stands for working with those.
How different do you mean Nothing in 8.4?
The sheets are flat, not folded. It's all going wrong!
I'm actually curious here -- is it the case that instead we have something oplax monoidal here?
Sorry, by "every ccc had strength" I mean "every ccc has the property that Set does, that endofunctors on it are strong". Thinking a bit more carefully, I see this is probably an overstatement. Every ccc has the property that _enriched_ endofunctors on it are strong. Since in "normal" category theory, endofunctors are all set enriched already, this is what makes Set "double-special". So the more general statement should be that we have this property for V-enriched endofunctors for any suitable category V. But not all functors on V-category are necessarily themselves V-enriched, and that's where the mismatch comes in... On Day convolution I think see your point -- the coend formula works if you're in a sufficiently (co)complete setting, not universally. So I think Phil's construction should largely go through, but with some caveats that remain to be spelled out -- the main one being that V is such that the category of V-valued presheaves has enough coends. His condition "you have a CCC, and Day is built from the categorical product for that CCC" should correspond to something like the enrichment of the functors under consideration...
Well then we ought to count [black holes](https://www.reddit.com/r/Showerthoughts/comments/78isdy/no_matter_how_many_lasagnas_you_stack_on_top_of/dou7g58/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=Showerthoughts) as lasagna as well.
This is not "managing a community via a mailing list" -- rather its just a longstanding convention that the "canonical" url for announcements is its post to one or another of a few core lists. The advantage to this is that the posts are archived in a uniform way not under third party control, and at the same point anyone can post. So for announcements intended to be disseminated through a number of channels, starting with a post to a mailing list as the "upstream" isn't too bad an idea...
ESR doesn't own common sense advice about collaborating with other folks on FOSS just because some cranky neckbeards made a webpage about it 13 years ago. There is nothing structurally similar about Michael's page, and the article linked. Even if Michael were aware of it, he'd owe these people nothing. I don't have to cite Herman Mellville if I decide to write a novel about whaling just because I'm aware of Moby Dick. That's not how authorship works.
&gt; the issues start to arise when you want to cross-compile This is something I hope we can eventually fix. Most TH is used for simple AST transformation that could run entirely in the interpreter. I found it totally surprising that to generate some lenses, ghcjs has to actually spawn nodejs! It seems unnecessary. "Full" TH has features like `runIO` which of course assumes running any code on the target platform. But maybe we can at some point distinguish between then "interpretable pure AST-transforming good TH" and the "evil platform-dependent impure TH", and you'd need the former for most of the time.
It's even abelian!
Thanks for the reply, makes sense!
But docs are better?
I think your best option will be to use an existing speech recognition engine like Kaldi or Julius. The reason for this is * You have a working/tested code-base with good performance (It takes A LOT of effort to get this right) * Availability of language models - Kaldi has an English model (which I think you can even use commercially) I am in currently trying to use the Julius speech recognition engine with Haskell, because it has a decent working language model of Japanese language. It is also based on C language, has minimal dependencies and light weight. Though the code base is horrible, it seg faults at random places, has functions &gt;1k lines, has tons of global variables... Kaldi on the other hand is a very big piece of software, as it is not just the speech decoder, but also capable of training models. Its code base is well documented, and actually quite well written also. You can learn a lot about the internals of speech recognition by going through its documentation and code base. If you just want to do use the speech decoder and focus on the actual conversation part, I would recommend just put some ugly hacks in the Kaldi code and make it work. (You can actually use my code to get started, I managed to get Kaldi to work with Haskell, but found it a bit hard to provide a good interface) On the other hand if you do have a lot of time, then you can work on providing a nice Haskell interface to it. 
We also use Servant. I am largely happy with it: I love what it promises, and it delivers well. If your usecase works with Servant it is pretty much perfect. But if you need to extend Servant or interact with its internals—we did, don't remember why—you pretty much have to be a Haskell expert.
Route parameters that are passed into controllers are typesafe. Check this out [this blog post](https://www.spock.li/2015/04/19/type-safe_routing.html) about it. There's also some more fancy stuff in [this post](https://www.spock.li/2015/08/23/taking_authentication_to_the_next_level.html).
I cannot stress enough how amazing servant has been for me. I had very little knowledge of Haskell (I can handle pure functions, Maybe monad and some exposure to Persistent after playing with Yesod a bit). I was able to pick up servant and build a complex backend application quickly and have managed to refactor it a lot without any difficulty. I used servant tutorials along with some guides to wire up my DB using Persistent to get started and I got productive very quickly. The two things I love about servant are: 1) being able to declare your API up front and 2) being able to generate swagger docs. I looked at other libraries and they all were very confusing to me and servant just clicked. A lot of people will say servant uses some crazy type-level-hacks and that may be true but it does not come in the way of building my apps. I still don't understand all the internals of servant but that's ok as I care mostly about getting my job done and servant helped me do that without getting in the way. 
In 8.4 Semigroup will be a superclass of Monoid so the Monoid instance for Maybe will only require Semigroup on the contained element. 
[removed]
Very cool, thanks for the response!
That's the state of the lasagna after being eaten.
Some bits of TH can be used in Safe Haskell, such as what's enabled via `TemplateHaskellQuotes` (since 8.0.1).
Abstract enough and you can make anything a monoid! "Combining two things will give you some other thing."
What do you mean by "actual implementation"? You can not implement servant route handlers using Spock, and you can not implement Spock route handlers with servant. What's possible though is to capture REST-API requests in a servant middleware and have all other requests be handled by Spock.
The error is "parse error (possibly incorrect indentation or mismatched brackets)". You don't have mismatched brackets, so it's incorrect indentation - specifically, layout requires that the body of the function `getPlanetSeconds` be indented. It does not work for me even with the semicolon, GHC 8.0.2.
The markdown messed up the indentation, try this: getPlanetSeconds :: Planet -&gt; Float getPlanetSeconds planet = let t = 31557600 yrs = (* t) in case planet of Mercury -&gt; yrs 0.2408467 Venus -&gt; yrs 0.61519726 Mars -&gt; yrs 1.8808158 Jupiter -&gt; yrs 11.862615 Saturn -&gt; yrs 29.447498 Uranus -&gt; yrs 84.016846 Neptune -&gt; yrs 164.79132 otherwise -&gt; t ageOn :: Planet -&gt; Float -&gt; Float ageOn = (flip (/)) . getPlanetSeconds
Works just fine for me. I think you probably have some issue in your actual file like mixing spaces and tabs or something that isn't actually present in your reddit post now.
Yep. I had tabs. I didn't know haskell didn't like tabs actually. I converted to spaces and works fine.
I've done this as well. One project I got to about 90% done by parsing my pseudo Haskell and generating C#
Were you pasting it into GHCI? If so, I'd recommend making a file instead and then loading that with GHCi. Whenever you change it, you can just reload the file by doing `:r` in GHCI.
No, that was a typo. I was using stack and building.
Ah, no problem. I don't think using tabs is necessarily a problem, but mixing spaces and tabs or using tabs in a way that's inconsistent with how Haskell interprets them could cause a problem. Spaces are probably better for consistent formatting anyway. Not like file size matters anymore!
They are all good and they all work. Use the API you like best.
The solution: See https://github.com/deech/fltkhs/blob/7f42a4f4a2f9180a3c714c470e680d27b3d02185/fltkhs.cabal#L183 Refer: https://github.com/haskell/cabal/issues/4677
[Coherence domains proposal](https://gist.github.com/djspiewak/9f6feadab02b16829c41484b394d16e4) for scala fits the bill.
You would carry the ordering as part of the map's type argument
In my experience, mailing lists: * often don't require users to create a distinct account; * are 'push' rather than 'pull', meaning that one doesn't have to continually check for discussion updates (and i've not had positive experience with systems that are supposed to notify me of updates via email); * don't require one to specifically log in to a Web UI in order to contribute to discussions; * allow for more clearly threaded discussions than at least some of the alternatives; * allow people to take advantage of existing email processing/filtering capabilities to sort and limit the messages one has to deal with. i'm now following Rust development far less now that they've moved away from using a mailing list. But i'm guessing that far more people now find following development to be easier, so \*shrug\*.
I *think* GHC just treats one tab as one space. Works just fine; just means you have to be consistent with which ones you use.
GHC treats tabs as tabs, and places tab stops 8 columns apart. So tabs will work, but getting them right means either using a style where you don’t need to align things (good practice anyway) or setting up your editor to use 8-column tab stops.
Great. Thanks for posting more info. I guess I am trying to understand what the company is trying to do, specially.
Very cool
I don't know. 
I think the Haskell community abuses the term "DSL", but that does not change the fact that this is not a DSL. 
Email is simple and decentralized. There's no need to create an account on some private service and, let's face it, *everyone* involved in tech is going to have an email address. You can't say the same thing for Facebook, Reddit or Twitter accounts!
Any yet, the absence of lasagna must also be considered pineapples. How to resolve this apparent contradiction? How can nothing be everything? Perhaps it is not the presence or absence of the thing nothingness represents, but the quantity of presence? Of course, that is itself not lasagna. Yet, it is. Another contradiction -- I can barely contain the explosion. Run.
Cool, but I was wondering where that `rewrite` function came from. At least, I had to do Ctrl+F to make sure I didn't overlooked something at the point of which I was pretty sure it had to be a primitive from `Plated`. Maybe use explicit imports or drop a link immediately before usage?
Hi all! I was wondering what would be a good strategy provide feedback to the user in a terminal app. Here’s my problem: I’m downloading from a bunch of urls in an array an then I traverse the result in order to get an IO [a]. At the same time I would like to give some feedback to the user. Something like percent of completion, the current url it’s downloading. You got the idea. All I managed so far was to sprinkle all over the place console logs, which I feel isn’t right.
Please suggest the alternative that we should be using instead.
Twitter polls
What if they are different colors?
Explicit imports is a good idea. I'll fix that up.
Running your code in ghci in production sounds very odd. That goes through a byte code instead of machine code plus some other subtle things related to threading. I'm not familiar with your `runJob`/`deleteJob` functions so I can't really comment there. Something you could try, which is also mostly language agnostic, is using a signal handler to process these special requests. Signals work okay if there are just a couple and context isn't really important. If those restrictions don't apply you could have a watchdog thread on a socket that listens for commands and executes them. There are some Haskell specific advanced things you could do with code reloading (like the hint package) but I don't think I would like that approach in your case, so I'm slow to recommend it.
We have a `PlaygroundJob` executable with sub-commands and argument-parsnig prepared via [optparse-generic](https://www.stackage.org/package/optparse-generic) and a `Makefile`-command which distributes it on servers where you can run it. So, if you want to quickly launch something on a server, you: - add a new sub-command with a name you want (by adding a new data-constructor in your `Parameters` structure with its command-line params, parsing is derived automatically) - run 'make deliver_playground' - ssh to one of servers where it was delivered and execute `./PlaygroundJob &lt;jobname&gt; &lt;jobargs&gt;` Obviously, not as rapid as with GHCi, but you still have all the "template" prepared for you and just need to make a machinery work every time you need to, without a need to be creative. 
It sounds like you might benefit from having `runJob` and `deleteJob` be wrapped as standalone executables that you can ship to your production server too. Then you could just `ssh` to your machine and run `run-job` or `delete-job`, and know that it's doing the same thing as your production code. A similar approach would be to expose these calls over a HTTP API, and then maybe whitelist it to only allow requests from localhost or expect a specific header. This might make more sense if your app is already serving a HTTP interface.
For languages that have significant whitespace like Python and Haskell, it is advisable to avoid tabs altogether. Try using `-fwarn-tabs`.
It likely depends on where/what your state is. What do `runJob`/`deleteJob` do? Do something with a database? Then running ghci on your developer or a bastion machine, and pointing it at your production database (directly or via SSH port forwarding) might be your method of choice (and usually is mine).
That is correct, but to my knowledge its implementation doesn't exploit the possibilities yet, for example even `TemplateHaskellQuotes` still invokes nodejs on ghcjs. Also, `TemplateHaskellQuotes` cannot do things like derive lenses because it doesn't permit top-level TH invocations like `deriveLenses`. I would be happy to be proven wrong on both though.
It sounds like a big reason they want to use rust is console ports. Resources are scarce but they also have a lot of cores they need to utilize. On top of that they need to port the language to the console, and the changes required for that are behind NDA. Porting GHC would require porting the runtime, but due to the NDA they wouldn't be able to share that work or get much help with the effort. They have a similar problem with rust, but due to differences in the languages it's a smaller task? Once you get Haskell running in that environment you need to think about the runtime characteristics of the language. You're making a (soft) realtime system. So you want predictable execution. Consistent time between frames. GC will work against you here. It's not insurmountable, but it may require writing unidiomatic Haskell and possibly a different GC design. Haskell frees you from details of representation, but this also means giving up a lot of control. In Haskell switching algorithms is easy but fine tuning the implementation to get every last bit of performance becomes different, often harder. If you really wanted to use Haskell in this domain using it to make your tools and automation makes the most sense to me. Or making a game specific embedded dsl and generating the code that actually runs on the hardware from that. Using Haskell doesn't have to mean the final binary you execute is written in Haskell. 
&gt; Running your code in ghci in production sounds very odd. That goes through a byte code instead of machine code plus some other subtle things related to threading. I think you misunderstood. I'm not suggesting running the entire code in GHCi, but just the code required for urgent maintenance. &gt; Running your code in ghci in production sounds very odd. That goes through a byte code instead of machine code plus some other subtle things related to threading. A special set of commands (DSL, essentially), or arbitrary Haskell code? If the latter, then you're essentially running GHCi in production, right? 
Partially agree. The problem is that this approach assumes that we've built special commands/binaries for all such maintenance tasks. I'm more worried about random stuff that we hadn't thought of, but needs to be done in production now.
Actually you know a lot more than me. I know almost nothing about internals of ASR, and therefore dare not take this path. I approached ASR from the point of view of a user and just wanted to be able to decode the audio. I felt that doing even that is non trivial, so I feel like creating a Haskell interface library is a good endeavor. But I would definitely encourage you to explore if it is feasible to implement the whole decoder in Haskell.
I don't think it'd ever be popular - you can certainly do game dev in Haskell (see: Elise Huard's talks on youtube), but high performance would be especially difficult - e.g. even if you ignore Kyren's points, reasoning about time/space complexity with lazy evaluation isn't as easy and those are both massive concerns. Rust is fairly approachable coming from a C++ background so I don't think it's a stretch to say that over time, the safety benefits of Rust might lead to new games/game engines being written in Rust instead of C++ as the language matures.
Does GHC support AIX or not? If it's supported, why wouldn't you able to use TH there?
Right I wouldn't recommend Yesod for an app you want to run on a Raspberry Pi.
FTI an oplax monoidal functor is one with an operation Δc,c' :F(c⊗c')→F(c)⊗F(c')
Not to sound snarky, but maybe you should slow your releases down. It's never a good idea to release without considering what you're going to do when things go wrong. It doesn't answer your question but making it easy to "just fix it on a prod" is a serious process smell.
The four point /u/kryrenn mentions 1) GC pauses are proportional to the working set of memory 2) One GC for all threads, so you can't work around problem 1) 3) Porting a runtime is hard 4) Haskell is not as good as other languages when you have to effectively write a bunch of C. are all things Rust solves by design. We could try to iterate Haskell to be closer to that but 1. We'd never get as close as Rust 2. We'd unavoidably sacrifice other things to get there.
This would probably be overkill, but if you need to use the resources that exist in the already-running process, [GHC does support some level of hotswapping and running new code promptly](http://simonmar.github.io/posts/2017-10-17-hotswapping-haskell.html). You could make something that links in arbitrary Haskell object files and calls them with the application context. Then it's just a matter of compiling a small object and uploading it. Though I wonder how easy it is to shoot yourself in the foot by uploading an object that was built against a different version of the code...
&gt; Is there a reason that chunks of the internet persist in managing communities via mailing list? Because it's free and open and everyone on the internet already has an account. 
I don't think OP's problem is necessarily a symptom of a problematic process. Some lines of work just actually need very high pace deployments. I'd guess a lot of fintech teams have to make changes to prod several times a day, for example.
&gt;Or is that kind of thing a domain where Haskell (currently) isn't well suited for and where other languages and platforms like say C++ or Rust should almost always be used? You'd have to ask someone with more knowledge of game programming to know if it's impossible in general, but the libraries aren't there and the paradigms for functional graphics haven't been explored yet as I understand it. &gt;I'm referring mainly to super high performance game engines here, but the question is also somewhat applicable to other systems that need to have really high performance, on par with C++. The only way to have performance on par with C++ is to either write Haskell that basically looks like C, or work in a domain where Haskell's strengths make optimizing the equivalent low-level code infeasible. 
While Haskell may not ever compete in this space, I don't think it's being imagination that a similar language *could*: What if we flatten all structures and enums and use pointer indirection only at recursion points? We could then use unboxed arrays all over. Next, what if we have totality and well-founded induction in our language? Now our heap graph should be well-founded (i.e., there are no dependency cycles), meaning we can simply use reference counting for our heap and don't need a tracing GC. It still wouldn't be as good as low-level bit twiddling, but it should eliminate a fair amount of the overhead from using a Haskell-like language without completely discarding our functional paradigm for an imperative one.
As far as I’m concerned it’s best to keep on doing lazy functional programming and pushing the limits there rather than trying to keep up with the Joneses. If there are bits of rust we can adopt easily, sure, but in the meantime it will just hurt the design and cohesiveness of Haskell. 
GHC supports AIX because I (re)ported it to AIX around GHC 7.10/8.0; and it doesn't support TH because AIX because of the limits imposed by the [XCOFF](https://www.ibm.com/support/knowledgecenter/en/ssw_aix_71/com.ibm.aix.files/XCOFF.htm) format which were already a big challenge to workaround to get the current TH-less AIX port working. Since having to work around XCOFF limitations makes it extremely difficult and laborious to implement support for TH, it's safe to say that AIX won't support TH any time soon, unless there happens to be someone who needs this desperately enough to invest time or funding into making this happening.
This technique won’t apply to the vast majority of free monads, because the vast majority of base functors that you might want to build a free monad on top of are not `Traversable`. To put it another way: the example computation at the end of the article doesn’t actually bind any variables (it’s just a list of instructions) so it’s not actually using the “monad” part of “free monad”.
Nice! What happens if we're actually using the _monad_ part of "free monad", i.e. depending on the result of a previous computation? I assume we can't define optimizations for these "monadic" constructors and so the optimizer will just halt at this point?
&gt; Porting a runtime is hard I wonder how much is it true now that most console have an x86_64 architecture. I understand that it was true when the playstation / xbox where running on a mix of power PC and cell and exotic stuffs. Perhaps linear types, coupled with compact region for static asset and a bit more of unboxing for sum types may solve most of the GC issues.
Porting the runtime is hard, but not prohibitively so if someone makes the time for it. I’d say the GC is the bigger problem, but also a solvable one. Haskell could use more GC options anyway; it’d be nice if we were like the JVM, offering multiple GC’s that are optimized for different workloads.
You really only need one extra command - resynchronize. Exactly how that works depends on your application, which we know nothing about. For example, if your data types are backed by a persistent store, you just resynchronize against that. Or you can provide a way to serialze and deserialize. Another approach: If uptime in production is so critical for you, you must certainly be using some failover mechanism, such as active/active or whatever. After all, software isn't the only thing that can fail. You can leverage your failover mechanism to swap in a new copy of your application where your emergency corrections have been applied. The bottom line is that GHC is a native code compiler, not an interpreter. So the techniques for the care and feeding of running production applications are very different than for interpreted languages like CL and Ruby.
That's interesting. On the surface, I would have thought that registerising only involves NCG, at the STG level or even Cmm and beyond, far at the other end of the pipeline from the AST where TH operates.
Indeed, it's optimizing a free mon*oid*, also known as a list+ + well, sort of
It'll work if your app can have multiple instances running at the same time. (No locks on files, etc.)
I think that's a very cool idea but it would be hard to iterate any existing Haskell compiler to that.
This is not a helpful comment. Also, QQ and TH aren't a big deal to use as an end user even if you aren't going to get what's happening under the hood. Chris's blog post is directed at people that are explicitly trying to learn Haskell. He's right that a beginner explicitly trying to learn Haskell will get more mileage in other ways. However, people have the right to play around, get stuck, and then learn things step by step without the condescension.
I agree. And likewise I think Rust will adopt bits of Haskell if they can do so easily. But in general I think each language should play to its strength.
There's a lot of work in this space already. The usual problem with static memory management disciplines is that they don't handle dynamic memory allocation needs well. If you enforce linearity you essentially have reference counting memory management where the reference count can only be 0 or 1. There are practical problem with that system even in the absence of static typing.
The example optimization also is something that would be typically possible with a Free Applicative, but not Free Monads in general.
Re: #4: inline-c works pretty well...
Don't do it. Doing "urgent maintainance" in production is going to be increasingly untenable for multiple reasons: - You must document why every possible employee that can do "urgent maintainance in production" have the ability to do that. - You might not be GDPR-compliant, or your compliance work increases significantly. - You must create procedures for monitoring and alerting for every such incident. - You must have at least 2 employeed doing this work, and setup multi-key access systems if you want to have any sort of assurance that the employee didn't disable the monitoring system. - etc.. What you _should_ do is expose a limited DSL with likely commands that needs running. This could be "restart server", "kill table &lt;X&gt;", etc. These commands will be logged as any other request and as long as the DSL is invoked remotely through magic HTTP requests or similar, the ability to extract sensitive data is eliminated. Never let an employee connect to a running production instance. Ever.
I don't think /u/kyrenn *actually* wants to write C, rather "C-style programming" in whatever safer language.
Wow, you're right. No wonder it seemed so easy.
&gt; it really is something that is roughly equivalent to a free applicative I think. I don't think so, as continuations can still depend on which branch is selected, just out of a finite number of choices rather than an infinite one.
I mean... it's an infrastructure management team. I find it ironic that a first party solution isn't on the table! But yes, I definitely agree with the dislike of corporate third-party platforms for this sort of thing.
I couldn't agree more! The number of times I heard "Oh fuck" and mutterings of so and so not commiting "fixes" shortly followed by chaos as people run around trying to work out what went wrong. If "urgent maintainance" is regular maintenance then, do as others have suggested, build tools (whether that is scripts or haskell code) to perform this maintenance and run them automatically. Use cron of something similar. The system should run *under normal conditions* without any intervention whatsoever. If "urgent maintainance" is bug fixing or something has changed then you should go through your normal release cycle. The key to making this workable is to ensure your deployment processes are simple and easy. Preferably, completely automated. You will also need to make sure your monitoring is up to snuff. As a general text around this subject the [Pheonix Project](https://booko.com.au/works/7764039) is excellent.
&gt; Now our heap graph should be well-founded I'm not sure this is true, at least unless you ban coinduction: There is no way to model `[1,1,..]` other than through a heap cycle.
Yes, that's what I was trying to say.
Unfortunately, since Haskell is compiled, not interpreted, you cannot hook up a console to an existing Haskell process and play around. Even if you could, all the top-level declarations are immutable, so you wouldn't even have access to your program's state. So you do need to build special commands into your binary in anticipation of such maintenance tasks. But here's the trick: that special command can be a console! Using a library such as [hint](http://hackage.haskell.org/package/hint), you can interpret Haskell expressions at runtime, and you should be able to implement a custom console using that. [You do _not_ need GHC to be installed on the target machine in order to use hint](https://github.com/gelisam/deploy-hint#readme), but you do need a (bytecode-compiled) copy of the libraries you want these Haskell expressions to have access to. More importantly, this Haskell expression can be a function operating on the types provided by those libraries, and that's how you can give your console access to your application's state.
&gt; Resources are scarce but they also have a lot of cores they need to utilize. Multiple cores are probably the only place Haskell would fare well for this :)
Yup. There's a reason that Elm, PureScript, Idris, and Agda were written in Haskell, just as there are reasons not to do game development in Haskell. 
&gt; It still wouldn't be as good as low-level bit twiddling At a certain point the human benefits from bit twiddling are outweighed by the size and complexity of the code.
It's still a work-in-progress (especially the documentation) but it's at a point where I think Haskellers will be able to make some use of it. 
As a reply jointly to this and my sibling comment, I think you might be right. `T` can be read as "path space of". A path on `X x Y` indeed gives rise to a path on `X` and a path on `Y`, so your hypothesis of a natural transformation `T(X x Y) -&gt; TX x TY` seems plausible.
I tried writing a game in Rust, but I've actually switched back to Haskell, at least for now. Let's say that in Rust, you have two approaches. You can 1. Go the low-level route, in which you write your own game objects and methods for their interaction. You make your own render calls, you want things to be as fast as they can be. 2. Use a higher-level approach. You use [gfx](https://github.com/gfx-rs/gfx) for graphics and [specs](https://github.com/slide-rs/specs) for game logic. Maybe even upgrade to frameworks like [piston](https://github.com/PistonDevelopers/piston) or [amethyst](https://github.com/amethyst/amethyst), both of which I think use gfx and specs. If you like the first way of doing things, then Rust is not a bad choice. It's definitely not as mature as C++, performance can be kind of opaque, but Haskell is _definitely_ not an option. You will, however, be writing a lot more code than with the next approach, and since you are interacting with OpenGL, you sacrifice some of Rust's safety. I chose the second way, but I found it to still be more frustrating than I had hoped. While I really like specs' idea, I found it to be verbose and even managed to get runtime errors, which breaks Rust's central promise. Similarly, gfx is the kind of graphics API that completely obsoletes manually writing OpenGL, but its performance can be very unpredictable and it's still pretty verbose. So, I tried writing [a port of specs](https://github.com/jonascarpay/apecs), which turned out to address most of the issues I had with specs and still has similar performance. Haskell's alternative to gfx is [GPipe](https://github.com/tobbebex/GPipe-Core#readme), the kind of DSL that could not exist in other languages. Both of these libraries use types for what otherwise would become runtime infrastructure, and as such manage to compete with Rust in terms of performance. Now, there are no serious projects yet that use these libraries, but I think the same could be said about Rust. All I want to say is that I think Haskell has the potential to be really useful for high-performance game development.
Haskell is compiled (like C) , not interpreted (like Lisp and Ruby). How would you accomplish this task with C code?
I never have problems using tabs. You just have to use them *correctly*. ;)
It is possible to write Haskell that doesn't touch the GC, e.g. by allocating once and re-using buffers. But typically people just write Haskell without caring about what their libraries are allocating as long as it's correct. You write some high frequency trading process or whatever domain problem and realize you need N libraries to do checksums, parsing, fast Fourier transformations, or whatnot and discover that when you combine them the result is quite a lot of allocation. Hexpat is a high-performance XML parser and the allocations are very high, to just walk a file at XML boundaries and do nothing with it: Case Bytes GCs Check 4kb/hexpat-sax 444,176 0 OK 31kb/hexpat-sax 492,576 0 OK 211kb/hexpat-sax 21,112,392 40 OK Meanwhile if you [write Haskell and pay attention to allocation](http://chrisdone.com/posts/fast-haskell-c-parsing-xml) then you can have constant memory overhead: Case Bytes GCs Check 4kb parse 1,472 0 OK 42kb parse 1,160 0 OK 52kb parse 1,472 0 OK With big speed difference, competitive with C that was written with performance in mind: File hexml-dom xeno-sax hexpat-sax 4KB 6.123 μs 5.038 μs 97.25 μs 211KB 263.4 μs 244.1 μs 24.03 ms &lt;- yes, you read that correctly You have to be disciplined with a test suite/benchmark suite that makes sure you're not allocating linearly or exponentially or doing too much work, be scientific about it. Cultivate a natural distrust in libraries and be prepared to rewrite slow ones. In Rust and C, you have to be disciplined by paying in time by manually allocating and freeing and proving certain characteristics about ownership, it's not like it comes for free. I understand the fear that one day your frame rate will be crippled by GC pauses but it doesn't just come from nowhere if you are diligent from the start. And you can always write a correct version first, make a test suite, and then write something faster. But actually rewrite it fast in Haskell again, don't just switch to C++ or Rust. But I haven't written any games in Haskell (I'd like to, though--the sdl2 package seems to be in good shape) so take my view with a grain of salt.
&gt; a style where you don’t need to align things Or, use spaces for alignment, and tabs for indentation, as intended. (I write tab-y Haskell and view it with an editor with a 2-space (visible) tabstop.)
In order for TH to work, we need either the RTS Linker to work or support dynamic linking for Haskell libraries. Either options requires to deal with XCOFF, especially its limited amount of relocation-types in combination with its TOC.
Since you mention HMMs, are you perhaps familiar with artificial neural networks as well? AFAIK RNNs (and similar models) are the main driving force behind current advances in speech recognition.
Isn't that distinguishment exactly what makes something Turing Complete? Inductive structures terminate, co-inductive structures don't, the middle ground is undecidable.
I think it's more than that. You need something about well-founded recursion, otherwise you can just say x : a x = x for any type `a`.
Isn't this sort of disingenuous though? Realistically, these are only obstacles under certain scenarios. Is there not a way to accomplish the goal while avoiding the obstacles, instead of trying to address them directly? How much of what makes manual memory management necessary is a solvable architecture problem?
That would be said middle ground - it's a non-productive infinite recursion. `x = Cons 1 x` is fine, despite being not-well-founded, since we can reliably always consume from it in fixed time. `f a c m = if c a then Cons a else id $ f (m a) c m` is where you have real trouble. It might always produce eventually, or you might have `f False id id` in which case it never produces. If we have a TC language, then there are constructs that we cannot reliably split between Inductive, Co-Inductive, Other.
Here here! I think one big problem with writing games in Haskell is that people hear these stories of big GC pauses, space and time leaks and they never try. I've run into space and time leaks - both were easily solved with some profiling and a light refactor. I'm still waiting to hit the big GC pause. IMO the real problem is the ecosystem. Haskell will be the right choice for many games in the future, but right now we need more libraries and maintainers. 
Of course, Haskell fares well with multiple cores for the very reason it's generally a poor choice for real-time applications (ie, lack of programmer control over representation) It's an interesting trade-off to be sure. I'm hopeful that one day more declarative languages will always be a performance win and an ergonomics win.
Oh, I see what you mean. In any case, I'm perfectly content to give up TC - this is far from the only case where it causes problems!
maybe someday Ed's [thc](http://ekmett.github.io/thc/) runtime will close this gap
Agreed there.
I've worked on huge deployments with hundreds of servers handling millions of requests per hour. We were even using an interpreted language (Python). And even still we never ran arbitrary commands in production like that. When uptime is *that* critical, the right approach is to be very careful and build your entire infrastructure with fallbacks and maintenance modes. Still, with all that said, it seems like you're thinking about maintenance that is mostly related to the database. In addition to the other suggestions (using HTTP requests, standalone exes on the server), you could also just write some "helper" functions in you application code that are able to connect to any database. You can then fire up `GHCi` locally and run them against production!
Me too. Haskell's accelerate library is quite impressive for GPU. But when you're doing anything but really heavy parallelism Haskell starts to creak. 
Yes indeed, but I consider finite branching as equivalent to `b -&gt; a`. For example branching over two choices is `Bool -&gt; a`.
Are there any internship positions available? I am a college junior with some Haskell experience, and I would love to work with it more!
We use GHCi for this. We can either 1. `ssh` into a production server, fire up GHCi, and do whatever we need with the entire haskell process, or 2. write a custom executable, push that up, and run it. This is *bad practice* but previous architecture decisions at the company have forced our hand here. We have a customer service/administration application. It's a Yesod app with full access to our libraries, so it can run jobs or do tasks just like a worker server. If we need to do an operation more than once, we build the functionality into the admin website so we can do it manually.
Oh, well if it doesn't have *any* form of branching, i.e. only one or zero continuations, then it's just a list.
It seems to me that the goal of writing a realtime game is a pretty well defined and specific scenario. A garbage collector can easily get in the way of that unless it has some fairly specific properties.
[Hotswapping Haskell by Jon Coens at Facebook for Haxl](https://simonmar.github.io/posts/2017-10-17-hotswapping-haskell.html) 
You'd need to provide both more of a vision of how you want things to look and say more about what information you have. If you have the current download status available internally then this just sounds like you need to make a TUI - consider using the brick package.
Thank you, isn't transforming a title into slug a pure function genSlug :: Title -&gt; Slug I did not get why it is implemented with date time and monad?
This is probably on the extreme end of 'beginner questions', but what is the idiomatic way to do something like this in Haskell: (define (validate something) (cond ((fails-req-1? something) (Left "fails req-1")) ((fails-req-2? something) (Left "fails req-2")) (else (Right (use-something something))))) This is how I'd normally do it in a lispy way (validate everything in cond steps, and return an OK if it gets to the end), and I see there's a library to do this in the lispy way in Haskell: https://hackage.haskell.org/package/cond ... But I have a feeling this isn't standard practise. I'm having a little trouble thinking in Haskell coming from Lispy dialects! More explicitly, in my use case the type of the function in reality is `ByteString -&gt; Response`, where response could be an error response if the ByteString fails the validation steps, or an OK response (not an actual sum type).
I guess that's sort of my question, is why are we assuming that the only ways to implement a realtime game will, by necessity, come up against the GC as a hard stop? As others have pointed out, the GC locking execution is a problem when large amounts of resources are collected at once. Understandably, with Haskell and all objects being immutable, having large objects at all can sometimes require that large swaths of data are freed and therefor GC'd when a significant update takes place, but it seems like linear types could be used to solve a great deal of that problem by reinterpreting the replacement of a with b as a mutation of a to b, given that a may never exist again. So in a world in which such optimizations are possible, it's also possible for us to work around our those limitations regarding large objects. That would mean that, specifically, the only scenarios in which we strictly need GC pauses as a matter of course would be scenarios that force us to de-allocate large collections of resources. This is likely an ignorance challenge, but I don't understand why realtime games would need to frequently de-allocate large amounts of resources. I see why they need to allocate large chunks of resources and keep them around - But why would they need to discard large amounts of data at a singular moment in time?
Linear types might help with this, but no more than mutability and careful coding can already. It's important to note that GHC's garbage collected causes pauses *even if there's no garbage to collect* and the length of the pause is proportional to the number of pointers in your heap. Here's an interesting article on the issue: https://making.pusher.com/latency-working-set-ghc-gc-pick-two/
From Julie's post: &gt; I think the most common pickled-onion answer I see is not people being overly rude or condescending – it’s people just answering whatever question they wish was asked. Maybe they didn’t read it carefully; maybe it wasn’t a well formed question; maybe they just felt like talking about this tangentially related thing. &gt; [...] it used to be that you’d ask what you thought was a reasonable beginner question and someone would tell you to start by looking into Generics or maybe read some Saunders Mac Lane and then all would become clear to you. And you hate to be rude because this person took some of their valuable time to try to help you but you don’t know what just happened and you still don’t have an answer to your question. We still get this over at /r/haskellquestions occasionally, and I would urge people (including myself sometimes) to be mindful of the level a student is likely at. When someone asks how to update a nested record, we should show how to do so and then *maybe hint* at `lens`, not the other way around. Essays about all manner of theory, while certainly written with good intentions, risk alienating beginners by making the learning cliff seem even steeper than it already is.
&gt; 1. We'd never get as close as Rust &gt; 2. We'd unavoidably sacrifice other things to get there. I think this is important to remember. I'm.. really sorry that we dropped haskell without making a big deal about it, I knew when I mentioned it in the AMA that might sting a bit. I think there are a lot of caveats to what I said, I think basically everything I said in that AMA has a lot of caveats on it. Even if *I* have difficulty with something, or think the cost vs benefit math doesn't work out, it doesn't mean it might not work out differently for someone else. Maybe porting haskell's runtime is far easier than I thought? I didn't actually *do* it, so I might be wildly wrong about how difficult it is. I also still.. really really like Haskell as a language, and I think that there are a ton of things that it's well suited for. In fact, we might actually use it in the very near future for an upcoming server-side task. Also, I know that there's still active development with things like linear types and compact regions and better unboxing stories for Haskell, just like rust is still working out its story for HKTs. If I could choose what to take away from that AMA, it would be two things. First would be that programming language development is incredibly important, because our tools really really matter, but the search space is very large and there's room in the world for more than one language. Second, which is really related to the first, is that the closed nature of console development is an anathema to this idea, and I think we would all be better off if we could bring that to a close.
I started playing with that approach to gc again the other day trying to see if I could use it to do optimizations on an old-school combinator calculus. It can rewrite applications of both K and I in SKI during GC. Upgraded to a larger basis like the one Turner used for Miranda, the B, C, B', C' combinators can all be done by the GC as well. (B = (.), C = flip) This has the interesting property that if you then plug in the Church or Scott encoding for pairs, that it will do the Wadler `fst (a,b) = a` gc space leak prevention optimization as an emergent phenomenon.
In a game engine setting almost all of the work I'm interested in doing would be done on the GPU anyways. If you structure things right then the stuff visible to the GC is a negligible component of your total pool of assets, making everything fast to GC. If my job on the CPU is mostly to coordinate what the GPU does, then I'm fine. Does this limit the applicability of Haskell? Well, it means you're mostly using Haskell as a code generation tool, as a way to produce some crazy kinds of shaders you'd need. On the other hand, if you start trying to use the CPU for things like software based masked occlusion culling, or tons of AI work, then doing that on the heap in Haskell may not be the best idea.
I've been reading as many papers as I can get my hands on and it does seem like recurrent neural networks play a large role in the more "modern" literature. I'm just more of a pure math person so the HMM was more accessible than the deep learning models but I'm certainly open to looking into a variety of avenues. Since you seem to have put some thought into this matter, do you have a general idea of where one might start? 
&gt; A special set of commands (DSL, essentially) Yes. I think that's what I would lean towards.
Except for the fact that rust does exactly that. `std::iter::cycle(1)` It can get away with it because it's not a list, but an abstract iterator, and since it's monomorphised, there's no dynamic dispatch.
What are Rust people using now?
Scotty and Snap strike me as really clean - very little magic. Scotty came after Snap, but doesn't provide as much functionality. Snap has everything Scotty has (simple routing). Snap also provides snaplets, and there are already useful snaplets for PostgreSQL access, for instance. Snap also comes with Heist templating, but I understand this is becoming less popular nowadays. From what I understand, all the cool kids these days are moving to Reflex for front-end and using Snap (or some other library) for routing. 
&gt; it's not a list That about sums it up... That tackles the problem from another perspective, but I don't think you can do that in general. E.g. I doubt that you can translate every coinductive structure this way.
It looks like Discourse (https://internals.rust-lang.org/).
Honestly I think things like these are some of the things that cause beginners to shy away from Haskell. I understand that mailing lists have some distinct advantages, but UI/UX is completely atrocious compared to a decent thread UI like Reddit. Obviously I'm not suggesting Reddit replace mailing lists, but I will say that I can browse and interact with Reddit threads much more conveniently and efficiently than I can a mailing list. And the physical appearance of a mailing list archive page is just an eyesore. When a new user sees something that says "Log in to comment," they already know they can just type in an email address and then the UI speaks for itself. Using a mailing list is foreign to these people and not super self explanatory, so it turns them away. I don't really have any suggestions here. I'm just airing the woes, and I think they're pretty valid. Honestly, I'm pretty sure I've literally never contributed to a Haskell mailing list thread. At first it was because I'd never used mailing lists before, but nowadays it's just because I find it much more pleasant to comment on the associated Reddit threads. And frankly, to draw a fairer comparison, https://internals.rust-lang.org has been much more successful than Swift's mailing lists at drawing people in and keeping them.
I mean even with mutation if you are dealing with large objects that the GC is maintaining you could still see some significant GC stalls. What you really need is the big data structures to not be touched by the GC, such as compact regions, linear types or manual memory management. This seems doable though, since it seems to me as though most of the data in memory for games is for storing all the objects. E.g the entity/components in the entity component system. Providing a monadic interface to some inner entity/component system type that manages memory manually seems very doable. Similar to how you can interact with a huge SQL database in Haskell just fine.
ping /u/kwaleko 
ah. should have put comment there 🙂 the API spec says that slug must be globally unique. so, what happen if two people happen to write a post titled "My Cool Post"? They will be sluggified to "my-cool-post". and they will conflict - meaning one of them need to change the post title. not a good UX right? so what I did was to add userid &amp; time to the slug so that it's most likely be unique. in the case above, one post will be sluggified to .. lets say "1876-18758587-my-cool-post" and the other sluggified to lets say "67-176844600-my-cool-post".
I mean, I guess I sort of inherently doubt that it is necessary for video games to carry an unusually large amount of that kind of information about game state. IE, yes, textures and assets are big, but most games don't need to live-load that stuff - They can dump assets using the power of the 'loading screen' and therefor gracefully hide a performance nightmare behind everyone's favorite, completely meaningless progress bar. This seems 100% accomplishable with compact regions. So what remains is actual, application specific logic about entities and general, currently relevant state - Which, realistically, shouldn't be more than a few MB of information at any given time, right? Or am I misunderstanding something fundamental about game development here?
&gt; I can browse and interact with Reddit threads much more conveniently and efficiently than I can a mailing list That's interesting as for me it's quite the opposite; I can manage high volumes of emails more efficiently with less distractions than keeping track of Reddit discussions where the comments tend to get rewritten, or even vanish, the ordering of comments shifts around depending on up/downvoting, keeping track of already seen comments is not available in the free reddit plan, you can't easily expand all collapsed sub-threads at once, there seems to be no support for saving drafts of a comment somewhere so you can finish to write your comment at some later point; that's just to name some of the problems I run into. Maybe you just haven't found a good enough MUA yet for you to consider Reddit's UX superior to good old RFC2822? Or am I just too old school? ;-)
A PHP bulletin board / FOSS forum framework of some kind was my default assumption. But, I must stress again, that I didn't ask this question as a critique of the existing system. It's just something I've seen in use in various places online, with surprisingly recent timestamps, in surprisingly technical contexts. So I was assuming that there must be some reason that people prefer to do this, and I figured the best way to find out why that was would be to ask some people who like to do things that way.
&gt; Unfortunately, since Haskell is compiled, not interpreted, you cannot hook up a console to an arbitrary Haskell process and play around. LISP is compiled and supports this, doesn't it? IIRC Clojure has facilities for this as well.
[Discourse](https://en.wikipedia.org/wiki/Discourse_(software)).
I don't doubt that email can be more *powerful*, but that's not really power I've ever felt necessary, and it doesn't have much to do with how easily new users take to it. Doesn't /r/Haskell get much more traffic than any Haskell mailing list? The fact that people seem to use pretty much any other available communication medium more often should say at least something...
I don't doubt that email can be more *powerful*, but that's not really power I've ever felt necessary, and it doesn't have much to do with how easily new users take to it. Doesn't /r/Haskell get much more traffic than any Haskell mailing list? The fact that mailing lists seem to be one of the less widely used forms of communication in the Haskell community should say something.
Great! Pageing any DataHaskell people ... as I understand it this is pretty much the representation used for data frames in R and Pandas (Python).
Ha, funny coincidence: I was in the middle of drafting a [blog post](https://github.com/benjamin-hodgson/benjamin-hodgson.github.io/blob/develop/drafts/2017-10-18-functor-functors.md) about this kind of records, which I call _functor functors_, because they're functors from the functor category. I messed around with an `Applicative` that looks exactly like yours but I concluded that it was too clunky for practical use because of the `newtype` bookkeeping for the arrow type was too clunky. Either way, great minds think alike!
If a programming language's primary community isn't based on tumblr then no cred.
I wasn't able to figure out how to wrap an expression with parentheses. I.e. `blah * x + y` -&gt; `blah * (x + y)`
You can do this with the `Monad` instance of `Either`, like so: type Error = String validate :: Thing -&gt; Either Error Thing validate thing = do when (failsReq1 thing) $ Left " fails req 1" when (failsReq2 thing) $ Left "fails req 2" return thing Or with a custom `Response` type that's a monoid: data Response = FailsReq1 | FailsReq2 | OK instance Monoid Response where mempty = OK OK `mappend` r = r r `mappend` _ = r validate :: Thing -&gt; Response validate thing = failsReq1 &lt;&gt; failsReq2 Or by using lists: type Error = String type Response = [Error] isOk :: Response -&gt; Bool isOk [] = True isOk _ = False validate :: Thing -&gt; Response validate thing = failsReq1 ++ failsReq2 As you can see, there's plenty of ways to express that pattern.
Exactly the kind of direction I needed! Thanks!
Need to handle pluralisation of the word: `data Lasagna = Lasagna` `data Lasagne = Either (Lasagna, Lasagna) (Lasagna, Lasagne)`
Are you missing a toWidget? Or whamlet instead of hamlet?
https://twitter.com/justinpervorse/status/923323395281903616
Make it easier to deploy new releases to production That way you check your changes into version control instead of silently mutating production
&gt; The RAM consumption of our app in GHCi is ~2 GB+ and increases over time, presumably, due to memory leaks in GHCi. Have you tried [`set +r`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#ghci-cmd-:set +r)?
This is some really great stuff. You should definitely finish that blog post. I really dig the name "Template". So much so that I was tempted to snatch it for `Conkin.AlternateNames` - but then I remembered about TemplateHaskell. I'm going to directly contradict one thing you mention in your `FApplicative` section &gt; since there are more useful `Applicative`s in the Haskell ecosystem than there are `FApplicative`s This is not actually true! [`Dispose :: (x :: k) -&gt; (f :: * -&gt; *) -&gt; (a :: k -&gt; *)`](http://hackage.haskell.org/package/conkin-1.0.2/docs/Conkin.html#t:Dispose) can lift any `Applicative f` to a `FApplicative (Dispose x f)`. You could try to get the best of both worlds with your `FFunctor`s using `TypeFamilies` approach: class FFunctor f where type C f g h :: Constraint ffmap :: C f g h =&gt; (g ~&gt; h) -&gt; f g -&gt; f h The point about the newtype clunkiness of `&lt;*&gt;` is very true. Perhaps we'd be better off with something closer to [the lax monoidal functor definition of Applicative](https://bartoszmilewski.com/2017/02/06/applicative-functors/): type (~&gt;) (a :: k -&gt; *) (b :: k -&gt; *) = forall (x :: k) . a x -&gt; b x class FFunctor (f :: (k -&gt; *) -&gt; *) where ffmap :: (a ~&gt; b) -&gt; f a -&gt; f b (&gt;$&gt;) :: FFunctor f =&gt; (a ~&gt; b) -&gt; f a -&gt; f b (&gt;$&gt;) = ffmap infixr 4 &gt;$&gt; class FFunctor f =&gt; FMonoidal (f :: (k -&gt; *) -&gt; *) where unit :: f (Seq '[]) (&gt;*&gt;) :: f a -&gt; f (Seq as) -&gt; f (Seq (a ': as)) infixr 4 &gt;*&gt; fliftA2 :: FMonoidal f =&gt; (forall x. a x -&gt; b x -&gt; c x) -&gt; f a -&gt; f b -&gt; f c fliftA2 g fa fb = runSeq g &gt;$&gt; fa &gt;*&gt; fb &gt;*&gt; unit fliftA3 :: FMonoidal f =&gt; (forall x. a x -&gt; b x -&gt; c x -&gt; d x) -&gt; f a -&gt; f b -&gt; f c -&gt; f d fliftA3 g fa fb fc = runSeq g &gt;$&gt; fa &gt;*&gt; fb &gt;*&gt; fc &gt;*&gt; unit data Seq (as :: [k -&gt; *]) (x :: k) where End :: Seq '[] x (:&gt;) :: a x -&gt; Seq as x -&gt; Seq (a ': as) x infixr 5 :&gt; class RunSeq (as :: [k -&gt; *]) where type Fun as (r :: k -&gt; *) (x :: k) :: * runSeq :: Fun as r x -&gt; Seq as x -&gt; r x instance RunSeq '[] where type Fun '[] r x = r x runSeq r_ End = r_ instance RunSeq as =&gt; RunSeq (a ': as) where type Fun (a ': as) r x = a x -&gt; Fun as r x runSeq f (ax :&gt; asx) = runSeq (f ax) asx The trick is that `&gt;*&gt;` and `&gt;$&gt;` are `infixr` rather than `infixl`, so we can run the function all at once. Having a `runSeq` at the front and a `&gt;*&gt; unit` at the back isn't too much overhead, I think. 
Looking at old implementations of random forests is actually what got me started on this train of thought :)
This is the right answer. You should have a deployment procedure that includes a trivial rollback mechanism, regardless of deploying a new executable, changing a SQL table or a configuration file.
Gave it [another shot](https://gist.github.com/rampion/20291bde6c8568c11f9cc5923d9639eb), and ended up with h &lt;*| fa |*| fb |*| ... |*| fy |*&gt; fz Where `h` is a normal function and `fa :: f a`
I was interested in following through reflex as well. Is it possible to not use mix and still achieve a similar working setup?
YMMV. I used do “tab indent, space align” for all my code, but the fiddling with editors, realigning things, and forcing people to use an “ignore whitespace” mode to read my diffs isn’t worth the (IMO usually minor) improvement in readability from alignment. I wish we as a field could move toward more richly formatted code that isn’t strictly textual. But I can’t even bring myself to use Unicode syntax in my open-source projects for fear it’ll deter contributors. (Not that people contribute much anyway, haha)
This is what I want to know, as well. I saw some work on using `stack`, but it's been awhile.
This look amazing - many thanks!
Lisp is a bit of an exception since it has basically no difference between definition and execution. Code is data, data is code, etc. Compiling becomes... Weird in lisp.
See here: https://www.reddit.com/r/haskell/comments/77mcny/development_setup_for_reflex/don8p0p/ 
Nice! It might also be worth pointing out that while we don’t talk a lot about structures of arrays, unboxed vectors are quite popular and the `Unbox` instance for tuples stores them as a tuple of vectors. But that’s mostly an internal implementation detail and not something that’s explicitly embraced in the API exposed to the user like you do in `conkin`.
I have a number of small functions of the form (MonadState s m) =&gt; a -&gt; m b Where each works on a small subset of some global state. How can I compose these together e.g. data GlobalState = GlobalState { _localA :: Int, _localB :: String } $(makeLenses ''GlobalState) a :: (MonadState Int m) =&gt; Int -&gt; m Int b :: (MonadState String m) =&gt; Int -&gt; m String aComposeB :: (MonadState GlobalState m) Int -&gt; m String I currently have a solution that sort of works, it doesn't let me keep the MTL constraints, instead it forces me to make a concrete type e.g. generaliseState :: (Monad m) =&gt; Lens' o i -&gt; StateT i m a -&gt; StateT o m a generaliseState l f = StateT $ \s -&gt; do (a, s') &lt;- runStateT f (s ^. l) return (a, l .~ s' $ s) but this is not really the behaviour I want. Any advice? 
I've been impressed by GPipe. People told me that to learn OpenGL I should expect to stare at a blank screen and wonder why things aren't working, but GPipe has always worked once I fixed the compile errors. I haven't had to many problems.
&gt; IMO the real problem is the ecosystem. Haskell will be the right choice for many games in the future, but right now we need more libraries and maintainers. Really? I see Haskell's laziness as a big distinguishing factor - the way XMonad was written (in 1200 lines!) was by thoroughly understanding functional data structures. I don't know enough about games to know how some of the bigger ideas will trickle into game design, but I will note that the community is not likely to change overnight. And right now there just aren't many people writing games in Haskell. 
There is a [package](https://hackage.haskell.org/package/rank2classes) for this. It even is able to generate instances via TH. Unfortunately all the type classes are defined over `f :: (* -&gt; *) -&gt; *` rather than `f :: (a -&gt; *) -&gt; *`. So I wrote my own [version](http://lpaste.net/9089861844587249664) (no `Applicative`, though) of those classes as everyone else here. BTW, does anybody know why `Distributive` is defined in terms of `distribute` and `collect`? `distribute` and `cotraverse` give better symmetry as witnessed by the lpaste above.
Definitely, but I wouldn’t recommend it. The Nix in `reflex-platform` does a lot for you, like binary caching (which matters a lot, especially considering how long it takes to build GHCJS and how long GHCJS takes to build everything else), mobile cross compilers, a curated package set that would be painful to manually re-encode in `stack.yaml`, and a bunch of compiler flags that can improve performance a lot.
&gt; even if you ignore Kyren's points, reasoning about time/space complexity with lazy evaluation isn't as easy and those are both massive concerns. Actually, laziness is pretty much orthogonal. Amortized and worst-case scenarios need different methods to reason about them. The lack of familiarity with laziness isn't necessarily indicative of something deeper. 
See also [hybrid-vectors](https://hackage.haskell.org/package/hybrid-vectors).
I understand the concern, but there's also an important sense in which your post on a public form is for *everyone*, not just the person you're directly replying to. It is healthy for the group as a whole to discuss the broader issues on the topic (e.g., lens, row polymorphism, etc) rather than always specifically catering to OP's understanding.
There was a [discussion](https://www.reddit.com/r/haskell/comments/5z3ue9/retrofitting_linear_types_pdf/) on a previous version of the paper in march. 
Yup. This is the camera ready version of the earlier draft (now accepted for POPL'18!) + appendices.
&gt; When someone asks how to update a nested record, we should show how to do so and then maybe hint at lens, not the other way around. Seriously, when you show them the code needed to update a record field nested 4 levels deep it's almost a disservice not to also show them how painless the code becomes if you use `lens`. You can easily explain how to use `lens` for this use-case in 5 minutes without going into the gritty details of how typeclasses work.
Thank you for the references!
It's a nice system. But it's solving a different problem. OP is asking about fixing broken *data* at runtime, not software upgrades. Also, that system is way overkill for just about anyone except Facebook. For example, we do hot software upgrades using [keter](https://hackage.haskell.org/package/keter). We drop a file into the incoming folder; existing connections complete on the previous version, and all new connections get the new version. That works fine for some of the largest enterprise sites on the web, but it's not good enough for Facebook's exceptional requirements.
my question may not have been very well phrased. What I'm mostly interested in is if there is any way for me to modify the `s` in `MonadState s m`. I know it sounds silly, but is it possible?
I have a question about the Controlling program optimisations section. It mentions multiplicity annotations to make inlining reliable. But I think that cases like f ( case b of p1 -&gt; e1 p2 -&gt; e2 ... ) =&gt; case b of p1 -&gt; f e1 p2 -&gt; f e2 ... Is guaranteed to not duplicate work if f has a linear context but it still duplicates code. Code bloat also can reduce performance because of cache eviction, I think. So wouldn't programmers need to rely on heuristics even with multiplicity annotations?
Personally I don't know anything about game development, but have you seen /u/edwardkmett's answer here? https://www.reddit.com/r/haskell/comments/78tnq7/haskell_for_high_performance_game_engines/doxbvhi/
&gt; 1) I have a slow internet connection and I don't want to download gigabytes of data for the nix environment Is this concerning data usage or amount of time to download? Considering how excruciatingly long it takes to build GHCJS and the Haskell packages on top of it, I'd be surprised if even a slow internet connection were slower than building from source. &gt; 2) I primarily use Windows. Sorry, I do not know the state of GHCJS on Windows. Nix doesn't have a native Windows solution, but for what it's worth, I have heard that Nix is coming close to working in the Linux subsystem for Windows. But I haven't been tracking the progress, so I don't even know if GHC works under it yet.
wow, Thanks a Lott. 
a valid point. Thanks for the info
Thanks for mentioning this package, I was looking for one with exactly those classes. &gt; Unfortunately all the type classes are defined over f :: (* -&gt; *) -&gt; * rather than f :: (a -&gt; *) -&gt; *. rank2classes has `PolyKinds` enabled, and I just tried the following, which typechecks: import Rank2 data T (f :: (* -&gt; *) -&gt; *) = T (f Maybe) instance Rank2.Functor T where f &lt;$&gt; T g = T (f g) 
`zoom` from `lens` does exactly that.
I feel like there are a lot of misconceptions about Haskell in this post, and I'm still not convinced by Rich's arguments about types and the fluidity of data. I'm really interested in seeing people's thoughts on the arguments presented in this post.
The author of that article apparently hasn't seen Ed's talk [The Unreasonable Effectiveness of Lenses for Business Applications](https://www.youtube.com/watch?v=T88TDS7L5DY).
Now compare to [how you do this Clojure](https://youtu.be/rh5J4vacG98).
One challenge is : exceptions allow you to violate linearity ... :(
Ha, I was surprised see SO answer in here! :) Looking at /u/rampion's and your posts, and the discussion here, there are a few possible variations in this area, and I am wondering whether there's a way to gather all of them in a single coherent story. (For me, this started with the remark that [hedgehog and quickcheck-state-machine were both defining their own "traversable functor functors"](https://github.com/advancedtelematic/quickcheck-state-machine/issues/170).) - Here we have mainly functors of kinds `Type -&gt; Type` and `(k -&gt; Type) -&gt; Type`, but surely there are similar uses for more absurd looking kinds `(k1 -&gt; Type) -&gt; (k2 -&gt; k3 -&gt; Type) -&gt; Type`. Do we have to define one type class for every kind of functor? - Is it worth reformulating all these kinds of functors in terms of the very general functor class in `categories`? It looks like a lot of wrappers will be necessary; ideally this would be transparent to a user: can they get all the relevant ways in which their types are functors for free? - What would be a similar generalization of "traversable"? I don't even know what they correspond to categorically; the naive way of looking at first-order traversables as functors between Kleisli categories is incorrect, as `traverse f . traverse g =/= traverse (g &gt;=&gt; f)`.
GHC already handles this sort of case by introducing so-called join-points (see *e.g.* [Simon Peyton Jones talk at this year's Haskell Exchange](https://skillsmatter.com/skillscasts/9182-keynote-compiling-without-continuations) (variations of this talk have been recorded at other times too)). The question is rather: do I want to unfold f and replace it by its definition. This almost always increases code size, in exchange for making other optimisations available. What is the best course of action when f is a linear function is still a matter of research: maybe it is worth inlining all linear functions, maybe not. But at the very least, we can inform the cardinality analysis so that all linear argument are marked as being used once, and not conservatively marked as "unknown" because the analysis gave up.
It's similarly easy: let d = [ 'a' =: 1 &lt;&gt; 'b' =: 2, 'c' =: 3, 'd' =: 4 ] d &amp; biplate %~ (+1) d &amp; biplate . filtered even %~ (+1) 
any updates on if it will hit GHC 8.4 :D
I feel like every point of contention in this debate comes down to mix of short sightness about ways of addressing a problem in an expressive type system and a zealousness to part ways with static typing. The expression problem seems to come up in the documents adt, but no proposals with respect to extensibility are raised. The discussion around Maybe's denotation is obsfucating: if the issue is about the state of "information" distinguished by the type then why not use monadthrow or accumulate some type that keeps partial results etc, it has nothing to do with an ontology of types. Relatedly the concretion vs abstraction binary seems disingenuous: the critique of the latter will not be overcome by the former, in any model one chooses to admit some differences and not others. And I doubt "situated" paradigms (patterns?) would yield anything more powerful than well-formed types. Tldr, seems like some people just don't like the way other people like type systems. The author's denigration of "puzzling" that he sees as characteristically Haskell-y is the only clear part of his argument, it horrified him. (but not so much as to give more than an unsubstantiated pan of some unnamed libraries. )
Take a look at the last example translating complex JSON structure though.
Are there really no good Mailing list UIs around? I just checked the one used in the OP link, and it is honestly super unwieldy super for even trivial correspondences. I feel like if you haven't been constantly subscribed, and therefore don't have the mails in your inbox, sifting through it is tedious work. Perhaps this is something where we could keep both parties happy, if we get a much better UI for going through the archive, and mailing list peeps can keep their mailing lists :)
I upvoted this because I think it will generate some good discussion, but the article (and Rich's talk) aren't ... the greatest. If I understand correctly, the main thrust of Rich's talk is that Clojure (dynamic) is better for "situated programs" than Haskell (static) because such programs are necessarily inelegant, stateful, effectful, and changing. I disagree with that and don't think Rich made a compelling argument for it. &gt; Any JSON value could be represented using that ADT. It was totally well-typed. Except that, even though you knew the type, you still knew nothing about the structure of that JSON. Sure, if you parse a `ByteString` into a `Value`, that's all you'll get. But usually you would use `FromJSON`, which does tell you something about the structure. And even if you didn't, I hope you'd be using lenses instead of pattern matching. &gt; You add a third argument. Then you follow down all of the compiler errors, adding that third argument to your pattern matching statements. If that particular piece of code doesn't care about the new arguments/fields, it can ignore them. I agree that things should have names rather than be positioned, but either way Haskell doesn't force you to care about them. `case x of { C {} -&gt; ... }` works fine, as does `case x of { C care _dontCare -&gt; ... }`. &gt; Rich mentioned that `Maybe` wasn't a good solution to lack of knowledge. He said "You either have it or you don't." This one is the most baffling to me. However baffled the author is about this, I am at least twice as baffled. `Maybe` exactly expresses that "you either have it or you don't". The author goes on to compare the "domain" model to the "information" model, but I don't understand what they're trying to say. They end by saying: "In a system like that, you can't write correct types for every kind of entity." If that's the case, what do you even know? If you don't know what your data looks like, how are you supposed to do anything with it? &gt; People often talk about a `Person` class representing a person. But it doesn't. It represents information about a person. Yeah, that's how it works in Haskell to. &gt; But can Haskell merge two ADTs together as an associative operation, like we can with maps? Can Haskell select a subset of the keys? Can Haskell iterate through the key-value pairs? No, Haskell can't. PureScript (probably) can. But even though Haskell can't do this *with ADTs*, it can do it with `Map`s, which is no worse than Clojure. &gt; Rich Hickey mentioned puzzles as being addictive, implying that it's fun to do stuff in the type system because it's like a puzzle. There is some truth to this. In the Haskell community especially, I feel like there's a lot of navel gazing about types and puzzling around how to get everything to be a compile-time error. In spite of that, there are tons of "real world" libraries that actually get stuff done. As a concrete example: databases. Want a low-level binding to libpq? `postgresql-libpq` is right there. Want a mid-level wrapper that is about the same as whatever you'd get in a dynamic language? `postgresql-simple`. Want something to "puzzle" with? `hasql`, `opaleye`, `persistent`, ... &gt; The challenge is to piece together an actual solution to the problem of "situated programs", not just point to features that might address one issue. This, to me, is the worst part of the whole thing. It's basically saying that even if there is an answer to each of the individual questions/problems, statically typed programs *still* aren't good for doing Real Work™. &gt; When I moved [from a Haskell system] into a Clojure job, I felt such a sense of freedom. I could treat information as information and not have to prove anything to a type checker. If that's what you want, use `Map String Dynamic`. But you won't because it's miserable. 
Hm, you're right: ``` blah&gt; :kind! Rank2.Distributive Rank2.Distributive :: ((k -&gt; *) -&gt; *) -&gt; Constraint ``` But I do remember I had some unexpected problem while trying to defined an instance... Maybe I tried to use `rank2classes-0.1` which doesn't have `PolyKinds` enabled. Also the docs still say that the package is for `f :: (* -&gt; *) -&gt; *` stuff.
&gt; use an “ignore whitespace” mode to read my diffs isn’t worth the (IMO usually minor) improvement in readability from alignment. I agree that alignment looks really nice the first time you are writing the code, but simply doesn't play well when you start changing things. Not only can it cause extra work for the person making he change, but it can cause extra work during review and merge, too. Indentation is mandatory. Alignment I can take or leave, the more read-only the code is the more it makes sense, but that can be hard to judge. &gt; I wish we as a field could move toward more richly formatted code that isn’t strictly textual. I think the code should remain textual, but then have an optional, non-normative, possibly binary section at the end of the file that more sophisticated tooling can use to carry information that enriches the textual parts of the code. Any tool trying to use this section of code would first verify a text-hash of the textual part; if it doesn't match, the binary data is ignored, stripped, and regenerated as needed. I agree on Unicode. And, I'm definitely needed to write code on a system than didn't have anything better than SysV vi, so being able to change changes without the enriched part is mandatory in my experience. Of course, that doesn't solve the diff/merge problem completely. I know Git supports diff/merge plug-ins based on file attributes which can be applied based on extension, and maybe other tools do too. A bad diff/merge of the binary section probably wouldn't cause any problems -- a hash verification would fail, and we'd just throw (parts of) it away. But, it could still confuse people that are having to work without those tools for whatever reason (libstdc++ won't load and I'm trying to recover or somesuch). Of course, if the diff/merge tool is at all aware of the enriched data, it could at least not show it, and may even be able to update and preserve it live.
Yeah, the description could be generalized.
Mainly download time. It's been over a year since I've used the try-reflex package, but I remember needing to let the nix packages download overnight. Not a huge deal, just a little frustrating. I believe I tried try-reflex in a virtual machine (which worked fine) and in the Linux subsystem for Windows. In the latter case, I ran into problems of GHCJS using path names longer than Windows supports (which is about 8000 characters, I think).
We're not "about to merge". Next step will be to write a proposal so the community can comment on it. If you like the design you should say so then :)
Can you condense the 40 minute video down into something I can read in e.g. a couple minutes?
In the sense that you might never reference an object, or in the sense that you might reference it twice? If the former, then that is given anyway, Haskell is not a total language after all. If the second, then that would be bad.
You and /u/HaskellHell make a good point. Perhaps the best way to go about it is to point out the simple, direct solution first, then optionally go into more advanced stuff, clearly labeling it as "only if you're interested". (Most of the time, this is what happens anyway, just to be clear.)
It was for me. I develop with stack.
https://github.com/nathanmarz/specter
Excellent!
Some of your points are exactly the same ones I have made on two separate posts which are about Rich's talk. Naturally, I feel that people should be free to use any language they want for whatever reasons they deem to be sufficient. However, many of the arguments in this post seem to be founded on an incorrect understanding of Haskell. Moreover, there are also arguments which I just don't understand, because they make no sense. The most aggravating example is toting Clojure's untyped maps as being better for dealing with data in the real world, because data is fluid and so on. To address why this is wrong, I think we need to consider what it means to know something about our data, and what kind of power that knowledge affords us when working with the data. There are multiple "levels" of knowledge about our data, that can kind of be ranked in increasing order of manipulation power. Continuing the on-going examples with JSON: if we only know our data is *some* JSON, then the only meaningful thing we can do with the data is treat it as some unknown data which happens to be formatted as JSON. In Haskell, we would have a tree data-structure that we can traverse, manipulate and so on, and the same applies to Clojure. The only difference here is that JSON is nearly first-class in Clojure. Above knowing that it's only JSON is know something about the structure of the data on top of JSON. Like knowing that it's a list of objects, and each object represents a person. A person object can vary a lot in shape, and can have a lot of different keys. It seems that the argument for Clojure in this case is basically that you don't need to know anything about it, since you can just deal with it first-class. You can also merge maps together to create new maps, and so on. This argument doesn't really make any sense, since there are really two possibilities if you're writing an application which needs to deal with optional data: 1. When the data is there, you need to act on it and have some code that deals with it. In this case, instead of paying the cost by representing it as an optional type and then handing the problem over to the parser, you're paying the cost in lines of code in your application by checking whether some key exists in a map, and then acting on it. In Haskell, you would do the former, and then any code that would deal with the data would just be in the `Maybe` monad where you just write code as if you have it, and nothing happens when it's not there. Additionally, you are secured to some extent against malformed data, since the data won't parse if it's not in the right format. 2. In your application, you don't care whether the data is there or not. If you don't then there is nothing you want to do with it, and it doesn't have to be parsed. Types are concretions, sure. But so is any code that actually interacts with the data meaningfully. If you ever need to access a key ".
(Not parent, but...) In the former sense, I think. I'm not sure that "it's not a total language" completely absolves this desire, though. If a program loops forever or crashes, no other piece of code runs; this is not true if a program throws and recovers from an exception. Consider using linear types to manages release of scarce resources (e.g. file handles). Say it's long-running and interactive, and you want linear types to ensure that when the user presses Ctrl-C (to interrupt an intermediate computation, not quit the interpreter), no resource is leaked. Is there any known way to ensure this?
Thanks for your response. I learned a lot. I have spent more time studying Clojure then Haskell so the observations here are meant has half questions. &gt; No, Haskell can't. PureScript (probably) can. But even though Haskell can't do this with ADTs, it can do it with Maps, which is no worse than Clojure. I find that a programming language's strengths come just as much from what it doesn't make easy. Sometimes having an option to do something makes the problem harder as the community becomes split and you follow to follow a convention or else you won't be able to communicate. In Haskell, in practice, do people use Maps where they could have used ADT's to avoid the problem of merging information? Or in practice do you hit this type of wall and work around it another way because converting to maps would cause issues? &gt; If that's what you want, use Map String Dynamic. But you won't because it's miserable. Is this the same Map you say left Haskell "no worse than Clojure"? If so then it seems to indicate the Haskell community rarely uses these maps, which means we're back to comparing two different approaches (Clojure's maps vs Haskells Types) even if there functionality overlap, which in practice is what we need to focus on, not what _can_ be done, but what actually happens. I think the Clojure community relies heavily on the REPL to work past compile time issues because were juggling the Type and business logic at the same time in small concise bits. I would imagine the biggest tradeoff between this approach and having haskells Type system is that a newcomer to the system wouldn't have the types to help them understand it. Clojure spec can bridge some of this gap, but I'm not qualified to compare it to haskells Type system. 
Some of your points are exactly the same ones I have made on two separate posts which are about Rich's talk. Naturally, I feel that people should be free to use any language they want for whatever reasons they deem to be sufficient. However, many of the arguments in this post seem to be founded on an incorrect understanding of Haskell. Moreover, there are also arguments which I just don't understand, because they make no sense. The most aggravating example is toting Clojure's untyped maps as being better for dealing with data in the real world, because data is fluid and so on. To address why this is wrong, I think we need to consider what it means to know something about our data, and what kind of power that knowledge affords us when working with the data. There are multiple "levels" of knowledge about our data, that can kind of be ranked in increasing order of manipulation power. Continuing the on-going examples with JSON: if we only know our data is *some* JSON, then the only meaningful thing we can do with the data is treat it as some unknown data which happens to be formatted as JSON. In Haskell, we would have a tree data-structure that we can traverse, manipulate and so on, and the same applies to Clojure. The only difference here is that JSON is nearly first-class in Clojure. Above knowing that it's only JSON is know something about the structure of the data on top of JSON. Like knowing that it's a list of objects, and each object represents a person. A person object can vary a lot in shape, and can have a lot of different keys. It seems that the argument for Clojure in this case is basically that you don't need to know anything about it, since you can just deal with it first-class. You can also merge maps together to create new maps, and so on. This argument doesn't really make any sense, since there are really two possibilities if you're writing an application which needs to deal with optional data: 1. When the data is there, you need to act on it and have some code that deals with it. In this case, instead of paying the cost by representing it as an optional type and then handing the problem over to the parser, you're paying the cost in lines of code in your application by checking whether some key exists in a map, and then acting on it. In Haskell, you would do the former, and then any code that would deal with the data would just be in the `Maybe` monad where you just write code as if you have it, and nothing happens when it's not there. Additionally, you are secured to some extent against malformed data, since the data won't parse if it's not in the right format. 2. In your application, you don't care whether the data is there or not. If you don't then there is nothing you want to do with it, and it doesn't have to be parsed. Types are concretions, sure. But so is any code that actually interacts with the data meaningfully. If you ever need to access a key `foo` in a map, then do something with the contents, then your code is also a concretion, because it is now coupled to some structure you expect the data to have. 
Finalizers/destructors maybe?
We could use finalisers, but these tend to be expensive, so would only work for resources that aren't assigned in a hot loop. I tend to favour scoping the resources like `bracket` and `ResourceT` do. In the pure case, we naturally introduce a scope, so we can simply use this scope to deallocate the resource in case of exception. For files and such, a `ResourceT`-like monad would be my choice. At any rate, there is a cost to this, typically one extra indirection to store a bit telling the scope whether it has to deallocate the resource or if it has already been done. So exceptions are not free, unfortunately.
&gt; easier to refactor Thats really why i like types. Sure you may have the "perfect" design, but when PHB changes the requirements you need to be able to refactor. Thats why types are important.
&gt; I find that a programming language's strengths come from both what it makes easy and what it makes hard Spot on. That's part of what makes discussions like these so hard. If a "dynamic person" says "static languages can't do X", "static people" will jump and say "but they can do X". Case in point: writing Clojure's `assoc-in` in Haskell is (presumably) possible but difficult; changing a field from, say, `Text` to `Maybe SSN` is easy in Haskell but annoying in Clojure. I think people generally do not use maps in Haskell when they could use ADTs. Things like [justified-containers](https://hackage.haskell.org/package/justified-containers) and [overloaded-records](https://hackage.haskell.org/package/overloaded-records) muddy the waters a bit. But really a better type system like PureScript's is the right answer in my mind. For example, merging two records can be done relatively easily: https://github.com/doolse/purescript-records/blob/v1.0.0/src/Data/Record.purs
Btw our approach in Lamdu is very similar to yours. For example currently for writing `f (x y)` you type `f ( x SPC y )`. The reason for pressing the `)` (which isn't listed in your impl's presses) is that it affects what will happen when you type more stuff after wrt operator precedence etc.
From little high-level knowledge I have about graphics and Haskell: I think that Haskell in combination with Vulkan would work good (not great, but also not only OK). Linear types should also help a bit. Multicore is happening, and perhaps there are approaches where it could work. For example: some of the data is mutable, some gets taken care of by linear types, and maybe a significant part of what is left can be discarded and recreated (could be faster than GC-ing it). I also have 2 vague notions: strict-by-default system (PureScript) would be an even better fit. And (this one is very foggy, I still have a lot to learn/explore) Idris would be an excellent fit if resources were put into making state of the art compiler. I know dependant typing adds some complexity when adding "rust style" memory management. No conclusion, just something I would love to see some research on... 
Here's his first example of transforming JSON: j &lt;- readFile "data.json" j &amp; _Value . key "res" . values . key "catlist" . values . key "points" . _Array . from vector %~ reverse . sortBy (comparing (preview (key "stop" . _Double))) This is pretty much isomorphic to the way you do it with specter. Here are the selectors side-by-side: :res ALL :catlist ALL :points key "res" . values . key "catlist" . values . key "points" And here are the modifiers: #(reverse (sort-by :stop %)) reverse . sortBy (comparing (preview (key "stop" . _Double))) His "walking an entire data structure" example can be done like this: j ^.. _Value . biplate :: [Scientific] j ^.. _Value . biplate :: [String] In fact, his final parsing example is actually easier in Haskell! It is simply: j &amp; _Value . key "res" . values . key "catlist" . values . key "points" . values . key "stop" . _String . from packed %~ show . round . read Thanks to types, we already have parsing and unparsing functions. We don't need to write hacky versions of them like he does.
&gt; In this case, instead of paying the cost by representing it as an &gt; &gt; optional type and then handing the problem over to the parser, you're paying the cost in lines of code in your application by checking whether some key exists in a map, and then acting on it. Except in dynamic languages you almost never write those checks and instead let the application explode in your face.
If you are interested in state machine specification, you might find it interesting to check out some of the work by the Abstract State Machine (ASM) community. The Z/B-method/Event-B community are basically using abstract state machines as well, and as is Lamport's TLA+. 
&gt; This corresponds directly to the two language's choices for what the lightest possible syntax of space-separated juxtaposition represents. In Clojure it represents grouping things into a list. In Haskell it represents function composition. Technically juxtaposition is the lightest syntax in Haskell and it's application not composition.
Ahh yes, that's what I meant.
I came out with almost exactly the same notes as you, after reading through this article. I wonder if the author having been away from Haskell for 4 years might be a big contribution to his misconceptions. There has been quite an uptake in community and industry adoption since then. One other gripe I personally have is the lumping together of Java and Haskell in the beginning - their type systems are worlds apart.
Got it. So it's a library for transforming data structures. Like `jq` or `sed`. That's cool. If I want to transform a JSON value which has a single type `Value`, which apparently is what all Clojure data structures are, being a dynamically typed Lisp, it's good to write a library to do that. That's basically what my tool [jl](https://github.com/chrisdone/jl) does. All the types are `Value -&gt; Value` etc. Obviously, we could do that in Haskell. However, Clojure people work with basically JSON which is one type: `Value`. In Haskell we like to define more types, we prefer to distinguish a list from a tree or a dictionary and ones we're not aware of yet. So we'd like to transform many types of values with one interface; we don't know the single type ahead of time. That's why we have classes like `Functor` and `Traversable` and `Foldable`, or `Data` or `Generic`, which are similar to what `specter` does, but apply to more than one type. I wouldn't say this puts Clojure at a compelling advantage. You can always just use a dumb `Value` type. Or you can use `Data` or `Generic` to make generic transformations. Sometimes I use that. Here's an example in the REPL: &gt; import Data.Data &gt; import Data.Traversable &gt; import Data.Tree I'll define a new data type representing a person (which I know Hickey doesn't like): &gt; data Person = Person { name :: String, age :: Int, lastname :: String} deriving (Data, Typeable, Show) Let's define a set of types of people. &gt; let people = [ Person { name , age, lastname } | name &lt;- ["Chris", "Mary", "Peter", "July"] | age &lt;- [ 82, 23, 52 ] | lastname &lt;- ["Jones", "Smith"] ] And let's define an ancestry made up of people like this, where as we go down the tree they get younger and they're the descendents. We stop at age 20. &gt; let ancestry = unfoldForest (\p -&gt; (p, filter ((&gt; 20) . age) (map (\c -&gt; c{age=age p - 15}) people))) people I can use `fmap` from the Functor class that lets me apply a function to every element in the structure. In this case I'm just using `show` to convert it to a string. I'm fmap'ing over the list of trees. Then I just take 8 lines from somewhere in the middle, as the output is a bit bit for reddit. &gt; let draw = putStrLn . unlines . take 8 . drop 15 . lines . drawForest . fmap (fmap show) So if I draw, it looks like this: &gt; draw ancestry | | | | | `- Person {name = "Mary", age = 22, lastname = "Smith"} | | | `- Person {name = "Mary", age = 52, lastname = "Smith"} | | | +- Person {name = "Chris", age = 37, lastname = "Jones"} | | | | | +- Person {name = "Chris", age = 22, lastname = "Jones"} If I want to collect all people who are 20 or older, I can write: &gt; map name (listify ((&gt; 20) . age) ancestry) ["Chris","Chris","Chris","Chris","Chris","Mary","Mary","Chris","Mary","Mary","Chris","Chris","Mary","Mary","Chris","Mary","Mary","Chris","Chris","Chris","Mary","Mary","Chris","Mary","Mary","Chris","Chris","Mary","Mary","Chris","Mary","Mary"] If I want to upper case all the strings in the data structure, for some reason, I can use the `everywhere` function, with `mkT` (make transformation) to say I just want to transform this type of thing. &gt; draw (everywhere (mkT (map toUpper)) ancestry) | | | | | `- Person {name = "MARY", age = 22, lastname = "SMITH"} | | | `- Person {name = "MARY", age = 52, lastname = "SMITH"} | | | +- Person {name = "CHRIS", age = 37, lastname = "JONES"} | | | | | +- Person {name = "CHRIS", age = 22, lastname = "JONES"} Equivalently for numbers: &gt; draw (everywhere (mkT (* (2 :: Int))) ancestry) | | | | | `- Person {name = "Mary", age = 44, lastname = "Smith"} | | | `- Person {name = "Mary", age = 104, lastname = "Smith"} | | | +- Person {name = "Chris", age = 74, lastname = "Jones"} | | | | | +- Person {name = "Chris", age = 44, lastname = "Jones"} Imagine that I'm messing with time travel and I want to, for some evil plan, put a curse on Mary that makes all women in the Mary family's offspring disappear. I just look in the tree for occurrences of Mary and set the children to empty. &gt; draw (everywhere (mkT (\n -&gt; if name (rootLabel n) == "Mary" then n {subForest = []} else n)) ancestry) | `- Person {name = "Mary", age = 67, lastname = "Smith"} Person {name = "Mary", age = 23, lastname = "Smith"} Generic functions like `everywhere` are different to e.g. `traverse` and `fmap`. The latter two operate on a *specific* data structure. Like, just work on the nodes in the tree, but don't delve into the labels. `everywhere` by comparison will look at anything however, not respecting any kind of structure. I don't recognize a big difference between Scrap Your Boilerplate that I've demonstrated above and Specter. SYB has a whole library of combinators for this. Lens and uniplate have alternative ways of doing it. 
Eh sry. Caused concurrency issues by mutating my comment. I'll edit some more and probably botch it more.
Lamdu is one of my inspirations. :-) 
I think there is value in pointing out that sometimes, a given method of representing a problem space leads to the invention of problems that don't need to exist. The point about maybe is convoluted, but, there is often a structural difference between a value that may or may not exist, and a key that may or may not posses an inhabited value. data Record = Record {someField :: Maybe Int} Record cannot model the full scope of possible valid serializations of Record in many formats. It's an incomplete model. That's not a fundamental limitation of ADTs, but it is a fundamental limitation of how records are usually used. The argument is that this is a bad model with artificial limitations regarding the use-case of unstructured tagged data, and I think that's absolutely correct - It is a bad model. I bump up against those limitations constantly and it's quite frustrating. I disagree that it's a fundamental limitation of Haskell's type system, or the philosophy behind Haskell's type system, but it is, in my mind, certainly true that the most 'natural fit' when writing Haskell has some serious limitations. Ultimately I think we (Haskell) need(s) a better model for representing the human need to put names and hierarchy to fields. We're describing the API and it's implementation at the same time when we shouldn't be, and that is the entire problem.
I think it's essentially a variant of the XY problem.
&gt; I find that a programming language's strengths come from both what it makes easy and what it makes hard. Paul Phillips (of Scala fame) had a great observation related to the INGSOC slogan "Freedom is slavery" which I think might be relevant: If you can do *anything* (dynamic typing) it becomes very hard to know what your code does -- and thus harder to refactor, etc. If you're constrained in what you can do (static, pure) it's easier to see what your code does because you already know what it *cannot* do. Anyway, that's the way it works for my programming mind. Mileage may vary.
Carmac agrees about GC in this talk https://youtu.be/1PhArSujR_A?t=1494
Writing any non-trivial optics-code without type-checker? Thanks but no thanks.
I really want to understand the `Maybe` argument in this post (and Hickey's talk). Can you expand on the record example you gave? You mention serializations, which makes me think of, say, JSON and XML. I could see that record represented in a number of ways: {} # missing key { "someField": null } # explicit null { "someField": 123 } # value exists &lt;record someField&gt; # missing attribute &lt;record someField=""&gt; # empty attribute &lt;record someField="123"&gt; # record in attribute &lt;record&gt; &lt;someField /&gt; &lt;/record&gt; # empty element &lt;record&gt; &lt;someField&gt; 123 &lt;/someField&gt; &lt;/record&gt; # record in element # ... and so on Is the argument that these representations are different and therefore aren't fully captured by `Record { someField :: Maybe Int }`? If so, I agree. Depending on the application, you may end up with a full blown ADT for `someField` that differentiates between present, present but empty, and missing. Similarly its representation in various formats can be given by instances and newtypes. That said, I don't perceive this as a limitation of static types at all. After all, `(contains? m :some-field)` and `(some? (get m :some-field))` mean different things! 
Ohhhh! I think you just explained the whole puzzling-Maybe thing. Rich may have been thinking of the *key* not existing as being different from there being *no value*. Do I understand you right?
Perhaps, but that also feels reductive and not productive. If someone says "I can do X in Clojure" and you respond with "don't do that", we haven't gotten anywhere. 
The direct translation while remaining idiomatic would be to use guards: type SomeValue = () validate :: a -&gt; Either String SomeValue validate x | failsReq1 x = Left "fails req-1" | failsReq2 x = Left "fails req-2" | otherwise = Right (useSomething x) failsReq1 :: a -&gt; Bool failsReq1 x = False failsReq2 :: a -&gt; Bool failsReq2 x = False useSomething :: a -&gt; SomeValue useSomething x = () Alternatively, you could use multi-way if: {-# LANGUAGE MultiWayIf #-} validateMWI :: a -&gt; Either String SomeValue validateMWI x = if | failsReq1 x -&gt; Left "fails req-1" | failsReq2 x -&gt; Left "fails req-2" | otherwise -&gt; Right (useSomething x) And certainly there are many more strategies as you have already seen with the other answer.
Sure, I agree that it's not particularly pedagogical and wouldn't suggest actually saying it that way, but I think that's where the dissonance perhaps stems from?
I dislike Rich arguments as he says "but `clojure.spec` isn't a type-system", what it is then? If one doesn't consider type-driven features, like instance resolution, then you can interpret Haskell with only run-time type-checking as well. And I don't want to mention what you can do if you use type-driven features to the max. It's not the code which is there, but the one which can be left out. Also Rich (and OP) forgets completely about generic programming: with `GHC.Generics` or `generics-sop` you,can, for an example, write code merging any data. There is `hczipWith` in `generics-sop` exactly for that. It's a bit on "type-system puzzle" side of things, but at least compiler will tell me if my generic algorithm doesn't make sense, or if I try to use it in completely wrong place. Taken to the extreme, one can write Haskell code for a family of records which have the same set of fields, which are present or not. There will be some boilerplate, but at least you'll be explicit about when you know the values are there, and when you don't know. -- Two sets of fields: (foo &amp; bar) and quu. data Rec f g = Rec { _recFoo :: f Int , _recBar :: f Bool , _recQuu :: g Text } -- if we need all fields, then: etc. type I = Identity printFullRec :: Rec I I -&gt; IO () printFUllRec = ... -- and we can increse `recFoo` even we don't know if it's there -- or many, because `[]` is a functor too. -- -- here I use `lens`, to illustrate that it's not boilerplate heavy -- given right tools (which may require time to learn though) incFoo :: Functor f =&gt; Rec f g -&gt; Rec f g incFoo = over (recFoo . mapped) (+1)
&gt; In Haskell, in practice, do people use Maps where they could have used ADT's in order to easily merge information? I have almost never done this, and I seldom come across code that does this. There are two reasons that I suspect that this is uncommon: 1. It's not that often that I actually need to merge two records. 2. Using a `Map` is only appropriate for fields that are optional 3. You typically lose type information when you do this I'll expand on (3) a little bit. Let's compare the following two approaches to representing a person that we may know varying things about. Notice that every field is optional, satisfying the gripe in (2). Standard approach: data Person = Person { personName :: Maybe Text , personAge :: Maybe Int , personWeight :: Maybe Int , personBirthday :: Maybe Date } Map-based approach: data Field = Name | Age | Weight | Birthday deriving (Eq,Ord) type Something = ... type Person = Map Field Something The problem is, what is `Something`? We can just make it be `Text`, but now `Age` and `Weight` can be assigned a textual value, which is wrong. We can make it some kind of sum like a json [Value](http://hackage.haskell.org/package/aeson-1.2.3.0/docs/Data-Aeson-Types.html#t:Value), but now any time we lookup any of the fields, we have to do an additional pattern match on the `Value` to make sure it's the right type. So, no matter what you do, you've created a type that isn't quite as restrictive as the original definition of `Person`. Another way of saying this is that the Map-based types have more inhabitants than the record-based type. (As a minor aside, we don't really have to define the `Field` type. We can just use `Text` inside, although this makes the `Map` able to hold things that we may not want it to hold). There is a way to get something as restrictive as the record-based `Person` but with some of the cool merging abilities of the `Map`-based `Person`. It's called a dependent map. The bad thing is that they're clunky in haskell (and clojure's type system cannot express them). Whenever Dependent Haskell lands (in two or three years), we'll have a more user-friendly way to deal with these. Also, with a dependent map, you can't merge arbitrary maps together. They need to agree on the universe of types and the interpretation function of this universe. Practically, that means that you don't want to use `Text` as your key (you'd want to do something like the `Field` thing I did above). But anyhow, to get back to the original point, the `Map`-based approach is not that common in haskell. It can be really practical though if you have a bunch of fields and they all have the same type (typically `Text`).
That's truly great news! I was sure Stack would be godsend for easing adoption of new ghc versions and this finally proves it.
I'll check those out. Have only heard about TLA+ before. Thanks!
Yes, I believe that's what is being said. Largely it's sort of immaterial... To the run time representation in Haskell. But to the serialization or parsing of a static representation, it's very much not immaterial. The point I'm making about limitations is more broad, but with regards to this specific quandry, and also the general problem, I agree that it's not a fundamental limitation of ADTs or the Haskell type system - I think it's just a fundamental limitation of the practice of using record types in this manner, which is a much more specific complaint and a much more addressable issue.
Yup, that is what I am getting at, and I believe that is what Rich was getting at.
&gt; If you can do anything (dynamic typing[1]) it becomes very hard to know what your code actually does -- and thus harder to refactor, etc. I think more relevantly and/or viscerally than "your code" is that you don't know what will happen with stuff you pass to _other_ code. Within a single reasonably-sized function, imperative stateful programming isn't necessarily a big deal if everything else was pure. It's when you have a complex object and you pass it to some function which turns out to change it, sometimes, according to some global variable and the phase of the moon, and then a few thousand other issues like this interact, that you have problems. The things you promise your code won't do is the things that other code can't do either. (And as many have observed, across a large enough time frame _all_ code because "other people's code", even the code you wrote.)
God, yes!
It almost seems disingenuous to assume that, since types are perfectly capable of expressing that. let m = Map.singleton "hello" Nothing :t m m :: Map String (Maybe a) :t lookup "hello" m lookup "hello" m :: Maybe (Maybe a)
My guess would be that the author has never truly learned to play Haskell by its strengths in the first place; got thrown into it, headed into that phase where you battle the compiler rather than use it as a brain extension, and never got out of it.
I honestly think that Rich is basically completely out of touch with what a good type system can do. I do know from interviews that he's done (MS Channel9, i think it was) that he has at least a passing familiarity with Haskell as of 5-10 years ago, but that's a *completely* different beast to what exists now.
I agree, but that's more an indictment of mutability than it is of dynamic typing. As far as these nightmare scenarios go, Clojure handles them pretty well by making things immutable by default. 
It's almost like he runs a business and profits off of people choosing his product over others. It would be unreasonable to expect that Hickey is going to be unbiased in a presentation of static types when his entire business model is promoting a language and ecosystem that doesn't have them. "Yeah, static types are great! Totally worth the effort. We didn't build them into Clojure, for, uh, no reason, but you should still pay for our consulting and use our product."
That's true of data, but even a seemingly innocuous "log-this-data" side effect in a function can be *devastating* if you're logging to a (blocking) network connection. The actual case I'm thinking of was actually logging to a RS-232 serial port, which caused *massive* slowdowns (to the point of "not usable"), but it *was* a long time ago that this particular scenario caused a disaster for my (then) company.
That may be true, but that's also not so great for Haskell: If people get turned off before they can reap the rewards, maybe something is wrong. 
I'm not sure we need to be *that* harsh and outright accuse him of being a shill, but still... It's definitely worth pointing out that such an arrangement *definitely* introduces bias regardless of how impartial a person tries or imagines themselves to be. The bias is just unavoidable by basic human nature.
Java has a type system? /s
It doesn't really matter which direction the causation flows (eg he's biased against static types and therefore runs a business based on dynamic types VS he's running a business on dynamic types and therefore is biased against static types). Uncle Bob has the same bias -- his career is based on promoting/consulting/training on TDD methodology, so we should expect him to decry any tool that *isn't* that. Hickey is obviously very smart and talented, and (like most smart/talented people) has a few blindspots. It's IMO pretty easy to excuse someone for having blindspots on the state of the art of static typing, considering that even Haskell is a bit behind the curve here.
I think we basically, agree. :) I like to think of myself as at least semi-smart/talented, but I have a little bit of training as an empirical scientist. That's a *really* good way to take one's confidence in one's own surety down a few notches :). It's a sort of meta thing -- you can still be confident without being *sure* of everything. In fact, it's probably easier (though not as profitable) to be absolutely sure that you don't know anything ;).
The `FFunctor` thing shows up in so many different people's code that I wish something like it were just in `base`. Also, great post. I also have very keen interest in this area, although I tend to approach it from the extensible records angle.
Failure to mention lenses in any comparison between Haskell and Clojure is immediate reason for dismissal. 
Oh boy, I have a strong opinion about this! When you have a bunch of systems that talk to each other by sending structs on the wire or saving them to disk, you really need a struct definition language that looks like Google's Protocol Buffers, and generate code from that. Here's the key feature that sets it apart from your language's native record system: You can take a struct definition, add some fields, remove some others, recompile the program that sits on one side of an API, leave the program on the other side unchanged, and things will still work. Or you can deserialize some structs that were serialized with the old definition, and things will still work. It's not a coincidence, in fact it's a guarantee that our code relies on every day. It's achieved by assigning a numeric "tag" to each field in a struct, and never reusing old tags when updating the struct definition. This is called versioning and it's absolutely indispensable. This means you cannot have non-nullable fields. All fields must use the moral equivalent of Maybe or Nullable. (Protocol buffers do have required fields, but people quickly figured out from experience that they cause horrible problems, and now they are frowned upon.) Protocol buffers are an amazing idea. Sure, if you have something simple like a point with X and Y coordinates, you might be better off your language's native pair or record facility. But once you have a data struct with five or six fields, I can almost guarantee that it will keep changing, that it will need to go on the disk or wire someday, and that it'll be better off as a proto rather than your language's native thing. The generated code of protos comes with tons of useful methods for introspection, serialization, versioning, etc that you will almost certainly need. To cover the static vs dynamic angle, you can generate code in multiple languages from the same proto definition (which will be serialization compatible of course), and in each language it will be as strongly typed as the language allows. I do feel slightly more comfortable using protos in a language with static checking, but it makes a minuscule difference compared to the choice of using protos in the first place.
&gt; Types are concretions, sure. But so is any code that actually interacts with the data meaningfully. If you ever need to access a key foo in a map, then do something with the contents, then your code is also a concretion, because it is now coupled to some structure you expect the data to have. I think this really hits the nail on the head.
Possibly, but sometimes the answer genuinely is "don't do that". If a solution lets you solve a problem in 5 minutes instead of 25, but shackles you with an extra 3 hours of technical debt, you just shouldn't do it.
For the problem with row polymorphism you mention, perhaps CTRex could provide extra primitives. A first attempt would be a coercion between a `r`-indexed record and a `(Remove l (Extend (l :-&gt; a) r))`-indexed one, but then there is an ambiguity problem. A more promising one would be to get a coercion between actions `FSM Empty r` and `forall s. FSM s (r :++ s)`... I haven't thought this through but I hope that gives you some ideas. --- &gt; A mapping between a resource name is written (...) This sentence is missing something! "between a resource name" and what? 
`distribute` is really the surprising one, especially since it jams up `GeneralizedNewtypeDeriving`. For the sake of efficiency, it would probably be better to have `collect` and a fusion of `collect` with `cotraverse`.
I think in the case of the simple example of record, sure, that creates problems that doesn't need to exist vis a vis serialization, but as you and others have said it's a bit of a straw man, and this makes me think even more that this whole enterprise is actually about legitimizing personal taste (and potentially shoring up business models). 
I agree with theQuatcon. While mutability of data is a _huge_ instance of the problem, it extends in general to everything a function may do, including everything we call "effects" and even things like memory consumption that we may normally not call "effects" but are. If your language permitted you to assert that a given function didn't allocate anything, then that would be one more guarantee that you could count on from things you call. I'm not sure if there's any such language. (Perhaps in the embedded space. Verilog, maybe.)
Brick would be overkill. There is no need for interaction. Just printing what url is downloading and the percent of the total of urls. I was thinking to use events and rise them on individual download completion. Is there another approach that might fit the scenario?
Well, there is of course always the danger of conflating value with taste. I think criticism can be valuable regardless of where it comes from, although certainly framing the arguments in this way can make deriving that value more difficult. I guess I feel like this (easily extensible, serializable records) is a problem that the Haskell community / ecosystem has that we frequently handwave away by talking about how great Aeson is, but we rarely ever get real with ourselves and question whether or not records are actually even a good way to model this problem space at all. I think the superrecord package comes pretty close as a way to deal with a lot of these issues gracefully, but realistically I'd like to see 'Haskell proper' have some way to model the presence of a labeled field in a structure as a class constraint instead of as a specific, inhabited type.
The `return` in `return (unsafeFreeze ma)` looks redundant. I imagine `curry` (and `uncurry`) can be linear polymorphic curry :: ((a, b) -. c) -. (a -. b -. c) but what if the linearity is mixed, do we require these functions? curry_1 :: ((Unrestricted a, b) -. c) -. (a -&gt; b -. c) curry_1 f a b = f (Unrestricted a, b) curry_2 :: ((a, Unrestricted b) -. c) -&gt; (a -. b -&gt; c) curry_2 f a b = f (a, Unrestricted b)
Well, there is of course always the danger of conflating value with taste. To be clear: I don't care in which order these are conflated. Or if they're conflated at all. 
Events as in using the async package and continuously `waitAny` till there are no more events? Yes, that seems reasonable and clean. f [] = putStrLn "Done" f jobs = do (j,res) &lt;- waitAny jobs putStrLn $ render res f (filter (/= j) jobs)
This is not bad practice, but it also is not really good advice for the person who's asking for help here. And truth be told, I'm not sure it's very realistic unless you've got a secret team backing you up who DOES do the machine provisioning and maintenance. Maybe Google's Kubernetes or (ugh: it's bad) Amazon's ECS is enough for you and you can avoid that aspect. If so, great. If not, well... there are transitions and technical debt for every organization.
If you find yourself in this scenario, you're in for more engineering work anyways. One way I've stopgapped this kind of problem in the past was to make my logic read from an SQS queue that I had access to. I could basically call specified methods and this process (which was part of the deploy) would validate and run them when it saw. I tend to build my systems as loose autoscale groups of queue consumers with robust queues, so this approach was easy for me. While it may not be as architecturally convenient for you, it will likely be less work than enabling a fully interactive interface and less unsafe than making a "wizard's console" command line box hiding in the edge of your cluster waiting for human interaction.
It's because the `distributive` package is meant to build with only Haskell 98.
could you please share an article about how to generate docs with servant if you have any
http://gmane.org/about/ is quite neat as it provides you NNRP access to *several* mailinglists which is particularly useful when you've just subscribed to a list (or not yet subscribed), or if you want to reach for really old posts in your familiar email/news reader.
The point of the `Record` type (presumably representative of types we tend to define in Haskell) is to model a type that you want to program with. It's not the point of types to model all the details and arcane nonsense of external systems; that's what a parser is for, to parse out the crap into something sensible. The fact that JSON or XML can have empty string attributes, attributes with no values, null, missing keys, etc. is generally not made use of, and is typically a mistake or a bad idea when it is used. A JSON decoding package or an XML package could let you handle these different variations of "no valueness", but they in practice mostly don't, because who cares for the most part.
GIGO, man! /s
This post loses any validity once the specter of "solving a real-world problem" was invoked. Can please just get over this "real-world" obsession and realize we are all trying to solve problems in this world.
It's actually quite easy to write a sufficiently injective function: just get the fingerprint and use that! The fingerprint is not mathematically certain to determine the type uniquely, but when there's a collision all heck is sure to break loose anyway, so you might as well assume it doesn't happen.
Why was linear typing chosen over uniqueness typing?
Stackage /= Stack! You can use it with cabal-install and even Nix just fine. In fact, it was *originally* just designed to aid cabal-install workflows by offering cabal config files.
 data Record = Record {someField :: Maybe Int, someOtherField :: String} deriving (Generic) instance ToJSON Record instance FromJSON Record Now if someone passes me an object that has null in `someField`, and I parse it, that'll work, but parsing and serializing are no longer isomorphic across disparate systems because my type doesn't have enough information to fully account for all possible valid serialization states. Depending on the remote parties implementation, this could be a point in the project where I need to go handroll my own ToJSON instance instead. That is not fun. It is a situation which only arises because `Record` is fundamentally bad at representing the serialization format. Here is another example of that scenario. import SuperRecord record = #otherField := "COOL I CAN MODEL THIS NOW" &amp; rnil Ta-da! I have solved this problem. Well, I guess someone else did. But whatever. I now have a representation that does actually carry all the information I need, and my problem goes away. Not a fundamental Haskell limitation. A limitation of using vanilla records to describe data that they were never really meant to model. There are ways for us to have our cake and eat it too - So let's shoot for that instead.
&gt; When evaluating a value in head normal form does not terminate, this value is not considered to be consumed exactly once, making the looping function linear Shouldn't it be "IS considered"?
You could create an additional library or executable target in cabal that selectively picked a subset of your modules that you might need and can run without maxing out your memory, then use ghci with this target?
One easy way to find a lot of low hanging fruit is to [run hlint over your code](https://hackage.haskell.org/package/hlint).
&gt; If that's what you want, use Map String Dynamic. But you won't because it's miserable. I'd actually love to use that on occasion! Only if Dynamic supported polymorphism, e.g. `toDyn id`, I might be fully satisfied with it.
&gt; I've only ever used arrows with tuples, honestly. The choice of arrow is the interesting bit - presumably "function"
Another option, not quite as versatile for every case, could be very good row type polymorphism, ala `rawr`, `bookkeeper`, `extensible`, `vinyl`, etc, but without the ridiculous limitations like max 8 fields, or worse-than-linear compile time slowdown (can be seconds, minutes added quickly).
I guess it arguably follows from a combination of "functions that return monoids" and "Boolean (disjunction)", but "Sets (union)" is another very useful example of a monoid.
I’ll be honest, I don’t think this is an improvement. Perhaps that’s subjective, but I promise I come from a place of some experience, so let me explain. There is a meaningful difference between the example I posted and your change: my example is using fakes, but your implementation is using mocks/stubs. For example, the `FileSystemT` implementation I wrote for testing makes an attempt to implement an extremely simple virtual file system, only implementing the parts of the file system necessary for the interface the code needed. If the code later grew to need a `writeFile` operation as well, it would be simple to update `FileSystemT` to use a `StateT` transformer instead of `ReaderT` and modify the transformer state when `writeFile` is used, and all our existing tests would still work! We’d still have a nice, clean notion of a “file system effect”, along with a separate, virtual, pure implementation of it we can use to test any code that uses the file system. In contrast, your approach hardcodes an implementation of `readFile` in the test. This doesn’t *seem* so bad, but I’m going to make the case that it is: it means your test knows that `readFile` is only going to be called with a specific argument. And this is a problem, since it means your tests know too much about the code being tested. No longer are you asserting that the code under test produces the right result, you are also asserting it invokes its collaborators in specific ways. This is very simple, but I find it breaks too often in practice, leaving you with tests that change every time the implementation does, something that defeats the whole point of an automatic test harness. Now, I want to make this very clear: these complaints are not philosophical in nature. I am wary of this sort of thing because *I tried it before*. We originally wrote a library called [`test-fixture`](https://hackage.haskell.org/package/test-fixture) to generate boilerplate for stubs. You still need the mtl classes, so it doesn’t eliminate that, but I find value in those, anyway—it gives you type-level control over your effects. What it *does* do is generate stubs for an arbitrary mtl class, much like how `hs-di` allows you to use `override` to stub out an implementation. Here’s the thing: after we wrote a large codebase with hundreds of tests using test-fixture, we found they were giving us much less value than we had hoped. Because we pushed the test-time implementation of our interfaces into each test, rather than creating a virtual implementation, we had to update dozens of tests whenever our interfaces changed. The tests were still useful, but they weren’t useful enough to pay for themselves—it was uncommon that code broke a test without being a false positive, and we spent a lot of time maintaining tests for code we didn’t even change. So we took a step back. We started asking ourselves how to reorganize our tests to better serve us. We stopped using `test-fixture`, and we decided to start doing two things: 1. We decided to eliminate mocks and stubs whenever possible in favor of self-contained, decoupled, virtual implementations of our interfaces. This was more boilerplate, which I loathe, but we grit our teeth and powered through it. While the boilerplate seems extensive for such a small example, in practice, we wrote it once per project and never touched it ever again. In fact, we originally intended to collect some of the common fake implementations and put them on Hackage somewhere so we wouldn’t have to write the boilerplate ourselves. We never did, because it was so inconsequential to our code size. In our real applications, the fake, test-time implementations of our interfaces make up a minuscule percentage of our code, and they actually made our tests *smaller* than they were with `test-fixture` because our test-time implementations were collected into one place, not reimplemented throughout our whole test suite. 2. We found there were cases where we really did want stubs/mocks, such as when we needed to interact with a third-party API with a very large surface area like AWS. Even then, we found we really wanted to keep our main logic decoupled from mocks, since they seemed to infect our tests rather quickly, so we usually came up with a high-level API on top of a low-level API, then created a virtual implementation of the high-level API and used stubs/mocks to test the “real” implementation of the high-level API against the low-level API. For our stubs/mocks, we could probably use something like `hs-di`, but we wrote something different that I think is a bit more idiomatic and less magical, [`monad-mock`](https://hackage.haskell.org/package/monad-mock). It also interacts with mtl classes, like `test-fixture` did, but it’s closer to mocking than stubbing, and it is unashamed about it. We found it generally makes things quite a lot simpler when we really just want to test that a particular set of foreign APIs are called in a particular order. The point of my `mtl-style-example` project was very much to show what the skeleton of a project looks like with a structure that **still generalizes to big applications** without collapsing under its own weight. Optimizing for code size for tests for a tiny, 7-line program is a waste of time and will not produce any useful metrics. Unfortunately, I cannot publish the much larger applications we’ve built because they are proprietary, but I would express caution in extrapolating too much from tiny examples, and I would be curious to know if you’ve applied `hs-di` to much larger projects, as we did with both `test-fixture` and our new style. And yes, I realize this is basically my word against yours, as I’m essentially just saying to trust my wisdom I accumulated from some unseen project, but really, I’m trying very hard to help make it easier for people to write *good* tests, not *short* tests. Sometimes, a little bit of extra boilerplate and structure actually helps before people get familiar with the patterns so that they don’t go off and write fragile tests that provide no value whatsoever.
I used to do a lot of code reviews in python. It was standard practice to request that any code that used dictionaries in a record-like way be rewritten using records. No one ever objected, they mostly said "I was sloppy here" or "I didn't know about that convenient record library." I never noticed any disagreement that all code reviews should insist on that. This is in python, where the records don't give you any type safety, mostly just a place to put documentation. So the gains were relatively small, but worth doing anyway because no one could think of why you'd want to use dictionaries. If they're so great for flexibility, shouldn't there have been a few people willing to go to bat for them, to at least loud enough to break my sense of unanimity? This is a company of thousands, with many "native speaker" python programmers (such as Guido), over more than a decade. People were doing plenty of merging information. But probably that key-by-key d1.merge(d2) is almost never appropriate. More like sum these numbers, append these lists, average these numbers, etc. and it's just as easy to write sum(x.f for x in xs) as it is to write sum(x['f'] for x in xs). Of course plenty of people used dictionaries, for all the same things you'd use Map for. Just not for record-like things, where different keys might have different types.
rank2classes author here. Yes, the 0.2 version got a little bit more general thanks to a pull request from https://github.com/clintonmead . I'll gladly accept other pull requests as well. Thank you for pointing out that I forgot to update the docs. 
Or you do, the choice is left to the dev to decided if it's a good use of time. Clojure spec gives you more that validation on a property based level, but it's time consuming. I rarely use it except on the outskirts of my system 
&gt; If the data type weren't recursive, this would be trivial. What about the following recursive type? It's much easier isn't it? data Bits = Nil | Cons Bool Bits What about this one? data Trit = Zero | One | Two data Trits = Nil | Cons Trit Trits Or this one? data Combined = Combined Trit Bits Trit A bit harder now: data Reversed1 = Reversed Trits Bit Trits data Reversed2 = Reversed Bits Trit Bits If you got through all this, finding an encoding for `RepCon` should be a piece of cake.
The base method is actually [distributeWith](http://hackage.haskell.org/package/rank2classes-0.2/docs/Rank2.html#v:distributeWith), which I found the most useful for my original use case in [grammatical-parsers](http://hackage.haskell.org/package/grammatical-parsers). I'd have no issue with adding the `cotraverse` method as well. 
I'm sure his primary motivation was money, what a great insight. /S.
I remember seeing something similar in the [MemoTrie](https://hackage.haskell.org/package/MemoTrie) library. The tree structure here follows from a decomposition of natural numbers into: 0, 2n+1, 2n+2, i.e., as an inductive type of three constructors, `0`, `2(_)+1`, `2(_)+2`. A `Tree a` is a representation of `Integer -&gt; a`, and to make such a function, we need three cases: one `a` for zero, and two subfunctions `Integer -&gt; a`, memoized as `Tree a`. Hence `Tree :: Tree a -&gt; a -&gt; Tree a -&gt; Tree a`. There is a similar logic for other datatypes. You need one tree type `TreeT :: Type -&gt; Type` for every type constructor `T :: Type`. Constructors: `TreeT` consists of one constructor, with one field for every constructor of `T`. Remember that `TreeT a` is meant to represent a function `T -&gt; a`. Fields: If `A :: U -&gt; V -&gt; T` is a constructor of `T`, then the corresponding field in `TreeT` has type `TreeU (TreeV a)` (representing the function `U -&gt; V -&gt; a`!) Here `T` is a simple type. What about higher-kinded type constructors? `F :: Type -&gt; Type` is represented by `TreeF :: (Type -&gt; Type) -&gt; (Type -&gt; Type)`, so that the type parameter of `F` corresponds to a tree parameter of `TreeF`. I think the general pattern is to replace every `Type` by `Type -&gt; Type`. For example, the tree of lists (used for example to define the tree type of `[SomeTypeRep]`) is: -- To write a function f :: [x] -&gt; a ... data TreeList t a = TreeList a -- f [] (t (TreeList t a)) -- f (x : xs) = g x xs, for some g :: x -&gt; [x] -&gt; a -- Note that an equivalent type would be TreeList t (t a), -- corresponding to a flipped argument order, g :: [x] -&gt; x -&gt; a 
Hey, thanks for taking your time to write a reply. What you are saying (e.g. a virtual, pure, fake filesystem) is also very much possible with `hs-di`. If you want, you may extend your mtl example with a `writeFile` function, and I can do the same. I think the latter will contain less boilerplate for the same functionality. Just like you could write an `mtl` instance that is just a stub as well, couldn't you? The two approaches might be isomorphic to each other, sans the boilerplate. Or maybe not. I can't say I have lots of experience with MTL yet, just like it seems you may not have as much with DI as I do. Speaking about experience: I've also spent quite a bit of my time writing applications with a TDD approach before I started learning Haskell. A large allure for me picking up on TDD was very similar to what drew me to Haskell as well: to be able to write stable, correct, regression-free software, quickly and efficiently. And then I noticed that despite the typesystem being able to pick up on certain classes of mistakes, it is very much not able to pick up on classes of others. So I reached back into my inventory of tools, realizing that I do want to write unit tests for Haskell as well. And I do want to be able to write as much or as little for mocking/stubbing/faking as the certain scenario calls for. I don't want to have to implement a whole virtual API for something as huge as AWS, as you also point out. And to attempt to quantify experience; one of the bigger proprietary projects I worked on that used a dependency injector similar in spirit to hs-di was around 60,000 SLOC. (Very quick and rough estimate, may be off.) So I'm definitely not mainly optimizing for 60 lines, but at least three orders of magnitude larger, or more. Is `hs-di` battle-tested with a large codebase? Not yet. The largest Haskell codebase I'm using it at is around 5000 sloc. But that's why I want to get the word out: let others know that here is a potentially better (and at least seemingly more concise) way of writing tests, so we can get there one day. I'd love to hear of others' experiences, questions and feedback after trying it out. p.s. I've also written my fair share of too-tightly-coupled tests to implementations, so I'm well aware of that issue. I'm not saying hs-di prevents you from writing such tests per se, but it definitely lets you avoid them if you have the wish and knowledge to do so. So perhaps hs-di doesn't have training wheels yet as much, but I think it is a very powerful tool in knowledgeable hands. And even still, the worst case scenario involves having to change and generalize overly-coupled test-specifications, so I think that's perfect avenue for people to learn the techniques that I suspect both you and I have acquired over the years. And if they wish, with hs-di they can learn those as they go, without having to do it upfront.
Part of the problem is that you have to Actually solve real problems at some point. I'm confident that with the Java interopt I'm going to have the tools I need to manage a large system. How does that story look In Haskell? Or in languages with even "better" type systems. We're doing a lot of lip service to the power of types but the world is being eaten by ml and through python and no one is arguing that's the world's best language. Its like the programing Language equivalent of "Sometimes it's not what you know, it's who you know." 
&gt; Either way, great minds think alike! [Thirded!](http://hackage.haskell.org/package/rank2classes#readme) You definitely should finish and publicize the blog post. I think you should give some more due to `Applicative`, though, I have not found it as unwieldy as you imply. The `liftA2` function is one easy way to use them; you can see several examples in the [source code](http://hackage.haskell.org/package/grammatical-parsers-0.2.1/docs/src/Text-Grampa-ContextFree-LeftRecursive.html) of grammatical-parsers. 
I've left a comment on the gist asking about the "ambiguous type variable" error.
*A relevant comment in this thread was deleted. You can read it below.* ---- Oh boy, I have a strong opinion about this! When you have a bunch of systems that talk to each other by sending structs on the wire or saving them to disk, you really need a struct definition language that looks like Google's Protocol Buffers, and generate code from that. Here's the key feature that sets it apart from your language's native record system: You can take a struct definition, add some fields, remove some others, recompile the program that sits on one side of an API, leave the program on the other side unchanged, and things will still work. [[Continued...]](https://www.resavr.com/comment/clojure-vs-static-typing-9759999) ---- *^The ^username ^of ^the ^original ^author ^has ^been ^hidden ^for ^their ^own ^privacy. ^If ^you ^are ^the ^original ^author ^of ^this ^comment ^and ^want ^it ^removed, ^please [^[Send ^this ^PM]](http://np.reddit.com/message/compose?to=resavr_bot&amp;subject=remove&amp;message=9759999)*
Sort of. UserRepo requires that instances that are declared for it are also instances of Monad. This does not mean that all Monads get to become instances of UserRepo - they need instance declarations to define how they operate. I think if you're new to classes in general you might want to start with something with less going on. Like, say, `Data.Monoid` - The `Monoid` class sounds super abstract and weird, but it's actually very simple, and has a TON of separate examples of implementations to help you understand how classes work.
They may have been originally designed to fix cabal but since the advent of Stack that use case is not supported and the cabal stackage files were discontinued. And there's little reason to provide them as everyone's either moved on to Stack by now or will soon now that Stackage has added support for GHC HEAD. There's [good reasons](https://github.com/fpco/stackage-server/issues/232#issuecomment-327068080) why Stackage shouldn't provide support for cabal anymore like Hackage revisions breaking Stackage and to avoid bad experience for users which we're trying to move away from cabal.
``` type MArray s a type Array a newMArray :: Int → ST s (MArray s a) read :: MArray s a → Int → ST s a write :: MArray s a → (Int, a) → ST s () unsafeFreeze :: MArray s a → ST s (Array a) forM ::Monadm⇒[a]→(a→m())→m() runST :: (∀a. ST s a) → a ``` Fig. 1. Signatures for array primitives (current ghc) --- There seems to be an error for runST. Shouldn't it be runST :: (∀s. ST s a) → a ? 
Nice observation! In other words to go from our data type `T` to a memo tree for `T` we have to convert sums to products and products to composition, recursively. Here's an encoding of the idea using data families. I think it is probably more accessible that carrying around parameters of kind `* -&gt; *` but it is slighly more of a hack. {-# LANGUAGE TypeFamilies #-} import GHC.Word (Word64) data Fingerprint = Fingerprint Word64 Word64 data RepCon = RepCon Fingerprint [RepCon] | RepApp Fingerprint RepCon RepCon | RepFun Fingerprint RepCon RepCon data family Tree a b data instance Tree RepCon a = TreeRepCon (Tree Fingerprint (Tree [RepCon] a)) (Tree Fingerprint (Tree RepCon (Tree RepCon a))) (Tree Fingerprint (Tree RepCon (Tree RepCon a))) data instance Tree [e] a = TreeList a (Tree e (Tree [e] a)) data instance Tree Fingerprint a = TreeFingerprint (Tree Word64 (Tree Word64 a)) data instance Tree Word64 a = TreeWord64 (Tree Word64 a) a (Tree Word64 a) 
I guess you mean data RepCon
&gt; we have to convert sums to products and products to composition, recursively In fact this becomes obvious in light of the fact that it's effectively the parameter on the left hand side of a `(-&gt;)`!
Here’s a nice intro to codata and corecursion, and how data and codata are the same in Haskell, but they don’t have to be: http://blog.sigfpe.com/2007/07/data-and-codata.html So there is no reason to require a co-list to have the same representation as a list.
&gt; It is a situation which only arises because Record is fundamentally bad at representing the serialization format. That was the point I was trying to make; that types aren't supposed to represent the serialization format. No Haskeller wants to program with a serialization format, they want to program with more abstract, simpler types. This example just isn't that compelling. Relatedly I do think row-polymorphism would solve plenty of problems.
I appreciate the attempt to reduce boilerplate, however as someone relatively new to Haskell the following "magic" (Template Haskell + Quasi-quotation) looks pretty scary: injLeaf "getCurrentTime" $(mainD &amp; override "logger" [qc| \a -&gt; modifyIORef logs (++ [a]) |] &amp; override "getArgs" [qc| return ["sample.txt"] |] &amp; override "readFile" [qc| \"sample.txt" -&gt; return "Alyssa" |] &amp; override "getCurrentTime" [qc| readModifyIORef clockStart (addUTCTime 1) |] &amp; assemble) Right now I prefer the boilerplate heavy typeclass/mtl approach because I can look at the code and understand what is going on. This might change with more experience, though.
Wow, damn that's a cool way to think about it! Added to the list.
When you say to just get the fingerprint, how do you mean that? A `TypeRep` has many fingerprints inside of it. I could just add them all together, but that's not going to be injective. I could do something where I bitshift by 128 bits each time and OR it with the next fingerprint, but if I'm not somehow tagging with a representation of the data constructor as well, I don't see how I would expect this to be injective.
Short answer in case anyone is apprehensive to read /u/Syrak's and my long comments: This is exactly what [MemoTrie](https://hackage.haskell.org/package/MemoTrie) is for. 
FWIW, I don't think verbiage is going to convince anyone. "Manage a large system". OK, define "manage", define "large" define "system". &gt; the world is being eaten by ml Ahaha... no. That's not happening. The world is being eaten by JVM languages, .NET languages (C#, maybe F#)... and if you talk to physicists Fortran. It's network effects, not languages being evaluated on their merits in a vacuum.
Hi, Thank you, this might sound later. I am trying to use `YeshQL` to make queries. however, I add it to stack dependency ` YeshQL &gt;= 3.0.1.3` but am getting an error ` YeshQL must match &gt;=3.0.1.3, but the stack configuration has no specified version` 
As a person, who previously used Java at work I don't really understand all complaints about boilerplate in Haskell... 147 LOC to implement application core type with all polymorphic interfaces and mocks and tests??? Wow, crazy. In Java one data type with several fields and implementation of all boilerplate default methods like `equals`, `hashCode` getters and setter implemented will occupy at least 300 LOC. I think the reason why Haskell people don't like boilerplate and why they invented a lot of clever techniques (`deriving`, `Generics`, etc.) is because we don't have good IDE which can generate all boilerplate for us. I don't like boilerplate as well. But at the same time I don't mind if it can be generated automatically. Boilerplate might be ugly. And it occupies some lines of your code (and we all know that there exist code lines shortage nowadays). But boilerplate has one advantage — it's extremely easy to understand boilerplate code.
Thanks! This is super cool and is a pretty concise explanation of how to do this for any data type. I had never really understood how `MemoTrie` worked before, but this lays out the rules pretty clearly.
I've only recently added yeshql to stack, so it is possible that your project uses an older resolver that doesn't have it yet. Either switch to a newer resolver, or manually add stack to your extra-deps (in stack.yml).
Thanks for providing feedback from the point of view of someone relatively new to Haskell. I can empathize with where you are coming from, so let me try to pull back the curtain and illuminate what happens behind the scenes. test-suite/HsDiExample/MainSpec.hs:(20,7)-(25,16): Splicing expression mainD &amp; override "logger" "\\a -&gt; modifyIORef logs (++ [a])" &amp; override "getArgs" "return [\"sample.txt\"]" &amp; override "readFile" "\\"sample.txt" -&gt; return \"Alyssa\"" &amp; override "getCurrentTime" "readModifyIORef clockStart (addUTCTime 1)" &amp; assemble ======&gt; let (main, _, _, _, _) = mainT in main (readModifyIORef clockStart (addUTCTime 1)) (return ["sample.txt"]) (\ "sample.txt" -&gt; return "Alyssa") (\ a -&gt; modifyIORef logs (++ [a])) &gt; A bit of an aside: I got this output by enabling the GHC flags `-ddump-splices -ddump-to-file` which in general allows anyone to peel back a layer of TemplateHaskell/QuasiQuote machinery and peek inside. &gt; &gt; Earlier versions of stack would place this at `./test-suite/HsDiExample/MainSpec.dump-splices`, right next to `./test-suite/HsDiExample/MainSpec.hs` but it seems newer versions tuck it away deep inside `.stack-work`: &gt; &gt; ./.stack-work/dist/x86_64-linux/Cabal-1.24.2.0/build/hs-di-example-test-suite/hs-di-example-test-suite-tmp/test-suite/HsDiExample/MainSpec.dump-splices &gt; I've also cleaned up the dump-splices output a bit so they are easier on the eyes. So what happens in there? What is `mainD`? It's really just a value for the [`Deps` algebraic data type](https://github.com/Wizek/hs-di/blob/857110913d8aaaa1360b2b970a8e2f4ac318cbda/src/DependencyInjector.hs#L92), representing the tree of dependencies that is constructed at compile time with the help of `inj, injLeaf, injAllG` declarations. How, you may ask? library/HsDiExample/Main.hs:12:1-24: Splicing declarations injLeaf "getCurrentTime" ======&gt; getCurrentTimeD = Dep "getCurrentTime" Original Pure [] getCurrentTimeT = (getCurrentTime) getCurrentTimeA = getCurrentTime getCurrentTimeI = getCurrentTime library/HsDiExample/Main.hs:13:1-17: Splicing declarations injLeaf "getArgs" ======&gt; getArgsD = Dep "getArgs" Original Pure [] getArgsT = (getArgs) getArgsA = getArgs getArgsI = getArgs library/HsDiExample/Main.hs:14:1-18: Splicing declarations injLeaf "readFile" ======&gt; readFileD = Dep "readFile" Original Pure [] readFileT = (readFile) readFileA = readFile readFileI = readFile library/HsDiExample/Main.hs:16:1-7: Splicing declarations injAllG ======&gt; loggerD = Dep "logger" Original Pure [] :: Deps loggerT = (loggerI) loggerA = loggerI logger = loggerA mainD = Dep "main" Original Pure [getCurrentTimeD, getArgsD, readFileD, loggerD] :: Deps mainT = (mainI, getCurrentTimeT, getArgsT, readFileT, loggerT) mainA = mainI getCurrentTime getArgs readFile logger main = mainA That's how. And you can also just `print mainD` (or `Text.Pretty.Simple.pPrint mainD` to get the prettier output) in `MainSpec.hs` to see what's inside: Dep { name = "main", src = Original, kind = Pure, cs = [ Dep { name = "getCurrentTime", src = Original, kind = Pure, cs = [] } , Dep { name = "getArgs", src = Original, kind = Pure, cs = [] } , Dep { name = "readFile", src = Original, kind = Pure, cs = [] } , Dep { name = "logger", src = Original, kind = Pure, cs = [] } ] } And what happens when we only override a single dependency? $(mainD &amp; override "getCurrentTime" [qc| readModifyIORef clockStart (addUTCTime 1) |] &amp; assemble) &amp;nbsp; -- print mainD Dep { name = "main", src = Original, kind = Pure, cs = [ Dep { name = " readModifyIORef clockStart (addUTCTime 1) ", src = Replaced, kind = Pure, cs = [] } , Dep { name = "getArgs", src = Original, kind = Pure, cs = [] } , Dep { name = "readFile", src = Original, kind = Pure, cs = [] } , Dep { name = "logger", src = Original, kind = Pure, cs = [] } ] } &amp;nbsp; test-suite/HsDiExample/MainSpec.hs:(29,7)-(34,16): Splicing expression mainD &amp; override "getCurrentTime" "readModifyIORef clockStart (addUTCTime 1)" &amp; assemble ======&gt; let (main, _, getArgs, readFile, logger) = mainT in main (readModifyIORef clockStart (addUTCTime 1)) getArgs readFile logger Then the original implementations are used for everything except for the one dependency we overrode. So, it really is just function application at it's core. The point is that this would become hugely tedious to do manually for anything larger than a trivial example, and that tedium is what TH alleviates for us. Please tell me if it's any clearer, you have some other questions, or something is still confusing.
Once your code changes your editor has to rewrite existing code on the fly. Why does abstraction matter? In part because you would have to patch up everywhere when you change a little thing. I honestly cannot understand how someone can study computer science and have that opinion.
How long is a average URI? Does it really make such a difference that they be represented as Text or Bytestring instead?
Perhaps. If I was forced to write in a language that only allows to express simple concepts with dozens of lines of code, and at the same time allowed me to auto-generate those, maybe my outlook would be different. But as it stands, the languages where I am coming from, I'm used to _even less_ boilerplate then the mtl style example. I really like code that is concise, to the point, and doesn't hide domain-specific logic in seas of irrelevant minutia. That let's me express the essence of the specification without cruft. And if at one day someone writes such a boilerplate-generating IDE for Haskell and I try it out, maybe my outlook will change.
Perhaps. If I was forced to write in a language that only allows to express simple concepts with dozens of lines of code, and at the same time allowed me to auto-generate those, maybe my outlook would be different. But as it stands, the languages where I am coming from, I'm used to _even less_ boilerplate then the mtl style example. I really like code that is concise, to the point, and doesn't hide domain-specific logic in seas of irrelevant minutia. That let's me express the essence of my domain without cruft. And if at one day someone writes such a boilerplate-generating IDE for Haskell and I try it out, maybe my outlook will change.
I didn't make a good argument (late night), I wasn't suggesting that someone can't solve real problems in Haskell. I was suggesting that I personally have to stop investigating different languages in order to get work done. Not indefinitely, but for some period. Can we agree That jvm has very little mind share in ml. And That there is a huge upsurge in the interest in ml? The clojure community is definitely feeling left out from the ml tabel, they realize the language is as much about it's community and libs and frameworks as it is the language basis. Because of this mindset, it's one of the few none oop languages one can find a job in. I would be upset to hear Haskell and other statically typed fp languages weren't concerned about being left out. 
That's a very good point. It doesn't necessarily have to be financial interest. Once you invest a lot into something that makes you biased to a degree. Personally, I think it's one of the greatest things to admit your own mistakes and short-comings. It really shows that you care more about the matter at hand and not so much about your ego.
It's not about length. I need to be able to consume Text because text is what I have as input. This is not a big issue when you use a standalone parsing function, you can use encodeUtf8 or something, but if you would like to integrate the parser into a bigger parser, then stream types must match. There will be also a parser that can consume ByteStrings as well in later versions. (We already can render to both.) Also note that I represent host, path pieces, etc. as (refined) Text values. This is a better design choice IMO because in the URI data structure they are already percent-decoded and thus can contain characters outside of ASCII range (Unicode). Percent encoding is handled for you by parser and renders. Once you have a URI, it's in "programmer-friendly" format.
Sometimes you can have a very long text file embedded in the uri. 
We have bots automatically resurrecting comments now? That's creepy. 
Thanks! My use cases so far have mostly been parsing database configurations from URIs, but I see how this could be useful in a more general context, especially when you already have the data as Text.
&gt; But as it stands, the languages where I am coming from, I'm used to even less boilerplate Which languages are you coming from? Just interested.
There are two sides to this view. Firstly, if there is no documentation, then a project is abandoned or a toy project. But among the ones that have documentation and fit your use case at all, the ones with the best documentation and healthiest communities should be considered for production use.
The differences are based on preference and opinions, e.g. classy prelude uses more generalized versions of most standard functions, and includes the somewhat debatable mono hierarchy (i.e., standard typeclasses of kind * -&gt; * rewritten as * using type families, so that you can do things like map over a Text). For a beginner, I would recommend sticking with the standard Prelude; the advantages of custom preludes won't really become obvious until you advance a bit more, and even then it is not necessarily the right choice.
Which lisp? Clojure seems to compile to jvm bytecode not machine code as well.
You could have a modern-iri and support https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier to handle the case where you need to parse something out of `Text`. `Iri -&gt; Uri` and `Uri -&gt; Maybe Iri` (`Nothing` = invalid percent-encoded UTF-8) could be provided in one or the other.
**Internationalized Resource Identifier** The Internationalized Resource Identifier (IRI) was defined by the Internet Engineering Task Force (IETF) in 2005 as a new internet standard to extend upon the existing Uniform Resource Identifier (URI) scheme. The new standard was published in RFC 3987. While URIs are limited to a subset of the ASCII character set, IRIs may contain characters from the Universal Character Set (Unicode/ISO 10646), including Chinese or Japanese kanji, Korean, Cyrillic characters, and so forth. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
shouldn't it work by adding `yeshql-3.0.1.2` to extra-debs in stack.yaml file?
&gt; it's extremely easy to understand boilerplate code Fantastic point. Haha.
I am not associated with the role, but it mentions wanting some quantitative background as well. That is probably the hardest part for them with respect to filling the position. If I were them, and I liked your background, I would pay for you to attend a course to learn the quant skills.
Part 1: https://www.barrucadu.co.uk/posts/concurrency/2017-10-14-writing-a-concurrency-testing-library-01.html GitHub: https://github.com/barrucadu/minifu/tree/post-02
Relapsespec might be simpler to write if you user Either as a monad
I have to ask, why does your `traverse` return `f (Compose t (Flip b))` instead of the simple `f (t b)` returned by the [corresponding method](http://hackage.haskell.org/package/rank2classes/docs/Rank2.html#v:traverse) in `rank2classes`? 
https://imgur.com/a/cOger For future reference...
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/wClR7F6.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dp0mrui) 
That's an interesting anecdote about the team's preference of records to dictionaries, even in a language where they are basically just different syntax for the same thing. When I work in a dynamically typed language (which is less and less these days), I have a similar guideline for myself. Even in the absence of types, it really does help communicate something to whoever looks at the code next. Something like: - Record: all of the fields will be present - Map: not sure which fields will be present, all values have same type 
It should; if it doesn't, then I'm afraid the problem isn't yeshql though, but stack, cabal, hackage, or how you use them.
Thank you for sharing :) really appreciate
How have you not been banned from this subreddit yet? By the way, the cabal stackage files have not been discontinued: https://www.stackage.org/lts-9.10/cabal.config
&gt; And natural numbers brings 2 bits of information (in average). Usually this is phrased as "the entropy of the discrete probability distribution {1/2, 1/4, ...} is 2 bits". The natural numbers are not really relevant here directly; the usual entropy formula is simply a summation that doesn't care what the underlying set (for the probability distribution) looks like. &gt; hence the type Nat is oversized for the job (2 bits) (but far less than a 32 bit integer) You are mixing the two meanings of "bits" here. The former corresponds to information theoretic bits (derived from a probability distribution), whereas the latter corresponds to the representation/encoding of the integers from 0 to 2^32 -1. &gt; Is there any argument in favor of nat instead of bit as a measure of information ? Well, they're just different units ... depending on what kind of log is convenient to take, you might choose to work with different units. If you are going to take a derivative, you might prefer to have the natural log. If your probability distribution has stuff like 1/2^n mostly, you'd prefer using base 2 ...
I don't remember using any article. I was able to do it using the library docs for servant-swagger. This is what I do: 1. Create your data type: `data User = User {field :: String} deriving (Generic)` 2. create `instance ToSchema User` (you can also use `genericDeclareNamedSchema` &amp; `defaultSchemaOptions` with `fieldLabelModifier` to drop any prefixes you may have on your records too. 3. Generate Swagger spec from the servant API import Servant.Swagger (toSwagger) import Data.Swagger (Swagger) swaggerDocs :: Swagger swaggerDocs = toSwagger userAPI 4. In my test main function I create the spec file like: import qualified Data.ByteString.Lazy.Char8 as BL8 writeFile "swagger.json" $ BL8.unpack $ encode swaggerDocs I had some one complex type that was hard to convert using Generics so had to hand roll the `ToSchema` instance with the help of the library author (posted a Github issue). HTH 
What follows is mainly personal and you can disagree (*De gustibus...*), but I'd stick to the more usual names when qualifying Data.Set and Data.Map (Map or M, and Set or S, instead of DataSet and DataMap), since it feels too verbose otherwise. In this case DataMap is even misleading, because you are actually using Data.Map.Strict rather than the deprecated Data.Map Also, guards tend to be clearer than if then else, so I'd try to use that more, specially for functions that are a single if then else, e.g. deriveReturn _ (Node _ _) ns = if head ns then (Empty, tail ns) else (Not ZAny, tail ns) versus: deriveReturn _ Node{} ns | head ns = (Empty, tail ns) | otherwise = (Not ZAny, tail ns) (You can also use C{} instead of (C _ _ _ _) when you don't care about the values and just want to pattern match the constructor). I also noticed you are using DataMap.filter, you can simply use filter since Map is an instance of Foldable.
UT
So I was bothered by the fact that mappending two such lasagnas causes the result to have a reversed (cheese, sauce) sequence in the middle of lasagna instead of the usual (sauce, cheese) sequence. That made me wonder of sauce and cheese was commutative. So I decided to mix the sauce and the cheese, resulting in the new recipe: sauce+cheese, (pasta, sauce+cheese)*, pasta, cheese which seems a much more monoidal recipe. [Here](https://imgur.com/ZZNfLgV) is a picture of the resulting lasagna.
I'm thinking about introducing Haskell at my work, as a backend HTTP server for a Single Page App. Any advice on making it accessible to beginners? My background is in Yesod, so I was planning on using that (though maybe I should try Servant?). Ideas so far: 1. Probably will remove classy-prelude. That seems to make error messages more complicated. 2. Encourage not genericizing functions more than necessary 3. Probably would recommend this book to beginners, http://haskellbook.com/ 4. Try to get familiar with popular editor setups. I think Atom + haskell-ide would be my recommendation to non vim/emacs users. Haven't thought about this too much, just broadly looking for advice on introducing Haskell to others.
This is a realization that I've recently got as well. I write Java for work. When I write Haskell, sometimes I feel it a bit cumbersome. It's puzzling to me because the overall LOC seems to be considerably less than Java counterpart. But then I realize that the IDE plays a big part in this. I use VSCode + intero for Haskell, but I'm unable to get as good experience as IntelliJ. I use these IntelliJ features heavily: - automatic import management - automatic refactoring (rename, convert block to method, convert block to variable) - getters + setters + eq + hashcode + toString - code completion (intellisense) (intero provides one, but not as snappy as IntelliJ)
I'm having a simple GUI app. Initially the form have a button, that when clicked, will create another button. That new button behave the same, i.e generate new button on clicked. So the code look like this: addButton :: IO () addButton = do btn &lt;- createButton attachButtonToForm btn -- set click handle to this function itself btn `onClick` addButton Now suppose at some point I need to get the list of all buttons created. In an imperative language, I would use a global list, and append the newly created button to that list. How can I do that in a functional way? Thanks :)
Maybe. .but what? 
Is it extensible? If you use `myFunc` in a context with other additional effects, then you must reconstruct the record of actions to call it. myOtherFunc :: Monad m =&gt; (Log m, CurrentTime m, FileAccess m) -&gt; m () myOtherFunc (log, currentTime, fileAccess) = do myFunc (log, currentTime) fileAccess A type class makes the glue code invisible. You also lose some safety with explicit argument passing, because it is not guaranteed that the `Log m` argument for example is always the same. These problems don't arise in the original example because it consists of just one `main` function. 
`mtl` style scales in terms of the number of classes and instances you write. This style scales in terms of the number of functions you write *and* in the number of effects you want to pass around. Furthermore, there's going to be at least two changes you need to make when adding an effect -- adding it to the type signature of the function, and adding it to the function arguments. `mtl` only requires adding the constraint to the type alias. This is a good way to implement "dependency injection" for a small, local function where the effects are tightly located to a single scope. If the effects are more pervasive, it's worth the boilerplate to move it into an mtl class.
The explicit style is how I would solve this problem in other languages (elixir, F#, JavaScript). Just pass the effectful functions as parameters, and swap them for alternatives in tests. Simple and easy for new FP-ers to understand.
What about in Haskell? What approach do you choose for managing effects?
[`Handler` is defined as a newtype over `ExceptT ServantErr IO a`](https://hackage.haskell.org/package/servant-server-0.11.0.1/docs/Servant-Server.html#t:Handler). The constructor `Handler` has the type `Handler :: ExceptT ServantErr IO a -&gt; Handler a`. When working with `ExceptT` values, it's often useful to use [`withExceptT`](https://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Except.html#v:withExceptT), a function that transformers the error type of an `ExceptT` with the given function. So we can write a helper function `convertError :: Error -&gt; ServantErr`, and then `withExceptT convertError` has the type `ExceptT Error m a -&gt; ExceptT ServantErr m a`. If we compose that with `Handler`, we get: liftError :: ExceptT Error IO a -&gt; Handler a liftError action = Handler (withExceptT convertError action) That lets you use `someFunction` with `ExceptT`. Note that the two definitions you gave are *very* easy to interchange: `ExceptT` will convert the second into the frist, and `runExceptT` will convert the first into the second. 
MTL approach also requires you to update the type signature. (I'm aware type signature is optional, but you write it most of the time for top level function). agree, manual plumbing is the downside compared to MTL. in my experience on the other project (which is larger than this example), it results in less code to write. &gt; not guaranteed that the Log m argument is always the same I don't think I understand. Mind to elaborate a bit?
I just used `ReaderT` recently and I wasn't really sure how much I should be using it. It definitely is great for config stuff, but beyond that I'm a little unsure. I felt like I ended up using it too much, and my code kind of became a bit of a n opaque "blob", I know that production code tends to have a lot of their code live in some kind of `Reader`, but I feel like it's easy to lose out on explicit type signatures, so I guess I'm wondering if anyone has any tips on some good practices with it and other transformers.
&gt; some way to write compiler-checkable cost model assertions (ie the stuff you put in a haddock comment that says O(n)) [some of that can be done with dependent types](https://twanvl.nl/blog/agda/sorting)
 You are mixing the two meanings of "bits" here. The former corresponds to information theoretic bits (derived from a probability distribution), whereas the latter corresponds to the representation/encoding of the integers from 0 to 2^32 -1. Just FYI, the two meanings are the same: the maximum Shannon entropy (aka information) of a discrete random variable with 2^32 possible outcomes occurs when all of them are equiprobable, so H_max = - \sum{1/2^32 * log_2(1/2^32)} = log_2(2^32) = 32 bits. Meanwhile, you need 32 bits of memory to represent the space of outcomes of the random variable. The Shannon information capacity (maximum entropy) of 32 bits is thus 32 bits. OP, I didn’t follow your reasoning, but choice of base of the logarithm in the Shannon entropy corresponds to a choice of units of information; as the Wikipedia article describes, “nats” are simply the unit analogous to “bits” when you use a natural logarithm rather than a base-2 logarithm in computing the Shannon entropy. I highly suggest reading [Shannon’s original paper](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication) (or the book version) if you haven’t. It’s one of the all-time greats and remains highly readable.
**A Mathematical Theory of Communication** "A Mathematical Theory of Communication" is an influential 1948 article by mathematician Claude E. Shannon. It was renamed The Mathematical Theory of Communication in the book, a small but significant title change after realizing the generality of this work. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Why not use `MonadError` instead of a concrete monad transformer? -- `MonadError` from "mtl" someFunction :: (MonadError Error m, MonadIO m) =&gt; m Int someFunction = undefined data Error mapError :: Error -&gt; ServantErr mapError = undefined -- `fmapLT` from "errors" handlerFunction :: Handler Int handlerFunction = fmapLT mapError someFunction
In this example explicit passing is clearly much more readable to me, but I have very limited experience in Haskell (I'm more an FP bystander). I couldn't assess how it scales to larger codebases. 
I think this would be hard to program with: - What happens if I call a `(CurrentTime m)` function from a `(Log m, ..., CurrentTime m, ...)` function? I have to do the plumbing manually! This works just fine when you use freer or mtl, with zero plumbing. - This approach requires you to order your effects all time time in your tuples/records. Which is also more work and two functions with same effects but ordered differently won't be compatible! Ugh! - What happens if I partially handle some of the effects and get a new function back that doesn't require a handler for that effect? e.g. I have a `Log` handler and I want to get a `Monad m =&gt; CurrentTime m -&gt; m ()` from your `myFunc`. Again more plumbing. Neither mtl nor freer have these problems. Btw, I had also ported that repo, but to freer. You can see it here https://github.com/osa1/mtl-style-example/commit/84cbf2ed27b156c0c4dca9f7469d9d11405fa56c . Not demonstrated in this example, but freer also has advantages like easily allowing you to have multiple `Error` effects and handling only some subset of them at a time: https://gist.github.com/osa1/5d6ca4e85d0b5bce90f5917405ab1388#file-errors-hs-L37
Why should I be banned? Have you even read the [linked issue](https://github.com/fpco/stackage-server/issues/232#issuecomment-327068080)? I'll quote &gt; We removed the link originally because it **was causing user confusion** with uncertainty on how to get started with Stackage. As **users standardized on the Stack+Stackage workflow**, it made sense to avoid the confusion. &gt; The fact is that Hackage revisions have broken snapshots in the past, this isn't a theoretical discussion. And the point of Stackage is to **give guarantees about build plans**, which currently **can only be done through Stack** &gt; **Confusing users** about how to get started. **We're recommending they get started with Stack, because it works reliably**. We are **not recommending they use cabal**, since it does not have first class support for Stackage, or even correct support in the presence of Hackage revisions If we include this information and **people have a negative experience, it will reflect badly on Stackage** and/or cause maintenance overhead for the Stackage Curator team 
What if this approach is combined with some sort of extensible records? (like the one in PureScript)
That solves (1) and (2) but I don't think it can solve (3), because here you'd have to pass a single record that handles all effects but for (3) you actually want passing a single record for each effect (so that you could pass some subset of them and get a function back that requires the rest). Extensible records are really great (I don't know about PureScript, what I really mean is row-polymorphic records) and we should have them in Haskell.
Great to see other approaches to the same case study. I hesitated to invest my time on Free / Freer though. They perform significantly worse than MTL [1][2]. [1] http://tech.frontrowed.com/2017/09/28/benching-free/ [2] https://cdn.pbrd.co/images/GzURdi6.png I agree about the plumbing. At first, I thought it will be so bad, but after trying it out myself, they are less cumbersome to handle compared to building all those instances in MTL.
Yes, freer is slower than mtl and explicit parameter passing, but I think it's so far the most expressive (see the exception handling examples I linked above), boilerplate-free (no need for instances to compose effects) and concise (no explicit parameter passing) solution to extensible effects &amp; handlers. At least among the well-known solutions. &gt; I agree about the plumbing. At first, I thought it will be so bad, but after trying it out myself, they are less cumbersome to handle compared to building all those instances in MTL. I think it'd get really bad as you start glueing libraries together. Imagine every library having an "effect tuple" as an argument, each with different orders and numbers of effects. I think what's great about this approach is (1) it's really simple (2) can be implemented with no library support.
&gt; Moved from Haskell to C++. Doesn't that kind of disqualify it from ever being upstreamed into GHC? Wouldn't it make more sense in general to approach this problem from GHC? That way you wouldn't have to write your own RTS.
I guess, so, but LLVM also disqualified itself from being upstreamed into GHC, but it's still used to generate code.
The hs-di library looks like a very interesting idea. But I agree with /u/chshersh that ultimate goal should not be removing boilerplate. That's really a minor issue in testing code. The goal should be reducing the cost and effort of writing and maintaining tests. That means, most importantly, test that are as easy as possible to read and write. Sometimes removing boilerplate can help that, but sometimes it can make it worse. I think your approach in hs-di can be very valuable to make writing good tests easier.
I've done something like this in an older code base and it worked fine for me. In my case I bundled the effects into a single record, something like `Context m = Context { ... }`. The one big advantage is, that you can substitute effects, e.g. switch to a different logger for a given action, without the need for explicit state. This allows for some interesting things, that aren't possible in MTL style. That said, I still wouldn't place my bets on this being a much better approach than MTL.
Since you're in IO, you can use an IORef to keep mutable state. You could use the StateT transformer on top of the IO monad but that's most likely not going to work right due to the onClick callback taking in just an IO action.
I generally prefer to use `Either` and drop into `ExceptT` as needed. Short circuiting transformers like `ExceptT`, `MaybeT`, etc tend to have less predictable behavior when you're composing them in stacks, especially when you get more than one short circuiting thing inside the stack.
 let m = Map.singleton (&lt;) 3 "foo" n = Map.insert (&gt;) 4 "bar" m n probably breaks the map invariants now.
I think /u/theindigamer was right in that OP conflates two meanings. Sure, for the equiprobable case (e.g. max entropy), the entropy conincides with our typical 32 bits we have in machine integers. But assuming a geometric distribution for nats and then saying that 32 bit binary integers are less efficient than unary nats is bogus. E.g. entropy (in bits) depends on the probability distribution, whereas 32 bit machine integers are just a specific representation optimized for the case of maximum entropy.
Yes, I agree, that IDE matters a lot. Regarding automatic import management — our company is working on the tool which goal is to do automatic import management for you. Unfortunately, in Haskell we don't have good standalone IDE (with big company behind it) which does everything for you.
I view it in a sort of "guarantee about return value" way. In this perspective `Reader a`, in a type signature simply means "the return value of this function *should be assumed to depend on* any value in `a` in addition to any of its arguments". That carries a cost, because now you know less about the dependencies of your function. The more narrow you can make `a`, the more transformations and ways to refactor, debug and test the code will become available to you. The largest amount of flexibility is when you dispose of the `Reader a` altogether, and the function only depends on its arguments. So I try to ask myself, is the loss of flexibility worth the gain in having to type less things? Sometimes it is, sometimes it is not. But what you have to remember is that `Reader` isn't an on-or-off thing. If you create nested record types, e.g. data Company = Company { ceo :: Person, employeeCount :: Integer } data Person = Person { name :: String, birthday :: Date } then you can always call a function with a "zoomed in" version of the record. Instead of having to run ceoBirthSign :: Reader Company BirthSign ceoBirthSign = _computation companySuccess :: Reader Company Boolean companySuccess = do ceoBirthSign &gt;&gt;= \case Aquarius -&gt; True _ -&gt; False you can do birthSign :: Reader Person BirthSign birthSign = _computation companySuccess :: Reader Company Boolean companySuccess = do local (ceo birthSign) &gt;&gt;= \case Aquarius -&gt; True _ -&gt; False where `birthSign` is now more localised and has more guarantees about the return value. This way, Haskell and the `ReaderT` monad makes it very convenient to ensure that functions only depend on as little of the reader data as they need to. ---- *Important:* Now that I've spent half a page writing about this, I also want to make sure you know that given how refactoring-friendly Haskell is, it's no problem if you accidentally make a function depend on too much. Just fix it later. So don't spend too much time thinking about these choices. Just go with something, knowing that it's going to be easy to change later.
That sounds like a very rare use case. When used properly, a URI is a name for a resource, not a resource. You sometimes have a cryptographic signature embedded in a URI, so a URI could be a few hundred bytes long; even that is a hack in my opinion. It shouldn't ever be longer than that.
Because in my `traverse` both `t` and `f` have kind `(k -&gt; *) -&gt; *`. Maybe I should alter `align` and `attribute` to be polymorphic in `a`, rather than fixing it to `Identity`: align :: (Conkin.Traversable t, Prelude.Applicative f) =&gt; f (t a) -&gt; t (Compose f a) attribute :: (Conkin.Traversable t, Prelude.Applicative f) =&gt; t (Compose f a) -&gt; f (t a)
Thank you!
&gt; MTL approach also requires you to update the type signature. What do you mean? If you don't change the implementation of `myFunc` then its signature doesn't need to be updated.
So, `addButton` never returns, but some other event handler in your GUI needs to know how many buttons there are? Yeah, this would be one of the few cases were using mutable variables (`IORef` or, much better, `TVar`) would be worth the trouble. 
I think that's an exception. GHC would rather not have to call out to external tools. LLVM is an exception because it's a gargantuan pile of 3rd party software that sits at a fundamental level of the toolchain (basically the C toolchain level). Something that sits at GHC's own invented level, which isn't supported by a major party, and which comes with its own GC and RTS is much less likely to be an exception. We've already seen numerous issues pop up due to LLVM's being an external command line tool. I doubt we would incur those same issues again for something less than LLVM
Not sure what you mean by 'never returns', I think it just return normally. Anw, since both 2 replies here mention IORef so I think I'll go with it. Thank you all :)
Thanks for this nice library. It has been needed for quite a while. Your "Features" section is plenty motivation for this library. I disagree with just about every one of your criticisms of the existing libraries in your "Motivation" section. But you don't need to waste your time in flame wars with me and others. I propose you just delete that section, or move it to a wiki or blog post or something. The library claims to conform to [RFC 3986](https://tools.ietf.org/html/rfc3986). So auto percent-decoding is a problem. In section 2.4 "When to Encode or Decode", the RFC states that percent-decoding is scheme dependent. I understand that auto percent-decoding is convenient in common cases, though. Maybe provide an optional way to turn that off. That way you get the convenience in the common case, but the library still supports general schemes in a compliant way. Having a URI parser that can be embedded in a larger parser is a cute idea. Thanks again for a nice library!
Ah! I thought the last line was a tail-recursive call! So it does actually return, after attaching itself to the new button? So how about (I have no idea if this will compile, by the way): addButton :: [Button] -&gt; IO () addButton butts = do btn &lt;- createButton attachButtonToForm btn -- set click handle to this function itself btn `onClick` (addButton (btn : butts)) Note how the callback now has a list of buttons as a parameter, and the callback attached to the current button is added with a list that includes the new button. So, at each invocation we know all the existing buttons. Now, I assume that the callback has to have type `IO ()`? In which case, if you wish for some **external function** to be privy to the list of buttons, then once again I say you should probably use a mutable variable. This is based on having zero understanding about the rest of your code, of course...
I think this is an argument that gets too much attention. Yes, freer is O(10x) worse than MTL, but then again, who cares? When was the last time you weren't IO bound anyway? Tons of successful companies are run on Python, Ruby and PHP and still manage to be fast enough.
What's wrong with listing motivations in the package documentation? I think it's a helpful way to educate readers about other libraries in the same space. For instance, my own [Salve](https://hackage.haskell.org/package/salve-0.0.9/docs/Salve.html#g:4) package does that, as does [microlens](https://hackage.haskell.org/package/microlens-0.4.8.1). Documentation can be opinionated. 
Thanks! I've indeed removed the "Motivation" section, sorry if offended authors of the packages. Good point about automatic percent-decoding, I'll see about making it optional.
Great question, I have no idea! Would like to know as well.
Ah, I should have written about this - I moved to C++ so I can quickly prototype about this stuff. My long term plan is to redo this in Haskell once I understand what the correct trade offs are in any case.
Ah. I dunno if it helps, but /u/angerman has done some work that he's calling "llvm ng" which IIRC may have some much nicer integration with llvm from the Haskell perspective. Might be worth looking at.
I don't disagree with you. But this particular library looks like a high-quality potential standards-track library. Why distract attention away from the library itself in its README? It sounds less professional. We can have fun discussing opinionated and/or controversial things in plenty of other places - here on reddit, on the café, on #haskell, etc.
I haven't heard of `GHC.ExecutionStack`. As far as I know, [`GHC.Stack`](https://hackage.haskell.org/package/base-4.10.0.0/docs/GHC-Stack.html) is the way to go. 
I think that's a great way to introduce Haskell! I would recommend something like [Spock](https://www.spock.li) rather than Yesod or Servant. Yesod encourages the use of Template Haskell, which is yet another thing to explain. Servant requires understanding a lot of stuff on the type level, which is a tall order for beginners. (They're both excellent libraries, though.) The rest of your ideas sound good. Keeping things concrete (like `length :: [a] -&gt; Int` rather than `length :: (Foldable f, Integral i) =&gt; f a -&gt; i`) should make things easier to understand and debug. 
Ha :) I thought this looked familiar when I saw the message in the inbox. Indeed /u/Bollu and I had a short exchange about it today on the issue tracker: [simplexhc-cpp/#1](https://github.com/bollu/simplexhc-cpp/issues/1)
It is still in development but you can read rfc 2822 messages with hackage/hweblib including mime entities.
[`GHC.ExecutionStack`](https://hackage.haskell.org/package/base-4.10.0.0/docs/GHC-ExecutionStack.html) says "Since: 4.9.0.0" while [`GHC.Stack`](https://hackage.haskell.org/package/base-4.10.0.0/docs/GHC-Stack.html) says "Since: 4.5.0.0", so maybe `GHC.Stack` _was_ the way to go, but there's now a new, possibly-better approach available?
Thank you, now I can write it as per the below : registerHandler :: T.Register -&gt; Handler T.UserId registerHandler usr = do result &lt;- runExceptT $ register usr case result of Left err -&gt; throwError $ liftErr err Right uId -&gt; return uId but the problem that remains is that the signature of `register` is ` (SomeMonad m) =&gt; T.Register -&gt; ExceptT T.UserError m T.UserId` and I cannot just add cannot add `(T.UserRepo m)` to my handler function
I’m pretty sure they’re just completely different. The older one is going to give you a more syntactic call trace that tells you where the erroring function was called from syntactically, and the newer one will give you a more runtime level evaluation trace showing where the actual thunks are being evaluated, possibly including higher detail than ordinarily necessary with RTS functions. *(This is all conjecture based on what I’m reading here. I haven’t tested anything)*
thank you, this sound a bit abstract, could you elaborate more please!
I am not sure actually, this is highly opinionated topic in Haskell and I found the `ExceptT` a bit more explicit about error for someone who is reading the code
Which OS are you using? [Apparently](https://downloads.haskell.org/~ghc/master/users-guide/debug-info.html#requesting-a-stack-trace-from-haskell-code) libdw comes from the [elfutils](http://sourceware.org/elfutils/) package, which seems really specific to Linux. Since I don't see anything in [GHC's `mk/build.mk` documentation](https://ghc.haskell.org/trac/ghc/wiki/Building/Using#Commonbuild.mkoptions) about either libdw nor elfutils, so my guess is that the build system simply checks whether elfutils is installed on your machine in order to determine whether to enable libdw or not. So, my recommendation is: use Linux, install elfutils, rebuild GHC.
It's a bit of a shame that this module exists, and apparently no one uses it. Perhaps I should investigate who added it to `base`, as I'd assume they would know what's up. 
It's a bit of a shame that this module exists, and apparently no one uses it. Perhaps I should investigate who added it to `base`, as I'd assume they would know what's up. 
Ubuntu. Yeah, I'm thinking I'll have to build GHC myself. That is, this can't be enabled by default as it relies on a third-party library, and I'd imagine that an `elfutils` dependency might be a greater "burden" on the end-user than `libgmp`. 
Thanks so much, useful answer
[Apparently](https://www.reddit.com/r/haskell/comments/5zzlet/how_to_get_a_stack_trace_of_a_live_process/df2kty9/) you don't need to import `GHC.ExecutionStack` in order to use the feature, so the fact that nobody is importing it doesn't necessarily mean that nobody is using it.
http://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/debug-info.html TL;DR Build your code with `-g` and GHC includes dwarf debugging information. `libdw` is used for interpreting dwarf information and backtrace support. You can send `SIGUSR2` to a compiled program to get it to dump stack traces on `stderr`. This module is still fairly experimental and I believe Ben only got DWARF information working reliably in the 8.2.x branch (there's simply a terse "More reliable DWARF debug information" in the release notes). It's new enough that it's not really surprising nobody is using it yet...
https://github.com/GaloisInc/mime/issues/13 and https://github.com/GaloisInc/mime/network might help if you want to take something and polish it up a little further. 
I haven’t run into either of those cases after more than 18 months of running on Keter.
let me thank you again for the above post and every post you are contributing in Haskell reddit community. I have tried somehow to follow the above abstraction. I create a class as the below : class (Monad m) =&gt; UserRepo m where register :: Register -&gt; ExceptT UserError m () so and on top of this function - that do some monadic computation- I create the following function register :: (T.UserRepo m) =&gt; T.Register -&gt; ExceptT T.UserError m T.UserId register = undefined and till now everything is fine and I don't have to specify the monad, until start using `Servant` to make handler for the API, the handler is as the below registerHandler :: Register -&gt; Handler UserId registerHandler usr = do result &lt;- liftIO $ runExceptT $ register usr case result of Left err -&gt; throwError $ liftErr err Right uId -&gt; return uId now I cannot write such function with being explicit in the `register` function that it should run in the IO monad. does this break the deferral or am I missing a point? 
How come you find C++ easier to prototype quickly in then Haskell? I know I have done the exact opposite in the past and prototyped in Haskell before having to integrate the result into a C++ code base, but I can't ever imagine doing it the other way around.
I’m not sure what the OP was getting at. You’re right; the point I meant to make was that the information capacity of a certain number of bits of memory is exactly the maximum entropy of discrete distributions over a set of that size. What the OP wrote in his fourth and fifth point is correct, by the way, if you set aside his interpretation: observing a low-probability event provides more information than observing a high-probability one. No, that doesn’t mean Peano numbers are a more efficient encoding of a subset of natural numbers than machine integers under an arbitrary distribution, but it is the case that the geometric distribution has maximum entropy among all distributions over the full set of natural numbers with finite mean, so the encoding he gave is indeed efficient for the full set of natural numbers under the distribution he assumed. This is quite intuitive: a typical sequence of natural numbers drawn from a geometric distribution with p = 1/2 looks something like this: 1, 0, 0, 2, 1 , 0, 0, 7, 0, 1 ... And you can encode that unambiguously in the OP’s scheme like so: 0100011010001111111001 ... It’s easy to see that this encoding requires the fewest bits of memory on average to record a sequence of natural numbers drawn from this geometric distribution: half the time you only need one bit, a quarter of the time you need two, and so on. But the set of all natural numbers and a subset of 2^32 of them are not the same thing, and efficient encodings depend on distribution, (with the information capacity of a number of bits corresponding to the maximum entropy distribution over that number of items), so I’m not sure what else the OP was driving at.
I like this! I should clarify that I'm not saying that I like it better than the alternatives. Rather, I think it's good to know about many approaches with various tradeoffs, so you can pick the right one for each circumstance, and I think this one exhibits a different set of tradeoffs than the existing approaches, so I think it complements them well. Your approach, "explicit-passing-style", is similar to the "[scrap your typeclasses](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html)" approach in which the methods are also passed explicitly, but instead of passing each of them individually, related methods are grouped into records which mimic the implicit dictionaries to which typeclasses desugar. To distinguish those two explicit approaches, I'll call your approach "method-passing-style", and I'll call the "scrap your typeclasses" approach "dictionary-passing-style". Here are the approaches I know about, their main advantages, and a circumstance in which that advantage really shines. I won't list the disadvantages, since it's mostly just "doesn't have that other approach's advantage". * *transformers* (i.e. an explicit monad stack, with explicit calls to `lift` and [`hoist`](http://www.haskellforall.com/2013/03/mmorph-100-monad-morphisms.html) as needed) This is the only approach in which the interaction between the different effects is chosen by the callee, not the caller. This is important if e.g. the callee's algorithm depends on the state being reset when backtracking, when parsing for example. In that case, `StateT s (Either e) a` would work but `EitherT e (State s) a` wouldn't, so it's better not to let the caller make that mistake. Make illegal monad stacks unrepresentable! * *method-passing-style* By focusing on individual methods instead of a hierarchy of typeclasses, this is the only approach which doesn't require a careful design establishing which methods go together, which laws they must satisfy, and what are the superclass relationships if any. Laws etc. are quite useful when implementing and reasoning about the meaning of a generic and abstract combinator like `(&lt;*)` or `replicateM`. But when working on a very concrete function like `sendBirthdayCards`, it's nice to be able to abstract over the implementation of `getUserList` and `sendEmail` without having to think too hard about their formal semantics. * *dictionary-passing-style* The two explicit styles are the only ones which don't enforce typeclass coherence. This is useful when you use so many different semantics that defining a newtype for each of them would be impractical, e.g. if you're writing a combinator library where every combination gives a slightly different implementation of the same typeclass. * *mtl* This is the only approach in which the compiler can automatically compute the union of the effects used by different parts of a computation. This is useful when your goal isn't to make the code polymorphic in the monad stack it is instantiated with, but simply to track which effects are used so you can debug more easily. For example, if you suspect that part of a computation is incorrectly changing the state, you don't have to look at the subcomputations which don't have a `MonadState s` constraint. * *free*/*freer* These are the only approaches in which the computation is a concrete piece of data, not a polymorphic computation. Now, we can of course instantiate that polymorphic computation to a concrete datatype and vice versa, so the question isn't "what can we do with this representation which we can't do with mtl", but rather "which things are more convenient with this representation". My answer is that when transforming one program into another, I prefer to traverse a concrete representation of the program and to return another concrete value. I know it's possible to use the finally-tagless approach to turn one polymorphic computation into another, but that makes the code more opaque for me.
From the section *Polymorphism &amp; multiplicities* &gt; Consider the definition: “`id x = x`”. Our typing rules would validate both `id :: Int -&gt; Int` and `id :: Int -. Int`. So, since we think of multiplicities ranging over `{1, ω}`, surely we should also have `id :: forall p. Int -&gt;p Int`? &gt; &gt; But as it stands, our rules do not accept it. To do so we would need ` x :p Int |- x : Int`. Looking at the (var) rule in Fig. 6, we can prove that premise by case analysis, trying `p = 1` and `p = ω`. But if we had a domain of multiplicities which includes `0` (see Sec. 7.2), we would not be able to prove `x :p Int |- x : Int`, and rightly so because it is not the case that `id :: Int -&gt;0 Int`. Could we bound the multiplicity of `id` without affecting future extensions id :: p `CanBe` [One, Omega] =&gt; a -&gt;p a and would the type of `joinIOL` be (fixed signature of `bindIOL`) type IOL p a returnIOL :: a -&gt;p IOL p a bindIOL :: IOL p a -. (a -&gt;p IOL q b) -. IOL q b joinIOL :: p `CanBe` [One, Omega] =&gt; IOL p (IOL q a) -. IOL q a joinIOL ioio = bindIOL ioio id If I understand correctly, otherwise we have to pick between `id :: IOL q a -&gt; IOL q a` and `id :: IOL q a -. IOL q a` flip bindIOL id :: IOL One (IOL q a) -. IOL q a flip bindIOL id :: IOL Omega (IOL q a) -. IOL q a ---- I am also interesting where this fits in our hierarchies, take the [`Category`](https://hackage.haskell.org/package/base-4.10.0.0/docs/Control-Category.html) class class Category cat where id :: cat a a (.) :: cat b c -&gt; cat a b -&gt; cat a c One way might be to treat `a -&gt;p b` as a type whose first argument is `p` type Arrow p a b = a -&gt;p b instance p `CanBe` [One, Omega] =&gt; Category (Arrow p) where id :: a -&gt;p a id x = x (.) :: (b -&gt;p c) -&gt; (a -&gt;p b) -&gt; (a -&gt;p c) (f · g) a = ... but we run into problems with `(.)` since the more general signature (that I vaguely comprehend) (.) :: (b -&gt;p c) -. (a -&gt;q b) -&gt;p (a -&gt;p·q c) (f · g) a = f (g a) is substantially different from `(.) @(Arrow p)` (.) @(Arrow p) :: (b -&gt;p c) -&gt; (a -&gt;p b) -&gt; (a -&gt;p c) My first thought is some kind of monoidal index type Cat ob = ob -&gt; ob -&gt; Type type IxCat ix ob = ix -&gt; Cat ob class MonCategory (icat :: IxCat ix ob) where type Unit cat :: ix type Mult cat :: ix -&gt; ix -&gt; ix id :: icat (Unit icat) a a (.) :: icat p b c -&gt; icat q a b -&gt; icat (Mult icat p q) a c or maybe a type family for the (`CanBe`) constraint class MonCategory (icat :: IxCat ix ob) where type MConstraint cat :: ix -&gt; Constraint type Mult cat :: ix -&gt; ix -&gt; ix id :: MConstraint icat p =&gt; icat p a a (.) :: icat p b c -&gt; icat q a b -&gt; icat (Mult icat p q) a c This gets us close class p `CanBe` [One, Omega] =&gt; OneOrOmega instance p `CanBe` [One, Omega] =&gt; OneOrOmega instance MonCategory (Arrow :: IxCat Multiplicity Type) where type MConstraint Arrow = (OneOrOmega :: Multiplicity -&gt; Constraint) type Mult Arrow = ((·) :: Multiplicity -&gt; Multiplicity -&gt; Multiplicity) id :: OneOrOmega p =&gt; a -&gt;p a id ... (.) :: (b -&gt;p c) -&gt; (a -&gt;q b) -&gt; (a -&gt;(p·q) c) (.) = ... maybe someone finds this useful
&gt; You are mixing the two meanings of "bits" here. Yeah, this one I slipped. I should have written "binary encoding" instead of "32 bits integer".
`Foldable` does not define `filter`. You need at least `Witherable` or `MonadPlus` for that.
oops, for some reason I thought it came from `Foldable`. Thanks for the correction!
&gt; But assuming a geometric distribution for nats and then saying that 32 bit binary integers are less efficient than unary nats is bogus. Sure. But I took as a starting point natural numbers as "number of occurence of an element in a multiset". I guess the question I wanted to ask was : is e the type of natural number ? And I merely constructed a type of natural number, caracterized by a peculiar probability distribution, that makes sense to me at being e.
Fair enough, I guess this whole thread of discussion is a little beside the point of this post.
This is on Hackage: https://hackage.haskell.org/package/emailparse
The concept of your project seems very interesting to me but I have to admit that it’s far less so given your choice to switch to C++. 
CS240H has some good info on Haskell's RTS as well: http://www.scs.stanford.edu/16wi-cs240h/slides/ 
You can try [emailparse](https://hackage.haskell.org/package/emailparse). I'm not familiar with it, but so far I haven't found anything else on Hackage.
There's a [library for polymorphic variants in PureScript](https://github.com/natefaubion/purescript-variant), which is used to this effect in [`purescript-run`](https://github.com/natefaubion/purescript-run).
&gt; If I have a record `t :: T` and a function from `T -&gt; T` that produces `t'` will identical records fields of t and t' be shared in memory? Yes. The keyword to search for if you want to learn more about this is "[persistent data structures](https://en.wikipedia.org/wiki/Persistent_data_structure)". &gt; Also if t' == t it produce a copy of t of will the compiler apply a "Copy on Write" mechanism to reduce allocations? Remember that records are immutable in Haskell, so there is no such thing as a "write". Perhaps the following examples will help. data Pos = Pos { x :: Int, y :: Int } deriving (Show, Eq) returnsSame :: Pos -&gt; Pos returnsSame pos = pos returnsEqual :: Pos -&gt; Pos returnsEqual (Pos x y) = Pos x y Both of those functions return an output which is equal to the input according to `(==)`, but `returnsSame` doesn't allocate any new memory while `returnsEqual` does. `returnsSame` receives a pointer and returns that same pointer. `returnsEqual` returns a pointer to a newly-allocated `Pos` constructor, whose `x` and `y` fields are the same pointers as the input `Pos`, so no new allocation is needed for those. That's the behaviour you get in interpreted mode, at least. There are many optimizations which the compiler might do, some of which are automatic, while others require manual annotations. As a result of those optimizations, `returnsEquals` might be rewritten to be the same as `returnsSame`, or the representation of `Pos` might be changed to store the Ints directly instead of pointers to them, etc.
&gt; Isn’t it a myth that PureScript is front end only? PureScript compiles to JavaScript, which is the "native" front-end platform. While it's possible to run PureScript generated JavaScript outside a web browser using Node, it probably makes sense from a performance perspective to limit the number of virtualisation layers. In that regard, PureScript is most useful on the front-end.
Elm is more popular than Purescript because of the go mentality. The idea that if a language is “simpler” and “slimmer” it’s better is an incredibly toxic idea IMO. Elm is truly a simple language. There are very few language features, and if you’re a JS dev coming from react there isn’t a huge jump in complexity. This simplicity on-boards new devs fast. The problem is you cannot express almost any typed functional abstractions in Elm. This cripples the language, and Purescript has all of Elms features and many more. I see absolutely no reason to use Elm, except to gain tech debt. 
Servant provides an [enter](http://hackage.haskell.org/package/servant-server-0.11.0.1/docs/Servant-Server.html#g:5) function that lets you define your server in a monad different from `Handler` and then supply a function that transforms you monad stack into `Handler` (often by providing some environment to a Reader monad). See [these](https://stackoverflow.com/questions/41783071/how-do-i-resolve-a-type-error-using-enter-from-the-servant-library) [two](https://stackoverflow.com/questions/42316500/servant-a-functional-dependency-error-with-enter) SO questions. See also [here](http://hackage.haskell.org/package/servant-server-0.11.0.1/docs/Servant-Server.html#g:7) for some common helpers. You might want to take a look at the *mmorph* package for more. Perhaps you could also avoid `enter` and transform your handlers before constructing the server.
Not to mention that the creators have dictatorial control over the ecosystem.
If your goal is to learn Haskell, then just learn Haskell. If you want a practical language for client-side web programming (that is also a full-fledged backend language thanks to node.js), that is similar to Haskell, then PureScript If you want a pure functional language for client-side web dev that is easy to learn, go with Elm.
&gt; If you structure things right then the stuff visible to the GC is a negligible component of your total pool of assets, making everything fast to GC. I think Haskell needs a better GC story to work in this environment. In a multi-core setup you want to either have a (soft) realtime GC, or have the tools to handle a partitioned heap (a process architecture - like Erlang).
What's the actual connection to Haskell here? I'm not seeing any aside from the throwaway reference in the title.
According to "[Let's be mainstream! User focused design in Elm](https://www.youtube.com/watch?v=oYk8CKH7OhE)", Elm targets JavaScript programmers, optimizes its landing page's feature list to make its advantages clear to JavaScript programmers, and optimizes the wording of its documentation to eliminate scary words like "Monad". Simplicity and learnability are high-priority goals for Elm. &gt; Why is Elm more popular than PureScript? In my opinion, popularity is obtained by making the advantages as clear as possible and making the switching costs as low as possible. Elm is doing a great job on both, as outlined in the above video. Haskell is doing a terrible job at both. Learning Haskell is much harder than learning yet another imperative language, and while I think that the clarity, correctness, and debugability which results is worthwhile, it's hard to succinctly explain how giving up on side effects magically leads to those benefits. I am not super familiar with PureScript, but my impression is that it is trying to be a better Haskell than Haskell. It has an even more finely-detailed numeric hierarchy, for example, and its lenses are based on Profunctors instead of functions. Both of those sound like improvements to me, but to someone whose numeric hierarchy only has `float`, `int` and `BigInt`, that's probably just going to be an even steeper learning curve. &gt; preparing yourself with Elm first [...] I don't think either Elm nor PureScript is designed to be a stepping stone towards Haskell, and I don't think most people who learn either language are using them as such. My impression is that Elm programmers come from JavaScript, while PureScript programmers come from Haskell. &gt; [...] would be a waste of time On the contrary, I do recommend learning Elm before Haskell. Like I said, Elm is designed to be easy to learn, so the learning curve will be a lot softer than Haskell's, and its teachings about purity, immutability and types carry over to Haskell. So even though Elm isn't designed to be a stepping stone towards Haskell, I think using it as such is a great idea.
Thanks for the insight! More in-depth instructions may be found here, including (roughly) what you need to do to enable this feature in GHC: [Status of DWARF debugging support](https://ghc.haskell.org/trac/ghc/wiki/DWARF/Status). Unfortunately [**this does not produce stack traces on exceptions**](https://ghc.haskell.org/trac/ghc/wiki/DWARF/Status#DWARFandExceptions), but being able to dump execution stacks whenever I feel like it would still be quite handy.
We only have three referentially transparent languages in general use today, flaming between them is silly. Elm has referential transparency, parametric polymorphism, and algebraic data types. This makes it better than 90%+ of the languages out there. I also with everyone would go straight to PureScript, but they're not going to and saying there's no reason for Elm to exist just causes unnecessary animosity. Or see Gabriel Gonzalez‏'s take: https://twitter.com/GabrielG439/status/901448855115382785
I think a lot of devs aren't necessarily looking for expressive power in a language. They're looking for a way to stop making nightmare code bases. I think there's an impression that Elm has just enough complexity to get the job done on the front end, while still providing many of the benefits of strict typing, even if its not as elegant as its possible to be in a more full featured language. Also, doesn't elm have better performance than purescript? 
&gt; I am not super familiar with PureScript, but my impression is that it is trying to be a better Haskell than Haskell. That's definitely not an explicit goal, but we did tidy things up along the way that we thought were worthwhile - Semigroup/Monoid, Applicative/Monad, Foldable/Traversable, Num vs Field, and so on. I started PureScript because the language I wanted on the front end didn't exist, and that's still why I work on it and use it. I want type classes, and functional dependencies, and higher kinded types, and rank N types, and various other things in my type system. And I want predictable code generation, and a minimal language and standard library. That combination just didn't exist when I started on PureScript.
I think Elm focusses on learning and the ease of getting started. It's not to say PureScript doesn't focus on those things, but it's not our number one concern. We've been focusing on building the language instead. If you're learning FP for the first time, I think it makes sense to start with Elm. Eventually, you might find you want more type system features, so you might move to PureScript - or maybe not, and that's fine too.
There's actually quite a few binary packages from hackage in Debian. I count nearly 900 visible on my Debian install. (It's actually 1793, but I have 2 architectures available.) I don't know if it's the current preferred tool, but cabal-debian exists and can quickly turn a pure haskell package into a Debian package. For things with C (or other language) dependencies, I assume it's a little harder, but there are plenty of examples among the 900 existing packages. I think Haskell has done good here. Joachim Breitner is the main name I think of when it comes to Haskell Debian packaging, but I hear he's quite a quite good team working with him. My thanks go out to all of them. (In particular, I recently installed a VM to do some Haskell development against a glibc-2.12 system; using the old stretch installer. After the base system was down, I told aptitude to install `!~v~n^libghc-.*-prof$`, and everything worked flawlessly! Without the attention of the Debian Haskell maintainers, I could have struggled with compiling and dependency resolution for weeks in order to get that many Haskell packages available. If any of you have Patreons or BTC tip jars, PM me.) 
That's not the best argument. Scala is despised because of its overly complex design, namely through things like implicits, overused operator overloading, and idioms that are in twain due to the massive divide between the OOP Scala community and FP Scala community. There is certainly something to be said for not bloating a language.
Sure I agree with that, but there is a difference between intentionally sandbagging a language and having needed features. Elm intentionally sandbags the language. I can’t think of any Purescript features that are bloat, it’s a very minimal language as it is. 
Sure I agree with that, but there is a difference between intentionally sandbagging a language and having needed features. Elm intentionally sandbags the language. I can’t think of any Purescript features that are bloat, it’s a very minimal language as it is. 
Who said Elm is more popular?
That means that it has clear focus and goals. It's not that a whole mixed bag of features make it into the language because it just about fits into the compiler. Elm doesn't have certain things and does some things oddly, but you can talk to the one person who made that decision and know that it had reason behind it, regardless of whether you agree. Having a language dictator is sometimes exactly what you want. 
If you are using something like mailgun let me know - I have a private mailgun library somewhere that I can opensource :)
I think Elm and PureScript both benefit from the "culture" generated by Haskell. We know what data structures work, that do notation works, that type classes work and which ones are definitely useful (functors, Monoid, etc.), parsers, ST, etc. That reduces the cost of making a useful pure functional language. You just have to do the implementation, make your decisions that distinguish this language from others and re-use the library tropes we've accumulated over 30 years. That also makes it really easy for us to switch between these languages. I barely had to learn anything for either PureScript or Elm to be productive. That's a real opportunity for trying out alternatives.
Probably https://trends.google.com/trends/explore?q=%2Fm%2F0ncc1sv,Purescript
The Elm people aren't really sold on type classes though :)
As a kind of typical case of JavaScript/Ruby/Java dev, I tried out Elm first because it was marketed quite loudly and it was the only option I knew of at the time (2015) other than F#, which I wasn't about to use for various reasons. I only found PureScript later when I was disappointed with Elm and I saw some stuff from Bodil on Twitter about PureScript. That's about it. Nobody in my circles told me there was even an option. Even today, if you talk to a JavaScript user who's "in the know" about things, they really only know about Elm and ReasonML thanks to the marketing efforts. PureScript, Fable, and GHCJS don't exist in their Twitter feeds.
Elm adopts far more of a "worse is better" mentality than Haskell or PureScript. In my mind, this is not to Elm's credit!
If you want to build games in that second category where you are going nuts with the CPU as well as the GPU? Sure. It'd be nice to have an Azul Systems style fully concurrent "pauseless" mark-compact collector. It isn't a trivial exercise, though, as much of what we have on the heap needs to get forwarded to its final destination. The Erlang style approach still leaves you with lots of little pauses, spread across all of those processes, but then robs you of par and easy access to the contents of other threads. It largely destroys Haskell's existing concurrency story. I also don't see a lot of games being written in Erlang. ;)
Elm is easy to move into. It doesn't ask you to learn anything but syntax. You just don't realize that the abstraction ceiling is so low until a month later when you start hitting your head.
Elm is simpler and easier. If one wants to build a todo application, One even without any functional programming language can create the app in an hour with elm. With Purescript, well there is no way before knowing how to handle side effects. It turns out people love simpler language especially coming from js background. As a haskeller, you will hate both or at least not like it as much as Haskell because after all it is not haskell. Both languages make you feel like it is a web dev version of Haskell, but it is going to fail you at some point with some features missing.
&gt;Scala is despised because of its overly complex design I don't think so. I think it is despised because of its inconsistent and contradictory design.
&gt; I think Elm and PureScript both benefit from the "culture" generated by Haskell. In many-perhaps-most ways, certainly. But for some such as me they also mildly benefit "culturally" from actually targeting JS and thus being bound tightly at the hip to "the real messy world I'm coming from and that will always feel like home". =) Just this simple reality of what they target makes them intrinsically, vastly more approachable, grokkable, down-to-earthy. Not just Elm, PureScript too (which I chose over Elm for its many extra powers and its FFI story). No matter how "hi-faluting" they're gonna get, it always boils down to JS I can inspect and grasp. Especially PureScript is a joy in this respect for me precisely *because* it mirrors all "hi-falutin" bingo terms for "plain-old lame-old same-old" programming concepts (heyting algebra, semigroupoid, euclidean ring etc it goes on and on --- "*anything* to not call a number a number and a bool a bool", it seems to the perplexed Joe Cowboy Coder =), all the while the sorely needed demystifying naked banality of the underlying essence is just a quick glance into the corresponding JS file away. Very nice to work on such a basis, no comparison to reading PDF conference papers or the GHC code-gen code base, or rabbit-holing down countless type-theory/category threads. I wouldn't have expected this to have such a stark effect on me, but it's palpable. I'd rather write a PureScript-(Core-)to-X transpiler than use Haskell for my backend / non-scripting programming, isn't that funny? Well so be it, it's *exactly* what I'm tackling right now! Inspiring projects, for sure. As I've traversed the PureScript code-base while adding the `--dump-coreimp` feature, I've become ever more impressed with the scope of it and become most fond of the project. Anyway, the moral of the story is, "culture" is moderated by the realities of emitting JS, and both are superior "beginner" languages for the very same reason, imho =)
I think there is a difference between the essential and inessential complexity. Elm simply fails to offer you enough tools for abstraction. You quickly get stuck when trying to encode the essential complexity of your problem domain if you ever try to do anything other than first-order content. It is lacking in essential complexity. Scala on the other hand gets derided because it is lacking on both fronts. On the essential complexity front. It tries very very hard to remain a super-thin layer over the JVM. This enables you to write a slightly nicer dialect of Java in it, much like Elm lets you write a slightly nicer dialect of Javascript in it. However, is quite painful to write functional code in Scala and the result comes at significant costs in terms of both performance and the ability to actually understand any sort of stack traces. On the inessential complexity front, it piles a whole ton of research questions on tops and assumes the answer to each of those questions is yes in conjunction. So there is a lot to learn, and when you're done, none of it really works all that well together.
I think it's more like "type classes are quite complex and don't give enough benefit to the narrow scope of Elm". That said, as someone working in both purs and elm atm, I miss them so much :( Boilerplate: ugh Also I much prefer purescripts FFI since you can pass in functions to JS (like just, left, right), whereas elm requires outside libs like sanctuary to start working monadically in the native js (ie you need more js with Elm, and more elm code too!)
The ecosystem of elm seems better to me - better docs and more libs. Pursuit and the lack of docs or answers to intro questions are a big problem for purs for me - enough to consider ghcjs and other Haskell alternatives. I had to write my own `mapKeys` function, for example.
&gt; *The problem is you cannot express almost any typed functional abstractions in Elm.* Not sure about that. IIRC people expressed "type classes" and "records" in terms of lambda calculus way before there was explicit syntax sugar for it. Surely you can express anything in Elm that you can in Lambda calculus? You have abstraction, application and lets. =)
I'm still working on my PureScript-to-Go transpiler though. Don't laugh, Go is a pretty low-level language --- hence a most suitable transpilation target =)
I see where you're coming from, but I don't think simple languages alone cause tech debt. In fact more complex languages generate it more quickly by misuse and neglect of "features".
&gt; *As a haskeller, you will hate both* Seriously, as a Haskeller, why would you "hate" PureScript, or even "not like it as much"? Fast builds, cleaned-up lib ecosystem and Prelude, all the fancy type-level stuff (rank-n, rows, func deps whatever that is (gotta catch up)), need I go on? 
Right, *thanks to* there being a well established culture, it's possible to implement these PureScripts and Elms that target specific domains and platforms in the first place. So everybody benefits. Muggles get a "way in" with a dialect that suits them better. Existing pure functional programmers get linguistic mobility that imperative programmers have always enjoyed. For a while there we had a bunch of Haskell compilers (YHC, UHC, NHC, Hugs and others I don't remember), but I think basically they weren't different enough to GHC, or eventually were obsoleted by GHC by absorbing features, so that there was never a reason to really stop using GHC for major projects, and so there was no competition. It didn't help that GHC added hot new features all the time. That's why there really are no practical alternatives to GHC. Yet many people wish we did have some competition, to keep GHC on its toes, try out new language features and cut away old cruft. And GHC has a lot of that. It's great to have one main compiler, but it sucks to be stuck with something that accretes but never takes away. However, Elm and PureScript have perhaps unwittingly *become* the new Haskell compilers, and redefined what it means to be "Haskell". The scope has broadened. Perhaps standard Haskell remains the same lazy, row type-less, rank N typeless language defined in the Haskell 2010 report. But I don't think people pay attention to that; people using GHC just care about GHC Haskell, and people using Elm and PureScript care about their own dialects. We now have a "Lisp" situation, where Lisp means something broad but definitely distinct, and then Common Lisp and Clojure and Scheme are all dialects. Maybe I'll write a blog post about this...
&gt; I thought that PS offered most of the expressive power that Haskell has, without the language extensions. It’s strict (which is big) and I don’t really think the language extensions are that big of a problem. &gt; Isn’t it a myth that PureScript is front end only? If your ultimate goal is to learn Haskell that you would use seriously As I said, laziness is big enough that if your goal is to learn Haskell then I’d suspect you should start with Haskell. Also Haskell is reasonably fast and PureScript is not as I understand it. &gt; I can picture some people 'settling' for Elm if they think PureScript or HS are too hard to start with. I think Elm was clever about the rhetoric they adopted, but I also don’t think it’s all true. Not that it hasn’t led to concrete differences in who makes up the community, though - watch some Elm talks and you’ll see differences. 
All very true! &gt; We now have a "Lisp" situation, where Lisp means something broad but definitely distinct, and then Common Lisp and Clojure and Scheme are all dialects. It's beginning to look like it --- could be a good thing to have numerous flavours blossoming and feeding into each other. Individuals vary as to what sort of "assemblage" succeeds in *first infecting them* with the programming bug, this should apply to FP just as much as to imperative/procedural/objective. With time, they hop from one to another much more easily.
How do you higher kinded types in Elm?
Because they haven't used them as extensively in Haskell.
Because it is not Haskell. Syntax is uglier. Strictly evaluated. Eff instead of IO. Record in Purescript has a different meaning than Haskell records. 
&gt; Eventually, you might find you want more type system features, so you might move to PureScript - or maybe not, and that's fine too. I think that there are some really interesting patterns that Haskell has used with regards to the type system, such as recursion schemes. I have no idea how applicable they are to the frontend but I think they might suffer in the long-term.
I'd go a little further and say that while PureScript is closer to JavaScript in some dimensions, there are others in which it's even further away. I just don't see a viable path to learning PureScript at all that doesn't start with "first, learn Haskell." There are not enough resources to walk people through the baby steps in PureScript itself; and as far as I can tell, the community is not at all interested in compromising their abstraction towers with any kind of simplified ideas at all!
&gt; optimizes the wording of its documentation to eliminate scary words like "Monad". I don't think that's an optimization, just exploiting rhetoric. &gt;In my opinion, popularity is obtained by making the advantages as clear as possible and making the switching costs as low as possible. I would agree, but I think "making the advantages as clear as possible" is not as noble as it sounds. &gt;On the contrary, I do recommend learning Elm before Haskell. It will skew your perspective immensely if you don't want to do frontend work. And fwiw I've used some paradigms from Haskell that would be impossible in Elm when doing frontend programming. &gt;Elm is designed to be easy to learn Most of why it's easier to learn has to do with the community or the fact that the documentation doesn't require you to learn 7000 new concepts to e.g. write a blog site.
&gt; and as far as I can tell, the community is not at all interested in compromising their abstraction towers with any kind of simplified ideas at all! What alternatives have been proprosed and where has this been discussed?
Is there _any_ way to construct two instances that differ only at the constraints (i.e. same instance head). I'm even willing to turn on `IncoherentInstances` if that's what it takes, but alas, no dice. For concreteness' sake: let's say I have a type class `Foo a b`; in the generic case, there are two, hopefully equivalent, ways to build an instance. If `Bar a` or if `Baz b`. Unfortunately, this won't work: class Foo a b instance Bar a =&gt; Foo a b instance Baz b =&gt; Foo a b Is there any way around this? Maybe by collapsing the two constraints into a single disjunction constraint (however you might do that)?
Might be a bit off topic, but sometime ago I had a spiritual quest to write the simplest possible implementation of self-balancing binary trees in Haskell, and ended up with [this code](https://repl.it/Kh81/31) for AVL trees. It's surprisingly short, and delete isn't much more complicated than insert.
I’ve used both and had an easier time with Elm than with PureScript, although I don’t think I necessarily “enjoyed” either one more than I would have just enjoyed pure JavaScript. For one, Elm has some really nice error messages. Second, there’s the whole “publish action -&gt; update store -&gt; notify subscribers” framework that tends to be a bit easier in terms of barriers to entry. It also feels like PureScript is unmaintained. I tried PS out twice over a year or two and the docs were more or less the same (very sparse). The package manager as of like 6 months ago still used Bower, which has been phased out pretty much everywhere (even Bower devs say to stop using Bower). A big issue for me, too, was that I felt like I was looking at Haskell docs to figure out how to do things more than I was using PS documentation. At that point, why not just use a Haskell compiler that targets JavaScript instead? And, if I end up using the FFI to include the majority of my JS code, can’t I just write functional JavaScript code in the first place and cut out the middle man? Even without the runtime requirement that Elm has, PureScript introduces a lot of framework and dependency cruft, which is in some ways worse (manually currying and wrapping functions to defer evaluation — the generated code is readable but uglier than, say, using Ramda). 
There is always the third alternative: http://haskell-miso.org/
Indeed. Typeclasses are just one way of looking into the more fundamental issue: How should we handle implicit parameters? Elm takes the most conservative approach: everything is explicit. This isn't ideal, since it adds a lot of noise to the language. But can be extended later without restriction. Agda didn't have typeclasses either and no one complains about it. But there language has other mechanisms to make parameters implicit. 
&gt; The package manager as of like 6 months ago still used Bower It turns out that `pulp` (the PS build tool of choice) will now accept the `psc-package` package manager as well.
I think that an ideal progression from Elm to Purescript would be: 1. Introduction to plain Elm. (I think various people in this thread *really* underestimate how different Elm is from Javascript.) 2. Introduction to problems that have enough size and complexity that people begin to itch for the features that Elm doesn't have (polymorphism, especially). 3. An introduction to Purescript tailored to the graduating Elm programmer. Something focused on how Purescript adds to Elm. [This is missing.] 4. A documentation aesthetic that caters (not exclusively) to the person who wants to learn the concrete before the abstract. I seem to remember that the early Java documentation just assumed that anyone reading the API for a class would naturally know everything about all the superclasses. That turns out not to be a great idea, but a lot of static FP documentation does the equivalent. (A pet peeve of mine: to really understand the Haskell lens library's `At` -- something I suspect people will want to use soon after being exposed to lenses -- you must first understand `Index` and even `Traverse`. I don't understand the appeal of such an approach.)
Most browsers don't support anything beyond 2083 character URLs, IIRC.
Please dont water down your opinions due to imaginary offense. If it were up to me, I'd force every library author to provide a short survey of state-of-the-art and really explain why his/her library should exist in the first place. The problem of too many half-baked libraries is a real thing in the Haskell community. Either existing libraries should be refreshed with new ideas and fixes (with releasing competing libraries), or they should be clearly marked as deprecated once better libraries have been built. 
It looks great. But yeah, the delete operation of a red black tree is a famously difficult problem. There was a paper[1] titled "Deletion: The curse of the red black tree" :) [1]http://matt.might.net/papers/germane2014deletion.pdf
Brian Marick is writing a book that goes from Elm -&gt; Purescript -&gt; (tentatively) Idris: https://leanpub.com/outsidefp
I see you are writing a book along those lines, so I may as well put in the link: https://leanpub.com/outsidefp
Elm is not a scary functional language that takes years to master and require reading bunch of PhD papers. It is a easy to learn web front-end development framework that can be picked up by any javascript junkie in a week.
That's correct.
The decision space has been discussed to death in the context of Haskell. PureScript has made a clear choice at one extreme of the spectrum. That's certainly cleaner than compromise, but it does have its consequences.
* /r/purescript = 1300 subscribers * /r/elm = 5500 subscribers
Here's a sneak peek of /r/purescript using the [top posts](https://np.reddit.com/r/purescript/top/?sort=top&amp;t=year) of the year! \#1: [purescript-halogen v1.0.0 released](https://github.com/slamdata/purescript-halogen/releases/tag/v1.0.0) | [0 comments](https://np.reddit.com/r/purescript/comments/5wqrtk/purescripthalogen_v100_released/) \#2: [PureScript Compiler v0.10.3](https://github.com/purescript/purescript/releases/tag/v0.10.3) | [3 comments](https://np.reddit.com/r/purescript/comments/5ho93f/purescript_compiler_v0103/) \#3: [Purify](https://np.reddit.com/r/purescript/comments/62kenb/purify/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
No such luck; despite the order in which you read the instance declaration and the direction of the `=&gt;`, what really happens is that your instance declaration teaches GHC that it can satisfy a `Foo a b` constraint with a `Bar a`. You could imagine the compiler trying to be clever by trying `Bar a` and, if that fails, backtracking and trying `Baz b`. But there are problems: * You could imagine this search becoming very expensive. * Say we know `Baz b`, so the second instance is chosen. Since we have an open world, somebody in some later module could add an instance `Bar a`; now what? should we go back and select the `instance Bar a =&gt; Foo a b` instead? * How do we know that the instance we get via `Bar a` is equivalent to the one we get via `Baz b`? So it could be done, but the points above suggest it would be *costly*, *unpredictable*, and *buggy*. Out of curiosity, what is your intended use case? This has a whiff of "XY problem" around it.
Deletion from 2-3 finger trees is probably even more annoying, because you have to work inside out while the type checker is trying to beat you up. See [Data.Sequence.deleteAt](https://hackage.haskell.org/package/containers-0.5.10.2/docs/src/Data.Sequence.Internal.html#deleteAt)
I felt the same way. Learning PureScript wasn't difficult for me but I already knew Haskell. I can't imagine what it's like for someone who doesn't know Haskell to learn PureScript, because there are very few tutorials designed for that audience. Most of the docs on PureScript are very much "reference" style so you have to already know what you're looking for to even find them.
I think elm is more popular than pursecript for two reasons. First of all I think purescript and typescript were popular around the same time and I think that they were seen as competing. Most of the jobs have gone over to typescript and so that has lost purescript some users. The second reason why elm might be winning out over purescript is that elm is a lisp and so it's easier to pick up if you already know another version of lisp.
elm is a lisp?
My apologies I got elm confused with arc. So scratch the second thing I said only the first thing applies.
Thank for all your great answers on reddit. I have tried the above building block but I get somewhere where I cannot defer the implementation anymore (maybe am doing something wrong). however I have asked a [question](https://www.reddit.com/r/haskell/comments/79hrce/declare_abstract_monadic_function_with/) on reddit to try to overcome the problem. but is it really that complicated?
Humans don't like feeling stupid, so we tend naturally toward systems that have shallow learning curves. Dynamic types are _easy_. You only need to convince the compiler your syntax is reasonable. Static types are not - you get error messages that you need to learn to read and understand. The more complex the type system, the more complex the errors. Rich should understand this trap, given his talk on simple made easy. Is it simpler to omit types? I don't think so, I think that just pushes the cost of understanding the types out to the reader, and the cost of keeping the types matched up out to the maintainer, I don't think those costs go away.
I don't think so, this url in this pastebin is on the order of 30kB and works for me in Firefox. I haven't tested it in Chrome but I was using a 28kB earlier version in Chrome. https://pastebin.com/uuFhQLT7
Edited videos are on the way.
"Rigid/skolem type variable bound by x has escaped" In all seriousness, I think it's because Elm has focused on bringing web devs to FP, not bringing FP devs to the web. Both have different niches and priorities. Elm puts a ton of effort into error messages and keeping the language minimal. As a result, it's not as generic and requires more boilerplate, but it's an easier sell to non experts.
I learned PureScript before Haskell (and my Haskell usage is limited to some scotty/haxl/generics-sop usage), and it really came down to using libraries I knew I wanted to use (Aff, Halogen, etc.), reading docs, and asking questions when I was stuck, and slowly learning the basics *after* I had already been writing programs I wanted to write. There are also many more concretely-typed aliases all over the place to help you use the libraries, so I don't anyone is uninterested in compromising abstraction for easier use.
A god among Haskell applications and, frankly, an "essential" application regardless. It's just so bloody useful. Massive congratulations to everyone who has contributed to the new release. 
I'm not laughing, just twitching at the use of "transpiler". Just call it a "purescript-to-go" compiler &gt;.&lt;
One thing that stops me from learning PureScript is performance. Elm probably has the fastest Virtual Dom implementation out there and I've seen some Halogen/Thermite benchmarks showing that the performance is much worse than even in JS frameworks. Not sure how accurate they are though.
Elm is --in most respects-- a DSL. PS is a much bigger language, and has good potential to be compiled to native (already an experimental compiler exists iirc). If you want to create browser apps, and you have to teach the language+stack to some devs, then Elm is a very good choice. It's quite simple (PS has a lot more features/syntax to learn). It has batteries included (comes with a DOM lib; in PS you can pick your own from some options). It prescribes you the architecture (TEA which is a bit like React+Redux; in PS there are several frameworks/libs to choose from). &gt; the purpose of Elm is to eventually move on to Haskell The go with PS. :)
Ah *this* discussion =) I'm with 'shakna' in [this recent discussion](https://news.ycombinator.com/item?id=15531027) on the very term, for the time being..
Holy shit! I guess I will stay on the "old" versions because I'm afraid to break compilation of my thesis... but hey, this is awesome!
Awesome that you can now compile to Groff! I wanted that just a few days ago!
Major props to you for not dissing Elm outright :) (Written as an Elm user. I understand the appeal of typeclasses, and I use them in Haskell, yet I like the direction Elm is heading.)
Hi folk. I see you have pretty nice business here. What about partnership? What skills should I have? Btw my name is Vlad too. So this is easy to aggregate requests to different Vlads like in call center.
Thanks, I will learn more about persistent data structures. How did you verify your findings in the repl?
Thanks, I'll check that out
I wish we could avoid the apologetic tone in articles like these: &gt; ...the operation of delete is inherently opposed to Haskell’s fundamentals of immutability... [but we can work around that "problem" by being clever and re-creating portions of the tree as we go...] Instead, just say that the natural way to implement algorithms like this in a pure language such as Haskell is for operations to re-create parts of the shape of the tree where the shape needs to change, without copying the data. Whereas in imperative language it is often more natural to implement them by mutating the tree. Either technique could actually be used in either kind of language.
Unfortunately, we don’t have enough free bandwidth at this time to guarantee a good experience for an intern, but perhaps in the next year or so…
Someone had fun writing the line insert x s = makeBlack $ ins s where ins E = T R E x E -- ... You may also be interested in [this previous discussion of red-black trees in haskell (with zippers!)](https://www.reddit.com/r/haskell/comments/ti5il/redblack_trees_in_haskell_using_gadts_existential/)
Wait, I didn't realize Pandoc is written in Haskell! Awesome!
Something that doesn't make sense is your proposed JSON representation of `Features`: {"feature":"foo","option":"enabled"} Where's the value of `bar` in this representation? I suspect that you actually want the JSON to look like this: [ {"feature":"foo","option":"enabled"} , {"feature":"bar","option":"noisy"} ] Although that's still a strange choice for the representation when you could instead have this: {"foo":"enabled","bar":"noisy"} In your question, you're asking about writing `ToJSON` and `FromJSON` instances for `Bar`,`Foo`,etc., although my understanding is that you actually only ever need an instance for `Features`. Do you actually need these instances for all of these components that make it up? That aside, I'll provide code that answers the question as it was asked, and if you update the question, I can probably give a better answer. Also, I've corrected the `featureName` since the one you gave didn't reference the typeclass variable. {-# LANGUAGE RankNTypes #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE OverloadedStrings #-} module Example where import Data.Proxy (Proxy(..)) import Data.Aeson ((.=),(.:)) import Data.Text (Text) import Control.Monad (when) import qualified Data.Aeson as A import qualified Data.Aeson.Types as AT class FeatureEncoding a where featureName :: Proxy a -&gt; Text featureToText :: a -&gt; Text featureFromText :: Text -&gt; Maybe a featureToJSON :: forall a. FeatureEncoding a =&gt; a -&gt; A.Value featureToJSON a = A.object [ "feature" .= featureName (Proxy :: Proxy a) , "option" .= featureToText a ] featureParseJSON :: forall a. FeatureEncoding a =&gt; A.Value -&gt; AT.Parser a featureParseJSON = A.withObject "Feature" $ \m -&gt; do name &lt;- m .: "feature" when (name /= featureName (Proxy :: Proxy a)) (fail "wrong feature name") option &lt;- m .: "option" maybe (fail "could not decode option") return (featureFromText option) ------------------------------------------------- -- Everything below here is written once per type ------------------------------------------------- data FeatureBar = BarDisabled | BarQuiet | BarNoisy instance FeatureEncoding FeatureBar where featureName _ = "bar" featureToText x = case x of BarDisabled -&gt; "disabled" BarQuiet -&gt; "quiet" BarNoisy -&gt; "noisy" featureFromText x | x == "disabled" = Just BarDisabled | x == "quiet" = Just BarQuiet | x == "noisy" = Just BarNoisy | otherwise = Nothing instance A.ToJSON FeatureBar where toJSON = featureToJSON instance A.FromJSON FeatureBar where parseJSON = featureParseJSON 
to be honest, I have checked the SO and it looks scary, it seems advanced for me. 
Thanks! The `Features` type never needs to leave memory, however in our event-source system we want individual feature flags in their own event (we communicate events with JSON). Are you able to explain what it is that `Proxy a` is doing here?
I don't have much experience with Haskell, I have tried this approach and got stuck at the end,in this [question](https://www.reddit.com/r/haskell/comments/79hrce/declare_abstract_monadic_function_with/) there is more details. I might be doing something wrong am not sure
You may be correct. My experience is outdated. Digging deeper I realised that limits are controlled by default configurations of nginx and haproxy nowadays. 
If the types are instances of `Enum` and `Bounded`, perhaps we could write a single `AFlagType -&gt; Text` function to avoid repeating names 
I found this section confusing at first read: &gt; When evaluating a value in head normal form does not terminate, this value is &gt; not considered to be consumed exactly once, making the looping function linear &gt; &gt; loop :: a -o b &gt; loop x = loop x &gt; &gt; If `loop` u is consumed exactly once (i.e. never) then `u` is, vacuously, consumed exactly once. It took me a while to realize that last paragraph was saying "since `loop u` does not terminate, it can't be consumed exactly once, and we can therefore vacuously make whatever claims we like about how many times it consumes `u` when consumed exactly once, including that it consumes `u` exactly once." 
Ok, I see now. The links generated by Haddock can be misleading without the module prefix. Do you get any mileage from this `Conkin.Applicative f` constraint in practice? I haven't encountered any use for which `Prelude.Applicative f` was insufficient, but of course I may just be unimaginative.
If you want to be able to send an event that communicates that a feature flag has been set to a value, then the `FromJSON` instance probably isn't going to work for you. When you call `parseJSON`, `fromJSON`, or `decode`, you need to know **at compile time** the type you are decoding into. In your case, you don't. It could be any one of those flag types, so the `FromJSON` instance cannot be resolved. Instead, you probably need a type like this: data Setting = SettingFoo FeatureFoo | SettingBar FeatureBar | ... And then you need a `FromJSON` and `ToJSON` instance for it. Then, on the receiving side, you know what type you're decoding into. 
Not yet. In theory it can be used to swap the order of a product of two Conkin applicatives: Product ItemF LocationF (,) -&gt; Product LocationF ItemF (,) But that's of debatable utility 
One way is to use [stable names](https://hackage.haskell.org/package/base-4.10.0.0/docs/System-Mem-StableName.html) to approximate the addresses: -- | -- &gt;&gt;&gt; main -- (3,2,1) -- (3,2,1) -- (4,2,1) main :: IO () main = do let pos = Pos 1 2 printPtrs pos printPtrs (returnsSame pos) printPtrs (returnsEqual pos) As you can see, the names of `pos` and its fields are the same as the names of `returnsSame pos` and its fields. The name of `returnsEqual pos` is different, but its fields still have the same names.
What is Groff? 
To answer your question about `Proxy`, the issue is that you arent' allowed to write typeclass methods that don't mention the variable. For example, this is not allowed: class Foo a where foo :: Int If you try to write this, GHC will complain about an ambiguous type variable and mention that you might want to turn on `AllowAmbiguousTypes`. You do not want to turn this extension on unless you really know what you're doing. The reason this isn't allowed is that there's no way to call `foo`. What if we have: instance Foo Bool where foo = 12 instance Foo Char where foo = 65 main :: IO () main = print foo What should this do? It cannot figure out which `Foo` instance to use. So, GHC forbids such class methods from being written. As far as the `Proxy` stuff goes, it's a way to get around this problem. The most naive way around it is to modify `Foo` this way: class Foo a where foo :: a -&gt; Int instance Foo Bool where foo _ = 12 instance Foo Char where foo _ = 65 main :: IO () main = print (foo 'x') This works (and it's what [Storable](https://hackage.haskell.org/package/base-4.10.0.0/docs/Foreign-Storable.html) does for `sizeOf` and `alignment`), but it's unsatisfying. What if we don't have a value of the type that we're trying to call `foo` on. We can kind of cheat and use `undefined`, but now we're kind of in this weird situation where we've written a typeclass method and have a unenforced rule that anyone who implements the typeclass method must ignore its argument. So, instead, we can use `Proxy`: data Proxy a = Proxy -- this is how Proxy is defined in Data.Proxy class Foo a where foo :: Proxy a -&gt; Int instance Foo Bool where foo _ = 12 instance Foo Char where foo _ = 65 main :: IO () main = print (foo (Proxy :: Proxy Char)) Notice how the call to `foo` no longer needs to take a value of type `Char`. The type of `foo` is also more honest about what it can and cannot do. Now, the `ScopedTypeVariables` part. First of all, we don't actually need `RankNTypes`. We just need `ExplicitForAll` (which is basically a small fraction of `RankNTypes` full power). Let's look at `featureParseJSON` again, ignoring the parts irrelevant to this discussion: featureParseJSON :: forall a. FeatureEncoding a =&gt; A.Value -&gt; AT.Parser a featureParseJSON = A.withObject "Feature" $ \m -&gt; do name &lt;- ... when (name /= featureName (Proxy :: Proxy a)) (fail "wrong feature name") ... What if we tried to write this without having `ExplicitForAll` and `ScopedTypeVariables` on? The body would look the same, but the type signature would look like this: featureParseJSON :: FeatureEncoding a =&gt; A.Value -&gt; AT.Parser a The problem with writing it this way is that GHC won't unify the `a` type in the function body with the `a` type in the type signature. This means that it is as though the `Proxy a` type is actually `Proxy b` or `Proxy c` or a proxy with some other random type variable we know nothing about. The `a` in the type signature is only in scoped to the type signature. So it won't use the `FeatureEncoding` instance in scope to resolve the call to `featureName`, and it will issue a compile time failure. Basically, what `ScopedTypeVariables` does is extends the scope of type variables introduced by `forall` over the entire function body, which makes this work.
We're using an in-house library for heterogeneous lists. We have data HList (f :: k -&gt; *) (xs :: [k]) If I want an `Ord` instance for an `HList`, there's three ways: * For this particular `f` and these particular `xs`, `Ord (f x)` holds for each of the particular `xs` * For this particular `f`, `Ord (f x)` holds regardless of the `x` * For this particular `f`, `Ord1 f` holds, which means that as long as `Ord x` holds for each of the `xs`, `Ord (f x)` holds. These are three different potential superclass constraints, each of which sometimes applies in our code base. Our current solution is to privilege the first of these and provide concrete instances for any `f`s we run into. My responses to your three problems: * Granted, though I'm willing to take a solution that only allows this kind of overlap within the same module, or even the same declaration * By saying that either constraint works, the programmer gives up control over which instance is selected; the compiler is free to use whichever instance is more convenient (as in `IncoherentInstances`) * The programmer thinks really hard about it and pinky swears. Again, as with `IncoherentInstances`, the burden of understanding that the compiler is free to pathologically choose either instance is on the programmer
Thanks for the link. In my blog I point to a presentation[1] by Stephanie Weirich where she compares the Agda implementation of RB Tree with a Haskell implementation written using GADTs. I suppose Delete would be a mess there too. [1]https://www.youtube.com/watch?v=n-b1PYbRUOY
Neat, will be trying this out
Well explained, thank you. 
 $ whatis groff groff (1) - front-end for the groff document formatting system But really, it's an old typesetting system mostly in use today for man pages.
My thought behind that was when someone mentions "delete", the immediate thought that comes to my mind is to mutate the structure. As in if I was doing a sketch, and I had to delete a portion of the sketch, in a natural sense I wouldn't recreate the entire sketch without the given portion. I would just "erase" part of the sketch. The concept of creating a new version when asked to delete might seem a little alien to newcomers, which is why I take that tone. But I appreciate your point.
I'm going to start a Moderately Priced Haskell consultancy now.
Start here maybe: [datahaskell.org](http://www.datahaskell.org)
Hurrah! Thank you jgm and pandoc devs.
Backpack.
I knew it when someone on /r/archlinux complained an arch AUR maintainer made `pandoc` binary package depend on developer libraries. Installing `pandoc` installs 750MB extra dev libraries. https://www.reddit.com/r/archlinux/comments/6jce9x/pandoc_minus_the_new_750mb_haskell_nonsense/
And what would be the reason to pay more? :)
The customer has no means to judge the quality of service so they will either assume that the services are the same and one is overpriced or that the more expensive one must be better because it's more expensive.
Disclaimer: I wrote [yesod-elements](https://hackage.haskell.org/package/yesod-elements-1.0/docs/Yesod-Elements.html). You may want to use `yesod-elements` instead of the Shakespearean templating stuff. I use this in my own projects just because I like it better. It may be easier for new people since it removes some of the magic from yesod.
I wonder if you could use this to produce man pages from haddocks.
We’re not doing any HTML on the backend, thanks though!
&gt; They have a similar problem with rust, but due to differences in the languages it's a smaller task? I'm coming to this thread late; I check in on /r/haskell every few days. But basically, Rust has the same amount of runtime as C, so there's not really anything to port. It's more about the standard library (if you want it) than a runtime.
We can offer a lower price because it doesn't take too long to answer a few questions every now and then, but you don't have to worry about the quality. If someone needs more attention (i.e. lessons) we can be moderately priced as well ;)
I'm not sure how haddock works, but you can run literal haskell files through pandoc and get man-pages.
Oh, c’mon join humanity and answer questions for the greater good just like the good folks who work on GHC. Quite honestly, I hope endeavors like this fail but obviously anyone is free to try—oftentimes just because you can, doesn’t mean you should.
Someone throw the 100 bucks out of the window and share the lols. The bogus "We" (no references, no code to be seen / success stories to be shared), the "no guarantees, but we'll try, oh how we'll try". This seems to me - at **best** - like "me and my friend just waddled through a haskell course, and we want to capitalize on it".
Thanks, good luck to you as well.
If you've read the announcement, we don't require to pay us up front. This should tell you something.
&gt; nobody ever went broke underestimating the intelligence of the American public
Our error messages have gotten quite a lot better since then. In that case, you would see something like: The type variable h, bound at File.purs line 9, column 14 - line 9, column 25 has escaped its scope, appearing in the type Eff t1 (STRef h0 Int) in the expression runST (newSTRef 1) in value declaration test which I think is pretty decent, given what's actually going on in this case. As you add more type system features, of course it will be harder to understand the ways in which it can fail. It's one of the reasons why Elm is better for newcomers to FP. I would love to see a language with Elm's error messages and PureScript's type system features, I just don't know if it's possible.
Is this for real? Sounds like a parasitic open-source scam monetizing the hard work of others—Trump effect for sure.
We're speculating anyway because of the NDA, but yes that is my point.
&gt; It also feels like PureScript is unmaintained. &gt; The package manager as of like 6 months ago still used Bower PureScript is very much maintained. We should have a new major release coming out shortly. The Bower thing is unfortunate, because JS developers don't use it, but it's still perfect for our needs. It's actually a very good piece of software, if you look at it as a semver layer on top of Git and not a JS package manager. &gt; A big issue for me, too, was that I felt like I was looking at Haskell docs to figure out how to do things more than I was using PS documentation. At that point, why not just use a Haskell compiler that targets JavaScript instead? Yes, GHCJS exists and it is excellent, but it makes some very different design decisions, so it doesn't really make sense to say that you would switch from PS to GHCJS for that reason. You wouldn't choose PureScript because you wanted Haskell-to-JS, because it's not that. 
&gt; I just don't see a viable path to learning PureScript at all that doesn't start with "first, learn Haskell." The PureScript by Example book doesn't require any previous Haskell knowledge and goes at a reasonable pace for the first six chapters or so. &gt; and as far as I can tell, the community is not at all interested in compromising their abstraction towers with any kind of simplified ideas at all! Not true - every aspect of PureScript is swappable, and there exist alternative standard libraries for beginners. See [Neon](https://github.com/tfausak/purescript-neon) for example. Our "standard" standard library just represents one set of opinions, but it's very possible to build your own ideal set of abstractions, and I know this because I've done it.
I've skimmed through it, my brain couldn't take it (it shuts off / optimizes away anything sounding too vague, promissy, ...). [And your pages still read like some naive 16yr old 'lets make a SW company'-kiddie wrote them !] If you are "try before you buy", it might not actually be a bad idea for people to give you a try. ---- To anyone thinking giving DirtCheapHaskell a spin: if it ever comes to some contract, read it very carefully and then still give it to your lawyer, the world if full of people crying their eyes out because "But they looked like so nice people". Migh be a couple of guys starting out, might be scammers as well (some indices: no payments up front, time-limited offer, no names, no nothing)
Brick author here - very nice!
I'm not sure what you mean by “Trump effect” – could you explain? As for monetizing the hard work of others – I understand this point of view but I feel that it can be applied to literally everything. E.g. why doesn't it apply to all programming in general? (If the compiler and the libraries you use are OSS, your share of work is most likely quite tiny in comparison with the effort that went into the ecosystem.)
"The greater good" doesn't mean "give away your services and time for free to anyone forever". Why should it? Because someone on the internet said so? Considering you aren't a GHC developer yourself (or I assume you otherwise wouldn't refer to them as if they were a 3rd party), your use of them as some kind of carrot is insulting. Fuck off.
Guy who sits next to the brick author here - very nice!
Guy who sits next to the guy who sits next to the brick author here - very nice !
my 5 cents: you can find learning material for elm which do not assume knowledge of JS/web, in purescript you cannot. So if you know haskell, but not JS, you would probably go with elm
(Allow me to introduce my personal colleague peanut gallery: tom-md and dmjio.)
So, yeah, no idea if you're part of OP's "we" or not, but *if* you are, my interest in working with your collective just plummeted to well south of zero. For the record -- and for OP -- I don't think it's a bad idea or business model, though I do think you'll have a bit of an uphill battle with some. Especially if the above is the restrained voice of customer service.
I might use this to commission GHC extensions
Thanks! It's a great simple to use plug-in that works really well.
&gt; no names, no nothing That was simply a not-very-well-thought-out decision I made at the moment of writing – not an intentional attempt to hide the identities of people involved :) I have added names and Github/Hackage links. &gt; no payments up front I don't quite understand why “no payments up front” is a red flag, and I feel that if we _did_ require a payment up front it would seem like a red flag as well (to other people). Is there some third option I'm missing? 50% of the payment up front? 100% of the payment up front but coupled with a money-back guarantee? Something else? &gt; if it ever comes to some contract, read it very carefully and then still give it to your lawyer Sure, that's good advice in general. / ^A ^side ^note: ^there's ^another ^option ^that ^lots ^of ^people ^don't ^know ^about ^– ^if ^you ^reuse ^a ^pre-existing ^popular ^contract ^it's ^easier ^to ^convince ^others ^that ^there ^are ^no ^intentional ^landmines ^there. ^This ^is ^commonly ^done ^with ^Contributor ^License ^Agreements, ^for ^instance. ^There ^are ^likely ^popular ^“consulting ^services” ^contracts ^that ^could ^be ^reused ^in ^similar ^fashion, ^though ^I ^haven't ^researched ^the ^topic ^enough ^to ^recommend ^anything ^in ^particular ^to ^other ^people ^doing ^consulting. &gt; I've skimmed through it, my brain couldn't take it (it shuts off / optimizes away anything sounding too vague, promissy, ...). [...] &gt; &gt; And your pages still read like some naive 16yr old [...] Okay, that makes sense – thanks for explaining! I know that not everybody can tolerate my writing style, but I'm okay with it because some people like it and I feel that this is much better than going 100% generic and having nobody feel anything at all. (This said, if you could PM me and say which bits in particular give you the naive-16-year-old vibe, I would really appreciate that.) As for sounding promis-y, that's quite unintentional and I'll try to tone it down.
&gt; Especially if the above is the restrained voice of customer service. More like the tired voice of GHC team :) ^/u/aseipp ^if ^we ^ever ^meet ^I ^owe ^you ^a ^beer
A little touchy?
I think Eff is going to be renamed to IO, now happy?
Explicit passing of effects is actually very closely related to free monads. In the explicit passing style, actions are parameterized by a tuple of functions of the form `a1 -&gt; a2 -&gt; ... -&gt; m r`, where `m` is a monad. This is equivalent to being parameterized by a single function of the form `forall i. p i -&gt; m i`, where `p` encodes the signatures of the functions. For example, data Log i where Log :: String -&gt; Log () data CurrentTime i where CurrentTime :: CurrentTime UTCTime data (p \/ q) i = L (p i) | R (q i) type p :-&gt; q = forall i. p i -&gt; q i myFunc :: Monad m =&gt; ((Log \/ CurrentTime) :-&gt; m) -&gt; m () You can package this general form as a new monad `ICont m p`: newtype ICont q p i = ICont { unICont :: (p :-&gt; q) -&gt; q i } instance Monad m =&gt; Monad (ICont m p) `ICont q` is a delimited continuation monad in the category of indexed types and index-preserving functions, but that isn't relevant here. To ensure that actions in `ICont m p` only interact with `m` through the passed-in function, you can quantify `m` over all monads: newtype FreeP p i = FreeP (forall m. (Monad m) =&gt; (p :-&gt; m) -&gt; m i) `FreeP p` is the free monad for a type constructor `p` (this is more general than the free monad for a functor). `FreeP` is also a monad in the category of indexed types and index-preserving functions. `FreeP p` is closely related to `Prompt p`, the [free (strict) monad](https://www.eyrie.org/~zednenem/2013/06/prompt) for a type constructor `p`: newtype Prompt p a = Prompt (forall b. (forall i. p i -&gt; (i -&gt; b) -&gt; b) -&gt; (a -&gt; b) -&gt; b) Essentially, this is `FreeP` where `m` is always a continuation monad. `Prompt` is closely related to `Freer` (possibly the same, I haven't checked recently). It has the nice property that `p :-&gt; Prompt q` is guaranteed to provide an implementation of the interface `p` using only the interface `q`, with no other effects. Since `Prompt` is also a monad in the category of indexed types and index-preserving functions, you can combine these using Kleisli composition. However, using `Prompt` conveniently requires some sort of union type or extensible variant for the same reasons that the explicit passing style is better with extensible records.
Sorry, I might made it seem more difficult that it really is. If are still interested, I would start by finding a *concrete* monad stack that enables al the effects you want. You can always create a more generic mtl-based interface later. Then I would try to find the required transformation into `Handler`, using the auxiliary functions from `servant-server` and `mmorph`. The `Enter` typeclass is a bit scary, but once you have the required transformation using `enter` would be easy.
A bullshit service that offers no real value—just ignore these bozos and post your questions on this subreddit or one from the sidebaror in stack exchange for no charge. Why buy stupid bullshit?
The awkwardness and necessity of this feature makes me think that semicolons are a better choice for such delimiters than commas. (yes, yes Wadler's law)
Do HIE needs already built ghd-mod for the exact the same version as of the project installed on the PATH?
Honestly, the distinction between the type and value namespace isn't really a good idea to emulate. It is already causing annoyances and will get worse once Haskell has full dependent types.
Well, it would certainly make things easier for diff/merge and version control systems!
In case someone doesn't recognize the names of these gentlemen: * One of them wrote the [Lens over (thousands of cups of) Tea](https://artyom.me/#lens-over-tea) tutorial. An excellent tutorial for understanding the lens library from what I can tell (I only finished part 1 of 6). * Together they are writing a new Haskell book, [Intermediate Haskell](https://intermediatehaskell.com). Any updates on this book? I want this book ready by tomorrow ;)
I'll admit this is a tad bit "odd" as a business model. But if it's honest-to-goodness (which I have every reason to believe that it is), then I can honestly see this being very valuable!
Actually I think the phrase "dirt cheap" is the one thing that triggers most of the the o_O reactions. If you were to drop that from your marketing campaign then I think you might get a better response.
Heh :) What exactly is odd about it, by the way?
So you mean by writing _ and letting the compiler find you the function from the libraries depending on your arguments? :D
Wow, the changelog is so massive, it feels like it never ends... Congrats!
Fair enough, and I can honestly see why someone in the GHC -- or any -- team doesn't want someone outside of it voicing their opinion for them... but, from a purely business relations perspective, that seemed like someone *apparently* or at least *potentially* inside in your business going to at least an 11 where a 3 would have sufficed. I mean it's Reddit, you don't control the conversation, and the critique of your business model isn't exactly legit or well reasoned -- plus the wish for failure line is plain bullshit -- but damn that went big quick, and at first blush (and before you updated your site with the people involved, making it clear the above wasn't an insider) doesn't seem like an attitude I'd want to fork out $100/month to interact with.
But should we permit leading and trailing semicolons? ;)
Your characterization was just quite severe (exclamation point included!). What gives you the idea that no one is interested in simplifying things for the purposes of teaching?
The post [already addresses this](https://lexi-lambda.github.io/blog/2017/10/27/a-space-of-their-own-adding-a-type-namespace-to-hackett/#2017-10-27-a-space-of-their-own-adding-a-type-namespace-to-hackett-footnote-1-definition): &gt; “But what about dependent types?” you may ask. Put simply, Hackett is not dependently typed, and it is not going to be dependently typed. Dependent types are currently being bolted onto Haskell, but Haskell does not have #lang. Racket does. It seems likely that a dependently-typed language would be much more useful as a separate `#lang`, not a modified version of Hackett, so Hackett can optimize its user experience for what it is, not what it might be someday.
Elm has the equality problem, doesn't it? Fails at runtime if you try to compare functions for equality?
Elm's virtual-dom is a 100% JavaScript library. This has nothing to do with the inherent performance of either language. Now, PureScript is not an optimized language, and certain language features (transformers) _can_ lead to GC churn. But I don't think there's any reason to say that one is particularly faster than the other.
Fair enough
Are you going to post an annoying type error here?
To be clear, I'm not claiming that people are uninterested in creating variants of PureScript for teaching. I wouldn't know. I was referring to standard PureScript. My observation was based on observing the standard library itself -- along with the knowledge that it wasn't a mistake that it ended up that way, because it was designed by people who are plenty familiar with the analogous situations and choices that have been explored in Haskell.
You aren't wrong.
The almost standard recommendation nowadays is [Haskell Programming from First Principles](http://www.haskellbook.com). I've read Hutton, I've read Bird, most of Real World Haskell, etc. Each has its merit, each teaches something else the others don't, but if I have to recommend a single book that will take you from zero to being able to at least write Haskell for bigger-than-toy projects, the haskell book it is.
I'm not involved with them, and although I'm a rather dormant GHC developer, I still consider myself "part of the fray" and don't take kindly to people using the free work of good-intentioned developers (who do immense amounts of work) and using it to attack other people -- especially an attack premised on some flat-out bullshit. If this came off as some kind of representation, which I can see how it would have upon a re-read, I apologize, so I've amended the text to make it clear I'm not involved. I'll be certain to make this immediate clear in the future, in some other circumstance, should I need to. But, I'll just say that -- in general? I'm not worried if some random bystander happens to randomly think "Wow, that guy might be an asshole". Being angry always runs the risk that some random person who has no context or understanding will misinterpret your emotions. Frankly, this isn't really my problem as far as I'm concerned, and beating around the bush to appeal to some imagined social contract (where you must be be "nice" to other assholes to prove you have the better morals) isn't really a good use of my time. So I'll just call a spade a spade and be done with it. But I do hope this doesn't dissuade you from using the OP's service, and if it does that really is my fault, so I appreciate you saying this. (Feel free to simply avoid me, if you must, instead.)
I'm excited to try this one out. I use Pandoc so I can write articles in Markdown and generate blog posts in HTML and also format them into PDFs and Word files. Without Pandoc I would go made when editing (and probably not even do it) because I'd have to make changes in multiple formats.
Brilliant! Thanks!
I always enjoy your writing, you have distinctive style
&gt; Elm is easy to move into. It doesn't ask you to learn anything but syntax. You just don't realize that the abstraction ceiling is so low until a month later when you start hitting your head. This is written as gospel but it isn't. We're happily using Elm at CircuitHub with 50k lines of code and are yet to hit this ceiling. I understand Ed writes code differently than I do, but consider that maybe he writes code differently to you too. For some applications, they are just big because they are, well, big. Abstracting to the nth degree isn't going to shift the complexity away from a large feature set. In this space, I actually appreciate Elm's limited means of abstraction in return for the consistency I get. I just get my head down, write the code in the same boring pattern, and get the job done. I wouldn't want this approach in Haskell, but... tool for the job and all that.
That's not even what the tweet says, or am I really really bad at understanding it?
With your edit I've no issue whatsoever with your response above; given the context of the top post being an advertisement of a business, people who show up seemingly to defend that business loudly, even against spurious nonsense, can't really be be discerned by passers-by (like me) from representatives of that business, especially in an online forum full of only usernames and with the original ad copy that did not identify the principals. At this point I'm more representing the hypothetical customer; I'm not sufficiently advanced yet in my exploration of Haskell to even know if I need their service yet, so no harm no foul for their bottom line, and it remains in my bookmarks to revisit if and when I get there. I do appreciate the thoughtful response, edit, and mea culpa. I also appreciated the thrust of your original response, BTW... I've no idea why people think open source should mean uncompensated.
Probably. Equality testing and ordering are the two most compelling use cases for typeclasses and Elm cheats to provide this functionality.
Thanks, this was a fantastic explanation! It's much clearer now.
It is a kinda confusing system. A "I recommend" button could be a better system for a newcomer to distinguish between multiple packages in a same category.
That's a polarizing Arch Linux Haskell packaging decision. While their justification makes sense, it highlights how many dependencies programs have and were hidden because of static linkage. Consider Rust's cargo to one day switch the default, and you will have the same situation. The Arch maintainers decided to link dynamically and the fallout has been big, affecting common uses of GHC or bootstrapping stack. I don't blame them, they have been at the forefront of radical changes by switching to systemd, python3, etc. to name a few. Arch works well but you need to know they don't shy away from making broad changes if they find it necessary.
And in use by OpenBSD developers for more things due to popularity and advocacy of mdoc.
Would it preserve the structure of typical manpages? I suppose that would be a matter of the author adhering to the typical sections, references and Pandoc being aware of those.
I meant to link the other tweet from that convo. But the question should rather be: Why do we need to have our packages on Hackage judged at all? Are we going to allow people write review-comments for packages on Hackage too? We already got a rating system to assess whether a package is worth your time: if a package makes it into Stackage you can be confident it's really a good package as it takes more effort and commitment to get into Stackage than creating a couple of sockpuppet accounts to inflate the votes on your package on Hackage.
Nice post, I especially liked the in-line comparisons between different approaches. You mention "FRP inside a multi-threaded program" in the end – I would really appreciate such a post. Is there any FRP system described anywhere that utilises multiple threads and also allows interacting with the outside world on multiple threads simultaneously? As part of FRP, I mean – I guess you could always use the async library and handle such things outside of FRP.
Common Lisp compiles to machine code.
I don't mind having my packages rated. I think it's a useful tool for discovering and comparing packages. I agree with the main argument that the package index part of Hackage should be decoupled from the rest of it. 
A package existing on Stackage is a signal that it's *well maintained*, not that it's a *good package*. 
Right, but being well maintained is a requisite for being a good package
I can assure you that they did not "waddled" through some course and that they are young but very knowledgeable individuals when it comes to haskell. You can make sure yourself, just ping them I am sure they will be happy to explain all you want to know.
&gt; What skills should I have? We don't know yet how much knowledge/experience an average question requires (to be more precise, “how much experience do you need to be able to answer at least N% of incoming questions”). Here's what we've got so far: * someone has questions about GHC extensions + “help me choose a library” * someone wants us to solve a memory leak in conduit code and that will likely be quite tricky * somebody has questions about [`singletons`](http://hackage.haskell.org/package/singletons) * another person wants a review of a ~1 kLOC codebase * for yet another person it looks like experience with Template Haskell would be handy * and finally somebody is having problems with monad transformers (could be either easy or hard, no idea yet) If this sounds like something around your level (or if you have better-than-average knowledge of some particular part of the ecosystem, e.g. you know `pipes` inside and out or you have done something non-trivial with Yesod), then PM me and – depending on how many clients we have in a couple of weeks – we might have a partnership.
&lt;sm&gt; @quote FAQ good books &lt;lambdabot&gt; FAQ says: What are some good books for learning haskell ? Haskell Tutorial And Cookbook (HTAC), Programming In Haskell (PIH), Haskell Programming From First Principles (HPFFP)
That's an awesome idea. That way Stackage would be decoupled from all those "Trust"e related shenanigans you have to put with on Hackage
Time constraints could mean that having more on-demand access to someone that both answers your questions and accumulates context is the difference between success or failure with your project. I think the niche they're filling could be very helpful. 
You can ensure man page structure with a [template](https://github.com/simonmichael/hledger/blob/master/doc/manpage.nroff).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [simonmichael/hledger/.../**manpage.nroff** (master → 39d13d6)](https://github.com/simonmichael/hledger/blob/39d13d68ef11daa49e0bfcc8c0b138c05a72e3cc/doc/manpage.nroff) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dp472fw.)^.
Just thinking about waiting for this to finish for more than a couple seconds kind of infuriates me...
I remember seeing slides from a talk that called it a "virus that makes people install haskell". Very impressive software.
HIE should solve this. https://github.com/haskell/haskell-ide-engine 
Will follow-up in the future, thanks for the reply!
Beyond this, Herbert and I have chatted a little about the prospect of implementing short-string optimisations directly in whatever eventually becomes of text-utf8 and text (and possibly dropping the stream fusion framework). It would bring some cost in terms of branching overhead, but the API uniformity seems very appealing. The state of the art of "let the user figure it out!" isn't exactly ideal...
&gt; What if we flatten all structures and enums and use pointer indirection only at recursion points? We could then use unboxed arrays all over and actually benefit from them. https://github.com/ollef/sixten
No skin in this game, but I can remember examples in various languages of packages that were certainly good, but hadn't been actively maintained in years. IE stable and fit for purpose breeds not a lot of active engagement from maintainers, even as relatively major shifts happen elsewhere in the language, especially shifts that are tangential to the package's core area or concern (which packaging and distribution infrastructure fairly often is). Can't speak at all to how often this happens or has happened in Haskell, so take my point with a half a grain of salt, but "requisite" just seems a bit strong.
Why do you constantly try to cause arguments?
Not sure who is the target here? A small company, who wants to dive deep into using haskell? They could just hire a developer that knows haskell inside-out, or go for fpcomplete and similar. The individual dev? 100 a month or 500 a month doesn't worth it IMHO, you can get help on reddit, stack overflow or numerous irc/discord servers for free, or just figure it out yourself. But that's just my opinion, some might see value in a service like this. However. The terms are really vague. Like, oh we might review your 30k loc codebase, if we are bored. 1k? Yeah, sure we'll do our best. 10k? Maaaybe? Would you still pay a mechanic, if he/she can't fix your car, but does his/her best?
I think I've come to prefer whitespace as the separator (parenthesize your syntactically non-trivial list elements).
&gt; Not sure who is the target here? Individual devs and small companies, yes. As someone who used to ask questions on IRC and StackOverflow: people tend to get great answers to beginner questions and to “interesting” questions, but a lot of real-world programming consists of very boring questions that are either answered seldom or require a good amount of luck (e.g. you might have a problem with `lens` and stumble upon Edward Kmett, or you might have a problem with `lens` but everyone will ignore you because there's some discussion about “what is a monad” going on on IRC and suddenly nobody cares about your lenses). Also see two comments below: &gt; Time constraints could mean that having more on-demand access to someone that both answers your questions and accumulates context is the difference between success or failure with your project. and &gt; Are you going to post an annoying type error here? This isn't useful for the subreddit, and nobody wants to debug your type errors repeatedly, for free. These aren't mine but I (mostly) agree with both. Moreover: _even if_ all questions were answerable with a combination of IRC, Reddit, SO and googling, it still makes a lot of sense to hire us if your time is valuable enough that sacrificing an hour to Google every day doesn't seem particularly appealing to you. &gt; But that's just my opinion, some might see value in a service like this. I'm not sure that whether our service “has value” is even a meaningful question if you don't specify the customer – i.e. if it has value for 50% of people and has no value whatsoever for the other 50%, I don't think that it's “a matter of opinion” whether the service has value or not. Something that's useful to a bunch of people and useless to others seems like a net positive thing to me. &gt; However. The terms are really vague. Like, oh we might review your 30k loc codebase, if we are bored. 1k? Yeah, sure we'll do our best. 10k? Maaaybe? Sounds reasonable, I'll update the site tomorrow to include more precise terms :) Something like this: * If you have a one-off question (e.g. “can you eliminate the space leak here? I'll pay XXX”), then we either succeed and you pay XXX, or we refuse to try (and you pay nothing), or we spend a lot of time and don't succeed and you still pay nothing because it's our fault that we overestimated our abilities. * If you want to be able to ask questions all the time, you are basically paying us for talking to you, where “talking” might very well involve us saying “sorry, I don't know” a lot :) Potentially you can spend 30 days asking questions and then decline to pay on the last day of the month and it would be alright. Other than sneakiness, there are lots of other reason why you might want to stop paying – e.g. if you gave up on Haskell, or if you hired FPCO, or if your questions have grown more sophisticated and we stopped being able to answer them, or if you're not satisfied with our success rate. For some people it might make sense to keep paying us even if we can only answer 20% of the questions, and for others it won't.
&gt;I'm not sure that whether our service “has value” is even a meaningful question if you don't specify the customer – i.e. That's actually true, you're most definitely peargreen.
dammit I'm laughing out loud at 4am
Yours is the first positive experience report I've read from a Haskeller. I'll take the extra data point under advisement.
I implemented the VOIP system for an online game one time. Getting data from the microphone looked hard, so I used the WASAPI api (we were targeting Windows). You'll likely want to FFI to your underlying OS's APIs for these things (on linux it looks like [Safe ALSA is the right approach](http://equalarea.com/paul/alsa-audio.html#captureex)). FFIing these things will probably require enough knowledge of C to read headers and understand what things are safe to do and which aren't. [It looks like maybe someone has done the work for you on this front](https://hackage.haskell.org/package/alsa-pcm-0.6.0.4/docs/Sound-ALSA-PCM-Node-ALSA.html), although it doesn't look very well documented :) Bonus points: [there is an FFT library](https://hackage.haskell.org/package/fft-0.1.8.6/candidate/docs/Math-FFT.html).
Go vimwiki!
I'm excited to see Reflex being used outside of the web. It looks to be quite capable in many different domains, though most of the focus has been on `reflex-dom`. But now it's time for me to get on my soapbox again and point out that there are many improvements to Reflex which are in master (judging from GitHub) which have not yet been released to Hackage. There was an issue created almost exactly one year ago talking about a 0.5 Hackage release (https://github.com/reflex-frp/reflex/issues/75), so apparently there were enough improvements to justify a new release a year ago, but no release was ever made. I haven't contributed myself so I can't complain too much, but if anyone is looking for a project to contribute too, maybe they could help get a new release of Reflex on Hackage?
&gt;I'm excited to see Reflex being used outside of the web. No, you're Buttons840.
&gt; I don't quite understand why “no payments up front” is a red flag In combination with no "no names [or company] behind it" &amp; "act fast, limited time offer" ? Red flag raizsed, scammers operate like this: * bait - offer something super cheap, describe how super-safe the transaction is * apply pressure - somebody is offering me more for it / we cannot afford prices like this forever * switch - either take only the money and give him something wothless, or let him sign a bad contract (ten years of $100/month support; or auto-renewing contract with high penalties or ...) But no-one can know, so if you are for true, good luck to you, and if you are not, mucho caution to others ;-)
Would you consider someone currently studying undergraduate CS?
That strikes me as an odd decision. What makes Hackett's goals so different that it will surely never consider dependent types?
As in, you would drop out to start full-time now, or you’re graduating soon (if so, when can you start full-time?)
I'm not graduating soon and I don't plan on dropping out! I just asked to know what you guys were looking for in term of education. I'm still very interested in working with Haskell in production and is wondering if working part-time is an option?
Ah, got it. We are not looking to hire a part-time role because we're such a small team. In terms of education: A CS background is a plus, but we're definitely willing to hire engineers without a degree or CS background (I dropped out of school to join the previous startup where all the co-founders worked together). We're pretty leery of recent bootcamp grads, though.
Ok, thank you. If you ever look for an intern or a part-time, let me know; i'll be glad to apply.
Thanks again, you are right, actually the `ReaderT` what make me think to find a work around otherwise I would have made an instance an `IO` instance for `UserRepo` and it would be fine. the thing is that I need `ReaderT` to pass a database connection and some other env variable.
There's actually two separate issues here: - Arch converted all binary Haskell packages from statically linked to dynamically linked. This meant lots of users who only used binary Haskell packages now suddenly have to install lots of Haskell library packages. This is mostly an annoyance because of the extra disk space that the library packages require, but there were lots of complaints nonetheless. - Arch removed static libraries from all Haskell packages, perhaps in response to the aforementioned complaints about disk space usage. Unfortunately, neither GHC, Cabal, nor Stack are designed to be used with dynamic-only packages, so this ended up revealing lots of bugs. This causes problems for Haskell developers who want to compile packages using Arch's official GHC distribution.
Dhall avoids this issue in a different way: by requiring that all pattern matches enumerate all cases and only match one level deep (i.e. no wildcard matches and no matching multiple nested constructors at once)
There's nothing parasitic about a voluntary exchange of money for services
As a Nix user, whitespace separated list elements has been the source of untold amounts of frustration. Though it’s certainly amplified by the type system not telling you that your attempted function application is being treated as two list elements.
Does the fusion get in the way of something else, or is it just not paying its way? I don't have any intuition for how much it helps and where... I'd imagine not much since I don't really transform text in pipelines, but I guess the proof would be profiling before and after removing it. Merging short text and normal text seems like a good idea... I use lots of text which is short but I didn't even know about short-string and even if I did it's probably not worth switching, given the API differences. Integer has separate short and long versions, and it seems to be ok?
I really liked your job ad and the way it's written. Good luck with the hunt. What sort of money do you pay remotes with say, 5 years production typed FP experience? If you can't say, all g
To be fair, I have never seriously used a language with whitespace as a separator, and when I did use one at all, it was s-exp based.
HIE has ghc-mod linked into it as a library. The only important thing is that hie is compiled with the same version of GHC as is used in the project, as it needs to be able to compile/load the modules using the project stack/cabal configuration. We are still looking at ways of having GHC-specific versions, and use them according to the project.
Type classes aren't just about implicit parameters. They also give us a nice thin category to work with.
I'm being handwavy about a lot of details, but basically a lot of the functions in text are defined as: f = unstream . f' . stream Where `f'` is a worker function that operates on a stream of characters rather than the byte array. The advantage is that if the user writes something like: foo = f . g With `f` and `g` both being defined in the above form, the inliner can first write this is as: foo = unstream . f' . stream . unstream . g' . stream And then the stream fusion optimization can kick in (using GHC rewrite rules): foo = unstream . f' . g' . stream This means there's only a single pass over the data, which is good. Of course, there are some disadvantages as well. If you're just applying a single function, you could be paying for the `unstream` and `stream` conversions (depending what other optimizations kick in). A few functions (`concatMap` iirc) can't be written in terms of the stream datatype (at least when I was working on text), so something like: f . concatMap h . g Needs to do the conversion to and from stream twice. But I think the main concern is that you can get small constant factors by working with byte asways directly. A lot of our programs spend more time doing single operations and moving `Text` around different datatypes, and we're paying the relatively high stream/unstream constant factors.
Forgive me if I'm phrasing this incorrectly as I'm still learning the terminology. I would like to "strip" a data type of its constructor in an expression, much in the same way you can when taking in function arguments. For example, say I created the following data type: data Food = Chicken | Lettuce deriving (Show, Eq) newtype Meal = Meal [Food] Now, if take Meal as an input, I can either use the entire Meal [Food] type, or label the [Food] list with an alias, like so: Eat1 :: Meal -&gt; Bool Eat1 dinner = True -- where dinner = Meal [Food] /* or I can do this */ Eat2 :: Meal -&gt; Bool Eat2 (Meal xs) = True -- where xs = [Food] In the above example, functions from Data.List won't work on dinner, because dinner's type is Meal [Food], and is not a list, strictly speaking. On the other hand, xs is just a list, which is a non-grounded type that Data.List that operate on. Now, is there a way to "extract" the list from dinner within an expression in the function Eat1, (aka "strip" the Meal type), or can that only be done when taking in the arguments?
[removed]
Ok, let's assume we have a `ReaderT Env ExceptT e IO`. We can transform it into a `ExceptT e IO` just by using `runReaderT` to supply the environment! I believe you could do this manually to the handler before constructing your server. Does this simple thing work? Ok, now assume you have *many* handlers in your API that require that ReaderT to have access to your configuration. In that case "pre-processing" each handler separately might be cumbersome. That's where `enter` is useful! You "pack" the transformation into the helper type `:~&gt;` and apply it to your whole server definition with `enter`. The [example in the servant docs does something](http://hackage.haskell.org/package/servant-server-0.11.0.1/docs/Servant-Server.html#g:5) like that but it does not match *exactly* our use case becase it is converting from a *pure* Reader. We would have to do something like this instead: &gt; let nt = runReaderTNat myEnvValue :: ReaderT Env ExceptT IO String :~&gt; Handler &gt; in mainServer = enter nt myReaderTServer Notice that we are using [runReaderTNat](http://hackage.haskell.org/package/servant-server-0.11.0.1/docs/Servant-Server.html#v:runReaderTNat) instead of [runReaderT](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Reader.html#v:runReaderT) because `enter` requieres the transformation to be packed into the `:~&gt;` newtype. Can you get something like this to work?
&gt; It's not that a whole mixed bag of features make it into the language because it just about fits into the compiler and some person wanted that thing. Elm doesn't have certain things and does some things oddly, but you can talk to the one person who made that decision and know that it had reason behind it, regardless of whether you agree. Having a language dictator is sometimes exactly what you want. We already know that absolute dictatorship isn't the only solution to this collective action problem.
Sure. I'm talking about the rendering performance of existing libraries, not the language itself.
The entire question seems to depend on the following. Can you elaborate? &gt; The project has the restriction that we disallow derivation of ToJSON / FromJSON as we don't want JSON out the wild to no longer be valid if we change our internal naming
I am also kinda missing the papers I could read in haskell to understand the intentions/tradeoffs behind a language feature. To work with rows purescript has `Union`, `RowCons`, `RowToList` and `ListToRow` for instance. I am honestly not sure how to merge two records while checking against overlapping fields and without breaking type inference - pretty sure it involves somewhat nasty typeclass prolog and unsafe ffi. If things don't match up with my intuitions that leaves reading the compiler source and commit messages.
But it is one solution.
Make this function: getFood :: Meal -&gt; [Food] getFood (Meal xs) = xs And then amountOfFood :: Meal -&gt; Int amountOfFood dinner = length (getFood dinner)
The really grating thing is how they have special-cased type variables (!) for Ord, Eq, and a combination of the two. IIRC it looks something like sort :: List comparable -&gt; List comparable Type variables!
I've been working w/ one of these guys for almost a year on several projects and I know him in person. I can't say if the business model of `Dirt Cheap Haskell` will work, but what I really know is that he is a Haskell wizard and I can highly recommend to work with him. 
&gt; Move fast but don’t break things :) Oh snap.
So there’s no way to do it in one expression? I have to write a separate function?
You can define your type as a record, that way you get the `getFood` function for free: newtype Meal = Meal { getFood :: [Food] } That creates the function `getFood :: Meal -&gt; [Food]`, and you can make new `Meal`s the same way you're used to: dinner :: Meal dinner = Meal [Chicken, Lettuce] 
Probably this talk by jgm: [slides](http://johnmacfarlane.net/BayHac2014/#/); [video](https://youtu.be/6TBpB-BEiIg)
Why would someone care about Hackage votes? 
Side question: On their website, on the "Tools" page they recommend a £25 closed-source IDE just for macOS, and list Sublime and Atom as alternative text editors. This seems out of place considering Vim+Haskell plugins is free, open source and FAR less resource heavy than Atom or Sublime, and compatible with most *unix systems. Is there a reason for this?
Yes!
Quoting a chunk of [my comment on the PR](https://github.com/ghc-proposals/ghc-proposals/pull/87#issuecomment-340710255): &gt; It's curious to me that, in a way, we already have this for case statements, do-notation (without curlies), value / type applications, pattern constructor arguments, etc, because they are delimited by whitespace, and it is allowed to be trailing and leading. So, I think it would be uniform for all sequence like things to allow trailing and leading delimiters
If that’s a function, getMeal::Meal, will that map over a list of meals? map (getMeal::) [Meal [Chicken, Chicken], Meal [Chicken, Lettuce]] [[Chicken, Chicken], [Chicken, Lettuce]]
Can't really say, maybe they consider the IDE worth it. You could ask one of the authors in the #haskell-beginners IRC channel. There is also emacs, which seems to have better haskell tooling than vim (though I use vim, so others might be able to go more into detail).
&gt; Does the fusion get in the way of something else, or is it just not paying its way? Well, for one, stream fusion adds a level of complexity that needs to be justified, and in fact there's been a quite scary and non-obvious bug that's been hiding in `text` since `text-1.1.0.1` and was discovered only recently, see [Text#197](https://github.com/haskell/text/issues/197) for details. Moreover, the authors of `bytestring` researched stream fusion (c.f. [Stream Fusion: From Lists to Streams to Nothing At All](http://fun.cs.tufts.edu/stream-fusion.pdf)) but ultimately it didn't end up being used in `bytestring` because there appears to be too little fusion potential the way `ByteString`s are typically used (how often do you `map` and `filter` over `ByteString`s?) . And the suspicion is growing recently that this may also be the case for `Text` and that we may open up other optimization opportunities by dropping fusion that may outweigh the benefits of fusion, but we need actual data for non-microbenchmarks to evaluate this theory... that's what `text-utf8` is all about.
Almost! map getFood [Meal [Chicken, Chicken], Meal [Chicken, Lettuce]] Will do the trick. But you can try all of this out in ghci - it responds much faster than I do. :)
Oh, I will. Just away from my computer atm. Still thinking about it though. :)
I'm worried that this project might drop UTF-8-based text onto us like a bombshell as a *fait accompli*, knocking Haskell back to the primitive western-centric world of twenty years ago in one fell swoop. The post states as if it were a fact: &gt; Using UTF-8 instead of UTF-16 is a good idea... most people will agree that UTF-8 is probably the most popular encoding right now... This is not a matter of people's opinions, and it is almost certainly false. As a professional in a company where we spend our days working on large-scale content created by the world's largest enterprise companies, I can attest to the fact that most content in CJK languages is *not* UTF-8. And a large proportion of the world's content is in CJK languages. It could make sense to have a UTF-8-based option for people who happen to work mostly in languages whose glyphs are represented with two characters or less in UTF-8. But throwing away our current more i18n-friendly approach is not a decision that can be taken lightly or behind closed doors. EDIT: The text-utf8 project is now linked in the post, but to an anonymous github project. EDIT2: Now there are two people in the project. Thanks! Hope to hear more about progress on this project and its plans.
&gt; &gt; Does the fusion get in the way of something else, or is it just not paying its way? &gt; &gt; Well, for one, stream fusion adds a level of complexity that needs to be justified [Michael Snoyman suggested](https://www.yesodweb.com/blog/2016/02/first-class-stream-fusion) that instead of composing the "non-streaming" version of functions (`f`, `g` in [/u/jaspervdj's comment](https://www.reddit.com/r/haskell/comments/79oyu1/short_bytestring_and_text/dp4ssp8/)) that we just compose the streaming versions `f'` and `g'` by hand, i.e. be more honest with the types. 
It is. It's a shame it currently needs a forked brick.
&gt; ### Other Perks &gt; * Exposed brick in the office For some reason this makes me think of Salad Fingers.
No, unfortunately I am not aware of a proper writeup around that topic. I gathered my knowledge from directly bothering heinrich apfelmus, the reactive-banana maintainer (and the reflex implementation is sufficiently close in spirit). But I agree that it should be addressed, as it naturally will be confusing when learning reflex/FRP usage in general. I talked to Dave Laing who has more in this direction planned as part of the ongoing post series, so I am optimistic that this situation will improve :)
Ah, in that case, ask away :D
&gt; Elm simply fails to offer you enough tools for abstraction. You quickly get stuck when trying to encode the essential complexity of your problem domain if you ever try to do anything other than first-order content. It is lacking in essential complexity. Yikes. How did we get to the point where proponents of *(purely functional ML-family typed language that compiles to JavaScript)* are tearing down *(different purely functional ML-family typed language that compiles to JavaScript)* while vanilla JavaScript takes over the world? Plenty of people haven't felt that Elm "failed to offer enough tools for abstraction" or that they "got stuck" when using it to build applications at their jobs. For example: 1. https://www.pivotaltracker.com/blog/Elm-pivotal-tracker/ - loving using Elm at Pivotal Tracker, where it "proved itself in the fullest way possible." 2. https://robots.thoughtbot.com/elm-native-ui-in-production - loving using Elm at Thoughtbot 3. https://www.youtube.com/watch?v=LZj_1qVURL0 - loving using Elm at Culture Amp 4. https://www.youtube.com/watch?v=D740qUZVcr4 - loving using Elm at TestDouble 5. https://charukiewi.cz/posts/elm/ - loving using Elm at Roompact Where I work we're at 200,000 lines of Elm in production, deployed for 2 years, students answering millions of questions on it daily, and we've still had zero production runtime exceptions ever. Adopting Elm has been an overwhelmingly positive experience, and without a doubt the best technical decision we ever made as a company. Elm is a great language. PureScript is a great language. Of course different people are going to have preferences for one or the other; that's programming. We can enjoy the tools we love without being acrimonious towards the tools other people love. As I see it, as ML family aficionados we're all on the same team here. :)
I agree. It comes down to who is willing to pay for API maintenance. And while I can understand that /u/jtdaugherty does not want to become maintainer of this additional non-trivial functionality, I stand by [my reasoning in this pull request](https://github.com/jtdaugherty/brick/pull/85) - without using internals, I'd have to copy or reimplement even more functionality. Of course exposing internals for this purpose is close to expanding the (to-be-maintained) API too, but I'd be fine with Interal being treated as Internal in the irrelevant-for-PVP-sense. I'd rather have to put tight bounds on the dependency than having to depend on a fork. Or are there other downsides to exposing internals?
"While Haskell is steadily gaining mainstream adoption in the industry, it still remains one of the most viable languages used as a teaching medium." Interesting start to this blog, but as time goes on I'm seeing more engineers that used Haskell whilst studying, starting up commercial projects in the language, be it in their own startups or introducing it to big multinationals! Huge credit to Abhiroop Sarkar for the blog! 
I think the idea is that while the _users_ of Hackett might one day want Hackett+dependent types, Hackett lives in an ecosystem in which the solution to such desires isn't to grow the language, but to create a new language, say "Dackett". In addition to abolishing the namespace distinction, Dackett would be free to add many other language-wide features which fit well with dependent types, such as a termination checker, universe stratification (`Set1`, `Set2`, ...), proof tactics, etc.
There *is* a link in the post and it links to https://github.com/text-utf8.
Would appreciate your thoughts on whether such approach to teaching Haskell is beneficial and such a series would be welcome or it’s a wrong track. Plus, any ideas and suggestions!
Exactly what I thought. How about adding a comment to those types, or keep the definitions in a package, and have the scm system require extra review on any edit to that package?
I think it would be more accurate to say the dhall avoids the issue by not users write pattern matches. All of the pattern matching is done by calling eliminator functions (I'm not sure what the technical name is for a function like `maybe` or `either`). More succinctly put, dhall avoids the issue by not exposing data constructors to users at all.
When we produce JSON of this nature, the format is set in stone as we write it to disk and it can potentially live forever. If we start basing JSON on internal symbol names then we lose some refactorability. It's also difficult for new starters on the project (who are usually new to Haskell as well) to grasp what the implications of changing constructors and field names would be. Generally we prefer to be explicit and to encourage refactoring as it's one of the reasons we use Haskell in the first place.
I think it makes perfect sense that brick-reflex is a separate package, but it would be nice if brick exposed enough to implement brick-reflex without needing 'bricki'. I agree with you that it's reasonable to expose internals without committing to it being part of the API, extensions like brick-reflex have a different relationship to the dependency than normal library users. Exposing an internals module does seem to be the most widely used way round that; the alternative of not allowing these sort of extensions at all (and ending up with forks/unnecessary alternatives) seems more painful by comparison. The nature of Haskell's module system doesn't seem to offer us anything better.
&gt; there’s less need for the bureaucracy of rigorous scrum/agile practices The irony is strong with this one.
I don't know how Functional Works, uh, works, but this is a repost of u/abhir00p's post from yesterday. https://www.reddit.com/r/haskell/comments/79kbog/persistent_red_black_trees_in_haskell/ https://abhiroop.github.io/Haskell-Red-Black-Tree/
You seem to be critical of ghci as a learning tool. I'm a Haskell learner and I found it very useful to evaluate expressions and types for different functions. It's also ubiquitous in all sorts of educational material: from LearnYouAHaskell, to RealWorldHaskell and the HaskellBook. Would you mind elaborating why you don't like it? I think REPLs are nice sandboxes that help us to get acquainted with the new constructs of the language.
One of the goals of HPFP was to make it as accessible to people new to programming as possible without sacrificing quality. Whether they achieved this goal is hard to say, but recommendations to use Sublime or Atom are for those, who haven't used text editors for programming at all (or use crappy ones, like SciTE or Gedit). I personally use Emacs for everything, including Haskell, and I believe it's as good as it gets.
I think that would be a great way of doing it. I’ve always been annoyed by the “in the OO world” examples, and find them not at all useful.
Thanks, fixed.
Well that's what threw me. Clearly someone wanting to learn Haskell isn't going to be completely new to programming, and wants to learn to do things the "right" way, which to me would mean learning a powerful editor. Edit: I was wrong.
This might be the perfect spot for Backpack to shine.
&gt;As someone who used to ask questions on IRC and StackOverflow: people tend to get great answers to beginner questions and to “interesting” questions, but a lot of real-world programming consists of very boring questions that are either answered seldom or require a good amount of luck (e.g. you might have a problem with `lens` and stumble upon Edward Kmett, or you might have a problem with `lens` but everyone will ignore you because there's some discussion about “what is a monad” going on on IRC and suddenly nobody cares about your lenses). I'm going to completely hijack this thread to share an "intermediate Haskell" question I've been trying to answer for *five years*. Basically: there is a concept of **mutable dictionary with effects** that seems to be highly recurring in production code. Something like the following, for a monad `M`, key type `K`, value type `V`: put :: k -&gt; (Maybe v) -&gt; M () get :: k -&gt; M (Maybe v) This comes out all the time when dealing with unstructured remote resources, whether REST endpoints, NoSQL databases, etc. Now, this is such a common pattern that I'm sure it (or some close generalization thereof) has a name in Haskell-land. I'd like to know what that name is, and what abstractions are closely related. I want to know this because I'd like to see what kind of magic other people are doing with this, and I'd like to draw my colleagues' attention towards this pattern. I'm at the point where I'm seriously considering paying that $100 just to get peace of mind on the topic. 
In recent versions of Ubuntu, I haven't been able to get audio capture to work at all. It used to be so easy; just use sox on the command line. Programmatically, you could either shell out to sox, or do the same thing directly using libsox. Now audio is wrapped in so many layers of complexity - sox, alsa, jack, pulse, etc.
Thank you for pointing it out and I apologize for not being clear enough. I do not have anything against ghci per se, you are absolutely right, it's an extremely useful and powerful tool to inspect your program internals. My point was rather you shouldn't *start* there, as it focuses on the leaves rather than the forest - need a clear system and the foundation of typeclass hieararchy and types and type functions in one's brain *first*, and then you'll be able to use REPL and other tools efficiently without trying to resort to imperative thinking. Does that make sense?
It *seems* to be okay with /u/abhir00p since it credits him, links to his blog, and /u/pforteath gave a shoutout in the comment right below you. I'm not him, of course, so I don't know for sure.
I don't think GHCi necessarily enforces any kind of imperative programming. Honestly, if I wanted to teach Haskell without getting into anything imperative, writing pure functions and testing them in GHCi seems like by far the least imperative way to do it, since you can avoid the whole concept of `IO` and `putStrLn`/`print`.
Ah in a footnote, thanks.
&gt; need a clear system and the foundation of typeclass hieararchy and types and type functions in one's brain first I'm having a hard time reconciling this with "teaching Haskell to kids"; perhaps if it was "teaching Haskell to mathematically-inclined kids ages 15+"? Types are pretty intuitive to kids, functions are pretty intuitive to kids, but type-classes and type functions are going to be a bit trickier, especially Haskell's particular treatment of types vs values.
Good one :)
Sure, but then you are adding types and monads later on and people start reading monad tutorials and break their heads on monad stacks and arrows. Whereas if you teach them types and typeclasses and type functions and other high-level patterns *first*, and only after go to actual value level functions - you (just might) make their life easier when they *do* need to use monad stacks and arrows. That's the working theory at least :) 
Considering that the book is explicitly targeted at those who are both new to programming and only new to Haskell, it seems a bit weird to say that "clearly someone wanting to learn Haskell isn't going to be completely new to programming"; after all, one of the goals of this book is to be so approachable that I could, for example, recommend it to a friend of mine to have them teach themselves a first programming language self guided. So I don't really see the IDE suggestions as out of line in that respect; the OSX one is a bit odd, but not especially so.
It does not seem like a good idea to start with the least tangible features Haskell has to offer. That’s like starting math with peano numbers.
Right, but: - aren't they the least tangible only because we've made them such? :) - Math *does* start with Peano numbers. You start with an empty set, you make Natural numbers, then you get to Rational when you need a reverse operation to multiplication, then you get Irrational, then Real, then you abstract the algebras to Categories etc etc. The whole math building builds up from an empty set, beautiful and elegant. Vs the mess in the heads we tend to create after school in pretty much any country :) 
I see what you mean, but then where do we start? As an example, the HaskellBook begins by teaching simple concepts on the REPL: function application, expressions with operators, binding names to values (using let, which is arguably not very good since in the source code that's not how you do global bindings), lambdas, currying, point-free notation, etc. Types and typeclasses are also inspected first in the REPL, using the :type and :info commands on polymorphic functions and typeclasses respectively. I'm still a beginner in Haskell and FP in general, but I always think about how I would teach this language at university if I had the opportunity (I do it as an exercise in thought), and the first thing that comes to mind is "REPL". You think all these things I mentioned should be taught by writing code in .hs files? And only then introducing the REPL?
&gt; aren't they the least tangible only because we've made them such? No. They're the least tangible because they take the most effort to convert to something an end user can interact with. To understand type classes, you must *first* understand functions *and* types, giving it a necessary minimum amount of effort to learn. Peano numbers are the same thing; you must first understand sets, and you must first understand recursion to understand addition with them (and fwiw, only one interpretation of math starts with peano numbers. There are plenty of other ways to represent numbers). Point being, these aren't artificial barriers. I'm not suggesting that the way things are currently taught is optimal, but I'd be shocked if someone learns best by reading about a large hierarchy of features and how each of them works before ever actually putting the most basic one to the test.
I've been programming for a very long time and I use Visual Studio Code for Haskell nowadays. If you can deal with the resource requirements, modern editors like Atom and VSCode are actually quite flexible and powerful. VSCode in particular not only gives you a _great_ Vim mode, but it also provides a modern environment for writing extensions that is much better than what's on offer in Vim itself. I don't want to provoke an editor war here. I just want to disagree with the assertion that the authors are somehow steering students wrong. If they were telling people to learn Haskell _and_ Vim at the same time though, then yes, I think they'd be making a mistake—and I say this as someone who has been using the latter for well over a decade.
For comparison: [Alligator Eggs](http://worrydream.com/AlligatorEggs/)
This is amazing, thanks!