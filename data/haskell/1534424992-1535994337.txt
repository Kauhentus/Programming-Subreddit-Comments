Hmmm. If you want to know *how* this works, then it is not hard. The whole algorithm is 200 to 300 LOC, so you can just read my implementations ([JS](https://github.com/maiavictor/abstract-algorithm) / [Rust](https://github.com/maiavictor/absal-ex)). [Here](https://github.com/maiavictor/absal-ex) there are some notes explaining the format used for graphs and how it is reduced. In [this album](https://imgur.com/a/DP9rAgm) there is an image explaining how λ-terms are translated to graphs. If you want to know *why* this works, though, then I can't think in any simple explanation. You may be interested in [this](http://homepage.cs.uiowa.edu/~astump/screencasts/june-5-2018-optimal-beta-reduction-1.mp4) lecture by Aaron Stump, though.
This *is* an example!
So any “dumb n times recursion” could benefit from the optimal evaluator and get fractional complexity wrt lazy Haskell? Where’s the catch (and the answer to my second question)?
I am not sure "easy" is the right word. Integration does not play nicely with products or function composition and these are probably things you will have in your ADT.
agreed, when people don't put spaces around (.) it's always a source of immediate confusion also, @spirosboosalis, see this package: http://hackage.haskell.org/package/flow-1.0.14/docs/Flow.html#v:.-62-
Hey! I think you're correct, but I'm not sure. That's really interesting. Could you provide a Haskell implementation of that idea? To make it simpler, we can assume `n` is a power of two. So, for example: ``` times :: Int -&gt; (a -&gt; a) -&gt; (a -&gt; a) times 0 f = f times n f = let g = times (n - 1) f in g . g main :: IO () main = print (40 `times` not $ True) -- applies not 2^40 times ``` This is still exponential though, but I guess I'm missing something. 
Anyone who is troubled by this could also look at: * The smacmpilot work, linked above, is the project the article is taking about. It's 100% open source, you can judge it for yourself instead of through the sensationalized article. * Our [boundary policy][https://lifeatgalois.com/articles/the-boundary-policy/] described a bit about why and how we decide to accept work. We have and will continue to turn down work that doesn't for in the boundary policy.
I think we need to be able to compute the function resulting from a composition in constant time, which I'm guessing the optimal evaluator does. But I really don't understand it well enough. So here is an example for functions `Bool -&gt; Bool`. I chose that because I can write down the composition explicitly, so that getting a "normal form" of a composition takes constant time. ``` import Data.Bits (shiftR, testBit) -- Make it strict so we know laziness isn't contributing data BoolFun = BF !Bool !Bool instance Show BoolFun where show (BF t f) = "{ True |-&gt; " ++ show t ++ ", False |-&gt; " ++ show f ++ " }" bfId, bfNot :: BoolFun bfId = BF True False bfNot = BF False True -- Apply a boolean function bfEval :: BoolFun -&gt; Bool -&gt; Bool bfEval (BF t f) x = if x then t else f -- Compose two boolean functions compose :: BoolFun -&gt; BoolFun -&gt; BoolFun compose bf (BF t f) = BF (bfEval bf t) (bfEval bf f) -- iter n f computes the n-th iterate of f iter :: Integer -&gt; BoolFun -&gt; BoolFun iter 0 _ = bfId iter n f = go n f bfId where go 0 _ acc = acc go k g acc = let acc' = if testBit k 0 then compose acc g else acc in go (shiftR k 1) (compose g g) acc' ``` Using it in GHCI: ``` λ&gt; iter (13 ^ 100) bfNot { True |-&gt; False, False |-&gt; True } (0.01 secs, 558,024 bytes) λ&gt; iter (13 ^ 100 + 1) bfNot { True |-&gt; True, False |-&gt; False } (0.01 secs, 557,656 bytes) ```
I'm curious about this. I've worked with FP Scala for roughly 2 years since my first job and am transitioning to Haskell, would they be open to it?
I'm wondering if this sort of approach wouldn't be worthwhile not for evaluation, but for strictness analysis. The advantage here is that you could always punt- if evaluating some sub-graph takes too long (i.e. is likely an exponential case), just revert to non-strict semantics.
That makes a lot more sense. So it normalizes the lambda as fas it can without an argument to substitute in and shares that more normalized form?
We actually do have some Scala developers somewhere in the company, and there are some junior Haskellers. I think that would be a reasonable thing to try!
Oh! That's neat. I now understand what is going on with Absal. Church-numbers are represented as the equivalent of `λf. λx. let a = f . f in let b = a . a in let c = b . b in let d = c . c in ...`. Moreover, Absal is capable of computing the composition of intermediate `f . f`s before duplicating it further. Finally, `copy . add` just turns out to be a function that, when composed together, fuses (as in Haskell's short-cut fusion), which is why it is asymptotically faster. D'oh! I can write a post about that.
&gt; To make it clear: so does the cost of copying terms in absolutely any other evaluation strategy. Sure, but you would be counting that cost in another strategy because they would show up as reductions. Are you laundering complexity into the bookkeeping when you apply copy?
1. That [isn't actually the sieve](https://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf). 2. If you are asking about performance then make sure you are compiling instead of interpreting, using optimizations, using `Int` if possible (probably not for such large primes), and post the compiler version as well as expected time.
I felt guilty about posting it on here myself, especially after I got a sudden case of free time failure that stopped me from really working on it. I'd love to hear what other people think about it, because so far it seems like a change that's either self-evidently important or impossible to see the use of.
This has been around for some time, but I didn't know about it, and people are talking a lot about GHC plugins. There are some example modules in the [Hackage](http://hackage.haskell.org/package/sbvPlugin) page that show it being applied for proving correctness of merge sort and others.
Awesome work!! :) Side-note: Some media queries or a %width (with a max-width) might help with readability on mobile. Currently I have to zoom in and scroll from side to side on each line to read it.
&gt; Are you laundering complexity into the bookkeeping when you apply copy? Reading through that other thread, and particularly [this comment](https://www.reddit.com/r/haskell/comments/2zqtfk/why_isnt_anyone_talking_about_optimal_lambda/csommh8/), it sounds like the answer may be "Yes, but it's still an improvement." It'd be nice to have a metric that included the bookkeeping costs.
Why is remote restricted to the USA?
Type level \`Char\`s would be great!
As for your first point I thought I avoided that issue? Initially I was using mod, but I realised that this would slow it down so ended up rewriting it in the (much less elegant but hopefully faster) solution above. If you do think this is not a sieve, as defined by the paper you linked, can you please explain why because I am quite possibly just overlooking something. I'm pretty sure I was interpreting, so when I get home I'll fix that and report back the results.
This is great stuff! Well done!
It has them, but they aren’t working for some users. I’ll take a look soon.
I think I learnt about it from James Deikun. 
A pleasant read, even though I understood essentially nothing of the technical part. Now I have to look up the lens part to turn hand-wavy knowledge of lenses into a formal one - for five minutes.
genPos :: Gen Int genPos = abs &lt;$&gt; (arbitrary :: Gen Int) `suchThat` (&gt; 0) prop :: Gen [Int] prop = listOf1 genPos
This looks great! I've only used Stack but I'd like to give Cabal a try. Is there any "Cabal for Stack Users" tutorial or FAQ to get me started?
&gt; This means that we can view x as a possible proof and s as a possible counter-proof. The formula is true iff there exists a proof that ‘defeats’ every possible counter-proof. This is kinda the reverse of epsilon-delta [definition of limit](https://en.wikipedia.org/wiki/(%CE%B5,_%CE%B4)-definition_of_limit#Precise_statement_for_real_valued_functions), isn't it? There, for every challenge ϵ, there is a response δ that defeats it. (I might be talking nonsense.)
There's a `readthedocs`: https://cabal.readthedocs.io/en/latest/index.html
I just want a wrapper called *nucabal* where the new-* commands are the default ones.
That’s called Cabal 3.0.
So I'm a Portland-based postbac CS student eager to go all in with Haskell. An internship opportunity with Galois and having some mentorship would be invaluable. Is there someone I can serenade over a coffee? Floors I can mop? How can I optimize my learning effectively to be a competitive candidate? My spare time is maxed out between haskellbook, courses like CIS194, random hackerrank challenges, trying to build my own projects, and absorbing through osmosis whatever passes through the subreddit.
This is probably okay, as we can be reasonably sure the runtime will be able to detect this very simple deadlock scenario and will GC the blocked threads (raising the special, unrecoverable, \`BlockedIndefinitelyOnMVar\` exception in them). But I think your intuition is right to be suspicious of this pattern; the runtime cannot detect deadlocks in all cases, and certainly I don't have confidence I understand all the performance implications of this pattern under various GC settings, etc. So I would prefer using \`tryPutMVar\` here and let the threads exit naturally. But there's another issue which is that (I think) all the forked threads will complete which could be very wasteful if we don't care about the results of the other \`Async\`s. It would be interesting to look at the implementation in the actual \`async\` library (on hackage).
`suchThat` isn't super great from a performance perspective. You can just say ``` myProp list = let list' = map getPositive list in ... ```
Awesome! I'd definitely be curious to understand why self-compositions of `copy . add` fuse where `add` doesn't. Looking forward to the next post!
Yeah, the epsilon-delta definition is a [pi-formula](https://en.wikipedia.org/wiki/Arithmetical_hierarchy) since it has the structure (for all x, there exists y ...) and the dialectica interpretation gives you an epsilon-formula (since it's: there exists x, for all y ...). Pi_2 formulas (those that are: for all x, there exists y such that P, for quantifier-free P) are unbelievably common in math and logic, so much so that it was a little exciting to see usage of Epsilon_2 formulas in the wild for me.
**Arithmetical hierarchy** In mathematical logic, the arithmetical hierarchy, arithmetic hierarchy or Kleene–Mostowski hierarchy classifies certain sets based on the complexity of formulas that define them. Any set that receives a classification is called arithmetical. The arithmetical hierarchy is important in recursion theory, effective descriptive set theory, and the study of formal theories such as Peano arithmetic. The Tarski–Kuratowski algorithm provides an easy way to get an upper bound on the classifications assigned to a formula and the set it defines. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I wrote a beginner-friendly tutorial about building projects with `cabal` and `stack`. You can compare how the same workflow can be done with both `cabal` and `stack`. * https://kowainik.github.io/posts/2018-06-21-haskell-build-tools.html
SBV (and -plugins) can state richer properties about a smaller set of functions (notably: those that have no dynamic structure). For example: Do you have a dynamically sized linked list? Forget it, use liquidhaskell. Do you want to prove your decryption algorithm is the inverse of the encryption? Sure, SBV is brilliant for that.
Yes, see my new post!
I'm not disappointing you. See my new post! 
You could rephrase your constraint as p^2 == 2 * q^2 and avoid talking about real numbers at all. This problem will be in the nonlinear integer arithmetic theory of whatever SMT solver you use to try to dispatch it, so your mileage may vary as far as showing UNSAT goes.
Perhaps is personal taste, but I think a better ui design would have been: cabal {--old | --new} build cabal {--v1 | --v2} build
I'd love to participate!
https://github.com/aisamanra/cab
Thanks for compiling those, I always love going through them.
`Positive` uses `suchThat` in the `arbitrary` instance: ``` instance (Num a, Ord a, Arbitrary a) =&gt; Arbitrary (Positive a) where arbitrary = ((Positive . abs) `fmap` (arbitrary `suchThat` (/= 0))) `suchThat` gt0 where gt0 (Positive x) = x &gt; 0 ``` Would the performance be different still?
Nice work! I've always enjoyed reading your stuff about the abstract algorithm and other optimal evaluation strategies. I'd love to get better acquainted with it's weaknesses and failings because it still sounds a little too good to be true for me (especially the parallel reduction abilities). But who knows? I've been looking around at what might qualify for being the best modern age mental model of computation for next generation hardware and languages and interaction nets seem exceedingly promising in an age of distributed computing and skyrocketing core/thread counts; basic lambda calculus and call by value doesn't give an intuition for performance, and the Turing or C-like models are all badly papered and leaky abstraction, but interaction nets seem to check quite a few boxes--not sure how "approachable" they are, though.
what level engineers are you looking for?
oh wow! I didn't know that. I believe that `suchThat` is implemented as a pre-filter on candidate values. I was so sure that the `Arbitrary (Positive a)` instance did something more clever than computing a random number and discarding it if it's less than zero, but it looks like I am wrong.
What is that idiom for?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tstat/markdown-nb/.../**Main.hs#L68** (master → faedab0)](https://github.com/tstat/markdown-nb/blob/faedab069544c2c7598e1b03f26f9184e7d6e04e/server/Main.hs#L68) ---- 
I can't reproduce with GHC HEAD. I was able to reproduce it with 8.4.3.
I also have to say that gcd(p, q) = 1. Otherwise (p, q) = (2n, n) works for any integer n. I do not know of a quantifier free way of saying gcd(p, q) = 1
Thanks, I did contact him. He suggested using \`sFromIntegral\`. And as he correctly pointed out, the default solver is not able to solve this. \`\`\` \*\*\* Exception: SBV: isVacuous: Solver returned unknown! CallStack (from HasCallStack): error, called at ./Data/SBV/Provers/Prover.hs:317:42 in sbv-7.10-49thk4vHRJ3B4GAvwivxDz:Data.SBV.Provers.Prover \`\`\`
What is the source code for `copy` and `inc`? 
You don’t have to assert that. There is a solution in reduced terms if and only if there is a solution in non-reduced terms. Also, if you plug in (2n,n) for (p,q) you’ll see it’s clearly not a solution.
IIRC he showed it in his previous blog post, where he first discussed this "negative complexity"
There's obviously a lot of power in evaluation strategies, but to me the part that makes me so nervous is that I no longer have any intuition for how my code will perform. Even an evaluation model as tame as Haskell's already trips up virtually everyone who comes to the language. You have to understand thunks and forcing and strictness and memoisation, and even after you understand all that, it's still quite tricky to actually *think* in that space. So now imagine trying to reason about why your code is fast or slow in a far more exotic evaluator like the abstract algorithm. I'm sure just like in every other system there's good and bad ways to write programs for it (like `not_a` vs `not_b` in the article) just like there are for every computation system, but I'm not sure that reasoning in that space is something that's in the scope of my ability as a human to do. But yeah, we're certainly not very creative with our computation models. Turing completeness is practically everywhere -- even stuff like cellular automata are Turing complete! But how do you write quicksort in cellular automata? I would not even know where to start to even make that happen, much less how to reason about the performance of my program running in cellular automata. So yeah, there's a lot we (or at least I) don't know. Perhaps the most limiting factor of many computation models is not the mathematical properties of the models themselves, but whether our monkey brains can even reason inside of them.
Fair enough. This is what happened: ``` Prelude Data.SBV&gt; sat con Unknown. Reason: smt tactic failed to show goal to be sat/unsat (incomplete (theory arithmetic)) ```
Use your distribution only for stack. From there on, stack can deal with GHC and Haskell libraries. No need for docker stuff.
1) Code contributions for each week's homework from logistic regression onwards. 2) Once we've identified common patterns roll them up into a library sort of like what's happening in Util.hs 3) For the iPython notebook better write outs so the the notebook also acts as a mini chapter on the topic. 4) Refactors into idiomatic Haskell and cleaning up all the existing solutions. In general a fully fleshed out use case of Haskell for some data science tasks but the Coursera class was a good place to start.
I can reproduce. Could you create the ticket?
Scissors! ✌ I win
It would certainly make it easier to create a wrapper script. cabal --new $@
Wtf. Bad bot
I would love to see this make use of the Haskell Automatic Differentiation package ("ad" on Cabal).
Don't use pacman to install stack. It pulls in ghc and all the dependent libraries - and will update them frequently. https://www.archlinux.org/packages/community/x86_64/stack/ is terrifying. It's on release 101! Just install using the instructions on haskell stack.org. As for the resource, u/lexilambda wrote https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/ which gives a lot of detail (but you did ask for comprehensive).
I am also doing the same course. I used Haskell [3] until the Neural Network stuff. Really had some fun/crazy/banging-my-head-on-the-wall time implementing back prop with gradient checking and all. I also ended up implementing a very crude version of tensorflow playground [2]. For the implementation fo algorithms, I used a data type to wrap hmatrix [1] matrices, which parametrized matrix dimensions. I think It really helped me sides step some of the potential bugs. But the back prop turned out to be pretty slow, and from then onwards, I have switched to python using tensorflow library. [1] http://hackage.haskell.org/package/hmatrix [2] https://playground.tensorflow.org [3] https://bitbucket.org/sras/linearregression/src/master/src/
`Int` is for sure fine for that problem, you aren't going to be hitting `2 ^ 31 - 1`, plus on most systems you get all the way up to `2 ^ 63 - 1` anyway.
Rather than `curl ... | sh`, more secure way to install the newest `stack` would be install `stack` with `pacman` and then upgrade it. (Then you can remove system Haskell packages).
Assuming you want to write the minimal amount of non-trivial code you just need something like: instance Functor Foo where fmap = liftA instance Applicative Foo where (&lt;*&gt;) = ap pure = ... instance Monad Foo where f &gt;&gt;= x = ... Everything else (`liftM#`, `return`, `&gt;&gt;` etc.) is not needed.
We don’t have a specified requirement, but at least to the level that you’d be comfortable making a basic web app over a two week timespan to learn about it. I’d say roughly college graduate or two years experience, but we’re open to non-traditional backgrounds (e.g. a mathematics background is common among Haskellers, instead of a software engineering one)
If an overflow is the cause, is using some constant smaller than maxBound a viable workaround?
Great stuff! I would like to add to the explanation why this "composition by sharing" does not work in Haskell: Haskell does *evaluation*, not *reduction*. This means it never evaluates under lambdas. So in your example -- Step 2 let f2 = not . not in let f4 = f2 . f2 in let f8 = f4 . f4 in f8 True -- Step 3 let f2 = id in let f4 = f2 . f2 in let f8 = f4 . f4 in f8 True the problem is not that the the arguments to `(.)` are not evaluated eagerly. The problem is that `not . not` evaluates to `\x -&gt; not (not x)` and no further. It is in weak head normal form. To evaluate `not . not` to `id` we would have to reduce under lambdas at runtime which no language that I know of does.
One thing I didn't understand about Conal's idea to convert into cartesian category form is that it seems the matrices you get will be very sparse and cause an unnecessary slowdown compared to more traditional approaches to AD. Does anyone have any thoughts about that?
I tweaked it to the below, which works well but is annoying when the system defaults to `IO Assignment` for the `MonadPlus`. One difference is that I don't bother to tweak the problem, I rely entirely on the call to `propagate` to do that. import Control.Monad type Literal = Int -- var num, negative for negated type Clause = [Literal] type Problem = [Clause] example :: Problem example = [[1,2],[-2,3]] -- x^y &amp; ~y^z type Assignment = [Literal] propagate :: Literal -&gt; Problem -&gt; Problem propagate l p = [ filter (/= negate l) c | c &lt;- p, l `notElem` c ] -- solve :: Problem -&gt; Maybe Assignment solve :: MonadPlus m =&gt; Problem -&gt; m Assignment solve [] = return [] solve ([]:_) = mzero solve (p@((l:_):_)) = msum (try &lt;$&gt; [l,-l]) where try v = (v:) &lt;$&gt; solve (propagate v p)
You can also try using [https://aur.archlinux.org/packages/stack-bin/](https://aur.archlinux.org/packages/stack-bin/), it seems to be working okay for me.
Why does `copy` and `suc` use `n : Bits` for self reference, instead of `Y` combinator?
[removed]
(Hopefully by studying interaction combinators we can achieve higher understanding on how it works and why and when it is fast, allowing us to make front-end languages that are easier to reason about.)
https://gist.github.com/MaiaVictor/a2c67dd3776592d3d432c705e8d77ec9
What's the command that results in the error?
You should definitely know the concept of imperative languages like C. C vs Haskell is like turing machines vs lambda calculus or state vs stateless. Haskell is nice when you want to reason about the result and C is nice if you want to reason about the computation itself. OOP is just a practical extension to the imperative model which gives you encapsulation and inheritance. From a theoretical point of view it is really not that interesting, from a practical point of view it can be really cool for GUI programming (widgets are built from widgets which are used to build other widgets etc, every widget has a state).
Nice video as always. Newbie question : your solution for the second problem seems to read the list over and over (at least two times since the record function is called 2 times but I'm guessing those call to inits and length do it as well). Is it a problem in Haskell? I'm having trouble wrapping my head around lazyness and time complexity 
That's a good question! Thanks! I was waiting for it. :) I decided to go with that solution, because it kinda reminds me of working with partial sums https://en.wikipedia.org/wiki/Series_(mathematics) and I like how mathy it feels. But yeah, I understand the performance concern. And in that particular situation it could be improved for sure. I think we could try to utilize Haskell's memoization https://wiki.haskell.org/Memoization and make memoized versions of `maximum` and `minimum`. Because intuitively I can see that for example the value of `maximum [1, 2, 3, 4]` depends on `maximum [2, 3, 4]` and if it's already memoized the calculation of `maximum [1, 2, 3, 4]` is just O(1), but I don't have enough knowledge on the topic of memoization in Haskell. I'll research a little bit more and probably bring it up in the future videos. :) 
Do you think that a strong background in OOP is required in order to get hired to do functional programming? or can you "skip" it, and just know what you learned during university.
Most probably very few people did this, but those that did, they are extremely good engineers nowadays. In general I'd say, you don't need to do bad things in order to do good things, if you know why you're skipping bad things. And you seems like you do :)
It is nice to know about OOP, but Category Theory is far more useful. OOP is something that you can learn on the go.
The issue was not meant to be this one but rather with the same input code. Perhaps it should be tested somehow in GHC testsuite. w.r.t. to weakref, right. You make some kind of ptr which stops BlockedIndef exception I really wish I remembered why do it this way. It may well be because of the bug OP pointed out... @qnikst do you remember, I think you showed this to me.
Yes, I think I didn't express myself properly. I meant it in this way: usually, companies are a lot happier if they see that you worked (even at McDonald's, or was a waiter) during a summer than if they saw that you did absolutely nothing. But not just that, because that would imply me consider OOP to be "just experience". I wanted to say something along the lines of: is OOP more than "just experience" on your CV and is it some sort of required step, some sort of prerequisite experience before someone trusts you with an FP job?
&gt; OOP is just a practical extension to the imperative model which gives you encapsulation and inheritance. I'd argue it is an *impractical* extension 😝. Modules/abstract types give you encapsulation. Various forms of polymorphism give you different kinds of code reuse. Objects often lump ALL these together into one big confusion.
https://aur.archlinux.org/packages/stack-static/ also works.
If you can get a job doing Haskell go for it. The language has its warts but it's so much better than anything else that I can't find a good reason not to use it. You'll be wasting your precious time on this Earth learning OOP/imperative languages. The future is functional programming. That being said we all have to be pragmatic and the industry presently caters to our collective laziness and hubris. Vast sums of money, time, and effort are spent making it easier to be a Java, .NET, or C++ programmer these days. There are a plethora of frameworks available for these languages. If your plan is to find an SME in your area writing run-of-the-mill business applications then you can get a pretty decent career going with .NET. Press a few buttons, change a few names of things, compile and go. Add a few text files and now you have a mobile app for all the major platforms. Go sip mojitos at TJI Fridays. Learn how to golf. It's harder to build a career in Haskell. Most SMEs won't choose Haskell because there are a lot of myths surrounding it. It's not impossible though. The way I see it there are three main options: 1. Find an opening at one of the few companies using Haskell. You will probably have to relocate or get lucky finding a remote position. This is the least likely way for most people to get a job in Haskell. 2. Get a job programming in something else and introduce Haskell later on. This is a tough sell. You'll have many co-workers who would rather eat sawdust than learn Haskell. Many of the myths surrounding Haskell are perpetuated by these people. Introducing a new language and tooling is seen as expensive or risky for a business entrenched in another ecosystem. But if you can put yourself in a position of trust on your team, attain a leadership or management role, and learn how to be an effective teacher you might find a way to bring Haskell into another company. This is hard but awesome. Also the most likely way to get a Haskell job. 3. Start your own company and use Haskell. This is incredibly hard. Depending on where you live and your situation it might be easy to get started but the odds being what they are that your fledgling enterprise will still be around in a year aren't great. Consulting is a small pond because most SMEs aren't using Haskell and there are several established Haskell consulting companies already. Product companies are risky and burn a lot of money up front until they prove themselves in the market. You could be a freelancer. Your mileage will vary.
Very nice! The first graph you show also provides a lot of evidence for this explanation. Compare it with this one that I just generated, of the graph of `y = x` and the graph of `y = popcnt(x) + log2(x)`, where `popcnt` counts the number of 1 bits (aka Hamming weight). So it sure looks like `not_b` does a chunk of work for every bit position in the binary representation of `n`, plus an additional chunk whenever the bit is `1`. https://imgur.com/a/pu4T7X2
&gt; https://imgur.com/a/pu4T7X2 Woa, that's very cool! Indeed looks the same.
Huh, I think I'd use something like `async action &gt;&gt;= await` for that.
Can I see what the snippet I linked would look like using `async` + `await`? I'm unclear on what the `action` is.
`async (fireNewClient (pconn, tid)) &gt;&gt;= wait`
This won't work, `fireNewClient (pconn, tid)` returns immediately.
I suppose it's forking a thread on it's own then -- I'd have it return the `Async` of the forked thread and then `wait` on it.
Sorry to ask again, can you clarify what you mean with code?
I have terrible internet access where I am currently so I didnt see the replies.
 fireNewClient :: (PConn, ThreadId) -&gt; IO (Async a) res &lt;- firewNewClient (pconn, tid) wait res Honestly, if all you're doing is launching a thread and then spinning another thread to wait on that one, why not just do the computation inline? Why fork a thread at all?
`fireNewClient` doesn't fork a new thread, it just fires the pending connection into an FRP network and returns immediately. Then what? Sleep forever :)
No, it's async. Threads that are found to be blocked on MVar are resurrected by [`resurrectThreads`](https://github.com/ghc/ghc/blob/63b6a1d44849c479d2a7cb59211f5c64d133bc62/rts/Schedule.c#L3019), which for threads blocked on MVars raises an async exception via [`throwToSingleThreaded`](https://github.com/ghc/ghc/blob/63b6a1d44849c479d2a7cb59211f5c64d133bc62/rts/RaiseAsync.c#L66). From a user's perspective, this exception is only raised when the thread is blocked on an MVar operation like `readMVar`, so you could think of it as `readMVar` throwing an exception, but implementation-wise it's async.
It's a synchronous exception in the sense that it is not a sub-exception of `SomeAsyncException`. Behold: &gt; asyncExceptionFromException (toException BlockedIndefinitelyOnMVar ) :: Maybe BlockedIndefinitelyOnMVar Nothing
You're gonna have to constrain the type of x enough for this to be a useful question at all.
&gt; I got a Haskell job as my first industry position What was your background before this and what did you do with Haskell at your job? I'm interested in hearing about how someone manages to get this as a first job after academia.
It is quite possible to use the arch provided ghc and haskell- packages, see [my comment](https://www.reddit.com/r/haskell/comments/8vu73f/haskell_on_arch/e1vy28x/) in a similar thread a while ago.
For anybody that is in any environment that isn't Windows: when in doubt, take your tools from its site and use them. Compile them if you need. That's guaranteed to get you a standard environment that will only have mainstream flaws. For anybody having problems developing in Windows: stop doing that. That applies to any language that is not a .Net one.
This was my main problem years ago. Now I do enjoy `stack`. Give it a try!
But acquiring it later is essential. Really. 
This is a difficult question in fact because you have many different choices. And depending on your sensibilities and preferences, one choice that is quite good for some will feel terrible for you. I think `stack` is the easiest to start with. It will make a lot of work for you. Still here are your choices: 1. for standalone executable, you can make haskell scripts (a single file mostly). For that you have two choices. With stack: ~~~ &gt; cat hello_world.hs #!/usr/bin/env stack {- stack script --resolver lts-11.6 --install-ghc --package protolude -} {-# LANGUAGE NoImplicitPrelude #-} {-# LANGUAGE OverloadedStrings #-} import Protolude main = putText "Hello, world!" ~~~ with [`nix`](https://nixos.org/nix/) ~~~ ! /usr/bin/env nix-shell #! nix-shell -i runghc #! nix-shell -p "ghc.withPackages (ps: [ ps.protolude ])" #! nix-shell -I nixpkgs="https://github.com/NixOS/nixpkgs/archive/16d475334409f7fa632929b2838421b4ffe34927.tar.gz" import Protolude main = putText "Hello, world!" ~~~ 2. for projects. To my knowledge you have three choices: - cabal [using the nix-style local build](https://www.haskell.org/cabal/users-guide/nix-local-build.html) - stack using a nice package template (for example you can use generic ones or an opinionated one like mine with `stack new myproject https://git.io/vbpej` where this links to a single template file that will generate a project directory) - nix for which I think a great guide is [Nix and Haskell in production](https://github.com/Gabriel439/haskell-nix) I personally use mostly stack and I think this is the best one for beginner. Still knowing the alternative doesn't really hurt. I think that just for learning purpose, if you don't want to really create a full library or project, just playing with haskell scripts should be good enough.
95: f=filter;s[]=[[]];s(c:p)=do(l:c)&lt;-[c];(l,p)&lt;-[(l,p),(-l,c:p)];(l:)&lt;$&gt;s(f(/=0-l)&lt;$&gt;f(all(/=l))p)
I've been learning Haskell for a few days now. There is a simple question I'm confused on. For the list comprehension implementation of a Fibonacci sequence how come the first of the examples below works, but the second does not. fib = 0:1:[a + b | (a, b) &lt;- zip fib (tail fib)] fib = 0:1:[a + b | a &lt;- fib, b &lt;- (tail fib)] 
Unless someone it MitM'ing you, or compromised haskellstack.org server.
How would `stack upgrade` be better in that case?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [commercialhaskell/stack/.../**Upgrade.hs** (master → 81963c5)](https://github.com/commercialhaskell/stack/blob/81963c5e365d40ada32dbfd65581aa4325e8b68d/src/Stack/Upgrade.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e4dkmfn.)
You can make the style of your second example work by using the `-XParallelListComp` extension. Put either {-# LANGUAGE ParallelListComp #-} at the top of your source file, or type :set -XParallelListComp in GHCi. Then you can write your second example as fib = 0:1:[a + b | a &lt;- fib | b &lt;- tail fib]. This means the same thing as the first example now. Generally, in [f x y | x &lt;- xs, y &lt;- ys] you basically have something like two nested loops, the inner loop traverses `ys` *for each `x` in `xs`*. Whereas in [f x y | x &lt;- xs | y &lt;- ys] you have a linear algorithm that traverses both `xs` and `ys` at the same time. The first example with `zip` is just the low-tech solution that doesn't rely on a language extension, but does the same thing.
I think you sufficiently answered the question. Bravo.
What does SAT refer to here?
Sat is just short for SATISFIABILITY, as in "boolean satisfiability problem".
Does someone have a lenses for people who just read LYaH article? This is way more math than I'm prepped for.
It's mostly discarding 0 and taking the `abs` otherwise, for reasonable Num instances. The `gt0` check is mostly to catch the corner case of `min_int` with fixed-size signed integers.
No. This is limited almost entirely by your will, capacity to learn, and ability to market yourself as a proficient individual. It has nothing to do with the language you used at your first job after college. Haskell is great but it's definitely not some magic career boosting elixir. 
Background: I have used Haskell before for a school project but we never got to use any fancy features. Most functions were pure functions, and occasionally we used IO monad. Question: I am learning about all different monads in standard library and the transformers. It seems to me that transformers is a good way to abstract out all the monads used in a certain part of the codebase and expose a monad-less interface to the user. Is that true? Follow up: Since IO monad doesn't have a transformer, does it mean any user of a function returning \`IO a\` will have to be aware of IO monad? (thus the whole codebase can end up being aware of IO monad and constantly unwrap/wrap values?)
&gt;it's the people, and Haskell doesn't help here. Is that really true though? Although I don't have a lot of experience, I've found so far that the communities of Haskell/OCaml/Rust and C++ are all very different. Of course, at your job, it isn't "the community" that's relevant but "your team" itself, but reading the "Haskell doesn't help here" just felt strange to me. Could you expand on that?
&gt; Do what you want to do not some arbitrary penance. Totally stealing this quote. :P
I think the reason this isn't the "real" sieve is discussed in the paper. `removeEvery` touches every single list element; it doesn't get too leap over all the irrelevant ones. So it doesn't get much cheaper as it works with larger and larger primes; its price stays almost the same from one iteration to the next. You can solve this by using a mutable array, or somewhat less efficiently but much more functionally using the techniques O'Neill describes.
As someone who hires software engineers, having Haskell experience is a huge plus, even if you go on to do other things in other technologies there’s a good chance the Haskell experience will set you up really well to do those jobs really well too. I have JavaScript engineers who have Haskell (and OCaml) experience and they bring so much to the team, even if they do keep trying to rewrite everything in ReasonML.
I think the point is that when it comes to your job, depending on the nature and size of the company, there are going to be a lot of people that don’t care about what technology you use - they just want to see results. Haskell might help you solve some domain challenges in creative, efficient, and easily refactorable ways, but at the end of the day you are almost always solving *people problems* and generally have to interact with *people* to get things done. Disclaimer: I manage a team of Haskell developers. My least favorite part of my job is unfortunately the most important part, the people interaction.
We are now going to see if us Haskellers, en masse, possess a recursive sense of humour.
This is a highly subjective topic, and I'm not too experienced (4 years since college), but here is my opinion on this: - First, I don't know if OOP is such a basic/important skill that everyone needs to master. After my first two years in college I only worked with Haskell and OCaml (except an 8-month Java gig) and in my last year I got a part-time Haskell job, and since then I'm working with Haskell full-time. My OOP side is very weak, but I don't think that's causing me any problems when I'm working with Haskell. Note that I also have quite a bit of C experience (and wrote a few kloc of Rust in last two years) so I can work on low-level code too, which is in my experience something that you'll need more often than OOP (which I literally never needed). - Secondly, IMHO being hired by Google does not make his opinions that much stronger. Surely the hiring process is challenging and they must be a capable person, but Google's needs may be quite different than stuff you'll need to work on things that _you_ like. Figure out what domain you like, and see what languages are used in that domain. - I don't know why should you learn OOP first and then master Haskell, as opposed to first mastering Haskell and then learning OOP if needed (e.g. when applying to a job). - I think one of the things you can do when graduating that you cannot do (at least as easily) afterwards is to land on a job that doesn't expect you to be an expert in the language you'll be working with. You may want to take this opportunity to land a job in a _domain_ that you're interested in, using whatever language they're using. I did this once and worked on a compiler written in Java. Using Java was tedious at times, but I really enjoyed that job and I'd easily take it again instead of a e.g. Haskell job that involves writing CRUD apps. Currently the biggest disadvantage I see in working with Haskell is, as others also mentioned a few times already, for on-site positions you have to be in US, UK, or Singapore (and a few other places like Berlin have a few companies), and remote work is sometimes not ideal especially when you're just graduating or you're inexperienced and need mentorship. Some companies even reject hiring an employee as remote unless they have enough experience.
Another thing that confuses me is the difference between `42` and `42b`, both have the same normal form, but `::copy 8 :::add 8 42 ex_8` requires twice as much rewrites compared to `::copy 8 :::add 8 42b ex_8`!
About 40% of a sense of humour, apparently.
I've always wanted to make a large project with Yesod but the Template Haskell is a bit intimidating. I usually end up going with Elm.
Oops, yes, you are right.
 upvote :: User -&gt; IO () main = upvote phischu
I would focus on challenging work and domains and really bright peers, then language. I think it's easier to break into Haskell, once you have domains in mind and experience within them, than it is to break into a complex domain with Haskell experience. That is, if it is a Haskell job doing advanced compiler stuff, that is awesome and will build you up. If it is Haskell CRUD, you may be lucky and have bright coworkers, but you will mostly be gaining familiarity with basic FP and Haskell syntax. You could be doing that at night with a little effort, probably don't need a Haskell job for that.
With the Fold-library it does. Even nicer. You compose folds with functor/applicative and they fuse to single pass folds. F.fold ((,) &lt;$&gt; F.minimum &lt;%&gt; F.maximum) With minimum and maximum being similar to `foldl min maxBound` from the library.
Bacrlays is hiring compiler engineers for domain-specific languages in London https://www.linkedin.com/jobs/view/haskell-quant-developer-markets-at-barclays-784673004/. The role is not really senior, but OTOH no previous commercial haskell experience is absolutely required to apply. The role is on-site, no remote. Cool stuff being worked on consists of: http://arbitrary.name/papers/fpf.pdf, http://www.timphilipwilliams.com/slides/HaskellAtBarclays.pdf and http://www.timphilipwilliams.com/slides/StructuralTypingForStructuredProducts.pdf.
Author here, do you mind if I steal this and give you credit? :)
I \`like\` this joke.
We didn't learn Functor and Applicative operations yet ;)
What about (IsProgrammer x) =&gt; ?
Why not? &gt; Should someone first go through OOP properly, i.e. work with Java, Python, etc. and gain a few years of experience before moving to the functional paradigm? No. &gt; A programmer told me that during an internship, and then he got a job at Google Lots of smart people in this industry say things which are false, and many people who are not actually smart find success.
You probably want a viewport meta tag (&lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt;), otherwise the browser will automatically scale the viewport.
Sure, that's fine.
42 is `λf. λx. f (f (f (f ... x)))`, while `42b` is obtained from `2 * 3 * 7`, which causes a few sharing nodes to be inside the normal form on the interaction net side (not reflected on λ-calculus), and `42b` is `2 + 8 + 32`.
There are a ton of getting started guides, the lens tutorial is a good starting poi t for instance. Lenses over tea is a great way to dig deeper though you probably want some intuition for basic lens usage first.
 instance Num a =&gt; Num (D a a) should be `Num (D a)`
I suggested the same thing over in [the relevant PR](https://github.com/haskell/cabal/pull/5358#issuecomment-396941968) but it appears it is getting ignored. Who needs long-term stability and compatibility anyways? &lt;/sarcasm&gt;
Well, I like it so much that I try to sneak the little I know in all my classes.
* [Versatile event correlation with algebraic effects](https://dl.acm.org/ft_gateway.cfm?id=3236762&amp;type=pdf) Oliver Bračevac, Nada Amin, Guido Salvaneschi, Sebastian Erdweg, Patrick Eugster, Mira Mezini * [Parametric polymorphism and operational improvement](https://dl.acm.org/ft_gateway.cfm?id=3236763&amp;type=pdf) Jennifer Hackett, Graham Hutton * [Handling delimited continuations with dependent types](https://dl.acm.org/ft_gateway.cfm?id=3236764&amp;type=pdf) Youyou Cong, Kenichi Asai * [The simple essence of automatic differentiation](https://dl.acm.org/ft_gateway.cfm?id=3236765&amp;type=pdf) Conal Elliott * [A spectrum of type soundness and performance](https://dl.acm.org/ft_gateway.cfm?id=3236766&amp;type=pdf) Ben Greenman, Matthias Felleisen * [Compositional soundness proofs of abstract interpreters](https://dl.acm.org/ft_gateway.cfm?id=3236767&amp;type=pdf) Sven Keidel, Casper Bach Poulsen, Sebastian Erdweg * [Graduality from embedding-projection pairs](https://dl.acm.org/ft_gateway.cfm?id=3236768&amp;type=pdf) Max S. New, Amal Ahmed * [Incremental relational lenses](https://dl.acm.org/ft_gateway.cfm?id=3236769&amp;type=pdf) Rudi Horn, Roly Perera, James Cheney * [Elaborating dependent (co)pattern matching](https://dl.acm.org/ft_gateway.cfm?id=3236770&amp;type=pdf) Jesper Cockx, Andreas Abel * [Capturing the future by replaying the past (functional pearl)](https://dl.acm.org/ft_gateway.cfm?id=3236771&amp;type=pdf) James Koppel, Gabriel Scherer, Armando Solar-Lezama * [MoSeL: a general, extensible modal framework for interactive proofs in separation logic](https://dl.acm.org/ft_gateway.cfm?id=3236772&amp;type=pdf) Robbert Krebbers, Jacques-Henri Jourdan, Ralf Jung, Joseph Tassarotti, Jan-Oliver Kaiser, Amin Timany, Arthur Charguéraud, Derek Dreyer * [Mtac2: typed tactics for backward reasoning in Coq](https://dl.acm.org/ft_gateway.cfm?id=3236773&amp;type=pdf) Jan-Oliver Kaiser, Beta Ziliani, Robbert Krebbers, Yann Régis-Gianas, Derek Dreyer * [Build systems à la carte](https://dl.acm.org/ft_gateway.cfm?id=3236774&amp;type=pdf) Andrey Mokhov, Neil Mitchell, Simon Peyton Jones * [Synthesizing quotient lenses](https://dl.acm.org/ft_gateway.cfm?id=3236775&amp;type=pdf) Solomon Maina, Anders Miltner, Kathleen Fisher, Benjamin C. Pierce, David Walker, Steve Zdancewic * [Finitary polymorphism for optimizing type-directed compilation](https://dl.acm.org/ft_gateway.cfm?id=3236776&amp;type=pdf) Atsushi Ohori, Katsuhiro Ueno, Hisayuki Mima * [Teaching how to program using automated assessment and functional glossy games (experience report)](https://dl.acm.org/ft_gateway.cfm?id=3236777&amp;type=pdf) José Bacelar Almeida, Alcino Cunha, Nuno Macedo, Hugo Pacheco, José Proença * [Functional programming for modular Bayesian inference](https://dl.acm.org/ft_gateway.cfm?id=3236778&amp;type=pdf) Adam Ścibior, Ohad Kammar, Zoubin Ghahramani * [What you needa know about Yoneda: profunctor optics and the Yoneda lemma (functional pearl)](https://dl.acm.org/ft_gateway.cfm?id=3236779&amp;type=pdf) Guillaume Boisseau, Jeremy Gibbons * [Generic deriving of generic traversals](https://dl.acm.org/ft_gateway.cfm?id=3236780&amp;type=pdf) Csongor Kiss, Matthew Pickering, Nicolas Wu * [Relational algebra by way of adjunctions](https://dl.acm.org/ft_gateway.cfm?id=3236781&amp;type=pdf) Jeremy Gibbons, Fritz Henglein, Ralf Hinze, Nicolas Wu * [Contextual equivalence for a probabilistic language with continuous random variables and recursion](https://dl.acm.org/ft_gateway.cfm?id=3236782&amp;type=pdf) Mitchell Wand, Ryan Culpepper, Theophilos Giannakopoulos, Andrew Cobb * [Strict and lazy semantics for effects: layering monads and comonads](https://dl.acm.org/ft_gateway.cfm?id=3236783&amp;type=pdf) Andrew K. Hirsch, Ross Tate * [Ready, set, verify! applying hs-to-coq to real-world Haskell code (experience report)](https://dl.acm.org/ft_gateway.cfm?id=3236784&amp;type=pdf) Joachim Breitner, Antal Spector-Zabusky, Yao Li, Christine Rizkallah, John Wiegley, Stephanie Weirich * [A type and scope safe universe of syntaxes with binding: their semantics and proofs](https://dl.acm.org/ft_gateway.cfm?id=3236785&amp;type=pdf) Guillaume Allais, Robert Atkey, James Chapman, Conor McBride, James McKinna * [Parallel complexity analysis with temporal session types](https://dl.acm.org/ft_gateway.cfm?id=3236786&amp;type=pdf) Ankush Das, Jan Hoffmann, Frank Pfenning * [Equivalences for free: univalent parametricity for effective transport](https://dl.acm.org/ft_gateway.cfm?id=3236787&amp;type=pdf) Nicolas Tabareau, Éric Tanter, Matthieu Sozeau * [Prototyping a functional language using higher-order logic programming: a functional pearl on learning the ways of λProlog/Makam](https://dl.acm.org/ft_gateway.cfm?id=3236788&amp;type=pdf) Antonis Stampoulis, Adam Chlipala * [Tight typings and split bounds](https://dl.acm.org/ft_gateway.cfm?id=3236789&amp;type=pdf) Beniamino Accattoli, Stéphane Graham-Lengrand, Delia Kesner * [Competitive parallelism: getting your priorities right](https://dl.acm.org/ft_gateway.cfm?id=3236790&amp;type=pdf) Stefan K. Muller, Umut A. Acar, Robert Harper * [Static interpretation of higher-order modules in Futhark: functional GPU programming in the large](https://dl.acm.org/ft_gateway.cfm?id=3236792&amp;type=pdf) Martin Elsman, Troels Henriksen, Danil Annenkov, Cosmin E. Oancea * [Fault tolerant functional reactive programming (functional pearl)](https://dl.acm.org/ft_gateway.cfm?id=3236791&amp;type=pdf) Ivan Perez * [Casts and costs: harmonizing safety and performance in gradual typing](https://dl.acm.org/ft_gateway.cfm?id=3236793&amp;type=pdf) John Peter Campora, Sheng Chen, Eric Walkingshaw * [Functional programming for compiling and decompiling computer-aided design](https://dl.acm.org/ft_gateway.cfm?id=3236794&amp;type=pdf) Chandrakana Nandi, James R. Wilcox, Pavel Panchekha, Taylor Blau, Dan Grossman, Zachary Tatlock * [Partially-static data as free extension of algebras](https://dl.acm.org/ft_gateway.cfm?id=3236795&amp;type=pdf) Jeremy Yallop, Tamara von Glehn, Ohad Kammar * [What’s the difference? a functional pearl on subtracting bijections](https://dl.acm.org/ft_gateway.cfm?id=3236796&amp;type=pdf) Brent A. Yorgey, Kenneth Foner * [Keep your laziness in check](https://dl.acm.org/ft_gateway.cfm?id=3236797&amp;type=pdf) Kenneth Foner, Hengchu Zhang, Leonidas Lampropoulos * [Merlin: a language server for OCaml (experience report)](https://dl.acm.org/ft_gateway.cfm?id=3236798&amp;type=pdf) Frédéric Bour, Thomas Refis, Gabriel Scherer * [Generic zero-cost reuse for dependent types](https://dl.acm.org/ft_gateway.cfm?id=3236799&amp;type=pdf) Larry Diehl, Denis Firsov, Aaron Stump * [Reasonably programmable literal notation](https://dl.acm.org/ft_gateway.cfm?id=3236801&amp;type=pdf) Cyrus Omar, Jonathan Aldrich * [Refunctionalization of abstract abstract machines: bridging the gap between abstract abstract machines and abstract definitional interpreters (functional pearl)](https://dl.acm.org/ft_gateway.cfm?id=3236800&amp;type=pdf) Guannan Wei, James Decker, Tiark Rompf 
 Error: No instance for Num x found.
Haskell development can be done on Windows. One options is using something like the Haskell Platform that has the tricky libraries like network pre-compiled, or, if they're are feeling more adventurous, try to build and install those libraries from Cygwin or even Git bash.
 &gt;Error: No instance for Num x found. "EFFECTIVE IMMEDIATELY: ghc will stop saying "No insatance for Num String" and will instead say "vewwy sowwy!! uwu you fawgot to pwovide a wittle cwassey instancey~" , as the Creators originally intended 27 years ago" - pltdril 
Hope this will help you to get started :) Imho, the use of Template Haskell in Yesod makes it easier - you don’t have to write the template code yourself, you just benefit from the code that gets generated for you. 
Am I understanding correctly that this feature would only work with trivial data constructors, such as your `data Alias ... = Alias ~ ()`? Or does it support non-trivial value level lists too?
I still want Absal in a module-level ghc plug-in, though. 
Would this just be a matter of generating interaction nets for a handful of implementations of a given function and classifying the performance characteristics? 
It's... Very annoying. It pulls hundreds of dynamically linked Haskell libraries in global scope that pollute ghc-pkg making it basically impossible to develop Haskell on Arch. Also happens when you try to install pandoc for example. The maintainers' philosophy is aligned with the arch philosophy of packaging language dependencies as system dependencies however this kinda conflicts with haskell's way of development 
Applicatives are a superset of monads. Arrows are also a superset of monads. Are there things that can be expressed as applicatives and as arrows, but not as monads? (Intuitively I don't think there are, but I'm not good enough with types to prove that a monad arises from combining their respective laws.) 
&gt; It seems to me that transformers is a good way to abstract out all the monads used in a certain part of the codebase and expose a monad-less interface to the user. Is that true? No. Transformers make out of a `IO (Maybe a)` a single monad, so you only have to `&gt;&gt;=` once to access the internal value. &gt; Follow up: Since IO monad doesn't have a transformer, does it mean any user of a function returning `IO a` will have to be aware of IO monad? (thus the whole codebase can end up being aware of IO monad and constantly unwrap/wrap values?) I think there's a misconception at play here - you never unwrap values in `IO`. Nothing can ever escape `IO` (unless you count `unsafePerformIO`). Instead you have HOF to transform the values inside of `IO`. But yeah, as soon as you return `IO` on any function, you'll have to propagate that upwards. There's a few patterns around that, e.g. free monads, or tagless final, where you specify the actual implementation of the side effect later - but that mostly means you carry another monad up the call stack instead of IO - but you'll still need some sort of monad.
A very small SAT is easy to solve. It's the big ones that are hard to solve. Oh wait... Seriously though, this is a beautiful post. Thanks for sharing it!
Thanks for your feedback ^_^ Regarding `reexported-modules`: this is needed for `mixins` field in the example. If you only have module with name `Map`, you can't use multiple implementations of signatures inside single package like this: mixins: containers-contrib (Map.Contrib.Group as Map.Contrib.Group.Int) requires (Map as Map.Int) , containers-contrib (Map.Contrib.Group as Map.Contrib.Group.Ord) requires (Map as Map.Ord)
But, when you write `requires (Map as Map.Int)` you are renaming the signature "hole" in containers-contrib from `Map` to `Map.Int`. Therefore, you are not using the module `Map` from containers-ordered-strict for anything! If fact I deleted that re-export and the code still compiles. 
As a start http://haskellbook.com/ but you have a few recommendations for it already. When you learn some basics I recommend having a look at: https://github.com/sdiehl/wiwinwlh
&gt; But when you write requires `(Map as Map.Ord)` you are renaming the in `containers-contrib` from `Map` to `Map.Ord` Not exactly. I'm not renaming anything at all. I'm substituting `Map.Int` module as an implementation of `Map` signature in `Map.Contrib.Group` module and then I'm making this `Map.Contrib.Group` module available by `Map.Contrib.Group.Int` name. I can't have only `Map` name inside implementation. Otherwise, how can I specify what exact module I want to substitute as an implementation of `Map` signature "hole"? I need to have modules with different names to be able to write `requires (Map as Map.Int)` and `requires (Map as Map.Ord)` inside `mixins` field to specify exact implementation. I don't see any way to rename module inside `mixins` field. And there's no syntax to specify from what exact package module should be taken like `requires (Map as containers-ordered-strict:Map)`. But I also want to have module to be available by name `Map` (hence the `reexpored-modules` field) so that in case of only single implementation I don't need to write `mixins` field explicitly and the implementations will be substituted automatically. So removing `reexported-fields` field won't break compilation of the existing codebase. But it's still very useful to have it.
&gt; I don't see any way to rename module inside mixins field. You could use `mixins` to rename the implementation modules, for example: containers-contrib (Map.Contrib.Group as Map.Contrib.Group.Int) requires (Map as Map.Int), containers-int-strict (Map as Map.Int), containers-contrib (Map.Contrib.Group as Map.Contrib.Group.Ord) requires (Map as Map.Ord), containers-ordered-strict (Map as Map.Ord) 
When in doubt always give the easier option to the downstream user.
Sure, but if we’re dealing with fundamentally complex interactions of code paths that might go beyond programmer intuition, my thought was simply to offload those “guesses” to tooling, say, analysis plugins. It’s eminently possible to take the call graphs of f . g . h, their individual interaction nets as graphs, and their individual performance characteristics and generate probabilisticslly an interaction net of their composition along with performance characteristics. It’s also possible to generate a whole space of call graphs and sort them by some metric. So one could get slightly better autocomplete, in a way. 
Why does `null (Left 1)` return `True` while `null (Right 1)` return `False`? and the same question for `length (Left 1)` returning `0` and `length (Right 1)` returning `1`. What's the rationale behind that?
The aim is to have a DSL which can do Spreadsheet style operations, v lame example: a3 = a1 + a2 Which means, banning general recursion is fine. But if I can have my cake and eat it, i.e. check generally recursive functions for totality with Liquid Haskell - why not.. Thing is, I am unsure how I will go about banning general recursion. I will need to inline all function calls perhaps and then check.. Hmm. On a side note, I think total functional programming is very appealing and have a lot of benefits - not least by ensuring that functions will never do odd unexpected things!
This is exactly what I meant.
It’s because the first parameter of `null` is constraint to `Foldable`: `null :: Foldable t =&gt; t a -&gt; Bool` `Left` is the failing part of `Either` so it’s intentionally empty, `Right` not. The same applies for `length`. 
Note that if your language permits user-defined functions then you may need to add a type system in order to prevent general recursion. For example, the following expression is an infinite loop in an untyped language and a type error in a typed language: (\x -&gt; x x) (\x -&gt; x x) Type systems aren't the only way to prevent general recursion in the presence of functions, but they are the most common solution.
&gt; Since the halting problem is undecidable, there's no silver bullet and there will always be some total functions that you can't automatically check for totality That is not correct. The halting problem only applies to Turing-complete languages, but a total language is by definition not Turing-complete. System F is an example of a total language that is not Turing complete and verifying that a function type-checks in System F is decidable.
Thanks for the answer. I've tried to do the following in ghci: ``` data EEither a b = ELeft a | ERight b instance Foldable (EEither a) where foldMap f (ELeft x) = f x foldMap f (ERight y) = f y ``` And I got the following error: ``` &lt;interactive&gt;:16:27: error: • Couldn't match expected type ‘a1’ with actual type ‘a’ ‘a’ is a rigid type variable bound by the instance declaration at &lt;interactive&gt;:15:10-29 ‘a1’ is a rigid type variable bound by the type signature for: foldMap :: forall m a1. Monoid m =&gt; (a1 -&gt; m) -&gt; GEither a a1 -&gt; m at &lt;interactive&gt;:16:3-9 • In the first argument of ‘f’, namely ‘x’ In the expression: f x In an equation for ‘foldMap’: foldMap f (GLeft x) = f x • Relevant bindings include x :: a (bound at &lt;interactive&gt;:16:20) f :: a1 -&gt; m (bound at &lt;interactive&gt;:16:11) foldMap :: (a1 -&gt; m) -&gt; GEither a a1 -&gt; m (bound at &lt;interactive&gt;:16:3) ``` Could you explain why am I getting this error because I still can't understand why it's not possible to write `foldMap f (ELeft x) = f x`.
You need to implement `Foldable` for `EEither a b` and not only for `EEither a` :)
It’s amazing to see the speed at which Eta progresses, great job!! :)
For those of you that use Emacs, is there a need to use both `interactive-haskell-mode` and intero? I've tried googling but haven't found anything. 
Why not Python? Mostly because parsing CSV is a fairly standard thing to do, and Python has some good libraries for fast numeric computation, e.g. numpy.
I think Haskell would be perfectly fine for such a problem. When you say it's your first project, have you tried learning Haskell before or already know some other ML style functional programming language (OCaml, F#, Elm...)? If not this first project will be quite difficult. For the CSV parsing, have a look at [cassava](http://hackage.haskell.org/package/cassava) :-)
Glorified CSV parsing was my first real project for getting into Haskell. It was a great learning experience; I'd recommend it!
Is there anyway to implement a list as a recursive data structure using infixr to bind an operator. I have this code `data Stack a = Empty' | Stack (Stack a) a deriving Show` and it works as expected. I was thinking of implementing some standard list methods on a stack just for practice. However I cannot declare infixr 5 -:- data Stack a = Empty' | (Stack a) -:- a deriving Show The latter gives me a parser error. "Cannot parse data constructor in a data/newtype declaration: Stack a"
&gt; In order to support better extensibility at compile-time while keeping good performance we took sometime to port Backpack into Eta. Looks like my blog post about Backpack came just in time ;)
&gt;There might not be any local Haskell jobs unless you're in NYC, Boston, Singapore, London, or San Fransisco. I'd also add Brisbane, Melbourne, and Sydney to that list!
Have been really thinking about getting into this. Can someone tell me what their experience has been like using it? Tooling, etc?
What problem does it solve ?
Similar to how regular constructor must start with an uppercase letter, operator constructors must start with a `:`. infixr 5 :- data Stack a = Empty | Stack a :- a deriving Show 
Not without dependent types
great pt
That's a great blogpost, thanks! I successfully managed to build a simple project with cabal new-build. Does new-build support projects with multiple packages as well? Has somebody written a feature comparison stack vs cabal yet? 
Is this possible with the Haskell type system? Do you have any reading suggestions?
&gt; Some parts of the Hackage ecosystem (such as conduit) have decided to break backwards compatibility by putting a lower bound on the base package. Why would they intentionally break compatibility? Have you tried talking to the authors of said packages? Maybe they weren't aware.
&gt; Does new-build support projects with multiple packages as well? Sure! You only need to create file called `cabal.project` and enumerate all packages there. See example in this package: * https://github.com/kowainik/containers-backpack/blob/master/cabal.project &gt; Has somebody written a feature comparison stack vs cabal yet? I'm not aware of up-to-date comparison. But nowadays `cabal-install` lacks only couple UI commands like list all dependencies of the project (but there're ways to see it anyways, so this is not really a problem).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [kowainik/containers-backpack/.../**cabal.project** (master → 1bc6dce)](https://github.com/kowainik/containers-backpack/blob/1bc6dcef4cb67d0a967b69ed1c5f99c9e73ddaf8/cabal.project) ---- 
You can use len's field type prisms to write functions that would work on tuples of any size 2-19, eg `over _1 (+1)` will increment the first field of any tuple (of size 2 to 19).
Simply put, it's a type error. For `ELeft x, ERight y :: EEither a b`, we have `x :: a`, `y :: b`. Specialised to `EEither a`, we have `foldMap :: Monoid m =&gt; (b -&gt; m) -&gt; EEither a b -&gt; m`, so `f x` does not make sense without a constraint that `a` equals `b`. If you had instead e.g. `newtype Join f a = Join (f a a)` then you could write a Foldable instance for `Join EEither` which would count both the contents `ELeft` and `ERight`.
What is to be expected is that some of the patched libraries will not work, at least not fully. I've tried the eta version of my tictactoe game few days ago, the ansi-terminal doesn't work [https://github.com/raducu427/tictactoe/tree/eta](https://github.com/raducu427/tictactoe/tree/eta)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [raducu427/tictactoe/.../**7903e74c65f6c7f0951959fe7942c7634ddc7df5** (eta → 7903e74)](https://github.com/raducu427/tictactoe/tree/7903e74c65f6c7f0951959fe7942c7634ddc7df5) ---- 
Yes! Thanks for mentioning it.
Using GHC Generics it's possible to write such a function for any product type that is an instance of Generic. It is currently derived only for tuples up to length 7 but in principle it could work for all of them and it's easy enough to declare your own tuple-like type.
But isn't there already this: [https://github.com/Frege/frege](https://github.com/Frege/frege)
They had reasons to do so. Mostly documented here :) [https://www.snoyman.com/blog/2018/02/conduitpocalypse](https://www.snoyman.com/blog/2018/02/conduitpocalypse)
Not that I have anything against using Haskell but you do know there are tons of decent libraries for C++ for reading CSV files, [for example](https://github.com/ben-strasser/fast-cpp-csv-parser) so I’m not sure the pain of reading CSV files should be the driver to switch. 
That does not invalidate my statement. While there are decidable criteria for a program being total, such as typechecking in System F, there will always be total functions that lie outside of those criteria. Moreover, it is typically much more difficult to implement a given total function in such languages than it is in untyped lambda calculus.
A heterogeneous list import Data.Kind (Type) infixr 5 :- data HList :: [Type] -&gt; Type where HNil :: HList '[] (:-) :: a -&gt; HList as -&gt; HList (a:as) h0 :: HList '[] h0 = HNil h1 :: HList '[Int] h1 = 10 :- HNil h2 :: HList '[Int, Double] h2 = 10 :- pi :- HNil h3 :: HList '[Int, Double, Bool -&gt; Bool] h3 = 10 :- pi :- not :- HNil is like () 10 (10, pi) (10, pi, not) but you can define `fst` and `snd` that where `(fst h0)` and `(snd h0)`, `(snd h1)` will fail to type check fst :: HList (a:as) -&gt; a fst (a :- _) = a snd :: HList (a:b:bs) -&gt; b snd (_ :- b :- _) = b &gt;&gt; fst h1 10 &gt;&gt; fst h2 10 &gt;&gt; fst h3 10 &gt;&gt; snd h2 3.141592653589793 &gt;&gt; snd h3 3.141592653589793
It should be enough to check that the call graph is acyclic to rule out general recursion. Acyclicity can be checked fairly quickly by [topological sorting](https://en.wikipedia.org/wiki/Topological_sorting). Haskell's type system should (I think) prevent introducing general recursion through self application. My feeling is this should be simpler than using Liquid Haskell, which is not to say that it's better.
Any Hindley Milner type system will reject the above expression, including Haskell's type checker. A good starting point is the "Typing Haskell in Haskell" paper
Sorry, I misread your post. I thought you were claiming that there were functions that you can't automatically type-checked in a total language
You can also use [*lens*](https://hackage.haskell.org/package/lens-4.17/docs/Control-Lens-Tuple.html) view _1 :: Field1 pair pair a a =&gt; pair -&gt; a view _2 :: Field2 pair pair b b =&gt; pair -&gt; b view _3 :: Field3 pair pair c c =&gt; pair -&gt; c These work ([*simple-reflect*](https://hackage.haskell.org/package/simple-reflect)) &gt;&gt; import Control.Lens &gt;&gt; import Debug.SimpleReflect (a, b, c) &gt;&gt; view _1 (Identity a) a &gt;&gt; view _1 (a, b) a &gt;&gt; view _1 (a, b, c) a &gt;&gt; view _2 (a, b) b &gt;&gt; view _2 (a, b, c) b &gt;&gt; view _3 (a, b, c) c
This is the right answer!
Really impressive how much has been done since I last looked at eta. I found the extensions in Eta2018 quite interesting. Would have expected the conservative expressions but it goes with basically everything including the kitchen sink. The fancy types implying MonoLocalBinds surprised me a bit but its pretty hard to run afoul of without existential or rankn types so it shouldn't actually create problems when learning?
I think the question was if there are things that are *both* applicatives *and* arrows but *not* monads. 
Unfortunately they have pretty script performance requirements for final product so it'll probably have to be a compiled language. It's running on an embedded device.
I've never done serious work in another functional language, but I understand the concepts and have written a couple HelloWorld programs in them
`intero-mode` can be enabled in `haskell-mode` buffers. Here's the corresponding section from my `init.el`: (use-package haskell-mode :ensure t) (use-package intero :ensure t :init (add-hook 'haskell-mode-hook 'intero-mode)) (with-eval-after-load 'intero (with-eval-after-load 'flycheck (flycheck-add-next-checker 'intero '(warning . haskell-hlint))))
I have the same setup, I'm just curious if I should enable `interactive-haskell-mode` too.
Frege is "less Haskell" than Eta. It's older, but only Haskell-like / Haskell-inspired. It is lazy and pure, but it's syntax is only somewhat compatible. While they may have been some hackage packages that would compile with Frege, they were rather rare. Eta is in a very real sense a fork of GHC that swapped out the backends for a bytecode one. The Haskell syntax you were using in 7.10 works fine in Eta; some of the GHC specific packages are not compatible, but even so, something like 80% of hackage worked without shims, and Eta's interface to hackage automatically included shims to make some important packages work. That said, GHC and Hackage have sort of moved on, and packages that need the GHC 8+ features are less likely to work with Eta; though this release is definitely an improvement there.
A number of posters have mentioned the likely need to relocate. This is actually a big argument in favour of going for Haskell as a first job rather than waiting a few years; by then you will have put down roots where you are, and moving is going to be a bigger issue than it is out of college, when you are probably expecting to move anyway.
I'm not sure that Haskell is a good fit for embedded devices. You'll have contend with garbage collection that may break whatever performance constraints/requirements that you have.
Or you can use a library that already does that like [this one](http://hackage.haskell.org/package/tuple-0.3.0.2/docs/Data-Tuple-Select.html).
good thought! thank you
Interestingly, though, list do not fuse at all when you write your own recursive algorithms (rather than using library functions like map, filter and foldr), right? A possible simple way to make an user-friendly language that makes use of Absal's speedups is to perhaps have an easy syntax for recursive algorithm, something like `transform list:`. That way the compiler can make the boring work of assembling the recursion the proper way for fusion.
The fact that this library exists is a strong indication Haskell terribly needs dependent types asap.
You don't "need" it but dependent types are the best way to solve problems like this.
Can you explain what this has to do with dependent types?
I guess C++ / Rust are your best options then
Because dependent type theory is a really good way of solving this kind of problems. When you want to talk about "heterogeneous" you're actually trying to express some logic about types, say "I want this list to contain that satisfy P(T)" where P is some constructive predicate about types. It can be that "P(T) = T is Int or String" or something more complex. In fact, it's not even necessary that T is a type, you can express types dependent on values i.e. `Tup N` can be of different types that can be processed at compile time. So that, you wouldn't need horrible things like [this](http://hackage.haskell.org/package/tuple-0.3.0.2/docs/Data-Tuple-Select.html). This can be expressed with one recursive function using dependent types. Normally, when I need things like this, I write agda code and compile to haskell, which is very bad taste. Haskell needs robust, portable dependent types. 
I don't see how dependent types help you here.
Why?
The actual problem is that built-in tuples aren't inductive. Dependent types have nothing to do with that. A `Tuple M` type can be expressed in Haskell today with GADTs. But regular tuples just aren't defined that way.
&gt; Looks like my blog post about Backpack came just in time ;) Link?
Thanks, I definitely missed the "and" part. I've edited my answer.
I suppose a PhD in the subject is required?
The problem here has nothing to do with what you described though. Heterogeneous containers are very much possible in Haskell today, and in fact, inductive heterogeneous containers are possible in Haskell98. While it's true that GHC Haskell provides only limited ways to constrain the types that can fit into the container (pretty much only allowing predicates on types, not values), this isn't really related to the problem of accessing the second element of such container.
It's been noted that `UnliftIO` is "actually" some more fundamental mathematical concept from category theory; I want to say [`Distributive`](http://hackage.haskell.org/package/distributive-0.6/docs/Data-Distributive.html) but I can't recall exactly.
Do you remember the source for this?
Actual `UnliftIO` is really restricted, to basically just cover `ReaderT` and things that look like it iirc? So you don't need a fancy categorical name for it per se. But the observation is that "Reader distributes through everything" which is related to Yoneda, and further it turns out that in Haskell, all inhabitants of the distributive typeclass (https://hackage.haskell.org/package/distributive-0.5.3/docs/Data-Distributive.html) (which can be obtained by swapping the arrows on traversable), are isomorphic to `Reader a` for some `a`. (which is to say that the function arrow is the _universal representation_ of what it means to distribute through everything). All of which is interesting but actually doesn't relate precisely to this post, since it is aiming to attack a broader class of monads than just the reader-like ones :-) 
Looking through the [Monads Categorically'](https://bartoszmilewski.com/2016/12/27/monads-categorically/) this seems related to [adjunctions](http://hackage.haskell.org/package/adjunctions-4.4/docs/Data-Functor-Adjunction.html)? Definitely don't know enough category theory to fully understand that, though.
`MonadUnliftIO` also only has valid instances for things that are `ReaderT a` for some `a`.
C++ and parallel eigen3 will beat that by an order of magnitude
We can generalize HList :: [Type] -&gt; Type to an [*N*-ary product](https://hackage.haskell.org/package/generics-sop-0.3.1.0/docs/src/Generics-SOP-NP.html), the difference is that the constructor `(f :: k -&gt; Type)` is applied to each type data NP :: (k -&gt; Type) -&gt; ([k] -&gt; Type) where Nil :: NP f '[] (:*) :: f a -&gt; NP f as -&gt; NP f (a:as) infixr 5 :* We can now define `HList` as `NP Identity` {-# Language PatternSynonyms #-} import Data.Functor.Identity -- newtype Identity a = Identity a type HList = NP Identity pattern HNil :: () =&gt; types ~ '[] =&gt; HList types pattern HNil = Nil pattern (:-) :: () =&gt; types ~ (a:as) =&gt; a -&gt; HList as -&gt; HList types pattern a :- as = Identity a :* as
That's it, yes!
A quick look at the docs, it says that Eta is a 'dialect' of Haskell, but looking at the examples they look pretty much like Haskell and there's even `aeson` library available? What are the differences with 'mainstream' Haskell?
The main difference is thet in mainstream Haskell you cannot use Java classes
Another weird thing about UnliftIO and monad transformers: if you only use UnliftIO-compatible monads, then you don't need monad-transformers! The reason we need monad transformers is because we can't use the `Compose f g` trick: if `f` and `g` are both `Applicative`s, then so is `Compose f g`, but if `f` and `g` are both `Monad`s, then `Compose f g` might not be a `Monad`. So instead of using `Compose` to combine arbitrary monads together, we replace our simple monadic types with more complex monad transformer versions, which compose in this weird nest-and-omit-the-last-type-parameter way, and then we `lift` and `hoist` our computations into the shape we need, and then we define type-classes to hide those `lift` calls... I mean, we're all used to monad transformers by now, but it you look at it objectively, it's a much more complicated solution than just using `Compose`. But if `f` and `g` both have `UnliftIO` instances, and are therefore also both `Monad`s, then [`Compose f g` also has `UnliftIO` and `Monad` instances](https://twitter.com/haskell_cat/status/1027193454768676866)! So we don't need monad transformers, we can just use `Compose` again.
Doesn't have some of the new GHC 8+ stuff, but the front end *is* GHC 7.10, from what I understand.
Yeah, I'm not sure about Adga, but in Idris tuples are literally inductively defined so you can do weirdness like that.
&gt; if its input is of type Careful, just because you can use dependent types doesn't automatically let you break parametricity. Neither Agda nor Idris allow pattern-matching on types (or any other runtime testable quality of `Set`/`Type` values).
&gt; Because dependent type theory is a really good way of solving this kind of problems. I think it's a really slick theory, but I think in practice it turns out that it's best to avoid types-depending-on-terms whenever you can, and that applies here.
ETA is basically like Haskell where they replace the C foreign function interface with a Java foreign function interface and patch Hackage libraries that depend on C libraries to instead depend on the analogous Java libraries
And the hype train continues
https://kowainik.github.io/posts/2018-08-19-picnic-put-containers-into-a-backpack
You can't "implement `Foldable` for `EEither a b`" -- it has the wrong kind.
As far as the `Foldable` instance is concerned, an `Either a b` is a container that holds either zero or one `b` values. The `a` value do not count. One way of thinking about `Either a b` that is in line with the instance is that it is what you get when you pick a `Maybe b` [and attach a tag of type `a`](http://hackage.haskell.org/package/errors-2.3.0/docs/Control-Error-Util.html#v:note) to the `Nothing` values. (If you really want something like `Foldable` but which handles both sides of `Either` in the same way, have a look at [`Bifoldable`](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Bifoldable.html).)
Cheers - will give it a good read!
I left my PhD for my industry job. I _think_ one major reason I got the job was that I managed to convey that I was good at using Haskell's type system to enforce data invariants (think GADTs and singletons), and they were (well, are) doing that a lot.
Agda has no ‘built-in’ tuples at all. The product type provided by the standard library is just a pair. In practice, people often just use nested pairs for larger tuples, and you can work over nested pairs generically just fine.
[removed]
It's the difference between an online and offline attack. Usually when downloading packages that are signed. The master key can be secure and offline in this case. The signing key is also known to be owned by the signert. When SSL is used, the key is not only online but also a large set of CAs are trusted. The key is not known to be owned by anyone except through trust in CAs. CAs can not be trusted. Both methods are weaker than just using a git repo with signed commits like stackage does.
Yep that's the route I am thinking about - the trick now is to figure out how to actually ban it : )
Huh?! Well at least your username is apt, this seems like the opposite of enlightened.
I don't think he meant that as negative.
Is UnliftIO just too new for us to be seeing it used more frequently? Or are there still valid reasons to keep using mtl-style monad transformer stacks?
I usually just think that Reader distributes through everything because of preservation of limits. Mind you the proof that right adjoints preserve limits requires the Yoneda lemma. ;)
It only lets you use `IO` and `ReaderT`, so if you want functionality other than that, you're out of luck.
But what would that functionality be? Continuations?
Pure state, pure exceptions, non-determinism?
Right, but you wouldn't be using the first two with `UnliftIO` anyway (or monad control or whatever), which I thought was more the question /u/DisregardForAwkward was asking. Non-determinism I believe *could* be recovered with exceptions, but yes using something more appropriate for the job would be better there. Usually non-determinism has me reaching for `streaming` or some other streaming library - but that's still essentially a monad transformer.
Ohh yeah you're probably right. I wasn't aware of the hype train meme. Interesting how existing downvotes color your interpretation!
/u/bgamari, I appreciate the tremendous amount of work by everyone involved, but since it was asked, I'll bit and give a personal answer. Making performance the #1 focus of a major GHC release would be a very welcome goal. Of course new features have been cooking and will be merged, but focusing on 1. compiler space and time efficiency 2. NCG emitting efficient code in more situations 3. RTS efficiency/scalability 4. specifically, STM scalability improvements, since it's such a nice abstraction would mean a much higher quality GHC than trying to close regressions introduced in the previous major releases. That and closing as many GHC and RTS bugs as possible would be my priority. For instance, I would have liked to see https://ghc.haskell.org/trac/ghc/ticket/15260 land in 8.4.4, which I don't think is planned. I have to restart xmonad because xmobar built with 8.4 terminates regularly. And let's not forget my pipe dream of some day being able to build fully static binaries like Rust and Go can if you avoid linking shared libraries.
[removed]
Now that I'm taking a closer look, I see that this isn't based on UnliftIO: on the contrary, this is a promising alternative to UnliftIO! To better illustrate where I think this approach fits in the landscape and why I think it's promising, let's recap the problem we're trying to solve. For simplicity, I will assume that IO is always the innermost base monad. MonadBaseControl and UnliftIO both try to make it easy to reuse higher-order IO functions like `withFile :: FilePath -&gt; IOMode -&gt; (Handle -&gt; IO r) -&gt; IO r` in monad stacks which contain more than IO effects, e.g. `withFile' :: c m =&gt; FilePath -&gt; IOMode -&gt; (Handle -&gt; m r) -&gt; m r`. Ideally we would want `c ~ MonadIO`, but that constraint is not powerful enough to implement `withFile'` in terms of `withFile`. `c ~ MonadBaseControl IO` and `c ~ MonadUnliftIO` are powerful enough, but both have some significant drawbacks. Very different drawbacks though. MonadBaseControl's drawback is that it sometimes loses non-IO side-effects. For example, import Control.Exception.Lifted import Control.Monad.IO.Class import Control.Monad.Trans.State -- | -- &gt;&gt;&gt; execStateT partialEffects1 "foo" -- ! -- ? -- "foo!" partialEffects1 :: StateT String IO () partialEffects1 = (liftIO (putStrLn "!") &gt;&gt; modify (++ "!")) `finally` (liftIO (putStrLn "?") &gt;&gt; modify (++ "?")) doesn't output `"foo!?"` as one might expect, even though the finalizer block clearly gets executed. The reason for this is that MonadBaseControl encodes the non-IO effects as a pure value returned by the IO computation, but `Control.Exception.finally :: IO a -&gt; IO b -&gt; IO a` drops the `b` value returned by the finalizer, and so there is no way for `Control.Exception.Lifted.finally` to use that value to inject those non-IO effects back into the `m` computation. Another way in which MonadBaseControl may lose side-effects is when combining the effects of two computations. For example, import Control.Concurrent.Async.Lifted import Control.Monad.Trans.State -- | -- &gt;&gt;&gt; execStateT partialEffects2 "foo" -- "foo?" partialEffects2 :: StateT String IO () partialEffects2 = modify (++ "!") `concurrently_` modify (++ "?") doesn't output `"foo!?"` as one might expect, this time because the first computation's non-IO effects are injected back into the `m` by replacing the state with `"foo!"`, and then the second computation's non-IO effects are injected back into the `m` by replacing that state with `"foo?"`. Note that not all monad transformers inject their non-IO effects by overwriting the effects of previous computations; the following, for example, returns the expected result. -- | -- &gt;&gt;&gt; execWriterT fullEffects1 -- "!?" fullEffects1 :: WriterT String IO () fullEffects1 = tell "!" `concurrently_` tell "?" All right, let's look at UnliftIO next. Unlike MonadBaseControl, it never drops non-IO side-effects. That's because it doesn't support _any_ non-IO side-effects, so injecting the non-IO effects can never go wrong. You can't use StateT or WriterT, you must use IORefs instead. Yeah, yeah, it does support ReaderT, but come on, that barely counts as an effect. One advantage of IORef over StateT is that their state changes persist when computations fail. One disadvantage, however, is that their state changes persist when computations fail. Sometimes we want changes to persist, sometimes we don't! Monad transformers allow you to pick `ExceptT e (StateT s m)` or `StateT s (ExceptT e m)` depending on which behaviour you want, but UnliftIO forces you to pick the behaviour in which changes persist. That's the drawback. Finally, let's look at your approach. You're splitting a computation's effects into the pre-IO effects, the IO effects to support non-IO effects by splitting them into a pre-IO computation, an IO computation in the middle, and a post-IO computation. That certainly covers many more effects than just `ReaderT r IO`. Does it help avoiding to accidentally drop non-IO side-effects? I think it does! I mean, it's certainly possible to use Split to implement a version of finally which drops some non-IO effects: finally' :: ( Applicative (Outer m), Applicative (Inner m) , Middle m ~ IO , Split m ) =&gt; m a -&gt; m b -&gt; m a finally' body finalizer = unsplit (liftA2 finally (split body) (split finalizer)) -- | -- &gt;&gt;&gt; execStateT partialEffects3 "foo" -- ! -- ? -- "foo!" partialEffects3 :: StateT String IO () partialEffects3 = (liftIO (putStrLn "!") &gt;&gt; modify (++ "!")) `finally'` (liftIO (putStrLn "?") &gt;&gt; modify (++ "?")) But since Split is a lot less magical than MonadBaseControl, I think it should be relatively obvious to the implementers of `finally'` that if they don't apply the finalizer's post-IO side-effects, they're not going to happen. My only gripe so far is that the semantics are far from obvious. I was really surprised by the `No instance for (Monoid Int)` error, and even when the state does have a Monoid instance, I don't think -- | -- &gt;&gt;&gt; execStateT partialEffects4 "foo" -- "foo!foo?" partialEffects4 :: StateT String IO ((), ()) partialEffects4 = modify (++ "!") `concurrently` modify (++ "?") is the expected output. The ideal solution would be to be able to use StateT with `withFile`, because the effects occur one after the other, but not with `concurrently`. And Split makes that possible! We just need to be a little more careful with our choice of `Inner` and `Outer`. You reused `(,) s`'s Applicative instance, but I think it would be better to define a newtype which only has a Functor instance.
Tweag, FP Complete, DFINITY, Digital Asset all have remote Haskellers in Europe, I think.
IOHK is developing the Cardano blockchain in Haskell. You should give them a try! https://iohk.io/careers/
how does ghcjs handle it?
Thank you, chshersh! Yes, we are completely remote and we are open for new people. Drop us a line hi@serokell.io :)
GHCJS tracks upstream GHC. I don't know how well it's doing in terms of minimising the distance to upstream or keeping forward-porting easy.
You guys list Agda in your technologies section. Are you actually using it in an industrial setting? If so, that's impressive!
This is supposed to be possible via `.haskeline` (https://github.com/judah/haskeline/wiki/CustomKeyBindings). I'll fiddle around until I get something working.
Haha yeah I meant to express excitement over this faster release cycle
Yep. Here's my `.haskeline`: ``` bind: f7 : r e l o a d keyseq: "\ESC[18~" f7 ``` 
IOHK has a ton of developers in the European timezones.
[How the Haskell hype train rolls](https://www.youtube.com/watch?v=fwJHNw9jU_U)
[Well-Typed](https://www.well-typed.com/).
We do! Probably, community will see that soon officially :)
Yeah, I figured it'd only work for some mix of product/abstraction, their compositions and stuff that follows like Identity. I think the theoretic reason has to do with Data.Functor.Adjunctions so it looks like sum/product would also work? Not sure what that monad would be used for, though. Category theory for programmers mentions that monads are generally decomposable into adjoint functors. I am assuming that would require more than endofunctors in set, though. Would this be expressible in haskell and could that make monads (more) composable? Assuming that composing adjoint functors generally still leaves adjoint functors.
A couple big reasons: 1. We'd like to stay on similar timezones for now, makes it easier to schedule calls 2. We can't offer the same benefits outside the US (healthcare, equity, etc) I hope to open it up more in the future, but not super short term.
By the proof sketch elsewhere on thread (https://www.reddit.com/r/haskell/comments/98xcc2/weird_unliftio/e4jvlo3/) the only monad that you can decompose into an adjoint _inside_ haskell (i.e., not just an adjoint of functors, but an adjoint of _endofunctors_) is State (which arises from `(,r) -| r-&gt;`). Further the three way factorization you have doesn't correspond to the state factorization (state isn't just reader + writer, but also a special join that relates them based on them sharing the same `r`). A number of papers from Hinze in particular discuss different ways of understanding and developing haskell code via adjoints more generally, and you might be interested in them: http://www.cs.ox.ac.uk/ralf.hinze/publications/index.html 
Gotta admit that I have some reading up to do to understand that proof. The lecture notes for 'Generic Programming with Adjunctions' seem fairly approachable and really interesting, though, so that seems like a reasonable starting point to rectify this. Thanks a ton for the link!
&gt; Follow up: Since IO monad doesn't have a transformer, does it mean any user of a function returning `IO a` will have to be aware of IO monad? (thus the whole codebase can end up being aware of IO monad and constantly unwrap/wrap values?) Yes, but also no. `fmap` , `liftA2`, `liftA[n]`, etc. mean that you don't need to 'constantly unwrap' the results of a monadic computation. Ideally, with IO, you should handle external operations with as little control flow as possible, and use pure functions to handle the data. "Making the whole codebase" IO aware is a potential sign of clumsy architecture and is fairly avoidable. IO should perform IO operations and then dispatch the results of those operations to pure functions. If you're using a case or if/then inside of a do block, think carefully about what you're doing and look for a way to factor that into a pure function. Even a function defined in a where clause could be an improvement. You'll probably end up with some amount of logic in an IO context, and that's ok, but by and large, the more code you can get out of IO, the happier you're going to be.
Relevant https://github.com/Jyothsnasrinivas/eta-spark-core
Wow haha, that's some strict requirements you're up against.
Thanks for the responses, I also found this great list if others haven't seen it [https://github.com/erkmos/haskell-companies](https://github.com/erkmos/haskell-companies) I know remote is always going to be difficult to find and with the added requirement of FP languages like Haskell/PureScript that narrows things down some more. But what is really pleasing is that I do see more and more companies taking on Haskell. :)
Thanks for the feedback, Carsten! There is a lot of great news on the performance front recently: * /u/AndreasK has done some fantastic work on code layout this summer supported by GSoC which has shown quite some promise * Sebastian Graf is dusting off some work introducing a [late lambda lifting](https://ghc.haskell.org/trac/ghc/ticket/9476) pass into GHC's compilation pipeline which has shown is active work on re-evaluating late lambda lifting * /u/tdammers has been doing some great work tracking down [all](https://ghc.haskell.org/trac/ghc/ticket/15304) [kinds](https://ghc.haskell.org/trac/ghc/ticket/11735) of [performance](https://ghc.haskell.org/trac/ghc/ticket/15019) [issues](https://ghc.haskell.org/trac/ghc/ticket/14737) * /u/alexbiehl has been looking at how we might [generate asympototically better code](https://ghc.haskell.org/trac/ghc/ticket/14461) for some deeply nested closure evaluations * I have a [patch](https://phabricator.haskell.org/D4766) which significantly improves typechecking efficiently in programs containing deep type family applications as well as efforts to improve the runtime We plan on focusing on performance in the coming months. We have also had a &gt; And let's not forget my pipe dream of some day being able to build fully static binaries like Rust and Go can if you avoid linking shared libraries. You can already do this today; just build GHC with `integer-simple` and be on your way. /u/nh2 has even been working on some [Nix tooling](https://github.com/nh2/static-haskell-nix) to make this more convenient.
Data.Functor.Adjunction only covers adjunctions from Hask -&gt; Hask, because it is built on the 'Functor' class. There is however, only one such adjunction. `(,e) -| (-&gt;) e`. Every other instance is isomorphic to that case. e.g. Identity -| Identity is isomorphic to choosing e = (), etc. Composition is equivalent to choosing e = the product of the two parts. This is why I started using the idea of representable functors more in Haskell. One fewer argument to pass around to talk about both halves of the adjunction. There is another adjunction easily expressed in Haskell, which is that (-&gt;r) -| (-&gt;r). That one gives rise to `Cont r a = (a -&gt; r) -&gt; r`, but it goes through Hask^op. If you stick a monad in the middle of that adjunction you're sticking a monad in Hask^op in, so it takes a comonad (full of continuations) and gives you a monad. This one is kind of nice because if you quantify over r newtype Co w a = Co (forall r. (a -&gt; r) -&gt; r) It takes Store/Costate and gives back (CPS'd) State. In general it transforms Co Cofoo into Foo. The proof that all monads can be decomposed into adjunctions relies on the existence of the Kleisli and Eilenberg-Moore categories. There is always an adjunction that passed through something there. Sadly that starts to step outside what we can express in Haskell.
Defunctionalizing the intermediate steps by building a data type to represent them and an 'interpreter' from them to your state actions would let you show what the transitions were, not just the states before and after.
Amazon has it too as an eBook (readable in Cloud Reader, desktop and mobile apps but not in their e-Ink readers). Just tried the sample and the formatting looks mostly alright: [https://www.amazon.com/Programming-Haskell-Graham-Hutton-ebook/dp/B01JGMEA3U/](https://smile.amazon.com/Programming-Haskell-Graham-Hutton-ebook/dp/B01JGMEA3U/r)
Also relevant, but for GHC: https://github.com/tweag/sparkle
&gt; You can already do this today; just build GHC with integer-simple and be on your way. /u/nh2 has even been working on some Nix tooling to make this more convenient. Are you sure that's enough? I think I tried to build a GHC with INTEGER_LIBRARY=integer-simple and WITH_TERMINFO=NO once and either the build failed or resulting ghc segfaulted. It's been at least a year, might be worth a retry. What are your thoughts on hooking up a BSD licensed bigint library as a libgmp alternative? There's quite a few out there, some as part of language VMs/runtimes. So I suppose, on Linux glibc system, I'd have to use musl to avoid all those other libs besides ncurses and gmp. Is that correct? $ ldd alex linux-vdso.so.1 (0x00007ffd4c977000) libm.so.6 =&gt; /usr/lib/libm.so.6 (0x00007f0e43bc4000) librt.so.1 =&gt; /usr/lib/librt.so.1 (0x00007f0e43bba000) libutil.so.1 =&gt; /usr/lib/libutil.so.1 (0x00007f0e43bb5000) libdl.so.2 =&gt; /usr/lib/libdl.so.2 (0x00007f0e43bb0000) libpthread.so.0 =&gt; /usr/lib/libpthread.so.0 (0x00007f0e43b8f000) libgmp.so.10 =&gt; /usr/lib/libgmp.so.10 (0x00007f0e438fc000) libnuma.so.1 =&gt; /usr/lib/libnuma.so.1 (0x00007f0e436ef000) libc.so.6 =&gt; /usr/lib/libc.so.6 (0x00007f0e4352b000) /lib64/ld-linux-x86-64.so.2 =&gt; /usr/lib64/ld-linux-x86-64.so.2 (0x00007f0e43d60000) ghc(i) also uses ncurses, which I think is controlled via WITH_TERMINFO libncursesw.so.6 =&gt; /usr/lib/libncursesw.so.6 (0x00007fdc592ae000)
The performance patches you've linked sound exciting. One of them has been sitting in Trac for two years. Isn't that an indication that a concerted release cycle focused on stability and performance would make sense? I mean, with the new release schedule, maybe there could be alternating cycles where one release still includes new stuff but tries hard to not introduce controversial/incompatible changes. I know this is complicated, especially release management and contribution management when you don't have a 2 months release cycle where features missing one window will land in the next. If we had 4 major GHC releases per year, then we could probably make two of those stabilization/speedup cycles. What do you think?
&gt; Are you sure that's enough? I think I tried to build a GHC with INTEGER_LIBRARY=integer-simple and WITH_TERMINFO=NO once and either the build failed or resulting ghc segfaulted. It's been at least a year, might be worth a retry. It should work; perhaps /u/nh2 can chime in with his experiences. &gt; ghc(i) also uses ncurses, which I think is controlled via WITH_TERMINFO Yes, you can disable the `ncurses` dependency when building GHC.
&gt; One of them has been sitting in Trac for two years. Isn't that an indication that a concerted release cycle focused on stability and performance would make sense? The last two releases (8.2 and 8.4) have been been focused on stability. In general the fact that one of the patches has been around for a while (are you referring to the lambda-lifting patch?) doesn't have much to do with our release focus. When I refer to the "focus" of a release I'm generally referring to the priorities of the small group us who are being paid to work on GHC. However, GHC is an open-source project; most of the patches I listed above are from contributors who are free to work on whatever they desire. &gt; If we had 4 major GHC releases per year, then we could probably make two of those stabilization/speedup cycles. I don't believe any more than two release per year will be feasible in the near future. We are already pushing extremely hard to keep to this schedule. The effort going into a release includes far more than that necessary to release the compiler itself. Releasing involves coordination with a dozen boot libraries, bumping bounds of commonly used packages, and, of course, community testing. All of these require effort from unpaid volunteers. We would need to try to streamline these friction points to make a faster release cycle feasible. Frankly, I'm not sure I see a great advantage to going much faster. I agree 12 months/release is long; six months seems just right. Even then, I know some within the community are already worrying that we may be sacrificing QA-quality to reach our release throughput goal.
Yeah, fair enough.
It was #11735 (Optimize coercionKind). I suggested shorter cycles for finding a balance between new features and breakage vs bugfixing and perf tuning. Like I said, I can imagine how much work it is, and it's a bad idea to shorten cycles if you don't have it all automated. Rust adopted the browser release cycle, admittedly with a considerable financial backing and support by Mozilla Org. The reason that works well for feature vs fix/tune is that they managed to shorten it to 6 weeks and avoid breakage in general. Haskell and GHC evolves at a much more drastically rapid pace, so it's unclear if shorter cycles would help. Consider one annoying breakage every 6 weeks and downstream developers will get the impression that GHC is broken every month. If you release once or twice a year, there's two points in time where you deal with build errors. Maybe a more practical solution would be backporting more fixes to the stable branch and going further than .3/.4, reaching maybe 8.4.8. One would expect to see zero issues moving from 8.4.2 to 8.4.7 and twice a year put aside time to deal with a major release. Does that sound like a better idea?
I'm not familiar with that error, but you might try updating your packages. You could also try installing haskell-mode from MELPA instead. Though I would assume the Ubuntu repository would be more stable. I'm not sure how complex your .emacs is, but could it have something in it that interferes with haskell-mode? Not sure, it's always just worked for me (I've always used MELPA).
[removed]
Thanks for the response! It helped to know that this works for other people; I was getting discouraged by the number of threads that basically just said "yeah, it's never worked, what're you gonna do?" Anyway, I uninstalled the package and then reinstalled but this time with `M-x package-install RET haskell-mode` and now it works fine. Thanks for your help!
Hello! I was wondering if there were any relevant/related concepts around the situation where the algebra of a catamorphism involves calling another catamorphism, or in general nested recursion schemes - e.g does fusion apply to this? Thanks loads. 
Oh interesting. I didn't know that; thanks!
Note /u/nh2_ on reddit, I wasn't hip enough to grab /u/nh2 first here. While static linking works great by now, I haven't looked into `integer-simple` yet. I am currently just linking in `libgmp` statically, see [here](https://github.com/nh2/static-haskell-nix/blob/7e41d26eb7403806d6be5f7046efcea3b05442ab/survey/default.nix#L259), same [for `ncurses`](https://github.com/nh2/static-haskell-nix/blob/7e41d26eb7403806d6be5f7046efcea3b05442ab/survey/default.nix#L472). This is fine for static builds of open source projects. If youwant to statically link proprietary code, we either have to make it work with `integer-simple` (probably not hard when you know where to pass the right flags), or you'd have to provide `.o` files [which also satisfies the LGPL](https://softwareengineering.stackexchange.com/questions/312758/does-providing-object-files-satisfy-lgpl-relink-clause).
I've started a branch to do this: https://github.com/nh2/static-haskell-nix/blob/28445162b26e5b9ef1d9b741132c83e122f7d1a9/survey/default.nix#L41-L49 Don't know yet if it works, it's still building.
Yes, it's a good idea. If you enjoy the tools you are using for your job, you usually have a better life and feel less of a need to play with the nice tools at home in your free time, giving you net free time. Starting your career with Haskell isn't a drawback. Note though that being proficient in other languages is always a plus, even if you are a Haskeller. Source: My first full-time job was a Haskell job, and I have participated in hiring people out of university into Haskell jobs; they are doing great.
Thanks, great writeup. I just bought one of the recommended books and bookmarked the Haskell Data Science web page.
Thanks, may I suggest WITH_TERMINFO=NO, too. Maybe separate builds to see which one might be a problem. Also since terminfo is involved, perhaps test resulting ghc with en_us.UTF-8 and C locales in addition to one or more variations on TERM. I'm pretty sure the code path without TERMINFO is less tested and might have problematic assumptions in place. Just gut-feeling speculation based on my last experience a year ago.
It would be nice to have a real-time mode for the GC or something that is tuned for games and such.
&gt; Sadly that starts to step outside what we can express in Haskell. In Haskell or in `base`?
That's top 1 reason (but not the only one) for us to create our custom Prelude — `relude`: * https://github.com/kowainik/relude#relude
I certainly wouldn't be opposed to someone writing a new integer library around a non-GPL library.
Could you maybe expand on that? What is `FileSystemImpl`, does it come with a `MonadIO` constraint? Do you have some example code? And how do you deal with handling semantically meaningful exceptions? Example: A directory does not exist, and you want to ask the user if they want to create it, or something like that. If you use `UnliftIO.catch`, you suddenly may have something like (MonadFileSystem m, MonadInput m, MonadUnliftIO m) =&gt; ... which of course means you are able to do arbitrary IO again. Making `MonadUnliftIO` a constraint of `MonadFileSystem` is a no-go. You might consider omitting `MonadInput` and making the interactive query an implementation detail of the corresponding `MonadFileSystem` implementation, but now we have a separation-of-concerns issue, or at least, when reading the code implemented in terms of abstract methods of `MonadFileSystem` alone, the behavior of asking the user is likely unexpected. So, do you use a `MonadCatch` constraint then? I found this approach particularly awkward in combination with `conduit`, which has a `catchC` function, again with a `MonadUnliftIO` constraint. When I got to fumbling around with `CatchT`, I got the feeling I was "doing it wrong" at that point.
&gt; Enchiridion - a Late Latin term (derived from the Greek word ἐγχειρίδιον (enkheiridion)) referring to a small manual or handbook. For those who, unlike myself, were unfamiliar with this word.
Is there a comparison of the various preludes somewhere? How is relude different from Protolude or Universum for example? 
Yes, there is such comparison! You can read brief description of `relude` here as well as short descriptions for Protolude and Universum: * https://guide.aelve.com/haskell/alternative-preludes-zr69k1hc#item-ikoqdt69
I thought this was going to be unfortunate news from the title. But we get *two* books now?! This is great! Thanks for all your hard work!
I wasn't talking about boosting your carreer, Just pointing out the observational distribution of engineering vs. crafts type of software development jobs contingent on different languages.
Good to know this. I wish however there website allowed discounted prices for people from developing countries based on something like Big Mac index or whatever. For example, LWN allows a starving hacker subscription which is appropriate for someone like me from a developing country. 
I'm such a fan of `rebase` that I now hide `base` in favor of `rerebase`. I'll definitely check out `relude`, though.
&gt; typeclasses.com That name is annoyingly good.
`FileSystemImpl` is something like this: class FileSystemImpl a where fileSystemImpl :: e -&gt; Impl `Impl` is a record of functions that would be called from `MonadFileSystem (RIO env)`, so maybe data Impl = Impl { readFile :: FilePath -&gt; IO String } &gt; And how do you deal with handling semantically meaningful exceptions? I don't have a great answer here, unfortunately. `MonadFileSystem` is actually not a great type class example - type classes should be much more "logical" than this, rather than tied to an implementation. Really the type class should be the things you want to do with a file system, not unbounded access to a file system. When you start looking at things through this lens, messy exceptions aren't so interesting any more, and get pushed directly into type class implementations. The exceptions themselves can then be truly exceptional - where you don't want to be catching them, only noticing them and doing reasonable cleanup. &gt; A directory does not exist, and you want to ask the user if they want to create it, or something like that. One option is to do this in the implementation of `MonadFileSystem` itself. If you're implementing in a monad that has access to IO, then this is trivial (you have access to `IO`s `catch`). &gt; So, do you use a MonadCatch constraint then? Maybe. Anything that can `MonadUnliftIO` I believe can implement all of `MonadCatch`, `MonadThrow`, and `MonadMask` - a `newtype` to do this would be a useful addition.
Unfortunately, simply omitting partial functions puts callers into an awkward position if they can prove that the use of the partial function would be safe. In such situations they would have to write error handling code for failure cases that can never arise. Because of that, some libraries provide partial functions in an `Unsafe` package, or with an `unsafe` prefix and call it a day. The Ghosts of Departed Proofs approach advertised on this subreddit on June 12th provides an interesting way to design more ergonomic APIs. It might be useful in many situations. Dependent types also might improve things.
29 USD a Month!
sweet! do you think this will make it into the official nixpkgs? as in a `static` namespace like haskell.compilers.static.{ghc843,...} 
Yeah, it is too much for me (I'm from a "developing" country). As comparison, The HaskellBook (by one of TypeClasses' authors) costs $59 USD and had taken **me** more than a year to read. Assuming [typeclasses.com](https://typeclasses.com) had a comparable amount of material, i.e. one-year worth of reading (for me), I would need to pay $300 USD. What is worse is that, the moment I stop paying, I would not have access to the material anymore (for example, to use as reference), regardless of the amount of money I have already paid. I certainly don't like this subscription model, at least no a such high price (there are books that cost less than $25 USD!).
What were the other reasons? Reducing manual imports for commonly used packages?
That's certainly the eventual goal, both for the compiler and `haskellPackages`. It's currently being worked on / discussed on https://github.com/NixOS/nixpkgs/issues/43795, help is very welcome.
I like `unsafe`, to me it means "please read the documentation, don't just rely on the types", not "if you fail to read the documentation, the consequences could be as bad as a segfault".
What exactly is the goal of `WITH_TERMINFO=NO`? Fixing the segfaults you mentioned? I don't seem to have any segfaults with my `INTEGER_LIBRARY=integer-simple` GHC in nix. Could you try my branch? It built fine and and managed to build the `dhall` executable statically with `integer-gmp`. On my branch, commit `b60fc8ee`, `NIX_PATH=nixpkgs=https://github.com/nh2/nixpkgs/archive/50677e46.tar.gz nix-build survey/default.nix -A haskellPackages.dhall --arg integer-simple true`. There's also a `cachix` if you want to use my binary cache, see README of the project.
Do we have a description of what's needed in terms of precision and performance? I guess there's a page in Trac about this. Let's find it...
Thank you! I did not imagine there were so many alt-preludes
Haskell.
I'm writing my own LISP language using Parsec &amp; the "Write Yourself a Scheme in 48 Hours", but with tweaks to make my language a little different. I'm trying to incorporate a "delay" function, where the REPL will sleep for a certain time, depending upon the user, but it does not work. (Code snippets are simplified) import Control.Concurrent (threadDelay) data Values = Number Integer | Float Double | List [Values] | Atom String data langErrors = NumArgs Integer [Values] | Default String type Env = IORef [(String, IORef Values)] type IOThrowsError = ErrorT langErrors IO eval :: Env -&gt; Values -&gt; IOThrowsError Values eval env (List [Atom "wait" : num]) = threadDelay (num) And here's the error: Couldn't match expected type ‘Int’ with actual type ‘[Values]’ In the first argument of ‘threadDelay’, namely ‘(a)’ In the expression: threadDelay (a) In an equation for ‘eval’: eval env (List (Atom "wait" : a)) = threadDelay (a) How can I use both Haskell and my custom types together? &amp;#x200B; &amp;#x200B;
Thanks for the in info! I am not so familiar with all the Haskell type system extensions
I dont use Nix and cannot find it. Can you link me to the ghc download?
Ryan Scott definitely deserves it, he was amazing to work with quite frankly ([`-XDerivingVia`](https://www.reddit.com/r/haskell/comments/8aa81q/deriving_via_or_how_to_turn_handwritten_instances/) would never have happened without him)
Remember that lists in Haskell are defined as data [] a = [] | a : [a], that is, they are build up by repeated applications of the "cons" constructor `(:)`, and the nil constructor `[]`. Whenever you see a list literal `[x,y,z]`, you only have syntactic sugar for `(x:y:z:[])`. You are confusing these two in your pattern match. Let's pattern match on the "wait" `Atom`. eval env _ -- a `Values` goes here eval env (List _) -- a (built-in Haskell) list goes here eval env (List (Atom "wait" : _)) -- By the definition of (:) :: a -&gt; [a] -&gt; [a], a list(!) goes here eval env (List (Atom "wait" : (_ : []))) -- Now we need a Values eval env (List (Atom "wait" : (Number num : []))) -- Now simplify eval env (List [Atom "wait", Number num]) But now we actually have three new problems. On the right-hand side, you want to use threadDelay :: Int -&gt; IO (). So you need to either convert your `Integer` to `Int` via fromInteger :: Num a =&gt; Integer -&gt; a -- Specializing to Int :: Integer -&gt; Int, or modify your definition of the `Number` constructor accordingly. Second problem. Your right-hand side does *not* return an `IO ()`, it has to return a `IOThrowsError Values`. In order to be able to obtain a value of this type, you have to do two things: * Apply some function to `threadDelay (fromInteger num)` so that it type becomes `IOThrowsError ()`. Then you can write eval env (List [Atom "wait", Number num]) = do -- We're in the IOThrowsError monad now changeType (threadDelay (fromInteger num)) somehowActuallyGetAValue * Secondly, you need to be able to actually return a result in `eval`. Do you know a function that may be of help? Why of course, it's `eval`. What you need to do is make your list pattern match more general: eval env (List (Atom "wait": Number num: rest)) = do _ (threadDelay (fromInteger num)) (ers :: [IOThrowsError Values]) &lt;- mapM (eval env) rest -- Apply eval on [Values] instead of Values let flattened = sequence ers :: IOThrowsError [Values] -- Hulk smash return (List [Values]) -- Fit, damn it Point being, either `eval` or `Values` need to be refined. You may make an `IO a` into an `IOThrowsError a` by using `Control.Monad.Trans.Class.lift`, making use of the fact that you are actually inside a "monad transformer stack". This is probably all a little much to digest. I would recommend the "Monad Transformers Step by Step" tutorial, which builds up an interpreter like this. Another thing to mention would be that nobody uses `ErrorT` anymore, you should use `ExceptT` (also from the `transformers` package) instead.
OK, now I'm in a predicament again. It's reporting that "\_" is a hole: `ErrorT -&gt; langError IO a0, where a0 is ambiguous type variable`,` Couldn't match type ‘Values’ with ‘ErrorT BubbleError IO Values’ Expected type: [Values`\] in the` (ers :: [IOThrowsError Values]`) an`d Data constructor \`Values :: Values\` is not in sco`pe in th`e retu`rn statement. I've tried to solve it on my own from what I know, but I am still having trouble. I've tried turning the "ers" statement t`o (erc :: [Values`\]), but it didn't really help, and I tried taking out the "\_" in th`e _ (threadDelay (fromInteger num`)). I've also tried assign`ing fromInteger (`num) in a let statement and then using that local variable in threadDelay, but no dice. 
There is a so-called [typed holes](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#typed-holes) mechanism in GHC. I just used it as a placeholder above, for where `lift` should go.
Btw you may need to add {-# LANGUAGE ScopedTypeVariables #-} at the top for the type annotations to work. I just included them for emphasis. The whole example isn't something you should too seriously.
I've always wondered why some pattern like this doesn't get used in the wild: ``` newtype NonZero a = UnsafeNonZero a unsafeNonzero :: String -&gt; a -&gt; NonZero a unsafeNonzero _reason = UnsafeNonZero -- Used like: unsafeNonzero "Increasing a non-negative number" (1 + abs x) ``` A kind of forced comment field, I guess. 
Reducing manual imports for commonly used packages and modules from `base` is also one of the main reasons. Turned out it's really convenient to not write imports explicitly for commonly used things like `Map/Set/HashMap/Reader[T]/State[T]/Text/ByteString/MonadIO/MonadReader/etc.`. They are used in almost every project anyway. These names are unambiguous and well-known. Though, I wouldn't use `relude` in my small libraries, because I like to care about optimizing depedencies. For example, if I write library for parsing files in TOML format, I would sacrifice convenience if favour of lower dependency footprint. But if I write CLI tool and web-application then difference in dependencies is not that significant and I can benefit from convenience provided by alternative prelude a lot! Other reasons to create `relude` are: 1. **Performance:** focus on more efficient string types — `Text` and `ByteString` instead of `String`, provide efficient implementations of `sum` and `product` types without space leaks, forbid `elem` and `notElem` functions from `Foldable` for `Set` and `HashSet`, etc. 2. Lift commonly used functions from `IO` to `MonadIO m =&gt; m` (it's just very convenient). 3. Explore new ideas and approaches for being included in standard library faster to figure out how this standard library should look like, what's useful to have and what's confusing, etc.
I think your understand of `unsafe` prefix is slightly exaggerated. `unsafePerformIO` doesn't violate semantics of Haskell programs. Semantic of `unsafePerformIO` is completely specified in language standard — this function just ignores given state token. It doesn't mean that you will get segfault every time you use this function. It just means that you need to be more careful. `superrecord` library uses `unsafePerformIO` to operate with arrays creation to provide safe interface for extensible records. `typerep-map` uses `unsafeCoerce` and `unsafeFreezeArray` to give convenient interface for fast implementation of dependent map. You just need to ensure manually, without help of the compiler, that your usages of `unsafe*` functions are correct, and then you're good to go. 
Thanks for writing that out and sorry for being lazy with my question; I just saw that you've already documented the reasons in your GitHub Readme.
Don't worry! I'm always happy to share my ideas and motivation behind projects I'm working on :)
&gt; Improper use of total functions: maybe wrong result. Improper use of partial functions: that and also maybe diverge. Improper use of unsafe functions: you're lucky if you only get one of the previous issues, but probably you have brittle and hard to debug UB on your hands. I like this part of your message! I guess it makes more sense to talk about classification of `unsafe` functions instead. Personally for me, having `Prelude: head: empty list!` is as scary as referential transparency violation. At least I have less incentives to use `unsafePerformIO` because of the `unsafe*` prefix and if I do, it's much easier to find improper usage of `unsafePerformIO` in my code (because there're won't be many of them) rather then figure out which `head` of my 10 `head` usages is wrong (though, now it's easier with `CallStack`s).
I never debugged `unsafePerformIO` problems in code. I guess I will understand you better after I suffer from this pain... But now you made things more clear to me, I see that difference is really huge. P.S. And I really like your proposal with `head!` and `head?` names! This would be so convenient.
the greek roots seem to mean "in hand", which i guess makes sense, manual's root means "hand".
love how everything is becoming subscription-only 
"manus" = latin for hand
I like "Previous message: Build entirely broken" Jokes aside, I would like to echo Simon here - thanks for all your hard work, Ryan!
Alternative preludes usually define head only for NonEmpty but I mostly use it for infinite streams. Some frequent combinations have library functions like `until` but not everything is covered. 
`relude` defines `head` for `NonEmpty` as well. Though next version will have polymorphic `head1` function that works for any `NonEmpty` type. It can be used for `Stream` type if only there was some standard `Stream` data type...
He already has [https://phabricator.haskell.org/rGHC44ba66527ae207ce2dd64eb2bce14656d474f6d1](https://phabricator.haskell.org/rGHC44ba66527ae207ce2dd64eb2bce14656d474f6d1) :)
The reasonml choice would be a blog post I would enjoy reading
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [nh2/static-haskell-nix/.../**b60fc8ee92db5841516a2225ac47061bf16b92ab** (integer-simple → b60fc8e)](https://github.com/nh2/static-haskell-nix/tree/b60fc8ee92db5841516a2225ac47061bf16b92ab) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e4orxjf.)
The main advantage is that it plays really nice with React, so you can reuse existing JS react components in Reason projects. Especially good for prototype web apps/tools where things don’t get too complicated (keeping in mind reason is still pretty new): built in (and very simple to use) redux-like reducer and router means you can get up and running really quickly with no fuss, know what to do reading some simple docs, feeling familiar with the way things connect (from a js background), while feeling the fp/type system confidence. FP gateway drug for reactJS aficionados.
Now we just need a GHC plugin to ommit an error when your argument isn't convincing enough
In this blog post I will properly introduce the popcnt, pext, tzcnt and pdep operations and how they relate to the performance of our conceptual succinct data-structure based CSV parser.
&gt; I guess it makes more sense to talk about classification of unsafe functions instead. I like this idea! Here is a suggested classification. The guarantees depend on receiving good inputs, so I will first need to classify values: * total: a total value is finite and contains no bottoms * correct: an incorrect value violates the invariants of its type, e.g. `NonZero 0` * valid: a valid value satisfies the preconditions documented by the function (or its postconditions, in the case of a result value) * good: type-correct, total, correct, and valid All right, now we can classify functions. All functions return good results given good arguments (otherwise the function's documented preconditions need to be fixed); what differs is what they're allowed to return given invalid inputs. 1. "total": Still returns good results given invalid inputs; that is, the function does not have preconditions. No prefix, e.g. `reverse`. 1. "partial": May return partial results given invalid inputs. Use the "partial" prefix, e.g. `partialHead`. 1. "dumb": May return incorrect or partial results given invalid inputs. Use the "dumb" prefix, e.g. `DumbNonZero`. 1. "unsafe": May break guarantees of the language (e.g. lack of segfaults, type-safety, referential transparency) given invalid inputs. Use the "unsafe" prefix, e.g. `unsafePerformIO`.
I like you classification. Only `reverse` doesn't work on infinite lists...
You might want to offer few free chapters. I'm probably not alone in being averse to shelling out cash after only ever seeing a table of contents. &amp;#x200B; Also, I don't want to tell you how to run your business, but I suspect you'll get more than twice as many subscribers if offered at half the price.
I so wish Haskell's grammar were such that tokens could contain non-word chars. Agda mixfix operators would also be lovely. One of my biggest peeves in Haskell is, as the Grinch would say, the noise noise noise NOISE -- to the point of making perl look tidy.
Could you elaborate on what you mean by "types depending on terms" and why it's bad? I have a vague idea of what you mean, but I'd love for it to be clearer.
Any tips on how to navigate ghcjs-dom? There are very little examples and the documentation is difficult to navigate. I want to create a new html input element, register input event and obtain value from it...
Things in that corner of the Lambda Cube. Basically, types parameterized (or indexed) by a specific value. `Vect n a` is an example, but it's rather a simple one. Propositions can be arbitrary complex.
Okay, I will delete it.
Alright, since we're having the "this is why Ryan Scott is the best" conversation, I just wanted to point people to [his excellent leadership on the `DerivingStrategies` trac ticket](https://ghc.haskell.org/trac/ghc/ticket/10598). It's a pretty long thread, and it's three years old at this point, and Ryan doesn't get involved until about a third of the way into the conversation, but I've always been impressed by his leadership through this particular bikeshed minefield.
Very unfortunate to hear. I hope there will be more opportunities and projects for Haskell Developers at NYU in the future. 
Then if you want to go for haskell I very much recommend this book : [http://haskellbook.com/](http://haskellbook.com/)
I very much recommend to not read HaskellBook. Instead it's better to read _Get Programming with Haskell_: * https://www.manning.com/books/get-programming-with-haskell
Looking forward to https://ghc.haskell.org/trac/ghc/ticket/14069 getting fixed (W^X violations) so that Haskell programs can run on OpenBSD again without having to be put on a wxallowed filesystem.
I second this recommendation. I've personally helped several people become productive Haskellers and this book is an absolutely fantastic resource.
It would be a lot better if you gave actual reasons for this recommendation rather than blindly throwing someone into a different set of reading materials without, at minimum, revealing pedagogical benefits of \_Get Programming With Haskell\_, let alone why it is better than \_HaskellBook\_.
Wait, what? I haven't read either book (nor the usual [LYAH](http://learnyouahaskell.com/) recommendation), so I can't comment on their respective qualify, but I'm starting to notice a pattern. LYAH used to receive a lot of praise, and then later it started to receive a lot of criticism in favour of Haskell Programming from First Principles (aka haskellbook), and now it is Haskell Programming from First Principles which is receiving criticism in favour of Get Programming with Haskell? I can understand the enthusiasm for each new book as it comes out, but why criticize the older books?
Haskell Programming from first principles is on my top list of amazing software textbooks that I've never regretted buying... for a single payment of $60 USD. I want to reward content creators for high quality content but $30 USD a month ($40 CAD) is almost $500 CAD a year. With that kind of cash, I can almost buy 2 full 3 book hardcover sets of TCP/IP Illustrated. (Picking an expensive but high quality software textbook as an example.) That's INSANE! I might shell out $60 USD to watch a set of high quality videos of my choosing. I expect those videos to be around in years time just like that PDF I bought a year ago. I don't want to add yet another overpriced monthly subscription to my credit card! It would be great if people like me had another option.
&gt; Javascript + Haskell As in "Javascript on the client side and Haskell on the server side", or "Haskell on both sides, by [compiling Haskell to Javascript](https://wiki.haskell.org/The_JavaScript_Problem#Haskell_-.3E_JS)"?
Javascript on both sides, but Haskell on client side - I cant imagine anyone wanting to compile Haskell to Javascript, just use Elm if you want functional programming in the web lmao.
GHCJS
&gt; Javascript on both sides, but Haskell on client side wat
&gt; I cant imagine anyone wanting to compile Haskell to Javascript, just use Elm if you want functional programming in the web lmao Elm and Haskell are both functional, but they have very different advantages and disadvantages! Depending on the project, one might make more sense than the other. Elm is much simpler than Haskell, but less powerful, so you might end up having to write a lot of boilerplate. Another reason to want to use Haskell on both sides would be to use the same types on both sides, or even the same code, e.g. to validate a form on both on the server side for safety and on the client-side for responsiveness.
It's an ad for a Discord server about both langs if I didnt make that clear enough lol.
I cant find the pirated copy of this, can someone provide me with one please? I dont have that kind of money to buy although I really wished I had to support the writers 
Probably because I had no idea Towards Functional Programming was a thing. Is it?
Thanks, I'd never heard about inductive Data Structures before. Maybe Tail Recursion and loops are actually equivalent? Is there some theorem I'm missing? 
Thanks for the feedback! I just added it, please check it!
So after a bit of googling, inductive data structures seem to be the same as recursive data structures, is that right? What would be a good source for learning more about how functional programming makes recursive data structures easy to represent?
Maybe not too directly, but I could see how the kind of person who likes Scala and Spark also would like to read something like this, and would go into TDS. I think if TDS thought it was ok to post it, then it is related enough. That's the reason why I submitted it to the programming section and not the data science section though. 
Tail call elimination isn't just for recursive (directly or indirectly) function invocations; that's just a special case which it transforms from "will blow the stack" to "equivalent to a loop". The general optimization can be applied any time the value returned from a function is computed by (just) another function call.
Is it common for hyper threading to result in worse performance in other languages?
That's general tail call elimination, not tail recursion, right? I didn't want to be too broad. What would be a good source to learn about other tail call optimizations? 
cachix is a binary cache; if used you'd still have to run `nix-build`, but it wouldn't have to build anything as it would fetch all pre-built outputs from my binary cache.
No theorem, but when you translate tail recursion to machine code, you get the same result as you do from a loop in C, for example.
Keep in mind that Haskell is lazy, so tail recursion doesn’t work as it would in other strict languages. You can end up with a bunch of nested thunks, and if there are enough it can cause a stack overflow.
Why would it be hard to imagine?
I don't get the argument about this being a natural construction. The goal was constant time reductions which is achieved with linearity. Then you arbitrarily decided to mess with scope and add the rest. You even answer the "why" question with "why not".
Thanks for the feedback. Linearity wasn't enough to achieve the goal of having a general model of computation, that is why I added duplications; which, again, is something that the λ-calculus already had, so, nothing new here. As per the scope thing, I agree that my argument was weak. I've edited the post. The reasoning is that adding a scope is an additional, artificial restriction, not the other way around. That is, without a notion of scope the implementation and specification becomes simpler, so one might use the Occam Razor to argue it is also more natural. Now, that is an absolutely subjective matter so I'd concede if you don't agree with that. The main point of the article was providing a new tool for interacting with the abstract algorithm, which seems to be more suitable than the λ-calculus. The rest can be considered fluff!
In Haskell, it's better to be *productive* than tail recursive. But, if you can't be productive, tail recursive is generally better than not. By *productive* I mean something very similar to productivity checking for codata, but I'm not sure it's exactly the same. Basically, you want as much calculation as possible to be "behind" constructors, so that reduction to WHNF (pattern-matching) does only as much work as necessary.
But that doesn't even make sense. JavaScript on both sides and Haskell on client side? Wtf. 
The optimization itself is the same (effectively, replace a call with a GOTO). We can call a recursive call in a tail position “tail recursive”; this is a very common pattern in functional programming. It’s more-or-less how a “for loop” is spelled in the functional world, and we’d like it to have the same space efficiency (i.e. not grow the call stack). Your article captures the gist of it. My only other comment would be that tail call optimization exists even in “non-FP” languages; every optimizing C and C++ compiler I know of will do it at least under certain circumstances. When languages don’t implement it, it’s usually either due to an ideological commitment to preserving an “as-written” stack trace for errors (e.g. Python), or because some feature of the language’s semantics or implementation depends on having the full stack at runtime (e.g. Java).
Thank you! I was looking for.information on this topic.
I'm NOT from a developing country and there is NO WAY I'm shelling out $30 a month. I enjoyed your first book and even bought it for a friend, but you have crossed the line. Best of luck.
Sure. Here is comparison of two books: * https://www.reddit.com/r/haskell/comments/82p0de/haskell_books_comparison/dvbt110/
HaskellBook criticised not because it old and there're new book. See my other comment with comparison of two books. 
I have heard reports of Haskell performing much better with N-1 before. Something about leaving a thread for something else...
Category theory is amazing! I only covered a bit of it in school so I'm no expert, but are all of those abstract data types inductive data types? Or are only the uniformly recursive ones inductive because there's some notion of induction that can be applied on them and not to the nested types? By the way if there's any good book you can recommend to me on this subject, I'd be thankful. 
Thanks! I'd actually read the whole list of languages that implemented it or not before writing the article, but removed that part from an earlier draft because it didn't seem relevant enough. Thank you very much for your comments! 
Put an indentation mode in your .emacs file. I will come back to this tomor
I think I can try to run benchmarks with `-N7` and see impact of that, and update post.
[removed]
Any particular reason you use proxies instead of type applications for the section on `the`?
The book is nice simply because it's up to date and quite big. Apart from that, it is just like any other Haskell resource out there. A new topic is introduced with a short paragraph about why it matters, then you go through the implementation of the typeclass and then you create instances for arbitrary data types. It's all very theoretical in the sense that you're expected to go from the abstract (typeclass, type signature) to the concrete (what do I do with this?). &amp;#x200B; I really wish someone would write a book where monads are just used. Without even mentioning the word monad. Humans are really good at seeing patterns. Just let people play around with maybe, either, traverse, sequence, bind and all those things and they are going to recognize the patterns for themselves. Only then introduce the typeclasses behind them, the buzzwords, the theory. &amp;#x200B; Also the difficulty of the book and of the exercises varies wildly and I can't help but feel that the motivation of the authors kind of tapers off towards the end. Implement a parser for the entirety of [the DOT language](https://en.wikipedia.org/wiki/DOT_(graph_description_language))? It's not like that chapter isn't already packed with exercises. &amp;#x200B; Don't get me wrong, the book is good. But it's an updated, polished, rehash of all the tutorials and books that came before it. And I just don't think that that's how Haskell should be taught, unless you already know a lot about e.g., category theory and lambda calculus.
&gt;https://www.reddit.com/r/haskell/comments/82p0de/haskell\_books\_comparison/dvbt110/ Contact the authors rather than explicitly asking for a pirated copy on reddit.
Wil do!
How do you evaluate this `(a. a) a` term? 
I've also seen the term "proper tail calls" being used when you have guaranteed tail call "optimization".
How does \`Y\` look like?
I just provided the reference in the hope that someone would find it relevant. I take it as a hint about the future of proxies. I am not pointing fingers at you.
I didn't take it as pointing fingers. It just felt odd as a reply to my comment until I realised it was meant as a 'see here for related things' rather than a direct reply.
Not requiring a type system and direct textual representation of inets is huge step forward! I wonder though, how much harder will it be to reason about your code, when variables can be used arbitrarily far from their introduction! 
&gt;ℕ is short for integer. O\_O &gt;Only bounded types can be monoids Why so? Integers under addition are a perfectly fine monoid... Apart from these two points, it is a nice introduction to roles :D. I wonder if the following would be possible: right now, the role is making sure that invariants aren't broken in a broad-stroke fashion. Perhaps it could be more granular -- say you allow coercions for `Heap` if you know the definition `newtype B = B A` and that the `Ord` for `B` was derived using GND. So something like -- :=&gt; implies there the derivation was generated by the compiler type role Heap (selective™ (forall a b. (Ord a :=&gt; Ord b)) -- ends up generating code equivalent to -- instance (Ord a :=&gt; Ord b, Coercible a b) =&gt; Coercible (Heap a) (Heap b) where -- ...
&gt; Indeed, I'm talking about monoids *w.r.t. `max`-operation*. Ah okay, I didn't realize that. Maybe I was going through the slides too quickly 😅
&gt; are all of those abstract data types inductive data types I've not commonly encountered the term "inductive data type". Structural induction would be a possible way to proof a proposition for all values of one of these types, so the name could be appropriate, but I've not heard/read it used.
&gt;selective™ &amp;#x200B; This something markdown did, or is this a legal identifier in Haskell? I'd be tickled if it were the latter.
Functors, Applicatives, and Monads are type classes, which are more analogous to Interfaces. Maybe implements these classes in one way, List another, and so on; their definition in source files starts with `instance`. Try browsing around Hackage or Hoogle, e.g. [https://www.haskell.org/hoogle/?hoogle=Maybe](https://www.haskell.org/hoogle/?hoogle=Maybe) I'd post more links, but Reddit doesn't care for it at my level. Still, if you want a deeper understanding of type classes and some of their implementations, get thee to [The Typeclassopedia](https://wiki.haskell.org/Typeclassopedia)
I'm intrigued: could you elaborate, perhaps with a small "productive vs non-productive" example? I'm also trying to explain elsewhere why explicit recursion isn't necessarily desirable (lack of abstraction, uncontrolled "GOTO-ish" nature) , but you may have an entirely different insight to add.
I don't think it is. I just made it up as a silly joke :P
The box analogy only applies to a basic interaction and "getting at" the values inside, but it suffices for many applications. It's a working understanding so you should be able to work with them until they start to intuitively click more. Just know there's a little more to it to that. Psst, you can use `print` instead of `putStrLn . show`. I recommend using `hlint` as it will recommend better ways of doing things and help your overall understanding
Not any harder since you can just forbit it! Keep in mind that variables couldn't move away from modules (if you consider a module a closed net with one free wire). If your consider a top-level definition as the unit of modularization (which I think is the way to do), then applications wouldn't be able to move things from a top-level function to another. In other words, your functions would be able to use that trick internally. And again, you can just ignore it!
Relevant: [`+RTS -qa`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/using-concurrent.html#rts-flag--qa) and [this GHC issue](https://ghc.haskell.org/trac/ghc/ticket/10229). Also, I discovered that on some machines Linux schedules threads badly on CPU cores when hyper threading is enabled. On my laptop with 2 real / 4 HT cores, when I have only 2 threads running, it regularly schedules the two tasks on two HT cores that belong to the _same_ real cores, reducing performance by ~2x. This doesn't happen on all machines I own. I started a kernel bisect about this in the past but so far haven't had the time to finish it.
LYAH starts out well, but later chapters turn into dense walls of text that no amount of quirky illustration can overcome. It desperately needs editing for style reasons. A bigger reason though is that it's not frequently updated enough to keep up with the current Prelude, so sometimes code in those later chapters simply doesn't work!
Pretty! ``` def Y: Y = λf. Y f Y ``` Using the "modularization" idea I commented on the other post. 
It seems that there is an obvious translation from lambda calculus to the abstract calculus, just using extra definitions to get around the no-duplication restriction: variable: [| x |] = x application: [| e₁ e₂ |] = [|e₁|] [|e₂|] lambda: [| λx. e |] = x = x'; (λx'. [| e |]) Would this ever result in the non-terminating or incorrect reductions that you mentioned? The lambda-copy rule can also be O(n) instead of O(1), since there can be many uses of v, right? And what is the exact semantics of the superposed copy rule? In (v = x0 &amp; x1) ~&gt; v &lt;~ x0 v &lt;~ x1 You replace `v` by two different things. Which `v` gets replaced by what? You can't just say the first `v` becomes `x1`, because it is possible to reverse the order of variables with some lambda terms. The way I understand the relation to interaction combinators, each superposition corresponds to a certain term, so this `&amp;` could be from a superposition in `v`, in which case the replacement depends on which branch you were in originally. But the `&amp;` could also come from superposition in another variable. I suspect that if you don't take this into account the reduction is wrong. Also, `&amp;` should be n-ary depending on how often a definition is used (which can be 0 or 1 times). Proper rules might look something like (v = λx. body) ~&gt; v = body v[#x=0] &lt;~ (λx0. v) v[#x=1] &lt;~ (λx1. v) ... v[#x=n] &lt;~ (λxn. v) x &lt;~ {x | 0=x0 &amp; 1=x1 &amp; .. &amp; n=xn} and (v = {x | 0=x0 &amp; 1=x1 &amp; .. &amp; n=xn]) ~&gt; v[#x=0] &lt;~ x0 v[#x=1] &lt;~ x1 ... v[#x=n] &lt;~ xn Note then that variable occurrences will look like `v[#x=i,#y=j,..]`, so they can have multiple arguments, which will come from nested lambdas. So you need to decide what to do with the extra arguments, perhaps (v = {x | 0=x0 &amp; 1=x1]) ~&gt; v0 = x0 v1 = x1 v[#x=0,#y=0] &lt;~ v0[#y=0] v[#x=1,#y=1] &lt;~ v1[#y=1] v[#x=1,#y=2] &lt;~ v1[#y=2] Maybe this mess can be avoided by clever use of new variables at the point of the lambda copy rule.
Oops, I messed up on. I should have clarified that, in this language, a variable bound by a definition `v = x` can only appear up to two times globally (whereas a variable bound by a lambda can only appear once). That's why I omit the indexes. If you allow `v` to be used `N` times, then, yes, `&amp;` would be N-ary and beta application would be `O(N)`. Note that in this case, there would not be a 1-to-1 relationship between abstract calculus terms and interaction combinators. In practice, I'd rather leave the core with binary nodes to keep it simple, and enable multiple `v`s as a high-level language syntax sugar. &gt; Would this ever result in the non-terminating or incorrect reductions that you mentioned? Yes, because, on your translation, `x = y; z` means each occurrence of `x` is replaced by `y` in `z`. Due to the rules of the abstract calculus, that would only be the case if `x` doesn't have any duplication inside itself. You could solve this issue by using the extension I proposed (which allows labels for `&amp;` and `=`) and having a different label for each definition. In that case, your translation becomes equivalent to what I was doing before. It is sufficient to reduce pretty much all λ-terms. The only problem is when a term is applied to a copy of itself (such as `(λx. x x) (λs. λz. s (s z))`), in which case the second duplication will fail to complete and you'll get an abstract calculus term which corresponds to no λ-term. 
It's possible to put role annotations on classes, though it requires `IncoherentInstances`. With that you can say `(Ord a, Ord b, Coercible a b) =&gt; Coercible (Ord a) (Ord b)` which just pretends your `Ord a` dictionary is an `Ord b` dictionary. Clearly insanity, doesn't really help in general, but you could use this.
\&gt; \[..\] \`IncoherentInstances\` \[..\] Totally nope-ing out of there.
`f. Y f shouldBe Y`
Yep, edited. Note that `Y` isn't even needed anymore as recursion (and mutual recursion) is trivial to express. For example, here is a function that multiplies a number by two: ``` @twice λnat. nat λpred. λSa. λZa. (Sa λSb. λZb. (Sb (twice pred))) λSc. λZc. Zc ``` [Here is its evaluation.](https://gist.github.com/MaiaVictor/41fe66bed568980dc4da1685ebecd35b)
What other comment?
I came up with [a toy example](https://www.reddit.com/r/haskell/comments/9208u7/why_does_my_recursive_factorial_function_loose/e35afcx/) last time TCO came up in the subreddit.
Different people have different approaches to learning things. The approach I would suggest (based on my experience) would be to try to work on some small application that you'd like to use. Once you see repeated code patterns, try to find out answers to "how do I do X (better) in Haskell" or. Once you do that, you'll naturally fall onto the path of understanding these concepts. The benefit of this approach is that you'll understand the ideas and their usefulness _in context_. Feel free to ask for code review on r/haskellquestions or here. 
This one: * https://www.reddit.com/r/haskell/comments/99gbxf/i_want_to_learn_haskell_any_book_or_video/e4qda0e/
Luckly hyper-threading is such a security issue now that nobody uses it. Problem fixed.
I love following along with this series. I meant to take some time to learn absal a while back, but didn't have the motivation, but maybe now I do. I wonder how well absal fits with total languages.
Although that book is great, **it is not good material for learning *Haskell.*** That is for learning *Category Theory*. Although it's very possible to apply a lot of category theory to Haskell, and a lot of structures are directly lifted from it, it is by no means a prerequisite to learn Haskell, and neither is it the best way to do so. In fact, it's a really, *really* bad way to learn Haskell, because it gets *very complex* and is likely to scare you away as a beginner. But you don't need to think Category Theory to think Haskell. Category theory isn't even necessary to program high-level Haskell. 
First of all, an in-depth look at type classes and their use: To clarify, type classes are a concept rather unique to Haskell, but bears strongest resemblance to interfaces and traits. A type class describes certain properties and methods (that is to say, functions. No relation to OOP methods.), that any instance of that type class must adhere to. Specific types can be made instances of these type classes by fulfilling these properties and providing implementations for the type classes methods. Then, in code, one may use those methods in reference to that specific type, or any other type that is an instance of that type class. It's important to stress that although type classes may be used for ad-hoc polymorphism, the real point of them is to facilitate code reuse and code reasoning. That is why laws are so important. Take [Functor](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Functor.html#t:Functor)/[Applicative](https://hackage.haskell.org/package/base-4.11.1.0/docs/Control-Applicative.html#t:Applicative)/[Monad](https://hackage.haskell.org/package/base-4.11.1.0/docs/Control-Monad.html#t:Monad). The methods these provide are incredible, but they are only meaningful because of the laws to these classes, for example, fmap id = id and a &gt;&gt;= pure == a This means that the methods provided by Functor/Applicative/Monad behave predictably. This why we can get functions like replicateM :: Applicative m =&gt; Int -&gt; m a -&gt; m [a] sequence :: Monad m =&gt; [m a] -&gt; m [a] which are free to be used and will behave sanely with any monad. You don't need to make special cases for these functions. Monoid is a superclass of Semigroup, meaning that any Monoid must also be a Semigroup. The laws of Semigroup/Monoid are the following: a &lt;&gt; (b &lt;&gt; c) == (a &lt;&gt; b) &lt;&gt; c mempty &lt;&gt; a == a &lt;&gt; mempty == a which establishes that the methods provided actually act like a monoid. Not only does these methods alone have a variety of different uses, a lot of those uses actually rely on these laws. For example, the Applicative/Monad instances for two-tuples, -- from GHC.Base instance Monoid s =&gt; Applicative ((,) s) where pure x = (mempty, x) (u, f) &lt;*&gt; (v, x) = (u &lt;&gt; v, f x) liftA2 f (u, x) (v, y) = (u &lt;&gt; v, f x y) instance Monoid s =&gt; Monad ((,) s) where (u, a) &gt;&gt;= k = case k a of (v, b) -&gt; (u &lt;&gt; v, b) rely on the type 's' really being a monoid, or else the Applicative/Monad laws would be violated. **Now, onto other structures powered by type classes:** The most popular examples of these, after Functor/Applicative/Monad and Semigroup/Monoid, are [Foldable](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Foldable.html), which generalizes the 'foldr' function, and [Traversable](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Traversable.html), which generalize the 'sequence' function above. Note that these are actually not inspired by Category, but are yet really useful and interesting structures. For more examples of leveraging type classes for powerful code reuse, check out [mtl](http://hackage.haskell.org/package/mtl), although it is somewhat advanced.
Any dynamic tracing tools can help?
™ is considered a symbol, so you can use it as an operator but not as part of an identifier.
Great slides, thank you!
&gt; Also if you know of any good example of a problem that can be solved recursively but not iteratively Every problem that can be solved recursively can also be solved iteratively with a stack. Every problem that can be solved recursively with tail recursion (without an explicit stack) can also be solved iteratively without a stack. Many problems are slightly shorter/more elegant with recursion, but there really isn't much difference. 
&gt;Psst, you can use print instead of putStrLn . show. I recommend using hlint as it will recommend better ways of doing things and help your overall understanding Thanks a ton! By the way what editor should I use? I m in VS code with Haskell plugins.
Whatever you're comfortable with. I use neovim with intero-neovim and hlint, and run ghcid in another window.
Is the strange scoping strictly required for this language to do what you say it does? I don't like the idea of removing scoping just because it seems arbitrary; it's pretty critical to my ability to sanely read code :P
I got this worked out. (I had installed an outdated version of the mode package). Thanks for the response, though! :-) 
Like the other person said, look through the Typeclassopedia and revisit it occasionally as your overall understanding increases.
sbvPlugin was a proof of concept I developed to see how one can bring SMT solvers closer to Haskell. (And to teach myself about plugins in general.) Unfortunately, the integration wasn't as smooth as I wanted: GHC-Core is still way too complicated for a reasonable translation. I've summarized some of the issues here: https://github.com/LeventErkok/sbvPlugin/issues/21 Also, it's hard to keep track of GHC internals: As stable as the core is, in each GHC release there're subtle changes in the API that make the plugins go out of date. Having said that, I think this would be a great project for someone interested in bringing SMT solvers closer to Haskell.
Just think of it as complex numbers, as long as you never take `sqrt(-1)`, they won't show up and you can just ignore their existence. But they are numbers and can show up, so it is nice for the underlying language to be able to deal with them.
From the Eta FAQ it sounds like it. It sounds like they intend to pull and push extensions and features from one to the other. They also mention they're waiting for GHC8 to "stabilize" and it will eventually be compatible.
&gt; But this made me quite insecure. These buzzwords seem to arise from Math and Category theory. I thought they were some kind of "Class" in Java or struct in "C"/"C++". `Functor`, `Applicative` and `Monad` are closer to interfaces that a type might implement. Also the implementations are expected to conform to some laws. This is not that different from the basic laws that implemenations of `Comparable` should satisfy in Java. Well, there is a slight twist after all. Unlike in Java, typeclasses can refer to the generic parameters of the types that implement them.
Ryan Scott is pretty much single-handedly the reason why all my packages continue to ship and remain well-maintained. 
Ah, well it is there because there are interaction combinator that correspond to lambdas for which their variables occur outside their bodies, and I wanted to make sure that every interaction combinator had a abstract calculus counterpart. I guess you can actually remove that and everything works just fine. But it will be somewhat harder to implement read-back, because you'll need to make sure to put "lets" inside their proper scopes (notice that, here, lets are always outside). Perhaps a topological sort of the variables would do it. Well all in all I'm not sure, probably works, but will make things more complex...
Negative excellent benefits means they give you an orange and a juicer and call it your health care package.
What do you mean ?
Do you have application-level logging? At least being able to narrow down which handlers are being called when your CPU spikes might be helpful.
Basically, setting N = number of cores means that the GC will get descheduled very frequently, resulting in bad performance (after all, you're normally not just running a single application on the hardware, and the kernel needs time slices, too), and spinlocks don't do well in this case of frequent deschedules among the threads competing on them -- because a deschedule effectively postpones all waiters in a busy loop. And the OS can't "wake up" threads like it can with a condition variable in a round-robin manner, spinlocks are often not fair -- so it can only keep rescheduling until hopefully progress moves forward. https://ghc.haskell.org/trac/ghc/ticket/3553
In some respects, the abstract calculus as described looks like a strange mix of λ-calculus and π-calculus. The global variables, at least, bear some resemblance to channels. 
Cores do not get to "double their frequency" just because HT is turned off, although they may get to have unshared cache. Frequency scaling is a dynamic process and HT is only one factor; in theory a chip should maintain base clocks even at max TDP. But, processors often can go higher than that when TDP is low, while HT increases your thermal footprint, making a tradeoff between bandwidth and maximum dynamic frequency. But it's nowhere near 2x, in my experience, on desktop machines (with adequate power/cooling). The real benefit of hyperthreading is that it masks pipeline bubbles and helps "hide" latency by allowing forward progress when threads stall; if a hyperthread must perform some IO (PCIe, SATA, whatever), or even handle a cache miss, that's fine because often the bubble can be masked by handing time over to the sibling thread so it can continue. Lots of workloads incur minor stalls like this, so this tradeoff is pretty OK for many things. Compilation is a tricky one and highly dependent on the tools and system in question. Realistically most developers have very good machines with plenty of RAM and IO to feed the CPU. But 4 cores vs 8 cores doesn't necessarily mean anything; on my 1950X for example, doubling the cores with hyperthreading gives a tremendous speed boost (50% to 80% I'd say) to nearly every C/C++ project I build. Haskell compilation on the other hand is notoriously CPU-bound (with many more allocations) and doesn't scale to parallel compilation very well, so better cache usage and higher single-core IPC is going to do better, I'd estimate.
You could try out Haskell-ide-engine, which works with a bunch of editors, including VSCode. Another nice one is to run ghcid in a terminal on the side (I think it might even have a VSCode plugin also). ghcid would only provide you warnings/errors though, not stuff like auto completion, but it is a lot more light weight :)
What was the book? They deleted their comment.
Now?
You might also want to check out [intero](https://commercialhaskell.github.io/intero/)
Pi calculus (channel) variables are scoped?
Yeah, that is what I mean. 
If your app is running in multi-threaded mode and has lot of heap data, then the idle garbage collector can also be a cause. Try with disabling idle garbage collector using RTS option `-I0` (that is a zero after uppercase i)
**Maybe** "check [http://learnyouahaskell.com/a-fistful-of-monads](http://learnyouahaskell.com/a-fistful-of-monads)"
Very clean, very simple.
See, I don't even know what that means. "GHC 8" doesn't exist as a thing; the super-major version number doesn't really have semantic significance for GHC releases. It gets incremented whenever GHCHQ thinks that it deserves to be incremented. There's no promise that significant new features aren't going to be added until GHC 9.0 or that things will 'stabilise' at all - in terms of features added, stuff changed, and general churn, there's no reason to expect the delta between 7.10 and 8.0 to be less than between 8.0 and 8.2, or 8.2 and 8.4, or 8.4 and 8.6 (well, except for the release cadence change meaning the latter two are likely to be proportionally smaller, but you get my point). In a very real sense, every GHC release is equally important, and if the Eta people are waiting for GHC to stabilise before forward-porting their stuff, they are going to be waiting a long time indeed; in the meantime, Eta and (GHC) Haskell are both going to be accruing new features and random incompatibilities, and the forward porting is only going to be getting harder.
Fascinating. I love reading about this kind of stuff. Thanks!
Yes , but there is nothing on the log . Also I am the only user so I know that no handler are called.
I didn't think of the GC but that could be that indeed. I might also try to run it with only one thread. 
https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/
agreed. 
Thanks for the post
I wanted to try it, but for some reason I couldn't easily install it (I'm using docker with an old version of Ubuntu). So I gave up this way for the moment.
It seems that idle garbabe collection is the culprit. I discover the \`-S\` options which shows when the GC kickstart. I cache LOTS of data in my app (2G alive according to -S) and have 20 threads running (20 processor). Each threads seems to be taking 10% which adds up to 200% CPU ... Anyway I disable it using \`-I0\` and see how it goes. (By the way setting anything greater or equal that 1 second seems to disable it). &amp;#x200B; Thanks.
same with \`StandAloneDeriving\`
You have to distinguish infinity and unbounded. Infinite sets can be (downward and/or upward) bounded as you pointed out. And I'vd never written "infinite sets cannot be monoids." That's the point. Anyway, if you enjoyed my slide, it's my pleasure!
&gt; a[b &lt;~ c] means all occurrences of a in b are replaced by c I think that's wrong given &gt; ((λx. body) arg) ~&gt; body[x &lt;~ arg]
Ooops, thanks.
Thanks for the video, There is a name for `on compare`: [`Data.Ord.comparing`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Ord.html#v:comparing) comparing :: Ord b =&gt; (a -&gt; b) -&gt; (a -&gt; a -&gt; Ordering) comparing = on compare and `sortBy (on compare length)` = `sortBy (comparing length)` is such a common pattern that we got [`Data.List.sortOn length`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-List.html#v:sortOn) in [GHC 7.10.1 (2015)](https://hackage.haskell.org/package/base-4.8.0.0/changelog) sortOn :: Ord b =&gt; (a -&gt; b) -&gt; ([a] -&gt; [a]) &gt;&gt; sortOn length $ group $ sort [1,4,4,4,5,5,5,3] [[1],[3],[4,4,4],[5,5,5]]
I already had a solution submitted about \`migratory birds\`, so before watching his derivation I did some code golfing on mine. I came up with &amp;#x200B; import Data.List main = interact $ show . snd . head . sort . map ((,) &lt;$&gt; negate . length &lt;\*&gt; head) . group . sort . map (read :: String -&gt; Int) . tail . words &amp;#x200B; It uses sorting on pairs in the later part, and I don't rely on stableness. Also I probably only compute length once for each group. Thanks, Tsoding, for motivating me to run through these exercises :-)
Oh, cool! Didn't know about this one. Thanks! :)
&gt; [pointing at “deriving Num” for `Id`] Underivable in Haskell 10, but we can share Word's impl[ementation] What do you mean by this, exactly?
In Haskell 2010 Language Definition, `Num` is not listed as a derivable class. But with `GeneralizedNewtypeDeriving` language extension of GHC, we can lift the instance definition of `Num Word` up to `Num Id`. That's what I want to comment there. Perhaps I must describe about `GND` before this point. Thanks!
Afaict, `NoStarIsType` makes you incompatible with GHC &lt;8.6, which doesn't strike me as a good default suggestion. I trust people who really want `*` as a type operator to read the linked documentation and choose a migration plan that suits them. To that end, I have changed the wording of my note to be a little less scary.
with N7 ``` Cumulative quantiles per tag 99% 98% 95% 90% 85% 80% 75% 50% Overall 4600 ms 4380 ms 3980 ms 3540 ms 3400 ms 3280 ms 3210 ms 1105 ms get 4600 ms 4390 ms 3980 ms 3550 ms 3410 ms 3290 ms 3210 ms 1145 ms put 4600 ms 4380 ms 3980 ms 3540 ms 3400 ms 3280 ms 3210 ms 1100 ms ``` with N4: ``` Cumulative quantiles per tag 99% 98% 95% 90% 85% 80% 75% 50% Overall 139 ms 105 ms 37 ms 17 ms 12 ms 8 ms 6 ms 2 ms get 139 ms 104 ms 37 ms 18 ms 12 ms 9 ms 7 ms 2 ms put 139 ms 105 ms 37 ms 17 ms 12 ms 8 ms 6 ms 2 ms ``` so performance with N7 was much worse. I haven't tried my patch on the installation yet, will update once do that
&gt;"We thus extend ScopedTypeVariables to bind type variables explicitly, obviating the Proxy workaround to the dustbin of history." Thanks `&gt; id @Double 4` `4.0` nice! I did not know.
Thanks for the link
Doesn't seem to be anything to do with Haskell. Maybe something to do with Emacs's terminal emulator.
According to the error : Variable not in scope: one :: t0 -&gt; t1 -&gt; IO a0 It looks like the input line is _interpreted_, it looks for a function `one` which could apply to `two` and `three` . So it's an intero bug, which disallows `getLine` without passing it to ghci - No haskell surprise here, so I agree : please correct the clickbaity post title :/
The user is saying it happens even when intero isn't loaded. I can't recall if you can use the intero REPL without starting intero...
I've always preferred the combinator approach over liftX functions, myself. Might be more verbose but it's consistent and goes up to N instead of stopping at 2 :)
Fun fact. Setting it to anything greater than 1 second has that effect because \`yesod\` uses \`auto-update\` to perform some background tasks every 1 second. So, the application never ends up idle for more than one second.
`import Data.List` &amp;#x200B; `main = interact $ head . head . sortOn (negate.length) . group . sort . tail . words`
It would be fun to know what happens when `"one two three"` is entered - with double quotes. If the `getLine` succeeds and receives the String, I wonder what surrealistic behavior the input `3.14` causes. So I'd bet the next REPL cycle has already begun and in fact the `getLine` is never reached - program already ended or stdin is muted ?
Overlapping instances only check the RHS of the instance: FooClass m and FooClass (Foo Maybe) overlap, since `m ~ Foo Maybe` would let them be equal. If (say, in another module) someone provided an instance for `IrrelevantClass` on `Foo Maybe` then you'd suddenly get incoherent instances. 
Thanks! I didn't know GHC ignored the superclass constraints. &amp;#x200B; I've tried putting \`{-# OVERLAPPABLE #-}\` in the \`IrrelevantClass\` instance and \`{#- OVERLAPPING #-}\` in the \`FooClass (Foo Maybe)\` instance, but it's still being rejected. I'm not too familiar with the semantics of overlapping instances, but am I heading down the right path?
I use environment files for great effect to run doctest. Specifying dependencies by hand doesn't work. (My) cabal-doctest is a hack. brittany should guard GHC calls, if envy shouldn't affect it. ghc API should make it possible. If it doesn't, it's a bug - but different one. Talking about reproducibility using what looks like dev environment example doesn't make sense to me. Show an CI / clear environment builds which break due environment files. 
This should work. Can you show the code and the error?
Yes, I'm quite well off for a typical Brazilian and 30 US$ is a price even I wouldn't be comfortable paying. This would br around 130 R$. It's quite a lot of money. It's more than I spend in a week for having lunch near my workplace.
That's not a fair price for a subscription model... Gosh! I imagine that it took a very high amount of work to write all the content, and that you have server costs, etc. But if you bring a 30 US$ perpetuity subscription to present value using the current 3.5% yearly interest rate is almost 1500 US$. If you actually wrote a 1200 page book you wouldn't sell it for 1500 US$...
You should also be able to reverse the shooting with the `Down` newtype, so `sortBy (comparing (Down . length))` or `sortOn (Down . length)` (not tested since on mobile)
You should also be able to reverse the sorting with the `Down` newtype, so `sortBy (comparing (Down . length))` or `sortOn (Down . length)` (not tested since on mobile)
E.g. for Nix users, when using new-build to develop incrementally, you will generate environment files that will break your `nix-build` targets if you're unaware and fail to filter them out of the source. These files have very frequently caused brittany to crash on me too, though I'm not sure of the reason (obviously brittany should pass the arguments to the GHC API to disable this, but breaking tools and just expecting them to make code changes without anything even resembling a warning is a problem). Environment files are a super useful concept. It just shouldn't be enabled so implicitly and by default. Your use case sounds like a good one, and it doesn't sound hard to make it do this stuff explicitly.
There is exactly *one* reason supporting this implicit behavior: convenience. But there are *many* important reasons, most outlined in the article, against them (there are even more for Nix users). The convenience can be reclaimed by introducing new cabal subcommands. This problem represents a massive departure from the principle of least astonishment. There is no way that these files aren't just another way to surprise cabal users, and they add an extra thing that GHC users *must* learn about to use GHC properly.
&gt; Specifying dependencies by hand doesn't work. ghc UI should make it possible. If it doesn't it's a bug - but a different one. I can only repeat - the fact that it is convenient does in no way justify going against fundamental concepts. If you can cope with a messy dev environment that is your choice. My examples does not need to make sense to you. All I am asking is that such features are never enabled by default.
Documenting [`Data.Ord.Down`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Ord.html#t:Down) for those that don't know, the definition is the same as [`Identity`](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Functor-Identity.html) newtype Down a = Down a The juicy stuff happens in [the `Ord` instance](https://hackage.haskell.org/package/base-4.11.1.0/docs/src/Data.Ord.html#line-60) instance Ord a =&gt; Ord (Down a) where compare :: Down a -&gt; Down a -&gt; Ordering Down a `compare` Down b = b `compare` a same as defining, (&lt;=) :: Down a -&gt; Down a -&gt; Bool Down x &lt;= Down y = y &lt;= x Down x &lt;= Down y = x &gt;= y (&lt;=) = coerce ((&gt;=) @a)
There are two interesting papers on relation of linear logic and interaction nets: * [From Proof-Nets to Interaction Nets](https://pdfs.semanticscholar.org/e18e/f7b6e9094e4bfa4514ef31379889f105ecea.pdf) * [Interaction nets for linear logic](https://www.sciencedirect.com/science/article/pii/S0304397500001985)
So when we sort on `(Down . length)` we are instantiating `cmp` to `(Down Int)` sortOn :: forall cmp a. Ord cmp =&gt; (a -&gt; cmp) -&gt; ([a] -&gt; [a]) sortOn @(Down Int) :: forall a. (a -&gt; Down Int) -&gt; ([a] -&gt; [a])
That makes sense ;-)
`sortBy (comparing f)` and `sortOn f` have different runtime behavior. - `sortBy` will run the comparaison function `O(n log n)` times, with a memory overhead of 0 (i.e. only the input and output list). - `sortOn` will transform your input list `[t]` to `[(a, t)]` internally, with `O(n)` calls to the comparaison function. On the other hand, it needs to store the intermediate list in memory, so there is a memory overhead of `O(n)`. So, if your comparaison function is trivial, use `sortBy`, if it is complicated (such as `length`) and you don't care about memory overhead, use `sortOn`. In any case, don't trust me, and run a benchmark. Here is a crappy benchmark: ``` {-# LANGUAGE ScopedTypeVariables #-} import Test.QuickCheck import Data.List import Criterion.Main import Data.Ord main = do lString :: [String] &lt;- generate (vector 100000) lInt :: [Int] &lt;- generate (vector 100000) defaultMain [ bgroup "abs" [ bench "sortOn" $ nf (sortOn abs) lInt, bench "sortWith" $ nf (sortBy (comparing abs)) lInt ], bgroup "length" [ bench "sortOn" $ nf (sortOn length) lString, bench "sortWith" $ nf (sortBy (comparing length)) lString ] ] ``` ``` $ nix-shell -p 'haskellPackages.ghcWithPackages(pkgs : [pkgs.QuickCheck pkgs.criterion])' --run 'ghc -O2 ./SortBench.hs &amp;&amp; ./SortBench' Linking SortBench ... benchmarking abs/sortOn time 213.0 ms (208.3 ms .. 218.3 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 215.4 ms (213.8 ms .. 217.1 ms) std dev 2.404 ms (1.701 ms .. 3.218 ms) variance introduced by outliers: 14% (moderately inflated) benchmarking abs/sortWith time 191.9 ms (187.2 ms .. 194.9 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 193.3 ms (191.8 ms .. 194.2 ms) std dev 1.595 ms (890.9 μs .. 2.329 ms) variance introduced by outliers: 14% (moderately inflated) benchmarking length/sortOn time 499.1 ms (481.2 ms .. 512.8 ms) 1.000 R² (0.999 R² .. 1.000 R²) mean 454.4 ms (423.6 ms .. 471.2 ms) std dev 29.71 ms (5.693 ms .. 38.90 ms) variance introduced by outliers: 19% (moderately inflated) benchmarking length/sortWith time 880.6 ms (849.6 ms .. 912.8 ms) 1.000 R² (1.000 R² .. 1.000 R²) mean 862.8 ms (856.4 ms .. 871.2 ms) std dev 9.002 ms (2.333 ms .. 12.12 ms) variance introduced by outliers: 19% (moderately inflated) ```
With quantified constraints, class (forall f. Applicative f =&gt; HeadConstraint f) =&gt; Head l where type HeadConstraint l :: (Type -&gt; Type) -&gt; Constraint head :: forall f. HeadConstraint f =&gt; (a -&gt; f a) -&gt; l -&gt; f l With lens one can use `^?` and `^?!` for both the total and partial version. For NonEmpty one can set the associated constraint to Functor to allow using `^.`. In practice, you’d probably want a less ad-hoc class and put it in a larger class like foldable.
And a quick test with `Weigh` gives: ``` Case Allocated GCs sortOn abs 137,518,672 132 sortOn length 138,352,344 132 sortBy abs 116,747,512 112 sortBy length 116,724,216 112 ``` (test code using the same context as the previous code) ```haskell mainWith $ do func "sortOn abs" (sortOn abs) lInt func "sortOn length" (sortOn length) lString func "sortBy abs" (sortBy (comparing abs)) lInt func "sortBy length" (sortBy (comparing length)) lString ``` 
Pretty sure the GHC compiler has a lot of work put into it to do a lot of fusing with lists; plus the prelude probably has a ton of rewrite rules all over the place to fire for lists (which, in theory, you could write yourself for your own implementations). Although I'm not sure how well this works (if it does at all) for any arbitrarily/naively written recursive function on lists.
Yeah, seems like it's not running interactively
If I was just reading the code, I wouldn't mind it (I use a font with ligatures which feels better with proper widths). If I was contributing to someone else's library like this, I would be terribly annoyed (because I switch computers sometimes). If I was an API user, I'd probably create a fork of the library `fancylib-ascii` and use that instead.
Unicode is cool, but input methods is a PITA. I wish there was some plugin that would autoreplace ascii ops with unicode during typing.
It's pretty easy to write something like this yourself for most text editors like vim or emacs or what not. The harder part is only doing it when you want it to happen instead of whenever you happen to input those characters. Doing that fully correctly ends up being a gigantic pain in the ass and basically impossible, so it ends up that the best route is usually to take the Agda route and do what they do for their [emacs mode](https://agda.readthedocs.io/en/v2.5.4.1/tools/emacs-mode.html#unicode-input). Or essentially do the same thing with an xcompose style input on your computer if you can (which is the route that I tend to take). Ultimately, the best tradeoff for me was to use a font with ligatures so I have the unicode look but the ascii input and easier interop with others. No fancy map symbols, and there's no way to "switch" ligatures or customize them per language in most text editors, which is somewhat unfortunate (it means changing the ligature for `&lt;$&gt;` is probably a terrible idea), but overall not too bad.
I like it!
The problem in this article is the following claim: "package environment files have no valid, sensible purpose. You always had the ability to pass package(-db)s to ghc as explicit commandline args." If we want to have a sensible discussion about this feature, we have to recognize the incredible difference between passing ghc a mess of command line args and having a local config file which can be generated and managed by tooling. Further, we have to recognize that there are lots of great workflows that package-env files enable. The fact that there are others that are impacted negatively is a problem, and we need more configuration. But having a bunch of people arguing with one another that their workflow is the only valid one is a recipe for an unproductive discussion.
Oh god, please no. The non-unicode syntax is well understood and more importantly _standard_. While I have some idea about conjunction and disjunction, I have no idea what ≪ is, nor do I even know what to google in order to find out. What are the syntax rules about unicode? Can they all be used as operators, or does it depend on their unicode block? Does (for example) `α ◇ β` parse as `(α $ ◇) $ β` or as `(◇) α β`? Looking cleaner is not a sufficient reason for introducing all of this cognitive dissonance imo.
I don't use Unicode in Haskell, but I do in Coq, and my experience has been a bit of a mixed bag. I generally don't like it when a library forces Unicode on me, though I appreciate that it can often make things look cleaner. The main disadvantages for me are: * Patchy font support. I quite often find that there are symbols a library uses that are not in whichever font I'm using. Sometimes I even end up using a proportional font, which I wouldn't want to do with Haskell. * Especially in fixed-width fonts, some unicode symbols can be more difficult for me to read, especially arrows and similar. They're just too small. I don't really see any advantage of, say, `»=` over `&gt;&gt;=` or `⧺` over `++`, and the latter are typically easier for me to read. Also mixing very similar symbols (e.g. `⋃` and `∪` or `*` and `✱`) can make life difficult (though I am sometimes guilty of this myself). * Entering unicode can be a pain. I've started using AutoHotkey for this, which works well enough for me. In summary, I'd suggest using Unicode sparingly, and to make sure that users of the library are not inconvenienced if they don't want to use Unicode.
The Haskell 98 Report does not say it directly, but from [_2.2 Lexical Program Structure_][1] it would appear that `ascSymbol` and `uniSymbol` are both included in `symbol`, on equal rights, and the same for `uniSmall`, `uniLarge` and `uniDigit`. The [Haskell 2010 report] does not appear to specify Unicode usage any further. [1]: https://www.haskell.org/onlinereport/lexemes.html [2]: https://www.haskell.org/definition/haskell2010.pdf As for standards, we may well assume that the [`base-unicode-symbols`][3], mentioned in the opening post, together with the [relevant ghc extension][4], constitute the current descriptive reference. [3]: https://hackage.haskell.org/package/base-unicode-symbols [4]: https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#unicode-syntax Does the above succeed in dispelling any portion of your dissonance?
"Standard" might be the wrong word, maybe prefer "lingua franca." People know how ascii haskell works without digging into the report or importing a library they've never used before. It still doesn't answer the googleability concern or how actual code gets parsed without looking at a unicode chart. I'm not your code reviewer by any means, but for my part I'd stay the hell away from contributing seriously from a project that required me to use unicode syntax. Just a data point! :)
It is very fortunate that you raised this question for discussion. Whatever the opinions are, the fact of the matter is that the ordinary _(one byte)_ symbol characters are next to used up by the operators in Prelude alone. _(I think `?` is free.)_ So, anyone who [has any mercy for the poor fish][1], and especially those who actually long to define their own operators, should look forward to the wider acceptance of Unicode. [1]: https://twitter.com/vamchale/status/1033068463928094727 Only a few fonts have a comprehensive coverage of Unicode. But this can be salvaged with overlaying fonts, so that most ordinary characters are displayed with your favourite font, that may have tiny coverage _(my monospace of choice, `Inconsolata`, barely supports greek)_, and the glyphs for more obscure characters are borrowed from some collection of _"fallback"_ fonts. For instance, `urxvt-unicode` is a terminal emulator that supports that. So, this objection is not of principled nature. It is the usual practice to constrain Unicode API definitions to modules postfixed with `.Unicode`, as you surely know. With that _(granted that every Unicode definition does have an alias in Ascii)_, anyone can use your library in the fashion they prefer. * * * I am very curious as to the link to your library. Is it of open source?
Assume that I will never willingly type a unicode symbol and program accordingly.
There is no consensus. There is, however, a pretty decent compromise solution: export both the ASCII and Unicode version of every operator. In fact, you can add the third version, a regular non-operator identifier. This is what the lens library does, sans Unicode. For example, instead of choosing between `(++)` and `(⧺)`, export both `(++)` and `(⧺)` and throw in `append` as well. If possible, keep all Unicode variants grouped in a single module, ASCII operators in another, and named variants in yet another module, and cross-link them using Haddock. 
You cannot search for `&lt;&amp;&gt;` in a usual search engine, as well. We have `hoogle` though. Does that invalidate the searchability concern that you are stating? Concerning your _"lingua franca"_ argument, it appears to be based on the assumption that the degree of memorization of operator definitions that you possess equals the golden standard thereof. I can hardly agree that such a standard exists, or that it should. Therefore, even though I am considering you an exemplary programmer, I would have a hard time finding your definition of the portion of the usual Haskell functions that everyone should have memorized, acceptable.
I say use ligatures or prettify symbols mode. You get the best of both worlds: anyone who prefers Unicode gets it, but anyone who doesn't can turn it off.
Yes and no. Nix doesn't need any special cabal config for the nix-shell / new-build combo to work out of the box. It's good to make sure there are no configured repositories but it's not required if you're a little careful. But making this opt out instead of opt in is almost as bad as the current state; people don't know about this problem at all until it breaks something and they spend an hour trying to figure it out. I had one guy having problems with their project. His nix-build worked, but he couldn't build with cabal (he was using old-build). But if he cloned his repo somewhere else, it built fine. We checked the project directory for hidden files or state of any kind and found none, not even a GHC environment file. We thought there may have been something elsewhere that had the path hard coded so we tried renaming. Did not work. So we moved it to another parent directory, and that did the trick. We were really perplexed by these observations, but eventually discovered an environment file in the original parent directory, which had gone bad because nixpkgs had been updated and the Nix GC had been run. Not sure where the one in the parent directory came from but it caused these extremely perplexing circumstances that took us almost two hours to figure out, despite that I already knew about these files.
&gt;If we want to have a sensible discussion about this feature, we have to recognize the incredible difference between passing ghc a mess of command line args and having a local config file which can be generated and managed by tooling. I don’t think anyone is proposing that there shouldn’t be a way to invoke GHC with the package database setup correctly. What people are proposing is that the difference between running \`ghc\` and running \`cabal ghc\` is fairly small and the latter does definitely not qualify as “passing ghc a mess of command line args”.
That's help. But it definitely still seems like a patch over an obviously fixable root problem.
Same here. 30 USD per month is crazy. 30 usd per year is more reasonable, with higher month to month subscriptions.
If you're using Emacs, you can do this on a per-language basis using Emacs's own functionality rather than ligatures in your font. Haskell mode supports this if you turn on the \`haskell-font-lock-symbols\` setting. The downside to this is that it messes up alignment if the replaced symbol has a different length than the normal one.
There's also [`sortWith`](https://hackage.haskell.org/package/base-4.11.1.0/docs/GHC-Exts.html#v:sortWith) function in `base` package and it has different performance than `sortOn`. You want `sortOn length` but `sortWith fst`.
Should I define \`Exception\` types as &amp;#x200B; data MahException = MahExceptionA | MahExceptionB &amp;#x200B; or &amp;#x200B; data MahExceptionA = MahExceptionA data MahExceptionB = MahExceptionB &amp;#x200B; what's the benefit of either variant?
&gt; a cabal config file If this is either ~/.cabal/config or the cabal.project file this would help.
Does filtering from the source means adding ".ghc.environment.*" to an ignore file. Can it be done for all people in a right place in nixpkgs?
At my $DAYJOB when I write code, I don't format it manually. We have tools for that. I do it explicitly but you can also have your editor format on save. Or even as you type. Similarly to that, you could use a formatter that replaces non-unicode with unicode and vice versa. All automatically without you even thinking about it.
I think tooling in Haskell is hard enough without adding more complications to the syntactical landscape. I avoid contributing to projects that use Unicode syntax. It's all Greek to me.
In PureScript, you can't produce a symbol operator with a corresponding regular identifier. That's an improvement over Haskell.
The more your code requires special software support, the more people you are excluding. 
How does [`traced`][1] work? What is it useful for? What is the motivation? With cool examples, if possible. [1]: http://hackage.haskell.org/package/traced
This is a duplicate comment, perhaps you'd want to remove it.
They might technically but input methods are lacking and I couldn't be bothered to set up a usable way to input them so I wouldn't use or issue PRs to such a library. If you're driven by aesthetic concerns, try a font with ligatures, like Fira Code so you can type &gt;&gt; and see » etc
The only symbol I would consider using is for \`forall\` (you see I don't even know how to type ∀, I have to copy/paste it ;-))
&gt; If we want to have a sensible discussion about this feature &gt; [..] is a recipe for an unproductive discussion. But calling opposing arguments unproductive and not sensible is fine, eh? The arrogance.
Languages live in their respective environments. If you are a noob and do not know what to expect from a language, that can be a killer feature for your enthusiasm.
it works fine with double quotes
Yeah it should ideally work in both. The ticket has been around for a while to add this I think?
Yeah, it can be pretty annoying if the tooling doesn't work, even though the Haskell Report explicitly says it should. Have you thought about upgrading to enterprise?
I'm a big fan of using the _correct_ symbols for things (e.g. "≠", "÷", "π" [emoji can buzz right off]), but inputting them reliably and efficiently is next to impossible if you work on multiple different machines and/or do not have admin-level access.
Your example code is pretty close already. The type `FilePath` is a synonym for `String` so the function `homeList` is almost what you want. The function `takeFileName` from `System.FilePath` will take a path and give just the name. E.g. "/home/user/foo.txt" -&gt; "foo.txt". So, you can apply the function `takeFileName` to each item in the list of file paths returned by `homeList` using `map`. For example: homeListNames :: IO [FilePath] homeListNames = fmap (map takeFileName) homeList `map` is of type `(a -&gt; b) -&gt; [a] -&gt; [b]` and takeFileName is of type `FilePath -&gt; FilePath` so thanks to currying, `(map takeFilePath)` is of type `[FilePath] -&gt; [FilePath]`. We would like to apply the function `(map takeFilePath)` directly to the list of strings returned by `homeList` however, the type of `homeList` is `IO [FilePath]` not `[FilePath`\] so we need to use `fmap` to apply the functio`n(map takeFilePat`h) to the list of strings inside of the IO action` homeLis`t. Common advice for learning programming languages is to jump right in with a project, but Haskell different from other languages, so often the amount of new concepts can be overwhelming. Don't be afraid to take things slow.
If the library doesn't build or load because someone is using a locale other than something like aa_BB.UTF-8, then that's a bug or at least an incompatibility for part of the potential user base. POSIX, C, en_UTS.iso8859-1 aren't going anywhere. Let's say someone has the required locale, then without something like an APL keyboard, entering the symbols is cumbersome and not worth it for most developers. This is one reason I stay away from Agda. If all you're after is code that's easier to read due to use of Unicode symbols, then Emacs' prettify-symbols or a font with matching ligatures is a good alternative. In either case, it's important that alignment doesn't break because symbols can have a different width than their ASCII version. I know Unicode is old news, but I highly value ASCII source code for simplicity, portability, and the confidence it will work in 30 years. There isn't even consensus on UTF-16 vs UTF-8 or which version of the standard to support. In comparison, ASCII is sufficient for writing source code and your editor can render Unicode symbols for you just like highlighting syntax. At a time when ASCII markup languages (Markdown, Org, ...) have won the mindsets over HTML files, I consider it a step back to require Unicode symbols for mutable documents freely. I work with developers from around the world and am glad ASCII has become the standard. Yes, it's a low standard, but so is English as the global language, yet it won partly because it has no umlauts. A variant of Spanish could have also won for consistency reasons. I have some coworkers that insist on inserting Unicode symbols in plain text files, and it's been a source of corruption and unneeded debugging.
I feel like this is really the most helpful response - it allows everyone to get pretty much what they want out of writing/reading, and you managed not to make a silly extreme statement about how you "hate" one way or the other (because really - if someone manages to muster hatred for something so trivial I find it hard to take them seriously!) Writing in ascii and having the editor display unicode in whatever form allows easy contributions from everyone.
When you catch `MahExceptionA` do you also want to catch any thrown `MahExceptionB` exceptions? If so, use the sum type (the first one). If you'd like to catch them separately then use the second method.
Unicode is _fun_ but makes things unnecessarily more complicated, in a strictly objective sense. If you write a library it should definitely export ASCII variations. On the flip side, someone refusing to read or contribute to your code just because of a few symbols seems like a pretty extreme position in my humble opinion. Using fonts with ligatures (e.g. Fira Code) can be a nice middle ground, you get pretty-looking glyphs in your own editor but you're not annoying anyone else. Many unicode symbols look unnecessarily dense in a monospace context anyway. I do find that `∀` instead of `forall` (or any very common symbol which replaces a longer English name) tips my scales a bit more to the pro-symbol side. But context matters. Personal projects are one thing, collaborations another.
&gt; Does (for example) α ◇ β parse as (α $ ◇) $ β or as (◇) α β? In any case, we should bump fixities to go from 0 to 1000, just to be future-proof.
Well, there's also [the Hasklig font](https://github.com/i-tu/Hasklig#readme), which lets you type your code normally, save it so that it uses the standard multicharacter representations, but to show some common multicharacter things squished into the width of one character.
I was talking about this just last night. I feel the ability to define Unicode operators is essential to a modern language. Yes, that can be abused, but so can single-letter identifiers, qualified imports, and lexical shadowing -- even user-defined operators are dismissed in many languages as a mere tool for obfuscation. I dislike anything I can't easily type across two ssh tunnels and a buggy, incomplete termios implementation -- because I occasionally need to do so to support production systems. https://www.goodreads.com/quotes/9168-programs-must-be-written-for-people-to-read-and-only and I do think there are occasions where Unicode symbols are the best way to meet that goal, but mostly ASCII does a pretty good job, it also makes the practical use of a library easier IME. I recommend exporting a ASCII, functional, curried version of everything, thinking twice before making your own operator, and thinking **thrice** because engaging in the use of Unicode, but if it really does make things easier to read *and understand* go for it.
Are there any monospace fonts that use ligatures to shrink multicharacter-width sequences into the space of one character?
While searchability is not great, it does work just fine for tools like `grep`. Call me old-fashioned. If I had to search for that one fancy emoji operator, all that Agda-mode input stuff from emacs wouldn't be of much help. Likewise, what if I don't know what a symbol looks like *exactly*? For instance, was that one lens operator I used `(&lt;&lt;%?&gt;!)` or something similar? In any case, I can grep for `&lt;&lt;%` or maybe even just `&lt;&lt;` and get a few hits. Same with using the haddock index or the "Quick Jump" form. With unicode, this is not so easy.
Ligatures don't mess up your alignment, so if the resulting symbol is one character wide it'll appear wherever the font designer placed it, but it'll still visually take up x spaces on your row. Font locking in emacs or vim will actually shrink the sequences into one character, though, which is why I tend to avoid font locking now since the alignment issues were way too annoying to deal with (I'm a big fan of vertical alignment)
Yeah, I used to do that too; I have vim's font locking method setup for a few things, but I can't really do the messed up alignment thing since I'm a fan of vertical alignment so I only do it to do things like replace . with the circle or other small replacements. (Although I think I still have most of the math symbols turned on since I like those more and don't generally align things vertically on stuff like `elem`...) Ideal for me would really be being able to have a mode that didn't screw up alignment but still prettified the symbols. Nobody's made something like that, though, and I don't know if either vim or emacs have a rendering engine powerful enough to handle things like that well. Their syntax highlighting engines are already fairly terrible and primitive, and a lot of the display stuff can't keep up with modern features either.
Bah, why tie them to a subset of the integers at all? Just have the fixity declaration give a few additions to the partial order and have the computer handle consistency checking, completing via transitivity, and ambiguity checking. Then there's always room to insert yet another symbol, and you don't have to guess at the proper "spacing" from another symbol.
So something like (+) :: ... deriving infix +
+1 Against unicode in papers as well, I audibly grown when I see a paper has decided to do so for Haskell code.
I agree, but Intero is not Haskell. It was a good idea to post your bug on the Intero issue tracker but I'm not sure why you forwarded it here.
I have a programmable keyboard with multiple layers, one of which is my 'unicode layer'. I hold a key on the keyboard and my keys turn into: ←↓↑→∀λℕ⇐⇒p∃∧∧¬αβερ×∩∪⊸ On a regular keyboard, I can imagine it's a huge pain. 
I think he means more like: infixl + lower than * You don't give a number, but a relation that helps build the partial ordering.
I see, very interesting. That makes me wonder how the fixity system we have right now works exactly, though. If I define two operators with the same fixity, all is well until I actually use them next to each other. Similarly to how a misguided C++ template might work out as long as you don't try to instantiate it with the concrete type it will fail with. But I wonder if the system you describe comes with a "stronger open-world assumption". I can't say for sure, but with the current system, more information is known statically - after all we do have a total ordering with respect to the fixity number. I think the situation closely resembles typeclass resolution, and we discourage overlapping instances, orphans etc. for a reason. And we do not have an ambiguity resolution method - it's either pick any via a language extension, newtype, or refactor. Do you know of a language where this kind of fixity system you describe exists? My point is, is it really impossible to silently change semantics? At first glance it should be, but there may be corner cases. One other aspect that comes to mind that is less hand-wavy is that this system might be very brittle. Operators now refer to each other. This reminds me of the situation in dependent languages where you rely on implementation details such as "Which argument does this function pattern match on?" in order to drive normalization. If one of these definitions changes, it may all break down. If not, I still have to keep looking things up, and remember more things.
It doesn’t require anything
I like the partial order idea, but I think I've found a problem. &amp;#x200B; Suppose you have this situation: &amp;#x200B; There are two modules, A and B, that don't know about each other. They each expose one operator. There are two modules, C and D, that depend on A and B, but don't know about each other. C exposes an operator that wants to be higher fixity than A's operator, but lower fixity than B's. D exposes an operator that wants to be lower fixity than A's operator, but higher fixity than B's. You want to import both C and D. &amp;#x200B; What should happen in this case? Modules A, B, C, and D are individually perfectly fine, but you are suddenly locked out of using C and D at the same time, due to an ordering conflict.
Are there any ways to used dependent types with fixed points?
Error would be reported on whichever was imported last. Could be resolved by hiding (any) one of the operators. A useful diagnostic is potentially difficult. There's no fixity declaration that could possibly be added that would make the full weight of the fixity declarations in C and D compatible. Note that you can't even *write* both C and D in the current system. So, I'm not concerned that this is a real limitation.
&gt; the ordinary (one byte) symbol characters are next to used up by the operators in Prelude alone. (I think ? is free.) `! # % &amp; ?` are the available one byte characters. 
Thanks for sharing
Are your keycaps programmable?
. . . I don't suppose it's possible to coerce (hijack) GHC Core's system fc into the abstract calculus, is it? (Still thinking about module - level ghc plugins . . .)
Function Programming for Mortals does just that: it's a very practical approach and _uses_ monads rather than _describes_ them. It just happens to be a Scala book, instead of Haskell. 
There's a particular pain point with Haskell I've struggled with for a while. Say you have data type with some fields: data Ex = Ex { exFoo :: Int , exBar :: Bool , exBaz :: (Float, Float) , exBaq :: Map String String } and a seperate data type with a constructor for each field: data ExField = ExFoo Int | ExBar Bool | ExBaz (Float, Float) | ExBaq (Map String String) What's the elegant way to write the function: makeEx :: [ExField] -&gt; Maybe Ex That construct (Just Ex) if all fields are provided in the list, and otherwise Nothing. If the same field has multiple occurances, the last one is used. Ideally, the list should ideally only be traversed once, and an intermediate data structure requiring a bunch of boilerplate shouldn't be necessary. It's possible to change 'ExField', if necessary, as long as it may be used to represent some field. A concrete use case of this would be parsing a list of attributes and then making an object out of it.
Hey, King\_of\_the\_Homeless, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
True.
what a lying prankster
Is there an open source application that implements /u/ephrion's [Three Layer Cake](http://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html) design?
Well it's NxM cost (as in, N list elements with hopefully unique constructors and M fields in the struct) but you can just use a list monad: ef :: [ExField] ef = [ExFoo 1, ExBar True, ExBaz (1,2), ExBaq []] mkEx :: [ExField] -&gt; Maybe Ex mkEx es = listToMaybe $ do ExFoo i &lt;- es ExBar b &lt;- es ExBaz f &lt;- es ExBaq m &lt;- es pure $ Ex i b f m
amazing!
Because I am a Haskell noob. Other Haskell noobs may not know if it is Haskell, intero or their own fault.
Those benchmarks comparing it to attoparsec are really impressive. 
I really like how idiomatic that is, but unfortunately it still traverses the list 4 times.
&gt; I guess part of the problem is that I do not understand how function arguments work in Haskell. Study how they work in a simple lambda calculus. Haskell has a number of complications above that, but to just get the feel of partial application and currying a simple lambda calculus will work fine. The specific calculus doesn't matter, as long as it does evaluation/reduction via non-dependent substitution.
&gt; my version using =&lt;&lt; operator. `mapM_ (\l -&gt; putStrLn l) =&lt;&lt; listDirectory "."` No need for the explicit lambda expression there. `mapM_ putStrLn =&lt;&lt; listDirectory "."` will print all the files in the current directory.
Fair enough. Hope the Intero devs manage to fix it for you!
&gt; My mistake was trying to pass the second argument to mapM_ like this: `mapM_ printFirstHalfOfSingleString (listDirectory ".")` It may help here to think of mapM_ having a simpler type than it does. If you start with `mapM_ :: (a -&gt; IO b) -&gt; [a] -&gt; IO ()` then `mapM_ printFirstHalfOfSingleString : [String] -&gt; IO ()` and `listDirectory "." :: IO [String]`. From there it's obvious there's no assignment of type variables that makes `[String]` = `IO [String]`, because `[]` != `IO`.
Well... ef0 :: [ExField] ef0 = trace "0" (ExFoo 1) : trace "A" (ExBar True) : trace "B" (ExBaz (1,2)) : trace "C" (ExBaq []) : [] ef1 :: [ExField] ef1 = trace "1" (ExFoo 1) : trace "D" (ExBar True) : trace "E" (ExBaz (1,2)) : trace "F" (ExBaq []) : [] ef2 :: [ExField] ef2 = trace "2" (ExFoo 1) : trace "G" (ExBar True) : trace "H" (ExBaz (1,2)) : trace "I" (ExBaq []) : [] ef3 :: [ExField] ef3 = trace "3" (ExFoo 1) : trace "J" (ExBar True) : trace "K" (ExBaz (1,2)) : trace "L" (ExBaq []) : [] mkEx :: [ExField] -&gt; Maybe Ex mkEx _es = listToMaybe $ do ExFoo i &lt;- ef0 ExBar b &lt;- ef1 ExBaz f &lt;- ef2 ExBaq m &lt;- ef3 pure $ Ex i b f m Yielding: mkEx ef0 0 1 D 2 G H 3 J K L Just (Ex {exFoo = 1, exBar = True, exBaz = (1.0,2.0), exBaq = fromList []}) 
&gt; If (say, in another module) someone provided an instance This is the "Open World Assumption" for type class instances.
Yeah, 4 times. Of course, the way it's written, and due to laziness, it will stop traversing the list at the first element that fulfills the pattern match. However, if you go with the restriction that you construct 'Ex' using the latest occurence of each field, you'd need to get the last element rather than the first, thus 'listToMaybe . reverse $ ...' which would traverse the entire list 4 times. Still that isn't strictly necessary. mkEx :: [ExField] -&gt; Maybe Ex mkEx _es = listToMaybe . reverse $ do ExFoo i &lt;- ef0 ExBar b &lt;- ef1 ExBaz f &lt;- ef2 ExBaq m &lt;- ef3 pure $ Ex i b f m produces 0 1 D 2 G H 3 J K L I E F A B C Just (Ex {exFoo = 1, exBar = True, exBaz = (1.0,2.0), exBaq = []})
No, because now you have to have a config file to *not* do the wrong thing. This is not difficult, surely?
&gt; Time, effort, and will 25% time, 50% effort, 25% concentrated power of will... [Remember the Name](https://genius.com/Fort-minor-remember-the-name-lyrics)
I assume you're talking about https://github.com/knupfer/haskell-emacs project. As far as I can tell it aims to address the same problem - writing Emacs extensions in Haskell, but uses different approach. It looks like it uses some kind of marshalling scheme to make Emacs data processable by Haskell with a caveat that not all Emacs types can be converted. It looks to me that an extension will look like an executable that reads sexps from stdin and produces output on stdout. Or, possibly, as a daemon process that communicates with Emacs over network. My project is a bit different. It wraps Emacs C API for writing new extensions that can manipulate Emacs values directly, without marhsalling. In this approach, an extension will look like a shared library/dll that can be loaded by standard emacs with `(load "/tmp/libmy-ext.so")`.
No, the keyboard is flashable with custom firmware: https://github.com/qmk/qmk_firmware
afaik, that package isn't abandoned. it works differently, via some rpc between your emacs process and Haskell program.
Yeah, it's mine. It should work, but admittedly I don't use it nowadays.
since you link against the GPL-licensed `emacs-module.h`, doesn't the project need to be at least as restrictively licensed too? e.g. something like license: GPL-3.0-or-later instead of `BSD3`? (i can just making an issue so we don't spam Reddit :-) 
The compiler could consider only the order of the operators involved at each grouping level of each expression. Then, it would only report an error if you tried to use the conflicting operators at the same grouping level. To fix the conflict, you would just separate the conflicting operators by usual means such as parens.
To be honest I've been using Haskell as my main language for over five years and I've never heard of intero :/
I ran into a similar issue and wrote this [tiny cpuinfo library](http://hackage.haskell.org/package/cpuinfo) to check whether or not HT is in use.
I would still prefer name like `alternative-combinators` or `alt-combinators` for the library instead of `parser-combinators`. Even if almost all of the combinators are naturally can be used as parser combinators, their types are quite general and having more general name for library stops people from thinking that this library can only be used for parser combinators: between :: Applicative m =&gt; m open -&gt; m close -&gt; m a -&gt; m a count :: Applicative m =&gt; Int -&gt; m a -&gt; m [a] eitherP :: Alternative m =&gt; m a -&gt; m b -&gt; m (Either a b) endBy :: Alternative m =&gt; m a -&gt; m sep -&gt; m [a] some :: Alternative m =&gt; m a -&gt; m (NonEmpty a) I would enjoy a lot using some of these combinators not only in combinatoric parsers implementation.
This isn't exactly a Haskell question, but I'm hoping one of you may nonetheless have an answer. &amp;#x200B; SPJ says: &amp;#x200B; \&gt;Plus you often bisect to find the patch that caused a bug and/or distil a much smaller test case. Both are extremely helpful. &amp;#x200B; I understand how \`git bisect\` works (overall), but how does Simon know he's bisecting? Are the \`good\`/\`bad\` labels somehow added to the commits in a way that other people can query/observe? &amp;#x200B; What magic is Simon performing, here?
Just have your `nix-build` run `cabal-sdist` first. `pkgs.haskell.lib.buildFromSdist` adds this for you.
This inspired me to do an evil solution, which runs at worst in O(n*log(n)): mkEx :: [ExField] -&gt; Maybe Ex mkEx efs = case take 4 sorted of [ ExFoo foo, ExBar bar, ExBaz baz, ExBaq baq ] -&gt; Just $ Ex foo bar baz baq _ -&gt; Nothing where sorted = sortOn (\ !a -&gt; I# (dataToTag# a)) -- From ghc-exts efs still not quite linear, but we're getting there.
Bos [doesn't like us anymore](https://www.reddit.com/r/haskell/comments/2olrxn/what_is_an_intermediate_haskell_programmer/cmohivk/).
[Bos just to busy](http://www.haskellcast.com/episode/010-bryan-osullivan-on-performance-and-efficiency) :D 
You can use [lucid](http://hackage.haskell.org/package/lucid-2.9.10/docs/Lucid.html) instead, because it's a transformer already: $ stack build lucid $ stack exec ghci GHCi, version 8.2.2: http://www.haskell.org/ghc/ :? for help &gt; :set -XOverloadedStrings -XExtendedDefaultRules &gt; import Lucid &gt; import Control.Monad.Trans.Reader &gt; import Control.Monad.Trans &gt; runReaderT (renderTextT (do p_ (do title &lt;- lift ask; strong_ (toHtml title)))) "Hello, World!" "&lt;p&gt;&lt;strong&gt;Hello, World!&lt;/strong&gt;&lt;/p&gt;" &gt; 
I think that the reason why people would use `ReaderT` in "realistic" applications is because if you have dozens of functions that deal with HTML, passing your data as an argument to every function is a pain. Also in such scenario the Reader would hold a more complex type than `User`, containing all the data that you could possibly need to render your document. If you have very few functions, or simply don't mind passing arguments all the time, then you don't need `ReaderT`.
Wow, I didn't know `getTag` exists. And for runtime, my version is O(n * log(min k n)), where k is the number of the fields. I think the runtime of `nubOrd` is also O(n * log(min k n)).
Lucid does it right. There is at most one `type` indirection which corresponds to convention (`Html = HtmlT Identity`), the major typeclass instances are there and even documented, and the internals are available for the ones that may be missing.
I've realized that rather than nubSortOn you can simply use discrimination: mkEx :: [ExField] -&gt; Maybe Ex mkEx efs = case sorted of [ (ExFoo foo : _), (ExBar bar : _ ), (ExBaz baz : _), (ExBaq baq : _) ] -&gt; Just $ Ex foo bar baz baq _ -&gt; Nothing where sorted = runSort sorting [(I# (getTag a), a) | a &lt;- efs] thus really making it O(n). I'm happy with that.
I'm not sure if "doesn't like us anymore" is a fair characterization :P. His complaint is clearly directed against _some_ actions of a _subset_ of the community, not against the community as a whole.
I see. I don't know much about it either, but I guess the "entertainment purposes" bit is hard to communicate implicitly if you're limited to using text. :)
I don't think that just because someone has made large contributions to the Haskell community, that they should be expected to continue contributing at the same level for the rest of their lives. If that's what they want to do, then great. If they want to scale back or refocus on other interests, also great. I say this because it just strikes me as weird to talk about specific people on a public forum in this manner. All of the libraries you listed have had comaintainers for quite some time. This is definitely the way to go. In terms of maintaining the contributions that Don and Bryan have made, I think everything is well in hand.
&gt; if you're limited to using text Remember that Putin kills a kitten each time you use an emoji. In any case, the comment is three years old. Regarding the libraries I am not worried either. Looking at the repositories themselves (instead of the empty contribution graph), it is obvious there is active development going on instead of just version bumps.
Grew up, found jobs
I believe Don Stewart works for Facebook now after a stint on Wall Street
And reports to BOS, yes.
My rare uses for GADTs are mostly when I need type tags for generic functions: data TypeTag a where IntTag :: TypeTag Int StringTag :: TypeTag String fun :: TypeTag a -&gt; a -&gt; a fun IntTag n = negate n fun StringTag s = reverse s Are there any plans to extend the GHC's TypeRep or type application so I don't need to declare my own GADT? The only missing part seems to be the pattern-matching. I imagine syntax like fun :: TypeRep a -&gt; a -&gt; a fun (_@Int) n = negate n fun (_@String) s = reverse s 
&gt; As such, one might wonder: what if we allowed x to occur outside of the function body? That’s what we’ll do, updating the definition of application to: [...] Does that break the world? Not really. Actually, I think it does. Unless you have *some* notion of scope, you'll end up capturing more variables than you intended. I think first you'd have to unique-ify all the names, which while possible isn't a constant time transformation. De Bujin indexes don't exactly help, since they change as you move in/out of scopes, so you can't eliminate scope entirely while using them for substitution.
They are still alive, but beyond that I don't think we need this discussion into particular individuals private lives.
Makes sense. Thanks :)
I would review some recursion, and maybe just code a few programs recursively in any language. That's what really tripped me up when I took my Haskell/Agda class in the beginning. It gets pretty abstract at points, so having those basics down really helped me. 
&gt;I would review some recursion, and maybe just code a few programs recursively in any language. That's what really tripped me up when I took my Haskell/Agda class in the beginning. I just took an algorithms class which covered recursion very wel (basic recursion, divide and conquer algo's, backtracking). I think I understand it quite well. (still tricky sometimes)
I agree. 
Howdy, folks. I'm the one who approved this post, but I see now that I probably made an error. I can only hope that we've all learned a thing or two about FOSS cultural norms in this discussion. **Thread locked.**
If it's haskell, then learn about list operations and the recursive definition of lists. This is probably the most important thing in haskell, at least when starting. Category theory helps, but only when you're interested in it. It isn't simple and you probably need a few reads and examples
How accessible is Category theory? I'm interested in math but have only taken Calc (Integrals, derivatives, series), linear algebra, discrete math and a logic class.
I'd suggest taking a look at Milewski's "Category Theory for Programmers": https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/ Iirc, he uses C++ for many examples, so you may follow it without needing Haskell knowledge.
[No different, only different in your mind. You must unlearn what you have learned.](https://www.youtube.com/watch?v=z4jeREy7Pbc)
Don't learn category theory if you are starting out
I enjoyed your investigation of that fusion phenomena, but I think this is a mistake. Keeping the whole idea of lambda binders but then removing scope doesn't make any sense. You can't let your algorithm dictate your language. You can use your understanding of the abstract algorithm to design a simpler interaction net that works with a sensible language, instead of just hacking off the piece of the lambda calculus that conflicts with it.
Honestly not that accessible, I think there are better things to focus on. If you are looking at Haskell, I'd look at iteration over lists, pattern matching, and list comprehension. If you get through that, maybe check out common functions like map and fold. You could also dip your toes into understanding the monadic elements of Haskell, i.e. IO, Maybes, etc. &gt; I'm interested in math but have only taken Calc I find it funny, earlier in my education, I had a notion that all high level math was calculus. It turns out, calculus isn't really important in most of Computer Science. Abstract algebra and category theory fit into languages and verification, discrete structures with algorithms and data structures, and linear algebra with graphics and data science. At least that has been my experience.
this. I went through this. This will happen, but only if functional programming is taught properly with program algebra (isomorphisms, anamorphisms, hylos, monads, etc), as it probably will. To me, functional programming is not only about learning recursion and a new sintaxe, it is a whole new way of thinking, where the goal is to actually calculate the signatures (types) and definitions of functions by mimicking mathematics by sort of replacing numbers and variables by functions (hence the name program algebra) 
If you know a language like python or JavaScript, try functional programming in those languages. Focus on things like recursion and first class functions , and lambdas. Try not to save data attributes into variables that are used all over, the whole point of functional programming is that just based on input and the function, you know what’s going to happen. No inspecting the data of objects or references to other files. Everything necessary to handle the input is inside the function (in theory). IMO, the functional part of Haskell isn’t that hard for beginners to pick up, it’s moreso the type system. It’s an ingenious system, so try not to get too annoyed with it. When I was first learning it I associated types with objects/classes so heavily that it was hard for me to understand types for functions. It’s actually way more straightforward though because, as I mentioned earlier everything happening is right there in the function, with very few calls to external sources. This means the type of the function is basically just the function itself, and that will make more sense as you bang your head against it. In Python, there’s a lot of really interesting and useful tools that are only available due to its functional nature. I think it’s unmatched in the realm of FP/OOP hybrids. Some python functional basics are things like the map() function, where you pass in a function and a list (or any other iterable) and it applies the function to each element in the list. For example you could have a function which takes in an integer and returns the square like def squareFunction(num): return num * num You could then declare a list like listOfNums = [1, 2, 3, 4] Now the magic: you can map the function to the values like this: map(squareFunction, listOfNums) This would give you [1, 4, 9, 16] As you can see, in Python you can pass functions as though they were regular variables (because that’s what they are in Python). Only hybrid languages can do something like this (an example of a first class function to use methods of other classes without inheriting the scope): Take the squareFunction() from earlier. Say it was a method which belonged to a class MathClass. Let’s assume MathClass is a massive class with a bunch of attributes, requires a bunch of initialization data, and takes forever to initialize. With first class functions, we can pass functions from class to class without having to initialize the class. It would be much better to just declare the function outside of the class, and simply set a method equal to it inside of the class For example the nonfunctional way: class MathClass: def __init__(self): ...lots of initialization... def squareFunction(num): return num * num Now you need to initialize a MathClass every time you want to use the squareFubction Here is a way that easily bypasses that constraint with very little change def squareFunction(num): return num * num class MathClass(): def __init__(self): ...lots of init stuff... mathSquareMethod = squarefunction That last line is the most important. It’s setting the squareFunction equal to the mathSquareMethod, so that it can now be used as a method of the class MathClass. Because python is built in C++, this does not cause any additional memory constraints. It just refers to the function by reference. Notice how the shift from the OOP way to the more functional way made each component more independent. You can call squareFunction from anywhere and not worry about any baggage, and you sacrifice nothing for it, because you can set the method in the class equal to it anyways. This is what is commonly known as “purely functional” or just “pure” programming is. You can tell everything you need to know about a function by looking at it. No need to look anywhere else, as opposed to OOP where reading a method usually involves understanding what data is associated with the class of that method. I have a giant python interview in 4 days, so I figured it’d help us both to explain that in depth. I need to know this shit ASAP! Lol
I'd say not to bother taking up Category Theory until later on, if you are still interested in it. It's incredibly interesting, and it's related to Haskell, but I don't find it really helps me program in Haskell, especially at first. If you would like to understand how Haskell lists are implemented, like the above poster mentioned, I wrote a few articles about this, and how the Functor, Applicative, Monoid, Monad etc typeclasses for List work, and how you'd write them yourself here: [https://safiire.github.io/blog/categories/haskell/](https://safiire.github.io/blog/categories/haskell/)
### Some Relevant Concepts I think some of the hallmarks of Haskell specifically are: - Type system - Hindley-Milner type inference - Algebraic data types - Type classes - Lazy evaluation strategy - Infinite data structures - As a consequence: totally pure Some bread-and-butter functional programming concepts include: - Currying / partial application - Higher-order functions (closely related to the above) - Function composition - Purity and immutability - Recursive functions / recursive data types Some deeper mathematical fields related to FP: - Lambda calculus (this is basically the heart of FP) - Category theory / abstract algebra (ties into how types behave) ### 2-Week Prep Ideas Figuring out what to cover / how to prep in the abstract, especially in only two weeks, isn't totally obvious to me. Predicting what will be the most bang for your buck (/time)… that's tricky. I agree with others that being very comfortable with **recursion** cannot hurt. You'll end up using it a lot in practice. I also second the recommendation to learn about **algebraic types** / type constructors if at all possible. A nice video showing how these sorts of things are used in practice (in F#, at least) is Scott Wlaschin's [Domain Modeling Made Functional](https://www.youtube.com/watch?v=1pSH8kElmM4). I also wrote an article on sum types in JavaScript that may or may not help you a little bit ([Better JS Cases with Sum Types](https://medium.com/fullstack-academy/better-js-cases-with-sum-types-92876e48fd9f)). If I were to go out on a limb and make a suggestion that might not be universally agreed on, I'd say that you could go a long way if you pick up the fundamentals of **lambda calculus**. LC is the heart and soul of how FP even _works_ from a "low level" perspective. Some people call it the "assembly language of functional programming," which isn't strictly accurate but has a nice sort of symmetry to it. Specifically, I like LC for the following reasons: - It shows you how powerful simple functions composed together can be - It forces you to learn the habit of mentally substituting values into parameters and running function evaluations in your head - It forces you to understand higher-order function usage, that is, taking in and returning functions as first-class values - It shows the power of closures in capturing data to be referred to later Lambda Calculus isn't programming per se, but it is a whole lot of function evaluation, combinators, higher-order functions, etc. There is a reason that Julie Moronuki &amp; Chris Allen lead with it in [Haskell Programming From First Principles](http://haskellbook.com/). For a crash course in LC concepts, I humbly recommend my own two-part talk ([A Flock of Functions: Fundamentals of Lambda Calculus](https://github.com/glebec/lambda-talk)) on the subject, which is illustrated via JavaScript examples. For a good book on the subject, I can suggest Michaelson's [An Introduction to Functional Programming through Lambda Calculus](https://www.amazon.com/Introduction-Functional-Programming-Calculus-Mathematics-ebook/dp/B00CWR4USM). I recognize that two weeks is not a lot of time when you have other demands on your time and aren't already experienced in some of these topics, so pushing an entire book may seem laughable; however, even just the first 60 pages will teach you quite a lot! ### More resources While I personally think [HPFFP](http://haskellbook.com/) is the very best text on learning Haskell, it is also a 1200+ page behemoth. Conversely, [Learn You a Haskell for Great Good](http://learnyouahaskell.com/chapters) is free, online, and a very fast &amp; fun intro to the language. This may very well be the most practical and straightforward advice I can give you – if you know you'll be doing Haskell, well, learn some Haskell! I personally don't think LYAHFGG is great for getting people to the point of _knowing_ Haskell well enough to use it at work or something, but as a kind of crash course tutorial? It might be the most efficient way forward. Well, I hope this is helpful and not merely overwhelming. Good luck, and **have fun**!
The `let` and pair syntax is a *much* clearer representation of that idea. Thanks, it makes much more sense now. (λx.f a) --------- f [x / a] Should `(λx.f a)` be `(λx.f) a`? I look forward to the PhD thesis you ought to write on all this :P
The later, I've edited to make it more clear. A PhD is not something I'll ever be, but thanks! :) 
You may have seen it already but google 'Learn You a Haskell for Great Good'. The entire ebook is available online to read for free. 
First off, I'm really enjoying the content you've put out about this calculus. But why is it called the "abstract" calculus? The word "abstract" tells us almost nothing here. Lambda Calculus is "abstract", high school calculus is "abstract", etc. Could you just not think of a name?
Because it comes from the abstract fragment of Lamping's algorithm, which is a long sentence so I've been for a while shortening to just "abstract algorithm". Perhaps I could call it Lamping's Algorithm, but that sounded too personal? The other choice would be Fragment Algorithm which, yea. Oracle-less Optimal Reduction Algorithm? Well, I don't know. I'll let you rename it if you have a good idea!
&gt; why, precisely, do you believe scopes are essential? I don't. I think having binding operators that pretend to be scopes but aren't is silly. For example, I'm perfectly happy to program with combinators, which do not have any notion of scope. &gt; if I told you you can still make explicit scopes on the Abstract Calculus; i.e., close sub-expressions under unescapable boxes, making the λ-calculus becomes merely a language where you explicitly close every λ-body. Would that make the idea more sensible to you? No, because that was obviously going to be the case, since this new environment is still capable of universal computation.
The lambda calculus allows repeated variable names. So, you'll need to unique-ify the lambda form before you try and manipulate it. In particular, a lot of programs written in both the lambda calculus and real languages have both shadowing, and even if that's disallowed, will have duplicate name in separate definitions. For example something as simple as `//#l #r #t #f //l //r t f f #t #f t #t #f f` has a number of duplications, without shadowing. Defning the canonical values for a Church- (or Scott-) encoded encoded values often uses a consistent convention for the bound values as a visual verification your used a consistent order.
Perhaps rebranding λ's as channels / linkers / movers instead? Anyway, I find it weird too, but that's just the way it is. It is not like I can shape math to fit my personal tastes. Hopefully you agree that "thing" (whatever it is a calculus or not) is at the very least useful. For one it provides a textual tool to program interaction-combinator nets that resembles λ-calculus terms thus feels familiar, and then it is also a way to use the abstract algorithm that feels better than trying random λ-terms and hoping they work. (Have you seen the [simplified specification](https://github.com/maiavictor/abstract-calculus) by the way?)
I'm assuming `p` and `q` are always globally unique variables, too? Initially I wasn't sure if they could be full terms or not.
Need an ELI5; does this mean we will have a functional language without the need of a garbage collector?
We started our applications from Three Layer approach, but now architecture looks slightly different. * https://github.com/holmusk/three-layer
If you haven’t done it already, it might be helpful if you take some code that can be written without recursion in an imperative language (i.e. using loops) and rewrite it using recursion. Being able to easily translate between loops and recursion tends to be a pretty useful skill when picking up a functional programming language.
Very cool. I was trying to get permission to release almost the same exact thing that we built internally, but it never went through. Glad to see something similar out in the wild.
Curious if there was a video of the actual talk?
Hi, /u/captaintattertot/. Please note that this question is basic enough that it might've been best to have asked it in the "Hask Anything" thread pinned to the top of the subreddit. I'll let this one through, but in future please use "Hask Anything" for basic and beginner questions. 😎
Thanks for replying, very useful! About the list (recursive) : I guess it’s like a binary tree then? A root, and two binary trees as children? 
The following can be written with the current extensions: fun :: TypeRep a -&gt; a -&gt; Maybe a fun = match @Int (\n -&gt; negate n) &lt;|&gt; match @String (\s -&gt; reverse s) where match :: forall a b c. Typeable a =&gt; (a -&gt; c) -&gt; TypeRep b -&gt; b -&gt; Maybe c (&lt;|&gt;) :: forall b c. (b -&gt; Maybe c) -&gt; (b -&gt; Maybe c) -&gt; (b -&gt; Maybe c) Is that good enough?
The same could be said of using Haskell anywhere... but we still push that :P
Medium I'd say. It's kinda type theory, but most the things you learned yet won't really help you because it's really different. Learning haskell (at least beginner-intermediate stuff) will help you a lot to understand it, and if you understand the theory behind it, everything can be a lot easier. The notation of category theory can be pretty strange, so it might be good if you find a "tutorial" including haskell examples or stuff like that. Recursive algorithms are obviously required as the other comment mentions, you also won't get around them for category theory. I might extend this comment later, only on phone xD
Yes, also my opinion. Later on, it might help to understand the theory behind much functional stuff, but it's not really easy. It isn't incredibly hard, but without examples, you won't get far. With code examples, it still takes you many reads
&gt;checkout common functions Definitely do that, OP! Without a bit of knowledge of basic functions before starting up, you'll have much more work. They aren't really complicated, but it's important to understand those extremely often used operations.
&gt;checkout common functions Definitely do that, OP! Without a bit of knowledge of basic functions before starting up, you'll have much more work. They aren't really complicated, but it's important to understand those extremely often used operations.
If `b` is your button I believe it would be `get text b`.
I know `fun` is just an example, but why wouldn't you just write two functions? Low-powered APIs and implementations are typically better than high-powered ones.
Because some people wanted to take things to their logical conclusion.
Anything that's an arrow is an applicative, so the answer to your question is anything that's an arrow but not a monad.
Posta na APDA
It seems like lambda projection deals with globally bound variables - or should we be thinking about lambdas differently than normal? For example let (a,b) = (y0,y1) in (λx0.λy0.λz0.a, λx1.λy1.λz1.b) ----------------------------------------------------- pair-projection (λx0.λy0.λz0.y0, λx1.λy1.λz1.y1) The y0 and y1 are captured by the lambdas even though they are let-bound outside the lambdas which does not match my intuition about lexical scoping
This is really cool, but I have questions about how you guarantee reduction without scope. How do you deal with `let (x, y) = x in y` or `λx. ((λy. x) y)`? Regular lambda calculus has `(λx. x x) (λx. x x)`, but that *can* be reduced, it just reduces to itself. How do you deal with these stuck terms? 
Did you try raw JS? If local jQuery works (e.g. from the debugger) then chances are good that you can get there via JS-FFI that underlies ThreePenny-GUI.
nix-collect-garbage
See [https://nixos.org/nix/manual/#sec-garbage-collection](https://nixos.org/nix/manual/#sec-garbage-collection), \`nix-store --gc --print-roots\` and \`nix-store --gc\` are probably going to be the most useful commands.
Well the GADT version allows you to make some calls polymorphic in the type and other calls specific to a particular instance. For example, in my Redis library, you can `get` data for any path type, but you can only `incr` a path if its type is `Integer`.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [agda/agda-stdlib/.../**FreeMonad.agda** (master → 983e148)](https://github.com/agda/agda-stdlib/blob/983e1487e67d02ff9f3ee79c74e4f959e98aa799/src/Data/Container/FreeMonad.agda) * [agda/agda-stdlib/.../**FreeMonad.agda** (master → 983e148)](https://github.com/agda/agda-stdlib/blob/983e1487e67d02ff9f3ee79c74e4f959e98aa799/src/Data/Container/Indexed/FreeMonad.agda) ---- 
This is a long and indirect partial answer. It sounds like you are using `nix-build` to build a lot of things without really installing them. Since there are no garbage collection roots to keep those assets around, when you run nix-collect-garbage they get removed. I use a similar workflow -- I use `cabal2nix . --shell &gt; shell.nix` and `nix-shell` a lot to create temporary environments. I have almost nothing actually installed (including the compilers). But then I was afraid to run `nix-garbage-collect` because it would wipe out the good with the bad. So instead of using `nix-shell` I have created this alias: alias nix-dev='mkdir -p .gcroots &amp;&amp; nix-shell shell.nix --indirect --add-root .gcroots/dep' And now I run `nix-dev`. The idea is that it creates a `.gcroots` in the project directory that keeps those assets around. If I delete the entire project directory, then those assets can be garbage collected. I think (but am not 100% sure) that `nix-dev` only keeps the most recent set of build dependencies rooted. If not, then a fix would be to update the alias to wipe the contents of `.gcroots` before invoking `nix-shell`. It sounds like in your build process you need to do something similar to make sure that you are adding garbage collection roots for the stuff you want to keep around (the latest versions of all the build dependencies, etc), while allowing the obsolete stuff to be garbage collected. 
haha, I guess I had something in mind and forgot to update this part :)
&gt;:(
Nice! :) Can you describe how your architecture differs from the Three Layer approach now?
I recall this project from previous year GSoC: * https://hackage.haskell.org/package/haskey-btree
Not only without the need of a garbage collector, but running massively parallel, in the GPU. There is nothing stopping us other than resources to actually develop that thing. I've achieved 150m rewrites/s in a high-end GPU, which blows Haskell / GHC out of the water for arbitrary programs, and I consider myself a GPU noob.
The hue is too strong to me.
Yes, all variables can occur globally simply because interaction combinators (graphs) have no notion of scope at all.
Yes, it sadly doesn't match our intuitions about lexical scoping, which is probably the weirdest bit about this language. But there is no way around it, and it matches fairly well with the graphical nature of interaction combinators which have no notion of scope at all. Think of lambdas as bridges connecting their location to other location in the program.
Hmm. It looks promising, but after adding {-# Language ScopedTypeVariables, TypeApplications #-} import Data.Typeable (Typeable) import Type.Reflection GHC 8.4.1 gives me TypeRep.hs:8:7: error: • Couldn't match type ‘a -&gt; Maybe Int’ with ‘Maybe c0’ Expected type: TypeRep a -&gt; Maybe c0 Actual type: TypeRep a -&gt; a -&gt; Maybe Int Am I missing some more extensions? 
To give you the current example, I'm trying to re-implement First-Class Attribute Grammars from 2000 with safer types. One important part of the paper are the aspects, which are supposed to generate attributes for any type of node that can appear in the tree. 
Could you expand on this? You mean the resulting code would be inefficient or the conversion itself would be an inefficient process? If the former, could we limit the the evaluation to the code in a single module?
I made a mistake, the arguments of `(&lt;|&gt;)` must take two arguments.
Thanks for the link, it looks like a quite nice and comprehensive implementation.
It would be nice if there was at least a subreddit rule against being blatantly evil.
Is there a guide anywhere that can help me interpret type errors. For example, I understand what the problem is when I see the error `No instance for (AClass AType)`, and it even tells you what function you are using that is requiring an implementation of `AClass` for `AType`. But what about other errors, such as "could not deduce...". What does it mean that "r is a rigid type varible", etc. etc?
Looks like it can be done this way ([https://nixos.wiki/wiki/Garbage\_Collection#Proper\_nix-shell\_GC\_pinning](https://nixos.wiki/wiki/Garbage_Collection#Proper_nix-shell_GC_pinning)), but I've had no luck making it _not_ collect the assets, despite having the root. Then again, I'm pretty new to this, so maybe I'm missing something.
Why is \`fun\` port connected with \`return\` port?
GHC Core is basically lambda calculus on steroids. The problem with lambda calculus is that not all terms are translatable to abstract algorithm (you will get wrong result) or their performance is hard to predict (you change your code a little, and suddenly your algorithm will run 5x slower for no obvious reason). &gt; If the former, could we limit the the evaluation to the code in a single module? What do you mean?
At a glance, I would suggest keeping the button text as a `Behavior`, which you can them sample and update via events, [and feed to the button using `sink`](http://hackage.haskell.org/package/threepenny-gui-0.8.2.4/docs/Graphics-UI-Threepenny-Core.html#v:sink).
yeah, i spent an hour debugging why `nix-shell` wasn't working for my haskell project, which was depending on another package I was developing locally. pushing it and fetching worked, while the "same" code didn't when referencing the filepath. deleting the ghc.environment file (which i had respectively added to gitignore, then forgotten about) fixed it. i felt dumb.
The "return" port represents where the term is located. The application node has two slots, `(fun arg)`. Since the `fun` argument connects to the "return" port of the `λy.x` node, that means the first slot is where `λy.x` is. So it is read as `((λy.x) arg)`. Moreover, the `arg` port connects to the "var" node of the `λy.x` node. Thus, that application is read as `((λy.x) y)`.
Maybe I am missing something, but a major point of anything having to do with mtl-style effect classes is the ability to reason about code locally. If you have `MonadIO m` constraint in the class definitions, that kind of goes out of the window - your user session can launch missiles now. You can provide default implementations without having to rely on these kind of constraints via `-XDefaultSignatures`: class Monad m =&gt; MonadSession m where getSession :: UUID -&gt; m (Maybe Session) default getSession :: (MonadReader AppEnv m, MonadIO m) =&gt; UUID -&gt; m (Maybe Session) getSession sessionId = do sessionsMvar &lt;- asks sessions sessionsMap &lt;- liftIO $ readMVar sessionsMvar return $ HashMap.lookup sessionId sessionsMap Now a function that uses only `getSession` still has the `MonadSession m` constraint, but can't do arbitrary `IO` because of it anymore.
&gt;I've written several small programs such as mergesort, maps, arithmetic, even hash functions such as Keccak 256. Hopefully I'll have actual / larger programs in a near future, as the plan is to use this as the base of a decentralized operating system that runs functional applications. The characteristics of the Abstract Calculus make it a great model for a decentralized virtual machine (like the EVM). Where can I follow this work?
Right, the naming made me think `return` as the body of the lambda. Maybe `self` would make more sense. 
I haven't yet read everything you've written about this (although I look forward to), but is this a major issue? It seems like it wouldn't be too bad, since (if I understand correctly) this system would be used as an intermediate language, so the programmer would not necessarily be directly exposed to the scopelessness (maybe by creating new unique names where needed, to avoid capture). Or is there a fundamental problem with translating a scoped language into this language (like maybe a translation like that "cancels out" the optimizations)?
That's how I see it. Yes you can just ignore the scopelessness. But some people find it too weird and kinda demerits the whole system.
I agree, edited...
Ah, sorry again. The type of `match` was wrong too. {-# LANGUAGE RankNTypes, TypeApplications #-} module A where import Type.Reflection fun :: TypeRep a -&gt; a -&gt; Maybe a fun = match @Int (\n -&gt; negate n) &lt;|&gt; match @String (\s -&gt; reverse s) match :: forall a b. Typeable a =&gt; (a -&gt; a) -&gt; TypeRep b -&gt; b -&gt; Maybe b match = undefined (&lt;|&gt;) :: forall tb b c. (tb -&gt; b -&gt; Maybe c) -&gt; (tb -&gt; b -&gt; Maybe c) -&gt; (tb -&gt; b -&gt; Maybe c) (&lt;|&gt;) = undefined 
Maybe I'm missing something, but it seems like reduction can transform a closed term into an open one. For example: (λx. y) (λy. x) ---(lambda-app)--- y [x/(λy.x)] ---(subst)--- y The issue only seems to arise because a lambda body is free to not use the variable bound by the abstraction, so some sort of linear type system that disallows weakening could be a potential solution.
I don't really have any comments on your code (this is way beyond where I am at), just wanted to say congratulations. If you haven't already I suggest you check out the amazing documentation for Servant [1,2,3]. One thing I love about the project is the extensive documentation around it. [1] https://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/ [2] https://arow.info/blog/posts/2015-07-10-servant-intro.html [3] https://www.andres-loeh.de/Servant/servant-wgp.pdf
You're not missing anything, `(λx. y) (λy. x)` evaluates to `y`, as the root of the interaction net is connected to a λ which immediately returns itself (and is, thus, unreachable). That is a "vicious cycle" on the interaction net literature. `(λx.x)(λy.y x)` is a syntax error because `x` occurs twice.
Also, `nix optimize-store` (Nix 2) or `nix-store --optimize` (Nix 1)
I've never used this. How much does it actually help? I think you can have this run automatically on NixOS; I might consider turning that on if it saves a lot of space.
I'm using [home-manager](https://github.com/rycee/home-manager) on an Ubuntu and used this today after seeing that my store had 4 different copies of ghc-8.4.3, even after `nix-collect-garbage`. Maybe I'm just fighting symptoms of a broader problem here (e.g. why are the different copies kept alive in the first place), but it reduced my store from over 30 to just 16GB.
Holy crap that's great. W.r.t. your GHC duplicates, you might have a profile that has many GHCs in its history of generations, even if only one is currently installed. I tell the GC to delete profile generations older than 30 days to avoid that; otherwise everything you've ever installed will always be installed
Right, I understand that the lambda calculus is less general than the abstract calculus (yes?) and so translation isn't an option. But that's it? There's no way to compile c-- using the abstract calculus and the optimal evaluator, and "call" (I guess?) that from the c-- output from GHC?
Thank you! &gt; I suggest you check out the amazing documentation for Servant [1,2,3] I think those are more like internals, than documentation. And in my experience, it is sometimes a bit hard to find out how to actually do something with Servant. When I was working with it, I found it I had to repeatedly spend time figuring how to do the same thing with Servant again and again. For example, how to add headers, how to throw errors (I believe these function are still hidden in an Internal module) etc etc. Thus I ended up making this [repo](https://github.com/sras/servant-examples) with stand alone examples that show how to do specific things with Servant, and this has ended up saving quite a bit of time for me.. 
Note that: - I'm willing to get some help for setting up automated/compatibility tests - Also interested in some more helper functions to map the horrible low-level HTTP2 + gRPC-status-codes return types to higher-level errors - Always happy to onboard someone over Hangout if they write a tutorial in return (a thing I'm pretty bad at) - I'm super thankful to all the proto-lens people - I'm roughly done writing the server part on top of warp but it's not released yet (but should be soon with an upcoming fixed warp)
What resources would you need to implement this?
Managing a giant line of command arguments is a pain in the ass, stupid, and prone to brittleness. This is a completely defensible position. Config files are a better paradigm. I agree that poorly documented, hidden, implicit config files are a bad. I will not conceed that explicitly managing your own command args in a separate tool is magically better. Literally the whole f-ing point of build systems, by and large, boils down to helping people manage the process of shoving a ton of explicit arguments to a compiler, and config files are a generally accepted tool to accomplish this. The problem is not the file. The problem is that the file isn't explicit and readily visible, or easily reproduced and controlled. 
I just took two weeks vacation learning only category theory. I know feel I covered some ground (I can follow "category theory for programmers" without hiccups) and it was a ton of fun. However I feel that I still have some way before me before CT serves me in my understanding of some Haskell concepts, and way more to go before it aids me in writing better programs. It is fun, it inspired some concepts in Haskell, but it really disconnected. You absolutely do not need it.
I'm guessing it's not possible to compile lambda calculus to this, but can that be proven?
Currently this is being funded by Ethereum Foundation, which kindly provided a grant for Aaron Stump to develop the type theory, and two devs (including me) to work on the project. Obviously it'd go much faster with more people, but I don't want to hire dozens of developers and use a lot of resources from the EF, which is technically a small a non-profit company, and I also don't think I want to make an ICO or similar...
What would be an important term? Yes, exposing the STLC + affine typing to the user = optimal reductions. I wonder if we could have a type theory directly for that calculus, though.
Easily. A simple counter-example is `λx.(x x) λf.x.f (f x)`, which, when evaluated as the equivalent Abstract Algorithm term, has a different normal form as when evaluated as a Lambda Calculus program, as the former will try to duplicate a duplication which results in a pair projection instead of a new duplication.
Besides everything on this thread, I have one minor tip: I tend to do lots of \`nix-build ...\` to try things, one consequence is that is that I leave lots of \`result\` symlinks as gc-roots around. This causes \`nix-collect-garbage\` to hold on to a lot of old dependencies just because some old \`result\` file forgotten somewhere. So, once in a while I do: `nix-store --gc --print-roots | grep ~` to show me the files, and I remove the unnecessary ones, which is almost every one of them.
I cannot recommend the "minimal servant" blog post enough, it has helped many people understand the idea behind servant's implementation without having to go through the whole paper.
When we wrote servant, I didn't have much experience with type level programming either. I suspect it all appears less complicated when you've got a concrete goal, a concrete motivation that leads you to investigating the type level tools we have in Haskell. Congrats for making it through :-)
An important term would be one whose removal would cause a significant loss of expressiveness. [Advanced Topics in Types and Programming Languages](https://www.cis.upenn.edu/~bcpierce/attapl/) has a pretty simple type theory that could probably fit the bill. I might see if I can whip something up in the next few days, that would be really cool if it worked out nicely.
Right, I was more asking about less direct translations. Preferably something that doesn't require writing a garbage collector with the abstract calculus, though I think that's unlikely.
If you’re used to the binary tree example of recursive data definitions, I guess you can draw some helpful comparisons. However, linked lists are different. A binary tree could be defined in Haskell as data Tree a = Stem a (Tree a) (Tree a) | Leaf where a tree of `a` is either a Stem of `a` and two child trees or a Leaf containing nothing. For comparison, a list of `a` as defined by u/CinnamonHeart is defined as either a Cons with an `a` and one child list or a Nil with no data (like a leaf of a tree). Notice that both examples have a base case (Leaf, Nil) and a recursive case (Stem, Cons). (For the tree definition, I could have also used Maybe, but I wanted to draw a comparison between Leaf and Nil.) 
Yea, you fall back to the same problem of implementing an efficient optimal evaluator for λ-terms.
I just have `alias nix-build="nix-build --no-out-link"` in my shell.
The types are crucial, lets write them out explicitly Cons :: a -&gt; List a Stem :: a -&gt; Tree a -&gt; Tree a and the type of empty lists and trees for completeness Nil :: List a Leaf :: Tree a
Thanks! Really great explanation. Got it mostly working. A few things are still getting garbage collected for whatever reason (maybe because I'm using a few things outside the actual derivation, like \`callCabal2nix\`), but even so, build time post-GC is down by 10 minutes (takes \~1 min instead of 11 now)!
Yea the cabal2nix stuff is fixed in 18.03, but reflex-platform isn't there yet :/
This is pretty cool, but what does this "re-framing" of supervised learning in a functional context afford us? Simpler implementations, more opportunities for parallelism, etc? I guess I'm asking why one would opt to implement a supervised learning in this way over a more standard approach.
I am still digesting this myself. So, for now, I shall simply quote relevant parts of the introduction: &gt;Our main theorem is that, under general conditions, gradient descent is compositional. This is formalised as a functor **Para** → **Learn**, where **Para** is a category where morphisms are differentiable parametrised functions between finite dimensional Euclidean spaces. In brief, the functoriality means that given two differentiable parametrised functions *I* and *J*, we get the same result if we (i) use gradient descent to get learning algorithms for *I* and *J*, and then compose those learning algorithms, or (ii) compose *I* and *J* as parametrised functions, and then use gradient descent to get a learning algorithm. \[formal statement of theorem\] &gt;This theorem has a number of consequences. For now, let us name just three. The first is that we may train a neural network by using the training data on the whole network to create training data for each subunit, and then training each subunit separately. To some extent this is well-known: it is responsible for speedups due to backpropagation, as one never needs to compute the derivatives of the function defined by the entire network. However the fact that this functor is symmetric monoidal structure shows that we can vary the backpropagation algorithm to factor the neural network into richer sub-parts than simply carving it layer by layer. &gt; &gt;Second, it gives a sufficient condition—which is both straightforward and general—under which an error function works well under backpropagation. &gt; &gt;Finally, it shows that backpropagation can be applied far more generally than just to neural networks: it is compositional for all differentiable parametrised functions. As a consequence, it shows that backpropagation gives a sound method for computing gradient descent even if we introduce far more general elements into neural networks than the traditional linear and activation functions. It is often said ^(\[citation needed\]) that neural networks are inscrutable. It could well be that the "categorification" of learning in or beyond this sense – even down to the level of finite dimensional Euclidean spaces and their associated fields – may yield scrutable networks. Personally, I am unaware if "scrutability" is a primary goal of research following these lines of thinking. However, I get the impression that it may emerge as a consequence. &gt;Let's think the unthinkable, let's do the undoable. Let us prepare to grapple with the ineffable itself, and see if we may not eff it after all. ― Douglas Adams In my mind, your question is very much open. I am confident others can elucidate on other advantages.
Not really. Using Haskell doesn't fragment my codebase. :) It's also not hard to expect the whole team to write Haskell. And using Haskell is not zero value. You can't turn on a font with ligatures and get all the benefits of Haskell.
&gt; Now a function that uses only getSession still has the MonadSession m constraint, but can't do arbitrary IO because of it anymore. This is exactly how it's implemented right now. We use this approach in our production projects and there was PR open for 13 days regarding bringing our approach to template that I forgot to merge. It was merged just now. * https://github.com/Holmusk/three-layer/pull/39
Ah, it looks different now. See my other comment: * https://www.reddit.com/r/haskell/comments/93gbdn/monthly_hask_anything_august_2018/e52uham/ Probably there will be blog post in some future regarding our architecture :) We're still in process of figuring out what is the best approach. But here is short difference: 1. We provide classes for common and useful things so yo have access to effect like this: class Monad m =&gt; MonadMeasure m where timedAction :: Text -&gt; m a -&gt; m a But implementation of this typeclass has more specific and polymorphic constraints, though Three Layers approach suggests to avoid `MonadReader` constraint: timedActionImpl :: (MonadReader r m, Has Timings r, Has Metrics.Store r, MonadIO m) =&gt; Text -&gt; m a -&gt; m a 2. Three Layers approach suggests to avoid `MonadIO` constraint to not have it in functions, but lot of API handlers have this `MonadIO` constraint as an implicit part of `WithDbPool` constraint. We have more SQL queries than we have functions so it's quite tedious to implement `MonadSomething` method for each query, especially when it's not clear how to group SQL queries into monadic typeclasses. 3. AFAIU, Three Layer Cake suggests to have something like `RIO` while in reality we also add `ExceptT` and `LoggingT` transformers over our `ReaderT`. 4. Our current approach is not mockable enough and we don't aim to mock-test our function. We have bad experience when mock tests pass, but application — doesn't. Especially when we have things like AWS, Firebase, SQL, JWT. This all is extremely hard to mock. So we don't have pure implementations of our typeclasses and our tests work with our main monad and test our real code that will go to production.
Does Megaparsec have a good story for indentation-aware / indentation-based parsing?
Type families can be a pain to use due to the lack of otherwise common 'value-level' language features. (Heck, we don't even have type-level first class functions.) I wonder what the equivalent experience in Idris would be.
&gt; I would like to have Nix install to an external drive. Never tried this before, but sounds like it be possible to mount a partition/directory of the external drive to `/nix` or `/nix/store`. It's possible to change the store directory of Nix, but then you lose the ability to make use of binary caches and spend hours compiling other stuff.
Thanks. I also have one doubt about Servant. In the "minimal servant" blog post, it says this about matching the request with one of the handlers... &gt; Then in a choice we can just try the first option, and if it returns Nothing, try the second. (Again, full Servant is somewhat more sophisticated here, and version 0.5 will have a much improved routing strategy, which probably at some point in the future deserves to be the topic of its own blog post.) What I have done here is that I just convert the server type into a list of string urls. Then when the request comes, I just look up the index that matches with the request (taking care of url params and route priorities). Then I use the index, say 'n', to fetch the n-th level of handler from a nested data structure of handlers. It is pretty naive. But just wondering if something along this line could be made to work reliably? 
I think type level functions like you said would be more than enough?
Hi, yeah this package should help working with this streaming API. If this package doesn't work, please open a bug on the Git repo. That said, I'm not familiar with Google cloud gRPC APIs: learning them could be a good example for a tutorial.
thanks! I'll give it a try then. 
I can understand that, but people around me at University still look at me as if I'm an alien if I mention Haskell. Especially around computer vision/machine learning - they're almost ready to accept python! 
I've been looking at switching to the yi-editor so that I can configure and write new features in Haskell for my editor, but I can't give up org-mode. This project seems like the perfect match for me :) Great work!
Are you planning on writing a new backend for GHC, translating STG to your Abstract Calculus?
Thanks!
The retraversal for error messages made me wonder if a cursor based interface for text could work better. With stepping as the normal opperation, seeking to jump to somwhere else and a weak ref map as fast path for seeking recent chunks/error messages and so on.
Hmm... I just tried \`nix-garbage-collect\`, this time with the -d flag, for a change. Good news: It released 10GB, my whole /nix is down to 9GB. Bad news: It also removed my home-manager installation and nix environment. Gah, at least I can reproducibly get to my old config.
I assumed the point of the paper was that you might be able to use fancier learning algorithms on some parts of your network rather than others, in your example `paraToLearn` is doing vanilla gradient descent. For example, maybe there's some second order method which can converge faster over particular part of the neural network? I don't see what the point is otherwise?! I could be way off base here.
See: http://reddit.com/r/haskell/comments/9b7y5l/nix_running_out_of_disk_space/e51uzqr You probably just need `keep-outputs = true` to make it work with `shell.nix` again.
Nice! This is blogpost material, if you want to take advantage of the article visibility right now. It really shows some qualities of `backprop`.
ELI5 - your pairs are not tuples right? What does the superposition thing mean?
&gt; It is not like I can shape math to fit my personal tastes or subjective definitions of "silly". It is exactly like that, that's what I'm trying to say! Math is invented, not discovered, you can just make a different interaction net. 
Pairs are just tuples, superposition is just a name that came from the fact that they, in some reduction rules, behave as if two variables were stored in the same place. After further observing the language, I realized they work exactly like pairs, so that's a much better presentation.
Humans did invent that Pi = 3.14. The actual interesting number is the imaginary period of the exponential function, which is more like 6.28. Why care about Pi? Because humans messed up and chose the wrong number to be important. That's what I mean.
Not really on my plans, it'll be the backend for a [whole new language](https://medium.com/@maiavictor/updates-on-ethereums-moon-project-535f8c0497ef) we're developing at the Ethereum Foundation.
With normal interaction nets you can add primitive operations for machine ints etc (see this diploma report page 17 onwards: http://www.dcs.gla.ac.uk/~simon/publications/diploma.pdf). I imagine you could do similar here.
The latest version is 1.7.1 rite
Well, I expect that scopelessness and non-repetition can lead to annoying things like "you can't name this variable `x` because you already used `x` 100 likes later" I think that if I were to program in it, I'd really prefer a surface syntax more like the lambda calculus which gets translated. Under what circumstances are oracles a problem? Like, If I have a surface syntax which has scope and then make fresh variables, does that cause problems?
I don't think it's fair to say they're tuples; a tuple is emphatically not a function, so an application of a tuple is stuck, whereas your pairs can be applied just fine (c.f. Rule 2, pair application).
Well, fair enough, I just think it's a better analogy.
This is a really complicated jargon-filled way to explain the backprop algorithm. Does anything extra fall out of this formalism, beyond "we can compute gradients of complicated feedforward circuits over an appropriate numeric basis", which we already knew? Seriously, I'm having trouble seeing the value-added here.
Yes, 1.7.1 is the latest version. I'm not sure how to solve this issue considering 1.9.1 hasn't been released yet.
Thanks loads, that's really helpful! Could you possibly point me towards some introductory reading material (for a Haskeller as opposed to a mathematician) into how they work, or examples in Haskell?
Does anybody know how this relates to simple essence of automatic differentiation? [http://conal.net/papers/essence-of-ad/](http://conal.net/papers/essence-of-ad/) Backprop as Functor talks about computing derivatives, but also about \*learning?\* itself. Simple AD strictly talks about calculating generalized notion of derivatives in a strictly categorical way. It seems that they could be reconciled... somehow. All in all, I think this is a very needed step for machine learning, making everything compositional and structued in a way we can reason about
I agree with you that categorification of learning is a needed step. Neural networks are interesting because of two things: * They're compositional. You can stack layers and get better results * They're discovering structure in data, the same way us humans are discovering structure in data. CT is all about compositionality and discovering structures in data: it seems painfully obvious that networks and categories go hand in hand. Both in the sense of how we implement networks (categorically, compositionally) and in the sense that networks are compositional structures discovering structures (preferrably compositional ones, but we have yet to figure that out). &amp;#x200B;
Thank you for the response. &gt; since we also potentially need to look at the request body etc.. You mean, we might have to look at request body while selecting the handler to handler the request. Right? If that is so then it explains the all the additional stuff Servant's routing code has to do, which I am not doing right now. But is this part of HTTP spec. I thought one only has to use the request method and the URL to select the handler.
&gt;Type families can be a pain to use due to the lack of otherwise common 'value-level' language features. Having a hard time understanding this. Can you give an example?
I agree. I think the "superposition" name better describes their role in incremental copying.
What resolver are you using?
You could try stack upgrade --git if git is in your PATH. But it might come with other problems as it's not an official release.
Unfortunately I'm not super fluent in Haskell's efforts towards dependent types (I rarely use anything more exotic than RankNTypes and GADTs), so I can't be of much help here. I'm sure there are plenty who disagree with me on this, but personally, I think Haskell code does well to just stick to relying on parametricity for most cases - and this is largely what the ecosystem does. But if you really want to do dependent types, I recommend just taking the plunge into Agda or Idris.
GHC-8.2.2
Sorry, I should have been specific. Do you know which stack resolver you're using? Like an lts or nightly resolver? Are you doing this in a local project? What's in your \`stack.yaml\`?
No, I think I should have posted more for context- I have a stack project and would like to use grapefruit in it, so I have that and related dependencies in my 'extra-deps', the resolver selected in yaml, and I'm running "stack install grapefruit-ui-gtk" in the project folder. If this isn't the correct proceudure, it's probably because I am new to stack. Here's the yaml without the auto-generated comments from stack: resolver: ghc-8.2.2 packages: - . extra-deps: [grapefruit-examples-0.1.0.7, grapefruit-ui-gtk-0.1.0.7, TypeCompose-0.9.12, arrows-0.4.4.2, fraction-0.1.0.6, grapefruit-frp-0.1.0.7, grapefruit-records-0.1.0.7, grapefruit-ui-0.1.0.7, Stream-0.4.7.2, lazysmallcheck-0.6, QuickCheck-2.11.3, base-orphans-0.8, colour-2.3.4, fingertree-0.1.4.1, glib-0.13.6.0, gtk3-0.14.9, semigroups-0.18.5, cairo-0.13.5.0, gio-0.13.5.0, gtk2hs-buildtools-0.13.4.0, mtl-2.2.2, pango-0.13.5.0, hashtables-1.2.3.1, random-1.1, text-1.2.3.0, tf-random-0.5, utf8-string-1.0.1.1, hashable-1.2.7.0, primitive-0.6.4.0, vector-0.12.0.1]
No comments? Let me point you to the HN thread that was about my post: https://news.ycombinator.com/item?id=17846940 Tl;dr * Elm is dead, use Pine * Qiita CSS is bad (I didn't know it was Japanese *Stack Overflow*) * I didn't read the article I think Elm is great * I didn't read the article Elm is not good * I didn't understand the article the post was unrelated to Elm
I switched to resolver: lts-12.8 and it still fails with the same error.
/u/Tekmo is right in that you can't type `(\x. x x) (\x. x x)` directly. But you can type something equivalent in any language with full recursive types (such as languages subsuming Hindley Milner type systems): newtype T = MkT { unT :: T -&gt; T } e :: T e = (\t -&gt; unT t t) (MkT (\t -&gt; unT t t)) main = do let MkT f = e f `seq` putStrLn "You'll never see this" "So what, this doesn't terminate. Why is this special?" Well, normally, the only way to introduce non-termination in Haskell is through expressions that are directly or indirectly recursive. There's no recursive binding here, yet when we try to compile this, we hit an instance of a [well known GHC bug](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/bugs.html#bugs-in-ghc). Here's the Y-combinator, from which you can recover general recursion and thus recursive let bindings: newtype T a = MkT { unT :: T a -&gt; a -&gt; a } fix :: ((a -&gt; a) -&gt; a -&gt; a) -&gt; a -&gt; a fix f = (\t -&gt; f (unT t t)) (MkT (\t -&gt; f (unT t t))) fac :: Int -&gt; Int fac = fix $ \r n -&gt; if n == 0 then 1 else n * r (n-1) main = print (fac 10) This also doesn't compile with GHC, but just to prove a point. As soon as you allow unrestricted recursive types, you got non-termination. Total languages like Idris, Agda, Coq, Lean, ... don't allow recursive occurrences in negative (e.g. argument) position for that reason.
`ghc-8.2.2` is a minimal resolver which only includes ghc, base, and the other packages which come with ghc.
Try [this slack configuration](https://github.com/gelisam/frp-zoo/blob/master/grapefruit-example/stack.yaml).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [gelisam/frp-zoo/.../**stack.yaml** (master → 1a1704e)](https://github.com/gelisam/frp-zoo/blob/1a1704e4294b17a1c6e4b417a73e61216bad2321/grapefruit-example/stack.yaml) ---- 
Thanks for the feedback. &gt; Config files are a better paradigm. Config files don't go stale easily. Packages (especially in-place ones) easily get removed, so now your package environment has dangling references. So I'd argue that package environment files, as implemented, don't behave like your usual configs (or do you have references for "generally accepted" use-cases where the config is similarly volatile?) &gt; I will not conceed that explicitly managing your own command args in a separate tool is magically better. I don't understand this. What kind of "explicit management" are you referring to? (Which are "my own command args" and what is the "separate tool"? brittany? cabal?) &gt; The problem is not the file. The problem is that the file isn't explicit and readily visible, or easily reproduced and controlled. This is something I can agree with, although it seems that there are a lot of attached "if"s: If you make the environment an explicit input, if you assume that the file does not go stale or implement it in a way that this is not a problem, and if you have a use-case where calling ghc via the build-tool (which is the correct dispatcher for a job that requires knowledge of dependencies, after all) is _not_ an option, then _maybe_ this functionality is a useful addition. And I realize that my phrase "package environment files have no valid, sensible purpose. You always had the ability to pass package(-db)s to ghc as explicit commandline args." might have shortened this reasoning too much and too aggressively. This I can certainly take as constructive input, thanks.
Ooh I'm actually learning Idris at the moment for the very purpose of my post, but I havent been able to find any solutions to my problem. Are you aware of any such implementations in Idris?
A file that controls how packages are resolved isn't inherently a bad thing, that's just a .cabal file of a different color. Having a mechanism to supplement or overload the build systems behavior by directory is a powerful tool, and also isn't a bad thing, that's just supplementary information that informs a build. Having these files generated by the build system also isn't really a terrible thing - that's basically just an intermediary build artifact. The problem here is, specifically, that you can violate the expected idempotency of new-build via these files. That could certainly be a problem - in scenarios in which new-build somehow steps on it's own toes. I don't think that's what is happening here? I could be wrong on that point. If someone or something that isn't new-build is dicking around with the contents of those files, then in essence, that should be seen as different than editing any other intermediary build artefact and expecting that it won't screw your build somehow, which in my mind, is kind of an unreasonable expectation. I can see the other side to that argument, and I think it's a defensible stance, but the idea of multi-stage builds caching information to the filesystem that informs package resolution in future stages isn't an inherently broken concept, and using files scoped to the working directory seems like a fairly reasonable way to accomplish that.
I don't know of any such guide and I don't think such a guide should exist -- ideally the error messages should be self-explanatory or we should add a flag to give longer explanations. My approach has typically been using Stack Overflow. 😐
Could you give a code example which gives that failure? Perhaps we could submit a PR to improve the message.
[removed]
Try a 'real' dependent typed language like Idris? 
Long story short, we try to follow the procedure that [this diagram](https://github.com/for-GET/http-decision-diagram) describes. It also mentions the HTTP RFCs that dictate most of this behaviour. But we're trying to be a little more flexible when we can (that is, when it doesn't make the router misbehave in other cases). We therefore first look at path components and method of course, but indeed we then have to look at the content type related headers, look for a request body, try to decode it, etc, erroring out appropriately whenever one of those checks fail. This whole additional business is what the `Delayed` type from the `RoutingApplication` module is about. It is also documented reasonably well, if I remember correctly.
The error was different this time, so that's something? I got this: &gt;Process exited with code: ExitFailure 1 Logs have been written to: C:\haskell\stack-projects\stratego\.stack-work\logs\Cabal-1.24.2.0.log &gt; Configuring Cabal-1.24.2.0... Cabal-simple_Z6RU0evB_2.0.1.0_ghc-8.2.2.exe: Encountered missing dependencies: process &gt;=1.1.0.1 &amp;&amp; &lt;1.5 &amp;&amp; ==1.6.1.0 After adding that and then adding whatever dependencies *that* asked me to add, I got to a point where it's telling me to add base-some-number to my extra-deps but I already had it listed.
Ah, so maybe I need to play around with different compiler versions? 
Part of stack's strategy to make sure builds are reproducible is that a given lts specifies a corresponding ghc version, and stack downloads and uses that version. [lts-9.0 uses ghc-8.0.2](https://www.stackage.org/lts-9.0), for example. The error messages you are getting do seem to indicate that you're using the wrong version of ghc though, which is strange. Are you somehow overwriting stack's choice of ghc version, or telling it to use your locally-installed version of ghc?
Is it just me, or does Flatpak seem like an ad-hoc, informally-specified, bug-ridden implementation of half of Nix?
It will rebuild all caches, as the caching behavior has changed in the master branch. 1.9.1 will be out soon.
Many compiler extensions to the haskell language (documented here https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html) require some associated functions or data types to be made use of. The `GHC.Exts` module is the standard place that GHC makes all those functions and types available in a single location, for simplicity. 
Yep, been trying to get people at work to consider Nix instead of Flatpack for some dependency management tooling we have... It's not going too well just yet (they know neither and my nixfoo is bad).
I think that applied to the parallel variant with local heaps from the paper (which incidentally also employs the immix idea). Or is the paper about the only reasonable extension of the idea?
Why though
I got it working, but it wasn't smooth going. I'll document the full workflow I went through. First, I unpacked the source code locally: stack unpack grapefruit-ui-gtk Then, within the generated directory, I created a minimal `stack.yaml` containing the following: resolver: lts-12.0 I ran `stack build --file-watch`, and it started generated a series of recommendations for packages to add to `extra-deps`, which I followed. Ultimately, it started building, and finally failed when `TypeCompose` had a build error due to missing a `Semigroup` instance. There's no way to know this part without prior experience, but: that error message is almost always due to lack of support for GHC 8.4.3, which added `Semigroup` as a superclass of `Monoid`. To work around this, I decided to switch to an LTS snapshot using an older GHC. Checking the "Latest LTS per GHC version" section on https://www.stackage.org/, `lts-11.22` uses GHC 8.2.2, so I switched to that, added a few more `extra-deps`, and built again. At this point, all dependencies built successfully. The `stack.yaml` file used is: resolver: lts-11.22 extra-deps: - fraction-0.1.0.6@sha256:06248ea9902efa0744d0491ab0e28a79acb2df9785a285aef0372ab08353019d,1194 - grapefruit-frp-0.1.0.7@sha256:b0c39ca99ff321fed85d467df66423863d08107a4db63464c54b13112306992b,3865 - grapefruit-records-0.1.0.7@sha256:8a9a3c90e1eed982041d13cb6651d774d434558f9a9bd46e529944f4b8b2ffcd,2350 - grapefruit-ui-0.1.0.7@sha256:5bc230734ece0b6db3ee70afc8266c4992473b463fc904f0f12477e2e911fe10,3464 - TypeCompose-0.9.12@sha256:038b7158deba8f68b9b32b05eb47d6ebc8709b1c960cb44d50469d1a5deb4748,1576 - arrows-0.4.4.2@sha256:a260222b766da922657e302aa7c0409451913e1e503798a47a213a61ba382460,1235 - Stream-0.4.7.2@sha256:ed78165aa34c4e23dc53c9072f8715d414a585037f2145ea0eb2b38300354c53,1009 - lazysmallcheck-0.6@sha256:dac7a1e4877681f1260309e863e896674dd6efc1159897b7945893e693f2a6bc,1696 - gtk3-0.14.9@sha256:574bdbd37f170ba0f1d068248fd39637c3babcc9a341d2dde8d649a1346b9c38,19462 - gio-0.13.5.0@sha256:1e02962f498f62ba68cff88e7f7379e2f8c192f311d867b22f4815518b2f9a86,3089 While the _dependencies_ built, I still got a build error in grapefruit-ui-gtk itself: • Couldn't match expected type ‘Maybe model0’ with actual type ‘Gtk.ListStore el’ Solving this was pretty easy: I replaced Gtk.treeViewSetModel gtkTreeView gtkListStore with Gtk.treeViewSetModel gtkTreeView (Just gtkListStore) Now, some thoughts on this: * Obviously, manually adding dependencies like this is annoying * A dependency solving approach (like cabal-install follows) makes this less annoying * However, as demonstrated, automatic dependency solving wouldn't work in this case, because the version bounds don't provide sufficient information to guide it * Stack follows the "use curation for the common case, and occasionally bite the bullet and do some annoying manual dep solving." You'll only have to go through that pain once (it took me about 10 minutes to do this, most of that time sitting waiting for `gtk3` to finish compiling). Then you'll get reproducible builds afterwards. * It's _still_ unclear to me how this package built in the first place. Presumably it was built against an older version of gtk3 or something like that, but I'm not sure. Including a package in Stackage will ensure that there's some historical record showing at least one version of each dependency that it works with. * Strong recommendation: if you're interested in using a package, try adding it to Stackage.
I think we have some commits on master to ensure we don't recommend upgrading base under any circumstances.
Haskell doesn't seem to have a option to import the whole module. Is it right? For example, in python, if `numpy` module is imported, all names under `numpy` can be used in current scopy by calling `numpy.name`.
The original talk was in Japanese, and unfortunately there is no video record. Sorry for that.
That's a Lisp reference, right?
Yep see here: https://julesh.com/2018/08/16/lenses-for-philosophers/
Ah, ya that's a bit tricky. There's probably a way to do it with indexed containers, but in this case it's probably simpler to just use regular inductive data -- something like this, I think: Matrix : Nat -&gt; Nat -&gt; Type -&gt; Type Matrix m n t = Vec m (Vec n t) data Nested : (a : Nat) -&gt; (b : Nat) -&gt; Type -&gt; Type where Base : Matrix a b t -&gt; Nested a b t Wrap : Matrix a b (Nested b c t) -&gt; Nested a c t 
It wouldn't be difficult to add a Nix expression generator to Stackpak. The main bulk of the work was figuring out how to reliably gather all data (urls/hashes) for a reproducible build. One would only need to write `generateNixExpression :: GeneratorInput -&gt; Either Text Text`. [https://gitlab.com/rszibele/stackpak/blob/master/src/Generator.hs](https://gitlab.com/rszibele/stackpak/blob/master/src/Generator.hs)
Can someone initiate this project please? Or at least begin a prospectus?
A tool which does this already exists. [stack2nix](https://github.com/input-output-hk/stack2nix)
If you export a module `M` with `import M` its exports are in scope both in their unqualified and their fully qualified form: λ import Data.Foldable λ Data.Foldable.asum [Nothing, Just ()] Just () λ asum [Nothing, Just ()] Just ()
I suspect it's not that easy, exactly because you need to somehow encode sharing.
Thanks for you reply. I was reading textbook [programming language pragmatics](https://www.amazon.com/Programming-Language-Pragmatics-Third-Edition/dp/0123745144). It has a section on modules implementation in different programming languages. I am a bit confused with this paragraph. &gt; Modules into which names must be explicitly imported are said to be *closed scopes*. By extension, modules that do not require imports are said to be *open scopes*. Imports serve to document the program ... Modules are closed in Modula and Haskell. C++ is representative of an increasingly common option, in which names are automatically exported, but are available on the outside only when qualified with the module name(unless with `using` directive`). Are you happen to be familiar with python or C++? What are the differences of module imports and exports between them and Haskell?
Cool, so we already have that covered!
Sorry, I'm pretty familiar with python, but I don't think I can describe the differences in detail. For Haskell, the module system is described in the Haskell Report: https://www.haskell.org/onlinereport/haskell2010/haskellch5.html
I'm not sure if there are reasons beyond this but, having looked through the patches myself, I suspect the reason is that the GSoC ended a bit early, meaning that it was left in a state that was rather hard for someone else to pick up. This, combined with a general lack of time (Simon Marlow's in particular), is enough to kill nearly any project.
Yeah, having just read the wiki on STG that seems a little onerous. Could we just take the parse tree and generate an \`AbCore\`, reduce and generate code from that?
&gt; It's still unclear to me how this package built in the first place. It builds fine, without the `Just gtkListStore` fix, using [the `stack.yaml` I linked here](https://www.reddit.com/r/haskell/comments/9bkd6m/errors_when_installing_grapefruit_with_stack/e54461l/). I followed similar steps to create that `stack.yaml` back then, including making (a different set of) tweaks to the code which I emailed to the author and have since been incorporated in the package.
It's a variant of [Greenspun's Tenth Rule](https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule).
**Greenspun's tenth rule** Greenspun's tenth rule of programming is an aphorism in computer programming and especially programming language circles that states: Any sufficiently complicated C or Fortran program contains an ad-hoc, informally-specified, bug-ridden, slow implementation of half of Common Lisp. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
If you visit the [overview page](https://summerofcode.withgoogle.com/organizations/5706672807346176/?sp-page=2#5641742712307712) you can find all projects and the corresponding code submissions.
From a skim of the paper, it itself is introducing newPrompt/pushPrompt and defining what they do.
The delimited continuations literature spawned a small zoo of control operators with different names, many of which can be defined in terms of other ones. The best summary of all of these I know of is [Racket’s documentation for the `racket/control` module](http://docs.racket-lang.org/reference/cont.html#%28mod-path._racket%2Fcontrol%29), which defines all of them in terms of Racket’s core delimited continuation operations. It provides reduction rules for many of the operators, and it cites where all of them come from. In general, though, I wouldn’t worry too much about understanding all of these different operators. In practice, knowing the model of prompts/aborts is enough, since it can subsume everything else. A Monadic Framework for Delimited Continuations mostly covers how to define those operators to prove that their framework is as expressive as other, previously presented frameworks for delimited control, and I don’t think that understanding all of those operators individually is essential. I think what’s important to understand are the four operations defined in the paper: `newPrompt`, `pushPrompt`, `withSubCont`, and `pushSubCont`. In Racket terms, these operators essentially correspond to [`make-continuation-prompt-tag`](http://docs.racket-lang.org/reference/cont.html#%28def._%28%28quote._~23~25kernel%29._make-continuation-prompt-tag%29%29), [`call-with-continuation-prompt`](http://docs.racket-lang.org/reference/cont.html#%28def._%28%28quote._~23~25kernel%29._call-with-continuation-prompt%29%29), [`call-with-current-continuation`](http://docs.racket-lang.org/reference/cont.html#%28def._%28%28quote._~23~25kernel%29._call-with-current-continuation%29%29), and continuation application, respectively. (Racket treats continuations like procedures that can be applied, but the model used in the paper treats them like opaque values that must be applied using the special operator `pushSubCont`.) For a crash course in the prompt-based model of delimited control, see [my Stack Overflow answer on the topic](https://stackoverflow.com/a/29838823/465378), which includes some helpful (albeit crude) diagrams.
simple rpc libs? i have a python client and a haskell server, and i'd prefer xml-rpc, because it's in the python standard library. but stuff like `msgpack` seem cool too. Links: * haxr: http://hackage.haskell.org/package/haxr-3000.11.2/docs/Network-XmlRpc-Server.html * msgpack: http://hackage.haskell.org/package/msgpack-1.0.0/docs/Data-MessagePack.html
Haskell IDE Engine (`hie`) tutorial? i installed it, but don't know what to do with it (no Google results for a tutorial or a wiki). should I read up on the Language Server Protocol generally? https://github.com/haskell/haskell-ide-engine/issues/338 https://microsoft.github.io/language-server-protocol/ 
okay, found this emacs mode https://github.com/emacs-lsp/lsp-haskell/blob/master/README.md `haskell-mode` seems to have `ghc-mode` integration, but afaict, it doesn't mention `hie` http://haskell.github.io/haskell-mode/manual/latest/
[Read this thread asking about exactly the same thing from three days ago.](https://www.reddit.com/r/haskell/comments/9b0deg/starting_my_functional_programming_class_in_2/) It hasn’t even left the front page.
Thx! This is exactly what I need!
Is Haskell reliable in production (web)? Are some frameworks more reliable than others? Can a Haskell web server handle as much load with the reliability of, say, the JVM or Erlang? Anyone have any horror stories about using Haskell in production?
Isn't the book you mentioned almost 2000 pages now? Conversely, a book i'm reading for Scala(FP in Scala) is 330 ish pages but covers a ton of material through exercises. 
I don't know what you mean by the same name for different ghc versions? Package env files have the ghc version baked into their name, necessarily afaik.
Try adding a hClose outf at the end of main :) 
I use Emacs with [dante](https://github.com/jyp/dante). I've also made an emacs configuration (fairly similar to my own) for people that want to get started fast writing Haskell with emacs: https://github.com/soupi/minimal-haskell-emacs/ There's also an evil branch for those coming from vim: https://github.com/soupi/minimal-haskell-emacs/tree/evil 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [soupi/minimal-haskell-emacs/.../**9a025785f365549d365698a02f352eb3d0cc03c7** (evil → 9a02578)](https://github.com/soupi/minimal-haskell-emacs/tree/9a025785f365549d365698a02f352eb3d0cc03c7) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e56g4v1.)
Visual Studio Code. Pretty lightweight and straightforward 
Which plugin? &amp;#x200B;
Anyone know or hear about the kind of Haskell that's going on at Dfinity?
vim with no extra plugins (beside syntax highlighting). I use the makeprg option to get the locations of errors into my quickfix.
The problem likely is that `Handle` does in-memory buffering and your program is exiting before that buffer is flushed. Like the other post says, you can `hClose` it. Calling `hFlush` should also work but closing files when you're done with them is the best approach.
+1
Yi, vim mode+syntax highlighting, ghcid in another window
Installation: Turn on Dante. Didn't know about Dante. Any reason other than the proclaimed lightweight operations for using it?
&gt; The Haskell support is actually what made me switch from vim to Emacs and I'm so happy I did. Same!
It's basically that. it works between projects, it works without a project, and i don't need to install anything or wait to start coding. I mostly just use the flycheck integration but sometimes i use the other stuff.
I enjoy my terminals so I live with tmux+zsh. Then it's vim (and a bunch of plugins)+ghci+ghc-mod+hoogle. The [lambdabot](https://wiki.haskell.org/Lambdabot) on irc also has nice features
I have been enjoying this! Looking to dig in deeper once I have time :)
Same here. haskell-mode plus dante. Somehow Dante does it right and works more often and painlessly than the rest I've tried. In the future, hie will be a viable replacement, but it's not there yet in terms of out of the box experience and also doesn't support new-build yet.
Is Yi's haskell major mode good enough? Admittedly, it's been years since I've tried Yi.
Yi is much better now. Lots of support, easy to configure, active development, haskell mode works well, just wish I had some vim plugins
neovim + haskell-ide-engine
Great. Is there a current tutorial on how to configure/build (ala xmonad.hs) it for Haskell coding? Maybe you can share your config?
What type of GC does Haskell have? I remember it to be concurrent and non-moving, but I may be wrong.
Or use [`withBinaryFile`](https://hackage.haskell.org/package/base-4.11.1.0/docs/System-IO.html#v:withBinaryFile) to make sure you don't forget to close it.
I use spacemacs! http://spacemacs.org/ 
VSCode + ghcid (you can use any editor with it in general and it's very lightweight setup, I just use VSCode because it's familiar in shortcuts from using it with other languages)
I have a relatively simple one up at https://github.com/chessai/yi-chessai It doesnt have much in it, but it's a good start. I make sure it builds in a nix-shell and then I install it with nix-env
Intero really changed the game for me, I couldn't put together a well-working haskell mode before that. Love that setup since it "just works"
Cool. I don't use Nix but I guess I can figure out how to build and run it like XMonad. Any reason you spawn ghc in TypeOf.hs instead of using the Haskell API? Being able to use all of Haskell is what makes me think Yi can be what Jetbrains IDEA is to Java.
As soon as someone ports org-mode to Yi, which is highly unlikely to happen, I can see myself transitioning over. Just putting out there which feature is probably going to keep me an Emacs user for quite a while :(.
That's something I stole from a friend (current maintainer of vim2hs). They're new to haskell, and work with me. I just copied their code over, I plan on actually using the GHC API.
I use [Leksah](http://leksah.org/) for all my work as well as working on Leksah itself. [Here](https://github.com/leksah/leksah#getting-leksah) are the current instructions for installing Leksah. Please give it a go and let us know how you get on. There has not been an official release version for some time, but the github version is fairly stable and has a bunch of new features.
question for the haskell ide engine users here. Are there prebuilt binaries availabe? because building it on my somewhat older laptop almost fried the darn thing and seemingly takes forever. 
I've been using [Haskero](https://marketplace.visualstudio.com/items?itemName=Vans.haskero) with a fair amount of success. I definitely had a better experience with it than ghc-mod.
intero-the-exe is agnostic of stack, it's pretty much a modified `ghci`. The intero emacs integration is stack specific, though.
Just plain Sublime Text + Sublime Haskell plugin is good for me (although the built-in Haskell support is fine). I don't really like IDEs, though, so there's that
Quick link dump: [https://simonmar.github.io/bib/papers/parallel-gc.pdf](https://simonmar.github.io/bib/papers/parallel-gc.pdf) [https://ghc.haskell.org/trac/ghc/wiki/GarbageCollectorNotes](https://ghc.haskell.org/trac/ghc/wiki/GarbageCollectorNotes) Some of it seems outdated, but I haven't dived into the GHC GC source, so can't say much.
Notepad++ and ghci
Thank you Hamish for continued development of Leksah. I really enjoy using it. Leksah is worthwhile checking out: It has the essentials covered (code sense / function lookup / hints, linting), a nice set of useful features, and is a fun, productive environment. The meta-data provides a great way to navigate, learn, and develop a code base. The installation story is improving (I use Nix with customized overrides – I haven't tried the provided install script). I think it's an especially great tool for new Haskell programmers, and hope that it can get an official release to make it more accessible to the Haskell curious. Cheers!
Hlint checks for a lot of store issues but mostly at the function level, not the design of the code. Still, it's been a good start for me.
AFAIK, not yet, but there were discussions about that on their issue tracker 
I'm unaware of anything that's really similar to what you described. That being said, HLint is one of the best "linting" tools I've used for any language. And it does have a few rules like you described; it can issue a warning about the standard capitalization convention and about unnecessary parentheses. In general, though, it's immensely helpful in telling you where your code could be simplified. Also, the GHC compiler itself has [quite a few useful warnings](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/using-warnings.html) that aren't enabled by default, so you should look at those too. I would say everything in `-Wall` is useful for the vast majority of code, and that in the few cases where it isn't, you can specifically disable the warning in question. A lot of the warnings not enabled by `-Wall` are pretty useful too. For instance, I personally think it's sensible to pay the up-front cost of always writing an export list even if every symbol in a source file is initially exported. So, `-Wmissing-export-lists` issues a warning if I forget.
If you don't mind me asking, How do you work with your backend? Do you have some automation to run hsdev/ghc-mode as sublime starts?
Same here. But with this python script [1] that wraps a running ghci instance instead. Everytime a Haskell file is saved, an autocommand (set from the neovim configuration file), sends a ":reload" command to the script, which is passed on to the GHCI instance. The resulting output is written to a plain text file. Vim has native support for navigating through a list of errors in a file. So I can use vim's native error navigation to view and jump through error locations. Using the RPC API [2] that neovim provides, the python script can also send commands to the neovim. I use this to change the color of the status bar, if there were any errors in the output. [3] is a video that shows this script working on the Stack tools code base. The best part of this is that little overhead is added to the Editor. And if something hangs on the script part, I can inspect it's output and just kill it and restart. Since it just wraps the "stack ghci" command, it is pretty much guaranteed to work if stack ghci works for your project. [1] https://github.com/sras/ghci-remote [2] https://neovim.io/doc/user/msgpack_rpc.html [3] https://youtu.be/cwUzDjgaI1c
You might like [ghcid](https://github.com/ndmitchell/ghcid), even has integration into neovim it seems.
Yes, I have tired it. Just couldn't see a good enough reason to make the switch. There are a couple of other things that the python script provides me with. One is a gui, where the errors and warnings are displayed. I can click on the errors to have them open in the editor. I can put this whole gui on a separate monitor (you can place a terminal on a separate monitor also, but I just find it easier to scroll through a gui than a terminal, if there are errors than span multiple screens), that gets automatically updated with each file save. I can also send custom command to the wrapped gui (because it is actually a server listening on a socket for commands as opposed to one that just watches for file changes), and have the output available in gui. So I can inspect types or evaluate simple expressions, or load a different file etc etc....
It should be a directory with a non-varying name and the version-dependent bits should be put in there. E.g. ".ghc.env/x86_...". That way you add the directory to your .gitignore and can just forget about it forever.
Nice work! Is it integrating with Vim?
Does it have autocomplete?
For my opinion, "Haskell from first principles" is the only book that you must read if you want to learn Haskell and FP in general. It gives detail and understandable explanations of lambda calculus and all Haskell' features, with a lot of exercises by the way. It's the only book that will give you solid and correct understanding of monads! I've try to read a lot of books about Haskell (appr. 10-15) through the years and I can say that this one is far more beyond the others in terms of quality of teaching process.
Intellij-haskell
Spacemacs + Intero
Kind of. You have auto completion but not for stuff that are defined in the current buffer.
VSCode/Neovim/Spacemacs + haskell-ide-engine (HIE). You can also add ghcid on the side if you want, but for auto-completion, types on hover, looking up docs, etc, you'll need something like HIE (or Intero).
I use VS Code as well, but with the `dramforever.vscode-ghc-simple` plugin. This plugin uses `ghci` internally, which means the dependency on `ghc-mod` goes away. It's working for me currently with GHC 8.4.3.
Spacemacs + interior + ghcid
I use the same. Somehow it just works as long as the project uses stack. 
This is what I use, plus weeder. 
HIE is a Language server protocol[0] (LSP) server, and then you install a client in your editor. In VSCode you do it via [1], but Neovim it’s just add the config to the LSP plugin. There’s also a atom plugin for HIE. The advantage of LSP, and thereby HIE, is that, as the languagserver site stats, you get a much smaller number of editor integrations needed for your plugins. Kinda like the advantage of IR for a compiler. Anyways, so HIE is started as a server via the editor, and then they communicate via LSP. HIE doesn’t care what editor you are using while talking to it. As for hie itself it gathers a lot of the tools that have been around into a much more stable and nice coherent layer. It’s still very new though. [0] https://langserver.org [1] https://github.com/alanz/vscode-hie-server
Just found out about weeder. Will definitely try it! 
Or you could do: ``` example :: Reader [(a,b)] c example = takeAll keys (&gt; 5) ```
As dante's author, I am surprised (and pleased) that this comes out on top. Quick tip: don't forget the spinoff package \`attrap\`: [https://github.com/jyp/attrap](https://github.com/jyp/attrap)
I'm currently configuring VSCode for Haskell development :) Works really well. Packages: \- Haskelly \- Haskell Syntax Highlighting \- Haskell-linter \- Haskell ghc-mod \- Haskell-ghcid [https://medium.com/@dogwith1eye/setting-up-haskell-in-vs-code-on-macos-d2cc1ce9f60a](https://medium.com/@dogwith1eye/setting-up-haskell-in-vs-code-on-macos-d2cc1ce9f60a)
Neovim with `nmap &lt;buffer&gt; &lt;space&gt;ö :w&lt;cr&gt;ö:r&lt;cr&gt;ö` where `ö` [switches back and forth from a terminal buffer](https://github.com/Tarmean/Vim-Set/blob/master/config/keybindings.vim#L2) with ghci open.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Tarmean/Vim-Set/.../**keybindings.vim#L2** (master → 4833d06)](https://github.com/Tarmean/Vim-Set/blob/4833d06b34f08ea8a5971a44810f6a787ad8e1f0/config/keybindings.vim#L2) ---- 
a week ago someone told me about boon, now I learned about dante, you're all over the place :)
I wouldn't recomment Haskell From First principles. This book is too long. You might be interested in Get programming with Haskell. See comparison of two books here: * https://www.reddit.com/r/haskell/comments/82p0de/haskell_books_comparison/dvbt110/
We are using Haskell at production for web-application backend and it's great! Here are some libraries that we use: * `servant` for describing API * `postgresql-simple` for DB * `ekg` for monitoring * `proto-lens` for protocol buffers communication * `aeson` for JSON parsing/conversion And much more. Authentication with JWT, hashing passwords, AWS, etc. It's quite fun and pleasant to use!
I recommend using `data-msgpack` instead of `msgpack` package. The reason for this is in the `data-msgpack` package description. Also `msgpack` Haskell package contains some bugs. But we used `data-msgpack` in production and it was great! * https://hackage.haskell.org/package/data-msgpack
Dante doesn't require Stack, Intero does. That's the only big difference. If you are using Stack, then Intero gives you seamless integration with it. 
Vim vs emacs is more than just hjkl vs C-x C- Vim is about the terminal as an IDE and emacs is about emacs as an operating system. Vim is part of a unixy separation of concerns while emacs is about bringing everything into the emacs universe so that you can manipulate it with elisp. I generally prefer the vim philosophy but as far as I can tell there is a better ecosystem surrounding haskell in emacs so it might be worth switching to use emacs as a haskell editor for this reason.
There is no distinguished one-tuple in Haskell. Any `newtype` will do the job (e.g. `Data.Functor.Identity`).
It's not an IDE *per se*, but I use a terminal window with two or three tabs open to do three different tasks: * Edit with Vim (because it or something like it is generally available on every Linux installation, so I can use it on my machine, via SSH, etc.) * Examine and test things with GHCi * Compile and run the whole program I also like to use Cabal with its nice sandbox feature.
how well does that work/how does it compare to "native" intellij languages?
So many successful projects! Amazing! Congrats to the students and their mentors.
I actually have a copy of this laying around. I might give it a go. 
And an Org lib in Haskell might be extracted from Pandoc and be useful in other situations, too. Though org-mode without all the auxiliary elisp around it is probably of limited use.
It sometimes does weird things when editing but works best from what's available.
Congrats to all involved!
Thanks, that's a great solution
Nope, I just have the reference open on my browser and a terminal next to me and just run stack build everytime I make a change. I wouldn't use the backend if I had it running
The installation is pretty complicated not? Is there any good tutorial? &amp;#x200B;
HPFFP? My copy is in the 1200 page range. I've not received any emails or notifications that the book has changed recently, and I'm subscribed to their list. Julie Moronuki, one of the co-authors of HPFFP, is working on new material. One is a book called The Joy of Haskell, and it recently got too big, she and her co-author (Chris Martin) have decided to split it into two books. Maybe that's what you are referring to? HPFFP has many exercises and some of the lessons are definitely implicit, embedded into the problem sets.
## Requirements The language client requires you to manually install the [HIE](https://github.com/haskell/haskell-ide-engine) language server, $ git clone r/https://github.com/haskell/haskell-ide-engine \--recursive $ cd haskell-ide-engine &amp;&amp; make build-all Alternatively you can just stack install , but make build-all will give you the best setup. stack is installed on my computer and running &amp;#x200B; developer@monad:\~$ stack install Error parsing targets: The specified targets matched no packages. &amp;#x200B; What am I doing wrong? &amp;#x200B; &amp;#x200B;
neovim
Hffp is not really that long. It has 2000 pages but the pages are really short, aimed as mobile devices. I'm reading it now and I would say it's well under 1000 normal pages. It's very good.
Doing the following should be all you need, git clone https://github.com/haskell/haskell-ide-engine --recursive cd haskell-ide-engine make build-all
It's great to see all the progress that these students made. It sounds like a number of these changes are expected to land in GHC 8.8. I'm particularly excited about Ningning Xie's work on a dependently typed core replacement. Even though there's nothing tangible that benefits end-users today, her work is the first real step toward dependent haskell that has happened since GHC 8.0. Also, special thanks to Alexis Williams. Few people ever want to do the dirty work needed to improve build tools, and she made a ton of progress on the cabal build tool.
I went to a presentation by Spivak about this paper, and he stated that the value was in allowing category theorists to give input from their own field. So the value-added is the ability for another area to work on the problem. He was aware as is stated in the paper that the work doesn't represent anything groundbreaking.
&gt; and non-moving The [documentation for `StablePtr`](http://hackage.haskell.org/package/base-4.11.1.0/docs/Foreign-StablePtr.html#t:StablePtr) says the opposite: &gt; ordinary references may be relocated during garbage collection
What do I have to do as next? &amp;#x200B;
Emacs with `haskell-mode` and `ghcid`.
Alexandre's graph benchmarking project makes me want to take a closer look at alga. However, her tutorial link leads to a 404. Anyone know a correct URL?
Never mind, I think I found it! [https://nobrakal.github.io/alga-tutorial/index.html](https://nobrakal.github.io/alga-tutorial/index.html)
After that you have HIE set up. Next install e.g. VSCode with the plugin and try it out in a Haskell project (preferably with stack).
For me the biggest benefit of Dante is that I can actually use it as it doesn't force you to switch your development workflow to a Stack-based one which would be a non-starter. JYP deserves major praise for having created Dante and I personally owe him already several beers for that.
Pseq should only do WHNF and thus not evaluate (a,b), just (_,_) with thunks in them. This I just a guess (on mobile without references), but a pdeepseq instead of seq should do the trick. And don't forget to run with +RTS -N ;)
Why did you want to take a look? Don't the charts indicate that alga is slower than the other approaches? I only see `transpose`, `addVertex`, `creation`, and `addEdge` where it comes out somewhat ahead. In all the other functions it seems to lag behind, sometimes by a large margin. Or is perhaps the usecase you have in mind heavily write-biased where such skewing is beneficial?
Thanks a lot works like a charm &amp;#x200B;
Am installing Leksah [0.15.2.0](https://0.15.2.0) now. Is there any chance newer binary builds for windows become available soon?
My guess is that (once you ensure that a and b are evaluated) the problem is the RTS opts. Without it, GHC will assume you only want to use a single core.
From your code I guess you're using GHCi for your tests. And as far as I know, it doesn't support multithreading (at least it doesn't support fork) Perhaps you should compile your program using the `+RTS -N` flags
Further to what others have said about +RTS, you'll need the -threaded flag when you compile 
Couple notes - If you want parallel execution you need to compile with` -threaded` - You need to specify the threadcount like `./myexec +RTS -N4 -RTS`, alternatively just pass -N to use all cores - If you compile with optimizations this probably will be cse'd so test2 n = let a = bigsum n in a `par` a `pseq` (a,b) 
According to your code, `a` and `b` in `(a, b)` are always the same. OTOH, your output shows different values for `a` and `b`. Also, since `a` and `b` are always same there's no need to compute both, which explains the lack of difference in time spent - but not memory.
Thanks for the report :) It is corrected now!
`pseq` forces its _first_ argument, which is an `Int`, so this is not a problem. It is the subsequent printing of the answer which forces the pair.
The problem is I was not sure either. Now that I understand the problem I am able to progress further, until I hit the next problem.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/post-rfc/.../**sotu.md** (master → 96e0052)](https://github.com/Gabriel439/post-rfc/blob/96e0052a1f3797e0257d43138813b6503bdb342f/sotu.md) ---- 
In addition to the all the other things which have already been pointed out: since the definitions of `a` and `b` are the same, the compiler may choose to optimize your program to let bigsumn = bigsum n a = bigsumn b = bigsumn in a `par` b `pseq` (a,b) in which case the two threads are trying to compute the same value, so one of them will block until the other finishes computing this value. The optimized version of `test1` runs twice as fast as a variant with `a = bigsum n; b = bigsum (n+1)`, so I think the compiler is definitely performing that optimization. The optimized version of `test2` runs as fast as a variant with `a = bigsum n; b = bigsum (n+1)` though, so I guess the `par` and `pseq` complications block the optimization somehow.
&gt; Also why is the memory usage shown as half? Interesting! I ran `test1` and `test2` several times, and while they both used 9.6e9 bytes most of the time, `test2` sometimes only used 4.8e9 bytes, whereas `test1` never did. I think the memory consumed by sparks simply isn't counted: &gt; bigsum 50000000 1250000025000000 (1.05 secs, 4,800,289,528 bytes) &gt; do {let {a = bigsum 50000000}; a `par` threadDelay 2000000; print a } 1250000025000000 (2.01 secs, 288,152 bytes)
To expand on the GHC warnings, you can also pass in \`-Werror\` to make the build fail on these warnings. Check out the "Warning flags for a safe build" section of [https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/](https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/) for some more suggestions on that area. Besides that, the linting tools others have mentioned are good for giving you warnings/suggestions in your IDE as you code.
Sorry, I messed around with the code a bit and forgot to update the output. I initially tried with letting b use (n+1) instead to ensure it wasn't an optimization of them having the same value but this yielded the same result.
I tried b with (n+1) at first to avoid this (which is why my pasted output doesn't match the code) but it actually had the same result as you also saw.
Thanks a lot! It's now able to import some of the packages correctly as shown in the example on https://wiki.haskell.org/Grapefruit with the exception of &gt;import Graphics.UI.Grapefruit.Circuit which I think is preventing me from actually running the example. Making progress on this though and you've helped me a lot.
Seems indeed to be GHCi. When I compile the program with threading and run it like that it does fully engage both cores. So either you have to enable it in GHCi or it's not supported as you said.
I'm not sure, but have you tried assigning actual values to your variables? This might (!) be the cause.
The problem here is that the "boxes" on both sides need to be the same. On one side, the "box" is `[]` and on the other side the "box" is `Maybe`, which is what the type error is trying to tell you. If you look at the type signature of `&lt;*&gt;`, it is (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b Notice that you need to have the same `f` for the first and second argument. Roughly speaking, since you have `Just (*2) :: Maybe (Int -&gt; Int)` on the LHS, so the compiler thinks that `f = Maybe, a = Int, b = Int`. Then you have `[1, 2, 3, 4] :: [Int]` so the compiler thinks `f = [], a = Int`. It isn't possible to satisfy both of these simultaneously. You will see a similar failure if you try writing something like: Just (|| False) &lt;*&gt; Just 1 In this case the problem is not with the "box" `f` but with the `a` type variable.
Which version of GHCi are you using? I get your expected result using 8.0.2: λ&gt; let f :: a -&gt; a -&gt; a -&gt; a; f = undefined &gt; f :: a -&gt; a -&gt; a -&gt; a λ&gt; let x :: Char; x = undefined &gt; x :: Char λ&gt; :t f x f x :: Char -&gt; Char -&gt; Char 
I do get `f x :: Char -&gt; Char -&gt; Char`, as expected. I'm using ghc 8.4.1, which version are you using?
[removed]
[removed]
To expand on why it makes sense to require this: the box analogy works fine for things like `Maybe` and `[]` because they have a notion of _unboxing_. You'd naturally expect to be able to combine different types of boxes because the analogy doesn't differentiate between them. The thing is, there are Applicatives/Monads that don't have a notion of unboxing. For example, if I have an `IO Int`, there isn't a way to get the `Int` out of my `IO` box. In fact, you'd have a hard time trying to find similarities between boxes and `IO`. The `Applicative IO` instance gives us a very precise way in which two `IO` things can be combined but since it doesn't provide a general way to take values out of it, we can't just combine it with arbitrary other boxes.
I don't like how you've nested things, it look s like multiple type ascriptions on the same value instead of kind and sort ascriptions. `x :: (Ex2 b a :: (Custom :: Type))` is probably the better way to parenthesize it (with another layer of parens on the outside if you must). The short answer is: No, but why would you want to? The longer answer is, you can definitely build values out of types. So, you can definitely have a function of type `f :: Type -&gt; Type -&gt; Custom`, and you can build types combining all kinds of things. Types can have "shapes" that depend on other values, and have values that depend on their "shapes" and everything in between. However, it turns out that if you have `x :: A` then you can *guarantee* that `A :: Type`, so you'll only ever be 3 layers deep `x :: (A :: Type)`. This doesn't require TypeInType; it's fairly deeply built in to context, type existence, and type of term judgements in MLTT, for example. (There's universes / universe levels, too, but they are uninteresting, mostly.) `Type -&gt; Type -&gt; Custom` is already a type, and you can create an alias for it if you'd like, and you can write a value of that type -- which would be a function, and you can use that function or a (potentially partial) application of it to create other types, but you'll always have that `Custom :: Type`, `Type -&gt; Custom :: Type`, `Type -&gt; Type -&gt; Custom :: Type`, `f :: (Type -&gt; Type -&gt; Custom :: Type)`, `f Nat :: (Type -&gt; Custom) :: Type`, and `f Nat Type :: (Custom :: Type)`. HTH
wait...If I have `x :: A` why can I guarantee `A :: Type`? isn't our (limited) levity polymorphism already violating this rule? Or do you mean that there's a fundamental difference between `x :: A::B` and `x::A`?
It is not TypeInType, but the DataKinds extension which allows you to create kinds other than `*`, `* -&gt; *`, etc. {-# LANGUAGE DataKinds, GADTs #-} import Data.Kind -- | -- &gt;&gt;&gt; :kind 'CustomConstr -- 'CustomConstr :: * -&gt; * -&gt; Custom -- &gt;&gt;&gt; :kind Custom -- Custom :: * data Custom where CustomConstr :: Type -&gt; Type -&gt; Custom Note that while `'CustomConstr Int Double :: Custom` and `Custom :: Type`, it would not be correct to write `('CustomConstr Int Double :: Custom) :: Type`, because that would be claiming that `'CustomConstr Int Double` has both kind `Custom` and kind `Type`; you probably meant to write `CustomConstr Int Double :: (Custom :: Type)`? Also note that it is not possible to come up with an `x` such that `x :: ('CustomConstr Int Double :: (Custom :: Type))`, because types such as `'CustomConst Int Double` does not have inhabitants, only types of kind `*` do.
This https://medium.com/axiomzenteam/functor-applicative-and-why-8a08f1048d3d article is what helped me understand functor and applicative.
You can't go wrong with following the types in Haskell Looking at the kind of [*Applicative*](https://hackage.haskell.org/package/base-4.11.1.0/docs/Control-Applicative.html#t:Applicative) &gt;&gt; :kind Applicative Applicative :: (Type -&gt; Type) -&gt; Constraint and the type of [*&lt;*&gt;*](https://hackage.haskell.org/package/base-4.11.1.0/docs/Control-Applicative.html#v:-60--42--62-) &gt;&gt; :type (&lt;*&gt;) (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b we can instantiate (pick) *f :: **Type** -&gt; **Type*** to what ever constructor we want with [visible type applications](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#extension-TypeApplications) (*@*). Once the choice is made though that's it, all *f* are instantiated the same: If we choose *Maybe :: **Type** -&gt; **Type*** (writing [***Type***](https://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Kind.html#t:Type) for `*`) &gt;&gt; :set -XTypeApplications &gt;&gt; :t (&lt;*&gt;) @Maybe .. :: Maybe (a -&gt; b) -&gt; Maybe a -&gt; Maybe b &gt;&gt; :t (&lt;*&gt;) @Maybe @Int .. :: Maybe (Int -&gt; b) -&gt; Maybe Int -&gt; Maybe b &gt;&gt; :t (&lt;*&gt;) @Maybe @Int @Int .. :: Maybe (Int -&gt; Int) -&gt; Maybe Int -&gt; Maybe Int then both arguments and the result are *Maybe* arg1 :: Maybe (Int -&gt; Int) arg1 = Just (*2) arg2 :: Maybe Int arg2 = Just 10 res :: Maybe Int res = arg1 &lt;*&gt; arg2 Same with *IO :: **Type** -&gt; **Type*** &gt;&gt; :t (&lt;*&gt;) @IO .. :: IO (a -&gt; b) -&gt; IO a -&gt; IO b list constructors *[] :: **Type** -&gt; **Type***, &gt;&gt; :t (&lt;*&gt;) @[] .. :: [a -&gt; b] -&gt; [a] -&gt; [b] and *Either _ :: **Type** -&gt; **Type*** and *(-&gt;) _ :: **Type** -&gt; **Type*** &gt;&gt; :t (&lt;*&gt;) @(Either _) .. :: Either w (a -&gt; b) -&gt; Either w a -&gt; Either w b &gt;&gt; :t (&lt;*&gt;) @((-&gt;) _) .. :: (w -&gt; a -&gt; b) -&gt; (w -&gt; a) -&gt; (w -&gt; b) *Either* and *(-&gt;)* both have kinds ***Type** -&gt; **Type** -&gt; **Type*** (slight fib) so we must apply them to a type before they even qualifies as `Applicative`s &gt;&gt; :kind Either Either :: Type -&gt; Type -&gt; Type &gt;&gt; :k (-&gt;) (-&gt;) :: Type -&gt; Type -&gt; Type
That's odd, I just ran it in 8.4.3 and got the same as above. How did you install your GHC version?
you are right. The nestings were wrong. But I wonder whether there lies also a fundamental problem with my idea. Would `('CustomConstr Int Double :: Custom) :: Type` not being the same as `('CustomConstr Int Double :: (Custom :: Type)) make it impossible to pass `'CustomConstr Int Double` into `Custom -&gt; Custom`?
Sure: -- | -- &gt;&gt;&gt; :kind 'OtherConstr ('CustomConstr Int Double) -- 'OtherConstr ('CustomConstr Int Double) :: Custom data Custom where CustomConstr :: Type -&gt; Type -&gt; Custom OtherConstr :: Custom -&gt; Custom 
Okay, okay, `A` could also be un unboxed type. But it can't be one of the types created by `DataKinds`, such as `Custom.
&gt; If I have `x :: A` why can I guarantee `A :: Type`? First, I'm *definitely* ignoring the difference between lifted/unlifted and boxed/unboxed types. I thought levity polymorphism was about those differences, and if I'm wrong about that I definitely don't know what levity polymorphism is about, so I'm definitely not trying to say anything about that. Second, it really shouldn't be surprising that `x :: A` implies `A :: Type`. There are types that are morally uninhabited, `Void` being the primary one, but many indexed types (GADTs) will have some index values where no constructor matches, or have even more subtle ways to be uninhabited, so it's not like `x :: A` is even necessary for `A :: Type`. Finally, I'll also appeal to the MLTT "metatheory" that a typing judgement `ctx |- term : A` is only clearly a typing judgement only when `ctk |- A : Type` (a type existence judgement) is also valid, and *that* is only clearly a type existence judgement when `|- ctx` (a well-formed context judgement) is also valid. You can also look at the specific bits of MLTT (and some extensions) where `A : Type` statements are established just before or simultaneously with `x : A` -- they generally appear before in the prose, and require the same or similar contexts; so become true over the same scope of a program. I'd also encourage you to try out dependent types in Agda or Idris. They are both fairly friendly to Haskell programmers, syntax-wise. Coq and other dependently typed systems would also be a good place to experiment, but I found Agda and Idris must friendlier. Agda also has the advantage of having the intent of following MLTT closely and being willing to work with any axioms consistent with it. Coq I think uses the CoCIC (Calculus of Co-Inductive Constructions) instead of MLTT, so it has a distinct flavor. I think Idris uses some internal system just called "TT", but I don't think that's nearly as well documented as the other two. My experience is that it is also much slower to typecheck things.
atom + atom-ide-ui + ide-haskell-hie + language-haskell
Eh in my experience using both Nixos and Nix/Flatpak on other distros, Flatpak works better and has fewer strange bugs. 
Didn't know about ekg. Seems great !
Thanks. This gives me a new perspective. Will play with IO and get some new insights.
Thank‘s for your detailed answer!
Using Single Instruction, Multiple Data (SIMD) instructions to build a rank-select bit-string to implement a fast word count program
Using Single Instruction, Multiple Data (SIMD) instructions to build a rank-select bit-string to implement a fast line count program
Is that just implementing what the LLVM backend in GHC already supported, but for the NCG?
[removed]
You could also use `memchr`, like [this](https://gist.github.com/chrisdone/c6a98656cc1867f5781ef9e4fd6bdbe2/revisions#diff-79614c71e16c5ecba392fe2fbce220bd). 
Generational GC. My answer is from digging into GHC years ago, and from rough reading and memory, so maybe it changed since - and may contain inaccuracies. Anyone who knows better, feel free to correct me. * A small(by-default) double-buffer "nursery": Allocations advance a pointer in the nursery. When it is full, a minor collection copies just the live nursery objects to the other (empty) nursery, and allocations resume in it until it's full again. Default nursery size is 0.5MiB meant to fit in L2 CPU cache. * Objects that are copied in the nursery (as specified above) multiple times, get allocated outside the nursery (not sure where this memory actually sits), in generation 0. * When generation 0 grows enough, a gen0 collection traverses live objects in nursery+gen0 to promote them to generation 1 (by default this is the highest generation). * A nursery collection happens very frequently. I think only nursery-&gt;nursery, and nursery-&gt;gen0 are moving collections. * A full collection (that also traverses gen1) is the rarest, and has to traverse all memory. * The invariant is that new objects (nursery &gt; gen 0 &gt; gen 1) only point to older objects in higher generations. Purity makes this usually true. However, IORefs (and cycles + laziness?) can violate this invariant. This is why there's a "write barrier" that detects a reference from old-&gt;new is created. At this point the new object is promoted to the older generation to preserve the invariant. * The invariant means it is safe to traverse+copy just the nursery roots and ignore older roots and older objects. This is what makes nursery collections very cheap.