Based on the indices I am going to go for AoP. EoP goes into specific algorithms much earlier in the book, compared to AoP, covering less of my current subjects of interest. EoP is coming right after though cause I will needed for C++ until Haskell phases it out, hehe.
That's why we have to have humans in the loop. I am not at all proposing to have build bots automatically tweak constraints.
Yes, people have suggested before that we have a different syntax for upper bounds where it's just conservative vs where you've checked and you know it doen't work. It's certainly plausible, but educating authors may be tricky. It's unlikely we could get it to be used consistently.
If an upper bound violation becomes just a warning, we could have something like `-Werror` for production where you want to verify that a particular version is known to work.
What do you intend `randR`to be? That type can't make random numbers(and `unsafePerformIO` wouldn't help either). What you want to do is relatively easy: just make `sampler` be `Double -&gt; Double -&gt; Vector3 Double -&gt; Vector3 Double -&gt; Int -&gt; Int -&gt; Int -&gt; (TraceResult, Vector3 Double, Double)`, and `sample` be a `Int -&gt; Int -&gt; Int -&gt; State StdGen (Vector3 Double)`: doing it in the first is trivial and `sample` can be rewritten as do notation by turning the where clause into let's, except that one line that calls `sampler` where you actually use the monad, and you what you have on the definition now at the end with `return`. `sample64` should also be easy. Also, you randR is basically [this xkcd comic](http://xkcd.com/221/).
This is very cool, and small! Looking forward for being able to find definitions in files other than the current. Looks like bert-1.1.2 isn't compiling, though: Data/BERT/Term.hs:150:10: Illegal instance declaration for `BERT String' (All instance types must be of the form (T a1 ... an) where a1 ... an are *distinct type variables*, and each type variable appears at most once in the instance head. Use -XFlexibleInstances if you want to disable this.) In the instance declaration for `BERT String 
There's bert-1.1.2.1 on hackage — just run `cabal update`. And ariadne is on hackage as well, so `cabal install ariadne` should be enough (for the server part).
These disconcerted efforts to create tiny, separate pieces of IDE functionality are leading us nowhere. We have ghc-mod, scion, haskell-mode, now ariadne. Hopefully with latest features of ghci (coming in 7.8?) maybe we will be able to build something similar to lisps slime-swank or clojure's awesome nrepl. 
PVP does not tell you what to put in your build-depends, it tells you what to put in your version field (and therefore what is *safe* to put in build-depends). I use an upper bound of 4.7 on base when I use a lot of things, including new things that might break soon, and 5 when I'm only using the Prelude and related simple stuff.
Very nice!
Are you talking about hot-swapping?
More complete IDE-like experiences would be nice, of course. But I disagree that these are leading us "nowhere". Haskell's syntax and semantics are deceptively complex. Code that appears to be simple and easily readable by experienced human Haskell programmers can be surprisingly difficult to analyze and use effectively by automated tools. So great work like Roman's is very important. While implementing a nice feature in VIM, it also lays the groundwork for future improvements to all IDEs.
xmonad and vim
https://github.com/yairchu/red-black-tree/blob/master/RedBlackTree.hs#L27
I, for one, welcome our separate pieces of IDE functionality. I live in my text editor, not in one language. It's a whole lot more likely that I'm going to add a tiny piece of language-specific IDE functionality to my workflow than jump to a whole new IDE for one language. The former lets me keep playing with the tools I'm already familiar with, and organically get used to using the new stuff people come up with, rather than starting from scratch and hoping the new IDE knows about &lt;s&gt;second breakfast&lt;/s&gt; regex-search replace. Now, we could certainly recommend funclet bundles for various editors to get people new to the language (or to the editor) up to speed.
this is great. I was thinking about writing something like that, but for finding usages(like "find usages" in Java IDEs). any ideas on how to approach this problem or how hard would it be?
Let's lynch for worrying 'bout the imaginary lynch mob. You did a good job. It'll be better if you fix the bug and shorten the code, of course.
It'd be nice to replace SublimeHaskell's existing go-to-definition functionality with something smarter like ariadne.
also: http://haroldcarr.com/posts/2013-08-24-type-level-computation-in-haskell-via-gadts.html
The problem with tags is that they are not context-sensitive. They don't know that the same name in different places can refer to different things.
Oh, that is too bad. I always hoped that this was a limitation of the tool that generated them, but it makes sense if you think about it.
Very nice! Now we just need an emacs wrapper, or better yet, a haskell-mode flag :) Also, I find your vim font+size very clear and easy to read. Any chance you can share some details there? 
The font is not my usual one — I chose it specifically for the video. It's Inconsolata 18. Normally I use DejaVu Sans Mono 11.
&gt; There are several caveats that I need to state about these proofs: &gt; First, there is the obvious caveat that these proofs are still not machine-checked Erm... isn't this a really, really big caveat? This person has shown that their design ought to work, but they have no way to verify that they implemented their design correctly. Wouldn't it be better to have some tests that check that things are working as expected for a bunch of different cases? I'm not convinced that being "the only [library] that does not have a test suite" is something to brag about.
http://blog.piechotka.com.pl/2013/04/10/statically-typed-red-black-trees/
...well, we'd just need to annotate a "version number" at the function level; so each time a non-semantically-invariant change not visible at the type-level occurs, one would increment the per-function version number :-) 
Neat. I'm going to study the code :-) How is the performance compared to the original ray tracer?
Step 1: build this into yi
 "find usages" would indeed be very useful.
As the saying goes &gt; Beware of bugs in the above code; I have only proved it correct, not tried it.
If you read the proofs, they're performed mostly by equational reasoning on the actual source code. So, so long as the proofs correspond to the actual code he's written (which right now they do) there is no point in writing test cases for these properties.
This is not about a test suite. This is about formal verification with a tool such as Coq or Agda. Which very few libraries have (read: practically none).
In my next post, I'll be elaborating much more on where these resource problems actually stem from. But basically: pipes-safe is correctly providing exception safety, but is unable to provide promptness. It is limitations in pipes itself which makes it impossible to get promptness.
Think of it like this: he *manually* showed that his library passes *all possible* test cases.
Why would you think that proofs are a replacement for tests?
The problem with Coq or Agda proofs is: as long as the resulting (Haskell) code isn't automatically derived from the proofs (and the derivation process is 'correct'), you only prove your theory is sound, not your implementation.
As plenty of other commenters have observed, enforcing colour and black-height invariants is not unusual. (Does your type enforce the black-height invariant?) For insertion and deletion, it can be helpful to construct the corresponding zipper structure, indexed by colour and black-height both at root and hole. That way, you can represent a red-black tree which maintains the invariants everywhere except at the point where you're editing it. You can then develop a type of "insertions", being things you might want to put in the hole, but which might not fit (because they're too black-tall). The action of insert is then firstly, to find the place to make a hole for the new element, and secondly, to bubble the resulting "insertion" structure from hole to root. The type of "insertions" at the root then becomes the return type for insert: it typically captures trees which are the correct black-height and colour, or black trees too tall by one. The same works for deletion. The basic plan is: find the node to delete; if it's not extremal, find an extremal neighbour to replace it; propagate a "deletion" from an extremal hole to the root. A "deletion" is either a tree which fits in the hole, or fails to by being one too black-short. By using the zipper to represent the single point at which the invariant is broken and suitable types of candidates for "what's intended to go in the hole but might not fit", you can represent just the not-quite-red-black-trees you need at intermediate points in the algorithm without compromising the basic presentation of the invariant.
great work. it surely is faster than using cursor keys ;-). can you give some rationale why it's not part of ghc_mod or hdevtools? i suppose it's because they use ghc while you use haskell-src-exts. it still would be nice to have them integrated.
No, he showed that his theory passes all possible test cases. This says little about the actual code in his library. If he has a typo somewhere and uses `foldl` when he should have used `foldl'` or something, then his code is not a correct implementation of the theory, and it still can have bugs in it that these proofs can never catch. The point of a test suite is that _everyone_ has good ideas and good theories and things ought to work as expected, but coding it up in practice is often tricky. No one sits down and purposely writes buggy code; the point of tests is to try to make sure that the code you wrote is the code you intended to write. I don't see how proofs can help with this.
Because that was the easiest way to do what I needed, I guess. I'm not even sure what being part of ghc_mod or hdevtools would mean.
All these laws are *low hanging fruit* for QuickCheck tests. If I was in Tekmo's place I would be all over QuickCheck and only bother with the formalities after they are written and passing.
That work is impressive. How is the status of Ajhc in the Android platform?. I saw the android demo at https://github.com/ajhc/demo-android-ndk and it is really promising, and, potentially, with a lot of programmers waiting something like that. What can be done and what can not be done in Android with Ajhc?
I'm sure he could get convinced by a good patch.
Right now all you've managed to do is spread FUD, imo. You've documented a case where your library works, and where another library doesn't - though I've demonstrated below that it can be done. You haven't said that it can be done, and you haven't said why it can't be done either. If there's something to the specific approach you took, then you failed to document that - and if it is about a specific approach, it would have helped to use an example where you didn't have to write non-idiomatic code. I find this pretty shoddy.
I think you missed a lot of my blog post. I was quite open about the fact that the specific problem I brought up was easily solved, and then presented a more complicated problem which couldn't be solved as easily. I also made it very clear that I'm trying to introduce an improvement to both pipes and conduit, and that establishing the problems in the two libraries is the first step. If there's no acknowledgement of a problem, a solution is not helpful.
I think my introductory paragraph was pretty clear; this is the first part in a multi-post series, and I'm establishing the fact that there's a problem. I'm going to give conduit the same treatment next time around, I just considered the resource management problem severe enough to deserve its own blog post, and this is an area that conduit is not currently suffering.
The proofs proceed principally by substituting definitions -- i.e. the code he wrote -- into other definitions -- i.e. more code he wrote. What is theoretical is that the user has supplied a valid monad instance. 
I enjoy these blog posts, and disagree about 98% with your sentiment. There is active disagreement between the two parties, and while they have tried to figure out a common solution, that didn't get anywhere. Therefore, the best option is to propose problems then debate whether 1) it is a real problem; 2) whether it can be solved. I wish we had more of that kind of debate at the web framework level, since as an outsider it gives valuable insight into the current state of the art. My 2% disagreement is from the pharse "I don't believe my concerns have made much of an impact" - that slightly ups the adversarial tone without contributing to the technical discussion. Having a technical discussion with people you don't know that stays civil is very tricky, and fairly small things can tip it over the edge. However, I don't for a second think Michael meant it in an adversarial way!
The "more complicated" problem is exactly the same as your first problem, and is fixed by `bracket`ing your resource usage: [nix-shell:~/work/pipes/pipes-safe]$ for i in `seq 1 5000000`; do echo "Hello" &gt;&gt; bigfile; done [nix-shell:~/work/pipes/pipes-safe]$ wc -l bigfile 5000000 bigfile [nix-shell:~/work/pipes/pipes-safe]$ mkdir -p out-pipes [nix-shell:~/work/pipes/pipes-safe]$ runghc -isrc split.hs [nix-shell:~/work/pipes/pipes-safe]$ wc -l out-pipes/* | tail 50 out-pipes/99991 50 out-pipes/99992 50 out-pipes/99993 50 out-pipes/99994 50 out-pipes/99995 50 out-pipes/99996 50 out-pipes/99997 50 out-pipes/99998 50 out-pipes/99999 5000000 total [nix-shell:~/work/pipes/pipes-safe]$ ls -l out-pipes | wc -l 100002 Where `split.hs` is: import Pipes import qualified Pipes.Prelude as P import Pipes.Safe import qualified Pipes.Safe.Prelude as P import System.IO (IOMode(..)) main = runSafeT $ runEffect $ P.withFile "bigfile" ReadMode $ \f -&gt; P.fromHandle f &gt;-&gt; loop 0 where loop i = do let fp = "out-pipes/" ++ show i P.withFile fp WriteMode $ \output -&gt; P.take 50 &gt;-&gt; P.toHandle output loop $ i + 1 I fail to see what I'm missing. The only problem I'm seeing is "`pipes` break if I don't use it the correct way", which may be fair enough - but that's certainly not really the point your emphasizing in your blog post.
&gt; the point of tests is to try to make sure that the code you wrote is the code you intended to write. I don't see how proofs can help with this. Ah, but the point is that it's not just the theory, it's the code that he's proving something about. This is not an algorithm like quicksort that you prove correct on a formal level and then try to implement in C. No, these are theorems about Haskell *code* itself. You could do something similar in C ("Hoare logic"), but of course it's a lot more pleasant and powerful in Haskell.
Ah, I see now. Thanks for explaining!
Every `Arrow` is what we in #haskell-lens call a `Strong` `Profunctor`, so you can go a fair bit farther. We've experimented with versions of `over` that have an even more general signature than the one you wrote there. type Lens s t a b = forall p. Strong p =&gt; p a b -&gt; p s t This fits with nicely symmetric definitions for Iso and Prism type Iso s t a b = forall p. Profunctor p =&gt; p a b -&gt; p s t type Prism s t a b = forall p. Choice p =&gt; p a b -&gt; p s t The strength of a profunctor is what lets us define a lens. This is a slightly weaker claim than saying it is `Representable`. A representable profunctor is one isomorphic to `a -&gt; f b`. This is effectively what we currently use. As you note, there are `Strong` profunctors (and arrows) that are not `Representable` You can use `rmap f` to replace `arr f &lt;&lt;&lt;` in the signature you have, and as I mentioned every `Arrow` could be a Strong profunctor. However, this leads to a number of complications for end users. Off the top of my head: 1.) There are a number of cases where when you to use it that users start needing to put in type signatures. 2.) You get a number of uninferable types you get when you go to combine this with indexed lenses. We rely on the concrete choice of (-&gt;) to drive inference when composing indexed lens-likes. 3.) Arrow is too strong. This rules out many of the subtypes of lens, like Getter/Fold. 4.) If you switch to `profunctors`, then we lose the benefit that currently an end-user can define a lens without incurring a dependency on the `lens` package, forcing them to at least incur a dependency on `profunctors`. 5.) Currently `traverse` is a traversal. With pure profunctor lenses this becomes a representable profunctor whose representation is an applicative. This annoyingly takes us outside of standard Haskell. Currently end users can define lenses and traversals without having to use any extensions or external packages. They can define getters and folds with only a dependency on the Haskell 98 `contravariant` package, and they can define prisms and isos using just `profunctors`. 
The main claim to fame the original had was that it fit on the back of a business card, not its speed.
If he showed it by equational reasoning, how does it help give any guarantees about memory/stack use not exploding in practice?
&gt; Every Arrow is what we in #haskell-lens call a Strong Profunctor, so you can go a fair bit farther. Yes that makes sense. One doesn't actually need the composition `(&lt;&lt;&lt;)` of an `Arrow` to define an `ArrLens`. Personally speaking, my concern is that as more and more library authors become lens-aware they will provide more and more `Lens`es in their APIs (a good thing) that are impossible to use for anyone who wants to use them with `Arrow`s (a bad thing) when they could easily just have defined `ArrLens`es (or `Strong Profunctor` lenses) in the first place. Personally I'm happy to define my own combinators to work with `ArrLens`es, so I don't particularly mind if `over` etc. don't support `ArrLens`, but if nobody exports `ArrLens`es in the first place that's useless to me. So what do you think of the suggestion of just switching to the definition type Lens s t a b = forall p. Strong p =&gt; p a b -&gt; p s t and leaving everything else as is? Your complication 4. still applies. Are there other objections? Being able to use `Lens`es with `Arrow`s is *extremely* important to me so I would like to make sure I have a thorough understanding of what's a stake here. (It's hard to overstate its importance, actually. It's crucial to be able to make any use of `Lens` in my API.) 
Test cases take less developer effort to verify than hand-written proofs, and this is important, because correctness has a social component as well as a technical component.
I don't. I just said that I prefer equational reasoning when it is an option. This is why I prioritize proving the laws over writing tests. See [this issue](https://github.com/Gabriel439/Haskell-Pipes-Library/issues/49), which I have had open for a while. I have not addressed it because right now I have limited time and I place greater trust in equational reasoning.
I don't believe there's any lazy I/O used in `listDirectory`, you can [see the implementation here](http://hackage.haskell.org/package/system-fileio-0.3.11/docs/src/Filesystem.html#listDirectory).
I also have to prove that my eyes are functioning correctly when I read the proof. All proofs are about degrees of certainty. I think most people would settle for a machine-checked proof that was not automatically extracted from the code and manually verify that the extraction is correct.
Ha, you're saying good patches are more persuasive than Reddit comments. I think you might be right! :)
[If this was a real conversation, I would have been nodding and saying ,"yes, yes, I agree," while you were talking. And then I would have said:] Yes, but you know, while there is currently disagreement, that does not mean they did not find a common solution. That means they *are finding* a common solution. (edited for grammar)
Actually, if you write a library, and your API somehow leaks what you build-depend on (which is effectively always, thanks to class instances), then you have no other choice than to restrict your build-depends in order to comply with the PVP
That version is that it doesn't work with `traverse` and the individual lenses are actually much harder to write and more verbose. There should be something like type Lens s t a b = forall p f. (Representable p, Functor f) =&gt; p a (f b) -&gt; p s (f t) If you instantiated your lenses with that type then they'd interoperate with both systems, since we can always choose `f = Identity` no power is gained or lost.
Hear hear. As I was trying to make clear in the post, Gabriel disagrees with me on the severity of this issue. I think it's severe enough to warrant rethinking some of the core design principles of pipes (and I'll make a similar argument for conduit). My goal here is to make that case that this truly is severe enough to motivate us for a different approach.
The demo-android-ndk repo is simple. It only can do change color a polygon, can not use more OpenGL ES function, can not have wrapper for Android NDK, da da da. I'll try to develop a bit more support for Android NDK. Is it good idea that entry kickstarter?
I guess 'eyes working correctly' could be considered a given thing, especially if others also read the proof: chances are low every single reader has some defect here. Anyway, I was just pointing out how it's not as easy as simply proving something, especially since implementations tend to be longer-living than a proof, so when the proof is written the implementation could be valid/proven, but later on implementations tend to be changed (e.g. adding an extra constructor somewhere for some reason), the proof isn't updated (e.g. since its existence is forgotten, or whatever), and things might break. Anyway, I do like pipes, and appreciate the way you strictly adhere to laws and basic concepts, equational reasoning correctness, and these proofs.
I want to preface this by saying that I think both tests and equational reasoning are worthwhile, but I prioritize equational reasoning. There is also a maintainer component you did not address. I have to choose the verification method that lets me sleep easy at night knowing that I'm not going to get an "existential" bug report that destroys everything I've built on top of pipes. Only these proofs give me that peace of mind.
Actually I meant type Lens s t a b = forall p f. (Strong p, Functor f) =&gt; p a (f b) -&gt; p s (f t) It seems that would work with everything in Lens and also with arrows. It would be nice if this was the standard way of defining a `Lens`. I don't think it is actually much harder to write lenses for! In some ways it's more natural. It's also strictly more powerful.
It does not. I would advocate tests to solve that.
You don't need the elements to be implemented with lazy IO in order for the result to be lazy IO; I can implement `hGetContents` with `hGetChar` and `hIsEOF` (or just `hGetChar` and `catch`) . But no doubt you have a more specialized meaning in mind. In any case, all I was thinking was that the IO operation is here structured by a lazy list, which it is. I don't think this is the principal problem with this program, though. But if this much were supposed to be legitimate, there wouldn't have been call for https://github.com/Gabriel439/Haskell-DirStream-Library and the still incomplete sequence of 100 messages about it at https://groups.google.com/forum/#!topic/haskell-pipes/ndTbGNX6yCk It is a public fact that at least one chunk of the would be 'correct pipes solution' to this problem is not yet released; and it is exactly the part that corresponds to the `traverse` you take from the conduit library. Thus the blog post and the consequent discussion are precipitate and belong on the mailing list. Only a few general libraries have so far been released and none includes a `traverse` akin to the one you use from conduit or any guidelines how to use it. 
I want to also mention that there is a less facetious reason why an automatic extraction is currently intractable: Agda does not let you encode free monads in the same way because of its positivity checker (this is one of the things Paolo has been showing me how to do), so any extraction to Agda must be manual for now.
i don't use `ghc_mod`, so i am speaking about `hdevtools`. afair `ghc_mod` is slower but more powerful than `hdevtools`, so _i guess_ it will apply to `ghc_mod` as well. (i'm also not sure i understand your confusion, but that's due to my english.) they feature great emacs and vim integration (the typechecker is way better integrated) and (using ghc) all the extensions magically work. i don't leave vim when refactoring anymore as i see type errors instantly when saving. they work cabal-project wide if configured correctly. learning to configure them is very easy, as you can adapt your knowledge of ghc(i). there is automatic type deduction and many more things. your go-to-definition is great and i envy it. but it would fit into these tools just as good. having said that. nih is a very valid thing. it's also about having fun developing it.
Congrats on your work. Of course it would be nice to have a test suite, but I suppose that's in Gabriel's plans anyway. But I think that being in the process of finishing a thesis keeps you from doing everything you want eh? :-) Also, I quite like the "proof first" approach. Ideally, Gabriel would have written the proofs and the test suite little by little, at the same time, but well that's not how it always goes. I don't mind using pipes as long as Tekmo keeps an eye on the correctness of his code (i.e how much it matches his formal model), particularly if we see a test suite appearing on the repo when he has a chance to work on them.
I believe reddit comments with excellent analogies are weighted slightly higher, but I haven't proven it. :-)
It would be excellent to have an instance Arbitrary a =&gt; Arbitrary (Producer a) though. I have no idea how one would write that, but it would be super handy for people building on pipes that want to inject some test data.
Test suites are on my radar. See [this issue](https://github.com/Gabriel439/Haskell-Pipes-Library/issues/49). Like you said, I have limited time at the moment because of my impending graduation so I have to prioritize what I work on.
Sorry, I think I'm missing something here. The `traverse` I used for pipes is almost identical to the implementation in conduit. There's no lazy I/O involved, there's just unbounded memory usage due to the use of getDirectoryContents/listDirectory. While that's suboptimal, the blog post was not about memory issues, but rather about file descriptor leakage.
I didn't imply that there wasn't a workaround available. To quote myself: &gt; It's true that the example I started off with could be rewritten to embed the SafeT block inside, and run two separate pipelines instead of one. That's certainly true, but that's only because of the simplicity of the example. But do you have an obvious solution for the [example I gave above](http://www.reddit.com/r/haskell/comments/1nw7ji/pipes_resource_problems/ccmq490)? Maybe I'm just dense, but I can't see one.
Could it be something like: `arbitrary = mapM_ yield &lt;$&gt; arbitrary`?
It would mean there being one Haskell server process running to serve data to an editor session instead of several. As it is, to get errors and type information I'm running hdevtools and now to get navigation I'd be running another. Two separate processes with separate marshaling formats, it would seem preferable to have one application supporting both features.
You can use `each` (specialized to lists) and then just reuse the `Arbitrary` instance for lists. However, my experience from my very first forays into pipes are that the pathological cases are not covered by randomizing inputs. Usually when I got it wrong it would break due to a particular combination of await/lift/yield logic spread across three pipes.
[Correct link for "resource".](https://github.com/duairc/resource)
I originally wrote this for a different comment, but that comment has since been deleted. Anyway, it's pretty relevant for this one too: OK, I think you're working off of a different definition of lazy I/O. hGetContents uses unsafeInterleaveIO to create thunks which will cause IO actions to be later run, during evaluation. Nothing like that is happening here. The full listing of the directory is being read into memory. Is that good? No. Is that lazy I/O? Not according to any definition I've ever heard of. Your definition of `hGetContents` would not be lazy I/O, since after the initial IO action runs, no further IO is performed while evaluating the resulting string. Similarly, evaluating the result of listDirectory performs no IO.
Oops. I just fixed it in my original post.
As long as both parties are having fun, let the Haskell wars go on.
I can explain why I believe this is not a severe issue for either library and why I prefer the `pipes-safe` solution. First, the core `pipes-safe` logic has no actual `pipes`-specific bits. It is just standard regions code with some minor variations (i.e. the ability to mask or catch computations). The most important consequence of this is that you can traverse pipes in any fashion, using any of the many composition operators available to you and finalization still works deterministically. This is why, for example, you can implement `ListT` on top of pipes and not conduits. For a compelling example of resource management combined with `ListT`, see the upcoming `dirstream` library, which you can find on Github [here](https://github.com/Gabriel439/Haskell-DirStream-Library). You can also safely single-step pipes for this exact same reason, something you cannot do with `conduit`. This is how pipes implements `pipes-parse`. I could claim that the inability to combine ListT or single-stepping with resource management is "severe", but I won't. Going back to `pipes-safe`, the guarantee I make is that finalizers are called when you exit the `SafeT` region. With a recent backwards-compatible update you can even nest these regions. You can also exit them early by using `runSafeP`, but that it is just a thin layer around exiting the pipeline, calling `runSafeT`, and reentering the pipeline, so you can only promptly finalize `Effect`s. Every time I try to promptly finalize `Producer`s or `Pipe`s or `Consumer`s I discover corner cases that do not work by equational reasoning. This includes alternative solutions that modify the pipe type or behavior of composition operators. For this I believe that prompt finalization is not possible to implement correctly without using indexed monads. I decided it was easier to teach users to use `pipes-safe` correctly rather than teach them to use indexed monads. I don't consider this bug sever because when you take into account `runSafeP` and nesting of `SafeT` regions you can solve the vast majority of use cases. For those remaining cases I can expose unsafe primitives to finalize things even more promptly like I did with `pipes-safe-1.0.0`, but I chose to adopt a "need to have" policy where I would only add them if a real world case came up where somebody actually needed them and so far this has not been the case, so the bug is not as severe as Michael makes it out to be. Finally, I am not convinced of the correctness of `conduit`'s finalization solution or its universal promptness. The last time I checked Michael had a unit test suite yet I still found [finalization bugs](http://www.haskellforall.com/2012/05/conduit-bugs.html). Also anybody who thinks the problem is now solved should read [this issue](https://github.com/snoyberg/conduit/pull/57) where Michael acknowledges that finalization order changes based on the order of association. I suspect there are even more problems with conduit finalization but it takes a long time to reason about conduit code because it has a lot of accidental complexity. However, I will eventually do it again and report what I find.
Why would proof not be an adequate replacement for a test? There's a reason it is called "proof" :)) Unlike test that cannot prove your code is working correctly EVEN if it passes this specific test. 
Can you give me a high level explanation of what `resource` offers which is different than `resourcet`?
&gt; This is why, for example, you can implement ListT on top of pipes and not conduits. Huh? What exactly prompts you to claim that ListT *can't* be implemented on top of conduit?
&gt; "arr s t a b" might sound like something a pirate does &lt;3
&gt; Why would proof not be an adequate replacement for a test? [Because](https://en.wikipedia.org/wiki/What_the_Tortoise_Said_to_Achilles) I also need a proof that the proof is correct, and a proof that _that_ proof is correct, etc. It comes down to the fact that it's easier to be confident in a test's conclusion than in a proof's conclusion, even though (or perhaps because) the conclusion is weaker.
Unfortunately not, because it would take _me_ more than 5 minutes, and I'd be less confident. In part, this is a social issue; if equational reasoning becomes standard, people will be (in aggregate) more confident in its results, but until then, tests are the standard.
It's still a work in progress and I haven't written any documentation for it yet. There are two different parts of `resource`: there's the `SafeT` part, which is more or less the same as `ResourceT` and then the `Resource` part, which has no equivalent in `resourcet`. They arguably could be separated, but I think they both complement each other in such a way that neither is complete without the other. It's been a while since I've looked at the `resourcet` package, but the last time I looked at it, it supported both `IO` and `ST` as inner monads, and it had a lot of ad-hoc type classes to enable this. My `SafeT` also supports this, but it uses the more general type class machinery provided by my `layers` package to do this. This means `resource` doesn't have to define its own versions of `MonadThrow`, `MonadUnsafeIO`, `ExceptionT` and so on. Also, `resourceForkIO` is given by an instance of the `MonadFork` class from `layers`, rather than being its own operation. The `Resource` part of the package is a similar idea to Gabriel's [`Resource` monad](http://www.haskellforall.com/2013/06/the-resource-applicative.html) idea. The main difference is that it supports base monads other than `IO` (again using `layers`). There are two ways (well, three) to "run" a `Resource`: using the `with` operation (equivalent to Gabriel's `runResource`) or lifting it into a `SafeT` monad transformer using `acquire`. The third way is `forkWith`, which is kind of like `resourceForkIO` but for just a single resource. The other thing is that both parts of the library support two types of finalisers: `onFailure` and `onSuccess`. Sometimes you only want your finaliser to run if the computation that preceded it was successful, e.g.: foreignPtr :: ForeignPtr a -&gt; Resource IO (Ptr a) foreignPtr p = resource' (return (unsafeForeignPtrToPtr p)) (const $ mempty { onSuccess = touchForeignPtr p }) This makes a `Resource` called `foreignPtr` such that: with . foreignPtr =~ withForeignPtr Except you can also `acquire` it inside a `SafeT` if you like. 
Let me ask you a question: Would you fly in an airplane where the control software had been proven correct, but never tested?
Enough with the cheesy allegories. Are you saying that no one ever ran the pipes code but only compiled it? 
"Beware of bugs in the above code; I have only proved it correct, not tried it." - Donald Knuth
Nice! It does look a lot better. Keep it up!
Thanks, I'll check it out (at least as a user :)
At one point we had a type in lens where one of its arguments started taking parameters of the form `i m a s t a b u`. Upon reflection, we let it win the argument and decided not to implement the function.
I wonder what it would use to stab you with, though. Are prisms sharp enough?
There's lots of stuff in there. Unsafe Profunctors and Magma sound rather dangerous.
People keep trying to get me to arm lenses with pointed functors. They say everything will be affine and I should stop worrying. I don't trust them.
What is it about "make test" would make you feel a lot safer than thorough mathematical reasoning?
There are different ways to build up magma, too that aren't terribly safe. [Mafic](http://geology.campus.ad.csulb.edu/people/bperry/IgneousRocksTour/HawaiiEruptHelikerUSGS2003.jpg) and [Molten](http://upload.wikimedia.org/wikipedia/commons/9/9f/Pahoeoe_fountain_edit2.jpg). You can break things down into `unsafeParts`, and you should watch out for the arrows. I should stop now.
I remember a short discussion on Reddit about the Resource monad, and it seems like a good idea, so no objection on that. Back in the very early days, conduit had the concept of ResourceT baked into the core, and at that point it allowed for different base monads, IO and ST in particular. But that was a long time ago (I'm surprised anyone but me remembers it actually). These days, ResourceT is a completely orthogonal concept, and is completely specialized to IO. As you say, it was a bunch of ad-hoc type classes, so I was happy to be able to simplify down to straight IORefs. I can also see `onFailure` and `onSuccess` being useful in some cases. But in theory, I could picture three alternatives: * An exception was thrown. * We're exiting the ResourceT/SafeT block non-exceptionally. * The cleanup action is being called prematurely. Anyway, cool idea, I'd be interested in hearing any progress updates on it.
Could you build an AST for common piping idioms and then compile arbitrary instances into an Effect? Is there a basis of combinators that could form a finally tagless EDSL?
I don't see how this discussion is relevant.
Hah. Hence why we only added relevant traversals to lens. Affine traversals will have to wait. Nobody could convince me affine was relevant. ;)
This whole back and forth between Michael and Gabriel is like the highest-level, most scientific, most behaved and subtle drama I've ever seen on the internet. The Haskell community is amazing in many ways :-)
Unless the code that is going to be executed is the same code used in the proof then testing is needed to ensure that the code being executed behaves the same as it is "proven" to. Another area where this comes up a lot is distributed consensus protocols. Paxos and Raft et. al. have proofs that document all the assumptions and properties of the algorithm. However production systems employ enormous test suites to ensure that the algorithm implementation does indeed behave correctly. So...proofs are no replacement for tests in production systems.
It is turtles all the way down. If you have a machine-verified proof in Agda, you still need to trust that Agda is correct; if you have a machine-verified proof _of_ Agda, you need to trust _that_ verifier. This is more of a philosophical problem than a practical one, though.
I agree. There might be a little tension here, but it is absolutely undetectable against the vitriol and childishness of similar debates that occur all this time in the larger programming community.
The progress bar. I'm only being 90% facetious; the fact that the tests are running makes it _feel_ like it's harder to fake or to mess up miserably. That, combined with the fact that there are ways to automatically tell (not perfectly, of course) the quality of the tests that are quicker and less error-prone than any such methods for a mathematical proof mean that I feel like I need to put more trust a dev who gives me proof and no tests than in one who gives me tests and no proof.
Git-annex 
I use http://gitit.net/ and I am not a haskell programmer.
http://www.joachim-breitner.de/blog/archives/606-Real-World-Haskell-Applications.html
* [pugs](http://en.wikipedia.org/wiki/Pugs) * [bluespec](http://www.bluespec.com/)
[geordi](http://www.eelis.net/geordi/)
FB uses haskell: https://github.com/facebook/lex-pass
The main compilers of some other languages are written in Haskell, such as Agda and Idris :)
It would taken you 5 *seconds* tops with a quickcheck test.
Also the Elm one, which IMO is more likely to become relatively mainstream: Agda and Idris are a bit niche and "academic" in comparison.
https://github.com/davean/waldo/
Xmonad
The backend of http://chordify.net is written in Haskell.
Bump &lt;https://bu.mp&gt; uses Haskell on their back-end.
Hoodle, pen notetaking tool http://ianwookim.org/hoodle/ Backend of Detexify, a widely used mouse-doodle-to-LaTeX-code converter http://detexify.kirelabs.org/classify.html Backend of Codepad, a paste service that executes pasted snippets http://codepad.org/
Your textual description sounds right to me, but I'm not sure it agrees with the code you've attached. It seems to me that your Nat index counts the total number of black nodes in the whole tree (one for a leaf, the sum for a red node's subtrees, one more than the sum for a black node's subtrees). That's rather the "black-weight" of the tree. For the traditional black-height, just as you described it, try data RedBlackTree (blacks :: Nat) (c :: Color) a where Leaf :: RedBlackTree Z Black a RedNode :: a -&gt; RedBlackTree n Black a -&gt; RedBlackTree n Black a -&gt; RedBlackTree n Red a BlackNode :: a -&gt; RedBlackTree n c a -&gt; RedBlackTree n c' a -&gt; RedBlackTree (S n) Black a It's fun to enforce the ordering invariant, too.
Here are some of the Haskell-based apps we produce at [Suite Solutions](http://www.suite-sol.com/). * [SuiteHelp](http://www.suite-sol.com/pages/solutions/suitehelp.html) * [SuiteShare](http://www.suite-sol.com/pages/solutions/suite-social-kb.html) We also have a DITA-to-ePub publishing tool. All of the above are based on DITA Accelerator, our (currently proprietary) Haskell-based alternative to the [DITA Open Toolkit](http://dita-ot.github.io/). SuiteShare is a [yesod](http://www.yesodweb.com/) app. We also have quite a large Haskell code base of products for processing and maintaining large documentation sets for the aerospace industry based on several XML and SGML standards.
Also [Haxl](https://github.com/meiersi/HaskellerZ/blob/master/meetups/20130829-FPAfternoon_The_Haxl_Project_at_Facebook/The%20Haxl%20Project%20at%20Facebook.pdf?raw=true) (PDF)
The [Haskell in industry](http://www.haskell.org/haskellwiki/Haskell_in_industry) page that Joachim links to is actually starting to become quite impressive, and it's still missing some important players.
Facebook has been snapping up some of the top Haskell developers.
IIRC bazqux https://bazqux.com/ uses Haskell in backend.
 data TraceResult = HIT | UPMISS | DOWNMISS deriving Eq Hello ! .. Please can you comment this line a bit ? .. what do those three possibilities mean and why should they do different things ?
I should add McGraw Hill Financial to that list.
I teach tidal to non programmers http://yaxu.org/tidal
Leftovers and finalization both interfere with the monad laws for ListT.
Elm is totally different, not being dependently typed. Still in Haskell and still awesome though. 
I'm going to blame the tooling in your hypothetical, so I'll address it directly. 10 disparate pieces of IDE functionality all loading my complete project separately, and none of you thought to abstract that out so that each either * communicated with a currently running process containing your project * or started one up if none existed? This just seems irresponsible. When I said "be separate", I'm not saying ignore the existence of other tools, just that I prefer my tooling like unix rather than Eclipse. Work together if you can, by all means. Especially when the alternative is to thrash my machine. And what are you doing loading my entire codebase into memory anyway? I'm not editing all 100k lines at once, I'm editing five files. You can check file modification times, you know what's changed. Record the important information in a proper data structure, and use a persistence layer!
This works with the latest builds of `dirstream`, `pipes-safe` and `pipes-bytestring` on Github: {-# LANGUAGE OverloadedStrings #-} import Control.Monad import Data.DirStream import Pipes import Pipes.Safe import Pipes.ByteString import qualified Pipes.ByteString.Parse as P import Pipes.Parse import qualified Filesystem as F import qualified Filesystem.Path.CurrentOS as F import System.IO main = runSafeT $ runEffect $ (`evalStateT` src) (loop 0) where src = for (every (childFileOf "/usr/bin")) readFile' loop i = do eof &lt;- isEndOfBytes unless eof $ do let fp = F.decodeString ("out/" ++ show i) runSafeT $ runEffect $ hoist lift (input &gt;-&gt; P.take 50) &gt;-&gt; writeFile' fp loop (i + 1) -- `childOf` returns all children, including directories. This is just a quick filter to get only files childFileOf :: (MonadSafe m) =&gt; F.FilePath -&gt; ListT m F.FilePath childFileOf file = do path &lt;- childOf file isDir &lt;- liftIO $ isDirectory path guard (not isDir) return path -- Work around `FilePath` mismatch. See comments below readFile' :: (MonadSafe m) =&gt; F.FilePath -&gt; Producer ByteString m () readFile' file = bracket (liftIO $ F.openFile file ReadMode) (liftIO . hClose) fromHandle writeFile' :: (MonadSafe m) =&gt; F.FilePath -&gt; Consumer ByteString m r writeFile' file = bracket (liftIO $ F.openFile file WriteMode) (liftIO . hClose) toHandle The reason it requires the latest builds are that: * I haven't yet uploaded the latest `pipes-safe` behavior that lets you nest regions. I was just waiting for confirmation from Renzo that this latest update is in fact backwards compatible so it will be up within the next couple of days. * The latest `pipes-bytestring` has the `take` primitive, equivalent to your `isolate`. I will also upload this soon. I just usually buffer uploads with 30 days in between them to avoid excessive version churn unless it's a critical bug fix. `readFile` and `writeFile` don't use `withFile` from `pipes-safe`, only because `dirstream` is experimenting with using `system-filepath` for the `FilePath` type instead. I've been discussing this with the `haskell-pipes` mailing list and Dag Odenhall is actually prototyping a new filepath library that I will test drive using `dirstream`. This is the reason `dirstream` hasn't been released, yet, because I am really excited about Dag's solution.
* [hlint](http://community.haskell.org/~ndm/hlint/), as you already mentioned is quite valuable, the false positives drive me nuts, and it tries to eta-reduce code in ways that will never typecheck like an overly enthusiastic undergrad, but it is a good sanity check to at least look at everything it throws at me * [hpc](http://www.haskell.org/haskellwiki/Haskell_program_coverage) is a really good tool for checking whether your tests are actually testing the whole program * If it typechecks it must be correct, but if you don't believe that then [quickcheck](http://en.wikipedia.org/wiki/QuickCheck) should alleviate the few remaining concerns. It dovetails very nicely with `hpc`. * I use `criterion` all over the place just to sanity check that I'm doing the right things from a micro-optimization perspective.
well, newtype Showable = Showable (Int -&gt; ShowS) instance Show Showable where showsPrec d (Showable f) = f d at least ;)
The main use of the existential form is that it can be more efficient if you are doing something like mapping over the result. data Fold a b = forall x. Fold (x -&gt; b) (x -&gt; a -&gt; x) x can be more efficient than data Moore a b = Moore b (a -&gt; Moore a b) despite the fact that both are "observationally equivalent" in terms of what you can write with them. The reason is if you fmap over Moore you have to pay for plumbing the fmap at every step, even if you don't use it, whereas with the existential form you only pay when you "cash out" at the end.
This. If you want to deal heterogeneously with different image types you really want an "abstract data type" rather than an "algebraic data type". Describe the data in terms of the set of observations and operations you can apply to it at any given point in time. E.g. http://www.cs.ox.ac.uk/jeremy.gibbons/publications/adt.pdf Of course, this is just really a record-passing transformation taking a bit farther to abstract over the interface in such a way that you _can_ work with different implementations, albeit often inefficiently.
Yeah, but then you can't do cool things like dirstream where you bind directory contents. Plus, most conduits that you would like to convert do use these extra features. Also, it still feels a bit wrong, sort of like saying that pipes or conduit could be a replacement for pure lists. It doesn't feel right to have all that unnecessary complexity underneath what should be an elegant and fundamental type.
5 seconds, and I would be much less confident as to the correctness of the code.
Airplanes are cyber-physical systems outside the scope of formal reasoning. You can't verify the hardcomponents. Proofs though really do make a difference. There are important things that are hard to test (say a Martian lander) but need to work none the less. Unit tests are good, but formal verification that components obey the model is better. Airplanes are not tested in all conditions they will encounter--doing so is essentially impossible, but a formal proof could show that the computer will not decide to restart when the inputs stop making sense because you are lost in a storm in the middle of the Atlantic. Tekmo's proofs are by hand. They are not machine checked. Still, I have more confidence in this library because of them than I have in 99.9 percent of the code I encounter on a daily basis.
this is discussed else where--writing the quickcheck code is exceedingly non trivial
Haskell among other things. They were aquired Google so "used" not "uses" would be appropriate now.
This is a life changer. Can't operate without it now.
Let it never be said that you don't get anything done! ;)
I've discovered a proof which, regrettably, this margin is too small to contain.
[hedgewars](http://www.hedgewars.org) has server written in haskell https://github.com/hedgewars/hw/tree/master/gameServer
Thanks, I've added your solution to the blog post. But looking at that solution, I'm struck by two things: 1. How easy it is to get it wrong in pipes and thereby have a resource leak. 2. How non-composable resource handling is with pipes-safe. In conduit, `sourceFile a &gt;&gt; sourceFile b` just does the right thing. enumerator also provides for this via `concatEnums`. As for Dag's new library: what's the motivation to not use system-filepath? I haven't seen a single complaint against that library. That's not to say that there *aren't* complaints, but I'm really saddened to hear that someone was bothered enough to write a brand new package, thereby fragmenting the ecosystem, and hasn't even bothered to bring up his concerns publicly.
Hakyll, http://jaspervdj.be/hakyll
Is pugs still alive? Perl6 thingie.
I presume it's because Simon Marlow is there now.
[I did document this:](http://hackage.haskell.org/package/time-1.4.1/docs/Data-Time-Calendar.html) &gt; fromGregorian :: Integer -&gt; Int -&gt; Int -&gt; DaySource &gt; convert from proleptic Gregorian calendar. [Proleptic Gregorian calendar:](https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar) &gt; Mathematically, it is more convenient to include a year zero and represent earlier years as negative, for the specific purpose of facilitating the calculation of the number of years between a negative (BC) year and a positive (AD) year. This is the convention used in astronomical year numbering and in the international standard date system, ISO 8601. In these systems, the year 0 is a leap year.[2]
Since Erudify and Facebook have been mentioned: we at [Silk](http://www.silk.co) also use Haskell for all backend services.
Thank you michael. Personaly, I am more pipe-inclined, mainly for historical reasons, as it is the first variant of iteratees that really catched my interest. However, I really beleive that pipe and conduit would not be as good as they are today if you did not investigate like so. Healthy job :)
Google's Ganeti is mostly written in Haskell https://code.google.com/p/ganeti/
And Bryan O'Sullivan.
The linked post is just a typical "what's important" list that happens to be getting a lot of buzz on the tech blogs currently. But note that typing is on the list, and in the first paragraph below the list he says, "Naturally, you can argue that I am missing many important things. Maybe you feel that functional and object-oriented programming are essential..." (Functional before OO!) And no one seems to be reacting with outrage. Also, [this current parallel reddit thread](http://www.reddit.com/r/haskell/comments/1nxn9w/wellknown_haskell_apps/) is an impressive show of how far we have come. On the other hand, I still didn't dare post this on [/r/programming](http://www.reddit.com/r/programming/). Maybe I'm just a coward, or maybe we're still not quite there yet.
Is the bar at the very top (listing your desktops, some sensor data etc) also part of your XMonad configuration, or is XMonad just the window manager in some other desktop envionment?
snap, yesod and happstack host quite a few websites.
I think the comments are the usual mess of people who didn't think to read to the bottom of the post. There he is! That guy who says "I can't believe you didn't include OO!". *sigh*
Hello, Dag here. I think `system-filepath` is great because it handles arbitrary bytes in file names (unlike `filepath`) while still supporting non-posix systems (unlike `posix-paths`), and because it has a dedicated type for file paths rather than reusing some stringy type (unlike both `filepath` and `posix-paths`). It's rather slow though, because under the hood it's a record of lists of strings etc. I think `posix-paths` is great because it also handles arbitrary bytes, and is really fast because it's just a raw bytestring and it doesn't need any runtime checks to handle different path separators etc, but all this comes at great cost: posix-only. I think the lesser known `pathtype` is great because of the extra type safety it affords by tracking whether a path is absolute or relative and whether it points to a directory or a file with phantom types, but it doesn't appear to handle arbitrary bytes and also uses slow string lists, and I'm not sure how complete the Windows support is because it has no concept of drive letters. It also doesn't appear to be very actively maintained. In summary, what I would like to have is: * The cross-platform support of `system-filepath` * The performance of `posix-paths` — at least on POSIX systems * The type safety of `pathtype` I don't think the "missing" features could be added to `system-filepath` or `posix-paths` without breaking backwards-compatibility completely anyway. It could maybe be done with `pathtype`, but it's not very widely used anyway so not a lot of fragmentation avoided by working on that instead. Writing a new package from scratch also allowed me to improve upon the features of the other packages: * Correct-by-construction and even more type safe than `pathtype` without hiding the internals by using "typed type-level programming" extensions like `GADTs` and `DataKinds` and `TypeFamilies` * Theoretical foundation by building on category theory and graph theory * Cleaner API because more is tracked in the types, so we can unify or even remove a lot of utility functions that the other packages require * Better support for arbitrary bytes than even `system-filepath`, by allowing a mix of locale-encoded `Text` components and pass-through `ByteString` components (`system-filepath` seems to take an all-or-nothing approach to this, unless I'm missing something), and also handling this transparently across platforms Standard disclaimers apply; it's very young and incomplete, but here goes: https://github.com/dag/path
Pointless post. It has as much value as a fortune cookie.
I don't know whether Haskell is going away in bump after their Google acquisition, but being at Google does not necessarily mean that Haskell will not be used. After all, tibbe and mzero both work at Google. EDIT: Another data point: The QPX airline flight scheduling engine developed by ITA Software is still alive and well in Common LISP several years after being acquired by Google.
Wow, there's a lot more answers here than last time I remember reading one of these threads!
Software from Galois, such as, [SMACCMPilot](http://corp.galois.com/blog/2013/10/2/smaccmpilot-open-source-autopilot-software-for-uavs.html) and [Cryptol](http://corp.galois.com/cryptol/). See their website for more.
 Some other people had success with haskell projects in kickstarter: http://www.reddit.com/r/haskell/comments/u2vt5/my_haskell_project_was_100_funded_on_kickstarter/ But I think that in that case the goal was not haskell-specific. But I think you can get some investment from companies like FP-complete that are directly interested in the advancement of Haskell as a platform for industrial development. I hope so!. Anyway, if you try it, I would give some money. of course.
I go to the Haskell meetup at Mountain View. tibbe &amp; mzero don't work on Haskell at Google and AFAIK there is zero Haskell deployed at Google from Mountain View except for the recent bump acquisition.
Quicksort? Not sure I think much of this blog post.
Thank you, much appreciated :)
Both. If the posted article has no essence, why would the linking have any value? &gt; non-mainstream technology, thus using it introduces a large amount of risk into a commercial project Define mainstream. Because it's one thing to win the blogging popularity contest and another to be used en mass in the Real World™ &gt; it's important for us to gather evidence to the contrary Ok. What evidence does this post bring? Why try for counter evidence? Why evidence? I'm sorry but I see your argument and way of thinking naive. Posts won't do a god damn thing, people that build startups with Haskell, cool new libraries, applications and the ecosystem; are the one that will move it forward. Want to bring Haskell to more people? Build amazing things with it. 
Manuel - very nice presentation. Your picture of David Hilbert jumped out at me. So I'll repeat an interesting fact that I've mentioned here a few times before: Haskell Curry (whose combinator calculus work was parallel to and equivalent to Church's lambda calculus work) and Saunders Mac Lane (co-founder of category theory and first to use the word "monad" in that context) were both students of Hilbert. They were the only two American mathematics Ph.D. students at Göttingen during that period. That would have fit in perfectly.
The topic of this talk seems to be essentially an expansion of a point that Phil Wadler made in [this talk](http://www.youtube.com/watch?v=2PJ_DbKGFUA). My general attitude towards these sorts of arguments is "no".
How is this Haskell-related? (It's not even Lambda calculus related if you ask me. Sure, it's in there, but that's not what the comic is about.)
Boardword.com is written entirely in Haskell. The backend processes a few tens of thousands requests per day.
Building amazing things is definitely the way to go and the most important thing. And we're doing that. As that progresses, you need to be able to communicate the amazingness to people who aren't fully equipped to recognize it themselves. This kind of stuff does help. I admit I didn't think through this particular list very carefully to decide whether or not it's a good one. I'm not planning on using it to direct my thinking about programming or on telling anyone else to use it that way. I just noticed that it was getting a lot of attention on the tech blogs, and that this list and the alternative lists people are proposing in response to it are almost universally featuring concepts from functional programming. That would not have happened a few years ago. For me - and others who are trying to get the attention of the decision makers - that is a major shift. But yeah, if this particular list happens to be bad, then don't use it as your example. Use one of the others that are popping up in response.
Well you'll have problems with decision makers anyway, because risk aversion is part of their decision process. And it will take a long time until that risk diminishes in regards to using Haskell. But the problem first is not directly with them, but with the rest of the programming community. I won't shy to say that I downvote most of the Haskell related posts on /r/programming, not because I don't want it to succeed, exactly because of the opposite. The problem I see with the way the Haskell community shares information (a.k.a. crossposts) is that they share high level Haskell specific posts, that are hard for beginners to the language to understand, let alone to programmers in other languages, and we paint ourselves in the box "you need to be an academic/genius to use Haskell". That is the reason why others as FPComplete try to push articles with practical examples, because we have less of those than other languages. Anyway, I digress, this is a rant that will have it's place another time.
Well the joke is clearly that the cartoonist wanted some 'indecipherable' calculus and accidentally picked lambda calculus instead. Which some of us find extremely funny!
I didn't know Blondie was still being written.
For most of the population, it *is* indecipherable.
It's xfce4-panel. The desktops are shown with help of [EwmhDesktops](http://xmonad.org/xmonad-docs/xmonad-contrib/XMonad-Hooks-EwmhDesktops.html). It's just launched by the X session startup script; I don't use any kind of DE integration.
So I thought that one possible solution would be to have a separate type, for example, RedBlackTreeUnsafe, without enforce any invariants, which one would use "internally" and convert it to/from RedBlackTree. However, I didn't like that solution because it would require essentially duplicating the datatype. Another solution that you are talking about and the one I saw on the Internet is to use zippers. But then you have another almost-duplicate datatype, except that it's (IMHO) more complex. Is there a better way to enforce the invariants without introducing new datatypes?
Dunno, but pugs is not the "main compiler" of perl. Plus, someone else in the thread already mentioned it :)
Not myself, but it might be of interest to you if you haven't seen it already. From Google Switzerland - [Experience Report: Haskell as a Reagent. Results and Observations on the Use of Haskell in a Python Project](http://k1024.org/~iusty/papers/icfp10-haskell-reagent.pdf)
Yeah, so I understand the arguments that people are putting forward in these sorts of talks. The problem is that *we're* aliens too, at least to Klingons, or whatever, and *we're* not using functional programming. I mean, we are, of course, but not in the sense that the title really intends to get at. We use, mostly, imperative programming. So who knows what aliens will use. Of course, the point of the title, and the talk, isn't to really talk about aliens, it's to make some point about how functional programming is "superior" on these grounds or on those grounds or whatever. And if we take *that* point as the primary point then obviously there's no strict truth to it either, because it all depends on what your logic is. Sure, we might say "ah but look, functional programming comes from logic!", but well, it comes from *this* logic. But there are lots of logics, which all yield different computational interpretations, and not all of those are "functional". That's the core of my objection -- that the points people are making are really unjustified, and are instead just veiled attempts to explain why functional programmers are best programmers.
WARNING: My only haskell experience is from learn you a Haskell and could be totally wrong (please correct me if so). I believe what this does is makes those values comparable when you use "=". This has to do with typeclasses.
Right. But I think sclv's point is that it's the wrong calculus, not that lambda calculus isn't indecipherable to the average parent. It's like having an middle schooler come home with algebra homework, and the problem is "is Z4 × Z10 isomorphic to Z2 × Z20?" aside: is there any way to get subscripts?
Why python? Why not generally dynamic languages? I moved from clojure to haskell and see huge benefits in my everyday work. And clojure is a much better language than python. 
I don't understand these arguments because I don't see "beauty" as sufficient justification for calling lambda calculus universal. I like it, but there are many others. They're mostly all in bisimulation relations with one another. Lambda calculus only really has two introduction laws and one elimination law. That's nice. It also as commonly presented has a really bizarre name capture methodology that is ugly as all get out. Maybe bigraphs are a better fundamental calculus. I don't know what a language based off of bigraphs would look like—maybe aliens do. More specifically, I wonder what would happen if something like Pict got anywhere near the attention that Haskell and Lisp do.
Technically, lazy IO is not the term for what you are describing. The anti-pattern of loading an entire list into memory instead of streaming the list of elements doesn't have a specific name, but I usually call it a potential stack overflow, or sometimes "ListT done wrong" (since it's isomorphic to the `ListT` in `transformers`). Lazy IO refers to the opposite: a solution that *does* stream in constant space, but does so unsafely using `unsafeInterleaveIO`. Perhaps the term you might be looking for is "unnecessarily strict IO" because it is loading more elements into memory than it actually requires to do its job. However, your original point is still correct, even if you gave the problem the wrong name. `traverse` still has a potential to overflow if one directory has a very large number of immediate children. `dirstream` gets this correct by also streaming the immediate children of a directory.
Counterpoint for debate's sake: If we're talking about how we're "aliens" to others, we must consider ourselves on the cosmological time scale. We've only been programming for mere decades. If programming is still happening in 100,000 years, what will its paradigm be? You might be inclined to say we couldn't guess, and that's a reasonable position. However, I'd note that there's a pattern that we've already got enough data to see, and no real reason to believe will ever change: As we incrementally get smarter and more capable, our aspirations grow even faster. It has been observed, for instance, that while our software is subjectively as buggy as it was twenty years ago, that pretty much _has_ to imply a significant improvement in its overall quality because there is now so much _more_ of it. Thus, it may still be reasonable to speculate that the programmers of even the distant future may still be affected by the trends we see today; if they are a million times smarter and resourceful they may be a trillion times more ambitious. To my mind, the key point of practical functional programming, as we've sometimes discussed here on /r/haskell, is the use of ever-simpler abstractions to accomplish more stuff, more safely and predictably. I am willing to predict that that will continue to be a concern, indefinitely into the future, and that programming with simpler building blocks will be an increasing desire, even as intelligence and experience increase. Whether that is _guaranteed_ to be functional programming, I don't know. FP doesn't necessarily lay _unique_ claim to that; concatenative programming, for instance, has a certain amount of overlap in that area. But it is at least a reasonable bet, whereas I really can't say that I would expect to see a mature alien civilization based off of a huge imperative infrastructure. (Although... one can not discount the possibility entirely. One of my "fanon" explanations of the famous Independence Day virus scenario is that the aliens have a sort of hive mind, as evidenced by some other scenes, and as such, they had no security in their computers because they had no concept of a _hacker_. As such, Our Heros were able to write a virus in a matter of a couple of days because they essentially started with total root access, and from there, well, its very easy to be destructive. Perhaps they really did have a massive imperative infrastructure. Similarly, I've often imagined an alien civilization that never really develops math the way we did, and instead, over the course of millions of years, evolved a high-tech civilization instead of creating it. (Example: Motie engineers from The Mote in God's Eye and sequel(s)). Who knows what they would program in?)
No, not in standard reddit markdown. It's only possible if the specific CSS of the subreddit allows it.
I'll definitely have a look at the library, thanks for bringing it to my attention. Ideally, I think it would be great if we can somehow avoid fragmentation in the library ecosystem here, but at this point I have to admit ignorance of the problem domain of file paths.
Thanks cody .. I was more interested in the meaning of HIT vs UPMISS vs DOWNMISS rather than the syntax of the expression.
As a researcher (not a company) in experimental biophysics, I've moved a great deal of data analysis code from Python to Haskell. 1. Python disappointed me in many, many places, * the lack of types leaves one in a very unpleasant situation when trying to understand code written long ago, use interfaces written by others, debug new code, or otherwise reason about correctness * Python suffers badly when operating on large datasets. One must either shoehorn the problem into regular array operations (which can lead to some very awkward code, if it's even possible) or abandon Python and write fast kernels in C or Cython * Many tasks that seem like they should be simple (e.g. writing parsers) just feel wrong * Abstraction costs performance * It was getting tedious and dull; writing complex code was a chore. Understanding complex code even more so. The language simply wasn't helping me the way I thought it should. 2. Several reasons, * I wanted to learn something much different than what I knew. It was clear that Haskell would challenge me which was quite appealing. * The community (#haskell on Freenode, here on Reddit, the mailing lists) is fantastic * The library ecosystem is vibrant, most library authors spend the time to write good documentation * The community is wonderful * There is an excellent collection of introductory learning materials * The language and its libraries are actively evolving; there is always something new to learn * Did I mention the community? 3. I'm lucky in that many of my tasks are clear pipelines: I slurp input data from files on one side and transform it to produce output data which again gets dumped to file. This keeps the interactions between the Python and Haskell worlds straightforward and well-defined. Over time I've been slowly phasing out Python components. 4. At this point the only task left to Python is plotting and data exploration. There are a few reasons for this, * While [Chart](http://hackage.haskell.org/package/Chart) is a fantastic library, I find that it does impose more overhead that matplotlib. The syntax is more verbose and the fact that it's declarative means that I need to put a lot of effort in before I can see my final result. I've been meaning to explore ways to make data exploration with Chart more friendly. * `ghci` is not a good substitute for IPython, much less IPython notebook. Perhaps my biggest pet peave is the fact that all of my bindings disappear when I realize I need to import a new module. In the process of learning the language, I made a few mistakes. Like many fledgling Haskellers, I was caught up in trying to _understand_ the notion of a monad. It wasn't until I stopped reading and started writing code that the understanding I was seeking came. I also had some difficulty getting used to reasoning about laziness. This is a problem that anyone coming from an imperative language must face: we are simply not used to reasoning about evaluation and execution as distinct things. I find that most of my datatypes are strict in their fields. I often use strict containers like `vector` and the `.Strict` variants of the `containers` library. When I want laziness, I take care to document why.
This project looks really cool! Does AJHC do lazy evaluation? Does this make any difference for implementing an operating system? そういえばHaskellは日本で人気ですか？アメリカにはあまり使われていないですが、ヨーロッパの方が使うと聞きました。アメリカには最近ちょっと興味が出ています。日本はどうですか？
In defense of Snoyman, I think his claim might be that while `pipes` might be a more purist, mathematically beautiful approach, the opportunity cost of the risk of user error by non-experts is greater than the benefit of purity. This can be seen in your example as the difference between the two code snippets: yours makes several aspects of the code more explicit to the detriment of my ability to understand how you derived the solution. To wit, here are several things which I don't quite grok: * `` `evalStateT` src`` I'm not sure how this plays into the code. * `runSafeT $ runEffect $ hoist lift ...` The interactions between the types here (esp. hoist lift) are difficult for me to sight read. There's a lot of intellectual baggage or unpacking necessary to comprehend this. * The use of bracket and `fromHandle` - it's not immediately obvious how these producers and consumers interact, but I imagine it's through the `... $ hoist lift` machinery above. * The fact that you needed to produce your own custom `readFile'` and `writeFile'` functions - will users be asked to produce their own safe versions for resource acquisition and use in future libraries? If so, I think this does not bode well for `pipes` usage. Your solution might be more beautiful mathematically and perhaps more powerful - certainly it offers more control - but it demands a great deal more understanding and care from users. Other languages and libraries for them have given users power and control and allowed users to do the wrong thing, and they have led to millions of developers doing the wrong thing. I think your efforts might be best dedicated to truly trying to solve this very important issue of resource management in `pipes` or `conduit` because as long as handling finalization is more work (i.e.: learn how to uses `pipes-safe` *correctly*) many, perhaps most, will choose not to. `pipes` seems to disagree with the accumulated understanding of the benefits of Haskell and other languages - the harder it is to do the wrong thing, the safer it is for regular people to use it.
Good point. Have heard about Ruby as well. Would you mind answering my 4 questions above with respect to your Clojure-Haskell experience?
Moving from SciPy/PyCUDA/C++ to Repa/Accelerate in multiple scientific contexts, I concur with most of your points. Since I write a lot of numerical algorithms, Haskell powerful abstractions are the second most important factor underlying the move (after the strong type system). I differ with your point 4 in that I do not use matplotlib (I really don't like the feel of it), and have done most of my data plotting/exploration with R. As a side note, I am writing a ggplot2-like library for Haskell (currently it only handles a few geoms and scales). Also I have never really been fond of notebooks and IPython. I suspect those two difference come from my having never used Matlab/Mathematica/Maple. Developing a true ghci kernel for IPython should be trivial though.
I don't expect users to provide their own custom `readFile'` and `writeFile'` functions. The reason there are currently no such functions is that I originally had them in `pipes-bytestring`, but then the haskell-pipes mailing list recommended that I remove them from `pipes-bytestring` add wait to add them to `dirstream` and turn it into `pipes-filesystem` after deciding on which `FilePath` abstraction to use. So the holdup is the `FilePath` debate and not any intrinsic flaw of the `pipes` ecosystem. There's nothing special about `bracket` and `fromHandle`. Like I said before, once I decide on the correct `FilePath` to use, their definitions would simplify to: readFile' file = withFile file ReadMode fromHandle -- fromHandle is like hGetContents writeFile' file = withFile file WriteMode toHandle -- toHandle is like hPutContents ... with the same types. I think that is pretty self-explanatory. It's identical to the equivalent conduit code. To understand how `evalStateT` works, read the documentation for `pipes-parse`, which you can find [here](http://hackage.haskell.org/package/pipes-parse-2.0.0/docs/Pipes-Parse.html), specifically the "Low Level Parsers" and "High Level Parsers" section. Those are basically the "conduit-like" sections. As for `hoist lift`, that's there because of the nested region. I'm not going to pretend that is pretty, but it's better than type class magic in my opinion. Finally, I have spent a great deal of my efforts on solving the important issue of resource management. See `pipes-2.1.0`, where I actually solved this problem using `Frame`s, which you can find [here](http://hackage.haskell.org/package/pipes-2.1.0/docs/Control-Frame.html) but at the expense of requiring indexed monads. I even proved that `Frame`s still satisfy the category laws. I have even reasoned more about resource management **for conduits** than Michael has. This was the subject of my [conduit bugs](http://www.haskellforall.com/2012/05/conduit-bugs.html) post. The only reason I mention all of this is to make clear that I'm not avoiding the resource management problem because it is hard or inelegant. The reason I'm offering the `pipes-safe` solution is that I've already solved this problem and taken the correct solution to its logical conclusion and the correct solution is even more difficult to use than `pipes-safe`. Michael wants a simple and correct solution to finalization, but I'm reasonably sure there is no such solution, not even an inelegant `conduit`-based one. Also, while we are on the topic about whether we should allow the user to do the wrong thing, I want to mention that the conduit code actually does the wrong thing. If you supply it with a directory that has a large number of immediate children it will stack overflow, whereas the `pipes` solution I gave will run in constant space. In fact, you will find that the correct solution (that allows recursive traversals like `dirstream`) is not correctly implementable using `conduit`. Consider that the next time you think that I'm avoiding the resource management issue.
Going from clojure to haskell. - What's missing in clojure No Types. Makes refactoring a pain. Code always breaks after every refactoring no matter how much i test it. In haskell i already performed several major upgrades of yesod that affected large surface of my code. Yet it perfectly snapped together and worked without a hitch after first compile. Misses a lot of my mistakes. Forces me to stick to very simple data structures, generally lists (vectors) of maps (key-value pairs) Anything more complex and it quickly overwhelms my poor human brain. I need a powerful helper to deal with complex hierarchical data structures (trees) and haskell provides me excellent help dealing with those structures and not losing my mind. - Integration with existing code. Via web services (rest, json) and shared database backend. - What would make integration easier. This is not an easy question to answer. Haskell is very alien to any other language out there. The only easy enough FFI is to C. Anything else and you generally have to resort to exchanging data via tcpip, http, some messaging services (0MQ etc), or some shared storage (database, files etc) Which by the way is surprisingly simple, robust and works well. So i would be hard pressed to tell you what would improve integration because it is generally the problem with alien programming environments. For now I'm sticking to the server side (linux) with haskell. For obvious reasons. GUI is not the strong side of haskell ecosystem yet. So something like good bindings to Qt would help. Sticking to the server side also alleviates integration problems because haskell code does not have to directly interact with clojure or delphi or any other client GUI apps. 
I don't actually use it (or know about it, really), I just remember reading about it somewhere when there was the crowd-funding campaign! Sorry :)
&gt; Perhaps my biggest pet peave is the fact that all of my bindings disappear when I realize I need to import a new module. This is a dramatic problem when doing data analysis in Haskell. I have no idea why it's done—it may very well be necessary—but each delta that reduces the amount of unloading done would make data analysis in Haskell exponentially more palatable.
This discussion was brought to you by Haskell™, it guarantees that: It will terminate! Evaluated as needed! Every side effect is controlled by a the Tekmo+Snoyberg monadT! something something Don't use Lazy Discussions. 
Regarding integration, I think [Thrift](http://thrift.apache.org/) helps tremendously. Yeah it creates services, but it is a very easy integration step.
I don't think you're avoiding it, but compartmentalizing resource management to a separate library seems like punting on the issue. A vital part of IO in the real world is handling resources well in those exact cases you mention - and if what you say is true about `conduit` failing in some circumstances that shouldn't be overlooked by Snoyman. I have read your previous posts, I've followed your blog for some time, but it bothers me that it seems like critical issues that are very important to streaming functionality appear increasingly isolated from `pipes`. If `pipes` were to be correct by default when handling complex resource scenarios and to make it difficult for users to write the wrong code, I'd prefer that over any degree of mathematical certainty or beauty of the inner works of the library. That's why I urged you to work to solve this issue, because I think you're probably one of the few people in the community prepared to engage it, and that it should be solved by one of the two libraries. I think whichever, `pipes` or `conduit` that resolves this first will become and deserves to become the primary streaming and IO interface for Haskell users.
If what you are saying is true then Frames would have swept the Haskell ecosystem when it came out, but it didn't. Similarly, `io-streams` would have never been used according to your theory because it provides no resource management either, but I know that is not true either.
That doesn't follow from what I'm saying at all. As I've said, when users are given the choice of power and correctness, they often choose the former, and profoundly more-so when it's more easily reached than the latter. So you gave them Frames, and it was hard. So you scuttled the solution because it couldn't be made easy. And `io-streams` of course sees use, because it fills a need - but the lack of resource management causes bugs which created the need for libraries that followed.
Thanks! I am aware that there are more calculi that are worth mentioning in this context, but I didn't realise that Curry and Mac Lane were students of Hilbert. That is an interesting fact.
Pure LC with only -&gt; has one intro and one elim. Hypotheses aren't intros or elims, they're more fundamental and are unrelated to any connective. They're a core aspect of proof systems, independent of the logic in question.
I assume you did not actually watch the talk. I do explain why the choice of lambda calculus is not arbitrary.
Application *is* the elim. Like `fst` and `snd` are the elims of pairs.
Only in the same way that values can be seen as functions, dependent on zero parameters. Most people dislike that interpretation of values, and I dislike the interpretation of type synonyms too. But there's nothing fundamentally wrong with it, it's just not useful to think of them that way.
I'm not going to repeat a failed experiment. If you really believe in this strongly then you need to make a concrete proposal.
That's what I figured, but then how do you construct syntactically `App f x`? I suppose I'm crossing worlds. In my mind I'm thinking of data Exp = Var String | Lam String Exp | App Exp Exp and ignoring `Var` to give two intros. Then `eval` is eval (Var String) = Nothing eval l@Lam{} = Just l eval (App (Lam name exp) var) = eval (subst name var exp) where only the third line encodes a real "elim". But I freely admit that I'm not sure what it takes to formally encode this in sequent calculus or something like it.
we moved our spider at meanpath.com from python to haskell. Made it much easier to maintain many threads at once (gevent is a bit of a minefield), and the speed &amp; consistency from the typesystem was very helpful. parts are still in Python, as they worked well and didn't need to be replaced: the system communicates largely through ZeroMQ, which was also extremely helpful: a clean separation of responsibilities helps a lot with incremental upgrades. 
&gt; 1. How easy it is to get it wrong in pipes and thereby have a resource leak. I think this is mostly due to the nesting pipes-safe not being release yet or out yet for very much time. The below code is rough but seems to work with a brief testing. If we look at the snippet for chunking a large file to several files: main = runSafeT $ runEffect $ P.readFile "input.dat" &gt;-&gt; loop 0 where loop i = do let fp = "out-pipes/" ++ show i P.take 50 &gt;-&gt; P.writeFile fp loop $ i + 1 All that it takes to make it safe is to: main = runSafeT $ runEffect $ P.readFile "input.dat" &gt;-&gt; loop 0 where loop i = do let fp = "out-pipes/" ++ show i runSafeTn $ P.take 50 &gt;-&gt; writeFile' fp -- writeFile' from Tekmo example above loop $ i + 1 where: runSafeTn p = (flip evalStateT) (cat') (runSafeT $ runEffect $ hoist lift input &gt;-&gt; p) cat' = go where go = do x &lt;- lift await yield x go So it does not look like pipes mandates much increased complexity, just annotating nested IO sections with something like runSafeTn and a new library with the correct set of helper functions. Missing feature might be a type error is a runSafeTn is missing and/or combining the functionality of runSafeT and runSafeTn so there was only one function. No extra annotation, like ResourceT? Would something like runSafeTn with a type error if it is missing would remove most of your concerns found in "1."?
Gotcha
It is supremely arrogant to think that just because I disagree with your argumentation, I must not have actually watched the talk. This only convinces me even more that the sole purpose of your talk was to pat yourself on the back for having found The Truth. Congrats, you've succeeded in turning functional programming into a brand new cult. Well done. Don't bother replying if you're not willing to engage constructively, instead of this immature bullshit.
Wait, so there is legitimate work being done to enable overloaded record fields? How did I not hear about this until now?
&gt; I have read your previous posts, I've followed your blog for some time, but it bothers me that it seems like critical issues that are very important to streaming functionality appear increasingly isolated from pipes. Do you have a concrete example where separating out the concerns mandates(Not corrected with a few helper functions or library since if this is the case it will be made give a little time) a noticeable power(or ease of use) reduction?
With you on the ugliness of names in the lambda calculus. De Bruijn indices solve the alpha-renaming problem, but they kind of suck to use directly. Combinator calculi seem more elegant to me—SK calculus, or Brent Kirby’s [concatenative combinators](http://tunes.org/~iepos/joy.html). Abstraction elimination algorithms tend to suffer from size explosions, though.
Would have backed if you shipped to the UK. 
Trust me, I really want to but, as this is my first Kickstarter, I don't want to get in over my head with international shipping. After the book is published I want to get it on Amazon so they can handle the shipping. I'm very sorry I can't accommodate you and I hope you can understand that I don't want to over promise.
I'm aware of De Bruijn indices. I imagine there's a non-algebraic presentation of lambda calculus which has better substitution properties (Victor's alligators or the pictures like in Keenan's To Dissect a Mockingbird) and I always internally imagine it as "wiring" instead of variable substitution. In any case, I feel like these are a kind of hidden complexity to lambda calculus—an implicit mechanism that hides some ugliness. After all, you do end up with a lot of complex work going into implementing `substitute` functions or alpha equivalences. I'd love to see more theory on concatenative combinators. Joy is such an interesting language. (I think I actually owed you an email from a long time back about this actually...)
Why don't you like that interpretation? I've never felt one way or another about it, but I'd like to learn what reasons exist to disfavor it.
In a higher order functional language, typically your functions are values too, so your functions are themselves functions of zero arguments returning functions? It goes back as far as you like, and you get no closer to a useful interpretation of what the value actually is. No, the word "function" should be reserved for things of type a -&gt; b for some a,b
I'm echoing Snoyman's remarks here in pointing out that correctly implementing routine IO operations requires difficult to derive library machinery. If IO (and thus, resource management, the two are inseparable) can be bolted in as a separate library without the overhead of having to understand "hoist lift" and what-not.
I take a practical approach to describing Dataflow with more of a hands-on format. There are already some great papers/books that cover the theory behind Dataflow but not many that look at it from a developers point of view. My book "Dataflow and Reactive Programming Systems" would be the first book a developer should read to learn about the field. Then, if they want to learn more of the theory behind it, I would recommend the book you mentioned or any one of the great papers on the topic. From the feedback I have received, there seems to be a pent up demand for a book like mine. In learning something new I first like to "get my hands dirty" and work with it, then I investigate the detailed theory behind it. 
If programming is still happening in 100k years, then I will decline to speculate about what it'll look like. But if you really want to speculate, then fine, let's speculate. Your commentary about quality and bugs is relevant. So here's my guess: we won't be programming. We'll be writing specs in some extremely high level language (constraint logic? who knows) that looks nothing like functional programming, and the awesome compilers of the future will do the programming for us. Or if we're really lucky, quantum computing will make efficient search trivial, because you can try 10^500 solutions simultaneously, or something like that. Functional programming will be long dead, and all our programs will be 100% correct (assuming we correctly wrote the specs).
That is a fair sell.
I understand, real artists ship (no pun intended)! Best of luck with it and I will look out for you on Amazon. 
I do not think that the complexity is mandated and is only due to lack of helper functions currently, at least for the most part. I offer a solution that seems to correctly handle resources as Tekmo's examples do but simplifies the structure so that the only addition needed to change the unsafe version to a safe one is the addition of `runSafeTn`: http://www.reddit.com/r/haskell/comments/1nw7ji/pipes_resource_problems/ccntaum This is minimally more complicated then conduit code if that. The conduit code requires a check for `Nothing` and the pipes code requires something like runSafeTn. Does this solution satisfy this concern? Are there other concrete examples?
You're mixing object language and meta-language here. In Haskell, `App` is a constructor. That much is a given. That means it's an intro. Specifically, an intro for the prop/type `Exp`! But that's a coding of the LC in the metalanguage. When you give normal logical/type-theoretic presentations, the syntax and the semantics are distinct, and at the level of syntax (which the `Exp` type here is really simulating) there is no notion of intro or elim, there is just a notion of a symbol that combines with other kinds of symbols. The intro/elim concepts are related strictly to the role of the symbol/inference rule as a means for inferring things or for how it acts on types. You might want to watch Pfenning's OPLSS 2012 lectures. They're really good for clarifying what this stuff is all about, at least on the logic side of things. Also, as before, your `eval` is encoding the reduction schemas, not elims.
And yes, substitution is a pain, bit is does hinge on the choice of names. Use, e.g., de Bruijn indices and the situation changes a quite a bit. (But it is still essential the same calculus.)
Several books I've seen lately, some that are published by O'Rielly like http://chimera.labs.oreilly.com/books/1230000000929 and other books, notably Learn You a Haskell, offer one that can be read online later. I've bought LYAH, though I read most of it online. But I also find that a searchable reference is ALWAYS helpful, I actually went back to the last chapter on zippers today. Will you be offering similar?
Yeah, as I said below, Wadler's observations in this talk (which is more philosophically inclined than /u/chak's talk) are illusory. It's a nice talk, no doubt, but ultimately it's a serious misunderstanding. Which, for Wadler, is actually kind of surprising. Wadler is no stranger to non-IPL logics -- after all, he's done serious work on finding a computational interpretation of linear logic. So it's somewhat surprising that he would up and proclaim the lambda calculus to be a pan-universal constant. At best, Curry-Howard is pan-universal, because it generalizes beyond IPL and its variants, to essentially all logics with a computational interpretation (of which there are, afaict, infinitely many), but the LC? No. The LC is entirely dependent on the logic you pick, and if you stray too far from the IPL core, it becomes a lie to still call it the LC.
This is amazing. What would be really cool is if there were an alternative "portable" format (i.e. using pragmas) to write these specifications in so that you could still compile code without liquid haskell using plain `ghc`, in which it just ignores the refined data types. This would make it easier for libraries to transition to use of Liquid Haskell.
I thought that was how Liquidhaskell worked? Aren't its directives just haskell comments? 
Oh wow, that totally went over my head. I didn't see that the data declaration was bracketed inside a comment. Thanks for pointing that out.
That pun was intentional
Yes, it is unnecessary. I use a Haskell where the interactive system is much more like entering module definitions. Loading new modules (or changing loaded modules) keeps the top level bindings as long as they are still correct. I find this much easier to work with. 
Thanks! Yes indeed, they are comments, precisely for this reason. We've also started some work on libraries -- [`text`] (https://github.com/ucsd-progsys/liquidhaskell/blob/master/benchmarks/text-0.11.2.3/Data/Text.hs#L657) and [`bytestring`] (https://github.com/ucsd-progsys/liquidhaskell/blob/master/benchmarks/bytestring-0.9.2.1/Data/ByteString/Internal.hs#L358)
Statically ensuring that you can't accidentally leak resources like this would IMO be a necessity. If you could get something like that put together, it would certainly be an improvement. Though personally, I still think that having deterministic cleanup calls in pipes itself would be the better route (which my next blog post is going to be about).
1) I would love to talk to any and all about Dataflow or Haskell (just for fun :) The book will mostly come from my experience in designing Dataflow system but intend to send questions to the "big names" hoping they will offer their take on the subject. 2) In regards to lazy evaluation... Funny you ask because just today I was working a short, topical video about the book (about 5 mins) and I included some slides on push vs. pull and how it relates to eager and lazy evaluation. I love how Haskell allows me to create data types that would be impossible with traditional programming languages. 3) It's probability very obvious to everyone, I made the video myself. After the fact I realized I sort of glossed over some topics that should have been addressed in a little more detail but I ran out of time. Rest assured, I will cover modularity and compositionality in addition to parallelism. A great selling point of dataflow is that many implementations almost force you to create isolated components. I thank you for your questions... hope I've been able to answer them to your satisfaction. 
That was interesting. I would be quite interested in something similar being done with a web socket or so. 
I'll admit that what I'm describing is a more concerted effort, but it's still nowhere near an IDE. What's missing isn't a full umbrella IDE, but a UI-less core that these funclets can use and reuse. A full IDE could be built on top of that for those who'd like it, but it'd also allow those of us who wanted just the one feature to do that instead. I'd argue is that you need a thriving community of "useless toys" so that you can get a real idea of where the problem areas are (as you've highlighted) and build up some expertise and interest before you can build what's needed. And the people who are writing these IDE-funclets may not be interested or qualified to write the architecture they should be riding on, so it's hardly wasted effort.
Isn't that what i described when i gave an example of clojure nrepl? A IDE agnostic backend used by emacs, vim, Eclipse, Netbeans, IDEA? They do it right. We shall too. 
I laughed at it :)
`Data.Traversable` in `base` provides `foldMapDefault` for your `Foldable` instance and `fmapDefault` for your `fmap` instance. I think it's a sane way to do it.
I think that last claim is also somewhat difficult to justify. It's not as if people haven't given real, formal semantics and type theoretic interpretations to (fragments of) imperative languages. After all, monadic semantics does just that. If I were channelling Bob Harper right now, in a slightly perverse fashion, I would say that all of this business of making monads explicit in Haskell is unnecessary, given that we understand more or less how to give a proper denotational semantics for their effects. Probably no one has actually gone to the lengths of actually doing this for, say, C, but then, no one's done it for Haskell either. We usually just gesture towards omega CPOs and PERs and whatever, but when pressed, we admit, yeah, no, we don't *actually* have a semantics for Haskell. Probably the same is true for most of the languages you mentioned. If we're allowed to gesture for functional programming, I don't see why imperativists can't gesture similarly. And then the whole argument collapses.
Of course, for any `f :: a -&gt; b`, there's also `const f :: () -&gt; a -&gt; b`, for any `g :: () -&gt; a -&gt; b` there's `g () :: a -&gt; b`, and it's an isomorphism. And if we're doing all of this in homotype type theory, isomorphism is identity, so now we're really stuck at accepting the unacceptable!
Pipes does ensure that you don't accidentally leak resources. `SafeT` guarantees that all finalizers are called before the `SafeT` region exits.
 Error connecting to gateway: InternalIOException socket: resource exhausted (Too many open files) EDIT: Went away immediately on a subsequent load, but I was still amused. :)
And few of the popular things we all feel to be correct.
I'm not debating a strawman. I said that most of the examples I gave have workarounds, *outside the core abstraction*. I'm talking specifically about core abstractions here, and why I believe it makes sense to get rid of automatic termination. You can say that, given the other approaches you have available, you don't think the change is worthwhile, but please don't misconstrue my argument to something it isn't.
Strawman: Misrepresenting someones argument to make it easier to attack ^^Created ^^at ^^/r/RequestABot ^^If ^^you ^^dont ^^like ^^me, ^^simply ^^reply ^^leave ^^me ^^alone ^^fallacybot ^^, ^^youll ^^never ^^see ^^me ^^again
Leave me alone fallacybot.
And to answer one of your points concretely (#5): it's a bit disingenuous to say that pipes solves a problem correctly, and then introduce a completely different abstraction, but ignore the same approach which is available for conduit. Firstly, the pipes-parse approach and FreeT *could* be implemented for conduit if so desired. But more concretely, my post demonstrated a `line` function in conduit that *does* solve the problem correctly. My criticism of the conduit solution is the same as my criticism of the pipes solution: they require the introduction of a separate abstraction to get that behavior. Either separate abstractions are acceptable solutions to the problems for this discussion, or they're not. You can't keep introducing new techniques for pipes and ignoring the fact that conduit could do the same thing.
Sorry about that, a problem with my EC2 instance. Should be fixed now.
We can give a semantics to anything, but if you seriously want to claim that reasoning about Haskell programs is like reasoning about, say, C programs, then you cannot have done either (or you are arguing for arguing's sake).
It turns out that in this case to a compile time check can be put in my putting a type constraint on loop: loop :: (Num a, Show a, MonadIO m, MonadCatch m) =&gt; a -&gt; Consumer String m b Though it would be preferable to if the it was more closely tied to writeFile' or readFile'. The type signature was needed because the first runSafeT would clean up everything inside of it. So a type safe solution might be to isolate the runSafeT sections like: main = runEffect $ runSafeProducerT (readFile'' "input.dat") &gt;-&gt; loop 0 where loop i = do let fp = "out-pipes/" ++ show i runSafeConsumerT $ P.take 50 &gt;-&gt; writeFile'' fp loop $! i + 1 runSafeConsumerT p = runSafeT $ runEffect $ hoist lift catFromLifted &gt;-&gt; p runSafeProducerT p = runSafeT $ runEffect $ p &gt;-&gt; hoist lift catToLifted runSafePipeT p = runSafeT $ runEffect $ hoist lift catFromLifted &gt;-&gt; p &gt;-&gt; hoist lift catToLifted catFromLifted = go where go = do x &lt;- lift await yield x go catToLifted = go where go = do x &lt;- await lift $ yield x go More explicit and it clearly marks where clean up will be happening. &gt; Though personally, I still think that having deterministic cleanup calls in pipes itself would be the better route I think the above method is deterministic, I think it is type safe, the runSafe*T regions connected by `&gt;-&gt;` should be associative. The potential down side is that is explicit and I do not think it is bullet proof yet.
Let me clarify what I meant. I'm very concerned that it was so easy and natural to write the examples that I did, I was encouraged to write the examples the way that I did by the documentation, and there is nothing in the type system to prevent me from making those mistakes. Yes, if the *user* is careful and adds some type signatures, he/she can protect him/herself from these bugs. But that doesn't address the library issue itself at all.
I've explained this to you in private correspondence but I will repeat it here for the benefit of others: `pipes` is designed to be a low-level correct-by-construction abstraction that you can build other abstractions on top of. This is why I adopt a layered approach, first defining the core pipe type and then building other abstractions (i.e. conduit-like parsers) on top of that central abstraction. `pipes` is specifically designed to be light-weight so that it can be easily used within other abstractions, rather than being an all-encompassing framework for everything that rotates your tires and mows your lawn. Asking why the core pipe type alone does not solve parsing is like asking why linked lists don't solve parsing. You have to build things on top of lists to create parsers and similarly you have to build things on top of pipes to create parsers. Notice that the pipe analog of a parser is literally an exact translation of a recursive descent parser, except with the list of unparsed input replaced with a producer: Parser a m r = StateT (Producer a m ()) m r That's not an accident. It's by design. Pipes is light-weight enough that you can use it as a drop-in replacement for pure lists to upgrade them to effectful lists. That's not to say that the basic abstraction is impoverished: it's not. It's just that expecting it to solve every conceivable problem within one data type is ridiculous. Functional programming is about composing small solutions together into larger ones, not agglomerating functionality into one large god type.
Yes, I understand that philosophy. I'm pointing out here that there are direct, negative outcomes from following that philosophy here: you've lost easy composition, added accidental complexity for the user, and made it very easy to write completely broken code. And I'm pointing out that most of these problems seem to stem from automatic termination.
This is the height of hypocrisy. You are simultaneously criticizing me for using `sum`, which is not a pipe, then proposing your `line` function, which is not a conduit. Can you do this? Cl.map someThing =$ line No. Your library is also the epitome of accidental complexity. Your entire strategy is "correctness by obscurity". You just make the implementation sufficiently complex that nobody can reason about it except me and when I do reason about it and find bugs you just pile on more complexity until I finally leave you alone and then you attack me on correctness to distract people from what a mess you have built. It should not be my responsibility to prove your library wrong. It should be your responsibility to prove it correct.
Of course, but I'm still curious if the author did any profiling, even for fun.
&gt; Your library is also the epitome of accidental complexity. Your entire strategy is "correctness by obscurity". You just make the implementation sufficiently complex that nobody can reason about it except me and when I do reason about it and find bugs you just pile on more complexity until I finally leave you alone and then you attack me on correctness to distract people from what a mess you have built. It should not be my responsibility to prove your library wrong. It should be your responsibility to prove it correct. Man, you sound like a complete asshole here. I am enjoying the back-and-forth, though. Gives us lesser Haskellers something to aspire to.
&gt; Yes, if the user is careful and adds some type signatures, he/she can protect him/herself from these bugs. But that doesn't address the library issue itself at all. My second example does not require the type signatures to get the prompt resource finalization required by the example, but is not perfect either. It looks like the library situation should be better after the next release of pipes-safe. I do not think any one is going to dispute that conduit has the more extensive library system and has more available examples to look at. It has been in heavy use longer then pipes, since pipes has just started to stabilize with pipes-4.0.0(at least that is the stated plan).
but that's (one of) his point(s): that by applying another paradigm you lose composition and add complexity. doesn't he even acknowledge that his conduit solution is not better in that regard than the pipes one? i am curious whether the design to be proposed fulfills the expectations raised by the article.
Interesting - as someone new to the topic and having used pipes for a bunch of things I never knew you could/would try to use them in those ways! I'm starting to understand what Conduits actually does and why it is quite a different beast - it really does seem like a: Producer a (StateT Producer (SafeT IO) a) r There's a whole different style of interface being used - even though superficially they're very similar. Vertical composition is common in the conduit style, while almost never used in the pipes style. Conduit does everything in the one type, where as pipes uses nested types and plays very well as a monad transformer. While it can be simpler to have one type which does it all, having a simpler core and transforming it in various ways doesn't seem like a flaw to me (where as this article calls it out as a flaw that the base type must resort to being wrapped in various ways). 
&gt; This is the height of hypocrisy. You are simultaneously criticizing me for using sum, which is not a pipe, then proposing your line function, which is not a conduit. I think you missed my point here, let me clarify. To quote what I said above: &gt; Does it work? Yes. I just think it points out somewhere conduit could be doing better. Notice that I'm giving conduit the exact same treatment as pipes here: workarounds are not an acceptable defense for either library in this blog post. I'm not proposing `line` as a solution; I'm *criticizing* `line` as a necessary workaround, in the same treatment I'm giving `sum` in pipes. Look at the blog post again, it's quite clear that `line` falls into the "criticism of conduit" category, not a "conduit is wonderful" statement.
It's not any different, that's exactly my point. ibotty's sibling comment is completely accurate, I also make the same point [here](http://www.reddit.com/r/haskell/comments/1o1ink/the_core_flaw_of_pipes_and_conduit/cco24jq).
Their libraries are definitely something to aspire to but this argument of theirs has been going on for a while. While the competition benefits the users, the discourse could have been more friendly. I don't know who threw the first perceived punch but the fact that it appears too late to forgive and move on is not something I think the rest of us should aspire to. I'm sorry if that's seen as harsh but every other library/framework pair in the Haskell ecosystem (and other languages that I'm aware of) have been able to keep it friendly. Look at the [Angularjs and Emberjs guys](http://vimeo.com/68215606). It's not perfect but they're making a solid effort at friendly competition.
You said above: &gt; You're side stepping the issue. OK, let's get back to that point. You gave five examples above and said they were "virtually identical" to the conduit solutions. I'm claiming that they all lose the easy composition that conduit offers, with the `CL.map (+ 1) =$ sum` being the tersest example. Are you disagreeing with that claim?
This kind of behavior is not what I have come to expect of the Haskell community. Can this verbal war not take place privately, or publically, but without "strawman" code sniping and veiled insults(conscious or not)? Over the last few days the two of you(tekmo,snoyberg) have left the forums filled with an air of arrogance and intractability. Instead of code, why don't we talk about the "ideal" abstraction for these streaming libraries? Should Leftovers and Finalization be baked into a core abstraction? Tekmo says never, they are not inherent in the nature of a pipe, snoyberg says that adding them is necessary to give higher level desirable properties to users. Is there a middle ground? Can you build those features into/on top of pipes without giving up "elegance", composition, and correctness? These are open questions, though snoyberg alluded to a forthcoming suggestion in his post. Would the "bugs/unexpected behavior" discussed by either of you be something that new users might implement accidentally? How about more experienced users? How much of this "accidental complexity" is exposed in practice? 
Actually, Gabriel and I quite regularly have very friendly personal interactions, I don't think of us as being "enemies" or anything of the sort. Some comments come across more harsh than they're intended, but that's just the nature of internet discourse: without vocal intonation and body language to soften the blow, words can easily carry connotations they did not intend. I want to make it clear, I have the highest respect for Gabriel, and think that the discussions (and yes, arguments) we've had have greatly improved both libraries. (Side note: boy, did I reread this comment multiple times to make sure I didn't accidentally connote something.)
I think you've got a very good idea of some of the differences. Let me clarify one point: I would certainly say the inability to solve certain problems with the base type in pipes is a limitation. Whether that's intended behavior or not is a separate question, but I'm making the claim that the reason for this limitation is automatic termination.
Well that's understandable. Explicitly stating your intent before every comment would be like haskell without type inference. As long as you guys aren't burning bridges I don't mind (for what little my opinion could be worth). &gt; (Side note: boy, did I reread this comment multiple times to make sure I didn't accidentally connote something.) ha :)
&gt; Instead of code, why don't we talk about the "ideal" abstraction for these streaming libraries? That's exactly what I'm getting at. I simply don't believe it makes any sense to try and establish an ideal abstraction without understanding concrete problems with the current abstraction. I'm not trying to prove here that conduit is better than pipes. Instead, I'm trying to show that there are flaws at the core of both pipes *and* conduit, and based on that understanding we can hopefully discuss a better abstraction more intelligently. &gt; Would the "bugs/unexpected behavior" discussed by either of you be something that new users might implement accidentally? How about more experienced users? How much of this "accidental complexity" is exposed in practice? My [previous blog post](http://www.yesodweb.com/blog/2013/10/pipes-resource-problems) gives some concrete examples of naive ways of implementing real world problems based on the pipes-safe documentation, which leads to programs which leak file descriptors and ultimately crash. Gabriel's [example code](http://www.reddit.com/r/haskell/comments/1o1ink/the_core_flaw_of_pipes_and_conduit/cco1abv) above demonstrates some of the accidental complexity I'm getting at, in particular his second example.
I'm probably misunderstanding your code, apologies in advance. My concern is that it seems with the following code: &gt; loop i = do &gt; let fp = "out-pipes/" ++ show i &gt; runSafeConsumerT $ P.take 50 &gt;-&gt; writeFile'' fp &gt; loop $! i + 1 the user could easily leave off the `runSafeConsumerT` call entirely, and the program would still build and run, simply without prompt resource finalization. That's the situation I'm concerned about. Also, for the sake of this discussion, I don't want conduit's library ecosystem to be a consideration. I'm happy to pretend that any auxiliary functions you define are actually part of the pipes family of libraries somewhere. In other words, if it can *somehow* be implemented based on pipes, I consider it fair game.
that's amazing. am i missing something or shouldn't it be possible to generate ordinary haskell type declarations from the liquid declarations? of course you would (in some cases) lose compilation without liquid haskell. (of course a `make release` or similar could generate them so it's only a development dependency which i would be fine with.)
Beautiful! You do know that when I nodded my head and said "yeah it probably arises from the Yoneda lemma, everything follows from the Yoneda lemma" at dinner after ICFP I was joking, right? =) This really is a very satisfying article to read, because it makes the choice of the van Laarhoven representation seem much more obvious.
Adam worked on it over the summer with SPJ with me on the sidelines throwing wrenches in the works. Overall, I think it has converged on a design that we can grow into what we want it to be in the long term.
We should ask moderators to ban it.
I just messaged the moderators.
&gt; [...] yesod [...] perfectly snapped together i did not expect that :D well maybe if it gave happtic feedback in scottland. 
If you had a tabula rasa with which to work, would you change the way that LeftO and Fin are integrated into your core? Do you think that there is a correct by construction way to create to integrate such concepts into pipes? If so, do you think that the interface to both Pipes and Conduit would be tangibly improved? Assuming the more likely case, that Pipes remains what it is, do you have a method or alteration in mind that would alleviate your concern? Do you think that a correct and elegant "externalization" of LeftO and Fin are possible in Pipes-safe or Pipes-bytestring, or if Conduit were to rebase on top of pipes? I suppose that the answer to many of these questions are tied to the next post, but they seem to be the heart of the matter. 
They're excellent questions, and yes, the meat of the answers is coming in the next post. The *very* short and incredibly vague answer is that, by switching from automatic termination to manual termination in the core type, I think that: * Leftovers can be implemented in a natural way on top of the core. * Finalizers become completely unnecessary; they're just a hack to work around automatic termination. * For performance, it may make sense to smash all of this functionality into a single construction instead, though for reasoning we can start by looking at a layered solution. I've implemented these ideas in conduit, and I believe they work, but I'm hoping to engage with the greater community to *really* analyze this stuff in detail for correctness. As for pipes-safe and pipes-bytestring: I think that leftovers has been solved there, but it costs us easy composition. I'm not at all satisfied with the finalization solution of pipes-safe, as I think it's far too prone to creating resource leaks (or at least delayed finalization, which in some cases may as well be a complete leak). Those are basically the reasons I decided that conduit couldn't be built on top of pipes as it stands right now, and why I'm proposing this modified core concept.
&gt; the user could easily leave off the runSafeConsumerT call entirely, and the program would still build and run, simply without prompt resource finalization. This is not the case in my second example. There is no runSafeT wrapping everything to take care of any `MonadSafe`s that are dangling. If runSafeConsumerT is missing then it does not compile. The `runSafe*T`s create a temporary monad transform layer so that runSafeT can be applied but isolated from the rest of the pipeline. where loop is: loop i = do let fp = "out-pipes/" ++ show i P.take 50 &gt;-&gt; writeFile''' fp loop $ i + 1 the error produced is: No instance for (MonadSafe IO) arising from a use of `main' Possible fix: add an instance declaration for (MonadSafe IO) In the expression: main When checking the type of the function `main' Failed, modules loaded: none. That said it is not bullet proof for example: loop i = do let fp = "out-pipes/" ++ show i P.take 1 &gt;-&gt; runSafeConsumerT (P.take 50 &gt;-&gt; writeFile''' fp) loop $ i + 1 results in bad resource behavior again. It is not clear to me why however. I will be asking on haskell-pipes for insight and manually expanding the combination of runSafeconsumerT and &gt;&gt; it I can make sense of it later. 
I *think* I see what I was missing; does `writeFile'''` somehow embed in its type signature the fact that it must be inside a call to `runSafeConsumerT`?
&gt; yesod is just weird. I'm not particularly partial to Snap or Yesod—and I've used both in anger, but “weird”? Care to elaborate? &gt; "the better way" fpcomplete is trying to sell. To be technically correct, the author is Nikolay Murzin. Anyone can write blogs on the School of Haskell. Sometimes articles are [“pick of the week”](https://www.fpcomplete.com/school/pick-of-the-week), though.
 Prelude&gt; let x = 1 Prelude&gt; import Data.List Prelude Data.List&gt; :t x x :: Integer To be clear, I expect we're talking about using the `:load` command that drops existing bindings. I don't want newbies to think that you can't _import_ without losing your bindings in GHCi. 
Of course it's lying, its real name is "useless dictionary bot". I can't imagine what dunce thought it would be useful to have pages cluttered with comments about definitions---it's bad enough when us humans argue definitions and semantics. If there's a subreddit where that bot is *necessary* and *useful*, that subreddit has cancer and needs to be shut down.
Sorry I was not being explicit writefile''' is just Tekmo's writefile', that he posted with his example, with some print out information. writeFile' :: (MonadSafe m) =&gt; F.FilePath -&gt; Consumer ByteString m r writeFile' file = bracket (liftIO $ F.openFile file WriteMode) (liftIO . hClose) toHandle
Ahh, I think I get it now. Thanks for clarifying.
Please, guys, keep the posturing to a minimum.
日本では地域によりますがHaskellは一部の人で人気があります。東京ではよく草の根の勉強会が開かれています。 http://partake.in/events/47d20a8e-6e33-4871-ad7f-ddc9b4e2727f http://partake.in/events/9441ad6e-0bf8-4c6f-849c-ab2b4872985c ただ、企業での採用例はあまり多くありません。
Of cause Ajhc and jhc have laziness. I think it's not useful for OS. But in future someone will get new idea to implement OS using laziness.
Thanks! If that approach is good enough for `Data.Traversible`, it's good enough for me.
Interesting! But I think most of the `forall t.`s should be inside the brackets.
That's very possible. Even this post was longer than I would have liked, and I definitely didn't want to have my main message (the next post) be lost in the noise. Hopefully next post release will clear everything up.
A type synonym can have type arguments too, so this doesn't work. But you could say that a type synonym is a type family with one type instance that is parametric in all its type arguments.
You mean into three parts? :) I agree, it was a bit confusing.
Head explosion before 10am :-)
Slides available somewhere?
It depends on how you look at it. IMO, the auto-terminating behavior is surprising and unintuitive, and therefore getting rid of it simplifies the API. However, Tekmo would probably disagree with that. Additionally, there's no question that the *implementation* of this idea is more complicated than the implementation of pipes as it stands today. Yes, it's a bit vague. I'll post part three my tomorrow morning, which should make it more concrete.
Tekmo, all this is doing is make you look flustered, egotistical, and irrational. I was going to just pick out the offending portions of this comment to exemplify what I mean, but really, the whole comment is pretty trashy. It does not look well for you or the Haskell community. People who read stuff like this are going to second guess our so-far pretty decent reputation for having rational, intelligence discourse. I'm pretty sure that you have totally missed snoyberg's points; you seem to have interpreted all this as blanket criticism of pipes, when in fact it is not. Please take a step or two back, read more slowly, and write yet more slowly.
I'll post them once they're available. Talk only happened this morning ;-)
&gt;No Types. Because core.typed doesn't exist in your bizarro universe? :)
Good clarification.
Which system is this?
Yeah, I was definitely confusing reduction schemes with elims. I'll take a look at the OPLSS lectures as I know there's a deficiency here for me.
Interesting but without context or explanation not very informative.
Sorry, this perhaps wasn't directly aimed at your talk. I skimmed the slides and wanted to raise this question to the community here not necessarily because it was your core argument, but instead because it's an argument that is folklore popular. I personally have a great deal of fondness for lambda calculus and I've seen it bandied about as a canonical processes calculi in many sources (especially when I was studying the pi calculus which is happily derided for being somehow "less canonical" than lambda calculus) but I've never seen a formal justification for why it is so privileged. The best I could do, and I tried to elude to this in my original post, though I made a major mistake on the technicalities, is to suggest that maybe it's somehow the "simplest" process calculus—the fewest number of introductions/eliminations, the fewest number of concepts needed to describe the reduction step. It might very well achieve this, but I'd love to see a proof. I'd also love to understand more why any notion of simplicity is important in this domain. And finally I think that—regardless of encoding—the symbolic "substitution" mechanisms are really complex. Not necessarily more complex than any other process calculus, but complex nonetheless. This became really clear to me reading Milner's "Space and Motion of Communicating Agents" where he describes his bigraph process calculus and spends a long time describing the "wiring" and reduction bits. Bigraphs are certainly more complex than lambdas, but I still feel that complexity is swept under the rug in all popular notions of substitution. And that all said, I don't even know that simplicity is the best goal, simply perhaps a measurable one. Cellular Automata are "pretty simple" too, but nobody is going to go around claiming that Rule 110 is the best foundation for reasonable thought about computers. I'm also influenced by John Baez's Rosetta Stone paper where he connected some physics concepts, some category theory concepts, some topology concepts, and some CS concepts all together. In this paper, he outlined his hopes for the existence of a general theory of processes founded upon braided monoidal categories. In this viewpoint, lambda calculus is frighteningly rich and thus an outlier compared to, say, the linear operator calculus of QM (which I know nothing about). Anyway, I don't mean this to be a criticism of your talk and I'm sorry if it came off that way. I'm simply genuinely curious if there's any useful measure at all on processes calculi. I wonder why lambda calculus gets the attention it does and if you stripped away the (human) history and precedent whether it would again.
I responded to this partially in the other comment. I know about de Bruijn indicies. I've implemented lambda calculus with all kinds of substitutions. I don't personally feel like it changes the situation very much and I tend to envision the things less algebraically and more like how it's done in To Dissect a Mockingbird or in Milner's Bigraphs when I'm talking about the complexity hidden in substitution. That said, I'm not going to deny that there's greater conceptual frugality in substitution when you use de Bruijn indices.
Never mind. As snoyberg pointed out, the confrontational style of their conversations sounds vastly amplified and exaggerated in the medium of a reddit thread. But Tekmo and snoyberg understand each other perfectly. The ultimate goal of both of them is to come up with the best possible IO library (or libraries) for us to use. The fact is, historically these "blow-ups" have always led to great progress in their aftermath. (Even though I too would prefer a tamer style if it were up to me.)
I personally find that focus on the core abstraction isn't desirable to me. If the core abstraction isn't very regular and airtight then those leaks can be very painful in the future. I would rather have explicit complexity interwoven into the system so that I can later decompose it if it breaks than to rely on a more sophisticated core abstraction which solves all of the problems at the cost of behaving less regularly. This means that concerns Gabriel keeps raising about the loss of laws or possible strange edge cases within just the core abstraction are really important to me. I don't claim to be extremely competent in both libraries and that certainly impacts my understanding of the situation, but I don't feel that focus on comprehensiveness of core abstractions is an important point of debate.
Result! Top work, thanks!
I think you should've made the forthcoming part 3 of this part 1, and then posted part 1 and 2 explaining the flaws in both of the prior systems. It makes it seem much less disagreeable I think. 
This is essentially my position as well. I feel like great amounts of this debate come out of trying to equate the two notions despite them being quite different. I also think the code examples tend to confuse the issue since they demand comparison which may not be really useful. I don't necessarily want a convergence of the libraries, but as they diverge I would like them to clarify the pros and cons of each approach. If there is a true middle ground which achieves both designs effortlessly then I'd like to see some definition of what those properties are and some proof of an existence of a model.
If the layered approach provided complete feature parity with the baked-in approach, I'd agree. My problem here is that the layered approach *does* have direct consequences on the user-facing API: * You need two sets of functions, one for "pipes" and one for "splitters." * A lot of complexity is added in order to do proper resource handling. And at least as pipes-safe stands today, it makes it incredibly easy to write bad code. * To my untrained eye, the equivalent pipes code for any of these more complicated cases ends up becoming much more difficult to write and understand than the corresponding conduit code, though I'll readily admit I'm not a good person to make such an assessment.
It must be ekmett's `machines`!
Is it intended to show the links even when they're a dead link? Because the top 2, for example, are dead links. Well, it isn't a critic, just an observation: depends on the purpose of the site. :)
Best talk so far... 
Aye, John Wiegley mentioned he was garnering a lot of Haskell-related papers and said it would be nice if IRCBrowse could display everything ever linked. So I whipped up this page. It can either be casually browsed through, or someone can copy it into a file and download all the PDFs in one go with a script. I suppose the YAHT links are historically interesting. If links are dead, you can still look at the conversation context to maybe search for the PDF on google.
It's certainly possible, I debated the order of posting these quite a bit, and ultimately posted them in the reverse order to which I wrote them. I have no problem admitting I may have made a mistake here.
What was the title of the "YAHT" papers? What does YAHT stand for?
Yeah, I do sound like an asshole. Sorry. :( I got really upset because he keeps repeating his own concerns about `pipes`, but has not yet acknowledged any of own concerns about `conduit`. What triggered my outburst was his use of loaded and vague words, where he described `pipes` tradeoffs as "buggy" and "inelegant", whereas `conduit` tradeoffs were "not buggy" and "elegant". This is why I prefer to stick to equational reasoning when discussing merits of libraries. When you remove equational reasoning from the picture the discussion becomes a matter of taste and opinion, and that does not end well, as I unfortunately proved. The problem is that `conduit` is very difficult to reason about equationally and this is why I keep complaining about the growing complexity of `conduit`. If Michael does not fix this growing complexity then every discussion about `conduit` will continue to attract uninformed debate (myself included) grounded in passion instead of reason. If you want to see the difference, compare the discussion in the comment threads for my `pipes` submissions vs his `conduit` submissions. You will see a remarkably improved tone and very little controversy for `pipes` submissions. Michael and I do always make amends after a while, but right now it still stings.
To be fair, you did say: &gt; as I mention below, I provided a fully working solution in the blog post. :) However, I do get that you're not saying that `line` is better. However, what I'm trying to say is not everything needs to be a conduit. You can have functions of conduits, FreeTs of conduits, pipes that yield conduits. There's a whole world of powerful abstractions that you can explore if you don't try to fit everything within the single conduit framework. 
After several years I still find it hard to recall/understand the difference between :load, :add, :module and import in GHCI. Could things be simpler ?
Excellent talk. Next I should watch Kmett's talk.
How can SPJ explain so well material he just learned.
Oh, perfect! Thank you for the explanation! In fact, I googled the addresses and found the correct link for the PDFs I wanted. And congratulations for the functionality: in fact, very, very interesting! :)
I agree that I went too far. I only want to repeat one thing that I mentioned in my other apology: Michael unintentionally attracts impassioned debates by writing libraries that are difficult to reason about. So I will improve my tone, but I also want Michael to make an effort to write simpler libraries that people can reason about so that these discussions do no devolve into matters of opinion. That was what was really frustrating me, which is that I can't make any rigorous statements about conduit without a huge investment of my own time to study the exact details of how conduit's internal implementation works. Meanwhile, Michael can easily reason about `pipes`, so it's a very asymmetric debate position to be in. That was what I was trying to say, but I did so in a really ugly way.
So what I can say about this is that I do believe that the parsing/leftovers/connect-and-resume functionality of `conduit` can be implemented entirely on top of `pipes` with a suitable newtype. That includes things like conduit-style composition. I will try to write this up into a post to explain what I mean and how the translation process would work for parsing features alone. The finalization approach of `conduit` *might* be implementable on top of `pipes`, but I don't know for sure yet.
I added web socket version too. Please check the link again.
No worries!
Maybe you could disregard conduit and focus on Michael's criticism of pipes without comparing it to conduit. He seems to have worded his blog post carefully to express his opinion of what should be improved. For example: code that looks good on the surface but still fails. Those are valid points, and dismissing them by counter attacks helps no one.
Yesod is very similar to Rails. So looking at it as web framework, apart from the language in use, it is not much weirder than that, I think.
Very cool, now I finally have an intuition for the van Laarhoven representation.
One of the greatest joys as a creator I personally have found is discovering the wacky ways your users will use the flexible features you design. I loved watching this talk because of the joy and surprise SPJ had at learning the particular wackiness of lens... despite having writing the compiler that enabled it. What a great talk.
Here's an uneducated question about pipes from someone who's only read the pipes library but not used it, and never read or used conduit: Would it make sense to remove termination from pipes entirely, and deal with all termination through the monad itself? Would that help address some of Michael's objections?
Don't we all ;)
I really liked how he started from the obvious "record of getters and setters" lens and moved to the more generalized abstractions. Now I wonder if there could be a way of Haskell helping library writers better hide all this abstraction from begginers - my impression is that its super common for there to be a tradeoff between simple types for begginers vs complex types for advanced users and implementors (monomorphic functions in the prelude, happstack.server vs happstack.lite, all the different versions of pipes and conduits, ...)
Yeah :)
One simple thing that would help as a partial solution is if GHC *didn't* expand types defined via `type` up front. The definition of `Lens` is something like `forall g . Functor g =&gt; (a -&gt; g a) -&gt; g b`, which is intimidating to the uninitiated. Moreover, from a more spiritual point of view, it is an *encoding* of a `Lens`. (And just one of many different encodings). The fact you are confronted with this expanded definition whenever you make an inquiry breaks one of the fundamental lessons we have learned from category theory and from type theory: "You are what you do." SPJ touches on this point a bit in his talk. Wrapping the definition inside of a `newtype` (rather than a `type`) would solve this leaky-gut syndrome. But it would force us to do something unpalatable, like hiding `(.)` and `id` from the `Prelude` and replacing it with `Category`. However, it would be nice, perhaps as an extension to GHC, to simply prevent the type expansions from being displayed to the user. It can still be definitionally equivalent to it, but the user need not worry too much. (The only consequence would be `(.)` and `id` would work somewhat mysteriously).
The issue is Michael's fundamental methodology is radically different from Gabriel's. Gabriel would rather grow a minimal principled, reasoned approach -- even if it doesn't cover all the use cases once could conceive of (yet). Whereas I think Michael would be happy with code that seems to correspond to informal expectations of how things should work, that covers the use cases that he actually needs in real code adequately, and passes tests. To be honest, I started using Haskell because I really hate designing software the way Michael does. 
I would argue that if you don't have category laws, you don't really have composition. So claiming that "conduit offers easy composition" is just false, seeing as you don't have composition at all. You have some "shove together" operations that are informally specified and don't always behave compositionally. Most of the time you can shoe-horn the various mathematical concepts of composition into a category of some form. I don't think you can make conduits a category no matter how much you stretch them. 
For those that didn't make it this year, and want to hear the talks, join the conversation, and meet the experts - you can book now at only £75! The eXchange will take place on October 8th! http://skillsmatter.com/event-details/home/haskell-exchange-2014/te-8516
&gt; if GHC didn't expand types defined via type up front As far as I'm aware, it doesn't. It only expands type synonyms (and type functions) if it has an equality constraint: T ~ U And one of T or U is a synonym/type function application. In other words, it only expands type synonyms when it _needs to_ in order to solve equality constraints (and probably other constraints too, but I never looked into it in that depth). What would be nice is if it kept those expansions around, and un-expanded them when possible when presenting type errors, but that's actually quite a bit more work on top of what GHC already does.
This is so helpful. Was generalizing "Lens a b" to "Lens a b c d" Edward's "third insight"?
Why did he have to run out of time? They should've just let him run over!
Huh... perhaps that behavior has changed since last time I looked? I swear it used to expand `type` definitions! 
Oh, how I missed that Comic Sans.
Reading about `machines` is actually what led me to `pipes`, and I'm excited to see where Edward takes `machines`.
&gt; ... it's quite difficult to avoid that problem. Definitely. It's a human-computer interaction problem, not a technical problem with the language. And those are the hardest to solve!
I don't know, but you can definitely get some explanations on how to explain things from SPJ - or at least how to give good research presentations and write good research papers. [Web Page here](http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm) includes some videos, though you IIRC there are better videos available if you hunt around. 
To quote John Carmack, "Everything that is syntactically legal that the compiler will accept will eventually end up in your codebase." So the user facing API does matter. The "safe" API, with batteries included should probably be the default API, with the internals exposed so people who want to play with fire can. So our trading platform is essentially a giant conduit. Previously it was an enumerator. We have hundreds (maybe thousands) of lines of conduit code that is quite complex so we have watched this discussion over the last year with interest. Conduit 0.5 was feature complete for us. The ability to return a separate upstream value turned out to be incredibly useful and simplified a number of cases. The contribution of the Pipe type turned out to be incredibly useful and simplifying for us. Pipes the package has evolved (and I think continues to) rapidly enough to discourage its use. I am glad to see this design space explored. I don't think that the contentious nature of these threads is damaging to the haskell community. We are operating on the edge of the design space, exploring some amount of unknowns, not building a Java Abstract Facade Factory thingy. 
3 of the first 5 links are dead...
What would break if Prelude exported id and . from category? 
More like Simon Peyton "oh drat" Jones :-)
I think Gabriel intends this latest `pipes` release to stabilize the API for downstream libraries, so hopefully that will increase adoption and experimentation.
Yup, this data goes back to 2001. I don't plan on editorializing it!
I'm still stunned that the videos are already out there for us to watch. I even was able to watch SPJ's talk before my courses this afternoon. I think that makes a 2 hour delay after the talk? You should write down somewhere how you guys do it, so that the other meetups/conferences can do the same. Cheers!
You can climb the abstraction ladder as high as you want. But the problem that we were addressing in this thread was hiding some of that complexity from the programmer. If you allow Category to be the default, as a new user, you suddenly have no idea what the heck `id` and `(.)` *do* by looking at their types.
And there is no such thing as a two-parameter function either, right? We curry instead. If f3 :: t3 -&gt; t2 -&gt; t1 -&gt; t0 is the "curried form of a 3-parameter function", and f2 :: t2 -&gt; t1 -&gt; t0 is the "curried form of a 2-parameter function", then f1 :: t1 -&gt; t0 is the "curried form of a 1-parameter function", and f0 :: t0 is the "curried form of a 0-parameter function". (In any of these cases, if `t0` is allowed to be of the form `u -&gt; v` then we have a "curried form of a function of at least n parameters", otherwise a "curried form of a function of exactly n parameters". Context usually determines which one we mean.) In actuality people don't usually say "curried form of a 3-parameter function", except for didactic purposes. They say "3-parameter function". So, it should be equally acceptable to call `f0 :: t0` a "0-parameter function". Now, it's quite useful to have a word for "curried form of a function of *at least one* parameter", i.e., something whose type is of the form `a -&gt; b`. But the notion of "curried form of an n-parameter function" is also useful when n = 0. Sometimes among Haskellers I feel like I've been transported back to Ancient Rome, where nobody believes me when I tell them 0 is a number! Consider our good friend Applicative. class Functor f =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b This is a good "basis" of methods for programming purposes but it obscures the structure: `pure` and `(&lt;*&gt;)` are (equivalent to) two instances of the same thing. Recall liftA2 :: Applicative f =&gt; (t2 -&gt; t1 -&gt; t0) -&gt; f t2 -&gt; f t1 -&gt; f t0 liftA3 :: Applicative f =&gt; (t3 -&gt; t2 -&gt; t1 -&gt; t0) -&gt; f t3 -&gt; f t2 -&gt; f t1 -&gt; f t0 ... `liftA`n applies an n-parameter function to n values wrapped in `f`, and produces a result wrapped in `f`. In the presence of `Functor`, `liftA2` is equivalent to `(&lt;*&gt;)`: liftA2 f a b = f &lt;$&gt; a &lt;*&gt; b f &lt;*&gt; a = liftA2 ($) f a But "`liftA`n applies an n-parameter function to ..." also makes perfect sense for n = 1 or n = 0, so we can extend the `liftA`n downwards: liftA1 :: Applicative f =&gt; (t1 -&gt; t0) -&gt; f t1 -&gt; f t0 liftA0 :: Applicative f =&gt; t0 -&gt; f t0 `liftA1` is just `fmap` of course, and `liftA0 = pure`. So we can equivalently define Applicative as class Functor f =&gt; Applicative f where liftA0 :: t0 -&gt; f t0 liftA2 :: (t2 -&gt; t1 -&gt; t0) -&gt; f t2 -&gt; f t1 -&gt; f t0 You may like to compare class Monoid m where mempty :: m mappend :: m -&gt; m -&gt; m class Monad m where return :: a -&gt; m a join :: m (m a) -&gt; m a In each case we've picked out the cases n = 0 and n = 2 of a general operation that operates on some structure that depends on n. For Monoid, we have n-fold multiplication; for Monad, collapsing n nested copies of `m` into one; for Applicative, lifting across a function of n parameters. It's only by taking the notion of "0-parameter function" seriously that these parallels become visible.
Speaking for myself here, I greatly prefer not to overload `(.)` everywhere. When reading unfamiliar code I would hate to wonder if `foo . bar` was the composition of two functions, or a composition in another category. Having `import Prelude hiding ( (.), id )` at the very top warns me to be on the lookout. I think it's generally a good idea to make use of explicit operators such as `(&lt;=&lt;)`. As long as you know that it forms a category with `return` you can use the category laws to reason about your code. But imagine if we used `(.)` and `id` for Kleisli composition everywhere; you could hardly use it without making your code unreadable.
I'm not really understanding what you want to say, however, compare your first sentence: &gt; And there is no such thing as a two-parameter function either, right? We curry instead. with my first sentence &gt; [...] in Haskell each function has **exactly** one argument [...] (emphasis added). So, you can curry a 2-argument function. You cannot curry a "0-argument function". Note that what I'm saying is about Haskell, not about mathematics (of course mathematically 1 -&gt; A is isomorphic to A).
`fclabels` tries to do exactly this. Not only for beginners, opaque types can have benefits for everyone. Like simon explains there are good reasons for `lens` to expose the internals, but it has some downsides as well.
Hmm, okay then.
Couldn't that be said for all type classes? If you see a `&lt;&gt;`, which Monoid does it refer to? You need to look at the context.
~~I think that part was in Twan's original paper.~~ I'm wrong, please downvote.
&gt; I'm not really understanding what you want to say, however, compare your first sentence: &gt;&gt; And there is no such thing as a two-parameter function either, right? We curry instead. &gt; with my first sentence &gt;&gt; [...] in Haskell each function has **exactly** one argument [...] &gt; (emphasis added). Yes, I am agreeing with you. From a strict point of view, Haskell has neither 0-parameter functions nor 2-parameter functions. &gt; So, you can curry a 2-argument function. You cannot curry a "0-argument function". &gt; Note that what I'm saying is about Haskell, not about mathematics (of course mathematically 1 -&gt; A is isomorphic to A). How can it be about Haskell? You just said yourself that "a 2-argument function" is not a thing that exists in Haskell. Mathematically, there is no reason not to talk about currying n-argument functions for any n &gt;= 0, as I described.
I think this is still relevant to the conversation, because we are discussing whether or not all abstractions should be modeled within the unextended pipe/conduit type. My point is that they are not.
I want to reiterate one of the questions raised here about the potential downside of automatic "downgrading" of `do` notation to applicative notation. I'm quite concerned about this having seen some problems arise when observable differences between the two interfaces crop up and are, in this case, silently interchanged. In particular, it's my knowledge that the uu-parsinglib uses constrains its lookahead based on whether a particular parser fragment is applicative or monadic, despite having them maintain the same type for easier composability. I'm not at all sure what reasonable tradeoffs here look like, but I think even post AMP care taken around whether you're using the `Applicative` or `Monad` interfaces will be important.
What is an annotation? I searched that page but I couldn't find a link to a definition or an explanation in the text.
Yes, it can. Given the choice, I prefer to use the most specific operator available. If I know I am concatenating lists, I write `a ++ b`, not `a &lt;&gt; b`. This gives me better error messages when I do something wrong and helps me read the code later. If I'm mapping over a list, I use `map` not `fmap`; I'd be sad to see `map :: (a -&gt; b) -&gt; [a] -&gt; [b]` go away. And so on. There's a cost to having too many different names, though, and at some point the names would get unwieldy. I don't want to combine `Ordering`s with `mappendOrdering`, I'll just use `(&lt;&gt;)` and do the instance selection in my head, thank you very much. Personally I think that (1) we have about the right number of specialized functions in the Prelude, but more importantly (2) any change to replace a common Prelude function with a generalized version costs more due to inertia/network effects/human factors than it is worth, even if it might be a good idea were we starting from a clean slate.
He [tweeted](https://twitter.com/kmett/status/387957181229719552) the following about it: &gt; I'll just sit back and let Simon Peyton Jones shill for my #Haskell lens library from now on! Great sales pitch! =) 
One day IDEs will show us which instances are being used (perhaps as a subscript) and this won't be a problem for anyone anymore.
I could easily see this having conflicts with the same flag being declared in multiple places. Libraries for example could cause conflicts, as I see anyway. What is there to protect, at the global scope, for all this?
interesting idea. we're actively exploring ways to avoid the "duplication", will think more about this, thanks!
[hledger](http://hledger.org)
I thought it was obvious that it came from yoneda, since yoneda lets you compose any category with the dot /id from the prelude. Since lens is a category that composed with the operators from the prelude, I assumed it came from yoneda, and even used this example as a way that category theory was usefull. This though was only a vague sense that it must be that way before Bartosz's post. I'm glad I haven't been lying all this time.
I fastforwarded some parts so might have skipped, but didn't find the slides about multiple records with the same accessor name. Any info on that? In fact, it makes me a little worried - I like that haskell names are mostly unique, and am happy to fall back on qualifying in case of clashes. This new development needs to threaten that. Also, a source code hyperlinker could not easily decide which name a name refers to anymore (without doing some type resolution).
Actually it did not exist 3 years ago when the web application was written, so yes.
He mentioned that fields will have a type like age :: Has r Age t =&gt; r -&gt; t or whatnot, similar to the `has` library or HList, but I believe only to the extent that the same name can be used twice, which is a small, careful improvement to records.
You are probably referring to some sophisticated workflow with bare git-annex and I don't know about that. But using git-annex assistant gives folder synchronization between my laptop and desktop like Dropbox would. It's a bit more tricky to set up since there's no centralized server (if you don't have one of your own) but that's also part of the point - to have the data only on my own computers.
&gt; How can it be about Haskell? You just said yourself that "a 2-argument function" is not a thing that exists in Haskell. My point is that currying makes sense for functions which have at least 2 arguments, but does not makes sense for a value. Basically, what I'm saying is that by definition, a function is something which matches `a -&gt; b`. "0-argument functions" do not match this type pattern, and thus are not functions, in any sense. At some I realized that it really makes sense to distinguish between functions and values in Haskell, independently whether one's mathematical intuition says the opposite or the same. I was trying to explaining the logic behind this.
`lens` works fine with either version of `(.)` and `id`. It does become a bit more awkward to explain things to new users.
http://ghc.haskell.org/trac/ghc/wiki/Annotations
I forgot to mention that only OSX and Windows are supported, as [hmidi](http://hackage.haskell.org/package/hmidi) does not support Linux. It probably wouldn't be hard to write a hmidi-compatible wrapper around [alsa-midi](http://hackage.haskell.org/package/alsa-midi), but I have neither the time nor the motivation at the moment to do that.
That was Russell's idea originally. My main contribution there was working through the way the laws hold after that generalization and figuring out what the sort of 'lens family' concept means. This led to reformulating everything in terms of profunctors.
Both statements are true. I have no problem if the solution ends up coming from an add-on library or helper functions. However, I strongly doubt there's any way to solve the problems I've identified in a way that's user friendly and not error-prone without changing the core issue of automatic termination.
True, it's not a _only_ a matter of opaque types. It's about hiding implementation details from your end-users. The same is the case with type classes. Some type classes are there to give a convenient, commonly known, interface to your library. Like `Functor`/`Foldable` for working with elements of a list, perfectly fine. Sometimes type classes are there because they are needed to solve some implementation detail the author came across and serve less need outside the library. That's suboptimal.
I'd also throw in that lazy I/O allows for exceptions from pure code, whereas pipes does not. On the other hand, it's certainly possible to do monad transformer stacks using lazy I/O (see Data.Conduit.Lazy for an implementation), and obviously a pure monad doesn't *need* lazy I/O. But just in case anyone's asking me about pipes vs lazy I/O, pipes certainly gets my vote.
I quite like substituting `~&gt;` for `cat` in the Control.Category definition, and we have: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) Becomes: (.) :: (Category cat) =&gt; cat b c -&gt; cat a b -&gt; cat a c vs: (.) :: (Category (~&gt;)) =&gt; (b ~&gt; c) -&gt; (a ~&gt; b) -&gt; (a ~&gt; c) (Requires TypeOperators)
However, you're giving advice to somebody who would be using it today when it does exist. This is not something that's currently missing in Clojure.
&gt; [...] it does make sense [...] It does makes sense in mathematics. However in Haskell, it does make *more* sense to separate values and functions. &gt; [...] "function of n arguments" [...] That's my point: There is *no such thing* as a "function of n arguments" in Haskell, except for n=1. (It seems that we are in permanent philosophical disagreement, so I think it is meaningless to continue the discussion)
:( This always seems to happen. "Haskell only has functions of exactly one argument!" and the discussion ends. Yet somehow, there seem to be many more people objecting to the idea of 0-argument functions than to the idea of 2-argument functions....
Looks like there is no version of leksah on Hackage for GHC 7.6. Your options include (1) build leksah with GHC 7.4; (2) install leksah through your distro's package manager, if available; (3) download all the relevant packages with `cabal unpack`, relax a billion upper bounds, and make any needed adjustments to build leksah with GHC 7.6. Normally I would unhesitatingly suggest one of the first two options, but in the case of leksah I'm not sure whether it matters whether it is built against the version of GHC you want to use for development (I see it depends on the ghc package, for instance).
I'm also objecting the idea of 2-argument functions! Thinking a few minutes about it, now I think that *even in mathematics* it makes more sense to distinguish values and functions, and to say that every function has *exactly one* argument (of course, Haskell is more-or-less a part of mathematics, so this is not that big a surprise). Your pattern of 0,1,2,3,etc argument functions should be looked as A^0 -&gt; B, A^1 -&gt; B, A^2 -&gt; B, A^3 -&gt; B etc. (of course you can have arguments of different types, just replace by the corresponding product space). In particular, category theory is a good example why this way of thinking probably makes more sense.
&gt; 0-&amp;gt;B, 1-&amp;gt;B, 2-&amp;gt;B,
Here you can find packages that should work with GHC 7.6.3: http://leksah.org/packages/ If I remember correctly the latest version is still somewhat experimental, so you may run into issues with it. Can't tell you any specifics though, because I don't use Leksah.
1. I am not giving anything. I am describing my personal experience. Is it my mistake that this is how it played out 3 years ago? 2. core.typed does not have type inference, does not give you GADTs. So my point regarding complex types (especially recursive trees) is still relevant. 
Also requires a good font. Two ways this can go wrong: 1. In some fonts `~` appears relatively high up, as if it was an `ñ` without the `n`: this makes `~&gt;` unsightly. 2. In some other fonts `~` is indistinguishable from `-` at normal size, with confusing results.
My gut feeling was that, if it identifies a functor-polymorphic function with a data type, it must have something to do with Yoneda in the Functor Category. It was finding the eta and the f that was hard for me. I was carrying around scraps of paper with the formulas at the ICFP trying to fit them together. Finally it just hit me.
I am still puzzled as to how something so compelling was so misleading.
+1 for the, uh, -1. :)
I don't think the word "composition" implies that. It does seem writing the correct code for Conduits in Snoyman's examples is significantly easier than in pipes. Maybe other examples would show otherwise, but is anyone complaining about buggy code due to Conduits' more complex semantics?
&gt; My point is that currying makes sense for functions which have at least 2 arguments, but does not makes sense for a value. Oh, hang on. I am not currying a value, and I agree that that does not make sense. I am currying a 0-parameter function, and the *result* is a value. I wrote in my first post &gt; [a value] `f0 :: t0` is the "curried form of a 0-parameter function". Eventually, I switched to the short-hand "n-parameter function" for "curried form of an n-parameter function", as is normal among Haskellers, but feel free to reinsert "curried form of" everywhere. And by "n-parameter function" you can imagine that I mean an n-tuple of sets (A1, ..., An), a set B and an ordinary function A1 x ... x An -&gt; B. (Or use types and a Haskell function whose domain is a tuple if you prefer.) There's nothing magical that only happens once n &gt;= 2. There is an irregularity in that the curried form of an n-parameter function for n &gt;= 1 is a "function", while the curried form of a 0-parameter function is a "value", but that is simply a consequence of the fact that the term "function" only has to do with the outermost type constructor of the type. Does that clarify anything?
I think Michael appreciates the principled approach as much as anyone. However, Michael is not a full-time researcher, he needs to get work done with his abstractions sooner rather than later. So Michael compromises and writes a library that lets him get his work done today, with a *better* approach than lazy I/O, and the simpler (non-streaming) parts can enjoy the principled approach today. Gabriel's library is more principled, but as Michael shows with his examples, it makes at least kinds of plausibly-looking code really easy to get wrong. It seems significantly easier to write that code correctly with Conduits. Surely a principled approach that allows both reasoning, and directing the user towards writing good code will be found. But until then, a working programmer has to choose: principled research or working code.
But I think there's no known way to decompose the solution while still giving the traits that Michael cares about. Michael is unwilling to compromise on those traits (that he convincingly shows are not attainable in the current pipes/decomposed approach, if you compare with your examples) and you are unwilling to compromise on the decomposition/laws. Both approaches have merit, and Michael seems to make a sincere effort to "meet in the middle".
This is awesome. I'm officially converted to lenses. I noticed this pattern that often in Haskell it is very hard work to do something which is trivial in other languages - however, if you actually manage to do this hard work, the final result makes it very worthwhile. Also I had a random brainfart about abstraction vs. specializations (category vs. functions, monoids vs. string concatenation, etc): What if all libraries were very general, but there was a language feature which would allow you to restrict type classes? Say (++) would be a monoid operation, but you could say that in this module, (++) specializes to `String -&gt; String -&gt; String`? (I haven't thought it through though...)
Lennart wrote his own proprietary haskell dialect called "mu" for Standard Charter. I assume that is what he is talking about. It also apparently has an IDE, works perfectly with the JVM, .NET, C++, and Excel, supports globably distributed execution, and has about as many full time users as GHC. For real. Banks are like that.
I'd still like to see that written up clearly.
You keep returning to "line" as if it was an example for conduit's superiority. It's not. It's an example of how conduit fails to compose well, just like pipes. Instead, look at the examples involving early termination that use simple composition and are not as easy to get wrong.
Building up an AST isn't directly comparable to intro/elim rules. That is, we're also going to want AST constructors for: Fst :: Exp -&gt; Exp Snd :: Exp -&gt; Exp CaseEither :: (Var -&gt; Exp) -&gt; (Var -&gt; Exp) -&gt; Exp -&gt; Exp ... But these are all just building up expressions; and when it comes to building expressions, every syntactic form is going to be a new constructor for the AST. All these AST constructors are actually the rules of your logical system. And from the natural deduction standpoint, some of these are intro rules and others are elim rules. (Or, from the sequent calculus standpoint we'd get left-rules and right-rules.) Which is which depends on looking at the *types* these rules are proving and then seeing whether a primitive type constructor is being introduced or eliminated. E.g., Gamma |- e :: (A,B) ----------------------- Gamma |- Fst e :: A is an elim rule because we're eliminating the `(,)` type constructor. The use of `Fst` is to construct a proof tree to witness this elimination.
Sorry, I was writing a lot today without reading and thinking. However my main point stands: I think that in both Haskell and mathematics, all functions have exactly one argument (and one result - "co-argument" if you want). What we typically call n-argument functions is a function from a product space (and the empty product is the ~~initial~~ terminal object in the given category). What we call a curried function is by definition a function of exactly one argument. There is an isomorphism between the two (which we call currying). Mathematicians are used to consider isomorphic things the same, working informally - however if you work formally, then there are functions and values (arrows and objects, in the language of category theory), nothing else.
I don't know if you've seen my [previous response](http://www.reddit.com/r/haskell/comments/1nw7ji/pipes_resource_problems/ccmtan4) on this topic, which summarizes the tradeoffs involved in adopting the `conduit` approach to finalization. It's not an issue of "simplicity vs features", but rather "features vs features". I have to sacrifice several compelling features of `pipes` if I reproduce the `conduit` model of finalization. I would also have to sacrifice correctness.
The tradeoff I have to make is not just decomposition/laws. There are several `pipes` features that I'm unwilling to sacrifice, such as finalization that works with `ListT` and single-stepping (I know that `conduit` has `($$+)` and `ResumableSource`, but this is error-prone), having a guaranteed order of finalization, and a symmetric semantics that makes things like the `Arrow` and `ArrowChoice` instances for push-based pipes work.
It's not just about laws being obeyed, but about correctness. Nobody actually knows if his finalization framework is sound, but people are asking me to adopt it. Why should I adopt something that hasn't been proven sound?
&gt; my guess: we won't be programming. We'll be [...] and the awesome compilers of the future will do the programming for us. But that's still programming. It's what we're doing already today: we write high-level specs in Haskell or whatever, and the awesome compilers of today convert that into assembly and machine code. While the conceptual distance between our programming languages and the target of compilation may grow ever larger, the fundamental structure is still the same: we use a structured language to convey instructions to a computational device. If what we're doing today can be called "programming", then how could what those folks are doing in 100k years be any different? The only way I could see it being different is if it becomes indistinguishable from natural language (since we prefer not to think of ourselves as "programming" each other); that is, so much like natural language that the actual response to our linguistic instructions are essentially unpredictable. This seems suboptimal for doing computation, gathering information, controlling mechanical devices, etc. While I'd expect the programming languages of the far future to be much more like dialogue systems (we're already seeing the beginning of this with Agda's emacs-mode and Coq's proof-general), and therefore much more like natural language in that sense, where the human and some (weak? strong?) AI work together to co-program a solution to whatever problem; the ultimate goal of utilizing computation machines is very different from the ultimate goal of interacting with our conspecifics, so I can't imagine how it would stop being considered what those humans of 100k would call "programming".
Banks. *thoughtful, oblivious nod* (Wow!)
But then, if you only allow things of the form `a-&gt;b` to be called "functions" then how can you even speak of "two-argument functions"? Interpreting this phrase colloquially, you can only mean either things of the form `(a,b)-&gt;c` or things of the form `a-&gt;(b-&gt;c)`. However, both of these are exactly "one-argument functions" by your choice of terminology. It just happens that the former takes a tuple as its single argument, and the latter returns a function after taking its single argument. You can't have it both ways.
&gt; It does makes sense in mathematics. However in Haskell, it does make more sense to separate values and functions. Let us call nullary functions, or values, "Foos"; and let us call n-ary functions (where n&gt;0) "Bars". The reasons why it makes sense to distinguish Foos from Bars in Haskell is exactly the same reason as it makes sense to distinguish Foos from Bars in mathematics too. In particular, we need a basis case for the inductive definition, and often times Foos have properties which make them quite radically different from Bars.[1] However, this desire to distinguish between Foos and Bars says absolutely nothing about the independent desire to consider both Foos and Bars to be instances of some more general FooBar pattern. For example, when discussing terms of type `Either A B`, we don't get into deep philosophical arguments about how `Left a` values are fundamentally distinct from `Right b` values, and how we should never group them together to discuss `Either A B` values. No, we recognize that it makes sense to simultaneously distinguish these two cases and also to group them together. [1] To pick an example that departs radically from the current discussion, consider the notions of local and global validity in modal logics. When the environment is empty, these notions coincide; but when they are nonempty, they do not. Similarly, the difference between manipulating open terms and closed terms is radically different in many systems. However, noone seriously considers saying that `|- e` and `Gamma |- e` (where Gamma is non-empty) should be considered unrelated statements because one is a Foo and the other is a Bar. 
Why should we look at it as `A*A -&gt; B` instead of as `A -&gt; B^A`? Both perspectives are important, and you can't always guarantee that one or the other is always available...
I think the best point was Neil's: why not just use a deep EDSL and build up the batching from there, as everything is just represented by data? It also gets rid of the applicative and monadic cruft (see the other talks today). 
&gt; But until then, a working programmer has to choose: principled research or working code. Having built two large projects using pipes (rewriting from conduit to pipes in the first case), I don't believe the above is actually true. The first thought I have when looking at Michael's examples in this debate is always some variation of _who writes pipes code like that?_. The libraries are very different, and translating conduit code snippets into pipes code snippets looks awkward and error prone. Actually using pipes in real world code just isn't like that (in my experience). I agree with what kamatsu said above. The whole Haskell attraction for me is the preference for solid theoretical foundations over the more prevalent "it seems to get the job done" pragmatism. Otherwise I'd just stick to C.
Oh, that should have been clear, I was just not thinking so straight. I read a (the?) proposal this morning as well and it had been on my mind. In particular, there are a lot of places where `Applicative` instances can be better than `Monad` instances and thus end up with observable differences between `(&lt;*&gt;)` and `ap`. I'm all for the AMP, but I'm really curious to see how these differences get hammered out.
Does anyone have a source on that magic value injection technique mentioned for producing deep embeddings of bind?
If you use a different newtype for Lens, Traversal, etc. You lose a huge amount of usefulness, which IMO is definitely not worth it. If you use a single newtype for all of the same type shapes ("LensLike"), you lose out compatibility with existing functions that are valid lenses (e.g: "traverse"). Also it doesn't make things that much clearer. Also, there's the issue of being able to export lenses from libraries without depending on the whole ecosystem behind the lens library.
Nice talk. I'm tickled to hear SPJ say that he finds point-free code reads like line noise, and he tends to write point-ful code.
But if you look at the way that code is translated into idiomatic pipes code (Tekmo's comment), it seems like the pipes API does a much worse job. The correct solution seems subtle and complex, and the simple/straight-forward use of the API is wrong. If these examples don't have a simple, idiomatic pipes way of writing correctly *and* do allow a simple, elegant and wrong solution to be written -- that is a problem. 
well, you know, when you're part of the design group of something like Haskell, you kinda have good abstraction abilities toward alien looking things.
It really should be Simon "oh drat" Peyton Jones. Peyton isn't his middle name, it's part of his surname, I think.
I want a good prompt finalization story, too, but I'm pretty sure `conduit` is not that story. I've done enough equational reasoning about both `pipes` and `conduit` to be very suspicious when Michael claims that `conduit` has solved this problem. That's also not taking into account the known issues with `conduit` finalization, such as reordering of finalizers.
I think he has said, like many others, that Comic Sans is a very readable font for presentations. I think it's a valid use, in addition to its original use (comics on the screen without AA).
I appreciate your concern, and I'm not saying that sarcastically. I understand that you wouldn't be asking this if you didn't care about `pipes`. You have to understand, though, that my development style is "slow and steady wins the race". This is not by choice but by necessity: I cannot afford time to maintain and defend an unsound implementation and I will not be able to promote something that I do not 100% believe in. I will continue to look for improved finalization solutions, but I will not make a move until I find a solution that is a definite improvement.
(4) Download from github https://github.com/leksah/leksah and install with cabal-meta and GHC 7.6. As described at the bottom of this page (http://leksah.org/download.html). (5) If you are on Windows or OS X download one of the prebuilt packages http://leksah.org/packages/ Version 0.14 will be in hackage soon, I hope. 
&gt; you know full well that core.typed does do type inference No, i do not. And last i checked the author of core.typed was asking for donations to implement something similar to type inference. &gt;You present it as the current state of things. And it is :)) Clojure IS a dynamic language. Or do you think you can call python static language just because python 3 has a optional type annotations or some random dude implemented [typed python](http://www.mypy-lang.org/)? You seriously going to compare a type system of haskell to a optional lib which is unfinished, incomplete, several months effort of one guy? 
&gt; I feel like I need to put more trust a dev who gives me proof and no tests than in one who gives me tests and no proof. Why? Would you not need to read some of the tests to make sure they are actually testing something important and not just sending back an "everything is ok" message? Just like you would probably read over some of the tests and not each one in detail, this will give you a measure of trust in the author and the rest of the tests. The same applies to the proofs you can read over some of the proofs, check them and that should give you a measure of trust in the author and the rest of the proofs. 
&gt; I don't think the word "composition" implies that. We'll more or less have to agree to disagree then, but I can't think of _anything_ in mathematics that is called composition that is _not_ from a category. Indeed, the whole point of a category was to capture this very idea of composition!
Ah, excellent. This is exactly what I've been waiting for: conduit implemented in terms of pipes. I sort of knew it could be done with pipes-parse 2.0, but it's nice that Tekmo has taken the trouble to sit down and do it. I'm interested to see where this will lead us in terms of library interoperability. It's worth noting that Tekmo built the whole conduit abstraction on top of `Producer`s, not the more powerful `Proxy` or even `Pipe`. Presumably, that means we could write the same thing using this definition for `Producer`: data Producer a m r = M (m (Producer a m r)) | Pure r | Yield a (Producer a m r) What remains to be seen is how well these representations optimize, compared to conduit's current implementation.
I like to call this one-way type `Generator`, in homage to Python. I agree that there is value in a simpler `Generator` ecosystem, especially if you build `ListT` on top of `Generator` instead of `Producer`. This is one reason I was interested in your line of research for your `yield` library. You could imagine a sort of triaging of complexity: * Generators/ListT * Pipes * Conduit
You saw he wrote "sans finalization", right? That's one of conduit's main features, and something which drives its design.
&gt;No, i do not. And last i checked the author of core.typed was asking for donations to implement something similar to type inference. So explain to me what's happening here then? (ann foo [Integer -&gt; String]) (defn foo [bar] (let [baz (* bar 5)] baz)) Type Error (typed-test.core:25:13) Local binding baz expected type String, but actual type AnyInteger in: baz Type Error (typed-test.core:24) Type mismatch: Expected: String Actual: AnyInteger in: (let* [baz (clojure.lang.Numbers/multiply bar 5)] baz) Type Error (typed-test.core:24:1) Type mismatch: Expected: (Fn [Integer -&gt; String]) Actual: (Fn [Integer -&gt; AnyInteger]) in: (def foo (fn* ([bar] (let* # baz)))) &gt;And it is :)) Clojure IS a dynamic language. Clojure with core.typed IS a statically typed language. Saying that it doesn't count because it's a library is kind of absurd, especially so when it comes to Lisp. Pretty much all of Clojure is implemented in the language itself. That's the beauty of Lisp, instead of language features you have macros and libraries. &gt;Or do you think you can call python static language just because python 3 has a optional type annotations or some random dude implemented typed python[1] ? There is an example of an unfinished, incomplete, several months effort of one guy. :) &gt;You seriously going to compare a type system of haskell to a optional lib which is unfinished, incomplete, several months effort of one guy? In what way is the lib unfinished exactly? Last I checked it's used in production for real world projects. The fact that it's being currently improved and features are added doesn't make it somehow unfinished. Can you please extrapolate in what way it's incomplete? And the project is over a year old with multiple contributors. All this information is readily available if you could only be arsed to check github before pulling things out of your ass. Also, why are we comparing it to Haskell. You simply made a false statement that Clojure does not have static typing. The fact of the matter is that it has much better static typing than most languages. Haskell has exceptional static typing, but surely you won't argue that anything less is outright inadequate? I'm very curious why you're so hostile towards the idea that Clojure has a static type system that's actually being used and people like. 
C# does that too. Yet no one says it has type inference. Why? Because &gt;All vars must be annotated &gt;All function parameters must be annotated, or default to Any. But ok, i'll accept it. **limited** type inference. Agree? &gt;In what way is the lib unfinished exactly? From the documentation: &gt;core.typed is early in development and there are Clojure idioms it **cannot currently type check**. Wrap top-level expressions in clojure.core.typed/tc-ignore to ignore them. &gt;Suggestion: If porting a namespace to core.typed, initially **use tc-ignore liberally to ignore problematic code** while determining the types for expressions. Once most vars are annotated, revisit these sites to determine the issue. As you see, unfinished and incomplete. &gt;Also, why are we comparing it to Haskell. That's the subject of this thread :)) &gt;I'm very curious why you're so hostile towards the idea that Clojure has a static type system that's actually being used and people like. By choosing haskell? I do not care if clojure has **optional**, **limited** and **incomplete** type system. I already got what i want. And it is vastly superior. 
Tekmo was the author, not I.
&gt; This is one reason I was interested in your line of research for your yield library. Ah yes. I'll get around to releasing that someday... hehe.
I confess I tend to write pointed code first, then transform away the last argument and see if pointfree is cleaner. If it is, then I tend to keep it because it is usually much easier to dualize in that form.
I revisited this thread and saw this question. The answer is yes. The trick is to evaluate to HNF instead of WHNF. The problem is that I don't think anybody knows of a way to do that anywhere near as efficiently as we know how to just evaluate to WHNF.
I've wanted to remove `Done`, but you can't get a `Monad` instance going in that direction. But I do agree with the sentiment of using an `EitherT` or something similar. I've just [posted part 3](https://www.fpcomplete.com/user/snoyberg/blog-posts/simpler-conduit-core) which steps through this.
You're right, as the example is structured right now. However, it's *not* uncommon to have an example where the long running computation has to take place inside the `Producer`. For example, consider a producer that will concatenate a few files and the results of some HTTP requests. Making those HTTP requests could (relatively speaking) take a while. Or another example could be a `Producer` that needs to wait for an `MVar` to free up before it can produce its next piece of output.
So instead of making shit up, you could've listed the actual advantages of Haskell type system over core.typed. I'm also pretty sure there's nothing ironic in that statement. :)
I have been in exactly this position and put a little bit too much gunpowder in the cannon. It was being argued against me that "your library cannot make me write this [insert crappy client-side code], while my [buggy, rather useless] library can let me write this [useless] client-side code, see!" The argument was pretty degenerate already, but it tricked me into confusing two facts: 1) It was being argued against me that I could not achieve a thing. 2) It was being argued against me that the importance of that thing was assumed and without question. Tackling point 2) is where all the effort lies in counter-arguing and so, is very frustrating. One day, when I stopped caring so much, I decided to tackle point 1). I wrote all the nonsense library support that allowed me to achieve that [useless] thing. While I was there, I predicted the response and wrote the nonsense to answer that too. Eventually, the argument was extinguished because I could respond with a superior solution every time, but importantly, it was *recognisably superior*. Never mind that we were playing silly games all along. I advise; decide if you wish to play what you perceive as silly games and do so efficiently, or just abandon the cause. I mean, silly games are fun and all; don't let me discourage that, but know that you are doing it.
[Point taken](http://www.reddit.com/r/haskell/comments/1o3viw/how_to_reimplement_the_conduit_parsing_api_in/.compact).
No one was asking me about core.typed. Why would i waste time of the person who asked a completely different question? Especially since I am NOT USING core.typed? :)) 
&gt;C# does that too. Yet no one says it has type inference. Why? Because Last I checked C# doesn't automatically infer types within nested expressions. Maybe C# in your alterverse behaves differently though. &gt;But ok, i'll accept it. limited type inference. Agree? Looks like we're making progress finally. Looks like you missed an important part of the quote in your highlighting: &gt;If porting a namespace to core.typed, **initially** use tc-ignore liberally to ignore problematic code That changes the whole meaning doesn't it. As I said it's clearly finished and complete enough for people to use it in the wild. &gt;I already got what i want. And it is vastly superior. I'm very glad for you, now if you can bring yourself to stop making things up wrt to how other languages work that would be great. :)
Now we're getting somewhere. Still, I got very muddled when you started talking about adding `Maybe`. It was not at all apparent to me why you did that or what problem it solves, but it was presented as though it should be obvious to the reader. Maybe it's because I never looked closely enough at these libraries, but it'd be nice to get an explanation.
What I meant was using one global StateT shared by all the stages, but I think that works, too.
In pipes, `await`ing always returns an upstream value. So what happens when there is no upstream value? Automatic termination, problem solved. If you get rid of automatic termination, then you need to give some kind of a result for `await` in the case when upstream has nothing to provide. So the "natural" solution (for some values of natural) is that `await` returns a `Maybe` value. Does that clear it up? I can go into more detail if it will be helpful.
&gt; It's YOUR problem that you're not using core.typed and no one else's. How would core.typed helped me with complex types? It would not. If recursive trees are not enough for you, I'll give you another example from my experience. I use commercial library pdflib with FFI. BTW C FFI is very simple in haskell. That pdflib accepts very complex options via string. We are talking about quite rich set of options (hundreds of them) all via one string. And different sets of options need to be passed to different objects (document, page, image, bookmarks, text flows etc) What i did is i created many types of GADT options and a type class unifying all of them that allowed me to correctly generate a string from sets of options when passing to low level pdflib procedures. So my haskell code operates only on types, never on strings etc. This prevents mistakes, typos, prevents passing wrongs sets of options to wrong objects. It makes my code much more readable with all the color coding etc, compared to my old code in delphi where i had to manipulate and concatenate strings. Of course clojure even with core.typed would not help me since marking a option as type String is completely useless :)) Is that enough? 
Excuse me, but i wrote it ON REQUEST :)) The only thing i wrote BEFORE it was the question why python and not other dynamic languages. As you see i made sure i am not wasting anyone's time. 
Sorry, saw it on my phone and thought he replied to Tekmo (his reply was just under Tekmo's at the time).
So, to sum it up you had to go ahead and manually map each set of options for each specific operation. The fact that you decided to go ahead and create a bunch of types doesn't make it the only way to get the same result. Have you used maps and keywords you would've ended up with the same result. The maps can of course be checked by core.typed, eg: (HMap :mandatory {:id String :links (t/Set String) :title String} :optional {:distance t/AnyInteger :path (t/Seq String)} :complete? true) Is that enough? 
And how do you make sure i pass the right map to the right object? I cold create a map for a page but pass it to a document. 
By the same mechanism. Since you can clearly specify what keywords the map should contain. If you wanted to get really fancy you could create a protocol or maybe a multimethod. It's basic polymorphism for crying out loud.
The simple case is to actually store a stack of finalizers for each direction, but another solution is that you can store just a single finalizer for each direction and use the pipes themselves as an implicit stack. See the register function in the [source code](http://hackage.haskell.org/package/pipes-safe-1.2.0/docs/src/Control-Proxy-Safe-Core.html) of `pipes-safe-1.2.0` for a specific example of this.
Please educate me :)) How would that code look? In haskell i only need to mention one type, not every combination of the option keywords. Also composability. It is nice to be able to write: [Parent parent, Destination [DestPage currPage]] ++ [OpenChildren | parent == Bookmark 0] 
[Here's](https://github.com/yogthos/Selmer/blob/master/src/selmer/node.clj) a real world example for you. I have different types of nodes in my template and I use a protocol to keep track of them and allow them to have different behaviors. Why would you not be able to compose things when using maps exactly?
It's a trade-off, definitely. And I'm not implying lens made the wrong choice here.
I do not see how the presented code prevents passing to a function map with the wrong keys. Could you please elaborate? 
Yes, that lens type is too big, and I don't like that I need it all. But I don't see this as a conceptual problem of the package, but a way to get lenses into Haskell's existing type classes. Luckily, it has near zero impact on the implementation of rest if the library, the usage or the performance. Nonetheless I technically agree with your sentiment here, it's bit awkward.
The presented code is showing that you can trivially do polymorphism with Clojure. The code I gave you earlier prevents passing a map with wrong keys. Now I recall that we had a very similar discussion earlier where I presented you with a [complete example](http://www.reddit.com/r/lisp/comments/1m1opf/parameter_lists_in_common_lisp_and_clojure/cc9e2av). This is getting a bit old at this point. You know as well as I do that what you outlined can be done trivially and safely outside Haskell.
Not true. The code you presented only prevents **building** a map with wrong set of keys, but i do not see how it prevents passing a map with correct (but different) set of keys to a wrong function. Please, i'm dying to learn from such a great prigrammer as you. A small example is all i ask for :)) 
Sorry, my bad on that one just looked at the parent. :) You **do** get a compile time exception when the function gets the map with the keys that aren't expected. What you see in that example is done at compile time.
Talking about timing! I've bought a Launchpa last Friday and next thing I wanted to do is to play with it in Haskell :) Seems like a nice project I might be contributing to :P Thanks!
That was enlightening! I am really excited about the discussion that has been going on recently with this! The dead-simple structure and then evolution for desired behavior seems to be a really nice way to prototype these ideas out, and be easy to understand.
Does it allow us to do simple things like `record.field` (or `field record`) or is it all generic post-fix apply up the Yoneda? ;)
Very interesting talk, as always, but the question (and the answer) in the end is very important I think. Andres mentions several times that is a big advantage is that you now have a datatype for you domain that you can inspect and interpret in several ways. This is of course only partially true, because you can still never look at the right hand side of a bind. So, without actual (or possibly abstract) interpretation, you can only inspect/print/optimize one layer of your monadic computation. This is why Andres calls monads a *"a bit of a peculiar structure"* here. Applicatives solve this problem by allowing you to fully inspect the structure of your computation without needing to perform an interpretation first. Sometimes monadic code is just applicative code in disguise and it would make sense to just write applicative style instead of using do-notation. The problem arises here is that because you cannot inspect the right hand side of monadic binds, you can also not just turn monadic code into applicative code. This is a problem that Simon Marlow described in his talk at Zurihac. Free monads don't solve this problem and don't intend to, but it is still a big limitation. I wonder if there is a nice solution for the static analysis of monadic structures.
1. It wasn't really in the talk due to lack of time 2. Don't worry--it is just a language extension AFAIK and will require a pragma. Nothing is going to break. If it is a bad design in practice it will die, get deprecated, and eventually removed from GHC. If it works people will use it. 3. The existing type class/type family machinery does all the work. So, your IDE issues etc are nothing other than issues that already exist.
propositional identity, not definitional. The univalence axiom really isn't that scary once you realize it only changes things at the eliminator for equality which, in ITT, is pretty limited. Univalence + ETT or Univalence + Axiom K is more interesting/terrifying.
Interesting. You can't even inspect the right hand side of an `ap` because it's implemented in terms of `bind`! I never realised that before. That's a big limitation of free monads.
... and you'd have a `SimplePrelude` that is monomorphic in pretty much everything? if you go that route, you can just specialize: (++) :: Text -&gt; Text -&gt; Text (++) = mappend sounds reasonable should work without any problem right now.
Um, no you are not. Your example only illustrates how to make sure the map is constructed with the right set of keys. It does not show how would i get a compile time error if i'd try to pass that map to a wrong function. You can use any of the paste sites to provide a minimal example of a function that would give me a compile time error upon passing to it a wrong map. I hope it is not too much trouble, because i get that result in haskell with one line of the type definition :)) 
I just don't know of a way to do it. You can represent bind and return as data (as in Andres' talk), but you still can't reify the right-hand-side of bind without knowing the result of the left. Injecting a magic value is possible if you know the type and have such a magic value, but in general bind is polymorphic so you can't do this. Here's a nice description of the problems that arise: http://www.cse.chalmers.se/~joels/writing/bb.pdf
Ah, I don't know either, it just seemed like the most interesting question to me :) 
Having "rest" in IO seems kind of counter-intuitive. Why not separate it into a pure recursive function?
Great talk. I wish I could have been there! 
btw, I wanted to post this to the haskell-art mailing list, but it bounces back with "unreachable address" or something like that, does anyone knows anything about that?
tshow is from classy-prelude hackage. I use it a lot recently, it allows me to use bytestrings, lazy bytestrings and text without all that qualified imports, I find it very helpful.
Nice! Last week I attempted to write a similar thing for the (slightly costlier) [NI Maschine](http://www.native-instruments.com/de/products/maschine/production-systems/maschine/). No results yet because I'm still struggling with time leaks with the [FRP system](http://www.haskell.org/haskellwiki/Reactive-banana) I choose to constrain myself too :) My original goal was to build a bridge between the controller and supercollider. Do you plan to include a interface to a synthesizer?
Instead of Await :: (Maybe i -&gt; Pipe i o m r) -&gt; Pipe i o m r What about using a finalizing continuation, something like Await :: (i -&gt; Pipe i o m r) -&gt; Pipe Void o m r -&gt; Pipe i o m r to get the type safety that you cannot call await again after it has returned nothing? (Not sure if Void is the right thing to use in conduit, I think pipes uses it for this kind of thing.)
Damned, sure… I thought they were constructors… damn me :D
The idea was that you only have general libraries, with the same symbols as now there are in the specialized libraries (eg. `mappend` would be called `(++)`), however, in any module, you could restrict (with a new language construct) the type classes. So `BeginnerPrelude` would just import the generic Prelude, add a few lines specifying that `(++)` should only work for lists, String and Text, and re-export it. One advantage would be that you don't need different symbols for the specific and general symbols. I mean you can do that now with qualified imports, but it's ugly and inconvenient. Another advantage is that in any module, you can choose the tradeoff between abstraction and better error messages, while all the libraries can stay very general (I imagine a typical very abstract library would have a few specialized way to import it included by default).
Thank you, I missed this. Added a link to the first sentence to the Annotation wiki link, so people who never seen GHC annotations before can start reading there.
Yes, you're right in principle, but not so much in practice. First of all, we have duplicate checking in $initHFlags, see https://github.com/errge/hflags/blob/master/HFlags.hs#L400. So we won't silently just use one of the flags, but instead we will fail to _compile_ the main program. Not fail at runtime, but at compile time. So this is not seen by the library authors, but at least seen by the first user, who tries to use the libs together and then can report a bug. Second, when we have the proposed patches in the compiler, it will be much more convenient to send messages between template haskell runs and know where a flag came from, therefore we could implement automatic conflict resolution based on module names. If two modules both define the flag --verbose, the main program will automatically convert them to --Module.A.verbose and --Module.B.verbose. And of course, the long, prefixed names will always work, so if you know that you want to set wget verbosity, you will be always able to say --HTTP.WGet.verbose=True. The user model of the library is stolen from google: http://code.google.com/p/gflags/. They do this for their whole codebase company-wide. That's a large-large-large codebase and it still works out. This is one of the biggest takeaway that I got while working there. Simple, but useful idea, this is why I'd like to see this in Haskell.
If I understand you correctly, `rest` has to be in `IO` because it might contain the result of `recvLn s`, which is in `IO`. `recvLn` has to be in `IO` because it contains `recv`, which is the serial receive function `serialport` defines.
One of the great strengths of LC is the correspondence of its type system with first-order logic. Sure, other calculi correspond to other logics, but FOL is very basic. On the practical side, LC incarnates the concept of functional abstraction. A concept that any common general-purpose programming language includes. In contrast, process calculi feel more low-level (as you point out concerning the number of reduction steps needed) and it is far less clear how you can add sugar to get to a general-purpose language. The join calculus etc may come to mind, but they are usually embedded in another language.
Splitting it up into two different fields like that is definitely something worth doing. The question of whatever to change the input type is a little bit trickier, mostly because it makes the `Monad` instance more complicated. If we were to change it, it might make more sense to use `()`, with the intuition being that there's an infinite stream of meaningless data sitting in front of the `Pipe`. I believe that's the approach that pipes uses.
using this ghc extension makes me more confident in using hflags. it makes it less magic. i'm still thinking i'd prefer to (optionally if you like) have (for a flag named `flagN`) flags_flagN :: MonadIO m =&gt; m a instead of flags_flagN :: a because i'd like to know where i am using the environment and it usually corresponds to when i am within `IO`. (edit: of course the type is more complex in the real world... the idea should be clear though.)
I think pipes uses () to denote that the upstream or downstream is not used bidirectionally, but Void (in one of the types, the other one remains () for reasons I don't fully understand) to denote that it is not used at all.
When I was a haskell newbie (years ago now) I wrote a library called `generators`. I got a lot of negative reaction about choosing such a general name. I'm sorry everyone, but I can't change the package name now.
The idea is that you restrict the *output* types of the primitive instructions, i.e. data Instruction a where Get :: Instruction S Put :: S -&gt; Instruction () ... Now, if you choose `S` such that you can reify functions of type `S -&gt; a`, you are done. For instance, you can disallow pattern matching on `S`, which allows you to treat it like an opaque variable. The user can still inject arbitrary types via `return` and `&gt;&gt;=`, but these will be "evaluated away" before your interpreter gets at them, similar to how a host-language macro gets evaluated before you see an AST. (I guess it's hard to explain. I should make an example.) 
Ah I see. But users now cannot just have the full power of monads, like: do a &lt;- act1 case a of S a | even a -&gt; act2 | odd a -&gt; act3 So, pick new actions based on arbitrary output values from other actions.
 I think "import" behaves just like a normal import in ordinary code. "load" exposes the guts of the module to your REPL (even unexported symbols). No idea what "add" is, and there may very well be more differences I'm not aware of.
This is unfortunate namespace pollution, but I don't know what can be done to address it. 
I'm going to backpeddle somewhat on this and note that there's not much difference between the free monads on `B x x` and on `Bool -&gt; x`. Still, being able to deal with appplicative computations directly with a free monad like structure would be useful.
Is this related to the [constrained monad problem](https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDEQFjAA&amp;url=http%3A%2F%2Fwww.ittc.ku.edu%2Fcsdl%2Ffpg%2Ffiles%2FSculthorpe-13-ConstrainedMonad.pdf&amp;ei=85ZWUo__FYPgiwLw9oCwDg&amp;usg=AFQjCNHut8vlp1zo9wdH9WPfrexXI9vbxg&amp;sig2=8loJ3Ew2qekYQNKGExFERg&amp;bvm=bv.53899372,d.cGE)?
Package things under a user name, like github?
Uhm hello? (ann height-option [String &amp; {:height Number} -&gt; String]) (defn height-option [name &amp; {height :height}] (if-not height (str "I have no opinion on " name ".") (if (&lt; height 6) (str name " is short.") (str name " is tall.")))) The above is the definition of the function that says it will take a map with the keyword `:height` in it. When passed a map with a different keyword you get a compile time error. So yes, if you define the maps each having a restricted set of keys and annotate the respective functions to only accept the specified keys you get exactly what you're asking. I hope it's not too much trouble to actually read before making your half baked insults? :))
+1 from me! It seems that a large percentage of people that are doing haskell backed web thingys (including us) are using anglular on the front end. It would seem to make perfect sense to extend Haskell just that little bit further to provide and little layer to do type safe communications between the front and back end!
Many years ago, I wrote a Python library (cinvoke) which is very similar to [ctypes](http://docs.python.org/2/library/ctypes.html). Unlike ctypes, my library had nicer declarative data structures which I could use to [de-]construct parameters to C functions, but also I could use those same definitions to parse/build from strings. The main difference, however, was that I had used FFI-over-file-descriptors, rather than via shared object loading. You'd add in a small `cinvoke` server to your C code (either by using a standard `main` provided with CInvoke, or hooking up the `cinvoke` server to your event loop). To make use of `cinvoke` easy, I had implemented debug information parsing -- that extracts all the data types, variables, symbols, etc from the executable so I can use it from Python. My main use was writing unit tests for C code in Python. Since Python (at that time) had no DWARF parsing library for the debug information, I parsed the textual output of `readelf --debug-dump`. This was error-prone and broke with every new version of `readelf`. It was also very slow and hard to make comprehensive. Then I saw Haskell has a pretty good Dwarf parsing library. So ironically, Haskell in this case was better due to its ecosystem. Haskell and the Dwarf library allowed writing a simple, stable debug information extractor that was **faster, more comprehensive, and took about 20% of the time implement**. Also, shameless plug: anyone who wants to read debug information of executables, the higher-level [dwarfadt](http://hackage.haskell.org/package/dwarfadt) library of mine makes it easier than ever. It does not cover all kinds of debug information you might want, but if you want access to the types, structs, functions, etc, it is great. EDIT: I forgot to give some explicit answers to the questions: 1. It had no good library (at the time, at least) for parsing ELF/DWARF debug information. It was slow. 2. Because it had a good library in combination with a relatively fast, and a very productive language. 3. The Haskell code generates Python files that are then imported by the Python library. 4. Don't have any insight here.
&gt; Do you plan to include a interface to a synthesizer? The "app framework" allows you to send out MIDI messages to a device other than the Launchpad, so you can control any synthesizer which has MIDI support (that is, almost all of them). Actually all four example applications are for controlling synthesizers or effects (for example two of them are step sequencers). 
Why not? It has no reverse dependencies, according to [packdeps](http://packdeps.haskellers.com). It is indeed a bad name - a very general name for a very specific package.
When I was a total beginner, one of my first questions was "why does map only work on lists if it's actually fmap" ? I think it was a hindrance to start off with the idea that lists are somehow special compared to other functors. It prevents people from realizing how useful functors are.
I can't change the name (or at least I couldn't) once i've uploaded the package to hackage I think. Maybe this has changed with hackage 2?
All `pipes` requests are parameterize by an argument, so `pipes` forbids awaits by `Void`ing that argument. Michael is right about the monad instance. Changing the upstream input type breaks the type of bind. This is why I frequently say that you need an indexed monad for this sort of thing.
Or any other [project](http://projects.haskell.org) registered on the [Haskell community server](http://community.haskell.org/).
The same holds for Yield. Instead of Yield :: Pipe i o d m r -&gt; Maybe o -&gt; Step i o d m r i.e. Yield :: (Maybe ([o], d) -&gt; Step i o d m r) -&gt; Maybe o -&gt; Pipe i o d m r you can split it up into Yield :: Maybe o -&gt; Step i o d m r -&gt; ([o] -&gt; d -&gt; Step i Void Void m r) -&gt; Pipe i o d m r so you can not yield again after downstream has terminated.
I think there is a slight problem with `pipeReturn` because it discards the original `mr` leftovers passed to it. If I understand it correctly I think `pipeBind f return` is not the same as `f` (i.e. `pipeReturn` is not a right identity of `pipeBind`). Also, your `inject` function in the definition of `pipeBind` is an example of what I would call "adding memory" to a function's behavior and usually every time I found bugs in my own code it was because of functions like `inject`. I find that a solution that works well is to reorganize the data type so that all operators on that type require no history to make decisions for any given constructor. This makes the operator less stateful and easier to reason about (and usually produces the right laws).
the api documentation is auto-generated. does anyone know how we can get at it as well. without scraping the webpage, that is.
A few counter-arguments to Chris' utility packages being "Hackage material". This was originally written in reply to that person but since they are gone, here is my response to them: **This is addressed to the author of these new packages** 1. I'm not aware of other package repositories supporting this sort of "utility library" pollution. MissingH is a *big* package with lots of things and people might use it as a dependency for its swiss-army-knife qualities. Your libraries are small and have few dependencies - they could easily be included by `cp`ing them in and you get a win-win for you: you have no dependencies to manage and no responsibility to people on Hackage. (Alternatively, maintain a private repo and use `cabal sandbox add-source`?) So, are these libraries for just you, or for general consumption? 2. Right now you're the sole author of `texts` and can choose to evolve it as quickly or slowly as you want, and it compounds any existing cabal-hell problems if you decide to change something in a non-backwards compatible way or add additional dependencies. These are *your* utility libraries and I suspect if someone in the community asked you not to break something or complicate dependencies, you'd hesitate and say: well I wrote this and this is mine to change. Are you ready for that? Do you intend for other people to be consumers of these packages? 3. And that's your prerogative, no one should tell you that you can't do that. But pushing things to Hackage should come with a social responsibility to be a good citizen, and are you going to accept that responsibility in the stewardship of these names? Here's the thing that scares me: &gt; Mostly the fi function is everywhere, and I'm sick of redefining it. I'm not partial to this name at all. Theoretically you change your mind and rename `fi`, and you break half of Hackage because of the viral nature of dependencies and one or two big core libraries including your utility libraries. Oops.
I had fun building a prompt finalizer into `yieldMany`: finalized :: Pipe i o IO r finalized (Just r) = Pure r finalized Nothing = Yield finalized Nothing yieldMany :: [o] -&gt; Pipe i o IO r yieldMany (o:os) Nothing = Yield (yieldMany os) (Just o) yieldMany _ mr = M $ do putStrLn "Finalization" return $ finalized mr 
An alternative to Maybe for Yield and Await constructors would be to restrict the input and output types to Monoid, and then use mempty to signal termination. This is not as restrictive as it may sound, we're discussing streaming of data here and every stream is naturally a monoid. You can always wrap the items in singleton lists as a fallback. In performance-sensitive applications you'd be working with ByteString or Text, so there would be no need for the wrapping overhead. 
I agree with you, I think there are problems with those implementations. I *think* that there's a correct way to implement these functions (including `inject`), but I only briefly started checking the laws so far.
Then you need a `Eq` constraint too - and suddenly your type signatures get a lot uglier :/ I wrote a parsing library on top of `attoparsec` and in the process of generalizing it I am finding myself hitting very similar issues as `conduit` and `pipes` have with termination. I am generalizing it to support custom `Parser` types, e.g.: `Parser [HtmlToken] HtmlDocument`. I'll probably rewrite it on top of `conduit` or `pipes` once the termination/finalization story is complete, because right now I'm constrained by the `IResult` type given by `attoparsec` - to generalize termination I had to add exactly the above constraints (and actually a few more).
yes, it was me. i noticed that i could as well report it there. thank you for your commitment. other would have been happy to keep the hack in place.
I've played with this a little before. One issue that I encountered is that the way that Fay represents thunks as anonymous functions doesn't play nicely with Angular's dependency injection. I think that is probably the biggest hurdle that needs to be addressed.
Did you use the `-threaded` option when compiling? It only works properly with the threaded runtime. (Also I uploaded a new version of hmidi yesterday, which may or may not have fixed some subtle issue which either existed or not).
I'm not saying `A -&gt; (A -&gt; B)` is not an important perspective! We were discussing how meaningful a concept are functions of multiple arguments (especially of 0 argument). My point of view is that any function has exactly one argument. The curried form satisfies this by definition. I was arguing that the uncurried form also has one argument, which is a product space. I was also arguing that one should not confuse the value `A` with the function `1-&gt;A`, even though they are isomorphic.
Thanks, I'll take a look at that! I may not end up switching to use it - right now I should probably be more concerned with shipping than tweaking libraries to make them more generic (since right now I'm the only consumer). So as it stands, I think I'll wait a while for the `conduit`/`pipes` finalization story to settle down. Lastly, there is the problem that using `mempty` as a special case is uncomfortably close to null pointers as a special case. What if a parser wants to actually handle parsing a network packet or a request that contains no data? I can't think of a case off-hand, but it seems messy.
&gt; distinguish Foos from Bars in Haskell is exactly the same reason as it makes sense to distinguish Foos from Bars in mathematics too Yeah, I fullly agree. I was writing too much yesterday without thinking (writing is easier :). 
&gt; [...] However, both of these are exactly "one-argument functions" by your choice of terminology. [...] That was exactly my point.
We are supposed to now copy paste hundreds sets of options to every function definition just to make sure they accept the right type of arguments? :)) What about empty types? Every object i work with supposed to (and most fo the time does) have no options at all. What about types that have the same options? I have Height in Image, Page, Box. What about sets of options that are generated dynamically? Which btw IS the entire point of this exercise. How on earth your compiler supposed to know at **compile time** which keys the map will have? Can you illustrate each of these cases? Btw i am having a hard time running core.typed. Your code is accepted by clojure fine and runs fine. But every time i try to check-ns it i get the error in the repl: IllegalArgumentException No implementation of method: :immediate-dependencies of protocol: #'clojure.tools.namespace.dependency/DependencyGraph found for class: nil clojure.core/-cache-protocol-fn (core_deftype.clj:541) Here's my project file: (defproject typed-test/lein-template "0.1.0-SNAPSHOT" :description "FIXME: write description" :url "http://example.com/FIXME" :license {:name "Eclipse Public License" :url "http://www.eclipse.org/legal/epl-v10.html"} :dependencies [[org.clojure/clojure "1.5.1"] [org.clojure/core.typed "0.2.13"]] :source-paths ["src"] :eval-in-leiningen true) And here's the test file: (ns bla (:use clojure.core.typed)) (ann height-option [String &amp; {:height Number} -&gt; String]) (defn height-option [name &amp; {height :height}] (if-not height (str "I have no opinion on " name ".") (if (&lt; height 6) (str name " is short.") (str name " is tall.")))) (check-ns) 
I didn't know that. Nice!
That's similar to how iteratee deals with chunking. It's an interesting idea. However, in reality we probably wouldn't end up using `Maybe` in a final API anyway, and would instead signal termination by calling a separate function with a different type signature, so I'm not sure if this change would buy us a lot. But definitely worth considering.
Yes, I was talking about Mu. I don't think we have as many full time users as GHC, though (the other bits are mostly true). 
I want this. Also, Unit tests for js in yesod lol!
Nice explanation of mvcc [here](http://en.wikipedia.org/wiki/Multiversion_concurrency_control). mvcc looks like a way to implement stm for a database. http://en.wikipedia.org/wiki/Multiversion_concurrency_control
&gt;We are supposed to now copy paste hundreds sets of options to every function definition just to make sure they accept the right type of arguments? :)) You'd be defining those exactly once same as with Haskell top level annotations. I'm not sure why you would need to copy paste this anywhere... &gt;What about empty types? Every object i work with supposed to (and most fo the time does) have no options at all. What about reading core.typed docs yourself? Since you're obviously never going to use it, why are you pedantically insist on wasting my time looking syntax up for you? &gt;What about types that have the same options? I have Height in Image, Page, Box. What about them? You can't possibly imagine having an `:id` key for each type? &gt;What about sets of options that are generated dynamically? Which btw IS the entire point of this exercise. How on earth your compiler supposed to know at compile time which keys the map will have? Why would it need to, it can specify which keys it is **allowed** to have. Isn't it what you're trying to restrict here after all? &gt;Can you illustrate each of these cases? I could, but let's face it I'd just be wasting my time anyways. You win by attrition. :) I generally just run it from lein with the [lein-typed](https://github.com/frenchy64/lein-typed) plugin: (defproject typed-test/lein-template "0.1.0-SNAPSHOT" :description "FIXME: write description" :url "http://example.com/FIXME" :license {:name "Eclipse Public License" :url "http://www.eclipse.org/legal/epl-v10.html"} :dependencies [[org.clojure/clojure "1.5.1"] [org.clojure/core.typed "0.2.13"]] :plugins [[lein-typed "0.3.1"]] :core.typed {:check [blah]}) lein typed check 
&gt; Lastly, there is the problem that using mempty as a special case is uncomfortably &gt; close to null pointers as a special case. What if a parser wants to actually handle &gt; parsing a network packet or a request that contains no data? I can't think of a case &gt; off-hand, but it seems messy. I agree, which is why I gave up on the approach in SCC, at least on the consumer side. There is no such downside on the producer side, however. My current solution would correspond to something like this form of the Yield constructor: Yield :: Monoid o =&gt; o -&gt; (o, Step i o d m r) When the consumer side accepts a Yield with a chunk of data from the producer, if it decides to terminate it provides back the leftover part of the chunk. If the leftover is not empty, the producer knows the consumer has terminated. You could argue that it's possible for the consumer to consume the entire chunk and then terminate. That's true, but the only consequence is that the producer will attempt to yield the next chunk which will be handed back whole. That's harmless except from the performance point of view, but it doesn't happen often enough to matter. 
Yeah, that sounds good. I'll subscribe.
So now we are supposed to prove any bullshit claim that some half-baked one man project is an adequate substitute for entire haskell type system? We are supposed to waste our time pouring over unknown to us code to prove YOUR unsubstantiated claims? We are supposed to figure out why on earth the lib that you claim is rock solid gives me errors on the **supposed** usage? Who is wasting whose time here? It is your assertion, therefore the onus of proof lies on you. Which you miserably failed of course. 
Can't you add a opt-in filter for working links (If link reachable)?
Thanks, that was the last piece. tshow was hard to find since it's not known by hayoo or fpcomplete's hoogle. Got it working here.
Shachaf and Elliot know it well and Shachaf once showed it to me. The heart seems to be that lenses might be better thought of arising from the nature of profunctors rather than comonads.
Well, module names in some of these are far worse than package names
Alright, witch hunt's over. `bools`, `nums` have been dropped and deprecated (now they're free) — possibly deleted (including other, older packages of mine like kibro and css) if I can get hold of Duncan, `eithers` has been merged with `either-unwrap` and `either` into the `either` package (thanks, Edward! Awaiting response from Gregory Crosswhite to deprecate either-unwrap). `strings`, `lists` and `texts` are going to stick around, I have more stuff to add and polish to add. 
afair you can also inject dependencies explicitly.
Yes, but you need to reify all of Haskell (or the subset you're interested in) which can be tedious. But I now see what you mean and this sometimes works. A bit. I need to check out sunroof btw.
A witch hunt is defined to be "a rigorous campaign to round up or expose dissenters on the pretext of safeguarding the welfare of the public". Your package upload merely triggered this post and the discussion, but the underlying issue/question (which wasn't about you) remains unsolved, specifically how to deal with Hackage's package namespace, and what responsibilities and obligations an upload to Hackage shall result in, an Hackage social contract so to speak.
Don't worry. We still like you, Chris! Also, to be fair, I grabbed `errors`.
&gt;So now we are supposed to prove any bullshit claim that some half-baked one man project is an adequate substitute for entire haskell type system? Why do you feel the need to be so derisive about the project exactly. Can we simply discuss the actual functionality instead? &gt; We are supposed to waste our time pouring over unknown to us code to prove YOUR unsubstantiated claims? Nobody is asking you to pour over anybody's code. As you said you're happy with Haskell and you're not going to be using Clojure whether core.typed worked exactly like Haskell or not. My claim is that the problem you've solved using GADTs has other well known solutions. &gt;We are supposed to figure out why on earth the lib that you claim is rock solid gives me errors on the supposed usage? The fact that you can't figure out how to use a library that plenty of other people are happily using smells like PEBCAK to me. &gt;Who is wasting whose time here? The person who keeps shitting on tools without having used them and who isn't going to be using them? &gt;Which you miserably failed of course. By giving you the solution that does precisely what you want. 
Your function has quadratic time complexity and is too lazy. I could tell you why, but why don't you try to figure it out yourself first for a while, and let us know here if you get stuck.
Well what I mean is not that the profunctor fomulation is "better", but that the comonad formulation is "wrong". I'd suspect we can tell a deeper story that relates them somehow, so that the comonad formulation is actually interesting and correct, but just less rich in certain ways (or just rich in different ways).
I wrote [pianola](https://github.com/danidiaz/pianola) out of my frustration with some aspects of the [Marathon Swing GUI Testing Tool](http://marathontesting.com/), which employs JRuby for its test scripts. I like Ruby but I dread having to refactor a module used by many scripts. Of course unit testing can help with the refactoring, that's the Ruby way, but IMHO having unit tests of test scripts is a bit overkill! Besides the benefits of static typing for refactoring, I conjectured that Haskell's higher order functions and packages like lens and logict could prove useful in dealing with a GUI's component hierarchy. I use msgpack to connect to the Java agent. Right now pianola sorta works on a basic level, but it is pretty experimental and nowhere as fully featured as Marathon or other tools. It has never been used in production. Still, I think Haskell could be productively used for GUI testing (in combination with "native" agents).
Okay, the first problem, if I'm not wide off the mark, is it's not tail-recursive. Ideally the function would build up one list of bytes in a single stack frame until it encounters `\n`. I appreciate the hint! I'll get back to you...
Ruby is fucking excellent. It's my absolute favorite OO language. Haskell is fucking excellent. It's my absolute favorite FP language. Ruby is starting to borrow some great Haskell features: * Lazy data structures, as iterators * Type checking, as [contracts](http://egonschiele.github.io/contracts.ruby/) Ruby integrates really well with the command line, with Unix shell scripting, with shebangs and other POSIX things. I love the command line, and I'd much rather write my CLI programs in Ruby than in many Lisps, even though I prefer FP to OO. Lisps and MLs, especially Common Lisp and OCaml, do a crappy job supporting *basic* command line features, such as shebangs. Haskell's command line support is better than that of most FPLs, but not as good as Ruby's. Haskell and Ruby are both new to cloud computing. Ruby has [Celluloid](http://celluloid.io/), Haskell has dozens of great libraries for different kinds of parallelism, from `parMap` to `accelerate`. Cloud Haskell is getting better, but it's still not at the level of ease of use and error recovery of Erlang/OTP. As an imperative, OO-optional scripting language, Ruby is much easier to learn than Haskell. Haskell places additional constraints on code, more than even other "pure" functional programming languages, and these concepts must be learned before you can translate your goal into a running program. I think Haskell is definitely worth learning, but it really requires a solid grasp of functional programming fundamentals beforehand. If only there were a Lisp on top of Ruby as an intermediate learning language... The Ruby community is really, really good. The Haskell community is tiny, but also good. The new Cabal/Hackage Haskell package system is borrowing ideas from Bundler, so hopefully, writing and using Haskell packages will one day be as easy as writing and using Ruby libraries. I think one reason Ruby grew so quickly, and Node.js is growing so quickly, is that it's super super super SUPER easy for beginners to deploy new packages, to install them, and begin using them in other projects. Ruby has some features of a functional programming language, like lambdas, but they're somewhat cumbersome to use. For example, Ruby has Lambdas, Blocks, Procs, and methods, so there's a confusion/redundancy of tools for the same job. One critical distinction between Ruby and Python is that Ruby doesn't require parentheses `()` after method names to call methods. For imperative programming, this results in slightly shorter code. But for functional programming, Ruby makes it a bit harder to pass around functions. In Haskell, you get a nice compromise between brevity and semantic flexibility, as a result of the currying semantics and space (` `) based syntax. It's really nice to chain together function expressions into a larger expression and pass it around without accidentally applying it, without having to wrap things in silly wasteful boilerplate code. For the moment, Haskell has little support for interfacing (calling and being called by) other programming languages. You can export and import *data*, as you can in any language, but that doesn't really count. There's [Fay](https://github.com/faylang/fay/wiki), to compile Haskell to JavaScript, but it's still alpha. If you want a Haskell-like language specifically for cross programming language compatibility, Scala is as close as you can get in the JVM to Haskell, and F# is as close as you can get in .NET. I don't know of an RVM language besides Ruby, so yeah. Beg the Haskell mailing list for a JVM version, maybe "JHaskell" could then talk to JRuby, which could talk to Ruby.
In order for Fay to be useful, it should include bindings for Node.js and jQuery out of the box. Users shouldn't have to roll their own code for `console.write()`. What is this, assembler?
You can, by setting the $inject property on a function. However, IIRC, if you set it on a function that is a thunk, then that function may be replaced later by a different function that is the evaluated result, at which point you lose the function that has $inject set.
What can we read/watch to understand all the further topics about lenses that were not covered in this talk?
&gt; strings, lists and texts are going to stick around, I have more stuff to add and polish to add. Btw, could you please add version constraints in `lists.cabal` on `list-extras` and `split` in order to satisfy the PVP (since you leak their respective APIs by fully re-exporting their modules)? 
Thank you, good sir! [hruby](http://hackage.haskell.org/package/hruby) looks cool.
I am a sysadmin, and I it seems that these days most of the useful tools are written in Ruby, as *Ruby is for devops*. There are also the third party rail applications that need supporting. I have found that when it comes to production, it is terrible : * takes tons of CPU for the most mundane tasks, and long lived process will consume all your memory. The recommended way to run long lived process is to restart them periodically. Yes. * the joy of gems and bundler (Haskell isn't that great on that front) * terrible security record, leading to frequent *manual* upgrades of applications and gems (as they are never packaged) * the community is supposed to be great, but I never managed to get help on the #ruby channel (probably because I had questions on internals) We are huge Puppet users at my shop. This tool is brilliant, and I don't know how I could work without it. But it is also extremely slow, which led to the creation of the [language-puppet](http://lpuppet.banquise.net/) package. I can compile the catalogs of hundreds of nodes and run tests on them in less time than it takes to run a simple unit test on a single module, using the Ruby libraries. This package embeds a [Ruby interpreter](http://hackage.haskell.org/package/hruby) for the parts that can't be replaced (ERB templates). We also used [Logstash](http://logstash.net/), but this has been [replaced](http://hackage.haskell.org/package/hslogstash) by fully native binaries. I chose Haskell because I wanted to learn it exactly at the time I was frustrated with the speed of Puppet. It turned out to be an excellent choice, as : * I am incredibly productive with it. I had a syntax checker for the Puppet language finished in a few days, and I had absolutely no experience with Haskell. I recently rewrote the parser from scratch in a few hours. I completely replaced Logstash (just the features I used obviously) in 4 hours. * Most libraries are *very* well designed.. * Even though I didn't care about performance (I was not equipped to reason about it anyway), my first program turned out to be ten times faster than the Ruby version, and consumed about 8 times less memory. * There is always something to learn, so it is never dull. I would not recommend it for production critical applications compared to something that runs on the JVM. The JVM has built-in monitoring and debugging features that are hard to live without.
Well, it is **quite rough** to use :)
The XML/HTML library was hexpat-lens. I can post a more full example later when I'm not on my phone.
Fay has two ways to let js call back into it (really it's the same thing, but different use cases). When you use the FFI Fay functions are encoded into normal javascript functions, that's why we can do `ajax :: Text -&gt; (Automatic a -&gt; Fay ()) -&gt; Fay`. Fay 0.18 also includes "strictness wrappers" which creates new version of all top level definitions in a module so you can call them as normal functions from js. I don't know how to create these bindings in angular, but I'm guessing one of these methods would be applicable? 
Maybe you should use the strict version of a function instead? [See my other comment](http://www.reddit.com/r/haskell/comments/1o4x3m/call_to_action_fay_ffi_bindings_for_angularjs/ccp6e4u) and [this wiki entry](https://github.com/faylang/fay/wiki/Calling-Fay-from-JavaScript). 
The fact that there are laws associated with lenses was not mentioned in the talk and a few examples of lenses didn't satisfy the required laws.
I will once I've decided what constraints are appropriate.
I don't think HNF works because compare: * (\x -&gt; if x then True else fix id) * (\x -&gt; if x then fix id else fix id) See also: http://cstheory.stackexchange.com/questions/19165/is-eta-equivalence-for-functions-compatiable-with-haskells-seq-operation
My company is transitioning all of our code from ruby to haskell. We do building automation for large commercial buildings. &gt; Where did Ruby (with or without Rails) leave you wanting more? * Ruby is slow (relatively). * I'd take cabal hell over messing with rvm/rbenv any day. * Very uneven quality in ruby libraries. * The standard code-maintenance and refactoring difficulties that plague all dynamic languages (even with large tests suites). &gt; Why Haskell? * We do a lot of interfacing with industrial binary protocols. It's certainly possible to do that in ruby, but there's nothing that compares to `cereal` in the ruby world. * Writing servers with `pipes` is a joy compared to ruby's `eventmachine`. * *Much, much, much* higher quality libraries overall. &gt; How did you, or will you, interface the Haskell components to the Ruby components? And what tools or packages or frameworks did you use to facilitate this, if any, on both the Ruby side and the Haskell side? Via sockets - `eventmachine` on the ruby side and `pipes` with `pipes-network` and `aeson` on the haskell side. &gt; What would make it easier? Easier compilation on ARM. We're still using qemu, and there are still some rough edges in the build process. `cabal sandbox` solved most of the other inconveniences.
&gt; Can’t we just get the essential core ideas out of lens and put that in a library? Sure. It is called [lens-family](http://hackage.haskell.org/package/lens-family). I'm planing on getting a new release done this long weekend.
Any plans for Traversals in there?
Yes. Hopefully I will have time this weekend for that too.
As we discussed on IRC, I think my claim is still likely to be true for the untyped lambda calculus, since it means we can't make assumptions about what a variable is allowed to be instantiated to.
Ah, that's good to see. I haven't tried Fay in a few months, so I'm sure that it has come a long ways since then.
Why it shows up as Lens.Family2 ? @.@
Yes, you're off the mark :-)
sudo apt-get install haskell-platform http://www.haskell.org/platform/linux.html
I typed that in and got this: "E: Invalid operation haskell-platform" I downloaded the source tarball from the website and tried it again. Same message. Not sure if I'm missing something?
The normal way to install programs on Ubuntu is to use apt-get. Like eccstartup said, all you need is to type the following command: sudo apt-get install haskell-platform The instructions you have linked to were a workaround for Ubuntu 13.04, on which the above command didn't work. Since you have Ubuntu 13.10, you don't need to compile ghc from source. Welcome to Linux! Feel free to ask more questions, no need to bribe us with Reddit upgrades :)
Actually, on Linux, you don't need to download source tarballs in order to install stuff! If the apt-get command succeeds, it will download and install packages for you. Did you type "sudo apt-get haskell-platform" instead of "sudo apt-get *install* haskell-platform", by any chance?
&gt;In my opinion lens doesn’t have enough focus &lt;/thread&gt;
So lens-prelude? 
Actually i installed 7.6.3 on several of my ubuntu 13.04 machines. It is fairly easy. Just run this script: https://github.com/chrisprobst/ubuntu-raring-haskell/blob/master/install.sh This is not from source, it uses compiled binaries so entire process takes no more than a few minutes. 
Here's essentially the code that I put on the mailing list when SPJ asked for examples p ^.. _HTML' . to allNodes . traverse . named "a" . traverse . ix "href" . filtered isLocal . to trimSpaces where `p` is a `ByteString`. `_HTML'` is a `Prism` that invokes the Hexpat Tagsoup parser, `allNodes` is essentially just `universeOf children` but goes through the `Plated` instance I wrote for `hexpat`'s `NodeG` type. You also have an example of a `Fold` in `filtered` and `named`, a `Traversal` in `traverse` and an indexed `Traversal` in `ix` which treats the attribute map of an XML node as a finite map on attribute names. There's also an `At` instance, but `Ixed` is easier to use here. (*Edit*: I should add that `(^..)` means that I'm retrieving all of the matches that this lens ends up touching. That's how I end up pulling all of the anchor hrefs.)
Here are some downloadable "installer" files that you can load into software centre to give you GHC-7.6.3 on raring* http://launchpadlibrarian.net/149255979/ghc_7.6.3-4_amd64.deb http://launchpadlibrarian.net/149255994/ghc-dynamic_7.6.3-4_amd64.deb http://launchpadlibrarian.net/149255991/ghc-prof_7.6.3-4_amd64.deb With these, you should be able to get the latest haskell-platform. https://launchpad.net/ubuntu/+source/haskell-platform/2013.2.0.0/+build/4721449/+files/haskell-platform_2013.2.0.0_all.deb https://launchpad.net/ubuntu/+source/haskell-platform/2013.2.0.0/+build/4721449/+files/haskell-platform-doc_2013.2.0.0_all.deb https://launchpad.net/ubuntu/+source/haskell-platform/2013.2.0.0/+build/4721449/+files/haskell-platform-prof_2013.2.0.0_all.deb *technically they are built for saucy, but they should still work.
I can admit that the lens library is a bit intimidating to a newcomer. I've watched a handful of videos on it, I understand the `Functor` magic, but the actual implementation is significantly more complex, and the operator-dense API has a bit of a learning curve.
Shachaf and Elliott both despise `Conjoined` and give me crap about it on a regular basis, so I am very well aware that it is a wart. Conjoined is literally everything we can say about `Indexed i` and `(-&gt;)` balled up so we can implement the combinators in such a way that they work with both indexed and non-indexed lenses. It has a more principled explanation in terms of profunctors that are both representable and corepresentable, so you can move the contents of the profunctor to either side of an `(-&gt;)`. Alternately/Equivalently they are corepresentable/representable by a pair of adjoint functors. This is important because we need to be able to slop the contents of input profunctor to one side or the other to implement various traversals in a way that makes them compatible with indexing. This let us cut out 30+ combinator names and let us share one combinator for the indexed and unindexed versions of most operations. Along the way we were able to shrink the surface of the API, and find and fix a number of small bugs where we'd fixed things in one version or the other but not both. These two instance are isomorphic to any other possible (correct) instance of this class. We could simplify that class, but then every user would wind up paying a performance penalty, as we dance around trying to do things with a more minimal API just for the aesthetics of a class for which nobody else will ever write an instance! We got by without `Conjoined` for a long time, but adding it let us speed things by more than a factor of 2 in many situations where an indexed lens was being used in a situation where the index wasn't being used. `lens` tries to make sure 1.) that everything works with everything else 2.) that everything performs as well as possible without `Conjoined` we sacrifice one of those tenets, and are forced to make a bunch of second class combinators to boot. Without it in the limit we'd be forced to doubles the number of names of either the combinators that make lenses or the combinators that consume them. I don't like the class. However, it is literally everything I can say about `Indexed i` and `(-&gt;)`, the fact that all of those superclasses are there are consequences of a surprising amount of theory saying all those properties must hold. One of the reasons the class is relatively underdocumented is because I hold out a glimmer of hope that some day we can kill it and get the performance another way.
Re: splitting out the core ideas of lens: https://github.com/ekmett/lens/wiki/faq#wiki-lens-core It doesn't save `lens` a whole lot of dependencies to split out a common core and instead makes it harder to use. It would necessarily loses the ad hoc overloading classes that let it make efficient use of the namespace as there would be no non-orphan place for the instances to live. Over the last few versions `lens` has been trying to shed parts of its core in a very different way, by making it so you can now define a `Lens`, `Getter`, `Iso`, `Prism`, `Review`, `Equality`, `Fold` or `Traversal` and the index-preserving variants of these things without incurring a dependency on the `lens` package at all, using only more or less Haskell 98 compatible dependencies. Right now you need to incur a dependency on `lens` only to define something indexed or a `Setter`, or to instantiate the classes we use for ad hoc overloading... that couldn't usefully exist if we shed the dependencies on core packages. Rather than focus on making `lens` itself smaller, we've been focusing on making it so you can supply lenses without incurring any dependency on it at all. We're not all the way there, but as we find principled ways for them to be derived from fundamental concepts that already exist elsewhere, we adopt them. Re: dropping the theory: The only reason we were able to generalize over so many things with `lens` is because of all that "corepresentable distributive indexed corep" stuff, so no, I'd rather not part with all the theory that makes the high end of the library possible. I for one use the 'high end' of lens a lot. Others use the indexed lens functionality for working with maps and containers and tables. The `tables` package without indexed traversals would be crippled. Lens as it is now built with the theory to back it up is much more powerful and flexible than the pile of ad hoc crap I started with. All this category theoretic mumbo jumbo gives me laws to reason about the correctness of the code. It gives those of us working on it in the #haskell-lens channel a common vocabulary to talk about the implementation. It gives us tools to understand why and how the pieces fit together the way they do, and it tells us when what we want to do is impossible and why. Dualizing lenses is how we found Prisms after I spent weeks fumbling around looking for something like them intuitively and failing. Dualizing indexing provides one path by which we can recover "failing" prisms. We haven't done it yet, but Elliott has some promising ideas. I "categorically" refuse to turn off the flashlight I'm using to find my way around in the dark. =P We're not done searching for better abstractions. Last time I split up a big package before I was done working on it was `category-extras`. The maintenance effort of maintaining the diaspora of `category-extras` almost made me want to give up on writing Haskell in the large, before I figured out ways to make it slightly more manageable, but more importantly and less personally, it also all but ground development on all of the packages I split out from `category-extras` to an absolute halt. `category-extras` was a living vibrant thing that was changing actively. The rate of change in the split-out packages is _vastly_ reduced. Few people know how to find anything but the most basic stuff in its scattered ashes, and I can't readily consolidate or split it further to fix that.
I think that there are a lot of people who want the benefits of Haskell but are put off by the *endless crusade* of mathematically framing every little thing in Haskell. These people don't give a rat's ass about Category theory and are being driven away from the language as a whole because the popular discussion is constantly about some new cofrankenmorphism that everybody is gushing over. Well anyway I sometimes find myself in that camp. Simon's recent talk really helped me start to like lenses, mostly because he was able to approach them as "this is just code, guys". It was only a secondary thought that there was all of this great mathematical elegance; the fact that lens composition was just function composition was ~sort of neat~ but not really the big picture of why the library is great.
Except there you get a bunch of Haskell 98 type aliases that make no sense and require stylized usage. =/
Right. While not a heavy lens user myself, I feel like it is akin to certain other things in the Haskell world in that it has a practical component, which is largely settled, and increasingly clear thanks to tutorials including SPJ's, and it has a richer experimental component where interesting things are still being worked out, and one can generally ignore the experimental component and just work with the basics. Ideally I'd like to see the experimental component worked out fully, documented fully, and with nice tutorials as well. But I buy that getting there means keeping everything in the same package-space for now. The category-extras diaspora was very painful I know, but I think its ultimately been to the good, and we do stand the chance of getting more of the basic pieces into more common usage (and indeed some of them, like the comonad and streams bits, the representable/memo bits, etc., as far as I know _are_ passing into such usage). The one place I think it may have gone too far is the H98-compat splits. Retrospectively H98 compat is less important by the month. It would be nice to have a new standard between H98/2010 and full-blast latest GHC, but I think at this point we need more dust to shake out and more of a story to develop with the "new gen" haskell compilers. But even in that regard we're seeing more of "GHC as a Frontend" and to my knowledge those compilers not going that way often, for practical purposes, lean towards "ignore what you don't implement" instead of "break on what you don't implement".
It seems like in this case 'community pressure' is the right policy. In general I think we should make a category for util packages and tend to hide it by default, now that we have a more flexible hackage. We could also enforce/request-at-least that people namespace util packages with `util` in the name.
was wondering where you did get 13.10 from :D (AFAIK 13.10 is not released yet)
You can do that, yes. Eventually you will want to do it anyway, so you might as well do it before you get all comfy. If you poke around the system updates menu stuff you'll find a button to upgrade to the next full release.
As edward's post discusses, the point is not that "everyone" should have to learn these details. However, using these details is what lets (some of) us begin to build and reason about new abstractions in the way in which we are most comfortable and feel is appropriately general. I think we strike a nice balance between practical and theoretical in the community, but we probably need to be better at messaging that you _really can_ ignore the theoretical until/unless you get the itch, and you'll be fine. Some amazing Haskell programmers who have contributed much serious work don't know a comonad morphism from a toyota camry. This isn't because they _couldn't_ learn, its just that they have decided they want to spend their time learning _other_ things instead. I feel the same way about certain elements of parallel systems programming. I'm glad someone is cranking away to make certain types of things amazing, and glad they learned all the gorey details. I might enjoy letting them ramble on about this at dinner while I smile and find out some interesting tidbits, but I'm not planning to learn those details in that depth. Instead, I'll eventually just reap the benefits!
I think about 10-20% of the category-extras was a good idea. The rest of it was probably done prematurely and is now set in stone. Things like `comonad`, `void`, `bifunctors`, those are all very well contained and stable packages. Things like `representable-functors` on the other hand, should likely have stayed embedded. The maintenance overhead of them in their current state almost exceeds their relative utility. I do agree that the Haskell 98 compatibility guideline I used to use is no longer the best rubric. I'd just as soon fold many of the `foo` and `foo-extras` packages together, and may well do so on a case-by-case basis now that its getting easier to deprecate with Hackage 2.
The talk I gave in New York goes a bit deeper, and with Simon's introduction it may go down a bit easier. http://www.youtube.com/watch?v=cefnmjtAolY There is also the lens wiki on github, which if you go up to Pages has a bunch of user contributed content about the ins and outs of different parts of `lens`. https://github.com/ekmett/lens/wiki Finally, http://lens.github.com has an RSS feed of articles folks have written about lens, but we haven't updated it in months. It also has an "operationally minded" short tutorial. That may complement Simon's explanation of the guts.
Well, its more that when we take: `s -&gt; Store a s` and go to dualize it you want to get something like `State a s -&gt; s`, but that isn't the "right" dual to do anything with. =/ Rather what we should do is go back to the parts that we used to get the store in the first place. (s -&gt; a) and (s -&gt; b -&gt; t) uncurrying the latter to get (s, b) -&gt; t and dualize those arrows to get (after alpha renaming everything) (b -&gt; t) and (s -&gt; Either t a). This gives rise to the formulation of a `Prism` we have. The laws for it precisely coincide with what you'd expect should hold for a `colens`, but the comonad coalgebra we had in the first place is nowhere in sight.
Define "explain so well". Here's my POV from a Haskell-learner, because I'm sad to see my opinion not yet represented here. :-) I find his talks to be rather hard to follow. I thought this would teach me how to use Lens and why they're awesome, but it's like he's speaking Chinese. I just wish he used layman's terms instead of the complex language inside his head. I spend hours reviewing his talks to fill in all the assumptions I'm missing. Does anybody else have trouble following him? Just me? Having said that, I love SPJ's talks, simply because I don't often see the unfiltered, yet humble, thoughts of a brilliant mind. It's refreshing. Edit: Just want to express thanks, for not talking down on the outlier's opinion. I was quite worried about posting! What a civil group, you Haskellers. :-)
The time lenses he used were examples of 'improper lenses' that violate the lens laws in one dirction. you can put back what you get without causing a change, but you don't get back what you put in all the time. This gives us something like a one-sided relative inverse, rather than a full inverse.
You should relax those dependencies so I can actually install this :)
Did you do **sudo apt-get update**, before you do that?
Interestingly enough the implementation of lens was first written predominantly because I disliked the name of that module so much that I embedded my own little 30 line lens implementation in a physics package I was working on, rather than use it.
If I mastered the lens package and used it extensively in my coding I am afraid the maintainability and understandably of my programs would actually go down -- a lot --- since I'd be the only person for miles who understands what's going on in the code and thus others would have to spend far too much time piecing together enough understanding to make modifications to the code base. That's not a win in any way shape or form and the exact opposite of why I like Haskell in the first place. That said, I totally appreciate the hard work and effort of the very talented and imaginative individuals who do the community the favor of creating these wonderful packages and letting others use them free charge and/or constraint.
It has no reverse dependencies. Why not just change it?
I think you probably misunderstood what I meant (my own fault). There is nothing wrong with using Category composition. There is even a class for it in base, everyone knows it and it doesn't add any cognitive overhead. Also, I don't see any problems with using category theory to shape your ideas and libraries. It might help you find the right abstractions. It's all about interface vs implementation. I just don't want so see it if the only thing I want to is modify something deep inside a data structure. It's cognitive overhead, it's very low level and it's esthetically unpleasing.
great explanation！
Stop is early termination, in the same way that a `Nothing` performs early termination for the `Maybe` monad. It affects monadic composition, not fusion. You can see the fusion function in the conduit branch [here](https://github.com/snoyberg/conduit/blob/022e69e2d817c29d3a36265443e56bc27a3c7515/conduit/Data/Conduit/Internal/Composition.hs#L67). The article also has a fuse function, you just need to use the arrow to the left to unfold the code.
What about this one? {-# LANGUAGE OverloadedStrings #-} import Data.ByteString.Char8 (ByteString) import qualified Data.ByteString.Char8 as B import Control.Monad (forever) import Pipes import qualified Pipes.Prelude as P recv :: Int -&gt; IO ByteString recv n = fmap (B.pack . (:[])) getChar serialReceiver :: Producer ByteString IO () serialReceiver = forever $ do lift (recv 1) &gt;&gt;= yield concatStream = P.fold (\acc x -&gt; x:acc) ["\n"] (B.reverse . B.concat) recvLn = concatStream $ serialReceiver &gt;-&gt; P.takeWhile (/="\n")
Hi. I don't have much experience with the lens library, can you compare lens-based parsing/modification of XML AST to the arrow-based approach of hxt for example?
I'm going to express an even more extreme viewpoint: not only do I not care for lens, I don't particularly care for any lens/record libraries. The only part that I regularly find use for is TH-deriving setters. I'm not entirely sure why this is, maybe it's an artifact of the type of coding I usually do. Maybe if I actually used lens more I would like it. I do regularly work with a lens-equipped codebase however, so maybe not. But I'm really happy to predominantly program in a language where something like lens can be written as a library.
Hm, I am getting 404 on that link 
Partially because Russell and I disagree about what a minimal lens library should be and it is his package.
I used to think I wanted a simple lens library. Then I threw the doors open to everyone to tug on it in different directions and I found that the simple library I wanted wasn't so simple any more and I let it reshape itself to meet everyone's contradictory demands. Lens has literally dozens of different related types of lens-likes that all interoperate cleanly. Basically every point in that design space is inhabited for a reason. We actually use all of those niggling fine-grained distinctions. Before `lens`, every other lens package tried the 'simple clean interface', even my own `data-lens`. `lens` is more or less me asking the question "what if it isn't that simple?" It is also a bit of a sociological experiment in trying to see what happens if I don't try to hide the complexity of an API from an end-user, but instead try to expose parts that shift smoothly under the surface in such a way that they always do the right thing for deep reasons. Almost every other library I've ever written has acceded to the design principles you espouse here. `lens` does not. Yet it is also easily ten times more popular with its users than any of the designs that I built that did follow those principles -- including `data-lens`! Consequently, rather than change it, I'm trying to see if I should let it change me, and if I should modify the principles I'm using elsewhere and learn from it instead. I'm not sure the answer is yes, but I'm not ready to cut the experiment short. For example, one observation I can give is that by not hiding all the implementation details behind a carefully crafted facade, I wind up with a lot more users with intuition for how the pieces fit together and who are willing to pitch in to craft new pieces that fit nicely with the others. Most of the projects you named above all wind up being single maintainer affairs, largely because the barrier to entry to move from consumer to producer of the abstraction is much higher. Does this generalize to other library designs? Not sure. It makes a lot of people uncomfortable, including me, but it does seem to be effective. With more people familiar with the internals, more people can help others on-board the ideas.
That's right, this change will be guarded by a pragma, so it shouldn't break anything. I implemented the OverloadedRecordFields extension as a GSOC project; it's not in HEAD right now because of the impending release of GHC 7.8, but it should go in after the release. Then we'll have plenty of time to tinker with the design before the release of 7.10. Edward has been explaining it a bit more [over on this thread](http://www.reddit.com/r/haskell/comments/1o08sz/another_status_update_on_ghc_maintenance/).
That's totally fine and I think that you did a great job with lens. It's a very valuable addition to the Haskell ecosystem and worth the exploration. Most of the argumentation in favor of all the design choices are hard to counter. I just personally don't hope that library will be at the core of future Haskell. I see a similar kind of development with `pipes`/`conduit`, they solve a *seemingly small* problem with a tons of machinery. It doesn't fit my view of Haskell that attracted me so much in the beginning. And sure, the whole group of users together use all of the library, but I'd rather see a core package that contains a least common denominator that everyone can safely use.
Oh, of course. That wouldn't result in quadratic complexity, but `B.append` being O(_n_) would. Once we hit the base case we spend _n_ time appending each of _n_ characters. As for 'too lazy', I'm still working on that...
Hi, in case you missed it, I finally released the code: * [reddit post](http://www.reddit.com/r/haskell/comments/1o36p9/ann_launchpadcontrol001/) * [hackage link](http://hackage.haskell.org/package/launchpad-control-0.0.1.0)
The 2 stands for "Rank 2 Polymorphisms" maybe?
This is my first `pipes` program =) I've heard it is cool for streaming, so I tried it. The only problem code is `concatStream`. I think it is not much more efficient than multiple `B.append`-s. That is ``` concatStream = P.fold B.append "" id ``` may have save complexity
Nice job! Need a twitter client in Haskell too.
We `INLINE` very aggressively inside lens and use `unsafeCoerce` in ways that should make me feel really bad, but which are actually oddly satisfying to make this work out.
If you keep in mind the distinction between host language (Haskell) and embedded language (particular monad), the inability to use the full host language in the embedded language makes sense.
I fully agree with the content of the post. However, I realized that on a bit longer time horizon, we need a solution to be able to smoothly move between the simple and concrete, and the general and abstract. Both has advantages and disadvantages, and in each situation one would like to choose a different trade-off. With my math hat on, I really like all the beautiful category theory-motivated abstractions (though I don't really fancy those very complex abstractions with 8 type parameters...). But with my programmer hat on, I'm all for KISS and absolutely minimal dependencies... In any case, just 2 days after learning about them, I think that van Laarhoven lenses should be probably in the base.
It's not really an "implementation detail" so much as a "design detail", for lack of a better word. The category theory is *not* integral to how the library is implemented, at all; rather, it is integral to how the library was *designed*. Also, the category theory exposes the underlying structure behind the abstractions. You could get rid of it, but you would just be throwing away otherwise valuable information.
Could you open this up a bit? I think I can follow it quite well, but I can't understand what does the 'id' parameter to P.fold actually do.
What complex language are you thinking of, exactly? I suspect it's just a matter of having a bit more experience with Haskell and picking up the basic vocabulary: he doesn't use any particularly outlandish terms. 
ticket here http://ghc.haskell.org/trac/ghc/ticket/3144 (about a much older version) says: &gt; Fixed by installing libffi5 I had an inordinate amount of pain trying to get a working haskell install in 13.04 and my fix essentially consisted of apt-get'ing all of the individual packages that comprise the haskell platform. 
Haskell platform is borked on 13.04 AFAIK due to the ghc version being 7.6.2. I had to install all of the haskell platform packages separately. These are: - ghc - alex - cabal-install - happy - libghc-cgi-dev - libghc-fgl-dev - libghc-glut-dev - libghc-haskell-src-dev - libghc-html-dev - libghc-http-dev - libghc-hunit-dev - libghc-mtl-dev - libghc-network-dev - libghc-opengl-dev - libghc-parallel-dev - libghc-parsec3-dev - libghc-quickcheck2-dev - libghc-regex-base-dev - libghc-regex-compat-dev - libghc-regex-posix-dev - libghc-stm-dev - libghc-syb-dev - libghc-text-dev - libghc-transformers-dev - libghc-xhtml-dev - libghc-zlib-dev - ghc-prof - libghc-cgi-prof - libghc-fgl-prof - libghc-glut-prof - libghc-haskell-src-prof - libghc-html-prof - libghc-http-prof - libghc-hunit-prof - libghc-mtl-prof - libghc-network-prof - libghc-opengl-prof - libghc-parallel-prof - libghc-parsec3-prof - libghc-quickcheck2-prof - libghc-regex-base-prof - libghc-regex-compat-prof - libghc-regex-posix-prof - libghc-stm-prof - libghc-syb-prof - libghc-text-prof - libghc-transformers-prof - libghc-xhtml-prof - libghc-zlib-prof I have them in and ansible script (hence the formatting) in theory when the platform is fixed
SPJ talks very quickly and doesn't always explain his code slides in-depth. I feel like a somewhat confident Haskell user, and even I have to pause the video and toy in my mind/on paper/with ghci with the code he shows to convince myself that what he's saying is true. So in a sense I share your experience, but at the same time I think that's an intentional tradeoff between talk length and information density.
http://lens.github.io/
Yeah, I guessed that by reading your discussions on #haskell-lens (been hanging around quite early on), but never bothered to actually look at the core for lens code. You did a good job guys, I'll look into your code to steal some tricks sometimes :-)
Merge everything into `categories`!
I understood this proposal as a complement to Applicative becoming the superclass of Monad. When someone uses do-notation where an Applicative interpretation suffices, the proposed mechanism allows the compiler to translate it using `pure`, `&lt;$&gt;`, `&lt;*&gt;` (and `*&gt;`?) rather than `return`, `&gt;&gt;=` and `&gt;&gt;`.
&gt; But I'm really happy to predominantly program in a language where something like lens can be written as a library. Great point.
I think they're ugly, annoying to type, difficult to edit, read (have to remember the precedences) and indent/format. See also: [idiom brackets](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/idiom.html), and [a TH implementation](http://hackage.haskell.org/package/applicative-quoters-0.1.0.8/docs/Control-Applicative-QQ-Idiom.html), which unfortunately must rely on HSE for parsing.
&gt; As a user, I do not care what the types look like, because as lens shows - you can hide that behind type synonyms so I just have one name for something that has various constraints. I think one problem is, that the error messages you're getting aren't really helpful. One nice property of Haskell is, that the types are telling and helpful, and this just isn't IMHO valid for the newcomer to the lens library. I really like the power of the lens library, and after playing a bit around with it I have to admit, that it's just damn useful. But the errors I'm getting aren't helping, I first had to understand some concepts - implementation details - of the lens library to be able to handle the errors. Perhaps that's just the case for powerful tools. A lot of people dislike git, because you need to understand some of it's implementation details, to be able to effectively use it. But I quite like it, because after knowing the details, it's a very powerful, useful and in a way even "easy" to handle tool. 
IMHO splitting out a lens core is not about dependencies or language extensions. It is about ease of use. It is saying: this is the stuff you should learn first, once you're comfortable with this you will find your way in the big lens package. See it like a big airport: huge and complex, but if you design it right, it is very clear where the entrance is, and where to go next.
I haven't cared to much for lenses up to two days ago, but SPJ's talk changed that really fast; now I'm a believer.
One can always use arrow sugar with applicatives when working with the usual combinators gets too awkward, so I don't think this is worth the extra complexity. I would rather see arrow support improved and/or generalised, than specialised to static arrows.
Would an Applicative `let` notation be better? e.g. let x &lt;- expr1 y &lt;- expr2 z &lt;- expr3 in x*y + y*z + z*x Advantages: 1. does not require the compiler to magically spot `return`/`pure` in the final line 2. allows the programmer to be certain that Monad is not being used in this context (`do` notation leaves the possibility that the programmer intended a particular block to use Applicative but inadvertantly referenced an earlier binding when producing a later binding) Disadvantages: 1. beginners already have trouble working out when to use `let x = expr` -vs- `x &lt;- expr` in a `do` block; now they won't know whether to use `x = expr` or `x &lt;- expr` in a `let` block either 2. order of bindings is insignificant in traditional `let` blocks, but is important in Applicative `let` blocks, possibly leading to confusion 3. not immediately obvious whether we can include both `&lt;-` bindings and `=` bindings in the same `let` block; perhaps an initial set of `=` bindings which can be seen by the entire block, then the `&lt;-` bindings, then a final set of `=` bindings which can reference all names bound in the block?
Strange, I think they're pretty, nice to type (I import Control.Applicative to use &lt;$&gt; instead of fmap), edit and read. I'm not sure about the formatting, but I prefer that to formatting do. I also like how we get an (IMO) good syntax without having a special case.
It helps if you have a math background. Then things like functors are second nature to you and his discussion of the isomorphism between the record style Lens and the one given by forall f . (a -&gt; f a) -&gt; b -&gt; f b will be easier to understand, simply because using the identity functor or constant functor are "obvious" tricks to you. What I find amazing is that all those notions are expressible in Haskell in such a natural way. 
I think advantage 1 is *very* important, but other than that, I don't really like it. Idiom brackets (or monad/applicative comprehensions) would be better.
It's not only about syntax, but also for optimizations (like geezusfreeek says). Simon Marlow's talk on his work at Facebook nicely illustrates this. I his EDSL a monadic bind means perform two network request sequentially, an applicative apply means perform two request in parallel. His EDSL users aren't very proficient Haskell users and don't know anything about applicatives or idioms. They just want to write more or less sequential/imperative code. Which ends up being `do`-blocks that translate to binds. There is currently no way to optimize multiple binds that don't require each others output as input into applicative combinators. Meaning meaningless sequentialization of request that could otherwise be done in parallel. This extension will magically fix this issue. That sounds like a big win.
[Here you go](http://releases.ubuntu.com/saucy/), have a nice day.
&gt; But why does Conjoined need to be exported in Control.Lens.Indexed, can't it just stay hidden in Control.Lens.Internal.Indexed? It shows up in the public API for functions. Giving users combinators that they can put together but not giving them the capability to write the signatures for the composition is poor sport, so we need to export at least the typeclass. We used to do just that for almost all of the helper classes we used to need. Nowadays almost all of them are basically stock classes: `Profunctor`, `Bifunctor`, `Functor`, `Contravariant`, but enough performance minded users actually wind up using `conjoined` enough that we decided to export the method(s) as well. I don't have a terribly strong opinion in favor of exporting the `conjoined` method itself from `Control.Lens`, and could be convinced to do otherwise. The alternative is of course, just exporting just the class name, and making a user fish around for how to use the thing that will make their code 2x faster. We did this for a while, too. It was confusing for users, who have enough essential complexity to deal with that adding the incidental complexity of discovering that we decided to hide 2 methods from them but give them the class painful. At this point the decision is between randomly hiding a method folks are using and making them run around and add an extra import just to make me feel better about how clean the API is, or doing nothing in which case all I have to do is finish writing this reply. ;) &gt; Also, I don't think the class context in (p ~ (-&gt;) =&gt; q (a -&gt; b) r) does anything, right? That absolutely does something. Try compiling without it. ;) I first discovered this was legal syntax when I was working on `constraints`, which needed to use a very similar type. (\\) :: p =&gt; (p :- q) -&gt; (q =&gt; r) -&gt; r But looking at `conjoined`: conjoined :: Conjoined p =&gt; (p ~ (-&gt;) =&gt; q (a -&gt; b) r) -&gt; q (p a b) r -&gt; q (p a b) r That says give me two things, one can assume `p` is `(-&gt;)`, the other cannot. That is clearer if we substitute p in for (-&gt;) in that signature: conjoined :: Conjoined p =&gt; (p ~ (-&gt;) =&gt; q (p a b) r) -&gt; q (p a b) r -&gt; q (p a b) r "Legally" conjoined x x = x But you can also use `conjoined` to special case the handling of a `combinator` (or `lens`) when it is fed something non-indexed or the combinator doesn't need the index.
I'm all for ways to restructure the contents of the library into a form that make it more approachable. If you can come up with a patch that makes the organization of the library more effective for users finding the content, I'm all for it. Right now it is organized in a fashion that makes sense from the standpoint of the internals, and if you can think through the lattice of types to know where something should belong if you thought through the minimal set of dependencies. I freely admit this doesn't expose things to the end user in the most incremental way. ;) With 3.10 we're finally removing the old `Control.Lens.Simple` as the deprecation period for it will have expired. Perhaps it would make sense to export a sort of training wheels API from there.
I'm more than open to taking patches that clean up the documentation. We do have a short 'getting started' tutorial in the README on https://github.com/ekmett/lens mashed inbetween some reference material. If you wanted to adapt something like it to the `.cabal` file, I see no real reason why we wouldn't take the patch.
One reason the types are more complicated than the ones Simon used in his talk is that it makes the types give better error messages. ;) If you go to do something like write to a getter you currently get something like No instance for 'Settable Accessor' rather than a screen long unification error. That said, yes, `lens` type error messages can be a thing of terror. =)
Personally I avoid `let` for the petty reason that there is no good way to indent it that I can stomach. ;) Not the most convincing argument against it, I admit. =)
Can we *please* make this explicit, e.g. using "ado" instead of "do" or something? If I'm using this I really want to know if something that I think is using Applicative suddenly degenerates to Monad -- I should think Simon Marlow's users would too!
"Currently" seems key here, seeing as [CHANGELOG.md](https://github.com/ekmett/lens/blob/master/CHANGELOG.markdown) suggests that `Accessor` is no more ;)
'let' implies pure expressions, and "interesting" applicatives are effectful. So 'do' resonates better with me.
Wouldn't it be the other way: a nomadic looking interface devolves into an Applicative?
Hope it goes well! I'll be passing by York but on my way to a funding council meeting, so can't drop in, unfortunately! Kevin
And when you turn on the extension, it would need to be documented in comments why it is there and which do-block it is supposed to optimize. I agree that being explicit is better.
I don't really like hardcoding this sort of thing into the language. I'd rather create a way to specify that X should... fall back to? optimize down into?... Y when possible, and then hook up Applicative and Monad to that interface while letting other things also hook in. Are Applicative and Monad so special they deserve this treatment, and nothing else does? (I mean that as a debate question, not a claim masquerading as a question. If a good argument is made, OK, cool.) The phrasing in the previous paragraph leads me to wonder if perhaps there is some way to make the rewrite rules smarter, so this could be done in a rewrite rule. Although the fact that this is not a behavior-preserving transform is dubious, admittedly. barants' question about ado reminds me of "mdo" (even if it is essentially unused); another direction is to extend do notation in general somehow so some user code has the power to perform this rewrite. I'm pretty sure turning on NoImplicitPrelude and trying to implement your own bind won't be enough, but another angle would be, what's the minimal additional information we'd need to add to get this functionality? I'm just musing. I just hate putting another special-case functionality in there without any thought towards generality. This _really_ smells like a wart (do notation, a convenient wart, sure, but a wart) growing another wart, which is not a good sign.
Your disadvantages outweigh the pros for this syntax. Especially number 2 is unsettling. Why reuse let and not introduce a new name?
Here is a link I found that shows this means: http://hackage.haskell.org/package/optparse-applicative-0.6.0/docs/Options-Applicative-Arrows.html. I wish I had known about it a long time ago..
+1
I feel like I've seen some of these kinds of things in the substitution monad stuff that occurs when embedding lambda calculus.
I have a general dislike of proposing new keywords, which would make existing code using that keyword as an identifier not compile. Also, I can't think how to spell this keyword. Any suggestions? (Edit: `apply`? `effect`?) (If we did use a new keyword, that would get rid of disadvantages 1 and 2, though.)
This is pretty ugly with all the extra `asA`/`()`/etc. noise, but it does meet my basic requirements and might be the best option at the moment, thanks.
Hm? Applicative is (potentially) more efficient than Monad and you can exploit parallelism in Applicative, but not (generally) in Monad. So I think I got that right :).
It's true, it is a bit of nostalgia. Haskell used to be simpler than it is today, that's good in some sense and bad in some other sense. The things with pipe/conduit is that we are now so far in the discussion about implementation details of the specific libraries that we (at least I) sometimes forget what lazy IO actually means. Why do we need it? What are the actual problems? How big are they in practice? Aren't there any solutions in other directions? Can't we maybe solve 90% of the issues with 10% of the machinery? Can't we put them deep inside our web frameworks and hide the existence from the end users? I'm not entirely sure what the answers are to those questions, but I do think they are relevant. Some things are indeed fundamentally complex, but let us at least work very actively to prevent any form of incidental complexity leaking through or solutions.
Someone else in this thread suggested 'ado'. You are right about the breaking code, but such a thing would only occur when the extension is enabled, not for all Haskell code. In any case, I think 'let' is particularly bad because of disadvantage #1 and especially #2.
An extension would be explicit enough, right? There are not that many cases that it really matters if something ends up as a bind or not. When people would turn it on it's pretty clear it has some important meaning for their module. Applicatives degenerating to monads would be bad indeed, but that's not what this is about. It's about monadic structure that are expressible as applicative not introducing unnecessary binds. Something impossible without this extension (when using do-notation).
That's really nifty! Since the 'applicative-do' proposal is (I suppose) inspired by Simon's work on Haxl at Facebook (the presentation has been linked on /r/haskell before), I tried this using the code he presents. Given existing utils like friendsOf :: Object -&gt; Haxl [Object] one can construct a library of arrowised utilities friendsOf' :: Object -&gt; A Haxl () [Object] friendsOf' = asA . friendsOf which allows for writing numCommonFriends' :: Object -&gt; Object -&gt; Haxl Int numCommonFriends' x y = runA $ proc () -&gt; do fox &lt;- friendsOf' x -&lt; () foy &lt;- friendsOf' y -&lt; () returnA -&lt; length (intersect fox foy) instead of 'plain' numCommonFriends :: Object -&gt; Object -&gt; Haxl Int numCommonFriends x y = length &lt;$&gt; (intersect &lt;$&gt; friendsOf x &lt;*&gt; friendsOf y) used as an example in the presentation. Full code at https://gist.github.com/NicolasT/c7c04ac8e9424ac4a927
It's not a lawful prism, though. Many non-canonical strings may all map to the same parsed structure, so `fmap (review p) . preview p` is idempotent, but not identity.
It might not be always better, but it will at least never be worse.
 myApplicFunc x y z w = f &lt;$&gt; x &lt;*&gt; y &lt;*&gt; z &lt;*&gt; w where f a b c d = blablabla though I guess this *might* be *slightly* worse than the Applicative-do for just this job.
&gt; We also used Logstash, but this has been replaced by fully native binaries. What kind of load do you have on your log server? [That's](http://plasmon.rghost.ru/49313565/image.png) how top looks on a log server (100+ servers streaming logs to it) in my company, and I wonder if load could be reduced by replacing it with haskell implementation.
That doesn't solve the problem, I still have to match up the (many, non-trivial) arguments at the use site of `myApplicFunc` with the formal arguments in the definition of `myApplicFunc`. With do-notation I avoid that altogether.
&gt; I think we strike a nice balance between practical and theoretical in the community, but we probably need to be better at messaging that you really can ignore the theoretical until/unless you get the itch, and you'll be fine. I started learning Haskell fairly recently and the community has been fine in terms of the theory/practicality balance. Library documentation however, tends to be far too abstract, and nontrivial examples too few.
Yes, that's what I mean. I think there is a `StaticArrow` newtype somewhere which you can reuse instead of defining your own `A` type (I defined my own anyway in optparse-applicative, because I wanted to minimise dependencies, and I also like short names :)). I agree that it's noisy, but I don't like the idea of creating new syntax for any monoidal category one comes across. I think this is good enough for the few situations where normal applicative style is awkward. If anything, I'd like a variation on the arrow syntax that works over any premonoidal category. I linked to this work a million times already on reddit, but here it goes again: http://www.cs.berkeley.edu/~megacz/garrows/. As for the generated Core, I'm not sure. At the moment, I don't really see why the specialised sugar for `Applicative` would make it better. Care to elaborate?
&gt; but I think that's not the sort of thing you're talking about. Actually, I thought of suggesting "stick in Template Haskell and leave it out of GHC", but a lot of people dislike Template Haskell quite a bit, and figured it wouldn't go over well. Indeed, I'm not sure I love lots of Template Haskell either, but I guess when I see a suggestion to put a new special case into GHC I actually find that _even cruftier_ than using TH. In the second case, it sounds like the proposal is that there would be a "do" notation that could be used, so there would be _something_ to desugar, even if not `&gt;&gt;=`.
It can be worse, actually: http://www.haskell.org/pipermail/glasgow-haskell-users/2013-October/022791.html Excess sharing is a tricky issue that GHC currently gives poor control over...
The tricky part there is figuring out good cleavage points. I do concede that that work flow has worked pretty well for others. It is a pretty big undertaking though. I have something like 160+ repositories on github, and it is hard to consolidate without losing history.
In some ways, `lens` would be an easier pill to swallow if we switched to the "pure profunctor" version of `lens` and shed the pretense that `traverse` is a `Traversal`. This would let most of the type synonyms in lens shed a parameter. However, from an ecosystem perspective it doesn't seem to pay off well enough to justify the increased pain for users. They go from not needing to depend on `lens`, giving us a credible story for putting combinators like `alterF` into `containers`, to needing to depend on an external package.
By the way, is there an article or tutorial somewhere explaining all these exotic profunctor/prism/indexed/etc/etc stuff? Even with my math hat on, I like simple stuff with good examples, and sorry to say so, but even with haddocks and the wiki the library looks rather impenetrable (I mean, if all I was doing for a week was to study it, I think I could decipher most of it, but 1) I'm way too lazy 2) I have a day job...)
There is a simple tool to consolidate git history, I forget if it is called git-subtree or what, but we used it when we consolidated yesod projects. If you would like I can help you out with this. Obviously it is up to you to figure out the cleavage points though. It would seem to me that if you used to have category-extras that everything that was split from that could go in 1 repo.
How about this alternative theory: * `lens` is super-popular because it does a really good job of solving a lot of really big problems, to a much greater extent than, for example, `data-lens`. It's incredibly useful. The popularity is not a result of the library being up-front about exposing complexity; if anything, it's in spite of it. * The high ratio of contributors to users is only partly because of the library's approach to implementation detail: the other part is selection bias. People are being scared away by the complexity. If the library were less surface-complex, the number of users-but-not-contributers would be *even greater*, pushing the contributor ratio lower. I'm not saying I necessarily believe this theory. But I'm wondering whether there are grounds on which we can reject it in favor of the one you described.
jQuery is frequently brought up as an analogy for `lens`, and I'm not familiar with jQuery, so I can't really evaluate that. The analogy that frequently enters my mind is Boost. Boost seriously pushes the envelope of C++, implementing a lot of really useful, advanced ideas which might otherwise be thought impossible. It puts a lot of effort into implementing them as cleanly as possible, but the result is still usually impenetrable for all but the most expert C++ programmers. Incidentally, many of the ideas are basic, bread-and-butter features or idioms of the Haskell language. This raises questions: * `lens` : Haskell :: Boost : C++? * exists FooLanguage. FooLanguage : (`lens`, Haskell) :: Haskell : (Boost, C++)?
Thanks. As it turns out, profunctors are not exotic at all, I just didn't know this name. However, I was thinking of an explanation of the whole ecosystem in Edward's lens library.
I doesn't look like this at my place :) We stream logs through syslog, so the volume of business related logstash message is not overwhelming. The reason I switched to Haskell in that case was memory usage, some features the original didn't have (such as supporting multiple character encoding on a single port) and the fact that I was looking for an excuse to play with Conduits. I really don't know how it compares performance-wise against the original Logstash, as my applications already spit native Logstash messages. My binaries only add a timestamp, store the messages to redis, while another binary extracts them and route them to the correct elastic search server. I can't see how this could be anything but faster though.
&gt; I just personally don't hope that library will be at the core of future Haskell. This is an important point. Lenses feel like something that should be at the core of Haskell. I'm interested in them largely because they look like an important part of a good answer to the record problem; if there were a strong consensus answer for the question of what a "field" actually means, then machinery could be put into place for building and using them from Haskell record syntax. From this standpoint, the lens library is hopeless. There may be 5% of Haskell programmers who are very excited about using the library in its current form, but there's no chance at all that might grow to a 75 to 80% that would be needed to reasonably replace record syntax. On the other hand, I like what Edward said about trying to make sure you can implement these lenses without depending on `lens`. That opens another path, that once the core type is defined, `lens` itself can become optional, and `base` can define some simple lenses for common data types and GHC can acquire language extensions for generating them for others. I don't think it's reasonable for a library with nearly 200 different type synonyms to become fundamental to Haskell programming; but I do think Haskell programmers could learn to understand the meaning of `forall f. Functor f =&gt; (a -&gt; f a) -&gt; (r -&gt; f r)`.
I'm new to this but is the original Pipe data type not implemented very similarly to how it would be done as a Free monad? data Pipe i o m r = Pure r | M (m (Pipe i o m r)) | Yield (Pipe i o m r) o | Await (i -&gt; Pipe i o m r) How is that better than the following? data PipeF i o m r = M (m r) | Yield r o | Await (i -&gt; r) instance Monad m =&gt; Functor (PipeF i o m) where [..] type Pipe i o m = Free (PipeF i o m) Is it in an attempt to keep the complexity low?
http://www.reddit.com/r/haskell/comments/1o08sz/another_status_update_on_ghc_maintenance/ccou0on
Going over the 'big' haskell libraries to learn, I rather prefer seeing [this](http://hackage.haskell.org/package/pipes-4.0.0/docs/Pipes-Tutorial.html) than what's on lens.
I guess this will answer my [SO question](http://stackoverflow.com/questions/19086059/is-there-an-optimization-similar-to-loop-unroll-for-functional-programming)
That's how it's implemented in `pipes-2.0`.
Lens is currently built to have a non-recursive module structure. If you wanted to spend some time putting together a field guide, or working with the folks on #haskell-lens to put together a field-guide for how to navigate the modules, we can definitely put that on the wiki, and if its popular enough, I'm more than happy to restructure the documentation to suit it.
This work sounds amazing. I'm sure the questions on everyone's mind is: Are there cases where this would be significantly slower? And, if not, is GHC going to integrate this?
I fully agree with all of this. &gt;On the other hand, I like what Edward said about trying to make sure you can implement these lenses without depending on lens. This is indeed very nice, but let's be honest, chances that anyone (at current state of affairs) will use lenses without the lens package are near zero. For this to happen we need some extra work. Like you describe, a compiler extension for generating them, but also some utilities to work them. A small lens core of utilities like `get`, `set`, `modify`, maybe a handful more. The sentiment that "Van Laarhoven lenses are awesome because we can use them without the lens package" and "we cannot make the lens package simpler because all of it so essential" just don't rhyme.
There are, however, libraries that want to provide lenses for those who want them, but don't want to _incur_ a dependency on lens, because they don't need such a dependency in their implementation. That pattern, I think, is fairly common!
I think I don't fully understand this. If your applicative instance is worse than your monad, why not just say `(&lt;*&gt;) = ap`? Or are there more trade-offs involved?
That's ridiculous. Yes you can do everything with jquery without jquery. However, the ability to concisely navigate deep into the DOM in a cross-browser fashion is pretty important. These days it seems less so, because browsers have converged much more, and we even have nice selector apis baked into modern browsers, and said APIs look fairly similar. But we have those because jQuery was there first! Back in the bad old days kiddo, let me tell you... there was no uniform way of getting the top level of a page, there were few unified events between browsers, nor was there a unified notion of event propagation. You added handlers differently, you build nodes differently, and you traversed them differently. Cross browser code was a mess of case statements. Until libraries like jQuery came along and hid that under a uniform interface. you don't know how good you have it.
Lens defines 121 operators. 100+ of them follow a consistent schema. You don't need to use them. The "core" of lens that user sees is actually only involves a handful of little classes, but it does involve a ton of type aliases. You know, to try to supply some kind of facade over the top of all the category theory. ;) The rest of it is used internally in the implementation of combinators or exposed to the user for ad hoc overloading of things like 'at', 'ix', etc. I'm not a huge fan of the ad hoc overloaded combinators, but they do have a pretty devoted following, and they greatly simplify the user experience for most operations involving sets, maps, bytestrings, text, etc. We could cripple that side of the user experience and there'd be maybe a dozen classs left. The modules come from trying to ship with 'batteries included' and to keep things organized so we can work on it. There are about 8800 lines of lens code currently. About 7000 of those lines are in Control.Lens, about 1800 of it is providing functionality for the rest of the Haskell Platform. I don't expect to win you over with any of this, but there is a lot of material in there, and I couldn't have written it all as 50 little packages. Given the choice between lens existing and being able to do all the things it can do and not existing, I choose to live in the world where it does. Feel free to take as much or as little of it with you as you want.
&gt; Are there cases where this would be significantly slower? http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html tl;dr: Yes. &gt; is GHC going to integrate this? http://ghc.haskell.org/trac/ghc/wiki/ActiveBranches#Limbobranches tl;dr: Probably not.
In the discussion on page 3: &gt; data U = MkU (U Bool) &gt; e = let f = x case x of MkU y y x in f (MkU f ) &gt; This program encodes recursion via a data type, and any attempt to apply all the simplification rules will not terminate. The problem only occurs for contrived programs which encode recursion via a data type, and it is a problem shared with GHC, which will also non-terminate when compiling this example. Huh? GHC terminates just fine here when I stick that in a file. 
[Here's the project](https://github.com/faylang/fay-angular) SO AWESOME! I use Angular and I want to start using Haskell! Question is...I have no idea how to map bindings...I don't even know where to begin! I want to contribute but I've never done something like this before...anyone have tutorials or practice projects to get started? The [stub project](https://github.com/faylang/fay-angular) doesn't really explain much.
I don't think the proposal is a good idea in its current form. Here is why: 1. It's potentially code-breaking. Not all Monads and Applicatives match -- and mind you it is legal Haskell to not obey class laws (wasn't that even the case in Simon Marlows HAXL slides, where `&lt;*&gt;` was not equivalent to `ap`?) -- and when a module has the `ApplicativeDo` pragma enabled because some `do` blocks benefit from it, it can change semantics of other blocks. (I understand that it is highly discouraged to violate laws of course.) 2. Indirectly mentioned in the previous point, it's a problem that the compiler can't find out whether laws are followed by a class. I therefore think a much better way of solving the problem is this: Allow `RULES` pragmas in typeclasses, and introduce a new pragma, say `{-# OBEYS_CLASS_RULES #-}` provided for every *instance* that supports these rewrites. This has the following consequences: * `do` blocks can be broken down arbitrarily into Functor/Applicative expressions where possible (assuming AMP), provided the instance used in a specific case has been marked as rule obedient. Semantics are preserved, contrary to the current proposal. * Other typeclasses benefit from this mechanism, it's not a hack about `do` notation anymore. Finally `fmap id = id` and friends can be used by the compiler.
I don't have a lot of experience with hxt. I've put together a few arrowized hxt programs and they worked well, but I always was daunted by the size of the hxt API. The same can be said for lens, of course. Hopefully lenses like these will be very reminiscent of lens-aeson or any other uses of lens you have in your codebase and thus become easier to read and train a team on. That's the primary reason I'd suggest using hexpat-lens over hxt. I'd love for some more in-depth comparisons by skillful hxt users, though.
It doesn't have to do with the difference between `(&lt;*&gt;)` and `ap`, just the fact that the desugaring into Monad involves an extra function: do { x &lt;- [1..3]; y &lt;- [1..1000000]; return (x, y) } = [1..3] &gt;&gt;= \x -&gt; [1..1000000] &gt;&gt;= \y -&gt; return (x, y) In the desugared expression, "[1..1000000]" appears inside "\x -&gt; ...", so in a straightforward implementation of laziness, it will be recomputed for each value of x, and not be retained. If we desugar into Applicative, or even the Monad equivalents: do { x &lt;- [1..3]; y &lt;- [1..1000000]; return (x, y) } = ap (fmap (,) [1..3]) [1..1000000] now [1..1000000] appears as the argument to a function, outside any lambdas, so it will be retained across the entire evaluation of this expression. Sharing is a confusing subject because seemingly obvious program transformations often change the sharing behavior. If you define `ap` by ap mf mx = do { f &lt;- mf; x &lt;- mx; return (f x) } then `ap (big_expr_f) (big_expr_x)` has the same sharing behavior as let { mf = big_expr_f; mx = big_expr_x } in do { f &lt;- mf; x &lt;- mx; return (f x) } but **not** the same sharing behavior as do { f &lt;- big_expr_f; x &lt;- big_expr_x; return (f x) }
If you want a simple, easy to understand and use lens library, I suggest `data-lens`. It's the only one I use.
 sorghum:~/programming% ghc --version The Glorious Glasgow Haskell Compilation System, version 7.6.1 sorghum:~/programming% cat test.hs data U = MkU (U -&gt; Bool) e = let f = \x -&gt; case x of MkU y -&gt; y x in f (MkU f) main = print 3 sorghum:~/programming% ghc -fforce-recomp test.hs &amp;&amp; ghc -fforce-recomp -O test.hs &amp;&amp; ghc -fforce-recomp -O2 test.hs [1 of 1] Compiling Main ( test.hs, test.o ) Linking test ... [1 of 1] Compiling Main ( test.hs, test.o ) Linking test ... [1 of 1] Compiling Main ( test.hs, test.o ) Linking test ... Seems to work just fine at any optimization level. What am I missing?
Okay, if I change `main = print 3` to `main = print e` then I can make it panic.
Any chance anyone outside of standard chartered will ever be able to use mu? 
The problem with that is the community doesn't know what it doesn't know. So at that point you're asking the very people who haven't used an abstraction to say what it should be. We have a means by which the community opines on the usefulness of an abstraction by using or not using a piece of functionality, people vote by using it. =) There are parts of lens that are downright vestigial and likely to fall off due to lack of use. An example that comes to mind is the Action functionality. Almost nobody uses it. I could kill it today and maybe get 2 complaints. On the other hand there are parts of it that are in a great deal of use getters/setters/folds that don't have an analogue in other lens libraries. If we look at `lens` itself, there is a pretty solid core that has been stable for the last year or so, which is utilized by `snap`. I've agreed to maintain a very wide support window, long deprecation cycles, etc. for this core, and there is a much larger body of work surrounding it. As we proceed we're getting firmer footing for the rest of this stuff, shoring it up and defining it in terms of abstractions people know or might be able to be expected to learn some day that ave some fundamental reason for existing. 8 months ago Prisms were a hack we thought might be useful. Now they are on equal footing with Lens.
One problem lens faced early on was people going "oh lenses, I understand those" and thinking they'd grokked the package. The UML diagram is there for three big reasons. 1.) it is a useful reference for folks who are trying to figure out what the composition of things means. 2.) That reference is also useful when you're trying to get a feel for how things connect 3.) it makes it abundantly clear that there is more than just lenses in there. It does come with the side-effect of scaring some would-be adopters out of their gourd. ;)
Just tangentially related: there is this great talk, [Playing Go with Closure](http://www.youtube.com/watch?v=v5dYE0CMmHQ), which is about doing game tree searches in Clojure, not for comprehensive search or even minimax, but to do Monte Carlo simulations. Ultimately, it doesn't have quite as much to do with Haskell, but I find the set of tradeoffs mentioned fairly salient. As nice as the Java code you've linked is, though, what's the point of running it on only one core? The Clojure talk mentions Java Concurrency in Practice, which has an example of a [ConcurrentPuzzleSolver](https://code.google.com/p/eclipse-technology-in-practice/source/browse/trunk/JavaConcurrencyInPractice/src/net/jcip/examples/ConcurrentPuzzleSolver.java) operating on abstract [Puzzle](https://code.google.com/p/eclipse-technology-in-practice/source/browse/trunk/JavaConcurrencyInPractice/src/net/jcip/examples/Puzzle.java)s. In Haskell, there are quite a few ways to parallelize things, of which [Parallel and Concurrent Programming in Haskell](https://plus.google.com/107890464054636586545/posts/SKiFz6M3fnL) has many. (Rather than having to submit to a ThreadPool manually, GHC's runtime is capable of a lot of magic here.) If you write a lazy tree traversal, like kamatsu mentioned, you can crawl it intelligently with [parallel](http://hackage.haskell.org/package/parallel). I'm not sure how to short-circuit when you actually do find a "solution", though ([race](http://hackage.haskell.org/package/async-2.0.1.4/docs/Control-Concurrent-Async.html#v:race) does this for async, but you probably don't want IO).
&gt; There are parts of lens that are downright vestigial and likely to fall off due to lack of use. An example that comes to mind is the Action functionality. Almost nobody uses it. I could kill it today and maybe get 2 complaints. I think both of them would come from me :&lt;
supercompilation is somewhat different than loop unrolling. It is more like a really radical version of constant folding/specialization.
Exactly right. The complex language is his speaking in types. "And of course, if we have A to B to A-to-B, then what type do we need?" I still haven't become used to thinking in types, though I'm practicing every day. I'm curious: Do most Haskellers find the Lens library to be a complex beast which is mostly magic? From this point of view, I can see how *any* explanation of how it works is a miracle and deserving of huge applause.
Except for the fact that we are getting packages that provide lenses that wouldn't ever dare pick up a lens dependency. For example, `containers` is adding an `alterF` function that happens to be a legal lens, which will let the instances of `At` for `Map` and `IntMap` become much more efficient. `linear` uses `lens` all the way down to its core. It is fundamentally using about corepresentable functors that use the polymorphic lenses as the representation to represent vector spaces, so I can get good field name punning between different spaces. However, it doesn't currently actually have a `lens` dependency, and could be used out of the box with `lens-family`. Many other packages, like `free` can use this to supply a couple of lenses or prisms here and there without incurring a `lens` dependency, so we do.
IMO, pipes has very little machinery, but lots of *simple* functions that build on a simple core. I see the same thing with lens. Nearly everything is just Overloading p f s t a b, for some comstrained p/f. The rest of it is just mostly simple functions that try to work on the most generic thing they can. (But in some places, there are complicated performace tricks). And with lens, I often even use the fact that `traverseOf l = l`. 
Is case important? Can you upload a package elm and Elm and ELM and be able to install them separately? If not, maybe the problem is actually in the parsing of the cabal file, should it normalise the case like CLI installer?
Pretty demanding listening material for me (and I like [Wisp](http://grooveshark.com/#!/s/I+m+A+Cube+In+A+Cube/2vvghJ?src=5), [Venetian Snares](https://www.youtube.com/watch?v=GSR_g3thb24), [Aphex Twin](http://www.youtube.com/watch?v=eRvfxWRi6qQ&amp;t=2m34s), [Nurse With Wound](http://www.youtube.com/watch?v=NbfxNoYxMhI&amp;t=4m0s) and [David Firth](http://www.youtube.com/watch?v=fGfrXyhUOUM&amp;t=0m20s) clip/drill). It's a bit all over the place, like three different tracks playing at once. But it has technical sophistication, I'll give it that!
Love it.
I see what you mean. Grantz flirts between unlistenable/having no discernable pattern and something you can follow/recognize. There's some of that around 2 minutes in on yaxu's track. Whether intentional or accidental, it comes together there.
Great tracks !!
TIL that lens does composition backwards. How confusing that must be.
ELI5: What is this library **for**? Why do I need it?
I've been listening to your stuff on Soundcloud recently. It's pretty cool. I'm not such a fan of this one, but in general~
It can't be helped; it happens because of the way lenses are defined. Maybe we've finally found a use for those reversed-order function application operators people used to like to define.
And I'm lost even in beginners cheatsheet. Why it's called “over” is for modifying? How do you suppose to remember what (%%=.~) do? Why 1000 of them are here?
I imagine that if lenses were given their own `newtype` and an instance of `Control.Category` defined, that it could be set up to work in the correct order. That's how `data-lens` works.
I don't know why the modification function is called `over`. I don't know what `(%%=.~)` does, or how I'd remember what it does (does it even exist?) That's why I made myself a cheatsheet with what seem — to me — to be the functions, operators and lenses most useful for me to know. (Less than 20, nowhere near 1000.) Are you complaining about the cheatsheet, or are you complaining about the library?
I don't think that's possible without losing the ability to update the type at the same time as updating a value. Lenses generally have 4 type parameters, not 2.
I'd like that. I resent having to type case for package names. It's utterly pointless.
About the current overwhelming state of things that lens is facing to you. 0 complainings about cheat sheet.
On the other hand, it would also litter the implementation with lots of wrapping and unwrapping, increasing the cost of maintenance. It's the Worse is Better question -- whether the cost of usage outweighs the cost of maintenance.
Who or what is "ELI5"? If you're saying the cheatsheet needs to explain that, I disagree: its target audience is people who want to use the lens library in their projects, but find the existing documentation too voluminous to get a handle on. Otherwise: I find it easier it access and update parts of complex data structures using lens than without (e.g. nested record update syntax sucks, having to write my own `mapHead` function sucks, having to keep track of separate functions for getting/setting/updating sucks). I haven't used competing libraries such as fclabels though, so I can't compare lens to them.
&gt; Who or what is "ELI5"? "ELI5" is reddit slang for "Explain Like I'm 5" [years old]
I think it's called over as that makes more sense for traversals.
In fact, I do believe this could be done nicely in the pure profunctors approach type Lens s t a b = forall p. Costrong p =&gt; p t s -&gt; p b a class Profunctor p =&gt; Costrong p where cofirst :: p x y -&gt; p (Either x c) (Either y c) 
I think it's called over for "map *over*", because it can be used for lenses (which can be seen as 1-target traversals) and traversals. 
It can be helped. Given any lens named `l`, you can flip it to generate `l'` by just writing: l' = (. l) So, for example, if you had `l1`, `l2`, `l3`, you would generate: l1' = (. l1) l2' = (. l2) l3' = (. l3) Then when you compose them you will get the flipped order: l1' . l2' . l3' = (. l3 . l2 . l1) Then if you want to use the lens you just have every operator that consumes a lens to apply the flipped lens to `id`. For example, if you wanted to change `set` to accept these flipped lenses, you would write: set' l' = set (l' id) For example: set' (l1' . l2' . l3') = set ((. l3 . l2 . l1) id) = set (l3 . l2 . l1) Note that I don't necessarily recommend doing this. It complicates the types and makes lenses more difficult to define and use without combinators, but it is possible.
I think that a lot of user of the lens library are using the operators for accessing and modifying values. -- get value of fieldB record ^. fieldA . fieldB -- set value of fieldB record &amp; fieldA . fieldB .~ newValue -- modify value of fieldB record &amp; fieldA . fieldB %~ (+ 1) In this case it would be even more confusing if the lenses would compose the other way. I also thought, that the order would be quite confusing, but the operators are for myself a quite good visual marker, how I have to read the expression, like the uppercase letter is a marker how I have to read a qualified function. 
`over` reads nicely sometimes. Many of the lens combinators aim for nice reads (but never nice infix reads—those are operators only). You can write modify target lens fn "modify this target using this lens with the function `fn`" Or using `over` over lens fn "over this lens perform fn". This form is nice when you chain lenses. It also has a parallel with `under` under :: Iso s t a b -&gt; (s -&gt; t) -&gt; (a -&gt; b) over :: Lens s t a b -&gt; (a -&gt; b) -&gt; (s -&gt; t)
The operators have a few "themes" which make them a little bit easier to remember. For instance, `~` on the end usually means it's a pure update a la `over` (%~) :: Lens s t a b -&gt; (a -&gt; b) -&gt; s -&gt; t (%~) = over -- Or for setting (.~) :: Lens s t a b -&gt; b -&gt; (s -&gt; t) (.~) l = over l . const If it ends in `=` then it's usually doing the same thing as the `~` version but as a `State` monad modification instead of a pure one. (%=) :: Lens s s a b -&gt; (a -&gt; b) -&gt; State s () (.=) :: Lens s s a b -&gt; b -&gt; State s () Then the `.` and `%` on the front are often replaced with some other common operator like `&amp;&amp;` in order to replace the `const` in `(.~)`, i.e. (&amp;&amp;~) :: Lens s t Bool Bool -&gt; Bool -&gt; s -&gt; t l &amp;&amp;~ b = over l (&amp;&amp; b) 
Yes, and then you completely give up all the benefits of the van Laarhoven representation regarding transparent upgrades. This is great if you never use anything but a Lens. It also doesn't support polymorphic update without hacks and types that are too big.
We originally had it called `modify` and `mapOf`. Then we wound up with `over` and `under` for working with isomorphisms in either direction. `over` turned out to work with any `Setter`, not just isomorphisms. In the end `over` turned out to be more popular, as `modify` collides with State` and `mapOf` is ugly. We eventually deprecated `mapOf` to get better consistency and removed `modify` to avoid collisions with `State` out of the box, since `modify` is used internally in many of the `State`-based combinators in `lens` and it was confusing.
&gt; It means that `foo^.bar.baz` means exactly what you'd expect. Maybe it's just me, but no, this is opposite of I'd expect. Especially with more complex exprs like `foo ^. bar . to quux . inside qaaz`
&gt; The alternative means you start from the object then jump to the end and read the accessors backwards. If by "backwards" you mean "the way the composition of getters would normally be read", then yes.
By 'what you'd expect' i mean if you'd never used haskell and were used to foo.bar.baz += 12 being field access.
&gt; By 'what you'd expect' i mean if you'd never used haskell OK. Now what those poor souls that did use Haskell do? &gt; and were used to foo.bar.baz += 12 being field access. Why it has to look exactly the same way as where *they* used to `foo.bar++`? It's somewhat different semantically isn't it? (Then maybe they will learn that spacing doesn't matter here and that (.) is function composition. And the mess from brain explosions will be hard to clean) I honestly don't follow you at all. 
This doesn't really work. I think you'd need a class like this: class Profunctor p =&gt; InvStrong p where invFirst :: p (a,c) (b,c) -&gt; p a b invSecond :: p (c,a) (c,b) -&gt; p a b
I, too, was uncomfortable with the fact that it composed 'backwards' for a while. Then I realized that this meant that it happens to coincide precisely with the other meaning for (.) in the rest of the world. This is a strangely beautiful and completely accidental thing. That (.) for function composition can match the semantics of the object oriented (.) right down to how things chain strikes me as a very funny coincidence. Now I wouldn't fix it even if I could, but I can't, so the point is moot. ;)
I don't think it's confusing at all. Unless I happen to think of (.) as function composition. But I don't. :)
The header of that cheatsheet pretty much explains it. It's like JQuery for DOM in browser. Only this lib is for any data structures in haskell. It's a data slice'n'dice toolset.
I'm not in a hurry to needlessly double CPS. :) 
This gives rise to what Shachaf and Elliott like to call an 'Unlens' in the pure profunctors variant.
composed, hah, I see what you did there :-)
Why are you writing a library for people who have never used Haskell?
You are indeed correct! Good intuition.
You can get by using `lens` quite effectively not touching any of the operators. The operators are potentially convenient, but they aren't what the library is about. Other than the basic [view,over,set] functions, what makes lens interesting are the "-Of" family of generalized functions. Build your own folds with foldlOf, foldrOf. Build your own length function with lengthOf. With all of these functions there's nothing to memorize. They do the same thing as their list counterparts, but are generalized to an arbitrary structure instead of the list structure. The lens library is about the realization that the traversal/applicative pattern captures a LOT of normal programming and expands out from there in a single framework. If you approach the library as a solution to getters and setters for records you're likely to be overwhelmed by what you find. Of course you can do simple getters and setters with a simpler library, but you'll be missing out on the point of `lens`.
Is there a tutorial for this kind of stuff? I've never heard of foldlOf before and I never knew that getters and setters wasn't the point of lens 
pycube was right. I wanted class Profunctor p =&gt; Costrong p where cofirst :: p (a, c) (b, c) -&gt; p a b 
Not to mention that `(.)` is already "overloaded" in the wrong direction for module namespaces.
Sure thing you don't. You write everything in [DSL](http://hackage.haskell.org/package/BASIC) of yours
...and that's why package maintainers should set proper upper bounds on their build-dependencies ;-) However, I guess you could write a script to interpret the upload log and generate a constraints-file for use with Cabal 1.18 which would restrict the versions available for the install-plan to what was available at a certain point in time.
I'm writing a library that can be instantiated without depending on the library. You can double CPS and make it nigh unusable. You can make a data type and break this capability. Neither of these options is better than just not breaking intuition and not needlessly complicating the presentation.
foldlOf is an instance of a larger pattern. If you know how to do some operation foo with a Foldable/Traversable/Functor you can generally find a fooOf that takes a Fold/Traversal/Setter and duplicates the functionality.
My comment was rather facetious and snarky, sorry. What I'm getting at is that "it looks imperative" is neither a feature nor a bug, merely a coincidence. If you were implementing a library called `snel` and you had two mutually exclusive options for the API, either 1. write it so that it can be instantiated without depending on the library, or 2. write it so that it looks imperative then I will confidently assert that you would not hesitate in choosing 1, nor would you consider lack of 2 to be a bug. Thus I don't think it's valid to assert that 2 is a feature in the case of lens. By itself 1 is a compelling enough reason to choose the implementation you have. Presenting 2 as a lens feature sounds like post-hoc rationalisation to me.
My point was there are two approaches, but one is viable, and one is not. Implementing lens with the van Laarhoven representation forced us into the model where we compose in this order. This had the magic coincidence that the notation coincides with what everyone else in the world expects. I personally spent 20 years hacking in imperative languages before I found Haskell, the notation 'feels' right to me. Even given the option to change it, I wouldn't recolor this bikeshed. My personal experience after having written and used `data-lens` extensively before `lens`, and having used `lens` after, has led me to prefer the `lens` composition order. Your mileage may vary. If you compose "semantic editor combinators" in Haskell, they also combine in this to first appearances backwards manner. `lens` isn't unique in this regard.
Ok, what are JQuery and DOM?
Neither am I. I think you made the right choice.
`lens` is a set of tools for separating the description of "what to do" from the description of "what to do it to". It provides a more compositional vocabulary for describing these two sets of things. This means you can describe a traversal or lens that knows how to drill into multiple layers of records, maps, sets, lists or other data structures and gets down to what you want to access, then you can use the combinators to take that composed accessor and read or write from it. The components of lens make it so you can cleanly move between things with one target or potentially multiple, things that always succeed and things that fail, and all the same combinators work, and smoothly upgrade or downgrade themselves as you need them to, based on pretty deep theoretical justifications. By making this separation you can use the same vocabulary for manipulating monomorphic containers like ByteString and Text as you use to access lists or strings. This helps fight API duplication. If you've ever used ByteString or Text you've seen how the API there is basically a cut and paste copy of much of Data.List. The lens API for working with `Text` consists of two combinators: One isomorphism from Text to String, and one traversal of the individual chars in a piece of Text. The rest of the combinators are all the same as you use to work with the rest of the `lens` universe. Once you know how to use them for any one data type, you generally know how they'll work with every other data type as well, and the majority of the operations are named in a fairly discoverable fashion. The comparison to "jQuery for Haskell" comes about because jQuery splits things up into the descriptor of what you want to iterate over and the actual operations you want to perform.
&gt; If you compose "semantic editor combinators" in Haskell, they also combine in this to first appearances backwards manner. lens isn't unique in this regard. That's the argument that completely worked for me at the time. If people argue to you that something feels unfamiliar and unidiomatic to the language, I think that's what you should use as a counter-argument first and foremost. Mentioning *other* languages would be counter-productive.
The explanation here is that supercompilation "runs the program at compile time", with the details arising out of how you handle run-time data dependencies and how you ensure termination. I haven't read much further yet. But I remember years ago something about brute-force searching for the best implementation of small snippets of functionality, with one common example being the surprising code that you get for x86 to calculate the C expression `(x &lt; y) ? -1 : ((x &gt; y) ? 1 : 0)`, and there do seem to be recent documents using that definition - e.g. [here](http://www.embecosm.com/2013/05/07/energy-efficient-superoptimization). So - are these equivalent in a way I haven't noticed yet, or is the term "supercompilation" overloaded? 
Another example of lens in comic form: http://www.girlgeniusonline.com/comic.php?date=20081205
I infer from your username that you wrote this. Could you go back and finish changing your example from using `age` and `lifetime` to `location`? Things were going great up until then. Thank you for posting this! I hadn't heard of lenses before yesterday, so I started going through the [video](http://youtu.be/cefnmjtAolY?hd=1) and [slides](http://comonad.com/haskell/Lenses-Folds-and-Traversals-NYC.pdf) linked from the [Hackage page](http://hackage.haskell.org/package/lens). After 2 hours, I was 45 minutes into the video and had only grokked a third of it. Your introduction is much more easily understood.
Ha, thanks for pointing out the error! Refactoring examples is quite tough sometimes...
First, run a new hackage server with: hackage-server run --delay-cache-updates=600 That last option makes things much faster when building the mirror. Next, add a `mirror` user to your Hackage instance, and then visit `http://localhost:8080/packages/mirrors` and grant it access. Next, create a file `mirror.cfg` with these two lines: http://hackage.haskell.org/ http://mirror:mirror@localhost:8080/ Finally, run: haskell-mirror -v mirror.cfg It will take many hours, but thereafter updates are quick!
http://jquery.com/ and http://en.wikipedia.org/wiki/Document_Object_Model
Quite nice, clarified prisms for me.
God, that's great!
I'm not sure if this is what /u/penguinland meant, but you still have a reference to `age` in a type signature that clearly should reference `location`.
My God, this is an esoteric comic. Edit: [I found one I understand!](http://ro-che.info/ccc/17) So I don't have to turn in my degree after all!
Case is important; for example, [checked](http://hackage.haskell.org/package/checked) and [Checked](http://hackage.haskell.org/package/Checked) exist. Browsing through the Hackage listing shows there's at least half a dozen other capitalization clashes.
Rewrote boardword.com from Python to Haskell. Old Python code is hard to get back into. Haskell type system is fantastic. Haskell parallelization is great. But the gc hangs noticeably on huge datastructures (over 1 GB). Python pickle is great. Haskell cereal uses too much memory.
I did some corrections while reading it. Thanks tel! https://gist.github.com/markus1189/6959959/revisions
I see it as "to turn a possibly empty list of elements into a single element we need a function `[a] -&gt; a`, which is exactly what a monoid instance is".
Oh yeah! And floating *point* separator. So fuck it, it's all broken anyway! Now here, let do it backwards! Edit: and while we at it, Edward, it cannot be just coincidental that (.) is function composition *and* floating point ;) Sure there is the way for it to happen 3 . 14159 
I maintain a couple of Rails code bases. I also program in Haskell when I can get away with it. I haven't ever interfaced the two, but I can provide my opinion on the two languages. You can predict much of my preference when I say that I am a strong believer in static typing and eliminating errors at compile-time. I find dynamic typing (and duck typing) just causes me anguish by revealing errors at run-time/test-time that could have been figured out at compile-time. Simple things like passing the wrong number of arguments to a function is a run-time error. The lack of types would be alleviated if the docs always made it clear what a function expected to be passed but they often don't. Another general irritation is knowing whether a function updates the current object or produces a copy. Even with the map/map! convention, I still make the mistake of using the wrong one. A handy thing about Haskell is you always know it will make a copy. The whole symbols thing is a little odd, and made odder by Rails which uses "indifferent hashes" that compares strings as equal to their symbol equivalents. All this means is that your code base accumulates symbols and strings and gets messy. What's that quote about your code base always ending up with whatever your language allows? Haskell's quasi-quotes are a great solution to dynamic problems with string substitution in Ruby. If I have an incorrect substitution in a string (or Rails view) in an obscure code path in Ruby, e.g. "there were #{fooo} items" (oops: extra 'o' in foo), I won't know until that code is executed. With Haskell's here documents (see "here" package), I know at compile-time. To me it's obvious which alternative is better. Similarly, Yesod's use of Template Haskell, while a bit intimidating, still seems like a better method than Rails' dynamic use. With Yesod, I get compile time errors if I refer to a non-existing route or helper. There are all sorts of Rails oddities that are also annoying (e.g. the auto-pluralisation magic) but they aren't much to do with Ruby. It's probably my bias towards static languages, but I fail to see any benefits to Ruby other than that Rails is a fairly comprehensive and mature framework. Maybe it's faster to develop in the short-run, but I trust my Haskell code more than my Ruby code, and find it easier to maintain. 
I agree that it's important for the semantics of package names to be defined clearly and well documented. With out any mention of this in the documentation, the assumption would be that there isn't any case-folding going on - i.e., package names are semantically just text - and that there is some other explanation for what you experienced.
I disagree. Why add complexity? It would certainly lead to confusion and errors. Let the semantics of package names be as simple as possible - just text, period. However - if there is already an undocumented more complex mapping between text and the space of package names, I certainly wouldn't want to change it now; just document it precisely and clearly. And if there is inconsistency within the Cabal toolset, that's an emergency that needs to be fixed immediately in whatever way that will create the least amount of breakage.
I remember a whole bunch of people (probably on /r/programming or maybe HN) were very annoyed with that particular comic :P.
I'm annoyed at how stupid it makes me feel. It does seem more abstruse than funny, though. At least the art isn't complete garbage.
when did you start developing it, because conduit and iteratee have been around pretty long, so you might not have been satisfied with them, but there was no absolute requirement to implement your own i suppose. it sounds kind of mean to not even mention them.
I don't see the concept of lens (or lens families) to be something inherently complex, but I do find it problematic to wrap my head around the particular lens library, mainly due to immense amount of typeclasses and types.
So is there a proper way to fix the time example? Or is there any other approach to use lenses to maintain invariants?
I like the lens composition order very much, but: &gt; The alternative means you start from the object then jump to the end and read the accessors backwards The alternative would probably not have (^.) but a flipped ($.) or such, so it would look like an ordinary function/getter chain read entirely from right to left.
It sounds like you're criticizing lens out of ignorance here. Anyone who mentions the number of lens operators as a negative without bothering to look up what these operators are is going to say misguided things.
Take a look at: [Tekmo's lens tutorial](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html). Do you think this is unmaintainable code that's hard to understand? Do you think replacing the lens use there with vanilla Haskell is going to make things *more* understandable? It seems most criticisms of lens come from people who are afraid to go ahead and learn what lens is all about.
&gt; Maybe Haskell simply does not scale and all this buzz around it is like a over-blown balloon ready to pop any time? What a strange comment to make. In any event, we only recently got an easy-to-use ARM cross-compiler for Haskell. Unlike many of those languages you listed, Haskell is not a language whose main implementation is an interpreter written in interoperable C, or a compiler to an interoperable byte-code. Instead, Haskell has a separate to-native compiler and its own run-time system. So, porting to mobile platforms is a little more difficult.
I can only speak anecdotally, but I believe that Haskell is virtually non-existent on mobile devices because historically GHC hasn't supported cross-compilation. There is an effort to fix this, and some people have been working hard to make it possible to write static libraries for iOS devices in Haskell, but there's nothing easy to use at this point. NOTE: I don't know what the story is for Android, I have only been paying attention to iOS efforts.
&gt; not caring about timely closing of file handles, and not concurrently modifying the things that are being lazily read And not caring about which thread does the actual IO. I got bit by this when trying to use a forkIO-based timeout on an IO read which turned out to use lazy I/O and happen in an unpredictable thread not protected by a timeout. And not caring about where you get the IO exceptions, which may significantly complicate error handling. And not caring that the lazy IO API's do not interoperate with any other APIs that do explicit closing. Worse still, this manifests as non-deterministic bugs. e.g: accidentally use lazy I/O with "withFile", you get buggy/non-deterministic behavior based on evaluation order. I think after you rule out all the use cases that may care about any of these, you end up with far less than 80%. Also, lazy I/O tempts people to go ahead and rely on evaluation order. For example, known Haskell experts advise people to control evaluation order manually on stack overflow to control lazy I/O. This is very un-Haskell-like, and at this point, why not just do away with purity? 
&gt; Can't we maybe solve 90% of the issues with 10% of the machinery? As a counter-point, I like this [quote](http://conal.net/blog/posts/designing-for-the-future) from Conal. &gt; I think I finally found the words for why I don’t design for “90%” of the use cases. It’s because doing so is designing for the past, while my creations will be used in the future, which I see as unboundedly creative. Moreover, the claim that “user’s won’t want to X” turns into a self-fulfilling prophecy.
Iteratee first entered the space around 2010, which would've been around when Tekmo was just starting. Pipes was first published by Paolo just after Conduits I think - it was originally a parallel development.
Oomph, yep, caught those now, too.
Thanks! :)
What about: How does Fold relate to Traversal? What is Action/MonadicFold good for? 
really - sorry then. For me this helped me get ghc/cabal and the basic packages I need for sublimehaskell installed on my openSUSE system. For the rest use only sandboxes ... Remark: I did not really try to install the complete platform - I stoped after the step for building ghc and cabal-installed alex, happy, etc. manually
I ran out of steam before getting to Folds. I'll add them a bit later, though.
I do BB10 development in Haskell, and I know work on android and ios is also going.
Java and "cross-platform compatibility" shouldn't be placed it the same sentence past 2007. You cannot target Android/iOS/WP using Java. Smalltalk images proved to be better than Java. I can load Smalltalk app on iOS/Android/desktop/... god knows where else. Performance is poor, but... JavaScript is simply useless. You could sum it up to WTF. Layer over layer over layer... Today there are at least 3 fragmented platforms to target to, if you won't use them your app will crawl: TypeScript(IE)/Dart(Chrome)/asm.js(Fx). All of this does not sound to be future-proof.
Thanks. My question was worded badly, I had found `_Cons` fine and was just wondering about `_Nil`.
They still form a category instance with composition in the current order, don't they? l1 :: Lens' a b l2 :: Lens' x a l3 :: Lens' s x gives: l2 . l1 :: Lens' x b l3 . l2 :: Lens' s a (l3 . l2) . l1 :: Lens' s b l3 . (l2 . l1) :: Lens' s b id :: Lens' a a So lenses are typed like a cateogry, and the associativity/identity laws look like they hold for values as well.
&gt; nobody is interested to make it useful on emerging platforms. Not sure where you got that idea?
Proper lenses themselves don't really give you tools to police invariants. If you have a `Lens` or `Setter` that targets values of type `a`, then you need to be able to take _any_ value of type `a`. Probably the most lensy way to model the invariant is to make constructing an `a` that violates that invariant impossible. You can do that by making `a` a newtype or something else which guards its construction. Another system we've been building up internally is working up a theory of 'improper' optics. For lenses this gives you something where `view l . set l` or `set l . view l` = id, but not both. With that weaker notion of an improper lens under which fusion becomes harder to reason about, but all the combinators still work. Composition of lenses that both have say, `view l . set l` hold yields one where that law also holds. Optics fall into four kinds, depending on if either, neither or both of those laws hold. Operationally nothing fails if you don't use a proper lens. However, some constructions in `lens` that are canonical in the presence of the laws become only one possible distinguishable implementation in the world of improper optics. So in that sense the `time` lenses are improper, but (if he fixed them to use `mod`) they still have the property that if you get something and put it back then you don't change their value. The `time` lenses he showed are what we might call "improper lenses of the first kind", which is to say they tweak values to impose invariants.
I think `lens` HEAD adds an `_Empty` prism that is overloaded to work with lots of containers, that can serve as `_Nil`, but its not on hackage yet.
I find it very useful. I've working with lens, but with your cheatsheet I actually understand how to put types correctly. We need more of this simple explanations for beginners.
Ideally people just wouldn't write upper case.
Thats mostly because `lens` has very little to do with lenses, and more to do with all the lens-like concepts that surround it. Ideally it would have been `traversals` or `optics` or something. ;)
As of 7.6 TypeOperators doesn't let you do this for operator variable names any more.
Hm, I'll look more closely at toListOf to kick off folds in a few days. Without talking about them it's a bit weird to even have the concept of a target I suppose, though as far as I know the only way to view targets is with a fold of some kind.
I don't use those often, so I unfortunately just guessed at its existence based on the existence of _Cons and _Nothing. My bad!
&gt; You cannot target Android... using Java. Most of Android is built in Java, and that's the primary language Android developers use. Furthermore, there are Java VMs for just about every architecture out there: x86, ARM, SPARC, PowerPC, etc. Perhaps what you're asking about is whether the OS on the architecture comes with the VM included. I agree with you that it's roughly impossible to get code to work for both Android and iOS unless you use web-based technologies like JavaScript (and I also agree with you that JavaScript is a mess). That's why most companies target one platform and then port their work to the other months or years later (or not at all). I'm not aware of any silver bullet to fix this.
Thanks! Where is forall defined ? It looks handy.
It's not really a thing, sadly—just a handy device I used to write the laws. Similar things exist in QuickCheck and SmallCheck though.
great post!
Why should he mention them? He didn't use them for the project which is the subject of this post. He used pipes. In fact, it's thrilling to see collaboration with Michael on other things mentioned as part of this work.
Pipes was first published by me! :) I announced `pipes` for the first time in this [reddit post](http://www.reddit.com/r/haskell/comments/ohjg7/a_new_approach_to_iteratees/) (January 2012). Then Paolo began contributing to `pipes` development soon after, but he was unhappy with some of the design directions I took so he forked it into `pipes-core`. In that post I described the main reason I didn't use iteratees for stream processing: they were inelegant and difficult to use. `conduit` came out about three weeks before because Michael had basically the same objections as me. I didn't learn about `conduit` until a couple of months after I released `pipes`.
Gabriel - this is absolutely beautiful work. One question though - why isn't it part of the "Tools" set on the [front page](http://www.degradolab.org)? In my opinion it definitely deserves to be.
Thank you! :)
Because I only just publically released it and I forgot to add it. This had to be under the radar for a while because I'm also negotiating a separate license with the company that owns `PyMOL` to incorporate this within their software. I only just got the green light to open source this under the GPL and publicize it.
There is a package that does this, `lens-family`. It is `lens` compatible insofar as it goes. You just need to depend on three packages to get started and it only covers lenses, getters and traversals.
Correction: laziness historically tended to produce memory hungry programs. Nowadays with stream fusion, deforestation and better strictness analysis we may get memory use well below Python, Ruby, and Java. (See hPDB parser, which uses less memory than BioPython, BioRuby or BioJava to hold parsed PDB structure: https://hackage.haskell.org/package/hPDB)
Ouch. I think cabal has it about right here (ensure correct case in a long-lived cabal file; be more tolerant on the command line) However, I do think there's a case for hackage preventing the upload of capitalization clashing packages. I also agree with the poster that a suggestion in the error message would be nice.
Is the non-GPL matrix library that was mentioned available? 
Cool :) Where can I find a more in-depth clarification about the reverse composition issue?
Naturally, `-XDeriveFunctor` works well with this. I think your suggestion does not work for the Haxl case, though, where the Applicative instance does not match the Monad instance. 
It is compatible with `lens` in the sense that you can use Lenses from `lens-family` in `lens`, but (unfortunately) `lens` is not a drop-in replacement for `lens-family`. But yeah, `lens` is also more popular :P
Oh, I was talking about the Venn diagram one in particular. 
What does `M.map . S.map . map` do? It maps over lists, inside sets, inside the values of a map. That's exactly lens composition order! If you think of lenses as "generalized functors" or better yet "a special case of traversals" then the composition order maps to native Haskell. If you think of lenses as "generalized accessors" then yes you'd write the opposite: `head . S.findMin . getJust . M.lookup key` or whatever. Until I realized this, I found lens sorta 'backwards' too. However, now, the order I think corresponds to a better "core notion" of what a lens is. Not something that extracts "but also happens to put", but rather something that "walks through effectfully" allowing getting, putting, etc. as a consequence.
So, I know what raspberry pi is, but what exactly is GPIO? (I think announcements in general should explain what they are about, typically they are of very specific interest)
IANA biochemist. Can someone give a dumbed down example of what you'd do with this?
It stands for General Purpose Input Output. This is a library for controlling the pins on the edge of the pi from Haskell. I'm not a hardware developer, by any means, but I thought it was perfectly clear. Besides, it's not an acronym that's difficult to lookup.
It's not trying to be C in any sense? Your question doesn’t make sense. 
Thanks. Of course it is not difficult to look up things in the internet era. But. * That takes time * It is in the interest of the poster that people find it easily * Very often, the slang is so particular to the subject that it is not so easy to find it on the net (for example, try to google the (well-known) kraut-rock band called "Can" without knowing that it is kraut-rock).
Oh. Huh, I wonder why? I guess if you were pro-dynamic typing and felt like you knew about the issue. On the other hand, he's not claiming there's no argument for dynamic typing, just that a minority of people seem to me making it.
I know almost nothing about Hackage, so I have no idea if what you propose is possible, but I think it's funny that you're essentially asking whether Hackage, which is software used extensively and (presumably) exclusively by the Haskell community - a community who are known for their heavy usage of persistent data-structures - has immutable history and your response is a somewhat awkward silence.
btw, I really don't want to be political here, but still, this is a good example of how GPL causes more harm that good: &gt; hmatrix was problematic because it is GPLv3 licensed, and my university, UCSF, forbids releasing code with a GPLv3 license because of the patent clause. GPL (already v2, not speaking about v3) actually prevents freedom instead of aiding it. I'm really happy that the Haskell community (mostly) chose BSD instead of GPL. (NB. I'm fine with GPL on final applications. I even think that makes sense. What I'm not ok on is GPL on libraries, not even LGPL, though that's somewhat better)
&gt; or a compiler to an interoperable byte-code. Instead, Haskell has a separate to-native compiler and its own run-time system. So, porting to mobile platforms is a little more difficult. I once ran across some old GHC documentation for a C backend. It looks like it has been discontinued and removed from the compiler by now. I wonder why that happened? Surely it can't be that bad of an idea because it would easily enable cross-compilation (given that everything the linker needs would be available on the target platform) since C compilers exist virtually everywhere. I understand that being able to cross-compile directly is preferable but for the time being it would at least provide an option.
There are several applications, but the first application our lab is marketing it for is protein design, since that is what we use it for. To frame it in software terms, right now protein design is very much like the oft-maligned "waterfall method" of software design. You come up with a spec, you try to implement it in stages, and then you fail and you go back to the drawing board. Worse, debugging is a very expensive and time-consuming process, so if it doesn't work you usually just scratch your head and try something else. This search engine makes the design process more "agile" because it is like a back and forth dialog with the protein data bank. You specify a couple of pieces that you want, you do a search, and then you get several results back for where you could take the design. You can pick one of those and then try that path and if it doesn't work you can easily back track. All of this happens before you even crack open a test tube. One of the reasons the search engine avoids errors in protein design is because it sticks to things that have been battle-tested in nature and focuses on gluing together natural solutions instead of experimenting with untested man-made solutions. I'm trying to make a movie for my thesis defense that will show what the typical workflow and when I do I will post it on my blog.
Thank you for this.
&gt; I wonder why that happened? Without knowing the specifics the answer is most likely that it was a lot of work for very little benefit if it is not used as the default backend.
How about all unnecessarily hostile criticism?
This means that Haxl violates the Applicative/Monad laws, and rewriting potentially changes semantics, so no rule should be applied.
Is this intentional or a regression?
That depends on whether you think being able to patent software is good or harmful. Obviously the GPLv3 authors think differently to you.
So have I got this right: with prisms you can build determinstic parsers, and with traversals you can build non-deterministic parsers? Has anybody actually done this, and does it have any advantages?
I have the radical opinion that all patents are harmful, but software patents are especially bad (also, software is more or less mathematics, and mathematics is not patentable by law, so it does not even makes sense). But I have to admit that the interaction of patent law with open-source licenses is not something I have though much about.
He's essentially claiming that nearly all dynamic typing advocates are not competent to argue about the relative merits of typing systems.
Some time ago I tried to write high level wrapper for BLAS and LAPACK. I've made mostly complete bindings for BLAS: all operations, different memory layouts for matrices. I didn't make good setup. It worked with particular version of library https://github.com/Shimuuar/blas-lapack https://github.com/Shimuuar/blas-bindings Completing it quite a lot of work and I don't have time for this. If someone wants to take over please go ahead
The `blas` package used to be pretty good back in the day, but it seems unmaintained for the last while? http://hackage.haskell.org/package/blas Curious if Tekmo looked at this, or can provide an eval on its current state?
Hi Tekmo, is your background in biochemistry? The quality of software engineering in the natural scientists can often unfortunately be quite poor. Nice to see great interdisciplinary work happening here.
Intentional. It made it possible for GHC to start using type family names like (+) (-), (*). This really makes a difference in the readability of a lot of code that works with type naturals. Also, where you could bind the variable `(~&gt;)` before, you couldn't give it any fixity information so it was always kind of a second class citizen.
The disclaimer was about me not *wanting* to be political, not about not *being* political. I think this is a very important issue, which is why I wrote that comment.
&gt; not least because you've not established which kind of freedom is at stake Let's say I write a library with a BSD license, then a GPL devotee forks it, improves it, and releases it under GPL. Now I'm not allowed to use those improvements in *my* fucking code, unless I relicense it under GPL. (Please correct me if I'm wrong here). GPL is viral, which is the main point of it. Proponents of GPL think that virality is good. I think it's bad.
Are you trying to ask why Haskell doesn't compile down to C instead of directly into a native binary? It used to, and there's a deprecated -fvia-c option to retrigger that path, but it's no longer supported and will soon be dropped. Code generated that way is much slower -- native compilation turned out to be a win on all sides. Why? Because Haskell isn't C, it doesn't share its execution model. The abstractions in Haskell translate poorly to C and it results in relatively poor C code. By instead using the information ghc has that gcc can't have, ghc is able to emit better optimized native code than compilation to C then binary. There's also a lot of work going on to transition to an LLVM back end to leverage the work going on there -- using the optimization platform they're providing to improve the produced binary. Your question still doesn't make sense though -- in what way is ghc trying to outsmart gcc? Gcc compiles C, not Haskell. It's optimized for that task. Ghc is optimized for compiling Haskell. If you're suggesting there shouldn't be any other languages, because C... you're in the wrong subreddit.
I like the protein-programs analogy.
Any examples?
It is not. I posted several links and questions and every time I got warming and beautiful answers from which I learned a lot.
Hello Tekmo, thank you for the practical feedback. Up to now, I thought you were doing a PHD in computing. Regarding problems with haskell, two of them derive from non-strictness. How did you play with this very particular part of the language ? Do you think you could have done the same program with a strict language ? For example, what do you think "pipe" would be in a strict language ?
Just browsed your comment history. We're not hostile, you're hostile. Stop trolling us. If you want to learn haskell, stick around. If you're convinced that you don't like laziness, then there are plenty of other language communities that you would enjoy participating in much more.
Ok, I'm fully convinced. From now on, all posts here should be github links with no context and no readme. The code speaks for itself, does it? 
Is this a response to [this thread](http://www.reddit.com/r/haskell/comments/1ocma6/why_haskell_is_virtually_nonexistent_on_mobile/)? I think the community responded in good faith, even given your fairly hostile opening comment of "Maybe Haskell simply does not scale and all this buzz around it is like a over-blown balloon ready to pop any time?".
That is correct, you cannot use the improvements as-is. If you want a license that allows you to use the improvements other people make (and release) on your own terms, the GPL would in fact be appropriate for that purpose -- permissive license aren't. Similarly that person could have made a draconian proprietary no-disassembly-allowed binary release. But why did you choose to blame the GPL here? 'The GPL is viral' is, again, prone to launching flamewars.
In my experience that issue is relevant to many reddit programming communities. Outside of reddit everything looks much better. No idea why.
"GPL is viral" is a fact. That's the primary point of GPL. Basically that defines GPL. Now, whether that's good or bad is indeed prone to launching flamewars, which I don't want to do, but I think it is too important not to talk about. &gt; Similarly that person could have made a draconian proprietary no-disassembly-allowed binary release. But why did you choose to blame the GPL here? Because I think it is a bad social contract. That there could be other bad (or even worse) social contracts does not really change anything. Also, GPL was explicit in the original post. (edit: also, GPL is widespread, more importantly, it is widespread in the same community (unlike no-dissasembly licenses). GPL propenents have pretty similar goals as say BSD proponents, thus it is stupid to fight against each other. My personal opinion is that a BSD-type license is a better global optimum than GPL. Both are for open-source, but (let me allow to be cheesy here) BSD is "peace", GPL is "war").
&gt; C code is not for humans. It is for GCC/Clang. Tell that to all the people still trying to write it... :/ &gt; Could you pass me some links to this LLVM version if it is not too much trouble? Here's the [trac page](http://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Backends/LLVM).
the -fvia-c backend does exist, and is used to support ghc on some neat cpu architectures like ppc64 and a few others. That said, not really suitable for cross compilation support currently (at least to my current understanding). Plus in many cases much slower than a registerized build with llvm or the native code gen
Yeah, my degree is in biochemistry, and yes, the quality of scientific software is pretty bad.
That's more like a symptom of the problem.
I think you missed his point -- He's saying the other person could have released your code in any number of worse formats *because* you licensed it under a permissive license. That's in fact, the point of a permissive license. So after doing that, on what grounds are you complaining about the guy who forked your code as GPL? Lack of control of your codebase is own of the downsides of a permissive license like BSD3, just as some people find the virality of the GPL a downside.
Overall I think laziness is a net win. For example, there's one part of the program that was greatly simplified by laziness (the final backtracking search algorithm). `pipes` is an interesting case because it is actually totally compatible with a strict language. The fully bidirectional implementation has the really curious property that everywhere you want laziness you get it protected behind a function call. This was the source of the spurious `()` arguments back in the `3.*` cycle. I got rid of them in version `4.0` as part of the simpler unidirectional API, but if I were to switch `pipes` to a strict language I'd add them back in again.
Cool, post links here so anyone who's interested can see them.
You might not have intended this to be inflammatory: &gt; Maybe Haskell simply does not scale and all this buzz around it is like a over-blown balloon ready to pop any time? But it sure comes off that way. 
 foo x = case bar x of x' -&gt; blah blah x' y Now I don't need to worry about accidental looping between x and x' and my context self-closes. ;)
&gt; My personal opinion is that a BSD-type license is a better global optimum than GPL. I think that's exactly what you should make obvious when you start off a discussion. 'Under the assumption that we need fewer license flavours, I'd rather we focus on permissive licenses' is much, much more amenable to an interesting discussion than a blanket, move-the-goalposts 'the GPL restricts freedom'. For instance, I'm of the opinion that we need as many flavours as there are needs. From that viewpoint, *not* having the GPL restricts the freedom of those that do, in fact, want to license under strong copyleft terms. &gt; "GPL is viral" is a fact. This is playing fast-and-loose with my policy of not feeding trolls, but I can't opt out of the flu, and I can't opt out of self-propagating malware. Those are viral. You can always opt out of *not* accepting the terms of any software available under the GPL (or any license), and go on your way. Yes, that means you can't use the software in any or all way you wishes. But if you think you have a right to, then licenses don't matter because you want to preempt the rights of the licensor. How's that for 'bad for freedom'?
I tend to think lens has exactly one target, prism 0-or-1, traversal 0-n. Prisms are a little more special though—you have to be able to review them.
&gt; if everybody used a permissive license [...] Then denounce the GPL as much as you should denounce everything that is not permissive. Don't make it special. By making it about the GPL, you make it look like you think it is flawed. It serves its purpose quite well -- it is that purpose that you disagree with.
This article certainly made prisms clearer, but they are still quite mysterious to me. What would be the prism equivalent of the simplification `Lens' s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; s -&gt; f s`, for example? I tried sticking a few `Maybe`s in that definition and moving the `f`s around, but I couldn't find a definition which allowed me to define `preview` and `review`. Any pointers?
I just came across the talk if anyone's interested: http://vimeo.com/15833948
`UndecidableInstances` is very powerful, it lets you essentially do arbitrary computation in the type domain. Thus it's usually a bad idea.
I'd say some combination of GADTs, TypeFamilies and DataKinds.
if you do care about what happens with your code after you release it, you will certainly like the gpl, because it guarantees (more or less and it depends on what version you use) that the code remains free. you get no guarantee at all with permissive licenses. i don't get your point here.
I'm pretty sure that DataKinds is basically just syntax.
CPP =P
and I'll ask on StackOverflow why GHC doesn't do AD automatically.
Thanks!
I hadn't even noticed that link was using a different word than what I Googled for. Oops.
I use `ScopedTypeVariables` often when I'm doing stuff with type classes. Hmm.. `NoMonomorphismRestriction` is also helpful.
You're welcome!
The lens library contains [lenses for zippers](http://hackage.haskell.org/package/lens-3.9.2/docs/Control-Lens-Zipper.html). Zippers are the [derivative of a regular type](http://strictlypositive.org/diff.pdf). [Scrap your zippers](http://michaeldadams.org/papers/scrap_your_zippers/) can automatically make zippers, and, therefore, be fairly said to do automatic differentiation. There: From lenses to automatic differentiation in only three steps. (Lrf, V xabj, jebat "qvssreragvngvba". Furrfu, lbh ehva nyy gur wbxrf. -13)
Strictly speaking, yeah, I think `GADTs` give the biggest oomph to the system. If `ImpredicativeTypes` actually worked always and well, then it would potentially take the cake, but its too powerful for us to tame :-(
The GHC implementation still had a pretty non-portable RTS, IIRC. So you didn't get cross-compilation for free with -fvia-c.
GADTs, DataKinds.
Not at all!
Are you thinking perhaps of `KindSignatures`?
Several people have said GADTs, a feature which I have up to now always completely dismissed. Could you (or another who suggested them) outline why you think they are one of the most powerful?
Many extensions can be seen as syntactic sugar over more basic constructs, or if not that, then at least as "desugaring" so something more basic in GHC core. MPTCs are pretty powerful. But the work they do is _before_ core, producing typeclass "dictionaries" GADTs, on the other hand, require an _actual extension_ to GHC core, in the form of type equality constraints. The extension to System F described is in this paper: http://research.microsoft.com/en-us/um/people/simonpj/papers/ext-f/fc-tldi.pdf
I have also been curious about the lens-prism duality. Here is my incomplete understanding, which should not be taken as correct, starting with the "classic" definition of lenses: data Lens s a = Lens { get : s -&gt; a, put : s * a -&gt; s } data Prism s a = Prism { teg : a -&gt; s, tup : s -&gt; s + a } I have found that [dualising the lenses laws](http://s11.postimg.org/cxn6wp85v/lens_prism.png) helped me to understand why the product turns into a sum when we flip the arrows here (and to grow my understanding of how a prism should behave). Now, if we get back to the Van Laarhoven representation of lenses, we are stuck with the following scheme if we want composition with lenses to still work: forall f. ... =&gt; (a ~&gt; f a) -&gt; (s ~&gt; f s) (This means that we can only act on the context `... =&gt;`.) Let's try to make a prism by considering the two possibilities when we `tup s`: - Either we get an `a`, and we can plug it into the `a -&gt; f a` and `fmap teg` to get the `f s`; - or we don't, in which case we have to create the `f s` out of thin air. The simplest way to do that is to use the Applicative `pure`. Hence, data Prism s a = forall f. Applicative f =&gt; (a -&gt; f a) -&gt; (s -&gt; f s) But this is just the type of Traversal! How do we guarantee that a prism points towards 0-or-1-but-not-more targets? We need a way of specifying that the `a -&gt; f a` can only be used once: This is accomplished in the library with the Choice typeclass, which provide us with the ability to *maybe* use the `a ~&gt; f a` (if we used it twice, we would'nt be able to merge the results.) data Prism s a = forall f. Choice (~&gt;), Applicative f =&gt; (a ~&gt; f a) -&gt; (s ~&gt; f s) -- Choice gives us: left' :: (a ~&gt; b) -&gt; (a+s ~&gt; b+s) And if we get back to Lenses, this means that their type could probably be defined as: data Lens s a = forall f. Both (~&gt;), Applicative f =&gt; (a ~&gt; f a) -&gt; (s ~&gt; f s) -- with Both providing: both' :: (a ~&gt; b) -&gt; (a*s ~&gt; b*s) such that the duality between prism and lens is now also visible in the Van Laarhoven representation (?).
As usual, we can get there in one step without leaving lens. `lens` provides zippers. `lens` also provides `biplate`. `zipper` your data structure `within` `biplate` to go find anything you want ;)
I can't see any way to efficiently fuse the individual ByteStrings. I guess the way you'd do it in an imperative language is allocate an array of the length required (once you know the length), then copy each element into it. Actually, [this](http://hackage.haskell.org/package/bytestring-0.10.4.0/docs/Data-ByteString-Builder.html) may be of interest.
Here are a few more views. 1. `Lens' s a` means `s ~~ (a, k)` for some `k` where `(~~)` is "isomorphic to". 2. `Prism' s a` means `s ~~ Either a k` for some `k` We can define "pure profunctor" lenses and prisms. For this, we use the classes in [`profunctors`](http://hackage.haskell.org/package/profunctors) which I'll spell out here. class Profunctor p where -- compare to Functor{fmap} dimap :: (a -&gt; b) -&gt; (c -&gt; d) -&gt; p b c -&gt; p a d class Profunctor p =&gt; Strong p where first' :: p a b -&gt; p (a, c) (b, c) class Profunctor p =&gt; Choice p where left' :: p a b -&gt; p (Either a c) (Either b c) And now we have type PLens s t a b = forall p . Strong p =&gt; p a b -&gt; p s t type PPrism s t a b = forall p . Choice p =&gt; p a b -&gt; p s t
Generally you'll have to dive into the actually representation of lenses `lens` uses. It's well beyond the scope of this tutorial, but some of my others dive into it from various angles. I think SPJ's recent talk does a pretty great job as well.
You need to reverse them to make the category instance. {-# LANGUAGE RankNTypes #-} import Prelude hiding ((.), id) import Control.Category newtype Lens s a = Lens (forall f . Functor f =&gt; (a -&gt; f a) -&gt; (s -&gt; f s)) instance Category Lens where id = Lens id Lens f1 . Lens f2 = Lens (f2 . f1)
Right, but it's just reversed from function composition -- normal lenses still form a category with (Category..) = flip (.).
Ah, my mistake.
We have a few prisms for dealing with parsing-like behavior in lens. Notably there is a `base` prism in `Numeric.Lens` that lets you read and write numbers at various bases and there is a `_Show` prism in head that can be used to `Read`/`Show`. 
because of the cafe archives being regenerated, the old link is dead, http://www.haskell.org/pipermail/haskell-cafe/2013-September/110320.html is the current one
Thanks for sharing this, this will probably find it's way into some of my projects at some point. I actually started work on some haskell bindings for the videocore libraries[1] earlier this year, but I haven't really been giving them any attention for months. I used the system-gpio library from hackage with my test program for it, but it looks like your library offers more than it does. [1] https://github.com/Wollw/VideoCore-Haskell I feel the need to say that this library was written mostly just to experiment with. I don't think I commented much or maybe at all and I just sort of hacked most of it out while trying to get a basic opengl example going.
I don't mean to rain on anybody's parade here, but in many cases symbolic differentiation can be orders of magnitude faster than automatic differentiation. This is especially the case when you will evaluate the derivative/gradient many times, for instance for an optimisation procedure. There may be pathological cases as the article points out, but those are unlikely to arise as loss / posterior / likelihood functions for some data analysis problem. That being said, computer scientists of course are interested in all possible functions, which I don't have a problem with. [This paper](http://research.microsoft.com/en-us/um/people/bguenter/docs/symbolicDifferentiation.pdf) has some benchmarks. Anecdotally, this is also what we found when implementing (gradient-based) Hamiltonian Monte Carlo in Haskell for Baysig.
I was wondering what happens when you move from Applicative to Monad in the type of lenses?? 
Very interesting links. Thank you.
Also, *ad* unfortunately keeps you from using unboxed doubles/floats all along your pipeline. I still had decent performances with *ad* when I wrote an *ad*-based version of hnn, but I choose not to use it for my rewrite of [hnn](http://github.com/alpmestan/hnn). 
Will any/all of this be recorded and made available online?
I've begun the fairly painful process of consolidating a bit. Fortunately hackage 2 makes it easier to deprecate packages.
Thanks for the link, which is very interesting, but I think you mischaracterize the result. First this is about a _new_ symbolic differentiation algorithm. Secondly, the algorithm is equivalent to a heuristic for doing mixed mode automatic differentiation. I think the argument is really more about "naive" vs. "clever" subterm sharing in differentiation algorithms -- the graph structure section of the paper makes this clear. The choices they make can be applied equally well to purely symbolic or ad methods.
Don't throw away the error value, and that should help. &gt; let parseIRC = parse message "irc" &gt; parseIRC ":CalebDelnay!calebd@localhost PRIVMSG #mychannel :Hello everyone!\r\n" Left "irc" (line 1, column 30): unexpected " " expecting uppercase letter or digit edit: formatting
Why not just use SimpleIRC? I have a bot that uses this, no problem at all, it's really nice to use and stable (my bot has been connected for a year or so, and whenever it disconnects it gets back online by itself just by handling the disconnect event). http://hackage.haskell.org/package/simpleirc
I have a (minimal) IRC message parser implemented here: https://github.com/gaeldeest/NaBot/blob/master/src/NaBot/Parser.hs I tried using SimpleIRC, as suggested by another message, but did not have a very nice experience with it. Crashes or parse errors or both, I can't remember exactly.
Thanks, but I'm writing my own for learning. I'll edit my post to note this.
Oh thanks! I'm always over-complicating problems. First I actually wrote: let parseIRC = \s -&gt; either (\x -&gt; error x) (\b -&gt; b) (parse ...) And got the *error* type error, didn't realize the error message had a Show instance and printing it was uneccessary. xD Okey, so now comes the question, why does not PRIVMSG get parsed by command?
You've not actually specified that there are spaces allowed, your parser is expecting something like this, at least as far as I can tell: :&lt;prefix&gt;&lt;command&gt;&lt;params&gt;:trailing&gt;
I've been using SimpleIRC as well, and it works well enough. The only thing I'm not very happy about is the `IrcMessage` datatype, and the fact that it and `Command` are different. I added some event handlers to capture WHOIS-responses into a single structure (and the only way to send a WHOIS is through `sendRaw`, there's no corresponding `Command` constructor), but the fields are mapped weirdly for some of them. I went with using the `mRaw` field, and reparsing that to the `Message` datatype of [http://hackage.haskell.org/package/irc](http://hackage.haskell.org/package/irc), which I much prefer.
Control.Applicative is nice to use when writing parsers also! E.g. action = many1 upper &gt;&gt;= return . Action is action = Action &lt;$&gt; many1 upper etc.
That was the problem, thanks!