Yes, we would want that. That's what I meant by "with the semantics of `mapMaybe`"
It sounded interesting so I tried to install it (GHC 7.6.3, cabal-install 1.18.0.1, OS X 10.8.5), but I got the following compiler error message: Graphics/Rendering/Chart/Backend/Diagrams.hs:106:35: Couldn't match expected type `DSVG.Options DSVG.SVG R2' with actual type `Maybe S.Svg -&gt; DSVG.Options DSVG.SVG R2' In the second argument of `D.renderDia', namely `(DSVG.SVGOptions $ D2.Dims w h)' In the expression: D.renderDia DSVG.SVG (DSVG.SVGOptions $ D2.Dims w h) d In an equation for `svg': svg = D.renderDia DSVG.SVG (DSVG.SVGOptions $ D2.Dims w h) d Failed to install Chart-diagrams-1.0 
I would think so, otherwise `PartialFunctor` is just `Monoid + Traversable`. There's something more interesting going on if you consider `mapMaybe` to be your model.
That's a mistake, those blahs are different commands :) 
How can Sweeden and the Netherlands have more accepted papers than they submited? :-) I guess scaling is playing some tricks.
What version of diagrams did you have installed. Diagrams has had a few breaking changes recently. 
Actually, it is possible to make a `PartialFunctor` that is not a monoid. The simplest example would be `Maybe`.
Haskell adds three things to conventional shell programming: better parsing, better error messages/error handling and safer filepath manipulations (see `system-filepath` and `system-fileio`).
Ah, but `Maybe a` is a Monoid through `First` and `Last`, just not uniquely. I'm only using Monoid because it's common and has `mempty`—really `Failure` is perhaps better. Then you'd have `(&gt;&gt;= maybe failure return . f)`.
You can still do that with `(a -&gt; Maybe b) -&gt; f a -&gt; Maybe (f b)`, the type is flexible enough.
When I do "cabal info diagrams" it says "Versions installed: [ Not installed ]". I thought it was a dependency for Charts-diagrams? I will try installing it now. EDIT: I installed diagrams 0.7.1.1 but I still get the same error message.
I wonder if you could use a zipper to say that a step is removed from the tape iff you get a Nothing.
Yeah, agreed.
Sure, but once your type is locally-prunable, isn't it going to have an empty case? Or maybe `NonEmptyList` is "almost" locally-prunable and could have a local-prune-or-Nothing function that gets used here?
That should be pmap (f &lt;=&lt; g) = pmap f . pmap g -- not &gt;=&gt; 
For good measure, add a `Category` constraint. class (Category cat) =&gt; FunctorC cat f where ... 
its percentagewise, not absolute.
I know reports like this are on their own not much, and only the beginning, but I felt a big endorphin rush as the report got rolling and I realized what he was going to address. Its very important that he put this issue out there so directly in such a public venue.
Thanks, I always get the wrong one. Fixed.
`FactorialMonad` gives you a notion of locality, though it's not polymorphic. If we had `Traversable f + FactorialMonoid (f a)` then we can start with the `factors` law id == mconcat . factors and slip in the `mapMaybe` bit pmap f = mconcat . mapMaybe (traverse f) . factors
 class Final f where final :: f a class Final f =&gt; JoinsMaybe f where joinMay :: f (Maybe a) -&gt; f a joinMay . fmap (const Nothing) == const failure Tailcalled pointed out that there needs to be a way to state that joinMayWrong :: (Traversable f, Alternative f) =&gt; f (Maybe a) -&gt; f a joinMayWrong = fromMaybe empty . traverse id is wrong.
I wonder how you got version 0.7.1.1 of diagrams. Hackage has nothing between 0.7 and 0.8. The problem is that the current release 1.0 of Chart does not have a upper bound on the diagrams-svg version, but of course from 0.7 to 0.8 the additional API for the font embedding stuff has been added. So the current release of Chart only works with 0.7. I am going to change this for the next release.
yes, I could have guessed. Making a smaller bar for accepted would have conveyed how many papers were accepted vs submited.
I'm not sure if WPF fits this category, but if I had to choose one technology for the rest of my life for designing GUI applications, it would be WPF.
&gt; Honestly, IMHO the best way forward is Fay &amp; GHCJS and all the other techs that are hooking into the browser. I'm currently trying to do exactly that with the [threepenny-gui][1] project. What do you think? [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
I did cabal install -p diagrams. I can see version 0.7.1.1 at [hackage](http://hackage.haskell.org/package/diagrams). Maybe I'm doing it wrong somehow. :/
My bad I looked at diagrams-svg. EDIT: Until we release the next version of Chart-diagrams (sometime next week I guess), you could try installing diagrams-svg 0.7. That should get you up an running for now.
Doc builder now running. That was the last bit I think. Just need to check everything looks ok and then can switch the DNS...
It means you can remove some items, while leaving other ones, like `mapMaybe` does http://www.reddit.com/r/haskell/comments/1n5ess/partial_functors/ccfrq6f
Why's that?
My apologies, that was the other project I was trying to come up with the name of, but I could not remember off the top of my head. I have not used it personally (my current Haskell project has no GUI on it), but whatever the state it may be in today, I'm pretty sure it's the way forward.
I'd be curious to see what the whole tabular worklist code looks like if you can share it...
It's [pretty ugly](https://raw.github.com/rhymoid/hs-sugarfree/master/womb/BriefCase.lhs). And still *very* much under construction, especially the haskell-src-exts appendix. You should probably unlit it. -edit- [Here](http://lpaste.net/93454) is an unlit version, without the HSEA crap.
&gt; We received 0 panel proposals, and accepted 3 of them. Wait... what?
As a german, this will never happen to me: Has-KELL, Pas-KALL.
This is not at all what he implied.
All three panels were solicited :-)
I never implied that he did. Calm down there, friend.
but distinguishing that bad instance is just a law, and there's an easy one to state. `justMay . fmap Just == id`.
Curious, why do a runtime in Scala?
Nice work!
C, for the feeling of "truly controlling the machine" that only it (or assembly, bleck) gives you. There are a very small number of things I wish I could add to C (namespaces, multiple dispatch, closures) that would make it so much more usable... but on the other hand, it's never going to be a productivity language, so why bother. Rust, because in many ways it's the language that provides those extensions I dream of adding to C, but with more besides (memory safety and the like). I'm not a huge fan of its syntax, though; for example I think that having to stick ~ in front of so many things stinks. Also it's a really finnicky language, but then again so is Haskell, with a bit of a learning curve to get comfy. Idris; I haven't used it too much but I'd really like to get into it. I like that it's designed from the ground up to be capable of systems programming, but simultaneously explores a lot of cool theoretical concepts. Dependent types are weird to wrap my brain around, but I have no doubt they are a powerful tool against runtime errors. Many others. I used to name a few dynamic languages (esp. python, lua), but since I started working professionally in Python, I would say more than 80% of the bugs I've encountered have been due to mismatched types, and it almost boggles the mind why anyone would voluntarily choose to work in a language without types.
What makes python feel like haskell to you? I'd put the two very far apart from one another.
admittedly, mostly the syntax, they both feel quite "loose", despite having strict layout rules.
Great article! I have a project where I use a haskell program on a raspberry pi, and I currently have to use QEMU to get an ARM environment on my macbook and do the compilation. There's just not enough ram to do it on the pi. This could be a great alternative.
The FFI to Java, mostly. The rest of the codebase is in Java/Scala, and most of the hard work done by the edsl is actually written in Scala.
I tried to do what you said but to no avail. I might have done something wrong when trying to uninstall packages or installing the right ones. In any case, I think it is better that I wait until the next version is out, since we have spent enough energy on this already. Sorry for wasting your time on this.
I had some weird issue when I copied your link(some non-printing characters at the end? Like I said, weird.) [Here is a clickable](http://page.mi.fu-berlin.de/prechelt/Biblio/jccpprt_computer2000.pdf) one that works for me.
While we're on the subject, what is up with some packages broken hyperlinks on hackage? This seems to be a problem with packages uploaded recently, beginning a couple of weeks ago (before the migration to Hackage 2). If you don't know what I'm talking about, go [here](http://hackage.haskell.org/package/snap-0.13.0.2/docs/Snap.html) and try to visit any of the links to other modules. All those links resolve to some `file:///...` locations.
The invalid instance passes your law though `traverse id . fmap Just == Just`
Well done!
You need to distinguish between the `mtl` approach to transformers and the `transformers` library. The `transformers` library is really underrated in my opinion because it leads to much better type inference, error messages, and no operational ambiguity. Explicit lifting is a small price to pay for those benefits in my opinion and allows much greater precision and communication of intent. It doesn't solve all problems (like lifting catch), but for the problems it does handle it works really well.
First, it should be noted that if one is using `mtl` then one is necessarily using `transformers` as it is a dependency of `mtl`, so it is not so much a choice of whether to use `mtl` *or* transformers but whether to use `mtl` *in addition to* `transformers` or not. (I am sure that you know this but the way you worded it made it sound a bit like an either/or scenario so I thought a clarification would be helpful.) Given this, I completely agree with you that it is better to start with just `transformers` as it is good enough most of the time. I do disagree with you, however, that: &gt; Explicit lifting is a small price to pay for those benefits in my opinion and allows much greater precision and communication of intent. I would argue that this depends on how much `lift`ing is needed. If access to a lower level in the monad stack is only needed infrequently, then I agree with you that using `mtl` is overkill. If this is not the case, however, then `mtl` can be a godsend by preventing one's code from getting cluttered with `lift`s littered everywhere.
I hope I'm not hijacking this thread with this, but I see these types of posts here a lot, talking about the problems with monad transformers and monad transformer stacks, and my [`layers`](http://hackage.haskell.org/package/layers) package (in my opinion) solves nearly all of them. See the [overview](http://hackage.haskell.org/packages/archive/layers/0.1/doc/html/Documentation-Layers-Overview.html) for more details. I posted about it here before and got some feedback, but it doesn't seem like anybody has actually tried using it (apart from myself of course!). I have a [new version](http://github.com/duairc/layers) that's very close to release as well, I just need to update the documentation. I would love to get more feedback on it.
Mm, interesting. Fortunately we can upload new docs, but would be good to work out why that was happening. Could you file a ticket for it please.
Yeah, your clarification is what I meant. Even if you have to access a lower layer frequently, you can always just define: liftState = lift . lift . lift . lift ... and then reuse that.
Where is the official issue tracker for Hackage?
I don't remember where I heard this, but I recall some mention of ranking and comments being introduced to Hackage. Is this still planned?
True, but you still end up with "liftState" being littered everywhere which can add a fair amount of noise if it is used frequently.
I really enjoyed reading that overview. I've tried to get a handle on the history of transformer hacks each in isolation and failed in the past. But that was well explained. Edit: Nevermind, I never updated ghc on this computer. It builds fine. Edit 2: Works as advertised, but I could not figure out how to make a simple wrapper type ie: newtype App m a = App (StateT Int (ReaderT Char m) a) It requires monadLayer instance and maybe some other stuff? I'm not sure what.
If you have all that many you probably want to lift $ do {} several lines then
I think Faith Hill might be the problem. Try running it again, but with Metallica this time. ;)
Thanks, I completely missed that.
You can use the standard functions within the `do` block. Just place `input` upstream of them. You can even remove the `runEffect` and it will stream directly out of the `do` block as a newly minted `Producer`.
First, that only helps if the code being lifted comes in convenient contiguous chunks, and second, even in that case you end up adding a lot of indentation noise to the code.
Have you tried using the `Maybe` type? That in combination with a `fold` (and possibly a `map`) is how I would solve it.
whoops, that's a crummy law I wrote. we want stronger conditions too I think. Valid instances should probably somehow "keep all the `a`s". If we also have traverse, we can use it to give us a safe notion of observation, via the natural transformation to list, I think. `justMay . toList === toList . justMay` (I'm getting toList from the Foldable superclass of Traversable, but not using Foldable directly because while Traversable now has laws, Foldable still doesn't for the time being).
Planned in the sense of desired as far as I know. The best way to get new features now is to get hacking! The code is documented and readable, the #hackage irc channel is helpful, and now that hackage2 is running, as they say "there is more immediate gratification for volunteers contributing fixes and new features." Stars, reverse deps, usage stats, etc. all seem nice ideas. Badges for packages with test-suites, coverage and performance suites, etc. also were mentioned a long time ago and would be awesome. Comments I'm a bit less keen on as they are either unmoderated and messy or too difficult to keep approving, and they have a tendency to go stale. Maybe there's a good design solution for that though, so its certainly worth considering.
Thanks for this post ezyang! I rolled my eyes at those `lift . lift . lift` examples because they were so clearly contrived by people that know better just to make the current situation look worse than it does. I like the idea of thinking with effects instead of directly with monads in some cases, and would like to really look at genuine examples of which approaches do best in terms of code reuse, equational reasoning, etc. as this research continues. I'd also like to see some research tackling the `catch`/`mask`/etc. exceptions story a bit more, since all the current approaches are basically community-built and decent, but as the link in the post shows, perhaps trickier than we realize. This of course is about the relationship beween algebraic effects and continuations, which is... subtle: http://homepages.inf.ed.ac.uk/gdp/publications/comb_cont_journal.pdf
write it with recursion - then have a look at the structure and compare it to the "usual suspects" (map,fold - more concrete: look at the implementation of those) and decide which of them you just "reinvented" - then try to find a way to use this
Jan, the new hackage is out, and you can amend / edit version constraints on packages you maintain. now might be a good opportunity to use that feature!
I have another thought to add to my earlier reply: you could also try writing an adapter package that provides `mtl` instances for your interface classes so that people using `layers` can use `mtl`-based libraries; your adapter package could also go in the opposite direction and provide `mtl` instances for your interface classes so that people will feel more comfortable providing `layers`-based instances since the end-user can always import your adapter library to turn them into `mtl`-based instances. You've probably already considered this idea, as well as some or all of my previous suggestions, but I figured I'd mention it just in case because I like what I see in your package and want to see it succeed for my own selfish benefit so that one day I can use it comfortably instead of `mtl` and `MonadCatchIO`. :-)
What.
What about targeting the web instead? There are more and more options for doing at least Haskell-like programming in this space (ghc2hs, Elm, Fay...), and it seems like a domain where Haskell and FRP techniques are poised to explode.
This is super exciting! Let the feature creep begin!
He's looking for something that is * cross-platform, * easily buildable, * GHCi-friendly, and * OpenGL-compatible. Web based frameworks would certainly satisfy the first three. Maybe WebGL satisfies the last?
Honestly sometimes you just want to write an actual desktop application that's responsive, has a low memory footprint, isn't integrated into an irrelevant DOM, etc. I can't speak for the OP, but I for one would not like to see the Haskell community telling desktop application developers to just build an in-browser app instead. It's just not the same...
Indeed. GUI for the Web is a great opportunity for Haskell -- but it is not a replacement for a good GUI on the desktop. Still.. GUI for the web is probably a much better opportunity at the moment. Nobody has a wonderful or ingrained solution yet -- so we can still make an impact. On the web people face the problem of trying to develop apps that can be used via a desktop browser, a tablet browser, a smart phone browser, and possibly even native apps for iOS and android. This is a really difficult problem. If we can find a solution where Haskell, declarative and reactive UIs are a big win, then it could be huge for Haskell. Whereas it is hard to imagine Haskell making a revolutionary impact on the desktop GUI world. The desktop GUI world is difficult because it largely involves binding to very large and very OO interfaces. By the time you are done -- there is maybe not much room for innovation.
GTK is not the newest hottest prettiest thing, but it does fit all of those parameters. And the haskell bindings are very good. I've used it successfully on a few projects.
Nice feature and input. The problem is that I am the maintainer, but not the uploader and I guess that is why the system tells me that I don't have permission to do so.
Not wasted at all! Without your input we wouldn't have noticed this that soon.
my stackoverflow post: http://stackoverflow.com/questions/19045380/failed-to-compile-pandoc-with-cabal
Gabe Dijkstra? :) Yeah, some furniture also sold to me... BTW, I watched the presentation of his work about Hs2Gallina in the CS Seminar, it was pretty awesome :)
i like the tooltip, because it's exactly right. and even nicer in strongly typed languages.
I hope nobody minds me advertising [threepenny-gui][1] every time a question about GUIs pops up... [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
Unfortunately someone was having problems with GTK and Haskell just this week http://www.reddit.com/r/haskell/comments/1mxpis/just_started_coding_in_haskell/
This is an insightful article, though I admit I don't fully understand the ramifications. It would be nice if top level binds had type signatures though. It's harder to understand without them.
i like the tooltip, because i like the tooltip.
Hi, Jan! Dunno if you have figured that out already, but the daigrams-svg backend API has changed a bit. In particular 'Options SVG' datatype has changed: https://github.com/diagrams/diagrams-svg/commit/14b8150e0656d4c223a3a6e864f854b07324f26b
I can only give you general advice: - at the most install haskell-platform (optional cabal-dev) in global - for everything else use sandboxes (cabal-dev, cabal sandbox) - if something fails do not use &gt;= 0.21 &amp;&amp; &lt;= 0.3 - use = 0.23 in your cabal definition (search for the exact lib you are using) 
I do use =0.23 for all the hackage... but it didn't help. It is not the sandbox problem, because it is the only haskell program I run.
to quote Girard's most recent paper "a vicious circle is neither true not false, it is just vicious"
I'm a little skeptical about OpenGL in the browser, but it looks like WebGL would be useful to Conal, as he's compiling to GPU code anyway. Actually, I'm reading about WebGL right now, and it looks like Threepenny could easily support it.
Never managed to get it to compile on my mac, after hours of attempts.
Using a sandbox is still a good idea: If things go pear-shaped, you can easily delete and recreate the sandbox; all you lose is a bit of time to download and install the requirements again. Also, you don't need the libraries to *run* your program, only to *compile* it. GHC is an inlining compiler, that is, all Haskell libraries, and even the GHC runtime, are linked into the resulting binary; the only libraries you need at runtime are libgmp (for `Integer` math) and any C libraries that FFI code links against dynamically.
Would a simple fold work here? foldl1 (==) Correct me if I'm wrong
nub probably runs in quadratic time though doesn't it?
Tail recursion is great because it can afford a very elegant and efficient solution for problems modeled iteratively. In a tail recursive function the stack is reused, saving call overhead and data passing, and eliminates the need for unnecessary state and control due to previous values inside the function being available to the next recursion. When compiled, a tail recursive function is guaranteed to be equally as efficient as an inner loop because it actually is an inner loop.
Another law could be pmap (const Nothing) a = pmap (const Nothing) b I'm not sure if this is implied by your laws, but I think it isn't. ~~EDIT: It definitely isn't, as `pmap _ = id` follows the laws you gave, but not this one.~~ EDIT 2: I forgot about the type change.
Are you allowed to use a list comprehension? (Because that's definitely a way to do it.)
`foldl1 (==) []` is a bottom. I don't like bottoms. Sure, I could write it with a fold, but I'd end up with the same thing.
Is there anyone run : cabal install pandoc or cabal install pandoc-1.11.1 and goes well? I tried for a whole day and I am feeling really upset.
For `Data.List.nubBy`, Hackage gives [the following source code](http://hackage.haskell.org/package/base-4.6.0.1/docs/src/Data-List.html#nubBy): -- | The 'nubBy' function behaves just like 'nub', except it uses a -- user-supplied equality predicate instead of the overloaded '==' -- function. nubBy :: (a -&gt; a -&gt; Bool) -&gt; [a] -&gt; [a] #ifdef USE_REPORT_PRELUDE nubBy eq [] = [] nubBy eq (x:xs) = x : nubBy eq (filter (\ y -&gt; not (eq x y)) xs) #else nubBy eq l = nubBy' l [] where nubBy' [] _ = [] nubBy' (y:ys) xs | elem_by eq y xs = nubBy' ys xs | otherwise = y : nubBy' ys (y:xs) -- Not exported: -- Note that we keep the call to `eq` with arguments in the -- same order as in the reference implementation -- 'xs' is the list of things we've seen so far, -- 'y' is the potential new element elem_by :: (a -&gt; a -&gt; Bool) -&gt; a -&gt; [a] -&gt; Bool elem_by _ _ [] = False elem_by eq y (x:xs) = y `eq` x || elem_by eq y xs #endif So you're right, but it isn't easy to spot in the actual case.
I checked it with this in GHCi... take 2 (nub [1..1000000]) and take 2 (nub (take 1000000 (repeat 1))) Both run fine even though n^2 is large.
is haskell good?
I don't think there's a need for a package for this function. Your type can easily be encoded with Maybe; and your function becomes simpler if you use "all" and pattern matching: allEqualBy :: (a -&gt; a -&gt; Bool) -&gt; [a] -&gt; Maybe (Maybe a) allEqualBy f list = case list of [] -&gt; Nothing x:xs | all (f x) xs -&gt; Just (Just x) | otherwise -&gt; Just Nothing If you don't want to differentiate between VacuousTruth and SomeDifferent, allEqualBy becomes a one-liner. import Control.Monad (mfilter) import Data.Maybe (listToMaybe) allEqualBy :: (a -&gt; a -&gt; Bool) -&gt; [a] -&gt; Maybe a allEqualBy f xs = mfilter (\x -&gt; all (f x) xs) (listToMaybe xs)
And now you have the basis for Homotopy Type Theory.
Slightly underwhelming, after reading the title I hoped for something that could help me explain my love of FP to my parents (who have no idea what it's about, but keep insisting on how I should be doing something more mainstream for the sake of my budding career.)
I really do not know - but in similar cases I did a hard reset (it's rather easy for me - I do most of my Haskell stuff in a Ubuntu-VM so I just have to roll back to a previous snapshot) - but I would try to clear out all - just to be sure
These are the types of comments I come to the haskell subreddit for
Does that mean that a tautology is left adjoint to an observation?
 &gt; :t foldl1 (==) foldl1 (==) :: [Bool] -&gt; Bool So that would only work for a list of Bools. EDIT: also, its semantics is not right. &gt; foldl1 (==) [True, False, False] True 
How about this: `\xs -&gt; all (== head xs) xs` Even though it's got `head` in it, it's a total function. I learned about this trick from someone of my ex-colleagues at Barclays (probably Ben Moseley).
It sucks, but I'd start over. Get rid of GHC and the Haskell platform, wipe your local .cabal directory, and reinstall everything.
Not at all! Seeing activity is important - even if I can't try it out any time soon, knowing that it's something that you're investing time in means that should be one of the first things I check.
trollololol.
The main issue with GTK is building on Windows and Mac (from what I hear, maybe it has improved). On Linux it is a dream.
 &gt; (\xs -&gt; if all (== head xs) xs then Just (head xs) else Nothing) [] Just *** Exception: Prelude.head: empty list 
&gt; In Haskell, a tail recursive function can cause a stack overflow And maybe a heap overflow before, because of the unevaluated thunks as parameters that will keep accumulating. So it's technically not the tail recursion that cause the stack overflow, but the evaluation of its result, and at this moment the tail recursion is already finished.
Of course, `all` can be vacuously true.
the tooltip is: "Functional programming combines the flexibility and power of abstract mathematics with the intuitive clarity of abstract mathematics"
Done, I think.
Yes, I know. And the comic itself references tail recursion. And ~~tail recursion~~ tail call optimisation isn't a part of abstract mathematics. Anyway, a program doesn't have the intuitive clarity of abstract mathematics if it's obscured by dealing with real-machine issues, and it doesn't have the flexibility and power of abstract mathematics if it blows the stack without giving the result. The reality for real-world programs is that you can choose one or the other - a beautiful program that doesn't work in practice, or an ugly one that does. Luckily, the ugliness is relative and usually quite limited, but the abstraction is still leaking and that still means you can't have the perfect ideal the tooltip claims. And that's even if you accept that abstract mathematics is a perfect ideal. Don't forget the old joke about mathematicians *almost* systematically abusing notation - if the notations and methods were perfectly clear and intuitive, that abuse wouldn't be necessary. 
&gt; Edit: Nevermind, I never updated ghc on this computer. It builds fine. The new version of `layers` will build with every version of GHC from 7.0 to 7.8. &gt; Edit 2: Works as advertised, but I could not figure out how to make a simple wrapper type ie: &gt; &gt; newtype App m a = App (StateT Int (ReaderT Char m) a) &gt; &gt; It requires monadLayer instance and maybe some other stuff? I'm not sure what. Yes, it requires `MonadLayer` and some other stuff :) I've given a complete example of all the instances that are required below. I know there are a lot of type classes, but bear in mind that your `App` monad automagically has pass-through instances for every monad interface. So, for example `App Writer a` is a `MonadWriter`. With `mtl` you would actually need much more boilerplate to achieve this. {-# LANGUAGE GeneralizedNewtypeDeriving #-} {-# LANGUAGE TypeFamilies #-} module App where import Control.Applicative import Control.Monad import qualified Control.Monad.Trans as T import Control.Monad.Trans.Reader (ReaderT) import Control.Monad.Trans.State (StateT) import Control.Monad.Layer import Control.Monad.Interface.Reader import Control.Monad.Interface.State newtype App m a = App (StateT Int (ReaderT Char m) a) deriving (Functor, Applicative, Alternative, Monad, MonadPlus, T.MonadIO, MonadReader Char, MonadState Int) instance T.MonadTrans App where -- for compatibility with transformers lift = layer instance Monad m =&gt; MonadLayer (App m) where type Inner (App m) = m layer = App . layer . layer layerInvmap f _ = layerMap f instance Monad m =&gt; MonadLayerFunctor (App m) where layerMap f (App m) = App $ layerMap (layerMap f) m instance Monad m =&gt; MonadLayerControl (App m) where newtype LayerState (App m) a = L {unL :: LayerState (ReaderT Char m) (LayerState (StateT Int (ReaderT Char m)) a)} restore = App . (&gt;&gt;= restore) . layer . restore . unL layerControl f = App $ layerControl $ \run -&gt; layerControl $ \run' -&gt; f $ \(App m) -&gt; liftM L $ run' $ run m instance Monad m =&gt; MonadTrans (App m) where type Outer (App m) = App transInvmap f _ = transMap f instance Monad m =&gt; MonadTransFunctor (App m) where transMap f (App m) = App $ transMap (transMap f) m instance Monad m =&gt; MonadTransControl (App m) where transControl f = App $ layerControl $ \run -&gt; layerControl $ \run' -&gt; f $ \(App m) -&gt; liftM L $ run' $ run m Admittedly I hadn't tried to do something like this before, and getting the composition of the two `layerControl` instances correct was very tricky, so I might include something that makes this a bit easier in the next version of `layers` (a bit like the [Defaults for `MonadBaseControl`](http://hackage.haskell.org/package/monad-control-0.3.2/docs/Control-Monad-Trans-Control.html#g:4) stuff from the `monad-control` package).
My intent was not to argue against your advice.
I think the issue on Mac is largely related to getting the right GTK libraries installed. I've never quite gotten GTK to play nice. wxHaskell might still be an option, but I imagine it's in need of an update.
Make use of laziness to simplify your approach: instead of finding one element that has key a, find *all* elements, and pick one.
OK, sorry.
Hetzner allows us to pretty easily set up an SSL certificate and we should do that, using the haskell.org funds.
Okay, well I see that I was on the right track, I just didn't keep going for long enough. I don't think I would have succeeded, so thanks for laying that out for me. I agree some defaults would have been very helpful.
[This blog post](http://alenribic.com/posts/2012-08-06-running-haskell-on-raspberry-pi.html) seems to get GHC working on a Raspberry Pi (and it's not _that_ recent), so it seems you don't need _this bad_ a diet GHC on a BeagleBone (*). Do you plan on trying it? (*) The only thing I haven't found is the state of cross-compiling for GHC, so that you don't have to wait for you BeagleBone for hours...
Yeah, it doesn't directly do what you need, it was more in the spirit of the /u/picklebobdogflog's suggestion that returned Bool.
I'm looking forward to most of them. I'm a little less interested in what various companies are up to, but even those talks should be pretty interesting.
Nice! Make sure you post to functionaljobs.com and haskellers.com that's where I check whilst toying with the idea of changing career paths.
Could you elaborate this point, please? Or point to a source?
`nub` runs in O(n*k) time if you observe the entire list, where n is the length of the list, and k is the number of distinct values in the list. If there is only one value, that is O(n). However, you are also only interested in knowing if there are two or more distinct values, which only requires observing until you see two values. This can run in O(m) time, where m is the length of the initial prefix of identical values. The first element will cause the rest of the prefix to be eliminated, and when you see the second element (or, observe that the tail is not nil), you are done, and nothing further need be computed.
Congratulations and good luck.
John MacFarlane's answer (you don't have enough RAM to build pandoc) is highly likely to be correct. Of course, any problem encountered when running `cabal install` is "cabal hell"...
I couldn't find where to do a job post on haskellers.com, in fact I didn't even know there was a job board there? I'll get a post on functionaljobs as soon as the man with the credit card gets back next week :-)
I like the type of the tooltip, very informative, and keeps me from confusing it with other data that might not be appropriate.
My memory is : ~$ free -m total used free shared buffers cached Mem: 1000 958 42 0 25 228 -/+ buffers/cache: 705 295 Swap: 255 255 0 I still have 295M, so I don't think it is because of not enough memory.
Pandoc takes a LOT more than 295M to build, and possibly even more than your entire 1000+255M of virtual memory.
Oh, I see, so it seems I have no way to build it on this machine?
I'm a versatile programmer, but I generally only consider FP jobs as they're much better at keeping me sane and motivated. I bade my time in Pascal, C, Java, now I can't stand imperative semantics, especially when it comes to concurrency and symbolic computing. I can write perfect Enterprise Java like a good little drone, but I feel like it sucks the life out of me.
Thanks :-)
This is my dmesg info: mppe_compress[0]: osize too small! (have: 1404 need: 1408) ppp0: ppp: compressor dropped pkt Is this means I failed because of lake of memory? 
I'd kill for that role, but I am a measly undergrad. Would you accept applications for a summer internship?
You could try [adding one or two gigs of swap](https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Storage_Administration_Guide/s2-swap-creating-file.html), though the compile would be very slow.
I'll float the idea, to see how loud the protests are - what about forgoing the bindings to an OOP gui library, and instead building a library from scratch, on top of GLFW-b or SDL? Cons: - Reinventing the wheel - Long time before we can reach feature parity - Bad fit for the haskell model, guis really need OO? Pros: - Possibly fun place to innovate (w/ ideas from diagrams, TV) - Hindsight makes API design targeted to Haskell users &amp; FRP easier? - End of all complaints about difficulty with building - Another place to show off how mundane tasks (making widgets) can enjoy terse Haskell syntax - And a place to show off how abstract math can structure - Quiet some criticisms that practical, real-world things are either too difficult for or not interesting to Haskellers (I think GUI library falls in this practicality category) If there were no GUI libraries on Earth, and the Haskell committee of yore convened to decide on a low-level (below the FRP stage) way for users to interact with programs via mouse and keyboard, what designs would they come up with? What abstractions? Would they have signals and sockets? Or something different out of left field?
I'm curious to see what I will say about Uniplate - should write those slides now my wife's thesis is finished!
Wow, it looks like your tech stack is nearly identical to what I'm using for my business that I'm working on. The only difference is that I'm using RabbitMQ instead of 0MQ. I have HAProxy fronting multiple HTTP APIs and my non-http stack items (Redis, PostgreSQL, Rabbit, ElasticSearch), and an nginx instance doing static file serving the Angular frontend. It's a shame that I don't live in the UK. I'm wondering what you've done for interfacing with ElasticSearch? There aren't any existing libraries for it on Hackage that I'm aware of. I've been working on writing a full client for it using http-conduit / aeson, but there's a lot of API surface area to cover for ElasticSearch.
&gt;1958 - John McCarthy and Paul Graham invent LISP. Due to high costs caused by a post-war depletion of the strategic parentheses reserve LISP never becomes popular[1]. In spite of its lack of popularity, LISP (now "Lisp" or sometimes "Arc") remains an influential language in "key algorithmic techniques such as recursion and condescension"[2]. --A brief, incomplete, and mostly wrong history of programming.
If there will be people who want to push GUI in Haskell, then I recommend checking out how Racket solves this problem. It have a signle GUI api but does actually use different toolkits depending on what platform it's compiled for. On OS X it uses cocos(i think it's called like that), on Linux it uses GTK and on Windows its native gui library. Here's a post regarding it http://blog.racket-lang.org/2010/12/racket-version-5.html Not saying you should copy its api, but there implementation choice, using the native backends beneath sounds really good if there will be progress on a serious GUI library for Haskell.
If you round up all prominent GUI frameworks you will see that practically all of them require millions of dollars investment and/or decades of work. Microsoft WinForms and XAML, Apple's Cocoa, Sun's Swing and IBM's SWT. QT and GTK (decades in the making). The barrier of entry into the GUI market is very high. And the chances that a couple of open source developers will make it from scratch in their free time are slim. 
&gt;you can't have the perfect ideal the tooltip claims The tooltip is not making that claim, it is derisive.
tail recursion sort of requires a base case, doesn't it? It is guarded recursion that is *really* its own reward data Stream a = Stream a (Stream a) smap f (Stream a stream) = Stream (f a) (smap f stream)
I think the most obvious solution here is allEqual :: Eq a =&gt; [a] -&gt; Bool allEqual [] = True allEqual (x:xs) = all (== x) xs ... which is O(n) in (worst) time, somewhat short-circuiting, traverses the list only once, doesn't blow up the stack, and all those good things. If you want to write that in terms of your custom datatype the conversion is straightforward, allEqual :: Eq a =&gt; [a] -&gt; AllEqual a allEqual [] = VacuousTruth allEqual (x:xs) = if all (== x) xs then AllEqual x else SomeDifferent
I mentioned a couple of days ago that GUI libraries are ___HUUUUGGGEEE___. This hugeness is not just expressed in source code size; GUIs are essentially conceptually huge, using "essential" here as a contrast to "accidental". Smashing together "a" GUI that, say, has a button, and the user can click the button, and maybe a terrible text input box that can barely wrap letters, numbers, and spaces properly is one thing. Writing a GUI that anyone would use on purpose is quite another. Just a standard multiline text widget has a mind-boggling array of functionality out-of-the-box in a real GUI toolkit. A really not inclusive list: * Implements selections, copy, cut, paste, and papers over the OS differences involved in the cross-platform case * Extensive navigation: HOME, END, right, left, up and down (non-trivial when using a proportional font), CTRL+ all of the above, SHIFT+ all of the above for selections * FULL Unicode support. Including embedded RTL and LTR markers, to say nothing of what else simply _rendering text_ properly requires, and to say nothing of what else this implies for the previous things too And that's still just a text widget that can take a hunk of text and just display it with no formatting. Add rich text support, watch the requirements skyrocket. And you've still got layout (harder than it looks), grid controls, tree controls, interactions for all of the above, who knows what else... it's an _enormous_ problem to make an even remotely competitive GUI, and FP isn't going to magically be 1000 times easier than OO here.
I hear he's curious about this too...
tl;dr
Yes, when this is possible I find it so much better than the implicit use of mtl instances. It makes the separation of concerns between each transformer layer apparent at a glance, and enables refactoring later. Much structure is hidden when you have a large flat `do` block that makes use of many typeclasses.
I'm working on a project management / wiki / calendaring / chat platform. At a TL;DR level, it's for helping people keep their shit together between work, home, and other areas of life in a more centralized way. Things that I'm taking inspiration from: * Campfire * Emacs' Org Mode * [The Hit List](http://www.potionfactory.com/thehitlist/) * Microsoft Exchange's calendaring system. I'm currently based out of Washington state, hoping to land a Haskell job somewhere in Japan within the next 6 months or so (GREE or Tsuru Capital or something).
myLookup :: Eq a =&gt; a -&gt; [(a,b)] -&gt; b myLookup key lst = head (filter (\ (k,v) -&gt; key==k) lst) this is where i have gotten but I am getting an error: Could not deduce (b ~ (a, t0)) from the context (Eq a) bound by the type signature for myLookup :: Eq a =&gt; a -&gt; [(a, b)] -&gt; b at zwehner_hw3.hs:33:13-37 `b' is a rigid type variable bound by the type signature for myLookup :: Eq a =&gt; a -&gt; [(a, b)] -&gt; b at zwehner_hw3.hs:33:13 In the pattern: (k, v) In the first argument of `filter', namely `(\ (k, v) -&gt; key == k)' In the first argument of `head', namely `(filter (\ (k, v) -&gt; key == k) lst)' 
Well yes that was the point; the definition is however `rekursiv` in the original syntactic sense, as can be seen by inspection.
Couldn't help notice you have a monoid (`SomeDifferent` is zero, `VacuousTruth` is identity) there. Actually after talking on the IRC channel, I've discovered it's a semigroup and you can use `Maybe` for its identity. import Data.Semigroup data Equating a = Diff | Same a deriving Show instance Eq a =&gt; Semigroup (Equating a) where p@(Same a) &lt;&gt; Same b | a == b = p _ &lt;&gt; _ = Diff λ&gt; foldr ((&lt;&gt;) . Just . Same) Nothing "" Nothing λ&gt; foldr ((&lt;&gt;) . Just . Same) Nothing "aaa" Just (Same 'a') λ&gt; foldr ((&lt;&gt;) . Just . Same) Nothing "aba" Just Diff λ&gt; Can't speak for its efficiency but yeah.
All these Haskell jobs and so few in the US =(
But no, functional programming is much more precise that common language of abstract mathematics :-). After all an English statement doesn't have to typecheck and have unambiguous meaning, or does it?
Thanks!
You may want to create an account on Quora and see if you can find more people. There is even a question currently unanswered "Who is using haskell in industry in London".. or something to that effect
Assembly was mainstream once upon a time...
I had a great idea for a typesafe API to ZMQ that would ensure you couldn't write from a socket in a read state, or vice versa. Ping me if you want to chat about it.
I'm guessing that there are more in the US than you think. They probably just never see the light of day because they get filled by personal connections before they ever need to put out a job posting.
That was my question! Pretty awesome that I can now go and answer it! :-D
ha! well I wish I could help more.. I'm a comp sci student noob, just learning haskell at the moment for fun!
Convert the list to a set, compare length to `1`.
Congratulations! I'll be in Brighton from the 14th of October, doing Haskell in the backend as well. If you are around ping me and we'll have a beer :)
In HoTT, you start with the notion of a loop at a type, which is analogous to reflexivity (that is, if you have an `x : A`, then you can construct an `Id x : Path A A`). From that basis you prove all sorts of stuff somehow. You can then start talking about a `Path (Id x) (Id x)`.
I'm very curious about this. I've always been interested in what all the plates are about, but never got round to using them.
This takes O(n log n) in the *best* case for a problem which can be done in O(n) in the *worst* case.
Nice, I'll see you there on the beach then :P
Is this path induction? Maybe I should try to tackle the first chapter of the HoTT Book. Or is somebody out there selling brain implants of this chapter? I'm so lazy .. anybody? 
Provided the HTTPS issue gets resolved, doesn't this leave the database vulnerable to rainbow table attacks? Most people in web dev now seem to be doing salted passwords and use a better hashing function like bcrypt?
But... a tail recursive function doesn't return _itself_, it retuns whatever _value_ the called version of itself returns. And that distinction definitely matters in a functional language. 
yay!
maybe we should just fix that then
Fitst, congratulations! Second, what do you do? Something finance?
For the love of god, generate Qt bindings already. :)) There's a [shitstorm](http://www.reddit.com/r/haskell/comments/1mxpis/just_started_coding_in_haskell/) that is [brewing](http://www.reddit.com/r/haskell/comments/1n8c5i/conal_elliott_searching_for_gui_and_graphics/) in haskell community around GUI tools
It's not even path induction. Path induction is something you can do with these paths. The first chapter of the HoTT book is very detailed, so if you have any background in type theory it may be a bit slow. [This](http://www.cs.cmu.edu/~drl/pubs/ls13pi1s1/ls13pi1s1.pdf) paper is much shorter and **much** denser.
Here is your code reformatted for readability: coherent :: Eq a =&gt; [(a,b)] -&gt; Bool coherent [] = True coherent lst = let total = 0 + map((k,v) -&gt; coherentHelper k lst) lst in if total &gt; 1 then False else True coherentHelper ::Eq a =&gt; a -&gt; [(a,b)] -&gt; Int coherentHelper key lst = if length(filter(\(k,v) -&gt; key == k) lst) &gt; 1 then length(filter(\(k,v) -&gt; key == k) lst) else 0 First of all, you have a type error in `coherent`, since `map` returns a list of ints, which cannot be added to an int. Running this in GHCi gives the following error: Could not deduce (Num [Int]) arising from a use of `+' I think what you're looking for is to sum the result of mapping `coherentHelper` over the list: coherent lst = let total = sum $ map((k,v) -&gt; coherentHelper k lst) lst in if total &gt; 1 then False else True which seems to give the correct answers. That said, there are a number of additional changes which you could make to your code to aid readability: 1. Consider using `not` instead of the if..then statement you have in `coherent` 2. Given that you never use the values in the array, you could prefix the entire computation with a `map fst` to just pull out the keys. 3. Once you've done that, a lot of your filter conditions can be shortened to the partially applied form `(== key)`. Finally, consider using standard library functions such as `nub` to detect duplicates. I won't give the shorter version since you might want to find it yourself, but I think you can make this into a one-liner using `nub` and `map fst`.
I've compiled a list of various resources I've used or wish to use sometime in the future. https://gist.github.com/leroux/6395804 There are LYAH exercises available at https://github.com/noelmarkham/learn-you-a-haskell-exercises.
Well, I wanted to say something about when you could get rid of lift, and in transformers you can't get rid of lift entirely. It is a bit hard to say what the type inference and error messages of effect systems will be.
My favorite: &gt; Bjarne Stroustrup bolts everything he's ever heard of onto C to create C++.
This sounds neat. I'd be interested in trying it out at some point, at least experimentally, for line detection (Hough transform) in a project of mine. I took a look at some other OpenCV bindings, and I remember there was one that used `unsafePerformIO` in ways that were not at all sensible, so I'm glad to hear there is someone taking a more principled approach.
[Yes](http://hackage.haskell.org/package/constraints-0.3.3/docs/Data-Constraint.html#t:Dict), in Edward Kmett's constraints library. The second, third, etc. things you want to do are also in there.
That's it. Thanks!
Buttons and drags? :) I don't know category theory yet, sorry. I'm more at the "chairs are a category, fish is a category" level of understanding. Is this a serious question though? If I were to write a (e.g.) GUI library should my first thoughts be about finding the category model that my thing could fit, and then ensuring that I follow the laws and whatnot?
&gt; Is this a serious question though? No :-)
Benchmarks, anyone?
i posted this exact same question a few weeks ago, but i'm glad to see that at least a major proponent of haskell agrees with me on the state of affairs on GUIs and graphics in haskell. for GUIs, would the WPF approach be something to consider? a haskell version of XAML could be composed, and designers could soon follow. it seems well suited to UIs given C# and F#'s success. given my disappointment in haskell libraries for graphics and GUIs, i have been trying to learn C# and F# due to their overwhleming collection of usable libraries and because of WPF. i personally think bindings are a poor solution. a native haskell solution is imperative. :)
Do you have a working recursive version? Stare at the Prelude functions a bit and try again but without using any of the functions you just tried to use :P
Sorry forgot to include I cannot use recursion for it as that is a parameter of the problem.
You can use `filter` to keep only pairs that match, but you can also use it to remove pairs that match...
I emailed the haskell.org admin alias about this. They definitely have the money but need someone to take the lead. Someone needs to figure out what kind of cert we need (single domain or wildcard cert) and how to set it up correctly on the Hetzner machine.
Well, foldable alone can't have laws, since a single method isn't enough to define laws with -- you need to at least have two methods to relate algebraically. And if you add functor, then you do get some laws, but they come from parametricity directly. However, if we add a dual class, Buildable, then we get some actually meaningful laws from the interactions between the two, at least: https://www.fpcomplete.com/user/gbaz/building-up-to-a-point-via-adjunctions
haha yea figured this out right before you posted. Thanks a lot though!
I was just expecting that with your nick, you wouldn't miss the chance to apply a "co". :-P
We don't need HoTT for this. I suspect classic Intuitionistic Type Theory suffices. "a tautology is a tautology" is a propositional equality. "A tautology is a tautology, is a tautology" is judgmental equality. Now to talk about that judgmental equality as a propositional one we need to work one universe up, and from thence an inductive hierarchy. 
This is a great collection of material. It would be doubly great, so this doesn't fade away, if someone (OP?) could go through and update the haskell learning resources page on the HaskellWiki http://www.haskell.org/haskellwiki/Learning_Haskell and perhaps any other pages that might benefit from links to some of this content.
Thanks for poking us about this. I think folks are fully onboard with SSL. Just as a plug, we can always use more experienced sysadmin/ops volunteers, and if someone would like to step forward to help take this on, that would be great! the admin@ address, the #haskell-infrastructure irc channel, and the haskell-infrastructure list are all good ways to get involved and get in touch. (Actually, our current sysadmins have _lots_ of security experience, but they're also stretched thin, so its not so much this particular capacity as just more hands on deck. Someone with experience munging mailman archives, on the other hand, would have a skill we have in very short supply :-) )
&gt; what is getting returned isn't a function Well, it *could* be a function, but only incidentally.
Thrice (embedded)?
Yes but for the sake of the joke I didn't want to be too wordy.
I worked in embedded stuff in the mid 90s, and that was in C - being able to read assembler was handy for debugging, but no need to write it. No doubt some embedded developers write in assembler even now, but it's not mainstream. I'm sure there are plenty of niches where assembler is still necessary, though - compiler writers, for example. 
I broke out the raw c wrappers and haskell bindings into its own package: [opencv-raw](https://github.com/arjuncomar/opencv-raw.git). I'll be pushing it to hackage as soon as I can. EDIT: Ok, it's [up](http://hackage.haskell.org/package/opencv-raw). You should be able to cabal install opencv-raw --with-gcc=g++ if you have OpenCV 2.4.6. If not, cabal unpack opencv-raw ./setup.sh &lt;path to opencv include directory&gt; cabal install --with-gcc=g++ Let me know if the above doesn't function for you.
 coherent = all (null . tail) . group . sort . map fst Shame about the applications, but at least there are no points. 
Can someone explain this to me? I don't get it, and explain xkcd has the same general explanation.
Haha, true, sorry for that :)
The first thing I wanted to do once I had `ConstraintKinds` was make `Set` an instance of `Functor` and `Monad` using `RebindableSyntax`. Add `MonadComprehensions` and get set comprehensions for free. :) Unfortunately this doesn’t work with GHC’s SQL-like comprehensions (`TransformListComp`) because the types are too restrictive. :( 
Uniplate is badass.
OH GOD PERL. At least it's cleaner than my original.
Thank you. Perl is lovely—unabashedly, hilariously eclectic, yet also eminently scalable and maintainable if you know what you’re doing. As dynamic languages go, it’s my #1. Haskell excels for serious software, but not all software is serious—nor should it be.
I'm curious to see why there are hardly any good functional programming positions in Los Angeles.
yes. Up to isomorphism `Int ~ Char` is initial and `Int ~ Int` is final or at least if we assume uniquness of identity proofs (the morphisms themselves are easy).
Well, these serve as defenses against different threat models. HTTPS is for preventing passive network analysis on your wire (although all the tubes your bits travel over are hostile, but this is still different.) A KDF like `scrypt` is designed to prevent people who will have the password in secure form, physically (abiding by Kerckhoffs's principle.) One is independently bad, apart from the other. But yes, it should be fixed after looking at it. And probably wouldn't be very difficult to do so. As for HTTPS, a rollout just for the site will probably happen soon (with something in `cabal-install` afterwords, hopefully.)
Yeah, well neither the US or the UK are the worst regarding the number of Haskell jobs :p
I'm sure you can do better (read shorter) than this, but this works in GHC 7.6.3 with relevant extensions coerce :: Dict (a ~ b) -&gt; a -&gt; b coerce Dict a = a type family Arg a b c type instance Arg Int b c = b type instance Arg Char b c = c newtype SameArg a b = SameArg (forall c d. Dict (Arg a c d ~ Arg b c d)) sameSameArg :: SameArg a a sameSameArg = SameArg Dict genSameArg :: Dict (a ~ b) -&gt; SameArg a b genSameArg Dict = sameSameArg sameArgImpossible :: SameArg Int Char -&gt; Dict (a ~ b) sameArgImpossible (SameArg d) = d makeCoercion :: Dict (Int ~ Char) -&gt; a -&gt; b makeCoercion = coerce . sameArgImpossible . genSameArg proveTrue :: Dict (Int ~ Int) proveTrue = Dict makeDict :: Dict (Int ~ Char) -&gt; Dict c makeDict d = makeCoercion d proveTrue liftDictFun :: (Dict a -&gt; Dict b) -&gt; a :- b liftDictFun f = Sub (f Dict) exfalso :: (Int ~ Char) :- a exfalso = liftDictFun makeDict That was just the "how do I make sure to avoid any spot where GHC might possibly yell at me" approach. So, for example, we only do the bad stuff indirectly using type variables. I can probably shorten it up if you want. EDIT: nicer version bizarre :: (Dict (Int ~ Int) ~ Dict b) :- b bizarre = Sub Dict type family Arg a b c type instance Arg Int b c = b type instance Arg Char b c = c sameArgSub :: (a ~ b) :- (Arg a c d ~ Arg b c d) sameArgSub = Sub Dict exfalso :: (Int ~ Char) :- c exfalso = bizarre `trans` sameArgSub
You can't quite do what you want, but you can get close. type family TF1 (a :: Bool) :: * type instance TF1 True = Int type instance TF1 False = Char type family TF2 (a :: Bool) :: * type instance TF2 True = Int type instance TF2 False = Char There's no way to do dependent case analysis on type level data directly, but you can force it to happen by doing the corresponding case analysis at the value level, using the singleton family for `Bool`. data Booly :: Bool -&gt; * where Truey :: Booly True Falsey :: Booly False Given a run-time witness as to which `Bool` you have in the types, you can work by cases. convert :: Booly b -&gt; TF1 b -&gt; TF2 b convert Truey i = i convert Falsey c = c It's annoying to have to keep this extra information at run time, because it doesn't really do anything here. But that's for a rather special reason, namely that `TF1 b = TF2 b` irrespective of `b`. It'd be lovely to be able to give a proof of this fact that was sufficiently trustworthy to live purely (by which I mean totally) in the static language. However, preventing us from being recognizably honest remains policy. Whether that's the best policy I leave for others to judge.
This is what I came up with: type family TF (a :: *) (c :: Constraint) :: Constraint type instance TF Int c = () type instance TF Char c = c thing :: (Int ~ a) :- (TF a c) thing = Sub Dict exfalso :: (Int ~ Char) :- c exfalso = thing 
oh yes, that looks nice. Much simpler.
Your Booly leads to a rather simpler statement of the problem. Define this: allBooly :: forall b. Booly b
It can't be done in Haskell, and I'm glad. I don't like it because the witness depends on the choice of instantiation variable. Parametricity is important to us. Both for the implementation options (type erasure) and for theoretical reasons (theorems for free). More generally [this paper](http://arxiv.org/abs/1110.1614) convinced me that parametricity is closely related to the very idea of being constructive. You don't need to internalize "uniform validity" into your system, but doing so seems like a good idea if it is so important to semantics. `allBooly` would have to have a pi type. I like pi types, but they have a different operational interpretation from the universal quantification in Haskell. The lack of Haskell style quantification is a turn off for me from languages that just have the one kind of binders (like Coq, Agda, and Idris).
I agree. The absence of "static" dependent quantification from dependently typed languages is becoming increasingly annoying (and, no, the "irrelevant" quantifier of Agda isn't what I want). I'd also like to see Haskell acquire a "dynamic" dependent quantifier (i.e., Pi) in addition to, not instead of, its `forall`.
Hey! Do you know about our [Functional Brighton meetup](http://www.meetup.com/Functional-Brighton/events/140863032/) on 15 October?
With apologies for thread noise, see [Functional Brighton 15 Oct meetup](http://www.meetup.com/Functional-Brighton/events/140863032/) :-)
Seems cool, but I couldn't understand the role of Haskell. Can you expand on that? :)
I talked to SPJ the other day about this. He still wants to turn Any into a type function rather than an actual value so it cannot be matched on in a type family, but it hasn't happened yet.
how can you use digest auth when storing passwords not as digest hash or clear text? hint: you can't. that problem cannot be fixed without another round of password rehashing.
http://mitpress.mit.edu/sicp/full-text/sicp/book/node14.html
Yay, glad you like it. I'm well aware that it can be improved, but I think it's a good start using BM25F. That ranking scheme can be extended to include a "package rank" which should make a big difference.
This is why I love `cabal` cabal: The following packages are likely to be broken by the reinstalls: haskell-packages-0.2.2 haskell-names-0.3 fay-0.18.0.0 fay-base-0.18.0.0 lens-3.9.1 force-layout-0.2 diagrams-contrib-0.7 diagrams-0.7.1.1 MemoTrie-0.6.1 vector-space-0.8.6 vector-space-points-0.1.2.1 diagrams-core-0.7 diagrams-svg-0.8.0.1 diagrams-lib-0.7.1 active-0.1.0.6 Use --force-reinstalls if you want to install anyway. Why can't I have several versions of the same package at the same time? 
What I have heard from more experienced Haskellers is that the more you use the language, the less recursion you will find yourself doing. Not because it is memory-inefficient. But because it has been abstracted out into things like 'map' and 'fold'. As you practice, when you find yourself wanting to do something with recursion, see if you can figure out a way to leverage higher-order functions instead.
That's an excellent article. Thanks!!
I probably don't understand the problem fully (my apologies if my question seems dumb). Normally I guess if I had a function that looked for the max element in a list I would just use a for loop that runs for a range equivalent to the length of the list.
The post you linked doesn't _really_ apply to Haskell. In Haskell, properly written recursive calls (strict tail calls, IIRC) perform exactly like loops. That said, good Haskell style is to avoid explicit recursion in favor of composable higher-order functions like `fold`, `map`, and `filter` whenever you can. GHCI is generally slow because it's interpreting and Haskell gains a lot of performance from compilation.
Thanks, I'll keep that in mind. I still have some ways to go in my Haskell studies, but I wanted to get the scoop before I got to that level as I find Haskell takes me far longer to make progress in than most other languages due to its functional structure.
There are a few things to be mindful of when writing a recursive algorithm. You have be careful not to build up a ton of intermediate values in memory, which can cause problems. Take the standard example of a factorial function. There are a couple ways to do it. The simplest is arguably: fac 0 = 1 fac n = n * fac (n-1) This works, but for big values of n, you wind up with a huge sequence of unevaluated `n * (n-1) * (n-2) * (n-3) ...` sitting around. Here, each call to `fac` requires using a new chunk of stack memory. In lower-level languages, you'd be adding a new frame to the stack. But in this case, that's fairly pointless. Unfortunately, there's no easy way to make that deduction with this algorithm. However, there is way to optimize this. You could write a tail-recursive algorithm. Consider: fac n = helper n 1 where helper 0 accum = accum helper n accum = helper (n-1) (n * accum) This example does away with the problem of the first example by forcing evaluation of each intermediate expression as you go. This is tail-recursive because the only actual recursive call will be the last one (i.e. is a tail call). This isn't necessarily immediately obvious, but when the compiler sees this kind of algorithm in your code, it can transform this tail recursive function into a iterative loop like you'd use in Python and avoid problems with ever-expanding stack memory and redundant stack frames via tail call optimization. So as to when to use recursion: it depends. With a compiler/interpreter that can optimize tail calls, there's nothing inherently inefficient about using a properly designed recursive algorithm. However, that isn't always the case. The CPython interpreter, for example, doesn't perform tail call optimization. There, recursion will always have overhead that can bite you even if you use it smartly. The interpreter doesn't care that your function could easily be transformed into a loop; it will still add an extra frame to the stack and give you the same trouble you have with the first function above. So unless you know that your algorithm does not have the potential to blow up like the factorial function above does, you should use an iterative algorithm in Python. On the other hand, in a language like Haskell, you need to use recursive algorithms just by nature of state mutation generally being impossible (or non-ideal as then performance optimizations cannot be as readily performed). In a language like C where some compilers, such as GCC, perform tail call optimization (with the -O2 flag and not by default in GCC's case) and others do not necessarily, whether you should use recursive is a matter of weighing the consequences of doing so and understanding the implications for other people and other implementations of C. In short: know your language and your implementation. Do what your implementation is best suited towards. EDIT: Messed up my example. D'oh. Also, NruJaC has a point. A factorial function could better be implemented as one of the following: i) fac n = helper n 1 where helper 0 accum = accum helper n accum = let accum' = n * accum in accum' `seq` helper (n-1) accum' ii) {-# LANGUAGE BangPatterns #-} fac n = helper n 1 where helper 0 !accum = accum helper n !accum = helper (n-1) (n * accum) iii) import Data.List fac n = foldl' (*) 1 [1..n] Using higher-order functions is the way to go in Haskell. EDIT (again): My brain wasn't cooperating when I wrote this. Fixed silly errors.
Yes, at this stage you should definitely be using a lot of recursion to get really familiar with it. Once you've written it so many times that you move from having a good understanding of recursion to being sick of writing the same pattern yet again (like I am with 'for' loops), then it's time to abstract it away like I said. Good luck and enjoy your adventure of learning Haskell.
Where did you do your internship?
I actually found the little schemer books and scheme to teach me the fundamentals of recursion better than haskell (not that it is "harder" in haskell but because idiom in haskell is to use composable functions so it's less natural to see explicit recursion in example). [Edit] I used what is now called Racket. Best language and learning environment, DrRacket the editor built for scheme has a stepper feature which was essential to my understand of more complex recursive patterns, continuations, etc...
&gt; I get the feeling that it is the standard way of looping in the language. Almost right. s/standard/only/
I'm still in Brighton, but I work half-time in France, so go over there from time to time :-) See you, hopefully! Will also be at the Haskell Exchange 
Haskell is an odd case. You would think that the following: maximum = head . sortBy (flip compare) would be an extremely inefficient implementation. After all, it has to sort the list first, and then take the first element. And if sortBy were implemented as selection sort, an inefficient O(n^2) algorithm, that'd only make things worse, right? Actually, because Haskell isn't strict, that combination would give you the exact equivalent of your for loop that you'd implement in a strict imperative language. Why? Because you only ever inspect the first element of the list, so the runtime doesn't bother calculating any of the other elements of the sorted list -- it only runs until it finds the first one. And it finds the first one by selecting the maximum element, exactly what your for loop accomplishes. Finally, here's the idiomatic Haskell solution: maximum = foldl' max 0 -- pick a different lower bound if you want to work -- on something other than the naturals. foldl' is a tail recursive function (so it can be compiled to a loop) and in this case it compares the elements pairwise and keeps the larger at every step, again equivalent to your for loop. The key here is that the idiomatic solution tells the reader exactly what you intend to do immediately. When someone sees a fold being applied, they know the list being passed in is being consumed to produce a result. They see the max function and know that it returns the larger of two elements, so they know immediately that the person who wrote this code intended to run down the list to find the largest element. Learning to read code that way is one of the most important parts of learning Haskell. When writing Haskell code, the idea is to be as obvious with your intention as possible. That means avoiding explicit recursion and loops in favor of using an appropriate combinator whenever possible. map, filter, fold, and scan are probably the big list recursion combinators that you'll see used frequently. There are other combinators for other data structures, but they keep the same flavor -- generalized folds, generalized maps, generalized traversals, etc. So in Haskell, the question isn't loops or recursion, it's what's the right combinator to express what I want to do?
Ah, nice!
I guess when it comes to Haskell, the phrase `premature optimization` is more relevant than when it comes to many other languages. Looks are very decieving when it comes to what would be inefficient - something that looks like it would create a gigantic amount of intermediate lists might just compile down to a tight for loop. Something that seems like it would never halt might halt right away due to laziness, etc. As far as recursion goes: either you'll write it explicitly yourself, or you'll use functions that are implemented by doing recursion, anyway.
I'll post a diagram that explains the data-flow if you like. Basically you can trace what is happening if you look at the way that the channels are wired up together and then the way that the processes interact with the channels.
Oh, I actually see what you mean (digest auth, not simply storage), now that I'm not sleep deprived. My bad :( Also, yes, another round of rehashing would be needed, but this is obviously doable in the server.
Wasn't `lens` supposed to solve the problem of traversing monomorphic containers? over bytes :: (Word8 -&gt; Word8) -&gt; ByteString -&gt; ByteString forMOf bytes :: (Monad m) =&gt; ByteString -&gt; (Word8 -&gt; m Word8) -&gt; m ByteString
Ah I forgot about ghci vs. compiled haskell. How are fold, map, and filter more efficient than recursion?
Thanks so much for the detailed explanation. It was exactly what I was looking for.
I see...I think I jumped ahead of myself by not reading about higher order functions first. Its hard to move on sometimes when you're fairly confused about something so important to a language :)
I guess I'll stick to higher-order functions then
To start, even if they aren't more efficient, you should tend use them because they are good style and because they keep you from duplicating effort and make your code more readable by making your intentions clear. As for how they are more efficient, they are written with a lot of tuning for the compiler and so involve a lot of tricks (like deferring the recursion to an inner function so that the compiler can inline them), and there are compiler rewrite rules which notice that you're using one of them and do local program transformations that speed up your code (stream fusion, for example).
Yup. Personally I rather dislike this propogation of APIs based on concepts that are only 1½th class citizens of the language.
There is probably some overlap. We originally started looking at things going on in lens (like each), but realized we just wanted something specific to the monomorphic problem. The code you are giving looks nice if you know you are using ByteString, but how do you write code that can traverse different monomorphic containers what will the type and the error message be? My hope is that MonoFoldable is the most specific and straightforward way to write generic code that works over monomorphic and polymorphic containers and thus will give the easiest to decipher error messages.
Traversals are surely perfectly generic aren't they? Not that I'm an expert on such things.
Another class that might belong in `mono-traversable` (though I'm not quite sure) is an interface to stream fusion: class IsSequence seq =&gt; Streamable seq where stream :: seq -&gt; Stream (Element seq) unstream :: Stream (Element seq) -&gt; seq This would allow all the polymorphic functions to be implemented efficiently and with good fusion. Additionally, it would allow the really useful function `convert :: (Streamable f, Streamable g, Element f ~ Element g) =&gt; f -&gt; g`. This function would subsume all the `toList`/`fromList` functions and would allow weird things like direct conversion between `Text` and `Seq Char`.
Has anyone else received tickets? I got mine in a dodgy reselling kind of way. 
~~The problem is that `Traversable t` expects `t` to be a type constructor, not a concrete type. We all know ByteString contains bytes, but because it is of kind `*` instead of `* -&gt; *`, we can not write a `Traversable` instance for it.~~ EDIT: I realize now that you probably mean `Traversal` is generic enough.
Combinators like `forMOf` and `over` already work over both monomorphic and polymorphic containers.
I think he means that the hypothetic function that should work on different "mono-foldable" things could just take a monomorphic traversal function (i.e. a Traversal from lens) as an argument. 
&gt; In Haskell, properly written recursive calls (strict tail calls, IIRC) perform exactly like loops. Haskell is a tricksy language, and this statement you've made here is, while strictly true, nonetheless dangerous. The thing that makes Haskell different is non-strict semantics and lazy evaluation. These mean that: 1. Many tail-recursive algorithms, when written naïvely in Haskell, will use linear space because of unevaluated expressions; 2. Many algorithms that use non-tail recursion, when written naïvely in Haskell, will use constant space. An example of the latter is this implementation of `find`: find :: (a -&gt; Bool) -&gt; [a] -&gt; Maybe a find p = foldr go Nothing where go x rest = if p x then Just x else rest -- For reference, this is foldr: foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b foldr k z [] = z foldr k z (x:xs) = k x (foldr k z xs) The `go` function in the definition of `find` will discard either `x` or `rest` at each step, so with lazy evaluation this uses constant space. Also, GHC's list fusion optimization knows how to compile this to a loop if the argument to `find` is a "good producer" (according to the docs).
Thanks! It's not in finance, but many of the ideas were born out of my time in hedge funds and investment banks. I hope to give a more in depth technical overview in the near future, probably at one of the Haskell or tech meetups in London. Have you guys switched over to fay on the frontend recently,or considering it?
Could you drop me a mail with your CV and an overview of what you did in your internship?
Cool, looking forward to hearing more about it. We will probably never really switch over to Fay entirely, the existing JavaScript codebase is just to big and we want to be able to hire broadly on the front-end. But, I am currently integrating a small Haskell library compiled with Fay into our main product and it seems to work out fine. It's a combinator library for pickling (combined parsing/printing) and it was just too hard to do it in JS. Blogpost coming soon.
Well then, just make that `Traversal` something you thread through your computation, for example in a `Reader`, and pass it in at the top level. 
I'm looking forward to hearing about that!! Any of you guys coming to the haskell exchange in London on the 9th?
Adam will be there. Erik and I will be in the UK for the FP days end of october.
It's not about efficiency (although a lot of very smart people have written papers about making map, fold, etc. faster, so you can reuse their hard work for free). It's about levels of abstraction. In C, no-one uses a goto to express a for loop. It's too low level, and the functionality is built into the language. You're sacrificing readability for literally no gain. It's the same with map, fold, etc.
What is magical about mono-traversable? It uses a well defined typeclass, so isn't that like saying you don't want to use fmap because it is magical? But again, comparing lens to omap isn't useful, that isn't what mono-traversable is about. Comparisons should be made with MonoFoldable.
Ultimately it just means that their monomorphic `otraverse` function is already a valid `Traversal`, so you can use it with `lens` today.
It is weird though I sometimes have the same package at same time, but most of time cabal would refuse to do so.
It's not well-specified what the element should be. When you supply a lens you specify what you are mapping over precisely. For example, why should mapping over a Bytestring map over the `Word8` as opposed to mapping over individual bits? With lens, both are possible and we can easily specify which one we meant by supplying the appropriate Traversal.
As I understand it this feature is not yet enabled. You can see a "tech preview" of it but you cannot actually publish a new .cabal file.
I think it is well-specified that the element of the ByteString interface is a Word8 since every function in Data.ByteString uses Word8. However, I agree that your example is a place where the additional flexibility of the lens approach is useful and could be preferable to newtyping ByteString to get a different element. But I have no idea why we are talking about lens vs. mono-traversable so much. I use lens and mono-traversable and classy-prelude. They are all targeting different things and are appropriate for different use cases. 
That's not what the argument says. It says that we shouldn't use type-classes in this particular case. It's an argument I agree with, generally. Type-classes in my code are usually reserved for abstractions, not just overloading/abbreviation.
I think I gave a pretty clear example of that fact that this *is* an abstraction, not just overloading. I could make the same argument that `fmap` is just overloading, and you should really use `List.map`, `Vector.map`, and `ByteString.map` in your codebase. Using those are strictly more general, because they can work with polymorphic and monomorphic containers. So my question is: why is `Functor` a good abstraction, while `MonoFunctor` is "just overloading/abbreviation?"
This might be harder than it looks, although a really good idea none the less. Adding a standard typeclass based approach to stream fusion could be a huge win for haskell. One possible problem is how the `unstream` function handles recycling.
&gt; Is there a reason why you seem to be recommending that approach here, but not for all other typeclasses? Yes. I'm not 100% clear on what that reason is but I have a sense of the justification: Monad, Applicative, Functor and the like have parametrically polymorphic methods and a very strong relationship between the instances. I'm personally not comfortable with using typeclasses as interfaces. My argument doesn't apply to classes like Eq, Show and Num though, so it's far from watertight!
I am missing something, why would `Set` violate the functor laws? That is, assuming well behaved `Eq` and `Ord` instances. I just can't see it.
Your [+$=](https://github.com/gereeter/chunked-conduit/blob/master/src/Data/Conduit/Chunked/Helper.hs) is interesting. IMO, it actually points out a shortcoming in conduit, in that a Conduit has no ability to continue operations on the upstream once downstream completes. I'm writing a blog post about this and other related thoughts. This same shortcoming also affects things like `take`. You can't, for example, write a function calls `takeExactly` that forces the consumption of that many characters. I think this kind of chunking approach makes sense. This kind of experiment is perfect for inclusion in the new [conduit-extra](http://hackage.haskell.org/package/conduit-extra) package. Would you be interested in including it there?
Well apart from all of them, I'm particularly interested in the talks on Fay, Free monads and generics.
&gt; So my question is: why is Functor a good abstraction, while MonoFunctor is "just overloading/abbreviation?" As I mentioned in [another comment](http://www.reddit.com/r/haskell/comments/1nbrhv/announce_monotraversable_and_classyprelude_06/cchh5et), one approach to answering this question would be that `Functor` contains parametrically polymorphic methods However that's probably just one step towards a more sophisticated argument that claims that the fewer instances a class has at any given type, the more useful that class is. 
Firstly I don't see why it would violate the functor laws. Secondly I don't see how you could define it anyway, because you still need an `Ord` constraint. In the list case the code is given as type instance Element [a] = a instance MonoFunctor [a] and uses the default implementation `omap = fmap`. However even if you define type instance Element (Set a) = a then surely there's no valid implemenation of `omap` is there? Any such implementation would require an `Ord` constraint on `a`.
&gt; Can I try rephrasing that? "It just doesn't feel worthy of a typeclass, but I'm not quite sure why." Yes that is a roughly accurate rephrasing, with the addition "and here are my thoughts about how we might start to construct a solid reason why ...".
What do you like so much about `MonoFoldable`? The entire functionality factors through the method otoList :: mofo -&gt; [Element mofo] (just like the functionality of `Foldable` factors through `toList`) so I suppose the benefit is that you can override the methods with more efficient versions specialised to the instance in question. I can see how this is useful if you write lots of code that has to be generic over many different types of ordered containers. In fact it seems to be exactly what ML modules do well, but I admit I don't know much about those at all. I suppose `TypeFamilies` are moving Haskell somewhat module-wards. (I'm surprised that `MonoFunctor` is not a superclass of `MonoFoldable`)
why is that a problem? type instance Element (Set a) = a instance Ord a =&gt; MonoFunctor (Set a) 
Is type instance Ord a =&gt; Element (Set a) = a not allowed? I don't think there should be any difficulty setting `omap = Data.Set.map` otherwise, should there?
TIL the game of life is marvellously musical :)
Ah yes, I wasn't thinking straight. For some reason I was thinking that `omap` had to be polymorphic.
I don't know if it's allowed, but philipjf rightly pointed out that there is an easier solution!
Interesting. This basically arises because `x == y` does not imply `f x == f y`, which is a rather strange property for an `Eq` instance to have. 
Actually it is the opposite. `f x == f y` does not imply `x == y`. And this could cause unequal values to collapse.
&gt; f x == f y does not imply x == y Yes it does. I guess you mean `g` rather than `f`. There's nothing strange, though, about that property, but there is something strange about `x == y` not implying `f x == f y`. Thus I consider non-equality-preserving to be the root of the problem, rather than non-inequality-preserving (i.e. non-injectivity).
I think most of your questions can be answered by looking at the `Foldable` typeclass itself. In particular: * `Functor` is not a superclass of `Foldable`, since some things which can be folded cannot be mapped (e.g., Set). * Yes, one aspect of foldable is "just turn it into a list," and in fact you can implement all of the foldable interface with such a `toList` function. However, the Foldable interface is more efficient. I guess if you have no love for Foldable, you're not going to like MonoFoldable either. If you do like using Foldable, then MonoFoldable is a straight-forward extension of it which allows it to work on monomorphic containers.
I suppose this kind of type class will become more commonplace now that associated types are more accessible. I'm yet to be convinced, personally, and my love for `MonoTypeClass` will always be bounded above by my love for `TypeClass`, but I can see that they are useful for certain sorts of programming where you want generic interfaces to collection types. Typeclasses are very powerful and can be used for amazing things, but they can also be amazingly abused, which is why I always approach them with skepticism. Still, I'm very happy that pepole produce implementations and test them out in the real world, so thanks for the new code! 
The problem here is that there are no clearly specified laws for the `Eq` typeclass. Most people agree at the very least that `Eq` should be an equivalence relation, and therefore we should have: * Relexivity: `a == a` * Symmetry: `a == b` =&gt; `b == a` * Transitivity: `a == b`, `b == c` =&gt; `a == c` But I've not seen any consensus beyond this. Therefore, relying on `f x == f y` implying `x == y` wouldn't be prudent. I *could* imagine having a newtype wrapper around `Set` that makes this assumption explicit, however.
Thanks! Like everything else in classy-prelude, this still comes with a big "experimental, not sure if this is a good idea" label. But I feel much better about this iteration than the previous one, which was certainly abuse of typeclasses. I'm still glad I gave that a shot, just to get a feel for what it would be like, but I can rely on this much more solidly now.
Yes I meant `g`. I thought you were refering to some generic `f`. It seems you are right. Also, we can find a type for which `x == y ⇒ f x == f y` does not hold even if `x` and `f x` are of the same type. So this problem affects monomorphic containers as well.
http://i.imgur.com/Q9cGdEb.png
Just as most (advanced) type systems have a linear cumulative hierarchy of universes, is there any need for a general pre-order on worlds, or would a linear order be sufficient? My experience with multi-staged programs (which can easily be seen as ordered worlds) suggests so. It is fascinating how 'time' is creeping in to PL [and not through any efficiency considerations whatsoever] via dependent types AND staging AND functional-reactive programming (Neel K.'s excellent work just presented at ICFP has a nice model of time which also deals with cross-time persistence nicely).
Thanks :)
I'd love to see it included there. It means I should probably go in and add some more practical features, as what I did here was mainly a demonstration, but that shouldn't be too hard.
On the other hand wrapping a GUI framework is also a lot of work and seems to be a lot less useful in the long run since it breaks again after a while, making all the work useless. Maybe building one from scratch makes more sense in the long run than trying to wrap existing ones again and again?
You could extend the class like so: class IsSequence seq =&gt; Streamable seq where stream :: seq -&gt; Stream (Element seq) type Mutable s seq streamMut :: Mutable s seq -&gt; MStream (ST s) (Element seq) unstreamMut :: Maybe (Mutable s seq) -&gt; MStream (ST s) (Element seq) -&gt; ST s (Mutable s seq) unsafeFreeze :: Mutable s seq -&gt; ST s seq Note that this still handles things without recycling just fine: instance Streamable [a] where stream = Stream.fromList type Mutable s [a] = [a] streamMut = stream unstreamMut _ = MStream.toList unsafeFreeze = return
I may be missing something, but don't we generally want x == y implying f x == f y rather than f x == f y implying x == y? `const anything` potentially violates the latter.
I created an issue for it (though I couldn't figure out how to mark it as an enhancement). As far as a pull request goes, I'm working on one, but it might take a while.
But there are also tons of applications with custom gui, that don't use any of those frameworks and not all of them suck or required millions of dollars to develop. Examples that come to mind are Sublime Text and Blender. Obviously it is a huge amount of work, but I think the larger problem is that while it is well known how to implement a gui library in an object oriented language from scratch, it is not clear to me how one would do that in a language like haskell and get something as flexible and extensible as say cocoa out of it. While actually implementing a gui library from scratch might take years, designing one should be less work. 
I think you mean x = y =&gt; f x = f y
He certainly meant _hand-rolled_ recursion (instead of using fold, map, etc.)
I can't see why or how `forall f . f x == f y =&gt; x == y` makes any sense.
Is there a reason why "Element" is not an associated type, but rather a separate type family?
Yes, but `otraverse` raises the same objections as `each`.
Yes, because it can be used by both `MonoFunctor` and `MonoFoldable`, but neither of those is a superclass of the other.
Sure. If you already don't like `each`, `otraverse` isn't going to do much to comfort you. ;)
- Don't use recursion manually when you have higher-order functions (fold, map, etc.) that would do the job _the same way_ (ie. both provide the same output and have the same behaviour wrt. strictness) - Beware Of Tail Recursion!! Yes, it's exactly the reverse of what other people would tell you from other languages, but in Haskell, hand-writing a tail recursion is usually a bad idea because of the presence of lazy evaluation =&gt; So when you want tail recursion, you usually want strictness too, and instead of doing it by yourself, see if foldl' or alike can be used. - Dealing with recursion in Haskell implies dealing with strictness. And dealing with strictness/laziness is a lot about thinking what you want to compute in the end. Is your end result an Integer (or any atomic value)? You most certainly want strictness everywhere, and tail-recursion (the kind of recursion done by foldl)! Is it a List (or any collection that can exist partially in memory, like a Tree)? You most certainly want laziness, and guarded recursion (the kind of recursion done by map or foldr). Is it a memory-compact structure (like a Vector or an Array, ie. a structure that either is fully allocated or is not at all)? In that case you want tail-recursion (you array is, comp-wise, atomic: you cannot ), but handling the strictness part is a bit more complicated: you have to look at the type of the elements of your structure, because in Haskell, since values are by default _lifted_, it means you actually handle _pointers_ to values, and then structures contain just pointers. So when you evaluate a structure, you don't necessarily evaluate the elements it contains, but just a bunch of pointers to awaiting computations (we call those _thunks_). Evaluating the elements _themselves_ is then not automatic, and this is where lies the dangers of tail recursion in Haskell: as the recursion goes, the structure will be evaluated, but its elements will become bigger and bigger thunks (because the thunks are not reduced to values, as they are left unevaluated) that accumulate on the heap, and you have a memory leak. **tl;dr** Producing an Int _without_ both tail-recursion and strictness (see foldl') is certainly always a bad idea, and producing a List _with_ tail-recursion and strictness is certainly always so. But be assured of one thing: bottomline, _everything_ rests on recursion in Haskell. You just don't necessarily have to write it manually.
We're talking about this because you're proposing a Prelude replacement, which can only do one of two things: (A) affect everybody if we all buy into it, or (B) fragment the Haskell ecosystem if there is not complete buy-in.
Yeah, I know. :)
It is really not that hard as long as you have as much hard disk space as you want to add as swap. Just run dd if=/dev/zero of=/swapfile bs=1048576 count=2048 mkswap /swapfile swapon /swapfile as root to get 2GB of extra swap and swapoff /swapfile to remove it again once you are done.
The type theoretic hierarchy has a different motivation: I don't think there's any reason why it should colour expectations here. A mistake whose persistence has impeded and continues to impede progress in our business is the conflation of the large/small distinction with the static/dynamic distrinction. I have never made this mistake and do not propose to start now. The preorder of "worlds" or "times" is about transmissibility of information, so reflexivity and transitivity are to be expected. I'd be reluctant to demand more unless it's clear why. If it becomes clear that there is no purpose to distinguishing worlds with mutually transmissible information, antisymmetry might be worth considering. The System F/Haskell instances of this "worlds" design are not linearly ordered, nor would they be made so by the "Pi" plan. This observation gives me some motivation to consider nonlinear orders, even if it turns out that the nonlinearity is somehow symptomatic of a lack of expressivity which we might hope to eliminate eventually. The arrival of "time" on the scene in various ways is indeed interesting: I expect it to exhibit a linear but lexicographic structure. However, we might also use incomparable worlds to model and coordinate spatially separated computations, so that the normal scoping rules forbid variable uses which in practice take expensive communications. That would let us enforce a cost model for data transfer between worlds. I'm keen to keep an open mind. This whole zone is heating up nicely.
Did you try to install those packages by hand? There is a "--reinstall" and "--force-reinstall" flag for cabal too, maybe you need to use them on the caball install step. I have no Idea how unsafe this is (I've seen `avoid reinstall` thrown somewhere, but could be unrelated) 
You can have several versions of the same package, what you can't have is several instances of the same package version. With the new sandbox feature you can do even that though.
Yeah but the English Channel (_la Manche_) is narrower than the Atlantic.
&gt; _while recognising that there's a somewhat untapped pool of people who love Haskell and are good at it, but haven't had an opportunity to do it at their day job yet_ Finally!
Takes one to know one right :-)
What is intended purpose? Learning or work? If learning, then eliminate the main source of the friction: windows. Install VirtualBox (it is free), then install on it some linux distro of your choice (for beginners i recommend ubuntu). You will find working with emacs, haskell and a lot of haskell libraries much easier on linux than windows. If you insist trying emacs on windows, hmm, ok then. haskell-mode is not on official emacs package manager. You would need to add a link to non official repository. This is done through .emacs, and unfortunately i do not know how .emacs file is called in windows. It could be _emacs i do not remember. Anyway, your first step is to find (or create) your .emacs (or whatever it is called on windows) file. There add this: (require 'package) (setq package-enable-at-startup nil) (package-initialize) (setq package-archives '(("gnu" . "http://elpa.gnu.org/packages/") ("melpa" . "http://melpa.milkbox.net/packages/"))) The package manager usually loads all installed packages at startup. This is what you would usually want. Unfortunately i do not know how to configure haskell-mode AFTER its been loaded :)) So i switch off automatic locading with (setq package-enable-at-startup nil) and then load haskell-mode from my .emacs file. Same goes of course for all other modules i use. But you can ask around haskell-mode devs and users how to configure haskell-mode without switching off the default loading, and perhaps there's a way to do that. 
If you did "cabal install yesod-platform" then it should have installed persistent-1.2.3.0 and not 1.2.0.1 (assuming you ran "cabal update" before). Maybe just try removing the broken package: "ghc-pkg unregister persistent-1.2.0.1"
Doubly confused now. If what you wrote is true then doing reinstalls shouldn't break anything, no?
I've seen this concern raised before, and I really don't understand it. We have codebases at work with up to three different preludes being used, and it causes absolutely no issues. It's not as if we're declaring any replacements for Monad or other type classes. Can you give a specific example of how this fragmentation will occur?
That works if you're a Haskell expert, but it increases the learning curve for people new to the language if they have to learn the quirks of three separate preludes (and how to mix them) just get anything done.
I would recommend, if you wish to use Windows as your development platform, installing Emacs24 + [bbatsov's excellent Prelude](https://github.com/bbatsov/prelude). If you want to do it yourself, you need to add the [Elpa](http://www.emacswiki.org/emacs/ELPA) repositories. To do this you need to setup a %HOME%/.emacs.d/init.el with the repositories mentioned in that link, specifically Marmalade. Let me know if you need additional information or you find any of this in conflict.
Thank you for the summary, Conor! I shall read the draft shortly.
It's learning/for studies: I want an IDE which supports all the languages I need (Perl/Java for university, Python/C/Haskell at home) so I don't have to switch around between 750 different IDEs and editors. How is it easier on Ubuntu than on Windows? I mean, I need Linux for university anyway so I guess I can start learning it, that's not the problem.
That didn't work though I've now had a look at using a sandbox. I have a different error with persistent-postgres (the fun never stops!).
Ubuntu makes it a lot easier with package managers and stuff. I strongly suggest you use 2 IDEs: emacs for python/Haskell/Perl, and a "real" IDE like eclipse (my choice for Java) or intelliJ. autocomplete and stuff is absolutely worth it for Java. C you can do in emacs (install flymake).
Emacs is a linux program first and only then a port to windows. MOst of the tutorials, stackoverflow questions, wiki pages on emacs you will find target linux specifically. So it will be much easier for you to learn emacs on linux. Haskell on windows also has fallen behind. Some of the libraries won't even work on windows anymore. Others require non trivial amount of additional installation and/or tweaking to make them work (gtk2hs for example). 
Yes, but this is not germane to this discussion, which is about `MonoFunctor`/`MonoFoldable`/`MonoTraversable` type classes, which will show up in type signatures. Greg himself said the entire purpose of these was to expose a container-type agnostic API in libraries, which implies that these would be constraints in exported type signatures.
Couldn't you write maximum = foldl1' max instead of your example? That way you don't have a "lower bound" (and besides, the maximum of an empty list doesn't make much sense, so an error would be appropriate) [Docs for foldl1'](http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-List.html#v:foldl1-39-).
Since we are on the topic of Emacs, could someone point me what's the big deal of Vi/Emacs?
I was a victim of the "classy prelude". My team inherited a large amount of production code that had been converted to classy prelude. That was a very lossy transformation. The resulting code was much, much harder to read. My assessment was that this code was effectively unmaintainable in that form. And it was hard - and time-consuming - to undo that conversion. Getting rid of the classy prelude was costly and painful. For me, one of the biggest advantages of Haskell is the expressiveness of its types. When every sub-expression has type "CanThis, CanThat ...=&gt; ...", much of that expressiveness is lost. It could be the real culprit was just "pack" and "unpack" - I could never tell if I was looking at a list, a Map, a Set, or a custom type with semantic consequences. I basically had to do whole-program Hindley-Milner in my head to decipher each line of the program. I do agree that the Prelude needs a lot of improvement, and I'm glad to see experimentation in that direction. I'll be happy to give the classy prelude another look. But having been burned once, I'm going to think very carefully before I allow any Prelude substitute to be used in our production code, at least not until it is fully battle tested and widely used.
We have been talking about mono-traversable. It is not a Prelude replacement, just an ordinary library. So I think you sent this discussion went off on the wrong tangent because you used the term Prelude replacement which refers to classy-prelude, but your actual concerns are about mono-traversable. It is true that using mono-traversable in a library and exporting the type signature could potentially cause some fragmentation. If that is the case you should be advocating for application developers to use classy-prelude but for library developers to be cautious about using mono-traversable. We should come up with some guidelines for library developers using this, what do you think they should be? * freely create Mono* instances * if possible, avoid exporting only a Mono* function, export a polymorphic version also
This is exactly why we completely changed the implementation of classy-prelude. I would apologize, except I was not involved in the creation of classy prelude, only just started using it, complained to Michael, and things are completely different now. I thought about suggesting a rename of the classy-prelude package for this new version.
I want: one IDE for all languages, being able to extend the editor with a scripting language, write code quickly with short cuts, indentation and code completion.
It breaks because you can not link several versions of the same package into the same library or program. That forces a recompile of some shared dependencies of the packages listed as likely to be broken and your install target against some different versions of their own dependencies (e.g. originally they were compiled against Bytestring 0.9 but now your new install target requires them to be compiled against ByteString 0.10 because your new target is incompatible with ByteString 0.9). Afterwards the instance of these packages originally linked against no longer exists, another instance of the same package and version exists which was built with a different constellation of its own dependencies.
The choice of what to include in a Prelude is a statement about best practices. I can't just say "Oh, I think we should include `errors` in the Prelude and don't worry if you don't like it because you don't have to use it." Nothing in the Prelude gets a free pass because the entire purpose of the Prelude is to be instructive for newcomers to the language.
That is great! And here I thought this would be a boring article about Ubuntu.
Most people will tell you to add a certain setting to your .emacs file, which is certainly the fastest way to do it provided you know what you're doing. Since you don't know where your .emacs file is, here is how to do it the point and click way in pictures: [http://imgur.com/a/9KfWz](http://imgur.com/a/9KfWz) Textual description (note: M-x means Alt-X on a Windows keyboard): * Start package manager. (M-x package-list-packages) * Find specific setting. (M-x customize-apropos) * Enter "package archives" * Press the INS button * Type in "melpa" and "http://melpa.milkbox.net/packages/" * Press the "Apply and Save" button * Start package manager. (M-x package-list-packages) * Select haskell-mode * Click install (Edit because I'm bad at formatting lists.)
So if odd 5 == odd 7 then 5 == 7 ? 
I am seeing this kind of confusion about use cases come up all to frequently. There are 2 entirely separate use cases. * Application developers (who rarely share source code) * Library authors whose intention is to distribute their library Generally speaking only application developers should use classy-prelude and they should leverage every part of it. And none of that will cause fragmentation. Libraries are what cause fragementation, and library authors generally should not use classy-prelude. 
thanks for clarifying. looks like ill need to get virtualenv up and running. is there any work on getting version imports into haskell ? seems like a pretty big issue to me. 
Vim's a great text editor, Emac's is a great **environment**. Both are the only real options for the polyglot programmer who wants to work in the same ecosystem instead of relearning how to do the same thing multiple times. The learning curve is brutal for both though, so most people don't make it far enough to appreciate their benefits.
The main benefit of using Linux or Freebsd is that there is a huge number of packages that target programmers, the environments are very scriptable and composable, and setting anything up is very easy with the packaging systems that are available. I would run a Linux distro inside a VM, using VMWare or Virtualbox, so that you can have the best of both worlds, at least until you feel comfortable with Linux. Ubuntu is usually the recommended option for newcomers, but an enormous number of options exist. But, be prepared to read tons of literature to make the most of the platform, especially if you want to take advantage of the large number of security/network admin utilities. The nice thing about emacs is that it is infinitely hackable and can be extended as a solution to any domain, and likely already has been; vim or emacs typically have the best support for newer or more esoteric languages as well. 
I was stating the condition for the particular `f` defined here: http://www.reddit.com/r/haskell/comments/1nbrhv/announce_monotraversable_and_classyprelude_06/cchhq1e
There are two important things to know: * Did you first install the Haskell platform? * Was `yesod` the very first package you installed? If so, then you should report this as a bug to Michael.
 allEqualBy _ [] = VacuousTruth allEqualBy eq (x:xs) | not . null $ dropWhile (eq x) xs = SomeDifferent | otherwise = AllEqual x
Add the [MELPA](http://melpa.milkbox.net/) package repository to Emacs. Then, use `M-x package-refresh-contents` to update the list of packages. Finally, use `M-x package-install haskell-mode`. Good luck, this is exactly what I use to edit my Haskell code, and other languages as well. If you'd like an example config file for tweaking Haskell indentation, check out my .emacs: https://github.com/mcandre/dotfiles/blob/master/.emacs
I'm not saying its a bad idea. I'm saying it's a huge idea. I hate to see someone set out on a journey of a thousand steps, only to find it's a journey of a million steps just to get to the first checkpoint.
On Windows 7, the `.emacs` file seems to live here... C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\.emacs [**EDIT** - sorry, I didn't notice the path needed escaping.] There's an "easy customization mode" (M-x customize) in emacs, and as a newbie, that may be preferable to directly editing the `.emacs` file. OTOH, I'm definitely still a newbie and (mainly out of don't-know-enough-hotkey-sequences frustration) I've ended up editing the `.emacs` file directly using Notepad++ several times. 
I installed the platform a while ago and had installed yesod as part of that. I'm re-installing it all again and if it's still there I'll report it as a bug as suggested.
I think you can reinstall all the packages. It takes a lot of time. But you will not be bothered with all the dependencies. packages files are located at *~/Library/Haskell* (remove or rename this folder) and you have also to remove files in */.ghc/x86_64-darwin-7.6.3/package.conf.d* then run *ghc-pkg --user recache* or *sudo ghc-pkg recache* Now you have removed all the packages, make sure that **cabal** is in your path, and *cabal update*, then *cabal install Cabal* if mecessary, and use cabal to install any package you want. I did it days ago.
Here are a few tips for Windows emacs users. 1: After you extract emacs to c:\emacs or wherever, there is an executable you should run to register start menu entrees. 2: Set an environment variable for HOME to c:\Users\&lt;username&gt;\AppData\Roaming. This will allow you to use the '~' character within emacs to refer to that folder. 3. The runemacs.exe executable should be used to launch a new emacs process. Call (start-server) from your .emacs file, and then you can use emacsclientw.exe to open additional files into your existing emacs window. Don't launch emacs.exe directly.
which package are you referring to? 
which package is 1.5 lens ir mono?
Right. My view of what "well behaved" means for `Eq` includes that `a == b` implies `a` is observationally indistinguishable from `b`. Clearly this does not hold here, so an API that exposed `unM` would be unsafe. Unfortunetly, being monomorphic is not good enough f = M . (*2) . unM g (M x) | x &gt; 10 = M 1 g y = y S.map (f .g) $ S.fromList [M 4, M 9] = S.fromList [M 1, M 8] --behaviour for the next one is undefined, but could be S.map f . S.map g $ S.fromList [M 4, M 9] = S.map f $ S.fromList [M 8] = S.fromList [M 8] the point is that `M` has an `Eq`/`Ord` instance that is not well behaved, and thus `Set` is not usable.
With only those laws `Set` is not usable as its observable behaviour depends on implementation choices.
\*Cough Cough\* Use vim \*Cough Cough\*
Yea, absolutely. I just forgot about foldl1'.
I've always interpreted the symbol used in stating laws to mean denotational equivalence, rather than meaning the `(==)` function defined in Haskell. That is, it's a metalinguistic symbol stating that two terms are "the same"; not an assertion that the expression in question should evaluate to `True`. One reason for this is the fact that it's impossible to define an adequate `(==)` for function types. Another, is because I'm familiar with the category theory where these particular laws come from; and in that context the equals symbol (just "`=`") is used to mean that the two sides are the exact same thing on the nose (as opposed to being equivalent, or the same up to isomorphism,...).
This is a really neat idea.
As the author of the CV package, I fully endorse this one. Making pure opencv bindings is like putting woolen mittens on edward scissorhands.
Yeah, and then we're going to need you to write up a library for type-level lenses to fix the type-level records problem :)
I second this! This would actually address a lot of usability concerns I have with some type parameter intensive code I'm writing. are the example motivating code for this online yet? :) probably worth hashing out more, but totally worth putting a feature request ticket on trac laying out the motivation / use case you have, so that it can be written down somewhere too.
I've been wanting to get into GHC hacking, just for funzies. Would this be a realistic first project? I imagine it would involve some minor changes to the parser and then just another desugaring step. This seems like it should be relatively simple in theory.
Well, of course. ;)
I still reckon having easily-human-readable and uncluttered type signatures is to be desired. Tool support is great, but you run the risk of tying usability (of a language or library) to an IDE.
I have a `CrossValidation` type in HLearn that the above demo is based on: https://github.com/mikeizbicki/HLearn/blob/dev/src/HLearn/Evaluation/CrossValidationHom.hs Here is one of my attempts at writing a quasiquoter: http://lpaste.net/93628 The main problem is that the parsing library [haskell-src-exts](http://hackage.haskell.org/package/haskell-src-exts-1.11.1/docs/Language-Haskell-Exts-Parser.html) doesn't support any of the fancy extensions I want to use, so the results are no good for me.
I don't see what problem `Get lossFunction CV` solves compared to `lossFunction CV`. In either case, `lossFunction` seems to be a type variable, and type variables are normally only bound inside type definitions (or under `forall`)... Maybe a notation like `Get_lossFunction CV` or `CV { lossFunction }` would be better.
I guess my first question is what parametrizing by those options (regardless of how you express them) at the type level buys you. None of the parameters in your example strike me as things that would benefit from being at the type level. Can you justify your example a bit more? Are all those types just marker newtypes for picking suitable typeclass instances at some point later on?
Apparently [this book](http://homotopytypetheory.org/book/) is quite hot.
I disagree about needing Linux for a decent Emacs experience. I used Emacs on Windows for a year while I worked at a Windows-only .NET job and everything worked fine, including tramp. I used ntemacs specifically. The operations in Emacs are very portable. The only place you need to be worried is launching external programs that assume shell scripts. I'd recommend Linux anyway but not due to Emacs.
yeah - very Hot(t) but not quite introductory (is it?)
in the same situation, I've been advised Practical Foundations of Programming Languages (PFPL) by Robert Harper over TAPL. But TAPL's still a very good choice.
Type theory in general, or type theory in the context of comp.sci. / programming in particular?
agreed. I think it'd be a very reasonable type system extension and an easy one that doesn't touch the internals at all! agree with edward on all points [edit: would at least be a simple enough ghc patch that you could hack it out for yourself and see how much it buys you usability wise]
Cabal 1.18 sandboxes already cover the virtualenv solution for this problem. 
No, the easier it is to express hard and complicated things, the wider view we can get of our software, and that elevates our perceptions of what is hard or complicated. It’s the difference between “Why would I ever need laziness/higher-kinded polymorphism/return type overloading?” and “How did I ever live without it?” This may not be the prettiest example, but real-world software often has to contend with realities that are less than pretty. 
true - I kindof interpreted this into the question - maybe wrongly so - sorry
You're right. I was thinking `Get` would be a special type function that could look at the name of its type variable and change behavior accordingly, but this is completely inconsistent with how Haskell currently behaves. I like the `CV { lossFunction }` better.
No. Every letter of every word of every sentence in the Lord of the Rings is perfect. And someone needs to drop Peter Jackson in Mount Doom.
Yeah no, once `IO` is involved all bets are off. However, we like to assume/pretend that `IO` actually does makes some amount of sense, even if we can't prove it, or even if we can demonstrate it's false by using "outlandish" functions designed solely to demonstrate the problems with `IO`. So while technically all bets are off with `IO`, in practice if the functor/applicative/monad laws fail for "normal" arguments under the "obvious" interpretations of I/O, I think most people would call that a bug. Point still holds, though: the laws are statements *in* the metalanguage (e.g., in/formal mathematics), even though they're *about* the object-language (i.e., Haskell). Thus, the equality symbol used in stating these laws is something that only exists in the metalanguage.
&gt; Point still holds, though: the laws are statements in the metalanguage (e.g., in/formal mathematics), even though they're about the object-language (i.e., Haskell). Yes I think this is a good way to think about it. 
There's also [ATTAPL](http://www.cis.upenn.edu/~bcpierce/attapl/) if you get through TAPL and are interested in more (or if you already have a basic background and are looking for more interesting/advanced issues).
It's worth pointing out that PFPL intentionally uses non-standard terminology. That's not necessarily a bad thing —Bob had his reasons afterall— but it's definitely something to beware of when learning a new field.
I'm in the middle of it right now, will report back when I know if it works for me too.
If you're interested in the semantic side of things, I'm rather fond of [Geoffrey Burn's book](http://www.amazon.com/Lazy-Functional-Languages-Interpretation-Compilation/dp/0262521601) for its quick and easy introduction to domain theory (and why type theorists care). It's not a full textbook on the subject (since, as the name suggests, it's about compilation and abstract interpretation) but it's a great place to start. Of course, the real meat of the book is the part about abstract interpretation and strictness; but if you're interested in semantics and in Haskell, then that should be right up your alley. IMO, it's definitely good to see a more semantical treatment of type theory along with the usual syntactical treatment we get from natural deduction, sequent calculus, etc.
I think your first point hits it on the head: the API exposes `unM`, and leaks information that allows us to observe the differences between "equal" `M`s. However, this may be entirely reasonable choice! From an API point of view, it seems there should be a distinction between the leaky and non-leaky functions. Use only the non-leaky ones and you'll get a working `Set`.
This is an important question and should not be overlooked. The meaning and significance of "type theory" varies significantly depending on which combination of philosophy, mathematics, logic, computational theory, programming, etc, you come from (or would like to be heading towards).
&gt; I could never tell if I was looking at a list, a Map, a Set, or a custom type with semantic consequences. Why does it matter? The point of ClassyPrelude is, among other things, to be able to switch data structures around without rewriting all of your code. There's always got to be some context or annotation *somewhere* that specializes down to a concrete type, and the operations and related types are usually self-explanatory. Maybe I'm just desensitized to the virtues of knowing exactly which type I'm working with since I've been doing a lot of Ruby on Rails and Coffeescript at work lately...
&gt; but whoever wrote the M datatype should have ensured that you could not write "functions" f with the property that there exist x and y such that x == y but f x /= f y. That is completely out of M's author's control. data Unique = Unique instance Eq Unique where _ == _ = False unsafeToUnique :: a -&gt; Unique unsafeToUnique = const Unique -- forall x. unsafeToUnique x /= unsafeToUnique x -- regardless of whether x == x And hey, my `Unique` data type even adheres to your rule that forall f. ((x :: Unique) == (y :: Unique)) ==&gt; (f x == f y) Albeit trivially, since `x == y` is never True.
Thanks to both of you! Both of these look good, though for now I'll probably stick with the first.
Good point. For reference, I was looking for something from the more CS kind of perspective, but my background is mostly programming, and mathematical logic. So far TAPL looks pretty appropriate for that.
Good to know, both of you. Thanks! I'll take a look at it.
I certainly agree, I was only commenting on yitz's particular migration problem which seem amenable to tool support. I'm a vi user and while I have in the past had ghc-mod set up to provide type information in the editor, it's not working on my current OS installs. I'm not endorsing Classy Prelude, nor do I wish haskell to become like java, only productive in a large IDE.
Would this run into similar namespacing and abstraction issues as value-level records? If you had two types in scope both with a type-level field called `k`, would it cause any problems?
Really funny.
s/vi/vim/g
Neat. Why not release it though?
As I said I wanted to rewrite it in monadic style, then forgot about the whole thing. I will see what shape it is in when I got home (assuming I don't forget it until then :) and upload to hackage if it looks ok.
Please don't do that. The rules on what you can fuse are a little bit subtle. As I explain in [my thesis](http://code.haskell.org/~duncan/thesis.pdf), the proofs have to be done per-concrete type, so you cannot in general abstact over the concrete type and have rewrite `RULES` pragmas that apply for all such types. For the details, see section 3.8.3 and 3.10. In particular see the example of `head` which fails for arrays.
For anyone who wants to tackle that, please read chapter 3 of [my thesis](http://code.haskell.org/~duncan/thesis.pdf) on the correctness conditions for stream fusion, and chapter 4 on what you need to do to get fusion to work in practice. As I mention in another comment, a standard typeclass is not going to work, because the `head` you use for lists is not going to be the same as `head` on arrays. That is, you cannot define: head :: Streamable seq =&gt; seq -&gt; Element seq head = Stream.head . stream Or rather you can, but it's wrong for most `Streamable` types.
Small caps? ;)
Is there a plain list of Stackage packages somewhere? Is my understanding correct that "using" Stackage simply involves depending on the packages from that list, and I don't need the actual code from the Stackage repo, unless I want to perform testing?
To be fair, the HoTT book _is_ written as an introduction to HoTT, and in doing so, it needs to provide an introduction to dependent types and type theory. What it doesn't do is provide an overview, any motivation, or any explanations of other ways of looking at types. One could read it, knowing nothing of topology or type theory.
The best Haskell seller I've seen in a while! Love the silly-serious humor and I think he touches just the stuff that can get people excited to try something more eg. LYAH.
This. I learned so much from it. It's a very well written book. 
Can you please provide a link to more information about these features and how to use them: * The ability to add local patches * Ability to snapshot builds * Fully sandboxed builds Thanks!
In the installation instructions, can we now replace Cabal 1.16 and cabal-dev with Cabal 1.18 and its built-in sandboxing?
Cunning, but I disagree. In that case the author of `M` wrote an invalid `Eq` instance. 
Well, I guess you could say the *type* of books he is looking for is too general.
Yes, if you pass the opencv header dir for your version of opencv, it will pull out the functions and data types and wrap them (with a few important exceptions that need to be handled manually because of templating). And I absolutely intend to try and get this generator into opencv proper. If I can talk them into taking the raw Haskell bindings, great. If not, I'll settle for just the C wrappers. I can't imagine the Haskell community are the only ones struggling to interop with C++.
&gt; So my question is: why is Functor a good abstraction, while MonoFunctor is "just overloading/abbreviation?" That is in fact a very good question. There is a line to be drawn here, and it's not at all clear where to draw it. I'm not sure which side of the line `MonoFunctor` will be on; my current gut feeling is that it's a good abstraction. Perhaps tomejaguar's idea about parametrically polymorphic methods is a step in the right direction. But in general, as far as I can see, the only way to tell will be to try different things and see how they work out in practice. That's why I think that Classy Prelude is an excellent experiment, even if in my particular case it caused me pain.
vim is *a* vi. 
Yes, and `&lt;font size=20&gt;` for kinds. In all seriousness, the limitation to two 'lexical categories' (lowercase and normal operators/uppercase and operators starting with ':') is beginning to show some strain. It works perfectly when all you have is normal values and types, but with `DataKinds` and type-level literals, the demarcation line is blurring.
Hey SQream team, this looks great! Personally I have implemented an embedded relational query language inside Haskell, so I know Haskell can be a good fit for this problem. I am currently primarily interested in the API, and compiling just to SQL not to a new exciting query engine. However my project has been very successful and suits my client's needs well. From my experience I conclude that Haskell is a very promising technology to tackle this sort of challenge. Relational data is one area of our industry that desperately needs attention and to be brought into the 21st century, so good luck! I'm excited to see what you come up with. 
And? People don't use vi, they use vim. Unless you're on some kind of esoteric (or antic) *nix distribution.
A large part of the problem is that historically switching containers using classy-prelude wasn't remotely semantics preserving. `filter` worked both for list-like things and conduits with _vastly_ different and unrelated signatures. The mechanism was used for punning, not for abstraction. You couldn't reason about any of the code without reasoning at the specific instances.
+1, except don't steal the word `Get` for syntax. There are some other ideas elsewhere in this thread.
I'm not very good at Haskell and the following is a bit cloudy to me, could someone please spell out the reason for the following: let very f x = f (f (f x)) Prelude&gt; (very . very) succ 0 9 Prelude&gt; (very very) succ 0 27 What would for example (very very very) succ 0 be? Thanks
Note that `very very` performs its argument 27 times. That means `very very very` performs `very` 27 times. Does that help? It may also help to note that 3 * 3 = 9 and 3^3 = 27. You could try running it in GHCi, but it might be best to use `somewhat f x = f (f x)` rather than `very`! 
 very succ = \x -&gt; succ (succ (succ x)) (very . very) succ = very (very succ) = very (\x -&gt; succ (succ (succ x))) = \x -&gt; succ (succ (succ (succ (succ (succ (succ (succ (succ x)))))))) (very very) succ = (\x -&gt; very (very (very x))) succ = very (very (very succ)) = very (\x -&gt; succ (succ (succ (succ (succ (succ (succ (succ (succ x))))))))) = error "I think you get my point" = error "And I might be missing some parentheses"
The first variant uses composition of functions. `(very . very) succ 0 = (very (very succ)) 0`. The latter is more interesting--we invoke `very` with `very` as its parameter. If we expand `very very` according to the definition you cited, we get `\x -&gt; very (very (very x))`. If we invoke this with `succ` as the parameter `x`, we get `very (very (very succ))`. The resulting function is then called with `0`. Since each `very` calls its function thrice, we get 3^3 = 27 invocations of `succ`. So, `very` is something that calls its function thrice. Thus, `very very` is something that (calls a function thrice) thrice, or 3^3 times. Thus, `very very very` has to be something that calls *that* function thrice, or 3^3^3 = 19683 times. Ow.
Things that Java's type system is better at than Haskell's: * Sucking hard.
Lol
&gt; `very very very` has to be something that calls that function thrice, or 333 = 19683 times. Ow. It's *much, much* more "Ow" than that. 19683 would be for `very (very very)`.
Oh, right, my bad. `(very very) f x` calls `f` 27 times on `x`, thus `(very very) very f` calls `very` 27 times on `f`.. Hm, is it 3^\(3^3\) rather than (3^3)^3 ?
Thanks, that helped.
I would say check your guess by trying it out in GHCi, but you wouldn't be back for half a year :)
Thanks, that's quite useful. So would (very very very) succ 0 be 3^27 ?
Yup :)
Yes, once upon a time there was a perfectly sensible lexical distinction, between uppercase constants (Bool, True) and lowercase variables/defined symbols (x, not). Sadly TypeFamilies screwed this up for some reason, so type-level not has to be called Not. I wonder if it is too late to make TypeFamilies lowercase?
It would be 3^27 I think. We get the function ```\x -&gt; very^(27) (x) succ```, and everytime we apply very, we call the function three times, and since there is a chain of 27 very's, we get 3^27 calls of the function succ. Trying it out in ghci gives stackoverflow, and I'm sure 19683 wouldn't result in a stackoverflow.
This type of thing always puzzles me. Isn't this like swiming against the current of the original language design? It's cool that you can do this, but if you wanted this kind of strong typing wouldn't just using Haskell make more sense? I mean you get it for free and at compile time.
I don't have particular complaints with DBMSes. Postgres is pretty decent. SQL itself is more-or-less satisfactory, and my embedded language compiles to it (via HaskellDB's internal language). However, to be able to shine like Haskell shines, a query language needs to improve significantly on SQL. SQL, for example, has trouble with the number zero. This is always a giveaway that an abstraction hasn't been thought through properly. I believe it does not support 0-column tables, and it's awkward to construct an query for an anonymous table that returns 0 rows. Furthermore I don't believe it supports a unit datatype (see Chris Date's complaints about tabledum and tabledee). However, the major issue is that SQL is a syntactic pain to use, and it's a real mess trying to write SQL queries that are composable. Haskell has learned a lot about abstractions for composability (monads, arrows, applicatives etc..) and its about time these kind of ideas turned up in the relational database world. 
&gt;However, the major issue is that SQL is a syntactic pain to use I find it quite pleasant. Any specific issues, or just a preference thing? &gt;and it's a real mess trying to write SQL queries that are composable Queries are composition. If you want to store a query to query against, that is what views do.
It's somewhat introductory if you have a background in mathematics.
And you get tools like type inference to help you work out what the code should look like. These kind of optional typing tools are nice to verify a few invariants, but they're not nearly as powerful as actual static types. 
They problem I have is that every time I require even slightly modular sql in my code, I end up concatenating strings together. ORMs do their best to stop that, but frankly all the ones I've used fail. They either fetch too much data or run inefficient queries or require raw sql whenever their abstraction fails. They also fail to catch mistakes until run time. Haskelldb was a promising project that allowed you to generate sql in a type safe manner combining queries together and optimizing them, but its problem is that it generates subqueries and the most commonly used sql databases can't run subqueries in an efficient manner for some reason. The prospect of being able to write an entire app full of sql and know for a fact that when this compiles it will just work is so tantalizing for me that I even hacked on haskelldb a bit at one point, but it just isn't going to work without some major elbow grease.
I agree that this implementation is inferior to Haskell's type system. But since lisp is homoiconic and easily modifiable, you could argue that adding features is actually swimming with the current of the original language design.
Speaking from my work experience, the fact is that there is a lot of code and entrenched knowledge out there in dynamic languages. It is a bit unrealistic to ask those companies to trash their existing code, learn Haskell and rewrite it. My feeling is that this is most likely a "better than nothing" situation. I haven't heard too many arguments that put optional typing for dynamic languages on the same level as Haskell's type system.
Really, core.typed isn't for people who would use Haskell for its type system, but for people who want to use Clojure despite the lack thereof. I don't think any current Haskell users are going to be too impressed by it. Developing in Clojure tends to be a bit of an organic process, thanks to the deep REPL integration in many tools. You can evaluate arbitrary bits of your code as you go, swap things out, tweak whatever you like, and through a more-or-less exploratory process end up with a working bit of code. It's very freeing and very efficient, but if you've ever used a dynamic language for a large project, you know that this tends to create issues refactoring later unless every component is attached to a unit test. The power of core.typed is to let you, after the fact, lock down those function signatures and create what amounts to an automatic test suite. So in that sense the comparison to Haskell isn't really valid, because the typing facilities exist at different levels in the stack. The article is really about Clojure, with Haskell examples thrown in to provide an idea of how the implementations look in a real strongly-statically-typed language.
&gt; I wonder if it is too late to make TypeFamilies lowercase? That's sort of incompatible with good old HM-style implicit `forall`s. f :: t Int Is that `f :: forall (t :: * -&gt; *). t Int`, or the application of a type family `t`? Not good if the meaning changes from the former to the latter when you import a module that exports a type family `t`, IMO. That would be CoffeeScript-style scoping, which I've argued against in the past, so I need to be consistent :)
Mysql literally falls down dead when I do even the simplest query. I tried on postgresql maybe a couple years ago and it was better. But it was very easy to write a query that just wouldn't optimize adequately, so I gave up. Ultimately my goal is to make things easier, and I couldn't quite reach that goal. I've heard that oracle and db2 deal with haskelldb subqueries effortlessly, which is unfortunate because I'm never going to use those. Eventually something will happen, if not in haskell then in scala or something. It is inevitable. It just hasn't happened yet (so far as I know).
Yeah, I'd imagine it would have all the same problems as normal record syntax. The mitigating factor would be it would be used much less often (presumably).
Hmm, I'm not sure what to suggest. Perhaps you tried it before the query optimizer was added.
&gt; What do you see as the problems with current SQL databases? I would say that for writing simple queries, SQL syntax seems fine. When you write complex analytic queries, it can become very awkward compared to e.g. relational algebra which is much simpler and at least as powerful. One of the other big issues with many current SQL DBMSs is that the options for physical storage are a bit limited, and the fixed trade-offs that they make for storage and for execution mean they are much less flexible than they could be. Having said that, I think the current popular SQL DBMSs are still some of the most useful and advanced bits of software in wide use. 
Have you looked into [esqueleto](http://blog.felipe.lessa.nom.br/?p=68)?
&gt;Views are a rather heavyweight way to "store" queries. Not really, no more heavyweight than defining a function in a typical language. Not at all like having to compile and link it. And there is the equivalent of anonymous functions: http://www.postgresql.org/docs/9.1/static/queries-with.html &gt;Furthermore, queries can only output columns, they can't take columns as inputs You specify inputs all the time. You just don't assign them to an intermediate variable and then pass that along, you do it inline. Just like doing f(g(x)) rather than y = g(x); f(y) &gt;How to you write a query that takes in two integer columns, adds them according to a cross join and then looks up the result in another table? You simply can't. I don't think I understand what you are asking for. The way I am interpreting that question, that is a pretty trivial query. You want to add up all the combinations of column a and column b, then join that to some other table yes?
&gt; it can become very awkward compared to e.g. relational algebra Hi Jake, nice blog post! What do you mean "compared to relational algebra"? Is there any viable DBMS that actually lets you write queries in any language remotely similar to the usual syntax of the relational algebra? SQL is itself an encoding of the relational algebra, more or less. It just doesn't look particularly similar to it. Thus I'm curious about what you mean here.
Thanks! At SQream we are focusing on multiterabyte analytic queries only. I would love to see a OLTP relational DBMS with e.g. relational algebra syntax instead of SQL, and more flexible storage for instance. 
&gt;They problem I have is that every time I require even slightly modular sql in my code, I end up concatenating strings together Yes, I find keeping SQL in the database is much nicer than keeping it as strings in some other language. &gt;but its problem is that it generates subqueries and the most commonly used sql databases can't run subqueries in an efficient manner for some reason. AFAIK, only mysql has broken subqueries? Everyone else should be fine.
&gt; Not really, no more heavyweight than defining a function in a typical language. Not at all like having to compile and link it. You actually have to store it in your database which requires permissions and so on. It's not simply client code, and that's frustrating. &gt; I don't think I understand what you are asking for. The way I am interpreting that question, that is a pretty trivial query. You want to add up all the combinations of column a and column b, then join that to some other table yes? Yes, but I don't wan it for for fixed, predetermined columns a and b of fixed tables. I want it to take as parameters *any* two possible integer (say) columns existing in my database, or indeed columns that are results of other queries. 
Random googling tells me there have been improvements in both postgresql and mariadb in the recent past. I will probably give it another shot in the future.
Besides if the history of programming taught us anything, it's that the guidelines, "best practices" and advises are ignored in 99% of the cases and never make any lasting impact. You want anything done for sure, give the job to compiler :) 
For reference, TAPL is the book by Pierce. Topol, on the other hand, looks like this: http://www.examiner.com/images/blog/wysiwyg/image/Fiddler_on_the_Roof_Chaim_Topol_farewell_tour_poster_-_cropped.jpg
&gt;You actually have to store it in your database which requires permissions and so on I don't see how that is a barrier comparable to compiling and linking. Especially given that 99.9% of people just use a single user with full permissions to do anything to the database. &gt;It's not simply client code, and that's frustrating. Why is it necessary to keep your SQL in "client code" rather than putting it in the database in the first place? &gt;Yes, but not for fixed columns a and b of fixed tables. For any possible columns existing in my database, or indeed columns that are results of other queries. The results of other queries are just like any other table, and can be queried as such. There is no difference: select test3.i, test4.name from (select test1.a + test2.b as i FROM (SELECT generate_series(1,10) AS a) test1, (select generate_series(1,10) as b) test2) as test3 left join test4 on test3.i = test4.id; You can query as many queries as you want. So the issue is just that you want to base the columns/tables you query on user input?
&gt; Why is it necessary to keep your SQL in "client code" rather than putting it in the database in the first place? Revision control. The possibility of using Haskell code in actually generating the queries. All sorts of reasons. &gt; So the issue is just that you want to base the columns/tables you query on user input? Not necessarily user input, but I want those columns to be parameters to the query, and not to have to write a new query for each set of parameters. For example, how would you write the following HaskellDB code in SQL? I believe you can't get anything equivalent: query a b = do x &lt;- a y &lt;- b z &lt;- t restrict (x .*. y == z ! col1) return (z ! col2) 
My problem with esquelito is that it seems to be missing the "combinator" portion of haskelldb. Mind you I haven't looked at it in a long time. But in haskelldb you would have a 'user' relation and a 'company' relation and then you could combine those monadically into an outer or inner joins on whichever column with wheres and limits and aggregate functions and tie that query to other full queries and it would just spit out valid sql with most of the redundancies removed. Esquelito, at least so far as I've seen, is basically writing manual sql in haskell instead of text. I'm still going to be manually concatenating things together and rewriting of parts of queries into other queries and you'll have to deal with all the same issues you would have had to deal with raw sql, except you'll get a bit of type safety and auto escaping, which is nice, but I'm not sure it is enough to warrant its use.
&gt; Is there any viable DBMS that actually lets you write queries in any language remotely similar to the usual syntax of the relational algebra? This is still a dream, but there are some here: http://www.thethirdmanifesto.com/ (click on projects) I think SQL was a failed experiment in syntax for non-programmers along the lines of Cobol. Unfortunately, we seem to be stuck with it. There is lots of material about the deficiencies of SQL syntax, I will try to dig up some references for you.
&gt; Queries are composition What if I want to compose filters? Are you suggesting that : SELECT * FROM (SELECT * FROM users WHERE name = 'Joe') WHERE age &gt; 18 is the "right way" to do composition, and then just let the query optomiser deal with it?
&gt;Revision control Revision control doesn't care what kind of code it is. All our SQL is in git. &gt;For example, how would you write the following HaskellDB code in SQL? Have the function take 2 arguments and then use execute to run the previous query with them.
I am not trying to force a "right way" on anyone, but of course that is perfectly fine. It is common/normal/expected to do a select from a view that already contains filters, and have the select filter it some more.
I re-installed GHC to be safe, it now all works. Thank you for your help :)
Reinstalled GHC and it all works now, thank you for your help.
Reinstall solved the problem, thank you for helping me.
create or replace function myCond(name, age) ... I understand that SQL is more verbose than haskell, and I'm not trying to be argumentative. It is just that they way you said "Relational data is one area of our industry that desperately needs attention and to be brought into the 21st century" implies something far deeper than "its a little verbose for my tastes".
&gt; Revision control doesn't care what kind of code it is. All our SQL is in git. OK, then you have to make sure your database is in sync with your git repository. Of course it's the same for deployed binaries, but personally I don't want the additional headache. And the example of being able to use Haskell to programatically generate the queries? That's a big bonus for me. &gt; Have the function take 2 arguments and then use execute to run the previous query with them. Hmm, well I don't know about you, but this kind of thing makes me want to cry: http://forumone.com/blogs/post/how-pass-mysql-table-and-column-names-stored-routine-arguments I don't know anything about stored functions though, so perhaps I just stumbled across a particularly poor example and there is a genuinely nice way to implement this functionality. 
&gt; create or replace function myCond(name, age) ... That doesn't allow you to pass around myCond as a first class value. Of course you can convert it to a string, pass it around and run EXECUTE on it, but that's really missing the point. &gt; "Relational data is one area of our industry that desperately needs attention and to be brought into the 21st century" implies something far deeper than "its a little verbose for my tastes". I don't think so actually. Giving first class status to as many entities as possible in your language is incredibly valuable. At worst SQL is the COBOL of relational query languages. At best it's the Java. I want the Haskell of relational query languages. Perhaps to you Haskell is just a less verbose version of Java and COBOL, but I and many on this subreddit would disagree with that. 
Oh, I guess you're right. Thanks for pointing it out! OK, how about this: restrictJoe = restrictWith name eq "Joe" restrictAge = restrictWith age gt 18 query = restrictJoe &lt;&lt;&lt; restrictAge &lt;&lt;&lt; users `restrictWith` isn't actually a combinator I've written in my library, but it could be, and now that I've used it here it seems to look nice so perhaps I will actually add it! 
&gt; What do you see as the problems with current SQL databases? I realize they aren't true relational databases, but they can be used as relational DBs. Here's another feature that SQL is lacking. That's not to say there's anything better in this regard, since HaskellDB, and my query language, and every other query language I know of do not support it either. The feature is reflecting functional dependencies in the types of queries. Darwen writes about this in "The Role of Functional Dependencies in Query .Decomposition"
&gt;That doesn't allow you to pass around myCond as a first class value Pass it around to what? Pass it around to other functions? No, SQL is not a functional programming language. I get that we're here in haskellville, but I am not in so deep as to believe that everything must be functional. A declarative query language doesn't really seem to stand to benefit much. I'm all for someone writing an actual relational database in haskell, using haskell as the query language. But I don't think that means the current state is "desperately in need of attention". I find most of the problems people have with SQL databases tend to be self-inflicted. &gt;At worst SQL is the COBOL of relational query languages. At best it's the Java I'm not really sure there's an actual distinction there, and if there is I could see it being the other way around in fact. I don't think haskell is just less verbose java. Because people can point to specific aspects of haskell that make it objectively more powerful than java. Haskell is also objectively more powerful than SQL, but SQL is not intended to be a general purpose programming language, so its lacking in that department doesn't bother me.
&gt;Are you saying that stored procedures aren't portable Of course not. &gt;That's even worse. You can't write functions in haskell and then run them in PHP either, that isn't a flaw of haskell or PHP. &gt;However, I'd like to look at an example of how you would write a stored procedure that takes columns as argument in your favourite DBMS's query language I wouldn't do that. That is like asking how I would use a state monad in C. Trying to fight against the system to make it work superficially like a totally different system doesn't work out well. &gt;You may be able to convince me that it can be done. You just showed it can be done? It is just a little less verbose in postgresql or oracle.
&gt; A declarative query language doesn't really seem to stand to benefit much. Well, fair enough. If you're happy with the status quo you're not my target audience. However, I personally have benefitted a great deal from a relational query language embedded in Haskell where everything is first class. &gt; I find most of the problems people have with SQL databases tend to be self-inflicted. There's nothing "self-inflicted" about SQL forcing subselects to be named. That's poor-language-design-inflicted. There's nothing "self-inflicted" about all of SQL's deficiencies outlined by Hugh Darwen in *The Askew Wall (SQL and The Relational Model)* (available at http://www.dcs.warwick.ac.uk/~hugh/#CS252). Maybe you're happy to use such a language. If so, great. Personally I'm not. I see all those deficiencies as standing in the way of me being productive in a decent query languange, in the same way that I see all Java's deficiencies as standing in the way of me being productive in Haskell. 
&gt; &gt; However, I'd like to look at an example of how you would write a stored procedure that takes columns as argument in your favourite DBMS's query language &gt; &gt; I wouldn't do that. That is like asking how I would use a state monad in C. Trying to fight against the system to make it work superficially like a totally different system doesn't work out well. Right, I wouldn't use C, instead I'd use Haskell. I wouldn't use SQL either, but the Haskell of relational query languages didn't exist, so I wrote one. Of course you can get stuff done in C. Of course you can get stuff done in SQL. Personally I'd rather not use either. &gt; &gt; You may be able to convince me that it can be done. &gt; &gt; You just showed it can be done? It is just a little less verbose in postgresql or oracle. Fair enough. I wouldn't like to write it in any of them.
&gt;Right, I wouldn't use C, instead I'd use Haskell. You would never use C for anything? &gt;I wouldn't use SQL either, but the Haskell of relational query languages didn't exist, so I wrote one. I just wanted to know what you think the haskell of query languages would look like, what problems it would be trying to address, etc.
&gt; You would never use C for anything? Perhaps not literally nothing ever, but pretty close. I wouldn't take a job where C was the right tool because I don't like programming in C. I wouldn't take a job where a soldering iron was the right tool either, because I don't like soldering. &gt; I just wanted to know what you think the haskell of query languages would look like, what problems it would be trying to address, etc. I hope I've given you some sort of flavour of what it would look like. It seems you don't see the benefits over SQL, but that's OK. Not everyone appreciates the same flavour of programming.
&gt; "optional" type systems seem completely pointless. the people who you wish would use this feature are exactly the ones who never will and &gt; You want anything done for sure, give the job to compiler :) Two very astute observations from two commentors in this thread! :)
You deserve it Jasper :)
Interestingly, with enough duplication and specialization and boilerplate for lambdas, we can directly and typesafely convert from Haskell (with some extensions, even) to Java, while the other direction becomes very hard (and impossible if you don't cheat the type system). One example is that the OO nature of Java let's you have some rank-n types (not restricted in n, but restricted in the kinds you can quantify over).
I'm envisioning the next step to be: sudo apt-get install stackage Or brew install stackage etc. It's basically Haskell Platform Plus, and it would be nice for Haskell users to be able to install it as such.
Yep, I think I posted the exact same response to you in the HN thread :) (I'm acomar there). EDIT: Hah, just noticed you cross linked this comment there. I'm a little amused by how this conversation circled back on itself. I'm not really disagreeing with you, except that I'm not terribly impressed with optional typing as a concept. I very much enjoy the power of static typing in my programs and ghci + vim allows me to do all the rapid prototyping I've ever done.
The answers don't seem to mention nullability. People are claiming pattern matching is unrelated to the type system, but *typed* pattern matching (Along with sum types) is very much related to the type system and adds a safety that Java doesn't have. They also don't mention soundness (Java covariance problems), or the various interesting GHC extensions (GADTs and the various invariants you can prove with them).
The easiest way to fight with concatenating strings is to use some kind of templating language. I for example use a simple string interpolation templating from Shakespearean template libraries. [st|select foo, bar from baz where blah1 = #{someId} and foo2 &lt; #{someNumber}|] Granted not nearly as neat as HaskellDB or Persistent, but very simple, utilises your knowledge of sql language, easily testable. And does not require to gain a lot of arcane knowledge and learn yet another complex library or framework. 
Good point!
You're welcome!
Haskell &gt; Scala &gt; Java
This could not be implemented using the existential antipattern either. Your problem is that you only have a set of operations over your data. You have lost all of the concrete information about the type itself. That loss was, in fact, a goal you stated. So you can only combine the operations provided, and nothing else. You could store the original item inside and then have an `Image a`, but then you lose the ability to have lists of multiple types of image. Or you could use dynamic typing (`Typeable` / `Data.Dynamic`). Basically, you can either be generic or you can be specific, but you can't do both.
I think this should be fine. class ImageClass a toBW :: a -&gt; IO (a) bin :: a -&gt; a -&gt; IO (a) Yeah? I don't intend to do binary operations between abstracted interfaces.
&gt; What this allows mathematicians to do, though, is to create new interesting data types corresponding to more interesting examples of these things. You get a data type for a Circle, or a Sphere, or a Torus. You can define functions between them via recursion the same way you'd define a function on lists or trees. These new fancy data types are called higher inductive types, and while they don't (currently) have any use for programmers, they pay the meager salaries of long beards in the ivory tower. So *homotopy* type theory lets us define morphisms between continuous objects in the same recursive/computable way we currently define them between discrete objects?
It's completely nutty to me that a binary operation over an abstracted interface is at all a nontrivial thing.
I would like to see your Java version of the `Monad` typeclass -- or even simpler, `Bounded`.
I may be misunderstanding your thesis, but it seems to me that the issues it brings up with correctness all end up with the optimization possibly increasing termination. While annoying, I don't see this as too big a problem - in fact, I believe that one of GHC's core optimizations has the same effect (I forget which one). If this issue is that big a problem, you can fix the implementation of `head` by introducing a new primitive to the class: seqStream :: MStream m (Element seq) -&gt; m () For lists, which are lazy in both spine and value, it would just be `const (return ())`. For boxed vectors, which are strict in spine but lazy in value, it would run along the stream, ignoring all the values but making sure there were no exceptional values in the spine. For unboxed vectors, which are strict in both spine and value, it would run along the stream, forcing then ignoring all the yielded values. With this primitive in place, `head` could just call `seqStream` after it had retrieved the first element and have the right semantics.
The "existential anti-pattern" is really more about Haskell's limited support for abstraction than anything else. Existential types are EXACTLY what one needs to handle binary methods over abstract data. One could define your interface thus: data ImageLib = forall image. ImageLib { toBW :: image -&gt; IO (), resize :: image -&gt; Int -&gt; Int -&gt; IO (), invert :: image -&gt; Int -&gt; Int -&gt; IO (), save :: image -&gt; String -&gt; IO (), bin :: image -&gt; image -&gt; IO image } Haskell makes this kind of programming terrible though. Since, you can only open an existential in a case expression, and every time you open the existential you get a different type. You can sem simulate modules, but only sort of. In a language with proper sigma types though, this is the correct way of handling abstraction. Anyone out there designing dependently typed languages: modules are just dependent records. You don't need a seperate module language (or can make it just syntactic sugar plus guarentees of static evaluation) if you provide flexible first class records. A better solution in Haskell is to make the type not existential but universal class Image image where toBW :: image -&gt; IO () resize :: image -&gt; Int -&gt; Int -&gt; IO () invert :: image -&gt; Int -&gt; Int -&gt; IO () save :: image -&gt; String -&gt; IO () bin :: image -&gt; image -&gt; IO image this lets you write functions to be parametric in the type of the image. This is at least as nice as working with your `Image` type, except that you can't invent new implementations at runtime.
I was just hoping to hide "image" inside the type so function signatures don't all have to be like userFunction :: Image a =&gt; a -&gt; blah Instead you define class ImageClass image where... data Image = forall i. ImageClass i =&gt; Image i as a wrapper
Keep in mind that a function taking an existential parameter is equivalent to a universal function. That is, `forall a. P a -&gt; B` ~ `(exists a. P a) -&gt; B`. So most things that want to consume images and produce them homogeneously can just do that, while preserving more informati on in the return type than you'd probably do in a subtypey OO language or an existential. In cases when you want to move between back-ends, you can still do that too, just by varying types explicitly.
That's "the Haskell way", though. It has a lot of advantages, and you don't have to deal with the annoying existential issues or imprecise types. Take an OO language: you can define a method on `Image` that takes another `Image` instance, but you have no way of referring to your most precise type (unless you do something awkward like the CRTP or Scala's equivalent), so you're forced to return an `Image` which might be a different flavor of image. So you're one flavor of image, are forced to deal with another flavor as a parameter, and return (potentially) yet another flavor. Edit: to be clear, you can do exactly what you're describing, but your type becomes less precise and you pay the price you've described for doing so. That isn't really a Haskell-specific thing, though. 
Assuming you never want to mix and match things from these backends, then typeclasses are exactly correct. If you _do_ want to mix and match, you need to "forget" how they are different. If you "forget" how they're different, then you can't make sure that your binary operations aren't themselves mixing and matching between different backends, so you don't e.g. convolve two images with different backends. Unless you _do_ want to convolve two images with different backends, and then that's a whole other question?
Existentials give you more that can not be ecoded using just universals though. For example, one can dynamically construct an existential based on the needs of the situation. What is more, even in the case of the existential occuring in a negative possition, converting to universals leads to an explosion of the number of type variables (Oleg's trickery not withstanding). Including existentials also allows you to more often program in direct style, rather than having to CPS convert everything. Finally, the true Sigma type is more powerfull type theoretically than the encoding of it into (impredicative) Pi types. Well, this is only sort of true, since if you have true Sigma, there exists an isomorphism. Here given the rules t : Type_n f : t -&gt; Type_m ------ Sigma t f : Type_{max m n} a : t b : f a --------- (a,b) : Sigma t f we have the projection based eliminators x : Sigma t f ------- pi1 x : t x : Sigma t f ------- pi2 x : f (pi1 x) one can try to encode Sigma using Pi, but these elimination rules are not derivable unless you already have the Sigma type. Instead you get something like the rule for existentials in Haskell Gamma, x : t, y : f x :- z : Z Gamma :- e : Sigma t f -------------- Gamma :- case e of (x,y) -&gt; z : Z Which is much weaker because it does not include the assertion that "existence implies a choice." OO gives a very poor idea of what existentials enable since most languages don't provide the projection functions (since that requires purity) or even associated types at all--ML modules are a much better example. That some sub typing is crappy does not make all sub typing crappy. I vastly prefer Haskell to ML. But, not having good support for existential types is a huge wort in Haskelll. ML does not have good support for existentials in the term language, but at least most of the major ML dialects have proper modules languages.
I need more upvotes to give or if I were a rich man I'd give gold. Hey one of you rich men, please take care of that for me.
&gt; at least most of the major ML dialects have proper modules languages. Yeah... Considering that I lied about this being about an image processing thing (I want to abstract over text rendering backends), and that I need short GC pauses, and perhaps eager evaluation so that I might be able to more easily reason about my code, and weak references so I can properly store scene graphs, maybe o'caml is a better tool for the job. Not to troll, of course.
I've never heard of an "existential antipattern." I have heard of a supposed "existential type class antipattern." It is where you create a class for the sole purpose of quantifying over it existentially in a data type. The suggestion is that you just use a data type directly. Whether you still use existential quantification, or use some isomorphic type without it is up to you at that point.
Typeclasses are fine here. It's the existential datatype hack used for heterogenous collections vs records of functions that are usually an issue.
I forgot to answer your question about `ArrowChoice`. The reason this wasn't included in the main library is that the correct instance for `Arrow` and `ArrowChoice` requires push-based pipes. I'm reserving this for a separate upcoming `mvc` library. I highly, highly, highly recommend you read this thread on the haskell pipes mailing list: https://groups.google.com/forum/?fromgroups#!topic/haskell-pipes/7mDzAD9Al-8 That shows a really fully developed example of a design that incorporates `ArrowChoice` and `StateT`, exactly like you were asking in order to collapse a concurrent system into a pure single-threaded specification.
Yet more proof that building in Haskell is like a built in funnel and filter for top talent! As someone hiring for Haskellers, I'm selfishly glad that so few startup realise this.
Haddock is the standard documentation tool (a la doxygen). I also make fairly extensive use of lhs2TeX. Hoogle can be run locally that allows you to search your codebase by name or type, incredibly useful. GHC has code coverage checking (hpc), and property-based testing tools like QuickCheck and SmallCheck are invaluable. test-framework is a good package to structure your tests. As for static analysis, there's not much (aside from Catch, which is not maintained), however refinement types like LiquidHaskell look pretty interesting. Often I find myself just leveraging the GHC type system as a substitute for external static analysis. Stylish-haskell can assist in reformatting code. Criterion is great for running benchmarks. For remote monitoring of long-running applications ekg is fundamentally awesome. Cabal with sandboxes is quite a nifty little build system, and I find myself hating it less with each passing year.
I never want to mix and match.
You seem to be committing very early to a definite conception of how this should go. Have you looked at libraries that deal with similar problems -- or indeed more or less this problem? There is nothing to be said against a type class like this `Pixel` https://github.com/Twinside/Juicy.Pixels/blob/master/Codec/Picture/Types.hs#L520 which refers to the semi-concrete data type (it essentially involves a type family) `Image` https://github.com/Twinside/Juicy.Pixels/blob/master/Codec/Picture/Types.hs#L93.hs Whatever you define with the eleven methods of this class can be instantiated at any of the ever-expanding series of image types supported by the library. I don't see why you are focussing on a blog post on gui libraries that doesn't actually argue what it says it will argue, has nothing to do with any of the problems you will face, and is anyway highly disputable. 
You did not provide a minimal sample code that reproduces the error. I use forking in yesod quite often. Never had any problems. 
Hmm, I was worried this was the case. How would I go about debugging this? I'd love to file a bug report.
&gt; ...That error message indicates a segfault... Where can I find the list of exit code -&gt; description mappings?
I'm beginning to suspect that the forking is not the problem. I'll try to compile a minimal example.
When I wrote &gt; with enough duplication and specialization I meant that the Haskell program would be turned into a monomorphic program first, with all typeclasses inlined.
I'm curious about the statements 'I wrote my embedded query language' and 'compiles via HaskellDB'. Did you write a wrapper for HaskellDB or what? Can you show examples?
Just turning on warnings also gives you some of the benefits other languages get through external static analysis tools. Some of the warnings are *very* useful; it took me far too long to realize this, largely because I did not know how to turn warnings on in GHCi. It turns out you can just set a flag: Prelude&gt; :set -Wall That single piece of advice would have significantly improved my life a few years ago :D.
Who are "they"? core.typed is an independent project and I haven't heard of any plans to make it part of the language itself. 
Erlang is a good counter-example, I think, where the core libraries _are_ using the optional type specifications.
I am of course forgetting the even more obvious type synonyms, which are defined symbols written in uppercase...
&gt; That I know for a fact that there is a better way that already exists and just needs a few quirks ironed out is what hurts most. But maybe Postgres is fine with subselects these days! Like I said, I use HaskellDB's internal AST to generate 300 line SQL queries that run in the blink of an eye, so I don't think it can be a problem these days. EDIT: this may also be of interest: http://www.reddit.com/r/haskell/comments/ypdox/my_view_on_why_relational_algebra_matters/c5xsnui
You would have the same problem in an OO language. If you had a class with only the operations toBW, resize, invert and save, you still wouldn't be able to solve this. The solution would probably be to add a way to get the individual pixels or something like that.
HaskellDB has two levels to its implementation. There's the Query monad which is the key concept in the API, and there's the internal PrimQuery that the Query gets converted to before the PrimQuery itself is converted to SQL. I call PrimQuery the "internal language", "internal AST" or "intermediate format". My language is very similar to HaskellDB's query, except it does away with the hacked-in record syntax and it replaces the monadic API with an arrow API. The record syntax is something that's best dealt with as a generic Haskell library or language structure. In my API you can use any Haskell datatype you like, tuples, your own user-defined datatypes, or even HaskellDB's records if you really want. The HaskellDB monadic API has a number of bugs that allow you to generate queries that don't mean what you think they mean, or even worse, that are malformed SQL. I'm pretty sure that the monadic interface is too powerful to ever be bug free, and I'm pretty sure that the arrow interface is bug free, so I took the conservative approach and went with the latter. I used your blog posts on HaskellDB as inspiration when writing the library, so thank you for those!
I looked at it yesterday, it was a bit worse than I remembered, but I started shaping it up to a release. If nothing happens in, say, a week that means that I probably forgot about it, then please yell at me :) (i'm extremely good at procrastination :) The "framework" as it stands is basically separation of state, configuration, "rendering" (of the leds), reacting to buttons, and reacting to midi sync signal. Furthermore there is a separate state called "mode" which I found useful (which is intended to be used for different "screen modes", similarly as Launchpad behaves in ableton)
It's worth noting that some tools are integrated with EclipseFP, so that's what I tend to use: * HLint for code quality (EclipseFP let me apply the HLint suggestion as a quick fix) * GHC warning and errors by turning -Wall (same thing) * Haddock generation (EclipseFP: export project -&gt; as haddock, opens the result in browser) * HTF integration for running tests and jumping to failure * Import reformatting (import only what you need) * Stylish Haskell for reformatting whole module 
Considering that FRP is really easy to integrate with impure code, just subscribe to the events you want and react with mutating variables or whatever dirty stuff you want, I'm not sure his final conclusion that it's unlikely to get popular is necessarily true.
* `lens` - it feels overkill for the first 5 minutes, but rapidly pays for itself beyond then. * `monad-loops` - half of this stuff belongs in `base`, I find it so useful * `vim2hs` provides me with my "IDE", if you will. On the fly syntax checking means that I get rapid feedback, and `-Wall` combined with HLint means I keep the kitchen tidy as I'm working. * `ghci` - indispensable when it comes to bottom up programming * `+RTS -s` to quickly eyeball that performance is looking healthy. I know you asked for tools, but I consider the choice of libraries equally important. Good libraries lead to good, compositional code, which in turn means I have less interest code quality tools because that quality "comes for free" by how the libraries direct my coding style.
a discussion about performance and render time on different scenes and different resolutions would've been interesting.
I hope to release a library for launchpad interaction pretty soon, if you can afford to wait a few days
As of this moment, no one has submitted a good equivalent so far. The solution submitted by Eric Lippert looks a little bit like the Haskell version. But as I wrote there in a comment, it does not appear to cache elements of the sequence that have already been computed. So for each fibonacci number, it starts all the way at the beginning, and then yet again for each fibonacci it needs along the way. That explodes into exponential complexity. So it seems like this will be useless beyond the first few fibonacci numbers. Is that correct, and if so, is there any way to fix that without writing the algorithm in a different way? Also keep in mind that the Haskell version, the cached elements are automatically garbage-collected when they are no longer needed. So if you iterate over the sequence only once without keeping a reference to it, only the current element and the previous one are kept in memory. Whereas if you keep a reference to the sequence so that you can iterate more than once, all of the elements generated so far are kept in memory. I doubt that there's any way to get that exact behavior in C#. 
not thought about it too hard but I think your first point is addressable, i.e. you could define a type similar to IEnumerable&lt;T&gt; that does cache element fairly easily, but i'm pretty sure the naive implementation of this would open you up to the problem in your second point i.e. a space leak
`-fno-code` means no TH splices get looked at though.
&gt; code-analysis tools for COBOL (in Java) Whoa, now there’s stuck with COBOL **and** Java. Well played.
Hmm, well TH definitely runs when I use `-fno-code` but I haven't checked whether the result is actually typechecked! I guess you're saying not ...
hlint
EclipseFP is great. It's a *bunch* of features bundled up into a very nice graphical text editor.
When chris done wrote that blog, almost exactly two years ago is when I last tried postgresql with haskelldb. It just didn't work out for me then. I will give it another go soon because if it works I'm going to start writing some kick ass yesod based stuff.
If you try it again please let me know how you get on. I'm keen to monitor the status of Haskell database stuff.
Comparison with Lens?
&gt; And sorry again if this is a bit of an inappropriate post Not at all — thanks for sharing! Screenshots or video would be nice, though :)
So even a lazy implementation of `Set.fromList` wouldn't do this efficiently?
I think one of the major points of difference is that it uses concrete types rather than type synonyms, and falls back to Control.Category for composition. Also, Lens has a bunch more stuff.
First, I don't think I'm in the best position to answer this question, biased by being the author of `fclabels` and not having that must experience with `lens`. :-) Of course both package solve more or less the same problem. I think `fclabels` has more focus on the original problem of first class record labels and is far less intimidating to use. Not only for beginners. Only looking at the module overview of both packages tells you a lot about a differences in approach. One of the things I personally like about `fclabels` is the explicit category used for effectful lenses. This allow for both total/partial lenses, but can ultimately lead to more interesting designs like lenses for IO/STM references and lenses for e.g. database access. Our team has been using `fclabels` now for the last 4.5 years without any problems and we're still very happy with it. I found the time and inspiration to improve a bit on the previous version, and so I did. Some people prefer `lens`, probably because of its completeness and theoretical foundation, both good arguments. Some other people take `lens` for granted just because Edward wrote it and there is a lot of talk about it, which make also sense But if you want a simpler (but surprisingly complete) alternative take a look at `fclabels`. It works pretty well and has been for a while.
I love that there are a lot of Haskell newbies able to whip up UI apps in Haskell so easily (on Windows!) thanks to Threepenny—all you need is a web browser! 
Anything you can do to minimize the reproducer will help a lot. One thing you can try is linking your program with `-debug` and just running it under gdb and inspecting the stack trace. Hopefully that will give you some idea of where the segfault is occurring. If you have reason to believe that the compiler is at fault, then please post (a link to) your reproducer on the ghc trac. Simpler is better!
What do you mean by "surprisingly complete"? Is there support for indexed lenses, traversals, folds, prisms, equivalences, effectful updates, state/reader zooming/amplification, and the various other things in the lens library? 
The type synonyms allow for subtyping which is extremely nice. It makes a tiny vocabulary cover so much that IMO it's amazing. I think "lens" is the single most influential library I've ever used in Haskell, and heavily transformed the way I write Haskell code. About 1 of every 4 lines of all my code uses a lens export!
You can't get lens and fclabels shoehorned into the same syntax. They both have to hack the polymorphic update stuff in in different ways. We also build on different foundations. Lens starts lower with Getter/Setter, but fclabels has a story for lenses with validation that lens frankly lacks. All in all, I wrote lens originally because nobody could tell me what the laws are once you smash a monad or arrow in the middle of a lens. As far as I can tell, that is still the case.
I may need to steal your GADT totality checker. That is a good trick.
A `Lens a b` doesn't give you a valid `Lens (Maybe a) (Maybe b)`, because you can't put a `Just v` into a `Nothing` and then get it back out. Algebraically, having a `Lens a b` should mean that "a = b * c" for some c. Then 1 + a isn't (1 + b) * c for the same c, or generally any other c. You would need to know what your c is, then you could produce a lens c + a = (1 + b) * c, I suppose.
You can't do it in a way that preserves the lens laws.
Mathematics is precise. If the language used to describe a mathematical concept/expression/etc. is imprecise/unambiguous, then it is not mathematics.
Also I don't think `lens` has anything like the applicative views. I don't know how to do `asTup3` with `lens`.
&gt; The type synonyms allow for subtyping which is extremely nice ...until you are trying to deal with error messages and type information at the repl. `lens` uses type classes for a lot of things to soften this blow. But once you grasp the "general idea" of how to use various lens operators, then things seem to "just work", without having to use manual type conversions all over the palce, which is nice.
Absolutely right. I couldn't tell you the laws. But that monad is damn right convenient. :)
Actually, not necessarily. If they were implemented with type families... data A { a = Int b = Char } = A a b data B { b = [Int] c = () } = A b c would be compiled to data A a b data A b c type family Get_b -- I'll skip the others type instance Get_b (A a b) = b type instance Get_b (B b c) = b
I think the problem is that the length function often isn't lazy and forces evaluation of the whole structure (e.g. with lists). I can't say for sure how it would work for sets though.
Examples of real-world Haskell projects are always encouraged here.
For debugging space leaks, [this page](http://www.haskell.org/ghc/docs/latest/html/users_guide/prof-heap.html) and [this page](http://book.realworldhaskell.org/read/profiling-and-optimization.html) are my Bible.
Wow. Could it be true that after all those years...?
Do you have some use case in mind for a particular `Lens (Maybe a) (Maybe b)`?
thanks for sharing :D (you MUST give us a video ;) )
Thanks! That looks great.
Well its a bit more than that, in that it can come up whenever you have existential quantification over typeclasses, not just those created for the purpose of being quantified over. If I have e.g. `data Showable = Showable (forall a. Showable a =&gt; a)` then I might as well just have `String`, if I have a Showable and a Serializable constraint I might as well just have `(String, ByteString)`, etc.
fclabels was the first proper lens library I saw after Twan Van Laarhoven's post on [functional references](http://twanvl.nl/blog/haskell/overloading-functional-references) and I always thought it was a nice upgrade to the record system. And it was the first package I remember seeing with a tutorial on the Hackage page. =) I never really saw anyone using it in the wild. I was kind of out of touch with the Haskell “what's hot” during the past couple years and it seems suddenly that lens is the new hot thing and everyone loves lenses. My immediate question was like yours, “uh, fclabels anyone?” 
Yes, it's about changing the results for partial values (and correspondingly changing the runtime behaviour -- perhaps quite significantly). You can make the argument that it's "only" going in the more defined direction, but we do generally try pretty hard to avoid that. So yes one can imagine adding more operations, but you need one for each strictness pattern, though in practice you could probably get away with just all combinations of spine and value strict. There are some data structures that have more complicated strictness properties (like lazy bytestrings). Then in your consumers you have to be very careful to use the class methods to (possibly) force the elements you don't touch and the stream tail. So plausible, but subtle, and it would not cover chunked structures like lazy text &amp; bytestring. My concern with this "open" class-based approach is that there is then a disconnect between the people writing stream-based operations and the people making types an instance of Streamable, and my concern there is people will loose track of what the rules are.
There is something like this called `mapping` in `lens` which looks like Functor f =&gt; Iso' s a -&gt; Iso' (f s) (f a) but it takes an `Iso` not a `Lens` which hints at the problems others are pointing out. I was playing with the algebraic "lens" here and came up with something interesting. `Lens s a` is equivalent to knowledge that `s ~ k * a` for some `k`. `Iso s a` says that `k ~ ()` so `s ~ a` directly. `Functor f =&gt; Lens (f s) (f a)` requires that `f s ~ c * f a`. We can get that with `s ~ a` and `c ~ ()` which just says `Iso s a -&gt; Iso (f s) (f a)`, i.e. `mapping`. If all we know is that `s ~ k * a` then we can get that if f (k * a) ~ c * f a which holds if `f` distributes over `*` f (k * a) ~ c * f a f k * f a ~ c * f a ==&gt; c ~ f k `Maybe` clearly doesn't (`1 + k * a ~/~ (1 + k) * (1 + a)`), while `Identity` and `Const` trivially do, as does `[]` with `unzip/zip`. There's the `Unzip` class in [`TypeCompose`](http://hackage.haskell.org/package/TypeCompose-0.9.1/docs/Data-Zip.html) so we might be able to write (Zip f, Unzip f, Functor f) =&gt; Lens' s a -&gt; Lens' (f s) (f a) Though I don't see how right now. 
&gt;Hmm, well I consider everything in Darwen's The Askew Wall (SQL and The Relational Model) a serious language-design flaw The only valid thing I could find in there are the complaints about NULL. Everything else is either wrong or outdated. "'A' = 'A '"? On what database? You have to use "as" to name columns with no names if you want them to have names? How is that a flaw?
I think that's pretty debatable. It's mostly true in some cases (probably `Show`), and probably not in others. For instance, if I have: data Stream1 a = forall s. Stream1 s (s -&gt; (a, s)) then I might as well have: data Stream2 a = Stream2 { head :: a , tail :: Stream2 a } except the if I hang on to a value of the latter, it will eat space as I walk down the stream (due to sharing), while the former may not. There's no way that every single type class in existence has equal or better performance for partially applying all its methods to a hidden value and sharing the results, which is what would be necessary to simply declare any use of an existential with a type class wrong.
I'm very excited about threepenny, it's a very pleasant way to think about GUIs and it's getting some FRP aspects. I think this helps to demonstrate this :) OP: glad you're liking haskell so far :)
`Iso Partial` is so interesting. Somebody just needs to go ahead and write `Retract` and `Section`.
Oh I totally agree. There are lots of legitimate uses for existentials, including with typeclass constraints. Its also that there are, especially among beginners, a fair amount more "pointless" uses. I can't think of a hard-and-fast rule to differentiate the two, but it seems to me that in "most" interesting uses, there will be at least one existentially quantified parameter on the left side of a function arrow. There's also e.g. a typeclass anti-pattern where people use typeclasses for abstracting over data instead of algebraic relationships. But again, there's not exactly a hard-and-fast rule, etc.
What does validation mean in this context?
Yes, as I said in the post, the goal is to compose fstLens with mapLens
Lens doesn't provide you with a mechanism to reject "bad" input. When you have Lens' s a, you can put any a you want in it.
We're making it happen for 7.10
Ohhhhh stupid me. Somehow I missed that. :)
I think somebody else here previously mentioned that there are a sensible set of laws for `ApplicativeIO`: liftIOA (pure r) = pure r liftIOA (f &lt;*&gt; x) = liftIOA f &lt;*&gt; liftIOA x
So presumably, what you want is mapFstLens :: Ord k =&gt; k -&gt; Lens (Map k (a, b)) (Maybe a) Where, for example, `mapFstLens k` is the lens which reads/writes the first portion of a tuple found in a map at key `k`. In that case, there's an obvious getter: getMapFst :: Ord k =&gt; k -&gt; (Map k (a, b) -&gt; Maybe a) getMapFst k m = fst &lt;$&gt; Map.lookup k m but not an obvious setter. setMapFst :: Ord k =&gt; k -&gt; Maybe a -&gt; Map k (a, b) -&gt; Map k (a, b) setMapFst k ma m = case ma of Nothing -&gt; Map.delete k m Just a' -&gt; case Map.lookup k m of Nothing -&gt; m -- a' got lost in this branch!??? Just (_, b) -&gt; Map.insert k (a, b) m This means that `get (set a b) == a` is not always true, because sometimes when you try to `set a`, there is no tuple in the map at that key, and you can't pull a `b` out of thin air, so you just have to discard the `a`. What you want cannot obey lens laws, hence your trouble composing lenses.
(+) small code example (+) integrates two interesting things (+) in haskell More like this! edit: Also: (+) doing "low level" and Gui things. &lt;- both of which I think haskell have a reputation of being impractical for.
The two simplest reasons at the time were: `lens` allows polymorphic updates and has an elegant solution to partial lenses (i.e. `Traversal`s) that lets you reuse the same functions as total lenses.
Can you have zero-column tables in SQL now?
Builds are fully sandboxed automatically, no special effort required. For patching, check out some current patches in: https://github.com/fpco/stackage/tree/master/patching/patches and the helper scripts for generating these patches in: https://github.com/fpco/stackage/tree/master/patching/scripts For snapshoting: each time you run `stackage select`, it generates a `build-plan.txt` file. That's your snapshot. If you put that file in the Stackage folder and run a build, it will rebuild the exact same set of packages again. You can also manually tweak that file if you want to cherry pick in some new versions of dependencies.
Yes, I would like to see a screenshot of the LaunchPad, too. :-) 
You've been able to for as long as I can remember: test=&gt; create table foo(); CREATE TABLE Why is that important exactly?
It's definitely a direction to look into. Up until now, Stackage has been doing what I personally and FP Complete in general need: creating a reliable set of packages for our users, while giving me the ability to do general audits of a bunch of packages to ensure dependency problems don't arise. But if people are serious about being able to use Stackage in this way, let's get to work on it. It's something I did intend from the beginning, but just haven't seen the demand for yet.
It's amazing how a slightly oddball but "instant gratification" approach can make such a huge difference. And thanks for the kind kudos.
Apparently, every release is Christmas.
Right. This is a good argument for why `PartialLens` is needed in this case.
What do you mean with `Retract` and `Section`?
Can you insert a row into that table? Can you select from it? Can you write a query to select zero columns from any other table?
Partial isomorphisms reminds me of the... [`partial-isomorphisms`][1] package (and associated [`invertible-syntax`][2]). [1]: http://hackage.haskell.org/package/partial-isomorphisms [2]: http://hackage.haskell.org/package/invertible-syntax
"Instant gratification" and "rapid prototyping" have many things in common ;). After bashing away for ages at C code for the MSP430, I installed Energia and had it spitting temperatures to serial in like half an hour. There's something to be said for getting a thing working ASAP!
How is that? (Traversals for partial lenses) Any references to read? :)
http://ro-che.info/ccc/21
I don't know, I have never tried to do random nonsensical things. I assume since he says it was corrected in 1992 that it was corrected in 1992. Again, why is that important exactly?
If I were, theoretically, to make a video for this, at some point in the future, would a separate post be appropriate, or should I just keep updating this one?
I didn't really convey just how much I've enjoyed my time with Haskell so far. Spoiler: it's _a lot_. Yesod and now Threepenny have been fantastic experiences. Never mind the language itself, which was mind-blowing having never done FP before.
I think that advice can doubly be applied to the lens TH code :-)
I'm looking at his section on table_dee and table_dum. It doesn't say there that they were introduced in 1992, so I assume they are still impossible. Furthermore this page http://www.postgresql.org/docs/current/static/sql-createtable.html suggests that zero column tables are not part of standard SQL. As to why one would want them, well without them you don't have as general and expressive language as you have with them. You might ask why one would need a `()` datatype in Haskell. Isn't that non-sensical? Why would I ever write a function that takes a `()` argument? It's the same as not having an argument at all. But without it your language lacks a lot of desirable properties. Same for table_dum and table_dee. I've just remembered another shortcoming of SQL: it's not possibly to write an expression which *polymorphically* represents all queries (of any column specification) that return zero rows. In Haskell this would correspond to the madness of having to write `[]` differently for every type. These things may sound non-sensical to some, but to those of us who like to program at high levels of abstraction they are absolutely vital.
It seems to me that `Eq` instances should have the following laws: * Reflexivity: `x == x` should always be `True`. Note that your `Unique` breaks this law. * Commutativity: `x == y` iff `y == x`. * Transitivity: If `x == y` and `y == z`, then `x == z`. * Substitution: If `x == y`, then `f x == f y` for all `f`. This is the problem with `M`.
Yeah, `fclabels` was my go-to lens library before `lens`, due to its simplicity and features. This is an interesting release - a little variety can't hurt! While I really like `lens`, I can't in good faith point someone that's new to Haskell at it and say "Here's how we do first class fields / traversals / isos / etc". It's excellent to have better lens training wheels, as without being familiar with an existing lens library and adept at typeclasses, it can be quite daunting.
No references necessary, just code: &gt;&gt;&gt; (set _Just 4) (Just 1) Just 4 &gt;&gt;&gt; (set _Just 4) Nothing Nothing `_Just` is both a prism and a traversal that lets you operate only on the `Just`, ignoring any `Nothing`s.
That seems very sensible.
I'm not quite sure I understand your first point. There will only be one method added to the `Streamable` class, and there need be no standard implementation of different strictness patterns. Each instance for each data structure will have its own implementation, specialized to whatever pattern it uses. Because of this, I also see no problem with lazy text or lazy bytestrings. Their strictness patterns are complex, yes, but not unimplementable. As for your second point, I think your worry is unfounded. People should never be touching anything with the `Stream` type unless they are writing instances of `Streamable`. The whole point of fusion is that you can use the generic functions, the ones that work on vectors and lists and such, and not need to drop into the low level (in this case working directly with streams) to get performance. With that in mind, the only issue becomes the correctness of `Streamable` instances. Since there are few axioms, it can easily be well documented. Additionally, there aren't that many sequence data structures in the world - it can't be too hard to check instances for them all thoroughly.
I would drop reflexivity so that `Eq` can support "improper" values such as 0/0 where `0/0 /= 0/0`.
I think the substitution requirement limits things pretty uncomfortably.
Sure, that was the point. It's easy to write basically any part of your code that works well with FRP in FRP, and if you have trouble extending the paradigm further, you can easily escape it by subscribing to the resuling events, as well as re-integrate back into the FRP by creating new stream of events from handlers or whatnot. Of course, the other parts would not be FRP.
&gt; What if f is a function that returns an IO monad... IO doesn't have an Eq instance even now. The rule if x == y, then f x == f y for all f only applies for `f :: A -&gt; B` where both `A` and `B` have `Eq` instances. I suppose that's a tricky law to state, because it cannot be isolated to just `A` or just `B`, it has to do with both.
For example?
As a hint for anyone else who is new to this all and doesn't yet know the vocabulary: a PER is a [Partial Equivalence Relation](https://en.wikipedia.org/wiki/Partial_equivalence_relation), which supports symmetry and transitivity but not necessarily reflexivity or substitution.
Ah, you're right. My mistake.
Ah, nice example. &gt;&gt;&gt; let x = 0/0 in x == x False I would be tempted to try and optimize `==` by checking whether boxed operands are pointers to the same thunk or value, but I suppose that wouldn't work when you *want* certain values to never equal others, as in this example. Should such "improper" values *have* Eq instances? How do you reason about code that uses Eq if you can't guarantee that `x == x`; isn't that ? (I'm not saying I disagree, just raising points for discussion.)
However, does it follow that if the applicative is also a monad, that liftIO x == liftIOA x ?
The concept you want is a quotient, and it's a requirement on destructuring/observing the type. That means that any function that takes it apart needs to go to the effort of making sure (or proving, in a language with quotients) that it preserves equality. No matter where you have such a thing, it's a far greater burden than any of the other laws, as I alluded to in my other post.
This could be made more precise by talking about an equivalence relation rather than an `Eq` instance, which requires decidability. In an idealized world, I can define a relation on `IO` actions that isn't decidable, but could nonetheless be proven. Something like "in all worlds and interleavings of concurrent actions, these produce the same set of observations" or similar. You'd still want to preserve that property, if you went with it. Otherwise it seems kind of slimy to be able to get around the "law" by simply not defining an `Eq` instance for your target type :)
The fact `Float`/`Double` are even instances of `Eq` and `Ord` is mostly a hack, and perhaps we should exclude them from discussions about laws? All code involving Eq/Ord for Floats would have to resort to numerical analysis for reasoning, rather than the laws which are useful in other contexts.
So what I do know is that `liftIO` is one solution to `liftIOA` (assuming that `(&lt;*&gt;) = ap`). What would remain to be proven (or assumed) is that `liftIOA` is unique and therefore `liftIO` is the only solution.
Separate post would be okay. Especially if you've worked on it in the meanwhile.
Several people have mentioned floating point numbers as an obstacle here. I'm not bothered by that; equality among floats and doubles doesn't really work in the first place. How you accumulate round-off errors depends on how you compute your value, even if the results should ideally be the same: let x = [0, 0.1 .. 1] y = foldl (+) 0 x z = foldr (+) 0 x in y == z -- This is False Even worse, x86 architectures these days use FPUs with 80-bit "extended precision" floating point registers. If the value you're working with is still in the register from when it was created, it has higher precision than if it was stored into memory (with a 64-bit representation) and then loaded back into the FPU later. let x = [0, 0.1 .. 1] y = foldl (+) 0 x z = foldl (+) 0 x in y == z -- This value can depend on the load factor of the FPU. Any software engineer worth his or her salt should tell you never to use comparison on floating point numbers because it's not reliable. So, even though floats and doubles violate the laws described, I don't think that should count against these laws.
I think that plays more into the decidable part of my proposed "decidable equivalence relation"? I can define an equivalence relation on streams that is reflexive, transitive, and symmetric, but I won't be able to write the decision procedure. "CoStreams", [on the other hand](https://gist.github.com/copumpkin/4647315#file-gistfile1-hs-L86)... :P
Reflexivity doesn't hold for Float/Double. Reflexivity is a property that "should hold", but IEEE is silly. Substitution only holds for structural equality, but nothing insists `Eq` is structural.
&gt; I certainly haven't come up with an example `f` that's badly behaved yet. instance Eq Bool where _ == _ = True f = not
Do we really want decidability or can we define it neatly over the lifted domain anyway? (Not that any other laws for typeclasses take bottoms into account :-P) I also really don't want substitution/congruence since there are many legitimate uses for values which we want to equate but are somehow distinguishable.
That's along the lines of what I was thinking, too. Internal functions are allowed to break laws as long as the law-breaking functions are not exported.
I'm not following. `x == y` holds for all `x, y :: Bool`, so as `not x :: Bool` we have `x == y =&gt; not x == not y`, right? f True = 1 f False = 0 would violate it, but the whole point of congruence here would be to outlaw these kinds of weirdnesses.
Technically we're talking about a binary operation into Bool, not a real relation, so commutativity isn't *too* bad.
the exported interface is the type for the purposes of laws. That is the entire point of a module system. It is also worth pointing out, that the behaviour of `Set` is undefined and just weird if the underlying Eq instance is an equivalence relation that does not respect substitution toList . fromList $ [b,a] could return `[a]` or it could reutrn `[b]` given `a == b = True`. Substitutability + Reflexivity is the meaning of Eq. Don't let anyone sell you anything less. EDITED as previous example was not true.
substitution works for any type system enforced setoid structure. Proper abstraction with modules (even Haskell modules) means that can be almost arbitrary.
Uh. Yes, `f = not` was a terrible choice given that instance. Your `f` is much better. As for whether that kind of `f` should be allowed or not, I have no opinion; I was just trying (badly) to help NruJaC come up with an example of something that broke that law.
The substitution requirment is best interpreted thusly: Every type is a setoid (or a thin groupoid if you are a category theorist). Every function must respect the equality relation as a setoid morphism (every function is functorial). We should permit the definition of arbitray setoids, although if the equivalence relation is something other than structural equality, it is the programmers responsibility to ensure that the module defining the type obeys this interface in its exports. A type is a member of `Eq` if its associated equivalence relation is decidable. Substitution framed in this way is sick. Every equivalence relation obeying substitution is the least equivalence relation up to observational equivalence. Thus, you can don't need to declare the `Eq` is the associated equivalence relation for a type--if it obeys the laws it is. I mean of course the laws with a fixed version of the last one: a == b =&gt; f a = f b you also don't even need transitivity, as that is implied by substitution. a == b =&gt; (a == c) = (b == c) You don't need symmetry, as that is implied by substitution. a == b =&gt; (b == a) = (b == b) You only need reflexivity and substitution. Substitution is better known in philosophy and logic as ["Leibniz' principle of idenity"](http://en.wikipedia.org/wiki/Identity_of_indiscernibles) This is not just some law. It is the law that defines equality.
I agree that this works in logic, and it's a solid notion of identity, but I don't think it works for Eq. We're dealing with computational equivalence, not strict identity. It seems like a negative definition -- two items are equal unless I can observe them behaving differently with respect to some function f. This is a very difficult notion of equality to guarantee computationally. A separate typeclass that could guarantee this very strong notion of identity might be better, and would be backwards compatible with current code.
I don't think defining it over the lifted domain is a problem. IMO, symmetry and transitivity should keep there definitions as they continue to be sensible in the lifted setting, the only thing that should change is a == a need only produce an answer that is less than or equal to `True`.
\forall x y z seems reasonable, but \forall f?
The "Map" or "Set" example is good for any datatype that doesn't keep its internals fully hidden. And in fact, even now, we can use nonlawful monoid instances to examine the tree structure of a Map. But just consider a ByteString. We obviously only care about value equality. But we can access the Internals module, peek inside, and tell if the pointers are the same or not. Or maybe an ADT for a functional language. Maybe we want equality up to alpha equivalence. Etc...
If I have two Set Integer values that are equivalent under the public API but not equivalent under the internals, should they be (==)?
Looks like the segfault is somewhere in the hint library which I'm using to dynamically load code.
I think this boils down to two (seemingly) independent questions. 1. When f and g are monads, is a morphism of applicatives from f to g automatically a morphism of monads? 2. How many morphisms of applicatives can there be from f to g? How many morphisms of monads? Every morphism of monads is automatically a morphism of applicatives so the latter is bounded above by the former (when f and g are both monads). It may also be enlightening to ask 3. Is the problem easier when f is IO? I don't know how to approach any of these questions. 
&gt; About 1 of every 4 lines of all my code uses a lens export! Do you have any lens-heavy code of yours you can point me to? I've never used lens in my code, and I'm beginning to feel I'm really missing out! I'd like to see what it looks like.
The lists may be different, but it will always be true that they are "equal": toList (fromList [b,a]) == toList (fromList [a,b])
I like this example: [Lamdu's AddNextHoles module](https://github.com/Peaker/lamdu/blob/master/Lamdu/GUI/ExpressionGui/AddNextHoles.hs)
If they break the substitution law then although `a == b` since `f a =/= f b` it gets weird: map f $ toList . fromList $ [b,a] This returns either `[f a]` or `[f b]` which could be different.
I posted this above, but I think the issue is that map f $ toList . fromList $ [b,a] doesn't have a clear result if `f a =/= f b` (which is what I understand from an equivalence relation not respecting substitution) Edit - A concrete example If `"Dr" == "Doctor"` Then `map length $ toList . fromList $ ["Dr","Doctor"]` Returns either `[2]` or `[6]`
Yes, but that is a problem with the `length` function, not with the custom equality of lists. For abstract data types, the substitution principle holds in the following form: &gt; Substitution': If `x == y`, then `f x == f y` for all *exported* `f`. In other words, equality should be preserved for all "allowed" functions.
"So it's kind of like a shading language for a shading language."
Wouldn't the substitution law imply that all types must implement the Eq typeclass? By virtue of that law *any* type returned by f (which can be any function) must implement Eq. 
&gt; Yes, but that is a problem with the length function, not with the custom equality of lists. It's not really important which is to blame, surely? The point being that if substitution doesn't hold then weird things start happening. toList (fromList [b,a]) == toList (fromList [a,b]) May be true, but if there is a function that shows a difference between a and b then you've got quite a big problem. To push slightly on the blame side, if there was a datatype like this (sorry if my syntax is wrong): data Expr a = Value a | BinOp (a -&gt; a -&gt; a) (Expr a) (Expr a) We could define equality as a == b = (eval a) == (eval b) That two expressions are the same if they evaluate to the same result: BinOp (+) (Value 2) (Value 2) == Value 4 We could also define a `numberOfNodes`, which would expose the difference. `numberOfNodes` isn't incorrect or invalid, I'd say the definition of the equality is wrong. 
Pattern match exhaustiveness isn't checked with `-fno-code`. Something to be aware of.
I would like the substitution law to hold when I am not accessing internals.
This is mostly because it doesn't generalize. If you accidentally use a field in a non-linear way or use it on a pair of full traversals then you wind up with an illegal lens-like construction. The approach is sound if you make sure to use non-overlapping lenses, that are at worst partial (affine traversals in lens parlance). If you go to generalize the concept to a full traversal that can have multiple targets it isn't sound. We do have combinators for combining folds that are always valid, but we tend to shy away from combinators that require you to reason about global invariants. This sort of problem arises all over once you start allowing arbitrary monads in the middle of a lens. Consider e.g. if you had IO-based lenses that could see the inside of an IORef, it can't be a legal lens without global reasoning. You need to know that you'll never find a reference to the same IORef transitively within itself, or you start breaking the lens laws. You need to know that globally your system forms a DAG. Here you need to know that your field accessors don't overlap.
Yes, but we have a pretty long history of making non-structural Eq instances to make maps out of sets, etc. Another example is the IEEE -0 == 0 despite the fact that when you divide by them on takes you to -Infinity and the other to Infinity making them distinguishable in an exotic corner case that is rarely observed. Things like this are why Eq is still lawless in the report. Symmetry and transitivity could be written in the report and nobody would complain. Substitution and reflexivity on the other hand don't always hold in practice but are a bar to aspire to when writing instances.
How about fold on sets?
Right. But you can also get this from `take 1 . sort . map f $ [b, a]`. Going through the `Set` just doesn't make things weirder than before.
Thanks for your work putting these together. the new `LVar` work seems to be a lot of fun.
I assume you mean `take 1 . map f $ sort [b, a]`, but yes, Set is just an example. Maybe I've misunderstood you, I thought by this &gt; but [a] and [b] are equivalent! you were saying they would act the same.
The case-insensitive platform package [breaks substitution](https://github.com/basvandijk/case-insensitive/issues/11). I personally think this is bad. An obvious use case for `CI` is for keys in a `Map` or values in a `Set`, and then `original` becomes unpredictable and can be used to observe the internal behavior of the container. I don't know if this is a big problem in practice, but one reason to use Haskell is to not need to worry about if this sort of thing is problematic or not.
http://www.haskell.org/ghc/docs/7.6.3/html/libraries/process-1.1.0.2/System-Process.html -- the various run* and read* functions are what you are looking for. Additionally, you can get the content of the $PATH env variable with [System.Environment.getEnv](http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-Environment.html#v:getEnv) or (safer) System.Environment.lookupEnv: import System.Environment main = maybe (putStrLn "no $PATH") print =&lt;&lt; lookupEnv "PATH" -- prints the content of the PATH env variable -- if it exists, prints "no $PATH" otherwise 
Well to be fair, formatting the list from the POPL webpage was half a minute of Emacs macro repetition. The real work is to add the link to each of the papers, and it is far from done. I like the LVar work indeed; you may have seen or heard about the presentation at FHPC last week. I plan to discuss it on Lambda-the-Ultimate soon, but am waiting for the imminent next release of the Haskell library. If you have a more thorough look by then, I'm sure the authors will welcome any comment.
&gt;These things may sound non-sensical to some, but to those of us who like to program at high levels of abstraction they are absolutely vital. Condescension really doesn't help make your case. I asked a simple question. Why does having a zero column table matter? Why does selecting zero columns matter? Saying "I am just better than you" doesn't answer that question.
Yeah, sorry! It wasn't supposed to take that long, but I got quite busy this summer.
Yes, please write symmetry &amp; transitivity into the report and then lets fix functions like groupOn to have an Eq constraint.
The most general function is `createProcess`, whic allows you to specify very detailed information about how you want your process to run. Most of the time that's too much control though; `runProcess` is more like the every-day way of starting system calls. In particular, it makes it easy to specify to which `Handle` STDIN/-OUT go, giving you access to the program's output for example. (And if you really just want to run a program without any configuration use `system`.)
&gt; Here you need to know that your field accessors don't overlap. Or learn to live with the consequences. I don't mind lacking full algebraic laws and full statically checked invariants if the idioms are powerful enough to help me with my daily software engineering. I don't really care that I can screw up my applicative views when not being careful. Screwing up is not that likely. I can still write problematic left recursive grammars with Parsec, but that doesn't prevent me from writing decent parsers with it. Maybe we have a different view on software engineering, or just have a different purpose in mind for our packages, but it can pretty much live with those 'problems', simply because they aren't really problems for me. And maybe the concept are sound, but under different assumptions. I couldn't tell.
Yay!
I'm not sure how useful that would be. Any attempt to use IEEE would explode into repeated reassurances to the type system that, yes, I really did mean to add those two together even though addition isn't associative, and yes, I really did mean to multiply those two together, and yes... and so on. IEEE floats are extremely broken from this perspective. You'd effectively end up with an entirely parallel set of operators out of the `Num` typeclass entirely. Abstractly, that's not necessarily a bad idea, but it's certainly something I'd hate to have to add to the list of things I have to reassure the Haskell beginner about, to say nothing of the backwards compatibility issues. However, it would be an interesting thought for Haskell-the-sequel; evict IEEE floats from the primary numeric hierarchy, and deliberately treat them as something only number-*ish*, rather than numbers per se.
The most convenient of those, in my opinion, is `readProcess`; it accepts a command, a list of arguments, and a string for the command's stdin, then it forwards the command's stderr to your program's stderr, and returns the program's stdout as a string. But unlike many of the other functions in that package, `readProcess` doesn't have a variant which accepts a shell-interpreted string instead of a list of arguments. Luckily, it is easy to [build](https://github.com/gelisam/submake/blob/master/src/Submake/Cooker.hs#L20) one: readShell :: String -&gt; String -&gt; IO String readShell cmd input = readProcess "bash" ["-c", cmd] input
&gt; When f and g are monads, is a morphism of applicatives from f to g automatically a morphism of monads? If m is a monad then I can make W m a := m (a, m ()) a monad: it's the the writer transformer that writes values of type m () (composition is (&gt;&gt;)), applied to m. In other words, W m is m enlarged by the ability to record "m-effects". There is an obvious f : m a -&gt; W m a, which doesn't record any effects and leaves the action unchanged. f is a monad morphism. There is also g : m a -&gt; W m a, for which g x records the effect x &gt;&gt; return () and then does x (or does x and then records the effect x &gt;&gt; return (); actions in the base monad commute with recording effects). g is an applicative morphism, because we can commute the recording of effects away from the actions, and the effects of x &lt;*&gt; y are the effects of x combined monoidally (by (&gt;&gt;)) with the effects of y. But g just doesn't interact well with join, so it's not a monad morphism. See http://lpaste.net/93759 for full definitions + QuickCheck tests.
That doesn't type check. It is not a natural transformation.
Oh, whoops, never mind. See my other post for an actual answer. (Though I can't imagine you really need to do something so convoluted to produce multiple applicative morphisms from IO to some MonadIO m.)
It's a law that would only apply when both the source and target types are Eq
Hmm, the GHC API (which is what hint is a front end for) is not really thread safe, see http://ghc.haskell.org/trac/ghc/ticket/3373. You can try the workaround in that ticket. Also, if this turns out to be your issue, it would be great if you could (1) write a little bit on that ticket about your use case (2) if possible, provide a small reproducer which causes a segfault. GHC still shouldn't be segfaulting when used from multiple threads, but maybe it's building some modules with the wrong set of flags, which causes segfaults later when they are loaded. EDIT: Or actually, maybe your issue is more likely to be this: http://darcsden.com/jcpetruzza/hint/issue/3 If two instances of hint are writing to (say) the same object file at the same time, that could obviously cause bad things to happen later.
Yes, it works for looking for environments.
I am not sure of that. When I tried *readProcess*, there is no environment information like *$PATH*. But you have given me a good example.
A Handle is what you also get when you open a file. You can read from it with hGetContents for example. `hout` should behave no different than `h` in `h &lt;- openFile 'foobar.txt' ReadMode`. Here's a full example of how to execute `ls -l -a` and store its standard output in a file `list-of-files` in the current directory: import System.Process import Control.Exception import System.IO main = let ls h = runProcess "/bin/ls" -- Executable location ["-l", "-a"] -- Parameters Nothing -- Working directory: current dir Nothing -- Standard environment Nothing -- STDIN (Just h) -- Connect STDOUT to the Handle opened above Nothing -- STDERR in bracket (openFile "list-of-files" WriteMode) hClose ls In the specific case you're only interested in the standard output, there's also a predefined function to do that for you. The above example could have been written as import System.Process main = readProcess "ls" ["-la"] "" &gt;&gt;= writeFile "list-of-files" `System.Process` includes a couple of these specialized functions to make systems calls. They're all implemented in terms of the most general case though.
As a learning haskeller who had been persuaded by the antipattern argument, I have just reverted a design to existentials by this exact reason. Could we say, more generally, that existentials might make sense if (i) you need an abstracted type and interface, as in the so-called "heterogeneous list" use case; and (b) nevertheless, you still care for the implementation details hidden by the abstraction, e.g. for performance reasons?
Hmm, maybe the behaviour of `readProcess` is platform-specific? I'm on cygwin, and it works for me: &gt; readProcess "bash" ["-c", "echo $PATH"] "" "/usr/local/bin:/usr/bin:/usr/bin\n"
Great to see other people as happy as I was when I first heard of Threepenny :)
Here is a specific example for m = IO, with liftIO' defined to be g from the above post. ex1 = liftIO' $ do n &lt;- getLine putStrLn n ex2 = do n &lt;- liftIO' getLine liftIO' $ putStrLn n run w = do putStrLn "Running w" Effect e &lt;- execWriterT w putStrLn "Running e" e `ex1` records the entire read+write action at once, so `run ex1` will read a line and write it back, then read another line and write the second line back. `ex2` records the actions of reading and writing separately, so `run ex2` will read a line and write it back, then read another line and write the *first* line back. I was only able to arrange for these to be different because the second action depends on the result of the first action, i.e., because I used Monad. With just Applicative I couldn't construct an example like this.
I'm the maker! Thanks for making it famous, friends. 20,000 views and 500 subscribers in about 3 days. And the 3rd most viewed Haskell vid ever made. I didn't think it would ever happen. Needless to say, I don't mind the 1 and a half months wait while that video stagnated in the oblivion. Thanks so much, everyone!
You'd be safer using /bin/sh, really...
Do you mean because it integrates with your editor, or because it provides better or different functionality somehow?
So would I, though "we can use nonlawful monoid instances to examine the tree structure of a Map" is an observation.
Interesting idea. I expect it would result in too many false positives, though. In your example, using `foldl'` already ensures that the accumulator is strict. And `e` is not an accumulator, it is an argument for the list values. Strictness in `e` will not have much effect.
Please do a favor to the community : abandon classy-prelude.
Yup, this is nice. Are you saying it's still an open question whether `g` is an applicative morphism, but you strongly believe that it is? That would agree with my understanding. 
 import Safe headMay [] == Nothing Includes several other total functions (i.e. they never call 'error'). See [here] (http://hackage.haskell.org/package/safe-0.3.3/docs/Safe.html). 
Perhaps in the section "Strictness to the Rescue" you should put the proper parentheses around the `+`'s to make it clearer why the sums can't reduce until the list's spine is fully dethunked.
I am thinking for a while to do something like this in the [frege](https://github.com/Frege/frege) compiler. The objective would be to give a warning on arguments of tail recursive functions that appear in a non-atomic expression that gets passed in the same place, like in this equation for foldl: foldl f a (x:xs) = foldl f (f a x) xs Here, no warning for the function argument, because it remains constant. Also no warning for the list argument, because the expression xs in the recursive call is *smaller* than (x:xs). But the accumulator argument would be flagged, because it is clear that the thunk gets bigger and bigger on every recursion, unless it is made strict.
Ah I see that it is indeed specified in the interface that fold will always fold in increasing order. However, one could imagine a set where this is not the case. Such a fold would still be useful, if the observable behavior of the program does not depend on the order, but it would break the substitution law because intermediate values of the program could depend on the order. More generally every time you have a data type that is really a quotient (like set is a quotient of lists with respect to permutations), you're gonna have problems with the substitution law because Haskell does not have quotient types.
How did I miss monad-loops? Thx.
There are two separate issues here. First, Eq on Float is broken because of its behavior on NaN. This violates the reflexivity property of an equivalence relation, and Eq clearly should represent an equivalence relation. This is a real problem. But I think in the case of NaN, so many basic things are broken that we ought to just treat it as en exception to most type class laws and throw in that caveat when talking about Float or Double in Haskell. Second, there's the issue that Float is often used to represent imprecise computations. I don't see this as a particular problem at all, really. It's clearly well-defined when two Float values are equal to each other. Many (though not all) floating point operations have well-defined results, and even for those that don't, their answers are typically defined within one ulp, so that it's easy to perform further operations that make the result well-defined again (for example, if you divide the answer by 2). How you obtained the values, and whether you've reasoned precisely enough about the results of your operations, doesn't affect the validity of a Float value. For example, it's easy to see how you might want to use Float as keys in some kind of hash map, which would require an Eq instance as well as being hashable. As long as you get keys out, store them, and use them again for later lookups (and guarantee never to use NaN as a key), you're fine. It's only if you perform arithmetic on your keys (and are not extremely careful) that you start running into problems.
The lack of QuickCheck in comments thus far is interesting... 
&gt; producing a List with tail-recursion and strictness is certainly always [a bad idea]. While I agree with this in general, I can think of one glaring counter case: `reverse`. If you *dont* reverse a list with strict tail recursion, you may blow out your heap with thunks that would ALL have to be evaluated the moment you grab the first element of the resulting list. You may choose to wrap the reverse worker function in a thunk, as an optimisation, but certainly the actual list reversal should be strict tail recursive. A better rule of thumb is to say that you should use strict tail recursive on lists only if you know that you would *have to* process the entire list (or even just &gt;90%) before it would be possible to get an answer. I.E. what is the first 3 elements of a reversed list? you would have to process the entire list to find out!
It's much-much faster. hdevtools finishes checking my code in the same amount of time it takes cabal to just get over 'Preprocessing executable xxx...' and actually start compiling.
Your "length1" function is incorrect. It should be like this: length1 :: [a] -&gt; Int length1 l = go l 0 where go [] acc = acc go (x:xs) acc = go xs (acc+1) 
There's this too: &gt; It seems to “deconstruct” the list by pattern matching on it, kind of forcing the evaluation of the list it is passed, not caring about how that list is produced, somehow being strict in its argument. So, passing it an infinite list will not terminate, right? Indeed it won’t. Am I missing something? As far as I see, both length1 are lazy on the list values, but one is `strict` on the accumulator. That is : one create a chain of `+1` and the other doesn't. http://ideone.com/1zyEgY 
I don't even understand the criticism. It is nonsensical to me.
&gt; If a prelude replacement is required, consider something with as few changes as possible, such as basic-prelude, or consider building your own from core-prelude. Just a little nitpick: `CorePrelude` is a module in the `basic-prelude` package; it is not in its own `core-prelude` package, although it could be if that is what people want.
Ok, so you need to turn `x`, which is an `a`, into some `x'` which is a `c`. You have the function `f`, which has type `a -&gt; c`, which means it takes an `a` and returns a `c`. So applying `f` to `x` should give you your `x'`. That compiler error is saying 'I expected this function to return a list of `c`s, but it's giving me a list of `a`s (which is `[x]`).' That's the part about 'expected type `c`' and 'actual type `a`'.
Please, use markdown codeblocks. indent code by 4 spaces.
Likewise, Andres Löh asks the same for "free monads" at http://permalink.gmane.org/gmane.comp.lang.haskell.libraries/20406
I just tried, and was referred to https://github.com/haskell/hackage-server/issues/52 :-(
...btw, would `IO` be the "God Monad" then? :-)
Sorry, both should have been the same, I somehow managed to miss that when re-reading. And they evaluate the spines of the list but not the values. That's what I show litle by little. Anything unclear now? I fixed the "second" length to be same as the one right before, should be clearer now.
The whole hierarchy of lens/prisms/etc is a huge feature in lens missing in fclabels though. We need first class data access, not just first class record fields.
Done, that's hopefully clearer now.
here is the properly formatted code data Tree a b = Empty | Leaf a | Branch b (Tree a b) (Tree a b) deriving Eq -- deriving Eq needed for tests to be able to compare trees preorder :: (a -&gt; c) -&gt; (b -&gt; c) -&gt; Tree a b -&gt; [c] preorder f g Empty = [] preorder f g (Leaf a) = [a] -- preorder f g (Branch b l r) = [b] ++ (preorder l) ++ (preorder r)
The very related problem of first class data constructor access also needs to be solved.
Congrats on the new vim-like keybindings!
I don't know if anyone is still reading this, but here is a recent post which quotes a Chris Date classic on some of the difficulties with SQL which would be absent in a more faithful representation of relational theory. It talks more about duplicate rows, but also shows how many ways you can try to write the same query in SQL. http://pyrseas.wordpress.com/2013/10/02/multisets-and-the-relational-model/ 
Sounds like a good reason to give Yi another try. I would love having a vim-like editor which I can hack on using Haskell. Congrats on the release!
I actually find it rather hard to understand Date's insistence on relations rather than multirelations (i.e. sets rather than bags or "multisets"). The latter seems more natural to me whilst being strictly more powerful and not sacrificing any mathematical properties, as far as I can tell. Furthermore, aggregation makes a lot more sense in terms of bags.
Oops. I think you put the parens associating the wrong way! Each time `sum (x:xs)` is expanded, it turns into `x + sum xs`. You can safely put parens around that, so `(sum (x:xs)) = (x + sum xs)`. Thus, you have: = sum (1:2:3:4:5:[]) = 1 + sum (2:3:4:5:[]) = 1 + (2 + sum (3:4:5[]))) = 1 + (2 + (3 + (4 + (5 + sum [])))) With this (the proper) order of operations, it makes sense why the addition can't reduce. On the second line, you're adding `1` to.... let me go and see... you're adding `1` to `(2 + sum(3:4:5:[]))`... which is... let me go and see... Again and again, you have to "go and see" before you can add the one.
Please please please, make it more hacker friendly. I tried to setup it in the last couple of days, but it's interaction with cabal sandboxes is sketchy at best. I cloned the repo, sanboxed, installed, started, generated the configuration file. Cannot import any module. Ok, obviously that file is loaded outside the sandbox. Now gone to the yi folder in the repo, start a new sandbox, install, generate configuration, copy it in the src/executable/HackerMain.hs, reinstall with -fhacking. Though luck, it doesn't work. Modify yi.cabal set main-is: HackerMain.hs, install, it works. You need to define a simpler flow, otherwise other people would have it just as hard to get started hacking on it. And it should be among the primary reasons of the editor, isn't it? Don't take it as a an attack, but consider that others would have given up way earlier than me. And still haven't had the time to get to the code.
Then you don't have an existential in the first place.
It may also be helpful to realize that &gt; The values in the tree cannot be collected to a list as such because the values on the leafs are of a different type than the values on the branching nodes. is not entirely true: you can create an `[Either a b]` and then map/fold over it.
I also found looking at the code before that it was quite difficult to grok, partly due to its use of its own prelude, and some rather confusingly named data types..
New!?!?! https://github.com/yi-editor/yi/commit/fc1914c238b665062f4202271e704981380ba932
This is the relevant bit for people who don't want to read the whole thing: &gt; JOSH: Well, if you build your software like that, it would probably be fine, but the problem is the data. And even in Rich’s talk, he kept talking about, “Oh yeah, we just built these large data structures.” It’s their — god object pattern where you say, “Okay great. We want to have all these functions that we can reason about using the techniques of functional programming and we want them to be composable and lazy evaluation. You take all of the data complexity and you put it in some big Hash in the sky and that’s your shared state and it’s mutable and you’ve sort of drawn a box around that and said, “Okay, this is where all of the complexity of the system lives and then all of the functions can be pure and mathematical and lazy, and asynchronous.” My answer to this is that good functional style isolates computations only to the subset of the data that they actually need to know about. You definitely would not use one big hash and instead you would use a nested data structure. Then you can bring to bear many tools to isolate computations, like using `zoom` from `Control.Lens` to limit a stateful computation to a deeply nested sub-field.
Simon, if you don't feel like such a complexity can be justified, don't try to justify it :) Most powerful solutions are simple and consistant solutions.
Yes, new. The new vim keybindings are replacing the old one. Did you read the release notes?
Right sorry, I edited it too quickly, thanks for keeping a close eye, fixed now :-)
Perhaps that's true for lone wolf or small shop application developers, as often happens in web development. But for commercial product development, it's even more important than for library developers to stick to vetted standards. Your code will need to be reviewed, maintained, and re-factored by teams of other developers and QA engineers whom you have never met, and may never meet. And this will continue (hopefully) for years to come, long after the latest buzz that "everyone knows about" is forgotten. Standards give you a common language with them. If you use anything that is not yet a widely accepted standard, you will likely be incurring additional costs that significantly reduce the value of your work. Whereas for open-source library authors, you can use whatever you want, no matter how experimental, as long as you clearly warn about that up front. If the community ends up not going with that - well, it just means they'll use someone else's library this time instead of yours. It's up to you to decide what gambles to take.
That said, [Yi uses its own prelude](https://github.com/yi-editor/yi/blob/master/yi/src/library/Yi/Prelude.hs) and I find it an annoying barrier to hacking on the project.
objects aren't algebraic data types.
How could anything with a name like costate comonad fail to be the most useful thing in all of programming history?
&gt; My answer to this is that good functional style isolates computations only to the subset of the data that they actually need to know about. Yep, that's my thought too. They criticize the "god object" because it has everything. But at some point you have to bring everything together to make a whole application. In C and C++ this is pretty much invisible because you always have access to everything. You can muck with pointers to access pretty much any location in memory. So the process's address space becomes the implicit "god object" that nobody actually thinks about. Languages like Java and C# are a bit better in this regard because of the automatic memory management, but I still think the same essential problem exists in all applications whether they're functional or object oriented. In Haskell, purity gives us the ability to do the vast majority of work in small self-contained functions that only operate on a small piece rather than the whole "god object". I agree with geezusfreeek that their criticism seems rather nonsensical. But if I attribute the maximum amount of sense to it, I still have to conclude that the "god pattern" is one of two things. It could be the above-mentioned inevitable collection of everything that *has* to happen to bring many separate components together into a large application. Or it could be just bad design, in which case the criticism is a straw man. 
Every one in 4 lines in much of my code uses lens. The use cases tend to be everywhere, but mostly small, and could of course be written without lens only more tediously. None of them alone would seem compelling on its own, perhaps, but the library transformed the way I write Haskell. Useful uses of lens are everywhere.
Lens has a *lot* of use cases. See for yourself after SPJ posts his slides.
yah after reading about it, lens is more like monad or inheritance in java, not like ldap or http library.
Peaker wrote: &gt; Every one in 4 lines in much of my code uses lens. The use cases tend to be everywhere, but mostly small, and could of course be written without lens only more tediously. None of them alone would seem compelling on its own, perhaps, but the library transformed the way I write Haskell. Useful uses of lens are everywhere. I'm repeating Peaker's post here because it vanished as a reply to a heavily downvoted post. Peaker, perhaps send some examples to SPJ. On the cafe nearly every nice example was actually a prism. Perhaps SPJ should restructure the talk to include them. Or do you have anything nice without prisms?
I don't think this comment deserves to be down-voted. Lenses in general and the lens package in particular do add complexity on top of the existing records system (e.g. about 100 operators). We might disagree whether this complexity buys us enough good things to be worth it, but we shouldn't down-vote people for questioning whether it does.
&gt; MonoFoldable and MonoTraversable fully generalize Foldable and Traversable, so it is possible to just export Mono versions of functions. This is not strictly true. map length ["hello","world"] There are very few times when I'm actually doing type preserving maps and traversals, even over restricted containers.
Am I the only talk author for the Exchange who already has enough of my own examples :P
&gt; could of course be *written* without lens only more tediously. But what what about *reading*? Of course using a huge library with a ton of operators can make your code more concise, but does it make it easier to understand? First of all there is the overhead of having to know all these operators. But then aside from that, how much is gained/lost in terms of clarity of code? And this really is a question. I'm not sufficiently acquainted with lenses to know the answer. 
Thanks for writing this by the way. Laziness, strictness, and how thunks work has been on my list of "known unknowns", i.e., things I know about, but don't know. I haven't finished reading it yet in detail, but I'm excited to get a better understanding of how to make sure that my relatively newbie code remains performant.
Snap uses lenses in a way that relies on their first-class-ness, to tell server components ("snaplets") how to read and write their snaplet-specific state out of a big record containing the whole server state. Maybe someone more familiar with Snap can elaborate.
Couldn't this statement be cut-and-pasted into a discussion about monads, applicatives, arrows, etc? I have seen it often about Haskell itself from Java/Python/etc programmers...
Sure. But the statement is a question, and the answer even if not objective, is not necessarily the same for all of those. 
That's great, thanks. I got this to compile and I'm about to test it. [EDIT: It works! Magic. Thank you for your help Edward] newtype FR a s = FR { runFR :: a } instance Reifies s (RowParser a) =&gt; FromRow (FR a s) where fromRow = fmap FR (reflect (Proxy :: Proxy s)) asProxyOf3 :: h (g (f s)) -&gt; Proxy s -&gt; h (g (f s)) asProxyOf3 a _ = a -- SQL.query_ with explicit RowParser query_ :: RowParser c -&gt; SQL.Connection -&gt; SQL.Query -&gt; IO [c] query_ rowParser conn sql = reify rowParser (fmap (map runFR) . asProxyOf3 (SQL.query_ conn sql)) 
No worries, happy to help! Good luck with the rest of it :).
This is a good example of how blurry the lines can become between applications and libraries. Is not every application just a composit of libraries? Where is the application really? Is it just Main.hs? If so, does Main.hs really need its own prelude. When can you assume everyone who interacts with your application's libraries is familiar with your application's prelude? How will you know your application's libraries won't become generally usefull enough to be consumed by seperate applications?
I could use the same argument you gave as one against using Haskell. That is because it is very true that just because something can be used by application developers without affecting the open source community does not imply that they should use it. The important thing is that the team is on the same page and clearly understands the trade-offs being made. 
I down-voted because it was a loaded suggestion. "If you don't feel like such a complexity can be justified..." SPJ said nothing to suggest this was the how he felt. On the contrary, he expressed faith in the justified-ness of the lens library: "I’m sure you are using them in all sorts of cool ways that I would never think of, and I’d love to know."
Co(state monad algebra)?
Purely a matter of taste, but personally I prefer to write this: sum :: Num a =&gt; [a] -&gt; a sum xs = go xs 0 where go [] acc = acc go (x:xs) acc = go xs (acc + x) Like this: sum :: Num a =&gt; [a] -&gt; a sum xs = go xs 0 where go [] acc = acc go (x:xs) acc = go xs (acc + x) When my function definition is a one-liner with a where clause, I like to put the `where` on the first line, similar to how everyone puts `where` on the same line as an `instance` declaration. It saves a lot of indentation.
Interesting pov: the combination of Eq and Num is problematic, but Eq without Num is reasonable. Sounds like this view might work, but it suggests maybe banning Float's Num instance?
At Haskell Exchange I am doing a talk about all the presenters who didn't come up with their own examples. Should I use my own examples or not?
The addMaterial function is not plausible because it allows to violate energy conservation. But it's been an interesting read, thanks. I've spent quite some time on an Haskell renderer myself (following the really good "Physically based Rendering" book).
Let me expand a bit more on my reasoning. When I read things like, &gt; Every one in 4 lines in much of my code uses lens. The use cases tend to be everywhere, but mostly small (...) This reminds me of those people who use arrow combinators everywhere. It makes their code shorter, results in all sorts of neat free-point style little functions, and gives you immense satisfaction. But in the end, 1. now anyone looking at the code will have to know arrows, 2. it really didn't help *that* much, 3. code sometimes becomes harder to read even for them. Of course these little code snippets on their own don't make a good argument for arrows. Arrows have their applications. But they're restricted enough that I rather not have arrow combinators laying around everywhere. Particularly when they won't make the code any simpler to read. 
Oh well, I just thought it looked nice that way. But I don't mind other styles at all.
 map co [state, mona, algebra]
Snap basically maintains application state in a `StateT` monad transformer. Snap uses lenses to zoom in on subsets of that application state. The `with` function from `MonadSnaplet` is identical to the `zoom` function from `Control.Lens`. The only wrinkle is that `MonadSnaplet` also retains a reference to the outermost state at all times, for user convenience. The `b` type parameter points to the outermost state (it's a contraction of "base") and the `v` type parameter points to the current state.
It'd probably have to be Co(algebra monad state)
Cool! Does this have M-x doctor yet? Or -gasp- org-mode?
Lens ultimately boils down to a rather simple and elegant core. Many of the operators are just there for convenience: they are simple to express using more basic lens constructs. Moreover, you do not have to memorize most of them--the naming is very systematic and consistent. You have to remember what a few basic symbols mean, and then most of the operators become clear. For example, you just have to know that ~ represents setting and then you also know what +~, -~ and so on do. There are only a few base symbols like this to know, and most operators are really "words" made up of these atoms. Once you know how to read these words, it all becomes clear. From the outside, an operator like &lt;?= just seems absurd. But you can read it piece by piece: = means it works on the state of a State monad; ? means it works on a Maybe value and &lt; means it returns the old value. So this funky operator take a lens that points to a Maybe, uses it to change the current state and returns the old value the lens was pointing to. Not so bad, I think.
Lens was written by an industry programmer, to be used in an industry project. About as far from purely PhD material as possible! Honestly, this is more akin to a JavaScript templating library. Except elegant, theoretically well-founded and generally more awesome.
Right. But it is bitwise equality aside from those two specific exceptions, right? floatingEq f1 f2 | nanCornerCase = False | zeroCornerCase = True | otherwise = bitwiseEq f1 f2 where nanCornerCase = ... zeroCornerCase = ...
The operators are not a fair measure of complexity. Lens has a simple and elegant core; much of the library is just made up of convenience functions over this core. Moreover, the operators are defined in a very systematic way so you don't have to memorize all of them. You just have to learn some basic symbols like ~ for setting and = for setting in a State monad; the actual operators are just combinations of these several basic symbols. Once you have =, for example, +=, -= and so on are trivial. The same for ~ and +~, -~ and so on.
I think it can be useful for memoization in some circumstances. False negatives aren't then a correctness issue.
Not sure if this counts as part of the "nested records" that SPJ has already heard enough about, but we're using it in [beeminder-api](https://github.com/dmwit/beeminder-api) to deal with namespacing. A good concise example goes like this. We have several records we can send as arguments to various API calls; many of them need a username to issue the call against. So we have: data UserParameters = UserParameters { upUsername :: Maybe Text , {- ... other fields -} } deriving (Eq, Ord, Show, Read) data CreateGoalParameters = CreateGoalParameters { cgpUsername :: Maybe Text , {- ... other fields -} } deriving (Eq, Ord, Show, Read) So each field gets its own unique name. Then, to make the Haskell side of the API cleaner, we have a type class like this: class HasUsername a where _Username :: Simple Lens a (Maybe Text) instance HasUsername UserParameters where _Username = lens upUsername (\s b -&gt; s { upUsername = b }) instance HasUsername CreateGoalParameters where _Username = lens cgpUsername (\s b -&gt; s { cgpUsername = b }) This lets you set the username in any API call in a uniform way -- you don't have to remember a different name for each call.
Exact equality to zero can be useful. If your computation is significantly expensive and there's a likelihood that you'll end up with exact zero values (defaults for instance) then the bitwise check might be valuable.
&gt; And this is a general pattern: if there is a free-forgetful adjunction F -| U : C -&gt; D involving categories of algebraic structures, then the category of algebras of the monad UF is equivalent to the category C. Would this provide a missing definition for what "forgetful" means? Rather, what properties of the categories and functors cause this to hold?
Thanks for the reply. Good to know there is a simple and elegant core. And I can see how such a consistent operator naming scheme may help keep things simple and intuitive. Like I said, I don't know that much about lenses at the moment so I don't really have a well formed opinion. But let me tell you that just looking at the [package webpage](http://hackage.haskell.org/package/lens-3.9.2/docs/Control-Lens.html) certainly does not convey that idea of a simple core with some more helpful functions. You're greeted with a picture of a pretty big class hierarchy. Lens aren't even at the bottom of it. Getters seem to descend directly from Lens alone which is OK. But Setters which you seemed to be talking about, descend from Traversal, which itself descends from Lens and Prism (both having the same parents). From the package page and accessing what seems to be the main module documentation, i.e. [Control.Lens](http://hackage.haskell.org/package/lens-3.9.2/docs/Control-Lens.html) doesn't really provide us with anything, but access to over 25 child modules. I'm guessing Control.Lens.Lens would be where the core resides. If there is a simple useful core, it would be nice to identify it. But here you find references to not only Lens, Getter and Setter, but also Conjoined, Overloading, Iso, Indexable, Pretext, Traversal, Fold, and others. And then the combinators provided in the module. You are right that they seem to be consistently named. But still... there's a ton of them. Do I really want to slowly learn them and start using more and more? Eventually that may reduce my coding time, true... but do I really want people reading my code to have to learn all that? And then there's the types... This pick certainly isn't the worst, I just went for one of the first in Control.Lens.Lens. (%%~) :: Overloading p q f s t a b -&gt; p a (f b) -&gt; q s (f t) Ouch. Type errors can't be fun. Now I know I'm sounding quite negative here which is a bit unfair. Truth is it does seem rather interesting, and eventually I'll get around to play with these libraries and possibly change my mind. My point is simply that at first sight, it really comes across as a pretty complex beast, and it has to make you wonder whether it is really worth it. 
This article is interesting, but it just barely missed explaining a question I had a while ago: What is a morphism of monads? Are they just natural transformations between the underlying endofunctors? Or do they need to satisfy any axioms?
Seems like treating 0 as 1 BC and (-1) as 2 BC is pretty self-consistent, if a bit strange-looking at first.
Its a co transformer.
I'm not sure there are any properties about the functors that matter, except that they form an adjunction. Consider `F -| U : C -&gt; D`. We have that `UF` is a monad and `FU` is a comonad. So there are: η : B -&gt; UF B ε : FU A -&gt; A The other halves come from further properties: Uε : UFU A -&gt; U A Fη : F B -&gt; FUF B we use these at `F B` and `U A` respectively to get: μ : UFUF B -&gt; UF B δ : FU A -&gt; FUFU A Now, consider `UF` algebras: (B:D, b : UF B -&gt; B) For every `A : C`, we can construct an algebra: (U A, Uε : UFU A -&gt; U A) The first requirement: Uε . η = id is part of one of the ways to define an adjunction. For the second, note: ε . FUg = g . ε because `ε` is a natural transformation from `FU` to the identity. So: U(ε . FUg) = U(g . ε) Uε . UFUg = Ug . Uε Uε . UFUε = Uε . Uε for g = ε So, certainly, for any adjunction `F -| U : C -&gt; D`, there is an embedding of `C` into the category of algebras of `UF`. The above is actually just the standard fact that for any monad `T : D -&gt; D`, the categories `C` and adjunctions `F -| U : C -&gt; D` that give rise to `T` can be seen as a category themselves, and the adjunction with the category of T-algebras (which is called the Eilenberg-Moore category) is terminal. The Kleisli category, on the other hand, is initial. So I suppose that is an example of a category where this doesn't work out. One way of characterizing the Kleisli category is that it consists of the free `T`-algebras: `(TA, join : TTA -&gt; TA)` and algebra homomorphisms. So the Eilenberg-Moore category for `T` is ostensibly not equivalent to the Kleisli category.
A monad morphism `tr` is a transformation between the underlying functors that should follow the rules: tr . return = return tr . join = join . tr . fmap tr (= join . fmap tr . tr -- these are equivalent via naturality) These correspond to rules for monoid homomorphisms: h empty = empty h (x `append` y) = h x `append` h y (the `tr . fmap tr` part of the monad case is the tensor mapping `tr ⊗ tr`, similar to `h ⊗ h`)
So monoid homomorphisms, when viewing monads as monoids in a category of endofunctors? 
yes. Although I think you could also see them as functors between the kleisli categories that obey a naturality condition. A monad morphism is a natural transformation `phi` such that phi . return = return (phi . f) &lt;=&lt; (phi . g) = phi . (f &lt;=&lt; g) which is equivalent to the statement that the map `(phi .)` is a functor of Kleisli categories. And so, given an arbitrary functor `f` between Kleisli categories, it is a monad morphism if f return is a natural transformation.
Apparently so, yes.
As monad morphism is a natural transformation that satisfies the following laws: morph (return x) = return x morph $ do x &lt;- m = do x &lt;- morph m f x morph (f x) An example monad morphism is `lift` from the `MonadTrans` class and the monad transformer laws are the monad morphism laws. Like philipjf mentioned, the monad morphism laws are actually functor laws between two Kleisli categories: (morph .) (f &gt;=&gt; g) = (morph .) f &gt;=&gt; (morph .) g (morph .) return = return
[There are differing conventions.](http://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar)
P.S. I'm interested to hear about what you're doing where this matters.
Thank you for working that out! I still have a lot of trouble tracking the Eilenberg-Moore category, but I see the embedding of C now. I really should play with it more, though.
Ouch. This seemed to me like an important feature to reduce cabal hell. :(
I found Parsec much easier to deal with than Happy, but if you still want to use a parser generator, the only way I've dealt with shift/reduce conflicts is to comment out productions in the grammar until the error goes away.
I can think of at least a few important numerical algorithms that require it. For example, in the classical CART segmentation algorithm, you'll sweep through each column's values in the dataset, sort them and then try a split in the tree for each unique value for the field (and finally pick the one that maximizes or minimizes a metric). That "each unique value" part typically requires you do have == on the values so you can collect the values that are the same. If your values are floats, which is often the case in the numerical domain, then there you have it: a need for == on floats.
Thanks for the suggestion! The Happy documentation claims that it's the parser generator used by GHC, which makes it sound like the standard parser generator. However, Parsec sounds much more powerful. Any idea why it's not the standard (or why it is but I just didn't realize it)?
It's perfectly fine if everybody in the company uses the same alternate Prelude. If anything, I think this is a *good* idea: you should have your own internal standard library that is optimized for the code you write. This is what Jane Street does with OCaml--they even open sourced their version. (They also have some proprietary code used as a standard library as well.) I think it works very well, better than sticking with the normal standard library.
Also, you can just not use any operators. They aren't required. There are quite a few users of `lens` who love the library, but don't like the operators and use the actual functions themselves. In fact all the operators will presumably be moving into a separate import in 3.10, precisely so you can import lens, and only buy into the operators when you want. (And before anyone else brings it up, module structure isn't a viable metric either - `lens` seems massive, but in reality it is designed to provide useful prisms, traversals and lenses for most of the Haskell Platform. The core body of lens itself is quite small - although it is made far more powerful and consistent by having this stuff at hand.)
Thanks! I'll give it a try.
Read this. http://dlang.org/d-floating-point.html My short answer: NAN, (+/-)INFINITY, (+/-)1 and 0 can be compared exactly. Also, `double`s can represent all 32 bit integers exactly.
FWIW: Parsec is not a parser generator. Parsec is a library. Happy generates haskell code. Parsec is haskell code. Happy based parsers are usually faster and more space efficient. Parsec based parsers compile (much) faster. Happy catches more errors at compile time. Parsec might have shorter development time. Happy is an LALR(1) by default parser, but has support for GLR so can parse any context free language. Parsec is a library for only LL(k) parsing, but the monadic interface allows for parsing of arbitrary context sensitive languages with some more effort.
You might be interested in the Template Haskell `makeLenses` and `makeClassy` from [Control.Lens.TH](http://hackage.haskell.org/package/lens-3.9.2/docs/Control-Lens-TH.html) which do basically the same thing.
This is perfect, thank you! I've found and fixed my conflict. I hadn't heard of Alex, and instead built my lexer by hand (going off the example from the Happy manual). It's good enough for the moment, but if I need to make it more complicated, I'll look into Alex. I clearly have a lot to learn about monadic parsers before I can have an informed opinion of them.
One thing I didn't understand initially, but now do, was your advice on IRC to make sure I unwrapped the `FR` before returning from the function that is passed to reify. The idea is to make sure the `s` type does not escape.
Awesome stuff!
Thanks! If you could see your way to upvoting over on the programming reddit, I'd appreciate it :)
Thanks, that is indeed very helpful. 
Subvert the paradigm! Don't provide any examples. Leave everything as an exercise for the reader.
One use case I've had so far is when timestamping messages with sub-second precision using `Double`, across different architectures that may have different clock resolutions. If two timestamps mismatch, the messages aren't identical; in that sense, they serve a hash-like purpose, but also contain the metadata of their creation time.
Doesn't the article explain the problem pretty well? Look at the queries in the article. If you don't have duplicate rows, they all mean the same, and produce the same results. Looking at them, I think they look like they all should mean the same. If you allow duplicate rows, and the duplicates represent something significant in the real world (and if they don't, then why have them?), these queries all give different results, therefore they all have different meanings. Even worse, they give different different results in different SQL DBMSs. Should your business make different decisions based on the preferred style of how the the SQL query writer likes to write his SQL? But this is what will happen if you assign significance to duplicate rows in SQL. I don't see how this can be anything but absurd. If you claim that duplicate rows are a good idea, how can you explain all these queries which I can't see why they should each mean something different, all produce different results. I think the only reasonable response to this is to say that duplicate rows is a very dangerous thing at best, and complete garbage at worst. I don't think that a any other system (using 'bags' or 'multisets') than SQL could give a working solution to this (except if the query language is completely hobbled), since indistinguishable duplicate rows cannot have a logical meaning to the user of the system who cannot see underneath the covers to identify each row individually. Once you add explicit ids to distinguish them, you aren't dealing with duplicate rows any more. 
Mid point rounding algorithms can use equality
I don't have access to the original paper, so I can't tell exactly what was intended, but the setup of his example seems odd to me. Why do the P and SP tables have duplicate rows? It seems like they shouldn't. Without them the results of the queries would probably make a lot more sense. I think duplicate rows are useful in general, but that doesn't mean that *every* table should permit them! 
A statically typed incarnation of org-mode would be really great. I keep stumbling on mysterious runtime errors when using third-party org-mode extensions, and can't help thinking of how this would never happen if the whole thing were built on haskell instead of elisp.
What is the method which takes a set of table definitions, which ones have duplicates, and a query, and tells you whether the query produces valid results or not? I think there is no such method. If you use duplicate rows, you have no simple and general way of determining if your queries produce nonsense or not. If you only ever write trivial queries, this is not a problem, as long as you are sure that neither you or anyone else will try to write non-trivial queries in the future. To me, it seems irresponsible to take risks like this in programs that people could possibly depend on in any way.
I've started using Alex and Happy while working through "Modern Compiler Implemenation in ML" using Haskell. I've been meaning to blog it acutally, so that anyone else working through the book can avoid the occasional flurries of googling required to get it all going. Anyhow, if you have access to a copy of that book, I highly recommend it. Once you get through the first 3 chapters it becomes much easier to work out / resolve where the shift / reduce conflicts are coming from and how to deal them (either through refactoring the grammar or by using precedence directives).
this is pretty cool!
Yeah, I've done `Map Float a` where there were a discrete (but apriori unknown) set of input floats, but the computation was expensive. If you don't have exotic stuff like NaNs, it's exact.
Great to see more companies cottoning on to what Haskell can bring you. Speaking to my recent experience in hiring a Haskeller, I would liken it to fishing alone in the highest and clearest pool at the end of a steep alpine river: The fish that get there are the strongest of the strong, but they are tired of swimming against the flow for so long and they *really* want to be caught!
It does.
I have three questions for you, in increasing difficulty. I don't know the answers to these, but I suspect the answer to 2 and 3 is there is no such thing, and I also suspect the fact that there is no such thing for both questions is provably true. 1. Can you give me some examples of tables with duplicate rows which do model their domain properly. 2. Can you give some rough rule of thumb as to when using duplicate rows is appropriate? E.g. in terms of tables, or in terms of queries. 3. Can you give a simple and unambiguous procedure which can be applied to any set of tables plus information on which contain duplicate rows, which will determine whether any queries on this database can produce bad results or not. By bad results in this sentence, I mean different sql queries which appear to mean the same thing, but use different syntax (such as the Chris Date example), and produce different numbers of rows in their results. 
Oh indeed it now does, thank you for correcting that. Last time I checked, it used data-lens.
Begging the Question: Presenting a circular argument in which the conclusion was included in the premise. ^^Created ^^at ^^/r/RequestABot ^^If ^^you ^^dont ^^like ^^me, ^^simply ^^reply ^^leave ^^me ^^alone ^^fallacybot ^^, ^^youll ^^never ^^see ^^me ^^again
Technically I think the gregorian calendar doesn't include year AD 0, but many other specs do in order to simplify the underlying representation, I suppose. For instance, the XML Schema spec (which I've been reading lately) made the change, most likely for compatibility with others.
Sorry, could you put that analogy in terms of burritos for me? 
If they are used sensibly, they make things clearer rather than less clear. See for example the simple demo https://github.com/ekmett/lens/blob/master/examples/Pong.hs Of course there is no limit to golfing with the `lens` combinators -- but that is also true of the combinators in the major modules of `base`
And i thought my analogies were cheesy :)) 
Saying that lens is complicated because it consists of lots of modules is like saying Haskell is omplicated because it uses a big compiler. The implementation details tell you very little about the semantics and/or the core concept. It tells you more about how many useful abstractions you get with that particular implementation!
In fact I have a more compelling example for a table with duplicate entries. Suppose I want a transaction journal for goods coming into warehouses with columns `datetime, good, warehouse, quantity`. This table naturally needs duplicate rows. If you don't want duplicate rows you have to add a `transaction_id` column, which seems to be an unnatural hack just to get around this limitation.
Good old SML. Doing all the unpopular things we all feel to be correct.
Fishy, my analogy is fishy :-)
Just a minor clarification here FYI - it is indeed the case that Happy is used in GHC, and Alex for the lexer - [source](http://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Parser).
A big part of what's going on is that the author is equating functional programming with the Gospel as told by Rich Hickey. The reference to "some big Hash in the sky" seems to be about the "Clojure way" of eschewing data types other than lists, sets and maps...
Can you create a non-terminating rule? What happens then? (I see the type is RuleFunc, but it's hard to eyeball things and prove the system is not Turing Complete... it takes such a small little hole for the Turing Chaos to come crashing in....)
This looks AWESOME. Heads up, I tried to sign up with my facebook account and got this: Server error: FacebookException {fbeType = " ()", fbeMessage = ""}
The burrito is given by the expression `return developer`, i.e. it doesn't require performing any action to receive a developer.
I stand corrected. *Nearly* every language... :)
`fromGregorian 1 1 1` is January 1st of 1 A.D. and `fromGregorian (-1) 12 31` is December 31st of 2 B.C.
Thanks!! Sorry, the facebook login is not working yet... You can still connect with the other methods (Google, Yahoo etc.)
of course, you can create a non terminating rule. However, as the rules are usually voted on by the other players (as described by a rule :), your rule should not be accepted. I'm planning on putting some watchdog and securities to prevent those rules to block the game in case it is accepted anyway. 
I don't know if you saw it, but you *can* create an account without any social media account.
Ahah I hope it will not shutdown :)
When I compiled data Foo a = Foo {-# UNPACK #-} !a {-# UNPACK #-} !a with ghc, there was no indication that it didn't like the type variable. Is that limitation documented anywhere?
There is a game running called "The Castle Builders" at : http://www.nomyx.net:8000/Nomyx In this thread you can discuss the on-going game: http://www.nomyx.net/forum/viewtopic.php?f=4&amp;t=1523 Some learning material can be found (including a video) on the main page (www.nomyx.net)
Would you hire someone who knows haskell but doesn't have much Web experience? 
... It's not really that complicated: Set is a functor from the category of equality-preserving functions.
Fishing contains a fish in the same way that cooking contains a burrito.
This reminds me of "Bytebeats", in which one tries to make music with the smallest number of bytes (in x86 code), or in a single loop of C code. Like running: main(t){ for(;;t++) putchar((t*3|t*2&gt;&gt;7)+(t*2&amp;t*3&gt;&gt;8)+(t*2&amp;t&gt;&gt;8)); } and then piping the output to your speaker. * http://www.bemmu.com/music/index.html * http://countercomplex.blogspot.com/2011/10/algorithmic-symphonies-from-one-line-of.html * http://www.pouet.net/topic.php?which=8357&amp;page=1
Lens code tends to be more readable than its lens-free counterparts. Tekmo's [example](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html) is nice: units.traversed.(around target 1.0).health -= 3 There may be a ton of lens operators, but there are not that many operator families: You only need to learn a little bit to understand all of: += -= *= %= +~ -~ *~ %~ %@= %@~ There's a certain convention that makes the meaning of the operator be a composition of the meanings of its components. I do not support libraries making up arbitrary operators in general, but lens does a good job of making a consistent set of operators that are actually generally useful.
Another cool point I think nobody seems to mention is that `lens` moves forward to solve some major issues in the Haskell ecosystem. One of them is the issue of polymorphic containers. Before `lens`, Haskell had no good way to abstract away the differences between `Set` and `IntSet`, `Map` and `IntMap`. And various other types. Sure, naming consistency meant that you could generally change an import to make things work, *unless* you need to have the same code work with both types. Lens doesn't solve this problem completely, but it takes quite a big step in solving it, with various nice classes (`At`, `Each`, `Contains`, ...) that capture a lot of these APIs. The `Contains` class for example, with its one method: `contains` can be used to implement a lot of the Data.Set API! insert, delete, singleton, member, notMember Add the Monoid instance to the mix, and maybe a few other classes, and the Set-specific API that needs to be duplicated for every set-like type is shrunk and shrunk. 
The doc for the pragma lives [here](http://www.haskell.org/ghc/docs/latest/html/users_guide/pragmas.html#unpack-pragma) but I discovered that limitation (although I had guessed it wasn't possible) in an article or on some mailing post.
As long as they're willing to work for the pay of someone who doesn't know haskell...
&gt;However, the way we specified free monads used an adjunction Free -| U between a category of monads and one of endofunctors. For such an &gt; adjunction to exist, it is required that there be a free monad for all F. This is true in Haskell, because it is 'nice' in many ways. But in a general &gt; categorical setting, it is possible for that to fail, so that there is no free monad functor. What is it that makes it true in Haskell land but not in general? 
Haskell is a lot like the mathematical category Set (except, more constructive). It has all sorts of limits, colimits, exponentials, (co)algebras, etc. Also, a lot of ancillary structure is defined in terms of their own objects (lots of category theoretic stuff is defined in terms of Set; like categories have hom-sets, natural transformations are sets of morphisms, etc. Types can serve a similar role). This makes even more category theoretic constructions available (like, a category has powers when you can take exponents by the homs; this is needed for all right Kan extensions to exist; but it's easy when hom objects are already objects of the category). Categories in general do not have this rich a structure. There are standard ways of enriching a category with lots of extra structure (by borrowing it from Set), but in general it isn't available. And while it's very useful (in both mathematics and programming) to have all this structure available, it can also be interesting to try to say what the minimal amount of structure needed to construct something is.
Thanks. I'm still on 7.4.2, so I guess the warning was added post 7.4. Now I'll have to change my code to see if it makes a difference! I'm glad I read your fine post.
Which parts of the problem does lens *not* solve?
Also, it's important to note that while both Haskell and Clojure describe themselves as "functional", they have very different ideas of what "functional" means and deeply different philosophies. Honestly, I think the difference between Haskell and Clojure is roughly the same magnitude as the difference between Haskell and Java.
If I understand correctly, does the Univalence Axiom make this the category of *all* functions?
Even in ordinary type theory without univalence, all functions are equality-preserving. Issue is that Eq has literally nothing to do with equality in the type-theoretic or category-theoretic sense.
So it's a bit like getting a burrito that contains bacon and cheese, unwrapping it and consuming the bacon, then re-wrapping it and sending it on its way for all the other people to fight over the much less tasty cheese.
One of the limitations of vi and vim is the handling of asynchronous tasks. I wonder if yi could go beyond them in that respect.
To elaborate my point about respecting equality, all functions in Type Theory must respect the given propositional equality. In plain Intuitionistic Type Theory, equality is not so big, but it is necessarily respected (or it wouldn't be much of an equality after all!). Having types extrude into higher dimensions than `1` makes equality a bit bigger; the constraint that all terms must respect equality does not change, but it is now quite a bit harder to fulfill. If you want to see what I mean, try building a toy two dimensional type system with types as groupoids and functions as functors between them, such that for every function `f` you must also satisfy `map : a = b -&gt; (f a) = (f b)`. It gets old quick, which is why it would be nice to find a way to make terms respect the richer equality by construction. I'm not clever enough to help with that, but perhaps you get the idea.
Do these classes have laws?
Hey! I would, but the person would have to feel confident that they could learn and become productive with Rails fairly quickly (which really isn't all that hard), as that still is a pretty big part of the project.
the new hackage makes it possible to adjust the compatible version range without modifying the package. that should eliminate most points. i guess.
What about `base &gt;= 4 &amp;&amp; &lt; 5`?
Not at all. Package authors still have to decide whether they want to stick with particular versions of dependencies or to support the latest versions. In both cases they want their constraints to be as inclusive as possible, and that's what Hackage 2 is supposed to address. But when the change affects their package in a non-trivial way, they have to follow the chosen strategy. So my point is that the strategy of depending on a fixed version doesn't work.
That's another good example of something that you can't control. There was a single precedent when GHC shipped with two versions of base simultaneously (I think it was base-3 and base-4), but nowadays it doesn't happen and you don't get to choose the version of base. So by putting constraints on the base version you only decrease the chances for your package to build.
Hackage requires an upper bound on base, I think. I usually put `base &gt;= 4 &amp;&amp; &lt; 5`, because base-5 would be far in the future. Note that PVP would require you to put something like `base &gt;= 4.5 &amp;&amp; &lt; 4.7`. [And we know it will affect a lot of packages very soon.](http://www.haskell.org/pipermail/ghc-devs/2013-September/002671.html)
That's true. But I think upper bounds should just mean 'has proven to work at least upto' and not 'will break with anything above'. What you/Cabal should do with that information is a different question.
&gt; A cabal flag to ignore upper while installing might solve a lot of problems. It would definitely help (it's [#949](https://github.com/haskell/cabal/issues/949), if anyone is interested), but it's not clear when it's going to happen. Right now we don't have that option.
Perhaps I've misunderstood. Isn't that discussed in point 1 of the article, that having interval bounds can still give incompatible constraints? Suppose I want to use module `A`, which requires `base &gt;= 4 &amp;&amp; &lt; 5`, and I use module `B`, which requires `base &gt;= 3 &amp;&amp; &lt; 4`. So, now I can't build my system because the constraints imposed by `A` and `B`, though either is fine on their own, cannot be met simultaneously.
Yes. I was engaged in a [debate](http://my.reddit.com/r/haskell/comments/1dbmk1/cabalinstall_is_evil_and_so_is_cabaldev/c9otlrg) a few months ago defending the "don't do upper bounds" approach. I think that a few more worthy points where touched in that debate.
I tried to read the source of snap once, but got stuck at the abstraction of RST, lensed and lensT.
I own a copy of both books, and have read both. They are both excellent books. However, the Elements of Programming is very C++-influenced. Some of the ideas in the book are paradigm-independent, while others are deeply imperative. This book is really an abstraction of the design of the STL. It works very hard at 'the big picture'. The Algebra of Programming can better be described has having influenced Haskell, rather than the other way around. It does have a definite functional bent to it. And it really is a series of (great) examples -- you won't find a unifying thread here, unlike in EoP. It is about "algorithm derivation" in a beautiful, *unified* setting. In other words, in the context of imperative programming, EoP tries to present **unifying** concepts which abstract away from a lot of details, whereas AoP shows how to use a **unified** tool (the so-called 'algebra of programming') to systematically derive algorithms in a functional context. Having said that, Paul McJones has translated most of the code in EoP to Haskell [he's even sent me a copy of that]. So a lot of the ideas do port nicely. Seeing that is harder: you need to thoroughly understand quite a lot before you can see the fundamental ideas inside EoP (of which there are a lot) and strip them of their imperative bias. Which one you prefer might come down to this: if you prefer abstraction, go for EoP, if you prefer algorithms, go for AoP. 
why not infer type of a package based on types of exports? could work better than having humans assign arbitrary numerals.
I rather keep "avoiding success at any cost" than keep bad API around.
Great answer, thanks!
It's worth noting that the `Note` versions of the functions in that module definitely do call error (eg. `headNote`).
Thanks for sharing you experience about these two books. I have read into the first chapters of both books. I know what you mean about the abstractions covered in EoP and found it very valuable, wanted to read more. Does AoP cover much about abstractions? I know its very arbitrary but could you vaguely suggest a percentage of how much of EoP's abstractions are covered in AoP? Based on what you said, it seems that AoP helps directly with Haskell, more than EoP. Would you say EoP is more 'object-oriented' than AoP?
Yes, that point is still valid, but in the case of base it's even worse, because you are stuck with whatever version of base your GHC ships.
The one thing you didn't talk about is "distribution of pain": there is always going to be some pain, the question is who feels it? I always put upper bounds AND I make sure my packages build with the latest everything. The point is that this is more pain for me and less pain for users of my packages: when something breaks it is me who discovers it first, not some hapless user. As a library maintainer my philosophy is always to move pain away from users and onto me.
There's a problem with unspecified upper constraints: If you later discover that you do need the constraints, cabal is going to find your older package that had no constraints, and use that. 
Do I understand you right, that you're effectively advocating to ignore the PVP altogether in which case we could use simple natural numbers for versioning, instead of carefully following the structured major/minor/patchlevel versioning scheme the PVP prescribes?
I think so, since Edward insists on it.
It doesn't cover the entire vocabulary from these standard modules.
No, I only refer to the upper bounds part of the PVP.
So basically this means, an author who decides to leave out upper bounds implicitly commits... * ...to never define stricter version constraints in newer releases, and * ...to react very timely when his packages break due to a new 3rd party package being uploaded to hackage and release a new fixed version asap The 2nd point becomes more important the more maintainers of popular packages decide to hop on the lazy-upper-bound train, as otherwise it'll become quite likely that at any given time a couple of important packages might be broken for all users, until their respective maintainers react and release a fix...
Then you just put a lower bound on my package, so it doesn't pick that old version. But it's rarely the case when you actually need an upper bound as your permanent solution. Then the package maintainer must have agreed to support more than one major version. I don't think we have any precedents of that — Parsec might be one, but I'm not sure Parsec 2 is actually maintained. And if that old version is not supported, then it will eventually bit-rot.
...but what value remains of the PVP if we don't constraint on upper bounds? We don't need its complex versioning scheme just for lower bounds...
I don't see any reason this is relevant in /r/haskell.
&gt; So my point is that the strategy of depending on a fixed version doesn't work. Just to be clear, someone who takes the conservative approach and uses: build-depends: containers &gt;= 0.3 &amp;&amp; &lt; 0.5 because they tested with containers 0.3.x and 0.4.x and there is no 0.5 yet. Now when 0.5 comes out the maintainer (or a nother hackage trustee) can check if the package does in fact build and work with containers-0.5, and if so then they can just tweak the .cabal file on hackage and it will then work. This does rely on some automation to work effectively: we need build bots to upload build reports on hackage, and we need a flag in cabal to optionally ignore upper bounds (of specific or all packages) so that build bots can try out packages with newer versions of their deps and report that to the maintainers / trustees who can then tweak the .cabal file deps.
&gt; I've written a compiler that uses the above techniques to turn Brainfuck code into MovDBz. The Haskell implementation is [available on Github](https://github.com/gergoerdi/brainfuck/blob/master/language-movdbz/src/Language/MovDBz/FromBrainfuck.hs)
Some more points in favor of the PVP can be found at https://www.joachim-breitner.de/blog/archives/615-Why-PVP-is-better-than-no-PVP.html
I see. I missed that part. Thanks.
&gt; I want to skip upper bounds when actively developing packages, but I certainly do want upper bounds to protect the builds on my build server. Assuming your build server is building executables that you plan to run in production, don't you actually want to fix the versions of all the libraries you use? When a new version of a library is released, don't you want to do more testing than "cabal says the version dependencies work out, ship it"? I don't understand what you're gaining from upper bounds in this case.
I don't find your argument compelling. Your reason #1 is mostly irrelevant because in my experience that's a rare case. If the library in question is maintained, then there will likely be a version of package-a that does build with containers 0.5.*. If there's not, then there's no guarantee that package-a will build with it. You seem to assume it will based on probabilities. But that's not good enough. Your reason #2 is simply wrong. You say you can't nail everything down, and then you construct an argument where you simply choose not to nail everything down. It's quite easy to nail everything down. In your example, don't upgrade to GHC 7.2. Sticking with GHC 7.0 is quite simple. Just stick with GHC 7.0. It's still available for download from the GHC homepage. And those who are concerned about building legacy code will probably have a dedicated build machine which will not be upgraded. PVP opponents are implicitly arguing that if code ever becomes unmaintained, then that means that it is unused. But that's not the case. I have code that I stopped maintaining years ago, but that I still make use of. PVP opponents would probably argue that if I'm still using it, I should maintain it. But that's a pretty large time commitment, and one that shouldn't have to be paid. Upper bounds are a simple and effective way to avoid that problem. By not using upper bounds, you're essentially mortgaging the value of the currently working ecosystem on the promise of future time and energy spent in maintenance.
I can only vouch for EoP, since that's the only one of the two I've read. It's an amazing book if you read it cover to cover, and will make you think differently about generic programming regardless of the paradigm. It is biased a bit toward imperative algorithms, but the core concepts are transferable. It is, in my opinion, one of the nicer examples of how programming really can seen as a mathematical exercise.
&gt; upper bounds are information, why throw it away? Because most people simply set upper bounds before any version of the dependency exists which exceeds them. 95% of the time, we are throwing away "information" that was just a bad conservative guess in the first place.
You just put a test in your package that fails wih containers-0.4
Why not have 'soft' and 'hard' upper bounds and let people decide what they prefer themselves? Soft upper bounds would be when we don't know if it will work past that bound, and hard upper bounds would be when we know it won't work past the bound.
My impression has been that the docs are never re-generated, so it shouldn't be a problem.
OK, that's a possibility, but it would be a new responsibility placed on package authors. And it might not always be easy to construct a test: perhaps containers-0.3 guarantees that a particular function has some property, that my package foo relies on, and containers-0.4 no longer makes this guarantee, but in practice the property holds for nearly all values of the input. What test do I write then? As a cop out I could write a test which just checks the version of containers directly, but wouldn't it be more straightforward if I could just specify that I need containers-0.3 directly in the .cabal file ... in the build-depends field, maybe ... ?
In my opinion, "Algebra of Programming" is really a book about understanding optimization algorithms like dynamic programming, greedy algorithms, divide-and-conquer etc. in a unified manner, guided by category theory. In other words, it is intended to be applied to problems like [linear paragraph formatting][1] or [counting word numbers][2], though the style is a lot more abstract in the book. This is extremely interesting stuff, but also a little niche. I can't say anything about Elements of programming, because I have never heard of this book. If you want a more down-to-earth version of "Algebra of Programming", I would recommend Richard Bird's [Pearls of functional algorithm design][3]. It covers different material, but Bird has a very mathematical/structured approach to programming that is definitely worth learning from. [3]: http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383 [1]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.7923 [2]: http://conway.rutgers.edu/~ccshan/wiki/blog/posts/WordNumbers1/
&gt; There would be no way I could use any version of astar with any version of snap, without getting the maintainer to upload a new version, or doing a non-maintainer upload myself, or forking the package, any of which involves a bunch of effort that solves no real problem. Obviously specifying upper bounds is not the perfect solution. The perfect solution would be to have build bots that constantly build all packages in every possible version combination and automatically set the version bounds to the maximum possible range that works. But that's not feasible. If astar didn't build right now, we would have no idea about the conditions where it used to work. If it had version bounds, we would have more information. In my mind, your argument is not an argument against version bounds, but rather an argument for better tooling. We could have a cabal flag that ignores upper bounds. That would be fairly straightforward to implement today and it would retroactively solve (or at the very least improve) your issue for all previous package versions. The same is not true of the lack of upper bounds. If no upper bounds had ever been written, we couldn't solve the resulting problems with a simple change to cabal. Upper bounds **add** information. In managing the complexity of this problem, we're going to need as much information as we can get. &gt; Your last sentence is a total non sequitur to me. How is it a non-sequitur? I have code that worked four years ago. It doesn't work today. If I had put version bounds in my package, then today I would know how to get it to build. It might not build with the current ecosystem, but there was never a guarantee that it would. Without upper bounds, you have to maintain your code to keep it building. With upper bounds, you will always be able to build it. 
Non Sequitur: Where the final part is unrelated to the first part or parts. An argument in which its conclusion does not follow from its premises. Regardless of if the conclusion is true or false, the argument is fallacious ^^Created ^^at ^^/r/RequestABot ^^If ^^you ^^dont ^^like ^^me, ^^simply ^^reply ^^leave ^^me ^^alone ^^fallacybot ^^, ^^youll ^^never ^^see ^^me ^^again
&gt; we are throwing away "information" that was just a bad conservative guess in the first place. No. Version bounds are a statement that package A works with a certain range of versions of package B. They are not (or at the very least, should not be) a statement that package A does not work with versions of package B outside those bounds.
Your update is better, but still not correct. &gt; As soon as you support the latest versions of your dependencies, upper bounds become harmless but also useless. They're not useless because old packages may still depend on that version of your package. &gt; But at the end, if you are diligent enough, it’s up to you whether to use upper bounds. My personal choice is still not to put them. I strongly disagree. If you are writing an application that will never see the light of day, then you're right. But if you are writing a library that you put on hackage for the consumption of others, then you need to use upper bounds. If you don't then packages that depend on yours can break spontaneously if one of your dependencies changes. &gt; So, my request to the library developers is: please support the latest versions of your dependencies. I do. [Religiously.](https://github.com/fpco/stackage/issues/117)
It seems that the problem is the inability to express the difference between "I don't know if my package works with containers-X" (because, e.g., containers-X doesn't exist yet) and "I know my package doesn't work with containers-X". If you could express both, then it'd be quite useful to have a flag to tell cabal to be optimistic about untested versions of dependencies or an automated tool to discover whether new versions are compatible.
Why do we need soft bounds? Couldn't we just store the date at which the package built successfully, and only use packages uploaded before that date when trying to install at a later data? Alternatively, we could automatically determine the upper bound in the same fashion. 
I'm not a cabal expert, so the following may have some significant flaws. However, it seems to me that these issues could be resolved with a relatively minor change to cabal and to the PVP. Suppose there there are two types of upper bounds, working and breaking. Then a library author of this-package-0.2.0 could specify in this-package.cabal bounds as other-package &gt;= 1.1 &amp;&amp; &lt; 1.3; 1.5 where 1.3 is the working upper bounds and 1.5 is the breaking upper bounds. This means that the libray author has tested his libray agaist versions 1.1 and 1.3 (and possibley 1.2) of other-package and determined that they are compatible with this-package-0.2.0. The library author (or perhaps someone else later) has also tested against other-package-1.5 and found that it breaks this-package-0.2.0 library. Suppose someone else wants to use use this-package-0.2. He could then specify this-package &gt;= 0.2 By default, cabal would be free to build with other-package &gt;= 1.1 &amp;&amp; &lt; 1.3. Optionally, there could be a command to cabal (--flexible-upper-bounds) in which case cabal could be free to build with other-package &gt;= 1.1 &amp;&amp; &lt; 1.5. Could something like that work? Does it solve some of the issues? (EDIT: I just noticed that tailcalled proposed something along the same lines.) 
Now I see your post, just after I posted mine. It seems to me that soft and hard bounds could work.
Haven't looked at the code yet but I am very excited about the project! Has anyone discussed adding contracts as per the SPJ papers?
Today I learned!
That's true, I hadn't thought of it that way. &gt; They are not (or at the very least, should not be) a statement that package A does not work with versions of package B outside those bounds. We should have a *different* way, then, to specify "hard" bounds, where versions outside are known not to work, and "soft" bounds, where versions inside are known to work. That which falls outside the soft bounds but inside the hard bounds is simply "not vetted" but functions correctly in most cases. So I suppose what I'm saying is that the fact that we conflate hard and soft bounds into the same field makes that information less useful, and practically useless if you are trying to stay on the cutting edge of new library releases.
I'm not sure what the right solution is. Having the notion of hard and soft bounds may be a good idea. It may just be sufficient to have a switch that can tell cabal to ignore upper bounds. &gt; practically useless if you are trying to stay on the cutting edge of new library releases. It's not useless if you care about stability for your end users, because without them your package and anything that depends on you may break at any time because of changes in libraries you depend on. I work with lots of Haskell deployed in production, and that's simply not ok. Specifying upper version bounds on all your dependencies in effect makes your package "pure". It insulates you from side effects that can occur through the actions of others. This is a property of Haskell that we like, and I think it's a property that our packages should also have.
So, I feel like this is the *right* argument, and from my understanding of the PVP (or mostly faith in People Who Have Thought Harder About This And See More Context Than Me) tend to think that adhering to the PVP is the *right* thing to do… In practice however, I have found that trying to keep up with the PVP for the handful of packages I maintain on and off to be “too much”, and knowing that I'm only handing pain over to the rest of the community, tend to drop the upper bounds for the sake of guilty guilty convenience :-/ (My only saving grace is not having packages popular enough that this makes a big impact…)
I suspect the people advocating no upper bounds are newer to the Haskell community. I remember back when we didn't have upper bounds and, despite all the problems with upper bounds, not having them was ten times worse. Essentially nothing would build ever. At least with upper bounds things sometimes build. The only way forward I see is to actually **fully** specify imports (and that goes way beyond Haskell's existing import mechanism) and then check import signatures against export signatures.
No - he emailed it to me directly. You would have to ask him for a copy.
As others have said, AoP is really about algorithms (as *influenced* by abstractions). Only a very small percentage, maybe as low as 10%, of EoP's abstractions are 'covered' by AoP. EoP is **not** object-oriented at all. It covers abstractions, as built on top of an *imperative* base.
Here's another one—suppose that the author of package foo wrote that conservative build-depends line above, and when the build bots get around to testing foo with containers-0.5, by some minor miracle the package foo really uses some part of the API that changed in containers-0.5, so the build fails. Great, the dependency is correct. Now what? If you don't record that result anywhere, then the next day, everything is the same and so the build bot will try to build with containers-0.5 again... so that seems to leave two options: either store it in some ad-hoc database on hackage, or extend the cabal file format to have two kinds of upper bound.
Presumably if you know that other-package-1.5 breaks your code, then you also have versions 1.3 and 1.4 available so you can test whether they work, and update your cabal file accordingly. (Though I guess that might not be true since versions of other-package might not be released in any particular order...)
When the author first writes his library, only other-package 1.1 exist, so his original cabal file has other-package &gt;= 1.1 &amp;&amp; &lt; 1.2 Later, other-package-1.2 is released so he tests his package against that, it passes, and he updates his cabal to other-package &gt;= 1.1 &amp;&amp; &lt; 1.3 This author drifts way and doesn't maintain this-package anymore. Later, someone else is building some other library or app that users this-package, and discovers this-package no longer builds with other-package-1.5, so he changes this-package.cabal to other-package &gt;=1.1 &amp;&amp; &lt; 1.3;1.5 If someone else happens to try this-package with other-package-1.4, then he/she can update this-package.cabal accordingly. It takes a village to update cabals. (Edit: Actually, a better syntax might be: other-package &gt;= 1.1 &amp;&amp; &lt;= 1.2 &lt; 1.5 ) 
Ok, I see your point. It just seems like this is a rather rare case and that in that case you're probably going to test until you find a working bound which means that just a hard bound will be sufficient.
Ok, that makes sense. Knowledge of category theory can be beneficial to Haskell. Am I right in saying that AoP covers it more (I remember reading about it in AoP) than EoP? Which order did you read the books? And are you happy with that order? PS: Ultimately I will read both, but I will have some 'off-time' to study one with less interruptions.
I do use C++ for my day job, but I could potentially use other languages. I would like to have good scientific/mathematical foundations, which I think EoP has a lot to contribute to, even if its examples are based on C++. So what is your rationale in saying that?
For production builds, users of libraries would use the more conservative soft (or what I called working) upper bounds. For users of libraries concerned more with flexibility, they would use hard (or breaking) upper bounds. Conscientious library authors would test every build of every library that they depend on and constantly update both working and breaking upper bounds. Less conscientious library authors would not test every build of every library, and hope that the community at least sets breaking upper bounds when they run into them. 
It would've been a completely natural transformation (Not to be confused with the categorical concept. Actually, maybe...) if it wasn't for a single problem that I just noticed.
Not every error is a type error, so I think the second of your options might be an idea, but with a possibility for manual override of upper bound.
&gt; You describe the exact situation in which upper bounds (would) cause me unnecessary pain. That's because you picked this example ex post to support your point. I can point to tons of other examples where your policy would have caused astar and all of its downstream dependencies to break spontaneously. I would much rather have the code I write today always work than sacrifice that to avoid a little maintenance burden. If astar had upper bounds, it would be trivial for me today to download it, modify the bounds locally, and see if it builds. In fact I have actually done this a number of times with a number of libraries. This is nothing compared to what I would have to go through to make it work if astar did not work today because of its lack of upper bounds. Why? Because upper bounds are information. &gt; Is this code a library, or an executable? An executable. For libraries, the argument is different. You want upper bounds there so all your users don't break when one of your upstream dependencies changes. &gt; Much more reliable and simpler would have been to specify exact version requirements on all the dependencies of your application. There is no cost in flexibility, since no one is going to depend on your package anyways (it is an executable). But that would have caused me more pain back during the time that I was actively developing the application. So yes, there is a cost in flexibility. &gt; By using upper bounds, you rely on the maintainers of all dependencies of your application to follow the PVP Of course. That's the nature of reusing code. This is why we have the PVP. We could protect against these problems by modifying hackage to not allow you to upload anything that has a public API change without a major version bump. In fact, I have a strong aversion to depending on packages in my code of those packages don't have upper version bounds. Enforcing that strictly across the board for all my projects is not feasible, but I really wish I could. My observation has been that packages with upper bounds are generally more stable than packages without them.
Suppose Trunc is a type constructor that overrides the Eq instance with one thst always returns true. In that case, the functor laws are broken for MkTrunc and getTrunc, since mapping with MkTrunc will collapse the Set to a singleton Set.
&gt; But until a package breaks, how do you know the meaning of a single upper bound? You know that it means that the package will build of the bound is satisfied. If we used my &lt;! suggestion, then a single &lt;! upper bound would mean that we know the package breaks if the bound is violated, and a single &lt; upper bound would mean that we don't know and it could possibly be relaxed in the future. That would be useful if we had a flag to tell cabal to ignore upper bounds. It would allow cabal to know when the bounds cannot be ignored.
All I can say is that my experience is diametrically opposite to yours, and leave it at that. Fortunately, the ability to edit package dependencies on Hackage 2 should make the issue largely moot.
Can't you just parametrize the `ParseTree` with a type variable, which you can then instantiate with `Pos`. If your `ParseTree` has kind `* -&gt; *` you can just derive `Functor`, `Foldable` and `Traversable` for it, which gives you a lot of power for free.
Actually this paper http://research.microsoft.com/en-us/um/people/simonpj/papers/financial-contracts/contracts-icfp.htm was an inspiration for me at the beginning of the Nomyx adventure. However now the Nomyx language is quite distant from what's in the paper. How do you think a "contract" could be included in the game?
&gt; If your ParseTree has kind * -&gt; * you can just derive Functor, Foldable and Traversable for it, which gives you a lot of power for free. That sounds good. How do I find out the kind of my ParseTree? I know I can find the type of anything by just doing ":t whatever" in ghci. But how do I find the kind? I'm only vaguely familiar with what a kind is. Something like a higher order type? Like kinds are to types what types are to variables?
OK, I understand your proposal now and agree that it solves the fundamental problem. You propose specifying either a soft or a hard upper bound. I think specifying optionally soft and/or hard upper bounds provides a little more flexibility, but don't know if the practical benefits would be worth the extra complication/work.
There is almost no category theory in EoP. Might be none. I believe I read them interleaved. Unhelpfully, I was quite happy with that 'order'!
Not sure. But it would seem to foster cooperation between players beyond one or two turns. Contracts would allow players to create resources (obligations) and administer them. Interesting about the paper being an inspiration. Have you discovered a set of nomyx combinators? Is there a mapping between SMP things like observables and things in nomyx? Just curious.
You can do ":k whatever" or ":kind whatever" to get the kind of something. Kinds are to types what types are to values, so kind `*` just means some type and kind `(* -&gt; *)` means given a type it will produce a type. Basically your `ParseTree` type has kind `*` because it doesn't take any type parameters, but if you changed it to `data ParseTree a = ...` then it will have kind `(* -&gt; *)` because it needs some type for `a` to complete the type. At that point you could give it a type like `Int`, making `ParseTree Int`, which has kind `*` for the same reason `succ` has type `(Int -&gt; Int)` and `(succ 3)` has type `Int`. (note that `succ` doesn't actually have the type `(Int -&gt; Int)`, but let's pretend for simplicity's sake).
Yeah, it might be a little more flexible, but seeing two numbers there makes me do a double take to figure out what they mean. &lt; vs &lt;! seems pretty clear.
All the philosophy and cabal/PVP design concerns aside, in practice, I've noticed that upper bounds (implemented as they are today) hurt more than they help. For context, I develop 10s of projects (read: individual cabal packages), from small libs to large apps, simultaneously, iteratively and pretty actively. I've noticed that once a dependency somewhat matures, it is more likely that a version bump will NOT change the API than it will. There is always some pain in the process but I get really annoyed when a currently unnecessary upper bound blocks my workflow. I often see strict followers of PVP specify upper bounds on containers, bytestring and several other very foundational dependencies. Every time, it leaves me thinking "Why the heck would you force 'bytestring &lt; 0.11' BEFORE you know your package breaks with 0.11??" Perhaps my mind works a bit more bayesian: The chances of a future bytestring version introducing an API or functionality change that will break my package, in my mind, are extremely low. Perhaps there is a point in there about maturity of dependencies: Specify upper bounds when you're hazy on whether the dep will maintain API consistency and leave them out when you feel the dep is fairly mature and unlikely to introduce major API changes. For some reason, I am always less annoyed by an API change causing issues due to a lack of upper bounds. Maybe it's because I see that as a more legitimate reason to experience pain, or perhaps it's because the error messages are more obvious as to who the offender is and I can just specify a constraint to get around it for now. I hate it when my workflow gets blocked and I am forced to waste time as a result of a precautionary upper-bound and not because of a legitimate change in the underlying code. Edit: It is possible I think this way because my main focus is on application development. I do maintain several libraries, but I do so because they constitute reusable pieces across our applications. My evaluation of the pain caused by upper bounds is based on dependency hell in applications with a very large dependency chain. As an example, a quick look into one such case shows a total of 282 dependencies for a single application.
Consider what happens when bytestring-0.11 is released and is incompatible with a libraryA-1.0. Version 1.1 is released with a "bytestring &lt; 0.11" constraint, but there is nothing that can be done about the already uploaded version 1.0. Now somebody comes along who wants to install libraryB which depends on both libraryA and bytestring. It seems to me that in this case libraryA-1.0 might be selected by cabal even though 1.1 is available. The reasoning being that libraryA-1.0 erroneously claims that bytestring-0.11 is okay. [edit]: another comment expressing the same: http://www.reddit.com/r/haskell/comments/1ns193/why_pvp_doesnt_work/cclivtw
Do you think having a soft and/or hard upper bound, as discussed in a couple of other sub-threads would help?
It also helps that we can build arbitrary initial F-algebras, instead of being restricted to strictly positive functors.
&gt; Since I am planning to use this in parallel with my Haskell learning, it would be nice if its relevant and supportive of concepts in Haskell. Category theory you mentioned there is one of these important concepts. In that case, I would really recommend to read "Pearls of functional algorithm design" instead. It teaches some great design lessons in Haskell. In contrast, category theory is not nearly as important for Haskell as it may seem. 
It was a mistake to make packages extra-language concepts. This is a type checking problem, pure and simple.
If you manage to always be up-to-date with your dependencies, you can as well write the supported upper bounds. This is a good information and can be used by tools to report that you are not up-to-date.
Jane Street invested a lot in that over a long period of time. They have a learning curve for new employees, but it's worth it for them. Not every company is in that situation.
Noted, but I don't have that book yet. Need to decide between the two although may well decide to try and take both :P.
I intend to read it. Bought it after many years of C++ programming and wanting to put more foundations and structure to my work. They both have credit in my view and they will both be read. Question is if one is to be read before the other, which is which?
As an application developer, I need package versions that are stable. I need to support released versions of my software for some time into the future. People who never put upper bounds on their dependencies make that nearly impossible to achieve - using the same bayesian logic mentioned elsewhere in this thread, it is almost certain that any version of any package with a significant amount of dependencies will soon fail to build, even if it builds successfully today. Roman's post presents an extreme point of view, claiming in effect that packages with upper bounds on their dependencies poison the Haskell ecosystem by making it difficult to move forward. Taking this other point of view to a similar extreme, it is the packages that ignore PVP and never specify upper bounds that poison the ecosystem, making it difficult to create stability. I think these two extreme positions are the reason why, as Roman says, this topic tends to be contentious. In my opinion, the real answer is that we need to be considerate to both groups of people. While developing new packages or new feature sets for an existing package: leave off the upper bounds, use Stackage, prefer Hackage packages that do the same. But every once in a while, take stock of what you have done. Pick a recent version of your package that seems usable, and upload a stable version with reasonable upper bounds for all dependencies. Perhaps we should modify PVP to reflect these two needs. We could adopt the convention that version numbers ending in ".0" are recommended to have upper bounds on most of their dependencies (without any claims about stability in any other sense). Developers are encouraged to upload a ".0" version regularly, say, at least once every month or two for packages with frequent uploads. That way, people who want to depend on a flexible version can use bounds of the form "&gt; n.0" and "&lt; n.0", and people who want the more stable versions can use "&gt;= n.0" and "&lt;= n.0". It's obviously not perfect, but I think it would help considerably.
It _is_ a lot more work to maintain the upper bounds, so I can empathize. When I maintained proper PVP bounds on every package it was a nightmare. I'd ship one update to a package that strictly broke PVP and then have to update literally 25-30 packages due to ripple effects. Worse, to get cabal to do all the upgrades and not trap users in awkward situations that would require --force to fix them, I'd just keep going and update 60+ packages that had transitive dependencies on the thing I changed. One release of a low-level package used to mean 2+ hours of patch/release storms flooding the #haskell IRC channel via hackagebot. I tried ripping them off after Snoyman did an anti-upper-bound rant. It helped for a while, but over time I ran into problems where users would more easily get trapped in cabal hell, with versions of packages installed that didn't work together that they had to use --force to get out of, or get them to prod library authors to fix previously orphaned instances to proceed. Removing upper bounds entirely means that you are omniscient enough to know every instance that may move into one of your dependencies or that you are willing to say the burden to detect such things should move onto the user. I do agree that fixing up the problem _is_ less work than updating in response to every dependency of yours releasing a new version, so this may well be a viable way to proceed, but it isn't a free lunch. On the flip side you wind up with users with difficult to reproduce build issues, and much trickier support questions. The compromise I eventually reached works for me, but not perhaps for everyone else. The PVP specifies that the first two digits of your package number are the 'major' version, and the next digit is the minor version. For dependencies on my own packages I use only the first digit of the major number. For dependencies on other people's packages (admittedly I cheat on base here and there) I follow the normal PVP process. This is effectively the same as removing upper bounds on 'self-dependencies'. Since much of my code is rather solipsistic, this works pretty well for me. It reduced the amount of boilerplate spam I had to hit `hackage` with by about a factor of 10. And has saved me literally weeks of boilerplate over the last year or so that I've adopted it. Again this doesn't apply to everyone, since not everyone as sort of a large self-contained web of them as I do. The cost is that occasionally a user has to use --force to get up to date. The utility and safety of --force is rather greatly improved by the existence of stackage these days ensuring that the majority of packages work together using their most recently released versions. If you do insist on removing upper bounds, please get your package added to stackage that way build failures can be detected early and reported to you. In theory hackage 2 has some machinery for supporting updating package version bounds without re-uploading. It is currently disabled and there is some concern about reproducability of builds, but it may provide a lighter weight way to deal with these updates in the future. 
It isn't so much about less conscientious as not having impossible tinderbox resources that can test all the permutations of the dependencies. Testing all of the versions of all of your dependencies is actually a massive search space. Since we often use CPP in the packages to make conditional code for different version ranges, there isn't any good way to test ranges of dependencies, you have to do it all pointwise.
That compromise might work if followed universally. In any case, I highly appreciate your efforts to include the upper bounds on third-party dependencies. I recognize that it takes extra work to maintain upper bounds. But creating builds of older versions of packages and applications is currently a *huge* amount of work - a nightmare every time. And almost all of that is caused by packages that omit the upper bounds on dependencies.
Thanks! That is much appreciated. Not only for the links to the older tutorials, but more importantly, so that older versions of other people's packages and software can still be built. Your packages are the basis of an ecosystem of basic infrastructure, so that's really important.
&gt; it's not clear when it's going to happen Quoting from nomeata in that issue: &gt;&gt; Maybe that is the reason why this bug has not been tackled yet: A difficult and large UI design space. But there does seem to be some convergence now. Hope it will get in soon!
You're welcome! I agree that it is nice to not worry about `cabal` builds failing. One of my policies is to not release packages on Hackage until I think they are going to be low maintenance (i.e. mostly feature complete and bug free) so that I can be a responsive maintainer and keep upper bounds up to date.
Yeah, apparently this warning has been added "recently", most likely in the 7.6 series. 
Excellent point. I can only imagine the amount of work you must do to keep your packages building for you and for others. Thanks for that. Do you think having soft and/or hard upper bounds might help the situation?
Maybe "hard bounds" should be specified negatively (as version ranges to avoid) rather than positively. I'm thinking of things like `hashable` where many people blacklist specific versions.
In practice there are also a lot of "mushy" bounds out there - where the package author doesn't even know whether versions out that far will work, but estimates (or hopes) that they probably will. Even "mushy" bounds do contain some information, though less than "hard" or "soft" bounds.
Well, it would alleviate/obviate my original problem and I would then be OK with specifying upper bounds. I'm not against the "it has been tested and known to work with bytestring &lt; 0.11" information capture; what annoys me is "it can only work with bytestring &lt; 0.11" interpretation that often is not true. 
The documentation for `Data.Time` mentions compliance with ISO 8601 in other places. This behavior is also consistent with that standard, but it is not mentioned explicitly in the documentation. I think it ought to be.
Ah, in that case I don't know. Maybe do them in parallel?
What lynch mob? lol p.s. I like your randR function
Lynch mob here. We're proud of you! You're new to Haskell yet you still wrote something useful, interesting, and you were brave enough to share it with the rest of us! I think the reason you're seeing some weirdness (the known bug in the README.md), is because you're missing a `+ epsilon` somewhere. I started looking for places where you return 0 directly because I suspect that might be the place. b2 = if (b &lt; 0 || m2 /= UPMISS) then 0 else b h2 = h * 0.2 p = if b &gt; 0 then (vdot l r) ** 99 else 0 I suspect that `b2` or `p` needs to have an epsilon. I don't know if this is the bug you're seeing, but often in raytracers there are a few places where numerical inaccuracy ends up determining what happens but you can make the calculation not depend on that by adding a small value such as 0.0001. My experience is that it's most likely to be an issue when you calculate the color (or shadow) at a surface collision.
http://www.reddit.com/r/haskell/comments/ti5il/redblack_trees_in_haskell_using_gadts_existential/
Just by reading the index of Elements of Programming it looks like a lot of its content are overcomplicated discussions of elementary mathematical objects, like rings, groupoids, orders, lattices and so on. To learn about them you will probably be better of reading a pure mathematical exposition and then think about how one would implement them in C++ some other programming language.
&gt; Something like `insert :: forall b c. exists b' c'. a -&gt; RedBlackTree b c a -&gt; RedBlackTree b' c' a` Yeah, I'd say something like that. In particular, you may want to consider: data AnyRedBlackTree a where ARBT :: forall b c. RedBlackTree b c a insert :: a -&gt; AnyRedBlackTree a -&gt; AnyRedBlackTree a
&gt; insert :: a -&gt; AnyRedBlackTree a -&gt; AnyRedBlackTree a Yes, I guess that would work. I will try that.
Haha, thanks. I'll try to get both but If I can't I'll be getting AoP, as it seems to cover more theory I need in general and more related to Haskell &amp; algebraic programming. Thanks again carette.
Reading in parallel is probably doesn't have advantages. Carette did that.
See [Stephanie Weirich's](http://www.seas.upenn.edu/~sweirich/) * slides ([pdf](http://www.seas.upenn.edu/~sweirich/talks/flops2012.pdf)) page 33 * [course/code](http://www.seas.upenn.edu/~cis552/12fa/schedule.html) - scroll down to "RedBlack[1|2|3]" 