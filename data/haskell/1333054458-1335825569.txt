do you mean reddit does not form a category ?
Now I have a monoid in the category of problems.
Good news, everyone!
Right, but it could have used a wrapper type (a la `Sum` or `Product`): newtype Sequence m = Sequence (m ()) instance Monad m =&gt; Monoid (Sequence m) where mempty = Sequence $ return () mappend (Sequence m) (Sequence m') = Sequence (m &gt;&gt; m')
Indeed. I would be happy to see such a wrapper in `Data.Monoid` (although I'd prefer a better name than `Sequence`). I think it isn't already there simply because nobody wanted it strongly enough to ask; I doubt it comes up all that often.
Go on :)
For the moment, I share illissius's concerns that the extra complexity here is too steep a cost to pay. We now have Pipe acting nice and obeying simple properties... except that you can't use it at all, because if you want good behavior with resources, then you have to use Frame instead, which has none of those benefits. You can understand Frame in terms of Pipe... but it's probably not recommended for mere mortals. Perhaps I don't understand why it's not possible to handle this question by modifying the Pipe type in a way that still preserves the laws we care about. And I definitely don't understand why you would be happy about things like "This requires no modification to the pipe type" when in fact you're just making Pipe obsolete instead. In the end, it's not going to matter how unmodified Pipe is if people just use Frame instead, because the prospect of mixing Pipe and Frame when the relationship is hard to even parse on sight is too scary. I'll try to take a closer look some time, though... is this on github or something so that it's possible to generate API documentation or see what is exposed and what is hidden implementation detail?
I concur completely — IO finalisation may be a "narrow problem domain", but it's also a completely *ubiquitous* problem when handling streaming data, and nobody is going to want to use `Pipe` for the few cases they can avoid it if it means they have to use an adapter to compose that with the `Frame`-based majority.
I'll put it on github later. For now, the exposed parts are: * `(&lt;-&lt;)` * `yieldH` * `catchH` * `Frame` * `produce` The rest is just an implementation detail. So, for example, if you were to write a logger, you would use: log :: FilePath -&gt; Frame IO String String log path = do h &lt;- lift $ openFile ReadMode path catchH (hClose h) $ forever $ do x &lt;- await lift $ hPutStr h x yieldH x Then if downstream downgraded to a producer, the logger would automatically be finalized appropriately. I think the above example illustrates that the exposed API is quite minimal. After all, you have to have some sort of a catch function. The only real "noise" is the `yieldH` instead of `yield`. Does this answer your question?
It's amazing what impression the layout of a website can do. Having little actual knowledge of the frameworks, I used to think of Happstack as the old, worse alternative to Yesod and Snap. But with this announcement and the slick new look their website has, I am now just as interested in Happstack as the alternatives.
I have to agree with this. I think Gabriel's analysis is very useful, and basing everything on the existing composition has the advantage of simplifying the reasoning and the proofs, but I believe a usable library should not expose more than one abstraction, especially if one subsumes the other completely. I'm also really skeptical about the infinite chain of downgraded Producers. Being able to return a value from any point in a pipeline is quite a useful feature, and dropping it in exchange for this finalization mechanism doesn't look like a good trade-off to me. Meanwhile, I have been working on the new version of pipes-core, which will be a category (I'll publish the proof). My mistake, as Gabriel helped me understand, was to try to add the 'ensure' primitive, which is probably too powerful to fit into the category structure. My new approach is inspired by Gabriel's finalizer-passing mechanism, but baked in the Pipe type, and extended to better deal with asynchronous exceptions. The high-level API is the same as before, and the semantics differs only in certain corner cases.
From the comments, I think one thing that's not clear from the post is what the end user has to know. The only things you need to use are: * `(&lt;-&lt;)` * `yieldH` * `catchH` * `produce` * `Frame` * `runPipe'` And here's an example of these in use: log :: (Show a) =&gt; FilePath -&gt; Frame IO a a log path = do h &lt;- lift $ openFile ReadMode path catchH (hClose h) $ forever $ do x &lt;- await lift $ hPrint h x yieldH x The above pipe logs everything that passes through it to a file. If downstream downgrades to a 'Producer', then it is automatically finalized (i.e. the handle is closed). Here's an example of how you would write the producer and consumer end: print10 :: (Monad m) =&gt; Frame m a () print10 = do replicateM 10 $ await &gt;&gt;= lift . yield produce $ forever $ yield () naturals :: (Monad m) =&gt; Frame m a Integer naturals = go 0 where go n = yieldH n &gt;&gt; go (n + 1) We can combine them with frame composition: runPipe' $ print10 &lt;-&lt; log "log.txt" &lt;-&lt; naturals The console output would be the first 10 natural numbers, which would also get logged to "log.txt". The moment `print10` yields a `()`, the logfile handle is automatically finalized. The issue of mixing pipes and frames has to wait until exception/error handling, which addresses this issue.
Alright, so if that's the extent of the exposed interface, then you've certainly got something simpler than the blog post made it appear. Basically, despite the round-about route via pipes to get there, you've implemented something very much more like conduits (but without the leftovers)... and if the composition is categorically sound, that's potentially a nice point in the design space. It's a different point in the design space than pipes was, though. In particular, you've lost the symmetric termination stuff from pipes, so there's a loss of composability in that respect. You might get other things in return. Here's one possible other thing you might get. For this new type, I'll now repeat a question I asked you a couple months, which is that if `Frame m` is a `Category` and `Frame m a` is a `Monad` for all `a`, then it's worth asking if `Frame m` is an `Arrow` with `arr f = liftM f idP` and `first p = ap (liftM (,) (p &lt;-&lt; arr fst)) (arr snd)`. Any thoughts on that? For people who like `Arrow` (which is not me), it could be quite interesting and open the door to a nice programming style. Also, I'd expect it to hold, so if it doesn't, then it would be interesting to know why expected behavior doesn't hold. I think you should be able to expect an `ArrowApply` instance as well. (*Edit:* Removed my last question, because I realized you meant `lift . print` and the yield was a typo.) 
I definitely look forward to seeing your new pipes-core now. It looks like the cleaned up version of Gabriel's work does give a nicer abstraction, but it's a very *different* abstraction from the original pipes, and apparently (from looking at Sjoerd's comment) the same one as conduits. If the original pipes turns into an implementation technique for conduits with proofs of nice properties, that's great... but I'm also interested in where the straight pipes approach goes.
I bet I'd like this a lot more if I knew anything about System F :D
Upvoted for "typesafe DHH". 
Anyone got a link to the rendered document?
Just because someone talks about something a lot, doesn't necessarily make it better.
Have a look at my example code too [on github](https://github.com/noteed/snaplet-auth-example).
Yeah, once I read past the title that was clear. But this is an overloaded term in GUI programming and at first I skipped over the post in my RSS reader based on the title. I don't know what other terminology you could use, but even just going with "Bidirectional data flow widgets" or something would probably work.
You shouldn't be thinking about your code operationally ("What is it doing at this point in time"). Think about it statically instead ("What is true about this data?")
&gt;and apparently (from looking at Sjoerd's comment) the same one as conduits. No, that comment just rephrases the `Frame` type in the same style as pipes-based conduits. conduit is still very different (it probably doesn't satisfy the category laws at all, IIRC).
How do you implement a fold with `Frame`? It has no result type.
Ha. In this case 'more secure' was intended to refer to the fact that it now comes with built-in https:// (aka SSL) support. 
Yeah, I figured they likely meant "support for new secure functionality"
Correct, but it helps increase adoption, and in the digital data world and networked world, that drives economies of scale in various dimensions. 
The part I am not sure about is "Frame m a" being a monad. I don't have a computer at the moment, so I have a hard time experimenting with ideas for what return and (&gt;=&gt;) should be. Paolo and I spent a lot of time on studying monoidal and premonoidal categories over pipes, but while you can get some stuff to work with Either instead of (,), we couldn't get first and second to commute.
Yeah, that sounds like a dead end, then... which is interesting, because I think it was still an open question if there were any *reasonable* examples of Category and Applicative related in that way that didn't form an Arrow as well.
I was one such person - thanks! I'm really looking forward to reading this!
Congrats. I'm excited to dig in and read it.
That's awesome. Keep up the good work!
also great! thanks!
Chris Done started one [here](http://www.haskell.org/haskellwiki/Web/Comparison_of_Happstack%2C_Snap_and_Yesod) though it's incomplete.
...you should really have noscript tags, at least *somewhere*.
Congratulations on finishing your thesis!
Yep, I have them in my personal copy of the compiler. I have not changed the one online in a while. Don't worry :)
My pleasure! I hope you find it interesting, and I would love feedback if you have concerns, ideas, etc. :) I am new to academic/technical writing, so anything helps!
Thank you! My brain needs some time off, but I'll be working on the compiler again soon :)
You can read Types and Programming Languages chapter 23 for an explanation of System F (figure 23-1 gives the syntax, evaluation rules, and typing rules). The wikipedia ariticle looks like a decent explanation, too, if you don't have TAPL nearby. http://en.wikipedia.org/wiki/System_F I give a partial explanation in the "terms" section of my blog post. System F terms are either: a variable, a type variable, a function abstraction (introduces a variable into scope), a type abstraction (introduces a type variable into scope), and corresponding function and type *applications*, which apply a specific value or type to a function or type abstraction, respectively. You can think of it as a beefed up typed lambda calculus, or as a dumbed down verbosified Haskell. However, since I use Higher-Order Abstract Syntax (HOAS), my abstract syntax tree for System F does not directly contain any notion of variables and type variables. Instead, it makes use of Haskell functions to deal with function and type abstractions.
Yeah. That was operator error. Someone was experimenting with the method we use to deploy the live server and it did not go so well. In happstack 9 we are going to investigate automatic deployment methods because it is a real problem. And it will only get worse when people start trying to deploy clusters instead of single servers.
Oops, meant to reply to someone. This post is an accident.
Yes, that is correct. I did not dynamic switching give too much focus in my thesis. I think dynamic switching only makes sense when discussing signal functions, which are not the primary abstraction used by Elm. And although the theory appears to work out, I have not yet included AFRP in Elm. Elm is still very young and there are lots of features to add, so AFRP is in the future. tl;dr: no.
Linux (any version, I just bootstrap GHC 7.4 from the latest one in the distro), emacs, GHC, and Yesod. Never done any GUI programming in it, so I have no opinions there.
What distro? (Linux)
Mac OS X, MacVim and Sublime Text 2, GHC 7.4.1, Snap. I still have to experiment with GUI toolkits. Something Cocoa-ish would be my preference, unfortunately, HOC development seems to be frozen. I played with QtHaskell, bit didn't really like its API. I'll try Gtk2Hs next, which seems to be actively maintained.
Guess I'll be the black sheep here: Windows Vista, EclipseFP, GHC 7.0.4 (you're kind of stuck with haskell platform on windows) and some Yesod. Haven't tried GUI toolkits yet, but will look into GTK2hs.
OS X(Lion), EclipseFP, ghc 7.0.4. Haven't used any web or GUI frameworks yet.
emacs, emacs, ghc-7.4, yesod, no gui. also repping ghc-mod - makes working with haskell in emacs (and presumably vim) an absolute dream. (ok, fine, my OS is ubuntu, but some jokes never get old)
Gentoo Linux, Emacs, ghc 7.4.1, haskell-mode + ghc-mode
Linux (Ubuntu) and OS X; Vim (no hs-mode); GHC (Haskell Platform); I have only done command-line stuff (no servers or GUI apps).
Mac OS X, TextMate 2, GHC. I don't write web or gui applications.
 * Ubuntu LTS * gedit * GHC 7.0.4 with the latest Haskell Platform * I'm coding directly on top of WAI for now, haven't really explored the web frameworks yet * haven't tried doing any GUI coding in Haskell until this weekend; I'm trying to get reactive-banana-wx to build atm and will experiment with that
Gentoo, ghc-latest, vim, xmonad and two (sometimes more) konsoles, zsh.
Haha, I haven't seen that! I nailed it though :P
Ah, right, the issue has always been there for iteratees as well. Hm, I do think that there is something more to this story, but I can't quite put my finger on it. In any case, the way you handle it is ok, I think.
Hi, has anyone tried to install it on osx platform, 64 bit or 32 bit eclipse? what is best config for osx ? thanks.
Vista... GET HIM!
Debian GNU/Linux, Vim, GHC 7.0, I don't use any frameworks for the web and Qt is the only GUI framework I have experience with.
Arch Linux, ghc 7.4, vim inside urxvt.
To illustrate how the `partial` function changes semantics: &gt; isJust $ listToMaybe [undefined] True &gt; isJust $ partial (head [undefined]) False &gt; let loop = loop &gt; listToMaybe loop ^CInterrupted. &gt; partial (head loop) ^CNothing 
arch linux, emacs, ghc 7.4
Ubuntu 11.10 with Gedit and whatever haskell-platform is in the Ubuntu repositories at the time. I'm not interested in web programming. Tried GUI programming in Haskell once with GTK2hs and gave up as a lost cause, deciding to use Java for the GUI and Haskell for the logic.
great, thanks. At last gtk working. installed 64 bit eclipse ee. seems like the syntax colouring is not working. 
No, but I would like to know.
FreeBSD 9, xmonad+xmobar, gvim and gona try EclipseFP some time. I liked snap, but haven't tried the other ones. And didnt really use GUI frameworks so far either.
I don't understand any of this but Google found: http://permalink.gmane.org/gmane.science.mathematics.categories/1194
 Tools { os = MacOSX (Lion) , editor = Leksah &lt;|&gt; SubEthaEdit , compiler = GHC7.0.4 `via` HaskellPlatform , webFramework = (WAI `on` Warp) &lt;|&gt; Snap , guiFramework = mempty } *reformatted to avoid EOF on devices that fail to scroll to the next sector*
Ubuntu Linux, vim, ghc 7.0.4. The closest I come to web/GUI in haskell is using vty for curses interfaces.
Ubuntu for now, but I'm considering switching to Debian.
Windows XP, Notepad++ or Leksah, GHC 7.4.1. I write mainly toy programs, so no opinions on web/GUI frameworks (if pressed, I would say Yesod and GTK2hs)
Xubuntu, Emacs, GHC, Snap, Gtk.
Haskell support is excellent. Anything in particular you're looking for?
Windows 7, EclipseFP, GHC 7.0.4. A test machine with XP and GHC 7.4.x. For web dev, Yesod. For GUI, nothing much yet, though I've played with integrating Haskell + Java + SWT. One day maybe I'll try to bake reactive-banana into that crazy mix.
EclipseFP has been tested on OSX, so it should work. Ensure buildwrapper executable has been built and is referenced properly in the preferences. Turn on debug messages and check. Post errors/logs/issues on the [sourceforge forum](http://sourceforge.net/projects/eclipsefp/forums/forum/371922).
The issue isn't for indices that are out of bounds but accessing array elements that weren't assigned, say array (0, 10) [(3, 4)] ! 5 throws an exception.
Vim + [ghcmod-vim]( https://github.com/eagletmt/ghcmod-vim) + [Syntastic](https://github.com/scrooloose/syntastic) is great for Haskell. I also have updated syntax/indent files, but I don't remember where I got them. 
Hasn't he already suffered enough?
"Haskell is based on a restricted version of System F" isn't quite true, but is nevertheless truthy. Of particular note, GHC makes use of an intermediate language called (as of 7.4) System FC-pro. See [Giving Haskell a Promotion](http://research.microsoft.com/en-us/people/dimitris/fc-kind-poly.pdf) for details.
&gt; Is it terrible? No, though I wouldn't suggest heavy reliance on such techniques. &gt; Does it already exist? Yes, though it is not commonly used.
I would recommend a different technique that doesn't require exceptions and unsafePerformIO. This requires that you are able to construct a function that can determine whether or not the function is defined for a given input: partial :: (a -&gt; b) -&gt; (a -&gt; Bool) -&gt; a -&gt; Maybe b partial f p x | p x = Just $ f x | otherwise = Nothing safeHead :: [a] -&gt; Maybe a safeHead = partial head (not . null) Example usage: http://ideone.com/pF44q
There are a lot of problems with `pureCatch`, but my attempt to construct an example exposed a little misunderstanding in the semantics of it, which would probably trip up others too: data AB = A | B deriving (Show,Typeable) instance Exception AB ugly n = pureCatch (throw A, throw B) handle where handle A = ( n, n) handle B = (succ n, succ n) f (0,_) = True f (1,_) = False g (_,0) = True g (_,1) = False Now, I was hoping to show that `f (ugly 0) == True`, `g (ugly 0) == False`, and `(let x = ugly 0 in seq (f x) (g x)) == True`, but this isn't true. So, can you figure out what happens without running this?
 Unexpected: EOF
There was some proof of concept of deploying yesod to heroku (just deploy a fully static binary and pretend it is python, ruby, whatever). I tried to mimic with snap, but it didn't really work - the only explanation that I could figure out was that accessing files made it totally unusable (it would take multiple seconds to return anything, often timing out completely) - since yesod compiles templates into the binary, they didn't have that problem (at least, that's the way I explained it. Tried to ask questions of support but they were mostly unhelpful). Presumably happstack would work as well / not well as those experiences would indicate.
Laziness. `evaluate` only evaluates its argument to [WHNF](http://stackoverflow.com/q/6872898/98117), so only the tuple constructor gets evaluated, not the fields inside. So `pureCatch (throw A, throw B) handle` results in `(throw A, throw B)` which will obviously throw an exception if you try to evaluate either of the fields.
Taking a pure partial function and returning an impure total function doesn't sound like a win to me.
&gt; I don't know how exactly that happens It's a tell-tale sign of PDFs that were rendered from PS rather than being created directly. Hence why using pdflatex fixes the problem of using latex followed by pdf2ps. Usually when I see those I just look for the original PS; they're easier to read since many PostScript viewers will antialias the bitmap fonts, thereby removing the jagged edges etc.
ArchLinux, ghc 7.4, emacs + haskell-mode
First of all: cool stuff! I'm a Haskell newbie so my comments aren't very Haskell specific, but: 1. simple*MM is not a very informative name, and there are no comments. What does "untrained" mean? transition matrix left unspecified? some reasonable default TM? 2. Looks like the latter, but what that default is is not immediately obvious. Maybe extract a function for it? especially since simpleMM and simpleHMM seem to have some commonality... 3. You could change | otherwise = memo_psi (t+1) (memo_x' (t+1)) ==&gt; | otherwise = memo_psi (t+1) $ memo_x' $ t+1 Not sure if its an improvement in this case.. 4. The implementation of viterbi uses lots of memoization, and is very hard to read... is that really needed? maybe you've tried it already and performance requires memoization, but... After all, the memory dependencies are very simple and pretty localized (each level depends on the previous one). Maybe you could restructure this as a tail recursion, where you compute for state t (optimal path into each state at time t, probability of each state at time t), then pass into the computation for time t+1? this would remember only what needs to be remembered (so is memory efficient), and seems more functional. 
Windows 7, Notepad++, GHC, I still have to look into web development but I know GUI development is in a horrible state (GTK+? \**hurl*\*).
GHC 7.0.4, Fedora (Openbox (LXTerminal (TMux (Zsh (Vim)))))
Just my two cents: I like to spell `given` this way: ensure p x = guard (p x) &gt;&gt; return x Then, `lookupArray` becomes lookupArray i arr = (!) arr &lt;$&gt; ensure (inRange (bounds arr)) i This particular example isn't as pretty, but that's mainly because we don't have a good notation that combines `&lt;$&gt;` with infix functions like `!`.
I moved from vim to sublime text.
Isn't it a wonderful feeling? I just went back and re-read my own thesis, for nostalgia's sake, and felt a glow of happiness from it. Congratulations!
Why do you think web apps shouldn't be written in Haskell?
Perhaps cabala would win from being written in the next generation language - Haskalah that has nice integration with the existing infrastructure for other languages.
Spare the man any more suffering.
Interesting. I use a similar technique, but with reverse approach. I have developed a similar system, called `cabal-uh`, wherein I guess an arbitrary *command* for `cabal-install` to meditate on, in hopes that will give cabal-install the enlightenment it needs to heal my broken Haskell world. I have had very little success special far, but I have heard that one should not begin study of Kabbalah before age 40, so I figure I need a few more years of traditional cabal study before I am ready to us cabal-uh successfully. 
Hilarious.. I read the bunch of apr1 jokes today already, all easy to identify -- but about half way through your post I thought: that poor Micheal must have lost it... Then I remembered the date. 
My (non---April Fool's) proposal is that Haskell should define hooks to enable Template Haskell to be used to generate record types with a sane syntax, and then get out of the way far enough to let libraries experiment with the solution space.
Thanks so much for double posting, or I would have missed this!
I was just thinking about this the other day. It turns out that you can extend it to the transcendental types and then define a universal cover where the integer theory is the special case of a quotient set. I have discovered a truly marvelous proof of this, which this margin is too narrow to contain
If you want Rails, that's pretty much what Yesod provides. I think there's even work ongoing for something like the Django admin for Yesod/Persistent.
Lots of people's reactions and more information: http://forums.xkcd.com/viewtopic.php?f=7&amp;t=82442 EDIT: more exploration http://www.reddit.com/r/xkcd/comments/rnst4/april_fools_xkcd_changing_comic/
Now that there are 70+ responses, will you tally them up for us?
 Where's evidence this was written in haskell? 
I think the idea is not that the list processing is parallelized, but that the work at each element can (perhaps) be parallelized. It is not as efficient as a map, in general, but for a task where the worm at each step is significant enough and parallelizable enough, par is a big win. But yeah, explicit parMap is better if parMap is what you want. Implicit parallelism is a bonus. 
That last [youtube video](http://www.youtube.com/watch?v=J---aiyznGQ) was vary informative on various subjects including catamorphism, piano axioms and how it could be applied to this new record proposal.
Yes the source code is evidence and I have it ;) I'll post it somewhere eventually. After a lot of sleep.
Thanks. Didn't mean to sound accusatory, it was just a little bit of let down not to find any details at your links. I wanted to see code!
Well, you could run each spawned work item through a thread pool with work queues, so the number of threads could be limited to the number of processing units plus a small constant. There is still work -delegation overhead, of course which I am making no attempt to estimate, so it is perhaps pointless to handwave in general terms about performance of techniques that feature combination of constant-time or linear-factor speedup and slowdowns without any sense of the actual values. :-) 
Is this a Fool's day joke?
Ya think?
OS X Lion, Sublime, GHC (whatever comes with latest Haskell Platform), Yesod, no GUI Framework needed yet - I'd like a framework for Cocoa, and one for Qt though.
[yay!](http://images4.fanpop.com/image/polls/647000/647717_1298405272636_full.jpg)
That's all well and good Chris, but shouldn't you have written that site in Happstack? -- Jeremy
&gt; The thing is that once the problem is expressed as a linear recursion, it's very difficult for the machine to turn it into an efficient parallel equivalent. The difficulty of this has nothing to do with the recursion's expression, and everything to do with the fact that automatically choosing an efficient distribution of work is hard. Note that `map` itself ([source](http://www.haskell.org/ghc/docs/7.0-latest/html/libraries/base-4.3.1.0/src/GHC-Base.html#map)) is simply defined as a recursion. It makes no difference to a Haskell implementation whether you use `map`, or write an equivalent recursion yourself: they are desugared, analyzed and optimized the same way.
Very wise observation.
home: OSX 10.5, GHC 6.12, Smultron, (Snap framework), OpenGL/GLUT. work: Windows 7, GHC 7, Notepad++
I think static typing can help a great deal here. For example, a lot of boilerplate conversions can be removed by using type classes. Since your domain functions should be typed anyway, that removes a lot of code. A lot of this stuff can also easily be removed by using the great abstraction possibilities that Haskell offers. For example, we've had great success with a DSL for defining APIs. Additionally, strong typing can prevent a lot of security issues like all kinds of encoding/injection problems.
Call-by-need is basically beta-reduction in the reverse order of call-by-value. This makes a stack trace virtually impossible. GHC only has stack traces because Haskell only has to *act* like call-by-need, not be call-by-need. Another way to look at it is to look at continuations. Call-by-need is ~~basically~~ the call-by-value of continuations, so it reduces them as much as possible, which eliminates stack traces.
It would probably work a lot better if Haskell had continuation-lambdas **instead** of value-lambdas, but that is kinda impure.
&gt;I believe good practices in untyped languages get you almost as far I believe the never ending parade of security holes in web apps written in dynamically typed languages proves otherwise.
Until we see hard evidence that people won't make just as many mistakes with statically typed framework, as they do with mature frameworks in dynamic languages (assuming they use them correctly), I won't make any claims either way.
So? I'm not thinking of arcane web programming in CGI or PHP. Most modern/mature frameworks give you ways of handling data in a way that encoding and injection problems are hard to create.
I think that's more of a stacktrace than a co-stacktrace, from the little info Google Search yielded..
I haven't had to think about escaping strings for years now, as common practices with templating and database interfaces just take care of it.
I haven't either. And then I find out that "oops, method X didn't escape its input" and my app has a security hole in it because "common practice" can't be enforced by a compiler. That's the whole point.
&gt; That's the whole point. Well, you are right. I have to agree with that.
Are there more backends for more obscure protocols like 9p or gopher?
I should note that acme-http is more real than fake. It really does really parse the HTTP Request and you can actually send arbitrary response bodies. It would be interesting to create a version that just dumped the incoming data on the floor and repeatedly responded with PONG until the client closed the connection (which it would presumably do after it had received the number of PONG responses it was expecting). The Response type includes a special PongResponse constructor which allows it to serve a pre-constructed PONG response. I've added an alternative implementation of the pong server that actually constructs the response for real: http://patch-tag.com/r/stepcut/acme-http/snapshot/current/content/pretty/Pong.hs It gets around 113,000 reqs/second. So, still pretty good. But having to concat those ByteStrings is a big penalty. Oddly, using blaze-builder totally killed performance.. around 24,000 reqs/second. That is exactly that type of information I am hoping to study using acme-http. Here is the another example that just echos back a pretty-printed version of the Request; http://patch-tag.com/r/stepcut/acme-http/snapshot/current/content/pretty/Echo.hs Of course, the lack of timeouts, max header size enforcement, etc, makes it impractical for real world usage.
One downside of moving back to a thirty two bit word size is that the four byte alignment of native words leaves only two bits available for pointer tagging rather than three. I'd expect that any extra cache friendliness would be lost in the additional thrashing as more of those pointers need to be indirected.
Debian Linux (Wheezy (testing)), GHC 7.0.x (whatever comes in the repo), Vim
I've been happy with the haskell platform on testing repos; I'm usually jealous of folks with the shiny new for a few months, but the debian maintainers seem to do a great job. More generally, I find the only open source packages I need to install by hand in testing are things like ruby (via rbenv) and stuff like clojure, node.js crap, etc. where you either have to keep up with the bleeding edge or use something three years old.
A good point, though if the benchmarks supported it, GHC could always choose to use 8 byte alignment in x32 and retain 3 bits of tag space.
Ubuntu Linux, Emacs, Snap
Yeah, it can refer to other elements, but never change them. I guess the important part of a `map` is that the changes to the elements are isolated.
What I think I meant is that I had the naïve assumption that if two threads of execution perform an action of the same number of elements, their runtime will be equal. With that assumption, it would be pretty easy to divide the work load of a `map` over a list, because each iteration of the map is guaranteed to perform an action only on one element. ...or well, that's what I thought until I typed it out right now and realised how stupid it sounds. Thanks.
care to elaborate?
Basically, if Haskell didn't have first-class values, but instead had first-class continuations, but then lazyness would make it isomorphic to a by-value language, which basically defeats the whole point *and* it is impure. There's more info about continuation-lambdas in [this](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8729) paper.
I'm not sure i fully understand what you mean, but it sounds like you're thinking about evaluation in strict (eager) terms, rather than Haskell's non-strict (lazy) semantics. &gt; each iteration of the map is guaranteed to perform an action only on one element. That's the thing: `map` does not provide any such guarantee over direct recursion (or `foldr`, or whatever). Any parallelism-preventing data dependencies that you can introduce with the other methods, you can also introduce with `map`, and vice versa.
Apologies for the C terminology. I believe they're talking about aligning the starts of structs to 8 byte intervals, not padding individual fields to 8 bytes.
No, conduits are not arrows. Arrows are synchronous, meaning that different parts of a composite arrow must consume input and produce output at the same rate, but this is not the case for conduit. In particular, there is no way to implement a `first` combinator, because in general there is no guarantee that as a conduit, `first f` will consume its input pair at the same rate as it produces the output pair. In a similar sense, pipes are not arrows either. 
Do wars about block comment syntax still go on? I thought cheap branching in version control had basically relegated that to the history books. Prefix comment syntax was always better for permanent comments anyway.
It was mostly in relation to the last bullet point in Wadler's law, which doesn't replicate in the smaller record debate.
Yeah. I think the furor has finally died down on that one. :) Or maybe we just don't see it because we aren't talking to new users of the language who can't get why they can't implement a -- operator just like they can in c++.
I was suggesting aligning each heap object to 8 bytes, not any of the pointers within it. This would probably mean some extra wasted space on the heap (on average less than 4 bytes/object), but if the three bits of [pointer tagging](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/HaskellExecution/PointerTagging) in 64bit GHC are as important for performance as trapxvi suggests then maybe it's worth it.
8 byte alignment is useful for a lot of things, In particular you don't have to deal with misaligned reads for doubles, and the extra tag bits are quite valuable as they make the difference is between being able to tag ~95% of time and 99% of the time. If we didn't need tagbits at all, we could use a scheme I use in a little language of my own, where I instead store the 'evaluated' bit in the most significant bit, and store the pointer `div` 16 in the remaining bits. That lets me address 32 gigs worth of memory on 16 byte alignment, and still use a tag-bit to mark evaluation with a simple add eax,eax jc evaluated ... and work with [rax*8 + offset] here at the expense of only being able to directly address larger structures. This scheme is close to the 'CompressedOOPs' scheme available in HotSpot. Alas, for Haskell this is a loss, because the tag bits have so many uses.
Ah, okay. I don't think arrays were intended to handle non-erroneous access to uninitialized element access, like this. If your code actually intends to distinguish between initialized and non-initialized elements, the right way to approach it would probably be for the array's elements to have a `Maybe` type, with `Nothing` for unassigned elements. In effect, this is just what your `Maybe`-returning `partial` function is trying to do, except that it's trying to insert the `Maybe` after the fact, which is not reliable.
@ninegua hit the nail on the head. I just wanted to point out that the type parameter order is important: it allows us to write a `MonadTrans` instance. Also, conduit 0.3 is kind of a "skipped release". Twan noticed that the types in 0.3 could actually be unified into a single type (called `Pipe` in his blog post, and I went with it). The result is now up on Hackage: http://hackage.haskell.org/packages/archive/conduit/0.4.0/doc/html/Data-Conduit-Internal.html
I think you would be better off in the first example with two separate data types so that you are not creating partial functions. For instance, you should not be able to try to solve a Solved.
It's not virtually impossible, the problem is simply that a "stack trace" isn't very meaningful anymore in a lazy setting. What you need is something more tree-like since that gives you the language to talk about what things have and have not been evaluated, rather than assuming that everything has been. The strict ordering of CBV languages is the only thing that makes their call trees degenerate into looking like a stack. Of course, what you really want is not just the call tree, but also the context--- so that you can discern why particular parts of the call tree were (or were not) forced. That is, you want both the trace and the co-trace. This is just as true in CBV languages. How many times have you gotten a huge stack dump from Java and then spent half an hour or more tracking down exactly what the context was that built up that stack in the first place?
Which arrow law do you have in mind when you say arrows are synchronous? Note that there is an ArrowChoice interface that allows you to construct the parallel composition of event stream transformers. It’s just that the tupling combinators are not particularly useful in this context, so you can’t really benefit from arrow notation.
All laws that involve `first`. Actually one can't even implement a generic `first` combinator to work with arrows that consume/produce at different rates. Note that even with ArrowChoice, both `f` in `left f` still takes 1 input to produce 1 output, only that `f` runs "slower" than `left f`. Yes, different parts of a composite arrow can run at different speed, but each arrow are still synchronized w.r.t. their own input/output rate. BTW, I was referring to "synchronous" data flow as an application of arrows. Arrows are abstract and applicable to a number of other applications that have nothing to do with data flow. 
That is, by far, the best kind of April Fool's Day joke.
I should perhaps have pointed out that the type parameters are in that order for a reason, and that a putative `Arrow` instance would necessarily involve a `newtype`, à la `Kleisli`. I look forward to updating my dependencies to conduit-4.0.\* :-)
no. The only backends right now are http and https. The only ones planned in the future would by spdy and web sockets. Though, there is no strong opposition to gopher. Just no volunteers to implement it at the moment. There is also a joke-ish telnet backend from a few april fools ago. (Useful for implementing a telnet-based BBS).
The hours of my life that I've wasted because I didn't know about this are flashing before my eyes. Hope this helps some others.
I'm still ignorant to pipes/conduit, but I believe Megacz' generalized arrows were meant to address this very concern (I suppose because they are parameterized by the sum product type), among other things. http://www.cs.berkeley.edu/~megacz/garrows/ 
As I said, I haven't looked at conduit or pipes closely, but you might find the abstractions you need here: http://hackage.haskell.org/package/categories
You can do this? I've been living a lie my whole life.
The timing of this is really good, it is just what I need right now. And I have to second tr0lltherapy, thanks for documenting your work, it is rough learning haskell when so many modules I want/need to use provide so little documentation. Is it possible with haskell to make a generic "makeForm" function that could create a form out of any record, rather than having to make a user record and a user form separately? Like if I create a dateForm, and a passwordForm, and a default textForm for anything else, be able to generate a form for any record that only uses those types?
I've been thinking about this comment for a long time, but I just can not wrap my head around it. Are you being sarcastic, trolling or dead serious?
Archlinux, Yesod 0.10.x and 1.0, GHC, vim
$(blah) syntax is a Template Haskell splice. It is a reference to code that will execute at compile time to generate other code. Sort of like a souped up macro. Splices can exist at the top level and are evaluated during compilation. That page describes the SafeCopy class immediately after the part you pasted: *deriveSafeCopy creates an instance of the SafeCopy class for CounterState. SafeCopy is a class for versioned serialization, deserilization, and migration.* "base" and "CounterState" are arguments to the Template Haskell splice. I've never used HappStack but from the context I would assume that it adds instance declarations and helper functions for using the CounterState type (which was previously defined) from within HappStack. 
Suggestions noted, and the post has been updated. Thanks for the feedback. =)
Thanks! &gt; Also note that a monoid (proper) is exactly a category with only one object (hence S = S 1 1). Yeah, I noticed that.
I think something like this is beyond my current abilities, but could you give me a hint at what is involved? Are we talking experimental ghc options or something like that? I'd use reflection in a dynamic language, but I'm not sure what the haskell approach to this kind of thing is.
Yep. That's how it ended for me with Happstack, and i moved on to other haskell web frameworks (yesod).
It's mostly GADTs, I've sent you a private message with more information.
"Referential transparency" may be a bit misleading. f x = inner x where inner y = y*2 g x = inner (x+1) where inner y = (y-1)*2 These two functions are equal, but `f=&gt;inner` and `g=&gt;inner` are not, which is bad. It ties in with referential transparency because you cannot replace `f` with an equivalent value (here `g`).
X-axis is number of lines. Y-axis is number of files that had that many lines. All files of all types are included (so this isn't only Haskell source files -- on the high line count end, some of the files have names like `foo.dat` and `data.h` and so forth), as generated by `cabal unpack &lt;pkgname&gt;`. Just a curiosity, not drawing any conclusions from it. =)
1. The key in that tutorial is the main function, which includes `openLocalState initialCounterState`. This opens an acid-state transaction log in `$PWD/state/Main.CounterState`. There is also a `openLocalStateFrom` function that also takes a path to the transaction log. 2. SafeCopy is an addition to the cereal binary serialization package that adds versioned migrations. This means if you change the type of something stored, you can continue to support the old type by telling SafeCopy how to convert to the new type on the fly. 3. `'base` is a quoted identifier in Template Haskell, referring to a value imported from `Data.SafeCopy`. This is related to the migrations explained in the previous bullet point. "Base" here means there are no migrations: we're defining the first version of the `CounterState` type. Later, if we change the type, the new one would be an `'extension` and the old one still the `'base`. There's also `'primitive` which is used for types that don't change such as `Int`. 4. `''CounterState` is a quoted type in Template Haskell. This is different from `'CounterState` which would be the value constructor. 5. The expression is evaluated at compile-time using Template Haskell, and generates code itself. In this case it generates an instance of the `SafeCopy` class for the `CounterState` type (version zero and migration kind Base). This has the same effect as writing the instance manually (`instance SafeCopy CounterState where ...`) but allows for less boilerplate via meta-programming. 6. Are you referring to the `deriveSafeCopy` line again? If so, the previous point should hint at the answer: it is evaluated at compile-time to generate code in its place. 7. Depends what you mean by "pure". AcidState is a persistence layer and so it involves IO. The transaction events are described in pure terms, however, and the storage effectively works by "appending" transactions rather than overwriting/rewriting the state, which is similar to how pure datastructures can emulate mutability efficiently. Maybe you're confusing AcidState for the plain State monad? The state monad can be used to emulate mutable variables inside a running program; AcidState is more like a database, sort of like Redis baked into Haskell. That's what the "acid" comes from, c.f. [Wikipedia](http://en.wikipedia.org/wiki/ACID).
"The use of Template Haskell in Happstack confused me, so I moved to Yesod instead." Nice non sequitur there.
I wondered if followup requests would start coming in... =) In case you want to play with it yourself, I've put it the full data [on my website](http://dmwit.com/hackage-wc-2012-04-03.txt).
[An implementation with runnable code](http://hpaste.org/56903) Doesn't have the fancy 'product category' typeclass, but I thought your definition of Monad was a bit handwavy. Here we have a version of Monoid and Monad where the types are very closely matching.
I dunno. I mean, what are all the files to look at? I know there's at least .hs, .lhs, .chs, and .hsc. Are there others? Should .h and .c files be included? I mean, they're part of the source, right? What's the exactly right line to draw? I don't know, so I drew a very conservative line.
yeah but .h and .c wont be haskell code, so they shouldn't count. .lhs and other fancier ones shouldnt count either, unless you can filter out non-haskell. it's silly to count up lines that aren't haskell!
Not really. The only copious use of TH in Yesod is in persistent/model. And that's exactly the part i am NOT using :) Besides, unlike Happstack was, yesod is much better documented. When i tried happstack it was just when they forked from happs. Their web site and documentation were atrocious. Perhaps if i tried them today, the result would be different. But i already found what i need. 
&gt; .h and .c wont be haskell code, so they shouldn't count What makes Haskell code the be-all and end-all of interest? &gt; .lhs and other fancier ones shouldnt count either, unless you can filter out non-haskell. Well, does this mean you want me to write a parser for the .hs files that filters the comments out of those, too? This is starting to sound like an awful lot of work!
Well, I would say that you're measuring code on Hackage is what makes Haskell code the be-all end-all of interest! &gt; Well, does this mean you want me to write a parser for the .hs files that filters the comments out of those, too? This is starting to sound like an awful lot of work! Nah, just ignore them! It shouldn't be too hard to make another graph, surely. Just change *.* to *.hs in your script!
Does that mean parsing is related to comonads?
The SafeCopy-instance defines how your state-type, CounterState in your case, is serialized, so that it can be written to and read from disk. You can do this manually, but it's more convenient to use Template Haskell, as in the crashcourse. The "$(...)" will be replaced by the compiler with an automatically generated instance declaration. In the documentation, http://hackage.haskell.org/packages/archive/safecopy/0.6.1/doc/html/Data-SafeCopy.html#v:deriveSafeCopy, there are examples of what the generated instance declarations look like.
You were probably joking, but in case not: that's the legend ;).
I'm not sure, but probably hard. It can't be done in regular Haskell, obviously, since `inner` isn't in scope. So it would have to be done in the compiler. But `inner` might not even exist anymore, it might have been inlined and compiled away.
I disagree with you there. I was concurring with the parent post that there exists a trade-off between static verification and interactive introspection of run-time behavior (debugging being a special case of the latter). There exists a wide class of problems (especially numerical problems) where in my experience, the latter approach is preferable. You on the other hand seemed to suggest that the parent post is approaching the problem from the wrong end, indicating to the best of my knowledge that static analysis makes debugging redundant. In the unlikely case that you would like me to elaborate, I'll be happy to do so.
What is that first resonance? 
Did you read the article?
This is why I changed my statement to mean code. You combine parsers to generate a new parser, using the monad instance to combine parsers.
Also, have you considered running SCC a second time on your graph of SCC nodes? You would keep doing that until the result converged, and then the brute force search might be faster.
I thought it was meant to clarify the CT behind that sentence, and explain effects on the way there.
Try using the vector package. It's better than the array package.
By your definition, calling Template Haskell code is not the same as writing Template Haskell code. With acid-state you only call Template Haskell code.
That's not helpful for the OP since InterviewStreet's setup doesn't have vector installed as a usable package.
I find the acid-state crash course an excellent overview of acid-state. Yet when googling for acid-state, one is most likely going to end up on [acid-state homepage](http://acid-state.seize.it/) which unfortunately does a less stellar job of explaining how to use it. Perhaps it'd be a good idea to link from acid-state's homepage to the crash course article?
&gt;I'm not sure what you mean by this, but it's almost certainly a misunderstanding. Isn't this basically how the State monad works? It passes the state around as arguments on the call stack, it's just hidden from you in the plumbing of (&gt;&gt;=). 
Good idea! I have added a link in the Links section on the acid-state homepage. 
I would think a memoizing solution would be very quick. All the distasteful state could be in the memoizer.
There's no "call stack"
The State type has nothing to do with GHC or any particular implementation detail.
Sorry if I'm not making myself clear. The State monad manages state by passing it around as an argument. Arguments are passed on the call stack. Therefore the original statement about monads using the call stack to store state, was technically correct. Albeit, this is an implementation detail that most would not need to concern themselves with, but it is correct. 
Well, I understand how that is true for free monads, but what about other monads like the state monad?
Shouldn't it be ”Galois's”? My brain hurts trying to internally pronounce the trailing ”'” in ”Galois'” given that the ”s” is silent...
When I think of free monad, I'm referring to: data Free f r = Pure r | Free (f (Free f r)) instance (Functor f) =&gt; Monad (Free f) where ... It's not clear to me what I would set `f` to in order to define the `State` monad. More importantly, regardless of what you consider to be a `Free` monad, it's not clear how the `State` monad involves substitution. You might define anything that has a `join` function to be substitution, but that's a very loose interpretation of substitution, as opposed to the `Free` monad where it's very obvious why we interpret it as substitution since you just substitute the `Pure` constructor with the continuation.
I'm only mad because you give me emacs envy! Love your emacs demonstrations.
Holy wow, bounds checking is 2x the work of all the rest of the inner loop iteration combined? 
Really cool. Worth to try it later...
You're reinventing iteratees. See the `pipes` library for my implementation. Long story short, neither one is "in control". More generally, one concept you should probably look into is the "free monad": data Free f r = Free (f (Free f r)) | Pure r You can find this in the `free` package on Hackage. Your PassiveCoiterable2 is just: Free (a -&gt;) b If you phrase it as a Free monad, you get a monad out of it for free (although that's not the origin of the word "Free" in the name).
It depends on who you ask. &gt; ("Galois'" ++) &lt;$&gt; ["s", ""] ["Galois's","Galois'"]
Yes, I've read that paper. I still don't see how: Free (s -&gt; (s,)) r =? State s r The example that Uustalu and Vene use in their paper is a Free monad (since Trees are just free monads over the list functor), but I don't understand the conclusion that all monads are interpretable as substitutions, since it's not obvious to me how all monads can be reformulated as free monads.
 notEq (x:y:z) | x == y = notEq (y:z) | otherwise = x : notEq (y:z) notEq xs = xs 
I really did not understand what was meant by "threading state on the call stack". It doesn't work that way in practice, and it isn't modeled that way in my head. It's not a "made me feel better thing", I really believe it is likely to indicate a mischaracterization of what's going on. If what he meant was "threading the state as arguments and return values" then that's right. But that's not what I think a reference to the "call stack" means at all.
Haskell idiot here - is that last line basically for the case of a one-element list? Or does it serve some other purpose?
You can get around the reversal by using difference lists a2p_coit :: ActiveCoiterable a b -&gt; PassiveCoiterable1 a b a2p_coit f = PC1 (\a -&gt; a2p_coit $ f . (a:)) (f []) a2p_iter :: ActiveIterable1 a -&gt; PassiveIterable a a2p_iter f = f $ a2p_coit id Though that still doesn't force incremental evaluation of the functions. 
x:y:z matches a list with at least 2 elements, x and y. The other cases are lists of length 1 and 0.
Right, 0 as well, of course. That makes sense.
Expand the current error, although I am not sure what happens if the ghci buffer isn't displayed --- I find popup buffers annoying at times.
Fun, I just learned about foldables the other day and this seems like a function that could be defined pretty succinctly for all of 'em. Really isn't this a fold over some data structure where your binary function is a special case of some concatenation function that returns it's left argument if the two arguments are equal and left `mappend` right otherwise? hm, now that I think about it, this would also require that the values contained in the Foldable be of instance Eq. I'm still a bit flaky on the syntax and not near a terminal right now, but I'd like to see an idiomatic way to solve this for as broad a typeclass as possible. :) 
 notEq :: (Foldable t, Eq a) =&gt; t a -&gt; [a] notEq = foldr f [] where f x ys@(y:_) | x == y = ys f x ys = x : ys I don't see how we could generalise `f`. Its arguments have different types, so I don't see that `Monoid` would be any help.
You might also like to look at co-routines; a random googling brings up [Stephen Blackheath's blogpost on the matter.](http://random.axman6.com/blog/?p=231) One way of thinking about co-routines is that you have two threads with co-operative scheduling: threads transfer control to each other by yielding explicitly, possibly with some attached data. 
Allowing effects means that the regular lens laws don't make sense anymore. get l (put l b a) = b put l (get l a) a = a put l b1 (put l b2 a) = put l b1 a Can you provide some similar laws which your lenses abide by? Can you prove that your pure lenses do abide by the original laws?
Isn't `nub` exactly the requested function? http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-List.html#v:nub Edit: oh, but `nub` doesn't require them to be adjacent.
The point about cabal install foo &amp;&amp; cabal install bar vs cabal install foo bar is very important for users to realise. The solver can actually do a good job of getting all the dependencies consistent if you show it the whole picture, ie the top level thing(s) you want to install. I'm not sure I get this bit though: &gt; So I changed the Yesod installer scripts to use this approach. However, even though I could install yesod from source, I found it did not guarantee that I could build my Yesod application. I needed to installi both yesod source and my application at the same time. This is another instance of the same issue. Ask to install your app, rather than asking to install yesod then your app. If your app depends directly on some of the things that yesod depends on, then the app might need different versions from what were picked initially for yesod. (Yes, it might be possible to cope with picking different versions, that's what private deps are about.) Another solution to this is to ask to use only installed yesod instances, and never rebuild from source (though of course that might not be possible): cabal install ./myapp --constraint='yesod installed' Also note that cabal already lets you install multiple local directories or tarballs. The cabal-meta thing of handling git repo targets is of course useful. When I added support for dir targets and local and remote tarballs, I considered adding darcs and git repos too, but there's rather more policy that you need there, because you have to have somewhere to cache the repo, you don't want it downloading all the time. Being able to write down the environment, rather than supplying it on the cabal command line is also a good thing. We've been considering various designs for this, covering this use case and others like local package repos/archives.
I completely agree with you here. The current toolchain works great for the first argument, where things only depend on the hackage snapshot. That's fine for end users of Haskell applications. But our ecosystem has grown complex enough that it is unacceptable for modern Haskell developers working on significant code bases. It creates a big incentive to dump all your code in one package, and for organizations developing proprietary code, this means that they'll also be less likely to release things back to the community. Having the policy be a pure function of the hackage snapshot is just not the right position any more.
&gt; what is the point of installing libraries? Libraries are applications. Sure, you can't do anything interesting by typing `./libfoo` at a shell prompt, but it's perfectly plausible that you'll find yourself wanting to `ghci` followed by `:m +Foo`.
That's indeed a useful thing to do, for example if you're testing out a new library. However, imagine if we had a working `cabal repl` command, that loaded your current package (application or library) and all of its dependencies into `ghci`. I bet you'd be calling plain `ghci` much less. After all, the reason you want to play with a library in `ghci` is often because your current project depends on it and you need to figure out how some part of the library works.
Hey, library author here, not original poster. I'll try to answer your questions above. Thanks for looking.
I agree that `StateP s` is not isomorphic to `State s`. It wasn't intended as a proof, just as a demonstration of how you represent state-based computations with the free monad form. I also agree that `Free (State s)` isn't isomorphic to `State s`. But `Free (State s)` is undeniably *a* state monad, and — handwaving ahead — "monadically", it behaves identically to *the* state monad.
&gt; Allowing effects means that the regular lens laws don't make sense anymore. Well, for now I say that a well-behaved lens obeys get-put, put-get, and put-put within some monadic context, or "where defined", and wave my hands a bit. &gt; Can you provide some similar laws which your lenses abide by? Can you prove that your pure lenses do abide by the original laws? "Obeying the lens laws" was never a property of a type, unlike the monad laws which deal with natural transformations and where an instance can be proven correct. The lenses I have pre-defined in the library are all (barring any omissions on my part) well-behaved, and only lens combinators that preserve the well-behavedness of well-behaved lenses are defined. Also note that Lens from data-lens is equivalent to `Lens Identity Identity`
See my reply to Daniel Yokomizo in the blog comments. The crux of the problem is that I absolutely need set to be `a -&gt; m (b -&gt; a)` and not `a -&gt; b -&gt; m a`. Concretely, because fclabels is built on `Point` [this function](http://hackage.haskell.org/packages/archive/pez/0.1.0/doc/html/Data-Label-Zipper.html#v:close), must return its result in `Maybe` which would not be necessary with the formulation in yall.
The `cont` explanation makes sense to me, because I understand that. Thank you for explaining it that way.
The first solution is not a category. I'm still trying variations on the latter solution you propose. I can summarize the issue succinctly: Let's say you use `Right` to signal a normal value received from upstream and `Left` to signal a termination's return value from upstream. The problem is that this protocol requires enforcing that upstream never gives you a `Left` followed by a `Right`. Unfortunately, the input type of `Either r a` enforces no such restriction. If you try to implement something like session types, then you break the monad (it won't even type-check. I can actually summarize the issue even more succinctly than that: You can't define a correct identity pipe with that formulation. Now, I'm not 100% convinced that there isn't a similar `Either` formulation that does have a correct identity pipe, so I still continue to experiment in that direction.
Your criteria are too strict here. No monad without an explicit `Pure r` could be a free monad in your sense, since that exposes information that would otherwise be opaque. When we say a monad is free, I think we really mean to say that there is a free monad which is adjoint to it and that the adjunction preserves not only the general monad laws, but also the laws internal to the operations of that particular monad.
It would also have the benefit of Cabal running any preprocessors needed when you type `:r`.
`cabal-dev ghci`?
It's a decent hack. The `cabal repl` is trying to do it properly. (I case you didn't know, cabal-dev ghci works by calling cabal build --with-ghc=fake-ghc-cabal-dev, collecting how cabal called ghc, fiddling with the args and then calling ghci. Obviosuly it's possible to do something much more robust and with more features.)
What's the music in the background? It sounds like it'd be nice to code.
Arch GNU Linux, GHC 7.4.1, vim,(n/a),reactive-banana+SDL+(patched for SDL)Cairo+Diagrams
Evan, are there any plans to rease the source code? (and website needs update in Status Update, I think)
Thanks, I fixed them.
By the way, the project page is http://code.google.com/p/frege/ and from there you can reach the downloads, wiki and so on.
Thank you I fixed it.
Lazyness -&gt; Laziness
I have to report back that cabal-meta is fantastic! I used it to build a large proprietary project of mine with ~120 package dependencies including several other packages from my local filesystem that either haven't been released to hackage or have modifications from something already on hackage. Prior to cabal-meta, I had only been able to build this project with cabal-dev, but doing so made it painful to use GHCi and gave me other pain points that I've [discussed before](https://github.com/creswick/cabal-dev/issues/45). In short, this functionality solves my biggest complaint about Cabal/cabal-install and really needs to be integrated as soon as possible.
Lots of typos or grammar errors. Also: if (*list == NULL) { // if the list is empty should be: if (list == NULL) { // if the list is empty unless I'm mistaken.
Thanks, this is great.
I still don't see the problem. Why do you have to return a maybe with fclabels but not with your lens implementation? What does you version return in the case of a Nothing and why can this not be done in the fclabels case?
I was referring to the Monad instance.
Nicely done! Would you mind x-posting it to [/r/haskellquestions](/r/haskellquestions)? I'll sidebar it there if you're OK with it.
I like it, but have someone look it over to fix all the minor language errors. 
Nice, kind of has the gestalt of Learn You. When you say *other imperative languages* I think haskell is not usually lumped in imperative languages, (SPJ joke aside), conventional definition is changing program state throughout code (Below 01_basic/10_Introduction/00_hello_world.lhs and ) -------------------- *Laziness This is a very uncommon feature. * Laziness as a language design default is uncommon, clojure has it but lazy collections and optional lazy eval are not uncommon in FP languages ---------- *IO is a … . Wait! No! I won’t say it now! I am afraid to terrify you.* Heh. 
'An Haskell tutorial' instead of 'A Haskell tutorial' This is how I figured out you were french.
Hmm... didn't consider that. I'll sidebar a link, though, if you'd like.
Well, you could set `m` = `Identity`, `w` = `(-&gt;) Address`. But I don't know if the laws would check out. Reminds me of [isomorphism lenses](http://twanvl.nl/blog/haskell/isomorphism-lenses).
Sans-serif please.
I really dislike sans-serif for reading on the web. But I have this for you: http://readable.tastefulwords.com/?setup&amp;change&amp;text_font=Helvetica%2C%20quote(Helvetica%20Neuve)%2C%20Arial%2C%20Tahoma%2C%20sans-serif&amp;text_font_monospace=Inconsolata&amp;text_font_header=quote(Times%20New%20Roman)%2C%20Times%2C%20serif&amp;text_size=20px&amp;text_line_height=1.5&amp;box_width=30em&amp;color_text=%23657B83&amp;color_background=%23FDF6E3&amp;color_links=%23B58900&amp;text_align=justified&amp;base=web_readability&amp;custom_css=pre%2C%20tt%2C%20code%20%7B%20%20%20%20background-color%3A%20%23EEE8D5%3B%20%20%20%20color%3A%20%23586E75%3B%20%7D%20pre%20%7B%20padding%3A%201em%3B%20overflow%3A%20auto%3B%7D
Maybe it's the italics that irritating me.
Thank you I fixed these.
I use Ruby's bundler utility for even the most trivial Ruby programs now. Even if I don't really need it, it still serves as documentation so I can quickly come back to the program laster. So part of this could be more of an issue of making cabal easy to use and scale down well.
Thanks a ton for posting this in French as well as english. In addition to being a great read, its super helpful for my vocab skills!
Very nice. I know it goes against the spirit of things, but section 3.1 can just be solved by evenSum :: (Integral a) =&gt; [a] -&gt; a evenSum = sum . filter even **EDIT:** nevermind, should have carried on reading!
I also (lazily) tend to start solving problems only when "evaluated" by an advisor :)
The mod function assumes that its arguments must be of the same type. What are you actually trying to do?
Protip: write the function without type signatures and then ask ghci what is the best :type fir it.
nice project, tnx for open sourcing. :) next game "go", anyone? (i heard haskellers like "go")
This is a really excellent tutorial.
Keep up the good work!
&gt; Why another Haskell to Heroku guide? Haskell needs more success stories. Also there’s great need for up-to-date documentation. YES! Good call.
 class Eq a where (==) :: a -&gt; a -&gt; Bool (/=) :: a -&gt; a -&gt; Bool Class Eq provides functions to test for (in)equality. class Eq a =&gt; Ord a where compare :: a -&gt; a -&gt; Ordering (&lt;) :: a -&gt; a -&gt; Bool (&gt;=) :: a -&gt; a -&gt; Bool (&gt;) :: a -&gt; a -&gt; Bool (&lt;=) :: a -&gt; a -&gt; Bool max :: a -&gt; a -&gt; a min :: a -&gt; a -&gt; a Class Ord extends class Eq to also provide functions for testing ordering. If you need one of the functions provided by Ord that aren't provided by Eq, then you need to demand Ord. But if you only need one of the functions provided by Eq, then you only need to demand Eq. (I think your other question has been answered already on the other post.)
I'm just starting to learn Haskell and I've been wondering, if GHC knows the type signature without me telling it what it is, what's the point? I've heared it's kind of like a comment so other developers can better understand the function, but they could just use GHCi to find out too. I've also heared it can be used to restrict some usage (e. g. make a function that could work on any list only work on strings) but that seems pointless to me too.
That explains the typical french grammar mistakes. :p
&gt; I've also heared it can be used to restrict some usage (e. g. make a function that could work on any list only work on strings) but that seems pointless to me too. Ah, but your function might work but make no sense on some types. For example: callPerson phoneNumber phoneBook = ... Might technically work with any integer for phoneNumber, but it doesn't make sense, so you're able to tie down the *concept* rather than just the type. Or perhaps an encryption function that requires two prime numbers to work: generateRSAKey p q = ... p and q can be any integer, and the function will certainly work although generate an invalid key (or an extremely weak one, I've not worked it through). You might want to restrict it to prime numbers, and you can then ensure you never accidentally pass a non-prime number to the function thanks to the compiler.
Sometimes it helps the compiler figure out what type you intended to use when there are many that would have fit the bill. Also sometimes the actual type the compiler infers is way more complex than what you really intended.
I will be at BayHac as well. Lets talk then.
5) The only way to use type aliases to beautify your types is with an explicit type signature.
They don't actually teach you anything there. They just tell you to say that you learned everything and learn it when someone actually asks you questions about it.
I've always wondered how can people get their monad code working without static typing..
Already seen on Channel9 as soon as they published it (I'm also the only guy who asked about his publication :D ). I was very proud to see Haskell inside this Lang.Next conference, and I really liked the talk itself because it wasn't an "Ode to the Haskell Language" but a pragmatic rationale of why Galois still uses Haskell. Well done! (ps. Good Easter day to everyone!)
Are there slides available?
I was thinking of a similar implementation of this CPU in Haskell but my main concern was to find an efficient *pure* data structure to store the 1.000 Words16 of RAM. You use an unboxed array which is really inefficient because it requires 1.000 copies on write. Another solution will be to use a Map Int Word16, which provides Log^2 (1.000) complexity in read and write operations, but it's still far away from an *impure mutable array* which provides read and write in constant time. I was thinking of an hybrid data structure where the RAM array will be divided in *pages*, i.e. 20 pages of 50 Word16, so you only need to copy the page of the edited cell. Obviously, the straightforward efficient solution will be to run the entire emulator in the ST monad and use a STUArray, so the emulator execution is impure but its result isn't. If someone has an idea for an efficient pure data structure, I'm all eyes.
Lazy education?
Thanks, I didn't know.
Look at a finger tree sequence. They tend to be pretty fast for these kinds of operations. And then you can page at the lowest level if needed. Oh, and they are present in Haskell too!
I also hacked something together: https://github.com/jaspervdj/dcpu16-hs
As you said there are multiple aspects to cabal-meta, but particularly if we stick to "writing the env down", this makes a lot of sense. Perhaps the problem is that Cabal does not have a big enough division between package author and application builder. Note I am using the term "application" builder because there is no point to building a package except as a dependency if it doesn't expose an executable. In Ruby there is a gemspec for the package author. This is exactly like a cabal file: it includes a lot of information about the package including dependencies. If I say "gem install package", then package is installed according to the gemspec. A gemspec should specify as large of range of package dependencies as possible. As an *application* creator, I don't touch a gemspec (cabal) file. Instead I use a tool called Bundler. Bundler is a tool for writing the environment down. It uses a Gemfile, which is analagous to using cabal-meta'a sources.txt (on top of a cabal file build-depends). When you install, it creates a Gemfile.lock, which includes *exact* dependency information which can be used to make sure everyone on the team has the same versions and to facilitate conservative dependency upgrades. There is some overlap between these tools: when developing a library it is convenient to use Bundler and a Gemfile. There is a Gemfile command that lets you import the gemspec of the library into your dependencies. 
I don't think it will be an huge improvement, if any, to the full Map version, cause Map's complexity grows logarithmically (so not an wide difference between Mapping a few dozen cells and the full RAM) and you still have the overhead of the array copy (even if it happens not often).
I didn't know a lot about finger trees, these look awesome and made some interesting performance boost compared to the BTree solution. Thanks.
From what I hear, DiffArrays have pretty abysmal performance in practice.
Yeah, I've never used them myself, so I'll take your word for it. I usually use `IOUArray` or a C-based implementation wrapped in a `StorableArray`.
Efficient for what?
Can't see the video online. I don't have Silverlight, and when I click on HTML5 format it brings me back to the Silverlight one... Currently downloading it before viewing.
I have a slow internet connection (yes, they do exist), can someone summarize the most interesting / key points?
Use fewer resources than a web server on a general purpose OS =~ potential cost savings 
Done! The submission is [here](http://www.reddit.com/r/programming/comments/rzo9s/tfoo_simple_five_in_a_row_game_in_haskell/).
Basically they said that the best way they have found out to make use of Haskell, was to find business domains where Haskell would be the best fit, instead of trying to attack all possible type of projects with Haskell. Except of a few cases, most of their projects were successful. They give an example of a such case. Now they are even offering courses on how to program in Haskell. 
The way it is being used nowadays, Haskell might eventually spread into the industry, even when the language motto is "avoid success at all cost". :)
Did you use vagrant or something like that for automating your vm stuff? (iirc the first guy who did haskell on heroku did that). I've also done this, and it worked well.
I'm talking about a different problem here. Not debugging an existing memory issue, but predicting memory usage and making sure that no memory issues are possible. It's like comparing Python (which has stack traces that allow you to find the problem when it occurs) and Haskell (whose type system ensures that certain problems will never occur).
Ooh, shiny. Hadn't heard of that work before.
What component is being removed?
OS
If I understand correctly from the example of their attempt to implement a wireless network driver, it's not that easy to match the performance of Linux' usually quite optimized network drivers and TCP/IP stack; so the benefit of avoiding the general-purpose-OS overhead might cost you performance in the long run.
Yes, you can email me at info@elm-lang.org :)
Replacing one OS with a different OS doesn't make it magically faster.
it seems I have found a bug (it says O's won, although it is not the case yet): http://i.imgur.com/l4s2i.png Moreover, one move before, it did not allow me to make a move to the cell I wanted to make it. Perhaps it uses slightly different rules, not these I am used to.
The basic idea is that memory usage is directly proportional to the size of a term as expressed *in Haskell*. It's not necessary to know details about the translation to a lower level language, like STG.
Also, 4) orphan instances cause trouble. (But, avoiding orphan instances 100% of the time is not practical).
We had a similar issue in how best to implement memory in writing an emulator for an 8-bit microprocessor to be formalized in a proof assistant for a verified C compiler. We went with a trie indexed by the same bitvectors we used to implement program counters, addresses, etc.
I didn't know what else to link to, but I noticed this morning it seems the merge went in from looking at my `cvs-ghc` email. Obligatory example: (vh-head)~HSENV &gt; ghci -XDataKinds GHCi, version 7.5.20120408: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done Loading package base ... linking ... done. Prelude&gt; import GHC.TypeLits Prelude GHC.TypeLits&gt; let x = sing :: Sing 4096 Prelude GHC.TypeLits&gt; fromSing x 4096 Prelude GHC.TypeLits&gt; let y = sing :: Sing "OMG" Prelude GHC.TypeLits&gt; fromSing y "OMG" Prelude GHC.TypeLits&gt; I imagine the work is still rough right now. In particular I would say it'd make more sense to have type literals behind their own flag rather than piggybacking `DataKinds`, but that's just this mans' opinion. EDIT: I borked the title. I meant for it to say "GHC HEAD now has type level natural number/**string** literals." EDIT 2, ELECTRIC BOOGALOO: [Here](https://gist.github.com/2345391) is another mindless example I whipped up right fast: {-# LANGUAGE DataKinds #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE ScopedTypeVariables #-} module Main (main) where import GHC.TypeLits data Proxy p = Proxy f :: SingI n =&gt; Proxy n -&gt; SingRep n f (_ :: Proxy n) = fromSing (sing :: Sing n) main :: IO () main = mapM_ putStrLn [show $ f n, show $ f s] where n = Proxy :: Proxy 4096 s = Proxy :: Proxy "lol" 
Congratulations! I want more pictures of your baby! 
You won't know till you try. I like the idea of a single address space with no switch between kernel and user space. I also like the idea of tiny environments running on a hypervisor host rather than full blown OSes for servers. If you only want really one or two things to run, you don't really need a time sharing OS. Also if you're concerned about security, it's nice to be able to run in a VM container, but those aren't fool proof either. Haskell's type system makes some of that a bit of a warm fuzzy, but even Haskell has evil backdoors.
I am deliberately not making any claims about this particular case as I haven't even looked at it. I am only pointing out that the concept behind it provides a far better basis for a secure system than linux does. That said, I think it is incredibly unlikely that halvm is less secure than linux, given that one would almost have to deliberately add tons of security holes to make up for the massive difference in code size.
Less code, particularly C and assembly, is great but that does not mean it is secure.
You might also take a look at Singularity from MSR. I believe they have a VM-bootable image or one that could be built from a source enlistment.
Comparing X lines of code that has undergone some amount of security testing vs Y lines that has received a different amount of security testing is simply not apples to apples. Smaller is better but one exploit is enough.
The linux kernel has far more than one exploitable vulnerability in it right now waiting to be found. You are putting far too much faith in a known insecure system.
I'm also doing a thesis on FRP, and I'd like to cite your thesis in my own. Do you have the relevant information?
&gt; Is this part way to dependent types? I'm probably no more up to speed than you are, but yes, this looks very much like a step toward dependent types. It reminds me a lot of [Dependent ML](http://www.cs.bu.edu/~hwxi/DML/DML.html).
Well those are good news. I wasn't pleased with the way cabal-dev worked. I had to make a custom bash overlay to cabal/ghc/ghci (mostly to pass good -package-db or -package-conf options) but it seems I can replace it by hsenv. BTW, I see only 'virtualenv' on hackage. However checking the gihub page of 'hsenv' both seem to do the same thing. Do they? Is it the same package but renamed? Is 'hsenv' soon to be on hackage?
Here's how I'd implement the console/kb emulation: data DCPU = ... simulate :: Int {- nCycles -} -&gt; DCPU -&gt; DCPU type Driver a = DCPU -&gt; IO (DCPU, a) videoDriver :: Driver () keyboardDriver :: Driver () powerDriver :: Driver Bool mainLoop :: Time -&gt; DCPU -&gt; IO () mainLoop t0 dcpu = t1 &lt;- getCurrentTime let cycles = getCycleCount (diffTime t1 t0) dcpu &lt;- return $ simulate cycles dcpu (dcpu, _) &lt;- videoDriver dcpu (dcpu, _) &lt;- keyboardDriver dcpu (dcpu, power) &lt;- powerDriver dcpu when power $ mainLoop t1 dcpu Basically, since all IO is memory-mapped, you can simulate however, and then just do all the IO at once. videoDriver reads from the dcpu memory and does whatever is required to update the display, keyboardDriver checks if the keyboard has any events and updates the memory of the dcpu with new events, and powerDriver reads from some address and tells you if the machine should be stopped, allowing a program to terminate.
And yours afterwards.
Yes, here's an example of doing what you're describing, allowing you to add two vectors of a typed length: https://gist.github.com/2345391 The `unsafeCoerce` is there to convince the type checker, essentially, that `m ~ (m + 0)`. I am not entirely sure why this is necessary, since my code is a fairly straightforward port of the typical peano encoding of the naturals for vectors. The solver seems to get confused by the first equation for `vappend` - it sees the first argument has a length of `n` and the second length `m`, and in the equation `vappend Nil v = v` the result type of that expression is `m` while the declared type says it is `n+m`. I would think the solver could handle this knowing statically that `n ~ 0`. I think this may be a bug but I'm not certain. Ultimately though the unsafeCoerce does not affect any runtime representation since the integers are type level only, and the laws here are obvious. It's just a cheap 'proof' that the resulting types are the same. Strageness aside, what this new feature basically amounts to is two new kinds, `Nat` and `Symbol`, which are the kinds of type level natural numbers and strings, respectively. So if some type constructor takes a parameter of kind `Nat` (example, `data Foo (n :: Nat)`) then its type parameter is actually a number, so you would use it by saying `t :: Foo 4096`. In other words, naturals and strings can now exist at the type level. They can also be reified to regular value level terms using singletons. See my example below (the `fromSing` stuff, which turns a type level literal singleton into a value); hopefully this didn't just confuse you further.
No, it's just the most likely entrance point (web development) Same thing that made Ruby: a good web framework.
I think a lot of Rails programmers had an incomplete view of the language and ecosystem because they basically just learned the Rails API. I think Yesod is a great way to learn web development and some of Haskell programming. But it would be a shame if users only learned to lean on the Yesod API and didn't also get to know Haskell outside the context of web development.
&gt;I think Yesod is a great way to learn web development and some of Haskell programming. But it would be a shame if users only learned to lean on the Yesod API and didn't also get to know Haskell outside the context of web development. I think we can take a bit of comfort that it'll be different in this case. Anybody taking the naïve approach will quickly find their head bumping into the type checker. If a potential user is really serious about using Yesod (or any Haskell framework) they are pretty much forced to learn Haskell in order to get anywhere.
What "security testing" do you think the linux kernel has undergone? Security is about correctness, not passively saying "people can look at the code so its probably secure or they'd have found the bugs".
Congratulations! I think Yesod will be a huge boon to the Haskell community and help make the language much more popular.
Exactly.
Did you try pattern matching on the 2nd argument in `vappend`? The compiler may only know that `m + 0 ~ m` and not that `0 + m ~ m`. You can't have both because `+` is a type family and then you would have overlapping instances for `0 + 0`.
Man, that guy *really* hates Hamlet, for apparently no reason at all.
For some non-haskeller like myself, does this mean that I can do C++ template metaprogramming now in haskell? 
Basically Just so it's clear: Type level programming in Haskell is, with some existing extensions, Turing complete*. C++'s templated metaprogramming is also Turing complete. So you could have done the equivalent of C++ template metaprogramming in Haskell for a while now. However, in Haskell type level numbers were very annoying: They could only be represented in Peano number form: Successor (Successor Zero) is 2 and so on. Don't even ask about floating point numbers! It was/is quite annoying. Now, with this feature, Haskell programmers can use regular-ol numbers just like C++ can use. Peano numbers in Haskell: http://www.haskell.org/haskellwiki/Peano_numbers * Haskell98 type inference is not turing complete. There are some restrictions that assures the inference algorithms will complete. Using an extension like Undecidable Instances lifts these restrictions.
Sometimes we all could use a reminder to work at keeping the Haskell community supportive, friendly, and accepting. I think this is one of those times. From a brief scan through the thread, it seems "papsosouid" is also trying to express an opinion that's not all that unreasonable, and is running into a lot of bad blood, defensiveness, and hostility. And, yes, admittedly responding to it badly. But now he's being called out by name in a different forum where we feel we'll find more support for badmouthing him? Please take a moment a look at how the Haskell community comes across here, because it doesn't look all that great from my perspective. Maybe I'm being biased and unfair... I am in agreement with some of the technical points raised. But I do think it would be nice if the Yesod community figured out how to have a conversation with someone about strong design choices and their consequences in composability and complexity, and not end up being condescending or hostile or making negative comments about others' experience or motives. This is not the first time this has happened, and it reflects badly on more than a couple people. I've spent a bit of time struggling now with how to handle this... I think Michael is doing some good things for the Haskell community. There's a lot of positive stuff we could talk about. Even from the perspective of someone who isn't a fan of the design, there's still a lot to admire about the level of automation in this release and the impressive screencast and example. But it's hard to keep things in a positive light when there's this level of defensiveness and calling people out by name going on in the background.
a first step towards typeEq, *and* a building block for custom record systems.
I would expect you to bend over backwards to defend Snap proponents. I would understand if you blamed me for said discussion. But I don't know why you are calling out Michael by name as a bad influence by referencing a Hacker News discussion he had no part of. That is completely uncalled for and deserving of an apology.
So how would you inspect that it found the one you intended in Haskell? Would it complain every time there are two valid instances in scope? (more like Agda's instance arguments?)
He gave a perfectly good reason: &gt; Personally, I can't stand the "lets mix code and markup" style of template systems in general, which includes most templates. I also hated the indentation system, it just doesn't work well in markup heavy templating. I'm fine with indentation in programming (obviously given that we're talking about haskell), but it drove me nuts in templates as I had to indent far too much, it isn't simple blocks of code being indented, it is "indent everything that is inside something else", which is of course everything. I'm not saying that I agree or disagree with his reasons -- just that I recognize they exist, and they're, broadly speaking, sane. It's not my place to judge his reasons. It's a set of reasons that makes sense for his choices and preferences. I mean, I'm not going to have to live every day with the choices some dude on ycombinator makes about template engines. I'm going to have to live with *my* choices, about *my* stack -- and that's what I'm entitled to have strong opinons about. I mean, horses for courses. Some people like that style. Some people like other styles. There are maybe four or five distinct points in the design space for template engines, each with their plusses and minuses. The people that don't like hamlet said why they didn't, and they didn't make the universal claim that hamlet was no good for others. I agree with cdsmith that the response they got -- as well as the response here from you and cies -- was much more hostile than they were. Lots of frameworks go through this moment, and I think Yesod is perhaps here now (with 1.0), where people begin to stop saying "we have the best answer to everything" and start saying "we have a coherent collection of opinionated components that fit well together. you may not like all our opinions. if you don't, you can swap out components. if you don't like any of our opinions, there are other options from other frameworks, and we encourage you to try those instead, maybe. it's a big world, we're just one part (though we're our favorite part, and we'd like to convince you that we should be yours too!) and there's room for a range of products." Ultimately, that sort of attitude will win more friends, users, contributors, and good will than the arrogant one. It will produce a better product in the end, too.
The nonbasic instructions are there to allow room for future extension. They're a bit of a wild card, so it makes sense to keep them separate. (It also kind of makes sense for making decoding/encoding safer by preventing accidental mixups. They are encoded very differently, after all.)
&gt; I think Michael is doing some good things for the Haskell community The context of this sentence implied, perhaps unintentionally, that Michael was one of those that "end[ed] up being condescending or hostile", exhibiting an inordinate "level of defensiveness", etc. It's always dangerous to couple compliments with criticism, lest they come off as insincere.
I think that more project leaders and proponents should try to learn from darcs. The team has a great attitude (and kowey does a great job setting the tone). When users say they have problems with darcs, they try to engage them and map out a plan to address their issues. And when people say they can't use darcs because of various issues, they say they *understand* and that over the long haul, they hope to make darcs a better product that can resolve those sorts of issues. Something like the latest sprint report (http://blog.darcs.net/2012/04/darcs-hacking-sprint-7-report.html) makes me want to use darcs for the development community it fosters as much as the technology. We can't all be kowey, and not all our products are like darcs (in terms of the cool parts and the difficult parts both), but I think there's something to learn from.
I agree that's where the implication came from. For my part, I took no offense, and just took the compliment. But maybe that's my over-inflated ego stepping in ;). Thanks Chris!
2011.2.0.1.2 on Ubuntu (from the repo) and 2011.4 on Windows (straight from the platform's website). I'll try to install the latest one on Linux from source see if it works better. Edit: Only GHC 7.0.3 available in the repo. Compiling 7.0.4 first.
Directed graphs are a fun time. My whole life is digraphs right now. When I implement digraphs I tend to cache the successor and predecessor sets with the vertices since these are used so heavily in traversals. I am about to write a bunch of graph stuff in CoffeeScript. If you want to write something fun and useful implement [Tarjan's Algorithm](http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm).
6) Type signatures provide a place for your haddock comments.
As promised, a blog post: http://www.yesodweb.com/blog/2012/04/working-together
If you call `Data.Map.insert` with one `Ord` dictionary and later again with another `Ord` dictionary, then you screwed up your finite map royally. Balanced search trees rely on the fact that the ordering between elements always stays the same. How do you solve that? Hrm, the right way to go about this is probably to store the dictionary with the search at creation time.
With that said, I am pretty unsure about my core instruction data types. They probably need to be simplified, and have some polymorphism thrown in for dealing with comments/labels/optimization annotations/etc.
You're right, there's clearly something not ok in this board. I know of one bug, sometimes the server-client connection breaks and events are not pushed from the server (I think it happens in longer games). If you ever run into this bug once more, can you try refreshing the page? The winning move may appear then. I'll try to investigate it too. Thanks for the info!
I haven't heard about Heroku buildpacks before. This is an interesting concept! I've found [this repository](https://github.com/mbbx6spp/cabal-heroku-buildpack) via google (didn't inspect it yet). I will try to do more research on this subject. It would be great if we have a way to avoid this 1GB problem. Deploying directly to Heroku, without any virtual machines, would be awesome.
Unfortunately I don't know vagrant - will try to learn more about it. Thanks for the tip!
&gt; Furthermore, the disconnect in operational semantics due to non-trivial program transformations and optimizations, as described by the author, applies to any industrial-strength compiler nowadays, not just GHC. Examples? In C++ I have a very, very good grasp of how much memory is allocated at each point.
Nice work Snoymaster. The screencast is quite nice. I think screencasts really show off the *time* aspect well, i.e. it's all well and good to describe in tutorials how developer workflow is supposed to be and typically is, but seeing it for real is highly compelling. Embedding it on the home page might be a nice idea. The development server kind of subverts my Emacs workflow, I might have to add some support for it. By the way the home page does indeed look very suave. Nice work, Luite! [Looking nice is a big deal.](http://uxmyths.com/post/1161244116/myth-25-aesthetics-are-not-important-if-you-have-good-us) I have some tweaks to contrib, presumably I can make pull requests to [the Github repo.](https://github.com/yesodweb/yesodweb.com) Cthulhu bless [the Github effect.](http://wiki.chrisdone.com/Github%20effect) I'm vaguely tempted to, after rewriting [Ji](https://github.com/chrisdone/ji) (which I will do anyway at some point), whack it on top of Yesod. Yesod for me combines convenience with the safety assurances I'd want. Ji adds a shed load of convenience to the developer UI-wise. The combination of the two will surely result in a screencast that will blow people's minds! ;-) Maybe I will do it now on the current version, to drum up interest, as for rewriting it I want to spend more time on efficiency and correctness from the ground up.
If my idea of functional programming is a "flexible combinator-based approach", what web framework would agree with me then?
Congratulations! I think Yesod has pretty good chances to become one of the main driving forces which push Haskell into the semi-mainstream programming like what RoR did to Ruby. The design looks really cool. Though it has one minor bug. In opera "homepage-info" is too big and is partly hidden behind "announcement".
In this C code, the return is missing: complex complex_square (complex z) { complex tmp; ... return tmp; } 
Works for me.
I don't think its correct to say Java programmers are implicitly using monads already. The function monad (Reader - static argument) is a particular use of function composition with a static argument and bind to make it monadic. "Bind" is a rare combinator, the applicative combinators for functions are used commonly in combinator programming S (&lt;*&gt;) and K (pure) but bind isn't. There isn't even a combinatory name for it. Similarly in about 10 years of using Haskell pretty much daily, I don't think I've ever used the list monad in my own code. Non-determinism is something I never need, if I were programming in Java I'd doubt I'd need it any more. Java, Python, SML, etc. programmers are already using **effects** which monads can model, but they would have to be using bind to be using monads. They aren't doing this.
A monad is just an abstract concept. So I would say in general that everybody will gain much from pure and abstract thinking, and in special that a Java programmer would gain from learning about monads.
for future reference: http://www.downforeveryoneorjustme.com/http://hackage.haskell.org/
The function print is also interresting. It takes on argument ... interresting =&gt; interesting on =&gt; one
Aah yep, \^_\^" I will try to reduce that number as far as possible ...
Big congratulations on the 1.0 release - enjoyed the nice screencast too :-) The pace of development has been impressive - in Fedora we are still playing catchup with *-conduit... This does feel like a significant milestone for Yesod and Haskell Web development. :)
So it does, quite nicely :) What's the overall plan with it? Debugger, RE tool, fancy analysis?
When did the Haskell community get so drama-packed? I just saw that someone made [a whole HN post](http://news.ycombinator.com/item?id=3819716) to "expose" that one of the Yesod critics on HN was a throwaway account. This is after there was a big IRC fight over the post, as well as some good name-calling on the original thread. It just seems ridiculous that [posts that weren't even particularly critical or non-constructive](http://news.ycombinator.com/threads?id=papsosouid) generated so much conflict! The major Haskell web frameworks are all great work! If you prefer one, you should be able to explain why you prefer it without being labeled a shill for "the other side". I don't know where all the poison is coming from, but the people involved need to start asking themselves if they're approaching this whole community business the right way.
Thanks for the report. I have to admit that the redesign was a bit of a last minute rush job, so I haven't tested it as well as I should have. It looks ok here in Opera 11.62 on OS X on default settings, but looking at the HTML, I see that changing the font size would cause these problems (also on other browsers).
Too much parenthesis: evenSum = (foldl' (+) 0) . (filter even) =&gt; evenSum = foldl' (+) 0 . filter even 
Or rather that the list monad *is* actually just non-determinism (plus a well-defined order of results), but that a lot of programmers use non-determinism and think of it by a friendlier name, like iterating over possibilities.
i agree that there is (snicker) a...."cabal" of mods here who seem to want to whitewash haskell's shortcomings
CSS gets tamed in much a similar fashion: the cascade of your stylesheets gets compiled for you as you nest selectors in http://sass-lang.com/ 
Setting sockets to blocking mode is a bad idea. You should be using wrappers that check for EAGAIN instead.
It was down for me about two days ago.
nice one michael. i was also thinking of posting something similar today but has to go to work :)
The next feature is the 100Mz * multiplier run mode. And maybe add more options to inspect state during run. What's the meaning of RE tool?
Interesting problem and proposal, but what about reducing the number of dependencies a module has?
Too bad it [doesn't seem like you can generate imports from Template Haskell](http://hackage.haskell.org/trac/ghc/ticket/1475), otherwise it probably wouldn't be too hard to do yourself.
Totally. :sort should just work.
&gt; If you prefer one, you should be able to explain why you prefer it without being labeled a shill for "the other side". Sure, just please don't explain it in terms of what you hate about the other ones. (I'm not directing this at you, godofpumpkins, just at everyone in general.) Rather than "I hate mixing logic in with templates, it's a horrible idea", instead express the reciprocal positive opinion: "I think logicless templates are easier on designers", etc.
Yes, he could have expressed his opinions more constructively, but that doesn't make him a troll, and certainly does not make him a shill for Snap. Online communities (especially reddit and hacker news) are full of opinionated tech people who have no tact, and aren't necessarily justified in their opinions. Taking one such person and reacting so badly to him (picking a fight with unrelated people on IRC and making an entire HN post attacking him) just makes (members of) Yesod ('s community) look a lot worse than one isolated "wrong person on the internet" ever did. And I say this as a completely disinterested non-web-programmer. I've dabbled with Snap but don't really do anything much with web technologies. I don't mean to single out Yesod here. I think it's just a vicious cycle of strong opinionatedness and lack of tact, and that isolated members of the various subcommunities have given the overall inter-community relationship a slightly bitter taste over time.
I think it is unfortunate that so much emphasis is placed on 'bind' as THE monad operator. 'join' is a much more appropriate one, and as far as lists(join=concat) go, I certainly use it every day.
Technically you can't (safely) implement monads in Java, because the presence of side effects means that you can break out of the abstraction. Not only can you, there's no way to prohibit it. In Haskell, bind works because the closure for the function argument is pure; in Java there's no way to enforce that purity. Though you can certainly use the programmatic style if you adhere to the Real Programmers Have Perfect Discipline school of thought.
Yes. The only thing the HaLVM RTS gets from the Hypervisor is memory, CPU time, and access to a limited set of hypercalls and events. We have written device drivers that fiddle directly with IO memory. (Which is an incredible thrill :-)
Cool. What hardware is supported? Are you coding the drivers in C or Haskell?
Or M-x sort-lines for the rest of us. :)
I've always thought it was silly that `qualified` was required at all. Particularly when leaving it out doesn't actually cause an error, but instead just does something dumb that no one ever wanted.
We should also consider other possibilities. Here are some random ones: 1. Only type `import` once but keep the rest the same: import ( Yesod , Data.Text (Text, unpack) , Control.Applicative ((&lt;$&gt;), (&lt;*&gt;)) ... ) I believe Go is one of the languages that does this. 2. Allow hierarchical modules (and here you thought we had them already.) import Data.Text -- Bring Text, Text.Lazy, Text.Encoding, etc into scope. 3. Make qualified imports the default. (Python does this.) Personally I usually find myself converting non-qualified imports to qualified imports as a module grows and explicit import lists get longer. (I'll save the argument about explicit imports for another time.) I feel that this space could be explored much more. 
Extremely interesting, thank you! &gt; hopefully this didn't just confuse you further. No I think I mostly get it, although I'm less familiar with unsafeCoerce. Were there to be a problem in the code hiding behind it, would unsafeCoerce stop the compiler throwing an error?
This is why we should continue to avoid success at all costs :) 
I get around this by delegating the import list to a separate program, fix-imports. I still edit it to add unqualified imports for operators, but by and large I no longer look at the import block. Then again, I use qualified imports by default. If you use unqualified imports and explicit import lists then you are in for more hand editing. So I wouldn't support fancier import syntax because I think people should minimize fanciness in import blocks (i.e. implicit import lists and 'as' renaming).
Do mean the eweek slides or the slides you see me presenting here: http://channel9.msdn.com/Events/Lang-NEXT/Lang-NEXT-2012/A-Means-to-Many-Ends-10-Years-of-Haskell-at-Galois The eweek article is an attempt -- and a mostly successful one, I think -- to extract high-level value statements from the above that its readership cares about. Not sure if that's PHB's, or not. If so, then I guess you're saying that Daryl hit the mark spot on :-) If you're referring to my slides, I'd love to hear more of your thoughts and reaction. For example, we don't claim to have solved fully homomorphic encryption, but I can see how you may have gotten that impression from the eweek slides. We're part of a team exploring ways of bringing FHE closer to practicality. The example in the talk is about a different kind of shared computation (and I did a poor job of explaining it).
We had low-level Haskell primitives that were essentially implemented in the C run-time (as normal GHC does). It certainly feels like Haskell :-) We think it could be better done in something like Habit (where you can be specific about layout and bit twiddling and things), and it may need specialized synchronization primitives (with priorities, for example). I think the HaLVM expects (no more than) a pretty vanilla HAL, but it also won't take advantage of many features. We know how to support multi-core, we just have time to do it.
I just tested this and it's fine: Prelude&gt; import GHC.TypeLits Prelude GHC.TypeLits&gt; :i Nat data Nat -- Defined in GHC.TypeLits' Prelude GHC.TypeLits&gt; data Nat Prelude GHC.TypeLits&gt; :i Nat data Nat -- Defined at interactive:4:6 Prelude GHC.TypeLits&gt; :i GHC.TypeLits.Nat data GHC.TypeLits.Nat -- Defined in `GHC.TypeLits' Prelude GHC.TypeLits&gt; :k Nat Nat :: * Prelude GHC.TypeLits&gt; :k GHC.TypeLits.Nat GHC.TypeLits.Nat :: * Prelude GHC.TypeLits&gt; data K = Z | L Prelude GHC.TypeLits&gt; :k K K :: * Prelude GHC.TypeLits&gt; :k Z Z :: K
&gt; Remember: changing your Ord instance locally can break Data.Map in horrible ways. That's because the Map should use the dictionary in scope when it is created, not when it is used. In Agda for example, this would be done as it allows for "local" uses of records as instance arguments.
&gt; Balanced search trees rely on the fact that the ordering between elements always stays the same The solution is obvious, the ordering used should be stored along with the tree, not required as an argument to insert.
Not so fast. With proposed syntax (radix trees) your sorting will be broken. 
 data Foo :: * -&gt; * -&gt; * where FooC :: a -&gt; Foo a Bool foo = FooC x :: Foo [Char] Bool x = foo "bar" Edit: Nevermind the above, I misread your post. What you're asking for isn't arbitrary type constructors, what you want is an elimination mechanism other than pattern matching. In particular, the function application as (-&gt;) elimination is written as juxtaposition, which I guess you want to overload?
I wonder, esp in regard to BitC's love of OO, if something allong the lines of what Martin Odersky proposed in [A Second Look at Overloading](http://lampwww.epfl.ch/~odersky/papers/#Types) might meet their goals. If i understand correctly this aproach allows you to mix Structural Subtyping and something looking very much like type-classes. The constraints you end up with on your signatures aren't very pretty, but there's no reason you couldn't fold that under some syntactic sugar.
I'm done compiling GHC 7.0.4 and Haskell-Platform 2011.04 and it works fine. As for the Windows machine, it's due to be reimaged so I won't spend time investigating.
Thanks, I appreciate it that you took the time to respond courteously. I allowed myself to get a pretty bad impression of you, but I've adjusted my view now. I'm sorry if I offended you in any way in any of my recent comments.
Obligatory [Stack Overflow](http://stackoverflow.com/questions/5645168/comparing-haskells-snap-and-yesod-web-frameworks) thread.
&gt; We've got two great web frameworks Two great web frameworks and Yesod. &lt;/dnr&gt;
I'm glad it's working out now, thanks for letting me know.
This looks like a good description of Yesod to me. Thank you!
More accurately, non-determinism with a total order over the results, allowing duplication. You'd need a set monad to get mere unstructured non-determinism. But more structure is merely more structure, not strictly different structure, so I suppooooose...
Please don't. As an outsider, there was nothing wrong with the HN discussion. This is what it looks like. Just dig through the linux-kernel archives to see examples of the same. People spend tremendous energy creating software that is later shot down. The shooting down part *is* important to everyone else, that is - the *users*.
I wish there were so much unbiased programmer like you, out there :) (Sorry if this phrase doesn't make sense at all :D , the point is that Michael is one of the most fair and "non-smug" programmer and is always a pleasure read what he writes!)
You can use '&gt;' to quote: &gt; Not exactly mainstream, but the very interesting - if still young - language Idris
Yes, though it would force me to maintain a sensible order, so I wouldn't get to the messy point where I'd want to sort the lines.
Generalising [idiom brackets](https://personal.cis.strath.ac.uk/~conor/pub/she/idiom.html) could make sense. * `(f x y)` --- normal function application * `(| f x y |)` --- same as `f &lt;$&gt; x &lt;*&gt; y` --- idiom brackets * `(&lt; f x y &gt;)` or whatever style of bracket you choose --- your own fancy elimination rule Edit: while we're at it, maybe we can also make list and tuple syntax something that's provided by the Prelude rather than hard-coded into the compiler? Perhaps a generalisation too far.
Scala has syntax sugar for monads as well. F# does too, although you can't actually write code that's polymorphic in the choice of monad, which makes it somewhat useless.
I tried to make a fair [comparison of the web frameworks](http://www.haskell.org/haskellwiki/Web/Comparison_of_Happstack,_Snap_and_Yesod) a while ago but like a moth following a pretty light I got distracted by client-side development ideas. At any rate I would still appreciate it if anyone could add to it more details and their experiences. With mightybyte's permission I wouldn't mind taking his blog post and adding the paragraphs to the wiki. 
Of the three, Snap is the one without a "default" persistence-layer. It would be interesting to know what people are using.
I think that would be great. Let me know if I can flesh out any of the details on Yesod. Once the page is up-to-date, I'll give it a link from the Yesod site.
Why isn't the type of `withForeignPtr` simply `ForeignPtr a -&gt; IO (Ptr a)`?
See also cale's nifty nest function: http://www.haskell.org/pipermail/haskell-cafe/2008-February/038963.html
close but no cigar. Their type vs. cale's nifty nest: bigotimes :: [(x -&gt; r) -&gt; x] -&gt; ([x] -&gt; r) -&gt; [x] nest :: [(r -&gt; a) -&gt; a] -&gt; ([r] -&gt; a) -&gt; a --or changing the type var names to make things simpler bigotimes :: [(x -&gt; r) -&gt; x] -&gt; ([x] -&gt; r) -&gt; [x] nest :: [(x -&gt; r) -&gt; r] -&gt; ([x] -&gt; r) -&gt; r So they're similar looking, but they do quite different things... 
When I was writing the post I considered a bit whether to put it on the wiki or do it as a blog post. To some extent I think there's value in leaving it there as a blog post date-stamped and not constantly massaged by an endless stream of wiki edits. But of course, as wikipedia has demonstrated, there's value in having the constant edits as well. I'm not sure. So I'll leave it up to you. You have my permission to copy, edit, link to, or do whatever you think is best with my post.
I see. They are doing sequence for selection monad, while Cale's function is for continuations.
Yes, so do I, and also in Haskell. For theoretical proofs, though, you would presumably need to list every single kind of potential optimization and show what effect that could have on complexity.
BayHac '12 is shaping up to be great: There are now over 60 people signed up! * Fri, April 20th — 10am ~ 7pm — Hacker Dojo * Sat, April 21st — 10am ~ 7pm — Mozilla HQ note venue change * Sun, April 22nd — 10am ~ 7pm — Mozilla HQ note venue change If haven't yet, please sign up, and let us know you're coming: http://goo.gl/xbwKS
Does it impact runtime performances or do calls to ContT functions (specially (&gt;&gt;=)) get inlined?
In that case, I strongly disagree with the premise of your post. Perhaps you are experiencing difficulty because you are trying to project onto Haskell the kind of thinking about time and space that you do in other languages. Or perhaps you and/or your team have not yet developed the kinds of style and techniques that make time and space leaks rare. (No, that does **not** mean throwing `seq` and bangs all over the place at random.) The following statements are true about all industrial-strength languages I know about, including Haskell: * The time and space complexity of the vast majority of code is very clear, at least when that code is written by a competent and disciplined professional. * There are some unusual cases in which time and space analysis is inherently difficult. How often that comes up varies between different problem spaces, but in practice teams of engineers working for a long time in a particular problem space develop techniques to minimize and isolate these issues. * Bugs can cause time and space leaks. Those kinds of bugs are costly to find and fix. So good engineers invest effort to ensure that the risk of them occurring is extremely low. In a sense there is indeed a higher cost to time and space complexity management in Haskell - the steeper learning curve for programmers who already have years of experience in other languages. But it doesn't sound like you are talking about that. I applaud all of the efforts that are being put into better understanding of time and space complexity in pure functional languages, and into better tool support for managing it in Haskell. But there is no need for apologetics; they are not trying to "catch up" with any other language.
Well, ; itself isn't a monadic operator explicitly either, so if you have _one_ non-explicitly monadic widget, why not two? Honestly, this is why I prefer languages that have fairly fluid syntax, so that you can think and talk about these things as you want.
I'm using acid-state myself. I think this says something about the modularity of all of these projects, that it is not only possible but even practical do pick components from each as you like.
Monads happen to be an incredibly useful general abstraction that is *particularly good* for solving the *particular* problem of making sense of effectful computation in (otherwise) pure and lazy languages. From a historical perspective, Monads were only introduced to Haskell because the current mechanisms for doing IO were embarrassing. If you don't have the problem of supporting side-effects in a language without side-effects by default, Monads don't seem incredibly useful.
That is more of a "get" than a "with"
&gt; Yesod seems to be philosophically closer to ... Zope (since sometimes it seems to be a bit overbloated). Errr, if Yesod is "bloated", give up now. You'll never find anything that isn't "bloated", except for those really thin shims that really don't _do_ anything except get people all excited about how small they are. It's certainly much smaller than Django right now, if only because it's still got small single-digit number of person-years put into it. Also, I don't think it's anything like Zope in any other way, either. There aren't many things that are.
They could be included as cited and dated blockquotes, perhaps?
I actually don't know if Yesod is bloated or not, that was an impression I got from the article :) Also, I didn't mean "bloated" in the sense of too much code, but in the sense of too many features, some of which I may never use but would still get in the way of getting things done. Is that the case?
No. It's pretty minimal. Again, it's hard to reach "bloated" in the amount of time that Yesod has existed, or any of these frameworks, really. You might disagree with the particular library chosen by Yesod, but no nontrivial website is going to fail to use _something_ to do everything Yesod provides. Nor are any of the individual libraries bloated by that standard.
Of course your eyes do not see things like a designer. Designers use Visual tools like DreamVeawer. And your broken hamlet simply does not render in there. 
Perhaps Snap could be compared to Flask: focus on clean code and APIs, has a preferred ("in-house") templating system (Heist, Jinja2) but no particular persistence preference, and provides architecture for modular components (Snaplets, Blueprints). The Heist templating system however is more similar to Zope templates and to a lesser extent Genshi templates. (Most similar, however, is the templates in the Lift framework for Scala.) I'd say Yesod is comparable to Rails and web2py. Not sure there is a Django in Haskell, but if you're building the sort of CMS apps that Django is best suited for, Yesod might be closest philosophically. Happstack isn't really similar to anything, but perhaps Pyramid and TurboGears are the least dissimilar from Python. The ZODB is vaguely similar to acid-state and Pyramid's traversal-based routing possibly the closest thing you get to type-safe routing in Python.
Wow, thanks so much, that was really helpful.
OK, but to a first approximation neither does anything else. They've got some stuff for PHP and probably some other big name things, but to name a couple of much larger projects, there's no support for Rails on the latest and a puny 3rd-party plugin for Django that does nearly nothing. And I bet if I were working on a PHP site I'd have it scrambled beyond Dreamweaver's ability to handle it in roughly an hour. If that's your standard for "broken" you're not using Haskell for anything on the web anyhow, because it's way too cutting edge for anything you're doing, along with things like Django, Rails, or goodness help us, any of the JS solutions like jQuery UI or YUI.
If there is an impact to runtime performance, it will be negligible I think. `&gt;&gt;=` just performs ~~trivial~~ function application. 
It's really incredibly easy to just use the underlying db adapter's methods inside your handlers. I often just define a runDB :: (Connection -&gt; IO a) -&gt; AppHandler a and use it everywhere with the DB layer's own methods. For reference, I have used the following in this way: - Cassandra (using cassy, which I wrote and maintain) - Postgres (using postgresql-simple) - Postgres (using Persistent) - Mysql (using mysql-simple) 
It's a fictional CPU for a video game. See [here](http://0x10c.com/) for information about the game, and on the right is a link to the specs for the processor. 
In general we hook into the Xen front-end / back-end virtualization system (where a full Linux Dom0 is providing all of the real device drivers). But we have done some native drivers, and some old example code (which is likely to be bit-rotted) can be found here: https://github.com/GaloisInc/HaLVM/tree/master/libraries/RealDevice/Device 
Don't know much about monads, but I can tell you I gained a ton learning about mynads.
I can think of two really important things that monads get you that you can't get from their imperative translations: The first is that monads form a category, namely the Kleisli category, where monad sequencing is (basically) composition in the Kleisli category. This is important because **anything** that forms a category can be composed seamlessly. "Seamlessly" is the key word. When I compose two things in any category and give you the result of that composition, there is no way you can detect the composition boundary: it completely disappears. This is the essence of abstraction in that I can compose as many things as I want without any increase in complexity. Translating that to monads, that means that if I sequence two monads and give you the result, you can't "unsequence" them. Similarly, if I gave you some `IO` monad and asked you to create a function to count how many `IO` actions I sequenced to create it, you wouldn't be able to do it. The abstraction is perfect and the boundary between actions is utterly destroyed when I sequence them and code that works with a single IO action transparently works with 99999 IO actions combined together. Now, that seems unremarkable for the `IO` monad since imperative languages already have this facility, but for the other monads their straightforward imperative translations do not have that similar ability to transparently compose like that. For a programmer, the essence of monads is code that combines absolutely transparently. In Haskell, sequencing monads "just works" because the abstraction is perfect, whereas in the imperative translations the programmer typically must maintain a detailed mental model of the underlying machinery to ensure various invariants are maintained and code isn't combined incorrectly. Note that all of this assumes the Monad doesn't violate the monad laws (which are just the Kleisli category laws in disguise). If it does violate them, then all hell breaks loose. The second big advantage of monads is monad transformers. Monad transformers also have a strong basis in category theory, since they just define a functor (from the base monad's Kleisli category to the total monad's Kleisli category). What this means for you practically, is that lift "distributes" over monad sequencing: lift (m1 &gt;&gt; m2) = lift m1 &gt;&gt; lift m2 This lets you stack monad transformers indefinitely with no abstraction penalty at all, something the imperative counterparts cannot do. In short, because Haskell structures code using category theory idioms, it composes much more robustly than a naive imperative translation of the code.
Using postgresql-simple myself =)
On that point we're in agreement in. Java's type system is not powerful enough to enforce the kinds of guarantees that monads require.
[Objects to Unify Type Classes and GADTs](http://lambda-the-ultimate.org/node/3837).
I'm not sure umbrella imports target the same problem. Importing umbrella module `X` is supposed to import `X.Y` for all `Y`. The more general problem is ad-hoc targeted imports. How would an umbrella module tackle the problem of wanting to do just import qualified Text.Search.Sphinx.Types import qualified Text.Search.Sphinx.ExcerptConfiguration Would I create a new umbrella module of my own to just re-export those? Or just import the umbrella `Text.Search.Sphinx` and live with the unnecessary things it brings with it?
Honest question: Could someone explain to me how much of the excitement around the DCPU-16 is due to its own merit? Or are people only really interested because Notch will embed it in a future video game?
I heard about it because it is Notch's game, but the appeal is the call back to "the good old days" of fully understanding *all* the code in your machine on a simple architecture and the competitive coding aspect.
I really enjoyed reading that. I would never have thought of using the ST monad in place of the IO monad for an "almost pure" process. I wonder if this technique could be used in game design to isolate the game logic layer?
While I appreciate your point and agree with it, saying “You are wrong.” as an opening statement to disagreeing with someone is quite antagonistic. Please don't do that.
&gt; unless your goal is to sell the language to your PHB Of all the people to sell Haskell to, PHBs I think are the most important people. However, these slides are 100% worthless to anyone who even *knows* about the existence of /r/haskell; they're just some basic feel-good claims about Haskell.
&gt; the lack of higher-kinded interfaces Hrm? interface Functor&lt;T&gt; { ... } Works just fine. The *real* pain point is trying to write a Java type signature for higher-order functions, like `fmap`. Another pain point is that declaring an instance of an interface is *not* decoupled like it is in Haskell. To make `ArrayList` an instance of `Functor`, the closest you can get is to create another class that inherits from ArrayList and implements Functor. [edit] scratch that, I see what you mean. You'd actually have to hack your way through with interface Functor&lt;F, T&gt; { ... } 
Setting a socket to blocking mode is a bad idea because GHC's IO manager relies on an event loop using non-blocking IO.
EDIT: Oops, I didn't realise that you were talking about a different freenet program. Nevermind. Have you tried running one of the existing Erlang servers? (E.g., https://github.com/archaelus/erlirc) IIRC, the Facebook chat system is written in Erlang. Github also uses Erlang for some load-balancing I think. While a system written in Haskell may obtain higher single-node performance, Erlang is probably easier to scale to multi-node setups or NUMA systems. Erlang has great libraries for building fault-tolerant, distributed systems. Its single-threaded performance is not great, but it isn't bad, either. There's also HiPE, but I don't know how mature that is. Writing a new IRC server from scratch would be a fun task to do in Haskell, sure, but I would seriously consider Erlang first.
I don't have time to do it right now, but for those of us who are not familiar with Freenet, could you tell us what USK and SSK stand for, and perhaps link to the specs?
If you want to learn monads, the best tutorial ever written is this one: http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html
Hello! I've somewhat responded to this thread in a followup post here: http://www.mgsloan.com/wordpress/?p=228
&gt; The first is that monads form a category, namely the Kleisli category, where monad sequencing is (basically) composition in the Kleisli category. This is important because anything that forms a category can be composed seamlessly. I'm confused, I have always thought that [monads do not compose](http://blog.tmorris.net/monads-do-not-compose/). What am I missing? 
That's a different kind of composition (namely, composition of type-constructors). I can explain what I mean by the Kleisli category. A category, `C`, is anything that has a composition function: (.) :: C b c -&gt; C a b -&gt; C a c ... and an identity: id :: C a a If you look in `Control.Category`, you can see there is a type-class that formalizes this: class Category C where (.) :: C b c -&gt; C a b -&gt; C a c id :: C a a All category instances must obey three laws: * Left identity: `id . f = f` * Right identity: `f . id = f` * Associativity: `(f . g) . h = f . (g . h)` The obvious category is the category of Haskell functions: instance Category (-&gt;) where (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) (f . g) x = f (g x) id :: a -&gt; a id x = x f . id =&gt; \x -&gt; f (id x) =&gt; \x -&gt; f x =&gt; f id . f =&gt; \x -&gt; id (f x) =&gt; \x -&gt; f x =&gt; f (f . g) . h =&gt; \x -&gt; f (g (h x)) =&gt; f . (g . h) There are lots of other things that form categories, and the one I was referring to was the Kleisli category, which is: type Kleisli m a b = a -&gt; m b instance Category (Kleisli m) where id :: a -&gt; m a id = return (.) :: (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) (.) = (&lt;=&lt;) (f &lt;=&lt; g) x = f =&lt;&lt; g x This must obey the three category laws: return &lt;=&lt; f = f f &lt;=&lt; return = f (f &lt;=&lt; g) &lt;=&lt; h = f &lt;=&lt; (g &lt;=&lt; h) On closer inspection, you'll realize that these are just the [monad laws](http://www.haskell.org/haskellwiki/Monad_Laws): f &lt;=&lt; return = f =&gt; \a -&gt; f =&lt;&lt; return a = f =&gt; f =&lt;&lt; return a = f a -- The first monad law return &lt;=&lt; f = f =&gt; \a -&gt; (return =&lt;&lt; f a) = f =&gt; return =&lt;&lt; f a = f a =&gt; return =&lt;&lt; m = m -- The second monad law (f &lt;=&lt; g) &lt;=&lt; h = f &lt;=&lt; (g &lt;=&lt; h) =&gt; \a -&gt; (\b -&gt; f =&lt;&lt; g b) =&lt;&lt; h a = \a -&gt; f =&lt;&lt; (g =&lt;&lt; h a) =&gt; (\b -&gt; f =&lt;&lt; g b) =&lt;&lt; m = f =&lt;&lt; (g =&lt;&lt; m) -- The third monad law I wrote `(&gt;&gt;=)` and `(&gt;=&gt;)` using their flipped versions so you can see the analogy to function application and function composition: f x ~ f =&lt;&lt; x f . g ~ f &lt;=&lt; g So the monad laws are just the category laws in disguise. Let's use a concrete example, the `Maybe` monad: return = Just (f &lt;=&lt; g) = \x -&gt; case g x of Nothing -&gt; Nothing Just b -&gt; f b f &lt;=&lt; return = f &lt;=&lt; Just = \x -&gt; case (Just x) of Nothing -&gt; Nothing Just b -&gt; f b = \x -&gt; f x = f return &lt;=&lt; f = \x -&gt; case f x of Nothing -&gt; Nothing Just b -&gt; Just b = \x -&gt; case f x of f x = \x -&gt; f x = f -- I'll leave associativity out, but it works Monad sequencing is equivalent to Kleisli composition. To see why: f &lt;=&lt; g &lt;=&lt; h = \x -&gt; f =&lt;&lt; (g =&lt;&lt; h x) = \x -&gt; do a &lt;- h x c &lt;- g b f c In other words, composing `f`, `g`, and `h` is equivalent to sequencing them within a do block. This is why you can transparently combine as many statements as you want into a single do block. Anything that forms a category can be transparently combined into a single "thing". Things that don't form categories tend to become more complex the more of them you combine together, while things that do form categories stay the same no matter how many of them you combine. This is why Haskell programmers use categories as the "ultimate design pattern", because it represents an abstraction that scales limitlessly.
I'm not quite sure what it does or what the goal of it is. Also, I thought some people had done work on dependency management using arrows.
I contemplated that, but since F# is much less of a pure language it would have allowed me to cheat instead of actually learning the paradigm. And Haskell seemed "cooler" for lack of a better description
Perhaps I should, haven't heard of it at all
When you come into Haskell as a rookie, you get the impression that there is this shadowy incomprehensible thing looming just out of sight called monads. This also makes it incredibly attractive to learn, since it's apparently so hard. I think I read this text, or skimmed it, then went back to mucking about trying things until they worked. I'm really out to do stuff, and to take lessons home with me that will make me better at programming in general - regardless of language. If monads will help me do that, I'll learn them.
I went there, and was incredibly attracted to the scheme/haskell hybrid. http://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours In fact so attracted that tried and failed it. Whoever put it under best places to start has a very different view of the world than mine. I still want to do it, it sounds awesome, but it is definitely not a beginners text.
link: http://eclipsefp.github.com/
Well, all things considered, ML is probably the closest thing there is to Haskell besides Haskell (not counting Miranda, which Haskell basically copied, and Haskell-grown languages like Agda and Idris)
The typical recommended Haskell learning path is [LYAH](http://learnyouahaskell.com/) then [RWH](http://book.realworldhaskell.org/), with whatever additional tutorials you are interested in sprinkled around. Both are freely available online. Theoretically [Mezzo Haskell](https://github.com/mezzohaskell/mezzohaskell) will be the next recommended book, but it's far from complete as of now.
In the pre-Hackage days, I found SDL to be one of the easiest FFI bindings to install on Windows. As the SDL project supplied a DLL you could just build the binding with Cygwin, you didn't have to build the original C library with MingW / MSYS. I don't know what the situation is like nowadays. 
Juicy.Pixels author here, what exactly didn't you understand in the library? I'm planning some helper functions for image writing, because they're missing for a simple use, any user input is welcomed :)
For any and all libraries: short and to-the-point examples of how to use it are the most useful thing ever.
The problem is that nothing can stop Freenet from being a memory and CPU instensive operation. Encryption, decryption, asynchronous data transfer, lots of threads, routing, querying. *Why* it needs memory and CPU is obvious. That equivilant programs written in other languages require less memory and execute a little faster... all that really does is remove the bottleneck in performance, but i think it might be a substantial bottleneck.
They actually are rather related. They are both just sequence applied to different monads and the JK morphism which converts from his J monad to K (nee Cont) from Escardo's paper commutes with sequence (on finite lists).
I figured as much, that the lazyness would explode on you. I probably need to read up on it, I think some performance problems I've been having is due to this. I really like your short sample, it seems like this would do pretty much exactly what I want to do in a simple raytracer. Thanks! And regarding the samples and github, If you don't store the docs on github at least add a link to the place where the documentation is found. That would have helped me out, coming from a non-haskellian environment I have no idea that Hackage is the place to look.
Is there a name for this idiom?
&gt; functional community has to yet to proove itself to the world. The functional community can point to numerous things that "the world" will consider much more proof of FP's value than any conceivable Freenet client, when few people have even heard of Freenet. The real stopper for FP at this point isn't proof that it works, it's the next five or ten years that it will take for the proofs that imperative doesn't work (as well as advertised) to get out into the collective consciousness.
I'm not sure how or why this became a "non-programmer designer type people using WYSIWYG tools" thing. Everyone here who does html of any sort is also at least a junior programmer. Nobody here uses a WYSIWYG tool of any sort. Everyone complained about hamlet not working with their tools. Some people are using emacs or vim, others are using some windows or mac editor or IDE. Everyone wants and expects basic things like syntax highlighting, jumping to matching tags, selecting blocks by tag, etc to work. None of that works with hamlet unless someone makes a hamlet-specific plugin for all their editors/IDEs. This is a big benefit of heist, all of that stuff just works everywhere.
Thanks for the explanation, it's going to take me some time to digest it. Quick clarification: &gt; (f &lt;=&lt; g) x = f =&lt;&lt; g x Is this correct or did you mean (f &lt;=&lt; g) x = f &lt;=&lt; g x ? If it is correct, what's `=&lt;&lt;`, which I don't see introduced in your explanation? 
That's one of the case were using type inference instead of enforcing type annotations helps spotting potential bugs. See also the 1994 classic essay by Andrew Koenig: [an anecdote about ML type inference](http://static.usenix.org/publications/library/proceedings/vhll/full_papers/koenig.a)
&gt; TL;DR: (see above) ಠ_ಠ
Are there any resources that precisely describe the behavior of a well-mannered Freenet node? I really don't want to have to wade through a huge pile of Java code to try and figure out how to act like a Freenet node.
Presumably =&lt;&lt; is the flipped version of &gt;&gt;= (i.e. bind).
code and data embedded together? Sounds like an object.... Let the confusion begin!
The lunch Rust wants to eat is C++'s.
The original was correct. `(=&lt;&lt;)` is the flipped version of `(&gt;&gt;=)`: f =&lt;&lt; m = m &gt;&gt;= f ... or: (=&lt;&lt;) = flip (&gt;&gt;=) `(&gt;&gt;=)` is the monadic bind. To understand what role it plays, any `do` block is just syntactic sugar for `(&gt;&gt;=)`: do x &lt;- m1 y &lt;- m2 m3 = m1 &gt;&gt;= (\x -&gt; m2 &gt;&gt;= (\y -&gt; m3 ) ) If you want to learn more about monads, I highly recommend you read [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html), which not only does a good job of explaining monads, but also explains how `(&gt;&gt;=)` relates to `do` notation.
idletoad: Are you Matthew Toseland, one of the freenet developers?
My mistake about not seeing StateT. My mobile browser cut off the Make type definition. To answer your question: yes. If your functions change behavior based on a configuration without an explicit reference to that configuration, then your functions are not referentially transparent. The simplest fix is to make them impure by using IO, to reflect the fact that the are not referentially transparent. 
How did Ketil sneak "Alignemnt" past the compiler? 
Related, but rather different. Koenig's note is an example of where the wrong type was more polymorphic than intended. Here, the "wrong" type is less polymorphic. You could have used an explicit annotation here, assuming you wrote a polymorphic type as opposed to a concrete type.
I wouldn't say that's so obvious. IO doesn't take much CPU, if it's done well. Encryption, *maybe*. But modern CPUs have AES-specific instructions, so one thing to check is that freenet is using those instructions. AES is still somewhat expensive-ish, but a lot less expensive than it was in the past.
tl;dr: Using a polymorphic type stops you using properties of the monomorphic type that aren't relevant, and thus probably incorrect. I would have thought some use of newtype might be appropriate here though?
&gt; But I did not come to redd.it for silly arguments about definitions. Dude, you are wrong. It's not a silly argument, it's a fundamental misunderstanding you have about a key Haskell concept. If "x :: Reader Int Bool" and "y :: Bool -&gt; Reader Int Char" and I go "x &gt;&gt;= y", please explain to me how that is not referentially transparent.
All expressions in Haskell are referentially transparent. The compiler always operates on this assumption. The context is threaded transparently. Monads just provide some nice combinators to hide it from view. A State s a is just a function from s -&gt; (a, s). This function is referentially transparent. All Haskell functions are (unless you use unsafePerformIO or similar).
Doing it alone might not be possible as we have many other engagements . If people are agreeing to collaborate , then I think we can get it going. 
If you reword dmbarbour's statement as "*statements* within `Reader` are not RT", then it makes perfect sense; an individual `Reader` action can take on many possible values depending on the context it is supplied with. (Admittedly, I am more confident in the equivalent statement about `State` than I am about this one.) That doesn't make Haskell itself not referentially transparent; it just makes the `Reader` EDSL not referentially transparent. After all, the reason we use monads in the first place is to extend Haskell with embedded languages with various effects that would be impure were they part of Haskell proper.
&gt; annotation that says "the type is not more polymorphic than" I'd love to have those annotations. The use cases are far less common, but when you want them they'd really help with the verbosity of using advanced type features.
idletoad is me. I created a new account so if a flame war broke out, my name wasn't involved. But i lost the password and feel pretty safe right now. I'm just a regular reddit user who thinks that Freenet needs competition. I wish i was one of the devs--it'd make re-implementing easier. 
I am new to Haskell myself, but I would say that the best place to start with Haskell is not actually with the language itself. I think you have to start with SICP: http://mitpress.mit.edu/sicp/ and try to learn functional programming as a paradigm separately from an implementation. I also don't really understand how an IDE can help a beginner learn a language. I have always used Vi, but my short experience using Visual Studio when I was forced to work in a Windows environment was excruciating. I found that IDEs let you write a lot of code really fast without know what the hell is going on. They are good for languages like Java where you have a ton of boilerplate but for most languages they really take you away from actually writing code.
I've also been looking for something like this. hope you find one :)
Thanks I'm going to keep searching and if I find one before anyone else posts I'll be sure to send it to you.
Ok, was just wondering because Matthew Toseland, the main freenet dev aside from Ian, goes by the nick toad on IRC. I completely agree with you. I've been trying to use freenet and following the freenet project since the beginning which is now over 10 years. All these years and they still have hardly anything to show for it. And a lot of it is because of java. My freenode would run out of RAM and get into all sorts of trouble all the time. I don't think I've ever had one up and running for more than a day or two before it crashed. I haven't played with it in a few years now. I've pretty much given up hope. I really do hope someone comes along with a good DHT or something similar to freenode because the world really needs it.
http://www.haskell.org/haskellwiki/Modern_array_libraries#Mutable_arrays_in_ST_monad_.28module_Data.Array.ST.29 It's not detailed, but it's correct. In my experience, when someone says they have trouble with arrays, what they really mean is that they have trouble managing mutability with monads in general, and arrays are the first case where they can't get by with persistent data structures. (For example, they aren't comfortable with MVars or STRefs either.) To solve that problem requires study and practice with monadic types to get the hang of it. If you want to be quick and dirty, you can just work in IO and use IOArrays (boxed or unboxed) just like arrays in impure programming languages. If you are comfortable with Data.Array in the small but find that you can't shoehorn your arrays into the rest of your program, this page will help you pinpoint why that is, and guide you to an appropriate alternative array library: http://nix-tips.blogspot.com/2011/03/how-to-choose-haskell-array-library.html
I don't see any disagreement about that (well, actually, you're slightly wrong; `get` and `set s` don't *return* `State` actions, they *are* `State` actions). dmbarbour's point is that, *within the context of the EDSL defined by the `State` monad*, we cannot treat such expressions as referentially transparent: the `State` EDSL is not referentially transparent, even though it is embedded within Haskell, which is. Consider the strong resemblance of applicative-style code to pure functional code; if you ignore the `(&lt;$&gt;)` and `(&lt;*&gt;)` noise (which are part of the embedding in Haskell, not the EDSL itself), then the difference is between code in a purely functional language to one with specific impure effects (such as mutable state). The latter admits less refactoring, because it is not referentially transparent.
Well the procedure names are, yeah. But even "put x" will always return the same function every time, given the same "x". I don't see how you can say that put expressions are not referentially transparent. 
Why do you need mutable state for this? Why not just have the game run as a recursive function that calls itself with the new version of the board? I think if you are trying to use mutable state you should try to think harder about how you could do it in a more functional style first and then if that seems impossible then go to something mutable. I doubt you will need mutable state for this.
Yes of course, I was a bit terse. I meant only that one should probably try not to use mutable state first and only then go looking for the alternative. By all means it would be good to have documentation for them, but it seemed that no one was pointing out the alternative. 
I didn't look for mutable state first; I determined that mutable state was better after writing out a couple of functions that would have required writing a lot more code than normal.
If you aren't running a high speed simulation, and only need interactive performance with a slow human player, you could probably get acceptable perform if you just use immutable arrays and reallocate the whole tiny 400-element board array at each step. Alternatively, DiffArray, the much maligned half-breed between mutable and immutable arrays, will probably be fast enough for your application while preserving a pure non-monadic (aka easy-to-use) API. 
Games, which many times are simulations, have an ever changing state. When you're changing thousands of things 30-100 times per second, you need mutable data.
I'm working on a Go game in Haskell myself. I didn't do too much yet because lack of time, but I did start to implement a board once using Data.Map and once using STUArray so I could later benchmark them and see the difference. I've implemented a flood-fill to find a group of stones in both ways, and now I uploaded it to hpaste because you might find it useful :) http://hpaste.org/67018 http://hpaste.org/67019 We can maybe join forces if you want to. I'm a Haskell beginner-Intermediate (not written any useful programs).
In pure TeX I don't know but in pdfLaTeX you can include PDF files using the regular includegraphics command.
I just tried an IOUArray, but it appears to have the same performance.
Honestly, unless I was trying to implement an AI and found that I needed better performance than that would give me, I would just use Data.Map for that (from board positions to stone colours). It'll turn out a lot nicer and doesn't involve mutation. It also has the nice property of representing empty board positions by simply not having those keys in the Map.
I disagree with the first point. When you want to use a package and there are no examples around, you will obviously look at the API in Haddock (what else are you going to do, guess?). However, it is a big waste of time trying to figure out how things work using only the type signatures and the function names. Short examples of how to use your package should be required or at least strongly encouraged. 
There are some people who (seem to) turn to asking on Stack Overflow before reading the haddocks. It's possible they don't know about the haddocks, or at least don't know where to find them.
As a side note, your set function is broken. The changes using `unsafeThaw` will modify the immutable vector's representation. If you use something like: v1 = V.fromList ['a','b','c'] v2 = set v1 1 'z' v3 = set v1 2 'y' main = do print v1 print v2 print v3 You get the output: fromList "abc" fromList "azc" fromList "azy" Notice that `v3` contains the change that should only be in `v2`. Basically don't use this `set` unless you're not going to reuse the old copy of the immutable vector again, but at that point, it'd better to just do the whole thing in ST.
I personally consider `MonadIO` bad style and I prefer `MonadTrans`, even if I have to write a lot of `lift`s. Also, these tips are more for slightly proficient Haskell programmers. I find that new programmers struggle the most often with: * Syntax (especially do block syntax) * Precedence (especially function application precedence) * Understanding compiler errors * Understanding how to nest different monads (with or without monad transformers)
We're in agreement here. I'm not arguing against tutorials at all. I'm just arguing that beginning Haskell programmers should be more willing to dive into API docs.
Tomorrow I have another post queued up that will discuss this in detail.
Great; thanks!
Try this [Kinds for C++ users](http://en.wikibooks.org/wiki/Haskell/Kinds) 
While I agree that even a single line of documentation can be extremely valuable, I disagree that "it is a big waste of time trying to figure out how things work using only the type signatures and the function names". In fact, this can be a great exercise for new Haskellers. It is super important that new Haskellers get used to leveraging the type system to its fullest extent, because it really can provide them with a *lot* of help.
I briefly touch on this as part of my (gasp!) monad tutorial: http://unknownparallel.org/monads.php Yes, I wrote a monad tutorial. Let he who is without sin cast the first stone.
The problem with Monad Trans is that if later you add or remove another monad into your construction, you then have to revisit and fix all those lift . lift chains. LiftIO provides a convenient shortcut. 
I'll probably try to see how an array would perform against a set, just because I really like comparing that kind of stuff. And the explicit stack is mainly because I've seen something on Wikipedia about using a queue instead and a slightly modified algorithm to get a lower stack/queue usage so I'm going to try that too. Thanks for sharing your experience. 
No problem. I'd love to see it when it's done :)
I agree that it is a great exercise. I still don't want to do it if I only want to find out how to use a library.
Arrrrgghhhhh it's September, all over again.
If I want a lift that invokes a particular monad transformer level, I define one: liftIO = lift . lift . lift and then use that. If I want to add another layer, I just add another lift
A square bracket is a function at the type level. It has one argument, a type, and it returns a new type. In this case its argument is the element type and its result is the type of the list. We actuall call things like brackets type constructors instead of type functions. Type constructors are just a special kind of type function that lets you pattern match against it to retrieve its argument, in the exact same way that value constructors are just a special type of ordinary function that lets you pattern match against it to retrieve its argument. The reason Haskell has only type constructors and not general purpose type functions is that the compiler has to be able to uniquely be able to determine type equality, and equality of general purpose functions is undecidable.
I think we could make a generic board game library that we'd release on Hackage. I really think there are elements that could be generalized between Go, Chess, Checkers, &lt;add board game name here&gt;... As it is a classic programming example, that would benefit a certain amount of haskellers. We should make a committee or something to determine the elements to be handled ;-)
Very nice, very constructive.
My boyfriend runs Ubuntu alongside his Windows for Haskell development and it works really well. My main OS is OS X, but I have Linux boxes for development as well.
I strongly dislike the `mtl` style. The most important reason why I dislike it is that there is no theoretical basis for it at all. With `MonadTrans`, the sound theoretical basis is that `(lift .)` defines a functor (which gives rise to the `MonadTrans` laws, which are just the functor laws). The `mtl` implementation is essentially ad-hoc with no laws that constrain its behavior and define a "correct" implementation. If I write a monad transformer using the `mtl` style, I have no way to prove that it is correct because there are no laws it is expected to satisfy. Second, in many cases the `mtl` style requires undecidable instances and functional dependencies to work, both of which are questionable extensions to the language. Also, for the narrow range of use cases that you have the `mtl` style might work for you, but what about if you wanted to stack two `Pipe` monad transformers (i.e. you wanted to embed a stream processor within a stream processor). You can't do that with the `mtl` style because you can't embed more than one instance of each monad in the class constraint. The overall point is that when you deviate from a sound basis in category theory, bad things happen and the abstraction breaks down rapidly (as it does with `mtl`). In fact, I just thought of another example of how badly the `mtl` style behaves. The other day I was toying around with making `Pipes` use the `FreeT` transformer found in the `control-monad-free` package. The problem is that it is written in the `mtl` style, so when I tried to write the code for the `yield` function, it should have been: yield x = wrap $ Yield (x, return ()) But sadly, this would not type-check **even when I provided the correct type signature**: yield :: (Monad m) =&gt; b -&gt; FreeT (PipeF a b) m () In fact, the only way I was able to get it to type-check at all was to insert a very carefully crafted and awkward type signature in the middle of the code: yield :: (Monad m) =&gt; b -&gt; FreeT (PipeF a b) m () yield x = wrap $ ((\b -&gt; Yield (b, return ())) :: (Monad m) =&gt; b -&gt; PipeF a b (Pipe a b m ())) x That's the only thing that works. I'm sure that with some effort I could construct a function that is completely incapable of type-checking at all, despite being valid code. Then I proceeded to write my own `FreeT` implementation in the `transformers` package style using ordinary `MonadTrans`. My yield function then type-checked immediately without even a type signature. This is just one of many reasons why the `mtl` style is a huge problem and why it's incredibly dangerous to stray from theoretically sound territory.
"Well, you said that you couldn't use the mutable vector (this is now different in your post, did you edit it, or did I misread it?). You also can't use the immutable one. Perhaps I just misunderstood :)." I fixed it. "However, even wrapping it in a newtype you can't really protect anyone from re-using it after you've set it." Yes, that's true. The right thing seems to be to put the whole application into the ST monad, if one vector copy per set is undesired. Kids, don't play with unsafe functions. 
No, `Identity` can be at the bottom of the stack. It's also not a huge maintenance burden to add or remove a lift. Compare that with the `mtl` style, which imposes a maintenance burden on the library writers that scales quadratically with the number of monad transformers and requires every monad transformer library to couple to every other monad transformer library ever written in the past, present, and future.
i'm happy about this. libraries are there to help me. functions i'm not using now aren't hurting me that said, it can be a downside to a type system like haskell...there are numerous instances of functionality recoded for novel types
I can see a downside. It can make it more confusing which library to use for a specific task. I think that slowed me down when it came to arrays, for instance. (If I'm understanding the point correctly)
It's more true regarding syntax to say that Haskell is column-oriented. For example: do a b means do { a; b; } but according to &gt; if the code on the next line is indented further in, it's treated as a line continuation it should mean do { a b; } (which it does not.) Similarly, do let a = b c is a parse error because a line continuation must be indented further than the *a*, not the `let`. This is not just with the specialized `let` in `do` notation; this too is a parse error: let a = b c in d
Most syntactical constructs are just syntactic sugar. The really important ones are just how you define a function and case statements. Everything else is really just built on top of that. For example, guards, if-then-else, do notation, etc., are all built on top of those core concepts. The Haskell library base is huge, and that's a very good thing. The important part is learning the core abstraction libraries and then whenever you need a specific functionality you just google "hackage &lt;functionality&gt;" or use Hoogle/Hayoo.
&gt; But if you think about it a little more, you'll realize that this function has to block. If it didn't block, then there might not be a value and there would be no way to return something of type a (because Haskell has no null pointers). It can return "undefined". That would be very stupid of the API developer, but actually nothing from the signature readChan :: Chan a -&gt; IO a tells you that it cannot return undefined. 
Also outside of do notation, lines can often be continued without being indented further: f = a b c means f = a b c
Yeah, I explained it completely wrong.
To expand on this commenter's initial point, notice that many tutorials have you reimplement library functions as part of the learning process but then note that the library functions are faster. A good illustration of the design elegance versus performance goals issue, I think.
I think one of my biggest difficulties learning Haskell is just knowing what exists in standard libraries, and then also knowing what *doesn't* exist for the reason that there's a perfectly simple way of doing it. A recent one I discovered was splitting strings and joining lists into strings. Coming from Python, you've got `str.split(delimiter)`, where you can supply your own delimiter and it happens, and then the reverse with `delimiter.join(some_list)`. After digging around, it looks like these things do exist in some libraries, but it's also really easy to just make your own function that works just as well using pattern matching or folding. Part of solving the problems in instances like these (maybe someone has an even better one than splitting and joining things) isn't necessarily being able to navigate and search through documentation, but just adopt a different set of thinking you haven't acquired yet if new to a functional language.
Actually, it becomes: a &gt;&gt; b &gt;&gt; c &gt;&gt; d but you are otherwise right of course.
I will do some syntax translations: if b then e1 else e2 ... translates into: case b of True -&gt; e1 False -&gt; e2 Guards: f x | predicate1 x = e1 | predicate2 x = e2 | otherwise = e3 ... translates into: f x = case predicate1 x of True -&gt; e1 False -&gt; case predicate2 x of True -&gt; e2 False -&gt; e3 Monads: do x &lt;- e1 y &lt;- e2 x z &lt;- e3 x y e4 x y z ... translates into: e1 &gt;&gt;= (\x -&gt; e2 x &gt;&gt;= (\y -&gt; e3 x y &gt;&gt;= (\z -&gt; e4 x y z ))) Lists: [1, 2, 3, 4] ...translates into: 1:(2:(3:(4:[]))) Infix operators are just ordinary functions and can be written like so if you parenthesize them: 3 + (4 * 5) ... translates into: (+) 3 ((*) 4 5) List comprehensions are just syntactic sugar for the list monad: [(x, y) | x &lt;- [1..10], y &lt;- [1..5]] ... translates into: do x &lt;- [1..10] y &lt;- [1..5] return (x, y) Enumeration syntax is just syntactic sugar for calling `enumFromTo`, which is defined for anything that implements the `Enum` typeclass. ['A'..'Z'] .. translates into: enumFromTo 'A' 'Z'
&gt; The most important reason why I dislike it is that there is no theoretical basis for it at all. With MonadTrans, the sound theoretical basis is that (lift .) defines a functor (which gives rise to the MonadTrans laws, which are just the functor laws). The mtl implementation is essentially ad-hoc with no laws that constrain its behavior and define a "correct" implementation. If I write a monad transformer using the mtl style, I have no way to prove that it is correct because there are no laws it is expected to satisfy. You can still implement your `MonadX` instances in terms of `MonadTrans` and `lift`. It merely shifts the implementation of what you would do anyway in your `liftIO = lift . lift . lift` helper functions, but generically as a method in a class. I think we agree on this point, but that my wording doesn't make that clear. I'm not against `MonadTrans` and stacking, the opposite, but restricting your monads to be monorphic limits people's use of them, and throws away the advantages of classes I gave. Using your `Foo (Bar (Mu (Zot a)))` and `lift . lift . lift` concrete type stack, how can you specify at the type-level that you want to use (or will only allow) Bar but not Foo, Mu or Zot? &gt; Second, in many cases the mtl style requires undecidable instances and functional dependencies to work, both of which are questionable extensions to the language. Sure, `MonadState` requires functional dependencies. I'm not sure how questionable functional dependencies are. &gt; Also, for the narrow range of use cases that you have the mtl style might work for you, but what about if you wanted to stack two Pipe monad transformers (i.e. you wanted to embed a stream processor within a stream processor). You can't do that with the mtl style because you can't embed more than one instance of each monad in the class constraint. I don't have time to produce a real example as I don't know the API. Here's one for `MonadState` I already have pre-written: demo = flip runState ('a','b') $ flip execStateT ('a','b') $ do foo ('a','b') demo2 = flip runState 'a' $ flip execStateT () $ do bar () 'a' foo :: (MonadTrans t,MonadState s (t m),MonadState s m) =&gt; s -&gt; t m () foo s = do put s; lift (put s) bar :: (MonadTrans t,MonadState s (t m),MonadState s' m) =&gt; s -&gt; s' -&gt; t m () bar s s' = do put s; lift (put s') This demonstrates two different monads with transformers, but retaining genericity. It makes sense that this is correct as it's just using the MonadTrans class. Are you saying this approach breaks down with the pipes type? &gt; yield :: (Monad m) =&gt; b -&gt; FreeT (PipeF a b) m () &gt; &gt; But sadly, this would not type-check even when I provided the correct type signature Sounds about right, type inference has limits here. Though I need more time to look at the types properly. I would've started by implementing MonadFree for PipeF a b, I think, but this pipes library is new to me. Maybe I'll come back to it later and answer your specific examples properly. Admittedly I had trouble generalizing MonadCont, I believe that is inherent in the method types and doesn't allow for generalizing, at least in Haskell. That's a case where I'd have to use a concrete transformer type.
Registered to respond since I'm one of the reasons this was written. I already do 1 and 2. The problem isn't that I don't know Snap is an instance of MonadIO, it just that piece of information doesn't help me at all. Either haskell people don't realize how exceptional they are, or they forget what it is like to be new. I have a solid understand of CS fundamentals, a decade of experience programming, and am comfortable with functional programming having used ocaml and then scala for a few years now. I've gone through LYAH and have played with haskell on and off for a few months. I have absolutely no clue what I am doing when things get into monads, which is basically anything beyond trivial problems like project euler. The reason I am playing with snap in the first place is because I need something practical to work on to actually have things sink in, just reading doesn't do it for me. Which brings me to number 4: I am trying to learn, that's why I am asking questions. If I could understand it just from reading then I wouldn't be asking. Saying "just learn stuff" to people who are trying to learn stuff is rather discouraging. I don't know how many people this is true for, but I've found learning haskell quite hard. Getting the basics is easy, but the leap from using haskell for purely functional little crap in GHCi to doing something practical with the language is massive and feels entirely unapproachable. That's always the reason I've heard from people who gave up trying to learn haskell too. Please try to be patient with those of us who are trying to stick it out, and remember that just because something is obvious to you, doesn't mean it even holds any meaning for me. PS: what is up with your blog being a javascript thing now? Did blogspot do that or did you?
I see. That simplifies the code a little. Thanks again.
I certainly wasn't trying to single anyone out here. I'm also not trying to say that the four things I'm suggesting here will be easy. If my post discouraged you or came across as a criticism of people I've helped, then I sincerely apologize. I've just seen a LOT of people asking about MonadIO, and it just struck me that that suggests that the Haskell community has failed to communicate these issues on some level. This post was meant as a concise road sign to point people in the right direction. Some of my more in-depth explanations of this stuff are coming in the next few days. You do bring up a good point though. Some of the common type classes you'll find in instance lists will be fairly hard to understand at first. The infamous Monad of course, Applicative, and MonadCatchIO are some that come to mind as more difficult. I'll even grant you that beginners may not immediately grasp what the liftIO function's type signature means to them, and how they should use it. For an outstanding resource to help in understanding these concepts, check out Brent Yorgey's [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia). Haskell is a hard language, but it has one of the most patient and friendly communities I've seen anywhere. I cant' tell you how much time I spent in IRC asking what I knew were probably stupid questions, and Haskellers were incredibly longsuffering with me. I try to do the same thing. But it seems prudent to create resources that might help answer some of the most common questions before they're asked. The theme change was my doing. I decided that the old theme looked a little old and replaced it with the one I thought looked best. EDIT: Well, I got enough complaints about the javascript that I changed to a different theme. It's sad that the best looking blogger theme is so javascript heavy.
May I suggest ` `[ι](http://en.wikipedia.org/wiki/Iota_and_Jot)` = \a -&gt; a (\x y z -&gt; x z (y z)) (\x y -&gt; x)`
I tried [basically the same thing](http://www.reddit.com/r/haskell/comments/sb80y/why_is_haskell_so_large/c4cssjt), and the typechecker might not actually like it when you try to use it. And of course there's also the problem of expressing the fixpoint combinator...
I believe "if ... then ... else ..." exists because it's more readable than "if (cond) (branch1) (branch2)", you don't have to put parenthesises for instance and it's nicer when then expression is big.
There's another annoying issue with the MTL approach. That of O(N^2) instance explosion (Every conceivable monad-capability-class needs to be instantiated by every conceivable monad).
Please, please define both a background and a text color in your CSS. Thank you.
Peg is at an early stage of development. I'm looking for opinions and related research. Although it is not yet that useful, I think it's unique and fun to play with.
Try comparing the API of Data.Map with that of Java's TreeMap. Some things are similar, but some things are missing because they just don't make sense in Java or are very inconvenient to use (e.g. higher-order functions). Functions can be about more things, so there are more functions about these things.
Tomorrow. EDIT: Here's the [link](http://softwaresimply.blogspot.com/2012/04/ltmt-part-2-monads.html)
One issue I had with HXT recently is that readString constrains the arrow to run in IO. Fortunately hread doc or constA doc &gt;&gt;&gt; xread can run purely, however, only readString allows for options such as permitting HTML parsing. I'm sure there must be another pure options arrow or something that will allow pure HTML parsing, but finding your way around HXT is quite a challenge (especially if you're short on time).
With respect to IO, your "IO token" approach can work but you need to restrict it somehow if you want to maintain purity. Have you considered uniqueness or linear types? I think they're both fairly well-suited to concatenative languages because you don't have partial application complicating things all the time as you would in an applicative language with curried functions (e.g. Clean). Linear types in particular can provide a rather elegant solution. As for your "explicit type checks", I'm unsure how you could get your suggested approach to work well. On the plus side, concatenative languages can be typed with standard techniques by simply viewing the stack as a series of (right or left) nested pairs with some standard (similarly right or left) element representing the empty stack. I suspect you may already be aware of this but I can fill in details if you have questions. All that said, I'm curious if you can motivate the choice of applying laziness to concatenative programming. (I realize I may be in the wrong subreddit to ask that.) 
See my 'pipes' package. The data type is equivalent to a Free monad. In the next patch it will even be written that way.
Please do it!
Every FooT transformer needs to instantiate the classes you'd want to use, yeah. All instances for monads defined in terms of those can be derived, so the user isn't inconvenienced. Newtype-derived instances have no performance hit AFAIK and I think the end result would equate to writing `lift . lift . lift`. Are you saying there will be a compiler performance hit? Another question is how many layers one has in practice.
How would the IO token need to be restricted for purity? The only problem I'm aware of is a word that does not return its IO token, but I plan on causing that to destroy the thread, so I'm not sure if that would technically be impure. I haven't started work on the type verifier, but I don't expect it will be easy. The nice thing with explicit checks is that the verifier could fail by just giving up and emitting a warning, and run time checks are still in place. Ideally, though, a result of "no" should be impossible when the program passes the verifier. I am aware of other attempts to statically type stack languages, but these approaches seem to restrict the language too much. I want the language to have the feel of a dynamic stack language, but still give the strong compile time guarantees of a statically typed language. Since references aren't possible in Peg, linear and uniqueness types aren't needed. I was motivated to use laziness because it is useful in Haskell. That said, laziness has some neat effects on Peg. An easy example is the implementation of `enumFromTo`, which generates an infinite stack with `iterate` and then uses `take` on the result. If you look at how `foldl` is implemented, it recurses by applying the function argument to the top of the stack argument and uses `dip2` to set up the stack underneath to calulate the rest. This general approach to recursion allows computations to proceed lazily. Another interesting thing is that any stack at a specific point in time can be divided into the evaluated part to the right, and the unevaluated part to the left. This makes reasoning about laziness easier than in Haskell.
`hread` parses the content as HTML. Maybe I should add this to the guide.
Whenever you write any monad to represent some abstraction you have to make a decision between making it opaque or transparent. This is a tradeoff. An opaque type lets you change the underlying implementation without affecting your user, but doesn't really allow the MonadTrans style. A transparent type lets you use the MonadTrans style to access functionality further down in the stack without implementing a bunch of type class instances, but it exposes your internal representation and means that refactoring the internals will be more likely to impact your user. Neither of the two styles are a silver bullet. Which one you use totally depends on the situation.
Not all libraries need to implement support for every monad transformer. For some, the mtl style is more appropriate.
By opaque I mean does not expose it's internals...i.e. you can't see through the type constructor. Consider the following example abstraction: newtype Foo a = Foo (ReaderT Int (WriterT [String] IO) a) deriving Monad If I use the MonadIO approach, I can export Foo, avoid lift sequences, and my user will be insulated against future refactoring. If I export Foo(..) and require the user to use lift . lift, then future refactoring might break more code.
One comment, if you don't mind. You could be clearer on the relationship between arrows and functions -- namely, that functions *are* arrows, but there are also some other things that are arrows, but are not functions. Looking at your example: -- makes the arrow version of odd: oddA = arr odd :: (-&gt;) Int Bool This is misleading. The type `(-&gt;) Int Bool` is just the prefix syntax for `Int -&gt; Bool`, just like `(+) 2 2` means the same thing as `2 + 2`. And `oddA` is not an "arrow version" of `odd`. It *is* `odd`; the two names `odd` and `oddA` now refer to the *exact* *same* *function*. Because you asked for the result as a function, `arr` just gave back the same function you passed in. To further belabor the point: -- it works like a regular function! ghci&gt; oddA 5 True This has nothing to do with arrows, and in fact it only works because `oddA` is a function... in general, you can *not* just apply arrows to arguments and get results like that! You can do a *limited* version of it using the `ArrowApply` class, but not all `Arrow` instances are instances of `ArrowApply` at all.
It's actually implemented in Haskell. It is only influenced by Prolog.
Each IO token is unique. You make a good point, though. I was planning on using `dup` to duplicate IO tokens, representing spawning a new thread. This would be an abuse of `dup`, though, because `IO dup` would push two unique items onto the stack. I will need a new word for this.
&gt; There are N lifter classes, and M monad transformers. You need MN instances to be either written Yeah, that's what I said in my first sentence. :-) But with the qualification of what you, the user would want to use. So either the library writer or the user can implement, if they want to be derivable. &gt; Even if they are derived, it's a problem of orphan instances or coordination. By deriving I meant users, not library writers. But yeah, there is always the tension of library-defined instances or user-defined instances, in libraries other than those that define transformers. &gt; The amount of code should scale linearly with the complexity of the problem, not more than that. Agreed, and library writers can forgo implementing all the instances for their transformer type, provided their functions are defined in terms of `MonadFoo` and they export `MonadFoo`. Then as a user I implement the classes I want for my monad, including this one, as it should be. I did this for `MonadError` and `Applicative` and such for standard monads for ages. Some got added to base and I removed my instances. The fact mtl exports some helpful for me is just a bonus, I would otherwise have to write a couple instances (that's linear) for my particular `MyApp` monad, but this is separate to my central thesis described [here](http://www.reddit.com/r/haskell/comments/sarkz/four_tips_for_new_haskell_programmers/c4cq9fv) (ignored) and [here](http://wiki.chrisdone.com/Rebuke%20of%20the%20inflexibility%20of%20monads) (seemingly ignored) that code should be generic upon the monad.
Wow fairy tales look like this ... truly befitting our language :)
A function doesn't have to return its IO token -- if it doesn't, it destroys its thread, therefore never returning. Its return type in Haskell would be `bottom`, I think. Two identical subexpressions taking IO tokens can't exist, because each IO token is unique. My reason for being unsatisfied with existing attempts to type stack languages comes from the difficulty of finding types for many words that I have defined. I also want expressions to be as self-contained as possible, not relying on external data definitions. For example, you could define a function that returns a number or the word `NaN` without having to first make a data type for this explicitly. My approach would also allow for dependent types. The type checker would still catch if you forget to handle the `NaN`, for instance, because it tries to prove that `no` is impossible, which would be the result of `NaN 1 +`. `World` or `Process` might be a better name for `IO`, although I'd prefer something short. I agree that laziness can exist in the left side of each stack (the building block for richer data types), so you can't just draw a line between the lazy and evaluated part of an expression. I still think it's a little easier to understand, though, For example: 1 [ 2 3 ] [ 4 + ] . popr evaluates to: 1 [ 2 ] 7 The `1` has not been evaluated, the stack `[ 2 ]` has been evaluated, but contains a `2` which has not. The `7` has been evaluated.
I can't wait 'till I get to write my own monad tutorial! I wonder what I can spice mine up with...
Very nice job, and knowledge I can put to use right now. My one tiny suggestion would be to change your introduction of the `&amp;&amp;&amp;` combinator to make it obvious that the list isn't being mapped over. Something like this: length &gt;&gt;&gt; (odd &amp;&amp;&amp; (+1)) $ ["cube","glue","raincoat"] That makes it easier to see that the (+1) is being added to length, rather than to an element in the list itself. Thank you for doing this! 
That link looks like it is for the first part.
Thanks for all the replies guys; they really helped clear it up!
I will look at Kanren more closely, but I'm not sure what you're refering to. What elements of Prolog has Peg taken that you think are suboptimal? Really, Prolog only had minor influence, since there is no unification, because variables are not explicit in a concatenative language. The main thing taken from Prolog is backtracking, which is implemented with [logict](http://hackage.haskell.org/package/logict), which is based on a paper written by Oleg Kiselyov, who I believe is the author of Kanren.
Why? The plain text is fine and easy to read.
&gt; Ah, I shouldn't have asked. I'm not a fan of "all codata, all the time"; I'd rather have elective laziness. Codata-by-default and laziness-by-default are quite different things. It so happens that Haskell does both, though it's easy to get data instead of codata by adding bangs to the type definition (which is part of the Haskell Report, unlike bang patterns; unfortunately there's no provision in GHC for strict components when using GADT syntax). For example, you don't need codata to define `if_then_else_` as a function. Nor any similar "syntax" like `when`, `unless`, etc. Tying-the-knot and stream processing are another matter. Though, again, strictly speaking codata isn't necessary for those either; though you do need a type system which is capable of ensuring termination and finiteness of results, while still using lazy evaluation. (Unfortunately, AFAIK I'm the only one interested in that particular combination of features. Others who are interested in provable termination either don't care about operational semantics, or are unwilling to mix a terminating core with a more liberal whole.)
&gt; Its return type in Haskell would be bottom, I think. In Haskell at least, bottom is a value not a type. The type of bottom is `(forall a. a)` since via Curry--Howard bottom (as a proof) gives you the principle of explosion (in the logic). This is also why continuations claim to return `(forall a. a)`--- because they don't return. (Though technically continuations are covalues, not functions at all.)
I've always wondered if there was something "under" values...
What's wrong with defaulting to white and black respectively? I'm no CSS wizard, and [I did take a CSS tip from someone for my website before](http://www.reddit.com/r/haskell/comments/lskew/zeroanalogy_monad_tutorial/c2vc88k), but I don't see the benefit here.
Correct me if I'm wrong but it isn't really that proofs are 'below' types, it's just that if you want non-trivial proofs you need types that depend on values, so really this post is about faking dependent types?
It was, indeed, logict that I was getting at. The Prolog implementation of backtracking is failure-based, while Oleg's work is success-based, yielding much better (computationally speaking) results. After lengthy convesations with Dr. Friedman, it has become apparent that it is the correct way to handle it and so whenever I see the words "Prolog" and "backtracking", I have conniptions. You've clearly sidestepped my primary concern.
Do you think the associativity proof is non-trivial? I think types (higher level) do not depend on values (lower level) there. I took a non-dependently typed language, shifted everything one level up, and shown that values work as proofs.
Actually, I switched to logict after having problems using the List monad, so I've experienced the problems with depth-first backtracking first hand.
The simply typed lambda calculus is strongly normalizing, so you can't implement general recursion in it. The typechecker is also getting in the way of defining the s-combinator. Fortunately we can embed the untyped lambda-calculus into Haskell. data U a = V a | F (U a -&gt; U a) infixl 9 $$ V a $$ _ = V a F f $$ v = f v run :: U a -&gt; a -&gt; a run (F _) = id run (V a) = const a smurf = F $ \f -&gt; f $$ s $$ k where k = F $ \x -&gt; F $ \y -&gt; x s = F $ \x -&gt; F $ \y -&gt; F $ \z -&gt; x $$ z $$ (y $$ z) i = smurf $$ smurf k = smurf $$ (smurf $$ i) s = smurf $$ k V constructor exists to actually use Haskell values; for example: -- church encoding of natural numbers -- exercise for the reader: implement in terms of smurf fromInt :: Int -&gt; U Int fromInt n = U $ \f -&gt; U $ \z -&gt; make f z n where make f z = go where go 0 = z go (n+1) = f $$ go n toInt :: U Int -&gt; Int toInt n = run (n $$ F succ $$ V 0) 0 where succ x = V (run x 0 + 1) 
Laziness and non-determinism are pretty cool features of this language, but I worry that these features will cause significant bloat to both memory usage and runtime (compared to the same language without these features). If you're serious about getting Peg in embedded systems, you'll need to justify including these features and show benchmarks that prove they are not problematic.
 unsafeReadMyMind
Yes, I think this is an excellent way to extend ReadArgs, and a very clever usage of type-level Strings.
Looks like a mad scientist's castle... Yeah, that's got to be it!
I've implemented monads for `Maybe` and `Either` in `lib.peg`, but I had a hard time implementing `IO`. When you consider how easy the equivalent of the `State` monad is in a stack language (just leave your state on the stack), it seemed too complicated. I'm not saying that the monad approach won't work in Peg, it just didn't seem to be a good fit. I may yet change my mind, but the current approach to I/O seems cleaner and a better fit for the language, and also possibly a good interface for concurrency.
The hope would be that static analysis done during compilation will remove most of the non-determinism and laziness. Because I know the compiler can't fix everything, I intend to make the compiler interactive, so that the user can easily see where these things can't be optimized away. I may provide integration with VIM to highlight areas of code that are non-deterministic, lazy, or run-time typed (due to inability to statically determine the type.) I believe that garbage collection would be greatly simplified in this language, allowing for low overhead implementation suitable for a microcontroller. In my experience with embedded systems, memory is a bottleneck, but speed is not an issue, except for in highly optimized real-time loops that are only a small fraction of the code. I've been appalled at the inefficiency I've found in these systems (such as implementing multiplation of two fixed point numbers using shifts and addition bit-by-bit in a real-time loop instead of integer multiply followed by a shift.) It will be a long time before Peg is suitable for use; I would just be happy to compile a simple program to run on an MSP430. Still, I hope that after I have finished the core language design, others might join in and help with the optimization effort.
It's only a half-joke, because I intend to make destroying an `IO` token equivenlent to terminating the thread that destroys the token. I guess it would be kind of like shooting yourself in the foot.
&gt; A function doesn't have to return its IO token -- if it doesn't, it destroys its thread, therefore never returning. Its return type in Haskell would be bottom, I think. You can do that if you like, but don't advertise it as "pure functional". You're also replacing the traditional elegance of concatenative languages—building programs up with function composition—with something much more "magical" where each definition implicitly introduces some sort of dynamic escape analysis… or something. This seems awkward and unnecessary. At the bare minimum, it breaks factoring and equational reasoning. &gt; My reason for being unsatisfied with existing attempts to type stack languages comes from the difficulty of finding types for many words that I have defined. This is a bit backwards, isn't it? Why not start with a standard type system and design around that? What problems are these new words of yours solving? &gt; For example, you could define a function that returns a number or the word NaN without having to first make a data type for this explicitly. What would this look like? Why would I not want to just declare a data type? &gt; My approach would also allow for dependent types. The type checker would still catch if you forget to handle the NaN, for instance, because it tries to prove that no is impossible, which would be the result of NaN 1 +. I think you'll find that trying to prove things via dependent types while simultaneously avoiding the need to declare new inductive data types is not going to work out too well.
I don't understand how allowing definition of a word that doesn't return breaks the pureness of the language. Haskell does the same thing; the type of `x = x` is `forall a. a`, but it will never return. It is impossible to prevent non-termination in a Turing complete language. With each `IO` token representing a thread, non-termination is indistinguishable from thread termination. At any rate, I think I will disallow non-IO functions to take `IO` tokens as arguments or to return them. This is simple to do. Just disallow the built in stack manipulators (`dup`, `pop`, `swap`, etc) from operating on `IO` tokens. I've programmed in Haskell for at least five years, but the type system can be restrictive at times, even though that is what drew me to it in the first place. I think this is because Haskell consists of two separate languages, one for values and one for types. Writing type-level programs in Haskell is very difficult. With Peg, I want there to be only one language for both values and types. I think the type system should be tool for proving properties of an expression, not a system to restrict what can be expressed. Therefore, ideally, all useful expressions in the language should be typeable. Thus, it makes since to define the language before the type system. An expression that returns `1` or the word `NaN` looks like this: 1 NaN \/ Actually, it returns both non-deterministically. So does True False \/ [1] [NaN] ifte Therefore, a brute force method of determining the return type of an expression would be to feed it all possible input values non-deterministically. Obviously, a more sophisticated method will need to be developed, but you can see how the type of [1] [NaN] ifte would be something that takes a boolean (`True False \/`) and returns `1 NaN \/` without needing any previous definitions. In fact `True` and `False` are never defined as a data type, they are just used as a convention in `lib.def`.
Yes, except you can also solve that opaquely by using `MonadTrans`: newtype Foo m a = Foo (ReaderT Int (WriterT [String] m) a instance MonadTrans Foo where lift = Foo . lift . lift Then the user uses `lift` once for the entire opaque stack and it's also future-proof against internal changes. 
However, you're still completely ignoring my core point against the `mtl` style, which is that there is no theoretically sound basis for what is a "correct" instance. I don't know how many times I've looked at `mtl`-style monad transformers with comments like: -- This typechecks, but I'm not sure if this is correct ... because there are no laws governing what constitutes a correct implementation and thus there is no semantic contract with the users of it. With `MonadTrans`, the monad transformer laws govern the semantic contract with the user and it's the reason that the `MonadTrans` style never breaks or has weird corner cases.
That is exactly the same as MonadIO (except for the difference in generality which is orthogonal to the particular issue at hand). Maybe I'm just not understanding what you mean by MonadIO/mtl style vs MonadTrans style.
Hm, didn't know that. I thought they were just syntactic sugar for data Foo = MkFoo !Bar
In your pipes-vs-conduits debate, I completely appreciate this theoretically sound approach. Maybe I just am not good enough at formal reasoning, but it seems to be that there are tons of examples in real world applications where you need a type class but formal reasoning won't work because the domain is complicated and ad-hoc enough that you don't have any theoretical framework. It sounds like you are arguing your position across the board instead of just in situations where it's appropriate. Is that the case?
If you have `foo (Foo _) = ()`, then with `data`, `foo ⊥` ≡ `⊥`, but with `newtype`, `foo ⊥` ≡ `()`. That is, pattern matches on `newtype`s always succeed. See [this HaskellWiki page](http://www.haskell.org/haskellwiki/Newtype) for more details.
Haha yeah that's pretty much what I expected. You're using hlists (sensibly) and then wrapping up the hlist stuff so that folks can use it with regular tuples. An interface with just the hlists would probably be equally as nice -- I'd be tempted to add the tuple instances in an optional module :-) 
Right. From the [docs](http://hackage.haskell.org/packages/archive/hxt/9.2.2/doc/html/Text-XML-HXT-Arrow-ReadDocument.html): &gt; but I couldn't find a way to set parsing options with hread "This is a simpler version of readFromString without any options" &gt; (such as allowing HTML parsing) "parse a string as HTML content, substitute all HTML entity refs and canonicalize tree. (substitute char refs, ...). Errors are ignored." 
Thanks! Comments like this is why I post this stuff here. I updated the post, so the section on arrows should be clearer now.
It turned out to be something trivial, so I added a section on working with lists: http://adit.io/posts/2012-04-14-working_with_HTML_in_haskell.html#working-with-lists
Thanks! I updated the section with this example.
IO should be native.
The industrial partners in the [Parallel GHC project](http://www.haskell.org/haskellwiki/Parallel_GHC_Project) are using parallelism for things like Monte Carlo simulations and traversals of large graphs. You could also see the examples provided in tutorials. Check out the [Parallel Haskell portal](http://haskell.org/haskellwiki/Parallel). Also, try the [Parallel Haskell Digest](http://www.haskell.org/haskellwiki/Parallel/Digest). The author is a bit of n00b when it comes to this stuff, but he tries to capture what's going on in the community. I'll tweet/G+ your question to see if it helps :-)
Nested data parallelism is made possible by purity, I'm sure someone more knowledgeable can talk about the status of that project. There are plenty of examples of the "par" annotations. There's the [Par monad](http://community.haskell.org/~simonmar/slides/CUFP.pdf). The parallel stuff mentioned on http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/lang-parallel.html But what I wanted to add was that even when using completely imperative concurrency (forkIO), which also yields parallelism, you're *still* gaining from the purity. Haskell's forkIO is much more similar, in terms of its ease and simple semantics, to independent processes than to shared mutable state threads. Because the only shared mutable data is that which is declared explicitly that way and is typically only exposed to very small pieces of code where the difficulty of reasoning about shared mutable state can be contained. 
Haskell's in a funny spot. Purity is *awesome* for parallelism, but laziness is pretty terrible for it. It's painfully easy to end up in a situation where you build the skeleton of a data structure in parallel and then end up doing all the work on the leaves in one thread later.
Nice! But I'm still having an issue: the operators (&gt;.) and (&gt;&gt;.) only lift a pure function over lists, what I need now is to use arrows on each element of the list (e.g. if I want to count the number of &lt;li&gt;'s per &lt;ul&gt;'s), so I need signatures like: a b c -&gt; a [c] d -&gt; a b d or a b c -&gt; a [c] [d] -&gt; a b d or else I would have to use some hacky-tricky stuff. **EDIT:** Nevermind, I think listA and unlistA can be useful here.
Just rnf to disable laziness wherever you need?
Obligatory nitpick: That's [tree sort](http://en.wikipedia.org/wiki/Tree_sort), [not quicksort](http://augustss.blogspot.com/2007/08/quicksort-in-haskell-quicksort-is.html).
&gt; If [a Visual Studio language extension] would exist it would have shortened my learning time considerably, taking away much frustration and wasted time. Ouch! If you think that then you *really* gotta start learning to use a lightweight customizable editor like Vim/Emacs. I mean it, no trolling intended. I was *so* happy each time I came back home and could leave the Visual Studio I was using at work to hack Haskell with Vim at home. With the proper plugins you can have the same productivity with a much lighter editor (not to mention the basic edition commands &amp; shortcuts, but I will not dwell on that or else I feel someone will accuse me of proselytism ;-) ).
Take, for sake of simplicity but without loss of generality, two `forkIO`'ed threads communicating uni-directionally via a `TChan`. Now do: foo &lt;- BS.readFile "approx1gigofdata" writeTchan chan (BS.length foo) What's going to happen is that you send the whole gigabyte over to another thread instead of the length, killing all caches doing that (at least if said thread happens to run on another core, which, for the sake of pessimism, should be assumed). That's the reason you see `NFData` scattered about everywhere in concurrent programs: Before you send something to another thread you have to make sure that it's actually evaluated in a sane way. And sending things like `hGetContents` can wreck even more havoc. Worse, some data structures really don't like being `deepseq`'ed, because they rely a lot on laziness and amortising their bounds.
which is exactly why we went for strict-by-default in [monad-par](http://hackage.haskell.org/package/monad-par).
Did you actually test that? It doesn't look like it would work to me. At the least, you need some strictness, e.g. rnf small `par` ... Sorting lists is difficult to parallelise. I tried it again recently and the best I could come up with was a merge sort using monad-par, that achieved a speedup of just over 2 on 4 cores.
If you'll excuse a bit of self-promotion, there are a few examples in my [tutorial](http://community.haskell.org/~simonmar/par-tutorial.pdf).
Still coughs a bit, but yeah, she's good now :)
`dup` as implemented is already partial, because it can't be applied to `[`. Another major restriction I am working with is *flatness* - requiring any subexpression to be valid, and any expression evaluated in fragments and then appended is equivalent to the evaluated whole expression. I think this property is interesting and useful, and will allow for large expressions to be split and evaluated in parallel. This property forces me to allow `IO` inside sub-stacks, because `[`, `IO`, and `]` are all valid expressions, and `[` + `IO` + `]` is equivalent to `[IO]`. Thank you for your suggestions. I will look into the Racket type system; it sounds interesting.
Thanks! I appreciate your help.
Maybe we are not in disagreement on this point. :)
&gt; As an aside, why when discussing monads from the point of view of category theory are the return and join methods emphasized rather that return and bind? The Typeclassopedia explains this nicely. Say you have an (applicative) functor, and you want to make it a monad: "To see the increased power of Monad from a different point of view, let’s see what happens if we try to implement (&gt;&gt;=) in terms of fmap, pure, and (&lt;*&gt;). We are given a value x of type m a, and a function k of type a -&gt; m b, so the only thing we can do is apply k to x. We can’t apply it directly, of course; we have to use fmap to lift it over the m. But what is the type of fmap k? Well, it’s m a -&gt; m (m b). So after we apply it to x, we are left with something of type m (m b)—but now we are stuck; what we really want is an m b, but there’s no way to get there from here. We can add m’s using pure, but we have no way to collapse multiple m’s into one. This ability to collapse multiple m’s is exactly the ability provided by the function join :: m (m a) -&gt; m a." So in other words, having bind is equivalent to having both fmap and join. When working in category theory, you assume you have already defined fmap, as you're working with a functor, and defining join is preferred because it is a smaller addition than defining bind.
Peg does not just stop evaluating when a word can't be resolved, it fails, which is like `scrap` in Ripple. Peg stops evaluating arguments when no more are required to evaluate the word on the top of the stack. In Peg, there is no way to examine unevaluated components of a computation, because, for instance, `[1 2 +]` and `[3]` would be considered equivalent in Peg, and thus indistinguishable. `[1 2 +] length` is `1`, and `[1 pop] null?` is `True`. The `pushr` and `popr` words are semantically equivalent to `] x swap` and `x ] swap`, respectively, where swap always evaluates its arguments, except `swap` cannot operate on `]`. 
Because return and join are analogous to mempty and mappend. This analogy can be made rigorous: "Monads are monoids in the category of endofunctors"
 import Control.Parallel.Strategies parMap rdeepseq f xs
Oh, I didn't realize that. I stand corrected.
But then you've got a serious problem figuring out who is supposed to close that file. But, yes, in general that's right, and there's non-problematic uses, like e.g. passing fibs over for the recipient to evaluate.
This is exactly what you get in ghc 7.4.1 with {-# LANGUAGE PolyKinds #-}
&gt; The type m a is just the value a in the context m. Nitpick: `a` is not a value, it is a type. Also, this sentence can lead you to the common misconception that a value of type `a` is *actually* somehow present inside of a value of type `m a`. See HWN for occasional quotes clarifying this in clever ways: &gt; ski: `getLine :: IO String` is a recipe for how to interact with the world to acquire a `String` &gt; ski: the recipe is not the cake Other than that, I quite liked it. Keep up the great blog posts, mightybyte!
Thanks, fixed. I am aware of that distinction, but it's easy to mix up the terminology. Thinking about it a little more...I think it's also a misconception that a value of type a is necessarily somehow present inside a value of type a. Of course, we know that this is all because of laziness, but it puts a different spin on things. On the other hand, aren't there also cases where a value of type a *is* _actually_ present inside a value of type m a? Take for instance Maybe Int. Can't a value of that type have an actual computed Int? I understand that (IO Int) is a recipe for computing an int. But it doesn't seem like that's always the case for all (Monad m =&gt; m Int).
You can't however explicitly abstract/quantify over kinds or mention kind variables just yet, I think. Overall the feature is pretty awesome, though.
Correct, sadly. This is why I'm not actually really using PolyKinds in anything until 7.6.1. =/
(i had to google this: http://www.scs.stanford.edu/11au-cs240h/notes/generic-slides.html#%281%29 http://www.haskell.org/ghc/docs/latest/html/users_guide/type-class-extensions.html
Just to throw this out... the reason some of us really like that Monad instance is that your `yielder` version: yielder stop = go 1 where go i | i &gt; stop = return () | otherwise = do yield i go $ i + 1 can be written much more briefly by using ordinary old monad combinators that we already know and use instead of a clumsy explicit recursion. yielder2 stop = mapM_ yield [1 .. stop] To me, that's actually the option that I'd prefer to see in my production code, by virtue of being about the same length, and not requiring familiarity with a specialized conduit-specific library of combinators like `CL.sourceList`. Yes, I see the performance difference, but if that's responsible for visible application performance issues, I'll be very surprised.
After doing a **cabal unpack -d sources blank-canvas**, I'm finding the code very neat and illustrative. A good resource to learn from, when combined with the slide set.
I'm actually in the class Professor Gill introduced this library for and from what I gathered, yes, it's to look like C syntax. Since the haskell code generates the JavaScript that does the actual canvas interaction, it saves a few keystrokes and looks visually similar. More impressively, I think this was a weekend project at my classmates request. There were some slides that dove a little more deeply into how and why the DSL works, but I don't have a copy nor permission to post them.
I would agree with this for most Haskell libraries, but the idea here is someone can almost cut-and-paste Javascript examples, and the code is **completely** imperative. Its would be trivial to add a layer that does Currying. What is interesting, rather than the specific syntax here, is what can be put on top of the library with a more functional API. Neil Sculthorpe is looking into putting a FRP API on top of this, for example.
I don't mean to be discouraging, but this is sort of defeating the whole purpose of Heist.
Or at least, if not defeating the purpose of heist, running totally contrary to good practices and separating presentation from business logic. It's a recipe for the most unmanageable code imaginable. If you want this kind of functionality, at least write one splice per table, and set up something like: &lt;cheese name="Dubliner"&gt; &lt;price/&gt; &lt;/cheese&gt; Which is infinitely more readable, and will allow you to change how you handle your data much more easily than if you are always mucking directly with SQL.
I think it's reaching maturity, but it would certainly be premature for me to claim it's there now. I would say if in a month's time there have been no changes to the underlying structure of the library, it's a good bet it's mature. If in three month's time that's still the case, I'd probably call it mature. Of course, `conduit` is in heavy use today for a number of projects, so it already has some level of maturity. I just wouldn't want anyone to start relying on a completely stable API quite yet.
tr0lltherapy - It is stand alone - a single big blank canvas on the browser, ready to draw on. However, given its use of a RESTful API, blank-canvas could be used by a jquery widget.
&gt; it would certainly be *premature* for me to claim it's there now I see what you did there. :) Now that conduit is starting to settle, perhaps it's time for some outside people to start porting it to other languages, to see how useful the pattern is outside of Haskell.
I will insert it back into my vocabulary then!
Very nice feature. Thanks!
OK, I've added some rules: https://github.com/snoyberg/conduit/commit/dec0305a3fdabb0c04cdfef7f8cd89c743459c91 `mapM_ yield` can now be turned into the more efficient `yieldMany`, which is just a non-type-restricted version of `sourceList`. Unfortunately, I can't seem to optimize away the intermediate list. `enumFromTo` has a rewrite rule which is firing before `conduit`'s rewrite rules. I'm quite a novice at this stuff, so if anyone has ideas, I'd be happy to hear them.
that's slightly wrong: runState :: State s a -&gt; s -&gt; (s, a) The reader and state monads depend on input. The writer monad can do it, though.
"method" is actually formal Haskell terminology for the stuff declared inside a type-class. methods are not necessarily functions but often they are. Bind and return are methods, join isn't, but probably should have been.
Actually, had the functions used idiomatic Haskell arguments (untupled, no unit args), they would take fewer keystrokes to type. True, if you cut-n-pasted from existing JavaScript source, you'd have to edit more... But how often does one do that?
Does this work with the arrow notation?
I know this is about functional programming in general and not just Haskell, but I just wanted to say that the majority of the interactions I have had with the Haskell community have been very warm and I really appreciate that.
&gt; What are monads? "Monads are just monoids in the category of endofunctors! Ha ha, you don't understand any of those words, do you, stupid mainstream programmer?" I don't know whether the original utterance was intended as a joke, but every use of it I've seen since I started learning Haskell about three years ago has been a joke. Many/most Haskellers have no clue what it means either :)
Yes, I agree. Even though most languages have helpful communities, the Haskell community has been particularly nice and helpful.
When I read that "history" some time ago I was quite amused at much of it. However apocryphal the actual statements might be there are nuggets of truth in them. Otherwise they would not be nearly as funny. The situation with monads as I found it a year or two ago was that there simply was no accessible explanation available that lead to actual understanding. The running joke was that every newbie who thought (wrongly) that they finally understood monads wrote their own (incorrect) tutorial. The assurances from those who understood monads boiled down to reflecting on the monad koan until you achieve enlightenment. I found almost every other aspect of Haskell to be accessible, and after a short period much of it seemed naturally expressive, compact and elegant. But none of that matters, because without understanding monads you can't really accomplish anything. I hope this has changed since I last looked, as I plan on revisiting Haskell in the near future.
I don't know. I imagine it overlaps with self-help stuff you'll find out there. Just Pet Theories really (which is not to say I haven't absorbed any fluffy self help books over the years and forgotten about it… actually now that you mention it, I very much admire the attitudes conveyed in [Leadership and Self-Deception](http://www.goodreads.com/book/show/180463.Leadership_and_Self_Deception). Maybe that's where the inward gaze comes from) Part of the reason I love that book is because instead of teaching you how to be a Winner, or how to be Nice to People to Get What You Want; it instead says “Argh! Stop thinking of people as means to ends”, which pleases my inner hippie to no end
I've not read the article yet but I think the tone of his voice, had he been voicing things, would have hinted that was a joke. At least it sounds like a joke.
&gt; Before GHC 7.6.1, I'd like to integrate all of this work back into the mainline branches of GHC, vector, and dph. Once that is done, all you will have to do to take advantage of SIMD versions of floating point operations in your DPH code is move to the new release. Nice. 
I think it's a double standard. Other languages have equally intimidating terminology, particularly object-oriented languages: "Superclasses, dynamic dispatch, subtype polymorphism, virtual functions, deep copy". The only difference is that the OOP terms are part of the current curriculum so familiarity breeds comfort.
I have a wiki page about that, [Haskell is baffling](http://wiki.chrisdone.com/Haskell%20is%20baffling).
I believe I had a fairly good grasp of types and kinds at the time. That tutorial looks interesting, though. Thank you! I also intend to check out the monad bits in LYAH, which didn't exist at the time I went through it.
Functional weenie is a much nicer term than the one he coined: http://c2.com/cgi/wiki?FunctionalWeenie But weenieism is parametric! http://c2.com/cgi/wiki?search=weenie
I see. I agree with you; However I cannot help but note that the OP was also on about equality at a lower level. So if I am off on a tangent here, then it happens to be the same tangent as the OP, I think.
&gt; I recently saw a situation where a feature was vetoed from a release because it required runtime initialization: a private variable would be null until an initialization function was explicitly called. "Mutability makes things harder to reason about," said the functional adherents, "and we don't have time to refactor this into a properly immutable data structure." So user value was not delivered because of slavish adherence to the paradigm. (And, to further the frustration, it was an implicit admission that technical debt was not being strategically managed.) This confuses me. If a project has declared a particular paradigm, then integrating a feature that breaks the paradigm *will* cause technical debt, and rejection is the only logical choice. Rushing features at the cost of code consistency is what leads to the nasty legacy code that pretty much anyone in the industry finds themselves dealing with.
I'll double check and see if that's unclear. Late last night I made a final pass through to remove a lot of the stuff about checking the various laws, in the interest of keeping a more conceptual tone. I might have axed too much.
The Pankratius study cited in the article is interesting, if a bit problematic. Essentially, the study gave developers some small tasks and a larger project to complete in an effort to compare how FP development stacks up against OO. A few points: * the sample size is 13 Master's students -- obviously too small to be statistically significant. * The languages under scrutiny were Java and Scala. * The subjects got four weeks of training each in both languages, which rather skews the results toward Java, since Java is often the first language taught to undergraduates, and the average grad student may be presumed to have had more exposure to Java, C, and C++ than to any functional language. * The developers took longer to develop in Scala, mainly due to problems with type inference and immaturity of tooling around testing and debugging. No surprises there. * Even given this, developers far preferred Scala's approach to concurrency (the actor model) over Java's. It's a decent first effort at studying the quantifiable differences between development efforts in FP and OO paradigms, but I suspect they would have derived similar results had they studied OO versus structured programming in the 80s. I'd be interested to hear what others thought of the study. 
Don't you know loops multiply when no one is watching ? 
Deugaring produces code that uses slightly different combinators. I didn't dump the output of the desugarer, but it wouldn't be hard to do.
No need, was just curious. Didn't actually know there was a desugarer tool. Thanks!
I'd be quite surprised if that were really the original source. I'm pretty certain I'd heard it before that, maybe even when I was learning circa 2005.
GHC does the desugaring itself. You can dump the output of the desugaring phase with the -ddump-ds flag.
its something you see when "my code" matures into "the large existing codebase that i and my 10 engineer team at various skill levels need to maintain"
For the code I write three loops is a minimum (the algorithms being implemented start at O(n^3 )), and there's another one or two at the top level in order to call the algorithm on each input. But I'm not sure I'd call that "basically every software project today"...
And even those who do, still mean it as a joke. Remember, category theorists are the ones who willingly embrace calling their work "abstract nonsense". Just because we're doing hardcore abstract math doesn't mean we don't recognize it for what it is :)
And identity
And I'm not sure that I'd hold Scala up as an exemplar of functional programming. You can do functional programming there, but you can also write Java code with minor syntactic changes--- which is in fact how many people start learning Scala. Thus, comparing Java to Scala isn't going to tell you a heck of a lot about what it'd look like to, say, compare Java to Clojure, or C++ to ML, or Smalltalk to Haskell. 
I think you may have shed some light on the real reason behind the enduring popularity of C.
&gt; For the power-set monad, P, it would take a list of sets, and give back a set of all the ways to choose one item from each list. Do you mean "from each set"?
Yes, it works exactly like testing.
Perhaps in the "four horseman" section, you could add a few words to explain the connection between functions with multiple values and nested loops.
Very nice heist example. One slight problem: somewhere in the middle of your text you have `&lt;img src="something.png"&gt;`, but that graphic doesn't seem to exist.
&gt; When most computer programmers say "function", what they really mean to say is "subroutine". Yes. Mathematicians have cleverly leveraged that misuse of the word "function" to get programmers interested in learning category theory.
OK here: http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html
Exactly. Which is another problem with the study. An Oracle engineer they threw into the mix (ostensibly to give the study at least one developer with Scala expertise) wrote Scala solutions that consisted, according to the authors, of a 50/50 mix of FP and imperative techniques. This obviously wouldn't have been the case had they used Haskell as the basis of comparison. 
&gt; When I started learning Haskell, I had no idea it was supposed to be a joke. I'm curious, did you also think Ashton Kutcher was part of the original language committee?
He's Bruce Willis' son. 
I wonder about "css" name - why all these getTagsByName :: m String -&gt; [Node] functions are named like they could to cool css selector stuff and not only match one particular attribute of a node... Is there a good and *real* css-like selector to do cool "ul li &gt; a[title]" stuff? Anyway, thanks for great tutorial!
Do you read much of other people's non Haskell code? Closed source code is chock full of that stuff.
I often suspect that Haskell beginners are probably pushed too early into Monads, and that a better approach would be the following: explain monadic IO first at a modestly concrete level, using the monad operations and `Control.Monad`. The way this would go is this: 1. Introduce the notion of an `IO` **action**. Key concept: Haskell deals with side effects by representing things that have or depend on side effects as values of type `IO a` for some `a`. 2. Introduce the notion of sequencing `IO` actions with `&gt;&gt;`. This is just a function that takes multiple actions and produces a composite action. 3. Introduce simple cases of functional abstraction over actions; e.g., primitive parametric `IO` actions like `putStrLn :: a -&gt; IO ()`. 4. Introduce `&gt;&gt;=`. Easy example: `getLine &gt;&gt;= putStrLn`. 5. Introduce the notion of writing user-defined action combinators. Basically, show how you can use the monad operations to implement control flow by implementing most of the functions from `Control.Monad`. Two simple ones: `forever :: IO a -&gt; IO b`, `sequence :: [IO a] -&gt; IO [a]`. (Yes, keep the types concrete for now.) This is also the perfect spot to motivate and introduce `return`, because you need it to write `sequence`. 6. Introduce the `Monad` type class. Point out that `return`, `&gt;&gt;=` and `&gt;&gt;` are the monad operations, and that `IO` is just one of the many ways of of implementing this "interface." 7. `do`-notation and desugaring. Rewrite some of the examples from (5) using `do`. 8. Tour of the `Control.Monad` module. 9. Brief example of other things that are also monads, and intuitions about what the monad operations "mean" in each of these monads. Hey, maybe I should write a monad tutorial... *ducks*
That's how it worked for me just fine. However, from teaching beginners, it appears to me that the real difficulty simply lies in the type of `&gt;&gt;=`. The problem is often that they get thoroughly confused by the higher-order type and have a lot of trouble writing code that is guided by the types.
A promise with result `a` can simply be represented with the Haskell type `IO a`; you can then use a function like [`spawn`](http://hackage.haskell.org/packages/archive/spawn/0.3/doc/html/Control-Concurrent-Spawn.html) to start the execution of a promise, and get an action to demand the result. With GHC's lightweight threads and non-blocking IO, this should be as efficient as you might hope for.
Just use threads. Fork a thread that eventually writes a result to a shared MVar and take from the MVar at the point that you need the result.
Don't forget that JS promises also usually have an exception handling component. Hopefully the enlightened Haskell hackers here won't forget to clear that up too ;)
This, a thousand times this. The answer to "what's the Haskell equivalent of [crappy synchronization mechanism]?" is "Don't use crappy synchronization mechanisms." The worst thing that has happened to programming in the past three years is that just as the programming community was about to collectively discover that compilers can manage this stuff for you and give you really nice synchronization mechanisms that make sane programs that are easy to reason about (Erlang, Haskell, Go, etc.), Node.js came along and convinced everyone that event-loop-based programming was some sort of fantastic breakthrough, rather than a deployed-for-decades well-understood technology that was generally considered a bad idea, and then further convinced a lot of developers that anything not Node must not be any good, even as they are stuck using bad ideas from 30 years ago. I have noticed the JS community rapidly recapitulating the developments of other software communities. They're up to the late 90s with promises being the new hotness now. Pity their language choice will stop them before they actually reach sane choices. (Also, as ehird points out, it's basically so simple it's transparent in Haskell. I'm discussing the more general case here. The Haskell answer is that the compiler worries about non-blocking, rather than the programmer, and does a much better job at it... especially if we're talking Javascript which still can't handle using multiple cores yet.)
In Haskell, this is just the [`Either` data type](http://book.realworldhaskell.org/read/error-handling.html#errors.either), with `Right` and `Left` representing successful and unsuccessful results. Addendum: While there's some discussion about I/O and threads here, note that these things are orthogonal to promises / Deferreds / `Either`, and they should not be confused with each other. The latter are just one way to construct computations involving results of the former, and there are plenty of ways to use either without the other.
Watch for his new movie, Oedipus Rex.
No?
Great job debugging, but to be honest this is truly scary about Haskell. In almost any other programming language we would have a stack trace and have this debugged in a fraction of the time. Would the new stack trace feature have helped here?
&gt; especially if we're talking Javascript which still can't handle using multiple cores yet. That's why promises/futures are a good fit for Javascript: all the asynchrony you want, without having to deal with the multithreading. Add some E-style promise pipelining, and the programming model can be somewhat pleasant. You can be as dismissive as you like, but the fact is that JS can't make the assumptions you can make in Haskell, namely that all references are immutable and thus thread-safe by default. Automated program transformations just aren't feasible. Node.js is actually a good start for a mutation-heavy language like JS. They aren't repeating the mistakes of the past, they're taking a different route entirely and avoiding the past mutation-heavy designs, and going share-nothing message-passing by default. Next they have to make spawning processes cheap and easy, and if they could wrap this all up in a simple API, *then* they have something truly compelling.
Since I wrote the original rant that turned into the article I have learned that both this comment and the "muggles" comment were intended as self-deprecating humor. Indeed, much of the reaction to the article has been a humiliatingly mature "Gee, that seems harsh." Having called y'all pricks, I'd appreciate a little blowback. But, just so the criticism is fairly aimed, I want to stand by my premise that there is a problem. It's not Haskell-specific, and the "Insufferable Haskell Prick" coinage is, as I hope you appreciate, only an evolution of "Smug LISP weenie," which I think we can all agree is a term-of-art. (Look, I spell LISP all in caps; I learned LISP circa 83; I was the Editor of AI Expert... I tease because I love, people!). Further, I want to say for those who aren't regular readers of my column that this comes from a guy who WANTS FP to be The Next Big Thing. I *personally* believe that FP should succeed, because I think that the manycore "is what it's all about" for the next decade and I think FP's mathematical foundation is necessary for the deep reasoning that I think manycore demands. Further, I think that the mainstream has become primed to FP concepts -- I've talked about things like unit-testing and REST being techniques that have familiarized the mainstream with things like referential transparency, etc. Finally, to clarify for people who don't know me (and my background in computer languages), to _me_ it's really interesting whether FP succeeds through relatively-pure languages (Haskell), hybrid languages such as Scala, or if "all" that happens is that FP concepts influence mainstream language design a la C#. OK, so having soft-soaped all my criticism, let me return to it and reiterate that I truly feel that FP has a communication problem. You guys in the Haskell community probably have NO IDEA what it's like to hear an FP advocate pushing Scala to a Java shop. I don't even want to repeat the claims because you, reasonable person that you are, would feel compelled to establish that you've read "No Silver Bullets" etc. etc., that you DON'T claim 10x productivity improvements, etc. etc. And although Scala is what's being pushed in the JVM world and F# in the CLR world and really it's unfair for me to call out Haskell guys as being the smug ones, well, you know, if the Haskell community is where the thought leaders are then the Haskell community has to take some responsibility fr leading the paradigm shift (or, as I've come to fear, taking a good measure of the blame if there IS no paradigm shift towards FP). I hate to get all gray-haired and "in my days we had to walk 6 miles..." about it, but dammit, guys, in my day the OOP guys busted their humps bringing the mainstream structured developers over the chasm. Communication with the mainstream was absolutely the primary goal of great communicators like Booch, Wirfs-Brock, Page Jones, and Cunningham. And by 1989, 5 years after it's release, the release of the first C++ compilers were watched BY THE MAINSTREAM as the epochal events they were (and yeah, yeah, obligatory "oh wasn't THAT a mistake..." blah blah blah. I get that you don't REALLY mean it, except that you kind of do). Here's the bottom line: I've been looking for the objective proof that FP provides the huge benefits in productivity and reliability that, let's face it, is the claim of FP advocacy. I've been looking for the texts that "make the lighbulb go on" in mainstream programmers. I haven't found them. Please let me know what I should read. I think "Learn You a Haskell for Great Good" is the best language tutorial since "Real World Haskell" which was the best language tutorial since "Practical Common LISP," but IMO "Learn You..." is a great companion text for a course, not a text a mainstream developer can pick up, work with side-by-side with a project, and come away from with the same kind of experience that, once upon a time, people got from Booch's "Object-Oriented Design with Applications" or Wirfs-Brock "Designing Object-Oriented Software." In terms of empirical studies, the Pankratius' study is definitely problematic in sample size (as are almost all empirical studies of software productivity) and if it disagreed with my premise I'm sure I could find reason not to pay attention to it. But, okay, what are you gonna' put up on the other side? Where's the objective proof that FP is more than an incremental improvement over other paradigms? And IF it is only an incremental improvement, is the strenuousness of the near-pure FP mindset valid? IF it is only an incremental improvement, why should it NOT be marginalized, an academic area of study that spins off occasionally-useful insights to the mainstream (ala LINQ)? Finally, I regret that page-hits and (hopefully) constructive dialgue seems to come from being obnoxious and using crude insults. Columns like http://www.sdtimes.com/l/36003 or http://www.sdtimes.com/l/36103 don't get hits, this one does. I regret that, but it's the way that it is. 
I thouroughly enjoyed that summary because it was easy to read and easy to understand. I did in fact not really know what that Kleisli stuff was all about before, even though I am an advanced user of Haskell and have little trouble using monads.
http://dekudekuplex.wordpress.com/2012/04/20/haskell-and-music-expressing-notes-signals-and-symphonies-in-haskell/
Probably a bit. The other thing to know is that that gdb does work pretty well for debugging GHC apps. If you use Peter Wortmann experimental DWARF emitter for the LLVM backend it's not too hard to piece together an evaluation stack... Line numbers and (a couple) local symbols help track down problems pretty quickly.
I generally prefer refactoring using `where`, e.g. foo = a `operator` b where a = ... b = ... 
They're exactly the same thing. `then()` is [`either`](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Either.html#v:either); `resolve` and `reject` are `Right` and `Left`. All this talk about I/O and threads is entirely missing the point: Promises and Deferreds do not specifically involve I/O, or threading, or anything else. They are just a pure-JavaScript way to structure two-path computations in exactly the same way as `Either` in Haskell. If you have some plain JavaScript code that returns or results in a promise or Deferred of some error and result type, the Haskell equivalent is the same code that returns or results in `Either ErrorType ResultType`. If you have some JavaScript that binds a promise or Deferred to the result of an I/O operation, like loading a Tweet, the Haskell `Either` would be wrapped inside `IO`, like `IO (Either Error Tweet)` (or alternately inside whatever other abstraction the Haskell program is using for I/O). For other things, it's the same: whatever you can use promises and Deferreds with in JavaScript, you can use `Either` with in Haskell.
You can interleave arrow notation with `&amp;&amp;&amp;` and co, right?
Then you can switch between both styles when appropriate.
yes, Debug.Trace. You can use my file-location package to get file &amp; line numbers also.
Regarding the install, I've installed Euterpea but I can't work out how to import it in ghci to play around with: Prelude&amp;gt; :m + Euterpea &amp;lt;no location info&amp;gt;: Could not find module `Euterpea' It is not a module in the current program, or in any known package. Does anyone know how to import via ghci modules that weren't installed from hackage? Edit: never mind, I had some unrelated issues with my ghc versions and everything started working again after a fresh install of 7.4
I disagree, or perhaps made my point unclearly: yes, technical debt would accrue, but if technical debt is properly managed (that is, paid off), you can be comfortable with knowingly-compromised code that delivers user value (since refactoring it is just another input to your next sprint / release cycle). 
Nice! Good to see Happstack news out there on an irregular basis.
I occasionally use this style too.
Haskell's unofficial motto is to avoid success at all costs. Some people like to advocate it, but that's based on personal preference. We're not the ones leading the paradigm shift here. We're just using Haskell because we think it's good.
thanks jasper, i enjoy using blaze and i appreciate how you document these changes
 runX $ doc &gt;&gt;&gt; getRows &gt;&gt;. tail &gt;&gt;&gt; listA ( (getChildren &gt;&gt;&gt; getCellContent) &gt;&gt;. take 2 &gt;&gt;&gt; arr (map toUpper . trim) ) &gt;&gt;&gt; arrIO (updateBase conn) ?
I think your timeline is a little off, in that you should be tracking the evolution of OO "hype" from smalltalk on, not C++ on. Also there were also-rans like Object Pascal, etc. And there was a major push from the top for OO programming with the rise of windowed UIs -- both from Apple and Microsoft. The success of OO programming, in my mind, is really a story about the success of windowed GUIs, and the programming paradigms that were bound up with them from the start. If FP does go mainstream, it won't be by a similar path. Or, if there is such a path, nobody I know of has yet envisioned it. Relatedly, this could just be my ignorance, but I don't know of any serious, reliable sets of data that ever demonstrated an improvement in software productivity for OO over classically procedural?
Yes, I think its always allowed this. It also worth noting I believe Hackage (correctly) will always reject a package with a `-&lt;foo&gt;` suffix, because it can totally screw up dependency resolution (is `foo-3.2-asdf` &gt; `foo-3.2-bar`, or what?) At least, I remember reporting this issue to Duncan years ago (because you could upload packages like that and there was one in particular I was attempting to use,) and that was the decision he took.
Yep. It's been there since the beginning, but it's been deprecated for about as long.
Wait, why is it deprecated?
That's great if it can be done. The benefits that I can see are: * we get to write the views in haskell * our client logic gets typechecked by ghc instead of a web browser's javascript interpreter * it is in frp style (a must have if it is to compete with modern javascript frameworks) I suppose some problems I could imagine are: * how do we interact with js libraries? Ace edit is a powerful tool which I wouldn't want to have to duplicate. Likewise other valuable js libraries. * it's nice that we have text input to play with but what do we do if there is a HTML element that you haven't implemented a runJS method for yet? Though this doesn't look like a scaling issue. * how would we use normal Haskell functions inside the runJS monad? * I've not heard of anyone else doing this for a non research project. You'll have your work cut out for you That said, it would be near impossible to do this in a language without excellent monad support. The yesod team is probably one of the best suited in the world to pull it off. Apologies for any typos, I typed this on my phone. Edit: most js frameworks, like knockout and angular, have live and documented examples on their front pages. A good way to test how well this approach works would be to try and implement their examples. 
Checkout [flapjax](http://www.flapjax-lang.org/) It's a javascript frp library written in haskell. You may find some useful stuff in there. 
uh...not sold on this client "thick" frameworks (angular, ember, etc) are becoming important all on their own and increasingly i only want my backend feeding data (json, etc) over xhr/websockets etc generated js seems like a headache and difficult to integrate with other popular js libs. 
Indeed providing the smallest possible set of primitives and working up from there would be the smartest approach. Maybe you'd want to provide a way to define new primitives, partly for optimisation reasons, leaving the safety up to the author to guarantee? Haskell and javascript are an odd fit though, with their type systems being on opposite sides of the spectrum. It would be nice if you didn't have to define a jsPlusInt, jsPlusStr, jsPlusX. Also, it sounds like you would be replacing some of Julius with the runJS monad, which automatically converts a small library of haskell code into executable javascript code. Have you thought about the relationship between julius and runJS? Would people replace their julius code with runJS code, or what? It's a shame about the timing of this blog post and GSoC. This is an exciting new direction for web development, which could possibly allow any language with monad support to replace their client side js with server side X (replace X for a language better then js). 
I was under the impression that flapjax has nothing to do with Haskell and in particular was not written in it?
I like the approach. Concerning FRP, I am actually not sure how useful it is. What I have observed is the following: simple combinators like the ones you just implemented work well for simple examples, but difficult examples tend to stay difficult. Expanding the combinators to cope with the difficult examples makes the FRP implementation very tricky (the `union` combinator for merging event streams is mainly responsible for that). And even then, difficult GUIs tend to stay difficult unless the GUI framework behaves "in the right way". So, basically, my observation is that basic FRP is useful for a certain class of GUI programs, but tends to be useless for anything that does not fall into that class. I have made a collection of [examples][1] that tests how large that class is. The CRUD example is particularly difficult. [1]: http://www.haskell.org/haskellwiki/Reactive-banana/Examples Since FRP is tricky business, I would suggest that you outsource it to other people and use an existing FRP library. I have recently adapted my FRP library `reactive-banana` so that it can be compiled with the JavaScript backend of UHC. If you spend your efforts on developing a good client API bindings for UHC-JS, you can get the FRP part for free from my library. In the long run, we want to program Haskell in the browser anyway, so that is effort well-spent. There is also the question of what a "good" client-side API is. I have put forth [a few principles][2] to that end. They apply to the traditional MVC model, too, so that would be no waste either if the FRP doesn't pan out. That said, the principles need a bit of refinement and a different explanation, but I think that I have a good grasp of their core message. I'm currently working on getting them incorporated into wxHaskell. [2]: http://apfelmus.nfshost.com/blog/2012/03/29-frp-three-principles-bidirectional-gui.html
In case it wasn't clear: I'm not sold on this either. This is young idea. As for the two concerns you're raising: * Yesod already supports the workflow of providing JSON over xhr/websockets and leaving the hard work to Javascript. I'm not sure how much more we would really want to provide at the library level to support this more (ideas definitely welcome). * In general I'm not a fan of generated JS. But I'm hoping this system would be different from (e.g.) GWT, since it won't be *truly* generated JS. Someone would still be manually typing in each bit of JS code, and then Haskell would just be used to compose the pieces together. I also think it could integrate really smoothly with existing libraries. Notice that this demo used jQuery pervasively. I see no reason why we couldn't do the same with other JS libs.
I think the relationship with Julius would be that Julius would be used to write the actual Javascript code, and then runJS would take the individual snippets of Javascript and compose them together.
I'm working in project where Snap framework is used on server-side to provide HTTP interface (with JSON as data format) to Redis storage where instances of models (collections of fields) are stored. Backbone.js is used to maintain ever-sync for models in client side and server side. For reactive interface Knockout is used. The two are bound via Knockback and bunch of extra code. This way we got ability to write in field and have it ever-synced to server (input events propagate to changes in Knockout ViewModel, which propagates to Backbone, which is bound to sync changes in model to server (delta-style)). Client representation may differ from what is stored on server (mapping by key-label or even more generic function), in which case custom Knockout observables (readable and writable) are installed to perform extra client—server transformations. I don't know how to do collaborative editing with that, smells like rewriting Wave. Javascript sucks to write by hand. On the other side, all JS-generating solutions for Haskell are still a bit too limited from my point, and existing frameworks for JS suck less and all are relatively simple. It would be interesting to rewrite this with more use of Haskell goodness for client-side, since currently it is like a cold shower to write JS after Haskell, and I can definitely say that quality and maintainability of Haskell part of project is higher than that of client side. You may lurk through code at http://github.com/f-me/carma for client side and https://github.com/dzhus/snaplet-redson/ maintains security on server and provides HTTP interface to storage.
One thing that struck me as odd with UHC+JS is that JS code lives in the IO monad. So, what does it do if you throw in a `readFile`, for example? Shouldn't JS be its own monad, perhaps based on the root object of the target JS runtime (window, for browsers)? I also wonder what happens if you say, use `Data.Map` in such JS code. Does it compile the whole collections package into JS to be sent to the client? And if not, at what point does compilation differ from generating code using some monadic EDSL?
&gt; Going the route of JS generation from Haskell introduces a very large dependency on UHC. I'm hesitant to go the route of compiling code to JS, I think it's one of the things that makes GWT so complicated. It also likely will make it more difficult to work with existing JS libs. Along this line of thought, it may be worth investigating [Elm](http://www.reddit.com/r/haskell/comments/rkyoa/my_thesis_is_finally_complete_elm_concurrent_frp/). Currently he has a compiler written in Haskell for a Haskell-like language with some reactive primitives that compiles to javascript. Of course doing separate syntax isn't necessary, one could write an EDSL which generates an AST to be passed to the javascript code generation phase. I haven't looked at his generated JS to see how messy it is, but he does have support for including inline raw JavaScript. This would also raise the possibility of writing bindings libraries for things like JQuery to extend Yesod.
Howdy and thanks! I'm working on a push-based (data-driven) implementation of signal-function FRP, together with a well-specified monadic evaluation interface. (Most other papers, aside from (Elliott 2009) don't speak at all about how the FRP system is evaluated). I actually have a couple of comments (sorry for the late reply, schoolwork has been catching up with me.) 1. One comment on the citation format: most templates tend to ignore the url field, so if I want to include a url due to lack of other publishing information, I usually use: howpublished={\url{http://foobar.com/path/document.pdf}}, 2. Unless I missed something, you're claiming to be able to embed AFRP in your system, but you haven't actually done so. Since you don't provide a switch combinator in your base system, how would switch be implemented for AFRP on top of this? Would AFRP still be data-driven as opposed to demand-driven (push rather than pull?) 3. I understand the simplifying reason for specifying sampling intervals for timers, but it would be worth noting that in the original FRP paper ("Functional Reactive Animation", Hudak et al, 1997 ICFP) one of the motivations for FRP was to separate time dependence from sampling intervals. Keep up the good work! Are you planning on going past undergrad? 
i would pick "other js libs in the wild", but for nontechnical reasons. angular/backbone/bootstrap etc are all becoming resume points on their own...this is the direction the web is going regardless of technical merits. i can't just stick my nose in the air and invest in an approach that only i know. on the server, i use haskell and will continue to. on the client, i have to follow the industry to a certain degree. and to be fair, some of the hype surrounding some of these js libs like angular is warranted.
This sort of echoes the "commas first" style of writing lists and imports. Having the operator leftmost in the indentation feels like lisp style, too, despite the operator being infix.
If the operator is pipeline-style I tend to like operators on the end of lines (and even more to begin subexpressions/parenthesized expressions on new lines with indent). If the operator is infix associative I like it either at the beginning of lines, or even on lines all by itself.
Very interesting. We're just starting to think about some similar issues for generating animated/interactive diagrams (http://projects.haskell.org/diagrams/) that run in a browser, so I'm keen to see what you come up with and what your experiences are.
Is there some reason GHC can't be used to compile Haskell to Javascript, given that GHC generates LLVM IR and [emscripten](https://github.com/kripken/emscripten) takes LLVM IR and generates Javascript code?
I don't understand why that should make it be deprecated. The Hackage policy (not accepting any packages with tags in their version) seems to take care of this "problem" completely. Only in-between-release versions are allowed to have tags, and those versions are incomparable to any versions without tags. That seems like exactly the semantics I want from the tags in the first place.
Thanks, I didn't know about the "github" bindings (I thought I checked Hackage for an existing library...). And it already uses the new http-conduit instead of the deprecated (yesterday or today) http-enumerator.
Haddock is already using GHC I think, so the idea doesn't seem all that far-fetched.
You make things a pain in the ass for distributions that want to package your package, since your version numbers don't map cleanly onto their permitted formats. Now, you could claim that that's the distribution's problem, and that they should update their format rules, but distributions also have to deal with Perl packages, Ruby packages and so on.
The problem with #2 is that it is not supportable. I can find (with difficulties) haskell programmer to maintain my server side. Javascript programmers are dime a dozen. So no problem supporting client side. But a haskell programmer who is good at generating js and is interested in wasting his life for such boring task (given the alternatives), where will i find him ? In fact i found that splitting the work along the natural borders (client side developed by js programmer, and server side is developed by me in haskell) works good, management easily agrees to it, js programmers are happy that they do not fiddle with the server and concentrate on what they know well (js, css, dom, jquery). They can utilize their favorite tools (Firebug, js, css, html editing tools). I am happy that idiosyncrasies of supporting several browsers are not my problem. 
It works (I tried), the LLVM output from GHC works with Emscripten, but the GHC runtime also needs to be compiled with LLVM and then Emscripten to actually run your compiled Haskell, which is hard.
It's worth noting that there are two orthogonal arguments for/against these, one, on a technical one-developer writing a program stand-point, and two, on a social many-developer teams working in Haskell-hostile environments. If you're on a Haskell-friendly team, like Silk or Galois (or myself), you can take liberties and not pander to everyone.
Yay! It might be me that resparked the interest in moving to Github, after I wrote a little patch to speed up "cabal update" and had to go through multiple hoops to have it sent to the cabal developers. Sending a pull request and having an automatic host for any of my forked changes is (IMO) much nicer, I can see myself hacking on cabal more often :-)
Ah, cool! I would really like to read your work when it is ready! Comments: 1. Okay, thank you :) I am new to this stuff. 2. Yes, I ran out of time. I made that connection fairly late in the writing process, so I just did not have time to add AFRP to the compiler. I plan to add this once I have implemented type-classes in Elm. When I wrote that section, I envisioned one or many signal functions (SF) contained within an *individual* node in the signal graph. This would require at least one special function (apply :: SF a b -&gt; Signal a -&gt; Signal b) which would connect the incoming signal node to the SF node. The "apply" function could potentially be a little bit fancier and map a signal function onto a bunch of nodes, but I have not thought this through in-depth. Switch would be able to switch out particular SF *within* a node, not changing the structure of the signal graph, but still changing how values flow through it. Does that clarify integration with switch? I also envisioned a version of AFRP that was push-only (discrete signals, data-driven, etc.). This entire idea needs more work and formalization, but based on the theoretical connection, I think it would be quite possible and convenient. 3. Good point. I definitely undersold that benefit. Although, if I understand you correctly, I *think* Elm still maintains such a separation. Try changing the sampling interval in this [example](http://elm-lang.org/edit/examples/Reactive/Clock.elm) to 5 and (1/50). Avoid using numbers with decimal points because they don't get parsed yet (a weird oversight, I know!). Changing the sampling interval changes the frequency of update, but the "clock" code is all independent of this. Does this separate time dependence from sampling intervals as you think of it? It may be possible to provide some built-in "smart signals" that appear to be continuous, or at least are more flexible than a totally fixed time interval. For example, it would be fairly straightforward to add a "time" signal that gets updates as quickly as possible (i.e. completing a "time" update triggers a new "time" update). From an efficiency stand-point, I suspect that such quick updates would rarely be desirable, so it may be better to specify an optimistic interval (say 60fps) and update as quickly as possible until you're getting 60fps. Would the thesis have benefited from a discussion of this possibility? Thank you for your comments and encouragement! And yes, I am planning on going back for a PhD in a year or two. It sounds like our projects have a lot in common, so hopefully Elm can be of use to you! The compiler is already open-sourced on [github](https://github.com/evancz/Elm), although I have not announced this until just now. I am planning on doing a more visible announcement in the next week or so (as soon as I stop fiddling with it and get it up on Hackage for easier installation).
 I agree; the more power that the "fat client" possesses the easier it is to interface with it directly from haskell. While this might be construed as "js generation", it would still leverage the power &amp; development momentum of the client side frameworks. Side note: My initial reaction to Ji was that, while interesting, it seemed monolithic. Making it compatible with the leading web servers certainly seems like a step in the right direction. I'm not sure how to further decompose it, but as you suggest use-cases would be very helpful
That is impressive. Not just the software, but the preparation they've put into it (a 10 minute vimeo intro, one line commands to add coffeescript and to deploy to a server they've prepared for public demos). It looked like it locked you into node.js, though. And they didn't mention authentications, which is a big question. I'd be interested to see where that ends up. The instant updating plus correction from the server is a killer feature, though.
At *no* point have I advocated using a tag in a release's version number.
One could try to push darcs into bitbucket as it supports hg and git already.
I'd tend to think we're either better off improving Patch-Tag or Darcsden; or forking Gitorious than trying to persuade somebody else to do it. I keep wondering if there's anything clever we can do with those GitHub bindings, somehow all that nice hosted fork, integrated tracking, etc; but still let people who want to use Darcs actually use Darcs.
Dude, it's hypocritical to call a whole class of people "insufferable pricks" and then special-plead that your own misrepresentation of jokes (still not corrected in your original article), straw men, and name-calling is justified because it gets pageviews.
This is a judgement call that the developers on the ground have to make. Your piece presumes that your point of view is correct, makes a straw man of the other side, and uses pointlessly vitriolic language to smear not just them, but the community of functional programmers that you somehow pin the responsibility on. Not cool.
As a killer feature for a github-like thingy, how about… decentralized? Think identi.ca vs twitter. Eventually, github is going to become evil, it would be nice to have a system that's robust against that threat.
I think this is a very interesting area for investigation. It's somewhat like the meteor framework that vagif mentioned. It also reminds me a little bit of how google wave worked. The approach I currently take is to serve a basic html scaffold which pulls in the required javascript (actually written as backbone/coffeescript app stitched together with require.js). The backbone collections are loaded over AJAX and are updated with with a WebSocket that receives events from the server. An implementation detail here that is quite nice - and should be possible in Haskell is that the AJAX and the WebSocket collection both live at the same URL on the same server. Using backbone gives me a nice event system that I can hook both user interactions and data updates into to change the DOM. If I could achieve the above in Haskell on both the server side and the client side (either by compiling to JavaScript, or with some sort of DSL that manipulates some sort of JavaScript RTS) that would be pretty close to nirvana :-) FWIW I don't think that having every client's DOM state stored on the server is desirable or necessary - if state can be reliably subscribed to from the client (and sensibly recovered if the connection is interrupted) then state should be handled as much as possible in the client. My current stack: python/eventlet &lt;- AJAX/JSON/WebSockets -&gt; coffeescript/backbone/jQuery Possible haskell future? Haskell &lt;- Transport (WebSockets/SSE/Ajax) -&gt; Haskell FRP running on JavaScript
I'm particularly frustrated by the slowness of "darcs get" as compared with "git clone". Can anyone explain what's going on there?
Thanks for the feedback! Three main sources of slowness that I can think of: * darcs fetches history files unless you tell it to use --lazy (or hit Ctrl-C when it tells you) * darcs fetches a bunch of little files individually * converting from old-fashioned (non-hashed) repositories is slow If you get a frowny face warning, then you're suffering from the third issue. We hope that deprecating our support for old-fashioned repositories will help push more and more users towards the future (ie. 2008). For the second issue, we have an experimental “packs” feature (which requires some deliberate use by the repository maintainer) to create a couple of big files that Darcs can download in one go. For the first issue, it's probably nicer to get in the habit of using --lazy. Also fetching repositories the second time (for example, making a branch), *should* be nearly instantaneous as it'd be largely making hardlinks from your local cache.
So here Ji is Transport and Haskell FRP part in my second diagram from earlier? Presumably this is being translated to something like &lt;script&gt; $(function(){ var el = $("#myid").onclick .... }); &lt;/script&gt; that's then executed in the browser somehow? 
"presumes that your point of view is correct,": True. "makes a straw man of the other side,": In what way do I misrepresent claims of the FP community? "uses pointlessly vitriolic language": Vitriolic, yes, but pointless is a judgment call. 
While it's currently very verbose, I'm growing fond of the simple JS FRP idea.
Roughly, yeah. The browser is connected via Comet or Websockets (whatever, as long as it can send/recv to the server) all the time. If the following Haskell code is run on the server: onClick el (\ev -&gt; alert "Hey!") Then it first generates a closure id (e.g. 123534) for that callback `(\ev -&gt; alert "Hey!")`, and sends something like `Bind "click" (Element xc9v87s) (Closure 123534)` to the web browser, which runs something like: $(getElement('xc9v87s')).bind('click',function(event){ signalToHaskell({ Event: 123534, EventData: event }); }); When the user clicks it sends the event. The server thread, when running `handleEvents` will handle this. Your standard event handling mechanism a la Gtk+ or w/e. Or if I run: el &lt;- getElementById "abc" … It sends roughly `GetElement "abc"` and then blocks, waiting on a "reply" Chan. The browser sends back `Just (Element 3bd43g)` and the thread continues doing whatever it was doing. It needs to generate these IDs so that the server can refer to objects in the browser. In my rewrite of Ji I am generalizing this so that the server can refer to any JS object, much like the normal Haskell FFI. You have some opaque type like `data Device` which refers to some C pointer or struct, and likewise we can do it for JavaScript objects (like Backbone etc.).
I think `--lazy` is like git's "shallow clones" (`--depth`) except darcs transparently makes the clone "less shallow" on demand.
Just a side-note, it would be nice if the examples you gave were online somewhere to try, even temporarily.
It is exciting to see Yesod headed this direction. I am looking forward to trying out the code.
Yes, git (and mercurial) will clone the repository with full history by default. So point 1 does not really hold, although it gives a useful workaround. I am pretty sure mercurial and git "pack" the commit log quiet often. Each packing gives birth to a checkpoint where the repository state is fully computed and flattened. Whenever you ask to get a working copy of a repository at revision R, you just travel the log backward to find the latest checkpoint before R, and apply subsequent diffs to it (up to R). Very fast indeed.
You conveniently ignore the "I think" before my statements.
What i could never remember is all those v v v === markers markers for clashing changes. It would be good to have human readable tags there. Maybe something like === original === === working copy === === patch bla bla === 
Perhaps someone familiar with deploying Yesod on Heroku could throw it on a free instance?
Thanks, I had not thought about that non-bottom values can't cause any problems.
Hmm. The [Haskell Platform Mac OS X page](http://hackage.haskell.org/platform/mac.html) claims that the latest HP works for 10.6 or later, and for Xcode 3.2 later or 4 or later. Does this post mean that those claims are not true? If that is the case, someone should fix that web page promptly. Also - that means getting out a new 7.4.*-based HP release ASAP should be a high priority.
Thanks for sharing. I have been looking for examples of projects of Haskell web frameworks that have weight in the front end.
Shameless plug: Also take a look at the [threads](http://hackage.haskell.org/packages/archive/threads/0.5/doc/html/Control-Concurrent-Thread.html) package which offers similar functions for all the different forks.
Awesome! I am doing my best to remain ignorant about git for as long as possible.
I think it'd be best to keep the javascript dsl looking like haskell. The do notation is very powerful in this case. As for the other stuff, I think Michael said persistent and template could handle the most of it. He probably left everything explicit for us all to see.
Looking at these parts: &gt; Let's pull in the todo items via Ajax. ajaxJson takes a type-safe route and returns two values: the data pulled from the server, and the name of the function that can be used to **reload** the data. We'll use that later, when we add new items. ... &gt; Our last parameter is the **reload** function we got earlier. This says that, each time we submit the form, we want the data to be reloaded from the server. We could theoretically do a local update here instead, but for future features it will be important to have the server-generated ID available. Had me thinking of the [meteor framework](http://meteor.com/screencast) vagif linked to yesterday. It abstracted over server updates. Basically client updates were immediately treated as fact, updating the ui, and rolled back, updating the ui again, if the server refused. I don't think this fits yesod as well, since in yesod you explicitly write your route responders, but the instant updating of the ui is an important feature for giving the user a snappy feedback. So if we don't attempt to abstract it away, how could we use the reload function or other tools to give the user an instant ui update that could potentially roll back? 
I miss darcs :-(. The interactive cherry picking was *so* nice, git's weird workalikes are just awful.
Things like "jsIf" and the like are hideous, and coding nontrivial client-side code would become an incomprehensible mess of nested "js*" calls in this notation, I feel like.
Heh, well I'm trying a bit harder to push the [consenting adults](http://www.reddit.com/r/haskell/comments/smwft/cabal_is_now_hosted_on_github/c4fd6fz) thing. In other words, while I very much like Darcs to have users, I want to make sure we are doing the Right thing, by working a bit harder to make sure users have a good idea what they're in for (small team, bugs, scalability issues); and what they're missing with GitHub (extra collaboration help) and also that they're not just being needlessly afraid of Git.
No! It's not principled! It's totally broken! Please do not use it! (Enough "!"'s?) As others here have said, the ordering of versions with these tags is not well defined. If you do use it and you're lucky then it'll just be ignored, if you're unlucky it'll go horribly wrong (for example, the tags are not reflected in install paths).
&gt; those versions are incomparable to any versions without tags. That seems like exactly the semantics I want from the tags in the first place. What do you think the constraint solver is supposed to do?
Remember that historically the Setup.hs is the standard entrypoint for interacting with a package. That means your Setup.hs has to just compile, distro packaging tools etc do not know that your Setup.hs might want some CPP flag. It's true that using Setup.hs code can be fragile and in retrospect this doesn't make it a good entrypoint. The only way to change this is to change what is the standard entrypoint, e.g. to be the .cabal file and have info in the .cabal file be used to declare the dependencies of the Setup.hs If you really need conditionally compiled code in Setup.hs, see how the gtk2hs packages do it. It's not very pretty.
Cherry-picking during commit was where it took hold, but once darcs' view of patches as individual (by default) independent changes to the codebase took hold, I found cherry-picking other operations a great way to tidy up code -- like an orthogonal dimension of annotation, that includes time. Git is ubiquitous. That's about it. And fast .. with darcs I had to be something of a purist about what went into the repository, but sometimes you just need to shovel crap in there, and let windows users access it, without resorting to SVN or scaring them off. "git" is also a lot easier to type than "darcs" (try it!), and perhaps easier to say. That said, the user interface is *horrible* -- I don't find it discoverable at all, or remotely well documented. What I miss most from darcs was a sense of freedom - I could do some quite adventurous things, and darcs would generally do the right thing or at least not make me resort to tarballs and shell scripting to get my repository back how I wanted it. With git I generally just grit my teeth and try to make up for bodgy merges with apologetic commit messages.
&gt; The ordering of versions with these tags is not well defined. So what? Why do we need a linear ordering on versions? Isn't a separate linear ordering for each set of tags enough? &gt; for example, the tags are not reflected in install paths Well... dang. That looks like a totally real problem, then.
Thanks for this pointer. It was educational to see how simply JSON interaction can be done, and apparently that's just before taking advantage of the newer GHC.Generics derivings for Aeson.
&gt; Why do we need a linear ordering on versions? Because overriding depends on it: If you import something, should ghc pick foo-1.1.0 or foo-1.1.one? Should cabal link to the former or the latter when installing a package that depends on foo? What you're asking for would mean having to specify your preference *each time* you use the package. In contrast, when you want to install a self-patched package you can call it foo-1.1.0.1 (note the fourth, additional, number) *once*, at install time, and the rest works just like you expect it from everything else. If you want a version number for "always bleeding edge", use 9999, Gentoo does that all the time for rcs versions. tl;dr: You don't want that misfeature, you want a predictable ordering.
Well, "procedure" is an appropriate term. SICP makes the point of using "procedure" specifically for algorithmic widgets, while using "function" specifically for mathematical widgets.
I buy the claim that OOP beat nothing at all for programming-in-the-large, but that's just, I think, because OOP went hand-in-hand with structuring modular code, and the benefits attributed to OOP per-se in programming-in-the-large were actually probably benefits accruing from modularity in general (which you can get without OOP just fine, but not necessarily, for historical reasons, in pre-OOP languages).
&gt; If you import something, should ghc pick foo-1.1.0 or foo-1.1.one? Should cabal link to the former or the latter when installing a package that depends on foo? Both GHC and cabal should pick the latest version with no tags. &gt; What you're asking for would mean having to specify your preference each time you use the package. Yes, that's what I want! I don't want my unstable xmonad-darcs to be building against my unstable X11-darcs bindings unless I explicitly say so.
ME ME! I got accepted! w0000t!
all of them look great. really hoping for some cabal relief!
Really exciting projects this year.
Actually we're very close. Mikhail sent me some updated patches about 20 days ago. About 10 days ago I sent him back some simplifications that I made. This simplified version of the patch set involves far fewer code changes and I would be happy to merge them. Mikhail got back to me with his comments on my changes 11 hours ago. I'll now go see what he said... [update]: he's happy, so we'll merge that as is. At the moment the output from all builds is randomly interleaved but we'll look at hiding that and just displaying summary info. Mikhail also has some additional patches to cache the compiled setup binaries which speeds things up a bit. On a related issue, the cabal repl patches are also getting close. Part of them has been merged already (a few weeks ago) and I'm working on the remainder.
The problem is that because their Ord instance does do what you ask for it means we cannot even put these things into a finite map.
Am I missing something, or there is huge overlap between the following projects : * Scoutess - a build manager for cabal projects * Sandboxed builds and isolated environments for Cabal * Enable GHC to use multiple instances of a package for compilation And for the two first, what is their difference from `cabal-dev` ?
As a newcomer to Haskell, I've also found the Haskell community to be extremely helpful!
I was researching HXT to create one myself today when I found this.
The type system I am working on for Peg will statically type as much of the program as it can, and fall back to run-time typing otherwise, issuing a warning during compilation. Ripple might benefit from such a type system.
Y U no use http-conduit?!
I agree the overlap between "scoutess", "sandboxed builds" and cabal-dev (existing, current solution as a wrapper around cabal) looks large. I hope that these SoC projects will manage to improve cabal core itself. However, "enable ghc to use multiple versions of a package" is quiet different IIUC. EDIT : actually I misunderstood this project description, so what follows is about an other project. See Duncan clarification below. The motivation here would probably be to accept to build more packages. Say you are the author of USER-PACKAGE and have the following dependencies : USER-PACKAGE / \ A B / \ C.a C.b Currently this won't build for good reasons. However, if and only if USER-PACKAGE does NOT carry constructors from A to B then it could be safe. 
Yeah, about that. We need to have a little chat.
If you want to use parallel cabal-install now, you can get the code from GitHub: git clone git://github.com/23Skidoo/cabal.git cabal-parallel-install cd cabal-parallel-install git checkout parallel-install cabal-dev install Cabal/ cabal-install/ # Don't forget the trailing slashes Usage: cabal install -j4 yesod
Thanks for the clarification
Yeah, i thought i was typing in the Text description (not link) area. :) 
Yo dawg, I herd you like links so i put a link into your link so you can link while you link. 
&gt; While 46% of the subjects agreed that this feature (type inference) was helpful when writing code, 85% of the subjects agreed that it leads to programming errors. Maybe they meant that it led to the compiler rejecting programs that would have compiled okay, and maybe even worked correctly at runtime, in another language. Simon Peyton Jones likes to talk about a venn diagram with two circles for "programs you want to write" and "programs the compiler will accept" -- the goal of the Haskell language designers is to make the overlap between those two circles as big as possible, and make the outer parts as small as possible. If there are programs you want to write, but the compiler won't accept them, you'll get frustrated and feel that Haskell is holding you back. If you make a mistake and accidentally write a program that's wrong, but the compiler accepts it, you get frustrated with errors at runtime, that perhaps could have been caught at compile time. 
Ok, thank you for correcting this false assumption. 
I simplified bc a little bit more: bc guess secret = (bulls, both - bulls) where bulls = length . filter id $ zipWith (==) guess secret both = length (guess `intersect` secret) 
Ah, I see.
Git branches permits you to better manage the diversity of alteration your project can be undergoing: a simple hashcode identifies a precise state of the project, if two developpers checkout the same hashcode, whatever the source is, they are sure to have the exact same state. I find it more convenient than sending patches through mail or whatever. Plus reorganizing commits is still possible if you have to (with git rebase), even if it's not the default behavior to have unorganized patches (as darcs does). Another advantage is that you don't have to copy your full repository to make a branch. But Darcs is indeniably simpler than Git, I'd agree with that.
See also Chris' [separate reddit thread](http://www.reddit.com/r/haskell/comments/stcwz/live_code_update_for_haskell/) about this.
Why not alias it to 'd'? I have 'g' for git, since I use it so much. Oh, and 'c' for cabal(-dev).
dons' [plugins](http://hackage.haskell.org/package/plugins) sort of provides hotswapping, though the package could use some modernising love. See also [plugins-auto](http://hackage.haskell.org/package/plugins-auto) for automatic hotswapping using inotify. I think [this GSOC](https://www.google-melange.com/gsoc/project/google/gsoc2012/mdittmer/18001) is meant to create the architecture for hotswapping.
As is common knowledge, nearly 99.999% of all Haskell developers in India have less than 6 or more than 20 years of work experience.
yes, we specifically mentioned in the proposal wanting to swap out new request handlers automatically. Great stuff Chris, please get in touch.
I intend to build a live coding interpreter on top of what we achieve in the [multiuser browser GHCi project][1]. My ideas on live coding are a bit different than what you probably have in mind, because I want to give live code a precise semantics. The point is that the program has to be aware that it is being live coded, otherwise you run into semantic ambiguities. Unfortunately, this also means that it's only possible on the value level, i.e. no hot-swapping of type declarations or type classes. [1]: http://www.google-melange.com/gsoc/project/google/gsoc2012/shapr/18001
which companies in india do use haskell ?
Symbol ~ Symbol does not work?
the underlying plugins library has not received much attention in the last year, aside from some patches I recently applied so that it would build under recent versions of GHC. However, I recently became the official mantainer and do plan to take a closer look at it in the next few months. I have big plans for plugins -- but there is some other work that I need to attend to first.
Cool thanks. I found several optimal algorithms for bulls and cows defined here http://bulls-cows.sourceforge.net/bullscows.pdf 
Great :) Is the exponential conflict situation solved?
Doesn't the list of instances for TEq grow quadratically in size? I can see that getting difficult quickly, for example in a scenario where you use two records with 5 distinct fields each. The writers of the original records have helpfully provided the 2x25 instances needed to use them separately, but it is up to the user to write (or hopefully generate) the missing 50... Anyway, hooray for Dutch (Utrecht even?) metasyntactic variables! 
Sure it works, but where would I put it? I'm assuming you're talking about the TEq type family. A type equality constraint requires the types to be equal, but I want to 'pattern match' on the boolean resulting from an equality check. The most common trick is to use functional dependencies, like this: class TEq x y b | x y -&gt; b instance TEq a a True instance b ~ False =&gt; x y b But I don't think you can go from that to a type family, and I don't want to start using type classes with fundeps as my type functions, since then I'd have to go all the way and use them for all of them. So as I say in the comments, I hope GHC 7.6 will eventually provide some type equality function, similar to the `(&lt;=)`, `(+)` and `(*)` that will work on type level naturals.
ah.. makes sense. Then I take that part back!
I think `Record ('("noot", String) ': '("aap", Integer) ': '[])` can be written as `Record '[("boot", String), ("aap", Integer)]` or at least `Record '['("boot", String), '("aap", Integer)]`, both of which are substantially less off-putting. However, I would instead define a `k := v` type, and use `Record '["boot" := String, "aap" := Integer]`, which seems even better to me.
You're right about the list syntax. I though I tried it before and couldn't get it to work, but it did work now. I've updated the gist. I think what went wrong the first time, is that a space is needed between the opening square bracket, and the parenthesis of the tuple. So `'[ '(` works, but `'['(` doesn't. As for the `(:=)` type, it does look nice, but has to be a data type, since it starts with a colon, and I couldn't get that to work. It should be possible, I think, but I'm not sure how. I've tried some type synonym operators like `=:`, but they didn't look as nice.
Yes, the `TEq` type family is not a serious option. As I've mentioned in [another comment](http://www.reddit.com/r/haskell/comments/stcmq/extensible_records_using_ghcs_new_kind_level/c4gyfbp) I hope a type equality function will be provided at least on `Symbol`. The type class option brings it too close to HList, and I wanted to do things with type families only. Oleg has an interesting overview of the [options for type equality](http://okmij.org/ftp/Haskell/typeEQ.html), including a form of generic programming on the type level. He mentions the approach would be nicer with data kinds, which we now have! It still won't help for `Symbol`, though.
&gt; Haskell has been designed for parallel and concurrent programming since its inception. Wait, what? Do you have pointers to explicit mentions of parallel and concurrent programming during the initial committee meetings, or any reasonable historical source on Haskell early design? Or at least in the Haskell 98 report? I find this assertion surprising, as I don't remember particular mentions of parallelism or concurrency in, say, "being lazy with class". Maybe I just forgot about them. PS: the shootout link in the "Concurrent Haskell" section ("is very fast") is out of date, it should probably be changed.
Haskell *was* kind of designed for parallelism in the sense that people back then thought being pure was all you needed.
How? Sounds interesting.
If I understand correctly, you would like to avoid switching from "State s" to "StateT s IO". That would make your newtype capable of implementing the type of IO operations you want, but the type also becomes capable of encoding any type of IO. Since you have a newtype you can hide the constructor and provide access only to safe constructors (no monadIO instance). This can be workable if you only allow a specifc set of IO operations. When you want multiple sets of operations depending on the circumstances, this approach gets very tedious since it is not composable. I'll give you an outline of what I'm doing to handle composable IO, but you may not want to get into that, since it is a bit of work to understand and use, and it might lead to some performance issues depending on your needs. I've started using free monads to represent subsets of IO. I define functors for all the operations I want, for example data GetTime a = GetTime (UTCTime -&gt; a) data SendMsg a = SendMsg ByteString a Then I use the type Coproduct GetTime SendMsg to represent doing either operation. I use Free (Coproduct GetTime SendMsg) to represent arbitrary sequences of these two operations. Now Free is a monad, but it has performance issues similar to appending at the end of lists. That has been adressed nicely by Edward Kmett's Control.Monad.Free.Church.F. So in the end you get F (Coproduct GetTime SendMsg), and you avoid the quadratic performance issues. However converting GetTime to F (Coproduct GetTime ...) needs to be done explicitely unless you invest in additional typeclass machinery. Then you need to implement the wrapper :: F (Coproduct GetTime SendMsg) a -&gt; IO a Peharps I could elaborate more if people are interrested.
When I wrote my game contest server in Haskell, I wrote a pure implementation of the game in terms of a game state (`data GameState = ...`), a concrete representation of the legal moves (`data GameMove = MoveA | MoveB ...`), and a function to take the game state and the next move and output the next game state (`advance :: GameState -&gt; GameMove -&gt; GameState`). This included taking a random seed in at the beginning and using that throughout the game. I then wrote a datatype with record fields that represented asking all the "questions" that you might ask of a player through the course of the game, which essentially had the datatype `PlayerInfo -&gt; Question -&gt; IO Answer`, where the answer either was one of those moves or something trivially translatable into those moves. data GamePlayer = GamePlayer { claimNextSpace :: GameState -&gt; IO (Int, Int), attackPlayer :: GameState -&gt; IO Bool } Then I could create data values that represented a local console player, a remote API player, a local AI player, etc. A small driver took the original game specification as a datatype, a series of players as values, and basically implemented a "while the game is not over, get the next move, apply that to the game state, and recurse". This is about five lines and unashamedly lives in IO. (These types are conceptual rather than real. For instance, `advanceState` is probably rather an `Either` to handle illegal moves. The type system may be able to capture some illegal moves, but it probably can't reasonably capture all of them for non-trivial games. The real system actually had a distinct type for the question asked of the player containing only the state the player was allowed to see, etc. But this is the core of the structure.) I fiddled with clever stacked monads and stuff but I found in the end that a simple, very direct specification of the game, the driver, and the small driver loop was perfectly adequate, and very clear. Pure things were pure, and only what had to be in IO was. Simple DRY goes a long way. The only monad _qua_ monad that appeared was the IO. The only kooky thing I encountered is that for a two player game, which mine happened to be, two players is enough to be uncomfortable at the repetition of code for Player 1 and Player 2, while not quite bad enough to justify the further complexity of abstracting the current player away. (That is, I tried, and it was a net loss in clarity.) For 3 and above, or arbitrary numbers of players, you'll have no choice.
Wouldn't `data k := v = k := v` work? That's the same way the tuple type is defined. Or is the problem that it'd have to be `'(:=) "boot" String` or similar?
piff there are is a whole extra year in there to have learned and not made haskell
Cross-post it to /r/programming. Posting it here is just preaching to the choir. :)
What you want is the [Prompt monad](http://hackage.haskell.org/packages/archive/MonadPrompt/1.0.0.3/doc/html/Control-Monad-Prompt.html). This was designed to handle exactly the problem you are posing and it does so in a way that is quite elegant and brilliant. The motivation behind the Prompt monad is quite simple. You have some sort of set of capabilities that you want to expose to the user, but you don't want to give them a full-blown access to the IO monad or something similarly unbounded. What you can instead do is specify an interface that the user of the monad is allowed to call using a GADT. Then the user of the prompt monad creates a monad specifying what capabilities they want to run. The brilliance of the Prompt monad is that the user only specifies the capabilities through a data type (a free monad, to be precise), and you as the library writer can completely swap out the back-end to whatever you want. Your back-end just takes that data type and "runs" it doing whatever you want those capabilities to do. The GADT you originally created defines the contract of what requests you can get from the user and what kinds of responses you must return for each request. The best example of how to use it can be found [here](http://paste.lisp.org/display/53766). I find it's pretty easy to begin from that example and transform it into your own. The underlying data type might look a little bit weird. It's because it's actually a church-encoding of: type Prompt p r = Free (Density p) r I used to understand what that type "meant", but I haven't had time to refresh my understanding, so I will cop out and "leave it as an exercise to the reader".
I'd be interested in elaboration.
 forever $ do ersp &lt;- try (HTTP.simpleHttp url) case ersp of Right rsp -&gt; maybe (putStrLn "decode json failed.") (\x -&gt; (void . atomically . forcePutTMVar state $ x) &gt;&gt; threadDelay interval) (decode rsp) Left err -&gt; print (err::SomeException) ? or (my preferred style): forever $ either (maybe decodeFailed decodeSuccess) handleError =&lt;&lt; try (HTTP.simpleHttp url) where decodeFailed = putStrLn "decode json failed." decodeSuccess x = ... &gt;&gt; threadDelay Interval handleError err = ...
"as productive" is a generous title for this post. I would've called it "why Haskell + Snap is better than Ruby + Rails", because basically every paragraph lists something that the author prefers in Snap/Haskell over RoR, except the last paragraph, where he admits that the RoR community is way bigger than the Snap community.
awesome! there were a few things that made me back off of snap and just try a plain wai/warp approach: documentation/tutorials. for example, one of the published snaplets is for redis, but i could not find a minimal example that took me through using this. i couldn't figure out how to make the snaplet and integrate it. i suppose i could have just hit stackoverflow...but still, i think something as new and radical as snap needs to be explained very well. the core snap product is indeed well documented anyway, i will circle back on snap as these snaplets etc mature....snap is an amazing tool and the people who work on it are a pleasure to deal with 
http://www.instapaper.com/text?u=http://www.altdevblogaday.com/2012/04/26/functional-programming-in-c/ This alternate link will work without requiring javascript. This comment generated by an [automated bot](/r/link_unscripter).
This also looks like a feasible approach. Thanks for sharing.
There's nothing new on that front in 2.8. On the other hand, the situation has been better than in darcs 1 for quite some time, and we're starting to see what a good solution to the whole conflict mess could be. So, it's not as bad as it used to be, and we see where we're (eventually) going.
Nice writeup! To address #5, templating: I've always found Erector to be more intuitive than erb / haml. I think of blaze-html as the Haskell version of Erector and I like it better than Heist. I like it because it allows me to write pure haskell, which means I can combine attributes just like I can combine in Haskell...so making partials / layouts / custom functions becomes really easy and requires no special knowledge of new syntax. With other templating languages I feel like I spend more of my time trying to work around poor syntax.
You forgot to speak about the ActiveRecord utter crap, it fully deserves its own paragraph. ORMs are all broken by design, but Rails AR really is the king in this land. Anything clever, efficient or powerful offered by your SQL engine is not only unsupported by ActiveRecord, but prohibited by pointless object-oriented pseudo-encapsulation. 
You can short-circuit this using MaybeT instead of using the full exceptions semantics: http://hackage.haskell.org/packages/archive/MaybeT/0.1.2/doc/html/Control-Monad-Maybe.html
Yes, that's what I tried, but I couldn't get the lifted version (either `':=` or `'(:=)`) to parse.
Valid points about the power of hierarchical routing, which *raises* the question how that relates to type-safe routing. One solution is to represent hierarchies with separate "sitemap" types: data ItemAction = Show | Create data Sitemap = Item ID ItemAction As opposed to a single type: data Sitemap = ShowItem ID | CreateItem ID Then in your routing function you can pattern match on `Item` before delegating to the `ItemAction`: route (Item itemID action) = do item &lt;- fetchItem itemID handleItem item action It should also be noted that with boomerang you can easily parse the `/item/id` first, without having to repeat it, and then parse the `/action` independently.
Without a way to overload type constructors, String keeps an edge because it can be pattern-matched. It would probably be an acceptable trade to allow different types to use the same constructor, in the presence of required signatures.
While it probably wasn't an explicitly stated goal, the designers of Haskell were well aware that pure functional programming had benefits for parallelism. Indeed, around the time that Haskell was being designed, Simon PJ was working on a custom architecture for parallel functional programm (GRIP = "Graph Reduction In Parallel"). The circuit board is still on a shelf in his office.
Someone to tell us how Yesod can be inserted in the comparison? (I know it a bit, but I've never used Snap, neither have I used Yesod on significant applications)
I just noticed that the "other discussions" link at the top of the reddit comment page points is in fact to a submission from dons 4 years ago.
Cool, it seems function combination style can go a long way, but IMHO, it's not so straightforward as procedure style code for this kind of program.
This is based on very very limited experience with Yesod (as in, I've tried it a couple of times, but never built anything large with it). I think the main difference is that there is a lot more convention in yesod, and in general it is a bit more of a framework and less of a library. This means that there are places it expects things to go, and a little bit of magic going on if you don't know about it beforehand. This is not a good or a bad thing, it really depends on your preference. It also has an ORM, which may be a good or bad thing depending on your opinion. The other thing is that there is a lot more effort to encode everything in static types with Yesod, so for example the default templating library, hamlet, is statically typed, whereas Snap's default, Heist, is not. Of course, these are things that can be swapped out, but they are the defaults. In general, I think they are both phenomenal frameworks, but they each have a really different feel, which is a really good thing! Personally I like Snap a lot better, but I know plenty of people who feel the same way about Yesod, so I think the only way to know is to try using both of them.
I had not seen Prompt. It is very similar to what I'm using. Its Lift is the same as Coproduct. At this point I'm not sure why with Control.Monad.Free.Church.F we need to apply it to functors, while Prompt is applied to a GADT which is not a functor.
Hamlet is not statically typed. The urls are statically typed. Hamlet can be integrated into executable at compile time, greatly improving performance (no files are read from slow hdd).
You don't need to know anything about category theory to use Haskell, Monads are an easy thing with a scary name, I never had to learn anything about GADTs to use Haskell's type system ...
I have not tried it yet, but I suspect it will work without too much trouble. If you have the Haskell Platform installed already, maybe give it a quick try with: cabal install elm and just see what happens. If you get a build error, you can report it here, and I'll help you debug. Later today, I may try to install in a fresh Ubuntu 12.04 as well, so I can report back if I run into any trouble.
Why use =&lt;&lt; over &gt;&gt;= and reversing the arguments?
I do not see any problem with hamlet too :)
I disagree. Before I understood monads, I was severly crippled in writing Haskell code. For anything but the most basic stuff (code without monads), I could not get the code to compile because of various type errors, and figuring out where it went wrong was a nightmare. 
Huh... not sure I agree it looks cleaner, but thanks for answering.
Just tried it. Installation works fine if you use this command: cabal install elm --constrain="containers==0.4.1.0" --force-reinstall I have updated the README to contain this advice. Good luck :)
Though it doesn't support function composition (I wish it did :-) C++ does treat functions (function pointers, function objects, lambdas, etc.) as first class citizens. You can pass them into functions, and return them from functions also. It's not as clean as it is in a real functional language, but it's pretty good.
How can a language support passing in and returning functions, but not function composition? Are you not allowed to use a function you've been passed?
The article talks about something higher-level than that. It's a worthwhile read... you should re-consider.
Well it of course supports calling those functions :-p What I meant was there's no real clean way of expressing something like void silly = f . g I suppose you could do some tricks with operator overloading to compose functions, but it's not nearly as easy to use like in a functional language.
I agree now, it's not straightforward for writing, but it's definitely straightforward for reading.
&gt; [...] first-class functions were not mentioned at all; these are the hallmark of functional programming. I'm not sure I agree. I've recently taken to this simple description of what functional programming is all about: *context-free semantics*. First-class functions aren't a requirement for context-free programming. I see no reason, apart from tradition, that functional languages must be higher order. I admit that my opinion is as subjective as any other, though, since we're just talking about the meaning of a word.
"My understanding is that in theory if you could enumerate all possible interactions with the outside world" But you can't.
I'm afraid that this is untrue, in Haskell: there are a finite fixed set of primitive functions that allow you to interact with the "universe". Opening files, calling OS primitives, etc... This is the case with pretty much every language, which is based on the combination of a finite number of primitive constructs.
Yes, but as soon as you allow the ability to call foreign functions into the language, the number of ways to interact with the world becomes unbounded.
`cabal update &amp;&amp; cabal install elm` worked out of the box on an ubuntu 11.04 with ghc 7.0.4 (installed as part of haskell-platform)
&gt; What needs to be done to create a denotational semantics for IO? A free monad representation of `IO` is in itself a good denotation for it, I believe. &gt; Is a free monad-based model for IO practical to implement as a replacement for IO as State? I think Edward Kmett's article explains pretty well how you would go about implementing something like this using existential quantification on the input and output types of FFI operations, and create an open system which can be interpreted by the runtime. One problem that I can see is how to ensure that the whole thing gets compiled to efficient code. In the current implementation of `IO` in GHC, `IO` operations are reduced down to the actual FFI call (or PrimOp) without any overhead. I'm not sure if the same thing could be achieved when using a free monad representation for `IO`. Essentially, the whole structure needs to be fused away by the optimizer, leaving only the sequencing of primitive operations.
I would love that. And not just the IO monad, but all monads should be free monads. I think this is what algebraic effects is about. See [eff] (https://github.com/matijapretnar/eff) and [Frank](http://hackage.haskell.org/package/Frank) But this is all very new, so that probably answers the why.
Free monads are basically just CPS, that's very easy to optimize.
There are at least two reasons this doesn't work. Firstly, the last associated type synonym `FunDep x y = b` doesn't work, because `b` is not in scope: all type variables at the right side of a type family instance must be bound on the left side. Secondly, the type family instances `FunDep a a` and `FunDep x y` overlap, and this isn't allowed, not even in the presence of functional dependencies.
I'm probably misunderstanding (I'm not familiar with free monads, for instance, just inferring from what I've read here) but... If you can resolve everything at link time, then there are only a finite number of names to resolve. And since you know which names to resolve already at compile time, basically the particular free monad (at least the set of operations - not their semantics) can also be determined at compile time. This could still be viewed in the operation-name-is-a-parameter model, but that function would be evaluated at compile time (and fully resolved at link time), not at DLL-load time.
Oh, I suppose maybe you could (with significant practical difficulties) generate an appropriate IOF functor at *compile* time by just generating it to include all of the library symbols by name. Getting the types right would require parsing and understanding C header files, and it would be slow as molasses, but it could work in theory. It's certainly impossible to define such a thing at library definition time, which is what I thought you meant.
But it's not unbounded. The number of interactions is precisely the number of FFI bindings, which may be huge, but is still finite. And I already proposed a toy example whereby the FFI translates the enumerated interactions into the data type and interpreter.
I intended the free monad formulation to be a denotational semantics, not an operational semantics. The IO data type **should** be independent of the interpreter function because there is an unbounded number of ways that you could interpret it. For example, I could I interpret it by hand using pencil and paper and calling a psychic hotline to retrieve all input requested from the data type and taking all output generated by the program and snail-mailing it to my friend. It would be a pretty weird interpreter, but it would be just as valid as any other interpreter. Even the "single" interpreter example I gave above is deceptive because it's not a single interpreter. If I type in a different input to the program, the interpreter has changed (because the input it receives from the world is part of the interpreter). If I run the program on a different computer, the interpreter has changed (because it is having effects on a different computer). If I run the program at a different time, the interpreter has changed. If my computer is overworked because it's running an expensive background computation and the program takes longer to run, the interpreter has changed. What the IO data type does is abstract over all possible interpreters (because no two interpretations of the program are alike anyway) and only specify the bare minimum needed to run the program (i.e. it needs some input here and provides some output there). After all, the point of denotational semantics is to allow us to say "These two programs are (or are not) equal". I can look at two IO data types and prove that they represent the same program and that, given the same interpreter, they will provide the same result. When we specify a denotational semantics for IO, we are basically saying what things we care about being equal and what things we don't care about. So when I specify that I want to represent my program as a series of inputs and outputs using the particular functor I defined in my post, I am declaring what things I care about when defining equality and what things I don't. For example, I'm saying "I don't care how fast or slow the program runs", or "I don't care what computer the program runs on" or "I don't care what time of day the program runs at", but I do say "I do care whether an input is followed by an output." And the interesting thing is that if I **did** care about those other concerns (like assembly, architecture, or user input), I **could** build those into the IO functor and extend the representation. Edit: You also don't even have to interpret it at all. An "interpreter" could just pretty print the program or compile it to Javascript, or rebuild the exact same data type (in the spirit of "foldr (:) []"). By specifying the denotational semantics this way, we give the user of the data type considerable freedom in how they want to interpret it, although, like I mentioned, there is nothing keeping you from defining the data type in a more stringent way.
This is very similar to what an application of mine does. The pseudocode in your case is: download: try simpleHttp if success: handle result goto wait else: handle failure goto download wait: threadDelay interval goto download Side note: I wouldn't recommend trying again immediately, as the server might be overloaded. But I'll play along. This sure makes you wish you had `goto`, doesn't it? Well, you actually have something even better than `goto`, and it's not a complicated monad transformer, either: tail calls! You can write it like this: let download :: IO loop download = do ersp &lt;- try $ HTTP.simpleHttp url case esrp of Left err -&gt; do print (err::SomeException) failure Right rsp -&gt; handleResponse $ decode rsp handleResponse Nothing = do putStrLn "decode json failed." failure handleResponse (Just r) = do void $ atomically $ forcePutTMVar state r success failure = do -- I'd recommend a delay here, but I'll play along download success = do threadDelay interval download First of all, how is this *better* than `goto`? Well, for starters, the existence of an infinite loop is checked by the compiler, thanks to the `:: IO loop` signature (`loop` is just an arbitrary type variable name, just like `a`). More precisely, `download` has a polymorphic return type that comes out of nowhere. This sounds absurd, but the reality is, `download` should never return. If it did return, then anything could happen (this is called the [principle of explosion](http://xkcd.com/704/)). When we declare that `download` does not return by giving it a polymorphic return value pulled out of nowhere, the type checker will ensure that any tail calls it makes are polymorphic, too. If we accidentally forgot to put a `success` or `failure` call somewhere, we'll get a type error. Second of all, you can parameterize "labels" when you do tail calls. If you wanted a `wait` label that subsequently calls `download`, you could give it a parameter indicating the amount of time to wait. However, it's still spaghetti-ish. It'd be nice to see a monad transformer (really, an application of `ContT`) geared toward imperative constructs like `while` and `break`.
Technically, the definition of free monad does not specify its implementation and the one in the "free" package is not CPS. However, Edward Kmett already showed how you can automatically translate it to its Church encoding by using Codensity to get the CPS style for free (as noted in the other comment next to you).
Some brief thoughts: &gt; how do I quickly create, test and document a database migration? I've used the dbmigrations package (on hackage) in the past, and it worked very well - the migrations actually have dependencies, so you don't necessarily even have to have a strictly linear path (though you can). They use SQL, but that really doesn't bother me - if I am messing with a database, I prefer to use the best language for the job. It hasn't been updated for a while, and one project on the backburner is to make sure it works with current libraries etc. &gt; how do I add a PDF or Excel view for certain views? For excel, just generate an html table and give it the right extension. Not sure about pdf - there was an HsPdf library I think, but I don't know where it is right now. &gt; is it easy to guess for different team members where say a specific view is located? Sure. Except at least with snap, you don't have "views", you have templates (which live at a consistent path, so you can tell simply by the name) and you have splices that go with them, which I always put into a splices module hierarchy, so the place to look would be Splices/WhateverTopic.hs. You can also just look at the imports to see where it comes from (good habit is to explicitly specify what you import, so you know exactly where everything comes from). &gt; are there integrated test solutions that can easily use a test database? This is probably a weak point. Most of the really amazing testing infrastructure is for testing pure code (ie, quickcheck etc). There is HUnit, for unit testing, but there isn't much in place for database stuff. &gt; can I log into the environment, and run some custom code, that interacts with the database? Sure. If you use cabal-dev (you should!) then cabal-dev ghci loads you into a live environment where you can run any of your functions. You should make you database functions not involve the web code (this is good practice), at which point you can run any of those functions, and anything else, interactively. &gt; can I setup an asset pipeline that automatically compresses my CSS, HTML and Javascript? I'm not sure. I've had stuff that automatically processes LessCSS (though with older libraries, not sure if it has been updated), that could be adapted trivially to do compression. &gt; can I add a resource/plugin/gem/extension/whatever that interacts with any of the common full text search engines? Not sure. Postgres does full text search, so I haven't run into this. &gt; can I easily generate a default directory structure? snap init wil do this. Also, any snaplets (extensions) that you include will automatically add files they need into the snaplets hiearchy. &gt; can I easily use the apache support for sending a file as a download? I don't use apache (nginx), so I'm not really sure what you are talking about! &gt; can I easily convert uploaded images to different dimensions and formats? is this functionality i can easily add as an extension/plugin/whatever? This is a somewhat weak point - what I've done (in a couple projects) is just use imagemagick. This ends up being a couple lines of code, which is pretty minimal, and certainly doesn't slow one down! &gt; can I easily scaffold a temporary interface based on a database structure? Nope, none of that dynamic stuff is really possible. &gt; is there a nice default caching layer that integrates with memcache or something similar? I don't believe so, though I could be wrong. This would be quite easy to implement with snaplets' so it might be a fun project! &gt; is there good documentation? Will a random question on google lead to the right answer? No, this is probably the single biggest problem right now! 
One approach would to make a type `FfiHandle a b` with no exported constructors representing a foreign function with (uncurried) arguments `a` and result `b`. Then one `IOF` constructor `forall a b. FFI (FfiHandle a b) a (b -&gt; x)` would suffice. The compiler would generate an FfiHandle for each foreign import, presumably by wrapping the function's address. That you can't construct your own handles ensures that no dynamic linking is required. 
The question of whether it's finite is different from the question of whether they can be enumerated (right?). Which one matters here?
no. A Monad that's built from `data Free f a = Some a | Moar (f (Free f a))` where `f` is some Functor.
This is precisely what the `IOSpec` package already does! http://hackage.haskell.org/package/IOSpec See also "Beauty in the Beast" here: http://www.staff.science.uu.nl/~swier004/publications.html
Ok, but the fact is that a trivial denotational semantics is better than no denotional semantics. You similarly can't reason about your two example programs using the current state formulation, whereas with the free monad formulation permits at least trivial equality reasoning between programs. You could then extend it and make it not free by then adding equations on top of that, which you can't do with the state monad formulation which doesn't even have a base denotation to build off of.
That's approximately right. Here's a ramble on the subject, now that I'm thinking about it: An effect has an associated signature S, specifying its set of operations and their types. A handler for an effect is an S-algebra, with a carrier set X and a mapping from plain values and from each fully-applied operation (with Xs as arguments) into X. The free S-algebra has as its carrier the set X of trees whose leaves are values and whose internal nodes are operations from S (with the right number of children). Any S-algebra is the image of a unique S-algebra homomorphism out of the free S-algebra. So we can write an algebra/handler/interpreter for an effect by writing down this homomorphism out of the free algebra. (See [here](http://math.andrej.com/2010/09/27/programming-with-effects-i-theory/) for a dramatically less terse and likely also much more illuminating explanation.) There's no built-in notion of sequencing in a general S-algebra; if an operation should return a value or proceed on to other operations, it takes a continuation as an argument. (Eff hides the CPS transformation in code invoking effects; only handlers deal with continuations explicitly.) The *free* S-algebra, however, can be made a (free) monad by defining `t &gt;&gt;= f` to yield the tree given by substituting `f y` for each leaf `y` in `t`. (I'm not sure if this `&gt;&gt;=` commutes with S-algebra homomorphisms; if it does, it induces a monad structure on all S-algebras...) The difference is that in eff, the free algebra is just an abstraction used to define handlers (via unique homomorphism), not datatype whose values you manipulate. So algebraic effects (and handlers and effectful computations) are related to free monads (and interpreters and monadic values). The big difference is that the algebraic approach gives a more complete account of how to construct and compose handlers, while the monadic approach has the simplicity benefit of making effectful computations just values and interpreters just functions.
What's default?
My pleasure! I've already gotten a couple high-quality commits, so I'm really happy with the choice to go open-source. As for the design choices, I now need to convince some conference that they are a good idea too :) Anyway, I'm glad installation went smoothly, and I hope you enjoy using it!
[no documentation, no example](http://hackage.haskell.org/packages/archive/template-default/0.1/doc/html/Data-Default-TH.html)... stop releasing stuff like this to hackage please..
You can click on the 'source' link. You can install it and try it. You can fork it on github. You can make a pull request to provide the documentation. You can take notice it is a 0.1 release and ask kindely to provide an example for 0.2. It is far more useful to release open source 'stuff like this' with liberal license than complaining on reddit.
See [data-default](http://hackage.haskell.org/packages/archive/data-default/0.4.0/doc/html/Data-Default.html) (which is linked in the dependencies).
There is nothing stopping you from adding equalities to the abstract IO monad either. Things that you decide must hold for the IO monad. You can then use them to reason. You always have the trivial semantic equality that identical terms are equal as the base. 
http://ncatlab.org/nlab/show/free+monad Properly speaking, it should be "monad freely given rise to by an (endo)functor" (see the section on algebraically free monads, I think). But in the Haskell world, free monad is a fine shorthand, just like "monad" is a shorthand for the specific type of monad that we have a typeclass for, and "functor" is a shorthand for "endofunctor on Hask." Of course not all monads (in the Haskell sense) *are* iso to free monads (in the Haskell sense). Notably, the continuation monad and similar ones. similarly there is no single free monoid, but a free monoid on a given set, produced by the free functor Set -&gt; Mon
&gt; similarly there is no single free monoid, but a free monoid on a given set, produced by the free functor Set -&gt; Mon And similarly, the functor Set -&gt; Mon produces a monoid that only satisfies the monoid laws and no other laws (which is the reason that it's corresponding F-algebra is always initial). &gt; Of course not all monads (in the Haskell sense) are iso to free monads Only free monads are iso to free monads (in the category theory sense, in the category of monads). Actually, only free monads have a monad morphism to free monads. Also, if it satisfied any laws (except monad laws), how can it be an initial object? You can't have a monad morphism to a monad that satisfies more laws than the monad in question.
I'm not sure what we disagree about. But consider the free monoid on the one element set. This monoid is commutative! It "satisfies a law" beyond the monoid laws. But the fact that it satisfies this law is determined by the intersection between the monoid laws and the context in which they're applied. Depending on what functor you take as the base of a free monad, you'll be able to make other definite statements, "laws" even, about the resultant monad. Those statements/laws will be derivable directly from the intersection of choice of functor and the monad laws, but they will exist nonetheless. It may be the case that the laws will only hold under a suitable functor (in the categorical sense, not the haskell sense), but that's a bit afield...
The problem of lacking semantics for IO is not lacking a semantic representation. The problem of lacking semantics for IO is that we do not have any good theory of those equations which make it non-free. This is an important distinction. Just having "a model" is pretty worthless. Given any formal language, I will give you a model for it: the free set of syntactic terms of the language. Yay! Semantics is done, we can all go home now. Alas, no. The point of constructing models is that they give *tight bounds* on the semantics. The free model is always a model (by definition), but it's the weakest possible model you can come up with. It doesn't teach us anything about the underlying structure. The thing we want is a specific theory (i.e., set of equalities) which we can, ideally, prove to be both sound and complete as a semantics for whatever. Soundness means that if we reason by looking only at the models, we're guaranteed not to come up with lies. Completeness means that everything we can show by reasoning on the actual thing can also be shown by reasoning on the model. Free models are as far from being complete as you can get while still being sound.
Haskell programmers are definitely not the intended audience for this post. It is a call to action for c++ developers to build more reliable code. John has talked about reliability, static analysis and techniques for reducing defects repeatedly over the last few years. He has even declared that if it was possible he would prefer large swaths of games to be built in a functional language like haskell or ocaml. Tim Sweeney (Unreal engine developer) has also weighed in on the topic repeatedly with similar views. Tim also has stated that STM is probably the only way to write lots of interdependent parallel code.
Btw, the use of a prefixed `template-` in the package name is rather unconventional; the usual convention for naming TH-related packages on hackage seems to be to use a `-template` (7 packages), `-th` suffix (19 packages) or even a 'th-' suffix (8 packages, although these seem to be extensions for TH itself, rather than uses of TH) instead. In general, a more uniform naming convention for hackage packages would be nice to have, but I guess it's too late for that now...
Refusing to commit to a specific database server is like refusing to commit to a specific functional (or object oriented) language. It makes one life harder without real benefit. I my company, things have greatly improved since we abandoned this illusion of freedom, ditched active record validations and migrations, and used our DBMS features in place. More static invariants, better error handling, (much) less lines of code, much better performances. So I had to read a book on the matter, but it has proven to be one of our more useful time investment.
I think it is a good practice to at least cross-reference the place where the documentation is at other parts of the page. But here it is so short, you could just copy it to the function. On Hackage, there are at least 3 places where you can look for docs: the "contents" page (where you put it), the top-level module, and the individual submodules (here these two coincide); this can be quite confusing, even for a seasoned haskeller. 
"Haskell is the world's finest imperative programming language" ~ Simon Peyton Jones
Thanks, are you implementing you own free monads or are you reusing the 'free' package? (I'm thinking about that because I saw your thread http://www.reddit.com/r/haskell/comments/swffy/why_do_we_not_define_io_as_a_free_monad/) Plus what convinced you to use this technique and not Cont or packages 'operational' or 'MonadPrompt'? Simplicity? Efficiency?
Please, just don't call it "function".
Assuming that free monads and algebraic effects are the same, you can read the [eff paper](http://arxiv.org/abs/1203.1539), it covers all the common monads.
It's pretty obvious what it does.
Yeah, their encodings of State, for example, don't encode the laws of State, whereas the usual presentation of it carries them in its definition.
I might be having a brain malfunction, but I can't figure out how to handle only some effects with this representation. data EitherF f g a = InL (f a) | InR (g a) handleL :: (Functor f, Functor g) =&gt; (f r -&gt; r) -&gt; (a -&gt; r) -&gt; Free (EitherF f g) a -&gt; Free g r handleL = ??? If you have two catamorphisms you can of course get a catamorphism from the sum, but this doesn't let you *nest* handlers. 
Happy to hear about build-in caching and send-file support! Out of curiosity, some more questions: - is it easily possible to share state at the application level? (without the need for memcache, or a DBMS) - are there alternatives to Heist? Because to me, it seems that one of haskell strength would be that you can could use Haskell to build statically typed HTML, without the need of a specific template engine. - is there an abstraction for routes? As in, can I rename a route, without having to update my templates? - why aren't models encapsulated? It seems the database support prefers raw SQL. I would rather specify my scheme in Haskell, and have it type-check my queries? I'm a bit surprised that it is this hard to have a lazy query (say a scope) in Haskell, whereas in Rails, all database-select queries are lazy by default. It's like the world is upside-down. In the HDBC tutorial, the example does a query, then converts all its results to a native list. The pragmatist in me screams ... select-queries should be lazy by default. Why not just use unsafeIO? But judging from the comments here, to not encapsulate the data in the DBMS is done on purpose, a policy? I can understand the complaints people have against the pseudo object oriented encapsulation in ActiveRecord and such, but to me it seems that Haskell would be able to encapsulate much better, being a functional composable language. What's the point of all spending all this effort on static type checking if it's not governing the data we care most about? It's nice to see, that many parts of Snap are fleshed out and maturing. But the emperor has no clothes! 90% of all my code on the back-end is glue-code governing _state_. It's kind of hard to argue that managing state (in the DBMS, on the application level and per session) is put front and center in Snap. Perhaps, its my ignorance at play? Nevertheless, when I look at the only type of blog-example I could find for Snap (the snap website), i notice the blog contents being part of the source. The hello, world of any webdev framework is a database-backed blog. And I would at least suggest a tutorial with that particular focus would do the learning curve some good. And with such an example, it's easy to tell, how much or little cognitive effort is required for all the unimportant details beyond managing that state and tying it to a template. 
&gt;Monads are easy to understand once you understand them. I beg the question. (in honor of cultic_raider) &gt; The monad theory can come later. The question is: does one need to understand monad theory to be able to use these frameworks? Or only to write them?
Actually, I've submitted a different free monad module for the `transformers` package and I'm waiting for Ross to include it there before I release my next `pipes` version. You can find the first draft of it [here](https://github.com/Gabriel439/Haskell-Pipes-Library/blob/master/Control/Monad/Trans/Free.hs), although I'm about to update it to a second draft and split it into its own repository later today. The new definition of the `Pipe` type based on that can be found [here](https://github.com/Gabriel439/Haskell-Pipes-Library/blob/master/Control/Pipe/Common.hs). Note that I actually use `FreeT` (a "free monad transformer") and not `Free` in order to obtain a correct `MonadTrans` instance. If you want to learn more about this type, just read issue 19 of The Monad Reader which has a concurrency article by Mario Blazevic using this type to great effect, except he calls it `Coroutine`. This changes the new `Pipe` type to run it through the base monad at each step instead of making the monad optional. I don't use the `free` package for two reasons: 1. It has an incorrect `MonadTrans` instance. See [this ticket](https://github.com/ekmett/free/issues/3) for details. 2. The `mtl`-style works really really terrible with sum functors. See [this comment](http://www.reddit.com/r/haskell/comments/sarkz/four_tips_for_new_haskell_programmers/c4cltmy) of mine where I mention this issue. Actually, this is a pretty game-breaking issue not just for my library but for all free monad implementations since the overwhelming use case is sum functors like mine, which the `free` package crashes and burns on. By `Cont`, I assume you mean something like: newtype Pipe a b m r = Pipe { forall x. runPipe :: (m (a -&gt; x) -&gt; x) -&gt; (m (b, x) -&gt; x) -&gt; (m r -&gt; x) -&gt; x } ... which is just the Church encoding (i.e. continuation-passing style) of the new Pipe type. I've heard people talk about the time complexity problems of free monads when not using continuation-passing style, but I don't actually observe this problem. GHCi interprets the code with linear time complexity and when I compile my pipe programs they run within a few percentage points of their hand-rolled C counterparts, quite amazingly. I think this is one of those cases where other implementations are prematurely optimizing without benchmarks to justify the optimization. `operational` and `Prompt` are basically the same thing (with `Prompt` being worse and providing some unnecessary extra complexity) and I don't use them as a matter of simplicity and taste. I always like to ground my code in terms of category theory design patterns. The free monad approach leads to much simpler and elegant code and doesn't require GADTs or universal quantification. My general experience is that code written using category theory design patterns is always the "best" approach, for a very vague definition of "best". Also, both of those libraries have broken `MonadTrans` instances that don't satisfy the monad transformer laws. I place a strong premium on correctness and law-based type-classes. Again, see my implementation for the correct way to define a monad transformer. The only other library on Hackage that has this correct implementation is [`monad-coroutine`](http://hackage.haskell.org/packages/archive/monad-coroutine/0.7.1/doc/html/Control-Monad-Coroutine.html), which basically implements Mario Blazevic's article. I didn't use that one because the library's scope is much narrower than the actual usefulness of the data type and I would like to standardize on a free monad transformer package that doesn't presume such a narrow use for it.
other suggestion? People recognize function from javascript. I particularly despise the keyword fun or fn after having programmed in sml and seeing the world's ugliest code.
1. Proof of concept. I was curious and bored. 2. To show imperative programmers that Haskell can do what language X can do. 3. To make learning Haskell easier for people stuck in old paradigms. 4. I learned while making a single static assignment optimizer that when you need imperative code for efficiency, it is best to do it with common imperative constructs. Not ST because I want to add more from the FFI, and the type signatures are complicated enough already without the extra type variable that ST introduces.
&gt; The truth is, almost no one, is using multiple applications to access the same database Really? That's not my (admittedly limited) experience of large business software.
Yikes, good criticism. I hadn't even considered the importance of the name.
I think there's a difference between recognizing equivalences and identifying them. If you accept this, then pretty printers pose no issues. We can just pretty print the source form, or we can run the source through some sort of normalizer or optimizer and then print it, and the fact that we get different strings out at the end doesn't impinge on the semantics. It's like saying that 1+1=2. In order to be able to print "1+1=2" as a statement (rather than being restricted to print "2=2"), we need to be able to identify differences which are semantically irrelevant. This is fine, because once we observe that 1+1=2, we're free to replace any instance of 2 with 1+1 and any instance of 1+1 with 2, and that substitution isn't going to change the meaning of the program. Now, distinguishing between equivalent things vs identifying them can be a controversial issue, both in mathematics and in computer science. Classical mathematicians like to take the quotient of things (ensuring that everything which is equivalent is in fact identical) because it makes life easier. Whereas constructive mathematicians take issue with (some) quotients, because there's no constructive way to operate on the quotient but it's perfectly constructive to work on the setoid. In computer science there's a similar tension, though the metatheoretic issues involved are different. Note that we already distinguish that which is equivalent. Given multiple implementations of some operation, we can say on the one hand that they are all "the same function" and yet we can also observe that one is more efficient than another. This gets back to the old "function" vs "algorithm" distinction. Mathematical functions don't care about performance, and so when talking about the semantics of functions we are explicitly quotienting out all the performance differences. However, the whole point of talking about algorithms is to *not* ignore performance differences. Thus, for algorithms, the fact that one implementation is O(n) whereas another is O(n^2 ) is considered to be part of the "semantics" of the algorithm. The question of semantics is always a balancing act. Semantics give us a map of the terrain. But if the map is too detailed, then it doesn't help us to abstract and simplify the terrain, and so it's worthless as a map. However, if the map isn't detailed enough, then it doesn't resemble the terrain enough to be helpful. The terrain is the free structure generated by the syntax. But there are many maps of that terrain. The level of abstraction you want is going to dictate the semantics you get. Sometimes we want to distinguish algorithms; sometimes we want to recognize that they're computing the same function. Neither one gives the "right" answer for all contexts. As far as IO goes, I think it's very worthwhile to come up with a theory that's more detailed than just the free syntax. If we only recognize syntactically identical things as the same, then that provides no advice for how to refactor or optimize programs while maintaining the semantics. Indeed, it is impossible to refactor or optimize because we have defined the result as being (syntactically and therefore semantically) different from the original. The problem with IO is that it can refer to itself, and therefore is pathological. If one algorithm takes more time or memory than another, it is possible to observe this within IO. But that violates the semantics we've assumed for the pure core of the language, namely that we are treating them as mathematical functions (as codified in the type system). So where do we draw the line between calling one `main :: IO ()` the "same" as another?
Another suggestion for function maybe 'def' like python or 'defun'
Wouldn't that be bad for the same reason `IncoherentInstances` is bad?
I really wish`Data.ByteString.Char8.pack` wouldn't fail silently (by truncating the code-point to `0xff` without any error)...
it won't help them learn anything about Haskell directly, but it will get them to use Haskell, and you can't learn anything about something you aren't using.
Your application code will indeed allow to write higher level validations, but they will remain run-time validations. Relational validations are lower level and statically guaranteed, so you *know* that when you query the database, anything coming out of it will be correct. Performance is indeed important for us, but so is code reliability. With good relational design, you use a lot of SQL views, almost one for each application view. Then your application job is merely to decorate SQL output with HTML tags. Blazing fast, very concise, highly reliable. If "email validation" means "checking a regular expression" against input, our RDMS can do it just fine. It does not mean you should not do it also at application level, or even at client-side (javascript) level for best user experience. Files are stored flat in a high capacity key-value store, and we have a simple form of file system implemented at the relational level, referencing external blobs by reference IDs. Don't get this wrong, this file system is very simple and only took half a day to design. Still, it handles nice things such as garbage collection, sharing and copy on write. The real illusion in the end was to think we could abstract over tools and languages with cheap DSLs. This is very rails-ish (and others frameworks-ish) and was totally wrong in the long term.
In case you feel like mixing in file or network operations with your other imperative code.
You are right. I didn't quantify that enough. I mean in the web-development sphere. Where, syndication usually takes place through web-based API's and all the model logic lives at the back-end (not in the DBMS).
This is overstating the case quite a bit. The truth is a bit more complicated: 1. `Data.ByteString.Char8` package provides a valid implementation of *both* the US-ASCII and ISO 8859-1 character encodings, except for error checking. So it's often used when those encodings are in use, the input is known to be correct, and speed is important. 2. Far more importantly, there are a large number of internet standards (HTTP being a major one) that are specified as exchanging binary data which is mostly semantically understood to be ASCII text. It's convenient from an implementation standpoint to compare HTTP header values with constants that are written as text, even though strictly speaking, an HTTP header value is a block of binary data.
That's like saying "do you prefer to write the number ten as 10 or as 012?" -- the latter is likely to confuse more folks, but it can depends on the situation. 
Well we also have word8. So not so much...
This is amazing. I like it a lot. Thank you for making this.
Quick terminology thing: most encodings are _not_ isomorphisms; usually, the best you can hope for is that `decode . encode` is the identity and that `encode . decode` is the identity when it's defined.
Yeah, you're right, I didn't think about the myriad of formats voluntarily restricting themselves to ASCII. MIME-encoded emails are among that stuff. In any case, `Char8` isn't supposed to be used for text.
True, they're only isomorphic if you restrict the set of byte strings they map to to the set of *valid* byte strings for that encoding. I should amend that. As a side note: that "best you can hope for" is the only thing I'll agree to. An encoding that doesn't honor `decode . encode = id` and `encode . decode = id` isn't of any use for textual data.
In this case yes, I absolutely agree with you.
Unicode code points do not necessarily represent glyphs. It's far more complex than that. Some glyphs in some writing systems can be represented in different ways with differing numbers of Unicode code points combining to form the glyph. There are a number of different normalization rules defined in Unicode that can be used to help determine if two Unicode sequences represent the same text. Many Unicode code points do not represent glyphs at all. There are the venerable old ASCII control characters. There are code points that help determine which parts of the text flow from left-to-right or right-to-left when rendered. There are code points that indicate whether or not line breaks can occur in certain places. Etc.
You have to be careful when you recommend the [encoding](http://hackage.haskell.org/package/encoding) package. You should at least also mention an important alternative, the [text-icu](http://hackage.haskell.org/package/text-icu) package. Both of them have their advantages and disadvantages. I'm not sure how well the encoding package has been maintained lately. The current version was uploaded almost a year ago, and even then not by the author. The cabal file has no upper bounds on the versions of its dependents, and it won't build with recent versions of some of them, like [HaXml](http://hackage.haskell.org/package/HaXml). That makes the encoding package tricky to use. text-icu depends on the external icu library via FFI. It would be a huge job to write and maintain a reasonably complete set of encodings, so it makes some sense to avoid re-inventing that wheel. But the binary dependency can sometimes make the package difficult to build. E.g., I've had trouble on Mac OS X, where the built-in version of icu may not be compatible with the Haskell binding. The speed of encoding functions can often have a serious impact on an application. text-icu is highly optimized, well maintained to stay that way (so far), and tightly integrated with Data.Text (it's by the same author). It would be interesting to see some benchmarks comparing text-icu and encoding. The APIs of text-icu and encoding seem quite different, making it annoying to switch between them. I would love to have both an icu binding and a well maintained, reasonably broad, and reasonably fast native Haskell library, with compatible APIs.
And why can't said parser use a package like utf8-string instead to encode the character? I don't see why Data.ByteString.Char8 is necessary here.
I understand your point of prefering the use of plain 'lift's instead of having an explosion of badly behaving MonadXXX classes, but then how whould you solve the inflexibility problem expressed in [chrisdoner's link](http://wiki.chrisdone.com/Rebuke%20of%20the%20inflexibility%20of%20monads)?
That's really helpful, thank you. I had given some thought to simplifying some of the code by using ErrorT. There are also errors coming from IO from eg the Network.HTTP code and I would like to handle the two types consistently. I'll take a look at this.
Yes
This style looks like it would be really convenient for cases when an imperative algorithm is called for. Does anyone more knowledgeable than me know how using this style affects efficiency? There's a good chance that if I want do to something in imperative style, it's be because of an efficiency concern. 
What he's asking for is broken code. Monad transformer capabilities do not commute. Let's use the simple example of a 'Maybe' and `Writer` monad combined using the mtl style: m = do tell "A" fail "Error" The behavior of this is completely unspecified based on how you run it. If I do: runWriterT $ runMaybeT m = (Nothing, "A") runWriter m :: Maybe (a, [Char]) = Nothing In other words, the flexibility that he is asking for leads to broken code whose behavior is undefined and depends on the ordering of monad transformers. Monad transformers do not commute and specifying there cabilities in a way that allows them to commute is broken and only works if people "run" the monad in the way you intended, which is not safe or robust or even semantically correct. However, you can specify the ordering of monads when using the `mtl` style, using something like the example Chris gave me: (MonadTrans t,MonadState s (t m),MonadState s m) =&gt; ... But then you are specifying the exact format of the stack anyway and then you lose exactly the flexibility he was asking for. My point is that the "flexibility" he's asking for is the flexibility to write broken code.
Its just a face over a of a reader/continuation transformer of IO to hold return/break/continue continuations. I think most functions involving V might get inlined out (add inline pragmas), but I haven't done tests.
TIL i've been doing it all wrong! thanks for this great post. not only did i learn that my continued use of Char8 is wrong (and why), but i also learned a lot about unicode. thanks!
Man, this looks great. The best part is the guide! Very well written. Is it possible to use this for Xelatex and Biblatex yet? :)
Be careful what you wish for! Some encodings may have specific common errors that they're "nice" to -- say, bytestrings which are not in the image of `encode`, but which the encoding says to decode anyway. Rejecting an encoding just because it has many representations of a given `String` seems a bit harsh. (After all, if you accept Unicode as a thing, you're already in a similar regime where many sequences of codepoints represent the same logical `String`: look up Unicode normalization some time.)
The obvious step would be to support a JavaScript FFI ; as we already do for a small number of other langs 
I'm glad it helped you :-)
&gt; Unicode code points do not necessarily represent glyphs. It's far more complex than that. Some glyphs in some writing systems can be represented in different ways with differing numbers of Unicode code points combining to form the glyph. There are a number of different normalization rules defined in Unicode that can be used to help determine if two Unicode sequences represent the same text. Yes, dmwit pointed it out in another post that glyhps/characters and byte strings do not have a direct one-to-one correspondence, and I amended my post. Thanks for your input! &gt; Many Unicode code points do not represent glyphs at all. There are the venerable old ASCII control characters. There are code points that help determine which parts of the text flow from left-to-right or right-to-left when rendered. There are code points that indicate whether or not line breaks can occur in certain places. Etc. Yes, but I think one can safely just treat them as being part of "the set of characters" as defined by what `Char` represents within Haskell (namely, a code point/integer value.) In that sense, it might be a good idea to make a difference between glyphs and the set of all `Char`s (a superset of the set of glyphs), but I guess I'm being lazy at this point.
I have a lot of questions. * How is it typed/type-checked? Regular Haskell types? * Do you import a special library and hide Prelude? * Is it intended for the browser, or a JS backend like node.js? (Or both?) * If the former, can you interact with the DOM? How? * If the latter, can you interact with the environment, network, and file system? How? * What parts of Haskell are you *not* implementing? * Is there an automatic way to know that you've written Haskell that cannot be translated? * Why roll your own solution, instead of adapting existing ones like UHC?
Indeed, that's the required command to clone the repo on your machine. The git repository web is: https://github.com/Daniel-Diaz/HaTeX.
&gt; Man, this looks great. The best part is the guide! Very well written. Thanks. That's a nice reward after the hours I dedicated to it. :) &gt; Is it possible to use this for Xelatex and Biblatex yet? I'm planning to add bibtex, but note that already exists the [bibtex](http://hackage.haskell.org/package/bibtex) package. I have not thought about xelatex yet. I will take a look, but I don't know if I want to work on it right now. Anyway, if you are interested in some feature in particular, go ahead and make a patch! :)