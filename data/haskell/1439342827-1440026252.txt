I find about two-thirds of "implementing new features" is about refactoring. That is to say, we're mainly making incremental changes to existing codebases, even when adding new features, and even in the latter half of a greenfield project.
&gt; basically an abelian group and a monoid operation in the same domain of discourse, connected through a distributive property Yeah, "just a monoid in the category of endofunctors" was starting to get stale, about time we had a replacement. ;)
It should be fixed now; there was a problem on Windows with the huge command-line arguments it passes to `cabal.exe`. I assume the patch made it into the 0.1.3.0 release.
This idea makes me uncomfortable.
I'm a fan of Pierce's 'Basic Category Theory for Computer Science' for initial exposure to anyone who has encountered abstract algebra in a CS environment before. Speaking of, Pinter's 'A Book of Abstract Algebra' would make a good starting point overall.
I feel like I said something that came off oddly and I apologize for whatever was interpreted. I'm not sure what I said came off oddly however. I just meant that someone could effectively increase the resolution in some sense by interpreting how things should look.
*My first real program in Haskell was a production project for parsing EDI (electronic data interchange) format files. Between the Conduit library and Haskell's raw speed and existing parser libraries it was not only a cinch - it was correct and fast as hell.* Did you put it on Hackage and/or Github? I need to do EDI in a business application I'm building and I want to use Haskell for it but was unable to find an existing EDI parser library so I started writing my own. If I can at least see your code it would help me a lot!
This becomes more true the more people have worked on a given code base, too. If you're turning over 100% of the code base by continually doing nothing but implementing new features, you're probably demonstrating that the company/product hasn't hit a true value proposition yet. The code that lingers is usually what has (at some point) delivered some value. Being able to maintain/update that code of value with strong guarantees of correctness is a godsend.
I'm really glad that you posted this discussion -- it is extremely interesting food for thought to a Haskell newb that's always looking to expand their horizons :D
To be honest I have no idea haha. I probably copied it from somewhere (I don't, normally) and forgot about it. I didn't even know that ghci had an "rc" 
I'd love to - unfortunately the implementation we are using is based on a rather expensive implementation guide created by the X12 organization - and they are super protective of their IP.
What is the snag there? Note, I'm biased towards alpine here.
You've given a new meaning to the phrase "GOTO considered harmful"!
Neat! Though I wonder that "stat not good enoug" means. I look forward to the day when GHCI has full support not just as an interpreter but also with a whatever equivalent byte code we have.. as well as TemplateHaskell which seems to depend on GHCI for such. 
Have you considered writing a wrapper around a C implementation? 
No big deal. I'm not even sure why I feel that way.
https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/template-haskell.html
Using cabal --reinstall after configuring Documentation:true works. As pre http://stackoverflow.com/questions/6563769/haskell-cabal-regenerate-documentation-for-all-installed-packages 1. Even though I'm a newbie and 2. Always use a cabal sandbox - dunno if applicable for older cabals than 1.20.x
Oh lol. I kept looking for inuendios and kept trying to interpret in the worst possible way haha.
&gt; Detect when a module is compiled but not listed in the cabal file Yes! Many times I've got burned in releasing a package to later discover it won't compile correctly because I've forgot adding a module in the .cabal manifest..
Btw, watch out for https://github.com/commercialhaskell/stack/issues/763 in the new version. Luckily @snoyberg is unaware of this whole "sleeping" thing and has a fix pending as 0.1.3.1.
The GHC testsuite includes a variety of [tests](https://phabricator.haskell.org/diffusion/GHC/browse/master/testsuite/tests/perf/) which verify that various runtime characteristics (e.g. heap allocations, ) of the compiler working on various example programs fall within an expected range. These tests are remarkably fragile even on the first-tier platforms, so it's not at all surprising that they currently fail on ARM. In particular, "stat not good enough" means that the measured statistic was "poor" (e.g. allocations too high) compared to the expected value.
Nope. If he was born in SA then we're claiming him! :)
Personally, I'm not great with the mathematical background stuff, but I'm interested and understand a bit of it. Especially with self-directed learning I think motivation is crucial, not only in terms of self-discipline or whatever but also in terms of understanding the motivations behind various developments, and a bit of history. Then you get a more fulfilling kind of interest in the topics; at least that's how it seems to be for me. A text by Lee Lady on the origins of modern algebra that I skimmed briefly: http://www.math.hawaii.edu/~lee/algebra/history.html A very brief history of logic by Moshe Y. Vardi: http://www.cs.rice.edu/~vardi/comp409/history.pdf Thierry Coquand's intro slides on logic in computer science: http://www.cse.chalmers.se/edu/year/2014/course/DAT060/pres.pdf (I took this course in Gothenburg and it was quite inspiring) The topic of "models of computation" is also foundationally relevant for any functional programming language or indeed any programming language at all since it's all about how we model computations in clear and correct ways. So, how does lambda calculus relate to Turing machines, how can we formulate abstract machines for stepwise evaluation of lambda terms, how can we characterize the various evaluation strategies (laziness, call by value, etc), all kinds of interesting questions. And then type theory, where I agree that Pierce's book is a good intro and possibly the best place to start overall.
Then you have have the problem back that stack/stackage fixes. A build that works "now" may fail "later".
I really don't want to Google that, right? 
Yeah, integrating with various bureaucracies through excessively bureaucratic protocols seems to be pretty much the definition of "enterprise programming." My first job involved CDATA-encoded XML messages inside of other XML "envelopes." I actually preferred integrating with the *real* legacy systems, the ones based on fixed-width text field formats and batch upload through some custom Frankenstein version of WebDAV, etc...
Travis CI eliminated that problem for me.
I'm somewhat confused now as well as feeling slightly patronised. Is using `cabal`/Hackage considered advanced usage while using `stack`/Stackage aimed at beginners?
With combining this and qemu, I can cross-compile to arm on laptop instead of building cross-compiler for hours.
* https://www.fpcomplete.com/user/marcin/template-haskell-101 * https://www.fpcomplete.com/user/marcin/quasiquotation-101 * http://illustratedhaskell.org/index.php/2011/09/24/template-haskell-tutorial/ * http://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell As you've probably noticed, most tutorials at some point tell you to use ghci to help you with writing TH expressions: $ ghci -XTemplateHaskell &gt; :m +Language.Haskell.TH &gt; runQ [| &lt;code-you-want-to-generate&gt; |] Then, you can understand an expression generated by looking at http://hackage.haskell.org/package/template-haskell-2.8.0.0/docs/Language-Haskell-TH.html You can start by trying to write the simplest of expressions. For example, to see how to generate a simple "Hello" string using TH you can: ➜ ~ ghci -XTemplateHaskell GHCi, version 7.10.1: http://www.haskell.org/ghc/ :? for help Prelude&gt; :m Language.Haskell.TH Prelude Language.Haskell.TH&gt; runQ[| "Hello"|] LitE (StringL "Hello") Then, you can try to write that expression yourself and play with it: ➜ cat THaskell.hs {-# LANGUAGE TemplateHaskell #-} module THaskell where import Language.Haskell.TH constHello :: Q Exp constHello = return $ LitE (StringL "Hello") sayIt :: String -&gt; Q Exp sayIt str = return $ LitE (StringL str) ➜ cat Test.hs {-# LANGUAGE TemplateHaskell #-} import Language.Haskell.TH import THaskell main = do print $(constHello) print $(sayIt "Hello world") ➜ ghc Test ➜ ./Test "Hello" "Hello world" You can use this template to work your way up towards more complex expressions. Once you can write expressions, then tutorials involving other TH aspects like QuasiQuotes start to make more sense. EDIT: Here is QuasiQuote support for the expression above: ➜ cat THaskell.hs {-# LANGUAGE TemplateHaskell,QuasiQuotes #-} module THaskell where import Language.Haskell.TH import Language.Haskell.TH.Quote constHello :: Q Exp constHello = return $ LitE (StringL "Hello") sayIt :: String -&gt; Q Exp sayIt str = return $ LitE (StringL str) sayItQuasi :: QuasiQuoter sayItQuasi = QuasiQuoter { quoteExp = sayIt , quotePat = undefined , quoteDec = undefined , quoteType = undefined } ➜ cat Test.hs {-# LANGUAGE TemplateHaskell,QuasiQuotes #-} import Language.Haskell.TH import THaskell main = do print $(constHello) print $(sayIt "Hello world") print [sayItQuasi|hello quasi world|] ➜ ghc Test [2 of 2] Compiling Main ( Test.hs, Test.o ) [flags changed] Linking Test ... ➜ ./Test "Hello" "Hello world" "hello quasi world"
I'll keep following `stack`'s progress and maybe at some point I'll hit issues with `cabal` that will make `stack` look more interesting.
I'll first try uninstalling MinGHC and starting from scratch again just in case...
Nice, but can this hide problems such as when an impossibility invariant is changed and now previously impossible code can happen? The proposal doesn't address that explicitly whether GHC will flag something up.
The quotation parser limits where you can splice because reasons. It's not a bug in terms of being unintentional.
Excellent. Keep up the great work with this IDE, it's much appreciated. 
Oddly fits his font choice for presentations... :) (not that this would somehow impede my huge respect for this man)
Is this related to the fact that most famous Germans are really Austrian?
The type checker will accept an `impossible` declaration only for cases that the type checker considers inaccessible. It can already verify this today; it currently won't let you write inaccessible cases. This proposal just changes that behaviour so it will let you write `impossible`, but only if the case is indeed inaccessible.
Perhaps for documentation purposes? After all, in most cases, the compiler doesn't need type annotations, but we use them for our own sake, not the compilers.
If the compiler can _find_ them automatically, sure, that would be even better. But the current type checker does a rather poor job at finding them (as opposed to confirming that a case you wrote is indeed inaccessible), and although there is a recent paper (cited in the ticket) that makes this a lot better, it still won't be able to find all cases (undecidable problem in general). (And yes, documentation is another reason: it's useful to be able to say "I left this case out intentionally, this cannot happen.")
What those reasons are? Why can't I splice a name in place of a name?
Actually we do support some basic real number arithmetic. For example, in the following link http://goto.ucsd.edu:8090/index.html#?demo=permalink%2F1439381341_1595.hs I defined a function `areCollinear x y n m a b`: liquidHaskell is satisfied only when the points (x,y) (n,m) and (a,b) are collinear. Thus, in the link above the call `areCollinear 0 0 1 1 2.4 5.8` fails from liquidHaskell. Note that because our support for reals is still restricted you need to use the pragma `{-@ LIQUID "--real" @-}` Also, the support for reals is as much as the underlying SMT solver gives us, thus currently we do not support operators like `fromIntegral` to check is a `Bool` is a multiple of `3`. 
Yes, I really want this too. It's really annoying to have to write case blah of This -&gt; ... That -&gt; ... TheOther -&gt; error "foo: impossible TheOther happened!!!" The original pattern match failure error message would have been a better error message (it has line numbers) and it'd be less effort for me to write. So yes, I want to be able to say: case blah of This -&gt; ... That -&gt; ... TheOther impossible (Though I don't care too strongly about the exact syntax)
But why not address both (very closely related) problems in one go? :-) The difference is that in one situation we're doing GADT magic and the compiler can check that the case is impossible due to being ill-typed, and in the other it's just ordinary pattern matching and some program invariant that means the particular case ought not to happen (but if it does, the generic error is perfectly ok). Supporting both would require some syntactic distinction.
I think this a separate issue. To me the word `impossible` means that "you will by-construction never see a value of this type in a well-typed program." I agree that it may be useful to have a convenient syntax to denote cases that you *shouldn't* see in a correct program, but this is another matter, one which, as you point out, will likely be be treated with different syntax than the issue at hand.
Perhaps for the same reasons he'd like to use a strongly typed programming language. 
If you can't trust the API specification, all bets are off anyway, and if a 503-code w/o any additional information is the only signalling mechanism available there's not much you can do other than retrying the request with some delaying heuristic :-( The nicest API I have seen so far is [Reddit's API](https://github.com/reddit/reddit/wiki/API) which exposes your current session's budget via `X-Ratelimit-*` headers
I assumed that's what Duncan was talking about? `TheOther` having a different type to `This` or something. If not, at best Duncan wants an `improbable`... =)
No, /u/dcoutts (and I am assuming /u/augustss as well) are referring to patterns that are _not_ necessarily type incorrect, but are impossible due to program invariants. Which is why /u/bgamari and I consider this to be a separate issue. I like your `improbable` suggestion though :) /u/adamgundry has suggested `inconceivable` :)
`liftA2 (==)`
Yeah, I read about that header value while doing my research. Unfortunately it seems like the standard way to do it in ``nginx`` is to just return 503 until some time window is up, and this API is using ``nginx``. This is the scary RealWorld of ``IO`` and dirty ``String`` parsing via ``aeson``. We need to code defensively, and for me that includes not believing that the API will be available after a 6 second hiatus...
Thanks. I just did that but it's not obvious where the documentation went and how I view it. Advise?
&gt; Hackage still uses 7.8.3 What do you mean?
I agree with that the blog is grating in tone because it's not about informing his/her readers but rather a red-herring public vent. State some popular corporate trope: ready for the "enterprise"--whatever that means--and place X, Haskell in this case, in not "ready for the 'enterprise'" without any empirical data or any rational basis. It's not much different than those celebrity tabloids in quality--no concern for intellectual honesty. It's all about truthiness.
`inconceivable` made me spit take.
Alternately you can use [`transformers-compat`](http://hackage.haskell.org/package/transformers-compat) with anything back to [`transformers`](http://hackage.haskell.org/package/transformers) 0.2, and get a compatible `ExceptT` and still work with basically every Haskell platform for the last 5 years. This contains all of the logic in a single line in your .cabal file build-depends: transformers-compat &gt;= 0.3 &amp;&amp; &lt; 0.5
does not typecheck, unfortunately 
Thank you guys for getting it out there so quickly, we ust got the latest bits from the ubuntu PPA and it seems to work as intended.
That whole "Why should I use monads and why are they so good" thing is so misunderstood. Using Monads to do IO is a bloody pain. It's not "good" it's a lesser evil. You need something other than side effects if you say that functions can't have side effects. Side effects of course are more pleasant to do I/O. The point is that you reap numerous other benefits by just saying that no function can have side effects, or at least the compiler is free to assume they don't even if they do.
Yeah, I am also wary of adding more reserved words.
yes, but that's not a type checking definition for `(==) :: a -&gt; a -&gt; Bool`. (==) = liftA2 (==) doesn't work as an instance
You asked for code critique: You're using absolutely terrible names. Don't do that, and put a dollar in the swear jar every time you catch yourself doing it in the future. What do I mean by that? Don't look at your source code, and tell me, what is the purpose of the `isSo` definition? What does it do? Also, the whole where block is unnecessary and reduces readability.
1) You're right. I should know better especially if I'm going to ask for a critique. 2) Wouldn't the where block, along with good variable names improve readability? Or at least speed up understanding? How would you have done this?
The article is [here](http://www.infoq.com/news/2015/08/frege-haskell-for-jvm).
Thank you! Time to port my Spock app on rpi2 to ghc 7.10.2 :) 
While I support not adding more reserved words, having `_` mean a wildcard pattern (`f _ = 1`), a hole (`f = _`) and an impossible pattern (`f _`?) seems to overload the symbol too much.
It would be nice if these builds were full continuous-integration builds, rather than just attempts at creating documentation. Hey, makes me appreciate travis-ci more...
Sorry I don't understand what you mean. I'm trying to do a mapreduce on it if that helps? Sorry, but I don't see how removing it helps me achieve that?
I'm typing this on an iPad in a kebab shop, so bear with me :-) You only need the reduce (the and), not the map in this case. Try typing the following in ghci (there may be typos: I mentioned the iPad and kebab shop above but not the beer...): map (==True) [True, False, True] Gives [True, False, True] Note that the input and output were the same. So there's no point in the map. Try: and $ map (==True) [True, False] It gives the same result as: and [True, False] In general, (==True) can only work on Booleans (True and False) and always returns what you gave it. So it doesn't do any actual work. Note that the $ above is required due to precedence. It's the same as putting parenthesis in. Shout if that is not clear.
Explicit binds instead of ordinary function calls are a very minor extra pain, IMO. There are certain specific things that are more painful but they're relatively rare.
Perhaps I'm missing something but I find this a bit confusing. The impossibility of the pattern(s) is really a property of the pattern as a whole, not any sub-pattern. Also, I haven't thought about it very hard but I suspect the parsing ambiguities might be a bit trickier here as well.
I've the impression that `EmptyCase` already covers this use case, and certainly it works fine for the examples in the GHC trac thread.
[Primes in Haskell](https://wiki.haskell.org/Prime_numbers) Reading the detail at the link was one of my first experiences with the quality of the Haskell community. I continue to be amazed.
The biggest issue is most likely the instances of: map floor [1..sqrt(fromIntegral x)] You're doing O(sqrt(x)) floating point additions and conversions to Int, it is faster to convert the upper bound so you do O(sqrt(x)) Int additions and 1 conversion. [1 .. floor (sqrt (fromIntegral x))]
I think the part about the tutorials is especially important, as I've often noticed that most of them tend to have the exit l exact problem mentioned in the article. People learning Haskell do not and should not care about the abstract definition of a monad. We really need to be teaching them when and why to use them before talking about all the theory and how they work.
Uhh, ohhh! How about posRoots = takeWhile (\n -&gt; n*n &lt;= x) [1..] instead of conversion to and from floating and doing sqrt?
thanks! It ran slightly faster :) I'll definitely have to remember this. Wish I thought of it earlier haha
&gt; posRoots = takeWhile (\n -&gt; n*n &lt;= x) [1..] Ahh unfortunately not much change from the suggestion made by [sjanssen] (https://www.reddit.com/r/haskell/comments/3grfiu/haskell_code_running_slowly_taking_like_a_minute/cu0t4kv)
Sure, because you re-evaluate all that stuff again and again. You need to replace the [1..] with the list of primes up to the one whose square exceeds x. I just posted because it makes me sick to see floor-sqrting. :) 
Wait, doesn't GHC do proper tail calls for LLVM, too?
I found this article quite useful for programmers trying to understand Category Theory http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/
Yes, I read that. I disagree with it.
BOO, type errors should never be acceptable. Your code should be set up in such a way as there is no inaccessible code. 
The point you're making regerding the interoperability with Java, especially with mutable data, is indeed true. But it must be said in all fairness that also in the Haskell FFI you can't just pretend that some mutable C data isn't mutable. So, I think this is an unavoidable dilemma. Either you want a pure language, in this case you have to deal with such problems. Or you compromise and use Scala or C++ right away. By the way, Frege has its own HashMap, based on a persistent hash array mapped trie. So you have the choice, but you can't (and don't need to) use Java Collections in pure code.
1. I don't know if it's accurate to call it "widely used" 2. No, the Void package covers something similar but different Consider the example: head :: (KnownNat (n :: Nat)) =&gt; Vect (n+1) a -&gt; a head x = case x of Nil impossible a:::_ -&gt; a I don't know how this would translate into using the Void package.
I have a fresh Mac that has never had Haskell tooling on it. Do I need to install Haskell Platform, or anything else, before using Stack? Or can I just start with Stack and let it install Haskell for me? Is this advisable? Thanks
great!
You could use PatternSynonyms and CPP to conditionally have `pattern ExceptT a = ErrorT a`, though /u/edwardkmett's suggestion is probably better.
I agree. I resent writing anything which doesn't typecheck, even if it's for the purpose of pointing out that it doesn't typecheck. I applaud /u/adamgundry for his neatly ironic proposal of the keyword `inconceivable` to label things which are clearly sufficiently conceivable to be present in the program text, but utterly wrong. A similar purpose might be served by `unmentionable`.
yes! Great trick! I have not thought about this. Yes, `transformers-compat` is definitely a way to go in this specific case. But I am also interested for compatibility tricks like these in general, when a nice compat package is not available.
I did that and got an error: Could not find module ‘Control.Monad.Except’ Perhaps you meant Control.Monad.Cont (from mtl-2.1.3.1) Control.Monad.Error (from mtl-2.1.3.1) Control.Monad.Free (needs flag -package free-4.11) Use -v to see a list of the files searched for. So, after changing to `Control.Monad.Trans.Except` in a few modules, I got the following: Not in scope: ‘throwError’ Not in scope: ‘catchError’ These are defined in `mtl-2.2.1`'s `MonadError` class I am using. I see now that there's another `MonadError` class in `mtl-2.1.3.1`in `Control.Monad.Error.Class`. I am a bit confused what to do now. 
yeah that's what I meant, I guess "not externally observable" makes more sense. I just mean I've also heard of issues caused by immutability in Scala-Java interop. how is C++ a compromise that makes things better?
They can't be particularly atomic, or else they would be Earth-shattering Kabooms.
I'm not actually compiling it and running it. I'm doing it via ghci 
Haskell Platform or [ghcformacosx](http://ghcformacosx.github.io/).
There's your first problem. Ghci isn't really a performance interpreter, it's largely for testing, once you get to performance you should be compiling.
 $ stack --version Version 0.1.2.0, Git revision 65246552936b7da4b64b38372feac903d96a8911 $ stack setup Using resolver: lts-3.0 from global config file: /Users/eccstartup/.stack/global/stack.yaml GHC on PATH would be used $ stack upgrade Fetched package index. Populated index cache. GHC version mismatched, found 7.10.2 (x86_64), but expected version 7.8.4 (x86_64) (based on resolver setting in /var/folders/tb/wpytxqpx111fsxk0tg1zsmgm0000gn/T/stack-upgrade27397/stack-0.1.3.1/stack.yaml). Try running stack setup
Now don't let's be silly.
try this: class Functor' f where fmap' :: (a -&gt; b) -&gt; f a -&gt; f b main = do let x = fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' fmap' (.) return () A little background, Functor now exports (-&gt;) instance, it didn't use to so you had to import it explicitly, without that import, ghc does some crazy things with it. Edit: That generates a 6878 line error in ghc 7.8.3 
There is a certain Vizzinian arrogance to the sort of comments one usually writes for such cases. Or sometimes you see them written as questions: `error "should never reach here?"`
If you use `inconceivable` where `impossible` would have been permitted it should say "warning: truly you have a dizzying intellect".
To make English look properly, look at [book](http://www.amazon.com/English-Grammar-Dummies-Geraldine-Woods/dp/0470546646)
thought you were trying to compare functions to 0, not implement `isEven` :) 
 import Control.Monad.Trans.Except (ExceptT(..)) It follows the `transformers` convention but works with the `Control.Monad.Error.Class` class. I could definitely be convinced to add the `Control.Monad.Except` export, though.
I can fix any arbitrarily large number of bugs by changing one character. g = id x = f f f f f f f f Produces an error for each f foo.hs:2:5: Not in scope: ‘f’ foo.hs:2:7: Not in scope: ‘f’ foo.hs:2:9: Not in scope: ‘f’ foo.hs:2:11: Not in scope: ‘f’ foo.hs:2:13: Not in scope: ‘f’ foo.hs:2:15: Not in scope: ‘f’ foo.hs:2:17: Not in scope: ‘f’ foo.hs:2:19: Not in scope: ‘f’ Which is fixed by changing one character 'g' to 'f'
I don't know anything about Clojure but I dislike anything that runs in the JVM. All that overhead and complication for a feature (write once run anywhere) which will never actually be used. And now that Oracle is involved the future and legality of the whole thing is questionable IMHO.
Very cool, thanks for putting that stuff together guys, we'll try to upgrade tmr.
As a clojurist I had a hard time refactoring a beefy system comfortably. Tests were my only guard rails. This is mostly a non-issue with Haskell. I haven't had that many situations where I thought: "Man, I wish I didn't have to specify types here!". I like Clojure, but I just sleep better at night with Haskell. Also, yes, typed clojure, but looking up the pros and cons of it is left as exercise for the reader.
I see this critique of the JVM a lot but I'm wondering what the basis of it is? As far as a I can tell the JVM is fairly efficient. [Java even slightly outperforms Haskell](http://benchmarksgame.alioth.debian.org/u64q/haskell.html) in the benchmark games. And it does better in spite of the fact that the JVM needs to boot up, which will suck up a fair amount of time in very short tests. I'm not necessarily a fan of writing code in Java but I haven't really heard a good case against the JVM itself.
They might be co-thunks.
I have recently been trying to learn Clojure, because I have had a great personal need for an interpreted Lisp that runs on the Java virtual machine. I especially like how simple it is to create Clojure wrappers around Java API functions, this makes scripting a large Java application much, **much** easier than something like JRuby or Jython. However, Clojure has all the failings of the old fashioned Lisp languages, especially the lack of good type checking, and the fact that functions with side-effects are not wrapped-up safely in an opaque "IO" data type as Haskell does. These features of Haskell are so useful that I actually depend on them now. And using a functional language that lacks these features makes it much more difficult for me to write good code. It is very frustrating after a day of hacking in Haskell, going back to writing in any Lisp. 
I think the critique is mostly about the the Java ecosystem with its bloated frameworks you throw together to create an even bigger bloated thing. As for the JVM it doesn't integrate well with the OS facilities resulting in wasted resources. One example is page-table+cache unfriendliness: Each Java program you start has its own bytecode which is individually JITed. Compiled programs on Linux (and probably on Windows too) however `mmap` their executable code via page-table entries into the process memory, and even if you have 100 processes started, they all share the same physical memory pages for the executable code. Whereas, if you start 100 Java programs, you'll most likely exhaust your memory right away. But I usually have a hard time arguing with Java programmers about such things as they simply don't even acknowledge this as a problem. :-(
If you're on Windows, I wrote a short tutorial on how to do it: http://hack-your-fridge-or-die-trying.blogspot.ie/2015/08/haskell-updating-ghc-and-leaving.html It's very easy :)
Performance wise, the JVM is a software engineering masterpiece. The downsides are the culture around it, and how it is outright hostile towards the cultures and conventions of the underlying systems. Unix, Windows, OS X, it doesn't matter, JVM ignores their customs and substitutes its own. This makes it hard and cumbersome to integrate JVM-based things with native citizens. And then there's the infamous security track record, which is so bad that at some point, the infosec company I worked for had a standing order that nobody was to install anything Java anywhere without written consent.
&gt; I'm not necessarily a fan of writing code in Java but I haven't really heard a good case against the JVM itself. The start up time issue you mentioned is devastating when writing short lived utilities.
Ahh yes. I hadn't thought of this. It feels a little bit like cheating to me since I was interested in the structural aspects of the type checker that lead to one code problem generating multiple type errors. Not in scope errors seem too easy. But nicely done. If this were the IOCCC, I'd give you the "best abuse of the rules" award and then change the rules to prevent this kind of submission in the future. :) (Inspired by section 18, question 35 on the [hacker test](http://www.armory.com/tests/hacker.html).)
Awesome, this was exactly the kind of thing I was looking for with this question. This question/challenge has been lurking in the back of my mind for awhile now, and today I finally happened to run across a situation where I noticed that a change of mine fixed more than two errors. I decided to post the question now knowing that I knew of one way to get more than 2 errors. I'll post my answer in another comment, but it's interesting that like yours mine also leverages "No instance" errors.
Awesome, I'm loving he answers posted so far. Here's the actual code snippet I was working with that prompted me to post this. {-# LANGUAGE OverloadedStrings #-} module Main where import Control.Error import Control.Monad import Data.Readable import qualified Data.Text as T import Data.Text (Text) prefix :: Text prefix = "foo" nextSuffix :: [Text] -&gt; Text -&gt; String nextSuffix strs cur = maybe (if cur `elem` strs then "_2" else "") (show . (+ 1)) (maximumZ nums) where getSuffix nm = if T.isPrefixOf cur nm then Just $ T.drop (T.length cur + 1) nm else Nothing nums = catMaybes $ map (fromText &lt;=&lt; getSuffix) strs This code generates the following four errors: No instance for (Show b0) arising from a use of `show' No instance for (Num b0) arising from a use of `+' No instance for (Ord b0) arising from a use of `maximumZ' No instance for (Readable b0) arising from a use of `fromText' If you make the change s/(+ 1)/(+ (1::Int))/, all four errors go away and the code compiles. This seems very similar to the solution by /u/gelisam which also leverages "No instance" errors. He was able to construct a scenario where it's easy to arbitrarily add more and more errors. In my case I just happened to run across a situation where four type classes work fitting together quite naturally.
Could you elaborate on your dislike of macros?
I think it's probably the nicest dynamic language. I don't really want to use dynamic languages any more though.
I think you are thinking of Java applets in the browser a technology famous for slow loading times tepid adoption and security issues. Java the Language and the jre has no such rep
http://zeroturnaround.com/software/jrebel/
Big thanks to the new Stackage curators: Adam Bergmark, Dan Burton, and Jens Petersen, who all did a lot to help with the nightly builds and sending reports upstream leading up to this release. Most of the work involved is just telling people to relax upper bounds, and letting them know why tests start failing. Most things are automated at this point. If you want to get an idea of what we do on a regular basis, check out [the curators guide](https://github.com/fpco/stackage/blob/master/CURATORS.md). To see why packages were added/removed, you can look through the issue history of the stackage repository, it covers just about every change and its reason. There's not really a short summary.
It isn't just about execution speed. It is also about memory usage. But imagine how much faster the startup/execution could be if they didn't have to deal with all of the baggage of java cross-platform compatibility.
[k | k &lt;- [3,5..], k^2 &lt;= x] will diverge once k^2 surpasses x, since all of the infinitely many remaining values of [3,5..] are checked.
That's just ridiculous. Writing an app on Linux and having it run on Windows and Mac is a win any day. 
I think the more charitable interpretation would be that Rich Hickey doesn't need the computer to do as much thinking/checking as lesser mortals do.
&gt;Performance wise, the JVM is a software engineering masterpiece. Yes. But the rest of your post was either wrong or incorrect. 
Good call. Someone actually opened an issue about this last week as well. I've implemented and deployed this change: https://www.stackage.org/lts-3.0
Actually, the snapshot page says at the top: &gt; LTS Haskell 3.0 - GHC 7.10.2
Most of the ghci failures are the same bug, namely https://ghc.haskell.org/trac/ghc/ticket/10375 . Unfortunately I've been really busy lately and haven't had time to continue working on this.
Note that this is a commercial tool with annual license costs per developer, and thus entirely unsuitable for community projects.
Yeah, this feature is unusable to a large swathe of people without boot2docker support :(
https://github.com/commercialhaskell/stack/issues/776 It works. It was a long wait, but OK. :)
The point is that you don't need to pay for the overhead of virtual machine to be able to do that. Haskell is the proof.
In the early days of using Clojure where I work, we used to have a saying when there was a design decision to make: "what would Rich Hickey do?" We don't say that anymore since diving into Clojure's internals.
It's a one of Guido's reasons that python does not have private variables or any real kind of information hiding. 
To be fair, that isn't a problem for most people. Very few people are actually writing the kind of software that needs that level of interest in underlying performance stuff. 
Install a fresh new one.
Memory usage isn't inherently *that* bad in Java. The reason it explodes is because the garbage collector is so good *most of the time* people think they can get away with just about anything *all the time*. They can't.
Haha, sounds about right. I question his judgement a lot without having delved into the internals. Unlike others though, he should know better. 
There are free alternatives. Nailgun comes to mind. Theres another one which came out recently, one of the clojure guys made it, iirc. The general idea is to spin up a hot fresh JVM in the background for use when needed. 
What am I missing? Everyone is hesitant about making `impossible` a new full-fledged keyword. But no one has mentioned the seemingly obvious possibility of making it *not* a full-fledged keyword, but only give it special meaning in a pattern expression. That would be far less invasive. We would only lose `impossible` as a free pattern variable, and as a record field name in some cases. Maybe a few other small things. Syntactically, we always already know that we are in a pattern expression without `impossible` being a keyword. EDIT: The exception is pattern guards, so in this variation of the proposal `impossible` could not be used in a pattern guard. EDIT2: Maybe it can also work in pattern guards after all, but we would need to work out some details.
After university I was looking for my personal projects language. While working through SICP, I discovered I really liked Scheme. Clojure was hitting the scene at about the same time. It seemed like Scheme with the additional benefit of features like immutability, STM, and protocols (typeclasses). You can probably see where this is going: these features had existed in Haskell for years, plus you get a type checker. As others have said, if I had to use the JVM, if use Clojure in a heart beat. But for my needs, I think Haskell is a better fit. 
Omit the impossible case, clearly. 
Smalltalk is the dynamic language that doesn't want to play with anyone else, not with your usual editor, not your usual VCS,... This makes it strictly worse than most other languages.
... which means you need the type checker to reliably identify all missing (but possible) cases without programmer assistance, which it cannot. 
Deleting the code can make an infinite amount of type errors to disappear.
&gt; The reason was that there is no way to make guarantees about what kind of data you're handling. There's no way for the compiler to do it, you mean. You verify the behavior with tests - usually "live" at the repl, and then you can curate that session into a permanent set of unit tests. Yeah, it's clunkier than a type system, but it can check any behavior, not just types.
I agree it is not bad, but might depend per person. I preferred the slightly more serious tone of RWH. Just saying that best is subjective and it is good to list alternatives (including LYAH).
* Try to use meaningful names. It's important in code and it's important in tutorials. `val1` gives me *no* intuition about what we're talking about. `age` on the other hand I know is something number-like at least. * You're not making it entirely clear you're treating `num2` as a regular integer, since you're never comparing it to a literal or anything. Perhaps this is intentional in which case it is fine, but if it's not it's something you should think about. * I think it could be made more clear why `IO a -&gt; a` is a bad idea. You really shouldn't need a convoluted example with two functions to do that – it's quite an intuitive concept! Otherwise I love the idea and the order of things introduced. Good job! :)
I have written a few things in Clojure, and I really liked it at first. However, it has the same issues as Ruby or Python (i.e. it's unityped, and you can't make guarantees at compile time about what data you're going to get). Testing seemed annoying. Namespacing always seemed hit-or-miss. Most fundamentally, though, was the realization that, theory aside, I was just writing imperative code with a funky syntax. I have come to believe that lisps are closer to Python/Ruby than they are to Haskell, and as such the exact same problems arise.
Here is a variation: Change the first '-' to '{' to eliminate arbitrarily many errors. -- x = f f f f --} 
Will this be a weekly series?
Regarding your edit, a recent refactoring effort of mine comes to mind. If you change a constructor of a data type, you can get plenty of errors of the type "can't match expected type" errors - usually by supplying too many or too little parameters to the constructor, or by not complying with the types. For me, it was adding another parameter to the constructor, thus it was too few parameters. That error suddently pops up everywhere in your code where you use that constructor.
Definitely not weekly. Would be nice, but I just don't have that much time :)
[Also worth a read.](http://www.cse.chalmers.se/~nad/publications/coquand-danielsson-isomorphism-is-equality.pdf)
&gt; * We decided that we don't want to be able to convert IO a to a, and so once we are in IO context, we can't get out. &gt; It looks like we are taking out the value out of the IO, but we aren't really It seems to me the important thing from the perspective of a beginner is that you _can_ gain access to the value, and here is how you do it in a controlled way... and here are the advantages of doing it in this controlled way. Whether the way you gain access to the value _really_ should be called _"convert IO a to a"_ or _"taking out the value out of the IO"_, or if it should be understood as something else that is not at all anything of the above may not be very important initially. At least that is my 2c
&gt;To me it's really important to know that the person variable has at least the name and age field, and that they're both non-null. There is [core.type](https://github.com/clojure/core.typed) for that.
The part where you hand-waive the fact that your language is lacking certain safeguards with the lame excuse that "we're all consenting adults here", which totally misses the point. Python does this a lot.
Not at all. What I meant is that extremely intelligent people sometimes have trouble imagining what it's like to be less smart. Personally, I find programming in clojure quite difficult, because there's so much to keep track of, but if you're good at keeping track of things, then it's probably not a big deal.
Haskell Platform and LTS Haskell are two different approaches to solve similar (but not identical problems). The big distinguisher is that a user can reasonably be expected to install every package in the platform (possibly not at the same version number), since its curated for quality and importance, but installing every package in LTS, while possible, is not intended. There are two ways to use LTS Haskell (which is one of the two "tracks" of stackage): you can set the resolver field in your `stack.yaml` (either global or project-specific) if you're using stack, or you can put the linked `cabal.config` file next to your `foo.cabal` file (and preferably, but not necessarily, use cabal sandboxes).
Hard to say in a few words; they just seem like the wrong abstraction to me, and I could list a few symptoms, but I believe that fixing the symptoms alone wouldn't really change things much. I think my complaints mainly boil down to how there is no way to make any predictions about the behavior of a call without knowing what the macro in question does, and since macro calls and function calls share the same syntax, this basically means that you need to know the semantics of everything your code calls in order to reason about its behavior. The burden on my brain is huge, and IMO not worth the power I'm buying, and I'd much prefer a metaprogramming feature that has more safeguards built in and uses a more explicit syntax. A typed language that has separate syntaxes for meta-code and actual code, for example, would work: there, I can see immediately whether something is a macro or not, and I can tell a lot about what it can and cannot do from its types.
- I changed `val1` to `numToMultiplyBy`. Hopefully this is better. - I tried to make it easy to follow how `num2` is used, so the effects will become easy to describe. - Hmm. Perhaps this could be made simpler, do you have a suggestion on how to rewrite this? Thanks for the feedback!
I'm sorry but I'm not sure I understand (English is not my native language), can you please explain again?
one of the best virtual machines out there. your point?
Actually saying that. When I prototype stuff implementations and interfaces change so quickly there's no chance I'll keep the same tempo with my test writing, so I prefer the automated proof checking the type system provides.
Rust does this in the form of the [unreachable!](https://doc.rust-lang.org/core/macro.unreachable!.html) macro.
Do people here never hear about repl driven development?
Oh, I understand. I will think about that and try to find better words for it. Thanks.
&gt; mostly due to its paradoxical bootstrapping requirement that you already have a Clojure installed before you can install Leiningen to install Clojure. Whut
Any word on what happened to the distributed-process packages? I can't find the relevant issue/commit.
Hi hamish: unfortunately this version takes longer to reproduce the error under Windows 10, but the effect is the same: 100% CPU usage and when I killed the process, leksah occupied 1.5 Gb of RAM and grew very fast until it blocks the computer. Back to leksah 13.4.2
I write Java, and for me, the jaw clenching when reading impure and/or poorly-typed code is literal. my jaw clenches. my body's getting ready to fight or flight. with haskell, I get that every now and then when writing cyclic data (which may trigger non termination) or something, but mostly I just typecheck my code, sit back, and take out the errors one by one.
this? http://clojure.org/transients would love to hear more of your thoughts about them :)
Perhaps a relevant discussion freenode #Haskell yesterday on informal descriptions of what it means to be pure. Learn you a Haskell [(link)](http://learnyouahaskell.com/input-and-output): &gt; "And if we're taking data out of an I/O action, we can only take it out when we're inside another I/O action. This is how Haskell manages to neatly separate the pure and impure parts of our code." Real World Haskell [(link)](http://book.realworldhaskell.org/read/types-and-functions.html): &gt; "In Haskell, the default is for functions to not have side effects: the result of a function depends only on the inputs that we explicitly provide. We call these functions pure; functions with side effects are impure." And the freenode IRC discussion: https://gist.github.com/robstewart57/9bfd940cd6dfb8e8e0bf
Care to elaborate? I've been developing on Windows, Ubuntu and Mac for several years now, while mostly targeting Linux and I have yet to see a problem with that.
I am aware that the namespace system in Clojure is *novel*. I am not convinced that it is *good*. I found myself annoyed by, for example, referencing namespaces in the REPL or a file only to find myself getting errors when referencing things that should've been available. This is not a problem I have had in any other language. I'd also say imperative code is not discouraged as much as it might be. Anywhere you see `doseq` or `doall`, you're writing code for side effects and imperative control -- and, unlike Haskell, it didn't seem idiomatic to avoid this except in the "imperative shell" portion of the code. At least at the time that I was working with it, examples and documentation made pretty robust use of those macros.
&gt; Like many languages, Clojure is terrible at shebangs. You would use [Boot](https://github.com/boot-clj/boot/wiki/Scripts) for this, not Leiningen.
Cross compilation is compiling an executable on X that targets Y - for example, compiling a Windows or Raspberry Pi executable on your Mac. That process is currently very painful. Your best bet currently is to compile on every platform you want to have an executable for. This is problematic on some platforms, like the Pi, which don't have enough RAM to run ghc.
&gt; And it does better in spite of the fact that the JVM needs to boot up, which will suck up a fair amount of time in very short tests. When "very short" means low-tenths of a second you are correct. When "very short" means seconds [it's mostly amortized](http://benchmarksgame.alioth.debian.org/play.html#java).
This is not how I think of IO, nor how I would explain it to a beginner, but, of course, that says more about me than about the post. Even avoiding the dreaded "m" word, the way I'd describe IO (and the way it maintains purity) is that an IO action is a _plan_, and an `IO a` is a plan for how to get an `a`. Naturally, if you have a value of some type at your disposal, it's easy to construct a plan to get a value of that type (that is `a -&gt; IO a` makes sense). If you have a plan and you have a function which produces a plan, you can just use that second function as part of your plan (that is, given `IO a` and `a -&gt; IO b`, you can get an `IO b`). Comments about the post itself: * You have a typo: We decided that we don't want **_do_** treat `IO a` as `a`. "do" should be "to" * Your description of what can go wrong by treating `IO Int` as `Int` raises the question, "Why don't other languages have that problem?" I don't know how best to answer it in your post; it's basically because other languages don't have `IO Int` as a type of value.
He's obviously very smart, but not *that* smart. I mean he dismisses stuff like pattern matching and folds purely based on some ideological stance on complexity, and thus completely misses the point that those are examples of why his approach isn't universally good, or even well-defined. It might seem like nitpicking, but I think that's warranted when we're throwing titles such as "most intelligent on earth" around.
&gt; You have a typo: We decided that we don't want do treat IO a as a. &gt; "do" should be "to" Thanks! fixed. You raise some really good points. This is definitely something I will think about if/when I rewrite this post. 
Thank you for the links. This is an interesting discussion I still need to process :)
Even when used figuratively i reserve the right to nit-pick :)
I share the "plan" point of view. For me, saying that a value of type `IO a` is impure implies that running the following code would print "foo" to standard output (since `a` below is forced, at least to WHNF, by `seq`): main = print test test :: Maybe Int test = let a = putStrLn "foo" in a `seq` Just 5 Is case you're wondering, this program only prints `Just 5` to standard output. 
How are they different from the ST monad?
I'm not trying to suggest it's good. Sorry for giving that impression. I'm just saying its novelty makes it easy to misunderstand, and a misunderstanding of the underlying concepts would lead to it seeming "hit-or-miss" as you describe it.
Is there a named principle that describes the phenomenon where a person demonstrates a certain amount of remarkability, but suddenly this somehow means they must be compared to perfection and thus fail this test? ;) He's just a human being. Me, I wanted Erlang with a Ruby syntax and got Elixir; then I wanted Elixir with the typing and strict control of side-effects that Haskell has, before I realized that the actor model's message-passing is somewhat incompatible with "strict control of side effects" :/ (Or, I think. I heard of [Control.Concurrent.Actor](https://hackage.haskell.org/package/thespian-0.999/docs/Control-Concurrent-Actor.html), but...)
The "plan" view is a very useful intuition for IO and doesn't involve any unnecessary abstract nonsense. Your example is a great illustration of the fact that *evaluation* is pure while *execution* is impure: `a` is evaluated (to WHNF) but never executed.
I actually think of most (if not all) expressions as just values or "plans to get a value" (especially in a lazy setting). I tried is to make the distinction between non `IO` types and `IO` types, where `IO` types, when executed, might have other effects. Also, `main = print $ seq (putStrLn undefined) 5` will also just print `5` to standard output!
Hmm. maybe a good way to explain the difference is by saying that `IO a` can also be executed and not just evaluated while non `IO` types can only be evaluated? how can we explain what gets executed and what gets just evaluated?
[Link to stack](https://github.com/commercialhaskell/stack) 
instead of `numToMultiplyBy` I think `multiplier` would be better.
Not saying I am smarter (hope i didn't give that impression), just saying that there are other people i consider much smarter.
Schema and annotate are both runtime systems; core.typed really is static.
As far as I know, `main` is the only "IO type" that gets executed and `main` is, usually, built up using other IO "plans".
With haskell I find that when it finally compiles it has a good chance of working correctly. With clojure there's a tendency for it to compile easily but not work, requiring significantly more runtime debugging. So I'm firmly in the haskell camp, but that said I prefer the simplicity and consistency of clojure syntax. Haskell culture seems to favor infix operators, of which I'm not a fan. I think haskell spends too much of its wierdness budget on syntactic trivia, making the language more inaccessible than necessary. ed: also I was doing a project on the raspberry pi and clojure ran horribly on it. Haskell has been a pain too but if it ever compiles it runs with decent performance.
Correct; I didn't say they were all *static* type checking systems. So far runtime checking has been more widespread in Clojure circles, because it fits in nicely with testing and is easier to use. You don't get the same level of safety as core.typed, though.
I'm sorry to say, but much of this is off-base and incorrect. Maybe lein is a full blown build system. But once you have it, creating and building a project is trivial. And lein is NOT difficult to install. It's available in linux package managers, and in OS X homebrew. And the official install method is just to download and run the shell script (http://leiningen.org/#install). There is no paradoxical requirement to have clojure installed first. Shebangs are now treated as comments by the clojure reader. I'm still not sure how or why you would use them, but there it is. 
I don't think Map is a very good example, since it is already a functionally-oriented data structure and you can share structure for updates without anything like a TransientMap (in the clojure docs it mentions there is no benefit for linked lists, for a similar reason). Clojure supports transients for hash-sets, hash-maps, and vectors. For vectors Haskell has freeze and thaw to convert between immutable and mutable vectors (where clojure has transient and persistent).
Yeah, I was curious how its dynamic data types were implemented. Object, Object everywhere.
A TransientMap lets you mutate in place and then freeze when done while not duplicating the entire structure and maintaining optimal sharing with the pure structure that it originated from.
Haskell's vectors aren't the same as Clojure's though.
Sure, but the question is "how?".
Not sure which part is unclear to you, so please bear with me. Clojure's implementation is here: https://github.com/clojure/clojure/blob/838302612551ef6a50a8adbdb9708cb1362b0898/src/jvm/clojure/lang/PersistentHashMap.java The basic idea is that if an element in one of the leaves in the underlying HAMT is edited, that subtree is copied first &amp; then edited. It's not unlike the technique used for copy on write filesystems.
I'm saying that it's a problem you can work around.
The presence of work arounds doesn't change that &gt; Cross compiling in Haskell is pretty awful, currently. Especially since the work around is "don't cross compile, just compile"
I don't write tests when I prototype stuff either. However, I always have a REPL session open and I run code as I write it. I'm always confident exactly what the code is doing at any point in time because I can see what it's doing. I'll often take the code I write in the REPL session and convert it to tests once the code is doing what I want it to. 
&gt;Is there a named principle... ["...the human race is on a mission to divide everything into two clear columns..."](https://www.youtube.com/watch?v=VGErC6QQdoc)
I agree. We use XML all over the place, and there's nary a validator in site. We *also* can't switch entirely over to JSON, but the lack of an XSD validator doesn't hurt. We do have a schema, but there's only one component that "validates" against schema, and IIRC it doesn't do so by using the schema. Instead, the schema is actually generated from how *that component* needs the data to be formatted. &gt;:) Honestly, the only reason I'd *want* to validate against a schema is so that I can get type-level guarantees or correctness... that implies at least a type level function and maybe even some dependent typing -- non-Haskell stuff. GHC might be able to handle it, but it would probably be easier in Idris.
I might further clarify by saying the k in your problem formulation should be positive and should not include comment characters.
Your `PATH` has the GHC app before `/usr/local/`. Just delete the `GHC_DOT_APP` related lines, or move `/usr/local` to the front of your `PATH`. You could also just download an [updated GHC app](http://ghcformacosx.github.io) and change `7.8.4` to `7.10.2`.
"And then there's the infamous security track record, which is so bad that at some point, the infosec company I worked for had a standing order that nobody was to install anything Java anywhere without written consent." Java in the browser has a horrible rep rightly but not otherwise 
In the parlance of core.typed, type checks *are* static. Anything else is verification and/or tagging.
Oh, I don't know if I was unclear but wai-devel should watch for file changes in the directory tree and recompile the code by itself. You don't need to run a script for this. @get-your-shinebox have you tried modifying files and found wai-devel not recompiling your code?
What are you talking about? This not related to Clojure at all. Could you just to the same connecting a nRepl session to Emacs or vim and have my integrate environment ready. Clojure's nRepl is editor agnostic.
Absolutely, I always break my projects into small components. The smallest self contained block of code is a function, and therefore any project can be broken down into small modules. I really see no value in writing software using a monolithic style. The same way I wouldn't write a huge line function, I don't want to have a giant module. Clojure community heavily leans towards small single purpose libraries that you chain together to do things. This way we end up encapsulating a specific workflow in a library that has a small surface and we can chain libraries together the same way we chain functions. I would argue that even with static typing it quickly becomes difficult to reason about large intertwined systems. In a way static typing is an enabler for that, because you can get pretty far with your code running and compiling, while the complexity in a project continues to grow.
The emphasis in my question was &gt; working on smaller, isolated parts of the system *one at a time* which was meant to be read in opposition of &gt; working on smaller, isolated parts of the system, *many at once* Sorry for the confusion. I often work on several small modules at the same time, which means it's harder for me to, as you say, &gt; take the code I write in the REPL session and convert it to tests once the code is doing what I want it to.
Then the answer is no, I tend to work on large systems that have many moving parts. However, those parts are isolated by design and I generally can safely modify individual parts of the system in isolation. Again, I would argue that this is a good practice regardless of the typing discipline. When I work with multiple modules I just open up a REPL for each one. Why do you find this situation more difficult?
...probably because I only have one REPL up. I should try doing multiple. Can't believe I never thought of that. Cheers.
Must be a misunderstanding. I meant to say, either you want to use pure FP, or not. In the latter case, you can as well use C++.
Spot on! I have "type" checks (really data checks) on the database, and on the few limited places where a user can enter new data into the system. This pretty much stops bad data from propagating through the system. The majority of my type errors then tend to be simple parameters around the wrong way, misspelling a keyword etc. And when you develop using the repl you pick these errors up straight away and without any cognitive overhead. I suspect the biggest problem many static typing proponents have with clojure is their development style depends on static typing. I know developers who write 1-2kloc of code before running it. If you take that style of development to clojure you will face countless problems.
Scala only has TCO for self-recursive functions. Outside of that you need to use a trampoline.
It happens ☺
Absolutely true. Other than maybe [Seaside](http://www.seaside.st/), it's not very usable. But it's still *really* nice.
Can you provide/show a short code snippet to that would be impacted by this limit? I see that there is a possibility to try Frege online: http://try.frege-lang.org/ If I understood it right it is possible to show with ":java" what Java code is produced. Or is the issue that you point out independent of the produced Java code?
also, can't the list of file extensions be passed in somewhere? and then the haskell/yesod (edit: file extensions) can be in defaultSettings. 
Upgrade was pretty painless. Nagging CircleCI to enable 7.10.2 in the meantime as well to avoid using stack's setup :)
I certainly like your style better. Thank you for your contribution. I especially loved using ```&lt;$&gt;``` to circumvent ```liftM``` and ```$```.
I realized that and quickly switched to using Data.Text. Thank you for your input.
I'd guess it's because Haskell doesn't have dependent types yet, and this is a way to get some of the benefit without adding them.
That depends what you value in a REPL. One thing that's great in GHCi is the ability to ask the interpreter questions like "what is the type of this expression?", "what can go here?", and similar.
My day to day work is in C#. While it's no ML/Haskell it still is a statically typed language. And, again in my experience, refactorings that impact major parts of the system are rare in a well designed c# application. There is plenty of small scale localised refactoring like rewriting the internals of a method, or renaming a class, or moving a function to another class/assembly etc, but these small scale refactorings have never been an issue in clojure either. If anything I've found its more hassle in c# than it is in clojure because mapping to another type etc is much more work than changing the data format.
&gt; In Clojure, we currently have to trust that the programmer ensures that invalidated transients are NOT used. This sounds a lot like `Vector.unsafeFreeze` to me.
ah, that is good to know about. I've been recombobulating my ARM computers, trying to find a distro/board combo with the right features including an up to date ghc. I was using arch specifically because of its more up to date GHC, but on banana pi the arch isn't fully baked in some ways. 
Does it recompile all hs files just because one template was changed? This is what yesod-devel currently does. It cannot figure out which files to recompile, so it recompiles all of them. This lead me to abandon all template files in template folder. Because once my app grows to at least 10 hs modules full recompilations become unbearable. 
_Writing_ null checks is easy. _Reading_ cascades of ifs is a pain.
It does have to do with the created java code. I started taking a look at the compiled version of [this example](https://github.com/Frege/frege/blob/master/examples/DigitSum.fr). * First thing I noted was it goes 3 stack frames in before it even hits the main function code. * any call to Delayed.forced is 2 stack frames, and there are a lot of these * any calls to map is at least 3 stack frames. I would have to compile PreludeList to see if it has any more stack frame usages. * the unsafePerformIO that is done on the main function seems to be performed at 1 stack frame, but it uses a Delayed.forced. So, 3 stack frames is the minimum for execution. * A plus is it seems to optimize top level functions to 1-22 argument function objects instead of Curried, but Lambdas do seem to always be Curried. This example I was able to count at least five stack frames being used before the unsafePerformIO, and at least 8 in the unsafePerformIO. This example is drastically simpler than any real world code, so I would definitely assume that any real world code will break 9 stack frames with ease. Edit: I don't know the exact of how many stack frames unsafePerformIO takes because it's hidden behind uncompiled code. So I'm giving it the benefit of the doubt and saying it's only 1 stack frame
&gt; I asked the community about it Can you post some links, i would learn a lot from those discussions i think
It may be that having a type system like Haskell's encourages more aggressive abstraction, and subsequent refactoring to use the new abstraction. It may also depend on your problem domain, of course.
Specifically: transcompose = (iap .) . iap . ireturn 
I never had to use the the JVM. I was indifferent to it when I started, but eventually came to think of it as more of a hindrance. Haskell libraries that wrap C seem to do a better job of preventing lower level implementation details from bubbling up. In Clojure it was far too temping to just pass raw java objects (and their intent mutable state) up the call chain.
I think this comes with the territory with dynamic languages. To be effective you need to know the dynamic introspection tools. If you assume your audience is familiar with those tools it seems like a waste to specify everything; if the care to know the details, they can just ask the data to explain itself. Out of necessity, again, you tend to care less what classes/types something is, and care more about what it does (or can be done to it). With this in mind, dedicating time to explaining what something is, again, feels like a waste. I've only done a small toy project in Clojure, but my job is Ruby and Perl. I think the culture is the same, but I could be wrong. That said, I did find Clojure documentation way easier to digest that Haskell stuff. Admittedly I am still rather green with Haskell, but it is what I am focusing on learning at the moment.
I have never been a java programmer and I work as a professional python dev, so perhaps I am biased, but I agree entirely. I never saw the point for the noise about private variables and "consenting adults". Side effects are another thing, but I don't think anyone in the python community regularly waves away side effecty code by talking about " consenting adults." 
Good point. This is actually something I was unsure of so I just wrote it and hoped that if it is problematic someone would bring it up, so thank you. I will think about how to change it, if you have a suggestion how to explain that please share it :)
I've actually been working on a `transients` package at https://github.com/ekmett/transients I'm exploring several rather radical approaches to implement them though.
Uh, it looks like the formulae you're trying to generate would contain variables right? With universal and existential quantifiers? The logic of such formulae is not enumerable. I remember seeing something about heuristically generating quickcheck properties at some point, which might be more like what you want, but your proposal of enumerating all formulae in a logic is probably not going to work for any interesting logic. It's also worth noting that a number of properties can be automatically inferred from parametricity. [This tool](http://www-ps.iai.uni-bonn.de/cgi-bin/free-theorems-webui.cgi) helps with that.
I'm sorry I don't fully understand what you mean by this.
Thank you so much for this. Truly made my night
Primarily on the IRC channel a long time ago, and possibly somewhere in the subreddit. Can't find links, unfortunately. :( Edit: if you feel lucky, you can do a hail mary grep for my username on both of those things heh.
There is work being done but no proposal as of yet. 
I am amazed at seeing so many clojure user in the Haskell subreddit.
Richard Eisenberg is [leading the charge](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell).
I don't know about Clojure programmers, but I don't consent to any side effects unless the other party uses an IO.
"Use protection, kids!"
Please just remember to add upper bounds to packages you upload to Hackage, as otherwise it will cause pain for everyone else not using those Stackage/LTS thingies &gt; This is a big thing given the ~200 packages needed for a Yesod project, now I can keep them reasonably up-to-date without having to go to cabal-hell and back on a regular basis. Observations like this make me think that Yesod is the main cause for `stack`(age) having been created in the first place. Granted, we had already issues before, but Yesod brought the cabal-hell level to a whole new level that `cabal` couldn't cope with anymore it seems. But it's not the amount of packages involved, as `lens` and the rest of the Kmett'ian package empire have roughly the same complexity, and yet `cabal` works great there...
The finite strings are enumerable, so the finite formulae are also, unless by an enumerable logic you mean something other than the number of its formulae being countable?
What do you mean by "construct a namespace"? Could you give an example of the kind of invariant you want to prove? I'm also unclear what you mean when you say "saving work by checking if two types unify" --- how are you going to use quickcheck to ask questions about ill-typed expressions?
If you are on call at night to fix P0s in production the quality of your software will directly impact the amount of sleep you get :p Also the fear of something going wrong... Part of the reason I'm a big proponent of multiple layers of testing and static type garuntees.
I.e., talk about the lambda cube. 
&gt; Also, yes, typed clojure, but looking up the pros and cons of it is left as exercise for the reader. I'm decidedly a novice with both Haskell and Clojure, so grain of salt and all that, but I found Haskell's type system intuitive and fairly easy to use after a little practice, whereas I just feel I'm way, *way* too stupid to get to grips even with basic use of Typed Clojure. I've tried to produce fairly straightforward Typed Clojure code that compiles properly multiple times and I haven't succeeded once.
Yes!
About the Application.develMain requirement this is so that we can pass as few arguments as possible to wai-devel to make things easier. Moreover, there is an issue on github that might change wai-devel requiring Application.develMain. Here it is: https://github.com/urbanslug/wai-devel/issues/1 *edit* If you add your cabal-installed packages' package-db to GHC_PACKAGE_PATH it should work. Since the issue is ide-backend being unable to find dependencies installed via some tools. The problem is that cabal-install complains when GHC_PACKAGE_PATH is set to. So you will have to keep setting and unsetting this environment variable. *Yet another edit* There is a commit addressing the concern you raised. The commit message should answer your first question at least. Here is the link: https://github.com/urbanslug/wai-devel/commit/f5c75164572787e6d341569516d56c3a30692c8d 
System F is a neat subject but is already taken by another student
I thought it'd be interesting to turn the question on its head: what are some critiques of Haskell based on what Clojure does well? (My answer to the original question is basically: static typing and insisting on encoding everything using maps.) Here are things that Clojure does well, relative to Haskell: # Programming against interfaces instead of concrete implementations For all the good things Haskell brings to the table, this is one of the good inventions in programming it (or its community) has "forgotten". This results in: * Overly constrained APIs: does your function really need a "size-balanced, ordered tree maintained by Milan Straka and Johan Tibell" (i.e. `Data.Map`) or did you really mean to say that you need some data type that supports lookups? By not having common interfaces for things like data structures we're forced to do costly (and annoying) conversions between types more than we should. * Exposed internals: this is basically the story of `String` and `Text`. We could perhaps have just kept `String` but with a better implementation if it didn't expose its internals and have avoided years of pain. * Hard to evolve APIs: reliance on concrete types usually over-specifies what you need, which in turns creates stronger dependencies than necessary. * Dependency hell: reliance on concrete types also increases reliance on concrete packages (vs e.g. interfaces specified in base). This increases the chance of version conflicts.
The search space isn't changed except for a constant factor, since (== True) turns any boolean into an equation and any equation is a boolean. I already thought of dropping redundant laws as something that I probably want to add later. That said, QuickSpec looks to be pretty much exactly what I hoped to achieve in a few weeks-years of development and that I don't have to do that makes me happy.
That's not correct. To generate an expression e1 == e2, where e1 and e2 are of size n, your method would need to generate expressions up to size 2n+1, while QuickSpec only has to generate up to size n. This is a huge saving, because enumerating all expressions up to a given size takes exponential time. Their congruence closure technique cuts this down by another order of magnitude.
Woah. *Nice*.
Thanks for the ideas! By recursion schemes you mean [this](http://patrickthomson.ghost.io/an-introduction-to-recursion-schemes/) ? It seems like a nice subject, functional logic programming too. PS I edited the post with the info you mentioned.
It's great if Stackage works for you, but if you say that Stackage is an improvement over what any other statically compiled language offers you're ignoring that Stackage is actually a kludge/workaround. And Stackage is not much different than what Linux distributions do by creating a snapshot of package versions. The real innovation lies in the ambitious Cabal/Hackage project with the detailed inter-package dependency specifications. In other languages you have at best lower-bounds, and mostly trivial install plan solvers (and if you only use one of the Stackage snapshots, you don't even need any solver at all), and libraries try hard to remain backward compatible as it becomes a huge mess if programs would have diverging build-dep graphs. When you say &gt; Now given that in a web app framework we need JSON parsing/encoding, unicode Text strings and bytestrings all over the place, this is going to cause issues. And in statically typed languages we will run into that problem at compile time. The only case when I run into problems as you suggest is when package authors didn't care about their users enough to maintain their version bounds. But Hackage has its own way of dealing with that. So, again, it's great if Stackage works for you, but there's a more principled way to solve this issue that involves more than just package sets. I highly recommend reading [How we might abolish Cabal Hell, part 2](http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) which provides a glimpse of what's being worked on while people are led to believe that Stackage was already the bee's knees...
The strength of C#'s type system is nothing compared to Haskell's type system. Yeah, it's statically typed, but it's still very primitive.
/u/reutermj seems to make the claim that a functional language must suffer performance degradation on the JVM, because the stack depth of 9 will be frequently exceeded.
Yeah I'm not sure what "JVM inlining" is.
Haskell's packaging story is still not written in stone yet. If you just entered the scene, then unfortunately you walked in on a messy house that's still being organized. Stack does seem to make many types of jobs easier. However, it's also brand-spanking-new and so there's virtually no documentation or press for it yet. Cabal has been the de facto standard for a very long time, but it has "danger zones." Those zones actually rarely hit beginners, but when they do, beginners are totally unprepared for them. Stack eliminates some of that, but it doesn't have enough docs for beginners to really find it useful yet. So if you had waited 6 months to walk in, you might have found the place cleaner. So sorry for your trouble! We promise you'll like the house once we're done. :D
Thanks. I'm a collaborator on MinGHC so I'm happy to look into any issues you find.
It greatly depends what you mean by "embedded". If your embedded platform has a reasonable C toolchain and a unix-y environment, well, GHC should be able to generate code for it. If you have a restricted host (bare metal, no operating system), you should look at the [HaLVM](https://github.com/galoisinc/halvm) project, which runs Haskell programs directly on top of the Xen hypervisor. If the hardware restrictions are severes (counted in kibibytes, not mibibytes), you may have to use a specialized runtime system. In the OCaml community we have [OCaPIC](http://www.algo-prog.info/ocapic/web/index.php?id=ocapic) that provides an OCaml toolchain and runtime for PIC microcontrollers.
There's a language you might wanna check out: Bluespec Verilog - it's the bastard child of haskell and verilog. It's not actual haskell, but it's a HDL that's inspired by haskell. I think though that you could have a hard time finding a freely available compiler - it's a proprietary language if I remember correctly, and a compiler license might cost you, but the book Bluespec By Example is available online for free, if you wanna take a look first. Or maybe your university or employer has a license. Edit: It's apparently based on SystemVerilog rather than Verilog
I don't have specific criticisms *per se*—I haven't spent enough time with Clojure—but a general observation: **Haskell and Clojure are less comparable than people make them out to be**. To me, Haskell's mental model and abstractions are such that Clojure is fundamentally, qualitatively different and could not come close even *with* some sort of type system. Taking full advantage of Haskell, types are a core facet of my program and my way of *thinking about programs*. I don't write code and try to get it to typecheck, as I would in Java—that's almost not meaningful in Haskell. Instead I write code *wrapped around* my types, guided by them. The type system is more like a system of goals I want to achieve, not a safety checker. So when I don't like Clojure because of its type system, it's not a matter of dynamic typing vs static typing—I would use Clojure over Java any day. It's a matter of Clojure, as an expressive system, falling short of the ML ideal: types at the core of the language, not to catch bugs but to help us better express ourselves. **It's not *just* static typing.** That's why I don't believe in gradual typing and why I don't believe in core.typed: a type system added to a language *post hoc* simply cannot be expressive in this way. It can only be a constraint on programs that want to do things their own way. The type system added after the fact inherently battles the natural flow of the language. A type system integrated into a language from the ground up, on the other hand, can work with the language to naturally push your code in the right direction. I think this dictates both how we write programs *and* how we design languages; it's an important idea! (I wrote more about it [on Quora](https://www.quora.com/What-is-the-place-of-the-type-system-in-the-design-of-a-programming-language/answer/Tikhon-Jelvis?srid=p9P3&amp;share=1) if you're interested.) The problem, of course, is that this is a big idea, so it's inherently unconvincing to people who haven't figured out the Haskell mindset in the same way. Perhaps it's fundamentally subjective or perhaps it's just distinctly non-obvious, but I doubt I could do more than preach to the choir. It's not a concrete problem with the language that either exists or doesn't, it's a *gigantic* fundamental difference on which people can and will disagree.
The simplest representation of a graph I've seen is an adjacency list. It's basically a list of lists. I think that's kind of what you're talking about.
&gt;The main designer and implementor of Bluespec was Lennart Augustsson. /u/augustss is *everywhere*
Dependent types feel like the natural next step from the topics you covered, but they can be difficult to understand. If you're feeling really ambitious, take a look at ["Simply Easy: An Implementation of a Dependently Typed Lambda Calculus"](http://strictlypositive.org/Easy.pdf) which guides you through implementing a simply typed lambda calculus and how you would extend it to dynamic types. (Which is actually a bit simpler than System F!) Both systems are implemented as Haskell interpreters. But be aware that it can be hard to understand, so try not to spend too much time on it if you get stuck. Another option is to talk about *total* functional programming with data and codata. Essentially, this extends the types you're familiar with from System F with a notion of codata which can be infinite, and constructs a framework for expressing well-formed, total functions over infinite inputs. (The key is that these functions have to be *productive*: they have to produce some new output in finite time when give more of the input.) ["Total Functional Programming"](https://uf-ias-2012.wikispaces.com/file/view/turner.pdf) is the place to look for that. If you want something totally off-the-wall, assuming nobody else has taken the topic, you could research how the lambda calculus is used in linguistics. The whole idea of trying to formalize natural language is out of style at the moment, but it's still interesting. I'm not super familiar with this myself, but [these slides](http://disi.unitn.it/~bernardi/Courses/CompLing/Slides_04_05/lambda_extra.pdf) look like a good introduction. The basic idea, as I understand it, is to model parts of a sentence that can't stand by themselves as functions in the lambda calculus. (Also, this doesn't have much to do with Haskell, I just think it's neat.)
&gt; I highly recommend reading [How we might abolish Cabal Hell, part 2](http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) which provides a glimpse of what's being worked on while people are led to believe that Stackage was already the bee's knees... Do note the final paragraph of that post: &gt; In the next post we’ll look at curated package collections, which solves a different (but slightly overlapping) set of Cabal Hell problems. Nix-style package management and curated package collections are mostly complementary and we want both.
I remember reading that a union-find data structure, which is extremely efficient, explicitly relies on destructive updates to get its efficiency, which is why it is hard to get the same performance in a pure language.
Sure, but if you run all the programs in parallel all terminating programs still return their outputs eventually. (Or you could just, you know, terminate programs that take too long.)
I've implemented something similar to transients, so I'm not sure if it's the same as Clojure's, but basically you keep track of which nodes of the tree you own and which ones you don't. Then when you want to update something you mutate the ones you own and you path copy for the ones you don't own.
Sounds like a tag you have to check at runtime. Does that really make things faster in practice?
The University of Kansas are working on a translator from our homebrew version of [hArduino](https://hackage.haskell.org/package/hArduino), into C. This will mean that you can compile monadic Haskell programs that use the given API into remotely executable code. A key technology inside the translator is the [remote monad](http://ku-fpg.github.io/papers/Gill-15-RemoteMonad/), as presented in our upcoming Haskell Symposium paper. The reification piece is not written up yet, and is work in progress. We are using some of the same technology as Conal used, and it requires a custom GHC pass.
Yes, there is an effort, though the company backing it has foundered. Conal Elliott describes it on [his site](http://conal.net/blog/posts/haskell-to-hardware-via-cccs), and [conal/lambda-ccc](https://github.com/conal/lambda-ccc) is where to get the code.
Like already mentioned, this is basically an adjacency list representation of the graph, except with asymptotically faster lookup times. I suspect you won't find much of a difference between lists and sets in sparse graphs though. I've been using that to represent my graphs in contests for some years now and it has worked well for me, if your graph is constant you'll probably find you get better lookup times in an edge matrix representation (which also has the advantage of being able to distinguish between orphan nodes and non existent nodes).
&gt; refactorings that impact major parts of the system are rare in a well designed c# application. My day-to-day work is also in C# and I'm very skeptical of this claim. But I'm not a [true scotsman](https://en.wikipedia.org/wiki/No_true_Scotsman), so I can't really tell if I'm writing code that's sufficiently *well-designed*. My anecdotal experience is that my haskell code is smaller and easier to refactor, which encourages refactoring and straight up rewrites. The C# code on the other hand is burdened with 'patterns' and boiler-plate as a result of its inability to express certain high-level abstractions. That forces us to spend more time 'architecting', writing even more boiler-plate, and so on. And then we end up with something that seems modular-ish and flexible, until a new requirement challenges a basic assumption, and then you're left trying to patch up a large complicated system instead of just rewriting a small system from scratch. 
You're only storing edges in an adjacency list. So if you don't keep track of the nodes separately, you're basically going to lose nodes that aren't adjacent to anything.
UPDATE: commit https://github.com/urbanslug/wai-devel/commit/f5c75164572787e6d341569516d56c3a30692c8d changes strict dependence on Application.develMain and reduces the number of files we compile.
like, instead of meeting upon request for every file extension desired, and then all projects tracking all those files, why not let the user pass in the list of file extensions they want for their project? maybe develMain could be made into a record with an I/O action and a list of file extensions, for example. something like: -- WAI-develop code data Devel = Devel { develMain :: IO () , develExtensions :: [String] } and: -- user code devel :: Devel devel = Devel (develMainHelper' getApplicationDev) [".hs"] Devel could just be a tuple, if you don't want the user to have to import anything.
I created this some time ago, tell me what you think. :)
My version is: Version 0.1.2.0, Git revision 65246552936b7da4b64b38372feac903d96a8911 (dirty) Were you able to import the package in the stack ghci console?
By not writing monolithic projects mostly. There's absolutely no reason to do it in any language and the fact that you can use the type system as a crutch to do that is not a positive. Most Clojure projects are composed to small single purpose libraries with a small surface API. These libraries are then composed the same way you would with regular functions by calling the public functions and threading data through them to get the desired result. 
you might want to watch the [video linked at the bottom](http://blog.cognitect.com/blog/2015/6/30/walmart-runs-clojure-at-scale)
I would argue that's a negative. The fact that the language acts as an enabler for writing giant monolithic projects is not a good thing.
This has been addressed in github issue #1, you may want to look at it: https://github.com/urbanslug/wai-devel/issues/1 The proposed method requires nothing from the user in terms of file extensions, is more expensive computationally but has the advantage of added flexibility. It involves listening for file changes in the the entry module and the modules and data files it depends on recursively.
&gt; member of the hidden package ‘hsexif-0.6.0.4’ With cabal, this message means that hsexif was not explicitly listed in the cabal file's `build-depends` section. So you probably need to add `hsexif` to some field of your `.yaml` file.
Fair enough :). Well I use Cursive with IntelliJ though ;)
e.g. I have a custom primop for detecting if a `SmallMutableArray#` is frozen or not, then I can advance a wave through the structure without copying in order to switch from a transient to a frozen structure, retaining invariants about if things below me are really frozen or not. Logically I consider a SmallMutableArray# as 'really frozen' if any ancestor of it is frozen and freeze it oppportunistically on encountering it. This requires me to deal with `SmallMutableArray () a` as a form of `SmallArray a` and to rely on the operational characteristics of all of the primops that manipulate a `SmallArray` or `SmallMutableArray`. This lets me avoid the double allocation costs at the expense of a rather complicated protocol of how to walk. I can do either O(1) amortized conversions or O(1) worst-case which temporarily leaks things on the mutable list. Done right I should be able to have a single code path for mutable inserts and the like, and then implement my immutable operations by using it. Alternately we could rebuild a bit of the garbage collector to make regions that can be 'frozen' all at once, but this requires duplicating almost every mutable data type in GHC to add a version with a reference to the region. This would get us O(1) worst-case, without mutable list leaks.
In stack, the "member of the hidden package" error usually means you didn't add the package to the `build-depends` of the cabal file. (Since `hsexif` is not in Stackage, you'd also have to add it to the `extra-deps` of `stack.yaml`; however, as you have already installed the package it is possible that you have already done that.) P.S.: Don't use `stack install` to install libraries. Just add the package to the `build-depends` of the cabal file and it will be installed the next time you `stack build` the project. 
I though the cabal file was auto generated by the stack tool, and I was not supposed to mess with it. Adding hsexif to the build-depends on the cabal solved the problem! :) Thank you! 
okay, maybe I'm misunderstanding the project, but a default builder would not require anything from the user either but a main function: type Devel = (IO (), [String]) defaultDevel io = (io, [".hs",".xml"]) since the project is named "WAI-devel", I would think that the watched file extensions would be end-user configurable. they might be using some random server that is configured in some random format, that's not specifically yesod (Lucius/Julius/etc.). either way, this is exciting :-)
Not yet...
One important issue here is the ability to make small refactors safely. You might have well-designed software with clear boundaries, which still manages to break in an unexpected way when you do a refactor. Maybe in a moment of weakness, you (or a coworker) introduced a hack, in order to meet a deadline. Maybe that hack isn't well-documented, and it leaves a Gotcha, so that later trivial-looking refactors can unexpectedly cause a bug. I have seen this happen before. I don't think it's responsible to say, "the solution is simply to have a team that doesn't put hacks in leading up to deadlines", because you are in trouble when that inevitably happens.
The code is at least as beautiful as the wallpapers it generates :D
That was my point as well, with Haskell it's unbelievably easy to break your projects apart into small independent sub-modules.
ah, still doesn't work on stack projects yet... :(
I've moved to stack.
&gt;Say you have x shared libraries that rapidly change their interface, and y applications depending on them. I've been developing Clojure professionally for the past 5 years, and I simply haven't seen this as an issue in practice. I find that library APIs don't actually change all that often and when they do, testing the new API via the REPL is a trivial task. It's not like APIs change randomly, and once you update the API in one place I've never found it to be a burden to make the same update in others. This once again goes back to how you structure your code of course.
Conversely, you simply don't let your code grow to that point using a dynamic language. I find when I work with Clojure I tend to keep modules small from the get go and I refactor as I write the code. I find the REPL makes a huge difference for me, as I can test something that I refactored immediately and see that the code is doing precisely what I intended.
Please try again with Leksah 0.15.1.3, as I believe this issue has been resolved now.
Please try again with Leksah 0.15.1.3, as I believe this issue has been resolved now.
Please try again with Leksah 0.15.1.3, as I believe this issue has been resolved now.
Please try again with Leksah 0.15.1.3, as I believe this issue has been resolved now.
use a REPL and many small modules? me too in Haskell... 
well, if I read a study that said "Haskell is more productive", I wouldn't even care. I think most such studies have an illusion of meaningfulness. what about sampling bias of subjects, long term impact, etc? my argument comes from personal experience, and the relation between language features and "software engineering principles" (like readability, maintainability, safety, etc). for example, I might say (1) hey, you can have your lsDirectory output a Maybe type in an IO type where Nothing means file not found, Just [] means empty directory, and Just (x:xs) means a nonempty directory and (2) this function is more safe, because it won't throw any runtime errors, while forcing you to handle each case at compile time. the link is between the language feature of algebraic data types, and the software engineering principles of being explicit and handling errors.
A very different experience from what I've seen.
P.S. https://github.com/ndmitchell/ghcid is even faster than --no-code. It's pretty much instantaneous for me.
Except that majority of Clojure code consists of transforming sequences and that's done by higher order functions from the standard library. Pretty much all of these functions handle nils intelligently and they end up bubbling up to a shallow layer of domain specific logic at the top. You don't pepper nils all over the place in Clojure as you would in an imperative OO language. If you look at any popular Clojure library on GitHub, you'll find very few nil checks there.
Dumping the conversation I had with /u/deltaSquee from IRC here. 04:23 &lt; spacekitteh&gt; has anyone got any resources on the "monads = generalised variable binding/substitution" interpretation? 04:53 &lt; bitemyapp&gt; spacekitteh: I usually find it easier to think about it in terms of join. 04:54 &lt; bitemyapp&gt; spacekitteh: the return bit isn't unique to Monad, that came from pure via Applicative 04:54 &lt; bitemyapp&gt; spacekitteh: so instead let us consider join for a moment 04:54 &lt; bitemyapp&gt; @ty join 04:54 &lt; lambdabot&gt; Monad m =&gt; m (m a) -&gt; m a 04:54 &lt; bitemyapp&gt; Expr (Expr a) -&gt; Expr a 04:54 &lt; bitemyapp&gt; spacekitteh: here our join, if our Monad implementing datatype is Expr (a datatype describing an AST/language), is a bit like an eval right? 04:55 &lt; bitemyapp&gt; spacekitteh: or rather, at least one step of an eval process. Getting to normal form means a fixpoint generally. 04:55 &lt; spacekitteh&gt; hmm 04:55 &lt; bitemyapp&gt; spacekitteh: I'm not sure I can square precisely what "generalized variable binding" means because I don't know the context for those words, but! 04:55 &lt; bitemyapp&gt; I can offer some intuitions 04:55 &lt; spacekitteh&gt; check the reddit thread i just made 04:56 &lt; bitemyapp&gt; I did 04:56 &lt; bitemyapp&gt; that's why I checked in here 04:56 &lt; spacekitteh&gt; ah 04:56 &lt; bitemyapp&gt; Functor -&gt; Applicative is taking you from a functor to a monoidal functor 04:56 &lt; spacekitteh&gt; yup 04:56 &lt; bitemyapp&gt; I don't need to say more there, you know more math/aa/ct than I do 04:56 &lt; bitemyapp&gt; but the way I think about applicative and monad is well demonstrated by parser combinators, I believe. 04:57 &lt; bitemyapp&gt; you can (conventionally, lets ignore Ed's trick) express a context-free grammar in Applicative 04:57 &lt; bitemyapp&gt; but a Monadic parser lends the ability to express a context *sensitive* grammar 04:57 &lt; bitemyapp&gt; which makes sense because 04:57 &lt; bitemyapp&gt; @ty (&lt;*&gt;) 04:57 &lt; lambdabot&gt; Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b 04:57 &lt; bitemyapp&gt; @ty (&gt;&gt;=) 04:57 &lt; lambdabot&gt; Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b 04:57 &lt; bitemyapp&gt; with the Applicative, we're functorially combining parsers, but the functions themselves don't generate any "parsers" 04:58 &lt; bitemyapp&gt; whereas with &gt;&gt;=, the monadic functions we're binding are generating more monadic structure 04:58 &lt; bitemyapp&gt; if that monadic structure is a list, then they're generating any sort of list they wanted, possibly based on "previous" values of lists that were bound over 04:58 &lt; bitemyapp&gt; if it's a Parser, then your functions are capable of modifying your parser based on input context 04:58 &lt; bitemyapp&gt; so we can think of Monads as being able to express contextually dependent computations 04:59 &lt; bitemyapp&gt; which looks like a *lot* of things we want to do in computation, but it's such a broad expanse that it's hard to pin it down to one thing. It's just a very common thing. 04:59 &lt; spacekitteh&gt; hmmmm 04:59 &lt; spacekitteh&gt; this is helping 04:59 &lt; bitemyapp&gt; Applicative is common too and very useful, but makes the dependence on previous computations/context/structure ineffable. 04:59 &lt; bitemyapp&gt; consider the Maybe Applicative and Monad. 05:00 &lt; spacekitteh&gt; @ty &lt;$&gt; 05:00 &lt; lambdabot&gt; parse error on input ‘&lt;$&gt;’ 05:00 &lt; bitemyapp&gt; with the Maybe Applicative, each Maybe value is judged on its Maybe-ness independent of the overall Maybe structure 05:00 &lt; bitemyapp&gt; @ty (&lt;$&gt;) 05:00 &lt; lambdabot&gt; Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b 05:00 &lt; bitemyapp&gt; incidentally, you can see a nifty progression here 05:00 &lt; bitemyapp&gt; @ty ($) 05:00 &lt; lambdabot&gt; (a -&gt; b) -&gt; a -&gt; b 05:00 &lt; bitemyapp&gt; @ty (&lt;$&gt;) 05:00 &lt; lambdabot&gt; Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b 05:00 &lt; bitemyapp&gt; @ty (&lt;*&gt;) 05:00 &lt; lambdabot&gt; Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b 05:00 &lt; spacekitteh&gt; : D 05:00 &lt; bitemyapp&gt; I highlight this in the book in the section on monoidal functors. 05:00 &lt; bitemyapp&gt; anyhoo 05:01 &lt; bitemyapp&gt; maybe Applicative has independent computations whose maybe-ness is independent of the overall structure 05:01 &lt; spacekitteh&gt; howso? i've never used the maybe applicative 05:01 &lt; bitemyapp&gt; whereas if you have a Maybe Monad you can make whether something is judged "Just" or "Nothing" based on the 'a' inside some previous "Maybe a" 05:01 &lt; bitemyapp&gt; well lets take a simple datatype like a tuple 05:01 &lt; bitemyapp&gt; and let us say you have some data you want to validate. 05:02 &lt; bitemyapp&gt; (String, Int), for example. 05:02 &lt; bitemyapp&gt; now if you have some straight forward constraints on the String and Int 05:02 &lt; bitemyapp&gt; say some smart constructors for turning them into proper datatypes like Name and Age 05:02 &lt; bitemyapp&gt; you could have functions like String -&gt; Maybe Name 05:02 &lt; bitemyapp&gt; and Int -&gt; Maybe Age 05:02 &lt; spacekitteh&gt; yup 05:02 &lt; bitemyapp&gt; where it's verifying that the Name isn't an empty string, that the Age isn't a negative number, etc etc 05:03 &lt; bitemyapp&gt; now if you want to collapse all those independent "Maybes" into one big Maybe (Name, Age) 05:03 &lt; bitemyapp&gt; that's applicative! 05:03 &lt; bitemyapp&gt; totally kool. liftA2 does it for you. 05:03 &lt; bitemyapp&gt; @ty liftA2 05:03 &lt; lambdabot&gt; Applicative f =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c 05:03 &lt; bitemyapp&gt; (,) &lt;$&gt; validateName &lt;*&gt; validateAge 05:03 &lt; spacekitteh&gt; ah so it's like "and" 05:03 &lt; bitemyapp&gt; liftA2 (,) validateName validateAge 05:03 &lt; bitemyapp&gt; spacekitteh: sure! It's like *any* Monoid actually 05:03 &lt; bitemyapp&gt; spacekitteh: so it can express disjunction, conjunction, first, last, anything you want 05:04 &lt; spacekitteh&gt; true 05:04 &lt; bitemyapp&gt; spacekitteh: it's just a Monoid of the "f" rather than the "values" inside them 05:04 &lt; spacekitteh&gt; ah right! 05:04 &lt; bitemyapp&gt; because the values are handled by the functorial (a -&gt; b) 05:04 &lt; bitemyapp&gt; you only need the Monoid because you have two "f"s you need to reconcile 05:04 &lt; bitemyapp&gt; it just happens to be a Monoid implicit to the semantics of the Applicative instance 05:04 &lt; spacekitteh&gt; sweet 05:04 &lt; bitemyapp&gt; this is also why there are often multiple Applicative instances possible for a given datatype 05:05 &lt; spacekitteh&gt; you a good teacher mang 05:05 &lt; bitemyapp&gt; because there are often multiple Monoids for a given datatype! Same deal. You'll often see a lot of similarities in the monoidal properties of what's possible for a given concrete Monoid and the Applicative as well. 05:05 &lt; bitemyapp&gt; spacekitteh: super obvious way to see it? 05:05 &lt; bitemyapp&gt; spacekitteh: look at the Applicative for (a, b) 05:05 &lt; bitemyapp&gt; spacekitteh: orderedness of the type constructor means the Functor *has* to map over the b (last remaining unapplied type argument) 05:05 &lt; bitemyapp&gt; spacekitteh: so, given that, what do you think the Monoid for the (a, _) has to do? 05:06 &lt; bitemyapp&gt; you have two (a,) values, what do you do? 05:06 &lt; bitemyapp&gt; you have to monoidally combine the a! 05:06 &lt; bitemyapp&gt; accordingly, the Applicative instance for (a, b) requires Monoid a =&gt; 05:06 &lt; bitemyapp&gt; now, back to Applicative vs. Monad 05:06 &lt; spacekitteh&gt; aaaah 05:07 &lt; bitemyapp&gt; spacekitteh: so, when we have independent Maybe computations that can be judged Just or Nothing independently of each other and we just want monoidal combination of them over some function application, Applicative is totes cool. 05:07 &lt; bitemyapp&gt; but! 05:07 &lt; bitemyapp&gt; (Name, Age) 05:07 &lt; bitemyapp&gt; what if people named Gertie aren't allowed be to aged 12? 05:07 &lt; bitemyapp&gt; what if we want Int -&gt; Maybe Age to return Nothing when Name == Just 12? 05:08 &lt; bitemyapp&gt; you have to bind over the Maybe Age, depend on *that* being Just, then check the underlying value you bound over, then return your Name. 05:08 &lt; bitemyapp&gt; there's your "context" dependence. 05:08 &lt; bitemyapp&gt; so, Monads express a sort of implicit linearization of sorts as well. Only linear insofar as nested lambdas and lexical scope do, but all the same. 05:09 &lt; bitemyapp&gt; Monad isn't particularly special here WRT lambda calculus, it's just nested lambas and lexical scope, but it's very nice for cleaning up the mess that can turn into syntactically. 05:09 &lt; bitemyapp&gt; spacekitteh: also thank you for saying I'm a good teacher. that made my day after a very rough week :) 05:10 &lt; bitemyapp&gt; spacekitteh: hope that helped, I need to get back to editing the book. 05:10 &lt; spacekitteh&gt; hehe no problem. thanks, it did, yeah Partial brain dump from [the Haskell book we're working on](http://haskellbook.com/)
&gt;With haskell I find that when it finally compiles it has a good chance of working correctly. With clojure there's a tendency for it to compile easily but not work, requiring significantly more runtime debugging. Anybody who does serious development with Clojure does it using the REPL. I don't mean it in a sense of popping up individual snippets of code in the REPL, but rather having it connected to the editor and having the entire application loaded there. Any time I write a function I run it to see that it's doing exactly what I want. I would never write even 10 lines of Clojure and then try to compile the code after. Literally the first thing I do when I start developing is open up the REPL.
&gt; This increases The end of your post was cut off.
I wonder how many of these will be solved by [backpack](http://plv.mpi-sws.org/backpack/)
Thanks both of you. This made my day!
So many times in my life I have been unofficially give that award. It's an honor every time. Thanks :)
In my simplistic understanding, the `do` notation is pretty much generalized variable substitution in a nutshell, and the bind operator deserves its name since it lets you bind a variable. It's generalized because it's not just binding a Haskell value to a variable, which we could already do, but extending the notion of binding to work on various functors (`[]`, `IO`, `Maybe` &amp; friends). Since these functors don't generally have a one-to-one correspondence with values of their type parameter, the semantics of binding (aka variable substitution) can involve zero values being bound (`Nothing &gt;&gt;= f`) or many values being bound (`[0..] &gt;&gt;= f`). Some readers may like to note that the "promises" of the JavaScript world have a monadic interface (if you ignore some weird details). You can make a new promise and you can bind one promise to a promise-returning function, so it's a lot like `IO`. Indeed I expect many beginning JavaScript programmers ask "why can't I just get the value out from a promise?" /u/edwardkmett's [`bound`](https://hackage.haskell.org/package/bound) package is a nice use of this interpretation by the way.
I noticed as well that `stack` requires a totally new mental model to work with. And [this comment](https://www.reddit.com/r/haskell/comments/3ghvzd/how_to_install_network_with_minghc_7102/cu2sstf) by /u/eacameron makes it sound like we should wait another 6 months till `stack` can be considered ready for inexperienced newcomers.
&gt; It's certainly better than anecdotal evidence used otherwise. I'd argue the opposite actually. The problem with "scientific" studies is that they have an air of credibility about them so people are more inclined to take them at face value than they should be. Personally my long term experience with Haskell and with reading what other people have to say about Haskell is far more convincing to me than any scientific study could be. I imagine it's the same with you and Clojure.
Sounds like you need a more sympathetic compiler.
Can you condense it?
I'm gonna ignore the attempt of discrediting my critique via ad-hominem-ization, but as to &gt; It's sheer fiction to assert that cabal hell is yesod's fault. It's there whether you have yesod or not. Yes, it's totally true that it *can* happen even without yesod involvment. But there's no denying there's very strong striking correlation between the yesod package ecosystem and the occurence of "cabal hell" that isn't apparent elsewhere. I just strongly disagree with the way `stack` attempts to "solve" the issue, as I honestly believe there's a better way to address the issues than `stack` does, and we shouldn't give up and settle with what `stack` does. It'd be quite sad for Haskell if `stack` only "won" because it's better at advertising and has more menpower behind it, rather than the more ambitious and principled scheme the Cabal developers are pursuing (which seems to take forever, I admit). And I'm really worried that `stack` may win, because too often it's not the better designed tool that wins but rather the tool that manages to win the time-to-market race... 
Sounds like a threat... ;-)
Why do you need `stack` for that? Isn't it enough to have `cabal` interpret the respective `cabal.config` specifying the appropriate LTS-constraints?
Idris is a haskell-like dependent type which compiles to c. In contrast to Haskell, Idris' strict evaluation by default makes it have less performance and memory issues. Athough It looks Idris is not designed specifically for embedded hardware, the team makes Idris practical for limited computing resources Edit: I forget to post website [Idris](http://idris-lang.org)
[The Dual of Substitution is Redecoration](http://www.ioc.ee/~tarmo/papers/sfp01-book.pdf) (2002) is another great reference on the topic and is the origin of much of the terminology in this space.
This is a little harsh IMO. Those remarks from the other conversation weren't as negative in their original context as they sound here.
Could you please bring this up in the github issue comments section because Michael Snoyman is my mentor on this project and he is the one who filed the issue. It would be better if he had a look at your idea.
haha be afraid :D
&gt; there's no denying there's a very strong striking correlation between the yesod package ecosystem and the occurrence of "cabal hell" that isn't apparent elsewhere Yes there is. For the third time of telling you, I, along with plenty of others, have had _lots_ of problems with cabal hell without ever attempting to install yesod. Your assertion is false, and repeating it more firmly doesn't make it true. Cabal install was by far the most unpopular aspect of haskell development in the survey. Yesod wasn't. Other than from you, I don't hear many complaints about yesod, and I've heard lots about cabal install. If stack "won"(?), it would be because it works quickly with far fewer problems and never renders itself inoperable, whilst cabal install didn't improve fast enough. Contribute to improving cabal if you want it to be more popular. Focus on speed, good defaults and error avoidance and recovery. Denying there's a problem with cabal install and insisting that all users must feel the pain from inaccurate bounds is the _opposite_ of improving its user experience. 
CrossClj (https://crossclj.info/) does many things that Hoogle does
&gt; I just strongly disagree with the way stack attempts to "solve" the issue, as I honestly believe there's a better way to address the issues than stack does, and we shouldn't give up and settle with what stack does. You seem to object to stack as a matter of principle. While there is nothing wrong with that in itself, there are at least two additional aspects to consider in this case. Firstly, package management is not a goal in itself, but a stepping stone towards doing other tasks, like a physiological need in [Maslow's hierarchy](https://en.wikipedia.org/wiki/Maslow%27s_hierarchy_of_needs). Most people are not willing to endure years of avoidable trouble with package management merely to wait for a more elegant solution, even more so when a tool which solves their concrete problems already exists. In this case, that is an entirely reasonable attitude. Secondly, you speak of "the Cabal/Hackage project" as if it was a sparkling new, radically innovative initiative at risk of being snipped at the bud by the corporate machine (sic) propelling stack. A glance at recent Haskell history, however, shows that is not quite the case. Cabal and cabal-install have been around for nearly ten years, and we are well used to both their merits (they indeed do a remarkably good job of making the tricky dependency graphs of Haskell packages manageable) and the pain points found in day-to-day usage of them. stack is an attempt at dealing with real problems amply documented throughout the years. It is not an attempt to win some "time-to-market" race. &gt; I'm gonna ignore the attempt of discrediting my critique via ad-hominem-ization But it isn't really an *ad hominem*, is it? Just read your posts: they are dripping with sarcasm, liberally decorated with rhetorical questions and ironic smileys. They generally try to show stack in a bad light at every single sentence (save for those which are digs at Yesod instead). Your words do make it sound like you have an axe to grind.
You've got a point there, although I'd argue that Haskell is the place where the first aspect you mention is often less emphasized. Haskell is still a research laboratory for software engineering, and this includes pushing the limits of package management beyond the status-quo. Backpack and Nix-style package management are the most promising new developments in this area. Stack is rather boring compared to from the point of view of language research.
You could copypaste the generated code to your source file and document that? To get the code, add the `-ddump-splices` flag to the compiler. 
If documenting the definition for purposes of export, that is, not also wanting to _generate_ documentation during Template Haskell expansion, then you can place the documentation fragments adjacent to the declarations and Haddock will sort of do the right thing: {-# LANGUAGE TemplateHaskell #-} module Foo where import Control.Lens data Bar = Bar { _fooBar :: Int } -- | Some documentation for the 'fooBar' declaration. makeLenses ''Bar The above will cause the generated `fooBar` `Lens` to have the haddock documentation `Some documentation for ...`. If you don't have enough control to place the comment directly above the declaration, you can munge it using exports like: module Foo ( Foo (..) -- $fooSomething , fooSomething ) data Foo = Foo {- $fooSomething Here you can place documentation/comments that will be rendered below the 'Foo' type and above the 'fooSomething' function in the resulting markup. -} deriveSomething ''Foo -- defines a function called 'fooSomething'. 
&gt;Haskell is the place where the first aspect you mention is often less emphasized. Haskell is still a research laboratory for software engineering, and this includes pushing the limits of package management beyond the status-quo. Indeed. It's just that it takes an unusually large amount of good will to endure experimentation with something as basic as package management, even for Hakellers. On a more optimistic note, another aspect of being a "research laboratory" of sorts is openness to multiple alternatives. I don't believe wide adoption of stack would lead to suppression of work towards more innovative approaches.
&gt; not also wanting to generate documentation during Template Haskell expansion Can I generate documentation during Template Haskell expansion? I couldn't find out how. &gt; Here you can place documentation/comments that will be rendered below the 'Foo' type and above the 'fooSomething' function in the resulting markup. Yes, I've already tried documenting stuff in the export list, but the documentation ends up between the declarations and not in the right place.
Actually, it's quite the opposite for me. In absence of studies I have to rely on anecdotal evidence. However, the difference with studies is that they allow looking at the big picture. Instead of relying on a story here and a story there, you can look at what happens in a general case. I find the assertion that anecdotal evidence is more valuable than empirical evidence rather surreal to be honest. 
Relevant issue: https://ghc.haskell.org/trac/ghc/ticket/5467 This seems to be a non-trivial problem, given that one doesn't necessarily know what will be generated by TH. The best solution I can think of would be to extend Haddock syntax to bind a documentation block to a name, e.g. data World = World {_worldTime :: Int, _worldTemperature :: Int} makeFields ''World -- generates lenses 'time' and 'temperature'. -- [bindTo] temperature -- |Gets the world's temperature -- [bindTo] time -- |Gets the world's current time.
&gt; I find the assertion that anecdotal evidence is more valuable than empirical evidence rather surreal to be honest. If we were talking about physics I would agree with you. When it comes to social issues everything becomes a lot more fuzzy.
If you only work on one or project, then yes, putting the LTS constraints `cabal.config` works very well. That was actually the setup I used for a long time. However (and this is specifically about my situation, so the conclusions don't apply to everyone), I have around eight different projects that I am frequently flipping between. Most of them are web applications that use `yesod`, so they have a ton of dependencies (over 100). Consequently, the ability to share package sets is very important. Yesterday, I upgraded half of them to LTS-3.0 (just the ones I happened to work on), and the all of the LTS packages only had to build once. In my situation, that's a useful feature that `cabal-install` does not offer at this point in time. As for your question about /u/mallai's situation, lts-2 uses GHC-7.8.4 and lts-3 uses GHC-7.10.2. I think the benefit here is that `stack` can manage multiple GHC versions.
&gt; I think empiricism is just as valuable in software development as it is in any other case. Do you think that empiricism can "prove" that language A is better than language B in the same way that empiricism can "prove" the existence of the Higgs boson?
It implies that something was broken down into smallers parts. It's a long step from that to criticising Haskell for "enabling writing giant monolithic projects".
http://trac.haskell.org/haddock/ticket/80 Opened 7 years ago... is it really that difficult compared to the returns?
I think empiricism can show patterns on large scale. The problem with the static typing argument is that you have a predetermined conclusion that you're trying to fit evidence into. The empirical approach would be to look at software as a black box and look at the defects reported by the users. If you can show that on a large scale a certain language produces less overall defects then you can start trying to account for that, not the other way around.
Defects in code are not a social issue. You can do a statistical analysis on the numbers and see whether software written in certain languages has less defects on average. This is precisely what the study does by the way. Only once you can illustrate that there are statistically less defects should you start trying to account for that. 
It doesn't magically write the proof for you, it assists you with the proof, but you're the one who has to prove to the compiler that your code is self-consistent. I've never used Python myself, but I would imagine that its imperative/OO nature would be much more problematic than its typing discipline.
It appears to be a common argument that Haskell allows you to work on very large software projects where dynamic typing wouldn't be feasible. People making this argument appear to see the need to structure software that way.
Pretty cool. thanks for sharing :)
&gt; The empirical approach would be to look at software as a black box and look at the defects reported by the users. If you can show that on a large scale a certain language produces less overall defects then you can start trying to account for that, not the other way around. Agreed, I'm just very skeptical that any convincing study of this topic is ever going to be done.
&gt; This is precisely what the study does by the way The abstract of [the study](http://macbeth.cs.ucdavis.edu/lang_study.pdf) ends with &gt; We hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages so colour me thoroughly unconvinced by this one!
I think with more and more open source software out there hosted places like GitHub makes such studies quite possible. The study I linked isn't perfect by any means, but it is definitely a step in the right direction. It confirms the intuition that immutability helps reduce defects. It also suggest that the functional approach is in fact better than the imperative. The languages with least defects are functional ones. However, what it doesn't show is that static typing makes a significant impact if both languages are functional and backed by immutable data.
Any time you have to create a record or describe relationships, you're proving something to the compiler. A simple example is that you're not allowed to simply carry around a map of mixed types and add and remove keys in it, or change the types of keys on the fly. 
I would argue that Y is bad under all conditions, it's just it takes you longer to realize it when using X. The problem with monolithic software isn't restricted to simply being able to track types in your program. It's less flexible, it's more difficult to reason about, it has more coupling, the code is not reusable, and so on. These are the main reasons why you should avoid X.
Ah, good point. I was considering writing this without `ContT`, but I figured that I would just end up implementing it. I've done enough mindbending for today but if someone feels like writing up your variant it might be interesting to compare. I guess it becomes simpler because you don't need a general `callCC`.
I like the analogy psince Alcoholics Anonymous has been [shown to have absolutely no impact on rates alcoholism](https://en.wikipedia.org/wiki/Effectiveness_of_Alcoholics_Anonymous). ;)
Did you actually read the study?
Links to threads on this topic: https://github.com/kazu-yamamoto/ghc-mod/issues/498 (issue) https://github.com/kazu-yamamoto/ghc-mod/pull/508 (PR) And the branch that work on this feature has been made on: https://github.com/ajnsit/ghc-mod/tree/stack-support I'm still curious how this may work; for instance how to deal with the fact that stack allows different version of GHC and uses implicit sandboxes? Another effort in this direction is [stack-ide](https://github.com/commercialhaskell/stack-ide) (based on ide-backend).
Once again, that the whole point of looking at a large number of projects. The only things that's interesting is the stats here. The differences such as programmer skill or personality average out on the large scale.
&gt; The differences such as programmer skill or personality average out on the large scale. Highly skeptical of that! Does programmer skill and personality average out over a large scale comparison of Coq vs BASIC?
&gt; The purpose of the study is not to come up with an explanation, it's doing the necessary first step of identifying whether there's something to be explained in the first place. Disagree. There's *obviously* something to be explained: two intelligent developers have different opinions. The only thing a *study* could do is give an indication of who is right. But I still think it's highly unlikely you'll be able to draw meaningful conclusions empirically.
It does however when we're talking about lots of open source projects on GitHub. If people actually wrote software in mass using Coq and BASIC, then we'd also see be able to compare defects between the two. First thing you have to identify is whether a pattern exists. One would expect software written in Coq to be of higher quality than that written in BASIC. The reasons for that could be numerous, could be skill, quality of language, personality, whatever. You can start digging into that once you've established that Coq indeed produces less defects, not before.
As I've already explained in the other thread, I disagree that this is a reason to discard the study or that this is of much interest in the grand scheme of things.
I think even the notion of "comparing defects" is very fraught. Anyway, it's been an interesting and fun debate. Thanks for taking the time! 
I'm looking at it from the user perspective. If I use a project from GitHub and I find a bug that affects me I report it. I think this is the most meaningful metric. And was great chatting, thanks for taking the time as well. :)
I think that's the crux of the issue. We simply don't know at this point, there are lots of smart and experienced people on both sides of the fence. My view at the moment is that it's safe to treat typing as personal preference. Some people clearly find it helpful while others don't. Whether it translates into other tangible benefits is still up for debate in my opinion.
https://www.reddit.com/r/haskell/comments/3flnyc/summer_haskell_course/
Seems to work!. the memory footprint is very low and no CPU load. Just to learn, What was the problem?
After I do the release this weekend, sure.
Reading your comment here and things you wrote elsewhere, it looks live you've picked the following X and Y. X: easier refactoring Y: monolithic code Is that right? When I wrote the script above I was thinking of the following values: X: monolithic code Y: painful refactoring I think an argument that ease of refactoring leads to bad code needs to address the apparent circularity shown in the script. When refactoring is easier, the line between problematic code and nonproblematic code moves. 
People tend to push their tools until they start having trouble maintaining the code using them. Given that static typing allows you to grow the code base past the point you could without it, the code bases tend to grow larger. However, the point you appear to be missing is that refactoring is only one of many problems with large code bases. As I already pointed out, there are issues with coupling, code reuse and so on. So, while your tool allows you to address one problem, it doesn't address the rest of the problems. There's absolutely nothing circular about anything I said here.
Here's a place where we differ, and where it's bad news for your "principled approach" - your principle that _every_ user should feel the pain of inaccurate version bounds versus my view which is that the package author ought to feel that pain, and there's _no need_ for everyone else to have a bad time. None at all. The reason I think that's bad news for what you want, is that making everyone feel the pain one author introduced is not likely in the long run to end up with popularity. I don't see it as a competition like you do, but spreading the pain as far as possible isn't advisable if you want to be the most popular installation tool. With stackage, and stack's default behaviour, if someone, _anyone_ (it doesn't have to be the package author) finds any version of that package that's co-compilable with the rest of stackage, they can make a request and have it added. Anyone else who wants to can then benefit from the existence and availability of the solution. At the same time, it would be helpful if they notify the package author of their inaccurate version bounds so that cabal install users can benefit too. Had it occurred to you that upper bounds that are too tight might actually be a source of _more_ failures than missing or too-slack ones? Is be surprised if it weren't the case that the reason you don't hear advice from the stackage team to tighten upper bounds is that it's much rarely a problem for co-compilation, whereas too-tight bounds are regularly a problem. They co-compile much more widely than most people ever do, so have a lot more experience of where the issues really lie.
Thank you for letting me know.
what do you find less easy about Haskell documentation? maybe people can help fix it :)
Hardcore Clojureist here. I always get a bit of Haskell envy, when I try to write a parser. IMO applicative parsers are really the thing, that ML is made for and Haskell's lazy evaluation as well as static typing help quite a bit with formulating recursive parsers in terms of applicatives. I'm now trying to apply core.logic (a minikanren) to the problem and it seems that this might be an adequate replacement for haskell's type inferrence + mutually recursive lazy top-level. Having it built-in, as well as linked up with type-classes, like in haskell, sure is nice.
that's a cool point, but I'd rather that distinction be blurred by a dependently type programming language, not an untyped one :/ 
oh woah that's a great point about string. Learning Haskell, I thought it was cool that a string was a list of chars. it made so much sense. but being able to re-implement it as text would be cooler even. as for the solution, what would you recommend? maybe using ListLike and MonadIO rather than [] and IO? 
You may be surprised, but I actually agree with your comment about the popularity aspect. And I can subscribe to leave users out of the pain-circle if package-authors can be made to feel that pain (but authors may counter that since users feel no pain anymore, there's no need for them to feel pain either...) The dual story in Hackage to your Stackage crowd-source example would be: If *anyone* finds any broken install plan, they can make a request a request to the Hackage Trustees, and once it's fixed everyone benefits from that specific version bounds fixup forever. I bet the only reason we don't hear from the Stackage team to tighten upper bounds is that they spend most time reminding people to relax their upper bounds until everyone has synchronised... Stackage doesn't benefit from upper bounds, Stackage rather has to continually fight against them and suffer from coordination overhead. But outside of Stackage upper bounds are essential for `cabal` to find working install-plans. Most compile errors you encounter with `cabal install` can be explained by some package having missing bounds, and almost never by too restrictive bounds. Actually, this seems to be the underlying cause of the conflict of interest: With Stackage too tight bounds cause serious problems (as Stackage itself is the tightest version bound -- i.e. to one single version -- you can have!), whereas without Stackage missing bounds cause more problems as the cabal solver relies on accurate build-dep specs to find proper solutions. Even more so with the PVP: Stackage doesn't need the PVP at all. Without Stackage however, the PVP is essential as otherwise upper bounds couldn't be predicted so easily.
They follow the same principle. Generating a rule for each possible input string is horribly inefficient, though. It's quite an old idea, btw. http://web.cs.wpi.edu/~jshutt/adapt/2level.html
That's not my blog post, it's Yorgey's. I'm Chris Allen, the one working on the Haskell book for beginners with my coauthor Julie – - http://bitemyapp.com/ - https://github.com/bitemyapp/learnhaskell - http://haskellbook.com/
&gt; we should wait another 6 months till `stack` can be considered ready for inexperienced newcomers. Actually, my experience working with people has been quite the opposite: inexperienced newcomers have no problem with the set of commands stack provides, it's people coming in with assumptions about how it should work based on experience with other tools (usually cabal) that get tripped up. That previous thread you linked to is a prime example: your complaints seemed to be: 1. There's no sandbox command. But someone without cabal experience would never have assumed that a sandbox command was necessary to get a safe build (and, in fact, that's the case with stack). 2. setup installed GHC (which is the documented behavior of the command), which I've yet to hear people without cabal experience be surprised by. Many common tools in the Ruby and Python world will install the relevant versions of the interpreter for the given project. 3. As is demonstrated in this very thread: cabal teaches you that `install`ing is necessary to get a library, when that's not the commonly accepted definition of "installing." tl;dr: stack's ready to use right now, just try to drop your cabal-based assumptions and you'll be fine.
Thanks for the link to the remote monad paper. It was a very interesting read!
&gt;Since the amount of punishment faced by a programmer (with proper planning) is under their own control, we can expect them to aim for the same level of punishment with or without the tools. However, you don't see anybody using a dynamic language advocating for larger code bases. People using these languages are much more keenly aware of how punishing such projects can become. For example, Clojure community has a very strong opinion that libraries should be kept simple and single purpose. By contrast, Java community leans towards the monolithic side of things with omnibus libraries and byzantine frameworks. My key point is that punishment can take many aspects. Some of those you can be aware up front and others not. Since you can't consider every angle, you tend to write project to your level of comfort. My experience is however that what you're comfortable with while you're in a mindset of a particular problem and when you come back to it 6 month later is radically different. The only solution that I found to work for me is to keep components small and single purpose.
I'm not talking about just refactoring. Refactoring is just something you have to do when your code gets too cumbersome to work with. Tooling is what facilitates writing large code that necessitate refactoring bases in the the first place. For example, Java projects could and I would argue would be written radically different if not for the tooling that allows you to navigate them. &gt;Does this accurately reflect your position or not? Sure, I think that's a reasonable summary of my position. 
In that case I don't think it's circular after all. We agree that in the presence of tooling, the line between managable code and unmanagable code can be obscured and extra care is required to not cross over it. So we end up with the following question: if the level of pain ends up in the same place (provided you take the proper care), what benefit has come out of the tools? And does it make up for the extra care required to not accidentally cross the line? I think that in this scenario the tools have reduced the amount of upfront discipline required to end up in the same place, and I think that this is not a trivial benefit. 
I agree that tooling can obviously provide a great benefit, but the benefits don't come for free. I don't find that the tooling simply allows you to end up in the same place with less upfront discipline. What I often see is that people end up in a worse place than they would've been otherwise by not understanding how to apply the tooling appropriately. Hence why I think the argument that you're able to maintain a larger code base thanks to tooling is misguided. You should strive to keep the codebase small, and the tooling will still be of benefit there. However, if you think that additional tooling is a reason to start growing the codebase then you may be using it as a crutch.
&gt;If the tooling mitigates your troubles to below your comfort level, it makes perfect sense to adopt more relaxed practices that bring your troubles back up to the previous level. Worse is better and all that. That's not what I'm saying though. My point is that if not applied correctly, tooling could make the situation worse than it would've been otherwise. In other words, the troubles can end up being far greater than the previous level.
The choice of strictness by default in Idris was *not* the result of any performance or memory issues in Haskell. Some languages are non-strict by default, some are strict. Traditionally most are strict, for various historical reasons, and Edwin chose that route when designing Idris.
No. I mean exactly the original proposal, except without making `impossible` a keyword. The token `impossible` would not have any special meaning outside of a pattern expression. Inside a pattern expression, `impossible` would always end the pattern expression, so your example does not work.
RWH is deprecated :/
&gt; You may be surprised, but I actually agree with your comment about the popularity aspect. And I can subscribe to leave users out of the pain-circle if package-authors can be made to feel that pain (but authors may counter that since users feel no pain anymore, there's no need for them to feel pain either...) I am surprised, indeed. :) Should cabal install auto-email package authors every time their package errors whilst installing against a dependency allowed by their bounds, then? This doesn't take my cabal-install pain away, but you certainly pass the pain on to the package author, who may be prompted to fix things instantly rather than just let it sit around broken for other folks. It would reduce problems on one side of the dividing line in the article you linked to. &gt; The dual story in Hackage to your Stackage crowd-source example would be: If anyone finds any broken install plan, they can make a request a request to the Hackage Trustees, and once it's fixed everyone benefits from that specific version bounds fixup forever. Whilst that's a nice idea, it's rarely apparent what's caused the problem, and there might not even be an answer to that question - it's possible to go from one place to another the long way round but end up at a different level. Similarly it's possible for someone to be happily installing packages and solving for consistency all the way, and then end up needing something at a different version number. This is a problem that can happen with all version bounds being accurate, and is no-one's fault particularly, so I'm not sure who should be pinged to fix it. I've been hoping that the backpack or nix-type stuff they're planning on introducing will remove a lot of that kind of problem, but I'm not convinced that there's a solution you can just upload to hackage. (An improvement I'm hoping for from cabal is for it to have a middle way between global and sandbox, with packages by default being globally available without being globally compulsory at that version.) There are things like if you use package A on its own, you need 2&lt;B&lt;7, and if you use C on its own you need 4&lt;B&lt;9, but if you use A and C together, then because of packages D and E you actually need 5&lt;B&lt;6. The more packages you include co-compilation bounds checks on, the closer you get to having (you've guessed it) just one version for each package, and in some cases none! My point is that the smaller the patch of hackage you have installed the less likely you are to have co-installation problems - it's like hackage is flat on the small scale but sharply hilly when you zoom out. &gt; I bet the only reason we don't hear from the Stackage team to tighten upper bounds is that they spend most time reminding people to relax their upper bounds until everyone has synchronised... Stackage doesn't benefit from upper bounds, Stackage rather has to continually fight against them and suffer from coordination overhead. Stackage benefits from accurate, up-to-date upper bounds because it saves time. There has to be a place to document what not to do, or you keep trying the same inconsistencies again and again. That's the version bounds. If a package author edited their upper bounds as recently as their dependencies updated, or documented the breakage point when there's an evidence-based upper bound, then there's no need to doubt them. Sadly since so many upper bounds are out-of-date, there's a lot of trial and error with the upper bound. It's my belief that an up-to-date, evidence-based upper bound is much more use than its absence to the stackage team, but I don't think either of us contribute to getting stackage builds working, so there's a little bit of deduction rather than knowledge going on. &gt; But outside of Stackage upper bounds are essential for cabal to find working install-plans. Yes. Cabal will fail to find working install plans unless the bounds are accurate and up-to-date. (If they're out of date, cabal install will erroneously reject viable install-plans, and if they're absent or too slack and there's a newer package that is incompatible, then it will erroneously accept a non-viable install plan.) I think we agree that accurate, up-to-date upper bounds are good. &gt; Most compile errors you encounter with cabal install can be explained by some package having missing bounds, and almost never by too restrictive bounds. Nope. If a solution exists, it might not involve the versions I have - it might involve older versions than I have installed, for example. Anyway, the point is that too-tight means unnecessary failure, and too-slack means unforseen compilation failure that could have been prevented before downloading. I hate both of these, and we agree, I believe, that inaccurate bounds are a problem for users. &gt; Even more so with the PVP: Stackage doesn't need the PVP at all. Just because stackage solves the dependency problem once a night and distributes the solution to all users, doesn't mean they don't need version bounds to discard incompatible sets. The space of possible solutions, together with the length of time to reach failure while installing (when you could just compare two numbers with bounds) means it would take much, much longer to find the solution. Where do you document the fact that these are incompatible to stop you trying that again tomorrow? In the dependency version bounds. It's the users of stackage that don't interact directly with the PVP. Stackage curators depend on it. &gt; Without Stackage however, the PVP is essential as otherwise upper bounds couldn't be predicted so easily. Yup. Nor lower bounds. There's a part of me that thinks we're starting to find more common ground with each other. It cheers me up. 
Yes...Can I understand them as a method of mapping different types? 
There's several options, depending on what you're after: * [Ivory](http://ivorylang.org/ivory-introduction.html) which is a DSL for writing safe embedded code in Haskell, producing C with stronger guarantees than C usually gives you. It is being used on the SMACCMPilot project which is an autopilot for small UAVs * [Tower](http://ivorylang.org/tower-overview.html) is a language on top of Ivory which allows you to compose Ivory programs into concurrent, real-time systems. * [Atom](https://github.com/tomahawkins/atom) is a DSL for writing hard real-time systems which gives compile time scheduling and atomicity guarantees, avoiding the need for an RTOS - this is used at Eaton for automotive control systems. * [Copilot](http://leepike.github.io/Copilot/) is a stream based eDSL for generating real-time C code with guaranteed constant time and constant space operations. It's build on top of Atom. * [Kansas Lava](http://ku-fpg.github.io/software/kansas-lava/) is a DSL which generates VHDL for use on FPGAs. I've had a bit of experience with it, and it was quite nice. * Others have already mentioned CLaSH, which allows you to create VHDL/Verilog/SystemVerilog for FPGAs, and HalVM, which lets you run Haskell programs on bare metal or inside Xen, but couldn't really be called embedded imo, since you still require all the resources of a full computer. To answer your actual question, I think the answer is, and will probably always be, no. Haskell isn't particularly well suited to embedded programming - having a good handle on resources is critical in embedded systems where memory constraints are usually your biggest limiting factor. Haskell pretty much requires a garbage collector unless written extremely carefully, and the compiler plays nicely.
&gt;Did you know that `ContT` is not really anything to do with this? [...] So yes, it can be cool and mindbending to express things in terms of `ContT`, but I've never seen much point. `ContT` is an extremely weird place! I haven't pondered deeply about this, but the gist of the issue seems to be: the flip side of the continuation monad being [the mother of all monads](http://blog.sigfpe.com/2008/12/mother-of-all-monads.html) is that it has no interesting meaning of its own.
I am still not sure. I did some tests and it seems like the CPU usage is quite high when Gtk2Hs waits for events using the default main loop `mainGui` and an occasional event is scheduled (even just sending on timer event a second that did nothing was using 5% CPU). The main loop I wrote for Leksah to use when it was using the multithreaded RTS seemed to have a similar problem. I switched Leksah back to the single threaded main loop and the problem seems to go away. This means Leksah polls for Gtk3 events (instead of waiting for them). The polling frequency is dynamic and drops quite low when no events are found.
hm, typeclasses are close to what people in Java call interfaces. they're at the very basic level a way of defining methods that may be overloaded for different types. So for the `Monad` typeclass, every type that is a `Monad` gets to define its own `return` and `(&gt;&gt;=)` however way it likes. Then when you use `(&gt;&gt;=)` in code, the compiler figures out what type you're using it on and looks up the definition that that type gave for `(&gt;&gt;=)` So every type gets to define its own `(&gt;&gt;=)`. `Maybe` defines its own, lists define their own, `IO` defines its own, etc., and when you use `(&gt;&gt;=)`, the compiler just uses the `(&gt;&gt;=)` defined for that type. `Comonad` is also a typeclass, so every type gets to define its own `(=&gt;&gt;)`. So when you use `(=&gt;&gt;)` with a type, the compiler looks up the `(=&gt;&gt;)` that is defined for that type and uses that definition. The key takeaway is, for practical coding, to not really focus on what `(&gt;&gt;=)` and `(&lt;&lt;=)` "mean" in general, but rather just look at how the types you're interested in define them and look at that. 
Everybody always comes back in the end when they go through their first big refactor
&gt; you tend to care more about what it does (or can be done to it). This statement confuses me. I'm not a programmer, but a mathematician, and most of my time understanding math is spent "type checking"---whether it's something actually in type theory where types are precisely specified, or something in set theory or category theory, where the "type" is often implicit. Usually, once I understand what sort of thing is being operated on, and what sort of things I can expect to get out, the actual description of what it does is fairly straightforward. I.e., looking at types usually gives you almost all the information you need. E.g., any haskell programmer will know the obvious function of type `(a -&gt; b) -&gt; [a] -&gt; [b]`, *just* from its type.
Dijkstra in 1995: "[Why American Computing Science seems incurable](https://www.cs.utexas.edu/~EWD/transcriptions/EWD12xx/EWD1209.html)" Also, Dijkstra "[[o]n the cruelty of really teaching computing science](https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html)". i also seem to remember that Dijkstra somewhere notes the significance of talking about 'comput*er* science' rather than 'comput*ing* science' - a focus on machines rather than mathematics - but i currently can't find it ....
The second link you've posted is the same as the first.
Gah. Thanks, fixed!
&gt; theoretical CS that's often neglected in the US Otherwise known as "theoretical C.S." Let's see... I took courses titled: - Foundations of C.S. I/II - Computer Ethics - Assembly Language - Data Structures - Digital Logic - Software Engineering - Computer Architecture - Object-Oriented Design and Implementation and electives titled: - Embedded Computer Systems - Program Translators - Operating Systems - Parallel Programming - Computer Systems Security "Foundations" was basically intro to C++ and OOD/I was the same for Java. There was some theory in the architecture class, but it was *not* the sort of theory we're talking about here. You'd think "Data Structures" would involve some theory, but no, it was mostly just implementing things like linked lists, trees, and quicksort and *trying* to teach recursion. Now, to be fair, there was some theory in the electives, but Program Translators (i.e., "Compilers") was the only one that touched on anything relevant to Haskell. Honestly, I think I learned more Computer Science doing the Mathematics half of my degree than the actual Computer Science half. Thank God for the Internet, or I wouldn't even know *about* half of the things I know about C.S.
&gt; It's obviously not necessary to dive deep into Category Theory to work with Haskell, but somehow everyone who wants to be a part of the crowd is doing it. I think this is a good thing and also something to be mindful and cautious of. It's truly wonderful that the Haskell community is inspiring lots of people to learn more about foundational math and algebraic topics that will hopefully improve the state of software engineering in the long run. The thing to be mindful of is that this abstract algebra stuff is actually difficult to teach and learn and too much constant babbling about it might make Haskell seem obscurantist and unwelcoming. Or to put it in a more optimistic way, I think there is room for more teaching material that introduces algebraic concepts in a gentle way and also clarifies how much of it is actually necessary to grok. There are enough tricky things to practice in basic Haskell programming, way before there's any *real* need to get into understanding the mathematical abstractions *underlying* the basic data types. I wish we would talk about concepts such as monads in a way that makes it clear that beginners do not immediately need to develop mathematical intuition about the abstractions *in themselves*. Because that's super hard. Category theory and abstract algebra don't just randomly happen to be where Haskell gets its most abstract concepts, it's because they are the most abstract ways of studying mathematical objects and functions... Those fields were developed as a way of *abstracting* common concerns, and they don't really make sense to think about unless you're either (1) very interested in abstract math for its own sake, or (2) are already familiar with some of the concrete topics that these topics generalize. Trying to learn group theory before you learn arithmetic is backwards. If you had no algebraic background and were just learning a new programming language that happened to use `Group` as a superclass, such that `(+) :: Group g =&gt; g -&gt; g -&gt; g`, you would probably want someone to inform you that you don't need to study abstract algebra in depth just to add some numbers. Similarly, using `pipes` to do some streaming I/O does not, or should not, require a thorough study of categories and monads *qua* mathematical abstractions. I think /u/Tekmo often does a great job of explaining things in concrete terms, but still the concepts come up in the documentation and can be intimidating. So are there ways in which we can combat "abstraction vertigo" for Haskell newcomers? Can we encourage people to keep on programming and not feel inferior because they happen not to have deep intuition for extremely abstract mathematics?
Lets look at some ruby: x = if rand(10) == 1 "hello" else 3 end foo(x) What type is `x` at the point when `foo(x)` is called? Well it is either String or Fixnum. If you are figuring out if you can call `foo` on `x`, you don't get far by thinking about what type `x` is. But if you think about what `x` can do, well that's the intersection of `"hello".methods` and `3.methods` (which was 82 methods when I checked just now; 16 did not belong to Object). If `foo` only requires that the parameter has the methods that are in that intersection, it will be fine. There are other examples of things that are common in dynamic languages that make a thing's type less practical to think about than what that thing can currently do.
This is exactly what I needed (just tried and failed to build an arm cross compiler to put stuff on my rpi). Thanks! 
Somewhat not up to date. But it is still very concise, and you only needed a small addendum of the differences to make it useful again. 
Concrete examples are very useful IMO for learning an abstract concept. In particular, it should be *repeated* several times preferably before the actual concept is introduced, so the user can get used to it. It should also be easy and obvious for the user to extend the examples and thereby learn hands-on. One thing that I find is a bit lacking in Haskell-land is the *operational* perspective of things. Everyone loves to talk about things in an abstract, *denotational* way about what some expression *means*, but a lot of the times I'm very interested in the *implementation* of things and would understand concepts better had I knew how things worked under the rug (so it no longer feels "magical). Things like: "How did you implement this Monad?" or "How did you implement this Arrow?". Most often there is no explanation of such except by digging into the code.
Yeah it's exactly extend (similar to duplicate)
x is an Int+String, and Ruby leaves the injection implicit. I'm not only trying to figure out if I can call foo on x; I'm figuring out *what foo does* by understanding what sorts of objects foo accepts as inputs, and what sorts of objects foo outputs. After that, I can look at what foo does to x. And often, I can find bugs by simply checking whether foo operates meaningfully on x. In both programming and math, you're taking things and performing operations on them to get other things. At some point in understanding what's written, you will need to see if these operations *make sense*, which requires you to understand which inputs makes sense and which outputs makes sense for which input. I'm not saying you should be working in a strongly typed language or that strongly typed languages are necessarily better; I'm saying that implicit in properly documenting something is giving the same information that types give. Adding an extra line with a type judgement just makes that explicit.
 Monad --&gt; Bindable Functor --&gt; Mappable
To be fair "bindable" doesn't mean anything to the uninitiated either.
You can see a more or less biased qualitative comparison of Clojure vs Haskell here: http://hammerprinciple.com/therighttool/items/haskell/clojure
If I call a monad a Monad, I give someone else -- and even myself! -- access to 70 years of literature. If I call it a Bindable, I'm cutting them off and limiting them to my understanding of the topic. I'm making the few who know what it really is wonder if I'm talking about the thing they know or if I'm making up my own laws. Moreover, once I understand what a monad is to a greater level of generality, the ones I learn in Haskell using that class inform my understanding of whole other areas of mathematics, or other categories, what more complicated indexed versions of them look like, etc. I use the standard names for things precisely because I'm not conceited enough to believe I know enough about the topic to say all the things that there are to say about it that are true. It enables me to enable people to find out new things about stuff I wrote and tell me. Now, there is a price to this. If you use a piece of jargon you need to be willing to back that up with explanations. Otherwise jargon is just a tool to lock people out of your community. If we want to stand on the shoulders of giants, using consistent vocabulary lets us wear in trails for others to follow us on our climb, and more importantly helps us meet back up after our paths diverge.
The way you program with monads tends to lend it self to long chains of computations. Chains of binds like foo &gt;&gt;= \x -&gt; bar &gt;&gt;= \y -&gt; ... notationally work very nicely for this. Next, there is an observation that `(&gt;&gt;=)` is a bit of a weird thing from a category theory perspective, if it were flipped then we could view `bind :: Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b` as lifting a Kleisli arrow to one in the base category. But this doesn't make nice chains or read well for most users. In this case the operator wins over the `bind` combinator in terms of usability. In the `Comonad` case it is less clear. You have access to the entire comonad in the action you perform. This usually requires a case analysis or some more complicated mess of combinators being applied to the argument you're given to work with it. With all this in mind, comonadic code tends to be made out of smaller definitions so the combinator that chains things, doesn't really _need_ to be an operator. This reduces the pressure to make a nice operator for comonadic syntax. I kept `(&lt;&lt;=)` from Dave Menendez's original version of `category-extras` as a form of `extend` but we actively encourage folks to use `extend` instead. The notation is what Dave picked originally, and was picked to avoid conflicts with `Control.Monad` while simultaneously trying to look similar.
Try using stack, it is much better than cabal, but it is a new thing and the tooling around it is not yet in place. For instance ghc-mod does not work with stack out of the box. Don't sweat haskell-mode, it is not anything like eclipse for java dev.
A major update to support GHC 7.10.2 and all the cabal changes was released on friday. I'm pretty sure there will have to be some tweaks to sort out the teething troubles with ide integration over the next few days.
Yeah the 5.3.0.0 release right? So you think I should just be patient and wait for bug fix patches?
Yes, hopefully this will all be resolved by the end of the coming week
Best post/nerdiest joke ever on reddit. Everybody go home, we're done now.
I'm considering releasing the tokenizer portion of the parser, which is simply the part that turns the raw text stream into a list of EDI Segments. This is how all EDI encoding works and doesn't really fall under the X12 copyright. It's when you start looking into how the the ordering of the Segments is used to construct a hierarchical document (834, etc.) that you start relying on the standards documents and possibly infringing.
https://hub.docker.com/r/plumlife/arm-ghc/ It definitely works, I'm using it to build a Haskell program that connects a PSoC over serial to control LEDS on our device :D
https://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu 
Ed Kmett once posted a huge list of things, many of which I agree with, that should be reposted here, but I can't find the link. I think Scala made several design decisions, usually to maintain Java compatibility, that are ultimately mistakes IMO, ranging from type system (Inclusion of null for "compatibility" is bollocks -- swift did it right. Subtyping is more trouble than it's worth, particularly when combined with the excessive generality of some of Scala's libraries), to the dynamic semantics (absence of good tail call optimisation makes it very difficult to encode many abstractions.). It's possible to encode the program I want to write in Scala, but this encoding is too difficult and cumbersome to actually be useful, and I often end up having to write imperative code just to get it to perform well. Lastly the Scala community is one of the most unpleasant I've seen in an FP community. A lot of uninformed people spreading misinformation, and hostile people more concerned with appearing intelligent than actually helping others. Edit: Also implicit coercions exist, which are pretty horrible. They're a sledgehammer solution to the problem of extending an existing library.
Disclaimer: I've never programmed in Scala. * Depending on your point of view, this is either a nitpick or a dealbreaker - [Scala's type system is Turing-complete.](https://michid.wordpress.com/2010/01/29/scala-type-level-encoding-of-the-ski-calculus/) * The syntax for sum types is [unnecessarily verbose](http://www.scala-lang.org/old/node/107), and it makes me question whether they're open or closed.
Going to demur here and agree with /u/edwardkmett and /u/chrisdoner. We call things what they are in [our book Haskell Programming](http://haskellbook.com/) – no exceptions. You can learn the word and stash the deeper investigations for later.
AWeSome :)
* Many features are tacked on without much thought to how they interact with other features * The compiler is full of bugs * Type inference and error messages are pretty bad * You pay a performance penalty for programming functionally * The syntax (for both types and terms) is cumbersome * Object-oriented idioms conflict with functional idioms Scala is particularly frustrating for Haskell programmers because it's our uncanny valley: you can usually get 90% of the way to the equivalent Haskell idioms and then you will inevitably hit some dealbreaking issue at the last moment that forces you to scrap everything.
Great! I'm downloading it now. Thank god for fast internet :p
what project? I have no project yet. All I want is to open a haskell file in emacs and have ghc-mod working.
Source? 
&gt; because their CS education was deficient. More like non existent :) 99% of programmers have no formal education in programming whatsoever, like ex accountants or even taxi drivers :) 
When I was working with AWS at `$OLDJOB`, /u/brnhy was very receptive and responsive when I filed issues for parts of amazonka that hadn't gotten much attention and didn't yet work (generated code goes a long way but there's often a last mile problem to get it to actually work). Definitely recommend these libraries.
Yeah I'm going to re-build it and fix that 6Gig image size. I'm sorry about that.
[It's beautiful :wipes away tear:](http://i.imgur.com/jak38Hs.png)
You should be able to "share" your project directory and call `arm-plum-linux-gnueabi-cabal install` on it after performing a cabal update. I also want to get rid of our custom vendor tuple but I spent a good 18 hours solid getting this thing to work, so that was first priority.
Yup, I'm just about to try a cabal project. I'm sharing my project dir the normal way `-v`. Vendor tuple = no problem for me. The mere fact this dockerised cross compiler exists in any form is awesome. I hate compiling GHC, let alone compiling it for cross compilers :p
Edit: I'm dumb, I specified the wrong bounds for `base` in my cabal file. It works fine :) --- ~~Ok, I'm having a bit of trouble with cabal. [Here is a gist with my code](https://gist.github.com/CRogers/ee2d6906501d05840561). I did:~~ $ arm-plum-linux-gnueabi-cabal update Downloading the latest package list from hackage.haskell.org $ arm-plum-linux-gnueabi-cabal build Building arm-0.1.0.0... Preprocessing executable 'arm' for arm-0.1.0.0... on the commandline: Warning: -package-conf is deprecated: Use -package-db instead &lt;command line&gt;: cannot satisfy -package-id base-4.5.0.0-40b99d05fae6a4eea95ea69e6e0c9702 (use -v for more information) ~~But as you can see it errors trying to find `base-4.5.0.0` :/~~ ~~[Pastebin of verbose build](http://pastebin.com/RF41pZ8N)~~
I've not written much Scala. Given that it is a Java-interoperable language on the JVM with a rather big community is a big reason to consider it. My main problem with Scala is that it feel haphazardly conceived/ not well designed. A bit like JavaScript feels. It would --IMHO-- have been great if Scala would have take off a few years later and in those year had a (few) overhaul(s). Scala is multi paradigm (OOP and FP), while Haskell pretty much enforces FP. Through this I have the most issues with Scala; it tries to do too much, resulting in: too many keywords and hard to understand (or opaque) types.
I tried with ghc-debug but it's not very helpful. Take a look at it [here](http://jpst.it/A-PW)
&gt;(generated code goes a long way but there's often a last mile problem to get it to actually work). I'm going to be using this library as an example next time somebody asks me if code generation or macros are worthwhile :)
In my experience, the Scala maintainers are nice.
Yep. The idea behind it is to have a single implementation of `map` function for different collections like `List` (single-linked list), `Vector`(array-list) without having to "lose" the concrete type in the process.
Thanks. I still have a little confusion. But I will remember your advise "just look at how the types you're interested in define them and look at that." which is helpful enough for me. Thank you very much.
[This](http://www.quora.com/Reviews-of-Functional-Programming-in-Scala-2014-book?share=1)?
As said above, it does not work with stack, at least not 7.10.2 but 7.8.4 is working but you need to "stack exec emacs -- blablabla.hs. 
He's probably referring to [this comment of Edward's](https://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu) which was linked to elsewhere in this thread.
I have only very limited experience with Scala, but my big gripe was that documentation about language features was basically non-existent. I got the impression that everyone who actually knows Scala learned about it by hacking on the compiler. 
Can we not with these questions, please. Discussions about the pros/cons of various languages are tedious and boring at best, and end in tears at worst. To the credit of all, the discussions have been pretty good thus far. But nonetheless...
Ask Paul Phillips: https://www.youtube.com/watch?v=TS1lpKBMkgg https://www.youtube.com/watch?v=uiJycy6dFSQ Both talks are VERY good and a must-see for everyone taking part in scala-discussions as this man knows what he talks about.
I came to Haskell because it has a reputation as an interesting way to make hardware. Well Scala has gone much further https://chisel.eecs.berkeley.edu with it. Okay, that's not really a criticism of Scala. But when I read the Chisel source code its kind of heart-breaking. The same approach in Haskell would have been far cleaner in my opinion. Just no one ever did a Haskell HDL DSL that compiles to C++ for simulation. But they should.
I test all my projects using Travis continuous integration with many versions of GHC. As a result, all my packages are tested with the most recent version of all dependencies on the day I release them - entirely regardless of what I have installed on my local machine. I strongly encourage everyone to test all their packages with Travis. If everyone did, then retroactive inference would work. Note that if your package won't build with the latest versions after applying your Cabal constraints, then it will probably fail to generate Haddock documentation on Hackage as well.
&gt; I strongly encourage everyone to test all their packages with Travis. If everyone did, then retroactive inference would work. Can you elaborate on this more? Because I don't see how you can assume that all packages will always be working with the latest version of all dependencies. There are tons of valid reasons that might not be the case. One is that you could check what version is available at May 20 21:18:00 UTC 2015, see that it's errors-1.4.7, upload at 21:20:00 and be unaware that errors-2.0.0 was uploaded already. Another is that it may be too much work for you to upgrade to errors-2.0.0 right now but you need to release fixes to some significant bugs. Bottom line: if you're testing against things, you need to tell the world what you're testing against. And the way to do that is with lower and upper bounds.
Yeah! Scala guy here and I really like this thread. I respect Haskellers and am interested in their point of view. Just because someone else doesn't like the language or the language doesn't fit their needs doesn't mean it's the wrong language for me and my type of problems! Also I think it's super important to be aware of the shortcomings of your tool of choice. If you blindly always choose the same tool even if it doesn't fit your problem, you're in the Golden Hammer anti-pattern.
&gt; Also I think it's super important to be aware of the shortcomings of your tool of choice. If you blindly always choose the same tool even if it doesn't fit your problem, you're in the Golden Hammer anti-pattern. Agreed, that's why I liked the discussions of Clojure. I learned a bit about a dynamic world that I normally never give consideration to.
I write a package. I test on fresh Travis machines, with updated Hackage databases, which will have the latest versions of all packages. Cabal then finds an install plan, and tests. If my packaged failed to compile or when testing, I would refine the bounds. Therefore, for all my packages, you can guarantee that the search plan found by Cabal using whatever bounds I give (whether that is no bounds, a concrete version, or anything in between) on the date of upload (or really 20 minutes before upload), will generate a working binary that passes all the tests. I imagine for everyone who uses a continuous integration server, particularly those using Travis and Cabal, the same property holds.
Can we has Rust next? :) 
I'm not arguing that using date based version assumptions is the right thing to do - merely that for my packages, it is (to a first approximation) a correct thing to do. I think having everyone test using continuous integration is a very good thing to aim for, even ignoring any impact about correct bounds.
Hackage has mutable metadata for package curators. I personally think that's even worse, since now a particular version can refer to a different set of bounds over time.
&gt; You cannot reliably claim your package won't work with a version that doesn't exist yet, just like you cannot reliably claim that it will. The post addresses this point in a very reasonable way IMO: &gt; When a package specifies version bounds like this [...] it is not saying "my package will not work with errors-1.5 and above". It is actually saying, "I warrant that my package does work with those versions of errors (provided errors complies with the PVP)". So the idea that "&lt; 1.5" is a "preemptive upper bound" is wrong. The package author is not preempting anything. Bounds are simply information.
&gt; Don't version bounds only works for well behaved users, since those who are not well behaved (or those who disagree with your arguments) will either omit bounds or leave them looser than you would like. With version bounds there is a lot more that we can do in this department than with a date-based approach. If you upload a package with no version bounds, we have no idea if you tested it with the most recent version or not. But we can automatically check for missing bounds and warn the user. We can automatically examine a packages exposed API and detect when there should be a major version bump. Since everything is explicit there are lots of possibilities. &gt; I hope that there is universal consensus that some form of continuous integration is a good idea though. Absolutely. We should work towards more CI as well. But that's a much bigger, costlier, and more involved problem. And at the end of the day it's still about making version bounds as correct as possible.
People in other contexts talk about binding variables.
he has some updates about how the new `dotty` compiler uses more functional ideas https://www.youtube.com/watch?v=48js0H6ooBY
Perhaps some libraries like Shapeless and Scalaz, but the language itself is well documented and there are various free books, including one of the Oreilly ones. http://www.scala-lang.org/documentation/
Speaking as someone with lots of Ruby experience I can honestly say that I find stack much more confusing than cabal &amp; sandboxes. In fact I just spent a couple of hours trying to use stack again but ended up removing it not seeing any advantage over cabal &amp; sandboxes. I think the thing that would most benefit stack right now is some good blog posts demonstrating common tasks.
...are you suggesting someting like SafeHaskell at the package metadata level? If so, I propose the term SafeCabal :)
Yeah, that's probably a good analogy.
wow that's the voice of someone at the end of their rope...
'Typeclasses' in Scala are very brittle. Instead of real typeclasses, you instead have an implicit conversion from your type to the implementation of the typeclass. For example, in the scalaz library, there's an implicit conversion from List[A] to FunctorOps[List[A]]. [/u/edwardkmett did a good video about reasoning about real typeclasses vs implicits](https://www.youtube.com/watch?v=hIZxTQP1ifo), but there's a number of annoyances that are Scala specific: 1. Implicit conversions have to be unique within a [baroque set of precedence rules](https://stackoverflow.com/questions/5598085/where-does-scala-look-for-implicits). That's really not a good thing. 2. Implicits only search for the current type. If the value you have is of type Some[A]? You won't find a ToFunctorOps[Some[A]], anywhere; you'll need to explicitly tell Scala that your Some is also an Option. 2. You can easily get false positives for implicit uniqueness using subtyping. For example, in Scalaz, ToMonadOps extends ToFuntorOps and ToTraversableOps *also* extends ToFunctorOps. If you import both scalaz.syntax.traversable._ and scalaz.syntax.monad._, suddenly your calls to map suddenly won't compile. Basically typeclasses always 'just work' for me in Haskell, but break on me on a regular basis in Scala, causing me to spend a few hours here and there figuring out how to fix it. edit: Since no-one else has posted it, Runar gave a good talk a while back about how [FP in Scala is terrible](https://www.youtube.com/watch?v=hzf3hTUKk8U).
I would pay a nickel to see that one. Rust is safe and fast, so complaints would have to be very academic in nature to make any sense.
Dude, time to spring for a better computer.
I started out feeling this way, and after a year or so i started to figure out that it's only partially true. The combination of OOP and higher-kinded types, generics, traits (basically type-class like behavior when combined with implicits), and macros makes the scala type system one of the most complex ever created. You can do real insane things with it (look at the shapeless source code) but the resulting types are not meant to be human readable -- they are assuming you're leaning on type inference in most cases. 
More generally, Haskell lacks a decent distributed analytics ecosystem. There is no Haskell analog of `scalding`, for example. I'd eventually like to contribute such a library in the future
&gt; If your point is that a non-strict language like Haskell is better for recursive algorithms than a strict language using TCO, then i don't think you'll get an argument. But it doesn't even have that. If Scala actually offered full tail-call optimization, then it'd have a much better story. It only does self-tail-call elimination, because those can be trivially turned into loops. It only eliminates tail-calls when your function calls itself in tail position (with a couple of niggling extra restrictions I forget at the moment.) But then what happens for `flatMap`? You call a function passing another as an argument and do stuff in there. This is necessarily not a self-tailcall! You're bouncing back and forth between two functions to deal with a chain of flatMaps. This is why we have to trampoline all the monads in `scalaz`. Otherwise all of the monads, even Identity monad, crash for non-trivial examples. All you have to do is traverse a large enough list and you'd blow the stack. Unfortunately trampolining is _awful_ for performance. I can understand why it makes this trade-off, but the consequences impact the viability of working with monads in Scala for non-trivial examples rather severely.
I've had similar experiences with leiningen and gradle. The JVM startup is a huge hit. Can't SBT run in daemon mode now? I last touched Scala in 2011 (never looked back). EDIT: Perhaps I worded it poorly. Not JVM startup, just a long startup time for projects that *happen* to run on the JVM. I know it's fast people.
The entire clojure vs haskell discussion can be summed up as dynamic vs static. 
`=&gt;&gt;` goes back at least as far as Kieburtz's "Codata and Comonads in Haskell" (1999), which also includes `.&gt;&gt;` and `coeval`. When I was making `category-extras`, I tried to avoid naming things with "co-" because it seemed confusing. That's how we ended up with `extract` and `extend`. Later, when I was working with an attribute grammar-like comonad, I came to prefer `this` and `coextend`, but by then the cat was out of the bag.
It can help to think of `&gt;&gt;=` as a composition of two operations: `fmap` and `join`. Using `fmap`, we turn some value `x :: M A` into `fmap f x :: M (M B)`, and then `join` collapses the nested structure: `join (fmap f x) :: M B`. The monad laws require that `x &gt;&gt;= f` equal `join (fmap f x)`, so you can think of it as a shorthand (and usually an optimization). Similarly, `=&gt;&gt;` or `extend` is a composition of `duplicate` and `fmap`.
But does `Functor` really need to be the name of the class to accomplish that? What if the *documentation* kept the reference to the concept, but the name didn't? Ultimately, labels are not descriptions, so the name of the class is never going to convey its meaning. So what strategy do we pick for naming things? It's a hard question, and I don't rule out your answer, but I think the argument you make here has the objection I made.
I tried to carry on the tradition of `extract` and `extend` because it really helped people come to grips with comonads as a thing in their own right rather than something you have to think of always in relation to some monadic counterpart. `this` is cute. Had I a time machine I'd probably go back and change `extract` to `this`, but now it is probably not worth the change.
Yeah, that was why I avoided "co-". Which is ironic, because the reason I was interested in comonads at the time was because I figured if monads were so useful, surely comonads must be too.
Introducing the second name just gives someone an extra thing to track in their head. I call a thing a `Monoid` because it gives someone a chance to have heard of it before and because they can then go grab any abstract algebra book off the shelf and learn about the concept... all without having looked at the documentation. An alarming number of users never even look at the docs, so shoving it in the documentation doesn't help those folks at all. Monoid isn't "appendable" or "addable" or whatever warm fuzzy concept you want to introduce that covers a small subset of the things that a `Monoid` could be. Calling it one of the latter thing requires the users who _would_ know what is going on to crawl into your head and extract your intuitions about a pre-existing construct and then have the rug torn out from them when you start using "addable" to work with products. Or worse, when you start debating with other developers about whether you should have a `Product` instance for the class when the "intention" of it is to capture notions of adding, when no such intentional baggage is captured by your laws.
That is exactly what grabbed me about them too. It was this great big largely unexplored frontier. =) Then it was all about the existence of comonad transformers to start building up bigger and better comonads and recursion schemes. I got a lot of the transformers wrong on my first go around, but eventually got everything flipped around right. When working with Russell O'Connor on lenses as 'costate comonad coalgebras' -- he wanted a name for the costate comonad that didn't have co in it and so I bounced like 50 different names off him before we settled on `Store`, and I went and renamed the other comonads similarly. It actually took me a long time to actually spot precisely how `Coreader` is dual to `Reader` to get it to warrant the colloquial `Co` we stick in front, so at the time I did so with a good deal of prejudice against the Co-reader, Co-writer, etc. names. Nowadays I use comonads a great deal in my code. They aren't as prevalent as monads as an overall structure for the entire application, but I do tend to use lots of little comonads here and there. Little product-like ones for tracking meta-information I don't want to forget, and bigger ones for working with concepts like Moore machines, for when I want to build resumable computations. This isn't the all encompassing thing that I had hoped for when I started investing in the concept, but I must say it has cleaned up a surprising amount of code.
Type classes don't use implicit conversion. They use implicit parameters. What you're describing sounds like it's probably a value class. 
&gt; Why can't Cabal collect information about which builds succeed and which unit tests pass on users machines, along with versions, and use that "experience" to make better dependency-resolution decisions? Yes, that idea has been discussed before. It would be great to be able to incorporate this kind of information somehow. I have no objection to this idea. But I think that needs to be added on top of a solid foundation of maintainer-supplied upper and lower bounds. We don't want the success of package builds to be contingent on whether lots of other people have built them or not.
&gt; [an upper bound] is not saying "my package will not work with errors-1.5 and above". Except... to cabal-install that's exactly what it's saying. Or at least, that's how cabal-install interprets it. You have to tell it to ignore that information if you want to build it against a newer version. And what about people that actually do want to express that their package does not build against newer versions of a dependency? Isn't that how we generally interpret lower bounds? Why not also upper? It seems that we would be well served by having different fields to express these different concepts. 1) known to work with, and 2) known to not work with. That way build tools would be free to try new versions of things, rather than sometimes just giving up or resorting to unusually old versions.
Did you see the wiki pages https://github.com/commercialhaskell/stack/wiki/Transition-guide and https://github.com/commercialhaskell/stack/wiki/New-to-Haskell? Also, if you have some concrete examples of things that you found confusing, that would be a huge help in figuring out what to document.
OK, question: Why would I want that? Function composition is so bread-and-butter it's not a lot of work to throw in a `fromList`, and it's far less magical I think. In a way the name almost makes me suspicious, kind of like the inverse of the fear non-Haskellers have of `Monad`: `CanBuildFrom` is so suspiciously sort-of-abstract-but-not-concretely-defined it's hard to intuit what behavior it should have anyway!
Sure, I'll try it out some time. It's simply something I've noticed coming from C/Obj-C tools where compile/run cycles were lightning fast (C++ notwithstanding). Gradle made me want to tear my hair out, and I just run Clojure from a REPL because it takes forever to ```lein test``` or ```lein run```.
I'm glad it's back. ghc-mod has been very useful to me and I appreciate all the work the devs put on it. On the one hand; if ghc-mod were part of haskell (or ghc) then: *Pro*: you allways can count on it beeing there *Con*: more work needs to be done for a new ghc In the current situation: *Pro*: ghc can move faster as they don't have to work on ghc-mod *Con*: gch-mod is not guaranteed to be work with whatever ghc version you are using^* (*) ^In ^an ^out ^of ^the ^box ^way. ^I ^was ^aware ^that ^you ^could ^build ^it ^from ^github.
I'd be interested in a reading a Haskellers view of Frege ( Haskell like language on the JVM ).
Why do people prefer revisions over uploading a new point release? With revisions you suddenly have the projects history in two places: hackage and your git repo. If you create proper point releases I can see in the git history what you changed and why. Also, if a hackage admin changes the bounds on my package without telling me.. WTF? Seriously, not nice!
&gt; Scala's type system is Turing-complete For the record: so is Haskell's with type families and undecidable instances.
&gt; Note that if your package won't build with the latest versions after applying your Cabal constraints, then it will probably fail to generate Haddock documentation on Hackage as well. I'm a bit surprised by that... does the doc builder on Hackage use something like Stackage constraints and thus force to use latest versions?
I'm not talking about non-strictness. I'm just saying that the lack of proper TCO makes monads incur a pretty nasty penalty. &gt; That being said, if you consider what real Java interop buys you -- well lets just say you make an expensive argument. There is simply too much good Java stuff to create a language on the JVM and not maintain interop. Swift has interop with Objective C, a language with null (well nil), but has statically checked nullability. Scala could easily have done the same: Proper null checking without losing interop. F# has the same, AIUI.
"Releasing a new version that is compatible" only works for relaxing bounds, not when you need tightening them. You'd have to have adjust the metadata of all affected releases. When package authors realise this, they usually either learn from their mistake and start putting in proper bounds or they start blaming everything else (PVP, cabal, hackage, ...) while arguing that everyone should just use Stackage...
At first, this *seems* like a good idea, but if you actually think about it, it's quite a chicken and egg problem, and I think you're underestimating the combinatorics of install-plans on Hackage with its thousands of packages and even more versions. How likely is it that a randomly picked "unproven resolution" even works at all? What if it doesn't, how do you select the next one to try? How are we to find that several-orders-of-magnitude smaller needle-space area of valid install-plans in the *huge* Hackage configuration space? Even just storing a database of good/bad build-reports (and you *need* to store the failure ones, as otherwise you keep re-computing the bad ones over and over again, as those represent the larger part of the space) would take up huge amounts of storage (and how do you transfer that amount of information to the client?). Just try to think through the bootstrapping procedure for when no positive build-report is available yet for an install-plan specification, and even if we had a few good known install-plans, how this would attract most users to reuse this few ones, mostly ignoring all unproven package version updates, even if just minor version bumps. 
So basically, you see no point in the PVP? (Also, doesn't `.cabal`-editing count as mutable metadata?)
Gradle execution speed sucks, it is still slower than either Ant or Maven. If it wasn't for Android, most of us in the Java world wouldn't even care.
Actually Swift kind of did it right. To avoid having to liter Swift code with null checks, Apple had to add support for nullability annotations to Objective-C code. Kotlin and F# have faced a similar problem, where you need to annotate external code to let the compiler know about possible nullability issues.
That is a much more preferable situation to Scala's.
You give up plenty when writing C++ over C. The name mangling breaks easy FFI, constructors and destructors break easily predictable performance characteristics of scope exit, exceptions break many, many things,...
To be fair, uploading a package to Hackage doesn't make that release automatically end up in Git either (git tagging is also often forgotten), unless your workflow ensures that. But why is it a problem that the edit doesn't show up in the version number? If you use things like `--allow-newer` it doesn't affect the installed package version number either (although you've ended up with a package-db state that can't be reached by default via Hackage)
&gt; end in tears at worst If you are so invested in your language that losing an argument about its merits and flaws makes you burst into tears it might be time to step away from the field for a while. And assuming you were referring to a less civil discussion where arguments become personal, those can happen on any topic and should happen for none. We shouldn't limit topics because of that risk.
Martin did a talk on this, so the context is that it wasn't possible to encode `map` in a pure form in Scala because it wasn't possible to implement `map` to work with things like `Array`'s to work with Java interopt. Java doesn't have value types, but Array's only work with primitives, so it isn't possible to map an Array that returns a non-primitive value without some implicit builder (which is your `CanBuildFrom`) There are arguably some cases where `map` is used where it shouldn't be (i.e. on a `Set`), but the primary reason was for Scala/Java compatibility 
Disclaimer: I use Scala predominantly, but did dabble in Haskell when I was younger I agree with you here, and its something I really don't understand. If you want to code like Haskell in Scala, then you are going to have problems, because (surprisingly) Scala isn't Haskell. It isn't trying to be Haskell, Martin has expressed this many times (he even made a Haskellator joke at his most recent talk). So if you are going to try and code like Haskell in Scala, well you are going to have problems. There definitely seems to be group of Scala people that hold a lot of publicly known animosity towards Scala for not being a "completely pure functional language", even though its been said, deliberately many times, that this isn't one of its design aims. Its like going to the python community and complaining why its not like Java. Its not productive, its not constructive, and its not going to get you very far.
&gt; any program you build simply won't run on the target if its glibc version isn't the same. Shouldn't that be "if the target's glibc version isn't at least the same"? Or is this different on ARM than on x86?
I think laziness might make up for that.
Very good reason, different data structures in their collections have their own performance characteristics. As an example, you standard `List` has very good memory characteristics and is fantastic for linear access, but has terrible lookup time. On the other hand, `Vector` has an effective constant lookup time (nLog(n)), but it uses more memory, and isn't as cheap to construct gradually. At least in my code, we spend a lot of time converting between different datastructures for reasons like this So yes, you could throw a `fromList`, but you would probably have a `fromX` for every single collection you could think of. Instead of that, in Scala, you simply have a `.to[T]` method. Using `CBF` means that this type of code is cleaner, and also faster due to what @seantparsons said
This is imho, a known problem, which is why dotty is being created (https://github.com/lampepfl/dotty). Its basically Scala 2.0, which is being coded from scratch, that fixes a lot of Scala's problems (and also introduces nice stuff, like proper union types, really aggressive DCE, TASTY). It is also attempting to unify things better, particularly stuff like existential types, which seem to have just been tacked onto Scala because people wanted it, rather than properly evaluating how it effects the Scala type system (which is already ridiculously complex)
I wouldn't trust my input on that one, because I'm hardly experienced with it, but ... The way I've read it somewhere, Frege's JVM-interop isn't very good and it compiles to a bunch of static methods. It makes it hard to use it for the Android development, for instance, as some folks reported. PS. I want to point out that I'm rooting for that project with all my heart.
This is not about whether or not to trust a package, but allowing the package author make the decision for you when to bump the dependency version. If you're not comfortable handing over this decision to somebody else, you can specific an explicit version of a package in your cabal file (foo == x.y.z) or use cabal freeze. But, that won't work in practice because packages are not truly immutable, because of revisions (that to me seems soooo weird because the Haskell community usually embraces immutable state, but not in this case?!?) 
re: startup time - It isn't really a JVM issue, the JVM itself takes less than 0.1s to start up nowadays. Most of the startup time in Clojure comes from the core library loading and bootstrapping, which has been improved a lot in the latest 1.8 alpha releases but it still a bit on the slow side.
But if you scrap all interop with other JVM languages, you get nothing for being in the JVM, in that case is better to just use Haskell... With Scala you can use most of the already existing libraries and tools, and some of the niceties from FP languages.
 &gt; IMO criticism of OOP is meant to show that for many problems OOP isn't the best fit, and FP can improve on this. Of course, OOP has a lot of weaknesses but sometimes it's really good(at least for me when I use it with Scala's functional features). Edit: Therefore, instead of eliminating every oop-related things we should choose its right features (carefully).
They're open. 
I have never "left" Haskell. I just didn't feel comfortable when people were saying it's a must to start with Category Theory.
I'd try with newer ghc - 7.10.2 was released recently. Did you tried copying to check if it helps?
Again no one is writing C++ here. It is a compiler output. Whatever FFI or exceptions has nothing to do with it. And if you want to talk about a language with unpredictable performance characteristics look no further than the subreddit you are on. We're talking about the highest performing digital system simulator ever made. You can criticize their ideas in theory, but you can not do better.
To be fair 1 and 4 are because rust was never designed to be a purely FP language. Also you should definitely report 3 because that is definitely a bug. 
&gt; Except... to cabal-install that's exactly what it's saying. Or at least, that's how cabal-install interprets it. You have to tell it to ignore that information if you want to build it against a newer version. Cabal's behavior can be improved. Packages without upper bounds cannot because they are immutable. Packages that don't give you an upper bound poison the well for the solver. The new concept of hackage revisions was created to handle precisely this situation because it allows trustees to go in and add upper bounds. Revisions don't change the fact that packages are immutable, they just give us the ability to manage the situation. None of this would have been necessary if everyone had been using upper bounds properly from the start. &gt; It seems that we would be well served by having different fields to express these different concepts. 1) known to work with, and 2) known to not work with. I've been [arguing for something like this](https://www.reddit.com/r/haskell/comments/1ns193/why_pvp_doesnt_work/cclp1sq) for a long time. But I don't think we need two different numbers. We just need two different operators, &lt; and &lt;!. However, implementing this idea is difficult and we need a precise explanation of what the solver should do with the new information. Also, even if we had this, that does not change the fact that the user still needs to supply one of them. Leaving off the upper bound causes irreparable damage to the ecosystem because we've thrown away a very important piece of information and the only way to recover that information is the very time consuming and costly process of building against all possible versions to see which ones work.
Yes, it only works perfectly if Travis rebuilds on each bump of dependency. It doesn't, but I would like it to. It does suck when things break. I've had plenty of cases where too strict upper bounds screwed everything up too, installing known broken versions of old packages. Alas, neither including nor excluding upper bounds really works - I put them in where I think on the balance of probabilities there is a reasonable chance that a future version will break my package - but it's very much a judgement call, and if I get it wrong, please let me know.
You can ask me here, too. Not that it's hard to find me online. 
You'll have to be more specific than that. Are you talking about Cabal (the library) or cabal-install, aka. the cabal executable? If you're talking about stack (the haskell executable) then support is on the way (https://github.com/kazu-yamamoto/ghc-mod/pull/549) otherwise I don't know what you are talking about ;)
Can you quickly cover or provide a link to a description of the "Option-Some" inheritance problem? 
&gt; I feel like the reason people find Haskell an eye-opening experience is because their CS education was deficient. This kind of attitude was exactly the reason for this post! False assumptions would be: a) Haskell wasn't learned in / during the university years b) Haskell was somehow eye-opening c) Those concepts were somehow unknown to OP before Haskell.
As far as database connectivity goes, HaSQL is worth mentioning, and HDBC-odbc does offer some level of connectivity to commercial offerings such as MSSQL, Oracle, and DB2. postgresql-simple can also be used with RedShift and possibly Greenplum, though some of the more postgres-specific features might not work.
AWESOME! I can run it every day to have new awesome wallpaper! Thank you for sharing! 
[**@GabrielG439**](https://twitter.com/GabrielG439/) &gt; [2015-08-17 05:02 UTC](https://twitter.com/GabrielG439/status/633141902305394688) &gt; I'm surveying Haskell's ecosystem, but I can't do it alone. Please review my draft if time permits: https://github.com/Gabriel439/post-rfc/blob/master/sotu.md &gt; &gt; PRs welcome!!! ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
If I'm not mistaken, the Kotlin people have given up the idea that they just annotate the whole Java universe and solve nullability that way. It was probably a too great or impossible task, not even talking about third-party libraries.
Of course FFI matters, name mangling will screw with any symbol, generated or not, as long as you use C++ classes or methods. If you don't use classes, methods, exceptions or any other C++ features you might as well use C. And as far as Haskell is concerned, yes, it would be a horrible intermediate language too but the comparison was C++ vs. C.
2 is a valid concern, but you can avoid it pretty easily using an impl over the type with a "new" method that returns your array instance -- the copy will likely be elided. 4, i think they are depending on the borrow checker and move semantics to control scope, and the type of the variable is the type of the variable in either scope (assuming i read and understood what you were saying here).
It remind me of the [Haskell Communities and Activities Report](https://wiki.haskell.org/Haskell_Communities_and_Activities_Report)
What about it? I read a "come help me finish this post" tweet differently than a "here's an article I've just finished". Of course, there's probably nothing wrong with just submitting to reddit twice :)
[I'm a friend?](https://41.media.tumblr.com/tumblr_lvrew4W0rv1qibz0jo1_r1_500.png)
Very little of that minute has anything to do with compiling code. Half of it is startup of sbt itself (including plugins) and if you clean, it checks your dependency cache and reloads libs as needed from your ivy cache. This can drag if you don't know enough to avoid it. If you cleaned out you cabal cache and built a haskell app that used various frameworks, it will take plenty of time to run. If you have an sbt console open on an app, and dont clean every five seconds, it takes moments to build a project. I use "~ compile" and I spent little to no time waiting for compilation.
I don't understand why people keep coming with the `map` signature to say that something is "incredibly complex". `map` takes a function that maps input elements of type `A` to output elements of type `B`. It takes a builder factory that determines the output collection `That`. This is an implicit element so you either get the same collection type as the input or the one you ask for with respect to the expected return type. This is a very powerful mechanism and not complex at all. I thought Haskell people love type classes. Well, think of `CanBuildFrom` as a type class. Scala collections against all hatred IMHO have an excellent design that works very well in practice. If I had to pick one thing from the standard library that I love Scala for, I would say it's the collections.
Ahm no… This example has particularly nothing to do with either inheritance or type erasure. There is an argument about decreasing the amount of mixins in the collections, but that goes for the implementation and not so much the API. And Martin Odersky acknowledged that a bunch of collection methods could indeed go into extension methods/ type classes if the API was to be redesigned. So it was a choice and not forced by the JVM.
Isn't code I can call only once breaking referential transparency?
&gt; Lastly the Scala community is one of the most unpleasant I've seen in an FP community. In my experience, the only unpleasant people were exile Haskellers talking down their Scala "peasants".
`stack` itself is quite stable and works wonderfully. The problem is really documentation, especially for new-comers. If you already know `cabal` then `stack` is an easy shift. But for someone who has never used Haskell before, `stack` will not show up in almost any books or tutorials, so (s)he will not even know to try it. And even when (s)he does, all that's available at this point is github repos and blog posts. All that said, at this point, I would still highly recommend `stack` to someone at your level. But you'll need to be somewhat aware of the state of affairs as to not be confused by the existing book/tutorial/blog space.
&gt; Implicits only search for the current type. If the value you have is of type Some[A]? You won't find a ToFunctorOps[Some[A]], anywhere; you'll need to explicitly tell Scala that your Some is also an Option. Not true. implicit class OptionOps[A](x: Option[A]) { def foo: String = x.fold("&lt;none&gt;")(_.toString) } Some(33).foo Perhaps your ops are not correctly typed in terms of variance, perhaps because in FP people think variance is wrong.
Just to be really clear, I think `stack`'s only problem has to do with documentation and "acceptance" into the Haskell ecosystem. The tool itself is quite stable and useful. See [my response to the other thread](https://www.reddit.com/r/haskell/comments/3ghvzd/how_to_install_network_with_minghc_7102/cu5udl2).
I suppose if your algorithm requires trampolines to implement that the object creation alone when implementing a trampoline can be prohibitive. I was just saying that Haskell doesn't necessarily create a stack frame every time you call a function due in large part to it's non-strict implementation, so it's benefits over Scala in this regard go far beyond simply have TCO. So im a Scala developer calling into a Java library and a method returns null. What does Scala changing that null to Nothing or Unit prove? It just hides the null from me -- whereas if I handled the API call directly in my code I can apply further understanding to what it actually means. I can still wrap it in an Option or turn it into a Nothing myself before propagating it, but i cant undo any assumptions that runtime made about the null values meaning. Hiding a null doesn't make it go away.
Maybe should include a bit about support for different OS's and architectures, and a mention that windows is a bit second class (if this is still the case).
Though things continue to be a problem if your `Some[A]` is more complicated than `Some[String]`. If the `A` is itself polymorphic, the wrong type can still be inferred. 
I don't understand your concern, why would you only be able to call it once?
Yes, it's very much in the spirit of that except organized to present information in a format directed at somebody considering whether or not to try Haskell and how it stacks against other languages.
I am talking about the incremental compilation though, not a full recompile. I built a Play app out of the box, started sbt, changed a single source file, and sbt took 47 seconds to recompile it. This was on my work-issue core i5 lenovo t440 that I got last year. So not a nice machine by any means but not ancient either.
Exactly. cats is basically "scalaz without the horrible Haskell people".
Regarding GUI libraries, it's probably worth mentioning hsqml: https://hackage.haskell.org/package/hsqml
You guys convinced me -- its hard to argue that Scala couldn't use a better TCO -- i think they just dont know how to implement it on the JVM without support from Oracle.
We're all friends here! Except /u/PM_ME_UR_OBSIDIAN. Screw that guy. ...ahh, who am I kidding. Lambdas for *everyone!*
Ok, fair enough. I doubt you will find anyone that praises Scalac for its speed. Personally, I find the incremental compilation mostly sufficiently fast, but a full rebuild can be PITA.
Could you expand on some notable features of the Ruby and Perl package managers?
The latter usually, but subtyping and implicit coercions sometimes leads to the former (e.g Option types problems mentioned elsewhere in this thread)
&gt; And even Swift has this implicit type conversion to a "I don't care, stop enforcing null handling" type. No it doesn't?
I think Haskell found a nice apporach to this problem, but I think it's not the best thing one can do. As seen in many Scala libraries already I think the best design will adopt a strategy in which computations are staged and then can be evaluated in multiple contexts (collections, streams, database operations). Scala's fantastic meta-programming system combined with the new IR will create amazing possibilities here.
 let assumedString: String! = nil let implicitString: String = assumedString Welcome to "implicitly unwrapped optionals".
You're conflating a number of separate things here. Popularity is different from technical merit. I think everyone would agree a Bugatti Veyron is a technically superior vehicle, but it's far from the most popular. Likewise a Honda Accord isn't amazing from an engineering perspective (although it is certainly well made), but it is a very popular vehicle. If you're trying to evaluate something on technical merit, popularity is an incredibly poor metric to use. As an example, PHP is surprisingly popular (even today), and yet is almost universally regarded as one of the biggest train wrecks of a language. The fact that the technically ill informed pick something based on either market hype or what their golf buddy happens to be using isn't a good argument in favor of that thing. The JVM and Java are popular because of primarily network effect. Java has a large body of available libraries, and it's a lowest common denominator language (nearly everyone knows it, or at least thinks they do). That doesn't mean it's a particularly good language (it isn't bad, but it isn't great either). Scala and other JVM based languages are trying to leech some of that network effect for themselves, but often in doing so they make technical choices that undermine their engineering principles and result in them being far weaker technically than they could have been.
For example, CPAN has established processes in place for: - [social reviewing](http://prepan.org/) of modules before they get uploaded to CPAN, to provide reviews of the proposed code/API, discuss the naming, and see whether that couldn't instead be part of an existing package - [distributed testing](http://wiki.cpantesters.org/) of uploaded packages, on exotic platforms etc. - [dealing with unresponsive maintainers](http://www.cpan.org/misc/cpan-faq.html#How_maintain_module) or taking over maintainership of modules How does Hackage fare on any of this?
You do know that Haskell's mantra has been "avoid success at all costs" for years, don't you? 
I think you're conflating technical superiority and mathematical/logical consistency. This is only true if you can show that consistency always equates to developer productivity. And programming languages are replete with counter-examples that prove that idea false. It's kind of like Adam Smith -- the best programming language is not the most consistent one, it's the most consistent one that still allows you to be productive (in groups). People vote with their feet and strike that balance quite well.
Most of that is wrong.
Huh? my point is there does not exist an: &gt; implicit type conversion to a "I don't care, stop enforcing null handling" type. The only implicit type conversions are _from_ the ! type to the non-nilable type or the explicit optional type. There are no conversions _to_ the ! type.
Comments for /u/Tekmo: * Although I can't argue with the "best in class" ranking for compilers, I have always experienced pain when I've tried, because the expression problem bites me when I try to implement intermediate languages that share a lot with each other, and none of the approaches that I've tried worked for me; that said, ADTs and exhaustive pattern matching are worth the pain. * I disagree that Haskell refactoring is "best in class", but only because of lack of tooling support; once a version of HaRe is released that supports modern GHC and it's integrated into editors (at least haskell-mode for me), I'd be able to get behind this. I agree that refactoring feels safe and is better than most dynamic languages (Smalltalk possibly excluded), but there's something really nice to be said for the kind of automation of trivial tasks that you get with Eclipse in Java. * I'd add a disclaimer where you say, "The compiler really infers **everything**." (emphasis original). This is not technically true with things like rank-2 types (as I know you know). * If this post gets published after GSoC finishes, you will probably want to take a look at some of the projects (summarized on Reddit [here](https://www.reddit.com/r/haskell/comments/342rvp/google_summer_of_code_18_projects_accepted/cqqt8mw)): * Data science will benefit from successes in "Layered grammar of graphics" and "Interactive widgets in IHaskell" * Refactoring will benefit from "Refactor a program with HLint suggestions" * There's also some projects which will help web-dev and concurrency, as well as other, less easily categorized improvements. * I have difficulty classifying package management as "mature", but the last big project I worked on started dying when cabal sandboxes were just starting out. I remain optimistic to see how stack will change this, but the fact that I often felt the need to delete my sandbox and rebuild all the dependencies to verify that the build failure was reproducible is not a good sign. My experience seems to be the opposite of yours, because I've had hard-to-debug issues when the wrong compiler was used for the wrong project (e.g. `cabal install --only-dependencies` was necessary before doing `cabal configure`, but the former doesn't respect the `with-compiler` field in `cabal.config` until after configure is run, so the entire build is wasted with a confusing error message), and I find that I have lots of wasteful rebuilds every time I install a new sandbox with `lens`. * Logging has also, in my experience, been not quite up to snuff, requiring boilerplate to work around purity so that e.g. your loggers would specify which module the code being logged is in.
&gt; Although I can't argue with the "best in class" ranking for compilers, I have always experienced pain when I've tried, because the expression problem bites me when I try to implement intermediate languages that share a lot with each other, and none of the approaches that I've tried worked for me; that said, ADTs and exhaustive pattern matching are worth the pain. Have you tried just untying the knot? This approach works quite well for me. A little bit annoying, but saves a lot of trouble.
Except it doesn't do that? The non-nilable type coercion raises a runtime error if the value is nil. It doesn't let nils into the type.
Of course. That doesn't invalidate my point. It doesn't in any way "stop enforcing null handling". It just implicitly inserts a check for nil. 
Pretty classic example of organically growing a programming language without any guiding principles beyond immediate pragmatism. I think everyone knows that PHP is an awful language to use, but that *immediate pragmatism* still provides a pretty compelling reason to use when there aren't other alternatives.
Ah, but I must box (and therefore allocate) even when it is not needed: If I can track what captures are available and only use closures where their captures are still in scope, allocation is not even necessary for such closures. But Rust makes me allocate for it anyway.
&gt;people were saying it's a must to start with Category Theory. You and I both know those people are wrong, though. There's no CT in the book either.
&gt; They encode the variance information as an implicit evidence parameter because it's messy (impossible?) to represent all of the proper variances just using type annotations. Sorry, again no. Unless you have a completely different meaning for "variance". There is simply no default builder factory for this: implicitly[CanBuildFrom[Set[Int], Int, Seq[Int]]] // not found Set happens to be invariant in the element type, but so are mutable collections: implicitly[CanBuildFrom[mutable.Buffer[Int], Int, Seq[Int]]] // ok You may force conversions by providing the builder factory explicitly. val x: Seq[Int] = Set(10,20,30).map(_ * 100)(breakOut) &gt; Also, without the implicit the List class would need another way to know what a That is (likely via a type parameter), and once erased they would have to either use reflection to get it back Again you are confused. `That` _is_ a type parameter already. The resolution of context bounds (type classes) is a compile-time operation, this has nothing to do with reflection and erasure. The whole concept of `CanBuildFrom` is akin to a type class, precisely because you don't want to overload `map` with a closed number of cases, and not because of "erasure".
This is hugely informative especially for a new Haskell programmer, unfamiliar with most of the linked tools and libraries. Thank you for posting it.
The lack of a really good database solution has always been the showstopper for me. There has always been a lot of activity in the database space trying to get that final solution all the way back to haskelldb. Opaleye and relational-record are two new ones that I hope gain traction, assuming they work (I haven't had a chance to try them out on a real workload). Unfortunately opaleye is postgresql only which makes it a non starter for me. Hasql requires sql quasiquoting, and I don't see how I can build a complex application on that. I do not want a solution that is built on string concatenation, which precludes all the -simples, hdbc, hsql, etc. What I'd really like to see is an sql generator that I could plug into any of these database backends, although I have a feeling that would have some downsides.
I'll give you a proud salute from under my bridge.
How does ghc-mod and ide-backend compare? ide-backend was open sourced by FP Complete and seems to do similar things as ghc-mod. https://www.fpcomplete.com/blog/2015/03/announce-ide-backend
&gt;I am not sure how hackage deals with that. There's a policy. You try to reach the maintainer, then announce intent and then there's a waiting period. People can chime in in the mailing list thread if they've had contact with the maintainer recently.
I could rant for ages about this topic, but it really boils down to two things: one, a culture that outright refuses to acknowledge the merit of theoretically sound abstractions, and two, as a consequence, a language and ecosystem that makes these abstractions virtually impossible for the poor souls who attempt to create them. Coupled with that is the powerful rhetorical device of always demanding examples and then refuting them individually through workarounds or practicalities. No unicode strings built into the language? No problem, we have a whole family of multibyte functions. What if people use them wrong? No problem, just don't use them wrong. Etc. etc. The culture never makes any decisions based on principles, "it works" is invariably the only seal of quality that anyone ever uses or accepts - *how* and *why* it works is meaningless in PHP land, and obsessing about these things is frowned upon. So really, my dislike is for the community more than its product, I guess.
/r/lolphp 
Proper union types?! My body is ready!
You're welcome!
Yeah you're right; "boot-up time" is probably a better way to put it. But I think it's still pretty slow compared to the speed at which node.js or JavaScriptCore can start-up... it does seem like there's less overhead.
I'm probably one of the few people coming to Scala after learning Haskell for a while. I personally prefer Scala because of its pragmatism. Sure Haskell has some language features solved in a nicer way, but trying to solve real-world problems with it has been a pain for me.
&gt;&gt; social reviewing of modules &gt; &gt; A mistake that is widely reviled. I'd like to read more about this. Do you know of any links to articles or forum threads discussing this? In the early 2000's I was heavily involved in perl and perl culture (perlmonks was awesome back then), but I don't recall any mention of this problem.
Very good post, and an excellent summary that matches my view of the language pretty well. I'm pretty new to Haskell; I've only tinkered with it in my spare time for about 2-3 years. I absolutely love the language, and all the tools (or features rather) that helps in maintaining a codebase, but I feel that it's still behind the "popular" languages when it comes to many real world use cases. I would, however, argue that Haskell is worse than average for scripting. The main reason for this, I think, is that it's very hard to manage dependencies when running "scripts" with runhaskell/runghc. Maybe I'm just a noob, and haven't found a good way to do it, but I really miss a tool like cabal/stack for that use case (asking users to install specific versions of libraries does not count). Maybe it works in a corporate environment, where you can have absolute control over the packages that are installed on users' computers, but for open source projects I think it's a real problem. Add to this the fact that Haskell API:s seem to be in a constant state of flux; even base sometimes breaks backwards compatibility.
I've been considering porting my [axiom](https://github.com/dkubb/axiom) ruby library to Haskell. I modelled it after Tutorial D and wrote it using immutable objects and pure functions throughout. I found Relational Algebra to be quite beautiful, and was able to generate SQL from the resulting AST. I even wrote a simple [optimizer](https://github.com/dkubb/axiom-optimizer) that could take axiom's AST and produce a simplified version that could be used to generate SQL. I didn't take axiom all the way and begin promoting it as a superior alternative to ARel (which it is) mostly because I lost interest in the Ruby OSS community and wanted to focus on learning Haskell. I'm still learning, so it will probably be slowly coming, but I definitely would like to have something comparable within the next year or so.
IIRC `ide-backend` is going to render `ghc-mod` obsolete, as I've heard the haskell-mode developers are planning to integrate `ide-backend`
That's exactly what I'm saying. When you already know that the value can't be `null`, there's no point in wasting resources on performing that check.
Besides all the worts that came about from it's inception PHP is now on the fast path to becoming a Java-esque language. On the surface additional facilities for type check seem like a boon to their community, but it is all run time, so any type checking is the same as littering asserts in your codebase. Add to that a lack of generics and you get a very feeble type system that forces you to interface all the things.
Honestly, that was also part of the confusion. When I get a chance later today, I'll try to rewrite this chunk how I had it in a way that seemed to be correct, but it was very close to what /u/penguinland wrote above. I found it really confusing to get the error message "ifThenElse not in scope."
&gt; Haskell doesn't necessarily create a stack frame every time you call a function due in large part to it's non-strict implementation Function calls don't great stack frames *at all* in Haskell (or at least GHC).
&gt; I would rather have row polymorphism than subtyping, though. In a nutshell, what is row polymorphism?
Yeah, I eventually realized that my Scala way of thinking wasn't appropriate in Haskell, and I just needed to stop solving problems in the same way. Like I said, I was a bit disappointed, but it's really not so bad.
Go's packaging is beyond terrible. `npm` and `cargo` are the best that I've used, although `npm` is pretty bad too.
Actually, I quite enjoy programming in PHP. Having say that + I still don't understand how comparaison (`==` and `===`) and cast to bool works + I don't like that you can't test (easily) if a key belongs to a Hash. + I don't like dynamic typing But on the plus side + There are some functional things but they are really verbose. + You can do pretty neat OO design with it (even though most PHP programmer don't bother with it). + Drupal codebase is really well written and they try to emulate ADT using hashmap, for example to represent HTML like this array('#body' =&gt; array('#div' =&gt; ... ) ) ,which is a good intend but it's much nicer with real ADT ;-) 
I think most people in this context assume "enforcing null handling" comes with an implicit "at compile time with the type system". This particular system you describe has the nicety of failing fast at runtime (which was interesting to learn, thanks), but following your logic I could make the argument that NullPointerExceptions are enforcing null handling too.
&gt; Swift has interop with Objective C, a language with null (well nil), but has statically checked nullability. Swift also has the benefit of 10+ years of hindsight. (I agree that Swift's approach is better.)
&gt; In my experience, the only unpleasant people were exile Haskellers talking down their Scala "peasants". That was my experience too. I killfiled a few of them (the first time I'd done that in a decade!) and then I heard they got banned from scala-user. Things greatly improved on that mailing list after that. (This was 3 or 4 years ago at this point.)
The vast majority of my applications need the flexibility, like in the where clause, some criteria is needed only sometimes or sometimes there is none at all. Also when you need to join but only sometimes, and when you do you need extra columns from those joins and extra clauses in the where related to those new tables. At best you end up with intense string mangling and inevitable runtime errors. Hasql doesn't seem to even let you modify the string after compile time so you'd just be writing a ton of queries for every possible use case. Relational, opaleye and haskelldb allow you to piece queries together, create and combine functions into type safe restrictions, limits, orderings, as code and get a valid query out of it. They even do aggregation. Groundhog and persistent allow you to avoid sql concatenation in a more limited fashion, but they have their own downsides.
This has also been my experience. +1 /u/brnhy
Unfortunately I'm stuck with mysql at work. There's no hope of change and any library that uses subqueries is doomed to failure there unless there have been some major advances in subquery performance in the last couple years. Edit: I think I'm going to test some things to see if the situation has improved. If it does I'd be happy to make opaleye work with it. I think opaleye is the best way forward.
To pull some quotes that resonated with me from the counter-thread in /r/Clojure: [permalink](https://www.reddit.com/r/Clojure/comments/3h4qdk/what_are_clojurians_critiques_of_haskell/cu5t4dd) &gt;Poor examples - even for things in the standard library, I spent a lot of time trying to figure out how do I use this? [permalink](https://www.reddit.com/r/Clojure/comments/3h4qdk/what_are_clojurians_critiques_of_haskell/cu56mj6) &gt;The documentation in Haskell is poor. Or rather, it often seems to assume you know already know the domain, and just need reminding of the details. [permalink] (https://www.reddit.com/r/Clojure/comments/3h4qdk/what_are_clojurians_critiques_of_haskell/cu53fst) &gt;Everyone seems to be allergic to example based documentation ("just follow the types!") To be a bit more specific, I find the following lacking in most documentation I read: * What is the entry point for this library? * What does a typical usage of this library look like? * What other libraries/tools/techniques might I need to be effective with this library? * 'Basic Usage' comes before 'Expert Usage'. * Why are things designed the way they are? This one is more of as needed than the rest, but when I find it needed it usually isn't there. For a good example, that I personally suffered from, is [System.Process](http://hackage.haskell.org/package/process-1.2.0.0/docs/System-Process.html) which I believe fails at all of the above. Instead of the above, I get the following: * A list of functions with type and docstring, sometimes with commentary. * A list of data types, sometimes with docstring. IMO, in an ideal world these are things I would be looking up via my repl instead of having to use a webpage.
No, the distinction here is that `null` is not an inhabitant of every (reference) type. 
I've just started a new commercial endeavour in a team of four Haskell lrogrammers. We use stack and it really just works! I can only recommend it!
Isn't ifThenElse used when you're using RebindableSyntax? I'm on my phone so I can't take a detailed look, but that's what I suspect is causing the problem.
Nothing against Nix as a system (used NixOS for a while), but it's a bit heavy. You shouldn't have to use a generic build system to nicely compile a Haskell package. People should be able to use a language's package management tools outright without another layer.
Row polymorphism is an alternative to record subtyping that fits better in a Haskell/ML style type system. Suppose you want to write this function: add_foo_bar x = x.foo + x.bar What should be the type of this? We could do `add_foo_bar : {foo : int, bar : int} -&gt; int`. In languages with subtyping this works for any record having a foo and a bar field, for example: add_foo_bar {foo=3, bar=4, baz=5} It works since `{foo : int, bar : int, baz : int}` is a subtype of `{foo : int, bar : int}`. Row variables let you do this kind of thing without subtyping. We change the type of `add_foo_bar` to `{foo : int, bar : int | rest} -&gt; int`. Here `rest` stands for any other set of record fields. So if we call it with `{foo=3, bar=4, baz=5}` then `rest` gets instantiated to `{baz : int}`. Here's another example: min_by_foo x y = if x.foo &lt;= y.foo then x else y We can give this type to the function: min_by_foo : {foo : int | rest} -&gt; {foo : int | rest} -&gt; {foo : int | rest} This shows the difference with subtyping; `min_by_foo {foo=3, bar=2} {foo=4, baz=5}` is disallowed, because there's only a single `rest` and `{bar : int}` is not compatible with `{baz : int}`.
haskell-mode itself tries to be versatile so both ghc-mod and ide-backend can be supported. It depends on voluntary effort. Yazu Yamamoto volunteered to move most of ghc-mod specific elisp to haskell-mode: https://github.com/kazu-yamamoto/ghc-mod/issues/544 
Needing MSYS2 is part of what makes it 2nd class. First class would mean direct integration with the Windows ecosystem, like Visual Studio, Visual C++'s compiler (for native linking) [edit], .NET, PowerShell, etc. That's a huge feat, but it would certainly buy Haskell a lot more commercial users. We'll see how Microsoft fares over the coming years, however.
Yes, I know. We all know. It's just a semantic argument about the word "enforce". I choose to go with the clearly expressed intent of the person who started this sub-thread e.g. you can use Swift's type system in a way that will ignore null checking at compile time. And you choose to look at his intent in an uncharitable fashion e.g. all Swift types can be made Type | Null. I see we're not getting anywhere. So I'll just leave it at that.
There are a couple of advantages. Using row variables fits in a HM type system, including full type inference. Subtyping makes type systems much more complicated (e.g. variance). There's also a more vague, but I think ultimately more important advantage. Row variabes fit into the Church philosophy of type systems as opposed to the Curry philosophy. I would summarize this as: * Church: types determine a set of values, and for different types those sets are disjoint. The question of whether a value of type T is also a value of a different type Q is not even meaningful. * Curry: there is a big set of all values, and types determine a subset of those values. Maybe it's just me, but I find the Church philosophy especially satisfying. In a language like Java if you ask "What are the possible values of type Foo?". It's either a Foo, or any subtype of Bar of Foo, and you can test a value of type Foo whether it's actually an instance of Bar. If you think about it that answer is incredibly complex and open ended. In a Church philosophy type system it's simpler: a type just has a set of possible values, and this is disjoint of any other type and can't be enlarged later.
Also, HaRe uses ghc-mod as a backend to manage cabal, config, etc. Having ghc-mod in haskell-mode will be a plus
I'm not sure if this is quite the same thing, but it's at least closer: there's ongoing work to rebase much of the compiler on top of a more minimal [mid-level IR](https://github.com/rust-lang/rfcs/pull/1211). Along with work to [formalize](https://www.reddit.com/r/rust/comments/3gzidr/rust_in_2016/cu32vf3) it, or something similar to it. 
&gt; Upper bounds can bitrot quickly, and require constant maintenance. No, they cannot bitrot. The software that I upload today with correctly specified bounds will still build in the future regardless of what other people do because cabal will know what versions I was building with originally. That is the definition of not bitrotting. You are saying that it is bitrot when my package today won't build with the next major release of a dependency. But that is not bitrot! Bitrot is when things that used to work stop working. But the package never used to work with the new version, and since we're talking about a breaking change you can't expect it to. What you're calling bitrot is actually called maintenance. You cannot just expect software to work with new things when they come out. The new configuration needs to be tested and someone/something needs to warrant that it works. That maintenance can be automated, but it must not be assumed to work. With upper bounds the software that works today will still work later. This is a much better default to have, than the one that you get when you leave off upper bounds. In that case, your software spontaneously breaks when a dependency makes a breaking change. And furthermore, your software breaks EVERYTHING THAT DEPENDS ON IT. That is absolutely the wrong default to have.
Well, it's unstable because it's new. As for not needing stack, I can see you're not a fan from your comment history :p For me (and the hoard of beginners I'm trying to teach) stack has been great. The great thing about stack-ide is that you don't need to install anything other than stack, which should handle everything for you. This is the difficulty when writing ide plugins that depend on ghc-mod or raw ide-backend (getting those installed on the user's machine which work with their combination of GHC and cabal). Obviously if you don't want to use stack it's not going to help you. 
Me too! Have you tried using it already? It is a fascinating effort indeed 
Good to see more 'n' more editor tooling supporting Stack.
The language level "features" for refactoring may be best in class, but tooling support for automated refactoring is basically non existent. Compare the state of refactoring tooling in haskell to java or C# - it's incomparable worse. 
I'm really curious how they stack-up (no pun intended) to each other, feature by feature. I've heard they are not yet on feature parity, but it is hard to find exhaustive lists of features of either project. Best I found was these: http://www.mew.org/~kazu/proj/ghc-mod/en/ghc-modi.html http://www.mew.org/~kazu/proj/ghc-mod/en/ghc-mod.html And these: https://github.com/fpco/ide-backend/blob/master/ide-backend/IdeSession/Query.hs https://github.com/commercialhaskell/stack-ide/blob/master/stack-ide/src/Stack/Ide.hs https://github.com/commercialhaskell/stack-ide/blob/master/stack-ide-api/src/Stack/Ide/JsonAPI.hs
The API supports that as well through the use of the view and force methods. It just doesn't make that the default behaviour for all actions.
Maybe not Hackage per se, but Stackage is certainly something I expect some other lang communities will steal/borrow over the coming years.
The flux link is broken: https://github.com/facebook/flux 
&gt; I tried to use if-then-else here and got the confusing message "ifThenElse Not in scope". Isn't "ifThenElse" built-in? Your .cabal file lists `RebindableSyntax` as a default extension. `RebindableSyntax` will cause functions that are currently in scope to be used for desugared functions. Among these is `ifThenElse`, the function the «if … then … else …» expression desugars to. Last time I toyed with the extension, using it would cause `ifThenElse` to be completely undefined until you provide your own version in scope of the module. So if you are using `RebindableSyntax` for a reason, you must redefine that function.
Somewhat, except it has to look more like this: data Expr a b c = ExprLit a Lit | ExprApp b (Expr a b c) (Expr a b c) | ExprLet c Var (Expr a b c) and it starts getting unwieldy pretty quickly; it also doesn't allow you to have heterogeneous trees such as you'd have in a partially translated expression. We did try something similar, using GADTs: data Expr a where ExprLit :: Lit -&gt; Expr A ExprApp :: Expr a -&gt; Expr a -&gt; Expr a ExprLet :: Var -&gt; Expr B but this got unmanageable when we needed to be polymorphic over some but not all intermediate languages, and still had problems with heterogeneous trees. 
Warp-WAI-Yesod is also rather full featured, with several leaner stacks (-Scotty, -Spock, etc.) available. Also a lot of new FWs are build on Warp, because it is really fast, full-featured, and allows middleware plugging. Finally developments in the Warp-WAI-Yesod camp seem to outpace the competition. TL;DR: in my opinion not "most popular [...] largely [because] they actually blog about things."
There are many options. Have you for instance seen Joey Hess' [http://hackage.haskell.org/package/shell-monad](shell-monad)? I try to [keep an up-to-date list on the Haskell Wiki](https://wiki.haskell.org/Applications_and_libraries/Operating_system#Shell) that's links to all options the Haskell ecosystem offers regarding "shell" kind of work. Feel free to extend. @ /u/Tekmo : maybe you could link to that list?
It was on the front page of HN for some time today. https://news.ycombinator.com/item?id=10071535
There's no fundamental reason Opaleye has to use subqueries. There's a lot of scope for optimization in the SQL generation.
Yeah, there need to be some type-level tuples that can pack all those arguments into one. Unfortunately my attempts to do that with type families have not worked out. Maybe you can use data kinds and do data ExprK = ExprLitK | ExprAppK | ExprLetK data Expr f = ExprLit (f ExprLitK) Lit | ExprApp (f ExprAppK) (Expr f) (Expr f) | ExprLet (f ExprLetK) Var (Expr f) 
&gt; &gt; postgresql-simple can also be used with RedShift and possibly Greenplum, though some of the more postgres-specific features might not work. According to my [limited experiments](http://productivedetour.blogspot.com.es/2014/12/connecting-to-denodo-virtual-dataport.html ), it can also be used with the express version of Denodo Virtual DataPort, a "data virtualization" tool.
It's only wasting resources if you have no idea how modern CPUs work.
Bitrot is also when a package that used to be relevant falls behind the state of the art and is incompatible with the new status quo. For example, if I supply a library "baz" that depends on an old version of Text, then anyone that wants to use baz and text will be stuck with the old text for their project. Worse is when the base package gets a bump and people are stuck on old versions of ghc even though a package might work perfectly well on the new stuff. &gt; You cannot just expect software to work with new things when they come out. The new configuration needs to be tested and someone/something needs to warrant that it works On the other side of the coin, you cannot just expect software not to work with new things when they come out. Said testing and warranting takes time, and not all maintainers feel the need to re-vet their software every time a dependency takes a major bump. If you are distributing a binary via source, then it is best to not rely on cabal to figure out the dependencies; provide a freeze file instead.
Even if said in jest, this kind of commentary can set a discussion-stifling tone, discouraging talk about ideas from outside the Haskell bubble merely because of their exterior origin. I'd like to see a Haskell community (if it's even meaningful to talk about such a monolithic thing—debatable) which criticizes ideas rather than the people who enjoy them. Saying things along the lines of "no true Haskeller..." doesn't set a tone for a welcoming community.
Disclosure: I know nothing about designing package management systems and this is a total pipe dream. I'm just wondering if it's plausible... I've never found version numbers very helpful in determining whether a given package will work with my application. This is not just true of Haskell. In general I don't believe there's a very meaningful relationship between a string of dots and digits and what's in a package. More important to me is the actual API provided by the package. A more predictive set of questions would be: "Are the modules still exporting the functions I'm using?" and "Are those functions still using the same type signatures?" So instead of a version number, how about an API manifest. Something like: { "Module.A": ["foo :: Int -&gt; String", "bar :: [a] -&gt; Bool"], , "Module.B": ["baz :: Int -&gt; Int"] } Then the resolution tool can accept the new version of the package based on these questions: 1. Which modules do I import from this package? 2. What functions do I use from those modules? 3. Do they exist with the same type signature in this version of the package?
This is the takeaway for me: &gt; What I know is: we lost a _lot_ of time dealing with dependency conflicts, but now things are better. If I were starting a project today, I’d be much more enthusiastic about Haskell thanks to Stack and other newer developments. Thanks for the great post! Interesting to hear your experiences.
Hackage has a [Package Versioning Policy](https://wiki.haskell.org/Package_versioning_policy) (PVP) linked from the [front page](http://hackage.haskell.org/). This policy describes a very clear link that package authors are expected to maintain between API and version numbers. Even if we did have your API manifest system, people would still want to have version numbers as a concise way to refer to a particular version of the API. And it would make sense to have those version numbers follow the PVP as that would make the correspondence to the API more visible and the version numbers more meaningful. Right now it works on the honor system. People make mistakes occasionally, but by and large you can usually rely on versions to comply with the PVP. In the future it's certainly possible for us to get some automated checking that verifies that the version number you're choosing and the API changes you're making comply with the PVP. You could even automate the choice of version numbers, but you probably don't want it completely out of human control because sometimes people like to communicate specific things with their version numbers (like 1.0 signaling stability, etc). So all this talk of version numbers is doing effectively the same thing that you are recommending. There's lots of room to improve, but the versioning guidelines do have an important API correspondence that has a significant impact on the stability of the Haskell ecosystem, which is why I'm writing about it.
That looks like exactly what I'm looking for. I will give it a test run. Thanks!
Indeed. Consider the Moore machine. data Moore a b where Moore :: (s -&gt; b) -&gt; (s -&gt; a -&gt; s) -&gt; s -&gt; Moore a b we can build a moore machine that computes, say a CRC32 or other hash function. crc32 :: Moore Word8 CRC32 and the 3 elements are: the finalization function, the step function and the initialization function. We can trivially write `feed :: Moore a b -&gt; [a] -&gt; b` or `feed :: [a] -&gt; Moore a b -&gt; b`. The latter presentation has the benefit of being a nice Cokleisli arrow suitable for extending. But really, `duplicate` changes the game. We never give the user access to the intermediate state type, but yet, despite this, `duplicate` lets us build a resumable hash function. duplicate (Moore k f s) = Moore (Moore k f) f s duplicate crc32 :: Moore Word8 (Moore Word8 CRC32) I used this approach in an article here: https://www.fpcomplete.com/user/edwardk/cellular-automata/part-2 If we know more about `s` we can use this for parsing in time based on the compressed stream size rather than the original: https://www.fpcomplete.com/user/edwardk/moore/for-less
`stack` still, ultimately, uses `cabal-install` to do all of its work, so I'm guessing it won't do anything appreciably different on this front, but I could be wrong.
That would be the niggling restriction I was thinking of. We were bitten by precisely this in the ermine runtime system.
Brash and critical though it is, I empathize with sklogic's [criticism](https://news.ycombinator.com/item?id=10072687).
Nope. 😀 preemptive Standardization in the land of types should wait till the Apis are right. Anything else creates dreadful worse is better. Which is worse for applied math. edit: basically we're still learning how to build nice tools in that space, and i'm not sure if theres a "one true" design, because ... math (and computational mathematics) has an incredibly diverse range of problems. And a one size fits all modality doesn't match up with recognizing that approach. Though having a one true solution would be cool too :) 
Speaking as a maintainer of one of these libraries that is maintained mostly by one or two people (IHaskell), we welcome casual contributors! If there's a library you'd like to help out with, just email the maintainer and let them know and as for some small tickets that they need help with. People are often incredibly receptive and glad to help get started.
That's it! I was totally confused because I did not include it in the module in question. My .cabal file has cruft in it from another project. Thank you! That was a good find.
I think it's a legacy from a cabal file that I used as an inspiration for my own. It's a problem of naively using things without really considering what they are. Unfortunately, I found `cabal` to be difficult to get a handle on and I had that part really early in this project without going back to it. Actually, I had forgotten that I had done that to start my cabal file. I am a bit embarrassed to admit that that's how that got in there.
Indeed. It shows in the standard lib, where many functions are outright, no-sugar wrappers of C functions. It's obvious that Ramsus, while writing what became PHP, was thinking, "Oh, I need this C library function right now, so I'll just import it", never to go back and refactor any of it.
Yes, and it's also worth mentioning that `turtle` basically gives you most of what you need for scripting purposes because it re-exports a lot of functionality, so that linked example is actually a realistic list of script dependencies.
Sounds a bit like 'no one every got fired for choosing IBM' which is a risk adverse approach, and very understandable. I am not sure if that makes Scala superior to Haskell though, but it depends what is meant by "superior". From an upper management perspective, JVM languages and particularly Java will be the most comfortable choice, given how easy it is to hire the developers, find devops/sysadmin people who know how to deal with it, etc. Haskell is used when Java/Scala just don't cut it and the hand is forced to use something, well, er... "superior" and put up with the other problems around hiring people etc. Same argument for Hadoop vs. MySQL for example although that is more 'best tool for the job' oriented where has Haskell and Java are both multi-purpose.
Interesting points he makes. And glad he shows also his alternatives further down in the thread. The alternatives are rather obscure though. (but then one could also argue Haskell to be obscure) Thanks for pointing this out. :)
Excellent write-up, particularly because it exposed me to Snowdrift. I've spent the past few hours looking through the website's articles and a few of your blog posts, and the goals of the project really jive well with my interest of driving society towards a post-scarcity environment. I'm really impressed with the thought that has gone into this project. It was very motivating to see how much you've been able to accomplish in a world that had been previously alien to you.
It is structural subtyping.
Also want to clarify things here, there seems to be an misunderstanding as to why Scala doesn't have full TCO. There is no design reason in the language why Scala can't have full TCO, its **fundamentally a JVM security model limitation**. The JVM security model doesn't allow you to "jump" to another method/function without allocating stack. All local TCO that is performed by languages targeting the JVM always transform the call to a while loop to get around this. As far as I know, there is no language on the JVM that has full TCO. This includes Clojure, and heck, it includes Frege, which is a **Haskell clone on the JVM** (reference: http://stackoverflow.com/a/10031879/1519631) So complaining about Scala not having full TCO from a design POV is a bit disingenuous tbh. If Scala was targeting another backend (likely to happen in the long term with something like LLVM, when dotty gets released), it will most likely have full TCO. Saying that Scala doesn't have TCO is a valid criticism, but its also important (if you want to be objective) to state why, its nothing fundamental to the language.
ide-backend only works with the latest version of GHC and Cabal whereas ghc-mod supports them all the way back to 7.4 and Cabal-1.16. So as long as you're using the fpco-way^{TM} and use stackage you'll probably be happy with it but if you're like the rest of us and don't want to solve problems by restricting the problem space then ghc-mod is for you.
1) is `ghc-mod` usable with `stack`? Currently I get errors. 2) How does `ghc-mod` fit in with `haskell-mode` for Emacs? What does one do that the other doesn't?
I'm extremely excited about Happstack 8, but I have absolutely no idea where to go to find out about progress. The packages link to hub.darcs, but I think current development is on Github, and I don't see many updates about it or changelogs. :(
The syntax is sometimes counterintuitive, e.g. `:`, but that's not a huge deal. For me the real deal-breaker was the near-absolute lack of documentation or self-documentation. I could never make heads or tails of where any given value was coming from, or how the various functions work. I found writing any expression not slavishly copied from an existing model prohibitively difficult. There's a REPL, but since all the system scripts are lambdas I found it very difficult to extract meaningful information. To pick a random example: in [this part of the `ghcjs` expression](https://github.com/NixOS/nixpkgs/blob/fcb8d47fcd3398132da439f601f449fd9c7e9f05/pkgs/development/haskell-modules/configuration-ghcjs.nix#L58), where does `overrideCabal` come from? What are its parameters? Even if I infer that it's included from `.\lib.nix`, the [definition there isn't exactly enlightening](https://github.com/NixOS/nixpkgs/blob/fcb8d47fcd3398132da439f601f449fd9c7e9f05/pkgs/development/haskell-modules/lib.nix#L5).
I agree with your concern; our "bus factor" is not high. That said, /u/NiftyIon's comment reflects my experience: I can't recall ever creating a Github issue or pull request that was dropped or ignored. Library authors, and the community in general, tend to be outrageously helpful and cooperative.
It appears ghc-mod can not infer types about in-memory objects (`String` or `Text`) as it only operates on files. Would a patch providing type checking of values be acceptable? Would it be easy? Background for the interested: One of my always-back-burner projects is to make a type-safe, but multi-typed, genetic algorithm framework. The notion is you provide a set of primitives (ex: some of prelude, containers, and a set of domain specific types and operations). A graph is constructed showing all possible applications of these primitives from the input type to the result type. Each path between these vertices represents a possible solution. I believe this task would require a run-time type checker (to construct the initial digraph) and think ghc-mod could be used to that end.
Oh please enlighten me.
1) I suppose you could give this https://github.com/kazu-yamamoto/ghc-mod/pull/549 pull request a shot and report back, it's currently blocked because I have more important things to test but if I get enough feedback that will probably go into 5.3.1.0 which will be soon^TM. 2) Right now the ghc-mod Emacs frontend (aka. the `ghc` package on MELPA) is an extension to haskell-mode, we're working with the maintainers of the former to get ghc-mod support upstream though, see https://github.com/kazu-yamamoto/ghc-mod/issues/544
I agree 100% :-) and thanks for taking the time to write this all up! 
&gt; How would you know what to create (Some/None) without actually looking at the value? When you generate the value you already know that it can't be null. When you interact with a Scala library it's conventional to assume that it doesn't produce nulls, because otherwise that library goes against Scala idioms hence it is its issue. &gt; The null check has to happen somewhere, especially when you are interacting with a language like Java (which is natural given that quite a few Scala libraries wrap Java libraries). Interacting with other JVM languages is exactly the purpose of the `Option` function. But when the value does not come from Java and you're already sure that the thing is not null, to perform that check that's just ... I can't believe I have to prove that doing redundant things is wrong. 
Your `if-then-else` example is exactly what I had tried before and I was confident it was correct because I've yet to have been led astray by `haskell-mode` in emacs. However, it was the `RebindableSyntax` in my cabal file that was leading me astray. Once I removed it, this worked. As for the second example, you are also right there as well: I first tried the simple version before realizing that the `IO Bool` would make it a bit more involved. As for the latter question about monads, I was not too happy with that chunk of code and I think I'll need to practice re-imagining it. I think this version is very imperative: it's the kind of thing I would have written in Python at some point (although I don't think I'd be happy with it in Python either). My issue, I think, is that I haven't thought about the issue in terms of data and contexts that would model it, and while I'm slowly getting more comfortable with Haskell, thinking first in terms of data and context instead of action is a very difficult habit to break. Thanks for the feedback. I appreciate it.
I agree with most points (and I'll make the necessary changes) but I want to spend a little more time on "refactoring" and "package management" to explain my reasoning behind those two rankings. I find Haskell extremely pleasant to refactor even without an IDE. That does **not** mean that I think an IDE would be useless; on the contrary I think that refactoring in Haskell would be even more pleasant with an excellent IDE. However, I believe that even in the absence of an IDE Haskell's refactoring story is still preferable to refactoring in another language with a proper IDE. The biggest bottleneck that I experience while refactoring in other languages are adding and updating tests and even when I'm very diligent about testing I still don't have the same degree of confidence that I do when refactoring Haskell code. The speed at which I edit code is low on my list of priorities while refactoring and correctness is very high priority. After all, the purpose behind refactoring is to pay down technical debt, not increase it. Haskell's significant advantage in refactor correctness was the primary reason behind my "Best in class" rating. The package management story is amazing now. I know that it's very hard to believe that things have improved so dramatically within the span of a few months but they really have. Most of the complaints that you expressed about `cabal` have been solved by `stack`, including: * All projects are sandboxed by default, including the compiler which is complete isolated just like another dependency * Guaranteed absence of build failures when using packages curated by Stackage (Seriously!) * Stack caches shared dependencies (and isolated compilers) between multiple sandboxes in a global cache to avoid wasteful rebuilds * User-friendly command-line API I've personally used `stack` for many projects and I've had excellent experiences with the tool. I've even been able to build libraries and projects which I used to deem impossible to build with zero issues. Just spend a few minutes playing with the `stack` tool on a project that you consider difficult to build and you'll be immediately convinced.
You *might* just be underestimating `turtle` a wee bit. ;)
`Cabal`, not `cabal-install`.
You can get a fresh cabal file by typing `cabal init` in a directory with no cabal file. It will even do smart things like guessing existing module names and source directories.
I haven't found the lack of TCO to be a problem with Clojure. Note that Clojure *does* have non-stack-consuming recursion via loop/recur which the compiler translates to an efficient loop. Scala does something similar. Either way, it is perfectly possible to have decent FP capabilities on the JVM. There is the case of mutual recursion that isn't covered by this, but in practice this has been sufficiently rare that I've never needed TCO for this case. And if you really do need it, you can always use a trampoline.
All of my packages were installed with profiling enabled. The problem is the base packages. This is why I tried to compile GHC from source to begin with
It uses both Cabal and cabal-install. See https://github.com/commercialhaskell/stack/blob/master/src/Stack/Solver.hs#L78
Still blows my mind how little I had to care about package management with clojure, it was such a non-issue that I didn't realize that was a problem until I started doing Haskell. Luckily stack seems to have made most of our woes go away.
I compile ghc from source with the default options and I've never had trouble profiling. I do `./configure` and it says something like "to compile with default options (full optimization with profiling) type (g)make".
Try https://hackage.haskell.org/package/digestive-functors-aeson
This is a significant shortcoming of existing Nix culture, but I don't think it's necessarily reflective of a technical problem. The language includes syntax for comments, and including documentation in a standard attribute would make `nix-repl` 1000x more useful. My hope is that a growing population of right-minded users can steer things in a better direction.
Come hang out on #snowdrift on freenode!
My complaints: * Equational reasoning not generally possible (subtyping, OOP, ad-hoc overloading, side effects, different binding strategies) * Functions do not compose well (syntactically) * Type inference often fails within expressions, forcing a rewrite or addition of noisy type annotations * null (ofcourse) * Untyped side effects and non determinism * Mixing &lt;object&gt;.&lt;method&gt; with &lt;function&gt;(&lt;object&gt;) gets awkward About the last point: If you write a new function over, say `Seq`, then this function will be called in &lt;function&gt;(&lt;object&gt;) style, because one cannot add methods to classes. Now chaining your function together with the existing methods will be ugly, something like: yourFunction(s.map(f)).filter(p) This hurts readability. EDIT: spelling
By forcing cabal install to choose an older version of a package, which had bugs. 
Here's an example of what you presumably mean about only searching for the current type. Again, one of zillions. ["Implicit search does not guide type inference to find a supertype"](https://issues.scala-lang.org/browse/SI-9169) Anyone genuinely interested in learning about the problems here would do well to spend some time paging through the [hundreds of open issues involving implicits](https://issues.scala-lang.org/issues/?jql=status%20%3D%20Open%20AND%20text%20~%20implicit%20ORDER%20BY%20key%20DESC).
Tried that too. Still can't get the `p_dyn` versions...
Here's an example of usage https://ocharles.org.uk/blog/posts/2012-12-02-digestive-functors.html digestive-functors-aeson lets you use this kind of parsing with JSON data.
How's that looking? Is it in the 'mature' category, or is it more early adopter? 
Are you aware of [cabal-bounds](https://github.com/dan-t/cabal-bounds)? If you've started a new project, put your dependencies into the cabal file and preferably build your project in a cabal sandbox, then you can set the bounds (matching the current build) of your dependencies by calling: cabal-bounds update --ignore=base *.cabal dist/dist-sandbox-*/setup-config This will set the lower and upper bounds of the dependencies matching the [PVP](https://wiki.haskell.org/Package_versioning_policy). Normally you want to have more liberal bounds on `base`, therefore the `--ignore` here. And it's even usable for guys hating ;) upper bounds: cabal-bounds update --lower --ignore=base *.cabal dist/dist-sandbox-*/setup-config 
what OS are you running?
I feel that cabal is hard for people when first learning Haskell, before growing used to upper and lower bounds on packages. Once you get used to the flow of starting a new package, using a sandbox, adding some dependencies and building - cabal really opens up and becomes a friendly helper.
Two instances off the top of my head: * Using a new GHC. The package has dependencies that don't work with GHC 7.10 but that have been updated. The bounds stop me using the ones that work. * The consistent dependency problem. You cannot combine two packages requiring a different version of network, so if one wants 2.6 and one wants 2.7 you cannot import both, even transitively. 
Are you sure about `ide-backend` only working with GHC 7.10? I've just tried `cabal install ide-backend` on GHC 7.8.4 and it installed fine (haven't tried using it though). I'd be very surprised if it didn't work with GHC 7.8 as `ide-backend` was developed long before GHC 7.10 was released.
This one in cereal was kind of funny too: https://github.com/GaloisInc/cereal/issues/37 The reporter was kind of upset/frustrated that nothing has been done about it, especially since he made some nice tests for them: https://github.com/GaloisInc/cereal/pull/38/files
Will it give me p_dyn versions?
The thing PHP makes easiest is Web programming using the baked - in framework, but said framework is kind of bad. Any reasonably modern website is going to use Laravel or Symfony or whatever, which are *fine* but you could just as well use another, better language if any of them had the kind of zero - thought deployment options PHP has. When serving Web requests, the execution model is aggressively Ye Olde CGI, even if the interpreter is *actually* speaking FastCGI. There's no way to do your set-up work once and then start serving requests, you have to pay at least a portion of that cost every time unless you cheat with a C extension (which can, of course, do anything it likes, including make demons fly out of your nose). You can write the FastCGI server yourself, but at that point why are you even using PHP? `php` and `php-cgi` are actually different binaries, with different libraries available. I don't know if there's a single place you can go to find out what the full set of differences is. `mod_php`, which somehow still sees use in places, is yet another separate thing. Despite having anonymous functions, functions are not first class. A named function has to be referred to with a *string*, and that string had better include whatever namespace it's defined in even if it's available unprefixed in the local scope. Methods are even weirder, requiring a specially shaped array. Anonymous function syntax is `function ($formals) use ($environment) { statements; }` with by - value capture of the environment, unless you prefix the captured variable with `&amp;`. Unlike even C++, there's no way to get the language to infer the captures, unless you don't want to capture anything in which case you can just leave off the `use ...` bit. "Type hints" are a thing, but there was tremendous resistance to making them work with anything other than a class, leading to the infamous "variable required string, string given" error message. Arrays aren't. They are actually some kind of hybrid array / ordered hash / abomination that in my experience is mostly useful for confusing maintainers. `php.ini`: it's a thing, and it shouldn't be. It *definitely* shouldn't require non - default configuration to make two different PHP applications load different INI files. It probably shouldn't be the mechanism that loads C extensions either, because now you have to pay for loading up all of them you could possibly need every single time. There's no text type; strings are just mutable byte vectors. It's up to you to juggle character encodings. Making this worse, the standard library (or is it a first party extension? Hard to tell the difference) provides `utf8_encode` and `utf8_decode` which *sound* like they might be useful but in practice indicate that the poor programmer had no idea what they were doing. Presumably because they didn't realize text is Capital H Hard and missed the two week correspondence course in all the ways encodings on the Web in particular hate joy and wish pestilence upon the good. I could go on, but it's now past midnight so I'll stop there.
Based on my experience as a Hackage trustee, *install-plan bitrotting* due to leaving off upper bounds is a real and *huge* problem and a greater source of problems than too restrictive upper bounds. Here's an example for what I mean, consider this [matrix builder report from March for `persistent`](https://ghc.haskell.org/~hvr/buildreports/persistent.html). You may notice that from version `1.1.0.1` almost all combinations of GHC version and `persistent` version result in a build failure. But they surely have been building just fine until some point in time in the past when the bitrot started to happen. And if you look at [what changed from `1.1.0` to `1.1.0.1`](http://hdiff.luite.com/cgit/persistent/commit?id=1.1.0.1) you'll notice the cause: diff --git a/persistent.cabal b/persistent.cabal index 4a5abc3..c81cba6 100644 --- a/persistent.cabal +++ b/persistent.cabal @@ -1,5 +1,5 @@ name: persistent -version: 1.1.0 +version: 1.1.0.1 license: MIT license-file: LICENSE author: Michael Snoyman &lt;michael@snoyman.com&gt; @@ -32,22 +26,23 @@ library , time &gt;= 1.1.4 , text &gt;= 0.8 , containers &gt;= 0.2 - , conduit &gt;= 0.5.5 &amp;&amp; &lt; 0.6 - , resourcet &gt;= 0.4 &amp;&amp; &lt; 0.5 - , monad-control &gt;= 0.3 &amp;&amp; &lt; 0.4 + , conduit &gt;= 0.5.5 + , resourcet &gt;= 0.4 + , monad-control &gt;= 0.3 , lifted-base &gt;= 0.1 - , pool-conduit &gt;= 0.1.1 &amp;&amp; &lt; 0.2 - , path-pieces &gt;= 0.1 &amp;&amp; &lt; 0.2 - , aeson &gt;= 0.5 &amp;&amp; &lt; 0.7 - , monad-logger &gt;= 0.2.3 &amp;&amp; &lt; 0.3 + , pool-conduit &gt;= 0.1.1 + , path-pieces &gt;= 0.1 + , aeson &gt;= 0.5 + , monad-logger &gt;= 0.2.3 , transformers-base , base64-bytestring , unordered-containers , vector , attoparsec , template-haskell - , blaze-html &gt;= 0.5 &amp;&amp; &lt; 0.6 - , blaze-markup &gt;= 0.5.1 &amp;&amp; &lt; 0.6 + , blaze-html &gt;= 0.5 + , blaze-markup &gt;= 0.5.1 + , silently 
What put me off initially with HLearn was the absence of connections with the mathematical background. The "X has Monoid structure / Y has Functor structure" statements are a bit hand-wavy and didn't make me want to jump right in. Or perhaps I should just give it another go. How is your progress on it and SubHask?
Neat! I also remember a tool that increases (and decreases?) the version number of a dependency until the package no longer compiles, doing this for each dependency.
I've been meaning to try [roundtrip-aeson](https://github.com/anchor/roundtrip-aeson), which creates a parser and printer from the same applicative / alternative structure.
Looks quite promising. Thanks!
&gt; Drupal codebase is really well written Are you *insane*? The entire Drupal codebase is a rat's nest of function callbacks and not-invented-here syndrome. It takes PHP's culture to the next level by writing their own `DrupalRealFixed` versions of functions. I've also had a look at Drupal 8 to see if they cleaned up the code with the move to OO: absolutely *zilch*. Render arrays are still in ("let's just put everything that has to be rendered in an array and have contributed modules modify it ad-hoc, that'll work"), the form API is still archaic, etc, etc... The worst thing is that the concept behind Drupal is pretty elegant: provide a very sparse functionality that generates a data type A, and then expose it via a hook, which is a function of type `A -&gt; A`. Contributed modules can then hook into it and modify the A. Executing a hook is exactly `fold (.) id [hook1,hook2,...]`. Still, it made a lot of bad decisions as well. The idea of a content type is outdated and inflexible compared to what you can do in other, more modern CMS.
The implicit resolution would start from the builder type itself, `CanBuildFrom`. This does not define a companion object with specific type classes, so no general candidates here. It would then consider the inferred type parameters. `That` obviously is not known, but `This` is known to be `List[Int]`. So we'll look at the companion of `List`, and there we find implicit def canBuildFrom[A]: CanBuildFrom[Coll, A, List[A]] Where `Coll = List[_]`. So we have a suitable candidate and `That` will be resolved to another `List[B]`.
I'd definitely say early adopter. I've recently found two pretty severe GC-related bugs in the library (but they were fixed very fast by the maintainer!), and the support for advanced models (read: everything more advanced than a simple list model) is still missing. But the API is still quite nice to work with, so definitely try it out, but there'll probably be some surprises if you use it for bigger application.s
&gt; IIRC ide-backend is going to render ghc-mod obsolete That seems rather premature; `ghc-mod` is being actively developed, has a large user base, and [good support in atom](https://atom.io/packages/ide-haskell) (as well as in other editors, I just happen to use `atom`). 
Scala who?
But is it the fault of the package manager though? I don't think so. That's a language level thing.
/u/mightybyte pretty effectively addressed what I was going to say.
I'm not raising the issue out to be deliberately disingenuous, I'm raising it because it causes pain. This middle ground is quite treacherous territory for users. &gt; Saying that Scala doesn't have TCO is a valid criticism, but its also important (if you want to be objective) to state why As I said above, "I can understand why it makes this trade-off [...]". FWIW- Ermine's Java backend avoids being bitten by this. Of course it does so by using a spineless tagless g-machine interpreter, which is a pretty heavy weight solution and one that is totally unsuited to Scala.
Yes, but like others said, I think this functionality needs to be in cabal-install. And frankly, those cabal-bounds command lines are horrible. They're a pain to type and hard to discover. Having this functionality inside cabal-install alsa paves the way for us make "cabal upload" or "cabal sdist" check for missing upper bounds and issue a warning that tells the user they should run "cabal gen-bounds". This VASTLY improves discovery for all users.
Once you have all your bounds set properly I never found bumping individual bounds to be difficult enough to merit a separate tool. The one case where it would be is when you want to check all your dependencies to see if any are out of date. But for this there are several existing options. There's [packdeps.haskellers.com](http://packdeps.haskellers.com/) and also a command line tool called [bumper](http://hackage.haskell.org/package/bumper). We can also certainly extend gen-bounds to be more powerful in the future. This is really just a proof of concept to get the ball rolling.
I've just realized that something's wrong with this subreddit... I can't access my post (page not found), but it's on the list and has comments page. It's the same for many other posts on this subreddit...
I understand State monad as something encapsulating a transformation function (which gets built incrementally as one chains different actions with `bind`) which is eventually presented with some initial value that is then passed through the composition of functions. I hope that's right, although it kind of gives me a headache because then it is not that easy to imagine a state being passed from one action to another, even if in the end of the day these two things are the same.
I have found `lens-aeson` to be helpful.
I have given up setting upper bounds. Stackage is a much better solution to the problem that upper bounds are supposed to solve. 
&gt; And frankly, those cabal-bounds command lines are horrible. You're talking about the `*.cabal dist/dist-sandbox-*/setup-config` part, right? Well, yes, this part could be automatically detected, but it doesn't have bothered me that much until now, because I'm just having an alias.
Yes! That's exactly right. You could write a function pureReadLine :: State Int String pureReadLine = do put 1 return "hi" And use it like myStateComputation :: State Int String myStateComputation = do input &lt;- pureReadLine count &lt;- get modify $ \x -&gt; (show count) ++ ". you said: " ++ x return x that when run trough runState would return (1, "1. you said: hi") But yeah, to use IO you need a monad transformer.
I have written something similar for haskell before. It used vinyl for the record types. Relational algebra is awesome, and I wish there was a way to make it more first-class in haskell. If you haven't already read the research paper [The Power of Pi](http://cs.ru.nl/~wouters/Publications/ThePowerOfPi.pdf), you might like it. The third example shows a simple implementation of relational algebra in agda (which is dependently typed).
In its very essence, there are two "special" operations you can perform in a stateful context: get and set. In Haskell `State` monad terminology, that's `get` and `put`. * `get` is a monadic action that will get the current value of the state. * `put` is a monadic action that will store a new value in the state. Put overwrites whatever was previously in the state. For an example of these used, see: plusOne = do x &lt;- get -- get whatever is in the state put (x+1) -- and replace it with something one number bigger To see this in action, we need to know about another function: * `execState st start`. The `execState` function runs the stateful computation `st`, with the starting state `start`. It will then return whatever was in the state when the computation completed. If we attempt to `execState plusOne 3` we will get `4` back – this is in line with what we expect `plusOne` to do: it takes out whatever was in the state (3) and then adds one to it (yielding 4) and puts it back in the state, replacing the old value. We can call `get` and `put` however many times we want: plusThree = do x &lt;- get -- get whatever is in the state put (x+1) -- and replace it with something one number bigger y &lt;- get -- get whatever is in the state put (y+1) -- and replace it with something one number bigger z &lt;- get -- get whatever is in the state put (z+1) -- and replace it with something one number bigger If we run this with `execState plusThree 17` we will, unsurprisingly, get `20` back. That's because: x &lt;- get -- gets 17, which was the starting value put (x+1) -- replaces the starting value with 18 y &lt;- get -- get 18, which is the new value in the state put (y+1) -- and replaces it with 19 z &lt;- get -- will now get 19, which is the current value in the state put (z+1) -- replacing it with 20 Of course, you could completely ignore the starting value: stubborn = do put 15 put 29 put 3 When run with `execState stubborn 8`, this will return `3`. In fact, it will always return `3`, regardless of the starting value. It is only the last `put` call that actually affects the final state value. Similarly, it doesn't matter how many times you `get` something – as long as you don't `put` something else in between, you'll always get the same number. tetrating = do x &lt;- get y &lt;- get z &lt;- get put (x*y*z) If you run `tetrating` with `3` as a starting value, you'll get `27` back – that's `3*3*3`. Each `get` returned the value in the state, and the value in the state never changed between the calls. Now, going back to our `plusOne` definition... it looks a little silly to first `get` the value, and then immediately `put` it again. It would be nice if we could just run a function on the value in the state. We would like a function that lets us define `plusOne` as plusOne = do modify (+1) or in other words, "take whatever is in the state and run `(+1)` on it". This function already exists in the library, though if it didn't it would be easy to create: mutate f = do x &lt;- get put (f x) This means that `mutate` takes a function `f` as an argument, and returns a monadic action which, when executed, will get whatever is in the state, and then put it back again but with `f` run on it. This hints at one of the things that are so nice about State: it composes nicely. `mutate` is a stateful action, and it can be used in another stateful action, `plusOne`. Guess what? `plusOne` is also a stateful action that can be used in other stateful actions, like... plusThree = do plusOne plusOne plusOne If you run `plusThree` with `17` as a starting state you'll get `20` back. Why does this work? Well, take a look at this: plusThree = do plusOne -- same thing as "mutate (+1)" plusOne -- same thing as "mutate (+1)" plusOne -- same thing as "mutate (+1)" so if we expand the definition of `plusOne` we get plusThree = do mutate (+1) mutate (+1) mutate (+1) ...and if we again expand the definition of `mutate`, we'll see that plusThree = do do x &lt;- get put (x +1) do x &lt;- get put (x +1) do x &lt;- get put (x +1) which looks similar to our first definition. This means that State does *not* break all the useful analysis tools we are used to having in Haskell.
You can have a look at json-stream. It does get rid of aeson Value (if you wish so). The performance is (sometimes significantly) better than aeson's (unless you have heavily escaped strings; I didn't test this, but it will likely by extremely slow in this particular case).
Game programming section misses link to "Nikki and the robots". One of the most serious gamedev projects made in Haskell https://github.com/nikki-and-the-robots/nikki http://steamcommunity.com/sharedfiles/filedetails/?id=107105028
Might be worth mentioning that since the `StateT` monad transformer is not magic, you can code the function you want without using it at all, instead keeping your state as a function parameter. Sketch: type MyState = [String] main :: IO () main = loop [] loop :: MyState -&gt; IO () loop xs = do putStrLn ("State: " ++ show xs) x &lt;- getLine loop (x:xs) This is not much different from what the `StateT` monad transformer does, except that you must pass the state back to `loop` "manually." So if you're stuck you can always do the state passing yourself, and it's good to know what the state monad is actually doing. The state monad stuff is merely a convenient abstraction that hides this state passing, by embedding it in the bind operation, as you mentioned. And the state monad *transformer* makes it work in combination with other types of monads. When you want to use both state and I/O in one monadic action, that means you are using two different bind operations. The monad transformer machinery lets you do this in a pretty convenient way. You still can't mix the monadic actions arbitrarily. But the transformer provides a `lift` function that lets you convert, for example, an `IO a` into a `StateT MyState IO a`, which you can then bind together with other such stateful actions. Essentially you are embedding an I/O computation inside of your stateful computation. When you "run" the stateful computation, you get an I/O computation back, namely the I/O computation that was `(&gt;&gt;=)`'ed together "inside" of the state computation. That's kind of a hand wavey explanation. Feel free to ask more if you don't understand.
Thanks for taking the time to reply! I agree with your reasoning about refactoring; it just feels weird to say "best in class" when considerable improvements are just around the corner. About `stack`, I have used it, and I have been impressed by it, and I have run into many issues with its immaturity. Even in spite of this, it's an invaluable tool, but in another couple of months, I expect it to be even better. An example of where `stack` has failed me and I needed to fall back on `cabal` (/u/snoyberg, I don't think I reported this use-case _per se_): I have an unreleased version of GHC, which is 7.10.2 but patched to fix [this](https://ghc.haskell.org/trac/ghc/ticket/10438) bug, and I would like to use it for a particular project. [This](https://github.com/commercialhaskell/stack/issues/725) response leads me to believe that this is at least difficult.
Well, most of the time I write scripts, there is exactly one user, and he already has Haskell and stack installed on the one machine he will be running the script.
Yes, the point for me was to see through the abstraction of using the state monad. Because I knew it should work similar to the manual argument passing, I guess I only need to practise to unblock this in my mind :).
Neither of those things qualify as "screwed everything up". That's called the realities of software maintenance. For your first bullet, if the updated dependencies have minor version bumps, then your package will work with them automatically--that's the whole point of the PVP. If you were you using Stackage you wouldn't even get that much because Stackage locks everything down to a specific version. So Stackage is demonstrably inferior in that case. If the updated dependencies have major version bumps, then you can't expect them to automatically work. And more importantly, cabal can't automatically test that for you without choosing a build plan that is not known to work. This is precisely why ---allow-newer was created. Use it. Your second bullet has nothing to do with version bounds. Even if you leave off all bounds, GHC cannot be expected to work in this situation. So this is not a problem with bounds. It's a much deeper issue with GHC. This issue was described very clearly by Duncan in his post on [how to abolish cabal hell](http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/). The solution is "private dependencies", which is also being worked on. 
Could the tool not automatically append the bounds to your cabal file? In the same vein, I don't know why we don't have a `cabal install --save` like npm. I'd contribute that myself if I had the first clue how :p.
Not all package authors make big fix releases for their old code. Cabal respects the version bounds on hackage. It will not use a newer version. &gt; Stackage... forces... older version Eh? As far as I'm aware, the stackage team are very keen on relaxing unnecessarily tight upper bounds, so I'm not sure how that relates to them forcing old versions on folk, and iirc from the original blog post, they're keen to apply bug fix updates retrospectively. 
It can be interesting to look at the definition of `StateT` and to see if you can reimplement it and understand what's going on. But even without doing that, using stuff in practice is indeed the best way to develop practical understanding. :)
&gt; First of all I am confused where it is defined (all the other monads I know about IO, Maybe, List, Functions seem to be easily accessible to me) There are several implementations of it, depending on the monad transformer package you are using. Then there are two implementations (at least in `mtl` and `transformers`) depending on whether you want a lazy or a strict State monad. In either case, it's implemented as a monad transformer, which means that it has some additional complexity that you don't need to worry about if you just want to learn what it is right now. If you were to implement it yourself, you would need a type for it (obviously), like newtype State s a = State { runState :: s -&gt; (a, s) } So a value of type `State s a` is a *function* mapping from a state `s` to a tuple `(a, s)`. In other words, this function returns not only a result but also an altered state. That's the essence of a stateful computation. To provide a monadic interface, we need to implement `return` and `(&gt;&gt;=)`. The first is pretty simple, since the semantics of return (as by the monad laws) just roughly require that we can wrap something into this type while leaving everything else untouched. return :: a -&gt; State s a return a = State (\s -&gt; (x, s)) So, since a `State` value is a function, we also need to build a function here. We don't want to do anything with the state, so we just pack it into the tuple together with the input to `return` and we're done. Now we need a way to combine those computations, since that is what makes monads useful in the first place. This is where the magic happens. So we're given some sort of stateful computation `State s a` and a function `(a -&gt; State s b)` and want to combine this. What makes the state monad useful is that we use the state as produced by the first value and pass it on into the next computation! (&gt;&gt;=) :: State s a -&gt; (a -&gt; State s b) -&gt; State s b a &gt;&gt;= b = State $ \st -&gt; let (a', s') = runState a st in runState (b a') s' So `&gt;&gt;=` evaluates a stateful computation (lazily), and supplies the resulting state and value to the next computation and wraps it all back up in a `State s b`. And with those two, the monad instance is already complete. There are also a couple of handy functions for things of type `State s a`, most notably `get` and `put` which let you access the state (the `s` in the type). get :: State s s get = State $ \s -&gt; (s,s) put :: s -&gt; State s () put new = State $ \s -&gt; ((), new) From there you can implement `modify` like modify :: (s -&gt; s) -&gt; State s () modify f = get &gt;&gt;= \s -&gt; put (f s) and so on. The real implementations look a little bit more complex due to being monad transformers but they're pretty much the same.
Which university was this?
The state monad is about computations that may read and transform a state of type `s` and return a value of type `t`. We represent those computations with the following type: type StatefulComputation s t = s -&gt; (s,t) It takes the state `s` as input and returns a new state and an output `t`. The simplest one is the `return x` operation that does nothing to the state and puts `x` in the output: return x = \s -&gt; (s, x) The `get` operation puts the state in the output, and leaves the state alone: get = \s -&gt; (s, s) The `modify f` operation applies the function f to the state, and puts a dummy value `()` in the output: modify f = \s -&gt; (f s, ()) Okay, so we've got these three operations, now how do we chain two of them together? Well, suppose the first one is `stateful1 :: s -&gt; (s,t)`, and the second one is `stateful2 :: s -&gt; (s,q)`. The `stateful1` us an output value `t`, and we want to be able to use that in the `stateful2`. So we'll give `stateful2` an extra input which will be fed the output value of stateful1: stateful1 :: s -&gt; (s,t) stateful2WithInput :: t -&gt; s -&gt; (s,q) Now how do we chain these together? We want to end up with stateful3 :: s -&gt; (s,q) Here's how: stateful3 = \s -&gt; let (s',t) = stateful1 s in let (s'',q) = stateful2WithInput t s' in (s'',q) We first call stateful1 with the state `s`, and we get a new state `s'` and an output value `t`. We feed those into `stateful2WithInput`, and this gives us a new state `s''` and an output `q`, and we return those. We can summarize this in a combinator: bind stateful1 stateful2WithInput = \s -&gt; let (s',t) = stateful1 s in let (s'',q) = stateful2WithInput t s' in (s'',q) This is the `&gt;&gt;=` operator: (&gt;&gt;=) m f = bind m f We can use it to chain together stateful computations: chained :: Int -&gt; (Int, String) chained = modify (+ 2) &gt;&gt;= modify (+ 1) &gt;&gt;= get &gt;&gt;= \s -&gt; return (replicate "Hello" s) Do notation makes this look a bit more familiar: chained = do modify (+ 2) modify (+ 1) s &lt;- get return (replicate "Hello" s) We can run this by giving it an initial state: output = chained 0 The produces: output = (3, ["Hello", "Hello", "Hello"]) We've got the output value `["Hello", "Hello", "Hello"]` as well as the last state `3`. In the Haskell state monad we have a function `runState` which feeds in an initial state and returns the output: runState s stateful = stateful s
&gt; In effect, what stackage does is set even more extreme upper bounds...i.e. fixing you to one specific version of every package. False. How am I fixing you to a single version of the package when I say "text &gt;= 0.11"? The purpose of upper bounds is to ensure buildability. Stackage does this, with far less work on my part. If you are wondering what versions of other packages are required to build my package, look at the Stackage set it was built with. You can do this even if you do not use Stackage. Then you can manually add bounds using the `cabal` command line options if you want.
I have once written a tutorial on `State` in a context of how it's applicable to parsers. http://nikita-volkov.github.io/a-taste-of-state-parsers-are-easy/
State is defined as (simplified): newtype State s a = ST (state -&gt; (state,result)) That is: you take a state and return a result, plus a new state. The newtype is just noise so that you can implement Monad and friends on it. The point of this is that you move a hidden parameter `state`, which you can access and change, along with you. Note that this hidden parameter means that a value `x :: State s a` would actually be a function. Let's look at the basic state-related functions get :: State s s get (ST f) = ST (\s -&gt; (s,s)) put :: s -&gt; State s () put newstate (ST f) = ST (\_ -&gt; (newtstate, ()) Ignoring the newtype-noise for a moment, `get` creates a function that takes the state and returns it as a result. `put` takes a state (newstate) and creates a function that ignores the old state and returns (newstate, ()). When you chain values with &gt;&gt;=, or &lt;*&gt;, or &gt;=&gt;, you're always chaining functions. Let's look at &gt;&gt;=: (ST stateFunc1) &gt;&gt;= f = ST chained where chained initS = let (intermediateS, res1) = stateFunc1 initS (ST stateFunc2) = f res1 (finalS, res2) = stateFunc2 intermediateS in (finalS, res2) It's a bit like (.), but you have to do some leg-work to shuffle the s-parameter along. Graphically (in 4020p, cine-alta ASCII-art™): initS ---&gt; stateFunc1 --+--&gt; intermediateS ----------+ | | +--&gt; finalS | v | +-----&gt; res1 ------&gt; f --&gt; stateFunc2 --+--&gt; res2 It might look complicated, but you just have a function (stateFunc1) that delivers a tuple (intermediateS,res1), and then you append a second function f that can inspect both intermediateS and res1 before it returns a second tuple (finalS, res2). Any value of type `State s a` is just a chain of functions, each of which delivers a tuple which the next one can read. The utility of it that one component of the tuple is "hidden" and gets moved around automatically by &gt;&gt;=, making your life simpler. State-chains can then be discharged by supplying an initial state via runState: runState :: s -&gt; State s a -&gt; a runState s (ST f) = f s runState is just like ($), but the newtype obscures this. **Addendum** If you want IO plus some state you wish to maintain, you need `StateT IO s`. StateT is like State, but it takes an inner monad and plumbs the whole thing together. get and put will work in the same way, but if you have a function of type `IO a`, you'll have to lift it into `StateT` via liftIO, like so: main :: IO () main = runStateT 1 main' main' :: StateT IO Int () main' = do line &lt;- liftIO readLine if line == "exit" then liftIO $ putStrLn "Exiting. Have a good day." else do st &lt;- get put (st+1) liftIO $ putStrLn $ "You entered: " ++ line liftIO $ putStrLn $ "The loop has run " ++ show st ++ " times." You can go fancy with monad-loops and whatnot, but this is pretty much the most basic way in which you can write a stateful IO-loop.
1+1=11
You don't have to ask users to install Haskell or Stack. Just compile the script and give them a binary. As a bonus you get a fast startup time.
&gt; It's like not fixing a buffer overflow vulnerability in your code because you're not using it in a way that makes it exploitable No, it's not. My software is not vulnerable merely because I have stopped adhering to a PVP [religion](http://www.yesodweb.com/blog/2014/11/case-for-curation) that would have me spend time bumping bounds. I apologize for the fact that the software I have worked on and given to the world for free is, in fact, causing suffering.
Where did state and get come from?
Me too! I'm really anxious to get my hands on this…
what if it's called "maintenance" instead of "refactoring"?
I remember haskelldb originally tried to collapse subqueries into joins because none of the free databases did subqueries well, but it was too much for them and so they stopped trying and ripped all that out. I looked at it myself years ago, decided the problem was too complicated for me to solve, and started drinking. As for mysql/mariadb, on mariadb 10, even a simple subquery like the first one in opaleye's tutorial generate multiple derived tables with no keys, which is about as bad as it gets. This is despite pages like [this](https://mariadb.com/kb/en/mariadb/derived-table-merge-optimization/) which strongly suggest this problem should be solved, it sure doesn't seem like it in my tests.
Of course I can do that, but then it's technically not a script anymore; at least not according to my definition of "script". A script should be easy to modify, and should not require a manual compilation step.
Doesn't that rule out things like bi-infinite lists (church encoded version doesn't give access to the last element, nor is it's version of `tail` very performant) and trees (filtering would have to forget any internal structure when converting to a list). There is a church-encoded version of a Foldable FWIW, which is `forall m. (Monoid m) =&gt; (a -&gt; m) -&gt; m`, though it doesn't address everything you want it to (just the `foldMap` function). Though the main reason Foldable has so many methods is that even with fusion a lot of datatypes have *much* more efficient versions than are implementable with the minimal Foldable interface. `Vector` for instance gives O(1) performance for a lot of operations, and prior to Foldable collecting so many methods there was a lot of API duplication for things that were functionally the same, yet more efficient. I wish there was a better mechanism for this particular kind of ad-hoc overloading, since there's no upper bound on the number of functions we might want to provide more efficient versions of for all Foldable datatypes, but sadly there isn't. I previously thought rewrite rules would fit the bill, but they aren't well-behaved enough for people to rely on.
`Tail` is bad in the sense it is `O(N)`, but it is not actually a problem since you almost never need it anymore (you already got structural recursion). &gt; Though the main reason Foldable has so many methods is that even with fusion a lot of datatypes have much more efficient versions than are implementable with the minimal Foldable interface. Makes sense. &gt; I previously thought rewrite rules would fit the bill, but they aren't well-behaved enough for people to rely on Why?
I knew there was another issue I forgot to ping about https://github.com/commercialhaskell/stack/pull/798. That will allow you to add the relevant path to your stack.yaml. Honestly, I wouldn't advise doing that, but would instead recommend just modifying your PATH. Either way, using a custom GHC should be trivial (I know, I use one regularly).
Well you might actually just need the `Tail` of something a single time, but I guess it is pretty useless in general, indeed. Thanks for the link!
Don't hesitate to ask if you have any question.
&gt; You said that I'm locking users down to a specific package version. I did not say that. I said, "Using stackage...locks your build to one specific version", which is exactly what it does. &gt; I maintain software. For free. And if you want your libraries to be as usable as possible for the most people you need to tell them what versions of its dependencies they can expect it to work with. And version bounds are the mechanism that you use to do that. You can use whatever tools you want to build. If stackage works well for you great...as long as you also supply accurate upper bounds. If you want to publish poorly maintained code on github, go right ahead. But it's in the best interest of the Haskell community to hold hackage packages to a higher standard so that people who try to use Haskell have a good experience. &gt; Since I use Stackage, my package has a known build plan. But you are not communicating that to your users so it doesn't help anyone except you. &gt; A hornet's nest of upper bounds does not get you a known build plan. Yes, that is exactly what it gets you. And it does even better than that. It gets you a whole bunch of possible build plans that make your package work for more people in more situations. I showed you a clear example of the bitrot that happens when you don't supply upper bounds. The damage is real whether you choose to believe it or not.
IMHO, the Haskell community suffers the benefits and problems of a disproportionate number of people from other language communities that are learning Haskell for long time, many of them looking for how to improve their skill in the other language: great expectation, a lot of influence, low software productivity, amateurism and excessive attention to relatively irrelevant but aestetically appealing things like lens. 
It's not vulnerable, but it can break builds; I was just making the first comparison that came to mind. The fact that you have given it to the world for free does not absolve you of responsibility for it; yes, it's very kind of you, and yes, you have no real obligation. Still, I choose not to treat the harm my software causes and the help it causes as balancing out. If I, for example, release software that runs `rm -rf $HOME` on 1/1000 machines (accidentally, of course; malice is a different story) but also solves some major annoyance that affects thousands of users, I don't feel like one cancels out the other, so I feel an obligation to mitigate the harm by fixing the bug. Similarly, breaking someone's build and forcing them to either change their workflow or mess around with `cabal` esoterica is a minor harm, and providing software that is useful to them is a (probably) major help.
&gt; Well, of course, Foldable is not powerful enough to write them all. We can write sum and length, but not filter, for example. The issue is that it doesn't allow us to build the original type back. That is, we could actually write filter for any Foldable - but it could only return a List with the filtered elements, not the original type Could this be used to expand the Witherable typeclass to consider Foldables too??
See, there's that pattern again...
But `filter . toList :: Foldable f =&gt; f a → [a]`, what I said is impossible is `ffilter :: Foldable f =&gt; f a -&gt; f a`. I think invoking even more classes is making it too complex.
[Currently trying to learn haskell and found this article quite helpful!](http://brandon.si/code/the-state-monad-a-tutorial-for-the-confused/) 
&gt; I wrote this chunk ... I find LambdaCase can often make those monadic cases look more reasonable. findModule pythonpath importpath &gt;&gt;= \case (Just pymodule) -&gt; return $ Just pymodule Nothing -&gt; findPyObject pythonpath importpath &gt;&gt;= \case (Just pyObject) -&gt; return $ Just pyObject Nothing -&gt; findPackage pythonpath importpath &gt;&gt;= \case (newpath, remainder) | null newpath -&gt; return Nothing | otherwise -&gt; locateModule newpath remainder 
You might have fusion happening and so no heap allocation is happening (you can look at core to find out). Or you might just be observing the entire nursery fitting into your L3 cache https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC
`Fold` given there isn't all that comparable to `Foldable`; it is something much more specific. `Fold h` is isomorphic to `[h]`. It is something that rules out using `Foldable` to walk trees, sets or the values in a map or other containers. Most of our `Foldable` containers are *not* isomorphic to lists they merely have some total ordering you can put in the values inside of them. It is also something that inherently assumes a left-to-right bias that costs you a logarithmic factor on many operations on many containers.
&gt; The fact that you have given it to the world for free does not absolve you of responsibility for it I have not released software that runs `rm -rf` or anything like it. If you want to get your software to work in the flawed regime of PVP upper bounds, go right ahead. &gt; breaking someone's build and forcing them to either change their workflow or mess around with cabal esoterica Force? Was it a gun, or a knife? Look, if you're going to continue to use stone tablets even after the ballpoint pen is invented, I'm not going to make that my problem.
(i've never use this library, but) can you link to some examples of generated code and the "macros" that generated them? (I assume template haskell). I've been interested in that stuff lately, and this seems like a large scale use.
inb4 jokes about haskell is for rocket scientists
&gt; Look, if you're going to continue to use stone tablets even after the ballpoint pen is invented, I'm not going to make that my problem. It's not just that I'm using stone tablets (I'm actually using `stack` as much as possible), but that a considerable fraction of the community still is. Not accommodating common, if outdated, usage patterns weakens your contribution and inconveniences some of _your_ users.
&gt; There seems to be an abundance of jobs in software so maybe learning Haskell is a good place to start? Sadly, Haskell jobs are still quite rare at the moment. Your C++ experience will be much more helpful when looking for jobs. That being said, the fact that Haskell is very different from C++ makes it useful to learn if only because it will allow you to have a broader set of potential solutions to consider than if you only knew the C++ way. See for example [my video on combinator libraries](https://www.youtube.com/watch?v=85NwzB156Rg), intended for Haskell beginners, in which I demonstrate how Haskell-style solutions can be useful in other languages.
My degree is in EE, but I never used it. I went straight into software. In school I loved VHDL and Verilog and what struck me about Haskell over C-likes is that pure-functional is very similar to those (i.e. take simple pure components and compose/abstract them to make a complex system). Also, as EEs we're not afraid of math (at my school, EE is the most math intensive engineering degree), so it helps to have a respect for math when you want to dig into the theoretical aspects (though it is not required). Haskell has a lot to teach and I think everyone should learn it, but I think it will especially resonate with EEs, so in short: Yes.
Learning a lot from [Yet Another Haskell Tutorial](https://en.wikibooks.org/wiki/Yet_Another_Haskell_Tutorial). The language design so far absolutely screams 'Here Be Mathematics Majors'.
True, but in fairness I've never been overstocked with rope.
I think Haskell is a very beautiful engineering project, if nothing else. It translates a mathematic model of computation into efficiently executable machine code. Correctness is built into the culture. You might want to look at it just for inspiration: software can be much more well-defined and rigorous than the typical C++ project.
For more from a mathematics perspective, I highly recommend [*The Haskell Road to Logic, Math and Programming*](http://fldit-www.cs.uni-dortmund.de/~peter/PS07/HR.pdf).
On my phone, so I can't go into lots of detail, but I can share a link to where I've started collecting since things: https://github.com/commercialhaskell/stack/wiki/New-to-Haskell I'm not at all satisfied with it, but it's hopefully at least a good starting point for discussion. I'd definitely like to get input from users at various levels of experience with Haskell, and ideally we'd have multiple authors contributing to this to better cover our bases.
I think it's hard to completely separate concurrency from distributed computing when trying to give a best in class score. 
Thanks all! I will take go ahead with the [edx course](https://www.edx.org/course/introduction-functional-programming-delftx-fp101x-0)!! 
Fun fact: in SML (which is something like Haskell's first cousin, it shares a common ancestor and is similar in many ways), the `.` operator is actually `o`. So you write things like: val h = f o g And it means exactly what you'd expect. 
&gt;&gt;&gt;&gt;&gt; Precisely how did too-strict upper bounds screw things up? &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; By forcing cabal install to choose an older version of a package, which had bugs. &gt;&gt;&gt; &gt;&gt;&gt; That is precisely why the PVP exists. It allows us to bound on a major version and get all the bug fixes for the version we're currently using automatically. &gt;&gt; &gt;&gt; Not all package authors make big fix releases for their old code. &gt; &gt; If the package author doesn't make a bug fix, nobody gets it. Not cabal, not stackage...nobody. So that argument is not applicable here. Sorry, I think perhaps I was unclear. By "old code" I meant superseded versions. Some authors make their bug fix release and their version bump the same. You might be tempted to do this, for example, if a bug report caused you to decide you had a fundamentally broken approach, and you might fix the bug with a massive refactoring. You'd then upload the shiny new one with a bumped major version because if the radical change in implementation and depreciate the old version. A dependent package that didn't update its version bounds would cause cabal install's solver to choose your old version (despite the deprecation, apparently) over the newer version, even if the new version didn't change the effect of any function used by the importing package. &gt;&gt;&gt; Stackage is actually the one that forces you to choose an older version, not cabal. &gt;&gt;&gt; &gt;&gt; Eh? As far as I'm aware, the stackage team are very keen on relaxing unnecessarily tight upper bounds, so I'm not sure how that relates to them forcing old versions on folk, and iirc from the original blog post, they're keen to apply bug fix updates retrospectively. &gt; &gt; Here is the cabal.config for the current stackage nightly. Notice how every package is fixed to a single version. You only get a big fix when it makes it into stackage, which is centrally managed. Yes, for a fixed date, you get a fixed set of package versions. Stackage aims for permanent repeatability of co-compilability. I had a look at the version numbers on the Nightly you linked to, and although I really didn't check that many, all but one were at the latest version. The one I found that wasn't was attoparsec 0.12.1.6, where the most recent version is 0.13.0.1. Why is this? It'll be because one or more of the other packages on stackage have an upper bound of attoparsec &lt;0.13, either because you can't co-compile them, or because the upper bounds are out of date. &gt; This is demonstrably inferior to hackage where I would get the fix as soon as the author releases it. Here's the thing. attoparsec is a package I use, not all the time, but it's certainly there. If I installed attoparsec via cabal install, then I won't be able to install something else (don't know what - I've spent too much evening looking at dependencies already!) that I _can_ install if I use stackage, and I won't be able to install that package until it's updated or I unregister attoparsec or I nuke my cabal again. Maybe I'll never install any package that was pinning attoparsec's version back - I'm hoping it's not an upper bound that's too strict ;) - but I can live with that, whereas I had got heartily sick of cabal install taking a long time and then failing. You called stackage "demonstrably inferior", but I'm afraid I have to disagree - in my experience, stack and stackage have proven very superior to how things were for me beforehand. Cabal install optimises for the most recent version that you can currently install with what you previously installed. Stackage optimises for the most recent version that you can install with any other package on stackage. &gt; It's also less scalable. With stackage the central management limits you. With hackage/cabal the whole thing is crowd sourced and therefore can be expected to scale much better. So stackage is out of date only as far as aids co-compilability, or if I deliberately use LTS if I want to fix versions for longer. It's a fairer criticism if you say it doesn't cover all of hackage. It's not too tricky to add something from outside the collection, though, so there's a limit to how limiting that is. The way I see it, stackage gives me a much larger set of co-compilable packages than I ever managed to accumulate using cabal install alone, because I re-use someone else's solution to the consistent package set problem. That's a more useful scalability for me personally than the scalability you're talking about.
Holy cow, I just barely wrote one. What do you think? http://seanhess.github.io/2015/08/04/practical-haskell-getting-started.html
Haskell is a great language, but do not learn as a pathway to getting a job. 
I just cross-posted asking for feedback here: https://www.reddit.com/r/haskell/comments/3hi3jz/practical_haskell_tutorial_series_for_beginners/. Currently 4 articles in the series. 
This is nice, but I would offer the following changes: - use `stack new` to create a new project - it populates a directory with a project skeleton - in ghci, just use `:l Main` to load a module (no need to specify a directory and `.hs` suffix) Might also mention that you can use tab-completion on module names. 
Ok, I submitted a pull request for a simpler template. https://github.com/commercialhaskell/stack-templates/pull/2. 
&gt; software that runs rm -rf $HOME on 1/1000 machines Hehe. I think you may have used `&lt;/hyperbole&gt;` a little early.
Here is an example I ran into today when helping with a Scala question at Twitter: &gt;&gt;&gt; import com.twitter.algebird.Monoid &gt;&gt;&gt; sealed trait TestTrait &gt;&gt;&gt; case object ValueA extends TestTrait &gt;&gt;&gt; case object ValueB extends TestTrait &gt;&gt;&gt; val a: List[Map[_, Int]] = List(Map(ValueA -&gt; 1), Map(ValueB -&gt; 2), Map(ValueA -&gt; 5)) &gt;&gt;&gt; Monoid.sum(a) &lt;console&gt;:13: error: Cannot find Monoid type class for Map[_, Int] Monoid.sum(a) &gt;&gt;&gt; val a: List[Map[TestTrait, Int]] = List(Map(ValueA -&gt; 1), Map(ValueB -&gt; 2), Map(ValueA -&gt; 5)) &gt;&gt;&gt; Monoid.sum(a) res3: Map[TestTrait,Int] = Map(ValueA -&gt; 6, ValueB -&gt; 2) In this case the `Monoid` implicit it is searching for is parametric in the `Map` key type `K`: implicit def mapMonoid[K, V: Semigroup]: Monoid[Map[K, V]] = new MapMonoid[K, V] Yet, the compiler can only discover that implicit when you explicitly annotate the key type. I'm not sure if this is a compiler bug or not, but this is a case of where Haskell's type inference and type class resolution is better behaved than Scala's (pretending that Scala's implicits are somewhat analogous to Haskell's type classes).
I'd throw out there that ARM support in ghc is 'immature' or maybe even 'early adopter'. It works, but there are bugs and it can be difficult to get complex projects to build. The only distro I've tried with a working ghci is arch, and that requires downgrading llvm, and the packages for downgrading aren't officially available. 
&gt; The type system is definitely "Best in Class" rather than "Mature" I think "mature" is perfectly appropriate unless/until Haskell gets full dependent types.
It's dishonest to not acknowledge `cabal` as the standard tooling which `stack` claims to replace (you merely mention it as an unimportant sidenote in passing). And then you'd need to explain why the absolute beginner should use `stack` rather than the established standard `cabal` tool he'll see mentioned everywhere else...
What license are these tutorials under?
Even for absolute beginners? To me it seems like when mentoring a beginner in real life I would just recommend something to them and run with it. It would probably be super helpful to link to a discussion of trade offs though. 
warm up the JVM, from a running SBT session your build should go from 47 seconds to 1 second. Also, yeah, i5 without SSD or compiling to ram disk will slow things considerably.
&gt; Once you have all your bounds set properly I never found bumping individual bounds to be difficult enough to merit a separate tool. To be fair, rounding the major version up and prepending `&amp;&amp; &lt;` is arguably much less taxing for package authors than testing their package against multiple versions of dependencies. _Writing_ PVP compliant upper bounds isn't terribly hard, whereas perhaps some folk would appreciate tool help with _mainintaining_ them. If upper bounds can be correct without effort, it'll help win the argument you're having over their existence.
A `filter` generalized for `Foldable` has existed in the [wiki](https://wiki.haskell.org/Foldable_and_Traversable#Some_trickier_functions:_concatMap_and_filter) for ages, but for the sake of education, lets get there from "first principles". The key to the puzzle is to realize that since `Foldable f` doesn't give us a way to create more `f`s, the codomain for `filter` should be some other type. We start with module FoldableFilter where filter :: (Foldable f) =&gt; (a -&gt; Bool) -&gt; f a -&gt; g a filter p = foldMap f where f x = if p x then undefined else undefined and we learn that we need a constraint for `(Monoid (g a))`. Let's add that. We can even use `memepty` for the false case in our if. By similar reasoning, if we had `Applicative g`, we could use `pure x` for the true case. filter :: (Foldable f, Monoid (g a), Applicative g) =&gt; (a -&gt; Bool) -&gt; f a -&gt; g a filter p = foldMap f where f x = if p x then pure x else mempty this is a somewhat awkward set of constraints, though. Searching around a bit we can find a single type class with the right methods, `Control.Applicative.Alternative`, alongside a newtype wrapper `Alt` to make the `Monad` instance evident filter :: forall f g a. (Foldable f, Alternative g) =&gt; (a -&gt; Bool) -&gt; f a -&gt; g a filter p = getAlt . foldMap f where f :: a -&gt; Alt g a f x = if p x then Alt $ pure x else mempty 
Maybe I will rename that section to "Single-machine concurrency" since that's what I had in mind when I wrote it
Liking the `&lt;` and `&lt;!` a lot. It does a lot to inform about what the number means.
&gt; This view wouldn't even allow an instance for Map, I believe. Why not? Fold in {ascending, descending} key order.
I think you should start by learning Python. It has very readable syntax which will enable you to learn more advanced programming concepts fast. It is also a general purpose language and it's easy to find jobs. Start learning functional language (Haskell) once you're comfortable with Python.
Absolute beginners _don't_ need a lengthy debate, nor do they need a history, trends or statistics, they just need clear, engaging simple instructions. Yes, skip the reddit argument over stack vs cabal-install, and explain how to use the one you think is easiest for them to get started with. If you like, end with a flag for other tools to learn about, but it's really important not to get bogged down in issues that aren't going to affect beginners. In my view, the benefits of stack are of immediate benefit to users who aren't uploading packages - ease, speed and reliability. The folk that argue strongly against it worry that stack may encourage them to omit upper bounds on their version dependencies when they're uploading their own packages to hackage. That's **so** out of scope for a "getting started" guide.
It makes sense to have an asymmetrical notation for an asymmetrical binary operation that's often flipped.
How about adding details on how to maintain stack package-db? I mean the `stack setup` and `stack update` stuff. 
Isn't function composition and it's conventional (little "o") notation taught in American high schools fairly early on?
I recall I did for Int, but somehow didn't manage to get the instance right and the error GHC gave made me cry so I gave up... I'd love to see it working!
I like leksah 
&gt; Look, if you're going to continue to use stone tablets even after the &gt; ballpoint pen is invented, I'm not going to make that my problem. Let's not kid ourselves, here. There is no perfect solution to the problem of dependency resolution. The stack approach involves considerable tradeoffs, and because of that there are still cases where it makes more sense to use cabal than it does to use stack. stack isn't strictly better, it's just different.
What could be changed or added to help you like it more?
Ah ok. So you should be able to do everything with stack build if you change stack.yaml and your .cabal file. For example I'm pretty sure if you switched a stack.yaml from lts-2.x to lts-3.1, stack build would yell at you to rerun stack setup if you didn't have GHC 7.10 installed yet, then it would switch over without a hitch once you did. You might still need to fix errors in your code, but I'm pretty sure you don't need to think about managing your stack package db. It does it for you. 
And the answer to that is no. I lied. The answer to that is yes, but it got lost in a million other little things also taught in American high school. The student may be forgiven for losing track of a little "o". 
Cheap and ease breeze deployment, low entry barrier, anything else?
I don't agree that would be more beautiful. That would make the meaning of `if` overloaded, and not in a clear way. Why don't you just create a function for that? mif :: Monad m =&gt; m Bool -&gt; m b -&gt; m b -&gt; m b mif c y n = do { nc &lt;- c; if nc then y else n }
Indeed. If one doesn't feel like rolling their own definition, [the `extra` package defines an `ifM`](https://hackage.haskell.org/package/extra-1.4.1/docs/Control-Monad-Extra.html#v:ifM).
Well, thought I'd follow it --- had to click/click/click....to actually get to a place where I could download stack for OS X So for a start, I suggest you change the target link for "Download Stack" so that it opens in a new window! Edit: then you might want to consider changing Download Stack and offer "cabal install --constraint 'mono-traversable &gt;= 0.9' stack" as an alternative. Seems a lot easier :-)
That's a really good idea! Thanks. 
maybe a bit unfortunate that the name for `state` is shadowing the central/common function of the same name from State api :)
&gt; I think Haskell's organization of list algorithms very confusing. Some functions are right on Prelude, some need `Data.List` [...] Why some functions from Data.List are not in the Prelude: adding functions to the Prelude would break any piece of code using those identifiers, so it isn't done without a very good reason. In my opinion, having to import Data.List is a very low cost, since we're likely to import tons of other modules anyway. &gt; [...], some need their own import such as `chunksOf` on `Data.List.Split`. This isn't "Haskell" deciding to divide its list methods among many libraries, it's some kind soul who has independently implemented useful list methods which were missing from base and has released them as a third party library. I do wish this particular library was incorporated into base though, as splitting strings is an extremely common task. &gt; Since `Foldable` is essentially a way to write list algorithms generically It's not. Like many other type classes, the `Foldable a` constraint expresses the fact that a particular set of methods, in this case `foldMap`, is defined for `a`. Lists and list-like data structures such as vectors both have Foldable instances, but many non-list-like data structures such as trees and maps do as well. &gt; My suggestion is that, instead of writing list algorithms for specific types such as `List`/`Text`, or for `Foldable`, we just do it directly for the [church-encoded list] I know how much you like the untyped lambda-calculus, so you might have a hard time believing me, but I don't see any advantage of working with church-encoded lists over working with the list data type. &gt; having a single point of failure Off topic, but the expression "single point of failure" is usually used to point out that something is a bad idea, not as a goal to be achieved :) &gt; -- A type class for "things on which we can apply list algorithms on" &gt; class Foldable l a where &gt; fromFold :: Fold a -&gt; l a &gt; toFold :: l a -&gt; Fold a Please use another name than "Foldable" for this, to avoid confusion with base's Data.Foldable, which already has a well-established meaning. I'd propose "ListLike", but there is already [a library named ListLike](https://hackage.haskell.org/package/ListLike) with a similar purpose. Have you tried it? Also, like I said earlier, I'd rather use actual lists than church-encoded lists. So something like this: class IsomorphicToList f a where fromList :: [a] -&gt; f a toList :: f a -&gt; [a] A typeclass with those methods already exists, it's called [`IsList`](https://hackage.haskell.org/package/base-4.8.1.0/docs/GHC-Exts.html#t:IsList) and it was introduced with the OverloadedLists extension. There doesn't seem to be any generic algorithms defined for it, but feel free to port your implementations above to IsList and to publish them in a third party package, like Data.List.Split did for lists. &gt; foo container &gt; = fromFold &gt; . map (* 2) &gt; . filter odd &gt; . map (* 3) &gt; . toFold &gt; $ container &gt; &gt; Which eliminates the intermediate structures, getting us to the same performance we would expect if we programmed the functions for the original structure. Not quite! Even if the list *can* be converted to a list and back, it doesn't mean that it is efficient to do so. Imagine a balanced binary three, and a filter operation which ends up removing only one element: the cost will be that of a traversal + a single node deletion, so at most one rebalancing operation. If you convert it to a list (church-encoded or not) and then reconstruct the balanced binary tree by inserting the elements one at a time, you'll need O(n) rebalancing operations instead. 
My favorite way to imagine `.` is to think of it informally as (f . g) x = f (g x) just kidding! it's not informal, it's actually the literal haskell-legal definition of `(.)`! :D
It is indeed nice that the most important operator in Haskell is also the most visually unobtrusive one.
[Only some forms are considered values in SML](http://mlton.org/ValueRestriction). Applying a function isn't a value by itself.
[{-# LANGUAGE RebindableSyntax #-}](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/syntax-extns.html#rebindable-syntax) and import a suitable `ifThenElse` from somewhere on Hackage.
 instance Ord k =&gt; Foldable (Map k) where foldMap f = foldMap f . elems You won't be able to go in the other direction, from `[a]`/`Fold a` to `Map k a`, because you'd have to pull out keys out of thin air. *edit*: right, you meant the case `k ~ Int`, presumably so that you can use 0, 1, 2, ... as the keys. Here you go: {-# LANGUAGE FlexibleInstances #-} import Data.Map as M class IsomorphicToList f where fromList :: [a] -&gt; f a toList :: f a -&gt; [a] instance IsomorphicToList (Map Int) where fromList = M.fromList . zip [0..] toList = elems 
Something like this using stack would be very helpful. https://howistart.org/posts/haskell/1
[heh](http://www.thestrangeloop.com/2015/the-programming-language-called-classical-chinese.html)
I've just started work at Standard Chartered. They have a similar set up to many other large investment banks, which consists of an infrastructure of delivering line of business apps which people from core developers to neophyte scripters use to put apps on traders (and others) desks. These apps can be summed up as "go find me some data -&gt; transform it -&gt; display it or write it somewhere else" and they are usually chucked together in an environment with a strong short term delivery culture. The idea of giving people a standard environment to do it in is that you get to share functionality, access to data has but one interface and it's supposed to increase re-use across lines of business. Most banks that have gone down this route have chosen python, mainly for the reasons that most people like python: It's approachable, discoverable and easy to get started with. By the way - these are the same reasons that businesses like excel. Some of these guys are up to tens of millions of lines of python. SC has chosen a dialect of Haskell for this. I cannot begin to tell you how much better it is to be able to build software in this environment when you have a tool that is built up of sensible, small abstractions and a globally discoverable interface (hoogle) for finding the bits you need to glue together. You also have the type system which lets you chuck stuff together with the assurance that you'll be able to refactor and improve it later. So the reasons that most people like python for: easy, simple, dynamic, fast to hack something together, are the same reasons that I no longer do. In this environment python leads to the same unmaintainable pile of pain that a business that captures its business logic in excel spreadsheets. One place where I think python does have a better story that Haskell (at the moment) is in exploratory data analysis. Ipython, pandas and numpy/scipy seem to be a pretty unbeatable combo for slinging data around. Just please, for the love of $DEITY do not build infrastructure with it!!
Isn't there also a performance cost to creating the string that way? Doesn't using `++` or `concat` mean that the string will be joined at runtime instead of compile time?
how would you compare hdevtools, ide-backend and ghcmod? which one would you recommend?
The only thing that stands out to me as something to at least be aware of is line endings. Line endings in code should be handled transparently by the parser, but line endings in a string might cause some behavioral changes when running the code on different OSes.
I've used Python (tech art work in games - rigging and animation tools and pipeline stuff) for about 7 years now (13 total, but no Python initially). I've been playing in Haskell for about 2. I no longer like Python, because of it. Haskell is just beautiful. If I want to try out an idea, I launch Vim and GHCi in tmux splits and play there. I still must use Python for Maya work, but I feel very unsafe, and very limited and less expressive in Python when I use it (and I feel even worse in C# in Unity (the game engine)). I now have a func.py with 70 one-liners (and more spread through other files), like these: ident = lambda x: x const = lambda x: lambda _: x juxt = lambda *fs: lambda x: [f(x) for f in fs] comp = lambda *fs: lambda x: reduce(lambda a, b: b(a), reversed(fs), x) cmap = lambda f: lambda xs: map(f, xs) mapcat = lambda f: lambda coll: list(chain(*map(f, coll))) flip = lambda f: lambda a: lambda b: f(b)(a) It lets me play with partial-application and expression composition—and I can do whole swaths of things I used to write a few blocks of code for as simple one-liners now—but it's not the same. All of those lambda args can be of any type, and the error messages are completely meaningless. I tend to attack small needs like this in Maya now, throwing together 5 or so new one-liners to each solve one little, composable problem, and then one more one-liner to finish the job, and then usually archive the one-liners, to what end I've no idea (just in case?).
 concat [ "Lorem ipsum dolor sit amet, " , "consectetur adipiscing elit, " , "sed do eiusmod tempor incididunt " , "ut labore et dolore magna aliqua." ] There, that was bugging me :)
ETH Zurich
As an EE student I want to warn you: learning Haskell might make you develop a bitter hatred for Matlab, especially if you'd end up having to use it for something serious.
&gt; I had to manually resolve dependencies on base and text packages... WTF? I think I misunderstood this bit. What did you have to do manually?
Perhaps, but this approach relies on: a) The compiler's inliner working, and Scala isn't as free to inline things due to the presence of effects. b) The ability to extend the compiler's simplifier with arbitrary rewrite rules like stream/unstream. 
This syntax is extremely annoying for e.g. SQL Statements where you might want to copy them out into a REPL to test them.
The problem with this approach is that the whitespace for indentation and line breaks becomes part of the string.
imo this should work: let o f g = fn x =&gt; f (g x)
Cygwin's GCC is incompatible with MINGW's gcc that GHC uses. Try to get openldap with MSYS2 which uses gcc-4.9.2. If that doesnt work then you will have to compile it with gcc that comes with GHC (its gcc-4.5 for 32-bit GHC and gcc-4.6 for 64-bit).
I may have been a bit rhetoric above (I think in the details we agree on the status of general recursion) and I did not mean to criticize the very real progress that has been made by Idris, in terms of usability but maybe more importantly in term of actual uptake and community-building, in the recent years. I think it is great that we have that, and that the functional programming community is so willing to be early adopters on these. Also, many of the things in the list I gave above are encodable in dependently typed systems (but I'm wary of the actual usability costs of relying on encodings, as for records in Haskell), and/or have been explored as experimental features of dependent systems -- Idris has Uniqueness Types for example. 
The shell script `h` can also be a program that awaits input on stdin. So in the (*exaggerated finger quotes*) "category" of Unix pipelining, it seems like programs could quite reasonably be seen as the composable morphisms, no?
That wasn't intended to be snide. I'm trying to encourage (albeit apparently not very successfully this time :)) people to read the report, since it's very readable and has many interesting things like this in it. 
True. I think that I'd be tempted to use a quasi quoter for SQL statements
Isn't there a QQ for multiline strings? As an alternative to `{,m}concat`, to make it easier to copy the string out of the source code.
Admittedly I haven't actually used Python much, it's always kind of slipped underneath my radar, maybe because it's just not very "interesting." Ironically that's what I admire about it. It seems very straightforward and mundane and happily unimpressive. And, like, nobody goes into a Python IRC channel asking for help with printing a string only to receive a massive line-by-line education about monad transformers. They just say "type `print`". There also seems to be a general interest in the community for beginner-friendly documentation and stuff. That kind of thing is always inspiring to me. Sometimes Haskell seems to encourage the spirit of the "abstraction astronaut," which can and does lead to utterly beautiful designs, but can also lead to endless vexation. Haskell right now isn't really a great "out of the box experience." I'm especially thinking of stuff like: * the contortions one must learn in order to wrestle with the five different types of strings (`String`, `BS.Lazy`, `BS.Strict`, `Text.Lazy`, `Text.Strict`); * the near-necessity for something like a lens library when working with nested structures, the confusing wealth of alternatives, and the lack of consensus; * the way some libraries expose their basic functionality only through abstract and undocumented type class instances; * etc. So in that regard, my imagination of Python as a beautiful land of obvious simplicity and established consensus is appealing as an inspiration.
If that doesn't exist yet, it'd be trivial to implement.
I still use `Debug.Trace` from time to time when I get confused. I'm accustomed to such an operational way of poking at the system to develop an understanding of what's actually going on. And when the pretty typesafe abstraction I've built up turns out to do something unexpected, I need some way of breaking the abstraction. Of course `print`-based debugging has always been a kind of hack. I'm looking forward to the development of more interactive ways of visualizing Haskell term reduction.
&gt; Yes. I have almost no ability to debug by sprinkling print statements now. `Debug.Trace`? The lack of a suitable `Show`constraint when working with type class instances still bites sometimes, though; I wish there was some Haskell equivalent of Oz's `Inspect`.
Is the runtime cost really noticable?
Hakyll doesn't work in ghci for me on my Windows machine, so I have to build the project to get my errors. It's... not nice.
multi-line strings are the most horrible feature ever... I took me hours to fix this bug: schemas = [ 'json', 'xml', 'yaml', 'html','ini','systemd-config' 'xhtml', 'blah' ] This is a list that contains the element `systemd-configxhtml` and does not contain the elements `xhtml` or `systemd-config`. Took me an hour to figure our the issue. `\n` is string concatanation in python.. It's really a poor choice syntactically speaking that bit my ass twice now. I'd much rather have `\` for line continuations like in C. Funny thing is. In python both are supported :/ 
Maybe I formulated myself rather obscurely. :) Here's a stupid example of how shell pipelines are composable in interesting ways: ((echo foo; cat) | tr o a) | tr f b It will first send `foo` through the pipeline, and then await a text and an EOF (Ctrl-D) and spit it back with some letters changed. Very much like `pipes` (which isn't very surprising).
You can also have a separate syntax for multi-line strings that makes it obvious what you're doing, preferrably also letting you avoid most escape sequences.
Things I like: * `tuple` (basically an immutable `list`) * `frozenset` (literally an immutable `set`) * `namedtuple` (actually a decent shot at an immutable `dict`, with nicer syntax for indexing to boot) * relatively strong dynamic type system * no enforced class hierarchy in your programs * decorators * generators and a standard library built around them * flexible destructuring bind for sequences Things I like better than in Haskell: * import semantics * `dict` comprehensions, set comprehensions * ubiquity of implementations * ubiquity of jobs * `with` statement * exception handling (`try`/`except`/`else`/`finally`, lots of introspection capabilities) * strong set of imported-by-default functions
&gt; There also seems to be a general interest in the community for beginner-friendly documentation and stuff. That kind of thing is always inspiring to me. I'm not so sure about this. Maybe in the biggest projects or something, but once you get down to the smaller stuff, Haskell libraries tend to be *more* thoroughly documented than Python libraries in my experience. Sure, Haskell library documentation basically boils down to "here, have type signatures", but Python library documentation is just "Ooh, you're interested in my library? That's such an honor! Because I'm so nice, I will actually write you one (or if you're lucky, two) short examples on the simplest possible usage of my library. Need more? Curious about configuration options? Go read the source code and figure it out." So basically, Haskell documentation tends to present the valuable information in the most concise way possible, which isn't particularly beginner friendly. But the opposite approach Python documentation takes, in *hiding* valuable information completely and making it unavailable, is also not being beginner friendly.
Thanks for responding hamismack. I really wanted to give you more examples, but the last time I closed Leksah it crashes and now it won't start. Thanks to that dreaded shell window (:P) I can tell you that it can't load libenchant_myspell.dll, which is weird, cause the file is right where it should be. While I have no problem believing that using Leksah is much more pleasant on Linux, I'm gonna stick to Windows on desktop. I have no problem with Linux though, in fact I use and manage multiple CentOS/RHEL servers almost everyday. It's just that Windows gives me greater tool selection for everyday tasks and in my opinion provides perfect balance between OSX's "it just works, but only our way" and Linux's "configure everything by yourself" :)
&gt; cmap = lambda f: lambda xs: map(f, xs) I have to admit it took me a pretty long time to figure out why this existed. My regular reasoning skills do not apply well to Python. :(
Yeah I do understand what you meant, but I'm not confident enough to speak decidingly about whether the shell pipe is most similar to function application or composition.
Probably not TBH.
I think postgresql-simple has an sql quasiquoter that basically only concatenates strings compile time. [sql| select foo from bar where foo = 'bar' |]
I like all the libraries for scientific computing centered around numpy. Iterators, and all the things you can do with them, are also really cool. Especially in Python 3 were they have become default.
I like the large amount of numeric/scientific/visualisation libraries it has &lt;3
* Generators * Syntax with just enough punctuation to guide your eye (particularly `:` to indicate blocks, like `if x &lt; 3:`) * The module system, and no 100 line imports * Batteries included * Decorators * Numpy and related libraries * IPython notebook!!! That said, I expect that Julia will fully replace Python for me in the near future.
&gt;&gt; Non-native looking, slow gui. &gt; Not on Linux :-). Perhaps a better way of putting it is it _only_ looks native on Linux. It looks pretty peculiar on OS X too. 
I wish there was a service like this in the US
Not in my high school. I didn't learn this until college.
&gt; As for Matplotlib, I haven't seen anything even close for Haskell. For example, in my other window right now, I'm editing a data visualization program. It loads a data file and displays two windows. The first window is a 3d scatter plot of the data set, which I can interactively rotate with the mouse. Each data point is coloured by the sample colour and uses the marker shape to reference the species. If I click on a data point, the second window displays a 2d intensity map of that individual measurement, with the colour key correctly showing the intensity on a log scale. All of this weighs in at about 140 lines and I could probably cut that in half while barely sacrificing any readability. One approach might be something like http://hackage.haskell.org/package/gnuplot which lets us piggyback on others' hard work in that realm. I do agree that the numeric and plotting stuff inspires intense jealousy in me. 
IHaskell is getting there, even though the experience is not yet nearly as amazing.
You are way over selling the type system. It is by no means capable of capturing all business logic such that you won't need runtime debugging. I suspect you never wrote anything substantial enough to have ever used Debug.Trace.
It's a lot easier to learn than Haskell. You can hand Python to a class of undergrad biology students who've never programmed before and expect them to become proficient in it by next week to start parsing FASTA files. That's pretty rad.
There's [Text.Shakespeare.Text](http://hackage.haskell.org/package/shakespeare-2.0.5/docs/Text-Shakespeare-Text.html), which isn't well advertised and basically lacks documentation, but it works. (Hint: the one you want is `st`).
There are libraries for almost anything, and a non-existing build cycle. I have seen some attempts to make haskell systems partially automates the saves file -&gt; refreshes ghci -&gt; runs test cycle but I have never had anything that I kept on reliably.
The name... reminds me of the best comedy show ever
Strangely my desire is the opposite: to not make a special case even for List and have the example above be declared like that: ``` Map.fromList $ zipWith (,) (1 : 2 : 3 : Nil) (4 : 5 : 6 : Nil) ``` This of course implies that I'm neither interested in any kind of comprehensions. And, of course, I'm just dreaming about the type signature being `List Int` instead of `[Int]`. IMO, the lesser exceptional cases &gt;&gt;= the simpler &gt;&gt;= the more consistent &gt;&gt;= the better.
Perhaps just a personal thing, but I almost always prefer using normal functions like map or filter to list comprehensions. Also, keeping the language small makes tooling easier; having written code that deals with PureScript ASTs, it's hard to overstate how nice it is that they are so simple.
Re data structures - do Data.Sequence, Data.Map, Data.HashTable not suffice? If not, why not?
&gt; SC has chosen a dialect of Haskell for this. Out of interest: What is the rationale behind using a dialect of Haskell, rather than Haskell itself? (Assuming that the reason is not a trade secret.) How far away is the dialect from real Haskell?
Multi-line strings are great, whitespace as string concatenation is error prone. Though to be fair if was probably only put in because of C. 
Which could be generalized to `(Enum k, Bounded k)` instead of `k ~ Int`, but it's still a big constraint.
(Disclaimer, I'm new to Haskell and an "old" python developer) There is many thing I love in Haskell (And that's why I want to continue my experience) but some stuffs really sucks (Or at least I did not understand the right way of doing it). I still don't know how to design a complex application in Haskell (I still have that feeling that If I'm not getting my 10 layers transform stack right from the beginning, I'll have a hard time, I cannot wrap my head around lens and my brain segfault when I have more than one lift/hoist per line). As many already said, I find the modules "broken" (I need to import too much). Simple stuff are sometime complex. For example, recently I had to get the length of a list then filter/fold it. My initial solution using: let l = length list res = fold f' (filter f'' list) 0 Was space leaking the whole list. I finally ends with a not satisfying solution using a StateT ListT like: foo :: [Int] -&gt; ListT (State Int) Int foo l = do x &lt;- (ListT . return) l i &lt;- lift get lift . put $! (i+1) --lift . modify $! (+1) -- modify space leak when get/put doesn't guard $ (x `mod` 2 == 0) return x I also have a solution using filterM and a filtering function (:: IVar Int -&gt; Int -&gt; IO Bool)) which modify an IVar, but I don't like it because of the IO. I may be able to write this using ST too, but I don't know yet. One stuff that really bug me is the dynamicity of the code (i.e: duck typing). For example, I was playing with Network.Socket (imported as NS in the following snippet). Initially I used send/recv and was storing the read leftover in an IVar. So I had: data MySocket = MySocket NS.Socket (IVar String) readLine :: MySocket -&gt; IO String -- Read a line on the socket, possibly using and changing the leftover stored in MySocket writeLine :: MySocket -&gt; String -&gt; IO String Then I discovered Network and the API on top of Handle, so I rewrote my code such as: data MySocket = MySocket Handle. Even if they have the same type signature, I had to rewrite the readLine and writeLine functions. To test both version for performances (As a side note, the Handle approach is roughly 10000 times slower than the IVar approach, I may have done something weird...), I need to comment out one or the other and/or use a preprocessor directive. In Python, I can just create an object of the first or the second type and let the duck typing do its magic. (I know, I can "easily" get the same behavior using a typeclass, but that's more painful). But here I'm starting to criticize the thing I love in Haskell, its robust type system.
I honestly think is probably is very near capable of doing so _if_ you're one of the wizards who can and does make full use of it. But that level of use isn't accessible to most programmers, I'd wager.
For simple scripts like csv/json processing with maybe a webservice call in the middle, using python has some advantages linked to its simplicity and its rich standard libs: - batteries are included, there is usually one way to do what I want to do. With haskell, I have to take many decision regarding the string type to use, the web library, etc - very low mental overhead. haskell forces me to define types and to perform case analysis. For one shot scripts sometimes it is faster to just assume that everything works because you really don't care about failure modes 
It is easy for simple queries, and join conditions can be dropped into the where clause, but when you start tacking things on like limits and group bys into subqueries, I'm not sure how to go about moving those up. I was experimenting with relational-record and it gets amazingly close to the ideal in this, but even it starts using subqueries when group by is involved with one of its relations that must be combined with another relation. And honestly, that's sensible. It is ridiculous that every oss db does not handle these as well as oracle and db2 do in this day and age.
I'm aware of ghcid [1] for ghci reloading on file change and arion [2] for test execution on file change. Did you try them? [1] https://github.com/ndmitchell/ghcid [2] https://github.com/karun012/arion
&gt; the justification for the added complexity of types I like precise static types because they constrain what can happen at runtime in a way which helps me reason about code, both code I write and code I read. When writing a function receiving a value of a particular type, I know exactly which values those could be, and I can make sure that I handle all the cases. When writing a function receiving some input argument in an untyped language, I have some idea of what kind of data the function will receive, but I might be forgetting some corner cases. The classic corner case is nulls: in a language which doesn't distinguish at the type level between arguments which can and cannot be null (an untyped language being an extreme example of such a language), it's easy to forget whether it is the caller or the callee which is supposed to perform the null check, so we end up with duplicate null-checks which are never needed and/or missing null checks leading to runtime errors. When reading code, I also find types very useful, for the same reason: if a value has a type, I know exactly which values it could hold, which helps me follow what the code I read is doing to those values. Yesterday at work, for example, I was struggling to figure out whether a bug was inside the implementation of a function or in its callee because it wasn't clear whether the values were supposed to be normalized be the function or by its callee. Had we chosen to distinguish between normalized and non-normalized values at the type level, I would have known the answer immediately. &gt; What about performance? Those numbers are quite impressive! I'd have to perform more experiments to understand them better, but at a glance, I'd guess only the fast version is using an accumulator? If so, the fact that the standard library's `sum` isn't using one is a great shame. &gt; Church lists are conceptually simpler I happen to think the opposite. Maybe I'm just more used to them. Although: The list datatype is very concrete, with the only tricky bit being recursion: "a list value is either Nil or a 'Cons', that is, a value followed by another list, which might itself be Nil or Cons, etc." Church-encoded lists are more abstract, and has to deal with recursion as well: "a list is a function which takes a binary function and a starting value, and either returns the starting value directly, or applies the binary function to a value and to the result of applying another list, which is itself a function, to the same binary function and starting value." &gt; got all kinds of great properties for free (fusion, totality) I don't understand what gives church-encoded lists those properties, or even what it would mean for a list (as opposed to a function) to be total. 
And by "much" you mean "1 character"
Drop me a line with what problems you were having in Accelerate, there might be something I can fix.
It's Haskell with strict semantics and without recursion. So it feels exactly like writing Haskell, with some intuition about evaluation being different. The rationale, I think, is that you get to use this language that looks a bit like maths to quants, but don't have to teach them about space leaks etc.
Better plan, stop using if and use an `if'` function instead, then you can use `ifM` and you don't need any sugar, just a function :)
&gt; The reaction of a lot of people is they get frustrated and they try to argue in terms of "Well, we've got to do a decent job as software professionals. Software architecture is important for moral reasons. We need to do a good job. We need to be craftsmen." Unfortunately, my view is that if you take that line, you've lost. because what you're doing is making a battle between craftsmanship and economics, and economics always wins, you have to instead cast it in economic terms. ... &gt;Yes, if I buy the product that is $100 cheaper and has lower internal quality, I win at the moment, but what will happen is that the better internal quality software will be able to make newer features more and more rapidly, and soon the slower one can't keep up any more. And we can probably think of competitive cases where we've seen this happen--where a product that looks like it's dominated has ended up being eaten away over time. ... &gt;That's the economic reason why software architecture is important, because if we don't keep good architecture, if we don't put that effort on internal quality, we are in the end deceiving our customers, in fact stealing from our customers, because we're slowing down their ability to compete. 
I think that it may still be a draw. On the one hand, I know your pain. Python regularly seems to have documentation that reads win_nobel_prize(data) Pass your data to the function and it will return a journal article with a properly formatted bibliography Example: bibliography, temperature = win_nobel_prize("/path/to/data") Everything seems wonderful until you realize that you have no idea what format their looking for the data to be in. Ascii? Packed binary? Hdf? Pel? Xml? Json? Edf? Nexus? Also, what exactly is this function returning? What is this temperature object supposed to be? On the other hand, Haskell seems to fall into the opposite trap. You just get a single piece of documentation winNobelPrize :: ExperimentalData -&gt; Either LargePileOfCash JournalArticle Which seems hopeful and promising enough. Unfortunately, then you find that there's only a single data constructor for ExperimentalData reduceData :: Context a =&gt; a -&gt; FilePath -&gt; YakHair -&gt; ExperimentalData The FilePath is easy enough. You waste an hour trying to figure out how to create a Context before finding out that the acme-nobel doesn't even define a single instance of the typeclass. On the irc channel, you learn that you also need acme-nobel-swedish-chef to make a BorkContext,. Even once you have a proper packages, acme-nodel-swedish-chef doesn't list a single function that makes a BorkContext, but you eventually realize that BorkContext is a monoid, so you pray that mempty will handle everything for you. YakHair, though, is impossible. There's not a single function in hoogle, hayoo, or hackage, that will return a YakHair value without requiring that you first pass it another YakHair. In fact, the only way you even get your code to type check is by abusing dyeYak :: Colour -&gt; YakHair -&gt; IO YakHair to create myYakHair :: IO YakHair myYakHair = mfix $ dyeYak Green Unfortunately, myYakHair is an infinite loop and winNobelPrize never returns. EDIT: That got a bit more satirical than I'd originally intended. If the Haskell story is longer than the Python one, it's only because I'd give up on the python function sooner. Haskell always gives you just enough tantalizing information to keep you headed deeper into the woods.
Agreed. Tools and build infrastructure for automatic checking of new versions and bumping of bounds would be fantastic. But it's a hard problem for one developer to solve for his own packages, not to mention solving in a generalized way that works for all developers. We should still work towards that goal. This is a small first step in that direction.
That's because it uses a Linux GUI toolkit (GTK). Unfortunately Haskell lacks bindings to the most popular cross-platform toolkit (Qt).
&gt; It is easy for simple queries, and join conditions can be dropped into the where clause, but when you start tacking things on like limits and group bys into subqueries, I'm not sure how to go about moving those up. Right, that's harder, but it's also harder to write by hand! If we can get as far matching what people would write by hand then I'll be happy with that as a first stage.
The rationale for having a different Haskell compiler is that we need to support serialization of everything, including closures. This isn't something ghc can do (but Clean can, so if the Clean compiler had used Haskell syntax we would probably be using that). An no, Cloud Haskell isn't even close to what we need.
AIUI the reason for strictness is that the original compiler was targeting a preexisting strict runtime. Perhaps /u/ndmitchell or /u/augustss can enlighten us.
I agree wholeheartedly about "import semantics"! Every time I import something in Haskell, I find myself wishing it were more like Python. 
FWIW I just dug up this nice article from my bookmarks, which could interest you: http://blog.johantibell.com/2011/06/computing-size-of-hashmap.html
No. You need \ if you want a newline within a string literal. But there's no difference between whitespace characters elsewhere.
Python has iterators. Advantage is they don't leak memory, disadvantage is you can only run them once. Generators are syntactic sugar to create iterators, reminiscent of coroutines: def range(n,k): i = n while i &lt; k: yield i i += 1 These can be used in for loops: for i in range(10,20): ... or in generator expressions which are the equivalent of list comprehensions: squares = (i*i for i in range(10,20) if i%2 == 0) You can pass these to other functions to e.g. create sets: set_of_products = set(i*j for i in range(10,20) for j in range(20,30)) Iterators/generators are the bread and butter of Python. They're used to do sequence processing without memory allocation (see also the itertools module), process files, do networking, etc.
The with statement could easily be packaged as a typeclass + the `with` function in a library.
That seems horrible, since all the text utilities for MS Windows are going to expect "\r\n" line separators and not just "\n". I'm sure using "type" against python source is fugly, too. On top of that, I'm pretty sure "\r" is considered whitespace, so most of the time using "\r\n" in your python source will "do the right thing", right up until you use the line-joining feature or (ta-da!) multi-line strings.
That is correct.
My mind is very divided on this. Semantically I find non-strict much nicer, but strict evaluation have big practical advantages, like stack traces on error.
&gt; I like precise static types because they constrain what can happen at runtime in a way which helps me reason about code, both code I write and code I read. My issue is that I'm looking for broader answers... I got some of the practical advantage of types some time ago, but that is just a human thing, I don't care a lot about about what humans do. What interests me is: on the book of all algorithms that solve a problem, one is the simplest. What is it? Why? What do that tell us about nature of computation, and the limits of the algorithm itself? Type systems, ints, etc., are a tool for humans, but they mess with an underlying beauty that tells something. I was trying to implement sorting on the λ-calculus, and I managed to write a very cool sort function for church numbers. It is short, is optimal, operates in `O(N)` and gave me an insight: that optimal sorting can't be done in parallel. It can, but any parallel computation will be duplicate work anyway. I couldn't have this insight if I programmed sorts normally. &gt; The list datatype is very concrete, with the only tricky bit being recursion: "a list value is either Nil or a 'Cons', that is, a value followed by another list, which might itself be Nil or Cons, etc." I agree with that view. But I view it as: "the list datatype needs so much additional information - you need to hardcode a whole ADT system on the core language and many more things - all for a feature that was already present on the systme". Lists are already there, adding it again is redundant... Think about it: if we were perfect robots, would we need them? Or: if we wanted to build the fastest parallel functional computer the universe can hold, would we need it? In order to have many cores, we'd want 1 core per cell. If we had only 2 possible cells (apply/lambda), that would be one bit per core. If we added native lists, ints and all that stuff... that'd bring us up to 32+ bits per core, making the system essentially 32 times slower. I'm bad explaining it, but I think you understand, I'm not here to make money or build stuff. Programming is easy anyway, it doesn't really matter a lot what language you use. I'm here for fun and insights, I want to learn about the universe, and types/native stuff doesn't help me with it. sort = (abc-&gt;(a(defg-&gt;(f(d(hijk-&gt;(k(lm-&gt;(li(no-&gt;(nhj))m))(hi(lm-&gt;m)j))) (hij-&gt;(j(kl-&gt;(k(bdh)il))(i(kl-&gt;(k(bdh)(mn-&gt;n)l))(bdh)))))e)) (de-&gt;e)(de-&gt;(dc(fg-&gt;g)e))c)) 
Every time I've used gnuplot, I've come away feeling like it's a very powerful plotting language, but with all the ease-of-use of JVM bytecode. Is the Haskell wrapper easier to use?
I'd put some value in `with` being generic and exception-safe.
I'm not able to give a review, but you might want to look at this project: https://github.com/TikhonJelvis/inductive-mazes
This is an issue of ubiquity, right? Haskell has support for iterator-style-things that don't leak memory (all the streaming data solutions: pipes, conduit, etc.); it's possible to write a generator monad (even a bidirectional one) (See [TMR19](https://themonadreader.files.wordpress.com/2011/10/issue19.pdf) and the [monad-coroutine](https://hackage.haskell.org/package/monad-coroutine/docs/Control-Monad-Coroutine.html) library). The problem is that they aren't in wide use, standardized, or thoroughly explained to newbies?
Regarding the transformer stack woes in particular, the "aha" for me was realizing that I really don't want to work with things like `FooT Bar baz` at all. What you want is `(MonadFoo m, MonadBar m) =&gt; m baz`. That leaves things polymorphic to the last minute, so you can refactor easily.
Nope, admittedly long time since I've tried to get this working. 
I absolutely agree with every thought you express there and have concluded the very same as well! Concerning the name-duplication annoyance, using some extensions you can already rewrite the following ``` main :: IO (); main = ... ``` as ``` main :: IO () = ... ``` I've been using that syntax in some projects until I've stumbled on it causing conflicts in signatures, which involve constraints. Still, however, this particular issue seems to be fixable without breaking the backwards compatibility. &gt; Please let me know if you can think of anything to add. Well, as the first order of business, I'd add strictness to that list. Actually I maintain a private dump of ideas for the "better" Haskell for about a year now. Besides the ones you mentioned there is a load of other promising and even radical ideas already, but unfortunately it's all in a too chaotic state to be published. I decided to just accumulate the ideas until I get the clear vision of what the new language should be, then I'll polish and publish the ideas and possibly get to the implementation in the spare time. Consistency and simplicity are my primary concerns. 
It has to be contextualized, explained, and demonstrated, but it is valuable yes. flip bind already has an operator, you can use that next time and the types'll match. Prelude&gt; import Control.Monad Prelude&gt; :t (&gt;&gt;=) (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b Prelude&gt; :t (=&lt;&lt;) (=&lt;&lt;) :: Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b 
&gt; . My initial solution [...] [w]as space leaking the whole list. This is what /u/tekmo's [foldl](https://hackage.haskell.org/package/foldl) library was designed to address; unfortunately, its approach to filtering before folding relies on lenses. import qualified Control.Foldl as F import qualified Control.Lens as L let (l, res) = F.fold ((,) &lt;$&gt; F.length &lt;*&gt; F.handles (L.filtered f'') (F.Fold f' 0 id)) list 
Notice that the linked-to wiki page is called DependentHaskell/Phase1. Removing that duplication -- and ending the Hasochism -- is phase 2, described in some detail on the parent DependentHaskell page (https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell) and my thesis proposal (https://github.com/goldfirere/thesis/blob/master/built/proposal.pdf). In the meantime, I agree completely with what you've said.
I think the correct solution is to remove `if`.
Another one for our lists: https://www.reddit.com/r/haskell/comments/3hj9hb/we_should_be_able_to_use_monadic_predicates_when/cu8fv1o
Everyone cannot use stack because stack is not a sufficiently general tool to be used for all use cases. Cabal-install is a sufficiently general tool, although it has some shortcomings. Those shortcomings are being addressed, but stack was created to try out some different ideas with a reduced scope. Stack's main focus is on people who can use a curated collection of packages. Curated collections are not appropriate for all people, and when they're not you need cabal-install's dependency solver. Dependency solvers rely on version bounds to work reliably. Stack is not tackling this problem at all. Some might argue that stack is sufficiently general because it does expose solver functionality. But it just calls out to cabal-install, i.e. it's not tackling the solver problem. So you cannot excise cabal-install from the system. And even if you could, it's not about any one tool. It's about people who need dependency solvers. That need will never go away. Therefore the need for version bounds will never go away.
&gt; Runtime debugging, proper tracebacks, walking traceback frames. Do you know why we don't have tracebacks on the `case` stack already?
It's mainly an issue of ubiquity, but also of simplicity and syntax. Even if the community settled on a particular library, it would still need to be integrated with list comprehension syntax, for example, and not via a monad comprehensions compiler switch. It should be right there when you fire up ghci for the first time, it should be described in all intro tutorials, should work with all data structures from libraries, etc. I view programming as making a series of choices; from the macro level which library you're going to pick and how you're going to architect your program to the micro level of how you're going to process this collection into that collection to how you're going to format your code. The difficulty of programming depends on how many of those choices you have to make, and for each choice how easy it is to pick the right alternative. In Python you have many fewer choices to make, and which alternative is the right one is much more obvious. At the macro level you'll use iterators/generators, no choice to make here. But even at the micro level there are fewer choices. For simple data processing in Python you just use a generator comprehension, always. In Haskell you can use a list comprehension, or use map/filter/etc, or a more sophisticated function. When you've decided to use map/filter, then you still have to decide HOW you're going to do it. Partially applied and composed with `.`? Or maybe applied to a specific collection? Will you then use nested function calls, or a series of composed functions with `.` and call it with `$` at the end? Across the whole spectrum from which library you're going to pick all the way to minor syntax variations there are just far more choices to make. There are many things I don't like about Python, but one thing I love is the adherence to its famous principle: &gt; There should be one -- and preferably only one -- obvious way to do it.
At first I thought so too but then I realised it's probably better to have separate with functions for separate things.
Cool, thanks for the info. 
I've only used the simple interface, and it was simple but not powerful, but sufficient for my needs. I too would be curious to know how far one can get with the full-fledged interface and how difficult it becomes.
 @vigilance$ cat t.c #include &lt;stdio.h&gt; int main() { printf("hello" "world\n"); return 0; } @vigilance$ gcc -o t t.c @vigilance$ ./t helloworld I'm sorry.
Why? 
Once this is merged, `*` will be both a kind and a type. All value-level types (such as `Int`, `String`, `Maybe a`, etc) inhabit the kind `*`. But `*` is also a type, so we can ask what values inhabit `*`. I can't think of any reasonable values that you want inhabiting `*`. Are there none (other than undefined)?
Seconded :)
There are none, other than undefined. But there is room for growth here: the natural inhabitants of the `*` type are type representations, such as `typeOf True`, the runtime representation for `Bool`. I fully expect that this idea will eventually come about, though likely not in the next year or so. 
You might want to look into using the insufficiently well-known [grid](https://hackage.haskell.org/package/grid) library (or not). I found your two workhorse functions, `randomTraversal` and `randomPath` very dense and hard to read; I wonder if they can be made cleaner using the `State` monad for the `visited` set and the `StdGen`.
IHaskell doesn't have a large funded team working on it :) That said, we're catching up, a little! A GSoC project this year has gotten us IPython's interactive backbone widgets in Haskell, for example. Stay tuned for a demo!
Explicit type quantification / application also allows you to write expressions which are ill-typed if you limit yourself to Hindley-Milner style systems where generalisation is only performed at the top level. Cf. [this question](http://stackoverflow.com/q/31931432/727667) for instance.
&gt; ...but mostly because we have not worked out how to hide it on Windows.. Adding `-optl -mwindows` to GHC options should work.
Thanks for fixing my ticket Richard &lt;3 &lt;3 xoxoxo ~~~
Ah, I visited once. Nice place!
I can't wait! I'm really excited for demos of this year's Haskell GSOC projects!
By "more generic", I mean that there is a single construct that works with a whole host of resources (files are the only one that actually comes to mind right now, though); by "exception-safe", I mean what `Managed` and `Conduit` try to be, which is that your finalizer is guaranteed to be called even if an exception is thrown.
At this point, I have to wonder if it would make more sense to invest in Idris performance / tools / libraries, instead of trying to add more type-level features to Haskell.
I wonder about the `#` kind. Currently GHC does not parse it, but in the future, if we allow that, there will be parsing problems similar to the ones with `*`. Perhaps `#` should be renamed to a valid identifier one day. I am not sure if the name `-XStarInStar` gives a correct impression. My first thought was that enabling it makes `* :: *` (analogous to `type-in-type` in Agda). However, if I understand correctly, `* :: *` will always hold, and the extension is more similar to `ConstraintKinds` and `MagicHash`.
For discussion on the dependency solver in stack, see: https://github.com/commercialhaskell/stack/issues/116 There doesn't seem to be a lot of demand to get a new solver into stack. I do think it's accurate to say that "stack is not tackling this problem at all"; rather, I think of the solver as a sort of plugin to stack.
Math education is abysmal in many places. I never learned that. 
Well, I bought into Haskell because it seemed so clean, and the type system made so much sense. Then, I learned about multi-parameter type classes with functional dependencies. It still seemed pretty clean, but the tricks Oleg had to use to code with it, were mind-boggling. Then came the type families, which were explained as type-level functions. Kind of like type classes with functional dependencies (which are still there), but not quite exactly. And even though they are sort of functions, the syntax is different, and any libraries had to be re-implemented. Then came this stuff with promoting some values to the type level. And, maybe some functions, like "+". And, those weird kinds which kind of look like types, but with the apostrophe before it. Or maybe without, if it's an integer. Then, we have data families, and we have GADTs, not sure which one to use and when. I would probably know if I took the time to look at the last few years of Haskell development. And now it looks like we have explicit kinds, implicit kinds, kind families, kind-indexed GADTs, types of kinds. Did I mention the old stuff like the type synonyms and newtypes yet? I could probably make sense of it again if I sat down and studied it for a month or two, but it's just no longer straightforward. Too much stuff to learn. Too many variations of the syntax. Too many ways in which something can be implemented. It almost feels like C# is a more consistent language at this point (even if it's less powerful, and does not isolate effects), which is a pretty bad sign. What's more, the majority of the recent developments in Haskell seem to aim to make it a poor man's Idris. Which feels awkward - why not pick the real thing then?
I think a sad truth is that there's really not a lot of direct incentive for anyone to spend time replicating all the nice machinery that exists GHC for Idris, like a heavily optimized compiler and runtime. It will probably happen some day though (and for good reason: it's the only dependently typed language that's *really* trying hard to be useful for day to day programming). In fact, I can tell you that finding people *today* who will work on GHC, not Idris, is already challenging - and it's something people base their businesses on. I imagine the same is true of many other language implementations. It's just not a very profitable or realistic thing to put effort into until you hit a certain scale. On the other hand there's quite a bit of incentive for directed research on GHC, precisely *because* it is something you'd consider a 'Production Ready' programming language implementation that is still advanced, and pushing limits in design and theory - and has had about two decades of constant work and a large, varying set of users. And frankly a lot of this kind of work for us is supported by universities through grants and degrees in higher education. They're going to have a desire for work that pushes things and is publishable, perhaps as opposed to the boring parts of building a production programming language (not that they are mutually exclusive - I think we're an example of that!)
Excellent point -- you're completely correct. Perhaps `StarInStar` is a misnomer. Any suggestions for improvement?
No CMS can be considered modern in my eyes. I highly prefer something like Prose.io. Perhaps ForkCMS is modern? There also aren't any CMS' written in a robust language like Haskell. `clckwrks` and LambdaCMS try, but they seem to be in eternal alpha.
`let` and `&lt;-` are two different constructs that do two different things. You want the same operator to do two different things. Those are completely different things.
Couldn't Haskell code be translated to idris ?
Were you around for /u/catamorphism's [How To Exclude Women Without Really Trying](http://geekfeminism.org/2012/09/17/how-to-exclude-women-without-really-trying/) and the [reddit discussion](https://www.reddit.com/r/haskell/comments/zxmzv/how_to_exclude_women_from_your_technical/) (albeit of an earlier draft)? The impression I got from the reddit discussion was that the community was pretty vocal in support of the point. Also, did you know about [Lambda Ladies](http://www.lambdaladies.com/)? It's true, the community doesn't do enough evangelism with the specific goal of diversity, but I feel like that can be explained, at least in part, by the fact that we're still a relatively small community that has to do a lot of general evangelism. On the other hand, I can only think of ~~one~~ two relatively prominent female Haskell users: /u/winterkoninkje ([blog](http://winterkoninkje.dreamwidth.org/)) and lkuper ([blog](http://composition.al/)), both academics, so representation in industry (AFAICT) will suffer from exactly the problem you mention.
&gt; abstraction astronaut I have a feeling that `(.)` and `&gt;&gt;= ` and `$` etc. just need apt visual metaphors in order to be made "obvious" to the rest of the world. Funnels/ plug adapters, factory floors .. which are much more intuitive than for loops and objects, under this light
Haskell has type classes; Idris has implicits. Haskell has pervasive laziness; Idris has (a pretty good story for) explicit laziness. Haskell has more powerful type inference.
Idris has type classes too. As far as the type inference goes, Haskell is doing a good job for the old style Haskell scenarios, but I am not sure how good it gets for all of those newer features like type-level equality constraints, values promoted to the type level, etc. I would guess it should be worse than Idris, because the type system was not initially designed for this kind of tricks. On pervasive laziness vs. explicit laziness, I am recently leaning towards the explicit, primarily because of the more predictable performance, debugging, and smaller gap with the existing VMs (think compiling to javascript / JVM / .Net / BEAM / etc.).
I didn't realize 2.7 was recent, I wrote a coroutine in this legacy version of python just yesterday. I use generator expressions as lazy lists, but I have a lot of other uses for actual generators. For example, in some of my code I have an action that needs to be performed at the beginning of a for loop and then halfway through the for loop, but in other versions it may need to do it every 1/4 of the way through. The loop body is the same, I just need to inject a little function call at just the right iterations. Generator functions work wonderfully for this purpose. Generator expressions, not so much. 
I recommend reading stuff by people who you know will be presenting or attending. Last time I went to a [similar event](http://www.composeconference.org/), I recognized several attendees and knew about their work, which made it easier to break the ice and to start very interesting conversations with them.
Thanks for the link! I have already watched that lecture. It is great. I mean something like that, but the target to be some ARM processor or PIC microcontroller and not FCPGA.
First of all, have fun! I'm going to give you a few materials to try and solidify your knowledge/add you some more knowledge about Haskell, but you definitely do not need to read everything, just take a few interesting parts, gain some more confidence and keep writing code. It is hard to grasp many concepts without actually writing code: - [Adventure with Types in Haskell](https://www.youtube.com/watch?v=6COvD8oynmI) - a video lecture by SPJ about Haskell's type system - [First part of chapter 2 of Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929/ch02.html) - You don't need to read the whole book, but this part of this chapter can give a pretty good understanding of Haskell's laziness - [What I Wish I Knew When Learning Haskell](http://dev.stephendiehl.com/hask/) - A really long reference, don't try to read it all, cherry pick things that interests you or you want to understand better and focus on them. - [CS240h](http://www.scs.stanford.edu/14sp-cs240h/) - a course about Haskell. Don't try to read it all in one go, go through the "basics" slide and cherry pick. Hope this helps and this is what you where looking for.
Would it be possible to go the other way, and unlift each type to a value? Thus, the values of `*` would be types-as-values.
Thanks :) I've also got a copy of RWH that is gathering dust (shame!) and this [How to Learn Haskell](https://github.com/bitemyapp/learnhaskell) repo starred... A lack of time stops me from progressing, so I'll take your advice and try not to get too bogged down in it all.
Very interesting! Thanks! I'll check it!
Will this mean obsolescence of `Symbol` over straight `String`? (please yes? ... I wonder what the compile-time overhead of a `String` is ...) 
Thanks for the reply! Yeah, I've read catamorphism's blog post (though it was a long time ago). I agree that there were quite a few constructive responses. And I know of Lambda Ladies, and I think it's great. I really wish they would accept some of my money to help them with outreach and whatnot, but I completely understand that my pittance would not be enough to enable anyone to dedicate real time to it (since I don't think they have any existing programs for outreach/teaching/etc, it'd require some more significant capital to bootstrap). I also understand that Haskell is a small community, and I don't blame "it" any more than the tech community in general for the lack of diversity. I really want to encourage positivity here and not try to cast the entire community negatively (though there are certainly *elements* that deserve being addressed). I want to work on increasing diversity in Haskell, and the only way I can do that *right now* is through monetary contribution, but I haven't found any groups that are taking it, or even individuals dedicated to diversity who have a Patreon or whatever. In the future I hope to help more directly, once I have dealt with a few of my responsibilities and free up some time. I would really love for prominent members of the community to speak up about increasing diversity. When people in the spotlight speak positively about something it can have a pretty significant effect, causing underrepresented people to have less fear about getting involved, and also encouraging people already in the community to try to help out. Guido's shirt speaks volumes, and so can Simon! :-) Oh! And I'm reminded of http://www.haskellnow.org/, which has a great mission but also still seems to need a lot more work/contributors.
I actually like the Haskell way more, especially with indentation-dependent syntax: main = do putStrLn "Some super long\n\ \multiline String,\n\ \the Haskell way" putStrLn "Some super long \ \singleline String, \ \the Haskell way" main = do putStrLn """Some super long multiline String, the Python way""" putStrLn"""Some super long \ singleline String, \ the Python way"""
What will happen with magic types like `IO` on the kind level? Will they just be present but not useful? Or can somebody inject some semantics into them?
I hear a lot about this Haskell moving to dependant types thing, so I was wondering. Doesn't dependant typing imply a lot of needed changes, like banned non-termination and stuff? Maybe even forced strict evaluation?
I've been writing Haskell code for 25 years, and I use `Debug.Trace` all the time. In fact, I'm the one who added the `trace` "function". :)
Of those, I think `-XKindTypes` is my favorite, followed by `-XKindsInTypes`. 
You opened my eyes, thank you. Any drawback with this approach ?
 &gt;&gt; But it's in the best interest of the Haskell community to hold hackage packages to a higher standard so that people who try to use Haskell have a good experience. &gt;Hackage is a huge bazaar. There are no quality standards there. With that attitude you're just contributing to the problem and at same time offend all package authors who do go the extra mile to ensure their packages are well maintained. Please tell me which packages you maintain, or rather abandon to Hackage so I can avoid them. 
However it does not work when you want to keep the filtered list. What I really want is something like let myList = somethingLazy l = someFold myList -- Such as length, or sum, or mean res = doSomethingElse myList -- Such as arrayAccum Here, doSomethingElse can consume myList in a lazy fashion, in O(1). However the someFold call must also consume the list, and hence will create the full list in memory. I'm not using the result of l before "res", and doSomethingElse does not depends on l, so it may be possible to compute l and res without storing myList in memory. This is the whole point. (If you want to put some real values on top of that, I have a 4 Gb file, that I read lazyly lines by lines using Data.Text.Lazy (readFile). Then I convert each line in a tuple of values which are used by accumArray to create an histogram. However I must normalize the histogram by the number of lines. For now, the most robust solution I found was to read the file two times... : l &lt;- fmap lines (readFile path) let arr = accumArray .... (map f l) l' &lt;- fmap lines (readFile path) let len = length l' doSomething arr len Which is O(1) in memory (without taking into account the size of arr, which is small) and performs well thank to the operating system IO cache. But that's ugly...
You can already make the typechecker loop by having non-terminating functions at the type-level. So we're already at that point. {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE UndecidableInstances #-} module Oops where type family Loop (a :: *) where Loop a = Loop a oops :: Loop a oops = undefined
If you need to compose multiple folds over a list so that they operate in one pass (don't leak memory) you could try the `foldl`package. Your example could be written like this: import Control.Lens (filtered) import qualified Control.Foldl as L import System.Environment (getArgs) -- Counts number of items divisible by n: divisibleBy n = L.handles (filtered (divid n)) L.length where divid n x = x `mod` n == 0 -- Gets the length, number divisible by 2, and number divisible by 6: wanted = (,,) &lt;$&gt; L.length &lt;*&gt; divisibleBy 2 &lt;*&gt; divisibleBy 6 main = do limit &lt;- read . head &lt;$&gt; getArgs print (L.fold wanted [0..limit-1]) Not sure if your actual use case would fit here, but it's worth a try.