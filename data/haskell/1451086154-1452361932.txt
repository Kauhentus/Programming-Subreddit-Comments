The enforced formatting consistency is totally worth any disagreements one might have with the style choices made. Really though, the tabs for indentation, spaces for alignment thing sucks for manual formatting because it's so hard to be consistent. It's amazing when you have a tool to format for you.
I'm actually becoming quite a fan of half indents combined with [Elm style](http://elm-lang.org/docs/style-guide). Elm prefers many line breaks, so indenting all your `where` and `let` lines four spaces, then every line after that, means you move uncomfortably far to the right. main = let x = "hi" y = "world" in print (join [x, y]) where join = unwords This almost just looks like 2-space indentation, but I tend to save the 'odd' number of 2-space indents for keywords, to keep actual value terms aligned: case list of [] -&gt; putStrLn "no list" _ -&gt; let len = length list in print len (so `print` and `putStrLn` are aligned)
No point having a standard formatter if everyone can choose a different standard!
Go was intended to be sane, hence tabs.
&gt;I don't understand why you would use spaces for indentation while coding The only reason I've ever seen is that a huge number of people insist that they need to control how the code looks on my screen. 
I just wanted to demonstrate `where` in the first example. I'd try to never mix them in practise, that's just diabolical! I do agree that introducing arbitrary rules leads to chaos and confusion. I've made things a little complicated for myself by aligning the values, but I think it's reasonably intuitive after a little while. If I were going with your approach, I'd definitely 2-space everything.
But, that's why I use tabs. So I can use tab = 4 characters, but you could use 8, or 2, or some random crazy custom tabstops.
2 spaces is indeed the overwhelming cultural standard. I wouldn't have guessed 100% completely, but I'm not super surprised 
ahh but 5 space `case` statements are so much nicer! case list of [] -&gt; handleEmptyList (x:xs) -&gt; doSomething x xs
You mean like the post I replied to? I think you're taking things a little too seriously.
The experience report said it was implemented in C because it would run faster (No GC etc). The repo says it is possible to run the Haskell version. 
Right, but I'm not sure how usable the Haskell code would be as an actual Kernel? I think it's just an executable specification? Which is awesome! But it's not really the same thing as building a usable Kernel in Haskell.
&gt; In theory the stack+stackage solution is less work total because even though some people are working hard to keep things working, the end users benefit by just having things work. With cabal-install, there is less upfront work, but results in lots of individual users having to solve problems on their own when things don't work? Yeah, I'd say that's pretty accurate. The amount of actual maintenance work on packages is essentially the same in either case. The difference is when you get notified of the breakage, and so also when it gets resolved. With stackage, breakages are detected and resolved early. For packages outside of it, things tend to get fixed when users complain. Detecting these breakages earlier improves the health of hackage in general, and it means that cabal-install's build plans are more likely to work out for recent hackage packages than they were pre-stackage. The cabal-install approach leads to "Maintainers better get everything 100% correct, otherwise users are going to feel the pain". Stack's approach is "Maintainers can't get everything 100% correct, lets shield users from these issues". Perhaps this could be resolved with adequate tooling for determining version constraints. However, right now it takes a lot of effort to maintain broad version bounds, and ensure that they are correct for every release. It seems like many folks just throw in the towel and put down `&gt;= version_im_using &amp;&amp; &lt; bumped_version`. The main concern I've heard about Stack is that it makes it easier to omit or use incorrect version constraints in cabal files. The source of this concern is that stack allows you to have no version constraints in your cabal file, and have the build still work. However, this is also true of cabal-install. If you leave all the constraints off of packages, roughly speaking, cabal-install will try to use the most recent versions. If these packages are all in stackage, building will probably succeed. It is the package maintainer's responsibility to add appropriate version bounds. If anything, issues with version bounds are now way more tractable, as you are dealing with an explicit set of dependencies, rather than a point in the space of version choices, which keeps on changing under your feet. Additionally, lets say that hypothetically stack did cause hackage packages to have less accurate cabal version constraints. I, for one, would much rather have a world where we can reliably build our code, and where newcomers aren't discouraged by their initial attempts at getting things working. Being able to deliver a project and all-but-guarantee that it will build a few years from now is HUGE for using Haskell in an industrial setting. Being able to sometimes use older packages along with newer packages is simply not worth skipping out on these benefits. Good news is that we *don't* sacrifice this, and the concern over stack negatively affecting version constraints seems to me to be FUD.
I'm new to Haskell, so I don't understand the difference between the two. Could you explain it to me?
That sort of layout just kind of bothers me. It's not something I find genuinely objectionable, just kind of not-right. I have no justification for that at all, but I'd just prefer something like: main = print (join [x, y]) where (x,y) = ("hi", "world") join = unwords And apparently I'm one of those very few Haskellers who prefers tabs. I just like the idea of 'one tab = one level of indentation'. Within lines, I align things with spaces (because I'm not insane) but at the beginning of a line, tabs just seem like the right thing to do.
You should probably also post the version of Stack that you're using. They can often fix things like this faster than the original authors. But that's bizarre, the exact same version installed for me on Windows with some pretty easy tinkreing. Did it download the GHCJS package successfully? Sometimes just running the binaries included with that will work on their own.
&gt; Speaking of changing under my feet, this wasn't part of your comment before :) . Almost made it look like I agreed with this. Sorry, pressed "send" too early, the edit was only 1-2 minutes afterwards... but you quoted the part you replied to anyway, so there's little confusion I think :) As for declaring universal version ranges vs stabbing single mono-build configurations on the global-set: This, I guess, is a fundamental philosophical difference in how things should work. I think PVP-structured version ranges are currently the best approximation we have right now to declare API requirements. Stackage doesn't seem to need this at all, in fact, Stackage would work just as well without any lower/upper bounds at all. Moreover, Stackage probably even *suffers* from properly declared upper bounds, as they have to wait on all maintainers to bump their upper bounds before every package in the Stackage snapshot can finally march on in lock-step. Also, I doubt we can ever rely on a fully automated process to infer version bounds, as it's not enough to just ensure that something compiles in order to be correct, unless we start encoding automatically verifiable contracts into the types. But Haskell's not there yet. I don't trust maintainers that leave off upper version bounds or blindly bump an upper bound without having verified the changes that prompted that version bump in the first place. Case in point: `aeson-0.10` broke almost everyone who blindly accepted the `0.9`-&gt;`0.10` changes without reviewing just because the code still compiled.
Pacman was ported on windows while ago and its now package manager used in MSYS2. There are already plenty of [packages](https://github.com/Alexpux/MSYS2-packages) that can be installed with it.
&gt; What about the objective truth of the rapid rise of popularity of stack and the praise it receives from its users? https://yourlogicalfallacyis.com/bandwagon :-) As I said, it's hightly subjective IMO. The current popularity of Stack could easily have reasons other than being objectively superior to Cabal. Maybe it's just hype of Stack being the hot new thing, maybe it's because the Stack community is being generally more actively helpful to help out each other, maybe it's just being advertised better. I tried Stack, and was rather disappointed. Maybe others force themselves to stick with Stack until they accept its different (but not necessarily better) mode of operation. I really don't know, and honestly doubt we can ever find an answer to whether Stack is *objectively* better than Cabal. Personally, I see Cabal and Stack as incomparable Apples &amp; Oranges. They have some overlap, but ultimately they cater to different use-cases. Stack seems tailored to enterprise users who want a manually vetted set of packages and a tool that magically does the right thing in most cases, while cabal seems more tailored to Haskell developers used to having full control over their toolchain.
There exists an open issue for stack: [Can't setup ghcjs with stack: ghcjs version could not be determined. #1496] (https://github.com/commercialhaskell/stack/issues/1496)
It depends how it's parenthesised. forall a. forall b. (a -&gt; b) is not the same as forall a. (forall b. a -&gt; b) (for example). 
Stackage doesn't require direct author involvement. There're packages which users of packages have submitted because they want them in stackage and are responsible for dealing when a build problem arises, typically they go and bother the author to update their package or submit a PR. As an author recently "bothered" by /u/snoyberg about making hindent work with the latest HSE, it's in any case a useful bothering that I'll have to deal with eventually when a user tries to cabal install and hindent won't install.
Thanks a lot.
Where can I read more about the game itself? &gt;this game doesn’t require human input, nor all the magical UDP networking and iterpolation/extrapolation tricks of a first person shooter This has intrigued me. Is the whole thing text based? Users type in text and they see the game change based upon that?
My knowledge here is second-hand, so I might as well direct you [to the wiki](https://wiki.haskell.org/Internationalization_of_Haskell_programs). You might also be interested in learning about Yesod’s [shakespeare](http://www.yesodweb.com/book/shakespearean-templates)
The winter holiday is in February!
At one point you could use spaces instead. I think they removed it around the 1.4/1.5 time frame.
The complication is that whitespace is significant in Haskell, so if you use 4 character tabs, you'll get funny issues when you see code like ba = do foo bar which might look aligned, but fail or succeed but look unaligned depending on your tab settings. This is either `do { foo; bar }` or `do { foo bar }`, but which? Regardless of if you choose to see them as 4 spaces in your editor, Haskell sees them as 8. Since so many people customize their tabstop settings in their editor, its easiest to just avoid the issue.
As an anecdote, I used biplate from A to B, where later the two turned out the same, causing biplate trouble. Sameness was obscured by type synonyms (TypecheckedSource vs HsBindsLR AFAIR).
First thing changed: I'm no longer using `liftA`, and can get rid of the import. Whoops!
&gt; version constraints are too tight and often plain wrong (upper bounds are bad) Can you elaborate why upper bounds are categorically "bad"? &gt; the haskell ecosystem doesn't really care much about API stability How exactly do you define API stability? Are suggesting packages don't ever do major-version increment requiring changes? &gt; The biggest problem I see here is that a lot of haskell library maintainers don't have the mindset of oldschool C library maintainers, where breaking the API/ABI is really a big deal. Maybe that's because in C-land you don't have the equivalent of upper bounds to begin with. So you don't have much choice than to avoid any changes that would require a major-version increment according to the PVP in Haskell-land. Haskell is much more flexible here. Our packages don't *have* to work with the latest major versions and bitrot away with every new major version released thanks to upper bounds. So yeah, in C-land breaking APIs is a big deal, because the ecosystem does not have the infrastructure to cope with it.
Thanks! I'll take a look at `Array` and the state monad. I also haven't seen `notElem`, but you're right that it's perfect for this code.
&gt; Can you elaborate why upper bounds are categorically "bad"? Because things *should* break, so they can be fixed. &gt; How exactly do you define API stability? Are suggesting packages don't ever do major-version increment requiring changes? Less random changes. Longer testing period. Working with branches, maybe even package-renaming (e.g. sdl vs sdl2). The frequency in haskell is so high that it can easily leave you in hell if you don't use upper bounds. That's why people use them. &gt; Maybe that's because in C-land you don't have the equivalent of upper bounds to begin with. Sure you have. Things break (e.g. with new libressl API removing functions), so people report bugs, stuff gets fixed. Sometimes it cannot be fixed so distro maintainers introduce upper bounds (for rolling release distros) or don't include the new version in the distro-release. But that's rather the last resort, not the first thing to do. &gt; So yeah, in C-land breaking APIs is a big deal, because the ecosystem does not have the infrastructure to cope with it. I see it the other way around. In C-land we have ABI-compatibility, API stability and working dynamic linkage. Haskell/GHC does not, so we work around it with sandboxes and other hacks, although it's a nightmare for any sort of real-world deployment and a practical maintenance/security problem. Static linking and sandboxing application bundles is bad and requires additional abstraction layers on top of it to cope with the security problems it causes. None of the current solutions (including Nix) handle that properly.
I always found that kind of formatting unnatural, like writing C code like this: if (something) { foo(); bar(); printf("ok, done!\n"); } To me it feels more natural to write: ba = do foo bar ... which works with both tabs and proportional width fonts!
Comments: * The `Light` type is not used at all in the specification, so why is it there? * If I turn on `-Wall` I get warnings about the name `lights` being used as a function argument and a record selector. * If both light and switch names are strings, it is easy to mix them up. A solution is to abstract the types: data Switch lightName switchName = Switch {switchName::switchName, lights::[lightName]} solve :: (Ord lightName,Ord switchName) =&gt; [lightName] -&gt; [Switch lightName switchName] -&gt; [switchName] * Your specification doesn't allow for failure or for multiple solutions. The function should really return `Maybe [switchName]` or `[[switchName]]`. * Since the order of the light names doesn't matter, it is better to use a `Set` instead of a list. * Your solution uses a 'stack' of partial solutions, which is to say, you use the list monad to give a list of solutions. If you are interested in only one, then you could use the Maybe monad instead. * This problem can actually be solved in polynomial time with Gaussian elimination. 
Congrats, the 1.0 (and expected stability) removes the last downside of working with stack! IMHO as someone who has both worked professionally with Haskell on different teams for a few years and observed new users at meetups and hackathons, stack has been the single best thing to happen to Haskell since I have been using the language. I can actually have those new to Haskell just type a command and get an application built without any hassle, and the multi-cabal project ability makes the tooling scale up to larger applications too. It is really a game changer to remove all the friction around building a project. Thank you again!
In a function named "runClient" I won't expect seeing code that deals with streams and parsing of data. You have a "sendMessage" function that writes to a stream. It would be reasonable to have a "readMessage" function that reads a message from a stream. "runClient" could then be something like: forever do msg &lt;- readMessage sock handleMessage server uid msg or `forever $ readMessage sock &gt;=&gt; handleMessage server uid` if you like point free notation. If later you'd want, for example, to make the client parallel or add prints, you could change it right there instead of having to waddle through stream reading code.
I see, so it's more of a naming and organization problem. Thanks for the suggestions!
When it comes to Haskell I am a newbie, but these are things that come up with 10+ years of programming experience. That said, the notion of composability that Haskell so strongly endorses makes it even clearer that functions should and can "do one thing and do it well".
Re. #2, something like this (untested): step :: Instruction -&gt; State -&gt; State step instruction = case instruction of Hlf reg -&gt; nextInstruction . modify reg (`div` 2) Tpl reg -&gt; nextInstruction . modify reg (* 3) Inc reg -&gt; nextInstruction . modify reg (+ 1) Jmp offset -&gt; modify Inst (+ offset) Jie reg offset -&gt; bool (step (Jmp offset)) nextInstruction . (== 0) . (`mod` 2) . readValue reg Jio reg offset -&gt; bool (step (Jmp offset)) nextInstruction . (== 1) . readValue reg where bool :: a -&gt; a -&gt; Bool -&gt; a bool x _ True = x bool _ x False = x (Generally I find it nicer to use `case` when there are many cases, rather than repeating the function &amp; parameter names.) Also, while you could use the `State` monad here, `run` can also be expressed as a fold, using (something similar to) `step`. I’ll leave that as an exercise—which is to say I’m too lazy at the moment. :) 
The suggestion about using case statements is an excellent one, and your justification will help me remember it in the future. Thanks! Your implementation of `bool` isn't quite right because we'd need to pass the `state` to both the boolean being checked and to whatever the result of that expression is. You're right, though, that a small amount of munging would do it (untested): bool :: (State -&gt; Bool) -&gt; (State -&gt; State) -&gt; (State -&gt; State) -&gt; State -&gt; State bool pred true false state = if pred state then true state else false state ... Jie reg offset -&gt; bool ((== 0) . (`mod` 2) . readValue reg) (step (Jmp offset)) nextInstruction However, this might become a moot point, because I'm currently reading up on the `State` monad, and I think that the problem will disappear if I move to using it. I don't see how `run` would be made into a fold. I'm used to folds operating over a list of inputs, and there isn't an obvious one here (note that the list of instructions, which I've since turned into an `Array`, is not the list of instructions to execute in order because it includes jumps to other parts of the list). I could make a list of states the machine is in, one after the next, but that's not very foldy because each one could be discarded after we generate the next state and all we care about is the value in register B of the very last one. I'm going to keep learning about the state monad, and ignore making `run` a fold for now.
&gt;* This problem can actually be solved in polynomial time with Gaussian elimination. Yeah, but what fun is that? Brute force solutions are more fun to write, especially in functional languages, IMO. 
That's a good question. If I defined `totalInstructions` inside `evaluate`, I suspect the compiler would be smart enough to figure out that it doesn't need to re-evaluate it for every recursive call (and so it would just store it once in memory, rather than once per stack frame as we recurse), but I'm not confident of that. I wanted the recursive call to be to a function that did not take the list of instructions each time, so that we didn't need to keep finding the length of the list over and over. and once `run` and `evaluate` were two separate functions, it made more sense to define `totalInstructions` outside of `evaluate` because it does not depend on the arguments to `evaluate`. I'm in the habit of trying to define things in the simplest scope I can (and to give things the smallest scope I can), so it's easier to reason about the code. However, that is not as big a problem in Haskell as it is in many other languages, because side effects are always mentioned explicitly. Perhaps this habit will get in the way and make my code harder to read in Haskell. Do you see an advantage to putting it inside instead?
Wow, thank you for such a detailed response! There's a lot of good information in here, but it'll take me a while to read through all of those links. I particularly appreciate the summary/comparison of the different data structures. I've tried reading through the `lens` package and tutorials on it several times, but never got far. I'm very glad to hear that there are easier starting places. I wasn't aware of /r/haskellquestions before, but am now subscribed. I see in your code you use the `{-# UNPACK #-}` comment. I assume it's a directive for the compiler and not a hint for us humans. What does it do?
Yeah: the same exact issue, it fails at the same exact step: https://gist.github.com/gromakovsky/660735e773213b989d44
Well, I had an easier time installing GHCJS on Windows with stack than I've had installing singular packages with cabal sometimes. As to being objectively better, before long stack-the-tool will probably be (if it isn't already) a complete superset of cabal-the-tool, which kind-of will make it objectively better.
Any information on how many interns you are looking to find? :)
Stack is objectively better already! The things not reimplemented are misfeatures anyway
Oh man, why are you wasting your time trying desperately to convince us that cabal is not obsolete? Don't you have better things to do? Why can't you accept that Stack is superior and everyone's using it because of that? When the next HP will be Stack-driven this will be another nail in the coffin ᕦ( ͡° ͜ʖ ͡°)ᕤ
&gt; I'm not talking checking every single version bump but rather reviewing major version bumps Yes. I was talking about every single constraint version bump, which is indeed major bumps due to PVP. &gt; &gt; If we make package maintainership overly onerous, we disincentive authoring and properly maintaining packages. &gt; &gt; On the other hand, making it too easy on Hackage uploaders would reduce the quality of software published on Hackage. That's why we need tools which make maintenance work easier. Cabal-install makes the price of maintainer mistakes quite large. Most people don't seem to have workflows that automatically catch these mistakes, and it turns into a huge pain point for users. I don't think I've even seen a workflow described, though it sounds like locally running hackage matrix may be a step towards a solution. "Cabal hell" is a problem, and with 4,360 google search results, it's objectively a thing people encounter, and it angers them enough to invoke biblical analogies. Currently, if a maintainer wishes to avoid all of these cases, they're going to need to be really careful and spent a lot of time ensuring they don't happen. Stackage / stack make it so that users usually don't need to take on the burden of tracking down these inevitable maintainer mistakes. That said, just because stack users can easily use a package that has no version bounds doesn't mean that the pressure is off maintainers to have decent version constraints. &gt; On the contrary, those useless "noise" packages make it more difficult to find the proper packages that users are actually looking for. This makes Hackage appear like a random pile of unmaintained software and doesn't reflect well on the Haskell ecosystem. Agreed, the Hackage's open nature is a mixed blessing, and it doesn't have a good way to organize "these are packages people actually use". Now, Stackage is not intended to curate "these are good packages". However, they do indicate that someone cares about these packages, which means they probably aren't nonsense. Ending up in cabal-install hell also reflects poorly on the Haskell ecosystem. Getting bizarre dependency error messages with `Warning: Note that reinstalls are always dangerous.`, just really doesn't encourage confidence in the ecosystem. Oh, this common thing you need to do all the time is also always dangerous?? Hrmmm
Ah, you’re right about my `bool`. Oh well, you could still just use a lambda: Jie reg offset -&gt; \state -&gt; if readValue reg state `mod` 2 == 0 then step (Jmp offset) state else nextInstruction state You *could* use `&lt;*&gt;` in the `(-&gt;) a` applicative to duplicate the state argument (`(f &lt;*&gt; g) x == f x (g x)`) but that tends not to be very clear. 
Glad to hear that my comments were useful. `{-# UNPACK #-}` is a so-called pragma, of which there are [quite a few](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/pragmas.html). This one tells the compiler to embed a data type with one constructor directly into the constructor for another data type. Consider the following example: data Foo = Foo !Int !Int Values of type `Foo` look approximately like this in RAM: &lt;info identifying constructor Foo&gt; &lt;pointer to Int&gt; &lt;pointer to Int&gt; | | v v 5 10 With `UNPACK`, this becomes &lt;info identifying constructor Foo&gt; 5 10 This can be an important optimisation because it avoids one level of indirection, which is critical in tight loops. In this case, I did it more out of habit; the code should easily be fast enough without this trick.
In some cases, a downvote simply doesn't cut it. You get a +1 from me, Rein
what does `h` choosing a functor mean? (does it mean that the function `h` can return *any* Functor it wants? which in the case of `g`, `g` had to work for *all* functors since the caller gets to pick the `f`? If so, is that why we say that "we can emulate existential types by using `forall`"? Since in this case, `h` is asserting that "it has a suitable f" for that particular type, but it doesn't have to work for *all* f?
I think you have the basic idea correct. It means that inside `h`, `f` could be instantiated to anything `h` likes. It could be `Const a`, `Identity`, `[]`, and there's no way for the caller of `h` to know.
Is it just me whose eyes could not tolerate that background? I went in and switched it to white in the CSS and the site looked much nicer.
Yes, if you tell me where it is and how to build it.
Can you show the code? I don't think there's anything anyone can do to help without it.
Yeah, I was kinda expecting someone to mention some magic with `&lt;*&gt;` or `liftM` or something here. It's good to know I'm not the only one who finds that hard to reason about outside the context of more "traditional" data structures.
I didn't try with `-O0` but I tried without any optimisation at all , and the performance are terrible. I'll try.
Sorry, I misread your comment. I'll try that.
I'm probably missing something but how is `Refined` different from `Tagged` ?
I think at the moment the consensus is we should be trying to move things *out* of base, unless moving it in is needed to e.g. define instances for types or classes is base itself. Instead, a package can become a de-facto standard easily outside of base: see e.g. `aeson`, `text`, `bytestring` etc.
I've heard that one pain point of denotational semantics is modeling concurrency. What is the "state of the art" for denotational approaches to concurrency? Also, it seems to me that denotational semantics ignores the whole "complexity" aspect of computation. Is computational complexity the preserve of operational semantics?
regarding the relative newness of the library, usually new ideas are tried and tested for years before becoming incorporated into base. Some things that feel essential or useful might later be found to have better alternatives somewhere else, or their usefulness might end up being more situational than thought, or unseen drawbacks might not be discovered yet. Things get widely adopted first, and get tested out for robustness and real world usefulness. things show up in real world use that can't be reasonably foreseen. After an idea is widely tested and vetted, people talk about moving it into base. It's not the other way around. Things don't get pulled into base in order to become widely adopted; they're pulled into base once they're tried and tested and already adopted. And, in some cases, functionality is decidedly better off left out of base, as a library. A lot of essential haskell libraries exist outside of base. text, mtl, transformers, semigroups, etc., are all widely adopted and used by almost everything project, but people generally agree that it's a good thing they're not in base. And transformers/text doesn't need to be in base for people to take it seriously and write instances for them in their libraries. If you feel like certain libraries should start supporting these, try opening a ticket or submit a PR :) 
&gt; really You are right. It's a typo, I mean OI has NOT been really a problem". I'll fix it. 
`Tagged` means something completely different than `Refined`, even if their implementations/interfaces are similar. With `Tagged`, there does not need to be any connection between the tagged value and the tag; it's an alternative to passing around a `Proxy`, which is a workaround for the fact that we can't pass around a type. `Refined` is related to refinement types and LiquidHaskell, as well as to runtime contracts and is meant to restrict the data that's being "refined".
I think you'll have to post a minimal example that shows the problem.
I'll try to do that. In the mean time, any idea why the profiling could show 97% for a function which do nothing ? Could that be a space leak somewhere ?
I'm afraid I don't know. I've never quite understood how performance attribution works.
Fair enough, however one could argue that tagging an object with a predicate is enough to *link* them and so expect the object to pass the predicate ...
OK, Also there is a bug in the hackage version of diagrams-svg/CmdLine that causes the svg to be pretty printed by default. If you are using the CmdLine module, you can try using the --pretty or -p option when running the code since that will actually turn pretty printing off.
This would entail `base` using multiparameter type classes and scoped type variables.
What flags are you using to profile? Maybe you need to force evalWithError it's own CAF (or make it strict?) Another thing to watch out is that the cost of evaluating the argument of `plan` may being attributed to `plan`?
I've tried with `-p` and it is much faster (1mn vs 10mn) and used much less memory. Now, I'm getting `plan` 12% versus the former 97% which is much more reasonable. 
&gt; I see it the other way around. In C-land we have ABI-compatibility, API stability and working dynamic linkage. Haskell/GHC does not, so we work around it with sandboxes and other hacks, although it's a nightmare for any sort of real-world deployment and a practical maintenance/security problem. These are all desirable things from an industry standpoint, but Haskell didn't get where it is now by doing what's best from an industry standpoint. Up until Haskell et. all proved that it worked and was a good idea, no one in the software industry would accept that global immutability, or global referential transparency, or needing a separate IO type and operations taken from highly abstract math just to get someones name and print it back to the screen, were good or even remotely acceptable things. In fact, I wouldn't be surprised if most programmers were *still* horrified by these ideas. The solution is, as always, to find our own way to do it and do it *right*. If that means the only package manager that can handle Haskell ends up being Nix, well, I'm okay with that.
Well, there is some momentum behind creating/researching refinement types for Haskell that don't require runtime checks. Check out Liquid Haskell (http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about/) to learn more.
evalWithError seems to have it's won CAF, however `mainWith` (used in plan) doesn't. If replace it with `myMainWith` and define `myMainWith = mainWith`, `myMainWith` appears in the profiling report but not `mainWith`. Is that possible that diagrams would not export all CAF or SCC (I'm not sure about the difference)?
I just reviewed my own [posts about profiling](http://gelisam.blogspot.ca/2014/08/homemade-frp-mystery-leak.html?m=1), and it looks like my profiling results were also dominated by a top-level function. Strangely, my post completely ignores that fact without a comment, and I proceed to examine the next few functions in the list, even if they contributed a much smaller percentage. I don't remember why I did that, but it seemed to work!
I had a quick look at your post. You probably assumed that the time given included children and therefore expected `main` to be at the top with nearly 100% .
I linked you to it above. You build it with `stack build`.
In my experience, stack doesn't work well at all if you want to run multiple different versions of GHC simultaneously. I'm sure there's a way to tickle it to do what I want but using the `-w` flag with cabal has been far easier for me. I have invested a lot of time learning how to use cabal and now I have a comfortable workflow which means I can get it to do what I want 99% of the time. Stack on the other hand I find quite opinionated with confusing defaults. For example, the `stack new` command creates a project with a bunch of crub I don't want (like benchmarking). `stack init` never seems to work correctly, when I know there is a build plan which is found by cabal, there are usually various things that I need to add to to the `extra-deps` section by hand. I also usually manually upload documentation for my packages by using a script - which assumes `cabal` - I am sure someone could write a script which used stack instead but everything works well enough for me that I don't have reason to make the changes myself. In addition, the version I have installed `0.1.5` has quite poor documentation on the command line. `stack --help` only lists the commands which you can use but no indication to the expected arguments. `stack help &lt;command&gt;` doesn't provide any more clues. Most recently I was trying to install the package in my current directory with a specific flag, the syntax turned out to be `stack install --flag *:theflag` which I would never have learnt from the documentation so I am thankful someone on IRC helped me out. My final comment is that as a library maintainer users sometimes want me to provide `stack.yaml` files. I usually oblige but I am left in a predicament where there is a sharp increase in maintenance burden to make sure that both the stack and cabal file are in sync with each other. As I don't use stack for day to day development, there will be long periods where I don't update it and only realise that something is amiss when a user reports that it doesn't work. This is just additional friction which I would rather not deal with. That being said, I am happy to use stack files which other people have prepared as working collaboratively becomes much easier. I also appreciate the efforts of the Stackage maintainers, all responsible package authors should try to get their packages on stackage as it's a form of CI that travis can't perform. Stack itself is not for me but I am glad that many people find it a useful tool. 
It is. Note, that they said that (with added parentheses): `(forall f. Functor f =&gt; f b -&gt; f t)` is isomorphic to `(b -&gt; t)` and not: `forall f. Functor f =&gt;` (`f b -&gt; f t` is isomorphic to `b -&gt; t`).
The culprit is in lucid-svg, I never bothered to make this fast since I didn't expect it to be used for large files: https://github.com/jeffreyrosenbluth/lucid-svg/blob/master/src/Lucid/Svg.hs#L60 but perhaps i should now. Anyway, I'll fix the bug in diagrams-svg and push is up to hackage as soon as a get a few free minutes.
It's a strange and interesting question at the same time. It turns out that the focus on compositionality can be put to very effective use for producing visual effects, as you can see in the `diagrams` library. Haskell can be used within Jupyter (the browser-based environment for interactive computation), and this project is called IHaskell. Free as in freedom but with a number of dependencies to be set up on your own. Extremely promising though. It also comes packaged for Mac as Kronos : http://www.kronosnotebook.com/haskell Also, if you own a Mac and feel like shelling some pesos, Haskell for Mac is a proprietary interactive environment, complete with "graphical REPL". Its authors are showcasing fractal generation with it, and hosting their own Haskell tutorial too (http://haskellformac.com/)
What is the difference with `-auto-all` ? Also, those flags seem to only impact my files, but the problem lies in diagrams which is a dependency.
I'm an intermediate Haskell programmer working through your book. I helped start Utah Haskell and we've recommended it for our members. I'm confused by your choice to use mathematical notation in the chapter on lambda calculus, especially with regards to beginners. Are you assuming that beginners will have a mathematical background? Why not just use Haskell syntax in that chapter? I felt like this would be a barrier to many beginners, especially those that didn't study much higher math in college. Many of the programmers I know who are interested don't have a CS degree. Thanks for your hard work! I'm excited to keep working through the book!
&gt; Also, it seems to me that denotational semantics ignores the whole "complexity" aspect of computation. I believe that's true, as e.g. in "Pearl's of Functional Algorithm Design" most every chapter consists of taking a naive O(n^2) algorithm and transforming it into something more efficient using equational reasoning. 
Thanks a lot ! Sadly, im on measly PC, because i dont have a lot in the way of income, so i cant afford a mac computer. But still it will be fun to learn Haskell step by step at first and then i can decide which way i can go
I happen to agree. I find that writing `foo = do something` is a fairly brittle way to write code, if I alpha rename foo to something of different length then it breaks my code. Hence I prefer to start my `do` blocks on the next line, like you. I just don't get to make that call for all the code I wind up editing and, since I prefer indentation by 2 in Haskell, if I just set my hard tabstop there, then someone looking at it indented by 8s would sometimes barely see the code on the screen at all without wrapping. 
Oooohhhhhh. I feel so silly. Thanks a bunch!
Thanks for the kind words and detailed response!
&gt; diagram Thanks
It loops infinitely, trying to traverse the entire (infinite) list in order to compute its list. You can see this from a definition of `length`: length :: [a] -&gt; Int length [] = 0 length (_:rest) = 1 + length rest The only way that it would hit the basecase and return zero is if it reached the end of the list, which an infinite list does not have. Since it can't hit the base case, it can never complete, and thus just loops infinitely. (PS. This isn't the real definition of `length` as it is included in `base`, but it's more or less equivalent.)
because haskell is lazy, it doesn't know beforehand whether the list is finite, so it just starts counting elements until the list ends, which in this case is eternity
I've heard good things about the early bits of the wikibook from another non-programmer who's been dipping his toes into Haskell.
I'm doing some indenting as well based on the tags
can you profile this using diagrams-rasterific and see if you still get the space leak. Also we should take this to the diagrams irc channel, #diagrams. Oh and would you mind filing a ticket at: https://github.com/diagrams/diagrams-svg/issues 
Hey, Thanks for the comments. I was able to fix most of the problems. Also your last comment get me thinking and I realized that I approach this from a wrong angle. I thought this was a version of traveling salesman problem. Every switch is a node and every switch combination is a route that salesman takes so far. So I was checking if the route we took on the tree let us reach to a solution. But there are 3 property of the switches. 1) Order of the switches are not important. So pressing switches in order ["A","B","C"] and ["C","B","A] results the state on the temple lights. 2) Pressing a switch odd number of times are same as pressing it only once. Pressing a button even number of times are same as not pressing the button at all. So solution can have same button 1 or 0 times. So solutions are limited to combination of the switches. So I change my solution and get combination of the buttons and check them one by one and try to find first solution possible. Which I believe is a polynomial time solution. I also thought about solving this with gaussian elimination but I could not figure out how I would convert switches into polynoms. If you have an example to show how to do that I would appreciate. Here is my new solution: https://github.com/huseyinyilmaz/light-switch-problem-solver/blob/solution2/src/Solver.hs EDIT: I am also working on getting a warning on type aliases. But as you suggested, I do not get any warning if I use wrong type aliases. Here is an example usage: https://gist.github.com/huseyinyilmaz/beaff50c8bcad2549ab0/63ebea70e0111983bf1827ecadeb0f1c989192ed I could not yet figure out why your version would generate a warning. 
Well, as you noted, it gets complicated once IO comes in, so do it before IO comes in. That is, I would build a pure structure (using a Free monad, for example) representing operations, a middle pure interpreter layer to modify it given outside information, and then a simpler IO interpreter to run it. The IO level interpreter does not do any authorization or authentication, that's up to the middle layer that works entirely on and with pure code and values. The IO interpreter running a login would take the login attempt and a DB query, and pass that to the middle layer which returns an updated version of the pure structure, and the interpreter interprets the results of that, whatever it may be. This is merely my instinct on the matter, I have not built such a beast.
You can delete your package databases, possibly using a [script like this](https://gist.github.com/simonmichael/1185421). The best way to avoid this problem in the future is to use stack, as you can't end up with broken packages with it (among the many other reasons to use it!).
If you indent with four spaces you get code formatting: class ApiRequest r i v v2 | r -&gt; i v v2 where verify :: r -&gt; IO (ApiResponse r i) process :: i -&gt; ApiResponse r v finish :: ApiResponse r v -&gt; IO (ApiResponse r v2) run :: r -&gt; IO (ApiResponse r v2) run req = ((&gt;&gt;= process) &lt;$&gt; verify req) &gt;&gt;= finish 
Reply will be a bit fisky, apologies. &gt;I'm confused by your choice to use mathematical notation in the chapter on lambda calculus, especially with regards to beginners. It's not "mathematical", doesn't require any math background save for perhaps arithmetic. Even a familiarity with the concept of a function is merely helpful rather than obligatory. &gt;Are you assuming that beginners will have a mathematical background? No. I have none, my coauthor took a symbolic logic course when she was getting her undergrad I think. &gt;Why not just use Haskell syntax in that chapter? Translate the stuff in that chapter into what you think the Haskell syntax would be, type it into a REPL and see what happens. [1] We can't/won't/don't put stuff in first half (two-thirds really) of the book that could cause confusing issues or errors. The second issue is that Haskell syntax is a strictly bigger universe and using Haskell syntax means we have to make an uncomfortable choice between syntax that works nowhere, syntax that works in a file that can be loaded, and syntax that works in the GHCi REPL (`let` statement). That is a sucky choice to have to make and then we're making it harder for readers of the ability to use the _rather large_ ecosystem of resources and tutorials available for deepening their understanding of lambda calculus over a _syntactic_ issue. It's more important to us that readers make independently capable of reading the follow-up resources and self-teach than that we avoid frightening them with something unfamiliar and seemingly pointless. Usually by the time people get to the currying stuff for Haskell in the book, they understand why we put the LC chapter in there. Haskell syntax is huge and complicated, lambda calculus is not. &gt;I felt like this would be a barrier to many beginners, especially those that didn't study much higher math in college. It hasn't been. The reason we added this is people new to programming. There was a specific set of 4-5 testers that triggered us adding the lambda calculus chapter in. Of those 2-3 had never programmed before and were the primary reason we added LC in. The reason it's in there is because we realized we couldn't assume people came to the table with a latent understanding of the following things are, mean, or how they work: 1. evaluation 2. execution 3. functions 4. variables 5. binding 6. alpha equivalence 7. currying (this one was really a bonus, not why we added it, but adding it helped with this too) Lambda calculus gives us an _extremely_ compact universe in which to convey the first bits of these ideas. Programmers with experience in an imperative language will already have at least half of these ideas and be able to proceed without an LC chapter under their belt, but total beginners have _nothing_ to go on. By repeating the "mathematical notation" thing, I think you're getting caught up on optics rather than content. I can and have taught lambda calculus to kids not yet of age to be in middle school. [My co-author's 10 year old son made his way through the lambda calculus chapter](https://superginbaby.wordpress.com/2015/04/08/teaching-haskell-to-a-10-year-old-day-1/) with little assistance and no background in anything other than the arithmetic you'd expect a 10 year old to know. Well, and a little Java and JavaScript from doing some tutorials, but that's not "math" per se. &gt;Many of the programmers I know who are interested don't have a CS degree. That's fine, I don't have any degree, let alone a BS in CS. Neither does Julie have a CS degree (she's a linguist, researched Coeur d'Alene) or most of our testers for the book. [1]: Say they try a simple example in the REPL, like `(\x -&gt; x) (\z y -&gt; z)`: Prelude&gt; (\x -&gt; x) (\z y -&gt; z) &lt;interactive&gt;:2:1: No instance for (Show (t1 -&gt; t0 -&gt; t1)) (maybe you haven't applied enough arguments to a function?) arising from a use of ‘print’ In a stmt of an interactive GHCi command: print it If we use Haskell syntax, they're going to expect a useful result to come out of this. This is not okay. We can't do stuff that'll potentially mislead people or we get emails about how `${thing_reader_expected_to_work}` doesn't work. It's not acceptable to ask them to install libraries (lambda calc stuff) either because we haven't really explained _anything_ at that point in the book and I'd rather it just be something with the decades of resources behind it that you can do on pen and paper. Even if we did have them install an evaluator for lambda calculus in Haskell, it'd give them an obvious means to cheat themselves on the exercises and it would probably only accept lambda calculus syntax as input because it's simpler! &gt;Thanks for your hard work! I'm excited to keep working through the book! Glad to hear it. _Thank you_! We've been excited to hear more from the Utah Haskell group as they go through the book :) Also pretty jazzed about seeing what people think of the upcoming release, it's a biggie!
&gt; why hasn't anyone mentioned gloss? It's a good thing to mention, but it's a fair bit lower-level than what people coming from Cinder or Processing would expect, I think. Example: https://github.com/paulhoux/Cinder-Samples/blob/master/BouncingBalls/src/BouncingBallsApp.cpp I didn't want to imply they were comparable, was hoping somebody would have something higher-level written on top of GL or Gloss.
Thanks a lot ! As a side question, what would you said on having a raspberry pi ( ver 2 ) put as a secondary, "learn programming" only computer ?
I would recommend using stack. If you don't want to use stack, then you can use a stackage snapshot with a cabal sandbox.
Run `top` or the like while you do it...you will probably see one of your CPUs go to 100%.
It does seem like that rewrite rule should always improve performance, but here are a few thoughts: * it would change the semantics for infinite lists from `_|_` to False. I think rewrite rules often have that effect, so that should be fine. * `length` is now a method of [`Data.Foldable`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Foldable.html#v:length), but `take` only works on lists, so the first expression works for any Foldable while the second only works on lists. I guess that's not really a problem though, as [the documentation for rewrite rules](https://downloads.haskell.org/~ghc/7.10.3/docs/html/users_guide/rewrite-rules.html) says that "a rule only matches if the types match too".
You seem to assume ghc uses lazy evaluation on the the level (look at IfOrd). It doesn't necessarily. In fact, it's unspecified. Rewrite your code assuming strict type level evaluation, and see what happens. 
I only get that message when I hit a diamond dependency afaik. So I work to disentangle that diamond dependency -- either through a careful upgrade of the packages that need upgrading (if I decide they're at the "leaves" of my dep tree) or through locating where the "knot" is in the graph, and manually tweaking the deps of the package inducing it. Often, if I notice a package is throwing that sort of thing at me a lot, I try to avoid it, or if I notice that something induces lots of weird dependency couplings (tying newer versions of some thing to old versions of others) then I only use it in a sandboxed setting. There are lots of ways, once you recognize that the system is under your control, to choose to have the packages you need besides just tossing force reinstalls around aimlessly.
It's possible (even straightforwardly, not by cheating a la generators), but it's always equivalent to an infinite loop.
The reason everyone likes stack, that I've seen, is that it can leverage stackage for consistent package sets. Given that you work on stack, of course its command line syntax is convenient to you. But absent the ability to smoothly tie into managed dependency sets, it goes from sometimes less work than cabal to nearly always more work than cabal since as you say, you would need to manually list dependencies everywhere, which is a fair amount of work. I agree that package version sets make sense outside of stack, but I tend to think the converse isn't as true...
&gt; The Utah Haskell group... &gt; pretty jazzed... &gt; Utah. &gt; Jazz. [Ayyyy!](https://s-media-cache-ak0.pinimg.com/736x/c7/2f/1a/c72f1a4b031c06f97d89138307717dbf.jpg)
Oh certainly, I'm just pointing out that without laziness, you'd get non-termination before you even got to taking the length.
That sounds like a very laborious and process to go through for something which I'd consider to be essentially a bug. No-reinstall cabal will aid this particular case. Why the heck is a diamond dependency a thing that should require manual intervention? I'm very confused that you think this is reasonable, cuz I know you're a smart guy! This is just one example of cabal-install not "making the right thing easy", thankfully one that will hopefully be fixed in the future.
Ah indeed ... http://lpaste.net/5437903044593319936 ghci does okay with a strict-list version of this exact program, but if I extend the lists a bit, the lazy one changes hardly at all, while the strict one goes crazy (see `t1` - `t4` at the bottom) 
Exactly as you'd expect: you get an infinite loop. The problem is that length isn't well-defined for infinite lists. Functions defined on infinite lists need to be defined by corecusion. But the definition of length is a recursive definition.
Well, depending on what exactly this means, this is probably not a great idea. If the sentiment is that they should be separate codebases but "base" should still depend on them, then I can see how this is okay. The problem becomes when a package has to depend on a half-dozen de-facto standards. You give up what could have been good defaults, you create more headaches for interoperability, you make things harder on beginners, and you increase the pain of Haskell's poor tooling.
Glad somebody got it :)
Does it automatically integrate with eg. FromJSON? instance (Predicate p x, FromJSON x) =&gt; FromJSON (Refined p x) where parseJSON value = do raw &lt;- parseJSON value case refine raw of Right value -&gt; pure value Left reason -&gt; fail reason
Agreed! I interpreted "highly coupled" in terms of software coupling, not in terms of utility coupling (when one thing is much more useful given something else). The distinction between stack and stackage is still valuable!
Gotcha, thanks for the explanation! Yes, cabal-install is certainly tenable for those who know all the ins-and-outs!
Thanks for writing this up at the value level. That was really helpful I'm so used to lazy evaluation it still took me a solid five minutes to realize that ifOrd' makes union' O( n^3 ).
Yeah, that's definitely the problem. Thanks for pointing that out. I'll see if there's a way to rewrite Union that doesn't force the evaluation of three subproblems.
In general you should install new platforms over the old ones, as it just swaps out the symlinks. If you just want to go from 7.10.2 to 7.10.2-a you should not do that but instead just download and use the new "activate-hs" which sets up the symlinks properly for you (https://github.com/haskell/haskell-platform/blob/master/hptool/os-extras/osx/bin/activate-hs). But now that 7.10.3 platform is out, I'd recommend just using that instead :-)
Perhaps something like `c ~ CmpSymbol a b =&gt; IfOrd c a a b ': Union (IfOrd c '[] '[] '[a] '++ as) (IfOrd c '[b] '[] '[] '++ bs)`. I'm sure `'++` isn't right but I'm not familiar enough with the current state of the type-level libraries to use the right name for it; hopefully the idea of pushing the conditional down into the calls is clear enough, though.
It's a simple rewrite, if you give up on IfOrd. 
&gt; the pain of Haskell's poor tooling IDK, Stack and LTS Haskell seem to do a very good job, haven't had any problem yet. &gt; You give up what could have been good defaults &gt; you make things harder on beginners Super 100% true.
Post Removed, User Banned, Sorry for not catching the spam quicker.
As augustuss said, strict evaluation is causing all branches to be evaluated. Here's a go without IfOrd: http://lpaste.net/147942. There may be a better way to express that, but it compiles nearly instantly on my machine.
Some refined types can be functors, for example ones limiting the size of a container without imposing any constraints its elements. I think classes like Functor should be handled on case by case basis: class FunctorPredicate p instance (FunctorPredicate p1, FunctorPredicate p2) =&gt; FunctorPredicate (And p1 p2) instance (FunctorPredicate p1, FunctorPredicate p2) =&gt; FunctorPredicate (Or p1 p2) instance (Functor a, Predicate p a, FunctorPredicate p) =&gt; Functor (Refined p a) where fmap a = Refined (fmap a) data MaxLength (n :: Nat) instance Predicate MaxLength [a] where validate _ value = ... instance FunctorPredicate MaxLength 
In ghc-7.8.4 and earlier, type level computations appear to be lazy. This program provided here doesn't compile on my 8G laptop (crashes with out of memory) and likewise for a particular project I've attempted to keep alive. ghc-7.10.1 feels like quite a significant breaking change for those that lived by that assumption. I don't get why they changed it.
+1 for being the only one to mention that the OP forced the evaluation.
Wow, this is pretty sweet!
One must differentiate between a contract applicable to the entire container (say, "this list is ordered") and a contract applicable to a single element in the container (say, "this number is positive")
I'm still a noob in Haskell and I hope the situation will improves in the next years, but for now I have the feeling that with Haskell I'll never have the performances I can achieve with C++ just for the reason that Haskell hides too much of what the code is actually doing. This situation is acceptable and wanted in some computer science fields where quality or reliability are more important than raw speed or where speed can be achieved by smart architecture, and in this case it seems that Haskell is competitive. But when you only cares about raw speed, you need to understand what the code is doing. I'd really like to be proved wrong on that point ;)
As it turns out, newer GHCs have an option to do exactly that (`-funbox-small-strict-fields`), enabled by default. ([Docs](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/options-optimise.html#options-f)) There's a remark in the docs about this 'rarely' making things worse; not sure what this is alluding to.
Excellent, thank you. One more argument for `microlens`.
Or using the same parts of the compiler backend that TH uses in GHC for this purpose, yes.
At first I thought it was some leaked proprietary code, then it just seems to be someone experimenting with the idea and don't want to use github for some reason, then I noticed they self-described as a "Big Data" business so I guess it's a company after all. The code isn't really complete but interesting to read I suppose. I'm still confused.
That is interesting. I wonder if the strictness causes better performance in some situations, given that GHC's type level is effectively total.
https://www.youtube.com/watch?v=6klfKLBnz9k&amp;list=PLDFjrbtSUOm7_0pUXwfln1P3qweb1Ol3t is a playlist for last year.
It would appear to be related to http://logicaltypes.blogspot.com/. I base this entirely upon the 'geophf' and '1HaskellADay' components reference in the .csv name.
&gt; semigroups This one should be in base because SemiGroup should be a superclass for Monoid which is in base.
FYI -- [the location via google maps] is incorrect because google maps doesn't give the right result unless you enter precisely "4 MetroTech Center". [This is one you want]. If you enter "4 MetroTech Plaza" or pretty much any other variation, you get 1 MetroTech Center as the result. Pretty much everyone gets a bit lost on their first trip to the location for that reason. Source: currently sitting in 4 MetroTech, got lost on my way to interview months ago [the location via google maps]: http://www.composeconference.org/2016/ [This is one you want]: https://goo.gl/maps/XQ8Dga9HamF2
Frankly, no, I wasn't wondering at all.
well, I am not sure about it... In what situation do you use that ?
Oh god the first video is earrapey
**YIKES!!!** Why would I want my Haskell code to touch *anything* related to Microsoft? The impurity of that company should be excised from, not integrated into, any codebase. Also the claim that these are the "latest technologies" is dubious at best...
In [subhask](https://github.com/mikeizbicki/subhask) `pand` becomes (&amp;&amp;) The type of `(&amp;&amp;)`¹ (&amp;&amp;) :: Boolean a =&gt; a -&gt; a -&gt; a Since we have an instance for `Boolean b =&gt; Boolean (a -&gt; b)` we get: (&amp;&amp;) :: (a -&gt; Bool) -&gt; (a -&gt; Bool) -&gt; (a -&gt; Bool) (f &amp;&amp; g) a = f a &amp;&amp; g a and (&amp;&amp;) :: (a -&gt; b -&gt; Bool) -&gt; (a -&gt; b -&gt; Bool) -&gt; (a -&gt; b -&gt; Bool) (f &amp;&amp; g) a b = f a b &amp;&amp; g a b In this system `(==)` becomes more interesting as well. Instead of always giving a Boolean, it returns a type family `Logic a` when comparing `a`-values for equality: (==) :: Eq a =&gt; a -&gt; a -&gt; Logic a with the following instance instance Eq b =&gt; Eq (a -&gt; b) where type Logic (a -&gt; b) = a -&gt; Logic b (f == g) a = f a == g a (==) :: Integer -&gt; Integer -&gt; Bool (==) :: Bool -&gt; Bool -&gt; Bool (==) :: (a -&gt; Bool) -&gt; (a -&gt; Bool) -&gt; (a -&gt; Bool) (==) :: (a -&gt; b -&gt; Bool) -&gt; (a -&gt; b -&gt; Bool) -&gt; (a -&gt; b -&gt; Bool) In my work on EDSLs (embedded domain-specific languages) this is tempting because to compare two ASTs for equality a language-level `Bool` is insufficiently rich. Let's say I have numbers and variables in my language,² how would I implement `(==)`? {-# LANGUAGE InstanceSigs #-} data Exp = INT Int | VAR String instance Eq Exp where (==) :: Exp -&gt; Exp -&gt; Bool INT i1 == INT i2 = i1 == i2 INT _ == VAR v2 = error ("Don't know value of " ++ v2) VAR v1 == INT _ = error ("Don't know value of " ++ v1) VAR v1 == VAR v2 = error ("Don't know value of " ++ v1 ++ " or " ++ v2) I do not know the value of `VAR v` statically, instead I can add `EQUAL` to my language and define it as: data Exp = ... | EQUAL Exp Exp instance Eq Exp where type Logic Exp = Exp (==) :: Exp -&gt; Exp -&gt; Exp (==) = EQUAL In that setup I would need to define the rest of the instances for `Exp` since it is it's *own* logic, and must be a Boolean logic. ¹ In a simplified [alternative hierarchy](https://ghc.haskell.org/trac/ghc/ticket/10592#comment:12). ² Viz. [Hutton's Razor](http://stackoverflow.com/questions/17870864/where-is-huttons-razor-first-defined).
You can write a version of `shorter` that works on any `Foldable` structure: shorter :: Foldable f =&gt; f a -&gt; Int -&gt; Bool shorter = foldr f (const True) where f _ a n | n &lt;= 1 = False | otherwise = a (n-1) 
One possible typo: &gt; `:&lt;&gt;` is a combinator that ships with Servant. It allows us to combine the various parts of our API into a single type. I believe the combinator in question is `:&lt;|&gt;` Other than that, great work! I have a similar Servant/Elm experiment [here](http://github.com/nicklawls/lessons). It has an example of talking to an external REST API from within the Servant handlers if you need some inspiration for future iterations on this project. 
Indeed, labeling sharepoint as "lastest technologie" made me laught so hard that my abs are hurting me.
I've tried this a little. It's super fun once you get it working, but I had a hard time wrestling with linux audio and various packages first. Ended up getting it to work only in Atom, but I really want vim.
You might be surprised to hear that Simon Peyton Jones, the closest we have to a [BDFL](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life), is a Microsoft [employee](http://research.microsoft.com/en-us/people/simonpj/).
I am not sure if it was exact reason for not letting a "real" language in nginx (and rather they meant only IO computations, not all types of calculations). But you're right: slow IO during a request is extremely harmful for nginx. In case of this module haskell type system eliminates accidental or intentional involvement of IO actions and this ensures that request won't slow down due to IO.
Yep, if you look [here](http://logicalgraphs.blogspot.nl/p/executive-summary.html) on Logical Graphs you'll see some of the same pictures as [this post](http://logicaltypes.blogspot.com/2015/12/november-2015-1haskelladay-problems-and.html) on Logical Types.
Good idea. https://github.com/commercialhaskell/stack/commit/dfbc221bc6fd7dfcce1fc9fad0474669b2f04c3e
This might help explain "higher rank" uses of forall: https://ocharles.org.uk/blog/guest-posts/2014-12-18-rank-n-types.html One prominent use is in `runST`, whose argument type `forall s . ST s a` means you can only use it on an `ST` computation that allows any choice of `s` (as in, `runST` could choose any `s` it likes), instead of you supplying a computation that uses some fixed `s` (like you would end up with if you tried to access an STRef that wasn't created within the computation). In GHC, `IO a` is actually equal except for newtypes to `ST GHC.Prim.RealWorld a`. So, `runST` is actually pure, but a function whose type was `forall s . ST s a -&gt; a` instead would be as bad as `unsafePerformIO`. 
&gt; When grouping by column 8, which results in about 200 distinct keys of average length 36, I get the following heap profiles &gt; &gt; [4 graphs saying that ARR_WORDS take 60k, 70k, 3800k, and 4000k] &gt; &gt; [...] &gt; &gt; When grouping by column 13, which results in about 28,000 distinct keys (so 1/23 of the file) of average length 14, I get the following heap profiles &gt; &gt; [4 graphs saying that ARR_WORDS take 32M, 70M, 200M, and 200M] &gt; &gt; [...] &gt; &gt; I'm trying to understand what the `ARR_WORDS` section represents Maybe the bytes allocated for the empty vectors? If I understand your code correctly, an "empty" vector isn't a vector of zero bytes, but a vector with the same number of bytes as if you wanted to copy your record data into it, but with the record-copying code commented out. Since you allocate one for each distinct key and you now have 28,000 distinct keys instead of 200, and you allocate 14 bytes instead of 36 each time, I'd expect the memory usage for those vectors to be `(28,000/200)*(14/36)` times bigger for the second set of graphs than for the first set of graphs, so I'd expect the second set of graphs to say that ARR_WORDS takes 3M, 3M, 200M, and 210M. That's about what you get for the last two cases at least. Anyway, if I were you, I'd try to get the file out of the equations entirely, by generating random bytestrings matching the distribution you expect to find in the file, this way you'd either eliminate or confirm your suspicion that the file being retained is the source of your problems. And then I'd try eliminating bytestrings entirely, to see if it's those empty vectors which are using all this space. And so on, testing my assumptions until I understand the true cause.
Nice work! Disclaimer: all of this is untested; maybe I missed something. Also, I'm relatively inexperienced with Haskell and might not have good advice yet. - In `getLight` and `getSwitch`, you use lambda functions of the form `(\l-&gt;lightName l == ln)`. I'd have made them by composing smaller pieces instead: `((== ln) . lightname)`. but that might just be personal preference. - In `isSolved`, you use `and $ (map (isOn) ls)` while I'd have done `all isOn`. - I think the call to `step` could be replaced by the use of the `State` monad. However, I'm brand new to it (I started reading about `State` [two days ago](https://www.reddit.com/r/haskell/comments/3ybcf7/please_critiquereview_my_code/cyc1fau)!), and don't fully understand it yet. So, I might be leading you astray here. - Instead of using lists to keep track of the options you've tried so far, I suspect there's a way to do it with sets, so you avoid duplicates and don't need to filter them out with `nubBy` afterwards. but I don't actually know; my knowledge of Haskell isn't that advanced yet. - Similarly, I imagine that using sets would give better performance than using lists to store the names, as in `turnLight`. - `turnLight` seems to be doing several things at once. My instinct is to break it up so that each function does a single thing. I'd make a function called `toggleLight` that changes whether the light is on, and call it from `turnLight`. Note that this also removes the need for pattern matching in that function: you can call `toggleLight` and `lightName` instead of having the `@(Light ln on)` in there. - Minor formatting suggestions: put spaces after commas, don't put spaces before commas, put spaces on either side of a `.`, the word "Reprets" is probably supposed to be "Represents" instead.
Ah, awesome! Finally a version I can actually read; The print quality in the hardcopy (at least my one at home and all others i've found in shops) is so poor that barely any of the symbols are visible. 
CT is "good to have" for advanced Hs understanding/usage, but the relationship goes both ways; Haskell lets us understand aspects of abstract algebraic theory through an experimental, hands-on approach. I think it's also good to know some Haskell to grok category theory.
So, well, when we put the forall in the brackets, the callee gets to pick a type, unlike the usual case where the caller gets to pick a type? If so, why is it that the brackets / explicit notation of types works out as an existential qualifier? Like, how does the syntax imply the semantics? 
This is fantastic. Thank you for posting this link.
[Computational Category Theory](http://www.cs.man.ac.uk/~david/categories/) is another interesting free book. The code is in ML, so it's not too far off from Haskell. 
I updated the maps on the site. If there are any places that I missed that are still incorrect, please let me know.
Do it, though? If you reduce everything in parallel, your program will both crash and give you the answer, even without laziness. main = print $ head [1, [0..]] This will print 1 even though you have strict evaluation, as long as you reduce redexes in parallel.
This really depends on what you mean by "lazy" and "strict". The formal definition I'm familiar with is that "strict" means that any function applied to bottom yields bottom. This means that since `enumFrom 0` evaluates to bottom, so does `enumFrom 0 : []`, and so on, so the whole expression evaluates to bottom. What you're suggesting is a possible evaluation strategy, but I suspect it would not qualify as non-lazy in any formal way.
I like the author's use of coproducts to separate concerns. I've really only used free monads to run the same programs/grammars in many contexts. Your file system calls could be a REST API, a database, windows, unix, and on and on. I'm currently enjoying the wonderful relationship between free monads and ghcjs.
I've been meaning to try out this sort of approach to programming, but I haven't yet been tasked with a suitable problem. My plan is to try [Van Laarhoven free monads](http://r6.ca/blog/20140210T181244Z.html) because then you get a record structure for operations. Use lenses one can pick particular operations from an abstract record of operations. By keeping the record of operations abstract you should be able to write code that is polymorphic over any free monad supporting the required operations. I also expect that the van Laarhoven free monad will have fast interpretation functions without needed to apply a Codensity transformation. But I'm only at the idea stage. Things might not work out as well as I hope when it comes to doing.
It's all good, you certainly bring up a legitimate concern.
show can be slow, try readInt. https://hackage.haskell.org/package/bytestring-0.10.6.0/docs/Data-ByteString-Char8.html#v:readInt Also remember that print uses show and can be slow as a result. Also replicate generated strings and is probably slow, use bytestrings replicate: https://hackage.haskell.org/package/bytestring-0.10.6.0/docs/Data-ByteString-Char8.html#v:replicate
Does a lot of what Haskell can but 15 years too late...
Hey thanks for the feedback you're absolutely right in general. I just stuck the show/replicates in there as the easiest way I know to generate some non-static bytestrings.
Rather than using the inefficient lazy Peano encoding of natural numbers, you can always [do the efficient thing](https://hackage.haskell.org/package/list-extras-0.4.1.4/docs/Data-List-Extras-LazyLength.html): -- The second argument is intended to be (&lt;), (&lt;=), (&gt;), (&gt;=), etc lengthBound :: Int -&gt; (Int -&gt; Int -&gt; a) -&gt; [b] -&gt; a lengthCompare :: [a] -&gt; [b] -&gt; Ordering FWIW, as noted in the documentation, I used to provide rewrite rules for these functions— before I saw the error of my ways. As mentioned by some other folks, these operators have different semantics than using 'length' but rewrite rules should never alter a program's semantics. Having programs suddenly loop forever after a minor seemingly insignificant rewrite (e.g., let-binding the length of the list rather than having it inline) is the most obvious problem, but not the only one. As /u/augustss taught me, having fewer bottoms is not a virtue in and of itself.
Could someone compare this approach to programming with interfaces and dependency injection in a language like Java? (John A De Goes calls this approach "ultimately criminal." :) Is the difference just that the Haskell version comes with a guarantee of purity? I used Free monads for a bit in Haskell after reading Gabriel Gonzalez's posts, but have not delved deeply enough to make this kind of comparison. I'm asking this as someone who hopes to gain some insight for writing better programs, but writes code all day in languages that have no free monads!
They don't appear to be licensed. Assume that you cannot reproduce or create any derivative work. Certainly don't distribute anything based on these. If you work in the field and want to be careful, consider not reading them until/unless someone can confirm it's not stolen code. (As always, I don't wear my attorney hat on /r/haskell; talk to *your* lawyer if appropriate.)
Being able to do all those things but still having mutability is scary. There's a theory that Lisp has never taken off because it is so powerful every user creates their own eclectic, incompatible dialect, mitigating the power of the language. I fear/suspect Perl 6 will out-Lisp Lisp. Even when Lisp goes macro-crazy, _most_ macros are still sexprs. How crazy will Perl 6 get at scale without that constraint? Even the grammar won't be reliable. It also looks to out C++ C++, in that simply looking at one line of code won't tell you much about what it really does by the time all the features are done firing. Alternatively, the community will develop a dialect of Perl 6 that is safe, by refusing to use most of it, in which case, what's the point? My opinion of it might be very different if Haskell wasn't there providing an existence proof that so many of those things can be done in such a safer way, and with so much less "power". I'm intrigued, but plan on watching at a distance for a good long time. Oh, and I expect years' worth of "look what I can do in three lines of Perl 6!" blog posts, most of which will involve techniques that wouldn't scale to so much as a 100 program.
I think the type you wrote is wrong: forall b . (forall a . [a] -&gt; b) -&gt; b Don't we need an extra [a] as a parameter so we can actually use our " choice function" like structure against it? forall b . (forall a . [a] -&gt; ( [a] -&gt; b)) -&gt; b Seems right to me since now we are actually getting an. [a] to use. The other explanation made sense, and the idea of having an exists is pretty cool. Are there languages that I can use to play around with these construct is? (Agda?) if so, could you point me to them? And, are there "classic" examples about rankNTypes that I can learn for intuition? 
Speaking as an organizer here, I certainly think/hope so :-) We're the only typed functional programming conference in the U.S. that is geared at a general and not just academic audience. We're doing this because we wanted something like this to exist, and it didn't, not out of any other motive. The feedback from last year was that it seemed to turn out pretty darn good, and I'm hoping for a repeat performance this time around. I'm pretty excited about the speakers lineup as a whole, and especially about our keynote, which I think will be quite different from the usual sort of talk you get at these conferences. Additionally, the "hallway track" where people socialize and talk programming was really great last year, and I imagine it will be again. We're certainly aiming not just to be an event for locals, but something worth taking a trip for -- I know some people came a great distance for compose last year and seemed to find it worth it :-)
No problem. I'd recommend using those functions or using [bytestring builder](https://hackage.haskell.org/package/bytestring-0.10.6.0/docs/Data-ByteString-Builder.html) if possible.
Will there be any official events outside the talks? I'm from the west coast and would love to meet some of the FP people on the east coast. Also, how do you think this compares to more traditional and longer conferences like lambda conf and strange loop?
[A lot more math books from Springer](https://gist.github.com/bishboria/8326b17bbd652f34566a).
Make sure you run `stack setup` first to install the ghcjs compiler. The README in my project explains this.
Optional and gradual typing are not the same. Which does it have? Or does it mix them somehow? 
I think the argument would be improved by removing the straw men. &gt; As functional programmers, we sometimes delude ourselves into thinking that functional code is always significantly easier to understand, more declarative, and more powerful. We do? I sure don't. This is a straw man. It is a belief held by literally no one I know that seems to have been created (and then ascribed to me!) just so that it could be torn down. More straw man examples of "idiomatic code" (according to whom? I think it's bad too) and etc. This is off-putting enough that it makes it difficult for me to appreciate the rest of the post and totally unnecessary.
&gt; This is off-putting enough that it makes it difficult for me to appreciate the rest of the post and totally unnecessary. Thank you! I was wondering why I felt uneasy reading that post. Now, looking at it again, I notice that I would never write a function like blahblah :: Boolean -&gt; Boolean -&gt; Boolean -&gt; IO () What the hell does `Boolean` stand for here? Boolean blindness is bad, regardless of the language. Popular languages have IDEs which show you the names of the argument, which helps tremendously, but I would rather use data RedrawMode = PartialRedraw | FullRedraw data FillMode = PartialFill | FullFill clearTerminal :: RedrawMode -&gt; FillMode -&gt; IO () than clearTerminal :: Bool -&gt; Bool -&gt; IO () (edit: whitespace in code. Alignment is hard without monospaced text in editors)
I will look into it thank you for the help. 
Despite not considering myself subject to such a belief either, I can't help but argue that most of the Haskell tutorials dealing with "real world problems" I've seen suffer from this. I mean there's a lot of awfully entangled imperative code being pushed into masses, and the newcomers read it and consider it the normal practice. What I'm saying is that it is a problem, and as much as with any other problem, the first step to solving it is the acknowledgement. Among other things the post does a good contribution to that.
Is anyone else bewildered by the jumps between languages? Sometimes, it seems like Haskell (unit being `()`), sometimes PureScript (`Boolean` and `Unit`), sometimes Scala. And even the language examples don't seem to consistent, e.g. blahblah :: Boolean -&gt; Boolean -&gt; Boolean -&gt; IO () uses Purescript's `Boolean`, but Haskell's unit `()`. The later example of the `Free` monad uses `Unit` as a value and type, which doesn't work in PureScript or any other language with Haskell syntax (as far as I know): type CloudFilesAPI a = Free CloudFilesF a saveFile :: Path -&gt; Bytes -&gt; CloudFilesAPI Unit saveFile path bytes = liftF (SaveFile path bytes Unit) -- ???? While the overall article conveys a good message, the ever-language-changing examples kind of threw me off-track, instead of helping to understand the matter. **Edit:** Also, wrong code: &gt; Interpeters compose, so we can take the cloudFilesI interpreter, and compose it with the logCloudFilesI interpreter, to yield a new interpreter: &gt; loggingCloudFilesI :: forall a. CloudFilesF a -&gt; Free (Coproduct LogF HttpF) a &gt; loggingCloudFilesI f = toLeft logCloudFilesI *&gt; toRight cloudFilesI Apparently, `loggingCloudFilesI`forgets to do anything at all (missing `f`on the right hand side). The article feels somewhat rushed.
If you are complaining that functional programmers as an entire (apparently monolithic) group write code that is difficult to understand and not intention revealing and the best example you could come up with to demonstrate this is blahblah :: Boolean -&gt; Boolean -&gt; Boolean -&gt; IO () then how can I possibly take your argument seriously? This is the strawiest straw man I've ever seen. There is a strong case to be made that `IO` doesn't "mean" anything—that it has no denotation, as Conal says—and that we should instead choose carrier types that both limit and denote our effects. It doesn't need to be bolstered by this army of shoddily-constructed straw people.
Sure, but I'd rather talk about the real problem without having to dispense with all these poorly-built straw people first.
[This definition fits](http://agiledictionary.com/209/spike/): &gt; # Spike &gt; A story or task aimed at answering a question or gathering information, rather than at producing shippable product. Sometimes a user story is generated that cannot be estimated until the development team does some actual work to resolve a technical question or a design problem. The solution is to create a “spike,” which is a story whose purpose is to provide the answer or solution. Like any other story or task, the spike is then given an estimate and included in the sprint backlog. The term originates from [extreme programming](http://www.extremeprogramming.org/rules/spike.html).
There's an error in the first snippet of code: saveFile :: IO Unit should be saveFile :: FilePath -&gt; ByteString -&gt; IO Unit with the appropriate types.
&gt; It doesn't need to be bolstered by this army of shoddily-constructed straw people. I think I saw that Doctor Who episode.
Yes, it's just about choosing more user-friendly defaults (calculations with monetary values come to mind). The silver bullet to slay the beast that is floating point arithmetics still awaits discovery ;)
Aren't they in the public domain, though, by being freely accessible by all?
Except Common Lisp, right? And my grandpa already did all that in Plankalkül while hiding in the trenches from Russian snipers, so there.
Curry-Howard has now grown well beyond the original correspondence between simply typed lambda calculus and the implicational fragment of intuitionistic propositional logic. There's now all sorts of computational interpretations of logics, and existing programming language features have found logical interpretations, for example: * classical logic corresponds to typed lambda calculi with control operators like exception handling mechanisms, * modal logics can be used as type systems for staged programming languages, a la Pfenning, and for languages with meta programming features like Nanevski's contextual modal type theory, * terms of typed lambda calculi extended with meta variables correspond to potentially incomplete proofs in logic, * etc. 
Definitely not. Items on the internet are even notoriously hard to make them universally (that is, over the entire globe) available to the public domain. You need to put in an explicit disclaimer, like so: http://beerendlauwers.be/project-ideas.html
Holy crap. Does anyone know the author of the blog? Why aren't these on Hackage? EDIT: I've contacted him via e-mail. EDIT2: I'll also try to summon him via Reddit: /u/geophf
Toy examples never show what would happen when you scale it to your real program. For example I currently have an rest api which talks to DB with over 30 tables. Most of which can be inserted to, updated and queried. And some have several query variants. I can't even imagine just how much pain it would be to represent this DB api in an ADT, and write interpreter for it. And what would I gain from it? mtl style is reasonably light, easy to use and works well enough There are a lot of toy examples for haskell on the net, but when your program grows beyond "toy" size - you are on your own. Trying to figure out how you should structure it, how to abstract stuff, how to deal with all the complexity.
Was about to post something similar. Very confusing while reading.
This is what they mention on their website. I think it means that the type system is complete, variables are typed either at compile-time or at run-time (gradual) and the type annotations are optional.
Compare these two bits of code -- Note that we require a Monoid instance that we take advantage of instance Monoid e =&gt; Applicative (Either e) where pure = Right -- this analyzes *both* sides at once, the "present" and the "future" Right f &lt;*&gt; Right a = Right (f a) Left e1 &lt;*&gt; Left e2 = Left (e1 &lt;&gt; e2) Left e &lt;*&gt; _ = Left e _ &lt;*&gt; Left e = Left e and -- No longer a need for Monoid instance Monad (Either e) where return = Right -- we only analyze the "present" since we can't get to the "future" unless -- the present "presents" an actual value Right a &gt;&gt;= f = f a Left e &gt;&gt;= _ = Left e This `Applicative` instance cannot correspond to the `Monad` instance because it takes advantage of the greater "analyzability" of `Applicative` in a way that `Monad` cannot follow. Free Applicatives exhibit this same structure (but perhaps more explicitly at the cost of some fairly complex types) in such a way that's always incompatible with Monad. If you structure your type as a free applicative then you'll always be capable of this "future" analysis which is useful for optimization.
I see. The question is then, why use a free monad? It seems the article implies that the free applicative is somehow a fix for some shortcoming of the free monad, but you clearly state that you can't have both at the same time. It seems like a bad fix to me if I have to choose between optimization and branching on previous results.
slides: https://nicolaspouillard.fr/talks/ling-32c3/#/
In my experience, the introduction to category theory in M&amp;M is very partial and requires a fair amount of background -- the "preliminaries" section covering everything from limits to adjoints takes roughly six pages. CWM covers the same material over the span of multiple chapters, in much more depth, complete with more proofs and references. M&amp;M as a whole is a great book, sure. But it is a very rough place to start without some supplementary material -- and the account it gives of category theory is very partial, since it only sets out that portion of CT which it needs in order to develop Toposes (and thus omits lots of material that would be considered important in other contexts). Anyway, that said, I don't think CWM the best intro text either, as it just presents so _much_ material. However, as a reference volume in basic CT results, it is a very good first stop.
I'm always curious about what methods people use to get around floating point incorrectness as the implementation should try to strive for good performance as it is competing with the FPU and I've always wondered how scientists and game developers deal with 0.1 + 0.2 == 0.30000000000000004
For games I can't come up with a simple scenario in which you need to compare with a specific value. Although pretty sure there are some. When I was interested in game programming most things where checks if a value is bigger/smaller and seldom comparisons for equality.
I didn't think of clicking the modules. The API documentation helps a bit yes!
I didn't find the API documentation indeed. I didn't think of clicking the modules. But even if I did, I still would be very appreciative for a little introduction or an example that helps me getting started.
You can use `coerce`. But what you're trying to do really just defeats the purpose of `newtype`s.
[`coerce`](https://hackage.haskell.org/package/base-4.8.1.0/docs/Data-Coerce.html) inserts as many newtype constructors as needed, even when they're nested inside other data types (and it's always no-op). What you describe would be very intrusive. Since any type could be contained inside a newtype, this means that all bindings with type `t` would generalize to `Coercible a t =&gt; t` (essentially wrapping expressions in `coerce`). It would greatly hinder type inference in any case. `coerce (0 :: Int) + coerce (0 :: Int)` already doesn't check without a trailing `:: Int` annotation. 
Using type classes for effects allows you to vary the interpreter backend, but does not allow you to compose and reuse interpreters in different contexts. The point of free is that you can have separate, standalone interpreters, which compose together horizontally (to interpret a coproduct of effects) and vertically (to translate one effect into another). You cannot achieve the same with type classes unless your type classes are just building a free-like structure, which puts you back to where you started. Monad transformers can also possibly emulate some of the benefits (if they use a similar free-like machinery under the covers), by do not easily express elimination of effects by reinterpretation to other effects, and are beset by all the usual problems of monad transformers (such as duplication of monadic machinery, something which free separates out cleanly).
A coproduct of effects is just having two typeclass contexts simultaneously. You can compose transformers in a stack to manage the "vertical" composition, and the mtl makes this easy. If you don't like the way the effects compose, just define a new concrete implementation... I don't know what you mean by "duplication of mondaic machinery".
You cannot define an interpreter for a type class, and compose that with another interpreter for another type class (at least, not without building a free data structure underneath). What this means is that, while you can require a coproduct of effects, e.g.: foo :: forall f. (Effect1 f, Effect2 f) =&gt; f () you will end up with a monolithic interpreter because it has to simultaneously interpret the gigantic coproduct of all used effects. What free lets you do is build separate interpreters, which have no knowledge of each other, and then compose them together both vertically and horizontally. By duplication of monadic machinery, what I mean is that every monad transformer must implement bind and pure, plus define its own set of effects. With free, you focus only on the unique effects, because the monadic machinery comes along...well...for "free". :) You going to be at LambdaConf this year? Maybe we should have a panel, "MTL versus Free". :-)
Thanks
Which operating system are you using? If you are on Linux try setting your locale, errors like that sometimes happen when you try to read Unicode with e.g. the C locale which defaults to ASCII encoding (and not just in Haskell but in other languages as well).
&gt; Effects can compose in multiple ways, and you must specify the way in which they do. This is true, but it doesn't negate what I said, which is that free makes it possible to compose interpreters that have no knowledge of each other. Finally tagless is *not* equivalent to free. Finally tagless forces you to commit to a given interpretation upfront, using a single monolithic interpreter; while free lets you dynamically change interpretation, and reuse modular interpreters. I will back up the above claims with examples, but I'm not going to do so on Reddit. I'll save that for another blog post. :) 
That reminds me of the "restricted type synonyms" supported by Hugs, or "opaque ascription" in Standard ML, where you can have a type synonym in some module or scope, but the type is opaque as far as the rest of the program is concerned. I'm don't know how that sort of thing could be extended to work with newtypes, or to account for things trying to define different typeclass instances for the two types.
Since the title is quite vague, here is why you should watch this video: it presents a type system in which fusion can be enforced at the type level. The solution involves describing whether the code is sequential or parallelizable at the type level, which is also pretty cool!
newtype allows us to define a different instance for a type class. 
`stack clean` Then try?
You named the answer: you might want to branch on previous answers. This idea has nothing to do with "freedom". It's just fundamental to how applicative and monad differ.
I feel like you just ignored everything past the first sentence. (Don't get me wrong, you obviously don't owe me anything :D)
Unfortunately that's not how it works. Under modern law, every work is automatically protected by copyright and owned by its author. The author has the right to prevent others from reproducing it or creating derivative works. The author posting it on the Internet doesn't really change that: she's distributed it to you, but that doesn't mean you can distribute it to somebody else or use it in your own work. Contrast something like the BSD licenses popular with Haskell libraries, which explicitly permits redistribution and use, subject to certain conditions. Absent that permission, you basically can't do anything but read the code. Public domain is a different and complicated beast under current law. The rule of thumb is that you can't assume anything made after the Great War is in the public domain. You may, of course, use the *ideas* in the code yourself, just like you can read a book and then use the information in it to write your own book. There's another potential issue: code could conceivably be a stolen trade secret. (I have no reason to believe this is; I just bring it up because somebody else mentioned the possibility.) A bunch of Microsoft source code got leaked a few years back, for example. If you know or have reason to know that something is a stolen secret, you really shouldn't even look at it, even for ideas. Trade secret law, like patent law, does protect ideas.
I think we are talking past each other: what I mean by *interpreter* is strictly more powerful than what you mean; and when you say "composable modular interpretation", you are focusing on horizontal composability, whereas I am focusing on (ad hoc) vertical composability. I think an MTL versus Free post or talk will be a good excuse for me to explore these differences in more depth.
Since I can't figure out what you mean by interpreter that I don't ("way of assigning semantics to algebraic operations as quotiented by some collection of additional laws") and since I don't know what you mean by "vertical" composition (since not all effects translate into other effects, and even if they did that's not composing them) then a blog post does seem like a way to clarify what's being discussed here :-)
Haha, sorry, maybe I'm not sure how best to answer the question then. There's a fundamental tradeoff between the two APIs arising out of the "power in construction" versus "power in analysis" tradeoff. So, there's no "fix" here.
Well, not being able to optimize "copy a b; remove a" into a "rename a b" seems like a deal breaker. Not being able to depend on the result of a previous computation also seems like a deal breaker. To me, this doesn't really paint a pretty picture. What I guess really bothers me here is that it seems like it's such an arbitrary constraint. If I'm already constructing a syntax tree of sorts, why can't I inspect it? I can write an interpreter for it, but I can't optimize it? Very strange.
It also has Roles ( a bit like Smalltalk traits ), and you can do duck typing. Also you can create anonymous classes on the fly. There is sub-signatures so that you can do very fine-grained multiple dispatch. Really subroutine signatures are at least two to three times more powerful than is shown above. Other than not being able to use a string as an array, I think Perl 6 signatures are at least as powerful as they are in Haskell. (my main experience with Haskell is translating example code into Perl 6 so I may be wrong) Before anyone asks, Perl 6 strings are grapheme based, so accessing it as an array doesn't make as much sense as it does in other languages. In Perl 6 everything is a class, except for native types which you should use exceedingly rarely. ( In fact it has been occasionally slower to use native types, as they have to get boxed, and unboxed to their non-native counterparts ) I'm not certain what other data abstraction you need.
I would love to read that blog post :)
 bool equal(float a, float b) { return abs(a - b) &lt; 0.00001; }
In Perl 6 there is several ways to do just about everything, so that you can choose the one that makes your code clearer. For example if you need a code object that can take two numbers, and adds them, any one of the following works: * + * # Whatever code sub ($a,$b) { $a + $b } anon sub Add ($a,$b) { $a + $b } -&gt; $a, $b { $a + $b } -&gt; \a, \b { a + b } { $^a + $^b } &amp;infix:&lt;+&gt; # the actual numeric infix addition operator set of subroutines &amp;[+] # ditto { [+] @_ } # reduction on a list { [[&amp;[+]]] @_ } # ditto but with some obtuseness added # actually install it in the lexical scope sub Add ($a,$b) { $a + $b } &amp;Add In that specific case you wouldn't use most of them, but there are some circumstances where you are limited based on syntax. # pointy block if $a.some-expensive-method -&gt; $value { say $value; } # placeholder parameter if $a.some-expensive-method { say $^value; } You can also use them as infix operators ( you can create new operators too ) say 5 [&amp;( * + * )] 6; say 5 [&amp;( { $^a + $^b} )] 6; say 5 [&amp;Add] 6; say [[&amp;Add]] 5,6,7; # list reduce Which would make more sense if you were using a variable instead of a immediate Callable construct. I doubt there is any part of the language that experts won't use. There are some parts which are abstracted away that most programmers won't use, but are necessary for how the compiler constructs your program anyway. I'm thinking of the meta-object-protocol in particular. Actually thinking about it there is one thing that a particular person won't use ( these three are all the same ) for 0..9 { ... } for 0..^10 { ... } for ^10 { ... } Instead he writes it with a C style loop loop ( my $i =0; $i &lt;= 10; $i++ ) { ... } You will note that in the first three the value in the loop is immutable, and is provably correct. Also did you spot the error? `$i &lt;= 10` should be `$i &lt; 10` As far as "*look at this cool short code*"; I have been creating code for golfing competitions, and the code I write is very often the same, or very similar to code I would write anyway, except with fewer spaces and newlines. Also I think that there is one design decision that will help with scaling in particular is "*similar things should look similar, different things different*" While I can appreciate your watch from a distance view, you can't really appreciate it without actually coding in it. Even if you only used the novice level features, it is very nice language. class Point { has Numeric $.x is rw; has Numeric $.y is rw; } Which creates a `new` method a `BUILD` submethod, settable `x` and `y` methods, and checks that the values you give them fulfil the Numeric role. Altogether that is in the order of 10 lines of code that you don't have to write, but could if you wanted. If you want immutable attributes just remove the `is rw`. If you want private attributes, use `$!x` instead. ( not as useful for a immutable structure ) Actually the biggest hurdle I see common with newcomers is that they want to rewrite things the way they had to with other languages, but are built-in to Perl 6.
Well, you could be using a newtype as a safer alternative to semantic type aliases without providing any type class instances...
Actually it doesn't have that much syntax, It reuses the same syntax for similar things in various places. Unless you are talking about operators, subroutines and methods then it has enough so that it doesn't reuse them for different things. For example there are separate string concatenation and addition operators, because they really are different operations. As for multiple paradigm, would you rather try to shoehorn a financial based approach in a web language. If you were really keen you could write a Haskell slang, and program entirely in Haskell. ( except for a `use Slang::Haskell;` at the top. ) Basically you can mold Perl 6 to exactly the language that a given situation would be better written in. ( most language modifying will be limited to adding operators, which are far easier to add ) I like that there are quite a few idioms that just naturally fall out of being able to mix and match language features. I would rather be able to write non-idiomatic code than have to write around a missing feature because the designer of the language never tried to write what I'm writing. Perl 6 can be written in a rigid typing discipline style, If you really want it. ( Actually you can think of it as being very rigidly typed with the default being either the Any or Mu type depending on the construct ) You will find that it fights you a bit, as things array literals don't have a type restriction for the values by default; so you can't just pass them to subroutines that are restricted to accepting a typed array without some form of coercion `Array[Int]( [1,2,3,4] )`, or just writing them the Java way `Array[Int].new(1,2,3,4)` instead of `[1,2,3,4]` which is really short for `Array[Mu].new(1,2,3,4)`
I'm still digesting this, thank you! Now, considering a function like map map :: (a -&gt; b) -&gt; f a -&gt; f b Where are the forall s here for a and b "ideally?" / "mathematically?" They must be there since we are parametrising over all a and b, but now after learning how to construct existential types, the construction of universal types confuses me.
&gt; In Perl 6 there is several ways to do just about everything, so that you can choose the one that makes your code clearer. Think about this for a minute.
Surely you would would expect to use a "type level DSL" to make this sort of representation composible 
When you are using newtype to encapsulate the representation, this might be reasonable. Maybe. When you are using newtype just to ensure that two types are distinct, this seems to defeat the purpose. When you are using newtype to have multiple instances of the same type class for the same underlying type, you need the constructor in order to disambiguate which instance to use, or else you need explicit type annotations, defeating the purpose of the implicit newtype constructor.
I share the same concerns, but I think an extension like this is constrained well enough (it only participates under certain well-defined conditions) and has precedence (with `OverloadedStrings` and `OverloadedLists`).
As a personal preference, I try to avoid get/put methods, because they insert a specific version of the state as a local name in your current scope (your "m" variable). Then it become easy to do a mistake as using "m" without getting after a put. Something like: stateFunction2 = do modify (M.insert "A" 1) modify (M.insert "B" 2) v &lt;- gets (M.findWithDefault 0 "B") modify (M.insert "C" (v + 20)) 
Yes, I dismissed that presentation on the title alone when I saw it posted elsewhere. When I saw it here I thought I'd give it a shot and was pleasantly suprised when I saw who the presenter was.
&gt; I'm sure the multiple paradigm thing is utterly deliberate. Yes me too :) But I prefer languages to be more paradigm-pure. Just my preference.
I think he meant `stack -v install transformers-base` :)
[removed]
I think it does have a lot of syntax. But maybe that's an opinion. I tried to find some definitions, but I'm not quite sure how comparable they are: https://github.com/rakudo/rakudo/blob/nom/src/Perl6/Grammar.nqp https://github.com/ruby/ruby/blob/trunk/parse.y https://github.com/ghc/ghc/blob/master/compiler/parser/Parser.y
some derivative mentions of this in [Folding Domain-Specific Languages: Deep and Shallow Embeddings - Jeremy Gibbons](https://www.youtube.com/watch?v=xS7TJrrhYe8) , quite interesting
Well, the newer generation needs to find the modern analogue of "line noise", for one thing. 
So what is the difference, platform ?
When to not use State monad? If you don't have anything that is nested but mutates state. For example you make a board game and you only have one function that changes the board, like a move function. It happens only once per game loop. Would that warrant a State monad or should I just return a new board?
I wonder what's the bottleneck, maybe being on hackage would make it smoother.. distribution is definitely an issue
&gt; More straw man examples of "idiomatic code" (according to whom? I think it's bad too) and etc. What do you think would be a better way to write this code? saveFile :: Path -&gt; Bytes -&gt; IO Unit saveFile p f = do log ("Saving file" ++ show (name p) ++ " to " ++ show (parentDir p)) r &lt;- httpPost ("cloudfiles.fooservice.com/" ++ (show p)) f if (httpOK r) then log ("Successfully saved file " ++ show p) else let msg = "Failed to save file " ++ show p in log msg *&gt; throwException (error msg) I'm not being confrontational, just trying to learn.
Maybe post that log it mentions too?
Thank you for the clarifications; now let me digress a bit: what if one renames all variables? or simplifies code by equational reasoning (Haskell being perfect for this)? or ..
Was useful, thanks
Here is the content of /home/notooth/.stack/global-project/.stack-work/logs/transformers-base-0.4.4.log Configuring transformers-base-0.4.4... Building transformers-base-0.4.4... Preprocessing library transformers-base-0.4.4... [1 of 1] Compiling Control.Monad.Base ( src/Control/Monad/Base.hs, .stack-work/dist/x86_64-openbsd/Cabal-1.18.1.5/build/Control/Monad/Base.o ) .stack-work/dist/x86_64-openbsd/Cabal-1.18.1.5/build/src/Control/Monad/Base.dump-hi: commitBuffer: invalid argument (invalid character) 
Sure, you could. My intuition is that if you have multiple things that change the same state, then it's useful to put them all in the state monad and sequence them. You may find yourself wanting to split up your one function that changes the board, into multiple functions at some point, that tackle smaller parts of the logic and change the board in different ways. Then it becomes useful to share the same state among them.
that lts-3.6 is the version the ghcjs compiler is built with. I am not sure what is going on. trying to reproduce.
Setting locale variables won't help if your system doesn't have locale files installed. Seems somewhat possible from what you show. I've seen cabal fall over on eg, author names containing characters not in the current locale. Looks like this one is probably stack (or ghc?) having the bug. This is absolutely worth root causing and reporting the bug. Here's a simple example of buggy code: import System.IO main = putStrLn =&lt;&lt; getLine echo ¡ | LANG=C runghc foo.hs foo: &lt;stdin&gt;: hGetLine: invalid argument (invalid byte sequence) It's rather unfortunate that such simple programs fall over so easily in such a common edge case. Of course you can use ByteString or even force ghc to use an aternate, robust encoding for a Handle's IO (hSetEncoding stdin =&lt;&lt; GHC.IO.Encoding.getFileSystemEncoding), but the common case is just wrong. :-/
I tried to use cabal to install the package, and it worked fine: $ cabal install transformers-base Resolving dependencies... All the requested packages are already installed: transformers-base-0.4.4 Use --reinstall if you want to reinstall anyway. Is that possible stack having the bug? How can I report it?
True. Yeah, I meant state monad since I had just finished reading the article. 
there is no ambiguity, it's just deferred and solved somewhere else, your type system is the intercessor 
the sanity wold not come from tabs, but from having a standard
With all due respect, that class declaration is only interesting in the context of the weakness of Perl 5 and the subsequent dogpile of OO systems that were bodged on to the side. There's many other languages that can do that. Many of them have acquired the ability to do that in the 15 years that Perl 6 has been developing. Generally speaking, it has lately been getting more acknowledged that having a billion ways of doing the same thing is generally a weakness in a language rather than a strength. Perl 6 is built with the sensibilities of the ~2000 time period it was born in, and is in some ways the most ultimatest biggest bestest 2000-era language that can ever be, but the general programming community consensus has been moving away from that lately, a consensus I generally agree with. Haskell's blazing the trail and things like Rust are building the roads. Neither profressionally nor for personal hobby programming do I have a desire to move from languages with 5 ways of doing things to 50 ways of doing things. I've already seen how nasty the interactions can get with the 5. I'm perfectly happy to let the community prove that it has some way of handling the 50 ways of interacting before I stick my toe in. In the meantime, my next language is already Rust anyhow.
I'm not seeing an opportunity for wrap-around. `lo` and `hi` are bounded between 0 and the length of the vector. 
It depends on the Haskell implementation, but I don't see anything that precludes an implementation from allocating a vector of length `maxBound::Int` (and problems start well before that).
That's still not good enough because the sum of `hi` and `lo` can still be larger than `maxBound` (over the integers).
Yeah but those programs don't provide a change log showing how what errors have been fixed. Also, it's a funny noteworthy coincidence that one of the authors of Helium will be one of the judges of my thesis results :P And about OCaml, I'm sure that the technique should be appliable to other functional languages, but I'm afraid that the &lt;time&gt;/&lt;improvement of quality of result&gt; ratio wouldn't be high enough to account for the required extra effort.
I thought they had a trace of successive compilations, from which you could extract the fix (i.e. if module `A` doesn't compile at time `t1`, but does at time `t2`, then the "fix" must be in `diff A(t1) A(t2)`).
Perfect! Thanks
Thank you. I'm not entirely sure yet if this is about Haskell code, but I'll check it out. (I edited the main post to mention that I'm only looking for erroneous Haskell code after you've replied) 
Can't check it at the moment, but here's a simple, practical one. Let's say we want to work with a database connection class Monad m =&gt; MonadDb m where withConnection :: (Connection -&gt; m a) -&gt; m a This obviously works via a Reader newtype DbM m a = DbM { _unDbM :: ReaderT Connection m a } deriving ( F, A, M, MonadTrans, MonadIO ) Note that we don't want DbM to "catch" `MonadReader`—it'll be a passthrough—but for all others we can use GND. instance MonadReader r m =&gt; MonadReader r (DbM m) where ... instance Monad m =&gt; MonadDb (DbM m) where withConnection f = DbM (ask &gt;&gt;= _unDbM . f) Then, runMonadDb :: Connection -&gt; DbM m a -&gt; m a runMonadDb cnx m = runReaderT (_unDbM m) cnx
Despite the hyperbole from L. Wall, it seems like a kitchen-sink language where more and more is conflated for quality.
You can use a concrete free monad or a one-off "mtl class remover" type picking just one concrete layer (see another response of mine on this thread).
if you do :l functions1.hs and then :browse does it show that anything is in scope?
Thanks a lot. I forgot you don't specify the arguments the same way as in C++.
Just asked the Author :) Dear Doug, Recently I found your collection of Haskell libraries: http://logicaltypes.blogspot.co.uk/p/haskell-libraries.html (And possibly http://logicalgraphs.blogspot.co.uk/p/trading-analytics.html) And Got this Awesome response: The libraries, both sets, fall under the 'please do' license. Please do use them if you like, and if you find them useful, please do let me know how you used them and please do spread the word. Share and enjoy. 
`HttpT` would best be defined by a free monad, no?
No. It is a nice and insightful book, i would recommend it anytime. But it wont give you a headstart into haskell
I always enjoy watching the videos: http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video-lectures/ Otherwise there is no prerequisite in doing anything before learning Haskell; just find a good tutor and have some time. PS. Having good understanding of Computer Science and Mathematics usually helps a lot. However again you do not have to know everything as long as you accept it is OK not to know everything beforehand.
I'd recommend [Haskell Programming from First Principles](http://www.haskellbook.com).
Not directly; the most immediate connections are with the broad purview of abstract algebra, namely "category theory", and formal logic. However these are the theoretical underpinnings; you won't need them for a day-to-day use of the language. What really matters is patience and practice. Understanding of the theory comes gradually and in phases, but it's not a precondition for becoming a proficient programmer.
Thanks.
Thanks.
I think this is the same approach translated into `mtl` terms: https://gist.github.com/ocharles/6b1b9440b3513a5e225e I'd be curious to see if people agree I got it right. CC /u/tel, /u/sclv, /u/buffyoda, /u/Peaker 
? the first video is Prof Weirich and she's awesome
&gt; stack exec liquid binarysearch.hs IMO It's preferable to just use the executable directly. Use `stack build --copy-bins liquidhaskell`. (Or its alias, `stack install liquidhaskell`, although we tend to discourage "stack install" because it is a misleading name that can lead to confusion such as "how do I uninstall?"). This will copy the binary into the folder where stack thinks you put your local binaries. By default, on linux, this is `~/.local/bin`, configurable via `--local-bin-path` or a similar config file option. If you use stack to upgrade itself (`stack upgrade`, or `stack upgrade --git` if you want the latest unreleased goodies), it installs itself in this location.
I'm also seeing a few issues trying to use it globally installed. It seems that liquid-fixpoint creates a binary called `fixpoint` which `liquid` relies on.
That's correct, it would be nice if we could express that dependency in our .cabal file so stack (and cabal) would know that we depend on the binary, not just the library. I don't think that's currently possible though. 
I think `MonadTwitter` and `MonadCloud` should live in the package where `app` is defined. Many client-specific interfaces are better than one general-purpose interface. In OOP world they call it "interface segregation principle", though it is not specific to OOP. If `MonadTwitter` is defined in `twitter` package togather with its implementation, then it is probably pretty big. It supports sending tweets, querying friends, likes, retweets, etc. Most likely you are interested only in a limited number of features. So it is better to introduce more specific interface that matches your specific needs. I'm not using `mtl` any more, so unfortunately I can't provide you a specific example. Interface segregation is tricky with `mtl`, I agree that it is a significant drawback. 
So are you talking about selectively importing `tweet` from the `twitter` package, creating an interface that just exposes that and using that in your own project vs using `twitter` directly, or are you talking about reimplementing enough of the twitter client in your own app to support `tweet`?
very cool: https://github.com/ucsd-progsys/liquidhaskell/blob/master/src/Language/Haskell/Liquid/GHC/Interface.hs
I am going to add this to my chess engine I am writing in Haskell, my goal is to make a competitive chess engine. So I can't exactly use things like the maybe monad or other type level hackery because that could possibly incur a run time slowdown, but because these checks only exist at compile time it sounds perfect for a chess engine. Is there an API documentation? 
I like State/Reader monads less for their "I don't have to pass parameters" property and more for the guarantees around their operations. They codify *intent* around what you want those parameters to do, even if there's only one parameter or it is a tiny set of functions.
Actually, that binary is a historical artifact, that we can eliminate. The real reason is the one you outline above, namely, access to modules your source will be built against. At any rate though, this is one of the rough edges of LH that needs a lot of work! 
Ok, assuming we really are just in the IO monad and with the tools at hand, we can still factor away all the string slinging from the main logic of the function. Let's assume we have a proper SaveFailure exception type at least, and other than that here's a sketch... saveFile :: Path -&gt; Bytes -&gt; IO () saveFile p f = logSaveStart &gt;&gt; doFileSave p f &gt;&gt;= \result -&gt; if httpOk result then logSaveSuccess else logSaveFailure &gt;&gt; throwException (SaveFailureException p) where doFileSave = httpPost (serviceUrl &lt;/&gt; p) f logSaveStart = log $ "saving file" ++ ... logSaveSuccess = log $ ... logSaveFailure = ... serviceUrl = ... Heck, even just properly indenting the example code so you don't have the weird scala-ism of the runin "if then" with an "else" on the next line with no indentation would make a huge difference. (and of course fixing the misaligned "let/in" block) i.e. if (httpOK r) then log ("Successfully saved file " ++ show p) else let msg = "Failed to save file " ++ show p in log msg *&gt; throwException (error msg) becomes if httpOK r then log ("Successfully saved file " ++ show p) else let msg = "Failed to save file " ++ show p in log msg *&gt; throwException (error msg) 
One of the annoyances I keep running into in my application code is when I write functions that operate on individual elements of a collection -- I want some nice way to have a function `f :: [Foo] -&gt; Foo -&gt; ...` that can be sure that the given element already exists within the collection, without resorting to runtime checks and returning a `Maybe`. I theoretically should be able to ensure that the `Foo` was retrieved from the collection in the first place in a statically analyzable way. It seems like LH should be able to help with this -- are there examples of similar things? 
Thank you very much! This looks really cool :) 
Call lift indirectly through a well named function.
If there is one take away from SICP, that is (IMHO) the idea of abstraction. I think it is very beneficial to read and work through SICP and it can certainly help you in your Haskell journey. But SICP is not a prerequisite for learning Haskell. SICP is not about Scheme either. It is about computation and abstraction and many other fundamentals. If you are working through SICP, I suggest sticking with Scheme, not Haskell or any other language, for working through the exercise problems. Not sure, if I answered your question. :)
There are two important parts in the `mtl` approach: the transformers and the classes. If you have `N` transformers and `M` classes, you'll need to provide `N*M - K` instances, where `K` is the amount of impossible instances (we can't lift `MonadWriter` over `ContT` and preserve the laws, for example). It is the `K` here that makes "extensible" approaches fundamentally invalid — you cannot lift everything over anything, you really need to know which specific combinations of transformers and classes can lift and in which way. What we really want here is to minimize the amount of transformers and classes to a minimal set of fundamental effects, define `N*M - K` instances for them, and express everything else in terms of those basic definitions. This is the approach I took when working on [Ether](https://int-index.github.io/ether/). You need to ask yourself a question: what *really* is `MonadTwitter`? Perhaps you need some configuration (MonadReader), state for session handling (MonadState), and IO for sending network queries. With Ether you can define a tag (`data Twitter` without constructors) and then `type MonadTwitter m = (Ether.MonadReader Twitter TwitterConfig m, Ether.MonadState Twitter TwitterState m, MonadIO m)`. Note that you haven't actually defined any new classes or transformers, and no instances either! I personally no longer crave extensible effects after finding this approach.
The former. The idea is to put interface closer to its client. That way you may even completely decouple the package where `app` is defined and `twitter` package.
Nicolas Wu had a very good point at this (2015) years Haskell eXchange. Taken some effect, there are `MonadEffect` type class, and you can build an interpreter in various ways. Transformers is one, comonad interpreters another, and there are more. Taken `MonadCloud` and `MonadTwitter` example, I'd like package maintainers to provide functions in say `MonadIO`, i.e. saveFileIO :: MonadIO m =&gt; Path -&gt; Bytes -&gt; m () So I can use those to build my own newtype `Monad` (yes there is some boilerplate), or functor for `Free` + interpreter (even more boilerplate?), or some other trick. Of course they could provide transformers as well, e.g. `CloudT`, `TwitterT`, and have `default` implementation for `MonadCloud` and `MonadTwitter` using `MonadTrans` instance; as this effects are very simple (i.e. they commute), so one could define `newtype` with very little boilerplate. class Monad m =&gt; MonadTwitter m where tweet :: String -&gt; m () default tweet :: (MonadTrans t, MonadTwitter m) =&gt; String -&gt; t m () tweet = lift . tweet newtype TwitterT m a = TwitterT { runTwitterT :: m a } instance Functor m =&gt; Functor (TwitterT m) instance Applicative m =&gt; Applicative (TwitterT m) instance Monad m =&gt; Monad (TwitterT m) instance MonadIO m =&gt; MonadTwitter (TwitterT m) where tweet = undefined --- newtype App a = App { runApp :: TwitterT IO a } deriving (Functor, Applicative, Monad, MonadTwitter)
So, you found a book overrated without reading it?
I've only seen goldfirere's presentation on GADTs so far .. great stuff
For an overflow to happen the vector needs to have at least 2^62 elements, right? I don't see that happening any time soon.
What do you get from comments like this, personally? Is someone forcing you to write Haskell the whole day so you get frustrated and this is you venting? Do you get a feeling of superiority because you can see the bigger, pointless picture that other people here are apparently totally blind to? Or maybe you have some alternative that you'd like to scare people into by making broad, sweeping and mostly untrue statements about Haskell and it's community?
[removed]
So you suggest building your non-atomic work in IO and just injecting the translation from your GADTs to IO? I think I understand the approach, but it doesn't seem to be solving the same problem.
Not really. I'd recommend Prof. Giesl's and Prof. Wadler's Haskell videos instead, they both are very nice intros to Haskell.
&gt; like incompatibility with side effects That's my favorite disadvantage! Or, as I prefer to call it, "advantage".
These ideas have also been developed in this paper: https://lopezjuan.com/limestone/vectorcomp.pdf
Cool! I'll give that a more detailed read. Hope the author continues it someday!
Thanks for the suggestion!
Not exactly an answer to your question, but assuming you have a value AST, then Expr isn't necessary. The way I've been handling primitive functions is like so: data Value = VInt Int | VClosure Var Type TExpr Environment | VPrim (Value -&gt; Value) then we can define: binIntOp op = VPrim (\ (VInt x) -&gt; VPrim (\ (VInt y) -&gt; VInt (op x y))) add = binIntOp (+) multiply = binIntOp (*) subtract = binIntOp (-) and so on without having to build these functions into the interpreter (and we can define more without having to touch the interpreter.) Unfortunately, I don't think this will work for a compiler... 
For a simple case like this, how about an overlapping instance using MonadTrans? The constraint solver can't deduce `Monad (t m)` from `Monad m` and `MonadTrans t` so you have to either drop the superclass on your new classes or use UndecidableInstances, but this all seems to be allowed: class MonadCloud m where ... data CloudT m a = ... deriving (Functor,Applicative,Monad) instance MonadTrans CloudT where ... instance (Monad m) =&gt; MonadCloud (CloudT m) where .... instance {-# OVERLAPPABLE #-} (Monad m, MonadCloud m, MonadTrans t) =&gt; MonadCloud (t m) where ...
That's a pretty imposing name for passing in a record of function implementations. It's interesting that it seems pretty similar to the "dependency injection" approach this blog post decries, while at the same time being equivalent to a free monad.
Sure. It goes both ways. You can integrate a zipper type to get a set of possible data structures.
Can you give sample inputs and expected answers ?
Out of curiosity, is your thesis related to CodePhage?
Ah, forgot to post the link to the problem. Already edited, and for your convienence http://www.spoj.com/problems/MKTHNUM/en/
Have you tried using strict fields? if you never intend on using an infinite tree or storing unevaluated thunks of integers 
The GADTs talk has been really interesting so far (I'm not far in yet). The slide at 16:00 and 16:30 says that data STy ty where SInt :: STy Int SBool :: STy Bool SMaybe :: STy ty' -&gt; STy (Maybe ty') is equal to data STy ty = (ty ~ Int) =&gt; SInt | (ty ~ Bool) =&gt; SBool | forall ty'. (ty ~ Maybe ty') =&gt; SMaybe but I believe that the second version should look more like: data STy ty = (ty ~ Int) =&gt; SInt | (ty ~ Bool) =&gt; SBool | forall ty'. (ty ~ Maybe ty') =&gt; SMaybe (STy ty') The SMaybe was missing an argument :)
&gt; I don't think that I'm trolling. I'm just being descriptive IMO. The original post you made in this thread is basically just complete and total shitposting, full stop. The remainder of your posts actually have some substance (namely, that you are disappointed in the way the Haskell community is moving). That, at least, can be possibly discussed. If you can try sticking to the second bit, people will probably let your comments float above negative karma (and not get skipped over and ignored, since you so desperately seem to want people to listen to you). The thing is, you may not think you're a troll, but when you post like one, the distinction is hardly meaningful. &gt; And being sincere and exposing the opinions bluntly over the table is an unavoidable prerequisite for fixing things. No it isn't, it's just your excuse for shitposting, one that you made up. &gt; is not an issue because the haskellers who do not match my descriptions will not be concerned. More lame excuses. You're sure you're not a troll? ("Don't worry everyone, I can say what I want because the people who I'm shit talking about are the only ones who will be insulted!!!!")
If I remember correctly an author spoke about the difficulties integrating twice in other words integrating data structures. You can do the math but the application of the resulting data structure isn't clear. 
I would totally recommend SICP before Haskell, the same way I'd recommend *Algorithm Design* before a class on distributed computing. SICP is only tangentially relevant to Haskell. 
Impressive, looking forward to getting a hold of a copy!
Try Church (`newtype Node = Branch {unbranch :: forall r. (Int -&gt; r -&gt; r -&gt; r) -&gt; r}`)
[removed]
Unfortunately I am storing unevaluated thunks. See sgtEmpty and sgtDiff. However I might try to "fix" them. Thanks for the advice anyway.
Are you sure? My structure relies on sharing to work, and your type might not be able to do so.
Sadly I'm "modifying" the structure many times to get multiple versions, so it's likely that we are not talking about the same thing
[removed]
I'd just like to note that while this approach does avoid duplication in the type definitions and is quite flexible, it does lead to some extra syntactic noise in functions using these types due to the additional constructors that now need to be inserted/removed. In my personal experience I've found it not to be worth the tradeoff, duplicating the type definitions ended up being more maintainable. [You're experience may differ.] Pattern synonyms could probably be used to avoid the syntactic noise but the definition of the pattern synonyms would contain all the same redundency we were trying to avoid in the type definitions. Still, it might be worth it if you need functions generic over multiple types of expression and this lets you write them. [I wasn't familair with pattern synonyms when I did this.] If you do decide to got his route you may also want to check out [Cofree](https://hackage.haskell.org/package/free-4.12.1/docs/Control-Comonad-Cofree.html). Without the `Prim` case `TExpr` would be equivalent to `Cofree Exp Type`.
Heavily depends on the modifications. For example, if you want to map over the entire structure, that's O(1), if you want to map over the entire structure except the top things, that's O(n). Concatenation of structures is O(1), gettings a top level piece is O(n) (its very much backwards of normal data structures.) It probably won't be good for what you are using it for though.
Unfortunately, a book with most of the examples not working without changes is still one of the 5 best books to learn Haskell in 2015. Why is there no updated version of RWH based on the crowd-souced review process of the original book? Or does just nobody know about it (me)?
It is definitely full of very useful content throughout. 
If you have `Value -&gt; Value` in your AST then you don't need `VClosure` 
Thanks, that improved the speed by about 30% EDIT: Really thanks a lot :)
no problem! :D In general, "strict data, lazy splines" is a nice "default" rule of thumb for all data you'll make in general. There are definitely exceptions, but it's just the accepted "default". If you compiled with -O2, then I think the strict Ints are actually unboxed...but you can also manually explicitly unbox them. Unboxed would be faster than boxed...so explicitly unboxing them would make you less reliant on magic compiler optimizations.
I'm *not* generating subtrees, I'm generating *versions* of Segment Trees. Each tree has O(log N) nodes different from the previous one, so it can't be O(N), and has to be O(N log N)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/french_commander] [JPMorgan Haskell Team is hiring : haskell](https://np.reddit.com/r/french_commander/comments/3z0f5o/jpmorgan_haskell_team_is_hiring_haskell/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
`diffTime = (realToFrac .) . diffUTCTime` This is unneeded point free notation that makes the code less clear. Make it into `diffTime a b = ...`. In this case, I might even inline it because it's only used once and doesn't do overly complex stuff. --- Just to be clear, reading of readTVarIO and the `iterateWorld` are *not* in a transaction together. So it is possible for `worldState` to change between the `when` and `iterateWorld`. It might be OK, but it definitely needs to be clearer. world0 &lt;- readTVarIO (server^.serverWorld) when (world0^.worldState == GamePhase) $ do sim0 &lt;- atomically $ iterateWorld server time dt mapM_ (broadcastSimulationMessages server) (sim0^.simulationMessages) --- In the call `threadDelay loopWait`, you assume a constant delay, but what you likely want is a constant frame rate. Fetch time after the iteration and only sleep the difference between that time and the length you want the loop to be. --- Why is the state monad required in the `stepWorld` function? If you just `get` it in the start and `put` in the end, everything in between is already pure. The `runState` just undoes that `get` and `put`. It only makes sense if you want to call a lot of `get` and `put` in the middle processing steps. --- The Actor class. Here we enter a world of pain in terms of game design. Game engines *used* to have game objects having several base classes similar to what you do, with `Movable` and `Rotatable`, but it gets very unwieldy very quickly, with objects rendering themselves and not interacting well with others. The more modern approach is using "component based objects", i.e instead of a Car being "is-a" "movable", it "has-a" movable geometry component. It "has-a" collider. It "has-a" graphic representation. Then you have a renderer that only needs "graphics" and "geometry" from the object. A sound engine make that only needs the sound part and the location. And so on. Objects do not render themselves. In general, not having heterogeneous lists makes it so you'd hold different objects in different structures. Having components means you can pull the common needed components from different kinds of objects and push them together to the lower level renderer etc, the one that only cares of model and position and doesn't care for the game logic parts. This works well in imperative, mutable languages, but might prove interesting to do in a pure functional language. Possible through the use of lenses.
Oh, lots of them.
Then Just do it: http://book.realworldhaskell.org/read/ ( Copyright 2007, 2008 Bryan O'Sullivan, Don Stewart, and John Goerzen. This work is licensed under a Creative Commons Attribution-Noncommercial 3.0 License. https://creativecommons.org/licenses/by-nc/3.0/ ) ~~Ideally set up the content on the github~~ https://github.com/bos/rwh ;)
I absolutely do not recommend it. My partner had me attempt SICP before Haskell and I don't think it was helpful at all. 
I've seen a couple of these "Best * Books for 2015" posts. It looks like the same person created all of the sites, has no idea of the merits of the books (many of them weren't even published recently), and is just trying to cash in on referral links. See http://scalacommunity.com/2015-12-best-scala-books-2015 and http://rustyprogrammers.com/2015-12-best-rust-books for example.
Chris's book is shaping up to be _the_ book even though it's still a bit rough around the edges right now. This list looks more like "these are the first 5 books i happened to stumble across" and clearly have nothing to do with 2015. How many years old is Hutton's book and RWH now? LYAH at the top really? That book might have done more harm than good on net by fooling readers into thinking they learned something. We need an alternative cartoony, concicse, first-encounter-friendly tutorial to supplant LYAH. 
If I understand correctly you are passing the result of get prices to total and are wondering if instead of traversing the list twice you can do so in one go? The easiest thing to do would be to call and on p in your go function before adding it to the accumulator. alternatively, you can pattern match the tuple inside the list and act on it directly since patterns can be nested. The first way: let newAcc = acc + snd p in go newAcc PS Second way: go acc ((_, p):ps) = let newAcc = acc + p in go newAcc ps
oke, and I can do go acc l where l is the complete list ? 
For 2016 I wish [the Haskell IDE engine](https://github.com/haskell/haskell-ide-engine) to reach a level that I can hook it into my Emacs have an IDE-like experience that "just works". 
I'm looking forward to using https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields
What is modus operandi when you work for longer than your tick?
The two things I'm looking forward to are beginner-friendly IDEs that have no setup issues and the completion of Haskell Programming from First Principles book.
solution: [`PatternSynonyms`](https://ghc.haskell.org/trac/ghc/wiki/PatternSynonyms): lets you do pattern EInt n = Expr (Int n) eval (EInt n) = ... alternately, some record syntax and `LambdaCase` can help: newtype PExpr = PExpr { runPExpr :: Exp PExpr } peval = runPExpr &gt;&gt;&gt; \case Int i -&gt; ... 
You are welcome! Now please report your favorite bug `andThen` contribute a failing tests case `andThen` contribute a fix `andThen` contribute documentation. 
Does this describe the same solution: http://blog.anudeep2011.com/persistent-segment-trees-explained-with-spoj-problems/
YES! But I just thought you guys won't understand it, because it does not describe segment trees, and most articles on the web on segment trees are actually talking about something else. So, I wrote my own (hopefully) self-contained article. BTW thanks for your attention
You can make them work in a single transaction if you want. Just change the code to: atomically (do world0 &lt;- readTVar (server^.serverWorld) when (world0^.worldState == GamePhase) (do sim0 &lt;- iterateWorld server time dt mapM_ (addSimulationMessageToQueue server) (sim0^.simulationMessages) )) ... where `addSimulationMessagesToQueue` is an `STM` transaction that adds the simulation messages to some `TQueue`. Then send the messages afterwards separately: loop = do x &lt;- atomically (tryPeekTQueue queue) case x of Nothing -&gt; return () Just msg -&gt; do broadcastSimulationMessage msg loop
I wish that those in charge of Haskell see the truth that Haskell == GHC and make so that Haskell is more like Python and Apple Swift - usable for writing non-completely-toy code by default (especially non-linked-list text and overloaded record fields, when it's done, by default), removes beginner traps like lazy I/O, etc. IMO right now Haskell defaults as of right now are an embarrassment, and will probably become even more so as other languages keep taking ideas from Haskell. It seems like Haskell has all tools to make this happen, they could simply make an alternative `Prelude` that isn't even super-radical like ClassyPrelude, just one that removes beginner traps like lazy I/O, imports a few basic utilities like `Map`, `Set`, `Text`, `ByteString` qualified etc. and an extension that uses that prelude by default and enables some basic extensions. --- Improved literal handling so that literals are checked at compile-time without having to use something like `$$(literalTH "some text")`, etc. --- Some form of qualified exports, so that it's possible to eg. write a custom Prelude that exports useful libraries that require qualified imports by default. --- Easy definition of newtypes with restrictions on content, where they would automatically inherit type classes like `FromJSON`, `Num` and eg. `Functor` for newtypes with type parameters. newtype Name = Text : { 1 &lt; length &lt; 12 } -- has FromJSON, Semigroup (not Monoid, length at least 1) etc. newtype UpToHundred a = [a] : { length &lt; 100 } -- has Functor instance I tried writing something like this myself based of Nikita Volkov's `refined` package, but it seems messy and it seems I would have to define `Refined p a = Refined a`, `RefinedP p a b = RefinedP (a b)` (eg. `RefinedP (MinLength 1) []`), `RefinedPP p a b c = RefinedPP (a b c)` (eg. `RefinedPP (MinLength 1) Map k v`), etc. and forward type classes for all of those independently.
You are looking at a university resource, which means that only enrolled students have access to submission systems. Also keep in mind that teaching materials are hardly ever comprehensive. If you tried mastering a subject based on materials posted online, without having access to the instructor or teaching assistant, you may only get frustrated.
You can simplify a bit by removing the first equation for `total` - `go 0 []` already evaluates to `0` so it's not required to separately define `total []`. This might be getting a bit ahead of what you have studied, but one way of looking at this problem is pretty close to how GHC actually optimizes a definition `sum . map snd` into a single loop. First, notice that your `go` function is `foldl (+)`, and because `go 0 [] = 0 = total []`, we also have `total = go 0 = foldl (+) 0`. Now look at the composition: total . getPrices == go 0 . getSnd == foldl (+) 0 . map snd Now we use the rule that `foldl f x . map g == foldl (\acc v -&gt; f acc (g v)) x` to change that into foldl (\acc p -&gt; acc + snd p) 0 Inlining the recursive definition of foldl gives a single recursive function like what other people have suggested: billPrice :: BillType -&gt; Price billPrice bills = go 0 bills where go acc [] = acc go acc (p:ps) = go ((\acc v -&gt; acc + snd v) acc p) ps clean it up a little more by reducing the application of the lambda function, and you get billPrice :: BillType -&gt; Price billPrice bills = go 0 bills where go acc [] = acc go acc (p:ps) = go (acc + snd p) ps That's more or less how GHC optimizes `sum . map snd`. It can't actually recognize folds or invent rules on its own, so `sum` is defined in terms of fold, and a `RULES` pragma tells it about the fold/map rule. (If you look into the source you'll see it actually only has a rule for foldr, and defines foldl in terms of foldr).
Could you expand a little bit, what would be an example of corecursion on an infinite list?
The obstacle to analyzing monads is that the second argument of bind is a function, some `a -&gt; m b`. That gives clients the power to depend on results of previous operations in arbitrary ways, but means the library can only inspect the rest of the computation by supplying some arguments. That's probably enough to handle your `copy a b; remove b` example, by passing `()` or some `Success` value to the rest of the code, and seeing if the next operation would be remove (if the copy succeeds). I disagree that not being able to do that sort of optimization is usually a deal breaker - I wouldn't it to happen in any of Java, Python, C++, etc. There certainly are some applications where analysis is needed (and where people generally use Applicatives in Haskell), especially parsing. On the other hand, Applicatives only allow dependencies if you provide an explicit operation to handle it, which generally means a clunky interface. If you are defining your own applicative `A`, you can include something like `aif :: A Bool -&gt; A a -&gt; A a -&gt; A a` if you like, and know that the library will be able to inspect both branches and everything that goes into the condition, except for plain functions used with `fmap`.
It's a nuisance (1), and it makes it hard to teach (2). In regards to (1), the *only* reason I prefer Haskell over other languages is that it doesn't get in my way. With all other languages I've used, there's constantly the "I would like to do this now but the language is not powerful enough to let me do that so I guess I'll do this instead." That's generally not the case with Haskell. Haskell is a fairly frustration-free experience... except for the silly defaults. Having to toggle various compiler flags and make a bunch of standard imports just to get the tools that are already there in other languages gets old. You can argue (2) and I'm not entirely convinced of whether or not it's true. In my experience, telling students "so you may think you want X, but actually you'll want Y in a real program" is distracting, whatever the reason and however good it is. AMP solved a tiny part of this problem (most of which may never be solved due to other more important priorities) and saner, more natural defaults with virtually no downside (Text for text handling) is another part solution.
I don't care how it's implemented: just that it's there and can address common needs like limited-length lists and text, all-uppercase strings, etc, etc. Also, about TH, it's impossible to document things generated by it properly: https://www.reddit.com/r/haskell/comments/3h34nl/documenting_generated_declarations/ - one more Haskell wish for 2016...
i'd find it very demotivating if i knew when i started that nobody uses the things i'm learning luckily i just got by on ignorance 
was it this guy? wadler: http://homepages.inf.ed.ac.uk/wadler/Pics/philtiebig.jpg i don't know the video, but it just feels like it's gotta be him
I really love stack. It is made to fix the current mess, and it does very well. But, for being backwards compatible, obviously it isn't as good as it would be if we were building haskell and hackage from scratch. I would really like to see something like that being done...
I'm watching it now. I just learned about the "untouchable" error you get when using GADTs and not providing a type signature. Would it be possible for GHC to just say "GADTs require a type signature" if we can exactly know when lack of a type signature is the cause of the error. Even if we can't exactly know (I recall it being said the untouchable error is *almost always* because you don't have a type signature), we could probably add "try adding a type signature".
Yes. Not solving the same problem? I may have misunderstood the problem then. Can you elaborate?
Same but for Sublime.
Edit: I read my reply again after I wrote it and it sounds pretty whiny. Please don't take it that way. I'm just trying to elaborate on why I think this is a harmful stance to take. ---- &gt;And if you're writing a toy program, then String and lazy IO aren't exactly a death sentence. It's not a death sentence, but it's really annoying. Haskell is supposedly great for quick prototypes, but then when it's time to harden it up you have to rewrite everything using different semantics because the "tutorial" way is O(n!) or something. It's almost like two entire languages. "Baby Haskell" where everything is cute and easy and you can give your Meetup talk with slides showing fake-quicksort and then "Real Haskell" for production applications where none of those things are acceptable and you have to do things entirely differently. I'd rather the difference just be made explicit via the prelude or some other mechanism. It's a real honest barrier to bringing Haskell into a conversation with other programmers that are used to languages that don't do this. For example, I work with Python programmers and it's tough to get off the ground because the "right" way of doing things tends to make difficult things possible and easy things look difficult (as opposed to making everything look misleadingly easy, which I'll admit is not a point in Python's favor). - Strings don't completely change semantics between baby's-first-script and "Here's the production release of the YouTube backend." - Lists don't go from "here's a list" to "Oh, lists aren't really good for data. Lists are actually for control flow. Use Vectors instead... and no they aren't standard you need to download a library... and no there's no syntax support you need to turn on this language extension..." - Data access goes from "prefix all your records with something because the most polymorphic language you've ever seen hasn't figured out to have two different data types with an "id" field in them in a single file" to "you should install Lens and read these series of blog posts and watch these several hours of conference talks." I totally understand that there are deep principles behind these things and that libraries like Pipes and Conduit and Lens are all doing active research on the right way to handle them, but you just can't act like that's the same language we learned in cis192.
Off the top of my head, ordered approximately by importance: - on-the-fly typechecking - autocompletion - documentation and type lookup; types also for compound expressions - semi-automatic management of imports, dependencies and LANGUAGE pragmas - auto-formatting - jump-to-definition - syntax highlighting that's actually correct and complete - HLint-style suggestions - profiling, testing and benchmarking integration Many of these features are already covered by existing tools, but either with unsatisfactory performance or not particularly well integrated. (No offense intended; I imagine that writing Haskell tooling is exceptionally hard.) If a majority of these features became a reality, that'd make me very happy indeed. Good luck to you venerable fighters for a better Haskell experience.
This is one of the use-cases we have in mind.
&gt; I read my reply again after I wrote it and it sounds pretty whiny. It honestly doesn't to me, and anyway I think it should be OK to sound whiny (because sometimes there are reasons to be). Tiptoeing around genuine nonsense out of fear of being accused of "trolling" or "sounding whiny" = kernel of groupthink, it's what sustains echo-chambers.
Note that Haskell does not let you re-export things qualified, although perhaps that is fixable. I've wanted this on many occasions for my own libraries.
What sort of backward compatibility are you referring to ?
You know, the one thing that the `IO` monad allows that can't just be factored into `Free X` is being able to catch `error` and other screwups. Ideally, you should program in such a way that those things don't happen, but unfortunately they do, especially in Someone Else's Code.
Well, you can have that in Haskell, with the [`unamb`](https://hackage.haskell.org/package/unamb) package. Your `or` above becomes: or x y = unamb (x || y) (y || x) which evaluates both possibilities in parallel and returns the one that returns first. However, `unamb x y` is only referentially transparent when, if `x` and `y` are both defined, `x ≡ y`.
Yeah I was already thinking of automatizing the process of actually finding the commits that don't compile. Maybe it'd even be viable to create a crawler that goes through Haskell projects on github. I never heard of .travis.yml before yet tho, so thanks for mentioning that. If that file is not present then at the least I can try a cabal install if a .cabal file is present.
Yes, I am. It does not matter whether I am sure about this or not, since we are talking about an objective fact, namely that significant resources of this course are only accessible to registered, fee-paying students.
Just get a book like Graham Hutton's and you're set. It contains excellent exercises, making it eminently suitable for autodidacts. 
Apparently, the consensus was that it's not worth it, which I strongly disagree with. Having to put $ in front of do which opens a block is completely redundant...
Ok, I'm now quite convinced that it was Wadler :) But while he delivers the punchline in the video, it is not the one I was thinking of. He wasn't on the panel to begin with and only came up to "answer" the question and left. Which was comedic gold. 
`$` is function application: `f $ x` is equivalent to `f x` `.` is [function composition](https://en.wikipedia.org/wiki/Function_composition): `f (g x)` is equivalent to `(f . g) x` This can be seen clearly in the type signatures: ($) :: (a -&gt; b) -&gt; a -&gt; b (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
`+` and `/` are both left-associative. So how can they be so different? The point is, that the difference is the behavior. Even though `+` and `/` have common properties (associativity) they both do totally different things. So do `.` and `$`. &amp;nbsp; It's important to understand that `.` does not call any function. It is function **composition** - that means that it is creating a **new** function. &amp;nbsp; `f . g` is a new function which calls `g` with the parameter and then passes the result as the parameter to `f`. f . g = \x -&gt; f (g x) Both operators are often used in the same place, like `f . g . h . i . j $ x`. It passes the value *right* of the `$` to the `f . g . h . i . j ` chain. The chain `f . g . h . i . j ` can be read most easily from right to left. It first calls `j`, then passes the result to `i`, then passes *that* result to `h`, then passes ... &amp;nbsp; `$` is often used to avoid using parentheses. As in `(f . g . h . i . j) x` becomes `f . g . h . i . j $ x` Always look out for the `$` - the value on the right is the parameter for the function on the left. &amp;nbsp; Hope this helps. :)
Correnct
`.` is a way to create functions, not to apply them negateSum = negate . sum sumOfEvens = sum . filter even numOfOdds = length . filter odd `$` is a way to apply functions...some of which you might have created with `.`. foo = length $ filter even [1..10] bar = negate . sum $ [1..10] -- apply the function (negate . sum) to [1..10] You don't apply any functions with `.`: you use it to create functions. 
Huuuuh. Well. Interesting. Under the hood, this is `unsafePerformIO` on actions which compute the values in different threads. To be honest, I don't have the theoretic background to say how `IO`, `unsafePerformIO` and threads would be formalised. It's interesting it is possible, though.
I have to say that HList from GADTs talk looks very nice (I remember reading Oleg's paper on HList and its implementation there seemed a bit more complex due to not using "extra" features). Are there even-better ones to check out?
What is deferred?
No that is not what I meant.
2015 didn't work out as I hoped. I plan to work and complete out of process template haskell[1] this year. This should give us TH for cross compiler in the same way TH works for ghcjs. Edit: fixed link. [1]: https://github.com/angerman/oopth
I was able to [compile stack](https://pr06lefs.wordpress.com/2015/09/21/ghc-7-10-2-working-ok-with-correct-llvm/) on a pi2 with ~1gb of memory and a 500mb swap. Took over 10 hours, maybe near to 20 hours. I limited the compile to just one cpu to avoid excessive memory use. Compiling is faster, but not radically so, with an SSD and the bananapi. Haven't tried compiling stack there (still on 7.8.2 and arch) so can't compare directly. 
Nice. BTW, your oopth seems to be a private repository currently.
$ takes 2 arguments: 1 function and 1 value. (.) takes 3 arguments: 2 functions and 1 value. As you can see by the type signatures, these functions are very different. The thing that makes them similar is that they both can be implemented in pure lambda calculus. Other examples of such functions are: id,flip,const and fix They can be found in Data.Function
That's really good news!
Sorry about that. Miss typed the URL, correct one is https://github.com/angerman/oopth . I updated the parent as well. 
Thanks for this. With "notes posted online" do you mean the notes given with each assignment : like [cis 194 assignment 2 adt notes](http://www.seas.upenn.edu/~cis194/spring13/lectures/02-ADTs.html) ? If they are different please share.
I'm pretty sure what you actually want is GHC support for Liquid Haskell. Currently, without GHC support it currently looks like this: newtype UpToHundred a = UpToHundred { getUpToHundred :: [a] } {-@ data UpToHundred a = UpToHundred { getUpToHundred :: { v : [a] | len v &lt; 100 } } @-} ... and you can imagine that you could avoid the repetition once it's a GHC extension. That will enforce at compile time that the length never exceeds 100 and it requires no Template Haskell and it's more flexible since you can enforce the limit even when you don't know the list's contents statically. See the [Liquid Haskell tutorial](https://ucsd-progsys.github.io/liquidhaskell-tutorial/01-intro.html) for more details.
One is function application, the other is function composition. One binds as tightly as an operator can, the other binds as loosely as an operator can. The only time they act 'the same' is that f . g . h $ i = f $ g $ h $ i Note that you need to keep the last $ for that to work
I hope for the next year that Haskell will have a killer application/web site/framework that will let programmers think: "I have to learn Haskell because xxxxx uses Haskell"
[OT - blog rendering] there's something wrong with the code blocks; the font appears too big on Chrome/OSX
We already have an http transport so the server part should not be a problem in that case. However there is not yet an editor integration that makes use of this.
`(.)`, like `map`, is just a specialized `fmap`.
We would also be happy to provide any assistance needed if somebody comes along and wants to provide vim support. The reason we don’t have it yet is simply because none of the people working on hie atm use vim.
I've also been reading quite a bit, was particularly interested in systems which don't have a fixed time step, i.e systems where certain sections can operate at different step sizes. During that search, I've came across N-ary FRP which seems to do some good on that front, but indeed again came with the notion of continuous functions. Thinking about it as well, I've came to realize continuous functions are really hard to model, specifically in defining semantics for `when`, i.e triggering an event when the function satisfies a predicate. What I came up with is that you can do a lot good work with just piecewise constant functions, even if those pieces are not the same for different functions in your network. It is easy to see the semantics of "when" for these kinds of functions. In such a system, there's very little difference between "events" and "behaviors", they are both a vector of `(time, value)`, with behaviors having an initial value as well. You then can have primitive functions of type `[(time, a)] -&gt; [(time, b)]` that you can build up with. I won't go into more details because I never got it to actually work. Was missing many primitives and loops were hell. I'm waiting for someone smarter than me to solve it well. One example I tried to overcome was collision detection in a physics simulation. Many systems use time slices and check if collisions happened. In order to model collisions in the proposed discrete system, I flipped the question on its head, "lets model the distance between the objects as a piecewise linear function, using relative locations and relative speed" and then setting "when distance &lt; epsilon". Of course if the values changed, the function will piecewise change to a different linear piece.
I could never see a difference between events and behaviours - it complicates systems that are too complex already. (time, value) is the right atom for frp.
Good one. I think it will also be a big push for Haskell-based tooling if GHC adopts Shake. It will make Shake a very "battle tested" and "blessed" solution. LLVM is important for me too as it is the route to ARM for me.
But... you can have continuous functions where the domain is the rationals. Reals don't really have anything to do with continuity, per se. All you need is for your type to entail that for any two distinct x1 and x2, there exists an x' between x1 and x2.
&gt; inefficient at best impracticable at worst What made you think so? I see no direct relation here.
You could use a combinator to perform these combinations on two (time, value) series. For example, two different combinators "only have the pair when 'a' changes" vs "have a pair when either 'a' or 'b' change". You don't actually have to model them differently, other than, possibly, the initial value for behaviors.
The *semantics* and actual *implementation* don't have to be directly connected. There can be system with continuos time semantics, but the primitive collection picked up in the way, so it can be implemented efficiently. For example lack of `sin(t)` - like continuos functions. I see that discrete time FRP is a subset of continuos-time one. Practical restriction. Yes, you can speak about *changes eventstream of the behaviour*, but IMHO if you *have to*, you aren't getting original FRP idea right.
I'm not quite sure You can have continuous function on the rationals if you only consider rationals indeed. But your problem then is that your domain is not complete (non-convergent Cauchy sequence) and that's why you build the reals as the completion of the rationals. Reals have quite to do with continuity. The function (defined on R) with value 1 on the rationals and 0 on the irrationals is everywhere discontinuous. &gt; for any two distinct x1 and x2, there exists an x' between x1 and x2 This is exactly what \u\atzeus meant by &gt; A better name is probably dense time: between any two points in time, there is (or might be) another point in time EDIT : Formatting
When designing [Hate](https://github.com/bananu7/Hate) I specifically wanted to avoid introducing the linear time idea. Since games were supposed to be one of the prime uses for it, fixed timestep updates in fact seem to me like something liberating in its simplicity rather than a limitation. I would say *equally expressive* is very strongly connected to what you're trying to express. A computer program is always a model of reality at best. If your model is still good enough to be useful without the continuous time, then I'd say that it might make many things simpler in concept, specification and the implementation.
But then again we might argue whether "getting original FRP idea right" is the goal on its own, or whether it's just a tool, right?
How is that any simpler than defining a new type Behavior value = "[(Time, value)] with an additional initial value" and writing the two (three) different combinators as (&lt;*&gt;) :: Behavior (a -&gt; b) -&gt; Behavior a -&gt; Behavior b apply :: Behavior (a -&gt; b) -&gt; Event a -&gt; Event b unionWith :: (a -&gt; a -&gt; a) -&gt; Event a -&gt; Event a -&gt; Event a ? I would argue that, in fact, introducing a new name reduces complexity, because it allows other interpretations for Behavior than "list of pairs with initial value" that make it easier to understand the above combinators.
I don't see what cantor's diagonal argument has to do with that diagonal traversal (which is also due to cantor). 
I agree with apfelmus. Moreover, making these datatypes abstract allows the implementation to forget past values, preventing space leaks.
No, that's not it, you told me earlier: https://www.reddit.com/r/haskell/comments/3ye2w8/how_come_refined_isnt_in_base/cygir3y - it does extensive invariant checking at compile time, but doesn't provide any code, so I still have to implement `FromJSON`, `Functor`, etc. for `UpToHundred` myself.
Yep, you can do it by overriding methods in the [`Enum`](http://hackage.haskell.org/package/base-4.8.1.0/docs/src/GHC.Enum.html#Enum) class: instance Enum YourType where toEnum = ... fromEnum = ... Merely overriding `pred` and `succ` won't work, because `[a..b]` uses `toEnum` and `fromEnum` and not `pred`/`succ`.
Actually, `(..)` is just a synonym for `enumFromTo`, which only requires that your datatype have an Enum instance (which is where `succ` comes from): module Main where data Foo = Bar | Baz | Buz | Bif | Bok deriving (Enum, Show) -- prints [Bar, Baz, Buz, Bif, Bok] main = print $ [Bar .. Bok] For future reference, I knew off-hand, but in your case, you could have used your suspicion to point you to the same answer using ghci's ":info" command: Prelude&gt; :info succ class Enum a where succ :: a -&gt; a ... -- Defined in 'GHC.Enum'
I want to see the numerical, scientific computing, data science ecosystem grow. There's huge opportunities here with Haskell's comparative advantages. A starting point might be to leapfrog visualization frameworks in other languages and go right to web-based charting. For numerics, things are still schizophrenic between hmatrix, repa, accelerate, etc. I also want to see higher level libraries (probability modeling, neural networks, simulations, etc.). Things seem to have stagnated with projects like hydra and hlearn. What's going on with distributed computing? Cloud haskell / distributed-process seems to be slowing down. Is it time to revisit or rethink those approaches in light of today's rapidly evolving cloud tech stacks? I'd like to see Haskell "succeed", despite our best efforts not to.
Python, Swift don't need to eg. import qualified eg. `string` and `dict` by default, because they provide string and dictionary literals like: `{ "something": x, "count": 6 }`, the rest is magic of OOP method syntax (I suppose this is one of the biggest reasons why people cling to OOP so much): // language-agnostic pseudocode Text func(Text text) { if (text.null || "FOO".isPrefixOf(text)) { return "HEY THERE"; else { return text.strip().toUpper().reverse().replace("A", "B"); } } Against Haskell's: -- But, hey, the type of 'text' is automatically inferred! func text = if Text.null text || Text.isPrefixOf "FOO" text then "HEY THERE" else Text.strip $ Text.toUpper $ Text.reverse $ Text.replace "A" "B" $ text
You need to make your data type into an instance of [`Enum`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#t:Enum), which means defining `fromEnum` and `toEnum`, or letting ghc guess what they should do, which is normally what you'd expect.
&gt; I don’ know how to css I wouldn't worry about that, no one does.
My question goes back to Conal's rejection of most FRP libraries. He seems to be saying that Elm isn't a true FRP because static signal graphs don't respect the requirement that time be viewed as a continuum whose behaviour we analyse at limits of internal partitions. Why is that a strict requirement?
It seems like it is the cause of the time leaks. 
Does the derived instance of `FromJSON` somehow guess from the name `UpToHundred` that the list can't contain more than 100 members? Did I miss the creation of strong AI? If not, Liquid Haskell, will probably reject such an instance.
"devop" question; what if a docker image runs a program that needs to talk to other images, as in a message-passing distributed application? Sorry for the dumb OT but I'm still a bit stumped re. what one can do with Docker.
The idea that an L-System is the fixed point of an m-coalgebra is the central insight behind my [lindenmeyer](https://hackage.haskell.org/package/lindenmayer) package as well. I wrote about it a little [here](http://reinh.com/notes/posts/2015-06-27-theoretical-pearl-l-systems-as-final-coalgebras.html). In fact, it is also a final encoding, which is a nice property. (A very similar final encoding can be given for Turing machines as well.) You can also plug these into the turtle implementation in diagrams and generate some nice fractals. You can also go one step further in abstraction. If, instead of functions `Monad m =&gt; a -&gt; m a` to represent rules, you have functions `(Functor f, Monad m) =&gt; a -&gt; f (m a)` then you can use `coiter` to get a `Cofree f` structure that represents a choice of determinacy: * `f ~ Identity`, which generates an infinite stream, a deterministc L-System isomorphic to the one given by `iterate`. * `f ~ []`, which generates a rose tree, a non-deterministic L-System where each letter can have zero or more productions. * `f ~ ((-&gt;) Bool)` or equivalently `f ~ Pair a`, which generates a balanced binary tree, where each letter has exactly 2 productions, and similarly for other representables. And so on.
What Conal states is not related to graphs, but that FRP *is* by definition a thing based on time being represented as a continuous thing. This lets you naturally sample it at any rate you need, and also gives it simple yet elegant semantics. I mostly refer to recent "The essence of FRP" talk http://begriffs.com/posts/2015-07-22-essence-of-frp.html
The cumulative cost of trying to refer to events in the past. 
We need a type checker for css so that it only typechecks if it looks correct in all browsers. Thinking about that it’s easy, just don’t typecheck at all :)
that's a space leak, right? old events can't be garbage collected because they might be folded over, or something. 
iiuc, soon with -XTypeInType, two are enough. 
This is not correct. Time leaks refer to the fact that some implementations inadvertently need to remember all events that ever occurred in order to implement the semantics they promise. But this can happen in systems with *discrete time* as well, it has nothing to do with continuous time! (The solution is to promise slightly less in the API, namely only offer combinators that are "forgetful" in a certain, but precise sense.)
There are some packages doing just that, e.g. [UISF](https://hackage.haskell.org/package/UISF)
Aye, OP, while Enum is what you asked for, iterate is what you want.
If your axiom is of type `a` and your inference rule is a function of type `a -&gt; a` then you might find `iterate` useful. iterate :: (a -&gt; a) -&gt; a -&gt; [a] iterate f x = x : iterate f (f x)
Unfortunately there isn't much interest in GUIs in the Haskell community. There aren't even bindings for QT (hsqml notwithstanding), which is arguably the most popular library. See [State of the Haskell ecosystem](http://www.haskellforall.com/2015/08/state-of-haskell-ecosystem-august-2015.html#standalone-gui-applications) for more information.
There are pretty real pain points in determining how many intermediate steps to use, and the fact that you don't get to use laws gained in a subclass to reduce the number of definitions you have to supply in superclasses in Haskell. Also things like abs/signum really want fundeps/TFs. Finally, assuming we can agree on what to replace the numerical tower we have with, the biggest issue with any sort of new numerical tower is figuring out how the heck to deal with the migration from the old to the new. It would be a pretty big jump. I'd love to see a nice numerical tower in Haskell, but I'm not fully happy with any of the efforts I've seen towards a more rigorous hierarchy (by me or by others).
Could you elaborate more on how stating the problem in terms of the language of FRP differs from specifying event flow, and how that might affect the ease of development?
&gt; Unfortunately there isn't much interest in GUIs in the Haskell community. I don't think this is true, and I'm sure it would make Haskell more popular.
You can make events "step valued" behaviours if you are careful about it. But making behaviours events makes no sense -- behaviours are defined at all points in time -- events only at specific ones.
Do you have an idea about the minimum set of requirements that a new numerical tower for Haskell must fulfill? A new numerical tower would be a large breaking change, but it feels like the "right" step for the long run.
I think the reason why there isn't much interest in GUIs is that Haskell (the language and core libraries) drives practical people away, so there's no one left to be interested in GUIs. Seems like most development in Haskell has been up in the heavens (Free monad effect interpreters! Category theory! Dependent types!), leaving the earth neglected and thorn-ridden. What I mean by that: — Haskell's type-driven development is amazing, best in class! — Cool! What about something simple, font size percentage from 25 to 1000? Should be easy in best-in class type language! — Here you go: http://lpaste.net/2425347252999421952 — ... And more nonsense like this: Prelude unusable for non-toy code, no qualified exports, so you can't put all the cumbersome imports you need to write non-toy code into one module and import only that, [no OOP style methods](https://www.reddit.com/r/haskell/comments/3z0ukd/reflecting_on_haskells_2015_and_my_wishes_for_2016/cyjer51), [innocent-looking duplicate code elimination leading to incomprehensible errors](https://www.reddit.com/r/haskell/comments/3rdceq/monomorphism_restriction_error_message/cwnkyqp), [Haskell basically has Java resource management](https://www.reddit.com/r/haskell/comments/3t5p5e/is_there_something_comparable_to_c_destructors/). Looks like it's improving now though that there's evidence of progress like OverloadedRecordFields and Foldable in Prelude, hopefully the progress won't halt here.
I have done some work on one a few years ago when working on a game. I like the way the API came out, but it would need a fair bit of work before I could open source it. It's built on top of Gloss for one thing. It has recursive views, an event driven model, encapsulated responsibility for handling of events etc. Lens are used extensively. I should revisit it. 
A library i found interesting (but it died in Aug 2014 or so) was LGtk: https://wiki.haskell.org/LGtk Basically you define your model, build an Iso-Lens to your view and have them always in sync. The videos on the Blog are quite nice: https://lgtk.wordpress.com/ Maybe an idea for someone who wants to build one from ground up.. Or just make a backend-agnostic thing.. and support multiple backends (GL, GTK, Qt, HTML/JS, ...)
gui programming in haskell is essentially a hopeless endeavor. i would recommend taking a look at f# if you want to do gui stuff. edit: i am glad i am getting downvoted for this when it is sadly true. i asked nearly identical questions over a year ago and got the same non-responses. here is a quote the op has made elsewhere: &gt; I made a list of GUI based programs I intended to write in order to better learn Haskell but every time I'd start a project I'd run into some complexity that EXACTLY defines my experience. there simply isn't a good solution in haskell, and i don't foresee there being one anytime soon. i find this particularly so given that better and still functional solutions exist (i.e., F#).
it is absolutely true. i asked many times about it as a beginner and was basically told i should contribute my own if i was so interested in it. it is one of the reasons i stopped learning haskell.
&gt; (but i died in Aug 2014 or so) Spooky.... and sorry to hear that. More seriously, thanks for the interesting link. 
Do you think research in that area is fruitful though? I'm trying to start focusing in on topics for my senior thesis and I'm interested in topics in the FRP and GUI domain. There doesn't seem to be a lot of literature to that end. The topic is of practical interest to me as well - I made a list of GUI based programs I intended to write in order to better learn Haskell but every time I'd start a project I'd run into some complexity... Whether it was the code seeming to imperative and uncomposable or the ongoing war with dependencies. I think having more people making small GUI based apps would be great. 
Looking through the [Haskell Prime list](https://prime.haskell.org/query?status=new&amp;status=assigned&amp;status=reopened&amp;group=section) there's a lot of tickets to do with the numeric tower. But it looks like the fundeps/TFs dilemma is where we are really stuck - there's not even a ticket for TFs. If you can't make a decent numeric tower without type-level programming of some sort, then type-level programming belongs in the core language. 
Context, context, context! Continuous time means that the semantics of a signal are a function from the reals, not that the function itself is continuous. It's fine in those semantics to have a point in space that suddenly shifts from (0.0, 0.0) to (0.7, 1.638) without passing through any intervening points. 
I'm a big fan of topology, but the continuity of a function/signal is not relevant - something might slide continuously across the screen or it might suddenly leap, but that's all fine - discrete vs continuous time is about which domain you use for time, not about continuity of functions. 
Because there are simply more and more complete options when it comes to graphics and GUI utilities in F# (i.e. .NET). People may hate on Microsoft, but they easily have the most advanced GUI environment with WPF, at least in my opinion. That and now that .NET itself is becoming more and more open source and cross-platform, you can even do cross-platform GUI development. I have batted around a project for a while now, trying to find the best solution, but I have decided to go with C# and F# and to just go for it, trying to use F# as much as possible. My project requires both GUI and graphics libraries, neither of which I am interested in writing myself from scratch. There are no good solutions in Haskell for such tasks. Here are the ones I am familiar with regarding F#. * [WebSharper](http://www.websharper.com): The web solution. Cross-platform. * [F# Windows App](https://visualstudiogallery.msdn.microsoft.com/ad49fd5c-930c-4fe6-a30e-2d0d6778c565) and [F# Empty Windows App](https://visualstudiogallery.msdn.microsoft.com/e0907c99-bb04-4eb8-9692-9333d5ff4399) templates: Traditional WPF and MVVM GUI programming in F#. Windows specific but WPF is very powerful. * [F# MVC Framework for WPF](https://github.com/dmitry-a-morozov/fsharp-wpf-mvc-series): An F#-centric approach to WPF applications. Windows specific. * [WinForms](http://www.mono-project.com/docs/gui/winforms/): Cross-platform approach. You will not find such a solution in the Haskell ecosystem. Take a look at the [getting started example](http://www.mono-project.com/docs/gui/winforms/getting-started-guide/). * [Win2D](https://github.com/Microsoft/Win2D): Windows-specific since it uses DirectX, but it is an easy to use, hardware-accelerated graphics library. * Then there are other solutions like XNA and MonoGame. I myself have decided to use Win2D and WPF and will try to use F# as much as possible.
i just posted a more detailed [response](https://www.reddit.com/r/haskell/comments/3z6961/designing_a_gui_library_purely_in_haskell/cyju60y). in general, cross-platform GUIs are very, very difficult and are essentially a compromise. i have not found an elegant solution really, but the best one i can see, particularly if you want to use a functional language is F# using WinForms. it is almost completely implemented in Mono, giving you a cross-platform GUI solution. i would assume there are also solutions using Scala and Java-centric GUI platforms that would also give you cross-platform GUIs. there is GTK but it seems painful. for my own project, i have decided to just forgo cross-platform for the time being because of the headache. it's stopping me from actually doing anything, so i've decided to go headfirst into Win2D and WPF using F# as much as possible.
Great speaker too. Lots of good looking talks on this list.
&gt; [Newcomers] want to rewrite things the way they had to with other languages, but are built-in to Perl 6. Same for Haskell, only they can't write it the way they had to at all, and that "built-in" stuff are only library functions.
okay. I thought "time leak" in Haskell meant recomputing unshared subexpressions, often from laziness. 
As far as the compiler is concerned you're asking it to do different things -iterate through the lists in a different order. A hypothetical compiler could infer that it makes no difference in this context and optimise to the same bytecode - but why would it even bother?. I'm a very long way from being an expert on the topic, but I'd imagine that this would be a fairly difficult class of optimisation to apply, and relatively useless since it would gain no real performance benefit here and compilers generally aren't concerned with anonimization. If you were compiling for size (not speed) and had both examples in the same codebase then perhaps the compiler may shave a few bytes by using the same bytecode for both place.
&gt; A hypothetical compiler could infer that it makes no difference in this context. I'm a very long way from being an expert on the topic, but I'd imagine that this would be a fairly difficult class of optimisation to apply. I (apparently, naively) figured GHC would be sophisticated enough to make said optimization - assuming it was warranted.
The evaluation order in your list comprehension is not the same. It desugars to `(+) &lt;$&gt; x &lt;*&gt; y` in the first case and to `(+) &lt;$&gt; y &lt;*&gt; x`, and the result is the same because `+` is commutative, but try with `(,)` for example and you will see the difference. I don't think GHC takes comutativeness into account, and that would explain the different compiled codes.
So you don't want CloudHttp to be an instance of MonadHttp? I'm confused... I really literally have no idea what is being asked here. For example you write "'vertical' composition is chaining two interpreters". What do you mean chaining? Not combining effects, but I suppose interpreting one into the other. So you have a MonadCloud class, and a MonadHttp class, and you can define a CloudT transformer, and an instance `MonadHttp m =&gt; MonadCloud (CloudT m)` and that seems just fine? And you can have a `runCloudT :: CloudT m -&gt; m` and that "interprets" the effects of one layer into the next. Is that vertical composition? What am I missing?
AFAIK GHC is not good at optimizing arithmetic. For example, it can't optimize `1 + (2 + (3 + x))` to `6 + x`. Yes, I'm talking about `Int`s. On the other hand it seems to be able to unbox those constants.
A number of folks prefer the `.` style because then you are composing functions. I tend to prefer the `$` style because it gets better inference due to the magic typing rule for `$`, so it works better when working on code with higher rank types.
Hi, I am the author of LGtk. Last time I worked on LGtk was around 2014 December and 2015 January, when I started to merge lensref (the core of LGtk) with elerea, but this was not finished. Now I am working on LambdaCube 3D (with Csaba Hruska), and I plan to work more on FRP GUIs this year too.
As a related aside would it be possible/beneficial/feasible to implement something like Python's kivy on Haskell?
Unfortunately, as you said, many of the good solutions are windows specific. But if you already mentioned F# in the context of graphics and games, I thought I'd mention the [Nu Game Engine](https://github.com/bryanedds/fpworks) in favor of those who haven't heard of it.
&gt;&gt; Having libraries based in bindings seems to be tedious. &gt; &gt; No, it's creating a GUI library not based on bindings which is tedious. Those don't seem necessarily incompatible. 
The Docker support in stack serves two purposes, one is isolating the build and the other is packaging the binaries for deployment using Docker. I'm guessing you're asking about the second use case :) Originally you could use something like [Docker links](https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/) if the containers are all running on the same host, but this has been superseeded by the [container network](https://docs.docker.com/engine/userguide/networking/dockernetworks/) functionality. So there are standard ways to allow normal communication between containers.
Sure, just use a turing-complete language where every string is a valid program. Brainfuck?
I maintained wxHaskell for about 5 years, so I think I am in a fair position to comment. There are quite a number of aspects to the problem of developing a GUI binding that are tedious to deal with (not necessarily hard, but time consuming and essentially uninteresting). None of the popular options for a cross-platform GUI binding are without their challenges. In particular, all of them have a significant number of dependencies - usually C or C++ libraries - that need to be built. * Gtk+ is great on Linux, tedious to build and customise (to an approximately native look and feel) on Windows and essentially non-functional on OSX. It is also LGPL, which is problematic for some (myself included). The major advantage Gtk+ has is that it is a C language binding, so that Haskell FFI can talk to it directly. * wx does a reasonable job of having a cross-platform look and feel, and builds (somewhat tediously) on Linux, Windows and OSX. It is a wrapper over the 'native' widgets of each of the above. The wx license is slightly more helpful than LGPL (explicitly allows binary distribution). wxWidgets is a C++ library, which means that C language wrapper functions need to be created for every method. * Qt has many of the same advantages and disadvantages as wx in practice. Most would perceive it to be more mature/stable/full-featured than wx. In practice, I found that most of the maintenance of wxHaskell consisted of: * Build system hacking - trying to find ways to express building a complex piece of C++ software in a Haskell build environment (Cabal). The fact that this is even possible is a testament to the design of Cabal, but Cabal (not unreasonably) doesn't really know about dependency management for C and C++. * Debugging the instructions for building the C++ part of the library on different targets. Although I was testing my builds on OSX, Windows XP, Debian (in a VM) and FreeBSD (in a VM), in practice many people struggled to replicate my build environment. Windows was always the worst for this, largely because GHC (used to - I haven't tried recently) contain a partial distribution of an out of date version of MinGW that did not contain some critical C++ libraries (as they are not needed by GHC). As such, quite what compilers and libraries were used was highly path dependent. * Debugging issues due to backwards-incompatible changes in part of the GHC ecosystem. In particular, we always seemed to have a small number of users who would move to latest GHC almost immediately and a larger number who stayed on a stable older version (usually a couple of versions back) - largely to avoid their own issues with backward incompatible changes in their codebases. It is essentially impossible to satisfy both sets of users with a single codebase. * Debugging issues in the (hand-written) bindings - this is fair game. * Debugging issues in the higher-level abstractions - this is also fair game, and I enjoyed this part the most (Daan Leijen who originally wrote wxHaskell was a much more proficient Haskell programmer than I was). I expect that the astute reader will see that, for a single, part-time, developer, too much of the above is just tedious busy-work, to the point where I found I never had time for actually doing anything using wxHaskell (which was the original motivation). I think **what finally did for me was the relentless pace of changes in GHC that continually break working code for no benefit to anything I was doing**. Trying to debug and test these on four OS platforms with 2-3 different versions of GHC was soul-crushing. In practice, I use Haskell rather infrequently now. It is just too hard to keep up with the pace of changes. I have OCaml code from 12 years ago that compiles and runs perfectly on the latest compiler, so I use OCaml where no GUI is needed and F# when I need a GUI. The F# GUI story is best on Windows, but it is quite acceptable elsewhere. I miss Haskell though - it is a much nicer language in many respects.
You're probably getting downvoted because people think it's the needless hyperbole (insufficient, difficult, cumbersome -- not hopeless) That said, what is an example of such a functional solution in F#? That sounds very intriguing.
I'm not familiar with Electron, but it seems to be used by the Atom text editor. The version I have installed (1.1) is quite sluggish for a text editor, doesn't have working menus, and when it tries to update to the latest version, the update process hangs and transforms one CPU core into a heater. While I'm sure these problems will be ironed out eventually, the web browser apparently isn't a mature GUI platform just yet, and I'm not even sure it's suitable for stuff like Maya, Premiere, or Pro Tools.
Several people have mentioned [electron](http://electron.atom.io/). Electron is a JavaScript library/engine; it is similar to Node.js, but runs on the client and provides a GUI. Given that architecture, what reasonable practical approaches are there for writing a Haskell app using electron for the GUI? Here are some guesses that come to mind immediately. Please comment about the ones which really are approaches that you would recommend in practice. And please add anything that I didn't think of. Please don't respond to the ones that are wrong; these are just guesses. * GHCJS, or other Haskell-to-JS libraries. * A native Haskell program that talks to the electron GUI via IPC. * Some sort of electron C API that a Haskell program can access via FFI. * Some sort of FFI in electron that allows talking to a Haskell program.
I can do the very similar thing in Haskell2010: http://lpaste.net/148344 With ImplicitParams it can be even shorter. Also my version seems to be more general -- e.g. I can have two loggers at the same time. Probably I missed the idea?
I think some unanswered questions were mentioned when [Conal Elliott was interviewed about FRP on The Haskell Cast](http://www.haskellcast.com/episode/009-conal-elliott-on-frp-and-denotational-design/), but I can't remember if they were "research-level" ones.
code is on hackage: [free-vl](https://hackage.haskell.org/package/free-vl)
yeah, not sure why this is happening. i fixed it all and then it reverted. thanks for pointing it out again, though!
I think it will choose the first one it encounters in the HList. In my original work with this I was using `Proxy` and explicit tagging, but this was really inconvenient. Ultimately if you need two different `Http` effects with the same structure, then `newtype` should be fine.
About a year. Then I learnt programming languages that I now miss when writing Haskell (of course, I still miss Haskell when writing those languages). I would say I miss other languages in Haskell usually less than I miss Haskell from those other languages, but it really depends on what I'm doing. When doing low level stuff I miss Rust. When doing GUI stuff I miss Swift. When doing type-heavy stuff or formal work I miss Agda or Idris. When writing large programs with heavy abstraction I tend to miss SML. In Rust I miss Haskell because of its more flexible type system and easier static reasoning capabilities. In swift, even more so, because the lack of linear types and the very OO APIs can be quite imperative (but they're very full featured). In Agda and Idris, I sometimes miss non-positive datatypes supported by Haskell, plus the libraries available are much more complete and wide ranging in Haskell. In SML, I miss working in a language with higher kinded types, an active community, and libraries like Haskell. Not super fussed about the "purity" of the language. There are plenty of effects Haskell doesn't track. The question isn't whether you track all side effects, but what side effects you choose to track in the type system and how you do it. I don't think there is enough work exploring the design space here, and Haskell certainly isn't the final word.
My version already contains `withLog`. Not sure what `program` refers too though. Could you please show an example you can implement using `FreeVL` that is hard to implement in my version. 
It seems to me the only difference is that the effects style above implicitly adds the effects for you where as your code (which is really records-of-functions functional OOP) is more explicit (you have to thread the interpreters through yourself). I think I actually prefer what you present; it's far less complicated, you still have "interpreters" visible in the type so you can see what effects a piece of code does and you can have multiple versions of one effect. Maybe someone can chime in if we've missed something?
&gt; It seems to me the only difference is that the effects style above implicitly adds the effects for you where as your code is more explicit I believe the difference can be eliminated with ImplicitParams. &gt; I think I actually prefer what you present I don't like both, they [break encapsulation](http://blog.haskell-exists.com/yuras/posts/effects-encoded-in-types-break-encapsulation.html). Though there are cases when encapsulation is not an issue...
As I said, it's not too dissimilar. The effect stack provides syntactical convenience and is just easier to work with. For example, if you need to add another effect to any of your methods (like add file handling to `withLog` for example) you need to add a function parameter, update your interpreters, etc. In the formulation with `HasEffects` you just add constraints to the typeclass. And the only value you need to update is the interpreter to add the new combinator. Try in your formulation to add a third effect (`Suspend` for example). Now your `MyEffect` type is an embedded tuple. So you'll move to using an `HList` because taking products of things using tuples is really ugly to work with. And once you've got the `HList` you can start using `HasEffect` to fetch effects rather fetching them from the `HList` manually and then **boom** you've recreated the same solution as mine! :)
Yep, I just tried out ImplicitParams, it's pretty cool (http://lpaste.net/148346). Sadly it seems you can't pattern match on implicit params like so: let (?http, ?log) = ioInterpreter Regarding breaking encapsulation, it don't think it does. Once you have applied `Logging m` to say `Logging m -&gt; m a -&gt; m a` you are left with `m a -&gt; m a`, which mentions nothing about logging (in OOP terminology this closure you have constructed is an object which encapsulates the logging behaviour).
Thank you, could you provide some reference material for further reading? Note that I am a complete novice in the area, and would very much need to progress through some introductory material first. I have yet to hear of coalgebras and final encodings for instance.
I'm not saying it's nowhere near Idris effects; it actually looks remarkably similar! I'm saying even the Idris effects paradigm is nowhere near what I'd consider the holy grail of effects.
Ah, I see. Yeah, fair enough!
The order of effects doesn't actually matter. If you use typeclasses all the way, the only time the order of effects matter is when you construct the `effects m` value. The order of types in `effects` must match the order in which you construct the value. But, if you use the `HasEffect` typeclass throughout, you won't run into any problems. At the egdge of your program you'll be free to construct `effects m` in whatever order you wish, and `HasEffects` will traverse the effect stack looking for the right effect until it finds it. Hopefully this makes sense!
When I started working in Java I really missed haskell. I looked at method definitions and despaired that you couldn't tell what it did from the types. Nowadays, [I think this tweet is quite relevant](https://twitter.com/oscargodson/status/639545506792738816). I really, really miss Java's tooling, ide support, automated refactoring and testing story. Haskell has many good things going for it, but so do other languages and their ecosystems.
[**@oscargodson**](https://twitter.com/oscargodson/) &gt; [2015-09-03 21:08 UTC](https://twitter.com/oscargodson/status/639545506792738816) &gt; When optimistic jr devs realize computers are awful and no programming language is best &gt;[[Attached pic]](http://pbs.twimg.com/media/COAfP-HUwAAyCB8.jpg) [[Imgur rehost]](http://i.imgur.com/TFNNDMX.jpg) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
I should probably try Emacs one of these days. Devout vim user so far, but at a glance, the Haskell ecosystem for Emacs seems more lively. Using GHCi(-ng) instead of ghc-mod as a backend sounds promising as far as performance is concerned.
check out the wagon talk if you haven't already: https://www.youtube.com/watch?v=mUAu7lcgYWE For the front end, it boils down to the "javascript problem". Most haskellers seem to like GHCJS for the front end, but it still feels a bit of an impedence mismatch due to imposing haskell semantics wholesale on the transpilation and because of the friction to wrap javascript FFI, relative to say purescript. Definitely more work is needed, but I think this is where most desktop apps (with a few exceptions like games) are going, even if the ultimate solution isn't the current-best tech stack. 
So this handles the case of nesting e.g. State with Exception? Question is whether the state that was just set is lost when an exception is thrown or not.
how you interleave effects will depend on the final `m` you choose as the result of interpretation, which is also independent of the effect stack ordering. 
Tooling in Java is absolutely great, must admit that. And the language is not that difficult, I guess you got it in a couple of months? If you learned Haskell first and then Java, then you probably never had that feeling of being restrained that affects Haskell junior developers.... 
That [Haskell Book](http://haskellbook.com/) isn't on here is cray cray 
If you are familiar or like `extensible-effects`, I would stick with that. It is based on a paper by Oleg and has an established ecosystem of effects to work with. There is lots of room for discovery with respect to the van Laarhoven encoding, which has not been battled tested as much as `mtl`, `extensible-effects`, or `freer`.
I'm having the same issue, with the same package. Running Stack on Arch. This is the one thing that always prevents me from seriously delving into Haskell. The toolchain is a nightmare and every single time I try to play with it, there is some installation or dependency bug. Having complained, though, using Atom editor with support for ghc-mod is like a dream. Being able to see type signatures of functions at a glance is a wonderful thing.
This feels very similar to the `mtl` approach except you take more control of the dictionary search and passing work. Normally with `mtl` you just construct the type of the interpreter and the typeclass resolution system will do a little work to figure out what the right dictionaries are. Here you'll have to explicitly build interpreters through values of things like `logIO` `httpIO`. To use a previous example, you might have the `CloudFiles` effect and the `HTTP` effect and use two interpreters like `cloudFilesToHttp` and `httpToIO` httpToIO :: Http IO cloudFilesToHttp :: HasEffect effects Http =&gt; CloudFiles (FreeVL effects) I'm not totally sure how you combine those two, but it seems possible in principle. 
Those that understand the basics of pure programming and with a little, but not much, experience of programming in the IO Monad. The working example was not very relevant to me, but all the commentary along the way was.
Every language puts up a cage one cannot see, except by using other languages. There's a pretty aggressive partial ordering here, and after Haskell, there is little one can learn from most conventional languages. Nevertheless, Haskell also puts up a cage one cannot see. I am nostalgic for APL, a language I dreamed in 40 years ago. I wish that I had written a Forth interpreter from scratch, back in the day. I wish that I had mastered Prolog, though it's a one-trick pony. I miss Lisp, one of the few languages that reveals Haskell's cage. Even if one believes that Haskell always offers a better way, one needs to understand what the lost civilization of Lisp once knew.
I felt comfortable with using Haskell for "general purpose programming" after about a (dense) year, but I sometimes still desire features from other languages, e.g. dependent types, first class modules, mix-fix operators, and even `1 : Int` instead of `1 :: Int` ;) I'm currently a M.Sc. student at the University of Freiburg, where I had my first contact with Haskell around 2 years ago in a [functional programming course](https://proglang.informatik.uni-freiburg.de/teaching/functional-programming/2013/) held at our programming language chair. The course was great, the professor and assistant enthusiastic, and we've covered a lot of topics: functors, applicatives, monads, monad transformers, parsers, quickcheck, arrows, GADTs, generics, free monads, Data Types a la Carte. We also covered Pierce's *A Taste of Category Theory for Computer Scientists* up until including monads (the students pushed into this direction `^^`). At the end of the course, we took an excursion to a small company (also running the local [Haskell Meetup](http://www.meetup.com/Freiburg-Functional-Programming-Meetup/)) welcoming us with pizza, cold beverages, and a presentation about using Haskell in production. The course was dense, but fun enough to spend sufficient extra time meditating on the content :) After the course ended, I was very eager to use Haskell, so I started a few small projects. Although most of the theory was still present, I had no practice in structuring projects and in *putting it all together*™. So another one or two semesters passed, until juggling monad transformer stacks felt natural, I got familiar with `lens` and other common libraries, got a working spacemacs/nixos/stack setup, etc. My choice of language currently depends mostly on the project's runtime demands: if it's less than soft real-time, I almost exclusively use Haskell, if not, I often use plain C++ or mix C++ and Haskell via a C-API. However, my long-term companionship with C++ might end, when I find a high performance language I like or get more familiar with the low-level Haskell structures' performance characteristics. 
Originally I just wanted to see if it was possible to extend the van Laarhoven encoding, but now that the dust has settled I have to agree somewhat. Though I prefer this model because, outside of `HasEffects`, I don't have to understand the typeclass resolution semantics to understand how things in `mtl` resolve. RE `CloudFiles`: this part of the design space still needs exploring w.r.t. the van Laarhoven encoding. To have an effect that requires another effect might look like: ``` data CloudFiles m = CloudFiles { save :: Http m -&gt; File -&gt; m () } ``` But this could probably get cludgy. Alternatively, you could supply an interpreter of `CloudFiles` where the `m` compiles to `HasEffect effects Http =&gt; FreeVL effects`, and this is where some pretty interesting this might turn up.
Hmm, that's interesting. Not quite sure. The van Laarhoven encoding is similar to `ReaderT (EffectStack effects m) a`, and so it's impossible to tell how `EffectStack effects m` is being used. However, if you have a morphism between two effects (e.g. `CloudFiles m -&gt; Http m`) it may be possible to modify an existing effect stack with that morphism.
So your code can't specify in any way that it needs a particular ordering to work properly?
No, the order of effects as listed in the `EffectsStack` does not impact the semantics of interpretation. You define an interpreter by providing the functions for each effect (`effect m -&gt; m a`). The resulting monad you choose can provide ordering-specific semantics, but ultimately each effect is orthogonal to one another; only `m` binds them (pun intended).
Totally agree! I also think in terms of "cages" a little bit. And to be honest, even if I find myself very productive using Haskell (no need to fight with the process, just think in the problem), I'm always afraid that it will throw off junior co-workers. 
Oh I agree with all that, yes. 
But my whole point is that a two element set has only two possible topologies. Neither can be homeomorphic to any infinite topology like that of the Sierpinski gasket. Oops - there are four topologies in total: discrete {{0,1}, {0}, {1}, {} }, trivial {{0,1}, {}} and two half way - {{0,1}, {0}, {}} and {{0,1}, {1}, {}}. In the half way topology, the checkbox could change one way to another but not back and have continuous signal. I'll grant that that's more interesting, but it's certainly not useful. (Aside: the only Hausdorff topology you can put on a finite set is the discrete topology.) I'm afraid there's no topology you can put on a two element set allowing the checkbox to toggle with continuous signal function that doesn't force you to use discrete time. Conclusion: continuity of functions isn't what you want, unless you want discrete time, from which all functions are continuous, so in either case, continuity of signals is irrelevant. 
if you don't want to prefix each file with a dozen extensions, you can put them in your cabal file (or pass them to ghc). with project scaffolding (or a bash alias) this makes it easy to use for new projects. anonymous records would be great. but the use case of passing them as keyword arguments to some API doesn't seem like a big deal to me. datatype declarations are oneliners. agreed on the lack of tutorials. 
Well, I for one would like to thank you again for your work on wxHaskell! Even more so, as it was most likely done in your free time! I wish that the Haskell community had ways to enable / pay for somewhat mundane but still highly valuable work like this.
This seems to induce the familiar `extensible-effects` type-inference torture if I try to make slightly general effects. This all looks great data Yield a m = Yield {getYield :: a -&gt; m ()} data Sink a m = Sink {getSink :: m a} data State s m = State {getState :: m s, putState :: s -&gt; m ()} await :: HasEffect effects (Sink a) =&gt; Free effects a await = liftF getSink yield :: HasEffect effects (Yield a) =&gt; a -&gt; Free effects () yield a = liftF (`getYield` a) get :: HasEffect effects (State s) =&gt; Free effects s get = liftF getState put :: HasEffect effects (State s) =&gt; s -&gt; Free effects () put s = liftF (`putState` s) but then I tried the inevitable one_two = do yield 1 yield 2 and get the familiar The type variable ‘a0’ is ambiguous When checking that ‘one_two’ has the inferred type one_two :: forall a (effects :: [(* -&gt; *) -&gt; *]) a1. (Num a, Num a1, HasEffect effects (Yield a), HasEffect effects (Yield a1)) =&gt; Free effects () Probable cause: the inferred type is ambiguous Similarly with the textbook tick = do s &lt;- get put (succ s) I get The type variable ‘s0’ is ambiguous and even if I write the maximally determinate signature, tick :: (HasEffect effects (State Int)) =&gt; Free effects () I still need to mark the type of the put: tick = do s &lt;- get put (succ s :: Int) to get it to type check. And so on. 
Interesting and different approach! Thank you for sharing. It is a linear runtime search whenever a function in one effect is used. Of course, all `transformers`-based effect libraries have a similar cost (`mtl`, `monad-classes`): they have to `lift` to the right layer. And the constant factors in the various free monad libraries are very high. So I am very interested in benchmarks. You might like to contribute an example to https://github.com/feuerbach/freemonad-benchmark. (Given a theoretically possible `O(log n)` hlists, that's less of an issue.) 
Normally, the younger they are, the faster they learn. My piano teacher got annoyed with adults who said that songs in 5/4 time or 7/8 time would be too hard for children and teenagers to learn, but they always picked it up faster than the adults, who struggled with the novelty. I think a similar thing applies to programming paradigms. The more experience you have elsewhere, the harder it is to think differently, both intellectually and emotionally. 
Your judgements of what are useful and mine are clearly different. Let's leave it at that.
yeah, I just figured the "going there" part happened at compile time. I should probably just look at the Core generated since the code is fairly small. If it has to crawl the dictionary for every function call, then you're right.
I'm ashamed to admit it, but sometimes I miss being able to `flatMap` over arbitrary nonsense like I can do in Scala.
I'm pretty sure you can write up your partial interpreter using the CoRec stuff I've talked about for the past while. I do something like this for optimization of an EDSL where I factor out language feature-specific optimizations.
Exactly what I was looking for, thank you! The latter version produces exactly what I requested. For anyone else reading: you also need the `RankNTypes` extension enabled.
This approach is how you can encode those domain specific rules into the types of your API. I do this for exposing a (usually parallel) computation API that stretches between micro controllers with tight constraints, to multi-core CPUs, to GPUs. I think we're struggling to figure out how to incorporate some of these techniques into Haskell because of the ill-informed anti-transformer and anti-monad marketing that has accompanied it. Their sweet spot is one level up the chain, but you need people who build things to demonstrate that. I like seeing what /u/aaronlevin does with it as he's thinking about actual systems.
Look at Leinster's work on *A general theory of self-similarity* (parts I and II) and the various work on universal coalgebras and especially their relationship to fractals, e.g., [*Coalgebraic Representation Theory of Fractals*](http://www-mmm.is.s.u-tokyo.ac.jp/~ichiro/papers/reprExt.pdf). To get a grip on the terms from Category Theory, I suggest Leinster's *Conceptual Mathemtics* and Awodey's *Category Theory*.
If you're talking about multiple hosts, are you thinking of some sort of distributed config system like consul or etcd? I've never used container linking, always preferring to bind all ports to the machine's private IP address (`docker run --port $PRIVATE_IP:$OUTER_PORT:$INNER_PORT`) and then pass environment variables with that address. Then if you need some services to be accessible from outside the machine, you can either put a load balancer in front of them, or bind them to a public IP.
Shameless plug. There is an L-Systems module in diagrams contrib. Just not on hack age yet. https://github.com/diagrams/diagrams-contrib/blob/master/src/Diagrams/TwoD/Path/LSystem.hs
&gt; meant-to-be-caught Well, caught and propagated non-stack-wise (e.g. included in a HTTP response body) is surely fine, but not caught and recovered.
What does vim-hdevtools provide, that neomake doesn't? Doesn't neomake give you the ability to run hdevtools without the other plugin?
There's also a missing `&lt;*&gt;` in the `Applicative` definition for `FreeVL`, as well as a `&lt;$&gt;` in the `repeatReq` example.
I'm sure I saw it before. Anyway, it is The Right Thing, so thank you!
Reminds me of the end of my Computational Models course, way back when. The prof was briefly digressing into what might be more powerful than a Turing Machine (paraphrasing: "basically, nothing we know of; maybe if we can add time travel..."). I shouted out, "divine intervention!" Without missing a beat, he says, "That works, but it's too expensive."
I don't know for certain but it's based on the fact that there simply aren't decent fully implemented high quality interfaces from Haskell to ALL the major GUI frameworks, including the native GUIs for Windows and OSX as well as for all the cross-platform GUI toolkits. It's pretty obvious that any interactive app these days needs to be GUI based. Haskell has been around for many years (20 or so, I believe) and I find it extremely difficult to believe that GUI tools haven't happened just because nobody is interested. When I talk to some Haskell experts that I know, I hear all about things such as FRP and so forth and my eyes glaze over. As someone who just needs (for the most part) to get the job done, it's orders of magnitude easier to use a tool like Delphi, C#, or (in the Mac world) XCode with interface builder, or C++ with Qt and so forth. Heck, even the old Swing and now JavaFX is trivial. I've tried a number of "mundane" tasks including data conversion, parsing and so forth. For one-offs, it's almost always easier to just throw the dataset into Excel. The haskell parsec and attrparsec things are cute and nice but it's a hell of a lot easier to just use something like ANTLR, specially since version 4 allows separate listener or visitor trees so you don't have to pollute the grammar file. I'm not even a Java fan but I can get job done way faster.
I doubt you'll notice as `n`is probably going to be &lt;10, unless you have millions of effects.
I don't particularly like the idea of having extra effects/instances in your transformer stack that are only visible because you plan to use them while executing computations, no. I suppose it's usually good practice to newtype your transformer stack and then you can control what instances you expose. The free monad interpreter I'd like to model are those functions like `runCloudOnHttp :: CloudF a -&gt; Free HttpF a`. One thing is that you can use such a function as part of executing a computation in some type `Free effects a` even if the combined signature `effects` doesn't yet include Http. For vertical computation, if you had another interpreter `runHttp :: HttpF a -&gt; Free SocketF a`, then you can compse them to run a computation `Free CloudF a` in terms of the socket interface. Rather than a "parallel" composition where you run a free monad computation with multiple effects given interpreters for each, the "vertical" composition lets you handle an effect with several stages of transformation. If you are trying to model with with mond transformers, you get pretty far but miss some functionality if you make transformers and instances like this: `instance (MonadSocket m) =&gt; MonadHttp (HttpFromSocketT m)` and `instance (MonadHttp m) =&gt; MonadCloud (CloudFromHttpT m)`. One thing you miss out on is the more exotic execution possibilities where you have a computation with both Cloud and raw Http effects, definable in `Free (CloudF :+: HttpF) a`, and you'd log just the Http commands that some from running the Cloud actions through the interpreter, but ignore (or log independently) the raw Http actions in the original code.
By the way, I should also add that the Haskell world needs a decent IDE in which to work. Leksah is an interesting start but when you look at the IDE environments like XCode, Delphi, Visual Studio, the JetBrains IDEs for Java, Python and others, the Haskell stuff is just plain primitive. Now this is not an indictment of those who are or who have tried to build such tools (more power to them) but if Haskell is to be a more approachable environment for the general practitioner (as opposed to language researchers), then its tools need to be far more more mature. While new tools like Stack are making it easier to develop Haskell apps in the sense that they're addressing previous problems, they're still command-line based and except for Linux "priests", that's just tired, particularly if you are experienced with IDEs like the ones I mentioned above.
Hmm, what about hypercomputation? Something something turing machine in a black hole. 
the last ambiguity error stays there even with NoMonomorphismRsstriction?
`vim-hdevtools` can tell you the type of the expression under the cursor. See its [Github page](https://github.com/bitc/vim-hdevtools).
do other communities? if so, how do they do it? I guess Java has Oracle, and Swift has Apple, but Haskell doesnt have that corporate support. Python bindings seem mostly supported by volunteers, only with more volunteers. I fear this is more a fundamental problem of a small open-source community. but if someone wanted to pay me (even a low wage) to develop random haskell bindings, yeah I'd be down. 
Agree with you about Python, but then I wouldn't attempt to build a serious interactive app with Python. I want strong typing, at least as much as possible. Qt is pretty good (given what they were trying to do with C++) but when you have used systems like Delphi (for Pascal), or even the old Visual Basic, JavaFX or even Interface Builder with XCode where you can simply design your GUI visually and trivially connect code to GUI events, you begin to recognize how much impact it really has on productivity, particularly if you're a top-down developer
Note that you can potentially get (hard to find/fix) problems with the higher-rank type, as the 'g' somehow gets monomorphized in a way that can't be re-generalized. More recent versions of GHC are better about this, but historically it has been difficult, as this example would often fail to compile: test = runST $ do r &lt;- newSTRef "hello" readSTRef r -- relevant types: -- runST :: forall a. (forall s. ST s a) -&gt; a -- $ :: forall a b. (a -&gt; b) -&gt; a -&gt; b -- the 'do' block :: ST s String (generalizable but not necessarily generalized) because the `$` function couldn't attach the generalized `forall s. ST s String` from the right side of the the `$` to `runST`'s required higher rank type. (I'm not sure how this works now!)
There are surely trade-offs. And you also have to keep in mind that dependent types, especially as a programmable language (rather than as a logic) are in their earliest infancy. The core logic for dependent types is actually only a mild step above the System-F inspired calculii that Haskell and OCaml use. There are some new rules involving pi, sigma, and identity types. But at the same time, the phase distinction is removed, which actually acts to simplify the logic by a tiny bit. Dependent types allow for an extremely high level of expressivity. Many newer extensions in Haskell are aimed at trying to bring in as many benefits from this paradigm as possible... but the cost is an enormous increase to the complexity of the language, both in the implementation and in the presentation to the programmer. However, no one quite has a sound idea of *how much* expressivity is needed. It's true that you can encode (nearly) arbitrary constraints in your types. But any constraints add complexity to the program. And while proving your constraints are satisfied becomes *possible at all*, it is still very difficult for programmers. There are also limitations to traditional dependent types systems which seem somewhat disappointing when you first learn about them. The two in my mind are that working with propositional equality is really a buzzkill. Things that are "obviously equal" are not so obviously equal, and the proofs to show, for instance, that two algebraic expressions are equal to one another are frustrating and difficult to write. (However, this kind of issue should become less sharp as more tools and techniques are developed). Another issue, which isn't so bad in practice, but still annoying, is that traditional dependent type systems based do not have extensional equality for functions, quotient types, or propositional types. It's extremely common that we want two functions defined on a type to be considered equal if they send equal inputs to equal outputs. But this isn't possible at the moment without sacrificing properties I'd rather not give up (like decidable judgmental equality and canonicity of objects of closed types). Quotient types would just be handy, since they capture the idea that the same object might have distinct representations. (Think of finite sets/bags/collections, where you didn't care about order or multiplicity of a list, and you wished you could treat [1,2,3] as equal to [3,2,1] without worrying about sorting them or representing them in a "canonical" fashion. Propositional types are types which only have at most one inhabitant (a "proof"). If you know x &gt; 0, do you really care what the proof is? No. But all three of these suffer from technical issues due to the way equality works. (And until the Homotopy Type Theorists work things out, it's unlikely to change). One a more practical level, a huge issue at the present is the error messages. After 50 years of compiler design, we're getting just okay with compiler error reporting for dumb languages. But the fancier the type system, the worse the situation gets. In particular, the compiler often doesn't know how much of a definition to "unwrap" when reporting an error. You end up seeing bits of code that only vaguely resemble what you wrote because the compiler has to perform evaluation to do type checking. Another thing is that many more parameters can be meaningfully designated as implicit. (Think back to math. Do you need to annotate matrix multiplication with the dimensions of the matrix? Hell no. You just multiply and figure out the dimensions from context). But much of the talk of dependent types is hype. But it's good to have that hype. Haskell's type system had a good 20+ years of research. I don't want to say there's nothing left to explore, but the design space has been pretty thoroughly saturated. Languages like Idris, on the other hand, are only just now tapping into what a programmer's language might look like if you gave them dependent types.
Honestly, I am liking refinement types..but liquid haskell is going through issues /right/ now.
MLers take the view that the computation and the value should be distinct things, and that the computation to produce the value shouldn't be confused with the value itself. This philosophical difference explains almost every difference between Haskell and ML except higher-kinded stuff and modules.
About 1 hour. Back in college, I kept on trying to write clever C preprocessor libraries to write the code for my various pet projects faster than I could do by hand. But of course, there is no type checking in the C preprocessor. Then I started looking at other meta languages and high-level languages like Perl, Lisp, Prolog, even M4, but I kept on trying to basically write macro libraries that let you generate fast, efficient C code. Finally, in grad school, someone introduced me to Haskell. He said it was a high-level language, and I was reluctant because I had already been trying to do my thesis project with Perl at the time. But then I saw it compiled to machine executable code, so I that piqued my interest. After playing with a few tutorials, I **immediately** realized it not only did everything I was trying to do with my home brewed macro languages, it had Prolog-like compile-time static type checking on the "macros" (called functions in Haskell) so the compiler would correct your mistakes. I dropped everything I was doing and started re-writing my code in Haskell. My first attempts were an awful mess, even if it did get the job done. But I persevered and slowly got better at it over a period of years after I had graduated. I don't know anything about category theory, so I am still learning new tricks and ideas that Haskell can do for you that I would have learned about if I had some formal training in category theory.
I wish this comment had been more useful. 
if i'm not mistaken lists desugar to normal do-notation: res :: Int res = length $ [(x,y,z) | x &lt;- [1..1000], y &lt;- [1..1000], z &lt;- [1..1000]] is equivalent to res :: Int res = length $ do x &lt;- [1..1000] y &lt;- [1..1000] z &lt;- [1..1000] return (x,y,z) what could be better is to use Applicative: res :: Int res = length $ (,,) &lt;$&gt; [1..1000] &lt;*&gt; [1..1000] &lt;*&gt; [1..1000] which could be optimized better, but i don't know if thats really the case. Maybe you could investigate.
One thing I like about python is that objects are (to simplify things) basically just dictionaries mapping names to attributes, along with some syntactic sugar to make method calls pretty (namely descriptors). Inheritance is mostly merging dictionaries. While putting types on this would be gross, the simplicity of the model is awesome and allows for great flexibility. There's some extra complication for a few things, but the main idea is that objects are just dictionaries of attributes, some of which might be functions. It's always struck me as a particularly functional approach to OOP. I actually toyed around once with HList to see if I could get something similar working in Haskell, and indeed you can, just with a lot of extra boilerplate compared to python. Some template Haskell or type magic beyond my capabilities might have been able to reduce that substantially though. 
it .. it ... it! .. I'm still here! :p
Yes, I agree that working with the Core is the best way to see what's going on. When I was working on optimizing `vinyl`'s field accessors (i.e. indexing into a HList), I found Core inspection, combined with benchmarks, invaluable. For what it's worth, my conclusion as of GHC 7.6 was [that implementing the accessors using essentially your typeclass approach was near-optimal](https://github.com/VinylRecords/Vinyl/pull/28#issuecomment-31378769). It still has linear asymptotics at runtime because it manipulates real runtime values, so there's no way to avoid deconstructing and constructing the nested constructors at runtime. It looks like that's what's going on in your situation: you're passing around a runtime `EffectStack` that needs to be unpacked at runtime. That's unfortunate in the common case where the implementation is static. Ideally, GHC should be able to eliminate the `EffectStack` value, but it might require almost full inlining to do so. `getEffect ioInterpreter :: Suspend IO` should optimize down to just `suspendIO`---no runtime cost from the HList. (I haven't tested that.) At what point does that stop being the case?
If you get a chance, please do post the issues [here on github](https://github.com/ucsd-progsys/liquidhaskell/issues) -- it would be _greatly_ appreciated! Thanks!
Just to poke at a few of these excellent points... &gt; 40 years after the initial publications on dependent type theory equality is still an absolute disaster. I am probably naive about this, but it seems like this could be one of the good things that univalence program could net us: a singular story about how equality ought to behave. There's surely always going to be the hassle of managing both judgmental and propositional equalities. But things would certainly be much nicer if we had a practical solution for the issue of extensionality. &gt; Definitional equality is antimodular in that it exposes implementation details of functions to clients I feel like this is really a symptom of our inexperience. Mathematicians don't worry about their proofs being antimodular. Perhaps there should be more guards against when definitions are expanded cross-module, and perhaps we should be writing our programs with a higher level of abstraction (working over semirings and rings, etc). &gt; Definitional equality is antimodular in that it exposes implementation details of functions to clients This is hard to overstate. Even though I feel like I have a fairly solid understanding of the theory behind dependent types, because I have never had my own hands soaked in blood from implementing my own unification algorithm, I am never able to predict what has caused my error. I think there would be some small changes to these languages, though, that would lessen the problem at least a little bit. Idris has a built-in function `the` (the identity function with explicit type argument) which allows you to annotate terms with types. However, I would seriously prefer annotation be part of the syntax of the language, potentially netting benefits with the unification algorithm aware of them. Annotations could also be placed on their own lines, acting as documentation, since often in these languages, the type is more important than the term. 
Working on a GUI is considerably more difficult than @absense3 implies. Just a different kind of difficult than creating new high-level type-based abstractions (and with far less kudos attached than the racy type-level stuff). * You have to care deeply about the intricacies of the intersection between Haskell memory management and C++ object lifetimes. * You have to keep details in your head about the memory layout of C++ classes in all of the different OS / compiler combinations you support. * You have to understand that Haskell runtime in terms of threading, library (DLL) management and more. * You need to think carefully about how to manage (C++) static constructors. * You have to be very comfortable in debugging native code with race conditions and the like. * You still need to be a sufficiently good Haskell developer to create mid-level bindings. There aren't many people who can do that, and still fewer who want to. Regarding paid development - my experience with the wxHaskell community was that it is nowhere near large enough to support even a single full-time developer. I suspect [citation needed] that if you combined all of the users of all of the GUI bindings on Haskell, that would still be true. Thought experiment: a good enough (for Haskell GUI binding development) developer in a developed world location probably expects to earn comfortably more than $150k (US). Suppose (and I doubt this) there are 1000 Haskellers prepared to pay for maintained GUI bindings, it is going to cost not less than $150 per user per year - to get a single developer. Bear in mind that a single developer would not have much bandwidth for support to specific incidents, and people who pay usually expect support. I recently saw the uproar over the new licensing regime for Jetbrains products at about $100 per year. This for an outstanding developer tool in a high-volume language niche, for a tool which would save users time every day that they use their computer.
The reason there aren't any Qt bindings is probably that C++ APIs tend to use lots of language features that are not available via C FFIs and that are incompatible with most other language's semantics (even there OOP ones).
Common lisp conditions are better for this
throw? throwIO? throwError (the MonadError or MonadExcept one?)? throwE? throwM? Just look at all of this: https://hackage.haskell.org/package/transformers-0.5.0.0/docs/Control-Monad-Trans-Except.html https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Error.html https://hackage.haskell.org/package/exceptions-0.8.0.2/docs/Control-Monad-Catch.html https://hackage.haskell.org/package/mtl-2.2.1/docs/Control-Monad-Except.html https://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Exception.html Poor beginners ;-) Q: Is ever any point in using the monadic exception typeclasses and support functions from mtl/tranformers (Control.Monad.Trans.Except.throwE, Control.Monad.Except.throwError etc.) , or should MonadCatch/Throw and Control.Exception directly cover everything? My takeaway here would be to always use extensible exceptions, always use IO exceptions (when available in the stack), only use EitherT/ExceptT in pure code, be generic with MonadCatch/Throw whenever possible, don't bother with MonadError etc.
Hmm, perhaps we need to add some `FunctionalDependencies` on `HasEffect`? I'll have to play around. Thanks for finding the example. Providing libraries for effects would be ideal (like key-value stores), so getting this to work would be great.
&gt; I feel like this is really a symptom of our inexperience. No, I don't think this is true, at least not for the definitional equality case. The antimodularity here arises because one particular form of equality is baked into the typing rules, and therefore becomes special, with the consequence that irrelevant implementation details are leaked all over the place. In other reasoning systems, you don't get *this* particular form of antimodularity to the same extent. For example, in HOL based systems such as Isabelle and HOL4 there is no concept of definitional equality. Rather, there's just a single boolean-valued equality constant that covers all the use-cases of definitional equality, propositional equality, bisimulation, equivalence relations, etc. of a typical implementation of intensional dependent type theory. In HOL, we then have `0 + x = x` and `x + 0 = x` no matter which argument we recurse on when defining addition, the only difference being one equality is automatically generated by the function definition system and automatically added to the ambient simpset, whereas the other one must be proved by hand and added to the simpset manually.
"Something in common" between `Free f a` and `f a` is given by a [retraction](https://hackage.haskell.org/package/free-4.12.1/docs/Control-Monad-Free.html#v:retract). Are you familiar with the way effect systems based on free monads work? The title post seems to explain it (although I haven't read it carefully); you can also read [Oleg's papers](http://okmij.org/ftp/Haskell/extensible/).
Your takeaway is the correct conclusion of the article, spot on. To help out a bit more: * `throwM` is a generalized form of `throwIO`, so if you want to simplify down, it's "always use `throwM` when the context allows." * The context won't allow for the EitherT/ExceptT in pure code case, in which case the specific error throwers (`throwE`/`throwError`) are necessary. The fact that there are so many variants of this is an accident of history, a better world IMO would be having only `EitherT` and `ExceptT`, `ErrorT` and `CatchT` not existing * `throw` is a partial function, and I wish the name made that more explicit. I'd be happier if it was called something like `unsafeThrow` or `throwEx`, and that `throw` was instead the name for `throwIO` or `throwM`. (It would then be a nice parallel to `catch`.)
Some more issues that I forgot to mention in my first post: * Repetition everywhere! A vector is a list indexed by its length. As a result, it shares many similar operations (e.g. map, filter, append, and so on) with a plain list datatype yet these operations all need to be reimplemented despite nothing much changing other than the types. For every different list-like indexed type, you also need to reimplement these same operations, and same proofs, over and over again. DRY is thrown out of the window. * Java's architecture astronauts are well known. Dependently typed languages enable something much worse: type astronauts. A function cannot merely be written that works on some concrete type. It needs to be abstracted and abstracted until it operates on elements of some obscure algebraic structure mentioned once, in passing, in a PhD thesis in 1983. Further, datatypes become decorated with invariants to such an extent it becomes impossible to work out what they actually do. KISS, and YAGNI, are both thrown out of the window.
[Issue #111][1] records the current state of affairs. The actual connection not very difficult. Essentially, I wrote a small Electron app that starts a Haskell server and connects to it via a slightly modified `index.html` file. The point where I'm stuck at is that I have no idea how to integrate this with cabal. If a user installs `threepenny-gui` from hackage, I want him to be able to list this package as a dependency and automatically wrap his Haskell executable in an appropriate Electron shell, perhaps similar to how `cabal-macosx` creates Mac OS app bundles. [1]: https://github.com/HeinrichApfelmus/threepenny-gui/issues/111
That's great. When was this added?
It was added in [version 2.4.2.4](https://github.com/agda/agda/blob/master/CHANGELOG#L198). And there are still some issues with the implementation and the underlying theory: &gt; Using rewriting, you are entering into the wilderness, where you are on your own!
Addressing above points + the points in your added comment * We can happily program in a Haskell fragment in a dependent language. Type inference, program speed, compilation speed, error messages - no fundamental reasons for anything to be worse than in Haskell; it just depends on the amount of effort from compiler developers. Idris' Haskell fragment is worse than GHC, but for example Lean's type checker is vastly faster than GHC's type checker (despite being much more sophisticated). * There's many more ways to do things than in Haskell, because the power of expression we have dwarfs that of Haskell, and there are uncharted lands and possibilities everywhere and there isn't nearly enough accumulated experience to establish best practices. This isn't a bad thing. If we give a sizeable community of software developers great power of expression, they won't spontaneously fragment into a thousand alternative solutions that can't interoperate. Instead, they would have commonly used practical libraries with limited power, and gradually feel their way into more and more powerful best practices. I'm confident in this scenario because this is what has happened in Haskell. On the other hand, if we don't give power to the people, they'll be forever treading the same water. * The notion that proofs are "antimodular" is just wrong, not in the sense that proofs are "modular" instead but that the concept is divorced from the very nature of proofs (to be poetic). Formal proofs are fully precise statements about properties of objects, and if proofs depend on some details of objects, then we can't change those details without breaking the proofs. We can't expect proofs about cats to be true of dogs. Instead, what we can do is to write proofs that don't depend on superfluous and unnecessary details and are as general as possible. This is not tied to any formal system or programming language, rather it's a universal phenomenon in all of mathematics. * Equality in dependent type theories has seen massive advances in recent years with HoTT. Even without HoTT, I can't label equality in type theory as a "disaster". Equality is difficult. We can forget that it's difficult when we're working in a weak system like Isabelle/HOL, but I would rather have a strong system and rich equality. If intensional equality seems cumbersome (and one's not yet willing to go HoTT), one can switch over to extensional PRL-s. * "Repetition everywhere" is not repetition. Vectors and lists aren't the same; even if their functions and proofs are similar, it's often difficult to find general abstractions that encompass apparently similar things. There has been interesting research addressing the "lists vs vectors" repetition, for example /u/pigworker's work on ornaments, or "Type Theory in Color". This research hasn't yet produced anything that ordinary programmers would happily use in their code, but that speaks more about the fundamental difficulty of the problem (again, it's a general problem of formalization) than any limitation specific to dependent types. * Precise specification and verification is just one facet of dependent types' usefulness. Another one is boilerplate reduction and safe code generation. With the introduction of `servant`, Haskell developers recently started to appreciate such things. Fragility and tedium are negative aspects of verification and proof writing, but they don't apply to (dependent) generic programming at all. 
Yes, where f is a monad, you can collapse `Free f` into `f` with `retract` - same as if w is a monoid you can collapse `[w]` into `w` with `mconcat`. If that makes it reasonable to "benchmark mtl against free" by testing `State s` against `Free (State s)`, then it is reasonable to benchmark `State s` by comparing it to `Free (Free (State s))` or `Free (Free (Free (State s)))`. The benchmarks expressly state that they are not about extensible effects systems. They seem rather to be bent on spreading the meme that 'free monads are slow', if I follow the fourth paragraph of the readme. But one only uses `Free m` in place of `m` for a reason. If mtl/transformers had a type and class representing the `yield` statement, there would be no difference at all. But transformers doesn't contain any recursive types. The distinction would be more obvious if you were using `FreeT`; then there would be a choice whether to associate the 'computation' with the monad parameter, using `lift`, or to put it in the functor parameter and call `retract` later. If I bench `FreeT (State s) Identity` it is the same as `Free`, but `FreeT Identity (State s)` is of course the same as `mtl`: time 113.2 ns (112.3 ns .. 114.1 ns) vs mtl time 98.15 ns (97.69 ns .. 98.58 ns) unsurprisingly since `computation n` and `lift (mtlComputation n)` are completely different computations. 
I'd like to slightly hijack this excellent discussion to ask a related question that I've had in the back of my mind for a while. Never having used any serious dependently typed language, but having read posts about Agda, Idris, etc., I find myself thinking that we might be in for a future where we get a lot of help _writing_ dependently typed programs (a la Agda), but not a lot of help _reading_ them. The example that sticks in my mind isn't fair - it was a dependently-typed fizzbuzz from last year, which the author made clear was a bit of a joke. Many people pointed out that large swathes of the code would have been helpfully generated by the compiler/editor for you. However, that means there were like a whole two paragraphs of dense code with a comment at the top to the effect of `this proves n is a multiple of 3`. No matter how much of that the programmer had to write, the next poor sap has to come along and read it. Is this a practical concern outside over-engineered toy examples, and is there work being done on it? EDIT: I went back and found the [original gist](https://gist.github.com/david-christiansen/3660d5d45e9287c25a5e) and the [reddit discussion](https://www.reddit.com/r/programming/comments/2zor5c/dependently_typed_fizzbuzz/) to go with. The part I was misremembering is [this](https://gist.github.com/david-christiansen/3660d5d45e9287c25a5e#file-fizzbuzzc-idr-L31) line which implements the 'decidability' of the fizz property, explained by /u/syntax as: &gt; The meat is in decFizzy - this checks to see if the type can be instantiate for the given Nat, and a proof that of that decision. The first few cases are clear: 0 does, 1 and 2 don't; and they supply the lemma to show that they don't. &gt; &gt; The final part is the recursion - if it's not 0, 1 or 2, then take 3 off the number, and the result is the same as for that smaller number.
Well, I suppose I'm one of these people, I'm worked on Java for years and millions of line of code, and the checked exceptions isn't high on my list of peeves. Sometimes you can recover from the exceptions, sometimes you log them and show a nice message to the user, sometimes you swallow them because you don't care, but at least you have to decide what to do. Isn't the suggestion to use MonadThrow in Haskell the same as declaring "throws Exception" in Java? "My code can throw any exception, handle it the way you want but handle it somehow"? 
Point taken about quotients. They are indeed terrible in ITT (but not in OTT and HoTT which are "vaporware" but undoubtedly we'll get production-strength implementation eventually). In my mind, the software development vs proofs distinction that is relevant to programming with dependent types is the following: * With proofs, you often have a job that needs to be done for one reason or another, for example proving semantic properties of microprocessor models. Here, you have to get yourself dirty, and if quotient sets suck in your language, there's no way around it. * Software developers try to push out working code with minimum amount of hassle. They're happy when they get new abstractions that enables them to write working code more quickly with more assurances, but they would avoid proof writing as much as possible. Dependent types give library writers much power to present safe and powerful API's that doesn't require much proof writing from library users. From this viewpoint, dependent types act as a refinement and continuation of trends in Haskell libraries. I was missing this from your stated points, which were focused on the ease of writing complex proofs.
The usual answer consists of things that aren't observable.
I don't know that much about dependant types, but one problem I could see is that you could easily end up duplicating the functions on the type-level if you are using types as a guarantee that the function does what it should. E.g. You give me type of (++) for your data structure (let's say vectors) and tell me to implement it. * Your type says that the result has to have the length of the first vector + the second vector. So I just write a function that replicates the first element of the first vector until it's as long as the result should be. * Your type now also says that the result has to contain elements from both vectors. So I do similar thing as above, except I alternate between the first element from the first vector and first element from the second vector. * Your type now also says that the result vector has to contain all elements from both vectors. So I do indeed concanate them, but then I reverse their order. * Your type now also says that the result vector has to contain the elements in the same sequence as they are found in the original vectors. So I finally write what I should have. At this point you made the type overly complicated for such a simple function and pretty much duplicated the function on type-level.
At this point, I think we're arguing semantics. In most other languages with exceptions (e.g., Python), any function can both have side-effects (perform I/O) and throw any exception it likes. In Java, you can always perform side-effects, and need to annotate exceptions (unless you derive from `Error`). In Haskell, you need to explicitly annotate a function for being able to do either of those (ignoring `unsafePerformIO` and exceptions from pure code). The reason "explicitly stating all exception types" is generally referred to as "checked exceptions" is due to this similarity to Java vs other programming languages. Typed exceptions seems like a fine name to use instead, but to my knowledge no one has been confused by nomenclature to date.
That's why I added several more comments ;)
Right, so we're back to it's difficult! Yep, there was certainly an uproar over JetBrains, and I don't remember the outcome. But it's not that people weren't paying for JetBrains, it was because JB wanted to move to a model where, if you stopped paying, the product stopped working. People hate that approach. However, if you're suggesting that the Haskell market is not large enough to be worth developing decent GUI tools, then that says something itself.
So I've tried it and it makes no difference. Interestingly, when compiling with -fno-full-lazyness as /u/AndrasKovacs mentioned, this version doesn't get compiled into a loop, while the original, monadic variant does.
Very interesting. It almost seems like there should be some tool that automatically tries various combinations of GHC options to find the ones that produce the optimal code.
Obviously, I could be wrong, but I think the real point is that doing cool things *requires* extensions, which then need to be explained to a new comer since they aren't part of the language proper. Declaring which extensions you want to use in the first place is easy enough, especially with stack cabal templates, but that they need to be enabled at all (at least to leverage some of the more interesting libraries such as lens, Servant, etc) is a bit disappointing.
FYI, concatMap is just bind (&gt;&gt;=) with the arguments flipped and a different constraint. In fact, there's (=&lt;&lt;) which has the arguments in the correct order.
How does it compare to `servant`? At first glance the reliance on type classes doesn't seem to buy anything and makes the code more verbose. Perhaps it gives nicer error messages / compile times?
Or some compiler's extreme optimization pass, which searches for the best optimization with brute-force. Either way, would be really useful for plenty of cases, where the compilation time of the "release" version doesn't matter as much as its performance. For instance, I wouldn't mind letting my project compile for hours in many cases.
Has anyone tried to use GPUs for brute-force optimization? My google results are all about optimizing code *for* GPUs. 
GPUs are great for doing the same operation many times. But they are terrible for code that has many branches. So I don't think running an optimizer on the GPU would be easy.
* We initially started off webapi as a backend-framework agnostic approach to describe web apis and derive other useful functionality out of it (like client sdks, api-console and so on..) * It was designed from the beginning to hook into other backend frameworks (as we had some of the apis running using yesod). So for the above two reasons we took the typeclass based approach. We added our own routing mechanism (one inspired from servant paper) much later. Main difference between Servant &amp; webapi lies in how the contract is defined: In Servant, all aspect of the api contract are described using a type-level list-like construct `((:&gt;), ..) ` In WebApi, routing info is kept separate from other aspect of api and only when a route is successfully matched, we will look into the other aspect of api contract (like deserializing the param into configured type and so on.)
The `master` version of `servant` also only looks into deserializing etc. only after the route matches, though you're right that the contract itself includes both together.
&gt; Is this a practical concern outside over-engineered toy examples, and is there work being done on it? I don't think that this is an issue. The intent with using dependant types is that it allows proofs of behaviour to be encoded into the type system. Edwin's mentioned before that he sees this as ways to give guarantees for library code. Precisely how these are implemented is _not_ needed for users to the library - so the only people who would need to fiddle about with it would be expected to be familiar with dependantly typed construction. So, at least within the vision of use in Idris, it is 'ok' to leave the reading of dependantly typed code to an expert - as that is not (normally) envisioned to be part of the main business of 'general purpose' programming. That aside, there is a a bit of a hill to climb to learn to read these - but I'm not sure if there's much that can be done about that. I would draw an analogy with static typing in Haskell: If you had never used a staticly typed language, then all the type signatures in Haskell would be noise that got in the way. There's not really a lot one can do to make it 'easier', without learning what they mean, and why they matter. Only then can one use and alter them to meaningful effect. All that said. I do think that a primer on how to _read_ dependantly typed code would be a good thing. Not because I foresee people _needing_ to read it a lot; but rather as a way to get familiar with how to _write_ them. Hrm … perhaps that's the sort of thing that I'm actually not badly placed to pull together, all things considered… although I'm not sure exactly when I could do that… 
&gt; Explicitly saying you don't care about anyone's opinions and are just trying to hurt the feelings of people you disagree with This one? &gt; I don't think that I'm trolling. I'm just being descriptive IMO. And being sincere and exposing the opinions bluntly over the table is an unavoidable prerequisite for fixing things. &gt; &gt; even if there is a risk of being too generalizing and sometimes unjust with particular cases is not an issue because the haskellers who do not match my descriptions will not be concerned. On the contrary; I think that they will agree with me partially at least. I don't think this is "saying you don't care about anyone's opinions and are just trying to hurt the feelings of people you disagree with", but: "sometimes the consequence of saying what you think is true aloud is hurting some people, but sometimes telling the truth is necessary". IMO there's a big difference between the two.
This looks really interesting. Looking forward to reading it. One question: What's the part at the start with the explanation of monads for? A person that doesn't understand monads surely will not understand them after reading that intro. Not to mention that State is very barely explained and then immediately used as StateT with no mention of transformers. I've seen this in a number of posts from various people. A sentence or two describing some concept that's pointless both for people familiar with the concept (since they already know) and for those who are not (because it's not enough).
This sounds like a question that could be answered by every Haskell blog/book ever :-) The most interesting characteristic about Haskell for me is that I know where to look for unexplicable runtime bugs first (those marked with effects). It's easier to reason about the behaviour of your program. Also the problem with just "teaching programmers to not do idiotic things" is that there is not one teacher that teaches all the programmers in the world. People learn differently. Furthermore people always pick the easiest way. So ... modifying global state seemed like nice quick hack when you were at function #1. But then you do it again and again. And at some point you can't return anymore.
&gt; but you could also just teach programmers to not do idiotic things 'Good luck.'
Oh awesome! Thanks for the reply.
I can't even teach myself not to do idiotic things when I'm working in a "mutable" language, and I'm aware of the downsides. Good luck teaching everyone you're working with.
Yes, definitely refinement types seem to hit the sweet spot between expressive power and usability. I am very much looking forward to refinement types maturing to the point they will become a standard tool in any Haskeller's toolbox. The [recent post](http://www.haskellforall.com/2015/12/compile-time-memory-safety-using-liquid.html) is a great intro and experience report. 
Can I ask what you mean by "fragile"? Is it that the program will die when an error is encountered? Got to agree with the others... the main function handles some impure things *that must happen* in order to start the web service. It can't run without loading config. It can't run without starting your database connection pool. Et cetera. If anything in main fails, the only thing you really want to do is immediately stop and explode, hopefully with a nice log/error message. What would less fragile code do here besides log the fact that an error happened and exit? Unless your main function explodes in size, it's probably not worth worrying about.
Yes, I think we are in violent agreement and it really is a shame what happened to Delphi and yes, IB isn't nearly as good. I agree completely about the problem selling tools (done that myself in the distant past) but nevertheless the fact remains that Haskell didn't have the tools that other environments have had. Maybe the Haskell community needs to persuade Microsoft to invest more in Haskell beyond supporting Simon! I was unaware that FPComplete had an IDE (which perhaps says something about their marketing). I'll take a look and if it is any good, I'd buy it if the price is any way reasonable. I think it's worth supporting developers
There's an impedance mismatch for sure, but Python and C# have Qt bindings.
I hope I am not violating any rule by posting a link to my own blog post. However, since I still consider myself relatively inexperienced in Haskell I was hoping for some feedback.
Thank goodness! ...and thanks for the update. :) 
Relevant: https://hackage.haskell.org/package/resourcet
There were Haskell bindings at one point too I believe. It is just a pain to maintain them and they have a comparatively bad usability and might be limited in features, even compared to low level C bindings.
Hi, I'm ghc-mod's maintainer (@DanielG on Github) here if you have any problems.
Perhaps you should consider the "dual" question: What is the *cost* of *immutability*? When variables may denote different values at different points during execution, you end up having to consider not a variable's value, but it's value at a particular instant in time. It's the difference between knowing the first commercially successful personal computer and knowing who makes the fastest computer today. The former is set in stone for all time: the Apple II. You can teach it, and it will be true forever. But the latter requires you be "up to speed" on the latest developments in business and technology. As long as you're willing to put in the effort (tracking global state, or keeping up with r/technology), you will be mostly right, most of the time. And that's good enough for most people. But when values (like historical facts) are fixed and unchanging, it requires much less work to reason about. A function's behavior will only depend on its inputs and not on any unseen factors intertwined with whatever the machine has been doing since it was turned on. 
Then you may be interested in the `unexceptionalio` package. That provides a `fromIO` function that forces you to check all exceptions in a style similar to Go (except that it returns an `Either` instead of a tuple).
I hope there's no rule against doing that. I've linked to my own blog and packages before. It's a great way to get good feedback or criticism.
But this kind of explanation is not pointless for people like me, who are just getting what are monads, and are eager of all kind of summary to fix that light bulb in their head :)
This guy articles are always awesome, i recommand the "write your own haskell compiler" series.
Check out hledger for ideas. It's a command-line double-entry bookkeeping utility written in Haskell.
&gt; * Fix #665, Reinstate internally managed CWD (no more `ghc-mod root` requirement for frontends) Yay! I'll try this as soon as I get home.
I understand having many balls in the air! I did see that the PR had been merged, so I had assumed it was just a matter of waiting for a release.
This assumes that we can have accounts with the same number. I think a better approach might be: data Account = Account {accountName :: Accountname ,accountBalance :: Integer} type Accounts = Map Accountnumber Account That would guarantee that all accounts have distinct numbers --- though it would still allow accounts with the same name. Map is from Data.Map in containers. If Accountnumber is an Int, then you can use Data.IntMap, which would be much faster. You can also change the account amount very easily; using lens or lens-simple would be the tersest solution.
It's nice and full-circle--assembly up to Haskell simulating assembly. His Kaleidoscope rewrite in Haskell from OCaml is great too.
You can't qualify them, but you can rename the them. It's not elegant, but it's something. However in general this is something that should probably be solved by tooling rather than complicating the language.
Immutability greatly improves modularity and reasoning ability. Not just for the human— but *for the computer!* (Reasoning is reasoning, no matter who's doing it.) Knowing that things cannot change behind its back means that the compiler is free to do things like read core memory once and cache the results for multiple accesses. Immutability also limits the possible graphs of pointers, which makes it easier to do garbage collection, alias analysis, etc. Immutability is so great for compilers that one of the early steps in most state-of-the-art compilers is to convert the source code into an immutable language (e.g., SSA)! Of course, when considering all this, do bear in mind that immutability in Haskell is only a façade: it's immutable enough for us humans to reason about, but under the covers all that laziness means mutation. But, by having that façade, the compiler needs to do less work trying to reason out the immutable parts in order to convert things to SSA etc. And, of course, the immutability of SSA is just a façade: physics means memory erases behind our backs, so whenever we read we also write. But again, the idea of immutability makes it easier to minimize complexity. Sure, it's lies all the way down, but the point isn't about some reductionist notion of "truth"; the point is that these lies are all true locally, and that abstraction gives us enough leverage to build up the next layer. &gt; "of course you can make a language idiot proof, but you could also just teach programmers to not do idiotic things, such as modifying global state". Like how we 'just teach' speakers to not mess up their grammar? Prescriptivism never presents a significant roadblock to people using language to get the job done. At worst, "you know what I mean"; at best, we change the language to suit our needs. The only way to prevent someone from saying something is to make it inutterable.
&gt; probably be solved by tooling How ? I've just com across the [QualifiedModuleExport proposal](https://wiki.haskell.org/GHC/QualifiedModuleExport), it doesn't really complicate the language, and would allow things like import Stack f = Data.Maybe.catMaybes [Nothing, Just 1, Nothing] I haven't thought about it much, so It might be a bad idea.
It's not a big issue IMO, but that does still add a layer of misdirection (looking at the module imports tells you less about where things are coming from) and also results in unnecessary imports for many modules, unless their needs are all exactly the same. Lastly, you still have to build your custom prelude manually and there's no help for Language pragmas. A tooling solution has none of these issues, and does not require the approval of a proposal on top of that. But still, hardly *major* concerns.
That makes sense. I'll be fascinated to see whether that approach pans out. My experience so far has taught me that implementation details have a nasty habit of leaking out, especially where types and compilers are concerned. Of course, my concern wasn't just for beginners reading the resulting code - even an expert may get swamped, and though they'll likely succeed eventually, it may have cost them a day or two more than they'd have liked! &gt; If you had never used a staticly typed language, then all the type signatures in Haskell would be noise that got in the way. Point taken - there's no such thing as a free lunch!
Sure. `stack.yaml` isn't actually in the source distribution though so there's no rush ;)
&gt; The notion that proofs are "antimodular" is just wrong, not in the sense that proofs are "modular" instead but that the concept is divorced from the very nature of proofs (to be poetic). The original idea of proofs as a paper trail for *arriving* at truth/understanding, sure. The constructive notion of proofs as a mechanism of communicating evidence to *demonstrate* truth, not so much. Once we begin to think of proofs as mathematical objects in and of themselves, we must immediately become concerned with all the things we wonder about other mathematical objects— for instance, when they should be considered "the same". Under the classical regime where the statement of truths/theorems is all that matters, the differences between various methods of arriving at that truth are mere curiosities; but under the constructivist regime, it is a central question whether proofs are principal or not. The whole HoTT enterprise is predicated on the fundamental assumption that proofs are not (in principle) principal. Indeed, to say that a proof is not-antimodular is to say that the particular proof in question is irrelevant beyond its demonstration that the theorem is inhabited, which is in turn to make a very specific assumption about connectedness between all possible proofs of the theorem.
&gt; extension allowing to alias group of import into one Isn't this just re-exports? You make a module `Alias` that re-exports `Group.One`, `Group.Two` and `Group.Three`. Then you can import `Alias` instead of the three other modules.
When working in Java you just need to embrace it. I hated OOP until someone untaught me everything and showed me how it's supposed to be done. The good bits(tm) are actually broadly the same as haskell (objects are closures, object construction is partial application, try to keep as much stuff pure as possible etc etc). 
Without immutable state there is nothing! Analogously, a hammer should be able to nail nails regardless of the weather or the kettle boiling earlier. Mutable state makes the order of excecution important - the return value of a function (using mutable state) cannot be predicted without resolving the whole excecution graph. With immutable state a function can be fully understood by its code. Mutable state requires the whole program to be excecuted to reason about the function that uses it - which is a bit of a paradox. The paradox means mutable state makes programs tend toward being impossible to reason about - to know what a program will actually do - will it perform as expected. Each mutable function added multiplies the complexity by its relationships - some of which may be distant, intertwined or different every time. Mutable state also breaks currying and memoisation.
Can you fill in (if necessary with bogus syntax) what comes after the where? Is it something like `fmap :: cat1 a b -&gt; cat2 (f a) (f b)`? Or `fmap :: cat1 (cat1 a b) (cat2 (f a) (f b))`? There are three appearances of `-&gt;` in the current type of `fmap` and I'm not up to speed enough to be clear on which ones would be generalized.
&gt; A vector is a list indexed by its length. As a result, it shares many similar operations (e.g. map, filter, append, and so on) with a plain list datatype yet these operations all need to be reimplemented despite nothing much changing other than the types. Would [ornaments](https://personal.cis.strath.ac.uk/conor.mcbride/pub/OAAO/Ornament.pdf) help? By making the link between lists and vectors precise, they probably allow operations like filter and append to be defined generically as well.
&gt;When working in Java you just need to embrace it. Haha. Agreed. When you're a hostage, just do what they say, and live to fight another day. &gt;...showed me how it's supposed to be done. I've tried to see how it's supposed to be done many times, but it's just a broken abstraction for me. If I want to turn off a light, I flip the switch to off. In OOP, I'm supposed to create a Light class to hold the state of everything related to the light, then accessor methods with access control levels set up just so to protect me from the world, in case anyone wants to make something based on my whole lighting setup. Then I need to create nouns to shepherd my verbs around, like LightSwitchToggleAccessor, and worry about interfaces and implementations and design patterns. In Haskell I'd say "A light can just be on or off; let's make it an alias for a boolean." type Light = Bool I want to be able to turn it on and off; that's just a morphism from Light state to Light state. toggleLight :: Light -&gt; Light toggleLight = not And that's it. If I realize later that I don't want Light and Bool to be interchangeable, I'd just make Light it's own type with a simple tweak to give it its own two states: data Light = Lit | Unlit And change the toggle to match: toggleLight :: Light -&gt; Light toggleLight Lit = Unlit toggleLight Unlit = Lit Then I could toggle a big list of lights: map toggleLight [light1, light2, mainLight, ...] Or turn them all on: map (const Lit) [light1, light2, ...] I have equational reasoning. I can do like-for-like transformations. I get all the goodness of category theoretic abstractions, giving me reusability the likes of which I've never seen in OOP (not even close). Etc. &gt;objects are closures Closures are immutable (hence the glory of [this](http://www.amazon.com/Purely-Functional-Structures-Chris-Okasaki/dp/0521663504)). Objects tend to be mutable, which is a nightmare (every day where I work in C#). &gt;try to keep as much stuff pure as possible But you just have no way of knowing what's pure and what isn't in any of the OOP environments I've seen, and it is so obvious in C# at work; it plagues us constantly - new bugs daily, and projects always slow tremendously as they grow, and things become unchangeable, because they're too ossified. Just that small thing, that need to specify effects in your types, makes it so much easier to reason about what actually goes on in a function. For example, my Lights up there actually can't do anything in the world. I know that because of their "Light -&gt; Light" types. All they can do is tweak data, the same way every single time they're called - you can replace them with table lookups. They'd *have* to get some kind of IO markup in their types before they could change anything, which is part of that equational, deterministic reasoning that makes FP so easy to understand, even as projects grow. I don't want to try to do things. I want it to be fun to do what's good, and impossible to do what's bad. The goal of a great type system is to "make illegal states impossible to represent." I made it impossible to mess with the world, and so I can know with 100% certainty what toggleLights does. I quite literally cannot know what the same function would do in C#. It could return a different result every time. Multiply that up to a few 100klocs, and I have no idea how our projects work, and no idea what I'm breaking when I push commits (and I often break things, and everyone else constantly breaks my stuff, because we can't properly reason about anything).
1.) Because you need fancier type features. Even then you run into problems with the `Category` class being too restrictive to model all the categories that you'd want. 2.) Because of type inference. Without the `Id` tag there there is nothing that helps you pick the instance. e.g. What type does `fmap reverse` have? `[[b]] -&gt; [[b]]` or `[a] -&gt; [a]` ? If you think about it that latter could be instantiated with a = [b] giving you the exact same type as the former, but one would reverse the inner lists, while the fmap for true identities you want would reverse the outer lists. The issue is that the `Functor` class isn't all functors, but rather only applicable for a certain kind of endofunctor on Hask, ones that are injective and tagged with a type constructor in this way. Sjoerd Visscher's `data-category` can model categories and functors with greater fidelity: https://hackage.haskell.org/package/data-category My `hask` package tries for a middle ground (but uses some hacks that no longer work.): https://github.com/ekmett/hask
I believe there is an extension planned for 8.2.1 called `-XTypeSignatureSections` to make `::` acts like a section.
What does an easy to use Monad Transformer example with exception handling look like these days? Does it use [exceptions](http://hackage.haskell.org/package/exceptions) or something different? Now, what does adding logging to that monad transformer stack look like? Now I find out [WriterT leaks](http://www.haskellforall.com/2014/02/streaming-logging.html), what does adding on streaming logging look like? If they aren't already, what does adding resource handling look like to our example until now? What other surprises might I run into? What are my options if performance isn't fast enough in my monad transformer stack? Is there a better way to write this without a monad transformer stack? Is the monad transformer stack good enough?
Is this release based on haskell-ide-engine? Or is it still old codebase? 
We should not forget that this also applies to the tool makers: Developers of compilers, verification systems, etc. also benefit from the simplified strata. To add a couple examples to the insights about fewer moving parts: One consequence of the simplified logistics is that immutability can yield more efficient machine operation with far less work. For example, LLVM's adoption of static single assignment in the intermediate assembler language makes many traditionally complex optimizations trivial by localizing effects. (It is interesting and fortuitous that SSA is just a specialization of continuation passing style.) The No True Scotsman fallacy is a natural form of jadedness amongst practitioners, but this isn't usually too hard to rebuke: For instance, there is likely a majority who silently agree that in an ideal world C++ should default variable binding to const (I am talking just of variable binding-- not of the other ways in which the same keyword is used). This default would be more in line with the best-practice ethos amongst C++ programmers as it has immediately obvious benefits for practitioners. (The 'true scotsmen' themselves?) 
The middle ground use by `hask` is pretty usable in many ways: class (Category (Dom f), Category (Cod f)) =&gt; Functor (f :: i -&gt; j) where type Dom f :: i -&gt; i -&gt; * type Cod f :: j -&gt; j -&gt; * fmap :: Dom f a b -&gt; Cod f (f a) (f b) That one can handle many things, e.g. `instance Functor Either` without the extra argument, but it still has the injectivity problem, and cant properly handle, for instance, most functors that end in a product category or sum category correctly. This requires a better `Category` than the one in `base`, however. GHC 8 adds some things that I need to make the `hask` approach work well though, as I currently have to use 2-3 levels of 'not quite category' and 'not quite functor' constraints to build up the actual constraints I want due to limitations that SPJ just lifted for me on cycles in superclass definitions. https://ghc.haskell.org/trac/ghc/ticket/10318 The `data-category` approach is probably the most correct, but also the least usable.
This! A powerful type system like Haskell (or better yet, Liquid Haskell) allows you to sanity check your code in powerful ways.
What's going to happen with "ambiguous type" errors? Surely, now we can disambiguate types easily, so could we get `AllowAmbiguousTypes` by default? 
Thanks for pointing me there. I was not aware of `ResourceT`.
Thanks for your feedback. I was actually not aware of either `ResourceT`, or `managed`. I tried to google for both kinds of solutions before writing this blog post but couldn't find anything. I hope that my post will help others to find these packages.
"Sorry, we could not display the entire diff because it was too big.".. Congrats on getting another Herculean task done!
Yes, definitely!
Here is the ticket: https://ghc.haskell.org/trac/ghc/ticket/8043. And a wiki page: https://wiki.haskell.org/GHC/QualifiedModuleExport. Nobody is working on it at the moment.
Why hasn't any open source game been ported to Haskell to show how it can be done (and that it can be done)? Something like Quake2 or Quake3 or Doom 3 or anything like that?
If all you want is a simple blog, [hakyll](https://jaspervdj.be/hakyll/) could work? Edit: Derp, you do mention hakyll. Too lazy to read the entire response before replying. I think your suspicion is correct.
I understand that import section can help reading code, but Haskell seems to be the only language to solve this problem ... Are you saying that the import system of Haskell is a feature then ? I've never seen anybody mentioning it whilst trying to sell Haskell. This problem can easily be solved by generated a tag file with code for example and putting it under version control. That way you can refer to it if needed.
The start of the article seems to be comparing apples and oranges. You write constructors and destructors in C++, but in Haskell you've written, well, non-magic functions. If you replace `~Resource ()` with `void releaseResource ()` and manually call that after throwing the exception you get the same output as in Haskell ([i.e.](http://npaste.de/p/7J7/)). Disclaimer: I don't actually know C++.
Typically the first thing I do is just look at the line and column number and take look there. Probably 50% of the build errors are just stupid mistakes which are immediately obvious if you look at it even if the error message might be confusing. For the rest, I think you just need a bit of experience to remember what kind of errors can cause what kind of error messages.
I just glanced through the text and it seems that there is not way to refer to a forward label. Maybe it could be possible to do it with an instance of MonadFix or the Tardis monad...?
How will this impact library functions that take types as arguments? Whereas before I might write: foo :: Proxy (tag :: k) -&gt; Int foo p = 10 `mod` someInternalFunc p How do I express that with "visible type application"?
You should have a look at the accompanying research paper, which starts with a well-motivated and very readable introduction. [Visible Type Application (Extended version)](http://www.seas.upenn.edu/~sweirich/papers/type-app-extended.pdf) Richard A. Eisenberg, Stephanie Weirich, and Hamidhasan G. Ahmed 2015 
Game written with FRP with low-level OpenGL graphics: https://wiki.haskell.org/Frag Quake3 map viewer written with higher level OpenGL: https://www.youtube.com/watch?v=JleoASegUlk source code: https://github.com/csabahruska/quake3 and https://github.com/csabahruska/gpipe-quake3 An MSc student is working on a shooter game with the Quake3 map viewer.
I solved it with a tiny program to automatically add and remove imports. It hasn't bothered me since.
There's too ways of missing things: sometimes I absolutely need something to solve a problem, and sometimes I just want something that's "nice to have". At this point, there's nothing of the former in Haskell, and I think it took me a few months, maybe half a year to get to that point. Working through my first project (an interpreter for a language I designed) got me to where I was comfortable doing all I cared to do in Haskell. But there are a lot of things that would be nice to have: * modules roughly like OCaml * records with row-polymorphism like OCaml should have had * *maybe* polymorphic variants (like OCaml) * first-class patterns (like bondi; note: I've never used them) * macros, for two reasons: * defining new syntax (TH is limited and the results are ugly) * interacting with identifiers (especially for variable names in DSLs) * easy interactive UIs like JavaScript and HTML * with interactive development And sometimes I long for the Scheme philosophy of a simple language that grows. Haskell is wonderful, but it is both substantially more complex and less flexible than it *could* be.
What I meant to compare was the RAII idiom in C++ with functions like `withFile` in Haskell. I tried to point that out by comparing `openFile`, `hClose` to C's `fopen`, `fclose`, and `withFile` to the RAII idiom. The reason why I start out with separate acquire-, release-functions is to properly motivate the need for with-functions. I felt that demonstrating a leak would be a good way to introduce `bracket`. I hope I did not give the impression that I was trying to pick on Haskell. On the contrary, I am quite amazed that something that is a language feature in C++ (automatic destructor call by the end of the scope) can be implemented in Haskell just by using the right monad.
This will be a far fetched and unhelpful answer, but I feel that the import problem happens because the language is too good at factoring. Basically, if people are encouraged to use many one-liner types and functions, then people will end up exporting them too, and everyone else will suffer. In my ideal programming style, you'd have big libraries with complicated functionality hidden behind small interfaces, and you'd only import a handful of them to get the job done. A lot of general purpose functionality (like strings, arrays, exceptions...) would be part of the language, and most libraries would accept and return built-in types instead of defining their own. Though I don't know if any existing language comes close to that ideal.
The outer arrow can be an arrow in whatever category both categories are enriched in. However, the way things are set up in Haskell today each such enriched base category would seem to wind up with a different whole class for implementing the Category construction, so I can't come up with a nice way to abstract over it today. In the very near future with kind classes that may be fixable, but I haven't worked it out. Consequently for now the definition would have to be the former. The latter is actually pretty weird, and in the future an `e (c a b) (d (f a) (f b))` is potentially possible but also problematic when it comes to talking about `id` and other inhabitants of the category in question, forcing some pretty weird gyrations to work with them, and lots of structure on `e`.
I remain rather terrified of this feature.
This sounds amazing! I'm not sure how to do that automation in cabal, but I would still love to see this on Hackage already. For now, how about providing a small shell script and/or batch file (or Haskell utility) that creates the wrapper? Then write a few words of documentation about how to use it, and link to it in the package comment. I have a feeling it might be easier to automate this using stack than cabal. But as a cabal user, the script would be fine with me.
Thanks.
While that would work, it remains a fair bit more verbose. It does have the benefit that unlike the type application syntax you don't have to know the order in which things will generalize / unify, though!
&gt; Quake3 map viewer written with higher level OpenGL: https://www.youtube.com/watch?v=JleoASegUlk Very impressive!!
I see. Thanks. The "vagaries of the type checking process" is more than just a problem with ease of use. When I need to specify a type I want readers of this code to understand clearly what the type is. So unless it's possible to understand the annotation without effort and without significant previous knowledge that I can't assume my readers will have, I would try hard to avoid needing this. Whereas type sections are crystal clear even to someone who has never heard of them before.
fix-imports on hackage. Only for qualified imports though. Lots of other versions have since showed up though, which are likely more sophisticated than mine and do unqualified. So there's probably no excuse for not using one of those by now.
Yeah, that's the one aspect of visible type application that makes me uncomfortable about it - suddenly having to know the order of quantified variables just doesn't feel very haskell-y to me. My gut says that verbosity in "partial type sections" wouldn't come up that much in practice, anyway. For instance, you could express that example as `(:: [_] -&gt; _) . fmap`, or even just `(:: [_]) . fmap f` if you're already passing `fmap` its function parameter *in situ*. (You could even write `(:: _ [_]) . fmap`, I guess... if you felt like going overboard with terseness over comprehensibility.)
That'd be this one: https://ghc.haskell.org/trac/ghc/ticket/10803
No, but you are defending the fact that the actual system is good because it allows you to know by reading the import section the source of a function. Haskell is the only language that I know (and is commonly used) which has this benefit. What I mean is, I'm not sure this _benefit_ is worth the extra cost. You seem to think to think the opposite and therefore that is one more reason to program in Haskell. Am I wrong ?
It enables you to think about an expression as having a value that doesn't depend on arbitrary temporal effects. So when you see `f x + f y` you can read that as a straightforward mathematical expression. You don't have to think that the first evaluation of `f x` might alter some hidden variable that makes `f y` evaluate differently. In math nobody writes functions with side effects. That's why we can be certain of even the most trivial equations like `sin x = sin x`. If you wrote a math paper using functions that had side effects, mathematicians would find that really weird and tedious because when they read the paper they have to mentally keep track state. And of course it would mess with the entire conception of a function. Suddenly the value of a function depends on evaluation order and stuff! It's crazy! You can't even do basic algebra without messing everything up! One of the premises of Haskell is that programming can often benefit from the clarity and formality of math. In particular, the idea is to treat a program as an algebraic composition of values, including higher order functional values. The impure ML languages are also inspired by that notion, but Haskell takes it more seriously. So I like to take purity as axiomatic for the whole Haskell experiment. Kind of like how in Smalltalk everything is an object, and if you ask "What's the benefit of that, really?" you're questioning the whole premise of the language—which of course is a good thing to do, but it's hard to give empirical proof and the benefits are not a clear cut bullet point list. Purity and "algebraic" or "denotational" programming give rise to a perspective on programs that's different from the imperative perspective. Put simply, you are encouraged to think less about the program's execution and more about its meaning, or less about "how" and more about "what." A simple example: // impure void setupWebServer() { this.server = new WebServer() this.server.addRoute("/", this.rootHandler) this.server.addRoute("/posts", this.postsHandler) this.server.run(80) } -- pure setupWebServer = runWebServer 80 (route "/" root &lt;&gt; route "/posts" posts) Here purity has encouraged us to define a web server as a composition of routes. We've made a simple "algebra of routes." You can ask whether the route monoid is commutative, and that's a nice use of mathematical concision applying to a concrete domain. Everything is clearly composable and nice from a denotational standpoint: there are no intermediate states, and since routes are values we can use them in interesting ways. Of course we could do all that in the impure language, but purity encourages it, and I can't prove that Haskell libraries tend to be more composable but it seems very likely. I know that when I write Haskell, I am naturally inclined to think about compositional semantics, and this practice also makes me a better programmer in other languages.
See the thread below started by /u/darkroom-- and carried on by /u/yitz. I mention the issues around it down there.
It is useful because you can easily lookup the source of a function in case you need to know what it does. It is not a particularly Haskell-specific benefit since at least 90% of the mainstream languages currently in use have the exact same feature with slightly different syntax.
The type sections only work for kind * since it's a shorthand for an identity function.
Could you give a quick overview of your workflow when fixing a erro message or warning? I guess the people who have the biggest problems with error messages and warnings also find ghc the most intimidating.
I disagree with point 1. My recommendation would be to read them - carefully! - as soon as you don't immediately see the problem pointed at. Having been trained by some other languages to skim error messages, I've had several experiences of looping many times through compile-&gt;error-&gt;edit until I sat down and actually read the messages. They are verbose, but that is because they are precise about what went wrong (they are not always precise about where the incorrect code is, though).
Thanks a lot ! I will try to look at (and understand) the ddump-simpl and will certainly give a shot with perf. I had in mind to use criterion, but for now my mistakes were big enough to be noticeable with the cost center report. I have to invest some time into it I guess... P.s: I will fix my mistakes when I have my PC at hand. 
I'd love to hear more about function calls from a GHC RTS expert. As far as I know, we have several different calling conventions, for under-saturated, over-saturated, saturated calls etc. and also for functions calls without unknown arity, e.g. you have `f x y` but can't figure statically what's the arity of `f` etc. Presumely, calling top-level functions vs. closures should be also different in some ways. These should have different performances. I should really study the RTS sometime...
In addition to what paf31 said, another idea would be to make use of the browser-environment to [render the output in a nice way](https://github.com/sharkdp/purescript-flarecheck/issues/5), for example: render complex ADT outputs as a tree where elements can be hidden. The canvas-graphics example is what really motivated this library. The idea would be to easily write interactive tests in the the style of http://sharkdp.github.io/purescript-isometric/.
I've already gotten good mileage using it *interactively*, using your own `hask` library as an example it is very liberating to quickly and mindlessly use `bimap @Either`, `bimap @(:-)`, `bimap @Rose` ... rather than trying to decipher the actual signature: bimap :: forall j i i1 (p :: i1 -&gt; i -&gt; j) (a :: i1) (b :: i1) (c :: i) (d :: i). Bifunctor p =&gt; Dom p a b -&gt; Dom2 p c d -&gt; Cod2 p (p a c) (p b d) It's certainly possible without the feature but you have to think and I want to keep *‘thinking’* to a minimal in the REPL. I imagine it being useful pedagogically, this type signature may be intimidating: length :: Foldable t =&gt; t a -&gt; Int but TypeApplications is a very terse way of getting the point across length @[] :: [a] -&gt; Int length @[] @Bool :: [Bool] -&gt; Int I'm wary of using it in real code though.
Hi everyone, One part of my Bachelor's Degree's thesis is evaluating the library I wrote (on Algebraic Effect Handlers). Evaluation is based on 2 criteria: 1. Benchmarking: Using Criterion, comparing with Monad Transformers &amp; Extensible Effects 2. User Friendliness: The ease of use (How easy it is to write or rewrite your programs) For the second point, I'm looking for a bit of help: I'm looking for small/medium sized Haskell applications that use Monad Transformers and are open-source. I will rewrite these programs using my library (which has yet to be made available on Hackage) and see how hard it is. Eventually, I might even compare the efficiency of the Monad Transformers version with my adaptation. Do you know any good projects/applications that fit in this category?
Awesome I always found proxies to be the opposite of elegant. 
Yes only. Three of the extra imports come from libraries in Haskell Platform, the others are third-party libraries. Even in python if I install a third party library I should be expected to import it. As for automating the process, did you try googling for a solution? I just ran a google check and found solutions for various editors and IDEs as well as a stand along tool called "halberd". /u/elaforge also seems to have written a tool to do the same. 
I love the Tardis monad, but it has two major problems: 1. It is easy to shoot yourself in the foot and produce code that crashes or loops forever. 2. It is slow since it relies on laziness and thunks.
Is there a FromRow equivalent for mysql-simple. In postgresql-simple, the FromRow type class makes it easy to map a database schema to Haskell datatypes. Alternatively, is it possible to deriving a datatype from a mysql database schema. The motivation for this question is that I am interested in sneaking some haskell into work by writing a service in haskell, which requires my being able to talk to an existing MySQL database.
ah, I see. So what is it's signature going to look like?
I know what a main stream language, I would prefer some example of how it is similar to Haskell in term of helping the reader to know the source of the function .
You might think you're describing a silly situation and that duplicating `append` on the type level is a bad idea. I would like to convince you otherwise: reimplementing `append` on the type level is useful, but not to prove the correctness of `append` itself. data Append : List a -&gt; List a -&gt; List a -&gt; Type where AZ : Append [] ys ys AS : Append xs ys zs -&gt; Append (x :: xs) ys (x :: zs) Here I have duplicated the two cases of `append`'s definition: if the first list is empty, return the second list, otherwise recur on the tail of the first list and cons the head in front of the result. How could this possibly be useful? Well, you might have heard of heterogeneous lists: data HList : List Type -&gt; Type where Nil : HList [] (::) : a -&gt; HList as -&gt; HList (a :: as) Ordinary lists have a [`splitAt`](http://hackage.haskell.org/package/base-4.8.1.0/docs/Prelude.html#v:splitAt) function which splits a list into two parts at a particular index: splitAt : Nat -&gt; List a -&gt; (List a, List a) splitAt _ [] = ([], []) splitAt Z xs = ([], xs) splitAt (S n) (x :: xs) with (splitAt n xs) | (xs1, xs2) = (x :: xs1, xs2) How could I write a similar implementation for heterogeneous lists? What would its type be? One which involves my type level `Append`, of course! hSplitAt :: Append as1 as2 as -&gt; HList as -&gt; (HList as1, HList as2) hSplitAt AZ xs = ([], xs) hSplitAt (AS n) (x :: xs) with (hSplitAt n xs) | (xs1, xs2) = (x :: xs1, xs2) There are other types we could give to `hSplitAt`, for example `(n : Nat) -&gt; HList as -&gt; (HList (take n as), HList (drop n as))`, but I think this one with `Append` is particularly elegant: if we know in advance which type indices `as1` and `as2` we want our output to satisfy, we somehow need to prove that the concatenation of `as1` and `as2` is the type index `as` of the input list, and we also need to prove that the `Nat` index at which we split is the one which will split this `as` into precisely the two parts we want. By using `Append`, we encode all of this into a single data type whose constructors also happen to mimic those of `Nat`, and so we can use a value of type `Append` as a type-safe index.
I don't mind importing a few thing. I do when I have to 20 to do. Regexp are fairly standard, map as well, applicative is part of baked in Haskell, so I would expect to import only Align, Csv and Tabular, which is what I probably would have to do in any other language. I'm aware of existing tools, but my question was if the community is aware of this problem and if so, and in which direction people are working. I understand tooling is workaround but I don't see it as a real solution. Even though it would be nice if it was integrated in GHC itself. At the moment, having to write 10 lines of import for a 5 lines script is really stopping Haskell to be a nice scripting language, which is a shame.
~~Don't think I can successfully log a session, could be wrong. (Trying it on (http://quicklift.parsonsmatt.org))~~ Talked to Matt on Slack, looks like the registration form response doesn't assign auth token. Revisiting the site and logging in should fix the issue I had. This'll make a nice purescript and servant example application! Thank you for releasing this :)
&gt; It'd probably be more obvious if you'd used something like openFile, since it can be shown[1] that filehandles will remain open after the only function that has access to the handle has thrown and finished. That might have been more obvious. I didn't think of using `lsof` for that purpose. One downside would be that the example code would be less portable. Maybe I should have defined some form of remote resource. Maybe a listening end that would be waiting for some release instruction. The garbage collector wouldn't help in that situation. I hope it didn't detract too much from the rest of the article.
Modified similarly to `foo`, ie. with a type parameter `forall tag. ...` instead of an argument `Proxy tag -&gt; ...`.
Nice, we have something similar in F# where `id&lt;int&gt;` has type `int -&gt; int`.
 Taking the Ruby example straight from your link # p031motorcycletest.rb require_relative 'p030motorcycle' m = MotorCycle.new('Yamaha', 'red') m.start_engine How does that tell me that `MotorCycle` comes from the `p030motorcycle` module ? That's not near what Haskell will provide with either import MotorCycleTest(newMotorCycle) newMotorCycle ... or import qualified MotorCycleTest as M M.newMotorCycle ... In both case, I'm sure that `newMotorCycle` comes from the `MotorCycleTest` module. Ruby doesn't give that guarantee at all. 
The question is not to make purity a religion. Good programmers have good intuitions about what mutable and immutability means in the concrete program that they have at hand. Talking generally without context looks with little meaning to me, because each one has in mind a different scenario. My scenario is a CPU with a big cache that due to pure linked list structures is constantly faulting, so it becomes useless and the code is much much slower than in a language with mutable structures. Haskell handles very well immutability but this also means that it can handle mutability as much or better: For example if the process perform repeated mutations to a well engineered structure that fits the CPU cache to produce a result that do not change if the parameters are the same, you can happily label that process as pure, event if it manages internal mutability. This is the way to take advantage of the way haskell type system delimits purity in order to perform mutability in a safe way. In the other side, procedures that are not pure are adequately labeled by the type system. So Haskell manages mutability perfectly. No one can be mistaken. What's the problem with mutability if it is required? And believe me, if you do not work in crunching numbers, ninety percent of the code for which you are paid is inherently impure by requirement. My answer to your friend would be: immutability makes your code more trustful, but if you need mutability, Haskell would let you know by the type system, the sections of code that are affected.
A very toy example, using `StateT s IO a`: https://github.com/jberryman/Befunge93
great work
Are there any plans of using this notation in error messages? Instead of saying that some `a ~ Int` maybe the type signatures could use the explicit type application.
What does ANN stand for?
From other comments in the thread it sounds doubtful. In simple cases like `length @[]` it is obvious what `[]` refers to but in more complex ones it is probably better to use the existing system.
I don't think `TypeApplications` currently does re-generalization as you describe: `fmap id @whatever` is an error. And it should be! The only way that should work would be if the type of `fmap` was `forall a b . (a -&gt; b) -&gt; forall f . Functor f =&gt; f a -&gt; f b`... Visible type applications are possibly only when the user has fixed the order of the variables (perhaps implicitly, by giving a signature over which the variables are quantified from left to right).
&gt; &gt; `return $! ()` &gt; &gt; Nit: () is already in WHNF so the $! is unnecessary. I do this in my code, but not for performance reasons; I do it because it shuts up false negatives from HPC, which insists on putting coverage testing trampolines around the '()' value you construct.
These points seem weirdly counterpoised.
I tend to agree. Also note that type application generalizes nicely to permit "impredicativity": I can write `id @(forall a . a -&gt; a)` and it just works. I don't think the same is true for type signature sections.
We sort of have it, `-fglasgow-exts`. However, it's deprecated and will probably be soon removed.
I took a look last night and they want $75/month. That may be fine for enterprise developers but I wouldn't pay that. I'd be happy to pay 300-500 or so for a permanent license and a reasonable upgrade policy (much like Borland/embarcadero had for pro developers vs enterprise developers) but this is too much, IMO. You're also right that a cloud based solution is of zero interest to me. 
What is the recommend way to do profiling with either cabal or stack ? With cabal I can do things `cabal config --enable-library-profiling/executable` what does that do ? It seems that I still have to add `-O2 -prof` in the cabal file. Same question with stack `stack build --profile` or equivalent is still a mistery. If I'm adding `-prof` into the cabal file is it a good practice to create a special profiling target instead of using the existing one ?
To reiterate one common point -- if one writes to a _typeclass_ then this guarantees your code is reusable whenever that typeclass can be inhabited. The choice to inhabit that typeclass by a free construction of a monad or a direct one is then entirely independent, and in that sense besides the point of reusability.
I like this comment from Tekmo: https://www.reddit.com/r/haskell/comments/3yksmn/a_modern_architecture_for_fp/cyh9489
I believe tel demonstrates this (inhabit the typeclass with arbitrary syntax) (with mtl) here https://youtu.be/JxC1ExlLjgw?t=12m16s
&gt; suddenly having to know the order of quantified variables just doesn't feel very haskell-y to me. Worse, now library authors can break client code in a new way by changing the order of type parameters or adding a parameter somewhere other than the end. I guess it’s not an issue in practice in languages that already have explicit type application, such as C++, but I imagine this change will expose places in Haskell where we have been making heretofore fine and now faulty assumptions.
You should look into react-flux package. We had a great time using it. It is very simple and at the same time has excellent documentation. Having said that, how is the ghcjs output size works for you? especially for mobile apps? Because of the huge output sizes we are now rewriting our app from ghcjs to purescript using purescript-thermite
There's been a proposal for adding [`LANGUAGE` pragma synonyms](https://ghc.haskell.org/trac/ghc/ticket/9642). All it takes is an intrepid coder to write it. :)
[removed]
True. There are many paths to reusability. I do much purely functional programming in a language that lacks nominative types and typeclasses. Consequently, one reason I favor free-er monads is that they entirely avoid the need for typeclasses - free-er monads work just as well in a dynamic or structural type system.
&gt; we are now rewriting our app from ghcjs to purescript using purescript-thermite I'd be interested to hear how that works out for you.
&gt; Precise specification and verification is just one facet of dependent types' usefulness. How do dependent types allow you to do precise spec and verification? 
So far so good. The reason we went with thermite is because of the reactjs components like react-bootstrap and others we can use. Another good thing is your recent change to use lens prisms. This allows us to create reusable components and plug them into the single atom of global state. Anyway, it is early to tell how all this will pan out. We are still in the process of rewriting. 
IDEs can also just run goimport, saving the ide plugin writers precious time and duplicated effort. 
That makes me to think that it is not efficiency and simplicity and getting the work done. What drives many developments is, simply, the epic of doing it. It is not to climb the mountain by car, but to climb it trough the most difficult side and tell the press about it. What do you think? Are the sellers of climbing sport material or the sellers of elevators and helicopters the ones that will have more visibility? Obviously, the sportsmen.
`{-# ANN ... #-}` is an [annotation](https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/extending-ghc.html#annotation-pragmas).
It's part of the site-wide rediquette that shameless self-promotion is frowned upon https://www.reddit.com/wiki/reddiquette . But is accepted within reason. I'd definitely consider this as "within reason" and not "shameless" as I totally love the article and would've otherwise missed it :D
What is a good way to go about designing a model for a game in Haskell? In PureScript, for example, to model game objects I'd build records that have data components (say, position or size), and use row polymorphism to write functions like: collisionDetection :: forall a b. { position :: Point, size :: Size | a } -&gt; { position :: Point, size :: Size | b } -&gt; Bool In Haskell I can use typeclasses, like `class HasPosition a where getPosition :: a -&gt; Point`, or even better: `class HasPosition a where position :: ALens' a Point`. But I'm not sure if using typeclasses for all this is a good fit. any thoughts or recommendations?
The design as implemented does something rather like this. It doesn't allow type applications to instantiate variables arising from generalisation, only those that come from user-supplied type signatures. Thus type inference implementation changes shouldn't cause changes in the meaning of programs. User-supplied type signatures are not required to be explicitly quantified, because that would be rather limiting. But there are (relatively) simple rules for determining the order of quantification from the written signature (basically just take the variables in left-to-right order).
Cool, thanks Adam for the info. I should've read carefully before I opened my mouth. With those rules it doesn't seem that frightening anymore, and it's still useful for cutting down on all the repetitive `Proxy` stuff that crops up in some of our `dimensional` code.
Are you allowed to mention `ScopedTypeVariables` in the `@` part?
Well the ingredients are always more or less the same: write a sum type of operations, make it a Functor, wrap it within Free, write the interpreter for each operation. This example is not exactly short but quite regular and self-contained : https://github.com/farnoy/torrent/blob/master/src/Network/BitTorrent/PeerMonad.hs 
One place where the flexibility of an interpreter has simplified things dramatically is in ghc-exactprint. The utility operates in two phases, each of which requires a full traversal of the GHC AST. The first is to convert the GHC API Annotations into a relative format, the second is to print out the (possibly modified) AST using the relative annotations to manage the layout. Matthew Pickering set things up so that the traversal of the AST is captured once only, and we run a different interpreter for each of the phases. This means the most complex part is captured once only, we do not have to try to keep two slightly different traversals in sync.
No, actually what the "NOTE" you copied used to say before it was edited. I didn't bother to save a copy.
Yes. For example: f :: forall a b. a -&gt; b -&gt; b f x = id @b -- f x = id @a &lt;- does not typecheck 
I'm not a fan. I prefer that for every symbol in the file that's not well-known (i.e. the contents of the Prelude), I should be able to Ctrl+F it and find a definition or an import statement. I do see the attraction of this idea when you consider being able to write a single `Imports.hs` file for a program, and being able to get the same imports in all files in the program, but I don't think what we need is more convenient ways to make code that's less readable.
One thing I really like is how a lot of scary stuff gets concentrated in one place: the interpreter. If there's a bug that seems to go outside the semantic realm of my program, it *must* be in my interpreter. If I use anything too coarse for specification (`IO` being the extreme), I need to audit a lot more code to see where my types haven't adequately constrained the realm of consideration my program is concerned with. Start with a runtime capable of exactly nothing and you have no bugs. Now add capabilities back in one at a time, and you end up confining the danger zone to the interpreter, which itself is free of the nuance of your program logic. It's liberating in a way similar to how types themselves make you more sure of your pure function logic.
Indeed: you're trying to calculate a [huge, huge number](http://www.wolframalpha.com/input/?i=2%5E3%5E4%5E5&amp;dataset=). Its *number of digits* has over 488 digits. A cool trick for the future: you can check an operator's associativity and precedence from within GHCi, by typing `:i (^)`. There are some other right-associative operators you should keep in mind: `:` and `++` and `=&lt;&lt;`, for example.
In all of these recent discussions about free monad, I have been quite surprised that the `operational` package is rarely mentioned. It even seems to me that many approaches were just mimicking it. Am I missing something ?
I had this question as well. In my one and only foray into using Free, I found that it wasn't very useful for defining a DSL, since many of the DSL terminals were no-ops (evaluating to Pure _). Does that mean my DSL was poorly written? Based on my experience (again, very limited), what Free was actually good for was encapsulating effects that programs written within the DSL would have on external systems. I was actually using FreeT and keeping track of internal state, which is what led to lots of (Pure _) terminals, and only using the Free type when I wanted to write things out. My experiment has a walkthrough of sorts: https://github.com/chreekat/stupid-game If anyone has feedback, that would be great.
Actually, if I'm interpreting that wolfram alpha page correctly, the number has more than 10^488 digits. That's far from "its number of digits has over 10^488 digits." A correct alternative would be "It's number of digits has over 488 digits." Still, a pretty huge number. Too big to really compute. Probably too big to store, given that there are 10^80 or so atoms in the universe. Actually, when writing this I wanted to tell you how many digits each atom in the universe would have to store to display that number, but wolfram alpha refuses to tell me: [link](http://www.wolframalpha.com/input/?i=%282^3^4^5%29+%2F+%2810^80%29) - it just prints out the same number again. Which actually makes me lose faith in wolfram alpha's initial result. I'm not exactly. Anyway, my self-calculated estimate says that in order to actually store a number that is 10^488 digits long, you'd have to store 10^408 digits **in every atom in the universe**. The implications of storing it are bonkers, that's how bonkers the length of this number is. Not even talking about how bonkers the number itself is.
If you think 2^3^4^5 is bonkers, think about 2^3^4^5 +1 for a while. It's even more bonkers!
Wonderful. Now I know how to generate large files for tests purposes! 
man, you had me worried my hand-crafted-and-double-checked math was off significantly. Had to recalculate the 10^408 figure before I worked out you were referring to the wolfram alpha link. [This's probably better.](http://www.wolframalpha.com/input/?i=%28log+%282^3^4^5%29%29+%2F+%2810^80%29&amp;a=*FunClash.log-_*Log10-) In other news, I wonder how wolfram alpha even handles those kinda requests. Working out the last few decimals is no biggie, as it pretty much just repeats at some point map (reverse . take 2 . reverse . show) [2^x | x &lt;- [1..60]] ["2","4","8", "16","32","64","28","56","12","24","48","96","92","84","68","36","72","44","88","76","52","04","08", "16","32","64","28","56","12","24","48","96","92","84","68","36","72","44","88","76","52","04","08", "16","32","64","28","56","12","24","48","96","92","84","68","36","72","44","88","76"] Should work for any number of suffix lengths. It should be possible to determine the cycle length c for the last 10 digits of a 2^y number. Using modular arithmetic, you can figure out what mod y c is. There's a lot of tricks like that, but they're bound to break at some point - either because wolfram alpha can't apply them anymore, or because they're not available due to computational limits (number too big, overflows, ...). [This](http://www.wolframalpha.com/input/?i=%282^3^4^5%29%2B1) not showing the last digits is the first reason: Wolfram alpha doesn't know (or doesn't know to apply) how addition affects the n last digits.
Xalem's Number: Actually bonkers enough to be totally bonkers while not being so bonkers as to appear less than valid when seen in a question.
To give a bit of background on why I'm asking - I've attempted to pick up a little bit of Haskell here and there, but I'm never sure exactly what to reach for for common tasks - records? my own custom data type? whatever custom data type some third-party library is using (for example, the MongoDB driver has a Field type). What are the most commonly used building blocks in Haskell? In most languages, they are some of the built in collections. For example: * Java: List and Map * Python: list and dict * Clojure: vector and hash-map What are the analogs in Haskell (if any), and do they exist in the standard library or somewhere else?
Types usually tell you that a value is within a particular set of values, say, the positive integers. There are also polymorphic types, such as `List`, which are indexed by other types, so we can form the types `List Int`, `List String`, etc. A "dependent type" is a type which "depends" on value, that is, a type which is indexed by a value instead of a type, for example `List 3` or `List 4`, etc. We can use this to define the type of lists which have a particular length, for example. Another thing we can do is to write a type which is only inhabited if a condition is true, for example you could construct a value of type `Odd 3`, but not of type `Odd 4`. And this isn't just that the `Odd` constructor throws a runtime exception if you pass it an even value: this is a compile-time check, so the type checker will only accept your code if it can statically verify that any number you pass at runtime is guaranteed to be odd, for example because you just checked that it is, or via some other more complicated ways which the type checker knows about. Given that, we can write extremely precise types such as `Sorted xs` which is only inhabited if the list `xs` is sorted; then if the type checker accepts that your function `sort :: (xs : List a) -&gt; Sorted xs` in valid, you are certain at compile time that your `sort` function satisfies the part of the specification of `sort` which says that the output must be sorted. So by constructing precise types like `Sorted xs`, you describe a precise spec, and by writing the code in such a way that the type checker accepts your code, you allow the type checker to verify that your code satisfies the spec.
I sometimes use a free monad as a stub because the type of my interpreter function often tells me what the actual definition should be in the end. For example, say I don't know what `State` should look like. I could start with a free monad instead: data StateInstruction s a = Get (s -&gt; a) | Set s a newtype State s a = State { unState :: Free (StateInstruction s) a } Then I write my interpreter function, using the type system to help me figure out what I'm really trying to do: runState :: State s a -&gt; s -&gt; (a, s) runState (State (Pure x)) s = (x, s) runState (State (Roll (Get f))) s = runState (unState (f s)) s runState (State (Roll (Set s m))) _ = runState (unState m) s Ah! So `runState` has type `State s a -&gt; s -&gt; (a, s)`. I can try the following representation of `State`: newtype State s a = State { runState :: s -&gt; (a, s) } With the free monad, my primitive operations are easy, but my interpreter is a bit harder. With the direct representation, my primitive operations are a bit harder, but my interpreter is easy.
I only desire to use a free monad when I want to interface to an "external" API that has a command-response interface. Then I can model my interaction with that API with a free monad.
Isn't `stack build --profiling` valid?
Uh, well I was just looking for examples of what things they can be used for and why it's better. People. Do this with of by writing combinatory and reducing pure imperative code to much simpler Haskell.. Surely this is possible with frees too 
Any number constructed by just addition, multiplication, and exponentiation can be handeled relatively easily. The last n digits of x are just x mod 10^n. You can keep intermediate results small by distributing the modulus over addition and multiplication in the obvious way (e.g. (a + b) mod c = ((a mod c) + (b mod c)) mod c). For exponentiation, a^b mod c = a ^ (b mod φ(c)) mod c. Our initial modulus has the form 10^n, hence φ(c) = 4 * 10^(n-1). Similairly, for exponents higher up in the stack, φ(φ(...φ(10^n )...)) = 4^m * 10^n-m. Once you have b mod φ(c) the exponentiation can be computed efficiently using standard algorithms for fast modular exponentiation.
I think the best solution is to push any standardized setup for a particular context out into a framework.
What's the story on distributed computing with haskell in 2016? Seems like there was some hype a few years ago with cloud haskell, distributed-process, parallel scientific and activity has almost all died out. Haven't even heard/seen many blogposts about haskell with services like AWS. Is this perception fair? Are people using Haskell with azure, google cloud, AWS regularly? How do people go about doing this? Where to begin with this?
Thanks for the explanation, that is fascinating. 
it can be a bit tricky. For things that are very stable, the convention is to define algebraic data types. They me composed of things like vectors, lists and strings. They may even be just a wrapper around something like a vector. The general principle is that the ADT to should be tightly wrapped around the known shape of constraints of valid computations to take advantage of type safety as much as possible. Now for more "fluid" data, for example, I just want to pull in a giant table with millions of records each having 10,000 columns from a database, there seems to be less of a clear answer. Obviously it doesn't make sense to manually construct an ADT by hand with 1000 fields, especially if it's a one-off or limited-use data analysis kind of task. There's packages like Frames and Vinyl that are supposedly better for this. I'm sure they're good packages but the are not exactly mainstream and battle hardened from heavy duty production use. I'd like to hear if there are more advanced users here with comments here.
As a beginner, everytime I think I'm getting used to syntax, particularly operator symbols, I stumble across another batch of new symbols that makes code look like chinese (or perhaps horrible perl code) to me all over again. After getting used to the functor/applicative/monad ones, I stumbled onto parsec combinators like &lt;|&gt;. After getting used to the parsec ones, I stumbled into aeson ones like .=. As I'm going through those, I start to hit libraries using lens, which seems like a galaxy unto itself. Does it ever get better? Is there a master list of the most common ones with some commentary on how to "read" them as concepts and where to look up additional info on where they originated?
unfortunately not a full game, but the most impressive stuff are these real time ray-tracing demos that smart people whip together https://github.com/ekmett/quine talk: https://www.youtube.com/watch?v=w763xvEQskQ For a more accessible hello-world version, there's the gloss demo: https://www.youtube.com/watch?v=jBd9c1gAqWs (only a few hundred lines of code!) but yeah, it would be nice if there were more full-blown projects to show.
hasql is postgresql only. My concern with persistent is the template Haskell requirement and lack of speed. Opaleye just got a sqlite version so maybe there is hope for MySQL?
&gt; in order to gain more mainstream popularity (
You want to look at [exponentiation](https://en.wikipedia.org/wiki/Exponentiation) and [logarithm](https://en.wikipedia.org/wiki/Logarithm) and some of their basic identities. 2 ^ 3 ^ 4 ^ 5 = { b ^ log_b(x) = x } 10 ^ log_10(2 ^ 3 ^ 4 ^ 5) = { log_b(x ^ y) = y * log_b(x) } 10 ^ ((3 ^ 4 ^ 5) * log_10(2)) = { focus on 3 ^ 4 ^ 5 } 3 ^ 4 ^ 5 = { b ^ log_b(x) = x } 10 ^ log_10(3 ^ 4 ^ 5) = { log_b(x ^ y) = y * log_b(x) } 10 ^ ((4 ^ 5) * log_10(3)) = { evaluate } 10 ^ 488.572 10 ^ ((10 ^ 488.572) * log_10(2)) = { focus on log_10(2) } log_10(2) = { b ^ log_b(x) = x } 10 ^ log_10(log_10(2)) = { evaluate } 10 ^ (-0.521) 10 ^ ((10 ^ 488.572) * (10 ^ (-0.521))) = { (b ^ x) * (b ^ y) = b ^ (x + y) } 10 ^ 10 ^ (488.572 - 0.521) = { evaluate } 10 ^ 10 ^ 488.051 Ok, so we know 2 ^ 3 ^ 4 ^ 5 = 10 ^ 10 ^ 488.051, but how many digits does that number have? If we can integer divide it by 10 once and have something nonzero left over, then we know it has at least 1 digit. If we can do that x times, then it has at least x digits. The number of times we can do that is just its (floored) logarithm. For a number x in base b, it has roughly floor(log_b(x)) + 1 digits. (Example, floor(log_10(100)) + 1 = 3, and 100 has 3 digits.) log_10(10 ^ 10 ^ 488.051) = { log_b(x ^ y) = y * log_b(x) } (10 ^ 488.051) * log_10(10) = { log_10(10) = 1 } 10 ^ 488.051 So, there are roughly 10 ^ 488.051 digits. I hope I didn't embarrass myself with the formatting or the arithmetic. Why isn't there a preview function?
"The first step is the first step." ...okay, thanks.
&gt; One general thing I ran into when writing cassava (which uses attoparsec) is that fmap and its friends are lazy, which means that expressions like ... might create intermediate thunks that might cause unnecessary allocation. It is indeed lazy, but after reading the dump-simpl it seems that in this case it is for the best. By using the lazy version, it appears that only one big allocation of memory is done for the string at the return. While when using a strict version, there are many allocate/reallocate.
I would install hoogle (https://wiki.haskell.org/Hoogle) on your machine and index packages, search from within your favorite editor.
&gt; Is it a matter of the choice of the particular monad the effect runs in (i.e. IO, ST, STM, etc.) or something else? Your code must sometimes run in several monads. I often have a pair of interpreters, one in IO and one that is pure for testing and other fun stuff.
Apparently this is more or less what has been done.
I wonder how many of those question would get away if we had better UI (for intermediate states of proofs, drilling down an application, sharing one such drilldown) Last I checked we were far from having polished practical tools for doing everyday math (even for easy things, no tactics in agda out of the box) On the other hand equational reasoning is so excellent in Agda I dont see how it could not appeal to mathematicians. --- ".. adjusting to the idea that proofs are first class objects that can be passed as arguments to and returned from functions. This idea was mind blowing!" It is mind-blowing !
what I have seen in other projects was the equivalent of a top level `src/Code2015.hs` which reexports modules in `src/Code2015/` directory. I'd be curious to hear how else to do it
Small little question. IIRC in one of his performance talk, Johan Tibell argues that "return $! foo" is always better than "return $ foo". But I don't think I have ever encountered "return $! foo" in open source code. Why is that ? Is it something that would be automatically optimized by ghc anyhow ?
There are a few options: https://hackage.haskell.org/package/caffegraph https://hackage.haskell.org/package/amazonka-ml https://github.com/alpmestan/hnn But I think "from scratch" haskell NN implentations are a bit thin on the ground.
Create script which will generate header (eg. myHeader) containing import Code2015.Prob01 import Code2015.Prob02 ... by looking on files in `src/Code2015` directory and then use {-# LANGUAGE CPP #-} module Main where #include "myHeader" main = ... in `src/Main.hs`. Line with `#include myHeader` will be replaced by contents of `myHeader` file. It's overkill imho - doesn't matter if you add another import or run script to regenerate your header.
&gt; How is it that one genius kid did quake3 in 2005 and there's basically been no progress since then? Do you mean Frag? &gt; lambdacube has fragmented off into its own DSL ... that's kind of a depressing state of affairs to be frank. LambdaCube 3D wants to be a high-level language for GPU programming close to Haskell. Anything useful outcome of it can be backported to a Haskell EDSL or if this is not true, then that is a witness that LambdaCube 3D should be a separate DSL.
Congratulations for managing the open-sourcing. It's a lot of work, but I'm sure the return will be worth it.
Pseudoradius pointed out my mistake. I did the math wrong.
I also had a similar moment in my programming when I saw that a certain operation could be either added to the interpreter (and my language of primitives be extended) or that it could be implemented "purely" in my language of primitives. Of course, in general, there is no single answer that would match all situations. But it's like deciding whether you want to program "in Haskell" or in your new special language. The latter decision would mean easier proofs of properties of your program (you can re-use the basic theorems concerning your small set of primitive operations) and easier portability to other interpreters. (If you ever need to write another interpreter or use someone else's one--in the case your primitives are standard like `MonadReader`, `MonadState` etc., then you won't need to re-implement your extra operation and the task would be less error-prone.) EDIT: But the first way would mean easier optimizations. Perhaps, optimizations could still be done by means of equivalent transformations of the sequences of primitive effects into the optimal code in each interpreter...
Honestly oftentimes precedence info doesn't really help me reason how something is going to interact with a few other operators. I'd like it if GHCi had a `:parse` command which would just parse an expression and print out a fully explicitly parenthesized version of it so that I could see how a set of operators interact directly.
Excellent! Will read it carefully; as a first thing, I notice you initialize your NN with random weights, which made me recall this: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?”, asked Minsky. “I am training a randomly wired neural net to play Tic-Tac-Toe” Sussman replied. “Why is the net wired randomly?”, asked Minsky. “I do not want it to have any preconceptions of how to play”, Sussman said. Minsky then shut his eyes. “Why do you close your eyes?”, Sussman asked his teacher. “So that the room will be empty.” At that moment, Sussman was enlightened. -- from http://catb.org/esr/jargon/html/koans.html
&gt; Basically any transient primitive can gain access to the closure where it is executing and to the list of continuations that are after it (one for each nested bind). That sounds an awful lot like the CPS translation of dynamic delimited continuations (`control` and `prompt`), where each term gets passed the continuation as usual, and a "metacontinuation", continuation to the continuation, which is a list of continuations (one for each nested `prompt`). Interesting.
Number theory is pretty bonkers, in general. No wonder the best minds of every generation that work on it are, well, bonkers. In a good way, but still. Hat tip to Cantor, Erdös ...
You might be interested into this https://bitbucket.org/wuzzeb/purescript-aeson-interop
PureScript's amenability to optimization is pretty great -- the dead code elimination performed by the compiler reduces the code size from ~1.3MB to 466Kb, and a pass with Closure takes that down to 184Kb. I remember GHCjs not minifying quite as well, though it is supposed to support Closure's advanced optimization settings
Ping /u/edwardkmett !
I want to upvote this because it sounds good. I have no idea what it's saying or if it's technically accurate.
oh! any idea how recent that is? I don't think there was a warning that I saw when I encountered the bug
Yes please! I'm also interested in how/if people use these new deployment/management workflows with containers and mesos/kubernetes.
To directly answer your question, I still find haskell records to be the most usable option for structured data with a certain set of fields (although `vinyl` is a cool alternative and I do a lot of experimentation with it). The examples of containers you give (lists and maps) exist in haskell as well: - The `[]` data type (pronounce as "List") is a part of the `base` package (which is the standard library). Relevant combinators are found in the `Data.List` module. - The `Map` data type is found in the `containers` package in the `Data.Map` module. People often import `Map` and related functions this way: import Data.Map (Map) import qualified Data.Map as Map 
I used monad transformers in [my project](https://github.com/soupi/pureli) as well. don't know if I can recommend using this project though, see for your self!
I know that Map and List exist, but are they the "go-to" data types for generally getting shit done and manipulating data?
Nice! I wonder how much work it would be to get Liquid Haskell integrated into School of Haskell. Liquid Haskell already has a really handy html source printer with type-inferred tooltips.
This is mainly how I've used them: to concentrate potentially dangerous code into one spot. One recent example is I had some deploy code that had to do some S3 operations and remote calls over SSH. It has a dry run feature that fakes out any behavior that isn't read only. I screwed up once and had a real command run. I reworked the code to a van Laarhoven free monad of effects and then created a dry run interpreter and a real interpreter which shared read-only operations.
If I'm reading this correctly, this declaration tells you that, given a type `V` and values `v1, v2 : StateF V`, the term `v1 ≃ v2` is going to be a type (in fact, it is the type of proofs of equality between `v1` and `v2` up to some simple equations, i.e. a term `t : v1 ≃ v2` means that `v1` and `v2` are "morally" the same).
"A DSL that compiles to JavaScript" sounds to me like an EDSL like [Feldspar](http://hackage.haskell.org/package/feldspar-language) or [Sunroof](http://hackage.haskell.org/package/sunroof-compiler). If that's what the proposal is about, then where does GHCJS enter the picture? I can also read the proposal as being about writing a few libraries for web development, but then what should these libraries be? "Everything that might be useful for web development" is pretty broad. There already quite a few bindings to JS libraries as well as native web development libraries, for both [Haste](http://haste-lang.org) and GHCJS, so what does this proposal aim to add, more concretely? All in all, I think this proposal would benefit from more background research and quite a bit of concretization. As it stands, it's very hard to get a grasp on what it is actually about.
Right, but where I have difficulty is that there is only such a term for certain `v1` and `v2`. Maybe I should start thinking about that as the empty set? 
I think Coq is better documented (though clunkier, in many people's opinion) and many of the concepts translate relatively well. For this, you could take a look at the [Software Foundations](https://www.cis.upenn.edu/~bcpierce/sf/current/index.html) book, which I have not read but have heard good things about.
Oh, I see. so I should install Coq, pick up Software Foundations and.. start reading?
But it is not the empty set! For example, consider terms v_1 = (set s_1 &gt;&gt;= (λ _ → set s_2 &gt;&gt;= λ _ → k)) v_2 = set s_2 &gt;&gt;= λ _ → k) Then, the type `v_1 ≃ v_2` is inhabited precisely by `set-set`. In general, `v_1 ≃ v_2` will either be isomorphic to `unit` or to `bottom`, *depending on* (dependent types, see?) the values of `v_1` and `v_2`.
Also, the Windows graphical console is very, very slow.
I meant `StrictData`, not `Strict`. Is there still a compelling reason not to use it everywhere?
It looks like `OverloadedRecordFields` will support hand-written instances: https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/MagicClasses#Hand-writteninstances
There are tons of examples and exercises to teach how to program with Coq.
I doubt there is a simple answer other than, "No one implemented it yet." I can certainly see how it would be useful and it's a feature I would like.
Because then they apply globally to all modules. In a large project, you sometimes don't want that.
The interesting thing is that (√2)^(√2)^(√2)^... isn't bonkers at all. It converges.
I'm going to say HistoricalReasons™. You could imagine a Haskell-like language that uses `Maybe` or `Either` instead.
I see `$!` in work codebases, but I don't remember seeing it often in open source either. I'm not sure the answer there. Of course, if the thing you're returning is lazy then `$!` can't really help that much. Just like putting `!` on a record field of type list is usually pointless. This is not automatically optimized. Strictness analysis is very hard (undecidable in general), so that's part of the reason we have `!`. When you know strictness will help you can add the annotations since the compiler can't infer strictness correctly in general.
Right, I meant that for say, `v_1 = get` and `v_2 = return 5`, then I should think of the type `v_1 ≃ v_2` as an empty `Set`.
My comment was meant to be more humorous than a cry for help seeking knowledge. I'm on [the Input and Output: Bytestrings chapter of Learn You a Haskell for Great Good](http://learnyouahaskell.com/input-and-output#bytestrings). I don't know if I'm ready for that comment yet, and I'd rather not pester someone to explain something that could come in due course.
But the "dot" syntax isn't used.
A common lisp condition is similar to a callback to an implicit parameter that is dynamically searched for on the stack. This callback is called with the "exception object", or really any parameters required, and the return value from the callback is a selected continuation, typically a symbol, which guides the library wrt how to continue processing. However, a "real" continuation that unwinds the stack, called `abort` is always available. An example is a parsing library. Let's say you want to parse jsonl, json line-delimited objects. The parser might encounter invalid json on one line. In that case, the library would have multiple continuations: `ignore`, `fix`, `returnEarly`, and `abort`. The `abort` is similar to what an exception does, it unwinds the stack. However, `ignore`, `fix`, and `returnEarly` are three ways the library can deal with the invalid line. The condition is signaled with the invalid line and the condition handler on the stack will get the invalid line and tell the library how to continue. This could of course be handled by passing down a callback for "exceptional circumstances" to all functions, but this is an implicit handler that is always available, and it subsumes exceptions, because the `abort` possibility is always there, and is the default (I think) for unhandled conditions. So this is a kind of in-between thing, and it lets an outer user of a library control details several layers down in a library. For example if I'm using a Hadoop library which is using a virtual file system reader that reads jsonl data, then I can setup a handler for how invalid json should be dealt with even though that is inside an abstract interface that is hidden by the hadoop library. We can really see this as throwing an exception where multiple continuations are given as arguments to the exception, and doing this in a structured way. 
But in OOP, the object has a type that is discoverable. Yes, the dotted methods are magic, but they are discoverable by tracing from the object that *is* discoverable. In Haskell with unqualified imports, there can be an extremely large set of modules to search through - and it becomes imperative to have tooling support to even trace where functions are coming from. That's bad for readability. Unfortunately when functions don't attach to objects some extra prefix is needed to help the user. Regarding your first point, all languages have *some* built-in stuff. That's true for Java as well. The question is how to scale that. In Java this is a fixed body of knowledge that is learned, and the Prologue is fine like that in Haskell as well. It's when we get large code bases with non-qualified symbols that the code becomes unreadable. It's important to always focus on readability, not writeability.
At a high level, you can think of `Set` as the type of types, of `T -&gt; Set` as the type of predicates over `T`, and of `T -&gt; U -&gt; Set` as the type of relations between `T` and `U`. In this case, `_≃_` is a binary relation between stateful terms parametrized by the return type `V` of those terms. (The reason why this vision is reasonable is no more obvious to the beginner than why `a -&gt; b -&gt; c` should be thought of as a two-element function. But this is how experienced people look at these types and what they immediately see. Well, at least when the notation suggests this is the favored interpretation, as it is the case here.) The mechanism by which the generators of this relation are defined (the interpretation of the constructors after the `where`) is the same as GADTs in Haskell -- or OCaml. The type `t` (possibly parametrized) has constructors of the form `Constr : foo -&gt; bar -&gt; t`. If this is a parametrized type `t a`, and all constructors are of the form `Constr : foo -&gt; bar -&gt; t Int`, then `t Bool` is an empty type.
Would it be possible to just give `String` a new type alias with "higher priority" than `String`, so that Haddock and GHC would report using the new name? Then we could un-confuse people without having to break any code.
For that particular error you can modify the `src/Imports.lhs` module to add this import: import Control.Exception (catch) The reason for this build error is that the Prelude used to export `catch` a long time ago and no longer does so now you have to explicitly import `catch`. More generally any time you see such a "not in scope" error it's usually because you need to fix the module's import list to make sure that the appropriate symbol is imported.
i had a pretty fun time implementing backprop and RBM training with Repa, https://github.com/aeyakovenko/rbm , convolutions shouldn't be to difficult. I would like to try the accelarate package one of these days for GPU acceleration, but I am on a mac and opencl support isn't great.
 No information found for ghc-7.10.3. Supported versions for OS key 'freebsd64': GhcVersion 7.8.4, GhcVersion 7.10.1, GhcVersion 7.10.2 Well, there's at least a binary dist of GHC on the downloads page. Hopefully this is as easy as a change to the stack source, but I'll be digging into this this afternoon.
Finally, aeson 0.10.0.0
Get RES, then you'll have preview.
I don't know. Think I shouldn't have mentioned String in my post though.
Well, I don't know. This kind of bugs me: data Stats = Stats { numNewlines :: Int, numSpaces :: Int, numLetters :: Int } calculateStats :: Text -&gt; Stats calculateStats = Text.foldl' step (Stats 0 0 0) where step = ... "Well, simple code and already out of memory. I suppose Haskell rumours are right... Back to Java and C# we go!"
Seems like as good a place to ask as any. Has anyone built the entire Stackage on their own machine or a VPS? I'm curious about how long it would take, say on the biggest spot instances on AWS or DigitalOcean.
&gt; and we'll see a lot of code with #x and #y in it. God I hope not, it's incredibly ugly. Edit: Actually, it's worse than just ugly. It's easily confused with the current use of `#` as a suffix for unboxed types.
I've seen this work on Ubuntu and on Mac. I'm sure it's trivial to add the info for freebsd. Ping /u/snoyberg
If you're interested, you can see the diff here: https://www.stackage.org/diff/lts-3.20/lts-4.0
Shouldn't `pprint &lt;$&gt; runQ [| 2 ^ 3 ^ 4 ^ 5 |]` show parentheses as well?
Last time I tried using the FreeBSD bindist with Stack it didn't work because it needed some special arguments to `./configure` and Stack expects the defaults to work. If those arguments to ./configure are consistent on all FreeBSD systems, we could teach Stack to pass them in, although in that case I don't understand why they're not the defaults for configure anyway. Adding FreeBSD support to Stack is tracked here: https://github.com/commercialhaskell/stack/issues/1253. 
Surely opinion is divided on the subject of the best default. Kmett a pro and Kiselyov an anti. Haskell showed lots of things that work well with lazy and lots that don't. It proved you need both. 
Also, how do I try to do this? Do I need to just paste it all into my cabal file? I'd like to try building them all on dragonflybsd's and see what fails.
There's some info here, under "Build the package set": https://github.com/fpco/stackage#readme
Just curious: why Backpack as the "One Thing" instead of anything else?
The module situation in Haskell is a serious pain point, from what I understand. Orphan instances, lack of expressiveness, and various other things I only remember in bits. The usual comparison is to ML, where modules say more about what they assume about their dependencies, rather than having to resolve dependencies in order to work all that out. So if, say, String and Text had a shared API fragment, you could write a module that was written against the fragment, but agnostic to the particular implementation. Or something like that.
Yeah, I meant to say that it's interesting that the metacontinuation approach has found use in monadic code, given the parallels between monads and continuations. I always get a kick out of rediscovering concepts from different points of view. That, and dynamic prompts tend to be harder to type in their full generality than static prompts, so anything regarding them in a typed setting is interesting to me!
Default hlint isn't always useful or correct. ...
Nice! I just bumped my largest stack based project to 4.0 and it rebuilt with no problems, as I expected.
You should get in touch with the team, or even just tweet at them if you'd have any short suggestions about how the tool could be improved. I'm sure they'd welcome feedback! 
Great! The structure of your code looks quite neat, so I think you could even generalize it and add some other ML algorithms if you're inclined to do so. There's a great need for a practical ML library in the haskell ecosystem.
Yes, that sentiment is exactly why I wrote this topic. 
At the primitive level it is pretty easy -- you can do dynamic loading using ghc-api or hs-plugins or other such tech. The tricky part was in figuring out how to prebuild all the plugins in a properly sandboxed way so that users could download and run them. Nix seems like an incredibly good fit for the build system part. Though, that is all just an idea at the moment -- I have not tried to implement it yet.
Doing it efficiently as caffe/torch would be interesting, they're all based on batch matrix multiplication mainly using cublas (the batch part is quite important for speed). Or on batch FFT. 
I think accelerate "batches" computations by coalescing them until you need the result out of the accelerate monad. From their wiki, "Computations on regular, multi-dimensional arrays are expressed in the form of parameterised collective operations such as maps, reductions, and permutations. These computations are online-compiled and executed on a range of architectures." How effective that is, I have no idea. The nice thing about using cublas is that you get nvidia's hand optimized functions. I would guess thats going to be a lot faster then whatever the code generator spits out. Maybe there is a way to call them via an FFI out of the generated code.
I meant ambiguous to the programmer who is looking the code over.
&gt; Do you mean Frag? Yes &gt; LambdaCube 3D wants to be a high-level language for GPU programming close to Haskell. Anything useful outcome of it can be backported to a Haskell EDSL or if this is not true, then that is a witness that LambdaCube 3D should be a separate DSL. That may be so, but too much of haskell's ecosystem has this "solution exists, we don't have to actually do it" trait to it. I don't think it's realistic to ask someone to adopt technology with the hope that it will get backported or the expectation they should take on backporting it themsevles.
Thanks /u/Temko and /u/skew for the help. It turns out it was pretty trivial to remove the SHE requirement, all it was using were pattern synonyms and idiom brackets.
We are now able to [add new packages in LTS minor releases](https://github.com/fpco/stackage/blob/master/MAINTAINERS.md#adding-a-package-to-an-lts-snapshot) as well, so hopefully the maintainers of removed packages will add their packages again when they are updated! 
Backpack would provide a more accurate and powerful way to specify build-dependencies than the PVP-style version-bounds we are required to specify currently.
You the real MVP!
Or get in touch with the HLint team if you find bugs
Okay, I'll go into a little bit more depth but first I'll say something RE: container management. We don't use kubernetes or mesos. We don't even use containers in production at the moment. As it stands we provision our own AMIs using Packer and deploy infrastructure to AWS using CloudFormation. I haven't entirely ruled out the idea of using containers in our infrastructure because they could be very useful at high scale (which we will hit at some point) but we haven't run into a situation that could not be solved more *simply* with stripped down custom AMIs containing everything needed to run the application. I imagine the process of provisioning a container with everything on it that you need using your build machinery would look very similar. You then just push the image to a repository where AWS Docker Container Service, or the other container orchestration tools can access it. --- Plum's production Haskell programs are pretty diverse and we use a lot of "common" utility libraries across many different applications so we have maximal reusability (we discovered that also comes at the cost of an intuitive cabal dependency list). Our dependency graph is extremely large and the biggest piece of advice I can give on this front is to never use cabal as a package manager. You should specify the *precise version of the dependency* you need, not with bounds (i.e use ==), and you'll have to discover what the intersection of all the dependencies is. That process is a bit manual and requires some trial and error but is pretty rock solid once you've done the work. Next is our continuous integration system - I think CI / build services are invaluable and we use CircleCI extensively for this but your own Jenkins server would work fine too (we chose CircleCI because their support for GHC is pretty good). You need to specify for the Continuous Integration service how your program needs to be built, what tests to run, where to put the artifacts, etc... We've CircleCI setup to build for staging and production on those branches and deploy the artifacts (resulting binaries, new configuration files, extra assets, etc...) to a bucket in S3. AWS CodeDeploy is configured using CloudFormation when we instantiate any resources so that it "just knows" about the S3 bucket - once CircleCI has successfully built an artifact and deployed it to the bucket we can then go into CodeDeploy and trigger a deployment of the binaries to whatever resources match the tags configured for that CodeDeploy Application (this could be automatic but our release process is deliberately manual - you could easily configure this to be automatic for your own needs). A lesson learned here: because we do follow a micro-services infrastructure layout I structured each micro-service to have its own code repository, which was fine for a while but quickly broke down because: 1. Some services needed to be deployed *before* certain other ones but weren't because they took longer than the other ones to build 2. All of the repos had to be "pushed at the same time" to trigger a build, which felt janky 3. And re-building a service in a repo way over here because you made a little change to the common shared util library for this service over here (and remembering) was a big pain in the ass We coalesced all of the micro-sevices into a single github repository with a single shared cabal dependency specification (cabal.config) but separate executable / library definitions in the .cabal file. Building the repo now re-builds all of the servies but it's much easier to integrate and deploy this way. Develop and debug cycles are much tighter now as a result too. Finally, something else we've done is to build our custom AMI to have everything any one of the services might need on it and to bring those services up based on what tags are assigned to the instance, so we have a very homogenous substrate with consistent deployment of artifacts across our instances all at the same time.
But for practical use that is solved by stackage, and the work needed by contributors is not hindered by expressiveness in that project.
Thanks for your detailed comment. Unfortunately I didn't find an approach I liked yet so I guess I'll give up for now until I have a better idea.
Stackage is only half a solution, as it was [said succinctly in another thread](https://www.reddit.com/r/haskell/comments/3y5z1a/ann_stack100/cydv5kc), &gt; *Stack(age) is not fixing the roof, it's just putting a bowl under the hole and saying "look, the floor doesn't get wet any more"*
Cool, thanks!
What does the exclamation mark meannin &gt; data ContentEvent = PostCreated !PostId 
It means the field is strict, that is the `PostId` value is always evaluated (to weak head normal form) when the `ContentEvent` is.
Yes, and the fact that this code isn't obviously wrong on first read is one reason lazy-by-default is problematic.
Glad to hear you managed it! If you're willing to share the non-SHE version, I'll gladly take a [pull request](https://github.com/adamgundry/type-inference). ;-)
Why `Num`?
[Oh, you!](http://vignette3.wikia.nocookie.net/random-ness/images/b/b8/Oh_you.jpg/revision/latest?cb=20110419012743)
Aargh! Sounds amazing. I wish I'd known this was a possibility before making other plans for the weekend. I could totally have used some face-time advice on my current project. Hopefully people are open to hanging out on Thursday and some of Friday night. To organizers, if something like this is even a remote possibility, even if it may not happen, please mention it early. I'll count on it next year I guess ...
Announcement? 
Yes, I use type erasure a.k.a unsafeCoerce to store the continuations, since each closure and each continuation has different types. That is the non elegant part of the solution. By construction I know that the closure and the continuations type match, but the state of the state monad does not know it. To avoid problems with these continuations without the help of types, I give some primitives for managing editing and executing them as black boxes 
With Java 8 getting functional features, it's only appropriate that FP starts moving closer to Java.
What is the difference to a minor LTS version bump? I already saw that unusually many new packages were added, how come?
Limiting anak is more about keeping your thoughts pure than your functions :-)
Nice approach. BTW, as an improvement for stability, some architectures require explicit instruction cache invalidation, so don't forget to perform that prior to jumping to the JIT'ed code address. See the various implementations of`flush_icache_range`in the Linux kernel for example.
It's really exciting to see organizations and companies outside of our community integrate with our ecosystem! I hope to see more of this! Thank you, CodeClimate!
Yeah I think announcement is right. I only ever see it with the haskell reddit it seems like though. example: https://www.reddit.com/r/haskell/comments/3zkfs5/ann_quicklift_a_haskellpurescript_single_page_app/
This may actually be a bit of culture/habit coming from the haskell mailing lists, where I used to see this use of "ANN:" a lot. Not that I frequent other language's mailing lists, but still :-)
In the readme you mention Raul Rojas' book on neural networks. I'm looking at the PDF now, but it's missing many of the diagrams. Do you have a link to a PDF that does have them at all?
The conference date is likely too close for me to get a Indian conference travel visa ... even if I was interested in FP.
Event sourcing is quite controversial. It has its own problems. Like for example how do you handle changes to the events structure over the time? In normal application, you add a new column to a table, no big deal. Or change a type of a column, no big deal. With even sourcing, changing anything in the event invalidates all prior events. So you have to implement versioning and that can get quite complex very quickly. 
I get why laziness is ultimately the right default, but here's a thing: people's motivation, when learning, is a limited resource. I think even PHP (and generally imperative programming languages with null, dynamic types, unrestricted mutability) has a lot of demotivating nonsense. But then you look around: Wikipedia, WordPress, Drupal are written in PHP. Motivation restored! Haskell doesn't have as many success stories that would restore the motivation as for example, PHP. What makes it worse is the people who come to Haskell have already gotten used to imperative nonsense. IMO if Haskell is to keep its laziness default and still get traction, it should eliminate as many other pain points, ideally the only remaining pain points should be the ones that are ultimately for the better (eg. laziness). A few obvious long-standing ones: --- — Haskell's type-driven development is amazing, best in class! — Cool! What about something simple, font size from 25 to 1000, in percent? Must a line or two in the best in class type system! — [Here you go!](http://lpaste.net/2425347252999421952) — ... --- — Ugh, that's so many qualified imports... Can I just put them all into one module? — [You can't!](https://www.reddit.com/r/haskell/comments/3zgork/is_there_any_hope_to_see_the_import_problem_solved/) --- — Ok, there's quite some duplicate code... checkSomething :: A -&gt; B -&gt; C -&gt; Bool checkSomething a b c = some (complex expression) a &amp;&amp; some (complex expression) b &amp;&amp; some (complex expression) c — Let's fix that! And currying makes it even simpler, nice! checkSomething :: A -&gt; B -&gt; C -&gt; Bool checkSomething a b c = exp a &amp;&amp; exp b &amp;&amp; exp c where exp = some (complex expression) — [The hell? What's that error message? Did I just break the compiler?](https://www.reddit.com/r/haskell/comments/3rdceq/monomorphism_restriction_error_message/) Also the defaults (lazy I/O and linked links strings by default). --- — Why am I bothering with this when I could've used C#? Yeah, time go back.
Even if you could get an Indian conference visa, I'm not sure it would do you much good for an Indonesian conference :-)
What package on LTS should I use to create windowed apps? Something that displays a image on the screen and receives inputs from keyboard, etc? 
&gt; Reimplementing ghc Mother of God... &gt; Reimplementing ghc --make Oh ok, that makes more sense.
&gt; you keep your [a], And with it every **f** for every version of **[a]** and **[b]** and **[c]** You have to keep all your code you ever wrote, never updating it, and come up with a versioning mechanism (still research field) that would allow you to cope with all the accumulated changes. I'm not saying it is impossible. Just pointing out the cost of such decisions. It is not all advantages. 
Backpack would arguably make a dependent-ish-ly typed Haskell more expressive. Agda, for instance, suffers from the same modularity woes as Haskell. With a better module system, code could be abstracted over the API rather than per implementation. What the exact implications of Backpack are for dependent types, I'm not certain, but it would certainly increase code reuse and modularity on a practical level, and might even allow neat synergies with dependent features. It might be possible (blue-skying, here) to, say, write our dependently typed string-handling or list-mangling API, then have any proof work we did be agnostic of the exact underlying implementation of that API. Write your proof for the API then get it for free for, say, lists, vectors, and what have you. Again, totally speculative. To answer your question more directly, adding more DT features to Haskell is a probably good, somewhat controversial choice. Backpack is a more or less uncontroversially good choice, since the current module situation sorta sucks (see PVP issues, Cabal hell, etc.), and it makes the language more expressive without breaking anything (as far as I understand). As I understand it, there are some knock-on effects that clean up some other edge cases, but I truthfully don't know enough about it to speak to them specifically.
I'm finding that with lts-4.0 and ghc 7.10.3 builds take much longer and use more memory than with lts-3.20 and ghc 7.10.2. This is particularly visible with pandoc-types, a small package without too many dependencies. Building pandoc with lts-4.0 in an ubuntu (precise 64) VM with 2GB RAM, I get: pandoc-types-1.16.0.1: build Progress: 71/91 -- While building package pandoc-types-1.16.0.1 using: /home/vagrant/.stack/setup-exe-cache/x86_64-linux/setup-Simple-Cabal-1.22.5.0-ghc-7.10.3 --builddir=.stack-work/dist/x86_64-linux/Cabal-1.22.5.0 build --ghc-options " -ddump-hi -ddump-to-file" Process exited with code: ExitFailure 1 Logs have been written to: /home/vagrant/pandoc/deb/.stack-work/logs/pandoc-types-1.16.0.1.log Configuring pandoc-types-1.16.0.1... Building pandoc-types-1.16.0.1... Preprocessing library pandoc-types-1.16.0.1... [1 of 5] Compiling Text.Pandoc.Generic ( Text/Pandoc/Generic.hs, .stack-work/dist/x86_64-linux/Cabal-1.22.5.0/build/Text/Pandoc/Generic.o ) [2 of 5] Compiling Text.Pandoc.Definition ( Text/Pandoc/Definition.hs, .stack-work/dist/x86_64-linux/Cabal-1.22.5.0/build/Text/Pandoc/Definition.o ) ghc: out of memory (requested 1048576 bytes) I had no such problem with lts-3.20. I believe that the problem is aeson-0.10. The build gets stuck on Text.Pandoc.Definition, which derives a bunch of generic ToJSON and FromJSON instances. When I added `aeson-0.8.0.2` to stack.yaml, the build finished with no difficulties. This is a serious problem. I've already had some users complain that `cabal install pandoc` halts with `ExitFailure 1` (which is almost certainly this same memory exhaustion problem). I could change pandoc-types 1.16 to exclude aeson-0.10, but this would keep pandoc-types 1.16 out of stackage. Does anyone know what might have changed in aeson (or possibly ghc) that might be causing this problem? How can it be fixed? (EDIT: I found a relevant issue here: https://github.com/bos/aeson/issues/296) 
That's an interesting observation. I've implemented CQRS/ES systems in companies where the domain model was already *reasonably* well-known. Usually, everything we needed to do could always be accomplished by simply *adding* new event types to the model. I actually fully expected to have to do on-the-fly upcasting and eventually ( once the complexity of upcasting had grown enough) to just read-upcast-write the whole event store once every year or two. That never happened in practice. My conclusion would be that /u/vagif's concerns are slightly overblown *unless* you're very unfamiliar with the domain model. (Of course you're also in some trouble in that case with a traditional relational model since you can't magick missing data out of thin air, so you *have* to have some sort of resort-to-defaults approach unless you're prepared to just throw away all the data.) EDIT: I should also mention: Even if you find out that you're missing some data that's "required" for the Query portion of the application(s), you can usually just create a new event type and generate artificial events for all the old aggregates which don't have that event. The Query in question can then just treat aggregates without that "required" data as completely missing until those events arrive. Problem solved! Where this *doesn't* work is where the *business rules* impose a new precondition on commands which cannot be checked without the data in question. In those cases you're pretty much screwed and have to magick up a default (or similar) out of thin air. EDIT2: I've also found that making events *as small as meaningfully possible* helps greatly to avoid this problem
&gt; You could say, that just updating the original event handler is ok (not inrdocuing a new code), but then if you made a mistake you can never reproduce the original state. You mean an event handler that generates new events? Suppose my existing event handler suddenly contains some test to see if the incoming event is v1 or v2. Before I introduce the change I can test it by running the event history of the system through the updated code, and if a mistake was made I can rewind and introduce v3 with corrected semantics. While this seems worse, I've seen many situations where RDBMS and ORM systems contain myriad data bugs because of layers of fixes and missing awareness of the changing semantics. If a piece of data got into its current state because the system had a given set of rules/behaviors, and we now fix one of the behaviors, the state may be completely invalid once the system is fixed, yet it may not be known (yet) that the data contains invalid state. Whereas with event sourcing we are forced to handle the change explicitly with no loss of data or coercion. In my view the main benefit of event sourcing is that all historical state information remains a first class part of the system, not something relegated to a db backup, etc. When we observe that a v1 event might be a very different thing than a v2 event, this lets us zoom out and re-assess the system's semantics. One tempting mistake is to assume that when an object changes it is still the same "thing". It often is not. Maybe it should be named something different, or maybe we need to introduce a new concept to our event ontology. Your points are definitely valid, but I think the main reason the event sourced approach seems harder is because it's a less common pattern and we don't have tooling to make it easy yet. 
 \x -&gt; p x &amp;&amp; q x is the same as liftA2 (&amp;&amp;) 
Unless you explicitly set it up otherwise, stack should work with the default ghc in your path, as far as I know. So just... install the latest ghc in your path?
I have a hard time seeing why the naive approach is quadratic in the size of the expression.
None so far. In fact I've had 3 messages asking if I've had any proofs but no actual proofs.
I find this compelling, but I also find that eliminating laziness isn't as easy as you make it out to be, especially for beginners. `seq` is mysterious, and why do we need a typeclass to evaluate something to normal form? If applying strictness as necessary were as easy as writing `force x` (without creating [boilerplate](https://github.com/haskell/deepseq/issues/4) `NFData` instances or using TH), I think there'd be fewer complaints about laziness by default.
Oh, but the AD only does one recursive call. That's pretty magical. So it's basically due to common subexpression elimination, right?
&gt; With events though, you have to create a new version of event and you cannot update the original event handler to know about the new field. That's just one possible approach, and as you point out, one with several drawbacks. You could update your event handler to handle the new version, and convert old events to the new version as you extract them from where they're stored, on the fly. You could do that up-conversion on the stored events, making it a one-time cost; the reason to do this is to avoid an accumulation of conversion functions, the reason to avoid this is it mutates history. &gt; You could say, that just updating the original event handler is ok (not inrdocuing a new code), but then if you made a mistake you can never reproduce the original state. No data is lost if you do this, so the only mistake which could mean you can't return to an old projection is one in _code_ versioning.
The implicit reason why this works is because derivatives are functorial: That's what the chain rule *is*.
As others have said, you can't, so writing a function that does what you want with the desired type signature is impossible. What you can do use bind to go 'inside' the IO value. someFunc path = do content &lt;- readFile path n &lt;- read content ... ( do stuff with n ) This is equivalent to (in fact, desugars to) someFunc path = readFile path &gt;&gt;= (\content -&gt; read content &gt;&gt;= (\n -&gt; ... ( do stuff with n ) ) ) someFunc will still have type an IO return type, though.
Denommus is correct about how to fix your code. Even if you don't think it's what you want, you almost certainly want readNumberFromFile to be a function from a FilePath to an IO action *producing* an integer. Otherwise, you have substantially less control over when the file is actually read or how many times, even if you're okay with hiding the fact that there's something impure going on under the hood (which you likely shouldn't be). All of that said, you *can* go from `IO String` to `String`, with the function `unsafePerformIO :: IO a -&gt; a` in System.IO.Unsafe. In that case, the read will happen the first time the `String` is actually used, and may or may not happen on subsequent uses of that `String` (depending on optimizations like what got inlined where, what got CSE'd, etc). Again, this is almost certainly not what you want.
To be more precise, tangent applications `(x,v) -&gt; (f (x), Df(x).v)` are functorial.
On a far less sophisticated note there is also a comonad on the `Jet f` data type in https://github.com/ekmett/ad/blob/master/src/Numeric/AD/Jet.hs given the ability to zip together `f` structures, and a monad if `f` is representable. I never did find a good place to put those.
Another problem for integrating with GHC is the use of the `.shake` directory to store metadata (which I've pointed out to you before.) It is definitely not a big deal, but it does break things (for example, I can't run GHC's test suite on `ghc-shake` in parallel, because many of the tests run in the same CWD and hidir, and thus Shake contends with itself.)
Hm, but i would need a book that teaches me the theory as well. Is there such a book?
Is repa+accelerate not a strong enough numerical base to converge around? I haven't tried them myself here so this may be a dumb question.
accelerate isn't well supported on macs? I haven't used it before, but this is the first time I've heard that.
Other people have already explained how to fix your example. I’ll try to give a more general intuition for how to use `IO` (and other monadic things). It was once explained to me like this. An `IO String` contains a `String` in the same way that `ls` contains a list of filenames—it doesn’t. In other words, `IO a` is a command that gives you an `a` when it’s run. You can’t run `IO` actions from *inside* Haskell without unsafe code, but you can do a few things: You can create an action that returns a pure value, with `return` or `pure`. You can sequence actions together with `do`: do -- run an action with a result, e.g., getLine :: IO String line &lt;- getLine -- run an action with no result, e.g., putStrLn "hi" :: IO () putStrLn "hi" -- do normal pure computations, e.g., read :: String -&gt; Int let pureResult = read line -- finish with an action return (pureResult * 42) You can use the `&gt;&gt;=` and `&gt;&gt;` operators—`do` is syntactic sugar for these: getLine &gt;&gt;= \line -&gt; putStrLn "hi" &gt;&gt; let pureResult = read result in return (pureResult * 42) You can assign them to `main` so that the Haskell runtime will run them for you. main :: IO () main = getLine &gt;&gt;= putStrLn With these tools, you can write ordinary imperative code, as long as you keep in mind the difference between actions and values. The advantage of doing things this way is that actions are “first-class”: they can be stored in lists, passed to functions, and whatever else you like. 
Fixed.
I read it as "Re-implementing GHC: make with shake" thinking Shake was some obscure language or tool being used to recreate ghc...
The accelerate-opencl packages are experimental and I couldn't get them installed on my mac, do to some issue with parsing the opencl headers and c2hs. I didn't try the interpreter, since I already had repa working.
Worst of all, the aeson maintainer has stayed quiet on all these issues. Is aeson abandoned and in need of a new maintainer? Here's a couple of tickets specific to 0.9-&gt;0.10 regressions that have been left uncommented for weeks or even months... :-/ - https://github.com/bos/aeson/issues/330 - https://github.com/bos/aeson/issues/320 - https://github.com/bos/aeson/issues/328 - https://github.com/bos/aeson/issues/305 - https://github.com/bos/aeson/issues/293 - https://github.com/bos/aeson/issues/287 - ... 
Does stack/cabal use ghc make? 
As mentioned last time this came up on /r/haskell I would consider that a misfeature, it makes writing code slightly easier at the cost of making reading it significantly harder.
Yes, that is a bit of an issue - Shake needs a global view of all files (which is partly why it's able to go faster), but storing that global view causes complications. For the test suite you can always redirect each .shake to a different location based on the test name, but it does break previously working scenarios.
Just to give you a bit more intuition about why your type signature must be `FilePath -&gt; IO Int`, you should realize that the integer value that you're expecting it to return is not completely determined by the inputs to the function! The file path itself is not sufficient to determine this value, it also depends on the contents of the file. You can call it with the same parameters multiple times and if the file changes inbetween calls, you get different return values out of it. In short, your function isn't pure and thus it can't have a "pure" type signature. It's dependent on the "real world" outside and thus needs to be an `IO Int`. The reason why you can't just extract something from an IO type, i.e. there's no (safe) function `IO a -&gt; a` is that once *one* value depends on stuff outside of your program (e.g. user input, file system state, etc), every subsequent calculation with this value must also depend on it, because their inputs do. To actually *use* such a value, you're presented with a variety of different ways. The `IO :: * -&gt; *` type is a monad, and thus also a functor (and in more recent versions of GHC also an applicative functor of course). That presents you with a few functions to modify and work with such values, most notably (specialized to IO for simplicity) fmap :: (a -&gt; b) -&gt; IO a -&gt; IO b (&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b And there's also do-notation, which adds some syntactic sugar.
This is an instance of a larger problem that https://snowdrift.coop/ is trying to solve. Perhaps you'd be able to either use their platform or work with them to get it running? Disclaimer: I have no idea what their status is at the moment.
Discussion of original article here: http://h2.jaguarpaw.co.uk/posts/symbolic-expressions-can-be-automatically-differentiated/ AD[1] is neatly expressed as a catamorphism. In fact, in the addendum I added to the original article, I noted that if you write `eval` and `diff` as catamorphisms then you can compose them to get `AD` for free! There's no longer a quadratic slowdown. [1] This is specifically forward mode AD
Haskell 2.0 will begin when `lens`'es land in `base`.
Is [HLearn](http://hackage.haskell.org/packages/search?terms=HLearn) not practical?
The issue is unrelated to OP. The problem is that Church encodings and predicative universes don't really mix. What we'd like to have here is like this: open import Level renaming (zero to lzero; suc to lsuc) λList : ∀ {α} → Set α → ∀ {β} → Set (α ⊔ lsuc β) λList A = ∀ {β}{R : Set β} → R → (A → R → R) → R But that fails with `Setω` error. The best we can do is to lift out the `β` level and plug it around manually: open import Data.Nat hiding (_⊔_) open import Level renaming (zero to lzero; suc to lsuc) λList : ∀ {β α} → Set α → Set (α ⊔ lsuc β) λList {β}{α} A = ∀ {R : Set β} → R → (A → R → R) → R [] : ∀ {α β}{A : Set α} → λList {β} A [] = λ nil _ → nil _∷_ : ∀ {α β}{A : Set α} → A → λList {β} A → λList {β} A _∷_ a as nil cons = cons a (as nil cons) infixr 5 _∷_ map : ∀ {α β} {A : Set α} {B : Set β} → (A → B) → λList {lsuc β} A → λList {β} B map f as = as {λList _} [] (λ a bs → f a ∷ bs) append : ∀ {α β} {A : Set α} → λList {lsuc β ⊔ α} A → λList {β} A → λList {β} A append xs ys = xs {λList _} ys _∷_ a : ∀ {β} → λList {β} ℕ a = 0 ∷ 2 ∷ [] b : ∀ {β} → λList {β} ℕ b = 0 ∷ 4 ∷ 5 ∷ [] c : ∀ {β} → λList {β} ℕ c = append a b However, this becomes unworkable very quickly. For example, how are we supposed to write `concat : λList {?} (λList {?} A) → λList {?} A`? Alternatively, one can use `type-in-type` in anger: {-# OPTIONS --type-in-type #-} List : Set → Set List A = {R : Set} → R → (A → R → R) → R [] : ∀ {A} → List A [] = λ z _ → z _∷_ : ∀ {A} → A → List A → List A _∷_ a as = λ nil cons → cons a (as nil cons) infixr 5 _∷_ map : ∀ {A B} → (A → B) → List A → List B map f as = as {List _} [] (λ a bs → f a ∷ bs) Or switch to Coq `Prop`-s: Definition List : Prop -&gt; Prop := fun (A : Prop) =&gt; forall (R : Prop), R -&gt; (A -&gt; R -&gt; R) -&gt; R. Definition cons : forall (A : Prop), A -&gt; List A -&gt; List A := fun _ x xs _ n c =&gt; c x (xs _ n c). Definition nil : forall (A : Prop), List A := fun _ _ n c =&gt; n. Definition map : forall (A B : Prop), (A -&gt; B) -&gt; List A -&gt; List B := fun A B f xs =&gt; xs (List B) (nil _) (fun a bs =&gt; cons _ (f a) bs).
Thank you ! I created a ticket on github =&gt; https://github.com/YoEight/corecursion-net/issues/1
Very cool. How does step look with the Cofree structure though? I can't get the pegs to fit the holes. :/ Mapping rules to axiom results in m (f (m a)), and even if f is loosened to Traversable, that can only be sequenced and joined to m (f a), whereas coiter wants b -&gt; f b.
Phooey! I had hoped not to find the unmentionable "function" among the answers.
HLearn is a very interesting proof-of-concept, but it's not a mature library. Currently, it supports LogisticRegression and GLMs, but doesn't support many other standard algorithms (random-forests, SVM, Lasso, not to mention deep-networks). I'd like to put in some development time myself, but my time constraints mean that's not feasible until summer at the earliest...
Will there be any kind of itinerary? I don't have a personal project so I don't know if there will be enough content to fill two whole days.
I think it's a valuable piece of perspective. It's not that we're preventing you from going `IO String -&gt; String` because we're malicious and like to make you do gymnastics. It's because going `IO String -&gt; String` is almost always a bad idea in this context.
There is no itinerary for the unconference. But don't worry, there will be **plenty** of really interesting people there to occupy your time. In my experience at similar events most of them **love** talking with just about anyone interested in functional programming topics they have worked with (and probably quite a few outside their area of expertise).
Ha. This is the exact question I asked when I first dealt with Haskell. Then my eyes were opened to a new world.
Sadly forward mode AD doesn't scale up to any interesting problems. =(
You could maybe try making sub1 :: forall p q a. (p a =&gt; Dict (q a)) -&gt; p a ⊢ q a sub1 = Sub then using sub1 @Ord @Eq Dict You might also try using `cls` instead of the more general `Sub Dict` for this concrete case. `cls @(Ord _)` would likely work. (I don't have 8 handy for testing and don't know the variable order on `cls` though.)
PureScript's Eff can be summarized as IO with an extra phantom row tracking what effects are actually in use. Row polymorphism does the right thing when you combine actions with different effect rows.
It requires more constraints on `f` and `m` than I gave, but these are easy to satisfy for the interesting instances (e.g., `m` is usually `[]`). The implementation of the `step` function for `coiter` is rather ugly, but conceptually it is relatively elegant: it distributes the `f`'s over the `m`'s when applying rules to bring the "`f`-shape" out. Here's an example: https://gist.github.com/reinh/9353595b9582d3d2197b I'd welcome any attempts to improve the step function. I'm sure it can be.
Yes `sub1` works and so does `cls`! (the variable ordering is reversed, `b` followed by `h` for `cls :: h ⊢ b`) ghci&gt; cls @(Eq _) @(Ord _) Sub Dict `cls` solves it for this particular case but I won't always be so fortunate to get help from functional dependencies and defining a new function defeats the point. [Named Wildcards](https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#named-wildcards) didn't work but I'm not convinced that they *should not*: All occurrences of the same named wildcard within one type signature will unify to the same type. So there's the rub, does `Sub @(Ord _a) @(Eq _a)` have two distinct type signatures? Or one? Or none? :) let's say I wanted to specialise `map` to `(a -&gt; a) -&gt; [a] -&gt; [a]` -- nope map @a @a -- add quantifier but now the type is [a] map @a @a undefined undefined :: forall a. _ -- oops a escapes scope! \f xs -&gt; (map @a @a f xs :: forall a. _) -- this works but is jolly inconvenient (\f xs -&gt; map @a @a f xs) :: forall a. _ If GHC identified wildcards in type applications within the same expression we could write these marginally motivating examples as `Sub @(Ord _a) @(Eq _a)` and `map @_a @_a`.
You and me both.
How do you guarantee that lazy IO will raise exception inside your handler instead of later ? I tried: blork' :: IO () blork' = do content &lt;- runUnexceptionalIO (fromIO (readFile brokenFilePath)) case content of Right s -&gt; print $ "OK" ++ show (length s) Left _ -&gt; print "ERROR" And I still get an exception during the evaluation of length. But perhaps I did not understand the documentation and use it incorrectly.
I suspect that the answer is to not use lazy IO.
It certainly is possible, however, not in the way you are structuring it. Currently, your depositAccount does nothing, as it returns its input and is a pure function. I suppose what you want is: - Ask the account name; - Check if it exists; - If it exists, ask how much. - If not, repeat. The first thing I would recomment you to do is to use [Data.HashMap.Strict](https://hackage.haskell.org/package/unordered-containers-0.2.5.1/docs/Data-HashMap-Strict.html) from the `unordered-containers` package to model your accounts container. It will be faster and the code will be easier to understand since there are lot of utility functions there to help. In the spirit of your code I would do: import Data.HashMap.Strict (HashMap, member, adjust) doDepositAccount :: HashMap String Account -&gt; IO (HashMap String Account) doDepositAccount accounts = do putStrLn "On which account to do want to deposit?" accountName &lt;- getLine if not (accountName `member` accounts) then do putStrLn "Account not found, try again." doDepositAccount accounts else do putStrLn "How much do you want to deposit?" quantity &lt;- read &lt;$&gt; getLine let newaccounts = adjust (addToAccount quantity) accountName accounts return newaccounts addToAccount :: Int -&gt; Account -&gt; Account addToAccount = ... The rest is up to you. The definition of these functions is there in Hackage. Also, [/r/haskellquestions](https://www.reddit.com/r/haskellquestions) would be a better place for this question.
And this is why I always import HashMap.Strict as HMS and so on. I would also re-export HashMap.Strict as HMS and HashMap.Lazy as HML; Data.Aeson as Aeson. ByteString.Lazy as BL, ByteString as B. ByteString.Char8 as B8, ByteString.Lazy.Char8 as L8 It's always the same. Always. And I've written each one of these so many times that I don't even want to think about it. This is why I want to import a module hierarchy and not flat names. So I can do this in each project once and for all.
Me too. I don't really see what's so bad about monad transformers, to be honest. They actually satisfy a lot of really nice properties (for example, though monads don't compose in general, monad transformers do), and they are more expressive than the alternatives I've seen pushed so far.
&gt; is there a lib inside hackage which wrap/implements common IO functions without lazy IO and without exception There's this: * https://hackage.haskell.org/package/strict/docs/System-IO-Strict.html It doesn't catch exceptions, but if you combine it with the library announced in this thread, you get what you want.
I got this working on Ubuntu recently, and I don't think I had to do anything fancy with zmq... maybe used a ppa?
This looks good but a bit above my level. What would be a good reading to get me to the level of maturity needed to understand it? 
`RecordWildCards`really surprised me with how much I enjoy using it.
What do you mean by "implementing" them? Do you mean that extensible effects (or mtl or what-have-you), gives you a vocabulary for describing certain types of effects in isolation from each other, but which can be recombined in a safe, sensible way? I can see value in being able to write interfaces that make their effects known, just like I currently value interfaces that make their input/output types known.
You may like the first few chapters of David Spivak's "Category Theory For the Sciences", which can be read for free online: http://category-theory.mitpress.mit.edu/toc.html It says it's for "sciences", but really it's a pretty gentle introduction that doesn't assume you've had an undergraduate math background. But without knowing your math background, I'm not exactly sure what to recommend.
Slides: http://reiddraper.com/production-haskell.pdf
One thing that strikes me is that if we compare pure :: a -&gt; f a ap :: (a -&gt; b) -&gt; (f a -&gt; f b) and pure :: a -&gt; f a pair :: (f a, f b) -&gt; f (a, b) then the former allows you to write a definition for `fmap`, namely: fmap = ap . pure On the other hand, there is no way to write `fmap` using only `pure` and `pair`. So using `ap` instead of `pair` makes it clear that an applicative is *always* a functor. It should be possible to define a "pair" applicative that is not a functor: data Transformation a = Transformation (a -&gt; a) -- Cannot be a functor, since -&gt; is not covariant in left argument instance PairApplicative Transformation where pure x = Transformation (const x) pair (Transformation f) (Transformation g) = Transformation $ \(x, y) -&gt; (f x, g y) I'm not sure which laws you were intending to adopt, but I'd guess that this instance would probably satisfy them. So the result would be that you have an "`Applicative`" which is not a `Functor`. 
RankNTypes is by far the most expressive addition not yet in Haskell, and many things simply cannot be written (or must be written very differently) without it
That's true for `const id`, sorry, my bad. It was a wrong example. What I wanted to say is that because Haskell do not enforce laws, we cannot be sure that the instance is right. But since Functor instances are unique, I think this does not holds for this case... I cannot think on a way how to write `ap` without using `fmap` in this case, so you may have a point as we lose this implication.
(I deleted my above comment because I had some fairly egregious errors). What I mean to say is this: *If* `ap` and `pure` satisfy the `Applicative` laws, then `fmap = ap . pure` *must* satisfy the `Functor` laws. On the other hand, there is no such definition of `fmap` in terms of `PairApplicative` so we cannot say it is that case that all `PairApplicative`s are `Functor`s. This makes `PairApplicative` more general than `Applicative` in some sense, but it similarly has less structure.
I believe the poster means that these two triplets of functions (when they satisfy the corresponding laws) are isomorphic: `(fmap, pure, ap) ≅ (fmap, pure, pair)` where `pair :: (f a, f b) -&gt; f (a, b)`. And, to justify that, of course `fmap = fmap` and `pure = pure`, and bf &lt;*&gt; bx = fmap (\(f, x) -&gt; f x) (pair bf bx) and pair a b = pure (,) &lt;*&gt; a &lt;*&gt; b So, intuitively, both definitions ought to be equally powerful, since we can write one in terms of the other. 
[removed]
That is exactly what they allow. In some cases you can even swap out different implementations to get different behavior - for instance use an implementation that mocks network effects during testing. Or that accumulates output in separate buffers and lets you inspect that.
given `(&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b` and also `pair :: Applicative f =&gt; f a -&gt; f b -&gt; f (a,b)`, consider: from `(&lt;*&gt;)` we can build something w/ the same type as `pair`: `pure (,) :: Applicative f =&gt; f (a -&gt; b -&gt; (a,b))` `\x y -&gt; pure (,) &lt;*&gt; x &lt;*&gt; y :: Applicative f =&gt; f a -&gt; f b -&gt; f (a,b)` And from `pair`, we can build something with the same type as `(&lt;*&gt;)`: `curry ($) :: (a -&gt; b, a) -&gt; b` `fmap (uncurry ($)) :: Functor f =&gt; f (a -&gt; b, a) -&gt; f b` `\f x -&gt; fmap (uncurry ($)) (pair f x) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b` some playing with the laws for each proves that the first is equivalent to `pair` and the second to `(&lt;*&gt;)`.
Ah, right, thanks! I think I was subconsciously trying to make the `b`s the same.
&gt; the variable ordering is reversed, `b` followed by `h` for `cls :: h ⊢ b` I could fix that with an explicit `forall` with the variables in the other order as `Eq` there is determined by `Ord`. I'd be open to that. 
This is similar in many ways to defining a monad with `return` and `(&gt;&gt;=)` vs. w/ `return`, `fmap` and `join`. With the former `liftM` is a valid definition for `fmap`.
People don't like writing _n_^2 - _k_ monad transformer instances, so instead they'll spend 2^_n_ time complaining about them.
Unification isn't happening there, _subsumption_ is, driven by needing to resolve the class constraint in negative position.
Ah, right, yes. Ok, so it's having to compute where `forall a. Show a =&gt; a -&gt; String` is a subtype of `forall a. c a =&gt; a -&gt; String`, which means computing if `c a` is a subconstraint of `Show a`, which would include `Funny a`. right, ok. i often forget about these subtyping things :)
Yes I think that's the way to go.
&gt; `ap :: (a -&gt; b) -&gt; (f a -&gt; f b)` Am I understanding this correctly? This looks exactly like the type signature of `fmap` to me. Did you mean `f (a -&gt; b) -&gt; (f a -&gt; f b)`?
Right. Also it's a common knowldege that lazy IO is an antipattern anyway.
to be explicit, forward-mode scales fine for functions f:R^m -&gt; R^n where m&lt;&lt;n where reverse-mode is best for m&gt;&gt;n. this is somewhat obvious considering the techniques. forward-mode seeds the input, which carries with it extra complexity as it passes through the function. i could be mistaken, but that is my understanding.
Standalonederiving is fun
Have you looked into [fgl](https://hackage.haskell.org/package/fgl) ?
&gt; Do you mean that extensible effects (or mtl or what-have-you), gives you a vocabulary for describing certain types of effects in isolation from each other, but which can be recombined in a safe, sensible way? Yes exactly. That's pretty much the whole point of them. If you thought monad transformers or extensible effects were just about effect *tracking* then someone drastically misled you. :)
A doubt: there are a number of closed-form results for derivatives of matrix functions, e.g. d/du u^T A u = (A + A^T) u . Can these be applied at compile time?
I personally dislike RecordWildCards, because you have to go and look at the record definition to see where a variable came from. To me a better alternative is to use NamedFieldPuns. This means that instead of C {a = 1, ..} = e you can write C {a = 1, e} = e 
`OverloadedStrings` is easily the one I use the most, but I would probably not call it my favourite, since it doesn't actually remove *that* much boilerplate. Similar reasoning with `TupleSections` and `LambdaCase`, which are the next most common on my list. I often incur a couple of `Rank?Types` when using lenses, but I don't actually know what they do – I just enable them when GHC tells me to – so I don't want to name them my favourite for that reason. `TemplateHaskell` and the related `QuasiQuotes` are highly useful general tools for avoiding copy pasting code, and I seem to use them every now and then, so those might actually be my favourites. The ones I've mentioned are really the only ones I've learned to use consciously. Others like `TypeFamilies`, `ExistentialQuantification` and such I seem to have used once or twice, but they are likely things I enabled when GHC told me to, much like with `Rank?Types`.
I remember that unification isn't used for constraints at all. Is that correct?
Rust macros are somewhat similar.
Check out T4
It just extends the scope with the fields in a record. It's no different than using a function defined at the top level except the "import" for it is right there at the call site.
Wow, no one's mentioned a couple of my favorites. `ImplicitParams`. They're a good way to thread configuration even if it's just for internal functions. `ViewPatterns` are just cool and though not a dedicated extension, I recently found out how much better closed type families are for type level functions compared to typeclasses.
How has no one mentioned `GeneralizedNewtypeDeriving` yet? I use it constantly. It's enormously fun to successfully identify a new type-level concept, pull a given data type out into its own newtype, and to have to change your code barely at all thanks to newtype-deriving.
A library to wrap the standard IO actions unexceptionally is something I've considered also. There shouldn't be any other kinds of pseudo-exceptions that "should not be caught" since any other exception type is thrown on purpose. If you don't want to handle it at the call site, use ExceptT
It seems that zeromq wrapper zeromq4-haskell needs a version &gt;= 4.x (from the [documentation](https://hackage.haskell.org/package/zeromq4-haskell)). And here are the versions that ship with each version of Ubuntu: * Precise: 2.1.11-1ubuntu1 * Trusty: 2.2.0+dfsg-5 * Vivid: 2.2.0+dfsg-6 * Wily: 2.2.0+dfsg-6ubuntu2 * Xenial: 2.2.0+dfsg-6ubuntu2 So it seems we always need to install a more recent version? Either via a PPA. But maybe Conda installs its own version, since zeromq is also a dependency of IPython. It could also be that the libzmq version doesn't reflect the real version of zmq. I also found the available versions of zmq in Debian: http://zeromq.org/distro:debian So maybe we can link it with each version of Ubuntu!
Would a vagrant box (virtualbox image) running a Jupyter server with everything setup would help here? Also, maybe http://mybinder.org/ would be helpful, but I don't know if they support IHaskell.
I would be glad to do that. Is that the one you are talking about? https://github.com/gibiansky/IHaskell
Could you explain more about this? Wikipedia says &gt; for functions f : ℝ^n → ℝ^m ... only n sweeps are necessary [for forward accumulation], compared to m sweeps for reverse accumulation. https://en.wikipedia.org/wiki/Automatic_differentiation However, I don't see why the partial derivatives for all n independent variables cannot dealt with all in a single sweep.
It can be, but you need to compute n partials to get the full derivative at the point in question. You can compute a _directional_ derivative in one pass for no multiplicative cost, independent of the number of inputs and outputs, but to compute the derivative at the point in question costs based on the number of inputs in forward mode. Consider a function of two variables x and y. you really have to compute z along with dx/dz and dy/dz -- We do so in forward mode by creating two infinitesimals lets call them X and Y, such that X^2 = 0, but X /= 0, and Y^2 /= 0 but Y /= 0 with X /= Y. data D = D !Double !Double !Double instance Num D where D a dadx dady + D b dbdx dbdy = D (a + b) (dadx + dbdx) (dady + dbdy) D a dadx dady * D b dbdx dbdy = D (a * b) (dadx * b + a * dbdx) (dady * b + a * dbdy) fromIntegral n = D (fromIntegral n) 0 0 etc. If you have 1000 inputs you do so in forward mode by creating 1000 such infinitesimals. The amount of work is proportional to the number of infinitesimals even if you do it in one pass. If you work with a dense vector like representation you pay for all of them -- even the 0s. If you work with a sparse Map-like representation you pay for the management of the maps and still have the same worst case where every result might depend on everything input. In reverse mode you deal with sensitivities instead of perturbations. You pay proportional to the number of results, not the number of inputs. When I have hundreds of thousands of inputs and a handful of results, which is typical in the problems I care about, forward mode slows me down by a factor of hundreds of thousands. Consider something like backpropagation in a neural network. I have the handful of outputs, but to do backprop I need to tune _all_ of the training weights using the sensitivities whether I do or don't like the answer the network gives me.
i didnt dive into the sources other than they're java and using maven, *shudder*. still, this is a cool looking project and idea
is frames a good fit for other high dimensional record sources other than CSV files, such as databases or web services? What if the structure of the data source is only accessible at runtime and not compile time? 
Here's a challenge: defend monad transformers with category theory that isn't as ugly as the transformers themselves.
Not really. I have CentOS server constantly running and I could install it there, but then autosave would save files on the VM and I would have to move them to desktop later. It's a minor issue, but I would prefer to run it on my desktop like other Jupyter kernels.
&gt; TH is "safe." From [Safe Haskell manual](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/safe-haskell.html): &gt; Haskell code compiled using the safe language is guaranteed to only access symbols that are publicly available to it through other modules export lists. An important part of this is that safe compiled code is not able to examine or create data values using data constructors that it cannot import. If a module M establishes some invariants through careful use of its export list then code compiled using the safe language that imports M is guaranteed to respect those invariants. Because of this, Template Haskell is disabled in the safe language as it can be used to violate this property.
In order of importance: * Everything that enables lenses: RankNTypes, FlexibleInstances, FlexibleContexts. * ExistentialQuantification, because it enables infinite sum types. * Syntactic conveniences: PatternSynonyms, TupleSections, LambdaCase, OverloadedStrings
`LambdaCase`
Vagrant can very easily sync directory with outside world, maybe it would remove the pain of saving it manually. Though I agree that running IHaskell natively would be the right solution in your case.
Control.Exception.catches ( http://hackage.haskell.org/package/base-4.8.1.0/docs/Control-Exception.html#v:catches ) seems to be exactly what you described. By the way, if you want to give special conditions for things like "list is empty", the more idiomatic way would be to return a Maybe Foo (where how it failed is obvious) or Either Err Foo (where Err describes the problem). Examples include lookup, which returns Just v for a found v, or Nothing if failure. It could be more idiomatic to throw an exception here for other languages, but not here in Haskell, because we have those cute little data types, and it works like a charm.
Can be especially handy when you decided to program circuits ([for instance](https://github.com/dangirsh/QuipperShors/blob/master/circuit.jpg)) in [quipper](http://www.mathstat.dal.ca/~selinger/quipper/)
You are wasting your time on bad product. Telegram should not be used - it's crypto is a joke. And it's partially closed source.
It's on the main page, as well as [here](https://github.com/wandernauta/viskell/blob/master/screenshot.png).
Yup, that's it.
It's in the README which github displays ([direct link](https://raw.githubusercontent.com/wandernauta/viskell/master/screenshot.png))
I've re-opened the relevant issue and added some pointers in a comment there: https://github.com/gibiansky/IHaskell/issues/528 The majority of the troublesome code is concentrated in one or two functions in one module, I _think_.