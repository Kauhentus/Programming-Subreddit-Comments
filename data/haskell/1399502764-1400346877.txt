&gt; Coming from Node.js, where npm install lib actually works makes me feel like As other's have pointed out, `npm` just glosses over all possible problems with the library because it doesn't have to compile anything, all it does is just place a bunch of a source files in the filesystem and register the package. If there is a problem with the library, you still have to deal with it in both languages. Cabal is just honest with you about the fact that there is a problem with the library instead of deferring it to a runtime explosion.
I think much of this is an illusion. Not that people encounter errors - that happens. Nor that people encounter fewer errors elsewhere - this is magnified by catching some errors earlier, but I don't know that there are not genuinely more incompatabilities encountered. The illusion, I think, is that it is cabal's fault. I think the root cause is the *structure of the dependencies of Haskell code*. Because we produce a lot of genuinely useful libraries that wind up used all over, we wind up with a lot of diamond shaped dependencies, and that is likely to go bad as things are updated at different times. I am not entirely sure how we address this, but if this is genuinely the problem then "make Cabal better" is not the solution. One possibility we take a collective look at the things that wind up at the root of diamonds in actual packages in Haskell, and figure out how to make those more stable in the important respects. Maybe factor out the most stable, broadly used pieces?
&gt; I've been trying for months to install GHCJS to no success. GHCJS is bleeding edge and not yet a normal package so it is understandable that it is harder then normal. Where you using the provided vagrant box. I have installed that on several different occasions, on mac and linux, with straightforward success. If the vagrant box is out of date currently it is sorta understandable that the developers are working on getting everything working out of the box with ghc 7.8.2 like a normal package, which will be awesome.
Thanks, I think you made it sound more appealing then the description I remember reading about the class.
There's an early-bird discount, and a further dicount if you book though Well-Typed directly.
`&gt; nix-shell -p haskellPackages.ghc --command 'ghc --make -O2 foo.hs'` `&gt; nix-shell -p haskellPackages_ghc782.ghc --command 'ghc --make -O2 foo.hs'` FTFY. :)
You don't generally do that, you do [this](http://www.reddit.com/r/haskell/comments/24t0sj/foldablemapm_maybe_and_recursive_functions_school/chc8zaw).
Interesting - any idea how much it is? If it's like the Haskell Exchange discount it may just be affordable. 
No, you are deluded. Sorry, but anyone working with `npm` knows it just works. There is almost never a "problem with the libraries" because it follows a simple concept: each time it downloads a library, it gets a snapshot of every single one of its dependencies in a sandbox. So, as long the library was working on the developer's computer, it will work on yours. Simple as that. The same could perfectly work for Haskell if Cabal followed that approach instead of sharing libraries, which is how it is done. The only cost is a few additional MBs that nobody will care about, for a huge quality improvement. Being interpreted language... cross-module inlining... nothing of that has nothing to do with issue or why `npm` works. At all.
I would love nothing more than to have a nice O'Reilly-level quality book on intermediate to advanced Haskell. Unfortunately the pool of authors who have the time and knowledge to write one is probably a very small group.
Why not just learn from `npm` and do what they do? When you download a library you get a snapshot of every single one of its dependencies as they were used originally. Simple as that!
Because that wouldn't work even a tiny bit. Let's say I have two libraries I depend on, A and B. A and B expose interfaces that operate on Text. I get Text from A, and I pass it to B. Now someone comes along and finds a more efficient representation for Text. Package A's developers have updated to the new version, package B's have not. Copying in the two different text libraries means that the things I get from A, I can no longer give to B. Kaboom. That's before even factoring in the increased code size, the increased build time, or the fact that we can now be bitten by bugs in *either* version of text. Note that restricting conflicts to *exposed* types is impractical because of GHC's (mostly desirable) penchant for inlining across modules. Now, once we have found a single set of packages from which we can build the individual package we are building, isolating *that* from other constraints is a huge win - but cabal now supports exactly that with sandboxes (and previously we could sandbox with cabal-dev).
Ah, I see where you're going. I didn't mentally limit it to just two cases however.
&gt; Ask for only what you need. Try and have small definitions with very general types and leave the glue on the outside This is really key. You don't have to anticipate in advance how general something needs to be. You can always apply a function to generalize a component later on only if you actually need the generality.
Okay, so YANAACP (Yet Another Not-Actually-A-Cabal Problem).
In my experience most interesting programs can be expressed as a compiler or interpreter. They take input (and as they become more mature that input generally becomes richer in expressivity), build an internal model, apply some meaning to it, transform the model, and produce your result. Take a window manager as an example, it translates input into some higher level notion, transforms windows on your screens, and then waits for more input. Therefore, I think it's accurate to think of xmonad as an interpreter. Web browsers, database engines, and video games fit this as well. I'm having a hard time thinking of programs where this mental model is not appropriate. Do you have some examples? 
That is wrong. When you are developing application C, you will have a snapshot of A and B in a point of time they worked together - or else your application would not work, to begin with. If someone updates A, this is completely out of your sandbox and won't affect you at all. How can you say something "Because that wouldn't work even a tiny bit." when at the same time it is actually working and being on Node.js?
&gt; or else your application would not work, to begin with. That's the point: you can't create a new application that uses both A and B. The library landscape becomes fragmented. (The PVP promotes a similar fragmentation in the Hackage ecosystem, but as long as all the packages involved are actively maintained, the libraries A and B will eventually become compatible.) &gt; How can you say something "Because that wouldn't work even a tiny bit." when at the same time it is actually working and being on Node.js? Javascript and Haskell are different languages. For example, in Javascript, if A and B depend on a Time library, it may actually be okay to pass a Time object obtained from A's dependency to B's dependency, even if those dependencies have different internal representations for Time objects—because an object's methods are attached to the object. (In fact you can use Time objects without a dependency on the Time library at all, you just can't create them.) The two versions of the dependency only need to provide the same interface. In Haskell, B will use functions from its dependency on values produced by A's dependency, and that won't work.
This is a definition of any program that does anything. Programs that are compilers or interpreters are a subset (and a small one at that) otherwise there would have been no need to invent those terms! Calling a tax program or word processor or SimCity a compiler or interpreter would seem to be the same as calling everything with moving parts a machine. It might be true but doesn't really help much. ----- They take input (and as they become more mature that input generally becomes richer in expressivity), build an internal model, apply some meaning to it, transform the model, and produce your result.
Cabal isnt perfect, though I feel like using it is pretty frictionless now since sandboxes have been introduced quite a while back. &gt; The same could perfectly work for Haskell if Cabal followed that approach instead of sharing libraries, which is how it is done That is plain false. Cabal has supported per project/build sandboxes too for quite sometime now (`cabal sandbox init`). The difference is the strict version freezing for the dependencies. However I don't think I would like this as a solution. I would hate to have to go and version bump all my dependencies and bump my own version every single time one of my many dependencies fixed an important bug. That propogates a lot of minor maintenance up the dependency chain. Something I've noticed with a lot of npm packages is poor maintenance. It looks like if an npm package is poorly maintained, the common solution is for someone else to just upload a new package with the same content, either just more uptodate or with some bug fixed. makes it really hard to choose the right package. Anyway, thats just my opinion. with hackage/cabal package the approach is to knock on the maintainers door if the following does not work: cabal sandbox init cabal install &lt;package&gt; And they will either fix it or if it turns out the maintainer has moved onto better things, you can generally start a motion to find a new maintainer (or volunteer yourself). In my personal experience these days its very very uncommon for me to find that `cabal sandbox init; cabal install &lt;package&gt;` doesn't work, and if it doesnt its a pretty reproducable problem for the maintainer to fix. Incidentally, on a separate note, ghcjs, haste and fay are all the hard ways to get JS out of Haskell, PureScript atm is a far more frictionless alternative at the moment (https://github.com/purescript/purescript). npm suffers from problem projects just like cabal does (but I generally feel that the Haskell community is better when it comes to maintaining packages) Give purescript a try if you havent already, I hope it helps.
My only and honest problem with purescript is that I have to code everything twice! Say I want to create a 3D game client and deploy it. How'd I do it? Create once with PureScript for web and once again with Haskell for native? How horrible could that be? (Which makes me wonder why there couldn't be something like a Haskell-&gt;PureScript-&gt;JS compiler. People talk about semantic loss ("Haskell Lists are just different!"), but sometimes it is tolerable. No?) Edit: missed a parenthesis. Comment wouldn't compile.
I agree entirely about compilers/interpreters and consider it in my toolbelt of "secret weapons", but there's still the step where you're folding in the pure and the impure code that is what drives people coming in to Haskell batty. Pure, traditional compilation where source code comes in one end and compiled results come out the other doesn't really exercise that.
Did you try with the new `--allow-newer` flag? Additionally, you may be interested in [this repo](https://github.com/glguy/GhcPkgUtils) and [this one](https://github.com/iquiw/cabal-delete). The way to prevent dependency errors is to keep nontrivial executables out of your main `ghc-pkg` tree. If you want to install new packages with an old package tree, just spin up a sandbox and go crazy. That's what they're there for. I only install the libraries I like to play with, and even those go to `--user`. Executables go to sandboxes (though I usually install alex and happy in the usual location since they have effectively no dependencies). `cabal-install` is not a package manager. It is a build tool and a dependency manager. `ghc-pkg` is the package manager. It's a bit flippant, but if you want to live on the bleeding edge, you shouldn't be surprised if you get a few cuts. To address your last point, cabal-install and ghc-pkg don't track executables, so they have no way to tell if alex or happy are installed. If those packages started providing, say, a tiny library of metadata (cf. ghc-paths), this could be remedied.
having used them all, i am pretty sure that julia will win this hands down. it's an attractive, unthreatening approach that "science programmers" will love. it supports a smooth transition between "easy" "dynamic" programming with a friendly syntax and no strange ideas, and typed code for inner loops that compares pretty directly to c (i recently got speeds that matched c for crc calculations for example). it's going to be "worse is better" all over again (not that i am complaining - i happily use it). [edit: changed "compiles" to "compares" because it doesn't actually compile to c; it uses llvm, the same backend as apple's clang.]
`cabal sandbox init`. Done.
 (╯°□°)╯︵ ┻━┻. The diagrams! Where are they? 
┬─┬ノ(ಠ_ಠノ)
Absolutely agree about Julia. Also, the fact that it's a [DARPA-sponsored](http://www.darpa.mil/OpenCatalog/XDATA.html) project provides a lot of validation for its use in the US government scientific communities (national laboratories, etc.) where Fortran is high on the list.
I actually would wish for clojure to win (i'm learning it now and having a lot of fun with it, despite the JVM [I love lisps]). However, I'll have to agree with Julia probably winning much for the reasons /u/andrewcooke stated.
How do I use nix-shell to do `nix-build --no-out-link -E 'let pkgs = import &lt;nixpkgs&gt; {}; hp=pkgs.haskellPackages_ghc782; local = import ./default.nix { inherit pkgs hp; }; in hp.ghcWithPackages (self : [local])'`/bin/ghci 
You can sandbox but it isn't default because that's not what Cabal was for. Shared libraries make a lot of sense in research settings with lots of small projects. Anyways, the community is working on the shared library idea. It might be really cool in the future. 
Digrams is a cool library but based off your previous comment it does not fit the simple bill. You have to be able to understand type families and kinds before you can begin to comprehend the api. Let alone its internals.
Like how Kolmogorov complexity is incalculable? :P
I've been trying to wrap my head around working with Cabal for awhile. Maybe you can help me out. To uninstall a package -- am I doing this right? `ghc-pkg unregister ` Then deleting the compiled code from ~/.cabal How do I check for outdated packages? How do I update all outdated packages?
i recuse myself from commenting on this article, and merely wonder who the unnamed author is. 
The byline does say "Lee Phillips May 7 2014, 8:00pm CDT".
please actually post a gist.github.com link of the transcript of your attempt and also your system configuration. if you don't provide us with information, we can't help you. Asking for help but not communicating *what* doesn't work means no one can ever help you. which is frustrating for everyone.
http://en.wikipedia.org/wiki/Program_synthesis http://research.microsoft.com/en-us/um/people/sumitg/pubs/synthesis.html http://www.cs.umd.edu/~saurabhs/pubs/popl10-syn.html
Clojure is a pretty poor fit performance-wise for numerically intensive computations. What's the basis for your opinion?
/u/luqui had an interesting project that deduced Haskell functions from unit tests using a proof search technique. https://github.com/luqui/Djest 
It's right below the article title.
In my humble opinion the numeric Python ecosystem is hitting the limitations of the language itself pretty hard these days. NumPy based programs just flat out don't compose all that well or efficiently, and it's a manifests as a lot of different problems. Arguably the whole ecosystem is a large experiment ( though very successful !) for seeing how far you can take this embedded domain language for arrays on top of a language that wasn't really designed for this use case. A lot of this is why I'm confident that Haskell will find it's niche in this space though. There's a lot of low-hanging fruit.
maybe. haskell has been around for as long as python has and look at the gap. one can argue, and i'd agree, that haskell is better suited for scientific programming, but where are the haskell libraries and success stories? while what you say about python might be true, it doesn't seem people agree or care. python is exploding in popularity. in my city, there are at least two scientific programming companies all in on python. at my company, python is quickly becoming one of the more used languages. then you have sage. the laff edX course, which was a computational linear algebra course, used python. mit has a few computer science courses on edX that use python. haskell is no where to be found in any of these domains. i would even wager that f# is likely to eclipse haskell in popularity if it already hasn't.
I would habe liked a simpler version of the merging too. But couldn't come up with one that has the same properties. ... Although ... `sort . concat` would probably have done it :-)
Not sure why the author thinks that fibonacci function in Julia is tail-recursive... also that function seems to be wrong anyway.
Make a CUFP submission!
Is there a quick explanation of what a semigroup is and how it differs from a monoid?
in a sense, we're already there with FPGAs. all someone needs to do is make a viable functional language to FPGA compiler. labVIEW already does this and shares some similarities with functional languages.
I think compilers could, and probably should be written as a server that maintains persistent state between invocations, but in general they are not. Persistent, long running programs require careful resource usage, and this can be tricky in Haskell. File handles can stay open, space can grow forever. These problems affect other languages, but the reasons solutions are different. 
"Beginning Haskell" [http://www.apress.com/9781430262503] has a couple of chapters devoted to studying functional dependencies, type families, natural numbers in the type level and in general many kinds of advance type-level programming. (Disclainer: I'm the author of the book)
R is horrible for anything except statistics. Even then it's a pretty terrible language. I hope it gets replaced too.
A monoid is just a semigroup with an identity element. That is, `Monoid` is just `Semigroup` equipped with an `mempty` method (with the obvious laws).
i don't want to bug you too much carter, but how is progress on your revolutionary haskell library?
It's basically a Monoid without the identity/neutral element.
I am working on a book that will contain chapters corresponding to this course. Still too early to tell when it will be ready though. 
Can you tell me in what way we could improve the description of the course to make it sound more appealing? 
I tried using Haskell for some signal processing a while ago. The proliferation of libraries that define their own matrix type, vector types, etc. made interoperability between libraries terrible. Haskell needs to agree on some basic types for signal processing and scientific stuff -- so an ecosystem grows, rather than many little ecosystems. The usefulness of one ecosystem of libraries would be far far greater than that of many tiny independent islands.
&gt; What's the basis for your opinion? He loves lisps.
Well what sort of diagrams? They come mostly from the OO world, so the ones which would make sense would be sequence diagrams or data flow diagrams but they don't cover the structure of the program. 
&gt; One general thread is that Haskell has less need for patterns and antipatterns due to patterns being more easily embedded in general libraries. That's a claim commonly made for functional languages, and one founded on a mis-understanding of what design patterns are, and what they do. The idea of a pattern has evolved a lot since the "gang of four" book was written. A pattern, really, codifies the advice that an expert programmer would give to a novice programmer in the form of mappings between less desirable solutions to a problem and more desirable solutions. It may be that once a pattern has been discovered and established in Haskell programming it can make its way into a library. One lesson from Lisp (where the same claims have been made), is that from a practitioner's point of view it isn't always wise to take every idiom and bundle it into a library (or macro, in the case of Lisp). For one thing, novices don't know where to look in the library for the preferred solution to their problem. What advice would an expert Haskell programmer give to a novice, and where is it codified? For example: &gt;Existentials are great features for certain cases, but they are often turned to at times where the strength-to-weight ratio is out of whack. That kind of good judgement about when to use a capability and when not is *exactly* the kind of wisdom that a collection of Haskell patterns would capture and make applicable. 
Thanks!!
&gt; Au contraire, those using NumPy know of Python's limitations. They can't work with large datasets without it exploding in memory use. And the solution is going to be to have a bunch of engineers trying to figure out *Haskell's* memory model? Go work with some atmospheric scientists sometime. They're using Fortran models in part because the code has been passed down from advisor to student for 30 years, and partly because it's an extraordinarily simple language to understand. You write a loop; it goes like stink. This is an enormously valuable property. They have neither the time nor the inclination to understand why they're not supposed to use `foldl` from the Prelude or to figure out what part of the program is keeping a pointer to the head of the list around. Purely functional programs should have an advantage in parallelism, but parallelism in the scientific world quite often still means MPI. You write a loop; it goes like stink *in parallel*. If forced to predict the future, Fortran (and C and C++) will continue to see use for some time, if for no other reason than inertia. Your bank still uses Cobol because it's silly to rewrite millions of lines of code to end up exactly where you started. No vulcanologist is getting grant money to take working Fortran code and port it to Clojure. Julia will gradually erode away the Matlab market share. It's syntax is pleasingly familiar for matrix math, and the rest of the language is better in just about every way. And Python will see a lot of smaller scientific work, because numpy/scipy/matplotlib/etc. are just so good when they're up to the task. If 10 years from now, Haskell and Clojure together get 10% of the market share in science that Fortran has today, I'll eat my hat.
Why do you say that about JVM based languages? Do they have some deficiency that is exposed by the types of work scientific computing involve?
I think this quote regarding Haskell is incorrect: &gt;Another obstacle to performance is the lack of motivation of CPU manufacturers to work on something as exotic as a Haskell compiler. Consequently it is unclear if ghc will be able to generate vectorization code, for example, for processors of interest. I recall a paper on the Haskell 2013 workshop about the Intel Labs Haskell Research Compiler which does some optimizations that GHC does not. Not specifically vectorization, but still relevant that Intel labs worked on this (even it is for research). 
It is being used for data modeling in big data, which depending on the angle one comes from, could be viewed as such.
Unless the standard runtime gets one of the many available alternatives integrated, people will start moving to other ones that can offer the same flexibility while being faster.
Lack of value types. Having the ability to properly define your data structures in hardware friendly way, can have huge impact in code execution. This is why value types and making unsafe an official package are so high in Java 8+ development items list. IBM J9 already has extensions available.
Only if a native AOT version of Clojure is made available, as currently the JVM lacks the required support for scientific computing. Currently commercial Common Lisps provide better support for such use cases.
I can't remember if it was tied to the Intel Labs research, but doesn't GHC have primitive support for SIMD intrinsics now? I remember reading the generalised stream fusion paper a while back that discussed implementation of auto-vectorization of `vector`.
&gt; Python is mentioned, but not as a "contender". That was my first thought, too. But, you have to consider that the whole Python scientific stack, from NumPy up is based on Fortran. If you install the stack through pip or easy_install, your computer needs to have a Fortran compiler. I do most of my data work in Python. Thus, most of my data processing is done using Fortran libraries.
I really wish it wasn't true, but according to my professor Python is my new favorite language. I really can't live without types... On the other hand, Haskell's math libraries aren't really compatible but at least it can immediately tell me that it isn't going to work. 
I vaguely recall some mention of GHC getting SIMD support, but my point was more on the fact that Intel is publicly interested in Haskell optimizations. 
There's a bunch of hot research happening on types for linear algebra so I think lib devs are afraid to choose early...
I was just pointing out another reason the quote is incorrect, as I'm fairly sure GHC 7.8 can produce some (very experimental) SIMD code.
Type declaration uses a colon just like in almost every other language out there. Haskell is the strange one in this case, and many regard this particular syntactic choice as a mistake. My understanding is that the early designers were caught up in the spirit of their times and assumed that most types would be inferred, and that list operations would be very common, so they optimized for that. I may be wrong there, though. Arrow doesn't make sense for a delimiter, because it's part of ordinary expressions. If I want to write a function that takes a type as argument and returns the type of a function from Int to that type, I can write something like `\x =&gt; Int -&gt; x`. This would be very confusing if it were `\x -&gt; Int -&gt; x`. Likewise for case blocks. In any case, the syntax is almost the same as Haskell - the truly weird things come when you need to understand the new ideas, not when trying to visually parse things.
Thank you very much for writing this up, it's an excellent read. As a beginner-intermediate Haskeller, this is the kind I'd love to see more of.
If you need full-fledged, real-world environment, maybe you can try to compile ghci using emscripten, or run ghc at server side and pretend it's running in a client (this will be costly). Or you can focus on teaching-FP aspect and write stripped Haskell interpreter. Whatever you do, your current project description seems too large to be doable unless you have lots of time and money to throw at it.
I think [this is the paper](http://www.leafpetersen.com/leaf/publications/hs2013/hrc-paper.pdf) you are thinking of.
To be fair, I mostly agree with you and sort of feel that this "stretching" of the terminology of pattern and antipattern is a bit of a confusing affair. I think it'd be nice to have some of that high level advice codified in more natural terms than the existence of libraries. I think that the original GoF tried to achieve this but did so within particular confines which gave rise to the idea of patterns. I also think that "pattern" and DRY are a bit at odds in a particularly meaningful way. But regardless of what you call them, it'd be nice to have some guidance on how to, say, feel out the power-weight ratio on existentials that's a bit better than "just try it!". The best I can think of (and they're quite good) are the Functional Pearls. I recommend them endlessly. http://www.haskell.org/haskellwiki/Research_papers/Functional_pearls
I didn't say I expected clojure to win, I said I wished for it to. The reason for that is simply that I enjoy the language and more users = more mindshare, which means the language is likely to progress more quickly. Performance and being on the JVM are the main reasons I expect it not to happen.
Between the two, I think SFML is a better foreign library, but the SDL bindings are lot more mature and better tested. I would also recommend that you check out GLFW-b. It's pretty simple and is a solid binding.
Yes, I would also recommend GLFW-b, I even wrote a small game[1] with it and liked its simplicity. [1] https://github.com/dan-t/layers
I vote for SDL. It's a tested solution, and recomended by actual videogames companies.
As a scientist who wants to do computations now, this is a very frustrating point of view. I honestly don't need my linear algebra libraries to have cutting-edge types (though obviously I would gladly take advantage of them if they were well-developed, standardized, and shown not to get in the way). It would be incredibly useful to just have basic math libraries that work and interoperate, so I could take advantage of Haskell's wonderful type safety and expressiveness in other parts of the code.
Hey, I would have a read of [this](http://www.vex.net/~trebla/haskell/sicp.xhtml#remove). It covers a lot of ground, and talks about where cabal can bite you in the ass. The thing is, cabal isn't a package manager, and so it kind of sucks at being one. It is however a pretty great build tool, as build tools go. Basically, to avoid issues, sandbox aggressively. I meant the hangout thing seriously though. If you are still having issues I'd be happy to get on a call.
Full conversation, since it seems to miss the nice conclusion http://ircbrowse.net/browse/haskell?events_per_page=50&amp;q=xQuasar &gt; &lt;xQuasar&gt; Alright thanks guys, I'll be back later to actually learn some haskell for real
It should be noted that GLFW pretty much only manages windows (and OpenGL contexts) and keyboard/mouse/joystick inputs since 3.0, while other libraries come with other functionalities like audio, 2D graphics, networking, ...etc, packaged in.
I've done ILP in uni (when it was still hip) and genetic programming for fun; always found it interesting. It has been the dream for 'enduser programming' for a long time. Just show the computer what you want by providing input and the expected output (this can be done in the way you show, but you can ofcourse also so this with GUIs, animations etc; doesn't matter) and get a working something. If that is not good enough, just show more examples until it's good enough. The problem basically is that we cannot really abstract over it; we need the computer to figure out what sub functions need to be created if you pass it something of a too high level. It very rapidly expands the search space until that's not feasible anymore. So how do you know what's small enough to feed into the algorithm without being a really good programmer who can actually write these snippets in the time it takes to type in the examples to learn from? A better type system would help narrowing the space by at least finding things that look like it; like you use Hoogle so then you could shortcut the search maybe by having a feasible starting point. If anyone would like to experiment don't hesitate to contact me; I really would like to work on this a bit; maybe some fresh perspectives can take it somewhere. 
"We are cooperating with you, you're just not aware that your goal is learning Haskell"
I like the cut of that person's jib :)
It's definitely not tail recursive, but why do you think it's wrong?
Ok, so I'm sure those functional pearls are good (if they live up to what I suppose is meant to be a reference to Bently's book), but, as described: &gt;Functional pearls are elegant, instructive examples of functional programming. They are supposed to be fun, and they teach important programming techniques and fundamental design principles. they don't quite do what patterns do, because patterns (are meant to) capture things that practitioners have found consistently helpful whereas this seems a bit too much like promoting what researchers and academics think should be the *right thing*.
Would it be possible to write an automatic transformation function function for a class CompatibleVectors a b where convert :: a -&gt; b, with as instances the cartesian product of all different vector types? That could help a lot.
I love the Haskell community.
Pre alpha, public but not yet announced and not yet on hackage. I'm amidst juggling my release checklist and work obligations this month. I'd suggest not looking at the current code unless you really wanna stare at some pretty abstract generics that are customized to array computation. Happily, I'll have sparse support from the outset
What, if anything, is going to happen with haskell and gmp/mpfr? I do a lot of work that requires arbitrary precision floats, and it's easy to use mpfr with python. But this page: https://code.google.com/p/hmpfr/wiki/GHCWithRenamedGMP is a little intimidating, and it's definitely not a good starting point if I want to convince my collaborators that learning Haskell is a good idea.
Please start marinating that hat of yours. 10% share of new analytical codes or new hpc codes? (Cause no one using legacy codes will ever move ever )
Haha yeah, that was pretty awesome :)
Now there's, not-yet-released-on-hackage, rounded: https://github.com/ekmett/rounded
&gt;We are cooperating with you, you're just not aware that your goal is learning Haskell I loved that part.
http://dev.stephendiehl.com/hask/
I don't think there's always such a huge divide between the two. Certainly there can be, but I use techniques and methods garnered from the Pearls quite often for practical programs. They also often turn into nice libraries, fwiw.
I learned Haskell by reading the Gentle Introduction to Haskell, a 9 page document on the syntax and semantics. And when I read it, I wished all introductory documentation was as succinct and clear. The effort/reward curve couldn't have been easier.
It doesn't have a high barrier to entry. How much math do you need to know? Not much. Just the stuff you shouldn't have slept through in your class on discrete math, and maybe a little extra on functors and monads (though you can pick up how monads work in about 10 minutes, if you just flipping work at it)
so what I tend to do is have a ~/build/&lt;package&gt; folder for all builds (haskell or no) and if I want to build an officially released haskell package I will make a sandbox in there and symlink. If it's something on git that I am taking the bleeding edge of, I have it in ~/git/&lt;package&gt;/ and sandbox there, and for my own code I have a projects folder and sandbox there. Sandboxes everywhere! I also have some fish scripts for managing blowing them away and re-installing dependencies. Some people will have sandboxes for executables that share a lot of dependencies, I don't bother because disk space is cheap. But basically, sandboxes are your salvation.
In that case, Fortran will always win. You can't install R, Octave, or Julia without using a Fortran compiler, and even many (most?) numeric c tools use libraries written in Fortran. Similarly, Haskell will almost certainly never have native, fast matrix-matrix multiplies. Instead, it will call into BLAS and LAPACK which are written in... drumroll... Fortran.
Great news, I look forward to buying it.
who said purely functional? I think a hybrid approach is actually pretty nice, and actually can express some nested / recursive algorithm formulations that aren't expressible in a flat *pure* array computation model Also, I don't think targeting grad students as the users is a valuable use of time, they'll grab anything that lets them science faster (which is the RIGHT priority for them, and what they should be doing I hope) :) Are you aware of ANY extant array libraries that actually treat sparse matrices as being first class, with all generic operations acting properly on sparse arrays? Are you aware of any extant array libs that actually make it easy for the end users to (sanely!) add newmatrix formats to exploit the symmetric/banded/blocked/hermitian? Are you aware of any extent libs where I can write high level routines that will exploit/preserve sparsity when the arrays have it, but will also work ok on dense matrices? Are you aware of any such extent libs that also make it easy to write good locality code, and where I can syntactically reason about memory locality quality of the code? Ok, now are you aware of any libs with all these features, PLUS a roadmap towards supporting low pain mixed cpu/gpu computation OR machinery for building high quality numerical backwards stable solvers for domain specific (non)linear optimization/minimizat problems OR non diskthrashy larger than ram memory mapped arrays as a first class option? Thats the feature checklist I want in my tools. I don't care what language, I want all of those features in a low complexity code base that lets me get back to focusing on math and data analysis :) Its not the language that counts, its having tools like that feature list :) 
Last question, I hope: How do you go about updating ghc and consequently uninstalling an older version of ghc? I assume it will require rebuilding packages, but it seems like a daunting task, especially with many sandboxes.
You might want to take a look at the ICFP 2013 programming contest ([link](https://research.microsoft.com/en-us/events/icfpcontest2013/)). It was a program synthesis challenge. If you scroll down to the "Let the games begin!" heading [here](http://icfpc2013.cloudapp.net/), there's a description of the challenge.
Everyone's goal is learning Haskell, whether they know it or not. 
On behalf of the JavaScript community I'd like to apologize recursively.
Hmm, Are those SDL bindings the same as [these](https://github.com/Lemmih/hsSDL2/)?
I've been making a game in haskell too, but I'm not a game dev. Here is my experience so far: - SDL v1 bindings are buggy and hard to use. The current maintainer recommends using the SDL2 bindings instead (the one you linked). However they are not complete. In particular, the audio ones are incomplete (I initially started looking at SDL for playing music). - For graphics, I've been using Gloss (which is built on top of OpenGL and GLUT). It has worked wonderfully for me so far. For sound, I use SDL v1 and those bindings are awful. I'm probably going to switch to OpenAL if I can figure it out.
cross-posting my comment from HN: &gt; it could benefit from the lens library, but last time I tried it did have problems with compilation with profiling information In my experience, template haskell does not play well with profiling. If you were deriving lenses with TH, you could try writing them out yourself. That might solve this issue. 
Actionscript is getting *more* popular? That seems hard to believe.
Well, I just upgraded to ghc 7.8 and I just rebuild the stuff I need to rebuild. So far it hasn't been too onerous. Cabal scripts need to be edited to take into account the new base version etc. I don't find it to be more challenging than say an upgrade to the .NET platform or the JVM.
I like using GLFW-b with the OpenGL bindings + a little OpenGLRaw. I haven't made it to audio yet. Stop by the #haskell-game irc channel on irc.freenode.net.
&gt; But don't we lose some parallelism as a result? If we allow more memory usage, and &gt; put processors in different threads. Processor 1 can accumulate some data then &gt; pass to processor 2, and while processor 2 is at it, processor 1 can continue to &gt; process. In fact, this is how java blocking queue is usually used. This can certainly be done, and I have tried it in the past. The monad-coroutine library has hooks for running multiple coroutines in parallel exactly for this reason, and the SCC package builds on top of that. Unfortunately, I was never able to get any performance benefit out of this. I have not tried if GHC 7.8 performs any better, but in my previous experiments the cost of synchronization always outweighed any gains from parallelization. In short, I'm not optimistic that a coroutine pipeline can be parallelized automatically. See also the Pipeline Parallelism section in chapter 4 of Parallel and Concurrent Haskell, it contains a brief discussion of the idea and some optimizations I never tried. 
Huh. So it seems like soon the JVM will be able to handle this sort of workload.
See Julia for better R.
Talking about factories and singletons and dependency injection is generally too abstract for beginners of OOP as well. Architecture strategies are generally built on top of a working understanding of the underlying ideas, and aren't targeted at true beginners, but intermediate programmers.
Have bought the book and have been reading it's chapter. It has been nicely written and I would recommend it for others. It even has a section on Idris! Thanks for writing the book!
I invite folks to help us hack on it. We need to get around to testing it on more platforms, figure out a way to get the build to work more consistently for more users, and we need to flesh out the interval arithmetic support some more. If you get interested, please feel free to send me patches, or hop on #haskell-lens and ask questions. Dan Peebles and I are usually around.
If you have a clean sandbox somewhere and run `cabal --config-file my/cabal.config install foo --dry-run`, you will get an installation plan with `(latest: ...)` marked for all packages that have newer versions. 
I'd rather have more vids like that than fewer, better rehearsed ones.
Let me take advantage of your familiarity with the design landscape to ask about the specific use case I want now: I need to work with large dense matrices with arbitrary precision floating point entries, do matrix multiplication, LU decomposition, Cholesky decomposition, extract eigenvalues/eigenvectors, etc.. What combination of libraries above lets me do this kind of thing (or will in the future)?
I dont know if Python is really that bad. See where the sci community is coming from: either proprietary and/or ancient. :)
Smells like a cabal shorthand for that would be nice :)
In case you decide to go on with SDL, you may want to consider [this higher level SDL2 bindings](https://github.com/Lemmih/hsSDL2) instead of the one you linked.
On behalf of the JavaScript community I'd like to apologize on behalf of the JavaScript community I'd like to apologize recursively. 
I'm not sure where exactly you got this command from, but if you want to compile a single, standalone Haskell program, you can simply write ghc filename.hs while `filename.hs` is the name of your Haskell source file. It will then create a file called `filename` which you can execute by issuing `./filename` in most shells.
`$` means your shell prompt: you don't type that.
&gt; Careful, you're entering "discard context to make a point" land. and what context is that? what i said is true isn't it? python has a better collection of usable libraries doesn't it? if you want to argue that python is easier to digest, then that's fine, but that's not the main reason for its popularity. it's popular because it's useful. &gt; Again, this is easy to say about a language that hasn't broken mainstream yet you have this backwards. python has taken the approach of "if you build it, they will come" by users spearheading the porting and creating of many libraries. the haskell community seems to take the approach of "we're researching the many ways to build it, but come anyway", which doesn't jive with new users since this compounds on top of the language's different approach from the start. haskell isn't mainstream because it doesn't have the useful libraries that python has. this is also why haskell stands in danger of losing a lot of ground to scala and f#. &gt; pop culture language, it appeals to the mainstream aesthetic, of course it's going to be popular this is disingenuous. billion dollar companies don't invest time, resources, and technology in a "pop culture" language, whatever that means. neither does one of the world's leading technological and research institutions (mit).
There are better alternatives anyway, like [Elm](http://elm-lang.org/).
Credentials aside they make good arguments, which effectively obsoletes the credentials. If they were using their past prediction record as an argument, this particular attack might have some value, as opposed to just being off topic.
On behalf of the JavaScript community I'd like to apologize on behalf of the JavaScript community I'd like to apologize ERROR: Stack overflow
Let me see if I'm understanding the difference. The original: foldl' f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = let z' = f z x in z' `seq` lgo z' xs Ties the demand of `z'` with the demand of `lgo z' xs`. Your version ties the demand of `z'` with the demand of `xs`. Your version will peel off one element of `xs` in each case of `lgo` and that in turn will force the previous `f z x`. In the original version, once the final value is demanded that cascades into forcing `f z x` at each step before recursing. Since `lgo` is spine strict in `xs` I wouldn't expect the difference to be observable.
i suppose. and that is a rather large assumption to make. there is also a difference between taking time to feel out major language features and libraries. the former requires that methodical approach, whereas the latter often requires expediency while retaining an extensible and usable approach. haskell lets the former's approach leak into the latter's a little too often.
I echo cookedbacon's sentiment, I know quite a few of us who are indifferent to seeing Haskell go mainstream. Whether or not people use Scala or F# more is kind of orthogonal to my concerns. There's a lively community here that continues to foster new ideas and libraries and the quality of these things matters more to me than the quantity of users or companies using it. The race is not to the swiftest.
Original, for reference foldl' f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = seq z' (lgo z' xs) where z' = f z x I am not sure why you think the original isn't tail recursive; it absolutely *is* tail recursive by Haskell's reduction rules. In particular, the last case effectively compiles to (in GHC-core) case f z x of z' { __DEFAULT -&gt; lgo z' xs } which says 'evaluate `f z x` to WHNF, then tail call `lgo` with the result and the rest of the list'. If `seq` isn't inlined, it's a tail call to `seq` which will tail call an entry into its second argument, with a heap allocation of the thunk for `z'` which should get GC'd.
Tail recursion is still important with strictness. That said, I think the OP is confused, since the standard definition of `foldl'` is already tail recursive. Edit: An example that demonstrates why tail recursion is still important: badLength :: [a] -&gt; Int badLength [] = 0 badLength (_:xs) = 1 + badLength xs Try applying that to a very large list.
I'm talking about the strictness of `(+)`. It wants both arguments evaluated before it will return. This is the root of the thunk chain problem. The solution is to make the function tail recursive with a strict accumulator.
Because the op have serious psychological issues about that.
Take my example and add `:: [a] -&gt; Int` to the types and you'll see that it's still not as strict as you need.
Any projects that are related to scientific computation, or physics in general that is worth investigating?
I'm confused. What exactly are you saying isn't strict enough? I do understand why strict accumulators are important, if that's what you're getting at, but they are not really the point of what I'm talking about. I'm just talking about the difference between guarded recursion (where the function returns before the recursive call has to be made) and unguarded recursion (where the recursive call must be made before the function can return, unless it's a tail call in which case the recursive call actually takes the place of the return).
&gt; But don't we lose some parallelism as a result? Coroutines are orthogonal to parallelism and/or concurrency. Coroutines **transfer** control from one subroutine to another within a single thread, but they do not increase or decrease the total number of threads. My preferred mental model is that you can have N threads running, each of which may optionally use coroutines internally to structure control flow. This is the model that the `pipes-concurrency` library uses, where each thread has a single coroutine pipeline running internally.
it isn't about going mainstream or the first in the race to me. it's about being *useful*. it seems everytime i have looked up a library, it's some half baked submit that was done by one person three to four years ago with no documentation. some of the libraries that do seem to get some attention are over designed and complicate simple things. then there are the research topics that most don't care about if they want to just use the language. even dan piponi, shares (at least at one point in time) my view that python is more useful than haskell.
Why does tail recursion matter here?
Thanks!
I don't really understand how to apply the expression "tail recursion" in Haskell and whether it's the right word for what matters, but don't you agree that a lazy length is best written with the 'guarded recursion' you mention data N = Z | S N lazyLength [] = Z lazyLength (_:xs) = S (lazyLength xs) but that strict length needs -- I don't know how to say it -- something like a strict accumulator, and something like what people call tail recursion: strictLength = go 0 where go !n [] = n go n (x:xs) = go (n+1) xs (or throwing `seq` in somewhere suitable)? So it seems to me we go back and forth between the two styles according to the material, avoiding things like `badLength`. 
GLFW is excellent. It just works out of the box on all platforms. And the API is modern.
Talking about a program as a compiler or interpreter emphasizes the task it accomplishes. I'm making a different point. I'm saying that the architecture of compilers and interpreters can be appropriately applied to lots of other type of programs. Whether people do this in practice is also a separate issue. I think of programs I write this way but I may be in the minority.
I want to understand this better. Do you have example programs (preferably in Haskell) that exemplifies the challenges you're talking about or that drive beginners batty?
ive had good experience in that channel too. I ask suoer noobish questions and they answer without too much rtfm and such. one of the most helpful channels in freenode imho.
&gt; What exactly are you saying isn't strict enough? To get rid of the stack overflow you need to reduce the accumulator to WHNF and that simply using a strict function like (+) is not sufficient. It sounds like we're in agreement on that point. I think I finally see what you're saying in your first comment. &gt; Tail recursion is still important with strictness. If I understand you correctly: The strictness you're referring to is the strictness of (+). Furthermore, you're trying to make the point that `badLength` cannot apply the intermediate WHNF reductions *without* a rewrite. Since you're going to rewrite it, the natural way is to use tail recursion and an accumulator, which you can reduce along the way. Thank you for clarifying.
Really well done. I laughed, I learned, sign me up!
Did you try applying my `badLength` function to a large list? It grows the stack like crazy (with GHC 7.8 this will just fill up your memory instead of failing with a stack overflow, so be careful). The reason is precisely that it's not tail recursive. The common claim that GHC doesn't have a traditional call stack is kind of bogus. You still end up with effectively the same kind of stack, even though it's allegedly for a different purpose. The cost model is basically the same as you would get if you just implemented laziness as a library in a strict functional language that has a "normal" runtime.
&gt; The reason is precisely that it's not tail recursive. Again, why does tail recursion matter here? I'm happy for you to prove me wrong, but you're going to have to do more than just say it over and over. Can you show me a version that does not grow the stack and demonstrate that this is due to tail call recursion and not some other change? &gt; The common claim that GHC doesn't have a traditional call stack is kind of bogus. You still end up with effectively the same kind of stack, even though it's allegedly for a different purpose. "Allegedly"? The pattern matching stack is not a call stack and afaik is not optimized via tail call elimination. Rather than stating your conclusion again, I would appreciate it if you would actually argue it.
Create an *embedded* DSL in Haskell. It gives you the best of all worlds. You get all the functionality of Haskell for free. While allowing you to restrict/specialize your custom language as needed. Google for "Haskell DSL" and you will find *many* examples of how to do it. Including examples of specialized embedded languages compiling to C and GPU specialized languages. Examples: http://hackage.haskell.org/package/atom https://leepike.wordpress.com/category/haskell/ (...many more...) http://www.haskell.org/haskellwiki/Embedded_domain_specific_language 
Haskell is useful to me. :-) But I mean the reality is that people are going to donate their time to work on projects they find interesting and scratch their itches, that's just the nature of open source work. Some people optimize their libraries for elegance, others for correctness or compositionality, some for attracting lots of users, others for some specific project at work. Analyzing the community as a whole really won't change anything, submitting patches to projects you care about will.
| we get a type error on 'simple' because the tuple type chances in the middle of the conjugation Not necessarily: λ let swap = let f (a,b) = (b,a) in Iso f f λ let under (Iso f f') g = f' . g . f λ let simple = under swap (first show) λ :t simple simple :: (d, String) -&gt; (d, String) 
as of GHC 7.8.1 Control.Category *is* [polykind capable](http://www.haskell.org/ghc/docs/7.8.2/html/libraries/base/src/Control-Category.html#Category)
&gt; Can you show me a version that does not grow the stack and demonstrate that this is due to tail call recursion and not some other change? This was added after I read an older version of your comment, so I did not address it in my original response. What was wrong with random_crank's tail recursive example? Do you claim there is some other change affecting it? You seem to be trying to draw a distinction between the tail recursive and the strictness (I assume you are talking about the bang pattern, although that's not actually the strictness I was talking about earlier). Perhaps you believe that it's the bang pattern that is making it efficient and not the tail recursion? Well, it is true that the bang pattern is necessary in order to actually reap the benefits here, but you cannot really isolate it from the tail recursion anyway, so I really don't know what you're asking me to do.
Idris is a fairly large project, but we have quite a few relatively small things that need doing. If you're interested in programming language implementation, then drop on by.
I don't even think this is the best example. This dude was very openly just trying to mess with them. The best examples are when people come in and more subtly hate on the useful abstractions in Haskell, or purity. That's been handled very well by the community as well.
I think I just don't see any point in calling them something different. They are literally, in every possible way I can think of, the same. Once could say this language focuses too much on operational details, but then... we *are* talking about operational details. As soon as you mention "stack" or "tail recursion" or anything, you're talking about implementation details. This isn't really about laziness. It's just about function calls; that's how thunks work. The stack really does have traditional stack frames just as you would expect with "normal" function calls. I think acting as though GHC does this magical special thing and the standard terminology doesn't even apply is harmful. It's execution model is not so revolutionary that it can't be explained to a "normal" programmer who understands what tail calls and stack overflows are. Edit: When I originally wrote this, I was getting pretty tired, and this thread had inadvertently taken on kind of a negative tone already (which thankfully I think we're over with now). I believe this encouraged me to take a more extreme position than I normally would. I don't actually think it's *so* bad. It is true that the way GHC maps your code to the machine is pretty different from most other compilers.
Not a game dev either, but I've played with a few things: * [gloss](https://hackage.haskell.org/package/gloss) - very simple and intuitive API, but seemed to only get me so far -- eg, text rendering is awful. The Coordinate system and transformations seemed a bit foreign as well, but your mileage may vary. * [sdl](http://hackage.haskell.org/package/SDL) and [cairo](http://hackage.haskell.org/package/cairo) (through [Helm](http://helm-engine.org/)) - worked really well. Unfortunately, SDL1 is notorious for being a pain to compile on windows. Cairo wasn't fun to install on Windows, but I at least got it installed. * [SFML](https://github.com/SFML-haskell/SFML) - As it's basically a 1:1 mapping of the C (not C++) API, it felt a bit too verbose and imperative for my liking. It also seemed to work fine on Windows. * [hsqml](http://www.gekkou.co.uk/software/hsqml/) - Just starting, but it at least compiles on Windows without a lot of effort (only real modification I had to make was to update my PATH variable). I'm still trying to find out the best way to create objects dynamically without having to resort to javascript (or a hs2js compiler). * [free-game](http://hackage.haskell.org/package/free-game) - Felt like a more polished version of gloss for the most part, but I found it a bit cumbersome trying to accumulate state. * [opengl](http://hackage.haskell.org/package/OpenGLRaw)/[glfw-b](http://hackage.haskell.org/package/GLFW-b) - I'm not a game programmer, so having to write fragment and vertex shaders seemed like a bit much for the simple games I'm writing. That said, the API easily maps to the C API, so most books and tutorials that I found on the internet seemed to translate rather trivially. Verdict? Each offers something I like, but none seem to offer *everything* I want. I'll probably continue to experiment with hsqml and check out [hsSDL2](https://github.com/Lemmih/hsSDL2) as well. The libraries that felt too imperative for me could probably be wrapped in a functional shell to make them more appealing, but I am neither experienced nor patient enough to go that route. Maybe some day... BTW, I'm not a Windows user, but my brother, who I force to play my boring creations is, so it's sort of a requirement that the library compile easily on that platform.
`+RTS -K8m` will give you back a 8 MB stack on 7.8.
Are you in ghci? To compile your program, run ghc directly from your shell (what you get when you first open your terminal window), not from inside ghci.
In light of [Brodnik et al 1999](https://cs.uwaterloo.ca/research/tr/1999/09/CS-99-09.pdf) I'm dubious that EASTL's vectors actually offer amortized O(1)-time snoc. Sure, you can get the memory overhead down by sacrificing time, but that seems to miss the point: the OP was wanting fast insert/append...
`wrap` comes from `construct` at the identity functor, but where does `null` come from? 
Yeah!
While with effort you could certainly make some progress, at this stage I would propose to consider writing server-side code in Haskell, and client-side code in Haxe/OpenFL (which can compile to android, ios, js..). Haxe is written in/inspired by ocaml. You will still miss a few things, but it is far better than any other options in my honest (and very subjective and possibly under-informed) opinion. Note that at one point I tried to write a Scala game for Android, but idiomatic Scala was very GC-heavy. I would expect the same for Haskell. And for mobile that would not fly nicely I expect (of course this is apples to oranges, since Scala uses the JVM GC, while Haskell app would use GHC's GC).
All publicity is good publicity =)
Why refuse to solve a problem that cannot be solved for the general case? Just have a failure mode after enough cases are enumerated where it tells you to fall back to being a bit more explicit.
Can you make your example available on http://lpaste.net/ ? If you got everything right with a main module you should be able to compile from the prompt with: ghc --make yourmainmodulename.hs
&gt; Really? How? When does this happen? Have a look at the operational semantics outlined in the below paper. "Notice that the call to f is a tail call. No continuation is pushed; instead control is simply transferred to f’s body". I'm not sure I'd call it a "tail call optimization". It's just that all function calls are tail calls and evaluated without pushing a return address onto the stack. Instead it's *case* which pushes a return address onto the stack. Page 4, section 4.1 Making a fast curry Push/enter vs eval/apply for higher-order languages March 17, 2004 Simon Marlow and Simon Peyton Jones http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.134.9317&amp;rep=rep1&amp;type=pdf 
I am aware Haxe was written in Ocaml but inspired by ? It says on the Wikipedia page as well, but how ? I don't see much , beyond the trivial, which has been inspired by Ocaml in that language. Agree with the GC, but that depends on the type of game. 
"amortized" is the key phrase, that paper seems to be about implementing structures with worst-case O(1) time per operation. Assume allocating n bytes takes no more than O(n) time. Then, for some structure T of size O(1), this algorithm wastes O(n) space and takes amortized O(1) per operation: class Vector&lt;T&gt; { T[] buf; int size; int capacity; } vecInit(Vector&lt;T&gt; v) { // O(1) v.buf = alloc T[kInitialSize]; v.size = 0; v.capacity = kInitialSize; } // O(1) amortized, O(n) worst case vecSnoc(Vector&lt;T&gt; v, T x) { if(v.size == v.capacity) { // O(n), but executes only once every n snocs newCap = v.capacity * 2; newBuf = alloc T[newCap]; // allocate new array copy&lt;T&gt;(newBuf, v.buf, v.size); // copy old array elements into new array free v.buf; // not used any more v.buf = newBuf; v.capacity = newCap; } // O(1) v.buf[v.size] = x; v.size = v.size + 1; } 
of course interacting with the outside world might still need tests. proofs are kind of exhaustive tests. for every possible value the proposition _must_ hold. that in general cannot be achieved with unit tests.
Here's a naive version which uses lazy I/O (which I'd like to avoid) while having a comparable run time to the Python version, but has has a significantly worse memory footprint (allocates 78MB, that's even more than the input and output file sizes added together): import qualified Data.HashMap.Strict as HM import qualified Data.ByteString.Lazy.Char8 as BL import Control.Monad import Data.Monoid main :: IO () main = do lines' &lt;- fmap BL.lines BL.getContents forM_ (HM.toList $ HM.fromListWith (+) [ (l,1::Int) | l &lt;- lines' ]) $ \(l,cnt) -&gt; do BL.putStrLn ((BL.pack $ show cnt ++ "\t") &lt;&gt; l) and here's the stats: $ /usr/bin/time -v ./hist2 &lt; testdata.log &gt; out2.log Command being timed: "./hist2" User time (seconds): 0.39 System time (seconds): 0.03 Percent of CPU this job got: 100% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.43 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 78476 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 0 Minor (reclaiming a frame) page faults: 19742 Voluntary context switches: 1 Involuntary context switches: 43 Swaps: 0 File system inputs: 0 File system outputs: 3704 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 Page size (bytes): 4096 Exit status: 0 
So it's kind of like compile-time QuickCheck when it comes to pure functions? If it is, why isn't this done in practice a lot more?
Are you sure? The instance I see is for instance Foldable (HashMap k) which means (if I get it right) I wouldn't be able to access the key during traversal, but only the values (i.e. the `Int` counts)
Good point. I've reformatted the entire file and added the missing lines from the log you posted.
t
You might be able to use `HashMap`'s `traverseWithKey`.
I've just decided to teach this course again in July: https://skillsmatter.com/courses/504-well-typed-s-guide-to-the-haskell-type-system
Shouldn't be much different than implementing the actual heap sorting: http://stackoverflow.com/a/2186785/383508
`uniplate` / `biplate` is probably the easiest to get into. Someone will be along shortly to show how you can do it in -10 lines of code using `lens`. There's also `SYB`, `GHC generics`, `compos`... Basically you want to look up generic programming.
I'm expecting Edward any minute now, once he gets home from the post-lambdajam pissup :V But yeah, I meant more along the lines of using catamorphisms/anamorphisms/paramorphisms/hylomorphisms/histomorphisms/etc :)
The read-ahead buffering can be taken care of by using something like `pipes` or `io-streams`, doesn't it? But what I'm more worried right now about is the memory footprint. Python's `dict()` data-structure not only seems to be fast, but seems also much better in terms of memory consumption. Is there no hope for coding up a similarly lean data-structure in Haskell? Maybe something for the `ST` monad (however, that might not help me in this case, as the point of this challenge is to construct a multiset while stream-consuming `/dev/stdin`)
The point is, if you don't want to write the boiler plate, you will have to use the techniques encoded in these packages because they give you the tools to view an ADT generically and thus only transform the bits you care about. A recursion scheme doesn't attempt to solve that problem at all. EDIT: as an aside, if you are going the Fix route you might want to look at pattern synonyms to cut down on the noise.
Hmm, alright then. Also, oh yeah, I have totally been meaning to look into pattern synonyms. Thanks for reminding me!
You are right of course. `Data.HashSet`, however, seems to have the instance you actually need (The hash set is implemented as a hash map with `()` values).
Regarding the lazy IO point, you can use `Pipes.Prelude.fold` to reduce `Pipes.Prelude.stdinLn`, so that narrows the problem down to writing an efficient left fold for histogram building: import Pipes import qualified Pipes.Prelude as Pipes main = do hist &lt;- Pipes.fold step begin done Pipes.stdinLn print hist where step = ??? begin = ??? done = ??? Also, once somebody figures out how to do this you can package the logic into a `Fold` from the `foldl` library to make it reusable.
This seems to perform similarly to the Python version on the same testdata but I'm on Windows and I don't have time : {-# LANGUAGE OverloadedStrings #-} module Main (main) where import Data.Conduit import qualified Data.Conduit.List as CL import qualified Data.Conduit.Binary as C import System.IO (stdin, stdout) import qualified Data.ByteString.Char8 as B import Data.Maybe import Data.Monoid import qualified Data.Trie as T main = do trie &lt;- C.sourceHandle stdin $= C.lines $$ CL.fold count T.empty CL.sourceList (T.toList trie) $$ CL.map (\(w,c) -&gt; B.pack (show c) &lt;&gt; "\t" &lt;&gt; w &lt;&gt; "\n") =$ C.sinkHandle stdout where count t w = T.alterBy (\w _ mc -&gt; Just $! maybe 1 (+1) mc) w 1 t I'm pretty sure you can do better (I have almost never used Conduit before, at least not in the last year). On the other hand, we're actively competing with the most C like part of Python here... 
echo "red blue green red" | xargs -n1 | sort | uniq -c
Here's some building blocks that make your program pretty easy: * [`interact :: (String -&gt; String) -&gt; IO ()`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:interact) - The interact function takes a function of type String-&gt;String as its argument. The entire input from the standard input device is passed to this function as its argument, and the resulting string is output on the standard output device. * [`words :: String -&gt; [String] `](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:words) - words breaks a string up into a list of words, which were delimited by white space. * [`sort :: Ord a =&gt; [a] -&gt; [a]`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-List.html#v:sort) - The sort function implements a stable sorting algorithm * [`group :: Eq a =&gt; [a] -&gt; [[a]]`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Data-List.html#v:group) - The group function takes a list and returns a list of lists such that the concatenation of the result is equal to the argument. Moreover, each sublist in the result contains only equal elements. * [`length :: [a] -&gt; Int`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:length) - returns the length of a finite list as an Int. * [`head :: [a] -&gt; a`](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:head) - Extract the first element of a list, which must be non-empty.
Thanks. This make sense to me, running coroutines in parallel only make sense in certain scenarios, and it needs to be assessed case by case. I'll look into the materials you mentioned.
Yeah, coroutines are just computations organized in a specific way so that they can be piped together. I think it is all up to the `pipe`s to decide how to run it(single thread vs multiple threads), much like different interpreters can run same piece of code in different ways. What I was trying to understand is if running a stream of coroutines in multiple threads automatically has been explored by some lib already. Because it does seem there are some unusual cases where this can bring a certain level of performance benefit. I do find the mental model you mention very nice and simple. This should be the way how people organize pipes concurrently, if they do need that concurrency.
&gt; If it is, why isn't this done in practice a lot more? Because historically speaking, this sort of thing has been a "requires at least an undergrad degree in math or grad degree in computer science" sort of thing... what you get with what is today considered a bachelor's degree in computer science is not enough to work with this stuff very effectively. (It does provide a platform you can bootstrap yourself up from if you like, but it is not, itself, enough to do this work.) Much of the work the dependent types community is doing right now seems to be focused on making it easier, and I think some exciting progress is being made... but it's very, very bleeding edge. In another few decades this may be just How Programming Is Really Done, but we're not there now.
`sort -u` does the sort and the uniq in one go. Eh... nm... that doesn't count...
`map` might come handy, too.
I respect your viewpoint but I can't say I agree
t
&gt; But guess what? &gt; They’re doing it. Wrong. I'm doing double takes here and I'm not sure what this means. Is it a bad case. Of using punctuation. For emphasis? Is it wrong to believe that they are doing it?
Multiplate is great for ASTs, too (anything with loads of mutually recursive datatypes, really)
I have not, no. How would they help?
I'm still not following why you think `foldl'` is written the way it is (or mmirman's way). Certainly this way of talking in widespread http://book.realworldhaskell.org/read/profiling-and-optimization.html#id678593 I wasn't thinking the issue was the actual underlying optimization strategy, but whether we sometimes benefit by writing in a tail-recursive style -- or the `foldl'` style -- in a suitably strict environment. For this it is enough that the compiler can recognize the strategy and make something of it. 
It's not a free theorem or anything, but from the example it looks like `construct` is supposed to give a non-`empty` result even on `empty`. In that case, it's something like `fromJust (construct Nothing)`. Also, Identity isn't Applicative, so it's not guaranteed there's a wrap either.
This is great and I actually didn't know this. I was using 7.6.3 but using 7.8.1 doesn't seem to help. It may be good for constrained categories as /u/deltaSquee mentioned, but looking at the definition class Category (c :: k -&gt; k -&gt; *) it's kind polymorphic in the first two arguments, but we really want it to be polymorphic in the kind function type of c, since in our case Iso has kind * -&gt; * -&gt; * -&gt; * -&gt; *. there's no way to instantiate k to give this form.
Unity 3D is moving mainly to JavaScript, isn't it? Increasingly JavaScript is the assembly of the 21st century and web gl is fine for games. That said, google pinacl might be another good target at some point in the future.
http://hackage.haskell.org/package/multiset
If we ever get bos on the Haskell Cast maybe I'll ask him why the book says that, since GHC does not have that kind of stack or do that kind of optimization *of Haskell expressions*, and certainly Haskell the langauge does not specify it. (I'd probably be more interested in talking about Facebook and Wreq though.) For reference: foldl' :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b foldl' f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = let z' = f z x in z' `seq` lgo z' xs foldl' is written this way because of lazy evaluation. Lazy evaluation basically amounts to two things: 1. [Outermost evaluation](http://en.wikipedia.org/wiki/Evaluation_strategy#Normal_order). 2. Sharing of previously evaluated values. In the form `genericLength (_:xs) = 1 + genericLength xs`, outermost evaluation reduces the (+) before it reduces the arguments. The recursive call to `genericLength` is in an argument, which causes the thunk chain problem that leaks space. In the form `lengthAcc n (_:xs) = lengthAcc (n+1) xs`, the outermost redex is `lengthAcc`, so it is evaluated immediately rather than adding to the thunk chain. The latter is the form that `foldl'` uses. This "tail recursive" structure, when combined with a strict accumulator (that's the `seq` in `foldl'` and the use of `(+)`, which is sufficiently strict) is what allows `foldl'` to operate in constant space. My point is that while this structure is tail recursive, this is of secondary importance to the behavior of lazy evaluation in understanding why `foldl'` doesn't leak space. There is no tail call optimization going on here of the kind you might expect. So yes, GHC uses a "pattern matching" or "evaluation" stack. If you want to call this a "call stack" I suppose you can, although it isn't really like what most people expect a call stack to be. Sometimes, during function application, GHC is able to use a tail call optimization (really just a goto) to avoid adding another frame to this stack. This optimization is *completely unrelated* to whether the expression being evaluated is tail recursive. The tail call that's being optimized is *in GHC*, not in your Haskell code. So if you want to point to this and say that GHC does TCO, I guess you can. But I don't think this is what people who are familiar with TCO in other languages would expect you to mean.
I don't know. Is it? I never heard any of that, to be frank?
Attribute grammars give you a nice way to express computations over trees (and you get efficient traversals for free). Just keep in mind, they are a special case of catamorphisms and as such have the same applicability/limitations of other catamorphisms. The nice thing for you, would be that the evaluation rules (if I recall correctly the term is 'semantic functions') for an attribute grammar are "ignorant" of the traversal of the tree. So you'd just program what to do for each constructor of `ExprF` along with the rule(s) for combining synthesized and inherit attributes (if you have any). Many things in a compiler can be expressed with attribute grammars. The Utrecht Haskell compiler is based on attribute grammars. They made a preprocessor/dsl for expressing them. This article is a good introduction and overview: http://www.haskell.org/haskellwiki/The_Monad.Reader/Issue4/Why_Attribute_Grammars_Matter Due to the data type encoding you use, I would recommend instead a paper submitted to ICFP this year (I don't know what papers made the cut, maybe it will be in the proceedings?). They focus on trees with sharing, but it works for trees without sharing to, and they use the same data type encoding as you use: http://www.diku.dk/~paba/pubs/files/bahr14icfp-preprint.pdf The authors also have code on github: https://github.com/emilaxelsson/ag-graph I think their library would be a natural fit for your catamorphisms.
I have a project that I'm working on that would be easy for someone new to haskell to help work on and will be hugely useful to community when it's done, pm me if you're interested.
On my system: &gt; time ./fdsared &lt; /usr/share/dict/words &gt; out-py.log takes 0.912 seconds^*. For comparison, `challenge.py` above takes 0.697 seconds. I was able to put together a variant on yours which uses lazy `ByteString`s and `HashMap`s which improves somewhat on your time, and is a bit shorter. It takes 0.777 seconds. import qualified Data.ByteString.Lazy.Char8 as B import qualified Data.HashMap.Strict as H import Data.List (foldl') type Map = H.HashMap B.ByteString Int count :: B.ByteString -&gt; Map -&gt; Map count l = H.insertWith (+) l 1 collect :: Map -&gt; IO Map collect m = do contents &lt;- B.getContents return $ foldl' (flip count) m $ B.lines contents printItem :: B.ByteString -&gt; Int -&gt; IO () printItem line n = putStr (show n ++ "\t") &gt;&gt; B.putStrLn line printMap :: Map -&gt; IO () printMap = mapM_ (uncurry printItem) ∘ H.toList main :: IO () main = collect H.empty &gt;&gt;= printMap ^* All times are averages of 5 consecutive runs.
1. current version is shipped with ghc 2. repa actually uses dph for its work 
I went into a bit more detail [here](http://www.reddit.com/r/haskell/comments/252wb3/a_leak_free_and_tail_recursive_foldl/chdu2yn?context=1). If I'm not mistaken, the TCO you're referring to is an optimization of a tail call *in the RTS* as described [here](http://www.reddit.com/r/haskell/comments/252wb3/a_leak_free_and_tail_recursive_foldl/chdhynf), not a tail call in the Haskell expression being evaluated. I think that's a significant difference. I'm not trying to act like GHC does a magical special thing. I'm trying to act like GHC does *lazy evaluation*, and the semantics of this (specifically outermost, leftmost redex reduction, a.k.a. normal evaluation) are directly relevant to the question of "why doesn't foldl' leak space"? The TCO done by the RTS on function application is not any more relevant here than when it happens on an expression that is not written tail recursively. I think it's inaccurate to suggest that tail calls in Haskell expressions are optimized using tail call elimination. They are "optimized" using lazy evaluation because the tail call structure causes the recursive call to be the outermost redex.
I am pretty sure you can use GeneralizedNewtypeDeriving to derive most of those instances for you, btw :) 
or import Data.List import Data.Ord (comparing) count :: Ord a =&gt; [a] -&gt; [(a,Int)] count = map f . group . sort where f xs = (head xs, length xs) main = mapM print $ sortBy (flip $ comparing snd) $ count $ words "red blue green red" to keep it simple. But, i think its not optimal to require an Ord, where only an Eq is needed. 
Unfortunately, stuffing data types in and out of `Fix` is mostly unavoidable when using recursion schemes this way. There are a few steps you can take to mitigate the associated boilerplate: if you enable `ViewPatterns` and write an `unFix` function (`unFix (Fix f) = f`), you can use it while pattern matching: `simplifyAdd (unFix -&gt; Sum xs) s = ...` As others have pointed out, pattern synonyms are even more powerful than view patterns. If you use Patrick Bahr's [compdata](http://hackage.haskell.org/package/compdata-0.7.0.1) package, you can use Template Haskell to generate constructors that have fixpoints already applied. (though compdata calls its fixpoint `Term` rather than `Fix`). `simplifyAdd (unTerm -&gt; Sum xs) s = iSum (s:xs)` compdata is really great, though a bit intimidating at first. `uniplate` is also good for this sort of task, though I prefer using morphisms with functor fixpoints. It seems less magical. 
Javascript is an option, but they also use C# (via mono) and something called Boo. I think C# is recommended for anything complex or CPU-heavy.
repa doesn't use DPH, but they're both by the same group of researchers.
you'll hear about whats new in repa after they submit some papers this month i suspect :) its a research project thats actually usable first, not a general purpose lib :) 
For funsies, he could consider making one from scratch without using monad :) (Free and FreeT allowed of course :) )
Great job! Already wasted an hour playing it - still going. Splendid writeup too!
Very nice writeup.
Assuming you're the author, is there a reason you used ports for the score display/new-game button instead of implementing them in Elm too?
Ah, I messed up about 90% of that! Just tested again, this works: First of all, yeah, you need an empty sandbox (I even said so ;) ) $ cd ~/proj $ cabal freeze $ mkdir ~/tmp &amp;&amp; cd ~/tmp $ cabal sandbox init $ ln -s ~/proj/cabal.config . $ cabal install ~/proj --dry-run 
I suppose I could argue that the grid display is something that lends itself to elm, whereas the score and new game button, being comparatively static, are easier to just put directly into the HTML document. The truth is however that it was mainly so that I could gain experience with using ports. *changed 'functional approach' to 'elm'
You are going to get exactly zero help with this kind of question. Please at least pretend like you care about learning something. Give us you best effort: explain what you don't understand.
FORTRAN has a wide codebase. I think it was Admiral Grace Hopper who made FORTRAN and COBOL when in the Navy they claimed computers could only do math and couldn't run programs and automate tasks. FORTRAN was for scientific work and COBOL for business work. Haskell is very hard to learn, but it is growing more and more popular. A friend got me into it, me being a programmer and him being a mathematician, he can do math but doesn't have much programming experience. I learned over 37 different languages in the past 30 years or so. Haskell will become number 38 if I do say so. I started out long ago in the early 80s with BASIC on Microcomputers before IBM got into the game and changed it. I got another friend who runs OS/2 in QEMU and has FORTRAN and C/C++ compilers for it. He made me an OS/2 1.21 virtual machine with many different FORTRAN compilers in it. I also got an OS/2 2.0 virtual machine from him. It would be cool if someone ported Haskell to older OS/2 versions just to see how OS/2 would handle the same programs as modern Windows and Linux. Then it can replace FORTRAN. Edit: https://mega.co.nz/#!UFQlxbCB!D-WiS2jnBOUrPzCeTjQhPAsXkLFVtZArW3_2RyLIAkM A link to his OS/2 FORTRAN virtual machine for VirtualBox, for those interested who have an OS/2 license to use it. There are still companies that run FORTRAN code on OS/2 1.21 because of incompatibility issues with modern machines. If Haskell is to replace FORTRAN, it has to address those issues and allow FORTRAN code to be converted to Haskell so we don't have to use virtual machines to run legacy code.
It's a shame you have to have the `Id` constructor. It'd be nice to be able to just construct `id` using the `id`s of the two categories you're taking the product of.
the problem is not how to attack this work, I made 4 problems, I just need the latter, please it is very important
Using `lens`, if you didn't bother with the base functor, and just used the direct recursion pattern (sans annotation), then you could use something like summands (Add a b) = summands a ++ summands b summands (Sum xs) = xs &gt;&gt;= summands summands x = [x] -- ... rewrite $ \xs -&gt; case summands xs of [_] -&gt; Nothing ys -&gt; Just $ Sum ys would recursively rewrite `Add` and nested `Sum`s into `Sum` throughout. With the noise caused by using an explicit fixed point you can still get there. You can put a deriving (Traversable) on the base functor. then instance Traversable f =&gt; Plated (Fix f) where plate f (Fix xs) = Fix &lt;$&gt; traverse f xs should just work and you can repeat the above with the extra Fix noise.
Unity3d compiles to JavaScript now, which is preferable to using a plugin. I'd try what /u/tluyben2 [suggested](http://www.reddit.com/r/haskell/comments/2542iq/could_a_plugin_be_made_to_run_haskell_games_on/chdibz5). Much better to run natively in browser than requiring a plugin. A plugin download creates a bit of a barrier of entry for players.
can you put in animation
Aunque el tema trasciende el problema de tarea, usted puede encontrar alguna información [aquí](https://www.fpcomplete.com/user/bartosz/understanding-algebras) si usted lee con cuidado. Los ejemplos de código deben, al menos, le dará algunas ideas sobre la forma de abordar el problema. Edit: No trate de entenderlo todo, sólo se centran en la forma en que 'fmap' y 'eval' interactúan. Además, este [artículo](http://en.wikibooks.org/wiki/Haskell/GADT) trata el mismo tema en un nivel más básico.
I think the original game uses CSS transforms, which aren't part of the core graphics stuff right now. [elm-d3](https://github.com/seliopou/elm-d3) seems to be a promising approach to making nice animations for this kind of thing though.
Holy shit unity3d is awesome. Now if I could just program stuff using Haskell and access from unity3d...
Every monad is applicative, ergo identity is applicative (using `pure = return; (&lt;*&gt;) = ap`). But also, identity is trivially applicative since it (trivially) preserves cartesian closed structure.
This is really nice. It ran faster with ByteString and HashMap; `3.27 real` vs `2.29 real` python: import qualified Data.HashMap.Strict as HM import qualified Data.ByteString.Char8 as B import Data.Monoid main = B.interact $ B.unlines . map format . HM.toList . HM.fromListWith (+) . map (flip (,) (1::Int)) . B.lines where format (bs,n) = B.pack (show n &lt;&gt; " ") &lt;&gt; bs 
As for the result being non-`empty`, I'd think that has to be a theorem if it's the desired behavior. The natural and obvious thing would be to return `empty` on `empty`. Returning non-`empty` is fine, just not the most expected thing (since it's not "preserving" anything). Usually with this sort of thing we say that since it has to work for all applicatives (or whatever) then it follows that it works for some particular applicatives, and those particulars suffice to give the behavior for all the rest of them. Which is why I was looking for an applicative that gives rise to `null`. This sort of thing doesn't always work out, but it does pretty often.
&gt;From the user experience it is. Then the user is wrong.
Yeah C++ -&gt; Emscripten works well, but Haskell seems to have issues because of the LLVM implementation? Found: https://github.com/sixohsix/build-ghc-emscripten no idea what it does :)
Your code is certainly concise. But I was seeking to avoid deep pattern-matching, which your code still has. If you match deeply, you assume e.g. Sum nodes directly under Add nodes, which may be too limiting in some cases. By "principled", I mean a true catamorphism that benefits from all the laws and reasoning provided by the theory. 
Using base functors also allows one to view an ADT generically. For example, deltaSquee has a simplify algebra that explicitly mentions only the Add case, all other cases are skipped. cata is defined for all base functors, so this is generic programming.
I definitely agree, and use `case` in most places as well. For single argument matches it does reduce clutter around the interesting expressions. Another minor benefit is that it makes clunky as-patterns unnecessary.
Is it really that bad to case on intermediate tuples?
I just do a case-by-case analysis: Use clauses when there are a few data constructors and not much repetition. Use cases otherwise.
Well done! Both the game and the write-up. A quick look at a [JS implementation of the game](https://github.com/gabrielecirulli/2048/) shows that it's more-or-less the same amount of code. Since it is sort-of a real world example of two functionally on-par code bases, I'm curious how they compare. How much code before compilation/closure'ing/packing/etc., and how much after? Maybe also to count more specifically: w/o whitespace/comments/type-annotations, w/o CSS/HTML.
I am really curious to understand how you could do better with the cata -&gt; function thingy that you are talking about (that doesn't explicitly need to match on patterns to rewrite them). Would you mind whipping up a gist or something to demonstrate? I think `rewrite` has some pretty solid reasoning behind it. It applies rules everywhere until a fixed point. EDIT: Also, I don't understand your objection about only pulling sums in that are below adds. That isn't a restriction imposed by the approach, just a case not enumerated. Some more serious code would probably have some rewrites that take advantage of commutativity.
i have to agree with mitchell here. secrets better vanish, one letter at a time.
No. Unless it looks less clear than equational style.
There is the library [fixplate](http://hackage.haskell.org/package/fixplate) which is basically early-style uniplate but for fixed point types like yours. So yes, there are all the scary-named *-morphisms and much more. It was actually written with computer algebra in mind, but haven't yet had time for applications... (end of shameless self-promotion)
This kind of laziness is why you'll never achieve your dreams.
I think someone explained in the other post that it is O(n) to add to the end?
If I wanted my code to look like case of case of case of case of case of then I would program in Core, thank you. Multi-clause defs are great and I shall use them predominantly. If I can't have math-stylishness in Haskell, then why program in Haskell in the first place?
If you're going to tell me to just google static linking, please actually read the wikipage on it first http://www.haskell.org/haskellwiki/Web/Literature/Static_linking. Basically, the wiki page says that it's unrealiable, complex, and impractical.
Yes, they are just sugar for ```case```. This doesn't make Roman's arguments any less valid, though. If none of his arguments are important to you then sure, use multi-clause notation.
Well, 3 and 4 are good points but I'm not sure about the others. 1. DRY, not a big deal when the number of cases are low (which they usually are for me.) * of course, if you do have more then say 4 cases, I do find `case` to be easier to read 2. Why does the function have more arguments then it needs? * again, this is more of a "the way I write there's only 1 or 2 arguments and I match on all for no more then 4 cases, which is IMHO much easier to read." 3. This is either number 1 or number 2 in the reasons why I use `case` (the other being &gt;4 cases.) 4. Again, a compelling reason. 5. I already map to case in my head, so I don't have this issue. (Also I'm too much of a noob to read Core on a regular basis...) 
If your users range over a limited set of distros/releases, you could compile your app on virtual machines (or docker images) matching each distro and just deliver the executables.
GHC statically links most stuff, just not system C dependencies like libc and libffi (statically linking libc especiallt is disrecomended under any compiler system). As long as the target has a compatible libc, should be fine. I usually compile a couple binaries: one for Debian stable libc, one for Debian testing libc
I actually know people who avoid case in all cases in favour of more functions with pattern matching.
Is there a reason we cannot get the go level of static linking? Where that's not necessary anymore?
Yes, `xs ++ [x]` is O(n), because the default lists in Haskell are singly-linked lists. So to efficiently add an item, put it on the front. `x : xs`. It's also fashionable these days to use lenses instead of record syntax.
 data Component = Position { pX :: Double, pY :: Double } | Velocity { vX :: Double, vY :: Double } I'd strongly advise against using sum types and records in combination - you are just going to hide errors which could easily be caught by the type system in other ways. For example, if you evaluate `pX $ Velocity { vX = 123.2, vY = 2.42 }`, the compiler won't complain, but it will obviously fail at runtime. It's not as big a problem in the case of your Entity, where the `components` "field" is total, as all constructors have it - but I would suggest it's still a bad a approach and can be made simpler (thus, there should be never be a reason to use records with more than one constructor.) data EntityType = Player | Monster data Entity = Entity EntityType [Component] addComponent :: Entity -&gt; Component -&gt; Entity addComponent (Entity e cs) component = Entity e (c:cs) Another addition you could make is a Functor instance for Entities, thereby removing the duplication from functions `gravity` and `velocity` and any others. Have entities parameterized by a type, then just make Entity an alias using Component as its argument. data MkEntity c = Entity EntityType [c] instance Functor MkEntity where fmap f (Entity e cs) = Entity e (map f cs) type Entity = MkEntity Component gravity :: Double -&gt; Entity -&gt; Entity gravity accel ent = fmap gravity' ent where gravity' ...
Go doesn't use libc, it builds its own library on top of raw syscalls (which are different for every supported platform). I don't know if this is possible to do in haskell.
Hm :( Wow, ok
Yes, of course I know that one can package for every single version of linux that every kid ever came up with... But one cannot do that for development versions, can you imagine rebuilding 5 or 6 packages several times a day? Do we have tools to automate that? And by tools, I mean "a tool" ;)
I don't think that we're going to solve the *nix packaging problem. But we can at least be a bit less overweight.
upvote. you know the quote about the messenger, right?
Would it be possible to make haskell programs depend on an older version of glibc? Does glibc provide indefinite backwards compat?
And as far as actual libc's go, glibc also by design doesn't really support static linking anyway, IIRC it's something to do with locale support being dynamically generated or other madness. musl is an alternative that does support static linking, but isn't quite a drop in replacement and you'd need to make GHC use it first.
It is only glibc that does not support static linking reliably, other libc implementations like uClibc or musl would work much better if this was your goal, but you've at least got to get GHC to use them first.
You could also go for records instead of lists: data Position = Position { pX :: Double, pY :: Double } data Velocity = Velocity { vX :: Double, vY :: Double } data Entity = Entity { position : Maybe Position, velocity : Maybe Velocity } velocity :: Entity -&gt; Entity velocity e@Entity { position = Just (Position x y), velocity = Just (Velocity vx vy) } = e { position = Just (Position (x + vx) (y + vy)) } velocity e = e 
The main problem with the memory usage is a combination of short lines and the overhead of `Data.ByteString.ByteString`, however, there's the new `ShortByteString` in the latest `bytestring` version which has a far smaller overhead (especially on 64bit): As no `io-streams` solution was here yet, here's one: import Control.Monad import qualified Data.ByteString.Short.Internal as BI import qualified Data.HashMap.Strict as HM import Data.Hashable import qualified System.IO.Streams as Streams import qualified Data.ByteString.Char8 as BC main :: IO () main = do h &lt;- Streams.fold go HM.empty =&lt;&lt; Streams.map BI.toShort =&lt;&lt; Streams.lines (Streams.stdin) forM_ (HM.toList h) $ \(l,cnt) -&gt; do putStr $ show (cnt::Int) ++ "\t" BC.putStrLn $ BI.fromShort l where go hm k = HM.insertWith (+) k (1::Int) hm -- orphan, ought to be part of `hashable` instance Hashable BI.ShortByteString where hashWithSalt salt sbs@(BI.SBS ba) = hashByteArrayWithSalt ba 0 (BI.length sbs) salt The version above is still a bit slower than the lazy-bytestring version (still need to find out why), which seems to beat all versions I've seen posted here so far in terms of memory usage *and* run-time: main :: IO () main = do lines' &lt;- map toShort . BL.lines &lt;$&gt; BL.getContents forM_ (HM.toList $ HM.fromListWith (+) ((,1) &lt;$&gt; lines')) $ \(l,cnt) -&gt; do putStr $ show (cnt::Int) ++ "\t" BC.putStrLn $ BI.fromShort l where toShort = BI.toShort . BL.toStrict instance Hashable BI.ShortByteString where hashWithSalt salt sbs@(BI.SBS ba) = hashByteArrayWithSalt ba 0 (BI.length sbs) salt 
The trouble with static linking of libc is that you then move the ABI that might change from the well-defined libc one to the kernel syscall one. Unless you want to include a whole kernel and OS into your binary too at which point we call the result a VM appliance.
&gt; can you imagine rebuilding 5 or 6 packages several times a day? Do we have tools to automate that? And by tools, I mean "a tool" ;) We tend to call that continous integration and there are many tools to support it, one of the most popular ones being Jenkins.
Another view on first example: f = \case Right (Right rr) -&gt; ... Right (Left rl) -&gt; ... Left (Right lr) -&gt; ... Left (Left ll) -&gt; ... 
I don't think this is really a problem in practice, at least on Linux (having shipped static binaries in the past commercially in several ways). Linux developers are painstakingly, crazily concerned about ABI compatibility for userspace, all the way to the system call level. It's like their #1 rule. Really the same could be said of Windows, too. Most system call interfaces have not fundamentally changed in years, some in decades. It's perfectly reasonable to support a large subset of the work, like the Go people have done, and have it be pretty stable. I'm not sure if it's the right choice. But there's no need to jump to the conclusion of appliances, and systems like musl support static linking exactly for this reason. A more reasonable concern is that statically linking libc means you can't get security updates as easily (e.g. locale issues have gotten glibc CVEs more than once). Arguably this is mitigated somewhat by the fact alternative libcs like musl are normally far less complex than glibc anyway (a fairly reliable metric of bugs/security problems), but that's another story anyway.
If security updates are a concern static linking of things like openssl and input parsers (XML, image formats,...) is more of a problem than that of glibc.
Useful link: http://www.cs.uu.nl/wiki/HUT/AttributeGrammarSystem In short, it allows you to define several catamorphisms that can rely on each other. For instance, let's say you'd like to know the total value of the subtrees under a node. You write some code to synthesize (goes from bottom of tree to a higher level) that attribute for that data type constructor. Then, you would like to push down that calculated value back into the leaf nodes (perhaps so you can calculate a percentage or something). You then define an inherited attribute and assign the synthesized one to it. The UUAGC system generates some extra code so that the inherited attribute ends up in the leafs. The UUAGC system handles these dependencies automatically, ensuring that everything is calculated in the right order.
I thought at first that you referred to the extreme redundancy of those `[Cc]omponents?` everywhere.
My concern with an implementation like this is that it appears to give up one of the principal benefits of an ECS - horizontal scalability. I don't see a way to add behaviours to this system without modifying the Entity and Component definitions themselves.
I faced a problem of making binaries that would work on different distributions recently. I can say this: * Statically linking against glibc is not an option. * But glibc's ABI is very stable. It is very unlikely that you run into trouble due to a different version of glibc. Other packages are a different story altogether. icu (a C++ Unicode support library, accessible from Haskell as Text.ICU) gave me some grief. It is a moving target, and trying to statically link against it results in unresolved symbols. Luckily I was able to get rid it this dependency. Haskell's own libraries are statically linked by default.
How does that help at all?
If you have to nest so many `case` expressions, multi-clause defs won't help you. And single `case` expressions aren't really less mathy than multi-clause defs: in fact, when defining mathematical functions like that, [the syntax looks more like case expressions than multi-clause defs](http://en.wikipedia.org/wiki/Signum_function). I think both syntaxes have their uses.
Not to mention f is many times much longer
How did Matlab get its market share and industrial usage? By targeting academia and the educational market first. When engineering and physical-/life-science students come out of undergrad already familiar with Matlab (or increasingly Python these days), companies build their codebases around that familiarity so new hires can hit the ground running. Your goals sound unrealistically ambitious, but would certainly be huge if they can be accomplished. Your choice of language does matter if you're the only person (or one of very few) who can develop or use the result however. If you can come up with something significantly faster and more compelling than what's out there now then *maybe* you'll convince people to put in the cognitive investment of using a substantially different programming paradigm than they're used to. Good luck. 
Or 0.0014% of a modern drive. Or about 1s of download time. What's the problem?
You can download 120 megabytes in 1s?! I want your internet!!!!
Came here to say this. `LambdaCase` is nice compromise in my opinion.
Which quote? That Pidgin's only flaw is that it's not written in Haskell? xp
Unless you happen to be browsing through your phone's tether, or on a bus that provides barely passable wifi, or in a cafe, or in an airport terminal, or are in a developing country (and not one of the ones with better internet than the US), etc. It's noise in a good environment and immeasurably frustrating (from personal experience) if I discover something cool while on my laptop on the go and it needs a huge download. Granted, not a frequent occurrence, but not something to completely ignore, either.
Great writeup! Interesting to see how some of the differences between Elm and Haskell play out in a non-trivial application. A minor suggestion would be to use a case statement to pattern match on `tileIndex` instead of using `maybe` later. I'll see if I can submit a PR once I get Elm compiled.
Well. I have money and live in an EU country, but I live in the mountains. It takes *long* to download 120mb here. Currently I'm working on a Chromebook with Crouton and also hd space wise, no it's a lot more than that %. We should not give up making things efficient just because some people in cities with non SSD drives think it's not important. Imho.
GHCi =&gt; Emscripten; you know if that works? Because I don't think it does. I think it would need a lot of work to get it running?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Superoptimization**](https://en.wikipedia.org/wiki/Superoptimization): [](#sfw) --- &gt; &gt;__Superoptimization__ is the task of finding the optimal [code](https://en.wikipedia.org/wiki/Code) [sequence](https://en.wikipedia.org/wiki/Sequence) for a single, loop-free sequence of instructions. While garden-variety [compiler optimizations](https://en.wikipedia.org/wiki/Compiler_optimization) really just *improve* code (real-world compilers generally cannot produce genuinely *optimal* code), a superoptimizer's goal is to find the optimal sequence. &gt;The term superoptimization was first coined by [Alexia Massalin](https://en.wikipedia.org/wiki/Alexia_Massalin) in her [1987 paper](http://portal.acm.org/citation.cfm?id=36194) and then later developed for integration within the [GNU Compiler Collection](https://en.wikipedia.org/wiki/GNU_Compiler_Collection) ([GSO](http://portal.acm.org/citation.cfm?id=143146) 1992). Recent work has further developed and extended this idea: ([2001](http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-171.html), [2006](http://dx.doi.org/10.1007/11799573_21), [2006](http://theory.stanford.edu/~sbansal/pubs/asplos06.pdf)). &gt;Typically, superoptimization is performed via [exhaustive search](https://en.wikipedia.org/wiki/Brute-force_search) in the space of valid instruction sequences. While this is an expensive technique, and therefore impractical for general-purpose compilers, it has been shown to be useful in optimizing performance-critical inner loops. [Recent work](http://theory.stanford.edu/~sbansal/pubs/asplos06.pdf) has used superoptimization to automatically generate general-purpose [peephole optimizers](https://en.wikipedia.org/wiki/Peephole_optimization). &gt; --- ^Interesting: [^Alexia ^Massalin](https://en.wikipedia.org/wiki/Alexia_Massalin) ^| [^Meta-optimization](https://en.wikipedia.org/wiki/Meta-optimization) ^| [^Peephole ^optimization](https://en.wikipedia.org/wiki/Peephole_optimization) ^| [^Index ^mapping](https://en.wikipedia.org/wiki/Index_mapping) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+chejuxm) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+chejuxm)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
It takes me over 2 minutes.
theres different time scales for different funnels. I think for supporting the basic development, targeting industrial to start with is key, and then once thats sorted, I can go out and figure out how to educate folks (but that outreach is expensive, gotta pay the bills first :) ) you have valid points, and I hope to hit those (really high) bars, but the proof will be in the pudding :), and things may windup being less usable than I'd like (though I hope not). 
the general rule is that downvotes are for comments that detract or harm conversation or contribute negatively or not at all, and that up votes are for comments that contribute to the discussion.
I think I had a conversation once in which we discussed how lenses were this amazing and useful idea that weren't necessarily Haskell-specific and yet no-one else seemed to have invented them. In part I argued that this was a byproduct of how primitive Haskell's records are by themselves. Until you've deprived yourself of the half-arsed solution you just don't invent the whole-arsed one. Not sure I recommend suggesting this as a language design strategy, mind :P
There's the appropriately named [`IntMap`](http://hackage.haskell.org/package/containers-0.5.5.1/docs/Data-IntMap-Lazy.html) for `Int` based maps. We can also make everything nicer/faster by doing bulk updates instead of one at a time: step :: Entities -&gt; Entities step es = es {positions = intersectionWith (+) ps vs `union` ps } where ps = positions es vs = velocities es
There is [Data.IntMap](http://hackage.haskell.org/package/containers-0.2.0.1/docs/Data-IntMap.html) for int based lookups.
What do you want to shop, source or an executable?
You can always use pandoc to convert the markdown docs to texinfo, if you want to have that too. 
If you do deep pattern matching you are writing context-sensitive code and no longer doing a true fold. The poster was asking for ways he could improve his code, hence my suggestion. Oleg discusses this in the link I posted. Sorry I don't have time to post a gist today, maybe in a few days. 
Both of these suggestions (the different types and the functor instance) are very illuminating, thanks! Is it a general rule in Haskell that one shouldn't combine sum types and records? EDIT: Re-reading your post, you stated "there should never be a reason to use records with more than one constructor", which answered my question. :D
I feel like you're going to have to do that no matter the implementation?
I think that this may lead to a ton of a build-up in your entity class. Every time you want to construct a new Entity, you'll have a bunch of Nothing's everywhere.
I would prefer to ship source, which is why the title reffered to a GHC light. I prefer to ship source, because I feel kind of weird telling random people on the internet "here, run this random executable!" I think people trust source more. But I'll suffice with an executable if I can have that...
I can also host things myself(that can be hosted). But that's still besides the point.
Yes, I didn't look up the name when I made the comment. I vaguely recall having linked from 6 to 7 without seeming ill effect... Do I need to look up the numbers? Is it important?
&gt; DRY, not a big deal when the number of cases are low (which they usually are for me.) Repeating the function name is noise. The more noise the harder it is to see the signal. 
Uniformity makes reading code easier IMO.
I'd really appreciate it, thanks!
no, i only asked because i wanted to help. i'll go wash the dishes instead.
I'm so glad you want to use Markdown. I've been supposed to document the snippets I added for a while now but ran into the same problem: I didn't know how. Markdown's just too simple not to use.
I find it to be the exact opposite. Repeating the function name gives you a clearer view on things, since it's telling you exactly what's happening for each case.
Could still be worth it for a cool Haskell application.
Sorry, I meant to say identity isn't Alternative.
You're going to have a hard time getting modern code used in the wild to work with Ajhc and Uhc.
Pretending the language is total, making `null` from `construct`, would need an applicative where you can build a `forall a . f a` (without having an `a`), for example `Const` - or the `empty` of any Alternative. Then I suspect that `fmap` leaves a value containing none of the argument type, and things like `fmap Just` have the right type to be `construct`. Back to the full language, you could try something like `construct (pure undefined)`, but might of course get `pure undefined` back.
I guess it comes down to the fact that on unix, programs are supposed to do one thing and do it well. Are there any cool unix programs? Is cat cool? What about tail? ;)
Normally you have a mapping for each component type from entity ID to some number (depending on the type of component) of values for that entity. You do not have a mapping from entity to every component that entity contains. With this, you can have subsystems only consume a subset of the components, and those subsystems do not have to use anything that changes when more components get added. EDIT: [This](http://cbpowell.wordpress.com/2012/10/30/entity-component-game-programming-using-jruby-and-libgdx-part-1/) is a good resource I've read before. Caveat, I've never developed anything of this sort.
It says it was unreliable on 6.8.2, which is pretty old. I've compiled with `-static -optl-static` and had no trouble, but I'm not sure what range of distributions people used the binaries on.
This is excellent, thanks so much for your emacs work, Chris! Could you please give more explanation about `haskell-indentation`? You say that what it adds over `haskell-indent` is that "indentations can be set and deleted". That sounds like it would be great. But is that per line, or per region, or per file, or what? And how exactly do you set and delete them? You say that it somehow uses "`&lt;RET&gt;` and `&lt;DEL&gt;`... as if they were real tabs." But the only thing I can immediately find about setting and deleting real tabs is the function `edit-tab-stops` which seems to be something completely different. Thanks!
Correct, and if you look at the Repa source you can see parts were indeed pulled from/heavily inspired by DPH (which makes sense, given what you said). DPH is mostly about recursive parallelism correct? I know Repa does not support that at all, and sometimes it is very much desired.
Hmm, without modifying the code? How about: (setq haskell-process-type 'ghci haskell-process-path-ghci "nix-shell" haskell-process-args-ghci '("--command" "cabal repl -ferror-spans")) Something like that?
Sure, but so do equations in some cases. show Nothing = "Nothing" show (Just x) = "Just " ++ show x Is nicer and more readable than case and repeats only the word show.
 Agnes Skinner: And you! Start over! I want everything in one bag. Bag Boy: Yes, ma'am. Agnes Skinner: But I don't want the bag to be heavy. Bag Boy: I don't think that's possible... Agnes Skinner: What are you, the "possible police"? Just do it!
OT, but just wanted to say I enjoyed your Turing-inspired username.
I did this a few weeks ago: [Play it](http://fosskers.github.io/2048) [See code](https://github.com/fosskers/2048)
How I wish I could download 120MB+ in 10 seconds over my DSL.
Thanks! &gt; Could you please give more explanation about haskell-indentation? I'm afraid you picked the one chapter in the whole thing I didn't write. That particular part was written by Herbert (ping /u/hvr_ — can you clarify it?). I've not used the haskell-indent or haskell-indentation modules for a couple years now (I now use structured-haskell-mode for all my indentation needs), I don't remember their differences. Sorry! 
Maybe `lens`, `pipes` / `conduit`, `aeson` / `xml-conduit` ?
&gt; If you compile on Debian unstable and then try to deploy to Centos 5, for example, you can expect trouble. If you do it the other way around, however, things will be fine. Newer glibcs are perfectly backwards compatible with older ones. The bigger catch is with *other* C libraries on the system that are more of a moving target like y'all said - those are more of a problem in practice. What would actually stop you from running CentOS 5 binaries on Debian, for example, would be the GMP version, not the glibc version (because the `.so` changed names, although in practice this can be 'fixed' by patching the executable `rpath`.)
Yep. This is definitely a style preference issue. It's not worth spilling much ink over this, since either style works fine, but to summarize why I avoid case and prefer multiple function equations: * This is not a classic case of DRY where repetitions can lead to errors. The compiler checks these repetitions, so they are safe. They only make the code more clear and easier to read. And you don't even have to type the repetitions if you are using emacs haskell-mode - just hit tab. * `case` adds more power in its own way - by allowing guards intermixed with the patterns, and by allowing nested `case`. In my experience, that power is error-prone and makes code harder to read. Whereas the direction where multiple equations adds power tends to make code more clear and readable in my opinion.
Agreed. But that particular example is a bit contrived, because you could write: show = maybe "Nothing" $ ("Just " ++) . show
Nah I don't use `case` there either. I prefer to break `where` and `let` functions into small digestible semantic pieces and give each piece a meaningful name. And I use multiple equations to define those pieces, not `case`. This results in concise but highly readable code.
Maybe so; I haven't looked into it in detail, and have only used ECS in other languages. I just hope that the benefit of more component type-safety doesn't have to come with the cost of building a closed system!
&gt; nested sum types f (Right r) = fRight r f (Left l) = fLeft l fRight (Right rr) = ... fRight (Left rl) = ... fLeft (Right lr) = ... fLeft (Left ll) = ... More repetitive, yes, but more clear. And since you picked `Either` here, you could even write: f = either (either leftLeft leftRight) (either rightLeft rightRight) leftLeft ll = ... leftRight lr = ... rightLeft rl = ... rightRight rr = ... &gt; long sum types I personally like your first version much better than your second version here.
Nested data parallelism, yes. Work is currently underway to add it to Accelerate, though.
Just please make sure that these strategies to make GC work more like the JVM for mutable arrays don't have any negative effect whatsoever on performance for the usual immutable data.
Those requirements for me seem ambiguous and maybe subjective. Since the keyword "expressibility" is used, I can only think of programs that emphasis the [expression problem](http://c2.com/cgi/wiki?ExpressionProblem)
hugs
&gt; As I read the LLVM implementation is not complete(?) It's complete and should handle what GHC can throw at it just fine, but there may be bugs of course (like with anything). &gt; but it's going to be the default GHC backend No, it isn't.
You can intermix guards with patterns on equational syntax as well.
&gt; GMP version Yeah. When I built my BB10 cross-compiler I just went with integer-simple because of this.
The bigger issue is platforms like OSX where the easiest install is actually the whole Haskell Platform -- which makes GHC look small.
Maybe your answer includes how Haskell is a very neat math like syntax but they will probably be looking for a declarative vs imperative sort of answer. Haskell expresses parsing via parser combinators libraries (parsec, attoparsec) very well. If you have ever written a parser in Haskell you'd see what I mean. Consider that in constructing your answer.
Haskell's records suck big time
Aeson is an excellent suggestion. Thank you. I do not know how to benchmark the other libraries, though. I'm open to ideas.
I don't think it work out of the box, but it should be still easier than full re-implementation.
Perhaps some rather strict and heavy computation folds over bytestrings? For example, the SHA or PureMD5 packages.
I know that both `pipes` and `conduit` have benchmark suites built in.
Not a Haskell-mode, nor an emacs user, but I'm always happy to see documentation being paid attention to. I even read the documentation just to see what I might be missing in my vim adventures. Thanks.
Yep. Although these pure Haskell libraries are rarely favored over the C implementations, I've included four checksum benchmarks, including SHA and MD5. If I'm missing any, I'd love to include them.
Multiclause defs make it a lot easier to deal with matching across several parameters, since the failover can cross multiple arguments. You often have to repeat yourself with explicit cases and to me, that is the job of the pattern compiler.
 foo (Just x) (Just y) = Just (x + y) foo _ _ = Nothing With explicit cases you wind up having to repeat the default case at least twice.
I have the same issue and it's driving me mental.
It's not something as "optional" as locale support. All name resolution (hosts, groups, users, etc.) is loaded dynamically and could break is odd ways if libc is statically linked with an incompatible version. Also, I believe this use an interface that's not considered part of the public glibc API, so it can non-backward compatible ways, with little warning. (The public API in glibc uses symbol versioning to stay backwards (binary) compatible.)
I dislike "maybe" most of the time, because it's basically a positional pattern match rather than a named pattern match... Named may be more verbose, but it's nicer. Consider that we prefer if-then-else to a function that does the same.
I was supprised, haskell-platform source is allegedly 2.4 megabytes on linux http://www.haskell.org/platform/linux.html. Doesn't the haskell platform binary on that site, for mac, include GHC?
I didn't intend for the discussion to be solely about static linking ;) You'd have to admit, that it seems that most people have ignored the title entirely.
Is this like the anti-moto of the busybox project?
1. Your definition of foldr doesn't make sense. It's defined as "go" but there are no "go"s in the where-clause. 2. Left folds and right folds are inequivalent for any noncommutative operation, so trying to replace all left folds by right folds is misguided.
Haskell cannot express unboxed mutable arrays. They're implemented through FFI calls to C code. It also cannot express records with mutable fields.
You could discuss the monads issue. They're basicallly a coding style that are used for (among other things) forcing an evaluation order, allowing a structured pseudo-imperative style. If you're still grappling with monads, don't: people fluff them out of proportion. Monads are like the '|' character in bash: they feed outputs of functions to other functions to create pipelines.
Monads don't force an evaluation order, seq does. Without seq the monadic code is evaluated lazily, hence the problem of lazy IO and the lazy Writer. Monads impose a sequence only in terms of data dependencies, not evaluation order. As an example, check out [this STRef source code](http://hackage.haskell.org/package/base-4.7.0.0/docs/src/Data-STRef.html#modifySTRef%27). Notice how there has to be a separate strict version of modifySTRef, modifySTRef', and it is defined using seq.
i think it might be a sandbox thing, actually: https://github.com/aloiscochard/codex/issues/1
The OP is also missing a definition for `onNil` in the definition of `foldr'`.
I don't think that the problem between foldr and foldl is a matter of "stack usage"...
I think I actually will go with hugs and haskell98 after having read about it further. It has OpenGL/GLUT if I need to draw things, it's really small :), If I stick to the haskell98 base libraries I should avoid cabal hell all together :)... Does hugs support the threading primitives forkIO and MVar and Chans?
If you have N components on average per entity and M component types, then you will waste M - N pointers to None per entity. You can alleviate this by introducing component groups, implemented as a record of components that are often used together. data Positional = { position : Maybe Position, velocity : Maybe Velocity } data Mortal = { health : Maybe Health, armor : Maybe Armor, shield : Maybe Shield } data Entity = Entity { positional : Maybe Positional, mortal : Maybe Mortal } 
I had somewhat the same problem, but with all the different kinds of strings. To some extend it also happens with the different web frameworks. This seems to happen more often in Haskell than any other language that I've worked with. Is this because the standard library is so small, or is it something inherit to the language?
Can you reproduce the following video? http://chrisdone.com/comments-fill.ogv So: Make sure you're using a recent haskell-mode (preferably Git version) or state your version. $ cd /path/to/haskell-mode $ emacs -Q -L . -l haskell-mode-autoloads.el # load no custom user settings, just haskell-mode itself * Then in Emacs paste in [some text](http://www.lipsum.com/feed/html). * Jump to half way in the text and hit RET. * Type `--` to continue the comment. * Hitting `M-q` now will merge it back with the previous paragraph, like normal text-mode `M-q`. * Undo that. * Add an extra comment line inbetween your two paragraphs, now hit `M-q`. It should format just the current paragraph properly. If it works correctly here, there's something in your Emacs user config causing this. If it doesn't work here, what is your Emacs and your haskell-mode version?
2 years ago I finished my thesis in the Clean group at Nijmegen. They're still working on the iTasks system, which is imo an pretty interesting system. Less work goes into the compiler, but it is still being worked on. People from outside the group do not hear much about all this. Compared to more popular languages, Clean is not community focused. It's not clear for outsiders how to contribute to the language/compiler. There is no package management, so there is no way to let the community contribute libraries effectively. Documentation is there, but only maintained by the Clean group. You'd have to use the mailing list to share or ask for information. This a makes it hard for an outsider to get started on Clean for anything 'productive in the real world'. By that I mean that you usually need many libraries to get productive. Libraries are scarce, aren't easy to find and there probably is no documentation for it. I think it would need some massive changes in the way things are developed and communicated to get picked up by a wider audience.
Please [try this](http://www.reddit.com/r/haskell/comments/2583sm/haskellmode_documentation/chf2psi).
 &gt; No, it isn't. Oh, I read that somewhere; sorry misinformed. After a bit more research, it seems that (however strange it sounds to me) the Emscripten implemention works basically only with C/C++; so the generated bytecode differs from frontend to frontend? And someone would have to implement an Emscripten solely for GHC? Or at least fix it 'a lot' (I read a few weeks of hard work)? 
I thought LHC was more of a particle accelerator...
 {-# LANGUAGE OverloadedStrings, TupleSections #-} module Main (main) where import qualified Data.ByteString.Char8 as BS import qualified Data.ByteString.Lazy.Char8 as BL import qualified Data.ByteString.Short as SB import qualified Data.ByteString.Short.Internal as SBI import qualified Data.HashMap.Strict as HM import Control.Applicative import Control.Lens import Data.Hashable import Data.Monoid main :: IO () main = imapM_ printLine =&lt;&lt; HM.fromListWith (+) &lt;$&gt; map ((, 1 :: Int) . SB.toShort . BL.toStrict) &lt;$&gt; BL.lines &lt;$&gt; BL.getContents where w2c = toEnum . fromEnum printLine s n = BS.putStrLn . SB.fromShort $ SB.pack (w2c &lt;$&gt; show n) &lt;&gt; "\t" &lt;&gt; s -- Shamelessly stolen from /u/hvr_ :) instance Hashable SBI.ShortByteString where hashWithSalt salt sbs@(SBI.SBS ba) = hashByteArrayWithSalt ba 0 (SBI.length sbs) salt On my machine, the python version runs ^[1] in ~1.38 seconds with the maximum resident set around ~14.9 MB, while this Haskell version runs ^[1] in ~1.74 seconds with the maximum resident set of ~23.1 MB. IMO this is not too bad considering you are comparing a Haskell solution to Python's dictionary which is very well-optimized C. Then, it basically boils down to time/space trade-off. If you just use lazy `ByteString`s instead of `ShortByteString`s and is willing to use up a lot more memory, you could get down to ~1.2 seconds, or you can play with GHC's GC settings and get down to ~10 MB maximum memory usage as well. I wonder how far we can get if we use `Data.HashTable` which is an actual mutable hash table. The current problem is that `Data.HashTable` doesn't provide an `insertWith` operation which either inserts or in-place modifies a value, and instead I had to do a lookup followed by a insert, which kills the performance. It may also be worth playing with Judy arrays, however that doesn't compile on GHC 7.8.2. :( [1] Averaged over 10 runs.
I'd speculate that Haskell still has a rather small ecosystem, so problems like this can "survive in the wild". A much larger ecosystem forces convergence. If some matrix type becomes more and more popular, the rest disappear and all libraries start addressing that matrix type. Also, Haskell refuses to abstract over various "collections" such as String, ByteString.Lazy, ByteString.Strict, Text.Lazy, Text.Strict in ad-hoc ways. Only principled abstractions are accepted, but our principled abstractions don't handle monomorphic "containers" well. So while Haskellers no longer wear a hair-shirt when doing IO, and we got a nicer IO system than other languages, we're still wearing a head-shirt when it comes to containers' abstraction (which will hopefully yield the same benefit, eventually). 
You can implement `foldl'` in terms of `foldr`. This is how `Data.Foldable.foldl'` is implemented: foldl' :: (b -&gt; a -&gt; b) -&gt; b -&gt; t a -&gt; b foldl' f z0 xs = foldr f' id xs z0 where f' x k z = k $! f z x This has the nice advantage that you get build/foldr fusion because it's implemented in terms of `foldr`.
I haven't really seen that opinion floating around. Why do you say this? I know the record system isn't that great, but very often a record is exactly what you want.
Sometimes it can lead to errors, e.g. when you mistype a function name: sum [] = 0 sun (x:xs) = x + sum xs
Hi, if you know TDD/Testing from other languagues I would start here: http://pbrisbin.com/posts/automated_unit_testing_in_haskell/ Most of what is said there is true for testframework as well (but Hspec is really simple) For a introduction to testing using quickcheck look here: https://www.fpcomplete.com/user/pbv/an-introduction-to-quickcheck-testing
I agree completely. Different benchmarks are useful for different things.
quickcheck, smallcheck, and smartcheck are all broadly similar. The big difference between quickcheck and smallcheck is that quickcheck generates random input for the test cases, whereas smallcheck methodically generates inputs within a certain range of inputs. Smartcheck is quickcheck except the way it produces a minimal counterexample is different, and it appears to mostly be a drop-in replacement/adjunct to quickcheck. One of the problems with quickcheck is that often the probability distribution of inputs it generates is not ideal for efficient and effective testing. Depending on the function you are testing, there might be improbable cases that quickcheck has no reasonable chance of finding without guidance... even if you could conceivably run it for a very long time, you'd be wasting most of that time re-testing already well-tested categories of input. You can often work around this by defining a custom input generator for a newtype-wrapped value, and then unwrapping these values in your quickcheck properties. Smallcheck wouldn't directly help here either, as there are always a lot more probable cases than improbable cases. (I seem to recall seeing a paper that had some discussion of this issue, but I didn't look at it too carefully and I forget the title, authors, venue, etc.) So while quickcheck and smallcheck can be very effective, they aren't a panacea either. You can pretty easily mix-and-match, however. 
I don't have any concrete examples to give you, but here are some things that I'd love to see explored: * Cache effects and codegen that improves cache usage. * Floating point tends to get less attention so it would be nice to have coverage for simulations using Double/Float. * Better constant folding and other compile time evaluations.
1. Fixed. 2. That's exactly my point. It's ugly that people use foldl' for performance when it has different behaviour from foldr.
Yes but that's still relying upon the compiler for performance. It's also not a general solution because it doesn't apply to folds on trees and other data structures.
I have to blog about it but the github repo already has the rules ... except there is currently no way to play, and they are not tested yet ;)
thanks that was the problem 
that was the problem too. thanks.
Someone on IRC suggested template haskell. Cons: * Requires GHC * Not always available on all platforms * Requires you to learn a new part of the language * TH code is written against specific GHC versions (conditional compilation bootstrapping issues). Pros: * The macros can be very powerful and shared through hackage.
I myself do use records sometimes, but I've talked with a number of people who feel the syntax is out of place, and the namespacing issues are bad enough to just avoid them entirely.
If you're going to supply a custom preprocessor, just use cpphs. I usually try to write my haskell code in such a way that ansi cpp also works on it.
Andreas Fuchs had written some tests for SBCL a couple of years ago to track performance changes between releases. You may want to take a look at them. https://github.com/antifuchs/autobench/tree/master/cl-bench/files 
https://github.com/ekmett/lens/issues/415 talks a bit about where that -traditional came from and alternatives we tried before we just fixed things to work on `clang`.
For a while we used ghc-options: -pgmP cpphs -optP That would at least give you consistent cross platform support if you add the build-tool dependency on `cpphs`, but it is another dependency.
Your program should call `withSocketsDo` once, in `main`, and not in `execCmd`. This may be causing you problems if you are running this on Windows.
"I am learning how to do testing in Haskell." ... but if you don't my answer downvote it or ignore it
I'd like a pointer to that paper if anyone has an idea which one it is.
Maybe I'm missing something here but it seems confusing to me that there seem to be two diverging manuals now (or are they to be kept synchronized?). I.e. the one available inside `haskell-mode` (via Emacs' integrated help-system, for example via `M-x describe-mode` or `[F1] m`) and the one on the project wiki?
I really like `cpphs`: it supports simple CPP usage, slightly more complicated things like string splicing, and is portable. It retains the light weight of CPP without the dependency on another ecosystem.
Thanks, I switched from Vim to Emacs + evil + haskell-mode and am loving it. This documentation should come in handy!
This was one of the options discussed during the 7.8 transition. Not sure why we went with `clang` instead. 
I assume something didn't work with getting GHC to build with it. It seems like a big portability win, as well as offering freedom to tweak things to suit Haskell-centric usage moving forward if necessary.
OK, runhugs is SLOWWW, giving up on that idea :/
The thing about Hugs is that it's an interpreter, not an optimizing compiler. But then, that's part of why Hugs is so light weight compared to GHC
Well, put it this way: if you don't use cpphs, it's not going to work on Windows outside of MinGW. I don't know many people who run their Haskell envs inside of MinGW :(
Are you sure? `ghc` knows where to find `gcc` and that's how it invokes CPP. It calls out to `somepath/gcc -E -undef -traditional ...`, where `somepath/gcc` is defined in a `ghc` configuration file. `gcc` uses an internal search path unless you add `-B` and it doesn't look like `ghc` does that.
Don't forget to add type signatures for your top-level helper functions.
The differences between Quickcheck1 and Quickcheck2 aren't that big in the grand scheme of things; i.e., they both still rely on the same basic idea. As others've mentioned, the big difference is between Quickcheck which generates *random* inputs to test, vs Smallcheck which *exhaustively* checks all "small" inputs. So, Smallcheck is good for ensuring coverage of all the cases. This is great when your corner cases show up on small enough inputs; but, of course, exhaustive search quickly becomes infeasible as you increase the threshold for what counts as "small".[1] Whereas, Quickcheck is better for testing medium-sized values, which is helpful if you have corner cases which might only show up after passing some size threshold (e.g., for list-of-chunks style data structures). However, random sampling is going to do a terrible job of uncovering the dark and hidden corners, unless you just happen to know how to tune the distribution you're sampling from (which you probably don't; if you did, why not use an explicit test case?) Of course, there's nothing prohibiting you from combining both approaches. That's the beauty of property-based testing. I use both of them on my projects, because why not? And if I know of specific corner cases, I'll explicitly check them in the test suite. I tend to prefer Smallcheck since it's a better fit for the sorts of things I'm testing; but that's more about the biases in my code, rather than about which approach is "better". ----- [1] N.B., you should (almost) always go with **lazy smallcheck** rather than the original smallcheck. Lazy Smallcheck is far more efficient at exhaustively exploring the value space, so it can test much larger values before the exhaustive search becomes infeasible. The way it works is that it uses partially defined values; for example, if the property holds for `(x:undefined)` then we know it must hold for *all* lists beginning with `x`, regardless of what the rest of the list looks like. So all those infinitely many inputs can be marked off as having been checked. Thus, the only reason not to use lazy smallcheck is if your properties are totally strict in their inputs, and thus can't benefit from this sort of thing.
this is planned... first certain patches need to get merged in :) 
As mentioned, though, Nix can do this, to some degree, and it will run on just about any sufficiently Unix-y system. This is mostly tractable (I think), cabal-install just isn't necessarily the tool for it at the moment.
No, probably not. My point wasn't that it would be impossible - but GHC currently ships binaries linked against glibc, and there are absolutely zero plans to change this. IOWs, I was just saying if you want to make GHC use musl, you'll have to compile it yourself and do it yourself. The fact it works - and is probably quite easy - doesn't really surprise me.
The reason I always heard: Because cabal is a build tool, and not a package manager. It is *out of scope*. For better or for worse.
Don, as an Australian I thought you'd know that not everywhere in the world has great internet, but I'll forgive you as you haven't been here in a while -- The average connection here would download GHC in 2-5 minutes.
Wouldn't coroutines be a perfect candidate for automatic speculative/optimistic evaluation? Run the coroutines eagerly in parallel, block when they need to yield, and the thread can be aborted and thrown away when the coroutine isn't needed anymore. Another idea, you could build a buffer in stead of blocking immediately. It could even be of dynamic size depending on how fast the consumer gets, to get a balance between having to wait and doing too much work. 
Agreed! This is the proper solution to the problem. Much of "cabal hell" would disappear if the package database was just a cache of build results. This would mean that "--force-reinstalls" would no longer be dangerous, and cabal might not even need to ask about it. Ideally, it'd also be easy to flush out rarely used bits of this cache. Maybe we can get over the "cabal isn't a package manager" dogma, and adopt "cabal manages the build cache".
OP does mention "compile once", so the raw source is fetched to the local machine first, and compiled there. The quesion is why not reuse the compiled package for each and every project. 
&gt; Much of "cabal hell" would disappear if the package database was just a cache of build results Yes. What would be left would be solver failures mostly, and we could make the solver better if we saved historical information about successful and failed installs on Hackage, I bet. Until we can save multiple instances of the same version with precise reproducible fingerprints, I don't think we can leave cabal hell.
&gt; I am learning how to do testing in Haskell. That's called "context", that's not the question.
The only real trouble I can see currently with this approach is that TH does not work in cross-compilers. This is a half-fundamental problem, as executing arbitrary binaries (runnable on the target arch) is impossible without (say, 'slogin') access to a machine of that architecture (or a binary emulator). However executing bytecode (this is what GHCi calls "interpreted") could work for a significant percentage of the cases. I believe all examples on the slides could be covered that way for cross-compiling.
There is a tool for that : &gt; pointfree "f x = g (h x) x" f = g =&lt;&lt; h This might not help :)
The problem is (in part) that `Even = Z | S (S Even)` doesn't make any sense as given, and it's hard to make it make sense in the usual type theoretic ways. Perhaps with subtypes you could do something, but subtypes are evil. At any rate, this works well enough for problems where it's easy to enumerate things, and where the domain and range can be enumerated in just the right way, but it won't work well for more complex problems. And some functions, like `prime :: Nat -&gt; Nat` which makes `n :: Nat` to the *n*th prime have no inductive definition as far as current mathematics knows, thus rendering whole problems unanswerable using the technique you propose.
Uh huh but what would be the definition of a "too complex problem" to be defined this way? Where is the boundary? 
Anything non-trivial will be very difficult to do this way. Try defining `map` like this.
`f = join (g . h)` might be clearer. Or it might not :)
`g &lt;$&gt; h &lt;*&gt; id` might actually be readable...
Duncan and Simon Meier worked on this during the last ZuriHac, and I've been looking forward to it since then. Very exciting! What's the current status of the new binary? Any estimates on when it'll be released? Also, is there a command line tool to do CBOR encoding/decoding?
To some extent, the distrust of binaries and the trust of source is irrational. But to a large extent it is not. If the source code turns out to be mallicious, it is relatively easy to prove that the code was intentionally so. Furthermore, when distributing development versions, I want to encourage people to send me patches :)
I'm just trying to wrap my head around this. Where's the monad here? `(=&lt;&lt;) :: Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b` `let h x = undefined` `let g y = undefined` `:t g h undefined` =&gt; `:: t` `:t (g =&lt;&lt; h)` =&gt; `:: t -&gt; b` Seems about right, but I don't get it.
&gt; I wonder how far we can get if we use Data.HashTable which is an actual mutable hash table. Btw, `fromListWith` already has [some in-place mutation](https://github.com/tibbe/unordered-containers/blob/master/Data/HashMap/Strict.hs#L165) going on as an optimization. However, I'm still curious how `Data.HashTable` may perform if had an [entry update operation](https://github.com/gregorycollins/hashtables/issues/6)
This is using the `(-&gt;) r` instance of Monad. `(-&gt;) r` can also be written as `(r -&gt;)`, and it represents all functions from `r` to something else. (=&lt;&lt;) :: Monad m =&gt; (a -&gt; m b) -&gt; m a -&gt; m b If we substitute `r -&gt;` for `m`, we see that (=&lt;&lt;) :: (a -&gt; (r -&gt; b)) -&gt; (r -&gt; a) -&gt; (r -&gt; b) Removing redundant brackets, we get (=&lt;&lt;) :: (a -&gt; r -&gt; b) -&gt; (r -&gt; a) -&gt; r -&gt; b which is the actual type of `(=&lt;&lt;)` in this expression. By examining the type, we see that `(=&lt;&lt;)` takes two interesting arguments, g :: a -&gt; r -&gt; b h :: r -&gt; a the only way we can make a function f :: r -&gt; b from these two functions is if we define `=&lt;&lt;` as g =&lt;&lt; h = \x -&gt; g (h x) x We see this because we need to return a `b` in the end, and we can only get a `b` from applying `g` to something. To apply `g`, we need an `a` and an `r`. The `r` is something we already have, and to get an `a` we need to apply `h`, because that's the only way we can get an `a`.
Fwiw, I'm willing to sync the in-Emacs info-manual up with the wiki-content once /u/chrisdoner's content-creation stabilizes a bit.
The monad is `(r -&gt;)`^(*): (=&lt;&lt;) :: (a -&gt; r -&gt; b) -&gt; (r -&gt; a) -&gt; r -&gt; b ^(*)(Which isn’t legal syntax technically but IMO more readable than `((-&gt;) r)`.) edit: arguments the wrong way round, sorry
Side note : if you don't know how to write something in point free notation, chances are that your mates (or yourself a few months later) won't be able to read it. More efforts to write, more efforts to read : not sure where is the gain ;)
&gt; ping /u/hvr_[1] [+2] — can you clarify it? tbh, I struggled myself back then to find the historic rationale for why each mode was created. The best information I found was for the `haskell-indent` mode which had a paper publication associated with it, namely [*Dynamic tabbing for automatic indentation with the layout rule*](http://dl.acm.org/citation.cfm?id=969588) (I remember having found a `.ps` file of that article back then, but I can't seem to be able to google it again)
&gt;I really like the Glascow Haskell Compiler and I need to know how to at least compile my darn programs How can you like something that you seem to barely know how to use? :)
Well, the problem is that the conversion to Markdown loses quite a bit of texinfo markup, so it can't be done fully automatically. Texinfo has specific support for being used in an interactive in-application help-browser, with an emphasis on navigation, as well as interlinking between Info manuals. Also, Emacs has specific support for linking UI elements to Info manual nodes, so you need to have specifically named anchors in the Info manual. I can totally understand that Texinfo is quite daunting, as it's a markup that predates the current wiki-style markup generation, but until Emacs opens up to alternative documentation systems, we're stuck with supporting it to provide a seamless Emacs experience :-( See also the original texinfo-generated HTML at http://haskell.github.io/haskell-mode/manual/latest/
Nice! I'll need to figure out if the succinct code I'd been working on for lazy deserialization of the binary/cereal/bytes format can be extended to CBOR. It looks viable.
I'm not questioning your motives but I'm really curios about the cost/benefit analysis of your approach. Most people use binaries that they never ever can get the src to. If you users are developers (of haskell as well) then the should have all of ghc installed (and then some). If you really want patches, why don't use bitbucket/github (open or private)? That said, I don't know anything about your application and how large ghc tool chain is acceptable for your taste if you think that a mere 120MB download is to much.
i read that when edward kmett mentioned speed work in binary. great that a standard is chosen. maybe i did miss something in the paper, but why was cbor chosen over thrift, protocol buffers, bert or asn-1 (just kidding)? i have some ideas on why but it's nicer to know from duncan.
It's worth pointing out that this is just the un`newtype`d version of `Reader`.
It is written on the card, so it just was the "obvious" thing to do for me. I agree this sucks in the sense that most card are copy/pasted once or twice. I think the "best" solution would be an external configuration file, but the "CardList" part was so tedious I am not ready to refactor it yet :)
This is specifically about the constraint solver in Cabal (and we can distinguish between failures like this one and that one, obviously), so recording failures here would be agnostic to those kinds of bizarro platform issues, and give a good view of where it currently chokes and insight into why.
While /u/CKoenig is not answering the question, the post is on-topic and the links are useful. You might also want to consider [tasty](http://documentup.com/feuerbach/tasty), which is a nice testing framework that also includes Hspec.
 f = g &lt;*&gt; h ~ f x = g x (h x)
Not quite sure how this is backwards and forwards compatible. They propose adding a new tag when a field is added. This means that deserialization code will slowly grow cruft. Similarly serialization code needs to be historically aware not to reuse tags. These problems don't occur with protocol buffers, for instance. Unless they plan some automated, Haskell data-type oriented, layer like protocol buffers on top of this, this seems like it could result in a maintenance nightmare.
`f = flip g &lt;*&gt; h`, I think...
GHC will flag that as a non-exhaustive pattern.
It's literally the same: newtype Reader r a = Reader (r -&gt; a)
&gt; You can intermix guards with patterns on equational syntax as well. True, but only to one level. And despite the similar syntax, guards on function equations seem instantly readable to me, whereas with `case` I often find myself squinting at the indentation to make sure I fully understood the logic. Perhaps it's just me.
So what you really want to record is solver success and failure, not install success and failure? I agree that that should be much simpler.
My initial reaction was wondering how that made sense, but after some thinking, I realised that *does* make a lot of sense.
 This is an important consideration. I often tend to stray a little too far into the terse point-free style, so I need to keep this in mind. Recently, I accidentally wrote chooseTwo g = flip evalState g . both (state . randomElem) when I probably should have been writing chooseTwo seed (xs,ys) = (x, y) where (x, newSeed) = randomElem xs seed (y, _) = randomElem ys seed But sometimes it's hard to tell! (In this case, deciphering what the first one does is helped a lot by its type signature, but you never know when it becomes a problem.)
[Sphinx](http://sphinx-doc.org/) with [sphinxcontrib-emacs](http://sphinxcontrib-emacs.readthedocs.org/en/latest/) can build Texinfo from reStructuredText (similar to Markdown), which is almost as good as manually written Texinfo, with a considerably better HTML output and easier source markup.
How come no one ever told me that the reader monad was the same as the SK combonator calculus before? edit: Also, B = join and W = fmap
Well, pretty much any code that involves the `(-&gt;) r` monad is unreadable to me. :(
For many platforms, virtual machines are ubiquitously available and work well. For mobile platforms, you are anyway cross-compiling.
Me too. You do need to leave blank lines - either totally blank or a blank comment line - between paragraphs, as for regular auto-fill. Also - I am using `haskell-indent`, not one of the others. Perhaps that is what makes it work. auto-fill also works for me for haddock comments, and even for `-- ^` when placed to the right of definitions.
I do believe anecdotally that haskell is going main stream judging by the number of stupid posts that are not about monads or category theory in this subreddit.
Interesting, thanks for that Info! `rST` would definitely be easier to handle for the uninitiated.
Does anyone have more details on this comparison between CPS and deep encoding styles? The folk understanding I have is that CPS ekes out more speed on GHC generally and that deep encodings can have exponential blowup. What causes the deep encoding to beat out CPS?
Sounds good. I've a few more things to document (next weekend) and then there'll probably nothing big for a while. 
Code can definitely get weird unintentionally. I saw this code in a where clause on a codebase I'm working on: fromEres d f x = fromMaybe d $ join $ either (const Nothing) (Just . f) &lt;$&gt; x Stared at it for a minute. Rewrote it to: maybeEither a = maybe a . either (const a)
Yes, although we do also want failures for builds too (and we'll support them in the form of users uploading reports, because really that is the report you need). Both would be very useful, to be clear. In fact we already have most of the infrastructure for build reports, we just need to use it...
Even imperfect information could inform the heurestic
Oh, I think I misunderstood the intended purpose. I thought it was meant as some kind of database to be used by the solver itself in which case false negatives would be a problem. If it is only meant to be used by developers there is no problem with the cases I mentioned of course. 
I would like to note that artificial neural networks could go in the same direction you are describing: Extrapolating a function by examples; where in theory everything is possible (if you have enough data). Read up on it, it is extremely interesting and quite easy to start with. ;)
Can we have [*safer*](http://www.reddit.com/r/haskell/comments/1q4r3b/mindbending_behavior_for_deserialization_in/) while we are at it?
The deep embedding is really only half deep; there's still the trick of making the actual DSL just a function that transforms the data structure, which gives you constant time append instead of linear time append. I think the win of the deep embedding is that it generates less code and allows you to pretty easily write a tight loop (albeit separately from the code that generates the intermediate data structure). I am still a bit surprised, though; I'm not convinced right now whether the speedup is due to this trick or due to the different serialization protocol. If the former, I'm intrigued.
@kamatsu: Thanks for the feedback! What do you mean by: "...there's not really a propositional analogue to "[a]" that is at all useful"?
It's extremely performant for C, because its format mimics the layout of C data structures in memory. So for C, the encoding time is zero - just dump the bytes as-is, pretty much. Whereas if we are talking about Haskell, I'm not sure why that format would necessarily be so performant.
I looked at it. It's interesting in its own right for various reasons but it doesn't fit what you'd want for a Binary class. It has most of the same disadvantages of the existing encoding that we use for the Binary class in the current binary package.
get your friendly binary distro from www.haskell.org/ghc/download_ghc_7_8_2 open the tar ball cd in ./configure --prefix=$yourSpecialPRefix make install then when you wanna use that newer ghc, add $yourSpecialPrefxi/bin to your path :) 
Here's one way: install the other GHC using the Linux binary tarball from the GHC site. Use `./configure --prefix=/usr/local/ghc-7.8.2` or whatever. Then if you want to use that version temporarily, run a subshell with `bash -i` and in that shell add `/usr/local/ghc-7.8.2/bin` to the front of your `PATH`. I recommend that you use the current version of `cabal-install`, not the one that your package manager installed, even for the default GHC. One way to do that is to `cabal install cabal-install`, copy the cabal binary to someplace useful like `/usr/local/cabal`, then add that to your default path in `.profile` (or `.bash_profile` or whatever). (I actually compile `cabal-install` in a sandbox. When I install a new GHC whose bundled cabal version is `&lt; 1.18`, the first thing I do after installing Haskell Platform is `cabal install cabal-dev`, then create a directory and do `cabal-dev install cabal-install` inside it, and then never do anything ever again outside of a native cabal sandbox with the new cabal.)
Right, if you keep the high performance in-memory format then you can't use your normal Haskell type, and if you do keep the normal Haskell type then you need a conversion step, at which point the only advantage is in the encoding format itself which is not all that different from our existing one (and bad in various ways I described in my talk).
I admit that I didn't read the paper yet, but if you do schemaless, how do you ensure type safety? Slapping a hash of the structure of whatever type is it that you're saving or something?
Oh it's definately that trick and not the encoding format. We started with that trick and switched the encoding later.
I'd like the format and decoder to be usable with untrusted input, at least for the instances for all the usual types (we can't enforce it for instances you write). So that means avoiding naive use of length prefixing (ie don't just allocate 1G because a length prefix asked you to) and unchecked data invariants (as was the problem in discussion you linked).
&gt; I don't know if this is possible to do in haskell Of course it is, GHC can even compile your programs to run on Xen. In the end, the only thing the RTS *really* needs is a way to acquire memory, the rest is fluff (who needs RTS on stdout?) and can be done via libraries. You should probably add a syscall primitive to the FFI, then, though. Relying on unsafeCoerce to run assembly doesn't sound like a good idea.
Your use of `-traditional` is redundant. All my tests of `ghc -v3` show it always adds `-traditional`, unless you use `-pgmP cmd`. I suppose it's also good for future proofing, eg., if ghc decides to switch to `-ansi`. As a package writer, I wish I could choose between `-traditional` and `-ansi` per file. I would just switch all the sources to `-ansi` and be done with it (clang supports ansi just fine). **Edit:** Oh, and calling `cpp` directly is hard for two reasons a) because you really want to get the same C compiler/CPP than ghc is configured to use (figuring that out requires `Setup.hs` code?); and b) `clang`'s CPP takes different arguments than `gcc`'s CPP. ansi has issues with # and ## but that's easy to workaround. `#define HASH #` and then `{-# INLINE foo #-}` becomes `{-HASH INLINE foo HASH-}`.
So you basically just used Data.Aeson in the simplest possible way, per the toy examples in the library documentation? Sorry, not seeing anything blog worthy here. Maybe I just haven't had my coffee.
(r -&gt;) **should** be valid syntax, just like TupleSections make (x,) valid syntax. We should have FunctionTypeSections.
evalState/runState/execState are always used flipped, I use them with the backtick idiom: (`evalState` g) . ... I personally find your point-free version quite readable! Also, it's less error-prone, note how you forgot to use "newSeed" :P
There are idioms that depend on it. e.g: Using `join` to duplicate an argument is quite a common one: sqr = join (*) 
Ok, so it's not zero value but it doesn't have a very big value. I would take it if we decided to an encoding and the schema description came along mostly for free. Being interoperable with something like Thrift is not a design goal (if you do: target that specifically), and achieving interoperablity in practice is quite hard: whether or not the Haskell data types can match up with an externally defined IDL nicely is basically down to luck in how the IDL was designed.
&gt; For example, template-haskell depends on containers. As a result, imagine if you try to use containers 0.5 and template-haskell when on GHC 7.4. Since template-haskell depends on containers-0.4.2.1, you'll run into issues. FYI, this one was recently defused in GHC HEAD by [dropping the dependency on `containers`](http://git.haskell.org/ghc.git/commit/07388af843ad61757207a54d75ab336606beed4f). This fix is a potential candidate for getting merged into GHC 7.8.3. 
**Here is my best attempt so far:** although I am compiling with -O3 (full optimization), the Haskell program is still taking about 3 times longer than python. I think with a bit more tweaking I can get the numbers down. import qualified Data.Map.Strict as M import qualified Data.ByteString.Char8 as B main = fmap (foldr (\line m -&gt; M.insertWith (+) line (1::Int) m) M.empty . B.lines) B.getContents &gt;&gt;= mapM_ (\ (line, count) -&gt; putStr (show count ++ "\t") &gt;&gt; B.putStrLn line) . M.assocs **First the control test** Command being timed: "python challenge.py" User time (seconds): 1.15 System time (seconds): 0.02 Percent of CPU this job got: 55% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.13 Maximum resident set size (kbytes): 14932 **Now my results:** Command being timed: "./challenge" User time (seconds): 3.20 System time (seconds): 0.46 Percent of CPU this job got: 72% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:05.08 Maximum resident set size (kbytes): 211972 
&gt; I will freely admit that cabal does not do this now, but it is by no means clear to me that it is impossible. I'll just leave this here: https://ghc.haskell.org/trac/ghc/wiki/Commentary/GSoCMultipleInstances
Try [envirius](https://github.com/ekalinin/envirius). It allow to create isolated virtual environments for haskell with any version of the haskell. For example: ➥ nv ls-versions --haskell-prebuilt | tail -n 1 7.8.1 7.8.2 ➥ nv mk --haskell-prebuilt=7.8.2 --on (haskell-prebuilt-7.8.2) ➥ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.8.2 (haskell-prebuilt-7.8.2) ➥ nv off exit ➥ nv mk --haskell-prebuilt=7.8.1 --on (haskell-prebuilt-7.8.1) ➥ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.8.1
oh, you're right. should have checked that.
Yeah, i guess so. my mistake :-)
**Got better results with Data.HashMap.Strict in the unordered-containers package:** although still takes 2 times longer than python. Not bad for simply swapping out an import (and changing M.assocs to M.toList). I am starting to see that the performance of the program depends entirely on HashMap and ByteString IO, the libraries rather than the code in your own program. The program itself is so trivial that there is not much one can do to optimize it. The best you can do is try to improve IO performance using the pipes package, which again is a matter of using more efficient libraries. It is up to us to decide the most efficient libraries to use. Too bad GHC's base package does not provide the most efficient solutions by default. Still, the libraries available to us are pretty darn efficient. There comes a point where we must make a choice in a trade-off between efficiency and code quality. import qualified Data.HashMap.Strict as M import qualified Data.ByteString.Char8 as B main = fmap (foldr (\line m -&gt; M.insertWith (+) line (1::Int) m) M.empty . B.lines) B.getContents &gt;&gt;= mapM_ (\ (line, count) -&gt; putStr (show count ++ "\t") &gt;&gt; B.putStrLn line) . M.toList **Results:** Command being timed: "./challenge" User time (seconds): 2.11 System time (seconds): 0.09 Percent of CPU this job got: 61% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.59 Maximum resident set size (kbytes): 212948 
You essentially need a `join` combinator equivalent to `join f x = f x x`, which I don't believe you can make using point-free code. Then you can transform it something like this: g (h x) x (g (h x)) x ((g . h) x) x join (g . h) x 
Is the Reader monad bijective then? If I stick to the map analogy then one key can only have one value. Additionally every key needs to have a value. Which would make it bijective, but I'm not a math geek.. :)
additionally it's easier to refactor things when using regular functions instead of record accessors. see the transformers-0.4.0 build failures.
This looks really nice. I run my save files through gzip and get 90% compression, I could maybe stop doing that. I also have my own manual versioning, I could get rid of that too. And I've had to find broken serialize instances by trial and error due to the useless error msg, I could stop doing that too. But I have a question about versioning. Currently, I just save a version tag. It's annoying and manual, so I don't it for tiny types that "never" change, and it's a problem when they inevitably do change, but on the other hand it's zero overhead. It seems like this approach to versioning stores the version in the constructor tag, so it only works for types with multiple constructors. So if you want to save a newtype do you still have to do manual versioning? I actually wouldn't mind paying a byte for newtypes if it meant I got versioning on everything.
How about this: http://byorgey.wordpress.com/2012/11/01/using-multiple-versions-of-ghc-in-parallel-with-gnu-stow/ 
It's probably going to fail quite nastily, though. That is, a program won't be able to tell an encoding/transmission error from a user error (opened wrong file, version mismatch etc). I'd suggest to at least offer (not necessarily require, may not make sense in all areas) some kind of standardised application/version/type-tagging, for purposes of sanity. CBOR natively supports CBOR-in-CBOR, so maybe have a default file writer/reader that [spits out|reads optionally] first 0xd9d9f7 as CBOR magic, then magic saying "this is some haskell data type", then, in whatever order, application/version and typehash, and on top of that whatever else. The first tag of type 24 is then the actual payload. Yet another upside: `file` is going to be able to tell you more about that file than that it's data. If there's no library-advertised standard way of doing that I doubt there's going to be any kind of standard and furthermore, people might just serialise stuff to disk in a raw manner.
Dangit. Can I blame it on being preoccupied with other things? Good catch, though. Those are the kinds of bugs that can be really painful in the worst of cases.
 join :: (r -&gt; r -&gt; a) -&gt; (r -&gt; a) join f = \env -&gt; f env env 
That was a *very* neat derivation! Thanks.
Download link for the paper: http://www.cs.tufts.edu/~nr/cs257/archive/guy-lapalme/layout.pdf
I know that's how it works, but I still need to sit down with pencil, paper and the type signatures to figure out why `join (g . h)` produces the expected result in this case. Edit: Though viewing it as `join (fmap g h)` makes it a little more intuitive for me, keeping in mind that `g` is already lifted, so mapping it over `h` will make a doubly lifted thing which `join` undos. This also makes it obvious why `g =&lt;&lt; h` works.
I've said this a few times, but here it goes again. My own heuristic for using pointfree code is simple: if it looks like a pipeline then pointfree is fine. To me, it's important to determine "indexing" point and "flowing points". As an obvious example, the predicate to `filter` is indexing while the list is flowing. Then, I bubble indexing points up to the top and eliminate flowing points. sth ixs p = reverse . filter p . zip ixs If you use this rule consistently then pointfree style becomes an indicator that there's a "pipeline" of some kind behind the code you're writing. It also means you'll heavily favor infix operators which are the compositions in some category. It's also fairly easily done recursively. A common utterance might be sthelse p = a &gt;=&gt; b (p . next . foo) &gt;=&gt; z
and you say that because of a post by roman cheplyaka (roche), one very prolific author of haskell packages. classy.
 Agreed on the field numbers, but those are nicely listed in the message definition file. I guess what's missing here is a message definition, which I was hinting at. Some template Haskell that autogenerates serializers and deserializers from a data definition with extra field number annotations (for instance in pragma comments).
I didn't quite mean it to come across as a complaint, as I am rather happy with having a central package repository that is the 'go-to' place for almost all packages written in Haskell. I didn't really even know that GHC allowed that -- as I said, I posted here because I was pretty sure there was stuff I didn't know about / would be misunderstanding. In my personal opinion, I don't care too much whether it's GHC that 'gets better at it' or cabal. Cabal nice as a build tool, so it would make sense to tell it to bring a certain version in of some package(which it would preferably fetch and compile only once). Whether most of that work would be delegated to GHC or not doesn't matter much to me. EDIT: For clarification: By most of that work, I mean the work of dealing with the packages, their versions and dependency trees.
Yes, Nix came up during the course of our discussion several times, yes, and having looked at it a bit it definitely sounds interesting. It's on my list of 'things to play with at some point' for sure.
It's not expressible any other way currently, except via an explicit Flip newtype.
Good advice. We try to have most core libraries support the last 3 major releases of GHC. We also use CPP to support older libraries.
Thanks for your reaction, which is a nice example of the tension that often exists between developers on the one side, and sysadmins on the other. If the application that you are developing lives on a single server that you are solely responsible for, then sure, having the developer patch their version of the library is likely the best possible solution. But I've been in many situations where 5 years down the road, the developer is long gone, there's not all that much documentation, nor useful comments in the source, and now news of a vulnerability in X is all over the news. But how do we find out if we have any instances of X around? It could be in a user's homedir, system wide in a Ruby gem, a Cabal package that's in the homedir of the www-user. And if I do find it, how to then solve the problem? Download the latest version of X, put it on top of the old version, and see what breaks? That's why I very much favour the installation of these things as OS packages. An OS distribution gives you a snapshot in time of a set of packages that are known to work together, and any vulnerabilities during the support lifetime will be backported, so you have a stable platform that you can keep fairly secure. And lifting the application to the next OS release is something that will require investment of development resources at that point in time again.
I had indeed been planning on making the file-at-a-time API use the CBOR magic header. However I don't think we can use application/version except as a non-checked description, and we cannot have a typehash. There is a versioning mechanism but it is not at the whole-file level so app version and type hash would not be right, at least not in a lowest common denominator API.
&gt; Agreed on the field numbers, but those are nicely listed in the message definition file. Which we do not have. &gt; I guess what's missing here is a message definition, which I was hinting at. Some template Haskell that autogenerates serializers and deserializers from a data definition with extra field number annotations (for instance in pragma comments). Right. The versioning mechnaism I've suggested here is just a mechanism. It's clearly not a nice user friendly system on its own. We would need automation on top to make it nice, and to avoid common pitfals. TH is likely the best tool for that. What I'd been thinking of was some TH-based system where you give some sort of change history / changelog of the datatype which is checked for completeness and used to generate the serializers and deserializers for the current and all old versions.
Thanks for the write-up. Minor quibble (from a novice Haskeller at best). {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE ConstraintKinds #-} {-# LANGUAGE RankNTypes #-} Personally, I'd have appreciated an explanation of why these were needed. `TemplateHaskell` is presumably to enable lenses (or rather `mkLens`) which you do discuss, but it took me visiting #haskell (thanks guys!) before I understood how 1) it was even possible to define `NonInteractive` and 2) that doing so relied on `ConstraintKinds`. As a beginner, I still don't understand what the other two are for. I hope you'll forgive my tone. It's not meant as a "you're doing it wrong" so much as a "my brain hurts...must...understand...argggg" type of thing. I look forward to the next installment :-)
You mean like [this](https://github.com/hvr/multi-ghc-travis/blob/master/.travis.yml#L34-L48)? (That doesn't require `cabal freeze` btw)
Perhaps you are right. I will freely admit that there are clearly perspectives from which I hadn't looked at this whole ordeal. But thanks for your comments -- the purpose of my post was to learn, after all. At any rate, I hope cabal keeps evolving, even though I'm rather fond of sandboxes as they are.
I too wish that some of these choices (especially the use of the typeclass) were better motivated, but overall I enjoyed the post.
Yes yes yes, but where is it, hmm? some of us want to use it, you know :)
For some reason YouTube won't allow me to open up the video description. Are there slides anywhere?
Here is an article about installing on Debian Wheezy, but you can use the same principles in Fedora: http://www.extellisys.com/articles/haskell-on-debian-wheezy
Oh, yup. So it seems like OP has a valid point. 
I can only see the text and the title in the slides. The figures and functions can't be made out. 
&gt; I think that […] look a lot better than the consing form. And I think that using cons when you have single elements looks better. If people are doing it for optimisation reasons, that’s a bit silly, but if it’s a matter of aesthetic preference, who cares? People do things with Haskell’s layout rule that bother me *much* more than this! Take a case where you want to generate a length-prefixed list: list(3, "a", "b", "c") You can do it neatly like so: "list(" ++ intercalate ", " (show (length xs) : map show xs) ++ ")" It wouldn’t occur to me to write `[show $ length xs] ++` instead of `show (length xs) :` because *what I actually mean to do* is to cons a single element onto a list. You could draw the opposite conclusion and I wouldn’t question it in code review—though I suppose HLint would. 
I can't answer #2 for you, but from what you say, it sounds like profiling is done by sampling: the program tells the OS to alert it every so many milliseconds, and each time this happens it records what is executing at the time. So the profiling code will run regardless of whether the main execution is doing anything or sleeping.
[ghc-mod](http://www.mew.org/~kazu/proj/ghc-mod/en/) is pretty helpful here, especially when passed `lint`. Hint: function application binds tighter than (.) and ($), so you can shed a few parentheses. Also, `intercalate " "` is just [unwords](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:unwords). Lastly, you can simplify tokenize and other functions by using `Data.Char` instead of `Data.Digits`. 
using a tag for newtypes is scary to me. Newtypes should incur no overhead. Ever. I don't want to have to have to worry that my using a newtype instead of a type synonym will incur an extra byte of overhead in serialisation.
If tempate-haskell doesn't export the container types, why can't an application link both template-haskell with an old containers with other code using a new containers?
 cEffect . traverse . _GainFunding . to computeFunding :: Fold' Card Funding 
This is what I ended up doing. Thanks so much. Looks to be working for everything so far. 
Thanks that's really helpful. 
Type level lambdas are possible but they add problems too. They're not free. If you had these two instances, how would you refer to the lambda type one? You can use the Flip newtype which is quite similar to that but lets you refer to it explicitly.
RankNTypes is for being able to write the cardEffects and playerEffects type signatures. I *think* it is possible to skip these extensions if I was to define an ASomething instead of Something, but I also think there is a performance hit in doing so. Not that performance matters much in this case. FlexibleContexts is also here so that you can write the "MonadState GameState m" constraint for the typeclass.
I failed on this, the choice of the typeclass was indeed something important to document, and I will try to explicit it in the next part. Basically, I wrote this typeclass so that I could start writing the rules without having to think on how it will be implemented. In practice I will probably end up using the operational package and scrap the typeclass in favor of a ProgramT definition.
Good catch, I just fixed it ! I should have checked in ghci ... (Also I think it's just Fold, not Fold')
I just added a small note on this in the article.
Being somewhat familiar with protocol buffers, I agree that matching protocol buffers probably doesn't make sense for your purposes. But, just to be clear about the comparisons and tradeoffs: Protocol buffers assigns a key to each record field so that fields can be added/removed for migration/versioning purposes. While this allows fields to come in any order, it is something that's discouraged, as described in the [protobuf docs](https://developers.google.com/protocol-buffers/docs/encoding#order). This doesn't change the fact that it's allowed and harder to parse. Having a key for each field is similar to the haskell/CBOR versioning model described in the slides. Differences being that in the proposed model in CBOR, it is done on the entire record constructor; where in protocol buffers, it is done for each individual field. Protocol buffers were designed with changing RPC APIs in mind. They traded off some compactness and some performance in favor of extensibility. Having it keyed on fields affords the ability to add/remove optional fields and have either caller/callee to use old versions without completely discarding any messages. In the entire record constructor model, an unknown (new/modified) constructor will have to be discarded/errored on entirely.
Maybe, but if I do the same on Windows, it is virtually no CPU even in the profiling case. So this might be related to the way things are scheduled on Linux (if so), and not the way GHC collects samples? Also, this was in the host OS. I will try a native Linux host to rule out the VM having some odness.
...mostly because Cabal &amp; GHC don't support non-exposed/internal build-deps (yet)...
My main problem with your code is that you don't have a nice abstraction for the runlength algorithm. The following signature `runlength :: Integer -&gt; Integer` is not very useful. I would rather write `runlength :: Eq a =&gt; [a] -&gt; [(Int, a)]` and have another function `encodeInt :: Int -&gt; Int`.
i skipped a few from the list, because e.g. the dismissal of messagepack is mentioned in the slides.
Pattern matching is often clearer than calling accessor functions. Also, `f` is an idiomatic name for a general function: it's usually confusing if you use it as the name of something that's not a function. expressGroup (count, noun) | count == 1 = phrase ++ "." | otherwise = phrase ++ "s." where phrase = unwords . map expressSingle $ [count, noun] 
I think the way it's written out is really hard to understand. p x = do s &lt;- get put $ s + 1 return $ x == 'c' I think this makes the function entirely more obvious. I can give you a more detailed breakdown of how I did the desugaring when I am not on my phone. EDIT: full desugaring. p x = (\s -&gt; (const $ pure (x == 'c')) =&lt;&lt; put (1+s)) =&lt;&lt; get { definition of &gt;&gt;= } p x = get &gt;&gt;= (\s -&gt; (put (1+s) &gt;&gt;= const $ pure (x == 'c'))) { definition of const } p x = get &gt;&gt;= (\s -&gt; (put (1+s) &gt;&gt;= \_ -&gt; pure (x == 'c'))) { definition of &gt;&gt; } p x = get &gt;&gt;= (\s -&gt; (put (1+s) &gt;&gt; pure (x == 'c'))) { removing redundant parenthesis } p x = get &gt;&gt;= \s -&gt; put (1+s) &gt;&gt; pure (x == 'c') { translation to do notation } p x = do s &lt;- get put (1 + s) pure (x == 'c') 
... did you notice the more than 10-fold memory usage increase? &gt; First the control test &gt; &gt; Maximum resident set size (kbytes): 14932 &gt; &gt; [...] &gt; &gt; Now my results: &gt; &gt; Maximum resident set size (kbytes): 211972 
What packages do I need to compile that? When I tried to compile that I got Could not find module ‘Data.Conduit.Binary’ Perhaps you meant Data.Conduit.List (from conduit-1.1.2.1) Data.Conduit.Lift (from conduit-1.1.2.1) 
Learning by doing is the best way. When I first started learning Haskell, I switched to using the xmonad window manager, as I would then be motivated to learn Haskell in order to customise my work environment. In general, (re)writing small tools that you use every day is a great way to learn a new language.
so, like writing grep or cat or something? I think I have something along the lines of an intermediate understanding of haskell. I could really use some more involved projects to develop a sense of best practices and patterns that I would use in haskell, just like I'd do for imperative languages.
Use [docker](https://index.docker.io/search?q=ghc). $ sudo docker run -i -t alanz/haskell-ghc-x-64 ghci where `x` is 7.4.2, 7.6.3 or 7.8. You can also easily make your own docker containers. Only works on 64bit, atm.
Just as a quick note: if you have missed the wonderful `Traversable` and `Foldable` classes, take a few minutes to get to know them too.
links to articles?
The [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) has sections on Foldable and Traversable. Stephen Diehl's [What I wish I knew when learning Haskell](http://dev.stephendiehl.com/hask/) also has a [small section](http://dev.stephendiehl.com/hask/#foldable-traversable).
and it's great to have good ctags support in your editor to jump to function definition instantly. see the codex project introduced a few days ago.
If you are as familiar with Haskell as you say, you can probably just read the documentation on [Foldable](http://hackage.haskell.org/package/base/docs/Data-Foldable.html) and [Traversable](http://hackage.haskell.org/package/base/docs/Data-Traversable.html) and you'll be good. They contain generalisations of functions you've seen before that have only worked for lists, such as mapM, sequence and others.
That second article looks fantastic. It seems as though it contains a few bits on parts of Haskell that I've been avoiding because I've been afraid of them. I'll be sure to read it more thoroughly!
Ah yes, that was be due to my use of **foldr** . I tried it again with **Prelude.foldl** and improved the memory performance: User time (seconds): 2.07 System time (seconds): 0.03 Percent of CPU this job got: 86% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.44 Maximum resident set size (kbytes): 86140 Still not as good as Python, but not a bad improvement for changing-out a single function. I fiddled around with the RTS options to see if forcing a smaller heap could improve memory usage, but it didn't. The memory usage seems to be a result of the implementation of **HashMap** and/or **Data.ByteString.Char8.getContents** . I could do profiling to try and figure out what the memory hog is, but that will have to wait until later. import qualified Data.HashMap.Strict as M import qualified Data.ByteString.Char8 as B main = fmap (foldl (\m line -&gt; M.insertWith (+) line (1::Int) m) M.empty . B.lines) B.getContents &gt;&gt;= mapM_ (\ (line, count) -&gt; putStr (show count ++ "\t") &gt;&gt; B.putStrLn line) . M.toList 
Thank you for the explanation but I'm still a bit lost here. I just wanted to point out that the course (at the point where I am) doesn't use do notation or &gt;&gt; (although I think i have an understanding what it does). My problem (even with your explanation above) is that I still do not understand how the "p x" looks like in the end in a form of State expression. :info p p :: Num s =&gt; Char -&gt; State s Bool From what I understand p x ends up resulting in something like State (\s -&gt; (x == 'c', 1 + s)) but I think I still need some guidance how we get there.
so foldable effectively defines or asks you to define a way to iterate over a recursive data structure/any data structure that can be described as a part and the rest? Traversable seems to be similar to foldable, but with applicatives rather than plain functions. I'd have to read the documentation over a couple times, but is that close?
[Slides (PDF)](http://community.haskell.org/~simonmar/USI2013.pdf)
Brilliant. Thanks!
Just wanted to point out that /u/simonmar posted the slides in this discussion!
This is exactly what I came in here to say. The reason I have so many projects in Haskell is it is amazing drill. It also helps you develop a sense for what works and what doesn't work for API design, when you just try out all the alternatives before settling. Even within one project you can play with this, as Haskell is very good at surviving large refactorings with its faculties intact, so whenever you question whether you have the right API, just try the other and see if it is better or worse in practice.
`Foldable` is basically anything you can extract a list of elements from. `Traversable` is like `Functor` where you can 'map' with applicative side-effects. Like with `Foldable`, it requires you to be able to visit the elements in some kind of order, but it is closer in sentiment to a more powerful `Functor` for things that only have countably many values to visit that you can use to change the type of the contents. The restriction that prevents it from working for all the things that Functor can handle is that `Functor` can sneak under functions, while `Traversable` can't glue together the applicative effects for everything down there.
Your understanding is spot on. We can carry on unfolding definitions. p x = get &gt;&gt;= \s -&gt; put (1+s) &gt;&gt; pure (x == 'c') { definition of get } p x = State (\s -&gt; (s, s)) &gt;&gt;= \s -&gt; put (1+s) &gt;&gt; pure (x == 'c') { definition of &gt;&gt;= } p x = state $ \st -&gt; let (y, st') = runState (State (\s -&gt; (s,s)) st in runState ((\s -&gt; put (1+s) &gt;&gt; pure (x == 'c')) y) st' { definition of runState } p x = state $ \st -&gt; let (y, st') = (\s -&gt; (s,s)) st in runState ((\s -&gt; put (1+s) &gt;&gt; pure (x == 'c')) y) st' { application } p x = state $ \st -&gt; let (y, st') = (st, st) in runState ((\s -&gt; put (1+s) &gt;&gt; pure (x == 'c')) y) st' { substitution of let bound variables} p x = state $ \st -&gt; runState ((\s -&gt; put (1+s) &gt;&gt; pure (x == 'c')) st) st { application } p x = state $ \st -&gt; runState (put (1+st) &gt;&gt; pure (x == 'c')) st { definition of put } p x = state $ \st -&gt; runState (State (\x -&gt; ((), 1 + st)) &gt;&gt; pure (x == 'c')) st { definition of pure } p x = state $ \st -&gt; runState (State (\x -&gt; ((), 1 + st)) &gt;&gt; State (\s -&gt; (x == 'c', s))) st { definition of &gt;&gt; } p x = state $ \st -&gt; runState (State (\x -&gt; ((), 1 + st)) &gt;&gt;= \_ -&gt; State (\s -&gt; (x == 'c', s))) st { definition of &gt;&gt;= } p x = state (\st -&gt; runState (state $ \st' -&gt; let (y, st'') = runState (State (\x -&gt; ((), 1+ st))) st' in runState ((\_ -&gt; State (\s -&gt; (x == 'c', s))) y) st'' )) st { definition of runState / application } p x = state (\st -&gt; runState (state $ \st' -&gt; let (y, st'') = (\x -&gt; ((), 1 + st)) st' in runState (State (\s -&gt; (x == 'c', s))) st'')) st { definition of runState / application } p x = state (\st -&gt; runState (state $ \st' -&gt; let (y, st'') = (\x -&gt; ((), 1 + st)) st' in (\s -&gt; (x == 'c', s)) st'')) st { application } p x = state (\st -&gt; runState (state $ \st' -&gt; let (y, st'') = ((), 1 + st) in (x == 'c', st'')) st { substitution } p x = state (\st -&gt; runState (state $ \st' -&gt; (x == 'c', 1 + st)) st) { definition of state / runState } p x = state (\st -&gt; (\st' -&gt; (x == 'c', 1 + st)) st) { application } p x = state $ \st -&gt; (x == 'c', 1 + st) That was longer than I thought it would be. I have tried not to skip anything. It's all just syntactic substitution and application. That said, I would never bother going to that level of unfolding. `get` grabs the state. It's bound to `s` in the original expression, put increments it. pure wraps up a computation that checks if the value passed to the function is equal to the character 'c'. All of that can be inlined as I just showed, but you _shouldn't_ have to. There is a function called modify that compacts the get/put part.
**Interesting.** It looks like this program has the exact same time and memory characteristics as: import qualified Data.HashMap.Strict as M import qualified Data.ByteString.Char8 as B import Data.Monoid main = B.interact $ mconcat . fmap (\ (line, i) -&gt; B.pack $! show i ++ '\t':B.unpack line ++ "\n") . M.toList . foldl (\m line -&gt; M.insertWith (+) line (1::Int) m) M.empty . B.lines **Results:** Command being timed: "./challenge" User time (seconds): 2.04 System time (seconds): 0.04 Percent of CPU this job got: 99% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.10 Maximum resident set size (kbytes): 87232 This is a strong indication that performance depends entirely on the "Data.HashMap.Strict" and "Data.ByteString.Char8" modules.
Wow. Just wow. Thank you very much for this. Tried doing the same but for some reason was unable to. It is very clear now. For the purpose of learning I think it is good to go through these details once in a while to grasp the concept what is going on underneath. In the future of course that will not be needed. Once again thank you very much for your help
I did a bit of this once: &lt;https://github.com/alexander-b/coreutilhs&gt;. A patch with grep would be very welcome! Refactoring it all for code reuse would also be an excellent exercise.
Agreed, it's good to get a feel for how things work. I would say adopting a calculational style and explicitly noting down for yourself exactly what you are doing is key. This kind of unfolding and application of functions is totally mechanical and no special insight is required. Just patience and accuracy :) 
Thanks!
isn't that already implemented as of [this commit](https://github.com/aloiscochard/codex/commit/d93b0bad3268cc3b891cf73584d0bde30b00ab2a).
That makes a lot of sense. I was using an old CS lab from last semester (still in school) that we had implemented in python. I had thought about possibly abstracting run length to work with more data types, and that seems like a logical 'next-step'. Thanks!
Additionally, those brackets could semantically wind up more than one character pretty easily, at which point cons wouldn't work anymore (well, it would be two conses for the one logical element).
What worked best for me was to take on the kinds of projects that was accustom to doing in other languages (in my case, web servers). By staying with a problem domain that I fully understood, I could focus on learning just Haskell instead of having to learn stuff like HTTP status codes as well. If you are in a similar boat, you could check out one of the Haskell web frameworks. The major players in the web arena are: * [Yesod](http://www.yesodweb.com/), based on template Haskell, can sometimes feel like magic. * [Snap](http://snapframework.com/), based on combinators. * [Happstack](http://happstack.com/page/view-page-slug/1/happstack), based on combinators. * [Scotty](http://hackage.haskell.org/package/scotty), drop dead simple, one monad for the routing table (ScottyT) and one monad for each route (ActionT) At the risk of advertising my self, I have a [GitHub](https://github.com/AndrewRademacher/routing-comparison) project that contains a small example of each web-framework. Each example implements the same features so as to make for good comparison. If however, you are more interested in doing HPC style work. There are a few tools that will be invaluable to you. These are libraries that allow you to deal with huge numerical computations, especially for things that can fit into a matrix. * [Vector](http://hackage.haskell.org/package/vector), efficient array manipulation. Support for boxed and unboxed types. * [Repa](http://hackage.haskell.org/package/repa-3.2.3.3), regular array library, supports N-dimensional arrays, automatic support for parallel processing. * [Accelerate](http://hackage.haskell.org/package/accelerate), regular array library, supports N-dimensional arrays, can offload computation to a CUDA enabled GPU. Again, at the risk of advertisement, I wrote a paper awhile back about how to optimize Haskell computations. It goes through using both the Vector and Repa libraries. * [Paper](http://blog.headcrab.info/haskell-optimization-and-the-game-of-life/) * [GitHub Repo](https://github.com/AndrewRademacher/game-of-life) If you are coming from an Erlang background, you are probably missing the ability to program clusters as a cluster. If that is the case you may want to checkout [CloudHaskell](http://haskell-distributed.github.io/). You should feel right at home there, as the projects unofficial motto seems to be "If in doubt, do it the way Erlang does it." There are also some specific language resources that you might be interested in. * [Monads for functional programming](http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf), hands down one of the best resources for how and why you can use monads. * [A monad is not a burrito](https://www.youtube.com/watch?v=46Z7Hq4fhN0), despite the fact that the examples are in Clojure, this video contains a fantastic couple of examples for when to use monads. This one is great because it contains zero "theory". * [Monad Transformers Step by Step](http://www.cs.virginia.edu/~wh5a/personal/Transformers.pdf), this is the best documentation available for the MTL library. * [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia), I didn't find this document until 6 months after starting with Haskell, hopefully this is getting to you sooner. This fully explains the important typeclasses in Haskell, including Monad, Applicative, Arrow, etc.
2^30 is about *one billion*. With a perfect binary tree you have twice that number of nodes. Your issue is not with the stack or recursion...
Estimate how much code you wrote in your very first language before you became good at it and write that much code in Haskell.
may I recommend refactoring your function to use a pure source of randomness, or even use State and take something that uses the state to produce a value?
Thank you "benzrf" but I am really new to Haskel and have no idea what a &gt; pure source of randomness really means. The randomness here is completely unimportant. I just wanted to have the tree be different every time and use IO for learning purposes.
Thanks. Yeah ... 2^30 is a lot :) I can get it to complete with 2^24, but now I am having trouble understanding my own code :) Why doesn't it produce the stack overflow? 
well, in most impure languages, functions that return a random number mutate the state of a PRNG and take the next output. In Haskell, there are functions that have types like PNRGState -&gt; (Int, PRNGState) so that you can manually and purely pull `Int`s out of states and get the next state. Relatedly, `m a` in the `State` monad looks like: stateType -&gt; (a, stateType) This gives `&gt;&gt;=` the type: (stateType -&gt; (a, stateType)) -&gt; (a -&gt; stateType -&gt; (b, stateType)) -&gt; (stateType -&gt; (b, stateType)) In other words, composing functions that also pass along a state... such as a PRNGState. If you didn't get my 'explanation' of the State monad, don't worry; it sucked.
As a fun parlor trick, if you extract out a pure RandomGen with one of the `IO StdGen` functions in System.Random, then use `split` to break it in two and pass it down the tree to generate it, you can run a `buildIntTree` with as large a number as you like, since it should be fully lazy at that point, and be one thunk regardless of what you do. This basic technique is often used to [memoize things in Haskell](http://stackoverflow.com/questions/3208258/memoization-in-haskell), at which point it ceases to be a parlor trick.
Why would it? You only get a stack depth of the size of the number you pass in, and any language can handle a depth of 30 no problem. You're running out of RAM generating all those nodes strictly in RAM, but again, any language would have that problem (depending on exact RAM usage characteristics, but your tree size is going up exponentially, that will defeat any clever encoding scheme pretty quickly if it is manifesting _anything_ proportional to the number of nodes).
I think you are underestimating how much code we write to become proficient at programming.
Thanks that helps. Got scared about making a copy every time...
There is a simpler way to selectively disable certain operations: monad morphisms. For example, I keep this useful function in my toolbox: readOnly :: Monad m =&gt; ReaderT s m a -&gt; StateT s m a readOnly m = StateT $ \s -&gt; do a &lt;- runReaderT m s return (a, s) Then I can embed a read-only computation within a larger computation: example :: State (Int, String) () example = do (x, y) &lt;- readOnly $ do x &lt;- asks fst y &lt;- asks snd return (x + 1, y ++ "!") put (0, show x ++ y) `readOnly` is a monad morphism, meaning that the following computation is equal: example = do x &lt;- readOnly (asks fst) y &lt;- readOnly (asks and) put (0, show (x + 1) ++ y ++ "!") This is because: readOnly (return x) = return x readOnly (m &gt;&gt;= f) = readOnly m &gt;&gt;= \x -&gt; readOnly (f x) I like the monad morphism trick because it is very general (no restrictions on what the source or destination monad must be), and requires no buy-in from existing code.
Oh, I see your point :)
ezyang answered: Yes.
Why exactly is it such a big win? I would have expected it to be marginal, or perhaps even a net loss. Which of the benefits do you think helps the most? Smaller generated code? Fewer closures (for things that aren't really small benchmarks, at least)? The ability to more easily optimize the interpreter? Something else?
I guess this would be something like this, if I understood it correctly. {-# LANGUAGE DeriveFunctor, TypeFamilies #-} import Data.Monoid import Control.Applicative import Control.Monad(ap) class Monoid u =&gt; Updatable u where type StateOf u act :: u -&gt; StateOf u -&gt; StateOf u newtype Update u a = Update {runUpdate :: StateOf u -&gt; (u,a)} deriving Functor instance Updatable u =&gt; Applicative (Update u) where pure a = Update $ \_ -&gt; (mempty, a) (&lt;*&gt;) = ap instance Updatable u =&gt; Monad (Update u) where return a = Update $ \_ -&gt; (mempty, a) m &gt;&gt;= f = Update $ \s -&gt; let (u,a) = runUpdate m s (u',b) = runUpdate (f a) (act u s) in (u &lt;&gt; u',b) get :: Monoid u =&gt; Update u (StateOf u) get = Update $ \s -&gt; (mempty, s) update :: Monoid u =&gt; u -&gt; Update u () update u = Update $ \_ -&gt; (u, ()) data Trivial r = Trivial instance Monoid (Trivial r) where mempty = Trivial mappend _ _ = Trivial instance Updatable (Trivial r) where type StateOf (Trivial r) = r act _ = id type Reader r = Update (Trivial r) runReader :: Reader r a -&gt; r -&gt; a runReader u r = snd $ runUpdate u r instance Monoid k =&gt; Updatable (Const k a) where type StateOf (Const k a) = () act (Const k) = const mempty type Writer k = Update (Const k ()) runWriter :: Writer w a -&gt; (w,a) runWriter u = case (runUpdate u ()) of (Const k, a) -&gt; (k,a) tell :: Monoid w =&gt; w -&gt; Writer w () tell = update . Const instance Updatable (Endo s) where type StateOf (Endo s) = s act (Endo f) a = f a type State s = Update (Endo s) runState :: State s a -&gt; s -&gt; (s,a) runState u s = case (runUpdate u s) of (Endo f,a) -&gt; (f s, a) modify :: (s -&gt; s) -&gt; State s () modify = update . Endo I didn't bother to make it a transformer, even though it wouldn't be hard. This seems cool, but in practice I don't know how useful it would be in Haskell: I'd say not much.
As you can easily tell from the type signature buildIntTree :: Int -&gt; IO (Tree Int) the function runs in the IO monad, i.e. it may (and does!) have side effects. Therefore it is not a *pure* function. Of course, it cannot be because there is random number generation involved, which might turn out differently every time you run it! This is one of the nicest thing about the Haskell type system: It tells you a lot of things about what you can expect from a function in terms of behavior. In this case, it tells you that it may return different results every time it is run. So how can we get "pure randomness"? What we want -- ideally -- is a function of the following type buildIntTree' :: Int -&gt; Tree Int However, just by looking at this, we know that no random number generation can be involved since this is a pure function and therefore completely deterministic. We can however build a function with a type signature along the lines of `buildIntTree' :: RNG -&gt; Int -&gt; Tree Int`, where `RNG` is some sort of (pseudo) random number generator that has already been initialized for us. We *seed* the random number generator in the IO monad and thus obtain randomness and *then* call this *pure* function to build our tree. For further reading, you may want to check [here](http://hackage.haskell.org/package/random-1.0.0.3/docs/System-Random.html), especially the `RandomGen` class (and the `StdGen` instance) . You can use `newStdGen` in the IO monad to create (and seed) a new random number generator and then pass it on to pure functions, which can in turn use `next` to receive a random number from the generator and produce a new generator. Since we're talking about pseudo random numbers the whole thing can be done in a pure environment, without any need for IO or side effects.
Your `Updateable` seems to be what `monoid-extras` calls [`Action`](http://hackage.haskell.org/package/monoid-extras-0.3.3.2/docs/Data-Monoid-Action.html#): class Action m s where -- | Convert a value of type @m@ to an action on @s@ values. act :: m -&gt; s -&gt; s act = const id
Absolutely agree on the pattern matching. I didn't know you could match patterns like that, but it makes a lot of sense since Tuples are fixed-size. Thanks!
So Functor generalized map to more than just lists. You can think of Foldable as generalizing sequence_, and Traversable as generalizing sequence :) sequence :: Monad m =&gt; [m a] -&gt; m [a] turns into sequence :: (Monad m, Traversable t) =&gt; t (m a) -&gt; m (t a)
If you have DR6's `Updateable` implementation from below you can have the best of both worlds. withUpdates :: (StateOf u ~ s) =&gt; Update u a -&gt; State s a withUpdates u = state $ \s -&gt; let (ups, a) = runUpdate u s in (act ups s, a) 
Just some random thoughts here... Have you thought about using arithmetic coding to do the encoding? If you have a data type with 3 constructors: data Foo = Bar | Baz Foo | Quux Foo Foo Then with an arithmetic coder you could directly encode the 3 possibilities, which means that it would be encoded using only ~1.58 bits for the constructor tag. Basically, you have an `encode n k` function which encodes choice number k within a set of n choices (so in this case n=3, k=0,1,2 with 0 &lt;-&gt; Bar, 1 &lt;-&gt; Baz, 2 &lt;-&gt; Quux). Then on the decoding side you have a `decode n` function which returns the encoded k out of n choices. encodeFoo :: Foo -&gt; ArithmeticCoder encodeFoo Bar = encode 3 0 encodeFoo (Baz a) = encode 3 1 &gt;&gt; encodeFoo a encodeFoo (Quux a b) = encode 3 2 &gt;&gt; encodeFoo a &gt;&gt; encodeFoo b and decoding: decodeFoo :: ArithmeticDecoder Foo decodeFoo = do k &lt;- decode 3 case k of 0 -&gt; return Bar 1 -&gt; do a &lt;- decodeFoo; return (Baz a) 2 -&gt; do a &lt;- decodeFoo; b &lt;- decodeFoo; return (Quux a b) A tree with 5000 nodes would be serialized in under 1 kilobyte. Secondly, have you considered just serializing a representation of the Haskell static type directly to solve the problem that you lost the type and can no longer decode old data? Serializing data to a representation tailored for a specific type is very elegant and should be more efficient as well than serializing it to a self describing generic representation. CBOR is also not *really* self describing for Haskell data. Yeah, you're no longer working on the bit level, but it's still far away from known Haskell data types. Serializing the static Haskell type should also have advantages wrt versioning.
Yes, it's the applicative instance of `(-&gt;) r`, a.k.a. Reader.
Don and Edward have covered it pretty well, but I'd like to emphasize the value of reading other people's code. The space of things that you can do with Haskell is so large, it's impossible to explore it all yourself. So I think that no matter how smart you are, if you only wrote lots of code and didn't read anyone else's code, you would get stuck in local maxima and miss really cool ideas that others have thought of. Interacting with other people is crucial. Read their code, get them to read your code, collaborate with them, and you'll find yourself advancing MUCH faster.
Definitely not free since type inference for type lambdas requires higher order unification which is undecidable. But I guess you could use type signatures.
(Speaking as someone who writes mostly Python, here, but this applies regardless of the language.) If you have programmer friends, discuss your designs and implementations with them. Be open to their suggestions -- don't fall in love with your own ideas; "kill your darlings".
That is more complicated because it's an indexed monadic system, not a plain monadic one.
Actually it's the other way around: W = `join` and B = `(.)` = `fmap`
Thanks! I fixed it. That's what I get for writing code on my phone.
It's always been justified as an intentional oversight, so as to avoid the problems associated with allowing type-level lambdas. However, I'm not entirely convinced that ruling out type-level lambdas necessitates ruling out things like `(-&gt; b)` and `(,b)`. It seems to me that we can allow a restricted subset of type-level functions which is good enough for capturing patterns like this without getting the full power of lambda-calculus at the type level. Then again, I've never sat down to prove the point...
Euler? Monads? Math? Well, you ain't seen nothing yet my friend. Language is only 0.1% of what you really need to write software. The rest are tools and libraries and these are real demons in Haskell hell. To be good at Haskell you need a lot, A LOT of patience and superhuman hacking skills. E.g. try to make a cross platform 2D/3D app using SDL2 or SMFL. Cabal will eat you alive before you'll write a single line of code. May your god have mercy on your soul. 
&gt; You can use the Flip newtype which is quite similar to that but lets you refer to it explicitly. Actually, this cuts to the core of the problem with type-level lambdas. Namely, the problem is that TLLs are evaluated freely and leave no trace of that evaluation. Thus, with TLLs we need to use higher-order unification in order to "undo" evaluation whenever we try unifying types. Whereas, by using newtypes we make the trace of the evaluation explicit, which in turn means we don't need to search for how to undo evaluation: we just invert the trace. This same sort of issue shows up in a number of other places as well. For example, using isorecursive types eliminates many of the problems with using equirecursive types. When using equirecursion the compiler has to infer where the equivalence should be used, whereas with isorecursion we (the programmer) have to make the calls to to the isomorphism explicit. Another example shows up when comparing intrinsic vs extrinsic typing (aka: typing a la Church vs typing a la Curry). In the intrinsically-typed world a term can only have a single type, thus we must explicitly differentiate between the terms of type `Flip f a b` and the terms of type `f b a` (even though those types are isomorphic, they are not identical). Whereas in the extrinsically-typed world, it's perfectly fine for terms to have more than one type. However, extrinsic typing is often undecidable since the compiler must choose a type for each term such that the whole program is well-typed.
We really need this. Is there anyone working on it at present? I vaguely recall it being part of a past GSoC.
An important feature of `Traversable` is that you can derive it for data types using the `DeriveTraversable` extension. This makes it even more useful.
At the bottom it's an array of strings. At the top, it's an individual string. *shrug* It's still simpler to keep the operations and types the same of the stuff on the left and the stuff on the right.
&gt; and requires no buy-in from existing code. Though it does look like it requires higher kinded types, something present in most ML-family languages but not in F#. Or am I mistaken?
I am teaching my two children how to program in Haskell. For now they just write interactive program with mostly imperative feeling. From the type, a program is an object of type IO (). Which mean an object transforming the state of the world. data IO a = World -&gt; (World,a) This isn't something that need to be instructions. This is more a filter that take a world state and return another one. This is certainly how I'll try to present them the notion of program. 
Well I was a kid who picked up Haskell (I was 13/14 at the time). I'd had some exposure to C++/Java so I knew *what* a program was informally, but I had no formal knowledge of anything. Things I remember: - Lots of playing in the repl - Infinite data structures were way cool - Absolutely no interest in this "monad" stuff for a solid 6 months - No interest in the underlying theory and design of Haskell's abstractions for 2 years I think I spent a solid 6 months just fidgeting off and on with Learn You a Haskell and trying to do math/list manipulation stuff in the repl. I didn't really get into Haskell for a long time but spent a lot of time treating it as a fun sort of "puzzle" I guess. If anything, that's how I'd phrase it. Something like &gt; Hey, can you teach the computer how to reverse a list if all you can say is X Y or Z? For my at least, the puzzles are the best part of programming. I've never really liked computer games but I loved how Haskell was just this sort of game that I could make as hard as I wanted.
Hey Tekmo, I'm racking my brain on the Pipes.Parse API, trying to figure out how to get from a Parser back to a Producer. I see the type looking nearly right from (view decoded) as you mentioned, but there's no space there for me to put my custom Get instance (I have a Get for my type, but not a Binary instance, and it seems like I can't get a Lens out of Pipes.Binary using the custom-Get part of the API). Do you have any other hints on this? I can't seem to wrap my head around the types.
There are two ways to describe a program, both being mathematically equivalent. One is as a series of steps (like in a Turing machine). The other is a lambda expression, where the goal is to reduce it to a certain result. Lambda calculus is probably not hard to understand if you use many analogies and refrain from mathematical notation. The explanation would have to depend on the level of education of the child. If they have an idea of what a function means, it would only be a step further to consider functions as data, and by then you have understood lambda calculus. Haskell is pretty much based on lambda calculus so I think it might be better to ignore IO and monads when explaining what a haskell program is for the first time.
Most people are deploying to Linux and don't have a strong reason for wanting to use Halvm. If you needed lightweight containerization, even then, most shops would be apt to use something more universal like Docker. I can understand why Galois made it though, given the work they do.
So, from what angle did you approach programming that Learn You a Haskell was the first thing you ended up exposed to? Did someone choose to show that to you in particular?
My view of it, aside from mind-share, is a there is some hard-to-quantify level of polish that the HaLVM team doesn't have time or funding to perform. This is anything from documentation to library support (ex: better cabal integration and support of Hackage package maintainers) to more complex things such as easy deployment to clouds like EC2.
Here you go: http://lpaste.net/104117 That will repeatedly parse a stream of values using a `Get`. You might ask Renzo (the maintainer of `pipes-parse`) to add something like that because I can see this function being useful. The reason there isn't a function to convert from a `Parser` to a `Producer` in general is because it depends on how you want to do error handling. We haven't settled on a conventional error reporting scheme for parse failures.
`intercalate` is in `Data.List`, `foldr1` is in `Prelude`.
But unwords is in Data.String, which contains Data.List, no?
This example made me wonder why there is no `&lt;&lt;` operator. Is it just bad style?
Halvm is very cool, but it is not a trivial matter to use it, so you aren't likely to unless you have a specific need. As projects go, Mirage is also in a bit of a better position: nicer webpage, more community. How it got like that I do not know, but these differences are, perhaps, also relevant.
puzzles are really great! It's a pity we have very little good puzzle sources in Haskell
unfortunately he makes only lots of static images. Photoshop is better for such things. He doesn't even try to do interactive programs (which is why I'd become interested in Pascal)
I taught my 10yro and five of his friends Haskell this (Australian) summer: http://justtesting.org/post/70871612766/lets-program What's a program? Well, it's a bunch of functions (a concept they already knew from school).
I happen to know Anil (one of the main driving forces behind Mirage), and believe me, he is certainly one of the reasons how it got like that :-)
My main fear is not about OOP but more about dynamic programming. OOP is more a style of programming, I am pretty sure you could do a lot of OOP in Haskell and that may be not so bad. I am more afraid they would prefer dynamic typing. Simply because the workflow feel better for children in my humble opinion. In dynamic language, the workflow is: I want feature X, write some code, try, argh typo error, correct typo, see X, yeah!!! Now feature Y, then write some code, argh typo error, correct typo, see Y, yeah!!! Argh, booh, there is a bug with X. write some code fixing bug for X, argh typo error, correct typo, retest X, yeah!!! Argh, forgot this small case about X. write some code fixing bug for X, argh typo error, correct typo, retest X, yeah!!! Argh, forgot this small case about X. write some code fixing bug for X, argh typo error, correct typo, retest X, yeah!!! Argh, forgot this small case about X. about 200 times... write some code fixing bug for X, argh typo error, correct typo, retest X, yeah!!! Argh, forgot this small case about X. Yeah could pass on feature Z, write some code, argh typo error, correct typo, retest Z, yeah!!! Argh forgot this small case about Y. write some code fixing bug for Y, argh typo error, correct typo, retest Y, yeah!!! Argh, forgot this small case about X. write some code fixing bug for X, argh typo error, correct typo, retest X, yeah!!! Argh, forgot this small case about X. write some code fixing bug for Y, argh typo error, correct typo, retest Y, yeah!!! Argh, forgot this small case about X. write some code fixing bug for X, argh typo error, correct typo, retest X, yeah!!! Argh, forgot this small case about X. etc... At each step you have the "Yeah!". While in Haskell it is more: I want feature X, write some code, compile, argh typo error, correct typo, compile, argh type error. Read doc. compile, argh type error. Read doc, think... think... fix the code, compile, it works !!! Yeah Now feature Y, write some code, compile, argh typo error, correct typo, compile, argh type error. Read doc. compile, argh type error. Read doc, think... think... fix the code, compile, it works !!! Yeah Now feature Z, write some code, compile, argh typo error, correct typo, compile, argh type error. Read doc. compile, argh type error. Read doc, think... think... fix the code, compile, it works !!! Yeah It is faster in Haskell to have code without bug, but, you pay a price at the beginning. You can't see your feature before your program is mainly correct. On the other hand, while my children learn to program, this is great. I provide them the best programming language I know to start. They will make their own path if they want to program then.
I missed this project somehow, so thanks!
[unwords is made available to the prelude](http://hackage.haskell.org/package/base-4.7.0.0/docs/Prelude.html#v:unwords)* &gt; Why not use unwords = intercalate " "? If I understand your question correctly, you are asking why not just redefine unwords locally instead of importing a new module. The answer is that it's already available without an import. That said, I'm not sure that, given the compiled nature of Haskell, the existence of qualified imports, and the extent to which ghc optimizes code (rewrite rules and such) that there many good reasons for copy-pasting function definitions in lieu of just importing that module. If your question was instead on why it was implemented the way it was, then I'm afraid I'm not qualified to answer that. *Technically Data.List uses the `USE_REPORT_PRELUDE`, but I presume that has the same affect as reexporting a module, though perhaps more effeciently, and specifically for use in the prelude.
&gt; Typical Parent: I taught Johnny how to throw a football. Meanwhile... &gt; chak: I taught Johnny how to Monad Very nice, indeed!
I think we've all seen the metaphor that imperative programming and its derivatives can be thought of as a recipe you're supposed to follow. Like you said, a series of steps. I'd say that functional programming can be thought of as a factory, where you have several machines that each do something simple, and then pipeline their products to the next machine.
For the purely functional parlour.
I haven't found one specifically, but using one of the [many implementations](http://cbor.io/impls.html) it should be easy enough to write a one-liner. For instance, [Flyyn](https://github.com/fritz0705/flynn) makes doing so trivial. The [haskell implementation](https://github.com/orclev/CBOR/blob/master/src/Data/Binary/CBOR.hs#L96-L100) is also available, but seems like a bit more trouble to do something interesting. Update: I couldn't play with the haskell implementation because the CBOR API relies on `Data.Binary.IEEE754.HalfFloat`, which doesn't seem to appear in any version of the API available on hackage.
They're not really comparable systems.
Woop! My haskell course only covered monads as an extra, so having this extra set of slides definitely helps :P
http://www.diku.dk/~paba/pubs/entries/bahr13wgp.html Patrick and Laurence have also other publications on this subject.
It means normal parametric datatypes carry trivial proof content. You need GADTs to encode non-trivial stuff.
&gt; From the type, a program is an object of type IO (). Which mean an object transforming the state of the world. &gt; data IO a = World -&gt; (World,a) No no no evil no no. This description is the biggest problem with people understanding IO. I don't care what the particular implementation details are, I don't care if IO is in fact this way. It doesn't matter. This is the wrong way to think about it and gets people into a tizzy. IO is nothing more and nothing less than an ADT that codes up IO commands. That's how to think about it.
This is an answer from Quora. This problem is a recursive problem, what you need to do, assuming your age is n figure out how to teach an n-1 year old what "a program" is, and instruct them to do the same until the base case of kid is reached.
Thanks but I was asking for a library...
I think `compdata` at least *uses* tree transducers, take a look in there. 
I think they also made animations and games.
Yes but there is no documentation...
Sorry, I should have read your description and seen you already know about compdata - Bahr has another paper on the topic, Modular Tree Automata, but I seem to remember it was quite theory heavy. I'm not sure there's a library for tree transducers yet. 
I see it more as higher-order functions being incomplete machines that need accessories to function. Stick blender engines always do the same thing, you can get different effects by using the whisk or the blender accessories. At least if they consume functions, rather than producing them. Though with the Haskell argument binding semantics, it's hard to say exactly what makes "a function that produces functions" special.
[Make the lambda calculus into a game!](http://worrydream.com/AlligatorEggs/)
&gt; Seems like children don't like monads, functors and abstractions, they like writeln. It was the complete opposite for me. I did not really get how to write programs, I simply enjoyed learning things. So when I got bored of other languages that I have learned (learned as in just the bare minimum) I simply tried find more interesting ones, since I was too scared of writing programs itself due to not getting past extremely simple toy stuff. Eventually I found Haskell, it seemed very interesting and everyone said it was hard to learn except if you didn't know an imperative language already, which I almost did not. So I gave it a shot and eventually it became the language where programming itself clicked for me. Haskell is especially good if you like learning stuff, since there is ton of stuff to learn after you understood the basics. It might be worth saying that you can program in Haskell without really knowing much about any of the abstractions, just try make the types fit together and you can get pretty far.
&gt; Absolutely no interest in this "monad" stuff for a solid 6 months This is what I think most people new to Haskell should take note of. They come to Haskell and are presented tons of what seems like super weird concepts, but instead of struggling getting that stuff before starting to program they should just start program! I got quite far simply by thinking monads was some sort of magical container, then I could just make the types fit together using (&gt;&gt;=) and suddenly I got it to work. Don't get me wrong, having a good intuition of the different abstractions is very useful, essential if you want to be proficient, but you certainly don't need it when starting learning Haskell.
Gah! Super, you are the best! I was thinking about this all last night, and realized that I was thinkig of a Parser as a 'Producer' - like a container that yields many things, but instead maybe the idea is that a Parser is a producer of one thing, and you would run that state in a loop. Getting the lifts and the cases right would have been another half-day of work for me. Thanks for this :)
I was making a joke about OOP. :P
I'm afraid I don't know that much about them - I briefly scanned Bahr's two papers a while back and looked at compdata (oh look, tree transducers make compdata fast through deforestation!) and that was that.
You're welcome! :)
I'd like to see an instance of something that can be represented cleanly with arrows, but is not representable in a bifunctor!
I can understand that about unwords being in the Prelude. It just seems like unnecessary redundancy for backwards compatibility.
Sorry to be that guy, but 'beginner' rather than 'begginer' in the third paragraph. 
Looking through these slides, I don't think I would have survived Stanford... :P Really detailed, awesome stuff! Gonna keep these slides bookmarked.
Oh that. Well played :). Have an upvote.
Oh my... thanks! It should be fixed now.
Yes. It resulted from this discussion, and it is linked on the article, in "The current effort". http://www.haskell.org/pipermail/libraries/2014-April/022844.html
The main difference between functional and imperative styles of programming is this: - In imperative style, a program describes a list of steps to take, one after the other. - In functional style, a program is a mapping from something to something else, described as a function. As a middle-schooler, I was given the exercise of writing down steps for a student to take, once walking in the door of a classroom, to turn on the television, in detail. This was later described to me as a program. In functional style the same program might be described as a function mapping the state of the room with the person at the door, and the television off, to the state of the room with the person at the television, with the television on. The distinction between ideals is a bit different, but in the end, functional and imperative programs do the same thing. In Haskell's case, a major benefit is safety. For instance, in the imperative program, maybe the student is instructed to stop, kick over a computer tower, spill his soda on the teacher's head, and then proceed to turn on the television. In Haskell, this might be considered illegal behavior, due to unsafe accesses.
Yeah, the halvm could spawn a company in its own right. But there's a small matter of investment to get there.
Arrow is a bifunctor (but `Profunctor`, not `Bifunctor`), covariant in the first parameter, but contravariant in the second. But more seriosly: they are different part of the stuff. In type signature first :: (b -&gt; c) -&gt; (b, d) -&gt; (c, d) you might either abstract (-&gt;) with Arrow, or (,) with bifunctor... You could do both, but I don't know what you can do on so general level...
You are soo lazy! Use the force (er, deepseq). See http://ro-che.info/ccc/11 for reference. Other than that, it depends on the depth, but LYAH might fit (http://learnyouahaskell.com/)
Yep, but the opposite direction does not hold.
Oops!
But in Haskell, those are called `Profunctor` and `Bifunctor` refers to ones covariant in both arguments, so arrows and bifunctors don't have any overlap in this sense.
The lens [lecture notes](http://www.scs.stanford.edu/14sp-cs240h/slides/lenses-slides.html#%281%29) are supurb. Your point about MOOCs and the importance of being there is well taken - I hope the students enjoyed this! In fact, how was the response? From the title it sunds like a pretty specialized class - not a general requirement. Did people like the pacing? What's the NonHaskeller | Haskeller transition matrix looking like?
Yes, there are some large-scale Haskell deployments in production. Obviously nobody makes a "C or Java could do it in 50% less hardware" argument in these situations, since who's going to build a big complex system twice. People tend to follow the lead of whoever starts a project, which is about as sociologically complex as the choice of language gets.
thank you
The same reason you should choose GCC, even though it only works with C, over Linux, which can run programs written in all sorts of languages...
The sole and only reason that Mirage gets lots of attention is that Anil is a juggernaut of never-ending energy, enthusiasm, and ideas, and this is a highly infectious property. This social factor is one of the most important and readily overlooked reasons why projects succeed or languish. In stark comparison, I frankly couldn't tell you the names of anyone working on HaLVM. Just doing the work isn't enough, you have to make some noise.
You're attempting to explain by using an analogy that leaves me more mystified than before. GCC runs well on Linux: are you saying Halvm runs well under Docker?
I'm saying the Docker is in some sense a platform, as is Linux. HaLVM and GCC are both compilers.
I fully agree... I did need to learn them at one time to use HXT (which I sometime later stopped using anyhow), but appart from that, I hadn't found much use to them. That's way I tried to make this a very simple introduction to the 'really useful parts of Control.Arrow that you can use without caring for arrows'.
For me, Java became a synonym of something that works slowly and eats huge amount of resources. Of course, no one argues that it can be fast, I'm just saying that in big projects it's almost impossible to measure what would be if you'd write it in another language, very often language is not the most important thing at all.
Edited my post, to make pun more clear.
That is amazing. Definitely using it.
Haha, this is really much more explicit! To make matters slightly worse, `left` and `right` apply to arrows that look like p (Either a c) (Either b c) and so then you have the bifunctor `p` which isn't a `Bifunctor` but instead a `Profunctor` which has in its `Functor`-targeted slot another `Functor` which is indeed a bifunctor and a `Bifunctor`. Finally, when using `left` and `right` as a nifty way to use `Either` you're really doing the same thing that the `Bifunctor` instance for `Either` would do which is a degenerate form of the `Profunctor`/`Arrow` concept. Ugh. (Oh, and of course both `Profunctor`s and `Bifunctor`s are `Functors`, but not all functors are `Functor`s since contravariant functors are `Contravariant`s)
Oh thanks! Given my very modest skills regarding CSS and web design, its good to know that someone actually likes it ;)
The last iteration was much easier to get to work. So if you have not tried it in over a year it might be worth another try. I got it to compile on other flavors than Fedora as well if that was a noticeable barrier to any one.
right. Arrows are something like monoids in the category of profunctors and profunctor morphisms.
Same. I learned it to use yampa and demonstrate some calculus. Shame that work on yampa stopped. Although I've heard reactive-bananas uses Arrows
This is completely wrong along two orthogonal axes at once. Congratulations! First, they did in fact create animations and primitive games. Second, it's *immensely* valuable to learn how to create static images programmatically. Both as an exercise in coordinate geometry and as an exercise in simple loops and conditionals.
This is a shame! Arrows are a very useful abstraction, and come up in a lot of FRP libraries. In modern api design they have been supplanted by trendier Applicative APIs, but that doesn't mean that they aren't still a powerful abstraction perfect for many applications. 
I think this article sort if misses the point of the Arrow typeclass... that being said, most people do and only use it for its `(-&gt;)` instance for fancy tuple manipulation. If that is all you need, then of course Bifunctor is probably a better choice. I actually always was sort of uneasy about Arrow using tuples to represent generalized side-chaining computation... perhaps it should have been parameterized over arbitrary Bifunctors in the first place? 
How did you get from "Haskell is magic" [1] to tree transducer in one week? :-) [1]http://www.reddit.com/r/haskell/comments/2542iq/could_a_plugin_be_made_to_run_haskell_games_on/
&gt;This description is the biggest problem with people understanding IO. But why would it be so? Because there is no such a thing like "state of the world" in Haskell? Because the interpretation gets in the way once the learner finds situations that do not fit it? Or because it tends to clog the conceptual gears of programmers, leading to confusion? This is not a rhetorical question; it's just that the precise nature of the issues might be such that they do not actually affect the specific scenario involving yogsototh's kids.
Abstracting over both leads to the notion of a monoidal category. When you add back in all of the properties of (,) and (-&gt;) as the base category together you get back to a Cartesian category. If you add back in the notion of an exponential, which gives you more ways (,) and (-&gt;) can interact you get to a Cartesian closed category (CCC). At that point you have a ton of structure. You may want to read http://math.ucr.edu/home/baez/rosetta.pdf to get a sense of what these abstractions are good for and the connections to logic, topology, physics that result from this way of thinking.
Arrows are what you get when you are an instance of profunctor's `Strong` class as well as a `Category`. That is it. They are strong profunctors you can glue together. You can recover `arr f` from `lmap f id`.
Very interresting and funny, thanks!
A GUI framework based on Diagrams. Native widgets are also supported (Gtk currently). The applications in both cases are generated from the same ADT behind the lgtk EDSL.
unfortunately, no IO. I assume it is easier to explain how programming can be usefull through IO exercises, algorithms are second.
&gt; it's immensely valuable to learn how to create static images programmatically agree
You gain your intuition by experimenting and using stuff. You can't start with the intuition part. That comes after experience, not the other way around. You are absolutely correct.
Hmm pardon? I don't understand what you are asking.
While I agree with you that the "state interpretation" of IO isn't productive, for some people it makes sense. I don't know if we can tell those people they are wrong. The "free monad interpretation" you talk about is also how IO makes sense for me, but not for everyone.
`-fdefer-type-errors` perhaps?
I wouldn't say it's more clear than just using the pointed form though.
Nice! Let us hear about it... 
I'm fairly certain a high-quality MOOC could be a big boost for Haskell, see what the Scala course(s) on Coursera did (in outreach and number of successful students). It might as well reduce the number of people who think they learned it but didn't actually and are mostly spreading FUD.
I don't know. The sense in which it's even accurate to describe Haskell's IO type as a world state transformer is itself a rather convoluted thing. Whatever it is that `State# World` actually represents isn't literally world state, but some other kind of thing, probably something like memory-mapped IO content or something like that. Even that description -- memory-mapped IO state -- would be more useful and insightful, rather than literal whole-world state while being truer.
I'd add Monoid, for sure.
How much abstract algebra are you familiar with? If your knowledge is already deep there, I'd recommend leveraging it for some of the patterns here. If your knowledge is *not* already deep there, I'd recommend *not* getting distracted by the parallels (yet).
;)
That was an impressive demo.
I'm especially fond of how bos makes explicit the transition between the different presentations of lenses. Getting from the traditional `s -&gt; (a, b -&gt; t)` to kmett's `forall f. Functor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t` isn't the easiest thing to do, and the vast majority of tutorials seem to focus only on the traditional/concrete presentations— which is helpful for basic insight, but useless for navigating the lens library.
Alas, they somewhat recently decided to [make this Monad instance available without imports in GHC](http://www.haskell.org/ghc/docs/latest/html/users_guide/bugs-and-infelicities.html#infelicities-numbers), in contradiction of the Haskell report. I am not convinced this was a good idea, for reasons like this.
right, isn't that trivially equivalent to being a monoid in the category of strong profunctors where data PCompose p q a b = forall c. PCompose (p a c) (q c b) is the tensor product and `(-&gt;)` is the identity?
Your point being?
I don't believe compression of this kind is really suitable in the definition of a serialisation scheme which is supposed to be easy to debug by eye, interoperate with other non-Haskell systems and have fast encoding and decoding. Of course arithmetic coding could be used in a further step (possibly an encoding aware encoder to eek out even more compression), but it shouldn't be used here IMO.
Seconded. dalaing is why my guide (and soon to be class) uses Yorgey's course: https://gist.github.com/bitemyapp/8739525
This won't help you now but I am planning to do a ghc-7.8 Software Collection for Fedora and EPEL later this year. (Currently working on updating Fedora Rawhide to ghc-7.8.) You could also look at JustHub.
As the maintainer of the HaLVM, I 100% concur with this remark. :) When push comes to shove, I don't have Anil's energy supply, and Galois is a small enough company that we have limited funds available to pay people to work on something we're seeing very little pull on. That being said, we have done some cool projects with it, and a couple of us have been doing some serious work on it over the last few weeks. I'm hoping to publish / speak about some of the work we've done and some of the things we've built later this year. Hopefully our successes with it will be inspiring enough to generate enough external interest that we can start making a stronger business case around it. Personally, I think the HaLVM's pretty cool, and I'd like to save it from oblivion if it's possible. Edit: Oh! And my name is Adam. Hi!
I've recently picked up a sysadmin gig, and been looking at everything backwards or outside/in. We are out here in the world and we are going to send some stuff into this pure haskell program, but haskell is going to want our info containerized so it can grab hold of the power of abstract math and do manipulations. Or something like that.
I've used justhub before and it worked alright. The software collection will be awesome. Thanks for working on that. 
Well, the slides explicitly say that inter-operation with other programs is a non-goal, and their chosen format CBOR isn't exactly "easy to debug by eye" either. In fact, if you serialize the type representation along with the data, it will be *significantly* easier to debug than CBOR. You have a point with encoding/decoding speed, though I don't think that will be an issue in practice. The overhead of the arithmetic coding will probably be small compared to the cost of actually traversing the data type you want to serialize in the case of algebraic data types, and in case of stuff like ByteStrings that you want to memcopy directly you can still do that in an arithmetic coder.
Maybe I'm wrong, but it seems like parametrizing over IO/network/... stacks is a killer use case for Ocaml functors.
Does he not let people record the lectures, or is it against Stanford policy? 
Sure, Except that's the opposite of the world-transformer description. Haskell wants your info -- some IO-relevant data -- sure, fine. But that's not the same thing as "the world". That's some minimal state that represents an IO system, whatever that would be. Big difference.
&gt; Note also that lift-and-compose" operators like `(^&gt;&gt;) :: Arrow a =&gt; (b -&gt; c) -&gt; a c d -&gt; a b d` can be easily defined for Invertible, but their Arrow implementations bake in the assumption that we have arr. But... if you have `(^&gt;&gt;)` you can do `arr f = f &gt;&gt;^ id`, so you *can* fully implement the arrow... right?
D'oh! I was looking at something like this like a year ago, so clearly I'm misremembering something. *sigh*
Arrow is just a clunky Category+Applicative. Better to converge on common abstractions, Arrow just duplicates the same structure of these other, more accepted abstractions.
These functions are now provided more powerfully/generally by the lens library.
&gt; Use the [`force`](http://hackage.haskell.org/package/deepseq-1.3.0.2/docs/Control-DeepSeq.html#v:force). Fixed it.
I googled but didn't found anything, so may I ask what "state-based" FRP is?
Should be, yes.
Right, on my todo list the next two items are: * write a tutorial for lgtk * explain what kind of FRP is used and try to compare it to others Basically, I use an FRP which is built around `(State -&gt; a)` signals instead of `(Time -&gt; a)` signals. And substates of states are composed by lenses.
That it's refreshing when parents engage their children in activities that require brain and not brawn. That isn't to say that parents shouldn't spend quality time with their children doing physical activities, but that I appreciate hearing accounts of children being taught things that stimulate their mind and help them express their creative sides. It wasn't meant to offend, I apologize.
Again... lens usually come much later on the learning curve, don't you agree?
No offense meant :) I was under the impression that there was a reddit article by you some time ago asking if Haskell can do several *magical* game-related things. I just searched through /r/haskell but couldn't find your post so I might just be mistaken.
i am looking for a sequel to conal's post about ccc use in haskell http://conal.net/blog/posts/haskell-to-hardware-via-cccs anyone know if there is anything published in code or paper?
[Profunctors](https://hackage.haskell.org/package/profunctors-4.0.4/docs/Data-Profunctor.html) provide the steps between `Category` and `Arrow`: `Arrow p` is equivalent to `(Category p, Strong p)`.
&gt; Haskell is a functional language. That is to say, functions are 'first class citizens'. No, "functional" does not mean "functions are first class citizens." That would be "higher order." Instead, "functional" means "like math." &gt; In fact, in haskell, everything is a function. [No, this is not true.](http://conal.net/blog/posts/everything-is-a-function-in-haskell) &gt; There aren't any variables in haskell [...] Where did you get this idea? It's not true. &gt; [...] you can declare a function that takes no arguments [...]. That would not be a function.
I was agreeing with you. Just in my mind it places haskell a bit better, for doing usefull things. My mind at least is addicted to the power of abstraction so much that I don't get anything done, so that if I learn about monads then categories and recently found myself reading about 2 categories, and wondering what the hell am I doing. I'm not a mathematician, etc. Just a tangent. 
They provide steps in between, but not always the right steps. It is often possible to implement `first` and `second`, but not `dimap`. For example for the `Invertible` type mentioned above.
he explained this many times, I think he's just not into this kinda thing. I first heard him said he was worrying there're bugs in the sources and in the recent blog post, he said not believe in MOOC. I actually really enjoy listening to him talking.
I'm always a little bemused when I see people asking "Mirage or HalVM" as if it's a competition between the two systems. For every 100 people that enquire about either project, only 10 will actually bother to try it, and 1 will forge through the various inevitable technical glitches to triumphantly come up with a useful application. So I'm always very happy to hear (and help, if my poor schedule will allow it) with *any* uses of unikernel-style systems. To give people an idea of the amount of work poured into building both HalVM and Mirage, they started at roughly the same time in 2006/7, and I recall discussing a Haskell Xenstore with Andrew Tolmach and Adam back when I was at XenSource. Seven years later, we've both got releases out of the door, and mindshare is slowly building that this might actually be a viable alternative. Replacing tens of millions of lines of OS code with ML/Haskell equivalents isn't a trivial undertaking. Both systems may look similar from a high-level perspective, but the details are very different (and in many ways, highlight the differences in style between Haskell and ML system construction). Mirage applications are entirely built out of functors, with the final application deciding on the set of device drivers, for instance. Since we built the OPAM package manager as a segway during Mirage development, it's an integral part of the library management workflow now. We've been very fortunate with the Mirage project to have some driving use cases that have built up a dedicated community (primarily due to our efforts to bring an open-source nymote.org data routing system to reality). Adam and his crew at Galois have done some staggeringly cool demos with HalVM, and have released it in great open-source shape. It's really down to the Haskell community to pick it up and start blogging about how to use it for real applications. For instance, this post was when I consider Mirage became "useful": http://amirchaudhry.com/from-jekyll-to-unikernel-in-fifty-lines/ -- an equivalent in HalVM would be great to see! I anticipate that as time moves on, there will be an increasing amount of code sharing between the projects. We've been upstreaming our changes to the Xen MiniOS to enable compilation on Xen/ARM (thanks to Thomas Leonard), for instance, and that should slot straight into the HalVM. Similarly, we've added PVHVM support (via Hwanju Kim) that lets MiniOS take advantage of hardware virtualization in the style of Dune from Stanford. Adam has pointed us to his extracted OpenBSD libm for math operations. Some GSoC students will be working on building command-lines to make uploading unikernels to the cloud less of a pain, and this should benefit HalVM as well: http://openmirage.org/blog/welcome-to-our-summer-hackers TL;DR: The goal isn't to compete with each other's tiny communities. Let's see some other people blog about using HalVM and get some code sharing going with Mirage, and topple the dominion of crap software stacks! We can argue about lazy vs strict in the pub later.
[You're not mistaken.](https://pay.reddit.com/r/haskell/comments/24tyx1/haskellers_would_you_give_some_advice_on_the_way/)
s/kmett/van Laarhoven/
&gt; Of something is a covariant functor, it can't be a contravariant one as well: and viceversa. You can think that contravariant arguments are "inputs" and covariant ones are "outputs". But you can have a type that is an instance of both `Contravariant` and `Functor`: data Proxy a = Proxy instance Functor Proxy where fmap _ _ = Proxy instance Contravariant Proxy where contramap _ _ = Proxy
and slow: http://www.techempower.com/benchmarks/#section=data-r9&amp;hw=peak&amp;test=db&amp;f=9zlfk-0-0-0
Eyebrow raised. Where's the source? How was it compiled? Why doesn't it mention which database Yesod was tested with?
He's been working on it pretty actively from the discussions on the ghc-devs mailing list.
It does, in the little grid on the right. One used Mongo and the other used MySQL. This is interesting as Warp always benchmarked fairly well IIRC. [Here's the source.](https://github.com/TechEmpower/FrameworkBenchmarks/tree/master/yesod) Also, it's interesting to note that on i7 hardware, Yesod jumps way, way up.
Looking at the benchmark it seems more likely that it's slow because of mongo (maybe the driver being used) and not because of Yesod the webframework? Also since this is comparing Haskell frameworks, it ranks at 3x speed of Snap in the same benchmark http://www.techempower.com/benchmarks/#section=data-r9&amp;hw=peak&amp;test=db&amp;l=1s
not really, i did some benchmark on scotty (scotty and yesod uses **wia** as it's http server) and nodejs and golang. simple hello world benchmark. **Language version:** go1.2, ghc7.6.3, node 0.11.10 **Result:** http://pastebin.com/GjAJAVab remeber i tested haskell code in scotty which is a framework, where on other hand i was using simple http servers for golang and node. ghc7.8 have some speed improvements too. 
Is there a good intro to HalVM available somewhere? I'll dig around for one in a bit, but I'm not terribly familiar with the space (I'll edit this comment with whatever I find).
It depends on what you look at : http://www.techempower.com/benchmarks/#section=data-r9&amp;hw=i7&amp;test=json&amp;l=8w 
Very nice. I have recently started one in the Houston area as well (http://www.meetup.com/Houston-Haskell-Users-Group). 
FWIW I've seen this too. --max-backjumps=-1 doesn't help. --constraint="transformer==0.3.0.0" does.
Hi there, one of the devs at TE here. There's a couple of things to note about these results: * These tests were built using GHC 7.6, and the IO manager in the RTS for that version has trouble scaling with the number of processor cores. The Peak hardware used in round 9 has 40 cores so this was a major pain point there, but even on the I7 hardware with only 8 cores this was a major bottleneck. * We're using, at this point, fairly dated versions of everything. This is because everyone was waiting for GHC 7.8 before making any changes. Since it came out roughly the same time we finished running round 9 we didn't update anything in the meantime, so all the versions used are pretty stale at this point. * We've already accepted pull requests since this round was released to update everything. Round 10 will feature the latest versions of Yesod, Snap, and raw WAI (and possibly others if I get time) built on GHC 7.8.2 at least. We've already done some preliminary testing with latest everything, and while I don't want to talk specific numbers just yet, I will say that everyone should be a lot happier with the new numbers once they're released.
Don't worry about that too much, Kazu just sent in patches to get their benchmarks moved over to GHC 7.8, which offers dramatically better performance on these kinds of benchmarks.
Great point. This isn't a zero sum game. If you can make the market bigger then you *both* benefit and there's clearly a lot of potential for that.
http://www.vex.net/~trebla/haskell/sicp.xhtml#sandbox However you'd have to create wrappers around ghc/ghci/etc for them to use the packages located in the non default folder
Yeah, can teach SECs before lens and then teach lens as a generalization of SECs. 
This is getting downvoted because it's shallow and has nothing to do with Haskell. It looks like a failed attempt to draw up some thin content so they can get backlinks to their main domain. I couldn't even find an author name, it's just a dystopian "MOJOTECH" everywhere. Borderline spam; know your audience next time.
Do we have to book? I think i'll be there but I'll have just gotten off a very long plane flight and I'm not sure I'll have the strength to show up. Is it ok not to RSVP but still show up?
--reorder-goals *may* help in that kind of situation, though it can also make things much worse.
Ah good to know. 
These are the resources we list in the /topic for #haskell-beginners, which you can totally join to ask questions. You can also ask questions in #haskell, of course, and they are quite friendly there as well. * https://gist.github.com/bitemyapp/8739525 * http://dev.stephendiehl.com/hask/ 
RE: comments Haskell obviously has variables, like maths do. however, Haskell doesn't have variables in the same sense that imperative languages have. you don't go x=3; x+=1;. so for newbies it might help to think of them as synonyms rather than variables. the misconception about everything being a function, and foo=3 being a "function of no arguments", is yet another act of oversimplification for newbies. so in general I think it's fine to make these simplifications, as long as you make it clear that they are just that.
hello, could you please re-enable the link &gt; Modular Tree Automata* by Bahr, found in pages 263-299 of [Mathematics of Program Construction, June 2012]( https://www.dropbox.com/s/gzjlzic9pp3pvl2/Mathematics%20of%20Program%20Construction.pdf) thank you very much!
I am inclined to agree. If some new syntax is brought it some old syntax should be thrown out to make way for it.
Making field access syntax familiar to programmers from other languages through a GHC extension that adds yet another syntactical overload to the dot *and* is incompatible with existing code doesn't look like a sound plan. If `lens` (or `lens-family-core` etc.) becomes really popular one of the upsides is that this proposal will become completely impracticable.
Judging project quality by commit activity seems a little odd... Snap hasn't changed much in a while, but for most of the people I know using it (myself included), that's been mostly because the abstractions in provides (snaplets, heist, and just letting you write regular haskell) basically work... Stability isn't necessarily a bad thing...
I had a similar issue while building something that required `transformers-compat`. I had to pass `-ftransformers3` to `transformers-compat` and then everything was fine, though I'm not seeing that anywhere in your dependency graph, so I guess it's unrelated.
I would really like to see Happstack 8, though. I prefer the smaller amount of magic, honestly.
Not sure if I agree with the proposal, but it is certainly not "for no reason" - the advantage of the dot is (primarily) in tooling. When you type "foo.", the editor/IDE you are using could figure out all the one argument functions in scope from the type of foo to X, and prompt them to you for autocompletion. Sure, you can argue from a PL perspective that tooling is a detail that shouldn't affect language design, but from the perspective of getting wide adoption of the language, ignoring tooling is foolish.
Encapsulation of functions inside objects is the OOP paradigm, but Haskell is a functional language. What you describe is already possible with module names - type the name of a module and the editor/IDE can figure out candidate functions from that module which have a corresponding type. Yet this proposition aims to introduce even more syntactical and cognitive complexity: &gt; But dot notation must be written with no spaces around the dot -- because it has strange syntactic properties that mean it isn't 'just an operator' How much more ambiguous special cases do you want to pin up on the simple dot? What other operators are going to become "not just operators"? How many Haskellers use more than 10% of the existing extensions, and are there more than two whose 10% of extensions overlap?
 import ShinyIDE.Sweeteners ((&amp;)) foobar x = x &amp; [insert autocomplete pop-up here] For best results, make the faux-OO-dot operator configurable through the IDE, so that users are free to use whichever version of `flip ($)` they like best instead of the IDE-provided one.
Thank you so much for pointing me at right direction however by using --package-db flag although i am able to add new package database, I am not able to remove the references to database at $HOME/.ghc/arch-os-7.6.3/package.conf.d 
How do you like Snap? Do you find yourself hitting walls particularly often, or does it mostly fall into place nicely? I've been meaning to give it another go after the changes (semi-)recently.
I would've noticed text that bold because I did check the footer for a note about who wrote it. So yes, I think it was added after I commented.
Tossing out one more comment: you need to learn Haskell if you want to have the verbiage and understanding to discuss FP. This article is exemplary of why. There's more to life than (.)
I've been pretty happy with it for a while. It handles most of what I'd want from a reasonably low-level framework - which are proper abstractions for building higher level functionality, and other than that, getting out of the way. The main thing it's missing is just the higher level functionality built on top of it (most of the existing snaplets are pretty basic - the auth snaplet is the only real (open-source) exception I know to that - though there is really nothing in the design of snaplets that prevents more sophisticated stuff being built). Which isn't to say you can't get real work done, just that you should plan on writing most of the stuff you need to use (ie, if you're looking for Django admin like functionality, it doesn't exist, yet). Also, I've found the design of heist to be phenomenal (and has been such a productivity booster over working with other languages / templating system), though there is no particular reason why you have to use snap to use heist.
&gt;Although I've heard reactive-bananas uses Arrows Not really; the `reactive-banana` API uses `Applicative` and applicative-like combinators instead.
It was one of the first Haskell projects whose source I tried to read and understand.
Nice. Thanks for clearing it up (:
My bad. I know Twan's the one that started them off, but for some reason I (errantly) recalled his version being somewhat different from the version used in the `lens` library. Too many lens tutorials rot the brain, evidently
I think it's avoided rather than overlooked. It might be good, but it's not finished or really usable. I've never really missed OO in Haskell, it's only when I start to emulate OO that I find myself wishing for all these hacks. That scares me. Edit: [front page thread from today](http://www.reddit.com//r/haskell/comments/25n5g8/dot_postfix_notation_extension_proposal/), case in point.
Very nice! As a next step you might consider learning about the various lens libraries. I think you'll find that lines like this will shrink down drastically using lenses. ``Character (characterName character) (characterHp character) (characterMaxHp character) (characterLevel character) (characterEquipped character) items`` See: [Program imperatively using Haskell lenses](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html)
As much as I like Gabriel's blog, I am not a big fan of this post.
Just out of curiosity, what do you think would make it better?
thank you!
The paper addresses object-oriented idioms as an *intellectual challenge*, demonstrating that we can do this in Haskell. We already know that Haskell is the world's finest imperative programming language -- this paper establishes it as a bleeding edge object-oriented programming language as well. I don't think anyone is saying that this is presently The Right Thing. Reading the examples makes every cell in my body scream "THIS IS SO WRONG!", and I hate the way it looks with every fibre of my being. That said, I'm in awe with how cool the paper is. It is a very interesting read, and I posted it above all because it is a fun intellectual exercise.
Besides, if you're really gagging for postfix accessors and other functions: infixl 1 (.$) (.$) = flip ($) (x,(y,z)) .$ snd .$ fst == y [0,...,10] .$ filter even .$ map (\x -&gt; x*x) == [0,4,16,36,64,100] You can always define your own infix function for `flip ($)`. Or even re-define `(.)` for local use as `flip ($)`, if you're perverse enough to confuse yourself and others. The idea that we need special syntax for this is absurd.
I don't find it very beginner friendly. I have found other resources to be much more helpful in terms of how to think about lenses. Once I understood lenses (somewhat), I came back to this post and realized that I was just missing a lot of context when I tried to learn from it.
And Arrow's got syntax sugar.
How do you replace (|||) and (&amp;&amp;&amp;)? Just either and a lambda? Are they in another class like biapplicative, bichoice?
I feel a little odd posting this seeing how I have nothing to do with jhc (other than admiring it), but it's been a day or two so I figured I'd post it before I forgot. Here's John's summary: A new version of the jhc optimizing haskell compiler has been released with a new licence, new libraries, and lots of improvements.
That title scares me. I don't know if I want to press it.
Colorblind note, I can't see the links in your text very well. Great article btw!
On common lisp forums there were regular proposals to "fix" lisp by removing parens. On haskell forums there are regular proposals to "fix" haskell by making dot behave in a "familiar" way. 
I've recently taken on maintainer role of [Salsa](https://www.haskell.org/haskellwiki/Salsa). It uses type families to do awesome things like choosing the best match of a method overload based on the arguments passed in. Ie Like .Net type system algorithms implemented in ghc's type system. Worth a look I'm gona implement so more features and experiment with different syntax soon.
You could use linux containers. A container can house a specific version of haskell and its dependencies and an independent home diretory.
This is very exciting. The license change means there's a great opportunity for cross fertilization
Nice to know that project is being maintained!
I'm always curious to see if there's a way to use techniques like this for better modularization in large applications. I don't want to replicate OO, but I'm sympathetic to some of the advantages of it. I think the advantages of Haskell would help to direct the use of modules to the places they're actually needed unlike OO where everything is coerced toward becoming an object as soon as it is conceived of.
Snap is actually in active development as well. A fancy new back-end server based on io-streams is nearing completion, as far as I can gather, and they're working on getting it integrated with the rest of the framework.
try using cabal-dev
This is only tangentially related, but if you're interested in text adventures and are looking for a fun little project, you could implement a barebones [Z-machine](http://en.wikipedia.org/wiki/Z-machine) VM in Haskell. 
I try to avoid installing anything into $HOME/{.cabal,.ghc}, using sandboxes for everything. This is easy to do when using GHC installations without Haskell Platform, as the latest version of cabal-install can be installed --global. As for Haskell Platform installations, I have been simply installing the latest version of cabal-install with --global, overwriting the included version. Once I have tested this Haskell Platform setup more, I will add it to my article about the subject: http://www.extellisys.com/articles/haskell-on-debian-wheezy
You don't necessarily need to bring in lenses to make that code in particular more manageable: `character { characterInventory = items }` ought to do the same thing, right?
Android support! Excellent :)
Pretty shameless, I admit, but Elm has built-in extensible records.
&gt; export/import lists now allow namespace qualifiers kind, class, type, or data to explicitly only import or export the specific named entity. As an extension allowed by this, classes and types no longer are in the same namespace and can share names. Isn't this incompatible with `ConstraintKinds`?
The previous versions of the package were tied to Gtk. I wanted to keep continuity so I just changed what the acronym stand for but I am open to name change too. Consider also that lgtk is unique (by google). Do you think lens-widget or lensref-widget would be a better name for the package? 
&gt; In other words, parseLaTeX is a partial function defined over the set of valid LaTeX files [...] What's meant by partial here? It sounds like a total function of type `Either Error LaTeX`
The big question for me: How many packages on Hackage `cabal install` with JHC?
only seen this note now my comments were on his question (no dtd involved) you are right, you can specify some ordering with dtds but be careful - your mileage may vary hxt supports both regular xml and xml+dtd (which is quite impressive imo)
You should check out [these slides](http://www.scs.stanford.edu/14sp-cs240h/slides/testing-slides.html), especially slides 48-50. They explain how to build an arbitary instance for truly random trees, and how you can use `sized` to limit the size of said trees. Bonus: when you've created this arbitary instance, you can use it to run QuickCheck properties on your functions.
Isn't `ConstraintKinds` a ghc feature, not a Haskell feature?
I wonder how much of the "All is not well in jhc-land" section of the [old page](http://repetae.net/computer/jhc/jhc.shtml) is still true. I guess a few of them are easy enough to test out by simply trying to compile nofib. I think I'll give it a go after breakfast. *update:* I was able to compile jhc with ghc-7.8 by making 3 changes: * I changed `USE_NOLINE` to `USE_READLINE` in `src/Util/Interact.hs` - This has already been fixed upstream though * I added `hiding (Proxy)` to the `Data.Typeable` import in `src/Fixer/VMap.hs` * I removed the `Show` instance for `Identity` in `src/Util/Inst.hs`. ~~Next is to see how much of nofib this version compiles.~~ On second thought, maybe I'll try something less ghc-specific. 
FYI, you seem to have not read about record update syntax, so I will massively improve your quality of life :) So, in Character.hs you have the following code: createCharacter :: [String] -&gt; Character createCharacter xs = Character { characterName = name , characterHp = characterHp default_character , characterMaxHp = characterMaxHp default_character , characterLevel = characterLevel default_character , characterInventory = characterInventory default_character , characterEquipped = characterEquipped default_character } where (name:_) = xs -- Decides what order parameters should be given in which is clearly an utter pain. However, Haskell has record update syntax which can easily update a single field. You could rewrite the above as: createCharacter :: [String] -&gt; Character createCharacter xs = default_character { characterName = name } where (name:_) = xs -- Decides what order parameters should be given in This will create a copy of default_character, but with the characterName field set to `name`.
Sure, but there's plenty of borrowing and cross-fertilization among extensions to various Haskell compilers, and some of them eventually wind up in the standard. It's not a problem if different compilers' experiments conflict; I just thought it was non-obvious enough in this case to be worth pointing out.
Not directly, no. You could add a newtype wrapper with the MonadIO instance, and not export the newtype. (Do this if you want to call external functions that take a MonadIO constraint.) Or you could just write a `myLiftIO :: IO a -&gt; MyType a` function and use that instead of `liftIO`.
Honestly, no: I don't think these are better names. The current name does the job of giving the right *hint* about the package. The other names you give aren't that clear, at least to me. If I were to come across such package I would probably dismiss it as being too obscure. My advice would be to stick to the current name for now and come up with a new name after the code matures enough and you consider it to be stable. At that point you can make an announcement with new, cool features and a new, **memorable** name. 
Yes, but record syntax makes us sad.
As yet another haskell newbie, why is that?
It isn't really meant as a "here's how you write code with lenses" post. It's more of a "look at this cool thing you can do! Doesn't it look neat?"
So what would you say is a better approach? Lenses? Also, how come the performance isn't that great? Compared to what?
discussion at /r/programming: http://www.reddit.com/r/programming/comments/25okot/transactional_memory_history_and_development/
Technically, `parseLaTeX` is total, yes. But I am interpreting the `Left` values as errors here. What I mean is that not every value of type `Text` can produce a value of type `LaTeX`.
I think that's because SO is currently in read-only mode...
OK, maybe my terminology is just wonky then. I'd understand it as a "partial" function over Text in the sense that it may produce Errors, but not over valid LaTeX files.
The original location cancelled their booking so we're temporarily hosting the event, and our largest conf room holds 12 (20 if you squeeze, which we will). So, there's a temporary wait list past 20, and it's probably a good idea to RSVP now on Meetup. Paul (OP) is looking for an alternative location. If you're reading this and you have a large conference room and you're willing to lend it, please get in touch with him.
OK. I see what you are saying now. Is it more clear if I rewrite it like this? &gt; `parseLaTeX` is a partial function that is only defined when the input is a valid LaTeX file.
Very good first project. Things I like: * Separate carefully chosen modules. * Small UNIX-like functions. * The program is librarified: `Main` just imports `REPL`. * Prompting is abstracted away. * State transition is done as pure functions. Things I would change/recommend: * Put all the game state in one data type rather than n parameters for each thing (this is a natural evolution from what you have here as the program increases in size), so `GameStateMod` would be `GameState -&gt; GameState`. * There is a library for this case-sensitive operation defined [here](https://github.com/UndeadMastodon/HaskellAdventure/blob/master/DataTypes.hs#L81-85) called [case-insensitive](http://hackage.haskell.org/package/case-insensitive). It's good because for the data types that are always case-insensitive, you put them in the CI type and now `(==)` upon that type is insensitive. * I would bang all the fields of your types, because you probably don't want laziness in any of them. So `{ field :: Type }` becomes `{ field :: !Type }`. * I would use `Data.Text` instead of `String` here because it's a good habbit, there are better text manipulation facilities for `Text`, and any modern quality library you use will presume `Text`. * Avoid code [like this](https://github.com/UndeadMastodon/HaskellAdventure/blob/master/REPL.hs#L28). If code should never return `Nothing` then the invariant has been satisfied earlier on in the code, so you should have no need to do a lookup. Keep the room and key in a tuple or make the key accessible via the room or whatever. * A library like [haskeline](http://hackage.haskell.org/package/haskeline) might come in handy later on if you want history and better line management or completion.
I think you may be missing the point of the paper. Their abstract said it best: &gt; Haskell provides type-class-bounded and parametric polym orphism as opposed to subtype polymorphism of object-oriented languages such as Java and OCaml. It is a contentious question whether Haskell 98 without extensions, or with com mon extensions, or with new extensions can fully support conventional object-orie nted programming with encap- sulation, mutable state, inheritance, overriding, static ally checked implicit and explicit subtyping, and so on. They proceed to answer that "contentious question." I didn't get the impression that the paper was making any normative claims on whether one should use such a system. That's obviously up to an individual programmer.
Thank you so much! That has been the single most annoying thing about Haskell thus far, now I don't have deal with it any more :-)
@bos may have a point regarding lectures: http://news.sciencemag.org/education/2014/05/lectures-arent-just-boring-theyre-ineffective-too-study-finds Regardless, my preference is always for notes or a paper. It's easier to me to learn from such materials rather than a lecture, video or otherwise. I do like the fact that he's making his notes available. 
I'll consider it, writing a VM or emulator seems like an interesting Haskell project.
Hey, I'm the author. Actually I didn't hit any wall and I really liked Scotty. The reason why I didn't write the app using it is that I wanted to move on to a bigger framework to try it out, since Scotty and Simple and about the same level of abstraction. Currently I'm trying to write that app in Yesod, just for the sake of learning.
Over 8 hours for me.
Oh ... actually i didn't notice that :(
From what I understand, the best way is to assign the Base associated type to the non-fixed datatype and it automatically converts between the two; you write your catamorphisms for the free type and use the normal type like, well, normal.
What's the connection between "object-oriented programming" and "mutable state"?
What are you measuring exactly?
Oh, too bad we missed you! Our team was over there a few months ago.
Just to add to Rehno's comment we (CircuitHub) would love to connect with other web companies using Haskell in production. 
The `Fix` in recursion-schemes is a newtype. I expect with optimization things should inline and give more or less the same code, but you'd have to check.
excellent article, I'm happy to see this approach gaining some more mainstream attention.
If you guys (or anyone else who's interested in visiting a "haskell in production" company) ever come to London give me a shout. http://fynder.io is building everything with Haskell in the backend and clojurescript in the front. I'm on ben &lt;at&gt; the domain above.
Or you could newtype-wrap the whole thing and derive all the instances you want to export except the MonadIO, then only export the newtype, but not its constructor.
Constructing and deconstructing the tree in several ways. Nearest Neighbor, 5 nearest neighbors and all points in a radius around the query point: https://github.com/fhaust/kdtree/blob/master/bench/MainBench.hs
Ah, grasshopper, what's the difference between (as in any 2) dot, spot, and lot?
You can contact me Jonathan Fischoff at IMVU. My user name &lt;at&gt; imvu.com
I don't understand what you are trying to say.
I'd be happy to take patches that improve the performance of the `recursion-schemes` package. Ultimately it just looks like it is a harder to optimize form when you get down to the core involved.
Lenses is the typical way. They are complicated, but somehow I'm getting used to them. So you can just go &gt; (10,(1,20)) ^. (_2 . _1) 1 &gt; (10,(1,20)) &amp; (_2 . _1) .~ "hello" (10,("hello",20)) Which (I believe) has the same performance as you would get using normal haskell syntax.
Not in a way that will let you sleep in the morning, no. The sanest way is that you can write a one-off combinator `io :: IO a -&gt; MyMonad a` or something, and code with that instead of `MonadIO MyMonad`. Methods involving constraints and unsafely casting dictionaries in local scope can also be used, but it isn't an approach I'd wish on an enemy, let alone a friendly Haskeller. =)
You might want to check on the proceedings at bayhac, a Haskell conference that is happening in the area this weekend.
The atomic action of updating a record isn't slow, but it's more challenging to update multiple "locations" in a nested record at once. It can lead to tending toward multiple traversals of your data structure when only one is strictly necessary. Lenses manage to make single traversal updates convenient to work with.
Same as Ben, but we (IrisConnect) are in Brighton. Drop me a line at alfredo.dinapoli &lt;at&gt; thepopularemailfromgoogle
Thanks for the feedback. I stick to the current name for now. I'll do a poll later if appropriate.
Just to be sure: * What is the expected use case for `recursion-schemes`, the `Thing` + `ThingF` + `Base instance` or the `ThingF alone` case? * Should expect the `recursion-schemes` to be as fast as explicit recursion?
I wouldn't say that "maximum" function is that hacky really. If we restrict ourselves to natural numbers(perhaps that's the "ugly" part), ints form a monoid with `max`, and 0 is the identity, so `maximum = foldl max 0` is the most sensible implementation.
`recursion-schemes` supports both primarily because using an explicit `Fix` form is awful for most users, but making up two data types is awful for other users. I would expect there to be overhead to `recursion-schemes`. It was written to document the style, not as a particularly pragmatic 'speed demon' of a library.
DOX is all haskell on the backend and typescript in the front end. email me (max &lt;AT&gt; usedox DT com) to set up a time. We are in S.F. (Soma / South Park) so very close to the caltrain and muni and would love to have you come by.
Better to adjoin a negative infinity perhaps? Or just use `minBound`?
Yeah, I started to rewrite something using `mempty` and `mappend`, then stopped and just banged out the function you see in the blog. That's why I termed it hacky. I should probably just re-write it. That said, the type being folded over is `Double`. Only the weights are restricted to `Nat`. If you couldn't index the subproblems with integers, I think that would preclude dynamic programming as a solution, no?
I don't suppose that there's anyone in the North East of England...
The integers^* form a monoid with `&lt;Z,-∞,max&gt;`, known as a [Tropical monoid](http://en.wikipedia.org/wiki/Tropical_geometry), available in [Data.Monoid.Inf](http://hackage.haskell.org/package/monoid-extras-0.3.2.1/docs/Data-Monoid-Inf.html). This is also a specific case of the fact that any semigroup can be made into a monoid by adjoining an element and defining it to be identity, which is provided by `Data.Semigroup.Option` (the `Monoid` instance for `Maybe` has a constraint which prevents this). \* And the rationals, and the reals, etc.
Getting something like https://github.com/hvr/multi-ghc-travis working for jhc and Travis could help.
&gt; if foo-1.2 does not provide upper bounds on it's dependency bar, the constraint solver is perpetually "poisoned" because foo-1.2 will always be a candidate even long after bar has become incompatible with foo-1.2 This is a very good point which I never realised before.
As a user, I still have no idea whether I should be using JHC or AJHC. Can anyone highlight the major differences?
Fwiw, I'd really hope that `BangPatterns` finally made it into the Haskell report in the next round.
You don't need to know anything about monads to use a `Maybe` type even though it is a monad, it is possible to achieve the same thing with a function `Maybe a -&gt; a`: &lt;Iceland_jack&gt; @hoogle Maybe a -&gt; a &lt;lambdabot&gt; Data.Maybe fromJust :: Maybe a -&gt; a but as u/chrisdoner mentioned you should avoid partial functions (like `fromMaybe`) and deal with the invariant some other way.
The benchmark has already had the glaring errors in it fixed; Haskell programs should do better the next time they post the results.
I did enjoy the 'fuck' section
Hackage already supports deprecating versions, but I think there are issues with cabal-install (maybe it is just older versions?) not using it. Rather than implicit blacklisting we should be able to make this explicit deprecation system work. One could be prompted on upload to deprecate older versions when version bumping is detected, or that could just be automatically done. Almost all the problems with cabal I view as a lack of tooling/automation that a community of programmers should be able to fix by programming.
Look up the following in google: "mutator c++" or "mutator java." 
A related proposal that could work is if foo-2 has a bound on bar, and foo-1 does not, then the bound on bar in foo-2 percolates back implicitly in the solver. I.e. we assert that for any given packages, the bounds on packages it depends on should be monotone increasing, and if they are not then we "fix" the past in the obvious way when attempting to resolve constraints. This behaviour might be a "gotcha" but I can't think of a case where it would be _wrong_.
Hmmm, this seems like a fragile thing to me. For one, the example of foo-2 and foo-1 is easy, but what about foo-1.3.8 and f-2.7.2? Second, and probably most importantly, if you percolate back, you're messing with build reproducibility. With implicit blacklisting, we have a single policy that essentially says, for every unique API that your package has put on hackage, there is one and only one representative version of that API that should be used at any one time. (Unless the user specifically requests otherwise by choosing bounds that rule out the one that would have been selected, or forcing a specific one.) If there is a problem with the representative that is currently being chosen, then you upload a patch release for the a.b.c in question that fixes the behavior. And doing so doesn't mutate any of the previously released versions.
This isn't messing with build reproducibility -- anything you specify explicitly you can still specify explicitly. This just messes with the behaviour of the _solver_. And as long as you rely on the solver rather than specifying packages explicitly, then you're never really reproducible, as the solver may produce varying results depending on the version of cabal, the state of hackage, the phase of the moon, etc.
I don't even think it accomplishes that.
Ahh, right. Implicit blacklisting would also be implemented solely as behavior of the solver, so they're the same in that regard. The difference I see is that implicit blacklisting is more general than percolating bounds. Percolating is just about the behavior of bounds. Implicit blacklisting is a general policy about the management of multiple APIs over the lifecycle of projects.
I've enjoyed all of jekor's videos that I've seen. He has a couple introductory videos on Fay, although they're more about the inner workings: https://www.youtube.com/watch?v=hS52OdD6mdo https://www.youtube.com/watch?v=PnMxT3cjYt8
comfort doesn't come quickly. You just have to write a ton of it and read a ton of it.
Beginning to read and go through some of the Functional Pearls in detail was a turning point for me. They tend to exhibit really great Haskell style, be fairly compartmentalized, and are aimed to teach a concept.
Apart from the books in the sidebar, it may sound obvious but reading through the [GHC User Guide](https://www.haskell.org/ghc/docs/7.8.2/html/users_guide/index.html) and the [Haskell 2010](https://www.haskell.org/onlinereport/haskell2010/) spec was something I put off for a long time and looking back would have saved me a lot of time. They don't tend to index well on Google for some reason though.
Project Euler: https://projecteuler.net/ Sounds simplistic, but it struck the perfect balance of simple, rapid positive feedback, wide coverage of basic features, etc. I did a few dozens and finally managed to get through the first learning "wall".
Instead of feeling confortable, you can try to find how beautiful Haskell is.
I'm addicted to projecteuler, I've done a punch of them in other languages before, and now solving new ones with Haskell, solved 7 with Haskell so far my favourite part about PE is getting into the problem's thread after solving it and getting disappointed
&gt; Functional Pearls just got into reading about them, this might be my new favourite thing, found this interesting book: Pearls of Functional Algorithm Design thx
I already did, it's what keeping me going.
For me it was converting python utility scripts and reading real world haskell.
I had completely forgotten about these videos, and I don't think I ever saw the second one :O Good stuff! 
Great book.
I was thinking about visiting the crossroads. That might help.
I think I got comfortable using haskell the first time I had to change the way a small feature in a big service worked. I had to change the way a value was parsed, and then in a completely different chunk of code, use the additional information that resulted from the parsing. It didn't take all that much time to insert what I needed at those two points - and then the compiler listed everywhere I needed to fix up in passing for those changes. I fixed those places, and the new feature worked. That was the first time I really had a sense of comfort with what the compiler was doing for me. It really did have my back. It really was there to help me, if I gave it the tools. That was quite a long time before I fully understood even all the basics of Haskell. But you asked when I was comfortable using it, not when I understood it. :)
I learned myself by just writing code and expecting the compiler to explode, which to my surprise didn't happen that much :-), and then starting to send pull requests. It depends on how you like to learn, which isn't an helpful answer so let's make a list with some things I can think of: * The wiki contains a lot of information, it's structured more as reference material than a tutorial * Blog posts/tutorials, ocharles did a great job, and the wiki section [Fay in the wild](https://github.com/faylang/fay/wiki#fay-in-the-wild) contains more entries like these that are meant for beginners, of varying depth and focus * The youtube videos toujw mentioned, for a deep dive into the internals, it shows how the compiler is structured and how to read the generated output * Existing fay packages * I wrote [fay-uri](https://github.com/faylang/fay-uri/blob/master/src/Uri.hs) to be an introduction to the FFI, it's pretty well documented and you can read the source top-to-bottom. * [fay-jquery](https://github.com/faylang/fay-jquery/blob/master/src/JQuery.hs) contains the most FFI trickery, without much explanation, it has guided a lot of the features of the language Some other things I can recommend: If you are using a web framework, start without using any of the existing integration libraries (yesod/snaplet/happstack-fay etc). Start small and run the compiler manually (or use a Makefile) to get a feel for the flags and settings you can use. You can always switch later. It is just Haskell after all, so syntax wise there should be no surprises. Some syntax is not supported, but he compiler will most of the time tell you if you are in unimplemented territory. Also see [What is not supported?](https://github.com/faylang/fay/wiki/What-is-not-supported%3F) The FFI is the source of most of the debugging I do, fay blindly trusts that you get the type signatures correct here since there are no types in JS land. Don't be afraid to run the generated JS in the browser and stick debuggers in there. The goal is of course that you shouldn't need to care about this, but it can help a lot, especially when there's some mismatched type in the FFI. Perhaps most important of all: Consider helping out with developing Fay itself! Fay is still a relatively small compiler, and while it has its intricacies I hope that it's structured in such a way that people can understand the code and make changes to it. This is the best way to learn anything in my opinion. Don't let the abstraction boundary stop you, go deep. This really applies to any programming project. I'm always happy to answer questions and get suggestions for where documentation can be improved. 
Working on Haskell full-time
As with anything, practise is key.
I don't think so. It's not partial at all! How about "`parseLatex` returns a `Right` if and only if its input is a valid LaTeX file"?
http://lpaste.net/104242
Also, I have no knowledge of what flags are turned on because ***I*** have only done: &gt; 4\. Installed EclipseFP &gt; 5\. Restarted Eclipse &gt; 6\. Checked the boxes for "Install optional helper executables (...)" and "Install for current user only" &gt; 7\. *Voila*, errors! EDIT TO ADD: If flags have been turned on, it is by the automation script without my knowledge. Again, this points to the not ready for uses other than academic nature I am seeing.
I did a university course about principles of programming languages which involved writing a large number of different interpreters in haskell, using monads to reduce how much code we had to change between each. Once I understood monads, I found I understood enough haskell to actually do interesting stuff/read other people's code. tl;dr: Write a lot of code. 
Just don't use Eclipse.
&gt; Why are you not installing Haskell with Homebrew or Haskell Platform? Allow me to highlight the relevant section from my original post: &gt; **What I was doing (following the instructions in "Beginning Haskell"):** Also, Emacs? If *you* want to use a workflow renowned for RSI, be my guest. I would prefer to be able to still type without pain when I'm 80. EDIT TO ADD: Allow me to ELI5 it: &gt; &gt; Eclipse? Why are you not installing Haskell with *A* or *B*? &gt; &gt; Allow me to highlight the relevant section from my original post: &gt; &gt; **What I was doing (following the instructions in *X*):** Why the fuck is *A* or *B* relevant to *X* when to have a problem with *X* means you have already successfully completed either *A* or *B*? 
Publishing a book about software without having an accompanying website that keeps it updated &amp; has discussion, feedback etc, is screams "unreliable information" nowadays. As another comment said, look for another resource.
I would try not using eclipseFP - I've always found it outdated. Instead install the haskell-platform and use a text editor - write some haskell code and then load it up with the repl, ghci, so you can play around with the functions you just declared. 
You will get very familiar with `ExitFailure 1`, trust me.
What are the ways the Haskell community have settled on to get better diagnostic messages out of the compiler?
`ExitFailure 1` usually have decent diagnostic errors up to 10 lines above.
Which Haskell did you install? Which "Beginning Haskell" did you follow? What version of Xcode is this? There are some stupid incompatibilities with some versions of GHC and newer versions of Clang, which can really ruin your day if you're using Mac OS X. Installing with Homebrew (which IIRC will install and use gcc) or installing GHC 7.8.2 should fix that. These instructions are known to be good: http://bit.ly/install-ghc
Could you delete these comments and repost them using a pastebin service such as [lpaste](http://lpaste.net/)? They make the discussion hard to navigate!
https://www.fpcomplete.com/ can be nice to start experimenting productively with a viable IDE. Offline IDEs are often broken in dependency hell. Probably the book author is a bit too much optimistic guy.
Which in these cases refers to warnings, which shouldn't hinder the build process. Either the build stops at warnings (because EclipseFP does something weird with the build flags) or there's something else going wrong earlier than those 10 lines.
Could you link to some of your favorite category-theoretic/type-theoretic tutorials?
I got comfortable with Haskell by writing a compiler for it. Otherwise, how would I have been able to run any programs?
Yep, I'll gladly help if I can.
Since it seems to be causing confusion, I am going to change it. I know the function is total strictly speaking. I was just trying to convey in few words that ``parseLaTeX`` only produces ``LaTeX`` values when the input is a LaTeX file, without talking about sum types. I am using ``Either`` there just to let the user know that a function ``parseLaTeX :: Text -&gt; LaTeX`` can't be total, and giving them a reason (a parse error) for each unparsable input. This is, of course, an interpretation. It's like when people use ``Maybe`` to say that a function does not always return a value. Well, it *does* return a value: ``Nothing``! PS: Not to mention that considering ``Right``s and ``Left``s values would make the idea of an inverse function harder to introduce, which is the actual point of that sentence.
Hacking on [uu-parsinglib](http://hackage.haskell.org/package/uu-parsinglib) helped me to get over the "but what are type classes argh"-anxiety (though the patches never made it).
This, but without the jokes. An intimate understanding and intuition about the execution model of a language is the *single most important thing* in gaining what I would call comfort in a language. When I read a program, I can see how that execution model would operate on that program-- I *imagine* that implementation operating on the program.
Having to write a compiler to write programs was just jokingly stated, it was totally true. There were no Haskell compilers available back then. 
Pop in to Facebook? I can put you in touch with the folks in the MPK office working with Haskell.
 $ cabal install exceptions Resolving dependencies... Configuring exceptions-0.6.1... /var/folders/3r/gvk584k50jb253024p4wxy3r0000gn/T/5187.c:1:12: warning: control reaches end of non-void function [-Wreturn-type] int foo() {} ^ 1 warning generated. Building exceptions-0.6.1... Preprocessing library exceptions-0.6.1... &lt;command line&gt;: cannot satisfy -package-id mtl-2.1.2-94c72af955e94b8d7b2f359dadd0cb62 (use -v for more information) Failed to install exceptions-0.6.1 cabal: Error: some packages failed to install: exceptions-0.6.1 failed during the building phase. The exception was: ExitFailure 1 $ What flags can I add to that to get a better error message? EDIT TO ADD: Again if there is another layer automatically adding flags to the above command, I know nothing about it. This includes if Haskell or cabal uses a config file. I am on a — to my knowledge — completely stock install of Haskell for Mac OS.
 cabal install -v exceptions May give more info about why the mtl package cannot be satisfied when it seems to have been installed.
First of all, you're being unpleasant which isn't helping you get help. Second, use pastebin or gist instead of reddit to paste code then paste links here. Third, why were you not installing using the big fat "download Haskell" button on the homepage for this language? First lesson: don't rely on outdated books. The source is Haskell.org.
&gt; Hackage already supports deprecating versions, but I think there are issues with `cabal-install` (maybe it is just older versions?) not using it. I believe you're referring to [cabal#1792](https://github.com/haskell/cabal/issues/1792) 
 $ cabal install -v exceptions Reading available packages... Choosing modular solver. Resolving dependencies... Ready to install exceptions-0.6.1 Waiting for install task to finish... Extracting /Users/person/Library/Haskell/repo-cache/hackage.haskell.org/exceptions/0.6.1/exceptions-0.6.1.tar.gz to /var/folders/3r/gvk584k50jb253024p4wxy3r0000gn/T/exceptions-0.6.1-5256... Updating exceptions.cabal with the latest revision from the index. Configuring exceptions-0.6.1... Dependency base ==4.6.0.1: using base-4.6.0.1 Dependency mtl ==2.1.2: using mtl-2.1.2 Dependency transformers ==0.3.0.0: using transformers-0.3.0.0 /var/folders/3r/gvk584k50jb253024p4wxy3r0000gn/T/5256.c:1:12: warning: control reaches end of non-void function [-Wreturn-type] int foo() {} ^ 1 warning generated. Using Cabal-1.16.0 compiled by ghc-7.6 Using compiler: ghc-7.6.3 Using install prefix: /Users/person/Library/Haskell/ghc-7.6.3/lib/exceptions-0.6.1 Binaries installed in: /Users/person/Library/Haskell/ghc-7.6.3/lib/exceptions-0.6.1/bin Libraries installed in: /Users/person/Library/Haskell/ghc-7.6.3/lib/exceptions-0.6.1/lib Private binaries installed in: /Users/person/Library/Haskell/ghc-7.6.3/lib/exceptions-0.6.1/libexec Data files installed in: /Users/person/Library/Haskell/ghc-7.6.3/lib/exceptions-0.6.1/share Documentation installed in: /Users/person/Library/Haskell/ghc-7.6.3/lib/exceptions-0.6.1/doc Using alex version 3.0.5 found on system at: /usr/bin/alex Using ar found on system at: /usr/bin/ar No c2hs found Using cpphs version 1.18.1 found on system at: /Users/person/Library/Haskell/bin/cpphs No ffihugs found Using gcc version 4.2.1 found on system at: /usr/bin/gcc Using ghc version 7.6.3 found on system at: /usr/bin/ghc Using ghc-pkg version 7.6.3 found on system at: /usr/bin/ghc-pkg No greencard found Using haddock version 2.13.2 found on system at: /usr/bin/haddock Using happy version 1.18.10 found on system at: /usr/bin/happy No hmake found Using hpc version 0.6 found on system at: /usr/bin/hpc Using hsc2hs version 0.67 found on system at: /usr/bin/hsc2hs Using hscolour version 1.20 found on system at: /Users/person/Library/Haskell/bin/HsColour No hugs found No jhc found Using ld found on system at: /usr/bin/ld No lhc found No lhc-pkg found No nhc98 found No pkg-config found Using ranlib found on system at: /usr/bin/ranlib Using strip found on system at: /usr/bin/strip Using tar found on system at: /usr/bin/tar No uhc found creating dist/build creating dist/build/autogen Building exceptions-0.6.1... Preprocessing library exceptions-0.6.1... Building library... creating dist/build /usr/bin/ghc --make -fbuilding-cabal-package -O -odir dist/build -hidir dist/build -stubdir dist/build -i -idist/build -isrc -idist/build/autogen -Idist/build/autogen -Idist/build -optP-include -optPdist/build/autogen/cabal_macros.h -package-name exceptions-0.6.1 -hide-all-packages -package-db dist/package.conf.inplace -package-id base-4.6.0.1-6c351d70a24d3e96f315cba68f3acf57 -package-id mtl-2.1.2-94c72af955e94b8d7b2f359dadd0cb62 -package-id transformers-0.3.0.0-ff2bb6ac67241ebb987351a3db564af0 -XHaskell98 Control.Monad.Catch Control.Monad.Catch.Pure -Wall -fwarn-tabs -O2 &lt;command line&gt;: cannot satisfy -package-id mtl-2.1.2-94c72af955e94b8d7b2f359dadd0cb62 (use -v for more information) Failed to install exceptions-0.6.1 World file is already up to date. cabal: Error: some packages failed to install: exceptions-0.6.1 failed during the building phase. The exception was: ExitFailure 1 $ Hope that helps. I don't know what to make of it. :-/
&gt; Third, why were you not installing using the big fat "download Haskell" button on the homepage for this language? Okay… let me spell it out for you, slowly: **I** **HAVE** **ALREADY** **INSTALLED** **HASKELL** **PLATFORM!**
They may be trivial, but they're important enough that `lens` uses them to define `Getter`s.
You're still being a terd. If you had already "spelled that out" in your post then it had been buried in the wall of text that you pasted in reddit instead of lpaste. Additionally, I don't think you did spell that out, you just listed your steps to installing Eclipse but claimed that to be your first experience with installing Haskell so I assumed it was all being done by Eclipse.
Neither do I! It seems cabal finds a mtl package to use, but when it passes the exact reference of the package (with a trailing hash to indicate the specifics of your install), ghc doesn't find it. At least it's nothing to do with EclipseFP, which is a relief (for me :-p). I suggest you post that to the Haskell-Cafe mailing list if nobody has any clue here. I know that MacOS is a bit weird sometimes with PATH and such.
Done! http://lpaste.net/104242
&gt; I suggest you post that to the Haskell-Cafe mailing list if nobody has any clue here. Okay, I think I'll do that. What is the URL to said mailing list? Again, thanks! :-)
haskell-cafe@haskell.org ([info](http://www.haskell.org/mailman/listinfo/haskell-cafe))
http://www.haskell.org/mailman/listinfo/haskell-cafe
Project Euler projects, with their primary emphasis on mathematics, are especially well-suited for functional programming. But for me to become comfortable with using Haskell for my professional work, I will need something much different. I still have very little idea how I would go about putting together any of the applications that I've written in C++, C#, Ruby, or Python in the last 27 years. Without that understanding, Haskell is practically a non-starter for me.
I think if you have a google account, you can correspond with -cafe at https://groups.google.com/forum/#!forum/haskell-cafe but it is if anything less practical than this venue. If you can get someone interested you will probably get better realtime advice on freenode irc #haskell. There seem to be two different problems. I don't follow the last bit, where the hash for mtl is not found. Can you do ghc-pkg check or maybe ghc-pkg list mtl and see what happens. I don't see how you could have deleted but not unregistered a version of mtl though, but the error is similar to what would arise that way. The other earlier problem, which arose in the other build attempts you pasted, is the familiar recent os x cpp problem. Note that the cpp errors use the familiar clang caret. int foo() {} ^ I have not been having any trouble with ghc-7.8.2 (marked ghc-7.8.2-x86_64-apple-darwin-mavericks.tar.bz2 ) from http://www.haskell.org/ghc/download_ghc_7_8_2 and the recent xcode on 'mavericks' (though if I understand, esoterica may arise. One way of looking at the matter is that the Haskell Platform you are installing predates Apple's rejection of letting the 'real' gcc into the 'command line tools' that come with XCode. In general, I would avoid the haskell platform installer for os x; it is not up to date and uses a strange os-x only method of installing. The only advantage to using it over the unpacking the ghc tarball locally + cabal installing stuff locally is that it handles installing a few delicate gui libraries. Just follow the sort of advice given by cartazio and yitz here http://www.reddit.com/r/haskell/comments/25d8f2/how_can_i_install_a_second_version_of_ghc/ Just unpack and do ./configure --prefix=/Users/person/ghc78 make install then add export PATH=/Users/person/ghc78/bin:$PATH to `~/.profile`, one way or another. Then the new ghc will be ahead of the old one in Libraries/Haskell. You can use the extra executables needed for building, cabal, alex, happy, cpphs etc. There will immediately be a delicacy about where the Haskell Platform is telling `cabal install` to install things. In the end you want a new `cabal install` located in `.cabal/bin` along with alex happy cpphs etc., and to scrap this Haskell Platform, which among other things puts you out of communication with other *nix users and thus out of range of advice on #haskell, but I have said enough for the moment. I think before getting involved with something as complicated as Eclipse, you should use a simple editor like sublime or textmate or vim or even nano, and get accustomed to the commands ghc -O2 Hello.hs -o hello cabal install exceptions cabal sandbox init, cabal sandbox &lt;other stuff&gt;, cabal sandbox delete ghc-pkg list ghc-pkg unregister exceptions Then you will quickly comprehend the peculiarities of the ecosystem which is fairly simple but slightly irritating to learn and will then better understand what you are seeing with eclipse. 
I get that. For me though, the practical bits in Haskell are mostly about libraries -- parsing JSON, messaging the outside, etc. Once you've grokked monads (not that hard for me), laziness (not that ha... OH CRAP! NOW I GET IT!) and purity (really annoying before I found Debug.Trace, and even then), using libraries (for me) is a big... meh. Nowhere near the brain-warp that monads, laziness and purity. So, comfort ;) (for me)
 $ ghc-pkg check Warning: haddock-interfaces: /Library/Haskell/ghc-7.6.3/lib/haskell-platform-2013.2.0.0/doc/html/haskell-platform.haddock doesn't exist or isn't a file Warning: haddock-html: /Library/Haskell/ghc-7.6.3/lib/haskell-platform-2013.2.0.0/doc/html doesn't exist or isn't a directory $ ghc-pkg list mtl /Library/Frameworks/GHC.framework/Versions/7.6.3-x86_64/usr/lib/ghc-7.6.3/package.conf.d mtl-2.1.2 /Users/person/.ghc/x86_64-darwin-7.6.3/package.conf.d $ &gt; I don't see how you could have deleted but not unregistered a version of mtl though, but the error is similar to that. From a previous comment: &gt; I am on a — to my knowledge — completely stock install of Haskell for Mac OS. That is, it is a clean install (from http://www.haskell.org/platform/mac.html ) and if something has been deleted, it wasn't by me. As for building Haskell from source, I'll do it if I have to — but it is not how I was wanting to spend my Saturday (I was wanting to spend it *learning* Haskell, not making sure the Yak is properly shaven and in alignment with the correct phase of the Moon).
I would like to be able to click on a function to inline its definition (just one level, if it is recursive).
Avoid interface bloat, because most IDEs don't
Most of Agda-mode's features. Interactive holes, inference, case split, etc.
Yeah, darn, I still don't know what to make of the mtl thing, then, it looks fine. Unpacking the tarball isn't using source at all; it took from 12:12 to 12:18 including download and unpacking time to continue through 513 cd Downloads/ghc-7.8.2 514 ./configure --prefix=/Users/m/trashghc 515 make install 516 mkdir trash 517 cd trash/ 518 cabal sandbox init 519 cabal install --with-compiler=/Users/m/trashghc/bin/ghc exceptions 520 history where the final result was: M:trash m$ cabal install --with-compiler=/Users/m/trashghc/bin/ghc exceptions Resolving dependencies... Notice: installing into a sandbox located at /Users/m/Downloads/ghc782/trash/.cabal-sandbox Configuring transformers-0.4.1.0... Building transformers-0.4.1.0... Installed transformers-0.4.1.0 Configuring mtl-2.2.0.1... Building mtl-2.2.0.1... Installed mtl-2.2.0.1 Configuring exceptions-0.6.1... Building exceptions-0.6.1... Installed exceptions-0.6.1 
Vim bindings
Partial counterpoint--I would like to be able to do everything without clicking.